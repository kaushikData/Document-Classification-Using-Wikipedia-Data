{"id": "4989310", "url": "https://en.wikipedia.org/wiki?curid=4989310", "title": "Asymmetry of lift", "text": "Asymmetry of lift\n\nAsymmetry of lift describes an aspect of the nature of aerodynamic lift generation along the length of an individual rotor blade of a helicopter.\nThe phenomenon is most easily envisaged when a helicopter is hovering, that is, maintaining a fixed position above a fixed point, and with no wind in the atmosphere.\n\nA helicopter rotor blade is an aerofoil, acting like the wing on an aeroplane being driven through the air to create lift. In the case of a fixed-wing aircraft in normal flight, the entire wing travels forward through the air at the same speed. In the case of helicopter rotor blades, the tips of the blade travel very fast through the air while the parts near the rotor hub travel much more slowly. Because lift increases with speed, the outermost sections of the rotor blades generate more lift than those parts closer to the rotor hub. So a rotor blade is said to be an \"aysmmetric generator of lift\", because of the difference in lift generated along its length.\n\nHelicopter manufacturers try to reduce this differential effect (that is, aim for more equality of lift along the blade length). This has two main aspects:\n\nWhen the helicopter is travelling forwards with respect to the atmosphere, a further phenomenon comes into play, dissymmetry of lift.\n"}
{"id": "17750374", "url": "https://en.wikipedia.org/wiki?curid=17750374", "title": "Benz plane", "text": "Benz plane\n\nIn mathematics, a Benz plane is a type of 2-dimensional geometrical structure, named after the German mathematician Walter Benz. The term was applied to a group of objects that arise from a common axiomatization of certain structures and split into three families, which were introduced separately: Möbius planes, Laguerre planes, and Minkowski planes.\n\nStarting from the real euclidean plane and merging the set of lines with the set of circles to a set of blocks results in an inhomogeneous incidence structure: three distinct points determine one block, but lines are distinguishable as a set of blocks that pairwise mutually intersect at one point without being tangent (or no points when parallel). Adding to the point set the new point formula_1, defined to lie on every line results in every block being determined by exactly three points, as well as the intersection of any two blocks following a uniform pattern (intersecting at two points, tangent or non-intersecting). This homogeneous geometry is called classical inversive geometry or a Möbius plane. The inhomogeneity of the description (lines, circles, new point) can be seen to be non-substantive by using a 3-dimensional model. Using a stereographic projection, the classical Möbius plane may be seen to be isomorphic to the geometry of plane sections (circles) on a sphere in Euclidean 3-space. \n\nAnalogously to the (axiomatic) projective plane, an (axiomatic) Möbius plane defines an incidence structure. \nMöbius planes may similarly be constructed over fields other than the real numbers.\n\nStarting again from formula_2 and taking the curves with equations formula_3 (parabolas and lines) as blocks, the following homogenization is effective: Add to the curve formula_3 the new point formula_5. Hence the set of points is formula_6. This geometry of parabolas is called the classical Laguerre plane (Originally it was designed as the geometry of the oriented lines and circles. Both geometries are isomorphic.)\n\nAs for the Möbius plane, there exists a 3-dimensional model: the geometry of the elliptic plane sections on an orthogonal cylinder (in formula_7). An abstraction leads (analogously to the Möbius plane) to the axiomatic Laguerre plane.\n\nStarting from formula_8 and merging the lines formula_9 with the hyperbolas formula_10 in order to get the set of blocks, the following idea homogenizes the incidence structure: Add to any line the point \nformula_11 and to any hyperbola formula_10 the two points formula_13. Hence the point set is formula_14. This geometry of the hyperbolas is called the classical Minkowski plane.\n\nAnalogously to the classical Möbius and Laguerre planes, there exists a 3-dimensional model: The classical Minkowski plane is isomorphic to the geometry of plane sections of a hyperboloid of one sheet (non degenerated quadric of index 2) in 3-dimensional projective space. Similar to the first two cases we get the (axiomatic) Minkowski plane.\n\nBecause of the essential role of the circle (considered as the non-degenerate conic in a projective plane) and the plane description of the original models the three types of geometries are subsumed to planar circle geometries or in honor of Walter Benz, who considered these geometric structures from a common point of view, Benz planes.\n\n\n\n"}
{"id": "59034115", "url": "https://en.wikipedia.org/wiki?curid=59034115", "title": "Black nor'easter", "text": "Black nor'easter\n\nA Black nor'easter is a persistent and potentially violent north-easterly storm that occurs on the east coast of Australia during spring and summer, usually about two days a year. It is not a convection wind, but a storm system that develops offshore which can last several days. This is heralded by the rapid build-up of dense black cloud that can convert to a gale in well under one hour, and also bringing with it a heavy rainfall event usually accompanied by a thunderstorm. This type of storm was first recorded during the 1800s. \n\nNortheasterly sea breezes are a common occurrence on the NSW coast during summer, generated by temperature imbalances between the sea and the land. A broader synoptic pattern however can greatly intensify these onshore winds, which results in a \"Black Nor'easter\", so-named because of its dark grey to virtually black clouds and the moist gale force winds (around 60 kilometres per hour) brought on by intense high-pressure systems, thus eventually bringing thunderstorms, dramatic rainfall and at times hail. \n\nOn a hot sunny day in Sydney, the land heats up rapidly during the morning. The prevailing early morning wind is a light south-west offshore breeze (a katabatic wind) that blows from land to sea, but as the land heats up a north-east convection wind develops. A typical sea breeze, it starts shortly after sunrise on the coast and gradually pushes inland as the day proceeds, typically reaching the city by mid to late morning and the Greater Western Sydney area by early to mid afternoon. Though it doesn't dramatically decrease the temperatures as the southerly buster does, it would bring heavy rain, thunder and even hail at times. \n\nReferred as an anticyclone intensification, the nor'easter causes a sharp leap in wave heights from the east-northeast for the east coast, conjuring a strong north-east air current and east-north east storm waves over the western Tasman Sea. The winds ofttimes produce large waves and, when combined with an east coast low, they bring forth the most intense storm wave environment. The north-east wind field leading warm moist tropical air to the subtropics and mid-latitudes can combine with a higher level cut off low from the Southern Ocean.\n\nThe 'Black Nor'Easter' swell of June 2016 remoulded areas of the east Australian coastline, with backyards tumbling down into the sea at Collaroy and the houses adjoining them damaged. Bridges, piers, and walkways were felled as well. Almost every north facing beach endured some level of erosion, from unsophisticated scouring of the foredune to waves devouring into the brown dirt. In Cronulla, several large boulders were thrown from beneath the tide zone up onto a rock platform.\n\nDue to the strong winds caused by the Black Nor'easterly storms, a number of people in New South Wales (especially in Sydney) have been killed by falling trees. Furthermore, such storms also bring torrential floods, strike down power lines (where thousands of homes and businesses would be left without electricity) and collapse roofs. \n\nIn February 2010, Sydney received some of the highest and heaviest rainfalls in 25 years which were accompanied by violent thunderstorms and gusty winds that cut out power and damaged homes. The heavy rain was caused by remnants of ex-tropical Cyclone Olga and by a massive amount of moisture blowing in on north-east winds which slammed into a low pressure trough.\n\nOn 18 November 2013, a tornado hit Hornsby, a suburb in the Upper North Shore. The tornado's path was long and wide. The tornado blew off roofs and toppled large trees. The winds in the tornado reached . A total of 12 people were injured in the tornado.\n\n"}
{"id": "4610011", "url": "https://en.wikipedia.org/wiki?curid=4610011", "title": "Bucca tornado", "text": "Bucca tornado\n\nThe Bucca tornado was one of the most violent tornadoes ever observed in Australia, being the first Australian tornado to be officially rated F4 on the Fujita scale. It occurred near the township of Bucca (near Bundaberg) in Queensland on 29 November 1992 at around 2:20pm AEST. The tornado was accompanied by cricket-ball sized hail across Bucca and Bullyard regions.\n\nThe tornado damaged or destroyed nine houses, some flattened to the ground. Trees were snapped and stones were found embedded into tree trunks. Several tree saplings were speared into the walls of homes. A refrigerator from one home was blown away and never found. A 3-ton truck was also thrown and approximately 20 cattle were killed.\n\nThe weather system that spawned the Bucca tornado was also responsible for an F3 tornado at Oakhurst.\n\n"}
{"id": "24551878", "url": "https://en.wikipedia.org/wiki?curid=24551878", "title": "Central Energy Italian Gas Holding", "text": "Central Energy Italian Gas Holding\n\nCentral Energy Italian Inc is a gas distribution and trade company. It is a part of the Central Energy ], a company related to Gazprom. A deal between Italian Eni and Gazprom picked CEI as a partner delivering gas to Italy.\n\nInvestigations by members of the Italian parliament revealed that Bruno Mentasti-Granelli, who is known as a close friend of Italian Prime Minister Silvio Berlusconi, owns 33 percent of CEIGH through two companies, Hexagon Prima and Hexagon Seconda.\n\nThe deal was denounced due to the spreading scandal. Italian parliament blocked the contract, accusing Berlusconi of having a personal interest in the contract.\n\nRobert Nowikovsky, an Austrian lawyer linked to Russian operations, was one of the company's directors.\n\nThe 2008 article \"Berlusconi, Centrex, Hexagon 1 and 2 and Gazprom\" suggested:\nCentral Energy Company Inc\n\n"}
{"id": "47727911", "url": "https://en.wikipedia.org/wiki?curid=47727911", "title": "Chamicero de Perijá", "text": "Chamicero de Perijá\n\nChamicero de Perijá is a small nature reserve in Colombia established in 2014. It is centered on the Colombian side of the Serranía de Perijá mountain range, part of the East Andes.\n\nChamicero de Perijá protects a area of cloud forest habitat which is home to several rare bird taxa, including the Perijá thistletail, the Perijá metaltail, the Perijá brush-finch, an endemic subspecies of the rufous antpitta, and the Perijá tapaculo.\n"}
{"id": "5074859", "url": "https://en.wikipedia.org/wiki?curid=5074859", "title": "CleanEnergy", "text": "CleanEnergy\n\nBMW CleanEnergy vehicles have both a hydrogen and a petrol tank. If one of the tanks is empty, the bivalent BMW engine unit switches over automatically to the other fuel system. One decisive advantage of the combustion engine is bivalence. It enables both hydrogen and petrol to be used, thus creating the ideal conditions for a transition from non-regenerative to regenerative drive energies. During electrolysis water is split into its components of hydrogen and oxygen. The hydrogen is stored and the oxygen is needed in the vehicle for the combustion of the hydrogen. The combustion process generates energy and water as a waste product, which returns to the natural water cycle. And so with the aid of regenerative energy suppliers, a theoretically emission-free fuel is obtained.\n\n"}
{"id": "13852948", "url": "https://en.wikipedia.org/wiki?curid=13852948", "title": "Composite epoxy material", "text": "Composite epoxy material\n\nComposite epoxy materials (CEM) are a group of composite materials typically made from woven glass fabric surfaces and non-woven glass core combined with epoxy synthetic resin. They are typically used in printed circuit boards.\n\nThere are different types of CEMs:\n\nCEM-1 is low-cost, flame-retardant, cellulose-paper-based laminate with only one layer of woven glass fabric.\n\nCEM-2 has cellulose paper core and woven glass fabric surface.\n\nCEM-3 is very similar to the most commonly used PCB material, FR-4. Its color is white, and it is flame-retardant.\n\nCEM-4 quite similar as CEM-3 but not flame-retardant.\n\nCEM-5 (also called CRM-5) has polyester woven glass core.\n\n"}
{"id": "16231600", "url": "https://en.wikipedia.org/wiki?curid=16231600", "title": "Energy minister", "text": "Energy minister\n\nAn energy minister is a position in many governments responsible for energy production and regulation, developing governmental energy policy, scientific research, and natural resources conservation. In some countries, environmental responsibilities are given to a separate environment minister.\n\n\n"}
{"id": "17904175", "url": "https://en.wikipedia.org/wiki?curid=17904175", "title": "Eolica Casimcea Wind Farm", "text": "Eolica Casimcea Wind Farm\n\nThe Eolica Casimcea Wind Farm is a proposed wind power project in Casimcea, Tulcea County, Romania. It will consist of six individual wind farms connected together. It will have 122 individual wind turbines with a nominal output of around 2 MW which will deliver up to 244 MW of power, enough to power over 150,000 homes, with a capital investment required of approximately US$300 million.\n"}
{"id": "53127507", "url": "https://en.wikipedia.org/wiki?curid=53127507", "title": "February 2017 North American blizzard", "text": "February 2017 North American blizzard\n\nThe February 2017 North American blizzard was a fast-moving but powerful blizzard that affected the Northeastern United States with severe weather in the time span of February 8–9. Forming as an Alberta clipper in the northern United States, the system initially produced light snowfall from the Midwest to the Ohio Valley as it tracked southeastwards. It eventually reached the East Coast of the United States and began to rapidly grow into a powerful nor'easter. Up to of snow as well as blizzard conditions were recorded in some of the hardest hit areas before the system moved away from the coastline early on February 10.\n\nPrior to the blizzard, unprecedented and record-breaking warmth had enveloped the region, with record highs of above recorded in several areas, including Central Park in New York City. Some were caught off guard by the warmth and had little time to prepare for the snowstorm, with some meteorologists calling the extreme weather changes \"unprecedented\".\n\nOn February 6, an Alberta clipper developed near the Canada–United States border and dove southeastwards. With relatively little moisture to supply it, initial snowfall totals in the Plains and Midwest were light, ranging from . The system quickly advanced eastwards through the heart of the country, and by 18:00 UTC on February 8, a surface low had developed over the Arkansas–Tennessee state border. This low would become the dominant low of the system as it continued towards the Northeastern United States, while also producing light to moderate snow across the Ohio Valley since there was enough cold air in place from a departing storm system. As the clipper system neared the Tri-State area, it began to steadily intensify and produce more moderate and heavy snow while beginning to make the transition into a nor'easter.\n\nBy 09–12:00 UTC on February 9, the low had begun to move just offshore near the Delaware coastline. As very heavy snow bands (with snowfall rates of in some spots) set up offshore, forecasters noted that the low was likely to rapidly deepen – or in meteorological terms, bomb out – due to the temperature contrast between the cold air to its north and moisture feeding into the storm from the warm waters of the Gulf Stream. Indeed, the storm had undergone this process, meeting the criteria for bombogenesis, with the central pressure dropping from at 18:00 UTC on February 8 to by 18:00 UTC on February 9 while it was offshore the New Jersey coastline. Snow slowly began to subside in areas such as eastern Pennsylvania and western New Jersey. The storm system was absorbed by another system forming off the coast of Greenland on February 11.\n\nOn February 8, winter storm warnings were issued for much of the Northeast, including a blizzard warning for parts of Long Island. More than 2,400 flights were cancelled across the affected regions.\n\nOn February 8, Mayor Bill de Blasio announced that schools in New York City would be closed the following day, due to the impending snowstorm. Advising residents to stay indoors and stay off the roads, Sanitation Commissioner Kathryn Garcia stated, \"This will be a very dangerous storm. Luckily it’s not likely to last very long, only a few hours, but during the period of time that it’s coming down it’ll be extraordinarily difficult to drive. So I want to encourage everyone who can take mass transit to please do that.\" To also avoid a disaster that had happened the year before where several residents were trapped even after the snow subsided due to plows being unable to access certain roads, several smaller equipment and trucks were bought in order to plow the streets in the aftermath. About 2,300 salt spreaders and plows were prepared to plow the streets as of early February 9.\n\nRegarding the sudden change from record-high temperatures to a snowstorm, Mayor Bill de Blasio cited that it was \"unlike anything [I've] seen in my life. What almost feels like a summer day almost, now, and tomorrow a blizzard.\"\n\nSlick roads were responsible for two jackknifed MTA buses early on February 9 in Manhattan, however no injuries were reported. In Long Island, a tractor trailer was invloved\nin an accident with multiple vehicles. No injuries were reported. Blizzard-like conditions were also observed in areas mainly near the freeways such as Interstate 95.\n\nOne death was confirmed when a 59-year old doorman slipped and fell through a glass door in New York City on 93rd Street, causing injuries to his face and neck. He was rushed to the hospital where he was pronounced dead.\n\nInterstate 95 was shut down in both directions on February 9 due to reports of several vehicles stuck on both sides of the road. The Connecticut State Police advised residents on Twitter to \"please stay home if possible\". Bradley International Airport also closed due to the snowstorm, as well as several transit services being temporarily suspended.\n\nOn Interstate 95, multiple sections were also closed due to one report of a jackknifed tractor trailer in West Haven early on February 9. The cause of the crash has not yet been determined. Another section was closed due to a three-vehicle accident on the southbound lanes of the Pearl Harbor Memorial Bridge.\n\nA state legislative meeting between Representatives and Senators was reportedly cancelled as well as all legislative meetings on February 9. Travel and parking bans were also put into place in many towns, including Providence, as well as the cancellations of most schools.\n\nBlizzard conditions were recorded in at least four locations in Rhode Island, including Block Island, North Smithfield, Providence, and Westerly. The criteria for blizzard conditions are met when falling and/or blowing snow drops visibility to or less, and sustained winds reach at least for at least three hours.\n\nSchools in Boston were closed because of the storms. Governor Charlie Baker announced on February 8 a two-hour delay for any non-emergency state employees, in order for crews to quickly treat roads and respond to any crashes that occur.\n\nBlizzard conditions were recorded in at least eight locations in Massachusetts, including Beverly, Boston, Chatham, Hyannis, Marshfield, Martha's Vineyard, New Bedford, and Provincetown.\n\nLate on February 8, Governor Chris Christie announced that all state offices would be closed due to the snowstorm, advising workers to stay home. Many schools were also closed as well.\n\nBy mid-day on February 9, state police had informed news outlets that they had responded to around 145 crashes since midnight. A speed limit was implemented on the Garden State Parkway from exit 125 to the northern state line. Transit services were also suspended.\n\nIn Philadelphia, a snow emergency was declared late on February 8, stating that all parked cars had to be moved off the affected areas.\n\nIn the province of Quebec, 7 people were killed as a result of the storm.\n\nIn Greenland, the blizzard was absorbed by another low pressure system.\n\n"}
{"id": "17843285", "url": "https://en.wikipedia.org/wiki?curid=17843285", "title": "Felsenau power plant", "text": "Felsenau power plant\n\nThe Felsenau power plant () is a hydroelectric power plant located on the river Aar in Bern, Switzerland. It was built in 1909 by the city's utility company, Energie Wasser Bern. After a 1989 modernisation, the turbine hall is now used as a museum. The plant's current power output is 11.5 megawatts.\n\nThe main building was designed by Eduard Locher and Alfred Brunschwyler in a then-popular historicising style as a \"palace of technology\". The 1989 expansion is by Vladimir Grossen and Bea Baumann.\n\n"}
{"id": "3369465", "url": "https://en.wikipedia.org/wiki?curid=3369465", "title": "Ferromagnetic resonance", "text": "Ferromagnetic resonance\n\nFerromagnetic resonance, or FMR, is a spectroscopic technique to probe the magnetization of ferromagnetic materials. It is a standard tool for probing spin waves and spin dynamics. FMR is very broadly similar to electron paramagnetic resonance (EPR), and also somewhat similar to nuclear magnetic resonance (NMR), except that FMR probes the sample magnetization resulting from the magnetic moments of dipolar-coupled but unpaired electrons, while NMR probes the magnetic moment of atomic nuclei that are screened by the atomic or molecular orbitals surrounding such nuclei of non-zero nuclear spin.\n\nFerromagnetic resonance was unknowingly discovered by V. K. Arkad'yev when he observed the absorption of UHF radiation by ferromagnetic materials in 1911. A qualitative explanation of FMR along with an explanation of the results from Arkad'yev was offered up by Ya. G. Dorfman in 1923 when he suggested that the optical transitions due to Zeeman splitting could provide a way to study ferromagnetic structure.\n\nA 1935 paper published by Lev Landau and Evgeny Lifshitz predicted the existence of ferromagnetic resonance of the Larmor precession, which was independently verified in experiments by J. H. E. Griffiths (UK) and E. K. Zavoiskij (USSR) in 1946.\n\nFMR arises from the precessional motion of the (usually quite large) magnetization formula_1 of a ferromagnetic material in an external magnetic field formula_2. The magnetic field exerts a torque on the sample magnetization which causes the magnetic moments in the sample to precess. The precession frequency of the magnetization depends on the orientation of the material, the strength of the magnetic field, as well as the macroscopic magnetization of the sample; the effective precession frequency of the ferromagnet is much lower in value from the precession frequency observed for free electrons in EPR. Moreover, linewidths of absorption peaks can be greatly affected both by dipolar-narrowing and exchange-broadening (quantum) effects. Furthermore, not all absorption peaks observed in FMR are caused by the precession of the magnetic moments of electrons in the ferromagnet. Thus, the theoretical analysis of FMR spectra is far more complex than that of EPR or NMR spectra.\n\nThe basic setup for an FMR experiment is a microwave resonant cavity with an electromagnet. The resonant cavity is fixed at a frequency in the super high frequency band. A detector is placed at the end of the cavity to detect the microwaves. The magnetic sample is placed between the poles of the electromagnet and the magnetic field is swept while the resonant absorption intensity of the microwaves is detected. When the magnetization precession frequency and the resonant cavity frequency are the same, absorption increases sharply which is indicated by a decrease in the intensity at the detector.\n\nFurthermore, the resonant absorption of microwave energy causes local heating of the ferromagnet. In samples with local magnetic parameters varying on the nanometer scale this effect is used for spatial dependent spectroscopy investigations.\n\nThe resonant frequency of a film with parallel applied external field formula_3 is given by the Kittel formula:\n\nwhere formula_5 is the magnetization of the ferromagnet and formula_6 is the gyromagnetic ratio.\n\n\n\n"}
{"id": "27006183", "url": "https://en.wikipedia.org/wiki?curid=27006183", "title": "Gavoshan Dam", "text": "Gavoshan Dam\n\nGavoshan Dam is an embankment dam on the Gaveh River south of Sanandaj in Kordestan province, Iran. It was created for the primary purpose of irrigation but also supports an 11 MW hydroelectric power station. Water from the dam's reservoir is transferred for irrigation in Kordestan and Kermanshah provinces via Gavoshan Water Conveyance Tunnel. About can be transferred annually for the irrigation of . Additionally, water from the reservoir is used to provide drinking water for the city of Kermanshah in the amount of annually. Construction on the dam began in 1992 and was completed in 2004. It was inaugurated by Iran's President Mohammad Khatami on 13 February 2005.\n\n"}
{"id": "238181", "url": "https://en.wikipedia.org/wiki?curid=238181", "title": "Gibbs free energy", "text": "Gibbs free energy\n\nIn thermodynamics, the Gibbs free energy (IUPAC recommended name: Gibbs energy or Gibbs function; also known as free enthalpy to distinguish it from Helmholtz free energy) is a thermodynamic potential that can be used to calculate the maximum of reversible work that may be performed by a thermodynamic system at a constant temperature and pressure (isothermal, isobaric). The Gibbs free energy (ΔGº = ΔHº-TΔSº) (J in SI units) is the \"maximum\" amount of non-expansion work that can be extracted from a thermodynamically closed system (one that can exchange heat and work with its surroundings, but not matter); this maximum can be attained only in a completely reversible process. When a system transforms reversibly from an initial state to a final state, the decrease in Gibbs free energy equals the work done by the system to its surroundings, minus the work of the pressure forces.\n\nThe Gibbs energy (also referred to as \"G\") is also the thermodynamic potential that is minimized when a system reaches chemical equilibrium at constant pressure and temperature. Its derivative with respect to the reaction coordinate of the system vanishes at the equilibrium point. As such, a reduction in \"G\" is a necessary condition for the spontaneity of processes at constant pressure and temperature.\n\nThe Gibbs free energy, originally called \"available energy\", was developed in the 1870s by the American scientist Josiah Willard Gibbs. In 1873, Gibbs described this \"available energy\" as\nThe initial state of the body, according to Gibbs, is supposed to be such that \"the body can be made to pass from it to states of dissipated energy by reversible processes\". In his 1876 magnum opus \"On the Equilibrium of Heterogeneous Substances\", a graphical analysis of multi-phase chemical systems, he engaged his thoughts on chemical free energy in full.\n\nAccording to the second law of thermodynamics, for systems reacting at STP (or any other fixed temperature and pressure), there is a general natural tendency to achieve a minimum of the Gibbs free energy.\n\nA quantitative measure of the favorability of a given reaction at constant temperature and pressure is the change Δ\"G\" (sometimes written \"delta \"G\"\" or \"d\"G\"\") in Gibbs free energy that is (or would be) caused by the reaction. As a necessary condition for the reaction to occur at constant temperature and pressure, Δ\"G\" must be smaller than the non-PV (e.g. electrical) work, which is often equal to zero (hence Δ\"G\" must be negative). Δ\"G\" equals the maximum amount of non-\"PV\" work that can be performed as a result of the chemical reaction for the case of reversible process. If the analysis indicated a positive Δ\"G\" for the reaction, then energy — in the form of electrical or other non-\"PV\" work — would have to be added to the reacting system for Δ\"G\" to be smaller than the non-\"PV\" work and make it possible for the reaction to occur.\n\nWe can think of ∆G as the amount of \"free\" or \"useful\" energy available to do work. The equation can be also seen from the perspective of the system taken together with its surroundings (the rest of the universe). First, assume that the given reaction at constant temperature and pressure is the only one that is occurring. Then the entropy released or absorbed by the system equals the entropy that the environment must absorb or release, respectively. The reaction will only be allowed if the total entropy change of the universe is zero or positive. This is reflected in a negative Δ\"G\", and the reaction is called exergonic.\n\nIf we couple reactions, then an otherwise endergonic chemical reaction (one with positive Δ\"G\") can be made to happen. The input of heat into an inherently endergonic reaction, such as the elimination of cyclohexanol to cyclohexene, can be seen as coupling an unfavourable reaction (elimination) to a favourable one (burning of coal or other provision of heat) such that the total entropy change of the universe is greater than or equal to zero, making the \"total\" Gibbs free energy difference of the coupled reactions negative.\n\nIn traditional use, the term \"free\" was included in \"Gibbs free energy\" to mean \"available in the form of useful work\". The characterization becomes more precise if we add the qualification that it is the energy available for \"non-volume\" work. (An analogous, but slightly different, meaning of \"free\" applies in conjunction with the Helmholtz free energy, for systems at constant temperature). However, an increasing number of books and journal articles do not include the attachment \"free\", referring to \"G\" as simply \"Gibbs energy\". This is the result of a 1988 IUPAC meeting to set unified terminologies for the international scientific community, in which the adjective \"free\" was supposedly banished. This standard, however, has not yet been universally adopted.\n\nThe quantity called \"free energy\" is a more advanced and accurate replacement for the outdated term \"affinity\", which was used by chemists in the earlier years of physical chemistry to describe the \"force\" that caused chemical reactions.\n\nIn 1873, Willard Gibbs published \"A Method of Geometrical Representation of the Thermodynamic Properties of Substances by Means of Surfaces\", in which he sketched the principles of his new equation that was able to predict or estimate the tendencies of various natural processes to ensue when bodies or systems are brought into contact. By studying the interactions of homogeneous substances in contact, i.e., bodies composed of part solid, part liquid, and part vapor, and by using a three-dimensional volume-entropy-internal energy graph, Gibbs was able to determine three states of equilibrium, i.e., \"necessarily stable\", \"neutral\", and \"unstable\", and whether or not changes would ensue. Further, Gibbs stated:\n\nIn this description, as used by Gibbs, \"ε\" refers to the internal energy of the body, \"η\" refers to the entropy of the body, and \"ν\" is the volume of the body.\n\nThereafter, in 1882, the German scientist Hermann von Helmholtz characterized the affinity as the largest quantity of work which can be gained when the reaction is carried out in a reversible manner, e.g., electrical work in a reversible cell. The maximum work is thus regarded as the diminution of the free, or available, energy of the system (\"Gibbs free energy\" \"G\" at \"T\" = constant, \"P\" = constant or \"Helmholtz free energy\" \"F\" at \"T\" = constant, \"V\" = constant), whilst the heat given out is usually a measure of the diminution of the total energy of the system (internal energy). Thus, \"G\" or \"F\" is the amount of energy \"free\" for work under the given conditions.\n\nUntil this point, the general view had been such that: \"all chemical reactions drive the system to a state of equilibrium in which the affinities of the reactions vanish\". Over the next 60 years, the term affinity came to be replaced with the term free energy. According to chemistry historian Henry Leicester, the influential 1923 textbook \"Thermodynamics and the Free Energy of Chemical Substances\" by Gilbert N. Lewis and Merle Randall led to the replacement of the term \"affinity\" by the term \"free energy\" in much of the English-speaking world.\n\nGibbs free energy was originally defined graphically. In 1873, American scientist Willard Gibbs published his first thermodynamics paper, \"Graphical Methods in the Thermodynamics of Fluids\", in which Gibbs used the two coordinates of the entropy and volume to represent the state of the body. In his second follow-up paper, \"A Method of Geometrical Representation of the Thermodynamic Properties of Substances by Means of Surfaces\", published later that year, Gibbs added in the third coordinate of the energy of the body, defined on three figures. In 1874, Scottish physicist James Clerk Maxwell used Gibbs' figures to make a 3D energy-entropy-volume thermodynamic surface of a fictitious water-like substance. Thus, in order to understand the very difficult concept of Gibbs free energy one must be able to understand its interpretation as Gibbs defined originally by section AB on his figure 3 and as Maxwell sculpted that section on his 3D surface figure.\n\nThe Gibbs free energy is defined as\n\nwhich is the same as\n\nwhere:\n\nThe expression for the infinitesimal reversible change in the Gibbs free energy as a function of its \"natural variables\" \"p\" and \"T\", for an open system, subjected to the operation of external forces (for instance, electrical or magnetic) \"X\", which cause the external parameters of the system \"a\" to change by an amount d\"a\", can be derived as follows from the first law for reversible processes:\n\nwhere:\n\nThis is one form of Gibbs fundamental equation. In the infinitesimal expression, the term involving the chemical potential accounts for changes in Gibbs free energy resulting from an influx or outflux of particles. In other words, it holds for an open system or for a closed, chemically reacting system where the \"N\" are changing. For a closed, non-reacting system, this term may be dropped.\n\nAny number of extra terms may be added, depending on the particular system being considered. Aside from mechanical work, a system may, in addition, perform numerous other types of work. For example, in the infinitesimal expression, the contractile work energy associated with a thermodynamic system that is a contractile fiber that shortens by an amount −d\"l\" under a force \"f\" would result in a term \"f\" d\"l\" being added. If a quantity of charge −d\"e\" is acquired by a system at an electrical potential Ψ, the electrical work associated with this is −Ψ d\"e\", which would be included in the infinitesimal expression. Other work terms are added on per system requirements.\n\nEach quantity in the equations above can be divided by the amount of substance, measured in moles, to form \"molar Gibbs free energy\". The Gibbs free energy is one of the most important thermodynamic functions for the characterization of a system. It is a factor in determining outcomes such as the voltage of an electrochemical cell, and the equilibrium constant for a reversible reaction. In isothermal, isobaric systems, Gibbs free energy can be thought of as a \"dynamic\" quantity, in that it is a representative measure of the competing effects of the enthalpic and entropic driving forces involved in a thermodynamic process.\n\nThe temperature dependence of the Gibbs energy for an ideal gas is given by the Gibbs–Helmholtz equation, and its pressure dependence is given by\n\nIf the volume is known rather than pressure, then it becomes\n\nor more conveniently as its chemical potential:\n\nIn non-ideal systems, fugacity comes into play.\n\nThe Gibbs free energy total differential natural variables may be derived by Legendre transforms of the internal energy.\n\nThe definition of \"G\" from above is\n\nTaking the total differential, we have\n\nReplacing d\"U\" with the result from the first law gives\n\nThe natural variables of \"G\" are then \"p\", \"T\", and {\"N\"}.\n\nBecause \"S\", \"V\", and \"N\" are extensive variables, an Euler integral allows easy integration of d\"U\":\n\nBecause some of the natural variables of \"G\" are intensive, d\"G\" may not be integrated using Euler integrals as is the case with internal energy. However, simply substituting the above integrated result for \"U\" into the definition of \"G\" gives a standard expression for \"G\":\n\nThis result applies to homogeneous, macroscopic systems, but not to all thermodynamic systems.\n\nThe system under consideration is held at constant temperature and pressure, and is closed (no matter can come in or out). The Gibbs energy of any system is and an infinitesimal change in \"G\", at constant temperature and pressure yields:\n\nBy the first law of thermodynamics, a change in the internal energy \"U\" is given by\n\nwhere \"δQ\" is energy added as heat, and \"δW\" is energy added as work. The work done on the system may be written as \"δW\" = −\"PdV\" + \"δW\", where −\"PdV\" is the mechanical work of compression/expansion done on the system and \"δW\" is all other forms of work, which may include electrical, magnetic, etc. Assuming that only mechanical work is done,\n\nand the infinitesimal change in \"G\" is:\n\nThe second law of thermodynamics states that for a closed system, formula_20, and so it follows that:\n\nThis means that for a system which is not in equilibrium, its Gibbs energy will always be decreasing, and when it is in equilibrium (i.e. no longer changing), the infinitesimal change \"dG\" will be zero. In particular, this will be true if the system is experiencing any number of internal chemical reactions on its path to equilibrium.\n\nDuring a reversible electrochemical reaction at constant temperature and pressure, the following equations involving the Gibbs free energy hold:\n\nand rearranging gives\n\nwhich relates the cell potential resulting from the reaction to the equilibrium constant and reaction quotient for that reaction (Nernst equation),\n\nwhere\n\nMoreover, we also have:\n\nwhich relates the equilibrium constant with Gibbs free energy. This implies that at equilibrium\n\nA chemical reaction will (or can) proceed spontaneously if the change in the total entropy of the universe that would be caused by the reaction is nonnegative. As discussed in the overview, if the temperature and pressure are held constant, the Gibbs free energy is a (negative) proxy for the change in total entropy of the universe. It is \"negative\" because \"S\" appears with a negative coefficient in the expression for \"G\", so the Gibbs free energy moves in the opposite direction from the total entropy. Thus, a reaction with a positive Gibbs free energy will not proceed spontaneously. However, in biological systems (among others), energy inputs from other energy sources (including the Sun and exothermic chemical reactions) are \"coupled\" with reactions that are not entropically favored (i.e. have a Gibbs free energy above zero). Taking into account the coupled reactions, the total entropy in the universe increases. This coupling allows endergonic reactions, such as photosynthesis and DNA synthesis, to proceed without decreasing the total entropy of the universe. Thus biological systems do not violate the second law of thermodynamics.\n\nThe standard Gibbs free energy of formation of a compound is the change of Gibbs free energy that accompanies the formation of 1 mole of that substance from its component elements, at their standard states (the most stable form of the element at 25 °C and 100 kPa). Its symbol is Δ\"G\"˚.\n\nAll elements in their standard states (diatomic oxygen gas, graphite, etc.) have standard Gibbs free energy change of formation equal to zero, as there is no change involved.\n\n\n"}
{"id": "37376713", "url": "https://en.wikipedia.org/wiki?curid=37376713", "title": "Heat transfer physics", "text": "Heat transfer physics\n\nHeat transfer physics describes the kinetics of energy storage, transport, and energy transformation by principal energy carriers: phonons (lattice vibration waves), electrons, fluid particles, and photons. Heat is energy stored in temperature-dependent motion of particles including electrons, atomic nuclei, individual atoms, and molecules. Heat is transferred to and from matter by the principal energy carriers. The state of energy stored within matter, or transported by the carriers, is described by a combination of classical and quantum statistical mechanics. The energy is also transformed (converted) among various carriers.\nThe heat transfer processes (or kinetics) are governed by the rates at which various related physical phenomena occur, such as (for example) the rate of particle collisions in classical mechanics. These various states and kinetics determine the heat transfer, i.e., the net rate of energy storage or transport. Governing these process from the atomic level (atom or molecule length scale) to macroscale are the laws of thermodynamics, including conservation of energy.\n\nHeat is thermal energy associated with temperature-dependent motion of particles. The macroscopic energy equation for infinitesimal volume used in heat transfer analysis is\n\nwhere q is heat flux vector, -\"ρc\"(\"∂T\"/\"∂t\") is temporal change of internal energy (\"ρ\" is density, \"c\" is specific heat capacity at constant pressure, \"T\" is temperature and \"t\" is time), and formula_2 is the energy conversion to and from thermal energy (\"i\" and \"j\" are for principal energy carriers). So, the terms represent energy transport, storage and transformation. Heat flux vector q is composed of three macroscopic fundamental modes, which are conduction (q = -\"k\"∇\"T\", \"k\": thermal conductivity), convection (q = \"ρcuT\", u: velocity), and radiation (q = formula_3s \"I\" sin\"θdθdω\", \"ω\": angular frequency, \"θ\": polar angle, \"I\": spectral, directional radiation intensity, s: unit vector), i.e., q = q + q + q.\n\nOnce states and kinetics of the energy conversion and thermophysical properties are known, the fate of heat transfer is described by the above equation. These atomic-level mechanisms and kinetics are addressed in heat transfer physics. The microscopic thermal energy is stored, transported, and transformed by the principal energy carriers: phonons (\"p\"), electrons (\"e\"), fluid particles (\"f\"), and photons (\"ph\").\n\nThermophysical properties of matter and the kinetics of interaction and energy exchange among the principal carriers are based on the atomic-level configuration and interaction. Transport properties such as thermal conductivity are calculated from these atomic-level properties using classical and quantum physics. Quantum states of principal carriers (e.g.. momentum, energy) are derived from the Schrödinger equation (called first principle or \"ab initio\") and the interaction rates (for kinetics) are calculated using the quantum states and the quantum perturbation theory (formulated as the Fermi golden rule). Variety of \"ab initio\" (Latin for from the beginning) solvers (software) exist (e.g., ABINIT, CASTEP, Gaussian, Q-Chem, Quantum ESPRESSO, SIESTA, VASP, WIEN2k). Electrons in the inner shells (core) are not involved in heat transfer, and calculations are greatly reduced by proper approximations about the inner-shells electrons.\n\nThe quantum treatments, including equilibrium and nonequilibrium \"ab initio\" molecular dynamics (MD), involving larger lengths and times are limited by the computation resources, so various alternate treatments with simplifying assumptions have been used and kinetics. In classical (Newtonian) MD, the motion of atom or molecule (particles) is based on the empirical or effective interaction potentials, which in turn can be based on curve-fit of \"ab initio\" calculations or curve-fit to thermophysical properties. From the ensembles of simulated particles, static or dynamics thermal properties or scattering rates are derived.\n\nAt yet larger length scales (mesoscale, involving many mean free paths), the Boltzmann transport equation (BTE) which is based on the classical Hamiltonian-statistical mechanics is applied. BTE considers particle states in terms of position and momentum vectors (x, p) and this is represented as the sate occupation probability. The occupation has equilibrium distributions (the known boson, fermion, and Maxwell–Boltzmann particles) and transport of energy (heat) is due to nonequilibrium (cause by a driving force or potential). Central to the transport is the role of scattering which turn the distribution toward equilibrium). The scattering is presented by the relations time or the mean free path. The relaxation time (or its inverse which is the interaction rate) is found from other calculations (\"ab initio\" or MD) or empirically. BTE can be numerically solved with Monte Carlo method, etc.\n\nDepending on the length and time scale the proper level of treatment (\"ab initio\", MD, or BTE). Heat transfer physics analyses may involve multiple scales (e.g., BTE using interaction rate from \"ab initio\" or classical MD) with states and kinetic related to thermal energy storage, transport and transformation.\n\nSo, heat transfer physics covers the four principal energy carries and their kinetics from classical and quantum mechanical perspectives. This enables multiscale (\"ab initio\", MD, BTE and macroscale) analyses, including low-dimensionality and size effects.\n\nPhonon (quantized lattice vibration wave) is a central thermal energy carrier contributing to heat capacity (sensible heat storage) and conductive heat transfer in condensed phase, and plays a very important role in thermal energy conversion. Its transport properties are represented by the phonon conductivity tensor K (W/m-K, from the Fourier law q = -K⋅∇ \"T\") for bulk materials, and the phonon boundary resistance \"AR\" [K/(W/m)] for solid interfaces, where \"A\" is the interface area. The phonon specific heat capacity \"c\" (J/kg-K) includes the quantum effect. The thermal energy conversion rate involving phonon is included in formula_4. Heat transfer physics describes and predicts, \"c\", K, \"R\" (or conductance \"G\") and formula_4, based on atomic-level properties.\n\nFor an equilibrium potential ⟨\"φ\"⟩ of a system with \"N\" atoms, the total potential ⟨\"φ\"⟩ is found by a Taylor series expansion at the equilibrium and this can be approximated by the second derivatives (the harmonic approximation) as\n\nwhere d is the displacement vector of atom \"i\", and Γ is the spring (or force) constant as the second-order derivatives of the potential. The equation of motion for the lattice vibration in terms of the displacement of atoms [d(\"jl\",\"t\"): displacement vector of the \"j\"-th atom in the \"l\"-th unit cell at time \"t\"] is\n\nwhere \"m\" is the atomic mass and Γ is the force constant tensor. The atomic displacement is the summation over the normal modes [s: unit vector of mode \"α\", \"ω\": angular frequency of wave, and κ: wave vector]. Using this plane-wave displacement, the equation of motion becomes the eigenvalue equation\n\nwhere M is the diagonal mass matrix and D is the harmonic dynamical matrix. Solving this eigenvalue equation gives the relation between the angular frequency \"ω\" and the wave vector κ, and this relation is called the phonon dispersion relation. Thus, the phonon dispersion relation is determined by matrices M and D, which depend on the atomic structure and the strength of interaction among constituent atoms (the stronger the interaction and the lighter the atoms, the higher is the phonon frequency and the larger is the slope \"dω\"/\"d\"κ\"\"). The Hamiltonian of phonon system with the harmonic approximation is\n\nwhere \"D\" is the dynamical matrix element between atoms \"i\" and \"j\", and d (d) is the displacement of \"i\" (\"j\") atom, and p is momentum. From this and the solution to dispersion relation, the phonon annihilation operator for the quantum treatment is defined as\n\nwhere \"N\" is the number of normal modes divided by \"α\" and \"ħ\" is the reduced Planck constant. The creation operator is the adjoint of the annihilation operator, \nThe Hamiltonian in terms of \"b\" and \"b\" is H\"\" = ∑\"ħω\"[\"b\"\"b\" + 1/2] and \"b\"\"b\" is the phonon number operator. The energy of quantum-harmonic oscillator is \"E\" = ∑ [\"f\"(\"κ\",\"α\") + 1/2]\"ħω\"(κ), and thus the quantum of phonon energy \"ħω\".\n\nThe phonon dispersion relation gives all possible phonon modes within the Brillouin zone (zone within the primitive cell in reciprocal space), and the phonon density of states \"D\" (the number density of possible phonon modes). The phonon group velocity \"u\" is the slope of the dispersion curve, \"dω\"/\"dκ. Since phonon is a boson particle, its occupancy follows the Bose–Einstein distribution {\"f\" = [exp(\"ħω\"/\"k\"\"T\")-1], \"k\": Boltzmann constant}. Using the phonon density of states and this occupancy distribution, the phonon energy is \"E\"(\"T\") = ∫D\"(\"ω\")\"f\"(\"ω,T\")\"ħωdω\", and the phonon density is \"n\"(\"T\") = ∫\"D\"(\"ω\")\"f\"(\"ω,T\")\"dω\". Phonon heat capacity \"c\" (in solid \"c\" = \"c\", \"c\" : constant-volume heat capacity, \"c\": constant-pressure heat capacity) is the temperature derivatives of phonon energy for the Debye model (linear dispersion model), is\n\nwhere \"T\" is the Debye temperature, \"m\" is atomic mass, and \"n\" is the atomic number density (number density of phonon modes for the crystal 3\"n\"). This gives the Debye \"T\" law at low temperature and Dulong-Petit law at high temperatures.\n\nFrom the kinetic theory of gases, thermal conductivity of principal carrier \"i\" (\"p\", \"e\", \"f\" and \"ph\") is\n\nwhere \"n\" is the carrier density and the heat capacity is per carrier, \"u\" is the carrier speed and \"λ\" is the mean free path (distance traveled by carrier before an scattering event). Thus, the larger the carrier density, heat capacity and speed, and the less significant the scattering, the higher is the conductivity. For phonon \"λ\" represents the interaction (scattering) kinetics of phonons and is related to the scattering relaxation time \"τ\" or rate (= 1/\"τ\") through \"λ\"= \"uτ\". Phonons interact with other phonons, and with electrons, boundaries, impurities, etc., and \"λ\" combines these interaction mechanisms through the Matthiessen rule. At low temperatures, scattering by boundaries is dominant and with increase in temperature the interaction rate with impurities, electron and other phonons become important, and finally the phonon-phonon scattering dominants for \"T\" > 0.2\"T\". The interaction rates are reviewed in and includes quantum perturbation theory and MD.\n\nA number of conductivity models are available with approximations regarding the dispersion and \"λ\". Using the single-mode relaxation time approximation (∂\"f\"/∂\"t\"| = -\"f\"/\"τ\") and the gas kinetic theory, Callaway phonon (lattice) conductivity model as\n\nWith the Debye model (a single group velocity u, and a specific heat capacity calculated above), this becomes \n\nwhere \"a\" is the lattice constant \"a\" = \"n\" for a cubic lattice, and \"n\" is the atomic number density. Slack phonon conductivity model mainly considering acoustic phonon scattering (three-phonon interaction) is given as\n\nwhere ⟨M⟩ is the mean atomic weight of the atoms in the primitive cell, \"V\"=1/\"n\" is the average volume per atom, \"T\" is the high-temperature Debye temperature, \"T\" is the temperature, \"N\" is the number of atoms in the primitive cell, and ⟨γ⟩ is the mode-averaged square of the Grüneisen constant or parameter at high temperatures. This model is widely tested with pure nonmetallic crystals, and the overall agreement is good, even for complex crystals.\n\nBased on the kinetics and atomic structure consideration, a material with high crystalline and strong interactions, composed of light atoms (such as diamond and graphene) is expected to have large phonon conductivity. Solids with more than one atom in the smallest unit cell representing the lattice have two types of phonons, i.e., acoustic and optical. (Acoustic phonons are in-phase movements of atoms about their equilibrium positions, while optical phonons are out-of-phase movement of adjacent atoms in the lattice.) Optical phonons have higher energies (frequencies), but make smaller contribution to conduction heat transfer, because of their smaller group velocity and occupancy.\n\nPhonon transport across hetero-structure boundaries (represented with \"R\", phonon boundary resistance) according to the boundary scattering approximations are modeled as acoustic and diffuse mismatch models. Larger phonon transmission (small \"R\") occurs at boundaries where material pairs have similar phonon properties (\"u\", \"D\", etc.), and in contract large \"R\" occurs when some material is softer (lower cut-off phonon frequency) than the other.\n\nQuantum electron energy states for electron are found using the electron quantum Hamiltonian, which is generally composed of kinetic (-\"ħ\"∇/2\"m\") and potential energy terms (\"φ\"). Atomic orbital, a mathematical function describing the wave-like behavior of either an electron or a pair of electrons in an atom, can be found from the Schrödinger equation with this electron Hamiltonian. Hydrogen-like atoms (a nucleus and an electron) allow for closed-form solution to Schrödinger equation with the electrostatic potential (the Coulomb law). The Schrödinger equation of atoms or atomic ions with more than one electron has not been solved analytically, because of the Coulomb interactions among electrons. Thus, numerical techniques are used, and an electron configuration is approximated as product of simpler hydrogen-like atomic orbitals (isolate electron orbitals). Molecules with multiple atoms (nuclei and their electrons) have molecular orbital (MO, a mathematical function for the wave-like behavior of an electron in a molecule), and are obtained from simplified solution techniques such as linear combination of atomic orbitals (LCAO). The molecular orbital is used to predict chemical and physical properties, and the difference between highest occupied molecular orbital (HOMO) and the lowest unoccupied molecular orbital (LUMO) is a measure of excitability of the molecules.\n\nIn a crystal structure of metallic solids, the free electron model (zero potential, \"φ\" = 0) for the behavior of valence electrons is used. However, in a periodic lattice (crystal), there is periodic crystal potential, so the electron Hamiltonian becomes\n\nwhere \"m\" is the electron mass, and the periodic potential is expressed as \"φ\" (\"x\") = ∑ \"φ\"exp[\"i\"(g∙x)] (g: reciprocal lattice vector). The time-independent Schrödinger equation with this Hamiltonian is given as (the eigenvalue equation)\n\nwhere the eigenfunction \"ψ\" is the electron wave function, and eigenvalue \"E\"(κ), is the electron energy (κ: electron wavevector). The relation between wavevector, κ and energy \"E\" provides the electronic band structure. In practice, a lattice as many-body systems includes interactions between electrons and nuclei in potential, but this calculation can be too intricate. Thus, many approximate techniques have been suggested and one of them is density functional theory (DFT), uses functionals of the spatially dependent electron density instead of full interactions. DFT is widely used in \"ab initio\" software (ABINIT, CASTEP, Quantum ESPRESSO, SIESTA, VASP, WIEN2k, etc.). The electron specific heat is based on the energy states and occupancy distribution (the Fermi–Dirac statistics). In general, the heat capacity of electron is small except at very high temperature when they are in thermal equilibrium with phonons (lattice). Electrons contribute to heat conduction (in addition to charge carrying) in solid, especially in metals. Thermal conductivity tensor in solid is the sum of electric and phonon thermal conductivity tensors K = K + K.\n\nElectrons are affected by two thermodynamic forces [from the charge, ∇(\"E\"/\"e\") where \"E\" is the Fermi level and \"e\" is the electron charge and temperature gradient, ∇(1/\"T\")] because they carry both charge and thermal energy, and thus electric current j and heat flow q are described with the thermoelectric tensors (A, A, A, and A) from the Onsager reciprocal relations as\n\nConverting these equations to have j equation in terms of electric field e and ∇\"T\" and q equation with j and ∇\"T\", (using scalar coefficients for isotropic transport, \"α\", \"α\", \"α\", and \"α\" instead of A, A, A, and A)\n\nElectrical conductivity/resistivity \"σ\" (Ωm)/ ρ (Ω-m), electric thermal conductivity \"k\" (W/m-K) and the Seebeck/Peltier coefficients \"α\" (V/K)/\"α\" (V) are defined as,\n\nVarious carriers (electrons, magnons, phonons, and polarons) and their interactions substantially affect the Seebeck coefficient. The Seebeck coefficient can be decomposed with two contributions, \"α\" = \"α\" + \"α\", where \"α\" is the sum of contributions to the carrier-induced entropy change, i.e., \"α\" = \"α\" + \"α\" + \"α\" (\"α\": entropy-of-mixing, \"α\": spin entropy, and \"α\": vibrational entropy). The other contribution \"α\" is the net energy transferred in moving a carrier divided by \"qT\" (\"q\": carrier charge). The electron's contributions to the Seebeck coefficient are mostly in \"α\". The \"α\" is usually dominant in lightly doped semiconductors. The change of the entropy-of-mixing upon adding an electron to a system is the so-called Heikes formula\n\nwhere \"f\" = \"N\"/\"N\" is the ratio of electrons to sites (carrier concentration). Using the chemical potential (\"µ\"), the thermal energy (\"k\"\"T\") and the Fermi function, above equation can be expressed in an alternative form, \"α\" = (\"k\"/\"q\")[(\"E\" - \"µ\")/(\"k\"\"T\")].\nExtending the Seebeck effect to spins, a ferromagnetic alloy can be a good example. The contribution to the Seebeck coefficient that results from electrons’ presence altering the systems spin entropy is given by \"α\" = Δ\"S\"/\"q\" = (\"k\"/\"q\")ln[(2\"s\" + 1)/(2\"s\" +1)], where \"s\" and \"s\" are net spins of the magnetic site in the absence and presence of the carrier, respectively. Many vibrational effects with electrons also contribute to the Seebeck coefficient. The softening of the vibrational frequencies produces a change of the vibrational entropy is one of examples. The vibrational entropy is the negative derivative of the free energy, i.e.,\n\nwhere \"D\"(\"ω\") is the phonon density-of-states for the structure. For the high-temperature limit and series expansions of the hyperbolic functions, the above is simplified as \"α\" = (Δ\"S\"/\"q\") = (\"k\"/\"q\")∑\"\"(-Δ\"ω\"/\"ω\").\n\nThe Seebeck coefficient derived in the above Onsager formulation is the mixing component \"α\", which dominates in most semiconductors. The vibrational component in high-band gap materials such as BC is very important.\nConsidering the microscopic transport ( transport is a results of nonequilibrium),\n\nwhere u is the electron velocity vector, \"f\"’ (\"f\") is the electron nonequilibrium (equilibrium) distribution, \"τ\" is the electron scattering time, \"E\" is the electron energy, and F is the electric and thermal forces from ∇(\"E\"/\"e\") and ∇(1/\"T\").\nRelating the thermoelectric coefficients to the microscopic transport equations for \"j\" and q, the thermal, electric, and thermoelectric properties are calculated. Thus, \"k\" increases with the electrical conductivity σe and temperature \"T\", as the Wiedemann–Franz law presents [\"k\"/(\"σT\") = (1/3)(\"πk\"/\"e\") = 2.44×10 W-Ω/K]. Electron transport (represented as \"σ\") is a function of carrier density \"n\" and electron mobility \"μ\" (\"σ\" = \"enμ\"). \"μ\" is determined by electron scattering rates formula_30 (or relaxation time, formula_31 ) in various interaction mechanisms including interaction with other electrons, phonons, impurities and boundaries.\n\nElectrons interact with other principal energy carriers. Electrons accelerated by an electric field are relaxed through the energy conversion to phonon (in semiconductors, mostly optical phonon), which is called Joule heating. Energy conversion between electric potential and phonon energy is considered in thermoelectrics such as Peltier cooling and thermoelectric generator. Also, study of interaction with photons is central in optoelectronic applications (i.e. light-emitting diode, solar photovoltaic cells, etc.). Interaction rates or energy conversion rates can be evaluated using the Fermi golden rule (from the perturbation theory) with \"ab initio\" approach.\n\nFluid particle is the smallest unit (atoms or molecules) in the fluid phase (gas, liquid or plasma) without breaking any chemical bond. Energy of fluid particle is divided into potential, electronic, translational, vibrational, and rotational energies. The heat (thermal) energy storage in fluid particle is through the temperature-dependent particle motion (translational, vibrational, and rotational energies). The electronic energy is included only if temperature is high enough to ionize or dissociate the fluid particles or to include other electronic transitions. These quantum energy states of the fluid particles are found using their respective quantum Hamiltonian. These are H\"\" = -(\"ħ\"/2\"m\")∇, H\"\" = -(\"ħ\"/2\"m\")∇ + Γ\"x\"/2 and H\"\" = -(\"ħ\"/2\"I\")∇ for translational, vibrational and rotational modes. (Γ: spring constant, \"I\": the moment of inertia for the molecule). From the Hamiltonian, the quantized fluid particle energy state \"E\" and partition functions \"Z\" [with the Maxwell–Boltzmann (MB) occupancy distribution] are found as\n\nHere, \"g\" is the degeneracy, \"n\", \"l\", and \"j\" are the transitional, vibrational and rotational quantum numbers, \"T\" is the characteristic temperature for vibration (= \"ħω\"/\"k\", : vibration frequency), and \"T\" is the rotational temperature [= \"ħ\"/(2\"Ik\")]. The average specific internal energy is related to the partition function through \"Z\", formula_36\n\nWith the energy states and the partition function, the fluid particle specific heat capacity \"c\" is the summation of contribution from various kinetic energies (for non-ideal gas the potential energy is also added). Because the total degrees of freedom in molecules is determined by the atomic configuration, \"c\" has different formulas depending on the configuration,\n\nwhere \"R\" is the gas constant (= \"N\"\"k\", \"N\": the Avogadro constant) and \"M\" is the molecular mass (kg/kmole). (For the polyatomic ideal gas, \"N\" is the number of atoms in a molecule.) In gas, constant-pressure specific heat capacity \"c\" has a larger value and the difference depends on the temperature \"T\", volumetric thermal expansion coefficient \"β\" and the isothermal compressibility κ [\"c\" – \"c\" = \"Tβ\"/(\"ρκ\"), \"ρ\" : the fluid density]. For dense fluids that the interactions between the particles (the van der Waals interaction) should be included, and \"c\" and \"c\" would change accordingly.\nThe net motion of particles (under gravity or external pressure) gives rise to the convection heat flux q = \"ρcuT\". Conduction heat flux q\"\" for ideal gas is derived with the gas kinetic theory or the Boltzmann transport equations, and the thermal conductivity is\n\nwhere ⟨\"u\"⟩ is the RMS (root mean square) thermal velocity (3\"k\"\"T\"/\"m\" from the MB distribution function, \"m\": atomic mass) and \"τ\" is the relaxation time (or intercollision time period) [(2\"π d\"\"n\" ⟨\"u\"⟩) from the gas kinetic theory, ⟨\"u\"⟩: average thermal speed (8\"k\"\"T\"/\"πm\"), \"d\": the collision diameter of fluid particle (atom or molecule), \"n\": fluid number density].\n\n\"k\" is also calculated using molecular dynamics (MD), which simulates physical movements of the fluid particles with the Newton equations of motion (classical) and force field (from \"ab initio\" or empirical properties). For calculation of \"k\", the equilibrium MD with Green–Kubo relations, which express the transport coefficients in terms of integrals of time correlation functions (considering fluctuation), or nonequilibrium MD (prescribing heat flux or temperature difference in simulated system) are generally employed.\n\nFluid particles can interact with other principal particles. Vibrational or rotational modes, which have relatively high energy, are excited or decay through the interaction with photons. Gas lasers employ the interaction kinetics between fluid particles and photons, and laser cooling has been also considered in CO gas laser. Also, fluid particles can be adsorbed on solid surfaces (physisorption and chemisorption), and the frustrated vibrational modes in adsorbates (fluid particles) is decayed by creating \"e\"-\"h\" pairs or phonons. These interaction rates are also calculated through \"ab initio\" calculation on fluid particle and the Fermi golden rule.\n\nPhoton is the quanta of electromagnetic (EM) radiation and energy carrier for radiation heat transfer. The EM wave is governed by the classical Maxwell equations, and the quantization of EM wave is used for phenomena such as the blackbody radiation (in particular to explain the ultraviolet catastrophe). The quanta EM wave (photon) energy of angular frequency \"ω\" is \"E = ħω\", and follows the Bose–Einstein distribution function (\"f\"). The photon Hamiltonian for the quantized radiation field (second quantization) is\n\nwhere e and b are the electric and magnetic fields of the EM radiation, \"ε\" and \"μ\" are the free-space permittivity and permeability, \"V\" is the interaction volume, \"ω\" is the photon angular frequency for the \"α\" mode and \"c\" and \"c\" are the photon creation and annihilation operators. The vector potential a of EM fields (e = -∂a/∂\"t\" and b = ∇×a) is\n\nwhere s is the unit polarization vector, κ is the wave vector.\n\nBlackbody radiation among various types of photon emission employs the photon gas model with thermalized energy distribution without interphoton interaction. From the linear dispersion relation (i.e., dispersionless), phase and group speeds are equal (\"u\" = \"d ω\"/\"dκ\" = \"ω\"/\"κ\", \"u\": photon speed) and the Debye (used for dispersionless photon) density of states is \"Ddω\" = ω\"dω\"/\"π\"\"u\". With \"D\" and equilibrium distribution \"f\", photon energy spectral distribution \"dI\" or \"dI\" (\"λ\": wavelength) and total emissive power \"E\" are derived as\n\nCompared to blackbody radiation, laser emission has high directionality (small solid angle Δ\"Ω\") and spectral purity (narrow bands Δ\"ω\"). Lasers range far-infrared to X-rays/γ-rays regimes based on the resonant transition (stimulated emission) between electronic energy states.\n\nNear-field radiation from thermally excited dipoles and other electric/magnetic transitions is very effective within a short distance (order of wavelength) from emission sites.\n\nThe BTE for photon particle momentum p = \"ħω\"s/\"u\" along direction s experiencing absorption/emission formula_49 (= \"uσ\"[\"f\"(\"ω\",\"T\") - \"f\"(s)], \"σ\": spectral absorption coefficient), and generation/removal formula_50, is\n\nIn terms of radiation intensity (\"I\" = \"ufħωD\"/4\"π\", \"D\": photon density of states), this is called the equation of radiative transfer (ERT)\n\nThe net radiative heat flux vector is formula_54\n\nFrom the Einstein population rate equation, spectral absorption coefficient \"σ\" in ERT is, \nwhere formula_56 is the interaction probability (absorption) rate or the Einstein coefficient \"B\" (J m s), which gives the probability per unit time per unit spectral energy density of the radiation field (1: ground state, 2: excited state), and \"n\" is electron density (in ground state). This can be obtained using the transition dipole moment \"μ\" with the FGR and relationship between Einstein coefficients. Averaging \"σ\" over \"ω\" gives the average photon absorption coefficient \"σ\".\n\nFor the case of optically thick medium of length \"L\", i.e., \"σL\" » 1, and using the gas kinetic theory, the photon conductivity \"k\" is 16\"σ\"\"T\"/3\"σ\" (\"σ\": Stefan–Boltzmann constant, \"σ\": average photon absorption), and photon heat capacity \"nc\" is 16\"σ\"\"T\"/\"u\".\n\nPhotons have the largest range of energy and central in a variety of energy conversions. Photons interact with electric and magnetic entities. For example, electric dipole which in turn are excited by optical phonons or fluid particle vibration, or transition dipole moments of electronic transitions. In heat transfer physics, the interaction kinetics of phonon is treated using the perturbation theory (the Fermi golden rule) and the interaction Hamiltonian. The photon-electron interaction is\n\nwhere p is the dipole moment vector and \"a\" and \"a\" are the creation and annihilation of internal motion of electron. Photons also participate in ternary interactions, e.g., phonon-assisted photon absorption/emission (transition of electron energy level). The vibrational mode in fluid particles can decay or become excited by emitting or absorbing photons. Examples are solid and molecular gas laser cooling.\n\nUsing \"ab initio\" calculations based on the first principles along with EM theory, various radiative properties such as dielectric function (electrical permittivity, \"ε\"), spectral absorption coefficient (\"σ\"), and the complex refraction index (\"m\"), are calculated for various interactions between photons and electric/magnetic entities in matter. For example, the imaginary part (\"ε\") of complex dielectric function (\"ε\" = \"ε\" + \"i\" \"ε\") for electronic transition across a bandgap is \nwhere \"V\" is the unit-cell volume, VB and CB denote the valence and conduction bands, \"w\" is the weight associated with a \"κ\"-point, and \"p\" is the transition momentum matrix element.\nThe real part is \"ε\" is obtained from \"ε\" using the Kramers-Kronig relation \nHere, formula_60 denotes the principal value of the integral.\n\nIn another example, for the far IR regions where the optical phonons are involved, the dielectric function (\"ε\") are calculated as \nwhere LO and TO denote the longitudinal and transverse optical phonon modes, \"j\" is all the IR-active modes, and \"γ\" is the temperature-dependent damping term in the oscillator model. \"ε\" is high frequency dielectric permittivity, which can be calculated DFT calculation when ions are treated as external potential.\n\nFrom these dielectric function (\"ε\") calculations (e.g., Abinit, VASP, etc.), the complex refractive index \"m\"(= \"n\" + \"i\" \"κ\", \"n\": refraction index and \"κ\": extinction index) is found, i.e., \"m\" = \"ε\" = \"ε\" + \"i\" \"ε\"). The surface reflectance \"R\" of an ideal surface with normal incident from vacuum or air is given as \"R\" = [(\"n\" - 1) + \"κ\"]/[(\"n\" + 1) + \"κ\"]. The spectral absorption coefficient is then found from \"σ\" = 2\"ω\" \"κ\"/\"u\". The spectral absorption coefficient for various electric entities are listed in the below table.\n"}
{"id": "17066992", "url": "https://en.wikipedia.org/wiki?curid=17066992", "title": "Home Power", "text": "Home Power\n\nHome Power is a bi-monthly American magazine. It is based in Ashland, Oregon. Circulation is greater than 100,000.\n\nPublished since 1987, \"Home Power\" has promoted a goal of reducing the use of fossil fuels for electricity generation by replacing fossil fuel generation capacity with currently available renewable electricity alternatives. Solar, wind, and hydro systems information is covered at a homeowner's do-it-yourself level with expert advice and examples. \"Home Power\" also promotes and presents information on energy efficient building and design practices. Electric vehicle information is also featured and its integration with renewable electricity systems and solar panels.\n\nRichard and Karen Perez started \"Home Power\" in 1987. From the start the magazine itself has been published with the use of alternative energy resources. Its publishers live the lifestyle they espouse. Today, \"Home Power\" is a recognized leading provider of detailed information for renewable energy installation. Its authors and editors are cited in other industry publications and blogs.\n\nAt its founding, \"Home Power\" focused on off-grid systems and Do-It-Yourself (DIY) information for homeowners. Focusing on \"home scale\" renewable systems, the magazine will not typically cover utility scale renewable energy issues such as large wind farms or utility scale solar installations. The magazine recognizes the up-tick in interest in, and expansion of, the grid-tied solar electric systems market for homeowners. Today the magazine covers both DIY systems and professionally installed grid-tied systems on its web site and print editions and in its articles, editorials and advertising. It also covers broader subjects related to home-scale energy production, including green building, energy efficiency, and alternative transportation.\n\nA controversial topic \"Home Power\" was once in support of (and coined the term for) is \"Guerrilla Solar\", (see Solar Guerrilla) or solar power installations by homeowners that are grid tied which are not permitted by the utilities they are tied to. This non-inspected practice has fallen out of favor given the current cooperation of utilities with homeowners wishing to install solar, and states' enactment of net-metering regulations. \"Home Power\" still covers the topic in its coverage of micro-inverters and small systems that can safely and legally be tied to the grid. \n\n\"Home Power\" also has a web presence. The web site complements the print edition with most past articles in html format, convenient because they are often referred to in current issues.\n\nBecause \"Home Power\" covers off-grid self-reliance it is a popular magazine in the prepper community.\n\nThe \"Home Power\" website states: \"Home Power Inc. will be publishing the final edition of Home Power magazine in November 2018. This November/December issue (HP188) will end the magazine’s 31-year publishing run...\"\n\nTo fill the growing need for reliable information for renewable energy systems professionals \"Home Power\" has also come out with an industry trade magazine: \"SolarPro\".\n\n"}
{"id": "335098", "url": "https://en.wikipedia.org/wiki?curid=335098", "title": "Jackfruit", "text": "Jackfruit\n\nThe jackfruit (\"Artocarpus heterophyllus\"), also known as jack tree, fenne, jakfruit, or sometimes simply jack or jak, is a species of tree in the fig, mulberry, and breadfruit family (Moraceae) native to southwest India.\n\nThe jackfruit tree is well-suited to tropical lowlands, and its fruit is the largest tree-borne fruit, reaching as much as in weight, in length, and in diameter. A mature jackfruit tree can produce about 100 to 200 fruits in a year. The jackfruit is a multiple fruit, composed of hundreds to thousands of individual flowers, and the fleshy petals are eaten.\n\nJackfruit is commonly used in South and Southeast Asian cuisines. The ripe and unripe fruit and seeds are consumed. The jackfruit tree is a widely cultivated throughout tropical regions of the world. It is the national fruit of Bangladesh and Sri Lanka, and the state fruit of the Indian states of Kerala and Tamil Nadu.\n\nThe word \"jackfruit\" comes from Portuguese \"jaca\", which in turn is derived from the Malayalam language term \"chakka\" (Malayalam \"chakka pazham\"). When the Portuguese arrived in India at Kozhikode (Calicut) on the Malabar Coast (Kerala) in 1498, the Malayalam name \"chakka\" was recorded by Hendrik van Rheede (1678–1703) in the \"Hortus Malabaricus\", vol. iii in Latin. Henry Yule translated the book in Jordanus Catalani's (f. 1321–1330) \"Mirabilia descripta: the wonders of the East\".\n\nThe common English name \"jackfruit\" was used by physician and naturalist Garcia de Orta in his 1563 book \"Colóquios dos simples e drogas da India\". Centuries later, botanist Ralph Randles Stewart suggested it was named after William Jack (1795–1822), a Scottish botanist who worked for the East India Company in Bengal, Sumatra, and Malaya.\n\nArtocarpus heterophyllus grows as an evergreen tree that has a relatively short trunk with a dense treetop. It easily reaches heights of 10 to 20 meters and trunk diameters of 30 to 80 centimeters. It sometimes forms buttress roots. The bark of the jackfruit tree is reddish-brown and smooth. In the event of injury to  the bark, a milky juice is released.\n\nThe leaves are alternate and spirally arranged. They are gummy and thick and are divided into a petiole and a leaf blade. The petiole is 1 to 3 inches long. The leathery leaf blade is 7 to 15 inches  long, and 3 to 7 inches wide and is oblong to ovate in shape.\n\nIn young trees, the leaf edges are irregularly lobed or split. On older trees, the leaves are rounded and dark green, with a smooth leaf margin. The leaf blade has a prominent main nerve and starting on each side six to eight lateral nerves. The stipules are egg-shaped at a length of 1.5 to 8 centimeters.\n\nThe inflorescences are formed on the trunk, branches or twigs (caulifloria). Jackfruit trees are monoecious, that is there are both female and male flowers on a tree. The inflorescences are pedunculated, cylindrical to ellipsoidal or pear-shaped, to about 10-12 centimeters long and 5-7 centimeters wide.\n\nInflorescences are initially completely enveloped in egg-shaped cover sheets which rapidly slope off.\n\nThe flowers are very small, there are several thousand flowers in an inflorescence, which sit on a fleshy rachis . The male flowers are greenish, some flowers are sterile. The male flowers are hairy and the perianth ends with two 1 to 1.5 millimeters membrane. The individual and prominent stamens are straight with yellow, roundish anthers. After the pollen distribution, the stamens become ash-gray and fall off after a few days. Later all the male inflorescences also fall off. The greenish female flowers, with hairy and tubular perianth, have a fleshy flower-like base. The female flowers contain an ovary with a broad, capitate or rarely bilobed scar. The blooming time ranges from December until February or March.\n\nThe ellipsoidal to roundish fruit is a multiple fruit formed from the fusion of the ovaries of multiple flowers. The fruits grow on a long and thick stem on the trunk. They vary in size and ripen from an initially yellowish-greenish to yellow, and then at maturity to yellowish-brown. They possess a hard, gummy shell with small pimples surrounded with hard, hexagonal tubercles. The very large and variously shaped fruit have a length of 30 to 100 centimeters and a diameter of 15 to 50 centimeters and can weigh 10-25 kilograms or more.\n\nThe fruits consist of a fibrous, whitish core (rachis) about 5-10 centimeters thick. Radiating from this are many 10 centimeter long individual fruits. They are elliptical to egg-shaped, light brownish achenes with a length of about 3 centimeters and a diameter of 1.5 to 2 centimeters.\n\nThere may be about 100-500 seeds per fruit. The seed coat consists of a thin, waxy, parchment-like and easily removable testa (husk) and a brownish, membranous tegmen. The cotyledons are usually unequal in size, the endosperm is minimally present.\n\nThe fruit matures during the rainy season from July to August. The bean-shaped achenes of the jackfruit are coated with a firm yellowish aril (seed coat, flesh), which has an intense sweet taste at maturity of the fruit. [3] The pulp is enveloped by many narrow strands of fiber (undeveloped perianth), which run between the hard shell and the core of the fruit and are firmly attached to it. When pruned, the inner part (core) secretes a very sticky, milky liquid, which can hardly be removed from the skin, even with soap and water. To clean the hands after \"unwinding\" the pulp an oil or other solvent is used. For example, street vendors in Tanzania, who sell the fruit in small segments, provide small bowls of kerosene for their customers to cleanse their sticky fingers.\n\nAn average fruit consists of 27% edible seed coat, 15% edible seeds, 20% white pulp (undeveloped perianth, rags) and bark and 10% core.\n\nThe number of chromosomes is 2n = 56.\n\nRipe jackfruit is naturally sweet, with subtle flavoring. It can be used to make a variety of dishes, including custards, cakes, or mixed with shaved ice as \"es teler\" in Indonesia or \"halo-halo\" in the Philippines. For the traditional breakfast dish in southern India, \"idlis\", the fruit is used with rice as an ingredient and jackfruit leaves are used as a wrapping for steaming. Jackfruit \"dosas\" can be prepared by grinding jackfruit flesh along with the batter. Ripe jackfruit arils are sometimes seeded, fried, or freeze-dried and sold as jackfruit chips.\n\nThe seeds from ripe fruits are edible, and are said to have a milky, sweet taste often compared to Brazil nuts. They may be boiled, baked, or roasted. When roasted, the flavor of the seeds is comparable to chestnuts. Seeds are used as snacks (either by boiling or fire-roasting) or to make desserts. In Java, the seeds are commonly cooked and seasoned with salt as a snack. They are quite commonly used in curry in India in the form of a traditional lentil and vegetable mix curry.\n\nJackfruit has a distinctive sweet and fruity aroma. In a study of flavour volatiles in five jackfruit cultivars, the main volatile compounds detected were ethyl isovalerate, propyl isovalerate, butyl isovalerate, isobutyl isovalerate, 3-methylbutyl acetate, 1-butanol, and 2-methylbutan-1-ol.\n\nA fully ripe and unopened jackfruit is known to \"emit a strong aroma\", with the inside of the fruit described as smelling of pineapple and banana. After roasting, the seeds may be used as a commercial alternative to chocolate aroma.\n\nThe flesh of the jackfruit is starchy and fibrous and is a source of dietary fiber. The pulp is composed of 74% water, 23% carbohydrates, 2% protein, and 1% fat. In a 100-g portion, raw jackfruit provides and is a rich source (20% or more of the Daily Value, DV) of vitamin B (25% DV). It contains moderate levels (10-19% DV) of vitamin C and potassium, with no other nutrients in significant content.\n\nThe jackfruit also provides a potential part of the solution for tropical countries facing problems with food security, such as several countries of Africa.\n\nThe flavor of the ripe fruit is comparable to a combination of apple, pineapple, mango, and banana. Varieties are distinguished according to characteristics of the fruit flesh. In Indochina, the two varieties are the \"hard\" version (crunchier, drier, and less sweet, but fleshier), and the \"soft\" version (softer, moister, and much sweeter, with a darker gold-color flesh than the hard variety). Unripe jackfruit has a mild flavor and meat-like texture and is used in curry dishes with spices in many cuisines. The skin of unripe jackfruit must be peeled first, then the remaining jackfruit flesh is chopped in a labor-intensive process into edible portions and cooked before serving.\n\nThe cuisines of many Asian countries use cooked young jackfruit. In many cultures, jackfruit is boiled and used in curries as a staple food. The boiled young jackfruit is used in salads or as a vegetable in spicy curries and side dishes, and as fillings for cutlets and chops. It may be used by vegetarians as a substitute for meat. It may be cooked with coconut milk and eaten alone or with meat, shrimp or smoked pork. In southern India, unripe jackfruit slices are deep-fried to make chips.\n\nIn Bangladesh, the fruit is consumed on its own. The unripe fruit is used in curry, and the seed is often dried and preserved to be later used in curry. In India, two varieties of jackfruit predominate: \"muttomvarikka\" and \"sindoor\". \"Muttomvarikka\" has a slightly hard inner flesh when ripe, while the inner flesh of the ripe \"sindoor\" fruit is soft.\n\nA sweet preparation called \"chakkavaratti\" (jackfruit jam) is made by seasoning pieces of \"muttomvarikka\" fruit flesh in jaggery, which can be preserved and used for many months. The fruits are either eaten alone or as a side to rice. The juice is extracted and either drunk straight or as a side. The juice is sometimes condensed and eaten as candies. The seeds are either boiled or roasted and eaten with salt and hot chilies. They are also used to make spicy side dishes with rice. Jackfruit may be ground and made into a paste, then spread over a mat and allowed to dry in the sun to create a natural chewy candy.\n\nIn Indonesia and Malaysia, jackfruit is called \"nangka\". The ripe fruit is usually sold separately and consumed on its own, or sliced and mixed with shaved ice as a sweet concoction dessert such as \"es campur\" and \"es teler\". The ripe fruit might be dried and fried as \"kripik nangka\", or jackfruit cracker. The seeds are boiled and consumed with salt, as it contains edible starchy content; this is called \"beton\". Young (unripe) jackfruit is made into curry called \"gulai nangka\" or stewed called \"gudeg\".\n\nIn the Philippines, jackfruit is called \"langka\" in Filipino and \"\" in Cebuano. The unripe fruit is usually cooked in coconut milk and is eaten with rice. The ripe fruit is often an ingredient in local desserts such as \"halo-halo\" and the Filipino turon. The ripe fruit, besides also being eaten raw as it is, is also preserved by storing in syrup or by drying. The seeds are also boiled before being eaten.\n\nThailand is a major producer of jackfruit, which are often cut, prepared, and canned in a sugary syrup (or frozen in bags or boxes without syrup) and exported overseas, frequently to North America and Europe.\n\nIn Vietnam, jackfruit is used to make jackfruit \"chè\", a sweet dessert soup, similar to the Chinese derivative \"bubur cha cha\". The Vietnamese also use jackfruit purée as part of pastry fillings or as a topping on \"xôi ngọt\" (a sweet version of sticky rice portions).\n\nJackfruits are found primarily in the eastern part of Taiwan. The fresh fruit can be eaten directly or preserved as dried fruit, candied fruit, or jam. It is also stir-fried or stewed with other vegetables and meat.\n\nIn Brazil, three varieties are recognized: \"jaca-dura\", or the \"hard\" variety, which has a firm flesh, and the largest fruits that can weigh between 15 and 40 kg each; \"jaca-mole\", or the \"soft\" variety, which bears smaller fruits with a softer and sweeter flesh; and \"jaca-manteiga\", or the \"butter\" variety, which bears sweet fruits whose flesh has a consistency intermediate between the \"hard\" and \"soft\" varieties.\n\nFrom a tree planted for its shade in gardens, it became an ingredient for local recipes using different fruit segments. The seeds are boiled in water or roasted to remove toxic substances, and then roasted for a variety of desserts. The flesh of the unripe jackfruit is used to make a savory salty dish with smoked pork. The jackfruit arils are used to make jams or fruits in syrup, and can also be eaten raw.\n\nThe golden yellow timber with good grain is used for building furniture and house construction in India. It is termite-proof and is superior to teak for building furniture. The wood of the jackfruit tree is important in Sri Lanka and is exported to Europe. Jackfruit wood is widely used in the manufacture of furniture, doors and windows, in roof construction, and fish sauce barrels.\n\nThe wood of the tree is used for the production of musical instruments. In Indonesia, hardwood from the trunk is carved out to form the barrels of drums used in the \"gamelan\", and in the Philippines, its soft wood is made into the body of the \"kutiyapi\", a type of boat lute. It is also used to make the body of the Indian string instrument \"veena\" and the drums \"mridangam\", \"thimila\", and \"kanjira\".\n\nThe jackfruit has played a significant role in Indian agriculture for centuries. Archeological findings in India have revealed that jackfruit was cultivated in India 3000 to 6000 years ago. It has also been widely cultivated in Southeast Asia.\n\nThe ornate wooden plank called \"avani palaka\", made of the wood of the jackfruit tree, is used as the priest's seat during Hindu ceremonies in Kerala. In Vietnam, jackfruit wood is prized for the making of Buddhist statues in temples The heartwood is used by Buddhist forest monastics in Southeast Asia as a dye, giving the robes of the monks in those traditions their distinctive light-brown color.\n\nJackfruit is the national fruit of Bangladesh, and the state fruit of the Indian states of Kerala and Tamil Nadu.\n\nIn terms of taking care of the plant, minimal pruning is required; cutting off dead branches from the interior of the tree is only sometimes needed. In addition, twigs bearing fruit must be twisted or cut down to the trunk to induce growth for the next season. Branches should be pruned every three to four years to maintain productivity.\n\nSome trees carry too many mediocre fruits and these are usually removed to allow the others to develop better to maturity.\n\nStingless bees such as \"Tetragonula iridipennis\" are jackfruit pollinators, so play an important role in jackfruit cultivation.\n\nIn 2017, India produced 1.4 million tonnes of jackfruit, followed by Bangladesh, Thailand, and Indonesia.\n\nThe marketing of jackfruit involves three groups: producers, traders, and middlemen, including wholesalers and retailers. The marketing channels are rather complex. Large farms sell immature fruit to wholesalers, which helps cash flow and reduces risk, whereas medium-sized farms sell the fruit directly to local markets or retailers.\n\nOutside of its countries of origin, fresh jackfruit can be found at food markets throughout Southeast Asia. It is also extensively cultivated in the Brazilian coastal region, where it is sold in local markets. It is available canned in sugary syrup, or frozen, already prepared and cut. Jackfruit industries are established in Sri Lanka and Vietnam, where the fruit is processed into products such as flour, noodles, \"papad\", and ice cream. It is also canned and sold as a vegetable for export.\n\nOutside of countries where it is grown, jackfruit can be obtained year-round, both canned or dried. Dried jackfruit chips are produced by various manufacturers.\n\nIn Brazil, the jackfruit can become an invasive species as in Brazil's Tijuca Forest National Park in Rio de Janeiro. The Tijuca is mostly an artificial secondary forest, whose planting began during the mid-19th century; jackfruit trees have been a part of the park's flora since it was founded.\n\nRecently, the species has expanded excessively, and its fruits, which naturally fall to the ground and open, are eagerly eaten by small mammals, such as the common marmoset and coati. The seeds are dispersed by these animals; this allows the jackfruit to compete for space with native tree species. Additionally the supply of jackfruit as a ready source of food has allowed the marmoset and coati populations to expand. Since both prey opportunistically on birds' eggs and nestlings, increases in marmoset or coati population are detrimental for local bird populations.\n\n\n"}
{"id": "1510685", "url": "https://en.wikipedia.org/wiki?curid=1510685", "title": "Kesterson National Wildlife Refuge", "text": "Kesterson National Wildlife Refuge\n\nThe Kesterson National Wildlife Refuge was an artificial wetland environment, created using agricultural runoff from farmland in California's Central Valley.\n\nThe irrigation water is transported to the valley from sources in the Sierra Nevada via the California Aqueduct. Minerals from these sources are carried in the water and concentrated by evaporation from aqueducts, canals, and fields. This has resulted in an exceptionally high accumulation of selenium and other minerals in the wetlands. Wildlife in this region suffered deformities due to selenium poisoning, drawing the attention of news media and leading to the closure of the refuge.\n\nKesterson Reservoir was a unit of the refuge but is now part of San Luis National Wildlife Refuge.\n\n\n"}
{"id": "2013362", "url": "https://en.wikipedia.org/wiki?curid=2013362", "title": "Klotski", "text": "Klotski\n\nKlotski (from Polish \"klocki\"—wooden blocks) is a sliding block puzzle thought to have originated in the early 20th century. The name may refer to a specific layout of ten blocks, or in a more global sense to refer to a whole group of similar sliding-block puzzles where the aim is to move a specific block to some predefined location.\n\nLike other sliding-block puzzles, several different-sized block pieces are placed inside a box, which is generally in 4×5 size. Among the blocks, there is a special one (usually the largest) which must be moved to a special area designated by the game board. The player is not allowed to remove blocks, and may only slide blocks horizontally and vertically. Common goals are to solve the puzzle with a minimum number of moves or in a minimum amount of time.\n\nThe earliest known reference of the name \"Klotski\" originates from the computer version for Windows 3.1 by ZH Computing in 1991, which was also included in Microsoft Windows Entertainment Pack. The sliding puzzle had already been trademarked and sold under different names for decades, including \"Psychoteaze\" \"Square Root\", \"Intreeg\", and Ego Buster. There was no known widely used name for the category of sliding puzzles described before \"Klotski\" appeared.\n\nA significant precursor of the Klotzki puzzle is the 19th century 15-puzzle, where fifteen wooden squares had to be rearranged. The 15-puzzle enjoyed immense popularity in western countries during the late 19th century. Around this time, patents appeared for puzzles using differently shaped blocks. Henry Walton filed in 1893 for a sliding puzzle of identically-shaped rectangles, which according to Edward Hordern, is the first known sliding puzzle with rectangular blocks. Frank E. Moss filed in 1900 for a sliding puzzle of six squares and four rectangles, which is one of the first known occurrences of sliding puzzle with non-equal blocks.\nLewis W. Hardy obtained copyright for a game named \"Pennant Puzzle\" in 1909, manufactured by OK Novelty Co., Chicago. The aim of this puzzle is identical to Klotski, and only its default blocks and arrangement are different. Hardy also filed in 1907, which is about a sliding-block puzzle similar to \"Pennant Puzzle\", but with a slightly different combination of blocks and a different goal—not must only the largest block be moved to a specific location, but all of the other blocks must achieve a specific configuration as well. The patent was granted in 1912.\n\nJohn Harold Fleming obtained patent for a puzzle in 1934 in England, with almost identical configuration as described in this page. The puzzle concerned has the same blocks and almost identical placement as forget-me-not, only that the unique horizontal 2×1 block is placed at the bottom instead of beneath the 2×2 block. The patent included a 79-step solution.\n\nIt is said that the game was already known in Japan around the 10th year of the Shōwa period, i.e. around 1935. The first account of occurrence of Klotski in China is in Shaanxi Province, where Lín Dé Kuān of Northwestern Polytechnical University noted children in a village playing a version of Klotski made with pieces of paper in 1938. One of the earliest books about standard Klotski was written by the Chinese professor Jiāng Cháng Yīng of Northwestern Polytechnical University in 1949，in his book 科学消遣. (translation: \"Science Pastime\") This book has been republished as (translation: \"Scientific mindset training and pastime\")\n\nIt is still unknown which version of the puzzle is the original. There are many confusing and conflicting claims, and several countries claim to be the ultimate origin of this game.\n\nThe minimum number of moves for the original puzzle is 81, which is verified by computer to be the absolute minimum for the default starting layout, if you consider sliding a single piece to any reachable position to be a single move.\n\nThe first published 81-step solution is by Martin Gardner, in the February 1964 issue of Scientific American. In the article he discussed the following puzzles (with Edward Hordern classification code in parentheses): Pennant Puzzle (C19), L'Âne Rouge (C27d), Line Up the Quinties (C4), Ma's Puzzle (D1) and a form of Stotts' Baby Tiger Puzzle (F10).\n\nFor earliest published solutions (not optimal solution), currently known is from Chinese educator Xǔ Chún Fǎng, in his book 數學漫談. (translation: \"Mathematics Tidbits\"; , March 1952) His solution involves 100 steps.\n\nThere are several variations of this game, some with names specific to the culture of certain countries, some with different arrangement of blocks.\n\nIt is still unknown whether these variations affected each other and how.\n\nThe following variations basically have the same layout and block arrangement, varying only in name (human, animal, or others), usually with some sort of story behind the names. It is completely unknown whether they share the same origin, though this is highly possible as they are identical to each other.\n\n\"Huarong Dao\" (alternatively named \"Huarong Path\" or \"Huarong Trail\", Chinese name: 華容道) is the Chinese variation, based on a fictitious story in the historical novel \"Romance of the Three Kingdoms\" about the warlord Cao Cao retreating through Huarong Trail (in present-day Jianli County, Jingzhou, Hubei) after his defeat at the Battle of Red Cliffs in the winter of 208/209 CE during the late Eastern Han Dynasty. He encountered an enemy general, Guan Yu, who was guarding the path and waiting for him. Guan Yu spared Cao Cao and allowed the latter to pass through Huarong Trail on account of the generous treatment he received from Cao in the past. The largest block in the game is named \"Cao Cao\".\n\nThe \"Daughter in the Box\" (Japanese name: \"hakoiri musume\") wood puzzle depicts an \"innocent young girl, who knows nothing of the world\" trapped in a building. The largest piece is named \"daughter\", and other blocks are given names of other family members (like father, mother and so on).\n\nAnother Japanese variation uses the names of shogi pieces.\n\nIn France it is well known as \"L'âne rouge\". It features a red donkey (the largest piece) trying to escape a maze of fences and pens to get to its carrots. However, there is no known and documented record of its first existence in France.\n\nThis is a variation from Thailand. Khun Phaen is a famous character in Thai legend, and the game is named after the epic poem \"Khun Chang Khun Phaen\", in which the character is imprisoned. The game depicts Khun Phaen breaking out of the prison by overcoming its nine sentries.\n\nThere is a slight difference between \"Khun Chang Khun Phaen\" and the standard layout – the two middle 1×1 blocks are moved to bottom. Other than that, all other blocks are the same. The origin of this variation is unknown.\n\nIn this context, the \"basic\" arrangement is assumed to be a 4×5 area laid out as follows:-\n\nThis is used globally as the \"basic\" game of Klotski. It is coded C27d in Hordern classification of sliding puzzle games.\n\nCoded as C19 in Hordern classification, it is first copyrighted in 1909 by Lewis. W. Hardy in United States. Standard Trailer Co. has it copyrighted under the name \"Dad's Puzzler\" in 1926 (also in US). Its arrangement is different:\n\nOther than these, the game rules are the same as Klotski. The minimum number of moves to solve the puzzle is 59.\n\n\"Ma's Puzzle\" is copyrighted by Standard Trailer Co. at 1927. It was the first sliding puzzle to use non-rectangular shape. Its goal is to join its 2 L-shaped pieces together, either anywhere or top right corner of the board.\n\nThe first known graphical version of Klotski was created for Windows by ZH Computing in 1991. It was later in the same year included in the third Microsoft Windows Entertainment Pack. Many versions of Klotski followed, either freely available or commercially available. For example, one is included in the GNOME desktop environment. Some include blocks which have special effects.\n\n\n"}
{"id": "42607541", "url": "https://en.wikipedia.org/wiki?curid=42607541", "title": "La Barrancosa Dam", "text": "La Barrancosa Dam\n\nThe La Barrancosa Dam, formerly known as Jorge Cepernic Dam and previously, La Barrancosa Dam, is a concrete-face rock-fill dam being built on the Santa Cruz River about west of Puerto Santa Cruz in Santa Cruz Province, Argentina. It was renamed after the former governor of Santa Cruz Jorge Cepernic. A consortium led by China's Gezhouba Group was awarded the contract to build the Jorge Cepernic Dam and the Néstor Kirchner Dam upstream in August 2013. The consortium will also fund the construction. Both dams are expected to cost nearly US$4.8 billion. It will be built by the firm \"Electroingeniería\", led by Osvaldo Acosta and Gerardo Ferreyra. The primary purpose of the dam is hydroelectric power generation and its power station will have an installed capacity of 600 MW.\n\nIn July 2015 machines arrived in Santa Cruz for the construction of the dams.\n"}
{"id": "8034347", "url": "https://en.wikipedia.org/wiki?curid=8034347", "title": "Life-cycle engineering", "text": "Life-cycle engineering\n\nLife-cycle engineering (LCE) is an approach to assess the environmental impacts in conjunction with economic impacts under consideration of technical boundary conditions. Scope of the assessment is usually the whole life cycle of a product consisting of production, use phase and end of life.\nThe environmental impacts are assessed using environmental life-cycle assessment (LCA). The economic impacts are assessed according to the life cycle costing (LCC) approach. Technical boundary conditions are taken into account providing some limitations on the model, thus verifying the technical feasibility.\n\nLCE aims to deliver a consistent standard of living for future generations that can be maintained indefinitely. Not only does LCE aim to reduce negative impacts on the environment but it can also be used to reduce economic costs for businesses and every day society. \n\nOne aspect of LCE for businesses is the extension of a part of a products life-cycle. The method of involving incremental innovation within production can be seen as a main way to achieve this. The combination of innovation together with technology allows for the function of products to be pushed further, allowing designers to improve the quality of how products meet expanding customer needs. \nThe key themes are economic, social, environmental and technological. All these themes are interlinking and can be influenced by life cycle engineering.\nEconomic factors\n\n• Economic costs\n\n• Productivity\n\n• Quality of products\n\n• Impact on future investments\n\n• Profitability \n\nSocial factors\n\n• Demographics\n\n• Future generations\n\n• Backing from environmentalist\n\nTechnological factors\n\n• Manufacturing\n\n• Efficiency\n\n• Innovation\n\nEnvironmental factors\n\n• Eco-design\n\n• Waste reduction\n\n• Land clearing\n\n• Nature conservation\n"}
{"id": "58831611", "url": "https://en.wikipedia.org/wiki?curid=58831611", "title": "List of countries by electrification rate", "text": "List of countries by electrification rate\n\nThis list of countries by of electrification rate sorts countries by the share of their inhabitants with access to electricity. Access to electricity is considered one of the prerequisites for a modern life. In 2016, 87.4% of the world population had access to electricity. Worldwide, there are major differences between urban and rural regions in the degree of electrification\n"}
{"id": "31291475", "url": "https://en.wikipedia.org/wiki?curid=31291475", "title": "List of rooftop photovoltaic installations", "text": "List of rooftop photovoltaic installations\n\nThis page lists large-scale rooftop photovoltaic installation projects. Photovoltaic arrays in buildings are often either integrated into them, or mounted on to their roofs. Arrays are most often retrofitted into existing buildings, usually mounted on top of the existing roof structure. In 2010, more than four-fifths of the 9,000 MW of solar PV operating in Germany was installed on rooftops.\n\nMost of the large-scale stations are not installed on the roofs, but rather integrated are ground-mounted. Most rooftop applications are done in small and medium-sized installations for consumption within the buildings that host the arrays. There are many significant installation projects which have been completed and many are under construction or proposed.\n\nThe following lists the rooftop photovoltaic installation projects that are larger than 1 MW in total installation size. It does not include rooftop installations of non-building structures such as detached carports and tunnels.\n\nMany photovoltaic power stations are installed at the ground level. There are some large rooftop installations for the sole purpose of being power stations by connecting the systems directly to the grid to deliver electricity to customers. Many of these stations are installed on properties of non-utility companies such as warehouses through roof-area lease agreements and power purchase agreements between utility companies and property owners.\n\nThe followings are the large rooftop installation projects with the main purpose of using the generated electricity as the supplemental power. When the systems generate electricity more than the host buildings can consume, the excess electricity is fed back to the grid to \"bank\" with the utility company through net metering. Below are large multi-site installation projects with total size of rooftop units of 1 MW or larger.\n\nEach installation may be done on a single roof structure or on multiple buildings within the same site or campus.\n\nSome property owners who have many smaller buildings located in multiple sites within the same area may take advantage of installing many medium-size systems in one project to achieve the large-scale power generation to offset their total power usage. The sites of the same owner are normally connected into the same monitoring system for easy maintenance of the overall system.\n"}
{"id": "1908560", "url": "https://en.wikipedia.org/wiki?curid=1908560", "title": "Mega Ampere Spherical Tokamak", "text": "Mega Ampere Spherical Tokamak\n\nThe Mega Ampere Spherical Tokamak (MAST) experiment was a nuclear fusion experiment in operation at Culham Centre for Fusion Energy, Oxfordshire, England, from December 1999 to September 2013.\n\nIt followed the highly successful Small Tight Aspect Ratio Tokamak (START) experiment (1991-1998) and is followed by MAST-Upgrade (2016 - ), which re-uses many of MAST's components and services. MAST used the same innovative spherical tokamak design as START, which has shown itself to be more efficient than the conventional toroidal design, adopted by Joint European Torus (JET) and ITER. START proved to exceed even the most optimistic predictions and the purpose of MAST is to confirm the results of its forerunner by using a larger more purpose-built experiment.\n\nIt was fully commissioned by EURATOM/UKAEA and took two years to design and a further two years to construct. It includes a neutral beam injector similar to that used on START and uses the same merging compression technique instead of the conventional direct induction. Merging compression provides a valuable saving of central solenoid flux, which can then be used to further ramp up the plasma current and/or maintain the required current flat-top.\n\nIts plasma volume is about 8 m. Density ~ 10/m.\n\nImage to right shows plasma in the MAST reactor, displaying its almost circular outer profile. The extensions off the top and bottom are plasma flowing to the ring divertors, a key feature of modern tokamak designs.\n\n\n\nThe magnetic field coils are not superconducting and (for longer runs after upgrade 1a) need to be cooled to -20 °C before each pulse.\n\nFrom 1999 to 2013 it made 30471 plasmas (in pulses up to 0.5 sec). Research by Melanie Windridge \"et al\" during this period demonstrated non-linear instability at large vertical displacements in the MAST tokamak. Windridge concluded that MAST plasmas may be more vulnerable to vertical disruptions than other tokamaks because of the magnetic field structure and the lack of a close-fitting wall.\n\nResearchers are carrying out a major upgrade to significantly enhance the device's capabilities in an attempt to address its primary objectives. The first stage \"1a\" should be completed during 2017. with plasma physics experiments planned towards the end of 2017.\n\nDuring the first upgrade '1a' :\nIt will be the first tokamak to use a Super-X divertor.\n\n\n\n"}
{"id": "4482716", "url": "https://en.wikipedia.org/wiki?curid=4482716", "title": "Metabolic waste", "text": "Metabolic waste\n\nMetabolic wastes or excretements are substances left over from metabolic processes (such as cellular respiration) which cannot be used by the organism (they are surplus or toxic), and must therefore be excreted. This includes nitrogen compounds, water, CO, phosphates, sulphates, etc. Animals treat these compounds as excretes. Plants have chemical \"machinery\" which transforms some of them (primarily the nitrogen compounds) into useful substances, and it has been shown by Brian J. Ford that abscised the parent plant. In this way, Ford argues that the shed leaf acts as an \"excretory\" (an organ carrying away excretory products).\n\nAll the metabolic wastes are excreted in a form of water solutes through the excretory organs (nephridia, Malpighian tubules, kidneys), with the exception of CO, which is excreted together with the water vapor throughout the lungs. The elimination of these compounds enables the chemical homeostasis of the organism.\n\nThe nitrogen compounds through which excess nitrogen is eliminated from organisms are called nitrogenous wastes () or nitrogen wastes. They are ammonia, urea, uric acid, and creatinine. All of these substances are produced from protein metabolism. In many animals, the urine is the main route of excretion for such wastes; in some, the feces is.\n\nAmmonotelism is the excretion of ammonia and ammonium ions. Ammonia (NH) forms with the oxidation of amino groups.(-NH), which are removed from the proteins when they convert into carbohydrates. It is a very toxic substance to tissues and extremely soluble in water. Only one nitrogen atom is removed with it. A lot of water is needed for the excretion of ammonia, about 0.5 L of water is needed per 1 g of nitrogen to maintain ammonia levels in the excretory fluid below the level in body fluids to prevent toxicity. Thus, the marine organisms excrete ammonia directly into the water and are called ammonotelic. Ammonotelic animals include protozoans, crustaceans, platyhelminths, cnidarians, poriferans, echinoderms, and other aquatic invertebrates.\n\nThe excretion of urea is called ureotelism. Land animals, mainly amphibians and mammals, convert ammonia into urea, a process which occurs in the liver and kidney. These animals are called ureotelic. Urea is a less toxic compound than ammonia; two nitrogen atoms are eliminated through it and less water is needed for its excretion. It requires 0.05 L of water to excrete 1 g of nitrogen, approximately only 10% of that required in ammonotelic organisms.\n\nUricotelism is the ridding of excess nitrogen using uric acid. This method is used by birds and diapsids, insects, lizards, and snakes, and these animals are uricotelic. Uric acid is less toxic than ammonia or urea. It contains four nitrogen atoms and only a small amount of water (about 0.001 L per 1 g of nitrogen) is needed for its excretion. Uric acid is the least soluble in water and can be stored in cells and body tissues without toxic effects. A single molecule of uric acid can remove four atoms of nitrogen making uricotelism more efficient than ammonotelism or ureotelism.\n\nUricotelic organisms typically have white pasty excreta. Some mammals including humans excrete uric acid as a component of their urine but it is only a small amount.\n\nThese compounds form during the catabolism of carbohydrates and lipids in condensation reactions, and in some other metabolic reactions of the amino acids. Oxygen is produced by plants and some bacteria in photosynthesis, while CO is a waste product of all animals and plants. Nitrogen gases are produced by denitrifying bacteria and as a waste product, and bacteria for decaying yield ammonia, as do most invertebrates and vertebrates. Water is the only liquid waste from animals and photosynthesizing plants.\n\nNitrates and nitrites are wastes produced by nitrifying bacteria, just as sulfur and sulfates are produced by sulfur-reducing bacteria and sulfate-reducing bacteria. Insoluble iron waste can be made by iron bacteria by using soluble forms. In plants, resins, fats, waxes, and complex organic chemicals are exuded from plants, e.g., the latex from rubber trees and milkweeds. Solid waste products may be manufactured as organic pigments derived from breakdown of pigments like hemoglobin, and inorganic salts like carbonates, bicarbonates, and phosphate, whether in ionic or molecular form, are excreted as solids.\n\nAnimals dispose of solid waste as feces.\n\n"}
{"id": "12440376", "url": "https://en.wikipedia.org/wiki?curid=12440376", "title": "Next Generation Nuclear Plant", "text": "Next Generation Nuclear Plant\n\nA Next Generation Nuclear Plant (NGNP) is a generation IV very-high-temperature reactor (VHTR) that could be coupled to a neighboring hydrogen production facility. It could also produce electricity and supply process heat. Up to 30% of this heat could be used to produce hydrogen via high temperature electrolysis significantly reducing the cost of the process. The United States Department of Energy issued in 2007 a \"request for expressions of interest from prospective industry teams\" that want to provide design services for developing the NGNP.\n\nWith an earlier focus on South Africa's Pebble bed modular reactor, in 2012 Idaho National Laboratory approved a design similar to Areva's SC-HTGR (formerly Antares) reactor as the chosen Next Generation Nuclear Power Plant HTGR to be deployed as a prototype by 2021. It was in competition with General Atomics' Gas turbine modular helium reactor and Westinghouse' Pebble Bed Modular Reactor.\n\nThe SC-HTGR is based on the GT-MHR. An industry alliance of General Atomics and Areva are targeting 2015 for submittal of a Construction Permit application.\n\n"}
{"id": "7726829", "url": "https://en.wikipedia.org/wiki?curid=7726829", "title": "Oxy-fuel combustion process", "text": "Oxy-fuel combustion process\n\nOxy-fuel combustion is the process of burning a fuel using pure oxygen instead of air as the primary oxidant. Since the nitrogen component of air is not heated, fuel consumption is reduced, and higher flame temperatures are possible. Historically, the primary use of oxy-fuel combustion has been in welding and cutting of metals, especially steel, since oxy-fuel allows for higher flame temperatures than can be achieved with an air-fuel flame.\n\nThere is currently research being done in firing fossil-fueled power plants with an oxygen-enriched gas mix instead of air. Almost all of the nitrogen is removed from input air, yielding a stream that is approximately 95% oxygen. Firing with pure oxygen would result in too high a flame temperature, so the mixture is diluted by mixing with recycled flue gas, or staged combustion. The recycled flue gas can also be used to carry fuel into the boiler and ensure adequate convective heat transfer to all boiler areas. Oxy-fuel combustion produces approximately 75% less flue gas than air fueled combustion and produces exhaust consisting primarily of CO and HO (see figure).\n\nThe justification for using oxy-fuel is to produce a CO rich flue gas ready for sequestration. Oxy-fuel combustion has significant advantages over traditional air-fired plants. Among these are:\n\n\nEconomically speaking this method costs more than a traditional air-fired plant. The main problem has been separating oxygen from the air. This process needs lots of energy, nearly 15% of production by a coal-fired power station can be consumed for this process. However, a new technology which is not yet practical called chemical looping combustion can be used to reduce this cost. In chemical looping combustion, the oxygen required to burn the coal is produced internally by oxidation and reduction reactions, as opposed to using more expensive methods of generating oxygen by separating it from air.\n\nAt present in the absence of any need to reduce CO emissions, oxy-fuel is not competitive. However, oxy-fuel is a viable alternative to removing CO from the flue gas from a conventional air-fired fossil fuel plant. However, an oxygen concentrator might be able to help, as it simply removes nitrogen.\n\nIn industries other than power generation, oxy-fuel combustion can be competitive due to higher sensible heat availability. Oxy-fuel combustion is common in various aspects of metal production.\n\nThe glass industry has been converting to oxy-fuel since the early 1990s because glass furnaces require a temperature of approximately 2800 degrees F, which is not attainable at adiabatic flame temperatures for air-fuel combustion unless heat is regenerated between the flue stream and the incoming air stream. Historically, glass furnace regenerators were large and expensive high temperature brick ducts filled with brick arranged in a checkerboard pattern to capture heat as flue gas exits the furnace. When the flue duct is thoroughly heated, air flow is reversed and the flue duct becomes the air inlet, releasing its heat into the incoming air, and allowing for higher furnace temperatures than can be attained with air-fuel only. Two sets of regenerative flue ducts allowed for the air flow to be reversed at regular intervals, and thus maintain a high temperature in the incoming air. By allowing new furnaces to be built without the expense of regenerators, and especially with the added benefit of nitrogen oxide reduction, which allows glass plants to meet emission restrictions, oxy-fuel is cost effective without the need to reduce CO emissions. Oxy-fuel combustion also reduces CO release at the glass plant location, although this may be offset by CO production due to electric power generation which is necessary to produce oxygen for the combustion process.\n\nOxy-fuel combustion may also be cost effective in the incineration of low BTU value hazardous waste fuels. It is often combined with staged combustion for nitrogen oxide reduction, since pure oxygen can stabilize combustion characteristics of a flame.\n\nThere are pilot plants undergoing initial proof-of-concept testing to evaluate the technologies for scaling up to commercial plants, including\n\nOne case study of oxy-fuel combustion is the attempted White Rose plant in North Yorkshire, United Kingdom. The planned project was an oxy-fuel power plant coupled with air separation to capture two million tons of carbon dioxide per year. The carbon dioxide would then be delivered by pipeline to be sequestered in a saline aquifer beneath the North Sea. However, in late 2015 and early 2016, following withdrawal of funding by the Drax Group and the U.K. government, construction was halted. The unforeseen loss of the federal CCS Commercialisation Programme, along with decreased subsidies for renewable energy, left the White Rose Plant with insufficient funds to continue development. \n\n"}
{"id": "184306", "url": "https://en.wikipedia.org/wiki?curid=184306", "title": "Perovskite (structure)", "text": "Perovskite (structure)\n\nA perovskite is any material with the same type of crystal structure as calcium titanium oxide (CaTiO), known as the \"perovskite structure\", or ABX with the oxygen in the edge centers. Perovskites take their name from the mineral, which was first discovered in the Ural mountains of Russia by Gustav Rose in 1839 and is named after Russian mineralogist L. A. Perovski (1792–1856). The general chemical formula for perovskite compounds is ABX, where 'A' and 'B' are two cations of very different sizes, and X is an anion that bonds to both. The 'A' atoms are larger than the 'B' atoms. The ideal cubic structure has the B cation in 6-fold coordination, surrounded by an octahedron of anions, and the A cation in 12-fold cuboctahedral coordination. The relative ion size requirements for stability of the cubic structure are quite stringent, so slight buckling and distortion can produce several lower-symmetry distorted versions, in which the coordination numbers of A cations, B cations or both are reduced.\n\nNatural compounds with this structure are perovskite, loparite, and the silicate perovskite bridgmanite.\n\nThe perovskite structure is adopted by many oxides that have the chemical formula ABO.\n\nIn the idealized cubic unit cell of such a compound, type 'A' atom sits at cube corner positions (0, 0, 0), type 'B' atom sits at body-center position (1/2, 1/2, 1/2) and oxygen atoms sit at face centred positions (1/2, 1/2, 0). (The diagram shows edges for an equivalent unit cell with A in the body center position, B at the corners, and O at mid-edge positions).\n\nThe relative ion size requirements for stability of the cubic structure are quite stringent, so slight buckling and distortion can produce several lower-symmetry distorted versions, in which the coordination numbers of A cations, B cations or both are reduced. Tilting of the BO octahedra reduces the coordination of an undersized A cation from 12 to as low as 8. Conversely, off-centering of an undersized B cation within its octahedron allows it to attain a stable bonding pattern. The resulting electric dipole is responsible for the property of ferroelectricity and shown by perovskites such as BaTiO that distort in this fashion.\n\nThe orthorhombic and tetragonal phases are most common non-cubic variants.\n\nComplex perovskite structures contain two different B-site cations. This results in the possibility of ordered and disordered variants.\n\nThe most common mineral in the Earth is bridgmanite, a magnesium-rich silicate which adopts the perovskite structure at high pressure. As pressure increases, the SiO tetrahedral units in the dominant silica-bearing minerals become unstable compared with SiO octahedral units. At the pressure and temperature conditions of the lower mantle, the most abundant material is a perovskite-structured mineral with the formula (Mg,Fe)SiO, with the second most abundant material likely the rocksalt-structured (Mg,Fe)O oxide, periclase.\n\nAt the high pressure conditions of the Earth's lower mantle, the pyroxene enstatite, MgSiO, transforms into a denser perovskite-structured polymorph; this phase may be the most common mineral in the Earth. This phase has the orthorhombically distorted perovskite structure (GdFeO-type structure) that is stable at pressures from ~24 GPa to ~110 GPa. However, it cannot be transported from depths of several hundred km to the Earth's surface without transforming back into less dense materials. At higher pressures, MgSiO perovskite transforms to post-perovskite.\n\nAlthough the most common perovskite compounds contain oxygen, there are a few perovskite compounds that form without oxygen. Fluoride perovskites such as NaMgF are well known. A large family of metallic perovskite compounds can be represented by RTM (R: rare-earth or other relatively large ion, T: transition metal ion and M: light metalloids). The metalloids occupy the octahedrally coordinated \"B\" sites in these compounds. RPdB, RRhB and CeRuC are examples. MgCNi is a metallic perovskite compound and has received lot of attention because of its superconducting properties. An even more exotic type of perovskite is represented by the mixed oxide-aurides of Cs and Rb, such as CsAuO, which contain large alkali cations in the traditional \"anion\" sites, bonded to O and Au anions.\n\nPerovskite materials exhibit many interesting and intriguing properties from both the theoretical and the application point of view. Colossal magnetoresistance, ferroelectricity, superconductivity, charge ordering, spin dependent transport, high thermopower and the interplay of structural, magnetic and transport properties are commonly observed features in this family. These compounds are used as sensors and catalyst electrodes in certain types of fuel cells and are candidates for memory devices and spintronics applications.\n\nMany superconducting ceramic materials (the high temperature superconductors) have perovskite-like structures, often with 3 or more metals including copper, and some oxygen positions left vacant. One prime example is yttrium barium copper oxide which can be insulating or superconducting depending on the oxygen content.\n\nChemical engineers are considering a cobalt-based perovskite material as a replacement for platinum in catalytic converters in diesel vehicles.\n\nPhysical properties of interest to materials science among perovskites include superconductivity, magnetoresistance, ionic conductivity, and a multitude of dielectric properties, which are of great importance in microelectronics and telecommunication. Because of the flexibility of bond angles inherent in the perovskite structure there are many different types of distortions which can occur from the ideal structure. These include tilting of the octahedra, displacements of the cations out of the centers of their coordination polyhedra, and distortions of the octahedra driven by electronic factors (Jahn-Teller distortions).\n\nSynthetic perovskites have been identified as possible inexpensive base materials for high-efficiency commercial photovoltaics – they showed a conversion efficiency of up to 15% and can be manufactured using the same thin-film manufacturing techniques as that used for thin film silicon solar cells. Methylammonium tin halides and methylammonium lead halides are of interest for use in dye-sensitized solar cells. In 2016, power conversion efficiency have reached 21%. In July 2016, a team of researchers led by Dr. Alexander Weber-Bargioni demonstrated that perovskite PV cells could reach a theoretical peak efficiency of 31%.\n\nAmong the methylammonium halides studied so far the most common is the methylammonium lead triiodide (). It has a high charge carrier mobility and charge carrier lifetime that allow light-generated electrons and holes to move far enough to be extracted as current, instead of losing their energy as heat within the cell. effective diffusion lengths are some 100 nm for both electrons and holes.\n\nMethylammonium halides are deposited by low-temperature solution methods (typically spin-coating). Other low-temperature (below 100 °C) solution-processed films tend to have considerably smaller diffusion lengths. Stranks et al. described nanostructured cells using a mixed methylammonium lead halide (CHNHPbICl) and demonstrated one amorphous thin-film solar cell with an 11.4% conversion efficiency, and another that reached 15.4% using vacuum evaporation. The film thickness of about 500 to 600 nm implies that the electron and hole diffusion lengths were at least of this order. They measured values of the diffusion length exceeding 1 µm for the mixed perovskite, an order of magnitude greater than the 100 nm for the pure iodide. They also showed that carrier lifetimes in the mixed perovskite are longer than in the pure iodide. Liu et. al applied Scanning Photo-current Microscopy to show that the electron diffusion length in mixed halide perovskite along (110) plane is in the order of 10 µm.\n\nFor , open-circuit voltage (V) typically approaches 1 V, while for with low Cl content, V > 1.1 V has been reported. Because the band gaps (E) of both are 1.55 eV, V-to-E ratios are higher than usually observed for similar third-generation cells. With wider bandgap perovskites, V up to 1.3 V has been demonstrated.\n\nThe technique offers the potential of low cost because of the low temperature solution methods and the absence of rare elements. Cell durability is currently insufficient for commercial use.\n\nPlanar heterojunction perovskite solar cells can be manufactured in simplified device architectures (without complex nanostructures) using only vapor deposition. This technique produces 15% solar-to-electrical power conversion as measured under simulated full sunlight.\n\nAlso in 2008 researchers demonstrated that perovskite can generate laser light. LaAlO doped with neodymium gave laser emission at 1080 nm. In 2014 it was shown that mixed methylammonium lead halide (CHNHPbICl) cells fashioned into optically pumped vertical-cavity surface-emitting lasers (VCSELs) convert visible pump light to near-IR laser light with a 70% efficiency.\n\nDue to their high photoluminesence quantum efficiencies, perovskites may be good candidates for use in light-emitting diodes (LEDs). However, the propensity for radiative recombination has mostly been observed at liquid nitrogen temperatures.\n\nIn September 2014, researchers at EPFL in Lausanne, Switzerland, reported achieving water electrolysis at 12.3% efficiency in a highly efficient and low-cost water-splitting cell using perovskite photovoltaics.\n\n\nSimple\n\nSolid solutions\n\n\n"}
{"id": "18353210", "url": "https://en.wikipedia.org/wiki?curid=18353210", "title": "Photoinduced phase transitions", "text": "Photoinduced phase transitions\n\nPhotoinduced phase transition is a process to the nonequilibrium phases generated from an equilibrium by shining on high energy photons, and the nonequilibrium phase is a macroscopic excited domain that has new structural and electronic orders quite different from the starting ground state (equilibrium phase).\n"}
{"id": "2883124", "url": "https://en.wikipedia.org/wiki?curid=2883124", "title": "Pitchstone", "text": "Pitchstone\n\nPitchstone is a dull black glassy volcanic rock formed when felsic lava or magma cools quickly. It is similar to obsidian but is defined by the International Union of Geological Sciences as having a higher water content. It is a volcanic glass; however, unlike a glass, pitchstone has an irregular hackly fracture not a conchoidal fracture. That is due to its coarser (than obsidian) crystal structure. Pitchstone has a resinous lustre, or silky in some cases, and a variable composition. Its colour may be mottled, streaked, or uniform brown, red, green, gray, or black. It is an extrusive rock that is very resistant to erosion.\n\nThe pitchstone ridge of An Sgùrr on the Isle of Eigg, Scotland, was possibly formed as a lava flow in a valley.\n\nPitchstone from the Isle of Arran was used as the raw material for making various items from the Mesolithic through the Neolithic to the Early Bronze Age. Mesolithic use appears to have been limited to the Isle of Arran itself, while in later periods the material or items made from it were transported around Britain.\n"}
{"id": "863469", "url": "https://en.wikipedia.org/wiki?curid=863469", "title": "Plymouth Voyager", "text": "Plymouth Voyager\n\nPlymouth Voyager is a nameplate for a range of vans that were marketed by the Plymouth division of Chrysler. From 1974 to 1983, the Voyager was a full-size van, sold as the counterpart of Dodge Sportsman (later the Dodge Ram Wagon). For 1984, the Voyager became a Chrysler minivan sold alongside the Dodge Caravan; as a minivan, three generations of the Voyager were sold from 1984 to 2000. Following the closure of the Plymouth division in 2000, the Voyager was marketed under the Chrysler brand (as a lower-trim version of the Chrysler Town & Country), where it was sold through 2003. \n\nFrom 1988 to 2016, Chrysler used the Chrysler Voyager name for export-market minivans; during the existence of the Plymouth brand, export-market Voyagers were produced with the body and trim of the Dodge Caravan. When including the Plymouth Voyager and Dodge Caravan with their rebadged Chrysler, Lancia, and Volkswagen variants, the Chrysler minivans collectively rank as the 13th best-selling automotive model line worldwide. \n\nThe Plymouth Voyager minivan was assembled by Chrysler at its Windsor Assembly facility (Windsor, Ontario, Canada); from 1987 to 2000, the Voyager was also assembled at Saint Louis Assembly (Fenton, Missouri). The full-size Plymouth Voyager van was assembled at the now-closed Pillette Road Truck Assembly facility (Windsor, Ontario, Canada). \n\nFrom 1974 to 1983, the Plymouth Voyager was the Plymouth counterpart of the Dodge Sportsman (the Dodge Ram Wagon after 1980). The first truck marketed by Plymouth since 1942, the Voyager was introduced alongside the 1975 Plymouth Trail Duster (a counterpart of the Dodge Ramcharger). As with the Sportsman, the Voyager was produced with 12-15 passenger seating.\n\nSimilar to Canadian Fargo vans, Plymouth badged the Voyager with \"Plymouth\" lettering centered in the grille instead of Dodge lettering on the hood. In 1978, the lettering was moved to the hood. Initially located on the driver side, the Plymouth lettering was centered for 1979, as the grille was enlarged and restyled. For 1979 and 1980, the Plymouth Voyager and Dodge Royal Sportsman were largely indistinguishable; latter examples of the Voyager are distinguished by the lack of large \"RAM\" badging on the door. \n\nIn contrast to its Dodge counterpart, the Plymouth Voyager was equipped with a V8 engine as standard equipment. However, the Voyager was only offered with the 318 and 360 V8s (the 400 and 440 V8 engines were available in Dodge vans prior to 1979). \n\nLee Iacocca and Hal Sperlich had conceived their idea for a modern minivan during their earlier tenure at Ford Motor Company. Henry Ford II had rejected Iaccoca's and Sperlich's idea (and a prototype) of a minivan in 1974, then rumored to carry the name \"Maxivan\". Iaccoca followed Sperlich to Chrysler Corporation, and together they created the T115 minivan — a prototype that was to become the Caravan and Voyager, known colloquially as the \"Magic-wagons\" (a term used in advertising).\n\nThe Chrysler minivans launched a few months ahead of the Renault Espace (the first MPV/minivan in Europe, initially presented to executives as a Talbot (which was made up of Chrysler Europe's disposed assets) in 1979, but not launched until 1984), making them the first of their kind — effectively creating the modern minivan segment in the US.\n\nIn 1984, Chrysler marketed the rebadged Plymouth variant of its new minivan as the Voyager, using the Chrysler's S platform, derived from the K-platform (Plymouth Reliant and Dodge Aries). The Voyager shared components with the K-cars including portions of the interior, e.g., the Reliant's instrument cluster and dashboard controls, along with the K-platform front-wheel drive layout and low floor, giving the Voyager a car-like ease of entry. The Voyager was on \"Car and Driver\" magazine's Ten Best list for 1985.\n\nFor 1987, the Voyager received minor cosmetic updates as well as the May 1987 introduction of the Grand Voyager, which was built on a longer wheelbase adding more cargo room. It was available only with \"SE\" or \"LE\" trim.\n\nFirst-generation Voyager minivans were offered in three trim levels: an unnamed base model, mid-grade \"SE\", and high-end \"LE\", the latter bearing simulated woodgrain paneling. A sportier \"LX\" model was added in 1989, sharing much of its components with the Caravan ES.\n\nSafety features included 3-point seat belts for the front two passengers and lap belts for rear passengers. Standard on all Voyagers were legally mandated side-impact reinforcements for all seating front and rear outboard positions. Safety features such as airbags or ABS were not available.\n\nOriginal commercials for the 1984 Voyager featured magician Doug Henning as a spokesperson to promote the Voyager \"Magic Wagon's\" versatility, cargo space, low step-in height, passenger volume, and maneuverability. Later commercials in 1989 featured rock singer Tina Turner. Canadian commercials in 1990 featured pop singer Celine Dion.\n\n1984-1986 Voyagers could be equipped for five, six, seven passengers, with an eight-passenger variant available only in 1985. Five-passenger seating, standard on all trim levels, consisted of two front bucket seats and an intermediate three-passenger bench seat. In 1985, on base and SE models, the front buckets could be replaced by a 40/60 split three-passenger bench seat, bringing the total number of occupants to six. Seven-passenger seating was an option on SEs and LEs, with dual front buckets, an intermediate two-passenger bench, and a rear three-passenger bench. Eight-passenger seating was available on SE models only, with both the additional middle two-passenger bench and three-passenger front bench. Depending on configuration, the base model could seat up to six, the SE could seat up to eight, and the LE could seat up to seven.\n\nThe two bench seats in the rear were independently removable, and the large three-seat bench could also be installed in the 2nd row location via a second set of attachment points on the van's floor, ordinarily hidden with snap-in plastic covers. This configuration allowed for conventional five-passenger seating with a sizable cargo area in the rear. The latching mechanisms for the benches were very intuitive and easy to operate.\n\nOn base models, the front buckets were low-back items, upholstered with plain cloth or vinyl. On SEs, the buyer could choose between low-back buckets with deluxe cloth or high-back buckets in upgraded vinyl. LEs came standard with high-back front buckets, upholstered in either luxury cloth or luxury vinyl.\n\nIn 1985 and 1986, there was also a five-passenger version with a back seat that could be folded flat with the pull of a handle into a bed that filled the rear compartment from the back of the front seats to the rear. This option was known as the Magic Camper. The Magic Camper back seat had an extra rear-facing cushion that formed the back-most section of the bed when folded flat and the seat, though very heavy, was removable. The Magic Camper option included a tent that attached magnetically to the side of the vehicle allowing access in and out of the sliding side door.\n\nFor 1987 the six- and eight-passenger options were withdrawn, leaving seating for five standard and for seven optional on the base and SE, and seating for seven with high-back front buckets standard on the LE, Grand SE, and Grand LE. Deluxe cloth upholstery was now standard on base and all SE models, with the luxury vinyl optional on SEs. On LEs, luxury cloth came standard and for the first time, leather seats were available on the LE models.\n\nFor the first 3 years of production, two inline-4 engines with 2 barrel carburetors were offered. The base 2.2L was borrowed from the Chrysler K-cars, and produced horsepower. The higher performance fuel injected version of the 2.2L engine later offered in the Chrysler K-cars was only offered in the Voyager for the 1987 model year, and would remain the base powerplant until mid-1987. Alongside the 2.2L, an optional Mitsubishi 2.6L engine was available producing horsepower.\n\nAt launch, the Voyager's low horsepower to weight ratio had not been much of a concern. Its main competitors were the Toyota Van and the Volkswagen Vanagon, both of which offered similar performance. In mid-1987, the base 2.2L I4 was replaced with a fuel-injected 2.5L I4, which produced , while the Mitsubishi \"G54B\" I4 was replaced with the new fuel-injected 3.0L Mitsubishi V-6 producing in March of that year.\n\nA turbocharged version of the base 2.5L producing was available in 1989 and 1990. Also in 1989, revisions to the Mitsubishi V-6 upped its output to . In 1990, a new 3.3L V-6 was added to the option list. Sales of the 2.5 turbo dwindled as a result, and it was dropped at the end of the year.\n\n\nBoth a three-speed TorqueFlite automatic transmission and a five-speed manual were available with all inline-4 engines, including the turbocharged 2.5 L (this was a rare combination). V-6 engines were only offered with the venerable fully hydraulically operated TorqueFlite, until the computer controlled Ultradrive 4-speed automatic became available in 1989. The Ultradrive offered much better fuel economy and responsiveness, particularly when paired with the inline-4 engine.\n\nThe Plymouth Voyager was modified for 1991 with new sheet metal. The S platform was still used, though renamed the \"AS platform\". These were the last Voyagers that were derived from the Chrysler K platform.\n\nTrim levels were carried over from the previous generation. 1991 Voyagers were available in base, mid-grade \"SE\", high-end \"LE\", and high-end sporty \"LX\". The LX which was available only on short-wheelbase Voyagers, was marketed as a sport-luxury minivan and came with the most standard equipment including alloy wheels, fog lamps, and wide array of power-operated features.\n\nIn later years various trim packages were offered on SE models. The \"Sport Wagon\" package available from 1993–1995 featured accent color (gray) bumpers and molding, fog lamps, and special aluminum wheels. The \"Rallye\" package offered in 1995, took the place of the departed \"LX\" model. It was more luxury-oriented, with lower body two-tone paint — regardless of upper body color, the lower body was painted \"Driftwood Beige\" — silver aluminum wheels, and special badging. The font first used for the Rallye's badging was adopted for all of Plymouth's badging from 1996 onward.\n\nInteriors were more differentiated in this generation than on the first with a redesigned dashboard for 1994 featured a passenger-side front airbag. and a seating package, marketed as \"Quad Command\" seating package, available on SE, LE, and LX models. Quad command replaced the 2nd row bench with two individual bucket seats with a center aisle to the 3rd row bench. Interior options varied with trim levels and packages. Cloth seating was standard on all models; leather seating was a standalone extra-cost option on LE and LX models.\n\nOnly badging and minor cosmetics differentiated the Voyager from its Dodge Caravan rebadged variant. The Chrysler Town & Country shared the Voyager's headlamps and taillights along with its own chrome waterfall grille.\n\nThis generation of vans brought additional innovations, including:\n\nThe turbocharged engine and Convert-A-Bed feature were dropped.\n\n\n\nThe 1996 Plymouth Voyager was completely redesigned from the ground up. Gone were its K-car underpinnings and architecture, replaced with more modern components and Chrysler's acclaimed cab-forward design. The third generation redesign used the Chrysler NS platform and included a driver's-side sliding door, a minivan first. The Voyager was on \"Car and Driver\" magazine's Ten Best list for 1996 and 1997.\n\nAs part of Chrysler's new corporate strategy to better focus the Plymouth brand on entry-level vehicles, U.S. market third generation Voyagers and Grand Voyagers were mostly available in base trim, better-equipped \"SE\" models and high-end \"LE\" models. The \"LX\" model was discontinued for the third generation, having been replaced by the Chrysler Town & Country SX and LX. A Rallye option package, carried over from the previous generation, was available on the SE model. It was renamed \"Expresso\" for 1998. All-wheel drive models were discontinued in the US market, remaining available in others. Unavailable in all Voyager models were fog lights, auto-dimming rear view mirrors, and a memory function for the power driver's seat in vans with that option.\n\nBase Voyagers and Voyager SEs were easily distinguished by their body-side moldings. Base models used a narrow accent color strip and SE models used a wide accent color strip with a \"Plymouth\" badge above it on either front door. If equipped with the Rallye or Expresso packages, the \"Plymouth\" badge is replaced with a \"Rallye\" or \"Expresso\" badge. LEs were identical to the Dodge Caravan and Grand Caravan LE aside from the front bumper, grille, wheels which were shared with the Chrysler Town & Country, and badging. The vinyl woodgrain-appearance side paneling was no longer available, as the new side sheetmetal was no longer flat.\n\nThird generation Voyagers introduced a new system of rear seats to simplify installation, removal, and re-positioning— marketed as \"Easy-Out Roller Seats\". All Voyagers and Grand Voyagers were equipped with this feature. When installed, the 2nd and 3rd row seats (either bucket or bench seats) were latched to floor-mounted strikers. When unlatched, eight rollers lifted each seat, allowing it to be rolled fore and aft. Tracks had locator depressions for rollers, thus enabling simple installation. Ergonomic levers at the seatbacks released the floor latches single-handedly without tools and raised the seats onto the rollers in a single motion. Additionally, seatbacks were designed to fold forward. Seat roller tracks were permanently attached to the floor and seat stanchions were aligned, facilitating the longitudinal rolling of the seats. Bench seat stanchions were moved inboard to reduce bending stress in the seat frames, allowing them to be lighter.\n\n\nIn 1999, Plymouth's demise was announced, resulting in the 2000 Voyager/Grand Voyager models in the US doing double duty as both Plymouths and Chryslers.\n\nThe 1996-2000 Dodge Grand Caravan (twin of the Voyager/Grand Voyager) received a \"Marginal\" rating in the Insurance Institute for Highway Safety's 40 mph offset test. The structural performance and restraints were graded \"Acceptable\", but the foot injuries were very high.\n\nIn the NHTSA crash tests, it received 4 stars for the driver and front passenger in the frontal-impact. In the side-impact test, it received 5 stars for the driver, and 3 stars for the rear occupant, and resulted in a fuel leak that could cause a fire hazard.\n\nFor the fourth generation of the minivan in 2001, the Plymouth Voyager was rebadged as the Chrysler Voyager in the US. It was offered in the short wheelbase only. The Chrysler Voyager was discontinued after 2003 and the short wheelbase Chrysler minivan became part of the Town & Country line. However, the short wheelbase Town & Country continued under the Voyager nameplate until 2007 in Mexico.\n\n\nChrysler's plant in St. Louis, Missouri was responsible for building the Voyager from 1990 to 2000.\n\n"}
{"id": "8264485", "url": "https://en.wikipedia.org/wiki?curid=8264485", "title": "Private wire", "text": "Private wire\n\nPrivate wire systems are localised electricity grids, that although connected to the local distribution networks have privately owned central plant that produces electricity. This enables it to operate a stand-alone supply in the event of the national grid failing. This provides localised energy security.\n"}
{"id": "10276017", "url": "https://en.wikipedia.org/wiki?curid=10276017", "title": "Qserv", "text": "Qserv\n\nQserv is a Scottish oilfield service company that specialises in oil-well services operations, including wireline, coiled tubing and pumping services. The company also performs other engineering services, most notably pipeline services.\n\nThe company, based in Portlethen, Aberdeen, was founded in November 2001 by Tommy Dreelan, who is its managing director. The company name is a contraction of the slogan \"Quality Service\".\n\nQserv has operations in the North Sea, Denmark, Indonesia, Norway, and Qatar and worked in 13 European countries in 2004. The company employs more than 300, and has an expected 2007 turnover target of £30m-£45m with EBITDA of £12m.\n\nQserv was awarded the 2004 International Safety Award from the British Safety Council because of its zero accident rates, controlled safety policies, and commitment to health and safety. Tommy Dreelan was named \"Grampian Industrialist 2006\" by Junior Chamber International Aberdeen. Qserv was one of the five finalists in the \"Sunday Times’s\" Entrepreneur Challenge. Qserv provides coiled tubing and pumping related services to BP in the North Sea. In 2006, it acquired KCA Deutag's wireline and well-service division, and the coiled-tubing unit of Weatherford, which increased Qserv worldwide operations significantly.\n\nIn 2012 Qserv were sold to Aker Solutions\n\n\n"}
{"id": "40135754", "url": "https://en.wikipedia.org/wiki?curid=40135754", "title": "Rayong oil spill", "text": "Rayong oil spill\n\nThe Rayong oil spill occurred on July 27, 2013, in the Gulf of Thailand, off the coast of Ko Samet and Map Ta Phut in Rayong Province.\n\nOn July 27, 2013, a pipeline owned by PTTGC Plc, a Thai state-owned oil company, burst while oil was being transferred from an undersea well to a tanker. PTTGC then followed the operation procedures for oil spill management. The Company used boats and airplanes to spray oil-spill dispersants, 0,612 litres of Slickgone NS and 6,930 litres of Super-Dispersant 25, which are permitted by the Department of Pollution Control to be used in Thailand, because they have low toxicity, are biodegradable and do not bio-accumulate or cause mutation and degeneration. It released also a boom to contain the spilled oil within the area. However, the boom did not work well due to bad weather.\n\nOn the night of 28 July 2013, the oil leak (from a pipeline) from the beach at Ko Samet's Coconut Bay (Ao Phrao) resulted in the beach being closed and its tourists evacuated, after spillage reached the beach. On 4 August 2013 media said that the crude oil spill had occurred off of Thailand's mainland, \"when a floating hose transferring oil from a tanker to a PTT refinery pipeline broke sending, PTT says, of oil spewing into the coastal waters\". Two beaches have been closed due to uncertainty about water toxicity. Ko Samet is a popular tourist island, not far from Bangkok, off the coast of Ban Phe', Rayong. Concerns have been growing about the oil spill and the inconsistent information released.\n\nKo Samet is close to Rayong and Map Ta Phut, and they have had a long history of environmental problems since they opened in 1990. About 25,000 people live in the Map Ta Phut municipality. In 1997, the pollution came to public attention when 1,000 pupils and teachers at a local school suffered from illnesses after inhaling toxic emissions and had to be hospitalized. An independent test carried out in 2005 demonstrated that airborne cancerous toxic chemicals released by Map Ta Phut Industrial Estate exceeded safety standards of developed nations by 60 to 3,000 times.\n\nThe oil spill is in part connected to the oil operations at Map Ta Phut. Anthony Zola, an American environmental consultant is reported to have said \"In rural areas, there is almost no enforcement at all. Water pollution, air pollution, noise pollution—you can make all the complaints you want, and no one pays any attention to you.\" Map Ta Phut was closed down in 2009 by the previous Prime Minister, Abhisit Vejjaijiva due to safety concerns. The New York Times reported \"Even among critics of the court decisions, there is widespread agreement that Map Ta Phut is heavily polluted and unhealthy for those who live nearby. But environmental experts remain skeptical that the court decisions will fix the problem.\"\n\nTwo days after the oil spill was discovered, PTT closed their War Room down according to Bangkok Post, and they have consistently since then tried to play down the effects of the spill. A week later they were saying it was safe to go to the beach and that it was clean. Greenpeace is concerned with the use of dispersants and the oil has sunk to the bottom and the full effects of the spill have yet to be understood. Mercury levels have also been reported to be 21 times safe limits. The dispersants also have negative effects and can cause cancers, kidney and liver problems. Some people swimming in the sea have reported dizziness. Bangkok Post reported \"In 2012, a study found that Corexit increases the toxicity of oil by 52 times. It can remain in the ecological food chain for many years and cause widespread and long-lasting health impacts.\"\n\nOfficial reports said of oil were spilled, but critics of the oil company and the government say that the spill was much larger and warned of health risks posed both by the spill and by the chemicals being used to disperse the oil.\n\nA higher estimate of the oil spill has been performed by Somporn Chuai-aree of Prince Songkhla University: \"as low as or as high as \". This estimate is believed by associate professor Siwat Pongniumchan from the National Institute of Development Administration (Nida), who disbelieves previous estimates.\n\nTourism operators and officials were concerned about the effects on tourism in the area, as beaches in Rayong and on the nearby resort island of Ko Samet were impacted.\n\nA 14 August 2013 Bangkok Post article said that \"Fishery Department director-general Wimol Jantrarotai said Chulalongkorn University lab tests on seafood found metal contamination in samples from fish markets near Koh Samet.\"\n\nThe oil company's information sheet given to media (on the beach) on 31 July, said that the oil dispersant used is \"Slickgone NS\".\n\nOn 30 July 2013 Bangkok Post said that \"authorities and more than 500 PTT employees have been struggling to prevent the slick spreading at the beach. They have tried to contain the spill, which has stretched almost along the bay, using containment booms to prevent further environmental damage in the area.\"\n\nA 14 August 2013 Bangkok Post article said that \"mercury levels found in sea water off Ao Phrao beach on Koh Samet were 29 times higher than safety standards allow, according to the Pollution Control Department. Furthermore \"Niphon Phungsuwan, a marine expert from the Department of Marine and Coastal Resources, said more than 70% of coral at Ao Phrao had been bleached.\"\n\nBangkok Post reports \"PTTGC has remained silent about what chemicals it is using but also said they could pose a hazard to the environment and people's health.\" In 2012, a study found that Corexit increases the toxicity of oil by 52 times. It can remain in the ecological food chain for many years and cause widespread and long-lasting health impacts. \"The use of dispersants is a solution that creates new and worse problems,\" Ms Arpa said. The main ingredients of Corexit include 2-Butoxyethanol which can comprise up to 60% of the dispersant and is known to harm the blood, kidneys, liver and central nervous system. Some people swimming near Ko Samet have reported dizziness, and two beaches have been closed. Swimming is not recommended until more is known and the full environmental impacts have been independently verified. As of 20 August 2013, tourists are recommended to use caution if and when going swimming in the water, and to be careful of seafood for now. They should report dizziness of feeling unwell to the relevant authorities or to their embassy.\n\nOn 31 August 2013 Bangkok Post said that the level of total petroleum hydrocarbon (TPH) \"still exceeds safety standards\".\n\nOn 30 July 2013 Bangkok Post said that \"The company PTTGC has insurance coverage of US$50 million with Dhipaya Insurance.\"\n\n\n"}
{"id": "16550580", "url": "https://en.wikipedia.org/wiki?curid=16550580", "title": "Romeite", "text": "Romeite\n\nRoméite is a calcium antimonate mineral with the formula (Ca,Fe,Mn,Na)(Sb,Ti)O(O,OH,F). Roméite is a honey-yellow mineral crystallizing in the hexoctahedral crystal system. It has a Mohs hardness of 5.5-6.0. It occurs in Algeria, Australia, Brazil, China, Europe, Japan, New Zealand, and the United States in metamorphic iron-manganese deposits and in hydrothermal antimony-bearing veins.\n\nIts type locality is Prabornaz Mine, Saint-Marcel, Aosta Valley, Italy. It was named after Jean-Baptiste L. Romé de l'Isle. Brugger, et al. (1997) used infrared spectroscopy to measure water content in Roméite crystals.\n\n"}
{"id": "16941912", "url": "https://en.wikipedia.org/wiki?curid=16941912", "title": "Rosa Hilda Ramos", "text": "Rosa Hilda Ramos\n\nRosa Hilda Ramos is the second Puerto Rican recipient of the Goldman Environmental Prize, a prestigious award given to grassroots environmentalists from around the world and popularly known as the \"Green Nobel prize\". A housewife and environmental activist based in her hometown Cataño, Puerto Rico, Ramos received the award for helping save the Las Cucharillas mangrove from development. She also successfully battled the area's main air polluter—the government-owned Puerto Rico Electric Power Authority (PREPA)-- forcing the public utility to dramatically reduce the lever of pollutants discharged into the atmosphere and forcing it to pay a $7 million federal fine.\n\nThe 2008 Goldman Environmental Prize, which includes a $150,000 monetary grant, was awarded to seven recipients in San Francisco, California on April 14, 2008.\n\n"}
{"id": "8604966", "url": "https://en.wikipedia.org/wiki?curid=8604966", "title": "Sino-Myanmar pipelines", "text": "Sino-Myanmar pipelines\n\nSino-Myanmar pipelines refers to the oil and natural gas pipelines linking Myanmar's deep-water port of Kyaukphyu (Sittwe) in the Bay of Bengal with Kunming in Yunnan province of China.\n\nTalks between China and Myanmar on the feasibility of the project began in 2004. In December 2005, PetroChina signed a deal with Myanmar's Government to purchase natural gas over a 30-year period. Based on this agreement, the parent company of PetroChina, China National Petroleum Corporation (CNPC), signed on 25 December 2008 a contract with the Daewoo International-led consortium to purchase natural gas from the Shwe gas field in A-1 offshore block.\n\nThe plan to build the oil and gas pipelines was approved by China's National Development and Reform Commission in April 2007. In November 2008, China and Myanmar agreed to build a US$1.5 billion oil pipeline and US$1.04 billion natural gas pipeline. In March 2009, China and Myanmar signed an agreement to build a natural gas pipeline, and in June 2009 an agreement to build a crude oil pipeline. The inauguration ceremony marking the start of construction was held on 31 October 2009 on Maday Island.\n\nThe Myanmar section of the gas pipeline was completed on 12 June 2013 and gas started to flow to China on 21 October 2013.\nThe oil pipeline was completed in Aug, 2014.\n\nThe oil and natural gas pipelines run in parallel and start near Kyaukphyu on Made island port on the Bay of Bengal in Myanmar (19°21'52.39\"N, 93°41'3.91\"E), run under the sea for 5.3 KM to mainland ( 19°21'26.09\"N, 93°44'3.41\"E) and then run through Mandalay, Pyin Oo Lwin, and Muse in Myanmar before entering China at the border city of Ruili in Yunnan province. The oil pipeline, which eventually terminates in Kunming, capital of Yunnan province, is long. The natural gas pipeline will extend further from Kunming to Guizhou and Guangxi in China, running a total of .. The China-Myanmar crude oil pipeline project operation corresponds with China's \"Belt and Road\" Initiative, which will provide more direct way for China's imports of crude oil to bypass the crowded Malacca Strait.. China plans to construct additional pipelines in coming years.\n\nThe oil pipeline will have a capacity of 12 million tonnes of crude oil per year. It would diversify China's crude oil imports routes from the Middle East and Africa, and avoid traffic through the Strait of Malacca. Oil storage tanks will be built on an island near the port of Kyaukphyu. For oil processing China will build refineries in Chongqing, Sichuan, and in Yunnan.\n\nThe gas pipeline will allow delivery of natural gas from Burma's offshore fields to China with an expected annual capacity of up to 12 bcm of natural gas. The pipeline will be supplied from the A-1 and A-3 Shwe oil field. China would start receiving natural gas from Burma's Shwe project through the pipeline in April 2013. The Shwe, Shwe-Phyu, and Mya areas in the A-1 and A-3 blocks, estimated to hold 127–218 bcm of natural gas in total, are operated by a group led by Daewoo International Corp. The operators group also includes Myanma Oil and Gas Enterprise, GAIL, and Korea Gas Corporation.\n\nThe total project of pipelines is expected to cost US$2.5 billion.\n\nIn July 2014 CNPC celebrated the first anniversary of the launch of the Myanmar-China natural gas pipeline by announcing that nearly two billion cubic metres of gas has been piped from Indian Ocean plays onto the Asian continent.\n\nA railway that will connect Muse and Lashio is part of the project. The railway will be 80 miles long and will include 41 bridges, 36 underground tunnels and 7 stations.\n\nA number of protests in Burma and abroad took place against the construction of the pipeline. The pipelines have sparked protests over environmental and safety concerns, and inadequate compensation arrangements for local residents. Critics have also said the contract, which was signed under the military regime, should be revisited and that Burma should not be exporting gas when three-quarters of the population lack electricity.\n\nThe project will be implemented jointly by China National Petroleum Corporation (CNPC) and Myanma Oil and Gas Enterprise (MOGE). CNPC will hold a 50.9% stake and manage the project, and MOGE will own the rest.\n\n"}
{"id": "2839957", "url": "https://en.wikipedia.org/wiki?curid=2839957", "title": "Solar Electric Light Fund", "text": "Solar Electric Light Fund\n\nThe Solar Electric Light Fund (SELF) is a Washington, D.C.-based nonprofit whose mission is to design and implement solar energy solutions to assist those living in energy poverty with their economic, educational, health care, and agricultural development. Since 1990, SELF has completed projects in more than 20 countries, using solar energy to power drip irrigation in Benin, health care in Haiti, telemedicine in the Amazon rainforest, online learning in South Africa, and microenterprise development in Nigeria.\n\nSELF believes that energy access is essential to achieving the Millennium Development Goals. SELF's Whole Village Development Model takes an integrated approach to community empowerment by using a mix of solar energy solutions to improve the lives of the 1.5 billion people who don't have access to electricity around the world. By working closely with communities and adhering to its principles of SELF Determination, SELF Help and SELF Reliance, it seeks to provide benefits in:\n\nSELF was founded in 1990 by Neville Williams, an award-winning journalist and author (), who had experience actively promoting solar power as a staffer with the U.S. Department of Energy during the Carter administration. For much of the 1990s, SELF's primary mission was to deliver solar home systems – 50-watt units installed at the household level that could generate enough power to run a few compact fluorescent lights, a radio, and a small black and white television for four or five hours each evening. The electricity generated by the solar panel is stored in a battery, which then provides power at night and during rainy weather.\n\nIn its early projects, SELF used funds donated by private philanthropies to buy home-size photovoltaic systems in bulk on the open market, usually enough for one small village at a time. SELF then sold the systems to villagers in developing areas, in partnership, where possible, with in-country nonprofit agencies. Each participating household made a 20 percent down payment on a solar energy system and paid off the balance – usually between $300 and $400 – over several years. The buyers'payments were pooled in a local revolving loan fund from which their neighbors could borrow to buy their own solar power gear. SELF used a portion of the proceeds on the equipment to establish a local dealership and train residents as solar installers and technicians. The revolving loan funds made it possible for villagers to finance the continued dissemination of solar systems in their areas.\n\nOver time, SELF began to evolve more elaborate project structures. In a joint venture with local partners in India, SELF formed a for-profit subsidiary using India's Ministry of New and Renewable Energy to tap World Bank funds set aside specifically for photovoltaic installations. In part, the company used the money to finance rural co-ops' bulk purchase of solar-energy systems for their members, to install the systems, and to train local technicians. The company then repaid the World Bank's loan from funds collected from the co-ops.\n\nIn 1997, SELF decided to launch a for-profit affiliate, the Solar Electric Light Company, or SELCO, based in Bangalore, India, whose goal would be to sell solar home systems in the states of Karnataka and Andhra Pradesh. Neville Williams stepped down from his role with SELF to run SELCO, and SELF's board of directors appointed Robert A. Freling as the new executive director. Since 1995, SELCO has sold, serviced, and financed over 115,000 solar systems.\n\nBeginning in 2000, SELF embarked on its next generation of projects that would seek to harness solar energy for things such as advancing water pumping and purification, purveying electrification to rural schools and health clinics, providing power to small businesses and micro-enterprises, and facilitating communication access.\n\nThe first opportunity to fulfill this expanded vision was found in South Africa, where SELF had been working on a project to install solar home systems in the Valley of a Thousand Hills, in the province of KwaZulu-Natal. SELF installed a 1.5-kilowatt solar array, which generated enough electricity to power approximately 20 PCs donated by Dell Computers and a small satellite dish that delivered Internet access to Myeka High School. This was the first solar-powered computer lab built in South Africa, and the pass rate at Myeka High School jumped from 30 percent to 70 percent within a year and a half of installation.\n\nIn 2003, SELF found the opportunity to implement a \"Whole-Village\" approach when the U.S. Department of Energy (DOE) invited SELF to carry out a solar electrification project in Nigeria. With support from the DOE, SELF equipped three villages in Jigawa State, in northern Nigeria, with solar power systems for a community water-pumping system, a health clinic, a primary school, street lighting, a portable irrigation pump, and a micro-enterprise center. Since then, SELF has continued to implement this model in other project countries.\n\nSELF has worked in over 20 countries, using solar energy to power health clinics, schools, community centers, water pumps, mosques, drip irrigation, streetlights, and micro-enterprise centers. In addition to its current project sites, SELF has worked in Bhutan, Brazil, Burundi, China, India, Indonesia, Kenya, Lesotho, the Navajo Nation, Nepal, Nigeria, Rwanda, the Solomon Islands, South Africa, Sri Lanka, Tanzania, Uganda, Vietnam, and Zimbabwe.\n\nIn partnership with the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) and Association pour le Developpement Economique Social et Culturel de Kalalé (ADESCA), SELF has installed a total of eleven of its Solar Market Gardens™ (SMG), an innovative, unique solar-powered drip irrigation system, for women farming collectives in Dunkassa and Bessassi, two villages in the arid, northern part of the country.\n\nA two-year study conducted by Stanford University's Program on Food Security and the Environment department appearing in the Proceedings of the National Academy of Sciences found that SELF's SMGs, \"significantly augments both household income and nutritional intake, particularly during the dry season, and is cost effective compared to alternative technologies.\"\n\nIn addition to the SMGs, SELF has also installed three community water wells, streetlights, and solar systems to power two schools and a health center. In 2014, SELF finished the installation of a solar micro-grid that will power a micro-enterprise center in Bessassi, and construction of a second micro-enterprise center in Dunkassa is nearing completion. SELF's future plans include replicating the potable water pumping stations in two more villages, assessing the potential for vaccine refrigerators at solar-electrified clinics, preparing for a pilot internet café, and planning a solar home lantern program.\n\nAfter the 2010 earthquake, SELF and Partners In Health teamed to develop the Rebuilding Haiti Initiative to fully power eight health centers. SELF has also installed 100 solar powered streetlights in tent camps to increase safety, and in collaboration with NRG Energy, Inc. and the Clinton Bush Haiti Fund, SELF has completed the Sun Lights the Way: Brightening Boucan-Carré project by installing solar systems to power a fish farm, 20 schools, a Solar Market Garden™, and a microenterprise center. The success of this project has increased the quality of education for students in remote areas and has contributed to ensuring year-round food security.\n\nIn 2013, SELF solarized an additional seven schools to serve nearly 2,000 students, and also installed 20 solar-powered streetlights around Boucan-Carré in dangerous areas. Currently, SELF is installing two solar micro-grids that will provide electricity to 15,000 people in Port-à-Piment, Côteaux, Roche-à-Bateaux, and Fe-Yo-Bien, to be completed in 2015.\n\nWith support from Acción Social (a governmental agency in Colombia) and Microsoft, SELF conducted a week-long site assessment and determined that deploying solar electric systems for the indigenous Arhuaco, Kogi and Wiwa communities in the Sierra Nevada mountains of northern Colombia is feasible. The project, a part of the Cordon Ambiental y Tradicional de la Sierra Nevada de Santa Marta initiative led by Acción Social, is intended to power the health and educational facilities in the villages, along with community lighting systems at select locations.\n\nSELF was selected as a Grand Challenges Explorations winner, an initiative funded by the Bill & Melinda Gates Foundation, for groundbreaking research in solar powered direct-drive freezers to support global health and development. To support immunization efforts at two remote village health posts in the mountains of Colombia's Sierra Nevada de Santa Marta, SELF successfully field-tested three solar powered direct-drive vaccine refrigerators and the first commercially available direct-drive, battery-free vaccine icepack freezer. Following the tests, the fridge and freezer were donated to the village of Sabana Crespo.\n\nSELF is also working on plans to install a solar energy based microgrid in the village of Sabana Crespo to power coffee facilities, the village general store, a health care clinic which includes a new laboratory, and the village's school and cafeteria.\n\nIn alphabetical order\n\n\n"}
{"id": "26897", "url": "https://en.wikipedia.org/wiki?curid=26897", "title": "Spice", "text": "Spice\n\nA spice is a seed, fruit, root, bark, or other plant substance primarily used for flavoring, coloring or preserving food. Spices are distinguished from herbs, which are the leaves, flowers, or stems of plants used for flavoring or as a garnish. Many spices have antimicrobial properties. This may explain why spices are more commonly used in warmer climates, which have more infectious diseases, and why the use of spices is prominent in meat, which is particularly susceptible to spoiling. Spices are sometimes used in medicine, religious rituals, cosmetics or perfume production.\n\nThe spice trade developed throughout South Asia and Middle East by at earliest 2000 BCE with cinnamon and black pepper, and in East Asia with herbs and pepper. The Egyptians used herbs for mummification and their demand for exotic spices and herbs helped stimulate world trade. The word \"spice\" comes from the Old French word \"espice\", which became \"epice\", and which came from the Latin root \"spec\", the noun referring to \"appearance, sort, kind\": \"species\" has the same root. By 1000 BCE, medical systems based upon herbs could be found in China, Korea, and India. Early uses were connected with magic, medicine, religion, tradition, and preservation.\n\nCloves were used in Mesopotamia by 1700 BCE. The ancient Indian epic Ramayana mentions cloves. The Romans had cloves in the 1st century CE, as Pliny the Elder wrote about them.\n\nThe earliest written records of spices come from ancient Egyptian, Chinese, and Indian cultures. The Ebers Papyrus from Early Egyptians that dates from 1550 describes some eight hundred different medicinal remedies and numerous medicinal procedures.\n\nHistorians believe that nutmeg, which originates from the Banda Islands in Southeast Asia, was introduced to Europe in the 6th century BCE.\n\nIndonesian merchants traveled around China, India, the Middle East, and the east coast of Africa. Arab merchants facilitated the routes through the Middle East and India. This resulted in the Egyptian port city of Alexandria being the main trading center for spices. The most important discovery prior to the European spice trade were the monsoon winds (40 CE). Sailing from Eastern spice cultivators to Western European consumers gradually replaced the land-locked spice routes once facilitated by the Middle East Arab caravans.\n\nIn the story of Genesis, Joseph was sold into slavery by his brothers to spice merchants. In the biblical poem Song of Solomon, the male speaker compares his beloved to many forms of spices.\n\nSpices were among the most demanded and expensive products available in Europe in the Middle Ages, the most common being black pepper, cinnamon (and the cheaper alternative cassia), cumin, nutmeg, ginger and cloves. Given medieval medicine's main theory of humorism, spices and herbs were indispensable to balance \"humors\" in food, a daily basis for good health at a time of recurrent pandemics. In addition to being desired by those using medieval medicine, the European elite also craved spices in the Middle Ages. An example of the European aristocracy's demand for spice comes from the King of Aragon, who invested substantial resources into bringing back spices to Spain in the 12th century. He was specifically looking for spices to put in wine, and was not alone among European monarchs at the time to have such a desire for spice.\n\nSpices were all imported from plantations in Asia and Africa, which made them expensive. From the 8th until the 15th century, the Republic of Venice had the monopoly on spice trade with the Middle East, and along with it the neighboring Italian maritime republics and city-states. The trade made the region rich. It has been estimated that around 1,000 tons of pepper and 1,000 tons of the other common spices were imported into Western Europe each year during the Late Middle Ages. The value of these goods was the equivalent of a yearly supply of grain for 1.5 million people. The most exclusive was saffron, used as much for its vivid yellow-red color as for its flavor. Spices that have now fallen into obscurity in European cuisine include grains of paradise, a relative of cardamom which mostly replaced pepper in late medieval north French cooking, long pepper, mace, spikenard, galangal and cubeb.\n\nSpain and Portugal were interested in seeking new routes to trade in spices and other valuable products from Asia. The control of trade routes and the spice-producing regions were the main reasons that Portuguese navigator Vasco da Gama sailed to India in 1499. When Gama discovered the pepper market in India, he was able to secure peppers for a much cheaper price than the ones demanded by Venice. At around the same time, Christopher Columbus returned from the New World. He described to investors new spices available there.\n\nAnother source of competition in the spice trade during the 15th and 16th century was the Ragusans from the maritime republic of Dubrovnik in southern Croatia.\n\nThe military prowess of Afonso de Albuquerque (1453–1515) allowed the Portuguese to take control of the sea routes to India. In 1506, he took the island of Socotra in the mouth of the Red Sea and, in 1507, Ormuz in the Persian Gulf. Since becoming the viceroy of the Indies, he took Goa in India in 1510, and Malacca on the Malay peninsula in 1511. The Portuguese could now trade directly with Siam, China, and the Maluku Islands.\n\nWith the discovery of the New World came new spices, including allspice, chili peppers, vanilla, and chocolate. This development kept the spice trade, with America as a late comer with its new seasonings, profitable well into the 19th century.\n\nOne issue with spices today is dilution, where spices are blended to make inferior quality powdered spices, by including roots, skins and other admixture in production of spice powder.\n\nA spice may be available in several forms: fresh, whole dried, or pre-ground dried. Generally, spices are dried. Spices may be ground into a powder for convenience. A whole dried spice has the longest shelf life, so it can be purchased and stored in larger amounts, making it cheaper on a per-serving basis. A fresh spice, such as ginger, is usually more flavorful than its dried form, but fresh spices are more expensive and have a much shorter shelf life. Some spices are not always available either fresh or whole, for example turmeric, and often must be purchased in ground form. Small seeds, such as fennel and mustard seeds, are often used both whole and in powder form.\nTo grind a whole spice, the classic tool is mortar and pestle. Less labor-intensive tools are more common now: a microplane or fine grater can be used to grind small amounts; a coffee grinder is useful for larger amounts. A frequently used spice such as black pepper may merit storage in its own hand grinder or mill.\n\nThe flavor of a spice is derived in part from compounds (volatile oils) that oxidize or evaporate when exposed to air. Grinding a spice greatly increases its surface area and so increases the rates of oxidation and evaporation. Thus, flavor is maximized by storing a spice whole and grinding when needed. The shelf life of a whole dry spice is roughly two years; of a ground spice roughly six months. The \"flavor life\" of a ground spice can be much shorter. Ground spices are better stored away from light.\n\nSome flavor elements in spices are soluble in water; many are soluble in oil or fat. As a general rule, the flavors from a spice take time to infuse into the food so spices are added early in preparation. This contrasts to herbs which are usually added late in preparation.\nA study by the Food and Drug Administration of shipments of spices to the United States during fiscal years 2007-2009 showed about 7% of the shipments were contaminated by Salmonella bacteria, some of it antibiotic-resistant. As most spices are cooked before being served salmonella contamination often has no effect, but some spices, particularly pepper, are often eaten raw and present at table for convenient use. Shipments from Mexico and India, a major producer, were the most frequently contaminated. However, with newly developed radiation sterilization methods, the risk of Salmonella contamination is now lower.\n\nBecause they tend to have strong flavors and are used in small quantities, spices tend to add few calories to food, even though many spices, especially those made from seeds, contain high portions of fat, protein, and carbohydrate by weight. However, when used in larger quantity, spices can also contribute a substantial amount of minerals and other micronutrients, including iron, magnesium, calcium, and many others, to the diet. For example, a teaspoon of paprika contains about 1133 IU of Vitamin A, which is over 20% of the recommended daily allowance specified by the US FDA.\n\nMost herbs and spices have substantial antioxidant activity, owing primarily to phenolic compounds, especially flavonoids, which influence nutrition through many pathways, including affecting the absorption of other nutrients. One study found cumin and fresh ginger to be highest in antioxidant activity. These antioxidants can also act as natural preservatives, preventing or slowing the spoilage of food, leading to a higher nutritional content in stored food.\n\nIndia contributes 75% of global spice production.\n\nThe International Organization for Standardization addresses spices and condiments, along with related food additives, as part of the International Classification for Standards 67.220 series.\n\nThe Indian Institute of Spices Research in Kozhikode, Kerala, is devoted exclusively to conducting research for ten spice crops: black pepper, cardamom, cinnamon, clove, garcinia, ginger, nutmeg, paprika, turmeric, and vanilla.\n\n\nBooks\nArticles\n\n"}
{"id": "20641200", "url": "https://en.wikipedia.org/wiki?curid=20641200", "title": "Stalagmometric method", "text": "Stalagmometric method\n\nThe stalagmometric method is one of the most common methods for measuring surface tension. The principle is to measure the weight of drops of a fluid of interest falling from a capillary glass tube, and thereby calculate the surface tension of the fluid. We can determine the weight of the falling drops by counting them. From it we can determine the surface tension. \n\nA stalagmometer is a device for investigating surface tension using the stalagmometric method. It is also called a stactometer or stalogometer. The device is a capillary glass tube whose middle section is widened. The volume of a drop can be predetermined by the design of the stalagmometer. The lower end of the tube is narrowed to force the fluid to fall out of the tube as a drop.\nIn an experiment, the drops of fluid flow slowly from the tube in a vertical direction. The drops hanging on the bottom of the tube start to fall when the volume of the drop reaches a maximum value that is dependent on the characteristics of the solution. At this moment, the weight of the drops is in equilibrium state with the surface tension. Based on Tate’s law:\n\nformula_1\n\nThe drop falls when the weight (mg) is equal to the circumference (2πr) multiplied by the surface tension (σ). The surface tension can be calculated provided the radius of the tube (r) and mass of the fluid droplet (m) are known. Alternatively, since the surface tension is proportional to the weight of the drop, the fluid of interest may be compared to a reference fluid of known surface tension (typically water):\n\nformula_2\n\nIn the equation, m and σ represent the mass and surface tension of the reference fluid and m and σ the mass and surface tension of the fluid of interest. If we take water as a reference fluid,\n\nformula_3\n\nIf the surface tension of water is known which is 72 dyne/cm, we can calculate the surface tension of the specific fluid from the equation. The more drops we weigh, the more precisely we can calculate the surface tension from the equation. The stalagmometer must be kept clean for meaningful readings. There are commercial tubes for stalagmometric method in three sizes: 2.5, 3.5, and 5.0 (ml). The 2.5-ml size is suitable for small volumes and low viscosity, that of 3.5 (ml) for relatively viscous fluids, and that of 5.0 (ml) for large volumes and high viscosity. The 2.5-ml size is suitable for most fluids.\n\nThe stalagmometric method was improved by S. V. Chichkanov and colleagues, who measured the weight of a fixed number of drops rather than counting the drops. This method for determining the surface tension may be more precise than the original method, especially for fluids whose surface is highly active.\n"}
{"id": "15551595", "url": "https://en.wikipedia.org/wiki?curid=15551595", "title": "Sustainable Development Online", "text": "Sustainable Development Online\n\nSustainable Development Online (SDO) is an international reference database and part of EnviroWindows, an environmental information website. According to the SDO website, its role is to \"map the continually changing area of sustainable development, and provide access to important websites built by organizations supporting sustainable development\". SDO contains a list of annotated links, with individual directories for green business, sustainable development tools, education and training, and organizations leading sustainable development.\n"}
{"id": "30048", "url": "https://en.wikipedia.org/wiki?curid=30048", "title": "Tantalum", "text": "Tantalum\n\nTantalum is a chemical element with symbol Ta and atomic number 73. Previously known as \"tantalium\", its name comes from \"Tantalus\", a villain from Greek mythology. Tantalum is a rare, hard, blue-gray, lustrous transition metal that is highly corrosion-resistant. It is part of the refractory metals group, which are widely used as minor components in alloys. The chemical inertness of tantalum makes it a valuable substance for laboratory equipment and a substitute for platinum. Its main use today is in tantalum capacitors in electronic equipment such as mobile phones, DVD players, video game systems and computers.\nTantalum, always together with the chemically similar niobium, occurs in the mineral groups tantalite, columbite and coltan (a mix of columbite and tantalite, though not recognised as a separate mineral species).\n\nTantalum was discovered in Sweden in 1802 by Anders Ekeberg. One year earlier, Charles Hatchett had discovered columbium (now niobium), and in 1809 the English chemist William Hyde Wollaston compared its oxide, columbite with a density of 5.918 g/cm, to that of tantalum, tantalite with a density of 7.935 g/cm. He concluded that the two oxides, despite their difference in measured density, were identical and kept the name tantalum. After Friedrich Wöhler confirmed these results, it was thought that columbium and tantalum were the same element. This conclusion was disputed in 1846 by the German chemist Heinrich Rose, who argued that there were two additional elements in the tantalite sample, and he named them after the children of Tantalus: niobium (from Niobe, the goddess of tears), and pelopium (from Pelops). The supposed element \"pelopium\" was later identified as a mixture of tantalum and niobium, and it was found that the niobium was identical to the columbium already discovered in 1801 by Hatchett.\n\nThe differences between tantalum and niobium were demonstrated unequivocally in 1864 by Christian Wilhelm Blomstrand, and Henri Etienne Sainte-Claire Deville, as well as by Louis J. Troost, who determined the empirical formulas of some of their compounds in 1865. Further confirmation came from the Swiss chemist Jean Charles Galissard de Marignac, in 1866, who proved that there were only two elements. These discoveries did not stop scientists from publishing articles about the so-called \"ilmenium\" until 1871. De Marignac was the first to produce the metallic form of tantalum in 1864, when he reduced tantalum chloride by heating it in an atmosphere of hydrogen. Early investigators had only been able to produce impure tantalum, and the first relatively pure ductile metal was produced by Werner von Bolton in Charlottenburg in 1903. Wires made with metallic tantalum were used for light bulb filaments until tungsten replaced it in widespread use.\n\nThe name tantalum was derived from the name of the mythological Tantalus, the father of Niobe in Greek mythology. In the story, he had been punished after death by being condemned to stand knee-deep in water with perfect fruit growing above his head, both of which eternally \"tantalized\" him. (If he bent to drink the water, it drained below the level he could reach, and if he reached for the fruit, the branches moved out of his grasp.) Anders Ekeberg wrote \"This metal I call \"tantalum\" ... partly in allusion to its incapacity, when immersed in acid, to absorb any and be saturated.\"\n\nFor decades, the commercial technology for separating tantalum from niobium involved the fractional crystallization of potassium heptafluorotantalate away from potassium oxypentafluoroniobate monohydrate, a process that was discovered by Jean Charles Galissard de Marignac in 1866. This method has been supplanted by solvent extraction from fluoride-containing solutions of tantalum.\n\nTantalum is dark (blue-gray), dense, ductile, very hard, easily fabricated, and highly conductive of heat and electricity. The metal is renowned for its resistance to corrosion by acids; in fact, at temperatures below 150 °C tantalum is almost completely immune to attack by the normally aggressive aqua regia. It can be dissolved with hydrofluoric acid or acidic solutions containing the fluoride ion and sulfur trioxide, as well as with a solution of potassium hydroxide. Tantalum's high melting point of 3017 °C (boiling point 5458 °C) is exceeded among the elements only by tungsten, rhenium and osmium for metals, and carbon.\n\nTantalum exists in two crystalline phases, alpha and beta. The alpha phase is relatively ductile and soft; it has body-centered cubic structure (space group \"Im3m\", lattice constant \"a\" = 0.33058 nm), Knoop hardness 200–400 HN and electrical resistivity 15–60 µΩ⋅cm. The beta phase is hard and brittle; its crystal symmetry is tetragonal (space group \"P42/mnm\", \"a\" = 1.0194 nm, \"c\" = 0.5313 nm), Knoop hardness is 1000–1300 HN and electrical resistivity is relatively high at 170–210 µΩ⋅cm. The beta phase is metastable and converts to the alpha phase upon heating to 750–775 °C. Bulk tantalum is almost entirely alpha phase, and the beta phase usually exists as thin films obtained by magnetron\nsputtering, chemical vapor deposition or electrochemical deposition from an eutectic molten salt solution.\n\nNatural tantalum consists of two isotopes: Ta (0.012%) and Ta (99.988%). Ta is a stable isotope. Ta (\"m\" denotes a metastable state) is predicted to decay in three ways: isomeric transition to the ground state of Ta, beta decay to W, or electron capture to Hf. However, radioactivity of this nuclear isomer has never been observed, and only a lower limit on its half-life of 2.0 × 10 years has been set. The ground state of Ta has a half-life of only 8 hours. Ta is the only naturally occurring nuclear isomer (excluding radiogenic and cosmogenic short-living nuclides). It is also the rarest isotope in the Universe, taking into account the elemental abundance of tantalum and isotopic abundance of Ta in the natural mixture of isotopes (and again excluding radiogenic and cosmogenic short-living nuclides).\n\nTantalum has been examined theoretically as a \"salting\" material for nuclear weapons (cobalt is the better-known hypothetical salting material). An external shell of Ta would be irradiated by the intensive high-energy neutron flux from a hypothetical exploding nuclear weapon. This would transmute the tantalum into the radioactive isotope Ta, which has a half-life of 114.4 days and produces gamma rays with approximately 1.12 million electron-volts (MeV) of energy apiece, which would significantly increase the radioactivity of the nuclear fallout from the explosion for several months. Such \"salted\" weapons have never been built or tested, as far as is publicly known, and certainly never used as weapons.\n\nTantalum can be used as a target material for accelerated proton beams for the production of various short-lived isotopes including Li, Rb, and Yb.\n\nTantalum forms compounds in oxidation states -III to V. Most commonly encountered are oxides of Ta(V), which includes all minerals. The chemical properties of Ta and Nb are very similar.\n\nTantalum pentoxide (TaO) is the most important compound from the perspective of applications. Oxides of tantalum in lower oxidation states are numerous, including many defect structures, are lightly studied or poorly characterized.\n\nTantalates, compounds containing [TaO] or [TaO] are numerous. Lithium tantalate (LiTaO) adopts a perovskite structure. Lanthanum tantalate (LaTaO) contains isolated tetrahedra.\n\nAs in the cases of other refractory metals, the hardest known compounds of tantalum are nitrides and carbides. Tantalum carbide, TaC, like the more commonly used tungsten carbide, is a hard ceramic that is used in cutting tools. Tantalum(III) nitride is used as a thin film insulator in some microelectronic fabrication processes.\n\nThe best studied chalcogenide is TaS, a layered semiconductor, as seen for other transition metal dichalcogenides. A tantalum-tellurium alloy forms quasicrystals.\n\nTantalum halides span the oxidation states of +5, +4, and +3. Tantalum pentafluoride (TaF) is a white solid with a melting point of 97.0 °C. The anion [TaF] is used for its separation from niobium. The chloride , which exists as a dimer, is the main reagent in synthesis of new Ta compounds. It hydrolyzes readily to an oxychloride. The lower halides and , feature Ta-Ta bonds.\n\nOrganotantalum compounds include pentamethyltantalum, mixed alkyltantalum chlorides, alkyltantalum hydrides, alkylidene complexes as well as cyclopentadienyl derivatives of the same. Diverse salts and substituted derivatives are known for the hexacarbonyl [Ta(CO)] and related isocyanides.\nTantalum is estimated to make up about 1 ppm or 2 ppm of the Earth's crust by weight. There are many species of tantalum minerals, only some of which are so far being used by industry as raw materials: tantalite (a series consisting of tantalite-(Fe), tantalite-(Mn) and tantalite-(Mg)) microlite (now a group name), wodginite, euxenite (actually euxenite-(Y)), and polycrase (actually polycrase-(Y)). Tantalite (Fe, Mn)TaO is the most important mineral for tantalum extraction. Tantalite has the same mineral structure as columbite (Fe, Mn) (Ta, Nb)O; when there is more tantalum than niobium it is called tantalite and when there is more niobium than tantalum is it called columbite (or niobite). The high density of tantalite and other tantalum containing minerals makes the use of gravitational separation the best method. Other minerals include samarskite and fergusonite.\n\nThe primary mining of tantalum is in Australia, where the largest producer, Global Advanced Metals, formerly known as Talison Minerals, operates two mines in Western Australia, Greenbushes in the Southwest and Wodgina in the Pilbara region. The Wodgina mine was reopened in January 2011 after mining at the site was suspended in late-2008 due to the global financial crisis. Less than a year after it reopened, Global Advanced Metals announced that due to again \"... softening tantalum demand ...\", and other factors, tantalum mining operations were to cease at the end of February 2012. Wodgina produces a primary tantalum concentrate which is further upgraded at the Greenbushes operation before being sold to customers. Whereas the large-scale producers of niobium are in Brazil and Canada, the ore there also yields a small percentage of tantalum. Some other countries such as China, Ethiopia, and Mozambique mine ores with a higher percentage of tantalum, and they produce a significant percentage of the world's output of it. Tantalum is also produced in Thailand and Malaysia as a by-product of the tin mining there. During gravitational separation of the ores from placer deposits, not only is cassiterite (SnO) found, but a small percentage of tantalite also included. The slag from the tin smelters then contains economically useful amounts of tantalum, which is leached from the slag.\n\nWorld tantalum mine production has undergone an important geographic shift since the start of the 21st century when production was predominantly from Australia and Brazil. Beginning in 2007 and through 2014, the major sources of tantalum production from mines dramatically shifted to the DRC, Rwanda, and some other African countries. Future sources of supply of tantalum, in order of estimated size, are being explored in Saudi Arabia, Egypt, Greenland, China, Mozambique, Canada, Australia, the United States, Finland, and Brazil.\n\nIt is estimated that there are less than 50 years left of tantalum resources, based on extraction at current rates, demonstrating the need for increased recycling.\n\nTantalum is considered a conflict resource. Coltan, the industrial name for a columbite–tantalite mineral from which niobium and tantalum are extracted, can also be found in Central Africa, which is why tantalum is being linked to warfare in the Democratic Republic of the Congo (formerly Zaire). According to an October 23, 2003 United Nations report, the smuggling and exportation of coltan has helped fuel the war in the Congo, a crisis that has resulted in approximately 5.4 million deaths since 1998 – making it the world’s deadliest documented conflict since World War II. Ethical questions have been raised about responsible corporate behavior, human rights, and endangering wildlife, due to the exploitation of resources such as coltan in the armed conflict regions of the Congo Basin. However, although important for the local economy in Congo, the contribution of coltan mining in Congo to the world supply of tantalum is usually small. The United States Geological Survey reports in its yearbook that this region produced a little less than 1% of the world's tantalum output in 2002–2006, peaking at 10% in 2000 and 2008.\n\nThe stated aim of the \"Solutions for Hope Tantalum Project\" is to \"source conflict-free tantalum from the Democratic Republic of Congo\"\n\nSeveral steps are involved in the extraction of tantalum from tantalite. First, the mineral is crushed and concentrated by gravity separation. This is generally carried out near the mine site.\n\nThe refining of tantalum from its ores is one of the more demanding separation processes in industrial metallurgy. The chief problem is that tantalum ores contain significant amounts of niobium, which has chemical properties almost identical to those of Ta. A large number of procedures have been developed to address this challenge.\n\nIn modern times, the separation is achieved by hydrometallurgy. Extraction begins with leaching the ore with hydrofluoric acid together with sulfuric acid or hydrochloric acid. This step allows the tantalum and niobium to be separated from the various non-metallic impurities in the rock. Although Ta occurs as various minerals, it is conveniently represented as the pentoxide, since most oxides of tantalum(V) behave similarly under these conditions. A simplified equation for its extraction is thus:\n\nCompletely analogous reactions occur for the niobium component, but the hexafluoride is typically predominant under the conditions of the extraction.\nThese equations are simplified: it is suspected that bisulfate (HSO) and chloride compete as ligands for the Nb(V) and Ta(V) ions, when sulfuric and hydrochloric acids are used, respectively. The tantalum and niobium fluoride complexes are then removed from the aqueous solution by liquid-liquid extraction into organic solvents, such as cyclohexanone, octanol, and methyl isobutyl ketone. This simple procedure allows the removal of most metal-containing impurities (e.g. iron, manganese, titanium, zirconium), which remain in the aqueous phase in the form of their fluorides and other complexes.\n\nSeparation of the tantalum \"from\" niobium is then achieved by lowering the ionic strength of the acid mixture, which causes the niobium to dissolve in the aqueous phase. It is proposed that oxyfluoride H[NbOF] is formed under these conditions. Subsequent to removal of the niobium, the solution of purified HTaF] is neutralised with aqueous ammonia to precipitate hydrated tantalum oxide as a solid, which can be calcined to tantalum pentoxide (TaO).\n\nInstead of hydrolysis, the H[TaF] can be treated with potassium fluoride to produce potassium heptafluorotantalate:\nUnlike H[TaF], the potassium salt is readily crystallized and handled as a solid.\n\nK[TaF] can be converted to metallic tantalum by reduction with sodium, at approximately 800 °C in molten salt.\n\nIn an older method, called the Marignac process, the mixture of H[TaF] and H[NbOF] was converted to a \"mixture\" of K[TaF] and K[NbOF], which was then be separated by fractional crystallization, exploiting their different water solubilities.\n\nElectrolysis using a modified version of the Hall–Héroult process. Instead of requiring the input oxide and output metal to be in liquid form, tantalum electrolysis operates on non-liquid powdered oxides. The initial discovery came in 1997 when Cambridge University researchers immersed small samples of certain oxides in baths of molten salt and reduced the oxide with electric current. The cathode uses powdered metal oxide. The anode is made of carbon. The molten salt at is the electrolyte. The first refinery has enough capacity to supply 3–4% of annual global demand.\n\nAll welding of tantalum must be done in an inert atmosphere of argon or helium in order to shield it from contamination with atmospheric gases. Tantalum is not solderable. Grinding tantalum is difficult, especially so for annealed tantalum. In the annealed condition, tantalum is extremely ductile and can be readily formed as metal sheets.\n\nThe major use for tantalum, as the metal powder, is in the production of electronic components, mainly capacitors and some high-power resistors. Tantalum electrolytic capacitors exploit the tendency of tantalum to form a protective oxide surface layer, using tantalum powder, pressed into a pellet shape, as one \"plate\" of the capacitor, the oxide as the dielectric, and an electrolytic solution or conductive solid as the other \"plate\". Because the dielectric layer can be very thin (thinner than the similar layer in, for instance, an aluminium electrolytic capacitor), a high capacitance can be achieved in a small volume. Because of the size and weight advantages, tantalum capacitors are attractive for portable telephones, personal computers, automotive electronics and cameras.\n\nTantalum is also used to produce a variety of alloys that have high melting points, strength, and ductility. Alloyed with other metals, it is also used in making carbide tools for metalworking equipment and in the production of superalloys for jet engine components, chemical process equipment, nuclear reactors, missile parts, heat exchangers, tanks, and vessels. Because of its ductility, tantalum can be drawn into fine wires or filaments, which are used for evaporating metals such as aluminium. Since it resists attack by body fluids and is nonirritating, tantalum is widely used in making surgical instruments and implants. For example, porous tantalum coatings are used in the construction of orthopedic implants due to tantalum's ability to form a direct bond to hard tissue.\n\nTantalum is inert against most acids except hydrofluoric acid and hot sulfuric acid, and hot alkaline solutions also cause tantalum to corrode. This property makes it a useful metal for chemical reaction vessels and pipes for corrosive liquids. Heat exchanging coils for the steam heating of hydrochloric acid are made from tantalum. Tantalum was extensively used in the production of ultra high frequency electron tubes for radio transmitters. Tantalum is capable of capturing oxygen and nitrogen by forming nitrides and oxides and therefore helped to sustain the high vacuum needed for the tubes when used for internal parts such as grids and plates.\n\nThe high melting point and oxidation resistance lead to the use of the metal in the production of vacuum furnace parts. Tantalum is extremely inert and is therefore formed into a variety of corrosion resistant parts, such as thermowells, valve bodies, and tantalum fasteners. Due to its high density, shaped charge and explosively formed penetrator liners have been constructed from tantalum. Tantalum greatly increases the armor penetration capabilities of a shaped charge due to its high density and high melting point. It is also occasionally used in precious watches e.g. from Audemars Piguet, F.P. Journe, Hublot, Montblanc, Omega, and Panerai. Tantalum is also highly bioinert and is used as an orthopedic implant material. The high stiffness of tantalum makes it necessary to use it as highly porous foam or scaffold with lower stiffness for hip replacement implants to avoid stress shielding. Because tantalum is a non-ferrous, non-magnetic metal, these implants are considered to be acceptable for patients undergoing MRI procedures. The oxide is used to make special high refractive index glass for camera lenses.\n\nTantalum receives far less attention in the environmental field than it does in other geosciences. Upper Crust Concentrations (UCC) and the Nb/Ta ratio in the upper crust and in minerals are available because these measurements are useful as a geochemical tool. The latest values for UCC and the Nb/Ta(w/w) ratio in the upper crust stand at 0.92 ppm and 12.7 respectively.\n\nLittle data is available on tantalum concentrations in the different environmental compartments, especially in natural waters where reliable estimates of ‘dissolved’ tantalum concentrations in seawater and freshwaters have not even been produced. Some values on dissolved concentrations in oceans have been published, but they are contradictory. Values in freshwaters fare little better, but, in all cases, they are probably below 1 ng Lsince ‘dissolved’ concentrations in natural waters are well below most current analytical capabilities. Analysis requires pre-concentration procedures that, for the moment, do not give consistent results. And in any case, tantalum appears to be present in natural waters mostly as particulate matter rather than dissolved.\n\nValues for concentrations in soils, bed sediments and atmospheric aerosols are easier to come by. Values in soils are close to 1 ppm and thus to UCC values. This indicates detrital origin. For atmospheric aerosols the values available are scattered and limited. When tantalum enrichment is observed, it is probably due to loss of more water-soluble elements in aerosols in the clouds.\n\nPollution linked to human use of the element has not been detected. Tantalum appears to be a very conservative element in biogeochemical terms, but its cycling and reactivity are still not fully understood.\n\nCompounds containing tantalum are rarely encountered in the laboratory. The metal is highly biocompatible and is used for body implants and coatings, therefore attention may be focused on other elements or the physical nature of the chemical compound.\n\nPeople can be exposed to tantalum in the workplace by breathing it in, skin contact, or eye contact. The Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for tantalum exposure in the workplace as 5 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 5 mg/m over an 8-hour workday and a short-term limit of 10 mg/m. At levels of 2500 mg/m, tantalum is immediately dangerous to life and health.\n\n"}
{"id": "38196037", "url": "https://en.wikipedia.org/wiki?curid=38196037", "title": "The Petroleum Dictionary", "text": "The Petroleum Dictionary\n\nThe Petroleum Dictionary is a dictionary covering terms used in the American oil industry. It was compiled by Lalia Phipps Boone and was first published by the University of Oklahoma Press in 1952.\n\n\"The Petroleum Dictionary\" contains short definitions for around 6,000 terms used in the oil industry in America, with a particular focus on slang. It is intended as a record of the history of these colloquialisms, rather than a reference work for individuals in the petroleum industry.\n\nWriting in the \"Journal of Geology\", G. Frederick Shepherd from the General American Oil Company of Texas commented on the incomplete nature of the dictionary, describing it as \"an excellent start...but not the end point\". As an example, he highlighted the fact that it contains only 68 of the 573 abbreviations listed by Rinehart Oil News Company in a guide to language used in oil reports. He attributed these omissions partly to the fact that the work underwent more detailed reviewing by language specialists rather than industry technicians. Shepherd, however, did praise Boone for her detailed and interesting research into the history of words, and her inclusion of a number of euphemisms, which made the dictionary \"remarkable for its freshness and occasional spice\".\n\nMaurice Merrill, comparing the Dictionary with a similar work entitled \"Manual of Oil and Gas Terms\", noted the absence of legal terms in Boone's work, suggesting that for individuals within the oil industry, the \"Manual of Oil and Gas Terms\" was a preferable reference work.\n\nIn the \"Southwestern Historical Quarterly\", reviewer David Donoghue highlighted several \"errors of the inexcusable variety\", and suggested that the dictionary had \"little to offer\" to the \"oil fielder who is seriously interested in what makes the business go\".\n\n"}
{"id": "15516194", "url": "https://en.wikipedia.org/wiki?curid=15516194", "title": "Thiophosphate", "text": "Thiophosphate\n\nThiophosphates (or phosphorothioates, PS) are chemical compounds and anions with the general chemical formula (\"x\" = 0, 1, 2, or 3) and related derivatives where organic groups are attached to one or more O or S. Thiophosphates feature tetrahedral phosphorus(V) centers.\n\nOrganothiophosphates are a subclass of organophosphorus compounds that are structurally related to the inorganic thiophosphates. Common members have formulas of the type (RO)RPS and related compounds where RO is replaced by RS. Many of these compounds are used as insecticides, some have medical applications, and some have been used as oil additives.\n\nOligonucleotide phosphorothioates (OPS) are modified oligonucleotides where one of the oxygen atoms in the phosphate moiety is replaced by sulfur. They are the basis of antisense therapy, e.g., the drugs fomivirsen (Vitravene), oblimersen, alicaforsen, and mipomersen (Kynamro).\n\nThe simplest thiophosphates have the formula [PSO]. These trianions are only observed at very high pH, instead they exist in protonated form with the formula [HPSO].\n\nMonothiophosphate is the anion [POS], which has C symmetry. A common salt is sodium monothiophosphate (NaPOS). Monothiophosphate is used in research as an analogue of phosphate in biochemistry. Monothiophosphate esters are biochemical reagents used in the study of transcription, substitution interference assays. Sometimes,\" monothiophosphate\" refers to esters such as (CHO)POS.\n\nDithiophosphate has the formula [POS], which has C symmetry. Sodium dithiophosphate, which is colorless, is the major product from the reaction of phosphorus pentasulfide with NaOH:\n\nDithiophosphoric acid is obtained by treatment of barium dithiophosphate with sulfuric acid:\nBoth NaPOS and especially HPOS are prone toward hydrolysis to their monothio derivatives.\n\nTrithiophosphate is the anion [POS], which has C symmetry. Tetrathiophosphate is the anion [PS], which has T symmetry.\n\nThere are a number of these anions known. There has been interest in compounds containing these anions due to their potential application as fast ion conductors for use in solid state batteries. The binary thiophosphates do not exhibit the extensive diversity of the analogous P-O anions but contain similar structural features, for example P is 4 coordinate, P−S−P links form and there are P−P bonds. One difference is that ions may include polysulfide fragments of 2 or more S atoms whereas in the P−O anions there is only the reactive −O−O−, peroxo, unit. \n"}
{"id": "57787323", "url": "https://en.wikipedia.org/wiki?curid=57787323", "title": "Three-Axis Acceleration Switch", "text": "Three-Axis Acceleration Switch\n\nThe three-axis acceleration switch is a micromachined microelectromechanical systems (MEMS) sensor that detects whether an acceleration event has exceeded a predefined threshold. It is a small, compact device, only 5mm by 5mm, and measures acceleration in the x, y, and z axes. It was developed by the Army Research Laboratory for the purposes of traumatic brain injury (TBI) research and was first introduced in 2012 at the 25th International Conference on Micro Electro Mechanical Systems (MEMS).\n\nThe three-axis acceleration switch was designed to obtain acceleration data more effectively than a conventional accelerometer in order to more accurately characterize the forces and shocks responsible for TBI. While miniature accelerometers require a constant power draw, the three-axis acceleration switch only draws current when it senses an acceleration event, using up less energy and allowing the use of smaller batteries. The three-axis acceleration switch has shown to exhibit an expected battery lifetime that is about 100 times better than that of a digital accelerometer. In return, however, the acceleration switch has a lower resolution than that of a digital or analog accelerometer.\n\nOne potential application of the three-axis acceleration switch is in studying the head impacts of players in high-risk contact sports. Due to the size of conventional accelerometers, measuring the acceleration requires the device to be implemented inside the player’s helmet, which is designed to mitigate the collision forces and thus may not accurately reflect the true level of injury potential. In contrast, the miniature nature of the acceleration switch makes it easier for the switch to be affixed directly onto the participant’s head.\n"}
{"id": "35855057", "url": "https://en.wikipedia.org/wiki?curid=35855057", "title": "Trigonal prismatic molecular geometry", "text": "Trigonal prismatic molecular geometry\n\nIn chemistry, the trigonal prismatic molecular geometry describes the shape of compounds where six atoms, groups of atoms, or ligands are arranged around a central atom, defining the vertices of a triangular prism.\n\nHexamethyltungsten (W(CH)) was the first example of a molecular trigonal prismatic complex.\n\nSome other transition metals have trigonal prismatic hexamethyl complexes, including both neutral molecules such as Mo(CH) and Re(CH) and ions such as and .\n\nThe complex Mo(S−CH=CH−S) is also trigonal prismatic, with each S−CH=CH−S group acting as a bidentate ligand with two sulfur atoms binding the metal atom. Here the coordination geometry of the six sulfurs around the molybdenum is similar to that in the extended structure of molybdenum disulfide (MoS).\n"}
{"id": "22984567", "url": "https://en.wikipedia.org/wiki?curid=22984567", "title": "Venturi flume", "text": "Venturi flume\n\nA venturi flume is a critical-flow open flume with a constricted flow which causes a drop in the hydraulic grade line, creating a critical depth.\n\nIt is used in flow measurement of very large flow rates, usually given in millions of cubic units. A venturi meter would normally measure in millimetres, whereas a venturi flume measures in metres.\n\nMeasurement of discharge with venturi flumes requires two measurements, one upstream and one at the throat (narrowest cross-section), if the flow passes in a subcritical state through the flume. If the flumes are designed so as to pass the flow from sub critical to supercritical state while passing through the flume, a single measurement at the throat (which in this case becomes a critical section) is sufficient for computation of discharge. To ensure the occurrence of critical depth at the throat, the flumes are usually designed in such way as to form a hydraulic jump on the downstream side of the structure. These flumes are called 'standing wave flumes'\n\nVenturi flumes have two advantages over weirs where the critical depth is created by a vertical constriction. First, the hydraulic head loss is smaller in flumes than in weirs. Second, there is no dead zone in flumes where sediment and debris can accumulate; such a dead zone exists upstream of the weirs.\n\nA modified version of the Venturi flume is the Parshall flume. Named after it creator, Dr. Ralph L. Parshall of the U.S. Soil Conservation Service, the Parshall flume is a fixed hydraulic structure used in measuring volumetric flow rate in surface water, wastewater treatment plant, and industrial discharge applications. The Parshall flume accelerates flow through a contraction of both the parallel sidewalls and a drop in the floor at the flume throat. Under free-flow conditions the depth of water at specified location upstream of the flume throat can be converted to a rate of flow.\n\nThe free-flow discharge can be summarized as\n\nWhere\n\nWhen the downstream depth is high enough that the transition to subcritical flow advances upstream into the throat and the hydraulic jump disappears, the flume is operating in a \"submerged flow\" regime, and the discharge is instead given by\n\nWhere formula_3 is the \"submergence correction\", and is found using pre-determined tables for a particular flume geometry.\n\nThe Parshall flume is an empirically calibrated device, so interpolation between listed sizes is not an accurate way to make intermediate size flumes. The flumes are not scale models of each other. 22 standard sizes of Parshall flumes have been developed, covering flow ranges from 0.005 cfs [0.1416 l/s] to 3,280 cfs [92,890 l/s].\n\nSubmergence transitions for Parshall flumes range from 50% (1”-3” sizes) to 80% (10’-50’ sizes), beyond which point level measurements must be taken at both the primary and secondary points of measurement and a submergence correction must be applied to the flow equations.\n\nUnder laboratory conditions Parshall flumes can be expected to exhibit accuracies to within +/-2%, although field conditions make accuracies better than 5% doubtful.\n\nNot all Parshall flumes have the energy-recovering divergence section. These flumes, called \"Montana flumes\", or \"short-section Parshall flumes\", must instead have a free-spilling discharge at all expected flow rates, which increases the drop along the whole flume system. The measurement calculations are the same as for free-flow in a standard Parshall flume, but submerged flow cannot be adjusted for.\n\nDifferences between the Venturi and Parshall flume include: reduction of the inlet converging angle, lengthening the throat section, reduction of the discharge divergence angle, and introducing a drop through the throat (and subsequent partial recovery in the discharge section).\n\n\n\nA venturi flume is similar to the Parshall flume, without the contoured base, but the cross section is usually rectangular, the inlet shorter, and there is a general taper on the outlet similar to the venturi meter. Because of their size, it is usual for these meters to be open to their surroundings just like a river or stream and therefore this type of measurement is referred to as open-channel flow measurement. Parshall flumes are much more efficient than standard flumes and generate a standard wave to effect a measurement.\n\nA good example can be found via google earth: 50°58'41.34\"N, 5°51'36.81\"E, eye altitude 200 m. This is in the 'Geleenbeek', near Geleen in the Netherlands.\n\nwhat is Cd value ?\n"}
{"id": "405532", "url": "https://en.wikipedia.org/wiki?curid=405532", "title": "W and Z bosons", "text": "W and Z bosons\n\nThe W and Z bosons are together known as the weak or more generally as the intermediate vector bosons. These elementary particles mediate the weak interaction; the respective symbols are , , and . The W bosons have either a positive or negative electric charge of 1 elementary charge and are each other's antiparticles. The Z boson is electrically neutral and is its own antiparticle. The three particles have a spin of 1. The W bosons have a magnetic moment, but the Z has none. All three of these particles are very short-lived, with a half-life of about . Their experimental discovery was a triumph for what is now known as the Standard Model of particle physics.\n\nThe W bosons are named after the weak force. The physicist Steven Weinberg named the additional particle the \"Z particle\", and later gave the explanation that it was the last additional particle needed by the model. The W bosons had already been named, and the Z bosons have zero electric charge.\n\nThe two W bosons are verified mediators of neutrino absorption and emission. During these processes, the W boson charge induces electron or positron emission or absorption, thus causing nuclear transmutation. The Z boson is not involved in the absorption or emission of electrons and positrons.\n\nThe Z boson mediates the transfer of momentum, spin and energy when neutrinos scatter \"elastically\" from matter (a process which conserves charge). Such behavior is almost as common as inelastic neutrino interactions and may be observed in bubble chambers upon irradiation with neutrino beams. Whenever an electron is observed as a new free particle suddenly moving with kinetic energy, it is inferred to be a result of a neutrino interacting directly with the electron, since this behavior happens more often when the neutrino beam is present. In this process, the neutrino simply strikes the electron and then scatters away from it, transferring some of the neutrino's momentum to the electron. \n\nBecause neutrinos are neither affected by the strong force nor the electromagnetic force, and because the gravitational force between subatomic particles is negligible, such an interaction can only happen via the weak force. Since such an electron is not created from a nucleon, and is unchanged except for the new force impulse imparted by the neutrino, this weak force interaction between the neutrino and the electron must be mediated by an electromagnetically neutral, weak-force boson particle. Thus, this interaction requires a Z boson.\n\nThese bosons are among the heavyweights of the elementary particles. With masses of and , respectively, the W and Z bosons are almost 80 times as massive as the proton – heavier, even, than entire iron atoms. Their high masses limit the range of the weak interaction. By way of contrast, the photon is the force carrier of the electromagnetic force and has zero mass, consistent with the infinite range of electromagnetism; the hypothetical graviton is also expected to have zero mass. (Although gluons are also presumed to have zero mass, the range of the color force is limited for different reasons; \"see color confinement\".)\n\nAll three bosons have particle spin \"s\" = 1. The emission of a or boson either raises or lowers the electric charge of the emitting particle by one unit, and also alters the spin by one unit. At the same time, the emission or absorption of a W boson can change the type of the particle – for example changing a strange quark into an up quark. The neutral Z boson cannot change the electric charge of any particle, nor can it change any other of the so-called \"charges\" (such as strangeness, baryon number, charm, etc.). The emission or absorption of a Z boson can only change the spin, momentum, and energy of the other particle. (See also \"weak neutral current\".)\n\nThe W and Z bosons are carrier particles that mediate the weak nuclear force, much as the photon is the carrier particle for the electromagnetic force.\n\nThe W bosons are best known for their role in nuclear decay. Consider, for example, the beta decay of cobalt-60.\n\nThis reaction does not involve the whole cobalt-60 nucleus, but affects only one of its 33 neutrons. The neutron is converted into a proton while also emitting an electron (called a beta particle in this context) and an electron antineutrino:\n\nAgain, the neutron is not an elementary particle but a composite of an up quark and two down quarks (udd). It is in fact one of the down quarks that interacts in beta decay, turning into an up quark to form a proton (uud). At the most fundamental level, then, the weak force changes the flavour of a single quark:\n\nwhich is immediately followed by decay of the itself:\n\nThe Z boson is its own antiparticle. Thus, all of its flavour quantum numbers and charges are zero. The exchange of a Z boson between particles, called a neutral current interaction, therefore leaves the interacting particles unaffected, except for a transfer of momentum. boson interactions involving neutrinos have distinctive signatures: They provide the only known mechanism for elastic scattering of neutrinos in matter; neutrinos are almost as likely to scatter elastically (via Z boson exchange) as inelastically (via W boson exchange). The first prediction of Z bosons was made by Brazilian physicist José Leite Lopes in 1958, by devising an equation which showed the analogy of the weak nuclear interactions with electromagnetism. Steve Weinberg, Sheldon Glashow and Abdus Salam later used these results to develop the electroweak unification, in 1973. Weak neutral currents via Z boson exchange were confirmed shortly thereafter (also in 1973), in a neutrino experiment in the Gargamelle bubble chamber at CERN. \n\nFollowing the spectacular success of quantum electrodynamics in the 1950s, attempts were undertaken to formulate a similar theory of the weak nuclear force. This culminated around 1968 in a unified theory of electromagnetism and weak interactions by Sheldon Glashow, Steven Weinberg, and Abdus Salam, for which they shared the 1979 Nobel Prize in Physics. Their electroweak theory postulated not only the W bosons necessary to explain beta decay, but also a new Z boson that had never been observed.\n\nThe fact that the W and Z bosons have mass while photons are massless was a major obstacle in developing electroweak theory. These particles are accurately described by an SU(2) gauge theory, but the bosons in a gauge theory must be massless. As a case in point, the photon is massless because electromagnetism is described by a U(1) gauge theory. Some mechanism is required to break the SU(2) symmetry, giving mass to the W and Z in the process. One explanation, the Higgs mechanism, was forwarded by the 1964 PRL symmetry breaking papers. It predicts the existence of yet another new particle; the Higgs boson. Of the four components of a Goldstone boson created by the Higgs field, three are \"eaten\" by the W, Z, and W bosons to form their longitudinal components, and the remainder appears as the spin 0 Higgs boson.\n\nThe combination of the SU(2) gauge theory of the weak interaction, the electromagnetic interaction, and the Higgs mechanism is known as the Glashow-Weinberg-Salam model. Today it is widely accepted as one of the pillars of the Standard Model of particle physics, particularly given the 2012 discovery of the Higgs boson by the CMS and ATLAS experiments.\n\nThe model predicts that W and Z bosons have the following masses:\n\nwhere g is the SU(2) gauge coupling, g' is U(1) gauge coupling, and v is the Higgs vacuum expectation value.\n\nUnlike beta decay, the observation of neutral current interactions that involve particles requires huge investments in particle accelerators and detectors, such as are available in only a few high-energy physics laboratories in the world (and then only after 1983). This is because Z-bosons behave in somewhat the same manner as photons, but do not become important until the energy of the interaction is comparable with the relatively huge mass of the Z boson.\n\nThe discovery of the W and Z bosons was considered a major success for CERN. First, in 1973, came the observation of neutral current interactions as predicted by electroweak theory. The huge Gargamelle bubble chamber photographed the tracks of a few electrons suddenly starting to move, seemingly of their own accord. This is interpreted as a neutrino interacting with the electron by the exchange of an unseen Z boson. The neutrino is otherwise undetectable, so the only observable effect is the momentum imparted to the electron by the interaction.\n\nThe discovery of the W and Z bosons themselves had to wait for the construction of a particle accelerator powerful enough to produce them. The first such machine that became available was the Super Proton Synchrotron, where unambiguous signals of W bosons were seen in January 1983 during a series of experiments made possible by Carlo Rubbia and Simon van der Meer. The actual experiments were called UA1 (led by Rubbia) and UA2 (led by Pierre Darriulat), and were the collaborative effort of many people. Van der Meer was the driving force on the accelerator end (stochastic cooling). UA1 and UA2 found the Z boson a few months later, in May 1983. Rubbia and van der Meer were promptly awarded the 1984 Nobel Prize in Physics, a most unusual step for the conservative Nobel Foundation.\n\nThe , , and bosons, together with the photon (), comprise the four gauge bosons of the electroweak interaction.\n\nThe W and Z bosons decay to fermion–antifermion pairs but neither the W nor the Z bosons can decay into the higher-mass top quark. Neglecting phase space effects and higher order corrections, simple estimates of their branching fractions can be calculated from the coupling constants.\n\nW bosons can decay to a lepton and neutrino or to an up-type quark and a down-type quark. The decay width of the W boson to a quark–antiquark pair is proportional to the corresponding squared CKM matrix element and the number of quark colours, \"N\" = 3. The decay widths for the W bosons are then proportional to:\n\nHere, , , denote the three flavours of leptons (more exactly, the positive charged antileptons). , , denote the three flavours of neutrinos. The other particles, starting with and , all denote quarks and antiquarks (factor \"N\" is applied). The various \"V\" denote the corresponding CKM matrix coefficients.\n\nUnitarity of the CKM matrix implies that \n\nZ bosons decay into a fermion and its antiparticle. As the Z boson is a mixture of the pre-symmetry-breaking W and B bosons (see weak mixing angle), each vertex factor includes a factor \"T\" − \"Q\" sin\"θ\" where \"T\" is the third component of the weak isospin of the fermion, \"Q \"is the electric charge of the fermion (in units of the elementary charge), and \"θ\" is the weak mixing angle. Because the weak isospin is different for fermions of different chirality, either left-handed or right-handed, the coupling is different as well.\n\nThe relative strengths of each coupling can be estimated by considering that the decay rates include the square of these factors, and all possible diagrams (e.g. sum over quark families, and left and right contributions). This is just an estimate, as we are considering only tree-level diagrams in the Fermi theory.\n\nHere, L and R denote either the left- or right-handed chirality of the fermions, respectively. (The right-handed neutrinos do not exist in the standard model. However, in some extensions beyond the standard model they do.) The notation \"x\" = sin\"θ\" is used.\n\n\n"}
