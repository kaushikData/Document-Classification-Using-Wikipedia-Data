{"id": "31263167", "url": "https://en.wikipedia.org/wiki?curid=31263167", "title": "Alexander Gorlov", "text": "Alexander Gorlov\n\nAlexander M. Gorlov (March 23, 1931 – June 10, 2016) was a Russian mechanical engineer who was Professor Emeritus and Director of Hydro-Pneumatic Power Laboratory at Northeastern University in Boston, Massachusetts.\n\nAlexander M. Gorlov was born into the family of a prosperous lawyer. His father was arrested and died in prison during Joseph Stalin's purges. His mother also spent a number of years in concentration camps in Russia, which forced young Alexander Gorlov to spend some of his childhood years in the orphanage in a remote Russian Urals area.\n\nGorlov received his Doctorate in engineering and had a successful scientific career in Moscow for a number of years during the relatively liberal period of the so-called Khrushchev Thaw. He was granted the Gold and two Bronze Medals for Achievements of the USSR National Economy. In 1975 because of his friendship with Alexander Solzhenitsyn, the Nobel Prize winner and outspoken critic of the communist system, Gorlov was forced to break with his Soviet life and emigrate, eventually establishing a new home in the United States.\n\nSince 1976 Gorlov has been teaching Mechanical Engineering in the Northeastern University combining it with extensive research work in the area of harnessing renewable energy from water flows and wind. In the pursuit of his lifelong dream of creating inexpensive, environmentally friendly hydro-power, Gorlov has developed helical turbines for use in the river, tidal, and open ocean currents. His innovation has led to a series of patents for the Gorlov Helical Turbine which shows great promise for alleviating the worldwide crisis in energy use. This invention was named one of \"Popular Science\"'s top 100 innovations of 2001. One of the other Gorlov's inventions - \"Terrorist Truck-Bomb Protection System\" - is certified by four US patents and is placed on the US Department of State list of certified equipment. That allows the system to be used for protection of vital Government installations such as nuclear power plants, military bases around the world, embassies, bridges and tunnels as well as other potential strategic targets from terrorist attacks. \nGorlov has over 100 technical publications, including books, and 25 US and international patents in such fields as renewable energy, structural analysis & design, theoretical mechanics and the design of bridges and tunnels.\n\n\n\nProfessor Alexander M. Gorlov was a resident of Brookline and Falmouth (both in Massachusetts, USA), where he lived with his wife Ella who is a local historian.\n\n\n\n"}
{"id": "5451561", "url": "https://en.wikipedia.org/wiki?curid=5451561", "title": "Bahamas Electricity Corporation", "text": "Bahamas Electricity Corporation\n\nThe Bahamas Electricity Corporation (BEC) is a government corporation that provides electricity to all of the Bahama Islands except for Grand Bahama. The corporation operates power plants at 25 locations throughout the islands, with 95,000 customers and a peak capacity of 438 MW. Most of the stations use diesel engines; a large gas-turbine plant is installed at the Blue Hills Power Station.\n\nBEC was founded in 1956 by the passing of The Commonwealth of The Bahamas Electricity Act to provide water for all homes on the western coast of the island.\n\n"}
{"id": "6854110", "url": "https://en.wikipedia.org/wiki?curid=6854110", "title": "Barra system", "text": "Barra system\n\nThe Barra system is a passive solar building technology developed by Horazio Barra in Italy. It uses a collector wall to capture solar radiation in the form of heat. It also uses the thermosiphon effect to distribute the warmed air through channels incorporated into the reinforced concrete floors, warming the floors and hence the building. Alternatively, in hot weather, cool nighttime air can be drawn through the floors to chill them in a form of air conditioning. \n\nBarra's are said to have more uniform north-south temperature distributions than other passive solar houses. Many successful systems were built in Europe, but Barra seems fairly unknown elsewhere.\n\nTo convert the sun's light into heat indirectly, a separate insulated space is constructed on the sunny side of the house walls. Looking at the outside, and moving through a cross section there is an outside clear layer. This was traditionally built using glass, but with the advent of cheap, robust Polycarbonate glazing most designs use twin- or triple-wall polycarbonate greenhouse sheeting. Typically the glazing is designed to pass visible light, but block IR to reduce losses, and block UV to protect building materials.\n\nThe next layer is an absorption space. This absorbs most of the light entering the collector. It usually consists of an air gap of around 10 cm thickness with one or more absorption meshes suspended vertically in the space. Often window fly screen mesh is used, or horticultural shade cloth. The mesh itself can hold very little heat and warms up rapidly in light. The heat is absorbed by air passing around and through the mesh, and so the mesh is suspended with an air gap on both the front and back sides.\n\nFinally a layer of insulation sits between the absorption space and the house. Usually this is normal house insulation, using materials such as polyisocyanurate foam, rock wool, foil and polystyrene.\n\nThis collector is very responsive - in the sun it heats up rapidly and the air inside starts to convect. If the collector were to be directly connected to the building using a hole near the floor and a hole near the ceiling an indirect solar gain system would be created. One problem with this that, like Trombe walls, the heat would radiate back out at night, and a convection current would chill the room during the night. Instead, the air movement can be stopped using automatic dampers, similar to those used for ventilating foundation spaces in cold climates, or plastic film dampers, which work by blocking air flow in one direction with a very lightweight flap of plastic. The addition of the damper makes the design an efficient isolated solar gain system.\n\nTo store the thermal energy from the collector, the Barra system suspends a \"spancrete\" slab of concrete as a ceiling to store heat. This is fairly expensive and requires strong support. An alternative is to use water, which can store 5 times as much heat for a given weight. A simple, cheap and effective way is to store the water in sealed 100 mm diameter PVC storm pipe with end caps.\n\nWhether water or concrete is used, the heat is transferred from the air in the collector into the storage material during the day, and released on demand using a ceiling fan into the room at night.\n\nWhere \"spancrete\" slabs are used, the ceiling also heats the house by radiation. Some houses are fitted with louvers (similar to those used on satellites) to adjust the radiation transfer. Warm air travels through the slab tunnels from south to north, where it exits and travels back south through the bulk of the room to the air heater inlet near the floor.\n\nIn most places a system designed for 5 successive days of no sun provides enough storage for all but a few days in a hundred years. Heat can be stored over a number of days using a large container of water. An 8-foot cube of water in the basement might store 15 kL of water, which is heated using a copper tube with fins in the collector. The performance of this can be further improved by putting the finned tube inside another layer of glazing at the back of the main collector, allowing the temperature to build up more than the surrounding air stream. On cloudy days the heat is transferred back out of the store to heat the house.\n\n"}
{"id": "5685017", "url": "https://en.wikipedia.org/wiki?curid=5685017", "title": "Behavior of nuclear fuel during a reactor accident", "text": "Behavior of nuclear fuel during a reactor accident\n\nThis page describes how uranium dioxide nuclear fuel behaves during both normal nuclear reactor operation and under reactor accident conditions, such as overheating. Work in this area is often very expensive to conduct, and so has often been performed on a collaborative basis between groups of countries, usually under the aegis of the CSNI.\n\nBoth the fuel and cladding can swell. Cladding covers the fuel to form a fuel pin and can be deformed. It is normal to fill the gap between the fuel and the cladding with helium gas to permit better thermal contact between the fuel and the cladding. During use the amount of gas inside the fuel pin can increase because of the formation of noble gases (krypton and xenon) by the fission process. If a Loss-of-coolant accident (LOCA) (e.g. Three Mile Island) or a Reactivity Initiated Accident (RIA) (e.g. Chernobyl or SL-1) occurs then the temperature of this gas can increase. As the fuel pin is sealed the pressure of the gas will increase (PV = nRT) and it is possible to deform and burst the cladding. It has been noticed that both corrosion and irradiation can alter the properties of the zirconium alloy commonly used as cladding, making it brittle. As a result, the experiments using unirradated zirconium alloy tubes can be misleading.\n\nAccording to one paper the following difference between the cladding failure mode of unused and used fuel was seen.\n\nUnirradiated fuel rods were pressurized before being placed in a special reactor at the Japanese Nuclear Safety Research Reactor (NSRR) where they were subjected to a simulated RIA transient. These rods failed after ballooning late in the transient when the cladding temperature was high. The failure of the cladding in these tests was ductile, and it was a burst opening.\n\nThe used fuel (61 GW days/tonne of uranium) failed early in the transient with a brittle fracture which was a longitudinal crack.\n\nIt was found that hydrided zirconium tube is weaker and the bursting pressure is lower.\n\nThe common failure process of fuel in the water-cooled reactors is a transition to film boiling and subsequent ignition of zirconium cladding in the steam. The effects of the intense hot hydrogen reaction product flow on the fuel pellets and on the bundle's wall well represented on the sidebar picture.\n\nThe nuclear fuel can swell during use, this is because of effects such as fission gas formation in the fuel and the damage which occurs to the lattice of the solid. The fission gases accumulate in the void that forms in the center of a fuel pellet as burnup increases. As the void forms, the once-cylindrical pellet degrades into pieces. The swelling of the fuel pellet can cause pellet-cladding interaction when it thermally expands to the inside of the cladding tubing. The swollen fuel pellet imposes mechanical stresses upon the cladding. A document on the subject of the swelling of the fuel can be downloaded from the NASA web site.\n\nAs the fuel is degraded or heated the more volatile fission products which are trapped within the uranium dioxide may become free. For example, see.\n\nA report on the release of Kr, Ru and Cs from uranium when air is present has been written. It was found that uranium dioxide was converted to UO between about 300 and 500 °C in air. They report that this process requires some time to start, after the induction time the sample gains mass. The authors report that a layer of UO was present on the uranium dioxide surface during this induction time. They report that 3 to 8% of the krypton-85 was released, and that much less of the ruthenium (0.5%) and caesium (2.6 x 10%) occurred during the oxidation of the uranium dioxide.\n\nIn a water-cooled power reactor (or in a water-filled spent fuel pool, SFP), if a power surge occurs as a result of a reactivity initiated accident, an understanding of the transfer of heat from the surface of the cladding to the water is very useful. In a French study, metal pipe immersed in water (both under typical PWR and SFP conditions), was electrically heated to simulate the generation of heat within a fuel pin by nuclear processes. The temperature of the pipe was monitored by thermocouples and for the tests conducted under PWR conditions the water entering the larger pipe (14.2 mm diameter) holding the test metal pipe (9.5 mm outside diameter and 600 mm long) was at 280 °C and 15 MPa. The water was flowing past the inner pipe at \"circa\" 4 ms and the cladding was subjected to heating at 2200 to 4900 °C s to simulate an RIA. It was found that as the temperature of the cladding increased the rate of heat transfer from the surface of the cladding increased at first as the water boiled at nucleation sites. When the heat flux is greater than the critical heat flux a boiling crisis occurs. This occurs as the temperature of the fuel cladding surface increases so that the surface of the metal was too hot (surface dries out) for nucleation boiling. When the surface dries out the rate of heat transfer decreases, after a further increase in the temperature of the metal surface the boiling resumes but it is now film boiling.\n\nAs a nuclear fuel bundle increases in burnup (time in reactor), the radiation begins changing not only the fuel pellets inside the cladding, but the cladding material itself. The zirconium chemically reacts to the water flowing around it as coolant, forming a protective oxide on the surface of the cladding. Typically a fifth of the cladding wall will be consumed by oxide in PWRs. There is a smaller corrosion layer thickness in BWRs. The chemical reaction that takes place is:\n\nZr + 2 HO -> ZrO + 2 H(gas)\n\nHydriding occurs when the product gas (hydrogen) precipitates out as hydrides within the zirconium. This causes the cladding to become embrittled, instead of ductile. The hydride bands form in rings within the cladding. As the cladding experiences hoop stress from the growing amount of fission products, the hoop stress increases. The material limitations of the cladding is one aspect that limits the amount of burnup nuclear fuel can accumlate in a reactor.\n\nCRUD (Chalk River Unidentified Deposits) was discovered by Chalk River Laboratories. It occurs on the exterior of the clad as burnup is accumulated.\n\nWhen a nuclear fuel assembly is prepared for onsite storage, it is dried and moved to a spent nuclear fuel shipping cask with scores of other assemblies. Then it sits on a concrete pad for a number of years waiting for an intermediate storage facility or reprocessing. The transportation of radiation-damaged cladding is tricky, because it is so fragile. After being removed from the reactor and cooling down in the spent fuel pool, the hydrides within the cladding of an assembly reorient themselves so that they radially point out from the fuel, rather than circularly in the direction of the hoop stress. This puts the fuel in a situation so that when it is moved to its final resting place, if the cask were to fall, the cladding would be so weak it could break and release the spent fuel pellets inside the cask.\n\nZirconium alloys can undergo stress corrosion cracking when exposed to iodine, the iodine is formed as a fission product which depending on the nature of the fuel can escape from the pellet. It has been shown that iodine causes the rate of cracking in pressurised zircaloy-4 tubing to increase.\n\nIn the cases of carbon dioxide cooled graphite moderated reactors such as magnox and AGR power reactors an important corrosion reaction is the reaction of a molecule of carbon dioxide with graphite (carbon) to form two molecules of carbon monoxide. This is one of the processes which limits the working life of this type of reactor.\n\nIn a water-cooled reactor the action of radiation on the water (radiolysis) forms hydrogen peroxide and oxygen. These can cause stress corrosion cracking of metal parts which include fuel cladding and other pipework. To mitigate this hydrazine and hydrogen are injected into a BWR or PWR primary cooling circuit as corrosion inhibitors to adjust the redox properties of the system. A review of recent developments on this topic has been published.\n\nIn a loss-of-coolant accident (LOCA) it is thought that the surface of the cladding could reach a temperature between 800 and 1400 K, and the cladding will be exposed to steam for some time before water is reintroduced into the reactor to cool the fuel. During this time when the hot cladding is exposed to steam some oxidation of the zirconium will occur to form a zirconium oxide which is more zirconium rich than zirconia. This Zr(O) phase is the α-phase, further oxidation forms zirconia. The longer the cladding is exposed to steam the less ductile it will be. One measure of the ductility is to compress a ring along a diameter (at a constant rate of displacement, in this case 2 mm min) until the first crack occurs, then the ring will start to fail. The elongation which occurs between when the maximum force is applied and when the mechanical load is declined to 80% of the load required to induce the first crack is the L value in mm. The more ductile a sample is the greater this L value will be.\n\nIn one experiment the zirconium is heated in steam to 1473 K, the sample is slowly cooled in steam to 1173 K before being quenched in water. As the heating time at 1473 K is increased the zirconium becomes more brittle and the L value declines.\n\nIrradiation causes the properties of steels to become poorer, for instance SS316 becomes less ductile and less tough. Also creep and stress corrosion cracking become worse. Papers on this effect continue to be published.\n\nThis is due to the fact that as the fuel expands on heating, the core of the pellet expands more than the rim. Because of the thermal stress thus formed the fuel cracks, the cracks tend to go from the center to the edge in a star shaped pattern. A PhD thesis on the subject has been published by a student at the Royal Institute of Technology in Stockholm (Sweden).\n\nThe cracking of the fuel has an effect on the release of radioactivity from fuel both under accident conditions and also when the spent fuel is used as the final disposal form. The cracking increases the surface area of the fuel which increases the rate at which fission products can leave the fuel.\n\nThe temperature of the fuel varies as a function of the distance from the center to the rim. At distance \"x\" from the center the temperature (T) is described by the equation where ρ is the power density (W m) and K is the thermal conductivity.\n\nT = T + ρ (r² – \"x\"²) (4 K)\n\nTo explain this for a series of fuel pellets being used with a rim temperature of 200 °C (typical for a BWR) with different diameters and power densities of 250 Wm have been modeled using the above equation. These fuel pellets are rather large; it is normal to use oxide pellets which are about 10 mm in diameter.\n\nTo show the effects of different power densities on the centerline temperatures two graphs for 20 mm pellets at different power levels are shown below. It is clear that for all pellets (and most true of uranium dioxide) that for a given sized pellet that a limit must be set on the power density. It is likely that the maths used for these calculations would be used to explain how electrical fuses function and also it could be used to predict the centerline temperature in any system where heat is released throughout a cylinder shaped object.\n\nThe heating of pellets can result in some of the fission products being lost from the core of the pellet. If the xenon can rapidly leave the pellet then the amount of Cs and Cs which is present in the gap between the cladding and the fuel will increase. As a result, if the zircaloy tubes holding the pellet are broken then a greater release of radioactive caesium from the fuel will occur. It is important to understand that the Cs and Cs are formed in different ways, and hence as a result the two caesium isotopes can be found at different parts of a fuel pin.\n\nIt is clear that the volatile iodine and xenon isotopes have minutes in which they can diffuse out of the pellet and into the gap between the fuel and the cladding. Here the xenon can decay to the long lived caesium isotope.\n\nThese fission yields were calculated for U assuming thermal neutrons (0.0253 eV) using data from the chart of the nuclides.\n\nIn the case of Cs the precursor to this isotope is stable Cs which is formed by the decay of much longer lived xenon and iodine isotopes. No Cs is formed without neutron activation as Xe is a stable isotope. As a result of this different mode of formation the physical location of Cs can differ from that of Cs.\n\nThese fission yields were calculated for U assuming thermal neutrons (0.0253 eV) using data from the chart of the nuclides.\n\nIn a recent study, used 20% enriched uranium dispersed in a range of different matrices was examined to determine the physical locations of different isotopes and chemical elements.\n\n\nThe fuels varied in their ability to retain the fission xenon; the first of the three fuels retained 97% of the Xe, the second retained 94% while the last fuel only retained 76% of this xenon isotope. The Xe is a long-lived radioactive isotope which can diffuse slowly out of the pellet before being neutron activated to form Cs. The more short-lived Xe was less able to leach out of the pellets; 99%, 98% and 95% of the Xe was retained within the pellets. It was also found that the Cs concentration in the core of the pellet was much lower than the concentration in the rim of the pellet, while the less volatile Ru was spread more evenly throughout the pellets.\n\nThe following fuel is particles of solid solution of urania in yttria-stabilized zirconia dispersed in alumina which had burnt up to 105 GW-days per cubic meter. The scanning electron microscope (SEM) is of the interface between the alumina and a fuel particle. It can be seen that the fission products are well confined to within the fuel, little of the fission products have entered the alumina matrix. The neodymium is spread throughout the fuel in a uniform manner, while the caesium is almost homogenously spread out throughout the fuel. The caesium concentration is slightly higher at two points where xenon bubbles are present. Much of the xenon is present in bubbles, while almost all of the ruthenium is present in the form of nanoparticles. The ruthenium nanoparticles are not always colocated with the xenon bubbles.\n\nAt Three Mile Island a recently SCRAMed core was starved of cooling water, as a result of the decay heat the core dried out and the fuel was damaged. Attempts were made to recool the core using water. According to the International Atomic Energy Agency for a 3,000 MW(t) PWR the normal coolant radioactivity levels are shown below in the table, and the coolant activities for reactors which have been allowed to dry out (and over heat) before being recovered with water. In a gap release the activity in the fuel/cladding gap has been released while in the core melt release the core was melted before being recovered by water.\n\nThe release of radioactivity from the used fuel is greatly controlled by the volatility of the elements. At Chernobyl much of the xenon and iodine was released while much less of the zirconium was released. The fact that only the more volatile fission products are released with ease will greatly retard the release of radioactivity in the event of an accident which causes serious damage to the core. Using two sources of data it is possible to see that the elements which were in the form of gases, volatile compounds or semi-volatile compounds (such as CsI) were released at Chernobyl while the less volatile elements which form solid solutions with the fuel remained inside the reactor fuel.\n\nAccording to the OECD NEA report on Chernobyl (ten years on), the following proportions of the core inventory were released. The physical and chemical forms of the release included gases, aerosols and finely fragmented solid fuel. According to some research the ruthenium is very mobile when the nuclear fuel is heated with air.\n\nSome work has been done on TRISO fuel under similar conditions.\n\nThe releases of fission products and uranium from uranium dioxide (from spent BWR fuel, burnup was 65 GWd t) which was heated in a Knudsen cell has been repeated. Fuel was heated in the Knudsen cell both with and without preoxidation in oxygen at \"c\" 650 K. It was found even for the noble gases that a high temperature was required to liberate them from the uranium oxide solid. For unoxidized fuel 2300 K was required to release 10% of the uranium while oxidized fuel only requires 1700 K to release 10% of the uranium.\n\nAccording to the report on Chernobyl used in the above table 3.5% of the following isotopes in the core were released Np, Pu, Pu, Pu, Pu and Cm.\n\nWater and zirconium can react violently at 1200 °C, at the same temperature the zircaloy cladding can react with uranium dioxide to form zirconium oxide and a uranium/zirconium alloy melt.\n\nIn France a facility exists in which a fuel melting incident can be made to happen under strictly controlled conditions. In the PHEBUS research program fuels have been allowed to heat up to temperatures in excess of the normal operating temperatures, the fuel in question is in a special channel which is in a toroidal nuclear reactor. The nuclear reactor is used as a \"driver core\" to irradate the test fuel. While the reactor is cooled as normal by its own cooling system the test fuel has its own cooling system, which is fitted with filters and equipment to study the release of radioactivity from the damaged fuel. Already the release of radioisotopes from fuel under different conditions has been studied. After the fuel has been used in the experiment it is subject to a detailed examination (PIE), In the 2004 annual report from the ITU some results of the PIE on PHEBUS (FPT2) fuel are reported in section 3.6.\n\nThe Loss of Fluid Tests (LOFT) were an early attempt to scope the response of real nuclear fuel to conditions under a loss-of-coolant accident, funded by USNRC. The facility was built at Idaho National Laboratory, and was essentially a scale-model of a commercial PWR. ('Power/volume scaling' was used between the LOFT model, with a 50MWth core, and a commercial plant of 3000MWth).\n\nThe original intention (1963–1975) was to study only one or two major (large break) LOCA, since these had been the main concern of US 'rule-making' hearings in the late 1960s and early 1970s. These rules had focussed around a rather stylised large-break accident, and a set of criteria (e.g. for extent of fuel-clad oxidation) set out in 'Appendix K' of 10CFR50 (Code of Federal Regulations). Following the accident at Three Mile Island, detailed modelling of much smaller LOCA became of equal concern.\n\n38 LOFT tests were eventually performed and their scope was broadened to study a wide spectrum of breach sizes. These tests were used to help validate a series of computer codes (such as RELAP-4, RELAP-5 and TRAC) then being developed to calculate the thermal-hydraulics of LOCA.\n\n\nExtensive work was done from 1970 to 1990 on the possibility of a steam explosion or FCI when molten 'corium' contacted water. Many experiments suggested quite low conversion of thermal to mechanical energy, whereas the theoretical models available appeared to suggest that much higher efficiencies were possible. A NEA/OECD report was written on the subject in 2000 which states that a steam explosion caused by contact of corium with water has four stages.\n\n\nWork in Japan in 2003 melted uranium dioxide and zirconium dioxide in a crucible before being added to water. The fragmentation of the fuel which results is reported in the \"Journal of Nuclear Science and Technology\".\n\nA review of the subject can be read at and work on the subject continues to this day; in Germany at the FZK some work has been done on the effect of thermite on concrete, this is a simulation of the effect of the molten core of a reactor breaking through the bottom of the pressure vessel into the containment building.\n\nThe corium (molten core) will cool and change to a solid with time. It is thought that the solid is weathering with time. The solid can be described as \"Fuel Containing Mass\", it is a mixture of sand, zirconium and uranium dioxide which had been heated at a very high temperature until it has melted. The chemical nature of this \"FCM\" has been the subject of some research. The amount of fuel left in this form within the plant has been considered. A silicone polymer has been used to fix the contamination.\n\nThe Chernobyl melt was a silicate melt which did contain inclusions of Zr/U phases, molten steel and high uranium zirconium silicate. The lava flow consists of more than one type of material—a brown lava and a porous ceramic material have been found.\nThe uranium to zirconium for different parts of the solid differs a lot, in the brown lava a uranium rich phase with a U:Zr ratio of 19:3 to about 38:10 is found. The uranium poor phase in the brown lava has a U:Zr ratio of about 1:10. It is possible from the examination of the Zr/U phases to know the thermal history of the mixture. It can be shown that before the explosion that in part of the core the temperature was higher than 2000 °C, while in some areas the temperature was over 2400–2600 °C.\n\nUranium dioxide films can be deposited by reactive sputtering using an argon and oxygen mixture at a low pressure. This has been used to make a layer of the uranium oxide on a gold surface which was then studied with AC impedance spectroscopy.\n\nAccording to the work of the corrosion electrochemist Shoesmith the nanoparticles of Mo-Tc-Ru-Pd have a strong effect on the corrosion of uranium dioxide fuel. For instance his work suggests that when the hydrogen (H) concentration is high (due to the anaerobic corrosion of the steel waste can) the oxidation of hydrogen at the nanoparticles will exert a protective effect on the uranium dioxide. This effect can be thought of as an example of protection by a sacrificial anode where instead of a metal anode reacting and dissolving it is the hydrogen gas which is consumed.\n\n"}
{"id": "3137756", "url": "https://en.wikipedia.org/wiki?curid=3137756", "title": "Blue rinse", "text": "Blue rinse\n\nA blue rinse is a dilute hair dye used to reduce the yellowed appearance of grey or white hair, typically associated with older women. In a manner similar to laundry bluing, the blue rinse can make yellow-white hair appear blue-white.\n\nThe blue rinse gained popularity after Jean Harlow's appearance in the 1930 film, \"Hell's Angels\". Queen Elizabeth also contributed to the popularity of the blue rinse in the 1940s.\n\nIn British politics, the term \"Blue Rinse Brigade\" is used to refer to affluent older women involved in Conservative politics, charity work and committees.\n\n"}
{"id": "25646039", "url": "https://en.wikipedia.org/wiki?curid=25646039", "title": "Boston Journal of Natural History", "text": "Boston Journal of Natural History\n\nThe Boston Journal of Natural History (1834-1863) was a scholarly journal published by the Boston Society of Natural History in mid-19th century Massachusetts. Contributors included Charles T. Jackson, Augustus A. Gould, and others. Each volume featured lithographic illustrations, some in color, drawn/engraved by E.W. Bouvé, B.F. Nutting, A. Sonrel, \"et al.\" and printed by Pendleton's Lithography and other firms.\n\nThe journal was continued by \"Memoirs Read Before the Boston Society of Natural History\" in 1863.\n\n"}
{"id": "15131394", "url": "https://en.wikipedia.org/wiki?curid=15131394", "title": "CarbonCast", "text": "CarbonCast\n\nCarbonCast is a precast concrete technology that uses carbon-fibre grid as secondary reinforcing or as a shear truss. It was introduced by AltusGroup, Inc., a North American partnership of 17 precast concrete manufacturers and seven industry suppliers founded to expedite the research and national commercialization of concrete innovations.\n\nThe CarbonCast line of commercial products includes:\n\nThe use of carbon fibre grid over conventional reinforcement yields a number of unique properties to CarbonCast components:\n\nThere are more than 700 projects totaling more than of surface area of CarbonCast technology in these markets:\n\nProminent structures that have used CarbonCast technology include:\n\n\n\n"}
{"id": "28538507", "url": "https://en.wikipedia.org/wiki?curid=28538507", "title": "Carbon retirement", "text": "Carbon retirement\n\nCarbon retirement involves retiring allowances from emission trading schemes as a method for offsetting carbon emissions. Under schemes such as the European Union Emission Trading Scheme, EU Emission Allowances (EUAs) represent the right to release carbon dioxide into the atmosphere, and are issued to all the largest polluters. Buying these allowances and permanently removing them forces industrial companies to reduce their emissions. Over time, the scheme will offer fewer allowances, making it much harder for industrial companies to sustain high emission levels without incurring financial penalties.\n\nUnlike traditional offsetting projects, retirement is straightforward and transparent. There are no complex projects, methodologies, brokers or intermediaries and the issue of additionality is overcome.\n"}
{"id": "5625377", "url": "https://en.wikipedia.org/wiki?curid=5625377", "title": "Chat (mining)", "text": "Chat (mining)\n\nChat is a term for fragments of siliceous rock, limestone, and dolomite waste rejected in the lead-zinc milling operations that accompanied lead-zinc mining in the first half of the 20th century. Historic lead and zinc mining in the Midwestern United States was centered in two major areas: the Tri-State area covering more than in southwestern Missouri, southeastern Kansas, and northeastern Oklahoma and the Old Lead Belt covering about in southeastern Missouri. The first recorded mining occurred in the Old Lead Belt in about 1742. The production increased significantly in both the Tri-state area and the Old Lead Belt during the mid-19th century and lasted up to 1970.\n\nCurrently production still occurs in a third area, the Viburnum Trend, in southeastern Missouri. Mining and milling of ore produced more than 500 million tons of wastes in the Tri-State area and about 250 million tons of wastes in the Old Lead Belt. More than 75 percent of this waste has been removed, with some portion of it used over the years. Today, approximately 100 million tons of chat remain in the Tri-State area. The EPA, the states of Oklahoma, Kansas and Missouri, local communities, and private companies continue to work together in implementing and monitoring\nresponse actions that reduce or remove potential adverse impacts posed by remaining mine wastes contaminated with lead, zinc, cadmium, and other metals.\n\nOre production consisted of crushing and grinding the rock to standard sizes and separating the ore. Ore processing was accomplished in either a dry gravity separation or through a wet washing or flotation separation. Dry processes produced a fine gravel waste commonly called “chat.” The wet processes resulted in the creation of tailing ponds used to dispose of waste material after ore separation. The wastes from wet\nseparation are typically sand and silt size and are called “tailings.” Milling produces large chat waste piles and flat areas with tailings deposited in impoundments. Tailings generally contain higher concentrations of heavy metals and therefore present a higher risk to human health and the environment through direct contact. Chat typically ranges in diameter from 1/4 to 5/8 inch. Intermingled material such as sands\nmeasure 0.033-0.008 inches in diameter and fine tailings are less than in diameter.\n\nAlthough poisonous, chat can be used to improve traction on snow-covered roads; as gravel; and as construction aggregate, principally for railroad ballast, highway construction, and concrete production.\n\n\n"}
{"id": "15464235", "url": "https://en.wikipedia.org/wiki?curid=15464235", "title": "Coulomb gap", "text": "Coulomb gap\n\nFirst introduced by M. Pollak, the Coulomb gap is a soft gap in the Single-Particle Density of States (DOS) of a system of interacting localized electrons.\nDue to the long-range Coulomb interactions, the single-particle DOS vanishes at the chemical potential, at low enough temperatures, such that thermal excitations do not wash out the gap.\n\nAt zero temperature, a classical treatment of a system gives an upper bound for the DOS near the Fermi-energy, first suggested by Efros and Shklovskii. The argument is as follows:\nLet us look at the ground state configuration of the system. Defining formula_1 as the energy of an electron at site formula_2, due to the disorder and the Coulomb interaction with all other electrons (we define this both for occupied and unoccupied sites), it is easy to see that the energy needed to move an electron from an occupied site formula_2 to an unoccupied site formula_4 is given by the expression:\n\nThe subtraction of the last term accounts for the fact that formula_6 contains a term due to the interaction with the electron present at site formula_2, but after moving the electron this term should not be considered. It is easy to see from this that there exists an energy formula_8 such that all sites with energies above it are empty, and below it are full (this is the Fermi energy, but since we are dealing with a system with interactions it is not obvious a-priori that it is still well-defined).\nAssume we have a finite single-particle DOS at the Fermi energy, formula_9. For every possible transfer of an electron from an occupied site i to an unoccupied site j, the energy invested should be positive, since we are assuming we are in the ground state of the system, i.e., formula_10.\nAssuming we have a large system, let us consider all the sites with energies in the interval formula_11 The number of these, by assumption, is formula_12 As explained, formula_13 of these would be occupied, and the others unoccupied. Of all pairs of occupied and unoccupied sites, let us choose the one where the two are closest to each other. If we assume the sites are randomly distributed in space, we find that the distance between these two sites is of order:\nformula_14, where d is the dimension of space.\nPlugging the expression for formula_15 into the previous equation, we obtain the inequality:\nformula_16 where formula_17 is a coefficient of order unity. Since formula_18, this inequality will necessarily be violated for small enough formula_19. Hence, assuming a finite DOS at formula_8 led to a contradiction. Repeating the above calculation under the assumption that the DOS near formula_8 is proportional to formula_22 shows that formula_23. This is an upper bound for the Coulomb gap. Efros considered single electron excitations, and obtained an integro-differential equation for the DOS, showing the Coulomb gap in fact follows the above equation (i.e., the upper bound is a tight bound).\n\nOther treatments of the problem include a mean-field numerical approach, as well as more recent treatments such as, also verifying the upper bound suggested above is a tight bound. Many Monte-Carlo simulations were also performed, some of them in disagreement with the result quoted above. Few works deal with the quantum aspect of the problem.\n\nDirect experimental confirmation of the gap has been done via tunneling experiments, which probed the single-particle DOS in two and three dimensions. The experiments clearly showed a linear gap in two-dimensions, and a parabolic gap in three-dimensions.\nAnother experimental consequence of the Coulomb gap is found in the conductivity of samples in the localized regime.\nThe existence of a gap in the spectrum of excitations would result in a lowered conductivity than that predicted by Mott Variable range hopping. If one uses the analytical expression of the Single-Particle DOS in the Mott derivation, a universal formula_24 is obtained, for any dimension. The observation of this is expected to occur below a certain temperature, such that the optimal energy of hopping would be smaller than the width of the Coulomb gap. The transition from Mott to so-called Efros-Shklovskii Variable Range Hopping has been observed experimentally for various systems. Nevertheless, no rigorous derivation of the Efros-Shklovskii conductivity formula has been put forth, and in some experiments formula_25 behavior is observed, for alpha which fits neither the Mott nor the Efros-Shklovskii theories.\n\n"}
{"id": "22831977", "url": "https://en.wikipedia.org/wiki?curid=22831977", "title": "Crimean Atomic Energy Station", "text": "Crimean Atomic Energy Station\n\nThe Crimean Nuclear Power Station (; ) is an abandoned and unfinished nuclear power plant near the cape of Kazantyp on banks of Aqtas Lake in Crimea (Ukraine/Russia). \n\nConstruction work on the plant started in 1976, and the nearby town of Shcholkine was constructed in 1978 to house workers working on the project. The station was inspected following the Chernobyl disaster of 1986, and was found to be located on a geologically volatile site. Construction of the facility was summarily abandoned in 1989.\n\nBetween 1993 and 1999 the plant was home to the electronic music festival Kazantip. The festival was referred to as the \"Reaktor\" for this reason.\n\nFrom 1998 to 2004 the station was under jurisdiction of the Ministry of Fuel (Ukraine). In 2004 it was passed to the government of Crimea. In 2005 the Crimean representation of the State Property Fund sold the station to an undisclosed firm.\n\n\n"}
{"id": "14518901", "url": "https://en.wikipedia.org/wiki?curid=14518901", "title": "Eileen Kampakuta Brown", "text": "Eileen Kampakuta Brown\n\nEileen Kampakuta Brown (born 1 January 1938) is an Aboriginal elder from Australia. She was awarded the Goldman Environmental Prize in 2003 together with Eileen Wani Wingfield, for their efforts to stop governmental plans for a nuclear waste dump in South Australia's desert land, and for protection of their land and culture.\n\nBrown, Wingfield and other elder women formed the Cooper Pedy Women's Council (\"Kupa Piti Kungka Tjuta)\" in 1995.\n"}
{"id": "3724479", "url": "https://en.wikipedia.org/wiki?curid=3724479", "title": "Enhanced Fujita scale", "text": "Enhanced Fujita scale\n\nThe Enhanced Fujita scale (EF-Scale) rates the intensity of tornadoes in the United States and Canada based on the damage they cause.\n\nImplemented in place of the Fujita scale introduced in 1971 by Tetsuya Theodore Fujita, it began operational use in the United States on February 1, 2007, followed by Canada on April 1, 2013. The scale has the same basic design as the original Fujita scale—six categories from zero to five, representing increasing degrees of damage. It was revised to reflect better examinations of tornado damage surveys, so as to align wind speeds more closely with associated storm damage. Better standardizing and elucidating what was previously subjective and ambiguous, it also adds more types of structures and vegetation, expands degrees of damage, and better accounts for variables such as differences in construction quality.\n\nThe newer scale was publicly unveiled by the National Weather Service at a conference of the American Meteorological Society in Atlanta on February 2, 2006. It was developed from 2000 to 2004 by the Fujita Scale Enhancement Project of the Wind Science and Engineering Research Center at Texas Tech University, which brought together dozens of expert meteorologists and civil engineers in addition to its own resources.\n\nAs with the Fujita scale, the Enhanced Fujita scale remains a damage scale and only a proxy for actual wind speeds. While the wind speeds associated with the damage listed have not undergone empirical analysis (such as detailed physical or any numerical modeling) owing to excessive cost, the wind speeds were obtained through a process of expert elicitation based on various engineering studies since the 1970s as well as from field experience of meteorologists and engineers. In addition to damage to structures and vegetation, radar data, photogrammetry, and cycloidal marks (ground swirl patterns) may be utilized when available.\n\nThe scale was used for the first time in the United States a year after its public announcement when parts of central Florida were struck by multiple tornadoes, the strongest of which were rated at EF3 on the new scale. It was used for the first time in Canada shortly after its implementation there when a tornado developed near the town on Shelburne, Ontario on April 18, 2013, causing up to EF1 damage.\n\nThe six categories for the EF scale are listed below, in order of increasing intensity. Although the wind speeds and photographic damage examples are updated, the damage descriptions given are those from the Fujita scale, which are more or less still accurate. However, for the actual EF scale in practice, damage indicators (the type of structure which has been damaged) are predominantly used in determining the tornado intensity.\nThe EF scale currently has 28 damage indicators (DI), or types of structures and vegetation, each with a varying number of degrees of damage (DoD). Larger degrees of damage done to the damage indicators correspond to higher wind speeds. The links in the right column of the following table describe the degrees of damage for the damage indicators listed in each row.\nThe new scale takes into account quality of construction and standardizes different kinds of structures. The wind speeds on the original scale were deemed by meteorologists and engineers as being too high, and engineering studies indicated that slower winds than initially estimated cause the respective degrees of damage. The old scale lists an F5 tornado as wind speeds of , while the new scale lists an EF5 as a tornado with winds above , found to be sufficient to cause the damage previously ascribed to the F5 range of wind speeds. None of the tornadoes recorded on or before January 31, 2007, will be re-categorized.\n\nEssentially, there is no functional difference in how tornadoes are rated. The old ratings and new ratings are smoothly connected with a linear formula. The only differences are adjusted wind speeds, measurements of which were not used in previous ratings, and refined damage descriptions; this is to standardize ratings and to make it easier to rate tornadoes which strike few structures. Twenty-eight Damage Indicators (DI), with descriptions such as \"double-wide mobile home\" or \"strip mall\", are used along with Degrees of Damage (DOD) to determine wind estimates. Different structures, depending on their building materials and ability to survive high winds, have their own DIs and DODs. Damage descriptors and wind speeds will also be readily updated as new information is learned.\n\nSince the new system still uses actual tornado damage and similar degrees of damage for each category to estimate the storm's wind speed, the National Weather Service states that the new scale will likely not lead to an increase in a number of tornadoes classified as EF5. Additionally, the upper bound of the wind speed range for EF5 is open—in other words, there is no maximum wind speed designated.\n\nFor purposes such as tornado climatology studies, Enhanced Fujita scale ratings may be grouped into classes.\n\nUsually, the classic version always suggests EF4 and EF5 are the most violent tornadoes.\n\nThe table shown to the right shows other variations of the tornado rating classifications based on certain areas.\n\n\n"}
{"id": "49450906", "url": "https://en.wikipedia.org/wiki?curid=49450906", "title": "Eskom Uganda Limited", "text": "Eskom Uganda Limited\n\nEskom Uganda Limited (EUL) is the largest generator of energy in Uganda and was incorporated in 2002 for a 20 year concession under a government regulatory framework.\n\nELU's headquarters are located outside the Nalubaale Hydroelectric Power Station in the town of Njeru, approximately , by road, east of Kampala, Uganda's capital and largest city. The geographical coordinates of Eskom Uganda Headquarters are: 0°26'36.0\"N, 33°11'02.0\"E (Latitude:0.443333; Longitude:33.183889). \n\nFollowing the dissolution of the erstwhile Uganda Electricity Board (UEB) in 2001, the electricity industry in the country was subdivided into four components as shown in the table below.\n\nEUL was formed in November 2002, following a successful bid to operate Nalubaale Power Station, the only existing power plant owned by the government at that time. Later, when the Kiira Hydroelectric Power Station came online in 2003, the Uganda Electricity Generation Company Limited contracted it to EUL as well.\n\nEUL is responsible for operating, maintaining and repairing two government-owned hydroelectric power stations. The older station, Nalubaale, which opened in 1954, has 18 generators with a capacity of 10 megawatts each. The newer station, Kiira, has five generators with a capacity of 40 megawatts each. The power generated is sold to the Uganda Electricity Transmission Company Limited (UETCL), the sole authorized bulk purchaser. UETCL in turn sells it to Umeme, the distributor, who in turn, sells it to end users.\n\nEskom Uganda Limited is a wholly owned subsidiary of Eskom, the South African energy conglomerate . Although fully owned by the South African parent company, EUL is allowed to exercise a wide degree of autonomy. Its chief executive officer reports directly to the board of Eskom Holdings Limited.\n\n\n"}
{"id": "478769", "url": "https://en.wikipedia.org/wiki?curid=478769", "title": "Evolutionary Air and Space Global Laser Engagement", "text": "Evolutionary Air and Space Global Laser Engagement\n\nThe Evolutionary Air and space Global Laser Engagement (EAGLE) is a missile defence plan being developed by the United States Air Force.\n\nThe project is a combination of two separate missile defense efforts: the Aerospace Relay Mirror System and a new, high-altitude airship. The project is designed as a means to destroy enemy missiles before they would have the opportunity to hit targets on American soil. It involves using either ground-based, air-based or space-based lasers deflected off a massive airship (roughly 25 times the size of the Goodyear blimp) covered in mirrors that could destroy missiles but also satellites or spacecraft in a low Earth orbit.\n\nThe first tests of the airship were scheduled to begin in 2006. Its current status as of 2011 is unknown.\n\n"}
{"id": "1290544", "url": "https://en.wikipedia.org/wiki?curid=1290544", "title": "Feroxyhyte", "text": "Feroxyhyte\n\nFeroxyhyte is an oxide/hydroxide of iron, δ-FeO(OH). Feroxyhyte crystallizes in the hexagonal system. It forms as brown rounded to concretionary masses. Feroxyhyte is opaque, magnetic, has a yellow streak, and has a relative density of 4.2.\n\nIt occurs in manganese-iron nodules on the Atlantic and Pacific Ocean floors. It is also found in the Baltic, White, and Kara Seas. Forms under high pressure conditions and reverts to goethite on exposure to surface conditions. It also occurs as cement and coatings on clasts in poorly drained soils and sediments, formed by the rapid oxidation of iron(II) oxide compounds.\n\nIt was first described in 1976 for an occurrence in soils at its type locality: Kolomyya, Ivano-Frankivsk Oblast, Ukraine.\n"}
{"id": "35632302", "url": "https://en.wikipedia.org/wiki?curid=35632302", "title": "Forecast verification", "text": "Forecast verification\n\nForecast verification is a subfield of the climate, atmospheric and ocean sciences dealing with validating, verifying and determining the predictive power of prognostic model forecasts. Because of the complexity of these models, forecast verification goes a good deal beyond simple measures of statistical association or mean error calculations.\n\nTo determine the value of a forecast, we need to measure it against some baseline, or minimally accurate forecast. There are many types of forecast that, while producing impressive-looking skill scores, are nonetheless naive. A \"persistence\" forecast can still rival even those of the most sophisticated models. An example is: \"What is the weather going to be like today? Same as it was yesterday.\" This could be considered analogous to a \"control\" experiment. Another example would be a climatological forecast: \"What is the weather going to be like today? The same as it was, on average, for all the previous days this time of year for the past 75 years\".\n\nThe second example suggests a good method of normalizing a forecast before applying any skill measure. Most weather situations will cycle, since the Earth is forced by a highly regular energy source. A numerical weather model must accurately model both the seasonal cycle and (if finely resolved enough) the diurnal cycle. This output, however, adds no information content, since the same cycles are easily predicted from climatological data. Climatological cycles may be removed from both the model output and the \"truth\" data. Thus, the skill score, applied afterward, is more meaningful.\n\nOne way of thinking about it is, \"how much does the forecast reduce our \"uncertainty\"?\" Tang et al. (2005) \nused the conditional entropy to characterize the uncertainty of ensemble predictions of the El Nino/Southern Oscillation (ENSO):\n\nwhere \"p\" is the ensemble distribution and \"q\" is the climatological distribution.\n\nThe World Meteorological Organization maintains a webpage on forecast verification.\n\nFor more in-depth information on how to verify forecasts see the book by Jolliffe and Stephenson or the book chapter by Daniel Wilks.\n\n"}
{"id": "11336837", "url": "https://en.wikipedia.org/wiki?curid=11336837", "title": "Global hectare", "text": "Global hectare\n\nThe global hectare (gha) is a measurement unit for the ecological footprint of people or activities and the biocapacity of the earth or its regions. One global hectare is the world's annual amount of biological production for human use and human waste assimilation, per hectare of biologically productive land and fisheries.\n\nIt measures production and consumption of different products. It starts with the total biological production and waste assimilation in the world, including crops, forests (both wood production and CO2 absorption), grazing and fishing.\nThe total of these kinds of production, weighted by the richness of the land they use, is divided by the number of hectares used. The calculation does not include deserts, glaciers, and the open ocean. \n\n\"Global hectares per person\" refers to the amount of production and waste assimilation per person on the planet. In 2012 there were approximately 12.2 billion global hectares of production and waste assimilation, averaging 1.7 global hectares per person.\nConsumption totaled 20.1 billion global hectares or 2.8 global hectares per person, meaning about 65% more was consumed than produced. This is possible because there are natural reserves all around the globe that function as backup food, material and energy supplies, although only for a relatively short period of time. Due to rapid population growth, these reserves are being depleted at an ever increasing tempo. See Earth Overshoot Day.\n\nOpponents and defenders of the concept have discussed its strengths and weaknesses.\n\nThe global hectare is a useful measure of biocapacity as it can convert things like human dietary requirements into common units, which can show how many people a certain region on earth can sustain, assuming current technologies and agricultural methods. It can be used as a way of determining the relative carrying capacity of the earth.\n\nDifferent hectares of land can provide different amounts of global hectares. For example, a hectare of lush area with high rainfall would scale higher in global hectares than would a hectare of desert.\n\nIt can also be used to show that consuming different foods may increase the earth's ability to support larger populations. To illustrate, producing meat generally requires more land and energy than what producing vegetables requires; sustaining a meat-based diet would require a less populated planet.\n\nOn average, a global hectare can be produced in the area of a standard hectare. A hectare (; symbol ha) is a unit of area equal to (a square 100 metres on each side or 328 feet on each side), 2.471 acres, 0.01 square kilometers, 0.00386102 square miles, or one square hectometre (100 metres squared).\n"}
{"id": "6869774", "url": "https://en.wikipedia.org/wiki?curid=6869774", "title": "Greengairs Landfill", "text": "Greengairs Landfill\n\nThe Greengairs Landfill is a landfill site in Scotland that receives non-hazardous household, commercial and industrial waste from the North Lanarkshire area. Greengairs was opened in 1990 and features landfill gas collection systems which are used to generate electricity for export into the National Grid.\n\nThe landfill is owned and operated by FCC Environment.\n\n"}
{"id": "12547481", "url": "https://en.wikipedia.org/wiki?curid=12547481", "title": "Grohnde Nuclear Power Plant", "text": "Grohnde Nuclear Power Plant\n\nThe Grohnde Nuclear Power Plant is located in Grohnde in the Hamelin-Pyrmont district in Lower Saxony. It has one reactor that uses 193 fuel assemblies and utilizes both enriched uranium and MOX fuel. In 1985, 1986, 1987, 1989, 1990 and 1998 the reactor produced more net electricity for the respective year than any other reactor in the world.\nThe plant is of the pressurized water reactor type, using four water based coolant cycles, kept under high pressure. About 80,000 people live within of the plant and a total of more than a million people within . The next big city is Hanover away.\n"}
{"id": "49778734", "url": "https://en.wikipedia.org/wiki?curid=49778734", "title": "Grüner Strom Label", "text": "Grüner Strom Label\n\nThe Gruener Strom Label association (the “Grüner Strom Label e.V.\" or \"GSL e.V.“) certifies green energy sources or production. The Bonn based association was founded at the initiative of EUROSOLAR in December 1998. The GSL e.V. issues two seals of approval – one, named \"Grüner Strom\", introduced in 1999, is granted for green electricity, and the other, named \"Grünes Gas\", introduced in 2013, is granted for biogas. By 1999, \"Grüner Strom\" was the first certification for electricity from green power sources in Germany.\"Grüner Strom\" and \"Grünes Gas\" are the only seals of approval in Germany, indicating endorsement by leading environmental organizations. The goal of products recommended by certification is to increase transparency in the green electricity and biogas markets and to advance ecologically sustainable energy supply.\n\nThe Grüner Strom Label e. V. indicates support and endorsement by the following organizations:\n\nMeeting the following essential requirements of the Criteria List is a prerequisite for certification:\nThe funding amounts are mostly used to make financial contributions or to co-finance new green power generating plants that are eligible for support pursuant to the Renewable Energy Sources Act (EEG) at the same time. But they are used for power plants generating electricity for direct sale and distribution as well. Also the funds can partly go to installations abroad and to energy efficiency measures and projects, including citizen energy projects, which are spurring the transformation of the energy system and the associated infrastructure. Electricity providers must report about the projects supported by certification on the Internet. In addition, successfully completed projects are listed on the GSL e.V. website. Over 70 power supply companies offer green electricity certified to the \"Grüner Strom\" criteria. The \"Grüner Strom\" certification is granted for products. The GSL e.V., thereby, recommends individual electricity product options by certification.\n\nFigure 1: Amount of \"Grüner Strom\" Certified Green Electricity Sold Since 1999:\n\nElectricity providers have invested over 55 million euros from their products recommended by the \"Grüner Strom\" certification since inception of the program through 2016 (see figure 2).\n\nFigure 2: Overview of Investments Since 1999\n\nUp to now, thanks to the \"Grüner Strom\" certification model, over 1,000 new electricity-generating plants have been initiated. They cover all sources of renewable energy. In 2013 for example, about 2.5 million euros have been made available, some 80 % of which for renewable electricity generating plants (see table 1).\n\nTable 1: Funded Projects In 2013*\n\nThe \"Grünes Gas\" certification was launched in June 2013. The seal is awarded for gas products when the production, use and distribution of the biogas meet the requirements as set out in the Criteria List. These include for example:\nConformity of suppliers and providers, producers and power supply companies is verified based on rating system credit points. Currently, \"Grünes Gas\" is the only seal of approval for biogas indicating endorsement by environmental organizations, including consumer information groups.\n\nThree biogas products have achieved the \"Grünes Gas\" certification since January 1,2014. Preliminary estimates suggest that some 12,500 consumers bought biogas, which was certified to the \"Grünes Gas\" standards. Gas sales were estimated at 120,200 MWh. Based on the annual average of total sales 14 % of the gas sales was biogas, i.e. the equivalent of some 16,830 MWh.\n\n"}
{"id": "20002436", "url": "https://en.wikipedia.org/wiki?curid=20002436", "title": "HCNH+", "text": "HCNH+\n\nHCNH, also known as protonated hydrogen cyanide, is a molecular ion of astrophysical interest.\n\nThe ground state structure of HCNH is a simple linear molecule. In addition, there are multiple higher-energy isomers such as , HCN, \"cis\"-HCNH, and \"trans\"-HCNH.\n\nAs a relatively simple molecular ion, HCNH has been extensively studied in the laboratory. The very first spectrum taken at any wavelength focused on the \"ν\" (C−H stretch) ro-vibrational band in the infrared.\n\nSoon afterward, the same authors reported on their investigation of the \"ν\" (N−H stretch) band.\n\nFollowing these initial studies, several groups published manuscripts on the various ro-vibrational spectra of HCNH, including studies of the \"ν\" band (C≡N stretch)\nthe \"ν\" band (H−C≡N bend)\nand the \"ν\" band (H−N≡C bend)\n\nWhile all of these studies focused on ro-vibrational spectra in the infrared, it was not until 1998 that technology advanced far enough for an investigation of the pure rotational spectrum of HCNH in the microwave region to take place. At that time, microwave spectra for HCNH and its isotopomers HCND and DCND were published.\nRecently, the pure rotational spectrum of HCNH was measured again in order to more precisely determine the molecular rotational constants \"B\" and \"D\".\n\nAs mentioned above, the most recent laboratory measurement of the pure rotational spectrum of the ground state of HCNH was performed in 2006. This study determined the most precise values of the molecular constants to date. Using these constants in conjunction with the program PGOPHER, one can generate the predicted rotational spectrum for the ground state of HCNH at a temperature of 30 K, as shown below. Being a linear molecule, HCNH follows the basic rotational selection rule of Δ\"J\" = ±1.\nAccording to the database at astrochemistry.net, the most advanced chemical models of HCNH include 71 total formation reactions and 21 total destruction reactions. Of these, however, only a handful dominate the overall formation and destruction. In the case of formation, the 7 dominant reactions are:\n\nUsing rate coefficients from astrochemistry.net and the UMIST Database for Astrochemistry in conjunction with model interstellar abundances\n\none can calculate the relative importance of these 7 reactions as shown in the table below.\n\nBeing an ion, HCNH is predominantly destroyed by the electron recombination reactions:\n\nUsing the same sources as above, the relative importance of these destruction reactions are calculated and shown in the table below. Also shown in the table is the ion-neutral reaction HCNH + HCO, in order to demonstrate just how dominant electron recombination is.\n\nHCNH was first detected in interstellar space in 1986 toward the dense cloud Sgr B2 using the NRAO 12 m dish and the Texas Millimeter Wave Observatory.\nThese observations utilized the \"J\" = 1–0, 2–1, and 3–2 pure rotational transitions at 74, 148, and 222 GHz, respectively.\n\nSince the initial detection, HCNH has also been observed in TMC-1\n\nas well as DR 21(OH)\n. The initial detection toward Sgr B2 has also been confirmed. All 3 of these sources are dense molecular clouds, and to date HCNH has not been detected in diffuse interstellar material.\n\nWhile not directly detected via spectroscopy, the existence of HCNH has in fact been inferred in the atmosphere of Saturn's largest moon, Titan. This was done using the Ion and Neutral Mass Spectrometer (INMS) instrument aboard the Cassini space probe. Models of Titan's atmosphere had predicted that HCNH would be the dominant ion present, and a strong peak in the mass spectrum at Z/M=28 seems to support this theory.\n\nIn 1997, observations were made of the long-period comet Hale–Bopp in an attempt to find HCNH,\n"}
{"id": "41697562", "url": "https://en.wikipedia.org/wiki?curid=41697562", "title": "Horace Mayhew", "text": "Horace Mayhew\n\nHorace Mayhew (20 June 1845 – 15 August 1926) of Broughton Hall, Flintshire, was a British mining engineer and colliery owner who founded the town of Broughton, Nova Scotia, now one of Canada's most famous ghost towns. He was the son of John Mayhew Esq of Platt Bridge House, Co. Lancaster, and Elizabeth Mayhew (née Rapley), JP Lancashire (1876), JP Flintshire (1888), Deputy Lieutenant (1900), and High Sheriff of Flintshire (1904).\n\nEducated at King William’s College, Isle of Man, Mayhew began his career as an apprentice with the Wigan, Coal and Iron Company Ltd. Mayhew soon established himself as a preeminent mining engineer managing and part-owning collieries throughout the Wigan district. He became Managing Director of the Brinsop Hall Colliery Company working in partnership with A. H. Leech.\n\nWigan experienced dramatic economic growth and the population grew rapidly as the coal and textile industry expanded in the 1870s and 1880s. Mayhew was well connected. His brother Walter was Mayor of Wigan between 1876 and 1878 and in 1883 Mayhew formed a partnership with W. E. Gladstone, then Prime Minister. He started managing Gladstone’s extensive collieries and brickworks in Hawarden and in 1888 took over Halls Collieries at Swadlincote, Burton-on-Trent employing close to 2,000 mineworkers.\n\nIn the early 20th Century Horace Mayhew and Thomas Lancaster founded the town of Broughton, Nova Scotia and established the Cape Breton Coal, Iron & Railway Company. Mayhew was President of the company and also President of the Canada Land & Development Corporation on whose land the town of Broughton was built. He was extensively involved in the planning of Broughton and invested heavily in the development of the town and mines, of which there were several workable seams.\n\nConstruction of the town began in 1905, with streets laid out and a number of large official buildings constructed, including the general mining building, a church, the Broughton Arms Hotel and the Crown Hotel. The town was one of the first planned communities in Canada and the Broughton Arms, a palatial hotel, was said to be the ‘best east of Montreal’. The hotel boasted all the modern conveniences including the first revolving door in the Americas. The distinctive architecture of rounded towers and verandahs marked it as an upper-class hotel.\n\nMayhew’s eldest son, Horace Dixon Mayhew Jnr, came out to Cape Breton with his father, spending the winter of 1906 at Broughton. Mayhew lobbied the Nova Scotia government (of George Murray) to increase rail subsidies without success and the untimely death of his son on 12 August 1906 coincided with the decline in Broughton's success.\n\nThe company went bankrupt in 1907 after it failed to secure rail transportation to get its coal to port, largely because of opposition from its Cape Breton competition, the Dominion Coal Company. Mayhew returned to Flintshire and established a new practice with his remaining sons. The firm Mayhew & Mayhew later owned several collieries in the Buckley and Hawarden districts, including Aston Hall, Mare Hey, Dublin Main and Main coal collieries. He died in 1926.\n"}
{"id": "31358838", "url": "https://en.wikipedia.org/wiki?curid=31358838", "title": "House of Taga", "text": "House of Taga\n\nThe House of Taga (Chamoru: \"Guma Taga\") is an archeological site located near San Jose Village, on the island of Tinian, United States Commonwealth of the Northern Mariana Islands, in the Marianas Archipelago. The site is the location of a series of prehistoric latte stone pillars which were quarried about south of the site, only one of which is left standing erect due to past earthquakes. The name is derived from a mythological chief named Taga, who is said to have erected the pillars as a foundation for his own house.\n\nThe prehistoric latte stone pillars (also called taga stones) at House of Taga stood high, and were quarried about south of the site. The original megaliths consisted of a base (haligi) and a hemispherical cap (tasa). When uprighted in spaced parallel rows, it is believed a house was built on top. Of the twelve upright stones sketched by British explorer George Anson during his 1742 visit to Tinian, only one remains standing. The As Nieves Rota Latte Stone Quarry is believed to be the Marianas origin of the period of these megalith structures.\n\nIn his 1957 published work \"Marianas Prehistory: Survey and Excavations on Saipin, Tinian and Rota\", anthropologist Alexander Spoehr noted that the House of Taga had most likely been the central latte structure among eighteen such structures on Tinian. According to legend collected by Spoehr, a tall Guam chief named Taga married a Rota woman. The chief began the Rota Latte Stone Quarry to carve stone for their home. Midway through, he changed his mind and instead built the house on Tinian.\n\nOne variation on that story also puts Taga's origins at Guam. As a child, he began demonstrating such super-human strength that his own father was jealous and tried to kill him. Taga escaped from his father by jumping off a cliff on Guam and landing on Rota, almost away. On Rota, Taga grew to adulthood and became a braggart about his prowess. According to the legend, it was Taga who began the As Nieves Quarry on Rota and abandoned it for reasons that are unclear.\n\nTaga married and had a family, and they sailed to Tinian. His reputation had preceded him, and the chief of Tinian presented several challenges to test Taga's strength. The chief was subsequently so impressed with Taga's abilities that he named Taga the chief of the island. Taga carved the latte stones to single-handedly build his Tinian house and a village for his people. He carried the multi-ton pillars all by himself. As the years passed, while on a trip to Saipan, Taga's wife gave birth to a son that Taga bragged about to no end. Taga's brother tired of all the bragging and challenged Taga to a contest, which ended in a draw between the two brothers. This was the end of Taga's bragging.\n\nEventually, Taga and his wife were the parents of twelve children. When Taga realized that his youngest son had greater strength than he, Taga flew into a jealous rage and murdered his son. Taga's wife died of grief. His youngest daughter then speared Taga to death and died of a broken heart. As each of Taga's twelve children died, their spirits inhabited the latte stones of his house. Each spirit was released by the individual latte stone falling to the ground. The lone standing megalith today is said to imprison the spirit of the daughter who murdered Taga.\n"}
{"id": "3028152", "url": "https://en.wikipedia.org/wiki?curid=3028152", "title": "Jasper conglomerate", "text": "Jasper conglomerate\n\nJasper conglomerate, is an informal term for a very distinctive Paleoproterozoic quartz and jasper pebble conglomerate that occurs within the middle part of the Lorrain Formation of the Cobalt Group of the Huronian Supergroup. It is also known by other names including pebble jasper conglomerate, St. Joseph Island puddingstone, Drummond Island puddingstone, Michigan puddingstone. The jasper conglomerate occurs on St. Joseph Island and the St. Mary's River area north and northwest of the Bruce Mines of Northern Ontario, about east of Sault Ste. Marie. This conglomerate consisted originally of gravelly sands and sandy gravels composed of subrounded pebbles of red jasper, white quartzite, semi-transparent quartz, and black chert, with coarse-grained sand matrix. Typically it contains between about 30% to as much as 90% pebbles. It has been cemented and partially metamorphosed into a quartzitic conglomerate. The beds of jasper conglomerates fill erosional troughs and channels of what are interpreted to be either alluvial fan or braided river deposits of the Lorrain Formation. These deposits are interpreted to represent nonglacial deposits that immediately postdate the Makganyene glaciation.\nBecause of its distinctive nature, pebble- to boulder-size fragments of jasper conglomerate can be recognized as glacial erratics in Pleistocene glacial tills and drift within large parts of the glaciated Midwestern United States. Fragments of jasper conglomerate were eroded by continental ice-sheets from Northern Ontario and spread across all of Michigan and as far south as Ohio and Kentucky during repeated glacial advances and retreats. For example, pebble to boudler-size fragments of jasper conglomerate are quite common on Drummond Island, Michigan where it is called Drummond Island puddingstone.\n\n\n"}
{"id": "13937333", "url": "https://en.wikipedia.org/wiki?curid=13937333", "title": "Jig doll", "text": "Jig doll\n\nJig dolls are traditional wooden or tin-plate 'toys' for adults or children. They are dolls with loose limbs that step dance or 'jig' on the end of a vibrating board or platform in imitation of a real step dancer. In London they were frequently operated by street entertainers or buskers. In England old soldiers from the Great War sometimes busked with them to supplement their meagre war pensions. Typically the dolls are between 20–30 cm tall and are jointed at arms, hips and knees; some also have ankle joints. Today, jig dolls of one kind or another can be seen in the USA, Canada, the UK, Ireland, Europe, parts of Asia, and Australia.\n\nIn the UK and Australia, a jig doll usually goes by that name, or any of the following: dancing doll; busker's puppet; clogger; jigger; Mr. Jollyboy or Mrs. Jollyboy (a commercial version made by Dover Toys, UK), etc. A Mr Jollyboy is in the collection of the Norwich Museum.\n\nIn the USA, a jig doll would be called a limberjack or limberjill or limbertoy; paddle puppet; stick puppet. A commercial version was called: Dancing Dan or Dancin' Dan; Dapper Dan; Dancing Jo or Dancin' Jo; Stepping Sam or Steppin' Sam, etc.\n\nIn French-speaking parts of Canada they are referred to as \"les gigueux\".\n\nIn one old patent the term Manipulable Doll was used.\n\nDancing dolls have been popular street entertainment for hundreds of years. Older versions dating back to the 16th century were known as \"Poupées à la Planchette\" or \"Marionettes à la Planchette.\" These puppets, operated by a horizontal string attached to the musician's leg, 'danced' on a board on the ground as the musician tapped his foot. They were, and still are, popular street entertainment throughout Europe.\n\nAt some stage, possibly in the mid-19th century, the string was replaced by a wooden rod fixed into the back of the body, or attached to a wire loop on the top of the doll's head, with the doll dancing on a vibrating board. Later, some jig dolls were automated.\n\nThe East Anglian Traditional Music Trust (EATMT) reports that the earliest jig doll yet discovered is one from the Victorian Great Exhibition at The Crystal Palace dating from 1851. A female figure, dressed in a skirt, petticoat, bodice and shawl, it is now in the Cliffe Castle Museum, Keighley, Yorkshire.\"\n\nOld ones have become collectors' items and can fetch high prices. Some antique clockwork tin-plate 'jiggers' can fetch anything up to £2,000 (in 2009).\n\nTraditional English folk singers and musicians such as Bob Cann from Dartmoor, Billy Bennington and Walter Pardon (both from Norfolk) sometimes made their own jig dolls. In England, jig dolls have perhaps survived better in East Anglia than other parts of the country; the EATMT has commissioned a collection of them.\n\nJig dolls are essentially home-made toys. Typical versions could represent sailors, male and/or female costumed folk-dancers, African-Americans, Native Americans, Morris dancers, Punch and Judy, Adolf Hitler, even animals such as frogs, horses, chickens, dogs, and cows, etc. They may be clothed, painted or left as bare polished wood. Sometimes the heads are whittled to show distinctive facial features. Historical figures such as Harry Lauder and more recent ones such as John Major (dancing on a board bearing an image of Margaret Thatcher) have been made.\n\nSome Punch and Judy Professors also use jig dolls to attract a crowd. One has a jig doll of Charlie Chaplin. In the UK, some folk dance bands have a jig doll to entertain the audience in the interval of a barn dance. Such dolls may occasionally appear at live traditional music sessions in English pubs (in the past, step dancing by members of the audience would have been a common feature of such a pub session).\n\nIn Québec and French-speaking Canada, jig dolls can feature as a percussion instrument for a folk dance band, even to the extent of the plank on which a doll 'dances' being fitted with a small microphone connected to the PA\n\n\n\n\n"}
{"id": "6169910", "url": "https://en.wikipedia.org/wiki?curid=6169910", "title": "Jiyeh Power Station oil spill", "text": "Jiyeh Power Station oil spill\n\nThe Jiyeh Power Station oil spill is an environmental disaster caused by the release of heavy fuel oil into the eastern Mediterranean after storage tanks at the thermal power station in Jiyeh, Lebanon, south of Beirut, were bombed by the Israeli Air force on July 14 and July 15, 2006 during the 2006 Israel-Lebanon conflict. The plant's damaged tanks leaked up to 30,000 tonnes of oil into the eastern Mediterranean Sea, A 10 km wide oil slick covered 170 km of coastline,\nand threatened Turkey and Cyprus. The slick killed fish, threatened the habitat of endangered green sea turtles, and potentially increased the risk of cancer.\n\nAlthough Al Jazeera compared the scale of the oil spill to that of the Exxon Valdez oil spill, later assessment found that the volume spilled was 15 000 - 30 000 tonnes compared to 42 000 tonnes for the Exxon Valdez oil spill. The coastline affected was between 150–170 km, while the Exxon Valdez oil spill affected 2,100 km of coastline.\n\nAccording to Lebanon's Environment Minister Yacoub Sarraf, Israeli jets deterred firemen from putting out the fire at the storage units, which continued for 10 days, and the Israeli Navy blockade stopped Lebanese and foreign officials from surveying the damage of the spill.\n\nThe spill affected one-third of Lebanon's coastline. Beaches and rocks were covered in a black sludge up to Byblos, north of Beirut and extended into the southern parts of Syria. The slick killed fish, and threatens the habitat of the endangered green sea turtle as well as the endangered logger head sea turtle.\n\n\n\nThe United Nations General Assembly passed a resolution by 170 votes to 6 (Australia, Canada, Israel, Marshall Islands, Palau, and United States voting against) calling upon Israel to assume responsibility for compensation for the costs of repairing the environmental damage and restoration of the marine environment.\n\nIn the United Nations Second Committee where the draft resolution was discussed, the United States explained its opposition in terms of its one-sided and inappropriate nature, while Canada felt that \"the General Assembly was not the appropriate forum to address questions of legal liability or compensation of the cost of repairing environmental damage\". Israel objected to the omission of any mention of \"the entire reason for the conflict\", namely that \"Hizbollah terrorists had crossed an internationally recognized border into Israel and kidnapped and killed Israeli soldiers\", as well as the lack of concern for the \"half a million trees and 52,000 dunams of forest that had burnt down in Israel as a result of fires caused by Hizbollah rockets; the 25 Israeli cement and asbestos buildings that had been damaged, polluting an area of 20,000 square metres; or the direct hit by a Katyusha rocket on a sludge-thickening plant in Tzafat.\"\n\nThe estimate by the UN of the cost of the oil spill in terms of harm to the Lebanese economy and cleaning up operations is $203million. Israel has refused to answer any request for compensation.\n\nFollowing this there was a precise rerun of the discussion and vote in December 2007, with the Australia, Canada, the Marshall Islands, Nauru, Palau and the United States coming out again against a General Assembly Resolution requesting Israel to take responsibility. In its defence the ambassador from Israel said the failure to mention the environmental catastrophes in Israel (half a million trees on fire), as well as the entire cause of the conflict (Hizbollah crossing the southern border of Lebanon to kidnap Israeli soldiers) proved that the resolution was an act of political demonization.\n\nA documentary by Hady Zaccak called \"The Oil Spill in Lebanon\" won first prize at the \"European and Mediterranean Film Festival on the TV of the Sea\"\n\nThe Lebanon Ministry of Environment, with the help of many donor countries and non-profit organizations, has been working on the clean-up effort. This includes legal issues, such as compensation, which is being dealt\nwith through the United Nations General Assembly. The government of Israel has still, to this day, not compensated the government of Lebanon for the damage that was done. The requests of compensation from the United Nations to the government of Israel went unanswered. The objective of the entire project is to manage the environmental catastrophe by attaining funds to complete the clean-up.\nThe total amount that was raised by donor countries and organizations was $2,919,400. Cleanup work was funded by the country of Japan through the UNDP and by the country of Norway through the Higher Relief Commission in Lebanon. Expenditures between the years of 2006 and 2007 where estimated to be around $1,835,587.\nThe clean-up effort included the Dalieh Fishermen's wharf and three sites between Jadra and Ras al Saadiyat and three sites between Jieh and Beirut. Surveys of the coasts from Southern to Norther Lebanon\ntook place in coordination with surveys of the underwater shorelines. A comprehensive shoreline survey of the catastrophic oil spill was put into action from the southern city of Tyre to the northern border of Lebanon, and it was undertaken between the months of November and December 2008, with funding from the International Development Agency of the Canadian government through UNDP in coordination with the Ministry of the Environment of Lebanon and under its technical supervision. The findings were finally, and with long anticipation, published by the United Nations in September 2009. Minor and pre-final clean-up activities at about 12 sites along the Lebanese coast, with a total length of approximately 7.5 kilometers, were quickly put into play and monitored by the Ministry of the Environment of the government of Lebanon.\n\nA waste management project was also put in place in order to safely collect and transport the polluting material from the shorelines to temporary storage sites. The most recent United Nations report was published on August 7, 2009. This was the final report made by the United Nations General Assembly. The General Assembly decided to establish an Eastern Mediterranean Oil Spill Restoration Trust Fund, based on contributions from donor countries and organizations, to provide assistance and support to the States directly and adversely affected in their management of an environmentally sound sea and wildlife, from clean-up process to safe disposal of the oily waste that is on the shorelines, of this environmental disaster caused by the destruction of the oil storage tanks at the Jieh electric power plant in Lebanon.\n\n"}
{"id": "10531004", "url": "https://en.wikipedia.org/wiki?curid=10531004", "title": "Lloró", "text": "Lloró\n\nLloró is a municipality and town in the Chocó Department, Colombia. It claims the second world record for highest average annual precipitation with , after López de Micay, which holds the record with . The rainfall data was measured in its Agricultural Farm, managed by the University of Bogotá, between 1952 and 1989. If accurate, that would make it the wettest place in the world. The town is named for Gioró, a pre-Columbian indigenous chief.\n\nAn 1853 watercolor by Manuel María Paz portrays two men in straw hats with a female vendor at a liquor stand in Lloró.\n\n\n"}
{"id": "631310", "url": "https://en.wikipedia.org/wiki?curid=631310", "title": "Mark (unit)", "text": "Mark (unit)\n\nThe Mark (from Middle High German: Marc, march, brand) is originally a medieval weight or mass unit, which supplanted the pound weight as a precious metals and coinage weight from the 11th century. The Mark is traditionally a half pound weight and was usually divided into 8 ounces or 16 Lot. The significance of the \"Cologne Mark\" (Kölner Mark) in the German-speaking areas corresponded to about 234 gram.\n\nLike the German systems, the French poids de marc weight system considered one \"Marc\" equal to half-a-pound (8 ounces).\n\nLike the pound of 12 troy ounces (373 g), the mark was also used as a unit of currency, e.g. in the 1087 poem \"The Rime of King William\" (\"marks of gold\"), in many Shakespearean plays set in medieval England, and in various incarnations in Germany and Finland until the adoption of the euro in 1999.\n\n"}
{"id": "10143800", "url": "https://en.wikipedia.org/wiki?curid=10143800", "title": "Mochovce Nuclear Power Plant", "text": "Mochovce Nuclear Power Plant\n\nThe Mochovce Nuclear Power Plant (, abbr. EMO) is a nuclear power plant located between the towns of Nitra and Levice, on the site of the former village of Mochovce, Slovakia. Two up-rated 470 MW (originally 440MW) reactors are presently in operation, with two further reactors of the same type under construction. Generating almost 7,000 GW·h of electricity a year, the power plant currently serves approximately 20% of Slovakia's energy needs.\n\nA power plant consisting of four VVER 440/V-213 pressurized water reactors was proposed in the 1970s. The Czechoslovak government began with a geological survey to find a suitable seismically stable site. After taking into account all factors the location of the village of Mochovce was chosen. Preparatory work was started on June 1981, and site construction for Mochovce-1 and Mochovce-2 started in November 1982.\n\nConstruction of the remaining two units, Mochovce-3 and Mochovce-4, began in 1985 but work on all four units was halted in 1991 due to a lack of funds. In 1995 the Slovak government approved a plan to finish the first pair with additional Western safety technology. The first two units were commissioned in 1998 and 1999 respectively. Commissioning of the plant has sparked protests in Austria, a neighboring country strongly opposed to the use of nuclear energy in general. Installed capacity of units 1 and 2 was up-rated by 7% in 2008.\n\nConstruction of Units 3 and 4 restarted in November 2008. They were planned initially to be completed in 2012 and 2013, but the completion date was shifted to 2016 and 2017. More recently the completion date has slipped to 2018 and 2019.\n\nThe owner of the plant is Slovenské elektrárne, which is 34% state-owned. Enel, an Italian utility company, was the majority 66% owners, but sold half its stake to Czech energy group EPH in 2017. Enel plans to sell its remaining stake after completion of units 3&4.\n\nSince late 2008 the two operating units at Mochovce NPP have uprated power output to 235 MWe per turbogenerator, ergo the total installed capacity of units 1 and 2 in Mochovce NPP stands at 940 MWe. In 2009, the plant managed to generate over 7 TWh of electricity during a one-year period for the first time in its history. This represents approximately one quarter of the overall annual electricity consumption in Slovakia.\nAll units at Bohunice and Mochovce NPP feature evolutionary WWER pressurised water reactors, which are characteristic with:\n\nThe principle of electricity generation in nuclear power plants is similar to that of conventional thermal power plants. The only difference is the heat source. In thermal plants, heat is produced from fossile fuels (coal, gas), generating vast quantities of greenhouse gas, whereas in nuclear power plants, nuclear fuel is used (natural or enriched uranium).\nIn the pressurized water reactors, fuel in the form of fuel assemblies is placed in reactor pressure vessel to which chemically terated water flows. The water flows through channels in the fuel assemblies and removes the heat that is produced during fission reaction. Water coming from the reactor has a temperature of about 297 °C (VVER reactor type); it is then led through the hot arm of the primary piping into heat exchanger - steam generator. In steam generator, water flows through a bunch of pipes and delivers the heat to water from secondary circuit and has a temperature of 222 °C. When cooled down, primary circuit water is led back to reactor core. Secondary circuit water is evaporated in steam generator and the steam is led via steam collector to the blades of a turbine. The turbine shaft turns a generator that generates electric energy.\nHaving delivered its energy to the turbine, steam condensates in condenser, and getting back to water state, it flows back, via heaters, to steam generator. In condenser, the mixture is cooled by a third cooling circuit. In this latter circuit, water is cooled by air flowing from the bottom to the upper part of the cooling tower due to so-called chimney effect. The stream of air takes along water steam and small water droplets, and that is why clouds of steam form above the cooling towers.\n\nUnits 3&4 of Mochovce NPP are currently under construction. This project is:\n\nUnit 3 is expected to be operational by the end of 2018 and unit 4 by the end of 2019.\n\nAlthough the original power plant design featured safety improvements such as seismically resistant attachment of equipment, it did not suit the safety and regulatory environment of the 1990s. To rectify this the German company Siemens supplied a new control system, and Western and EU safety measures were implemented during the final phases of construction. According to the plant operator Mochovce nuclear power plant was the first Soviet-sourced nuclear plant in the former Eastern Bloc to meet the safety standards of Western nuclear power plants.\nThe nuclear industry was conceived and developed with a conscientious awareness of having to face and respect strict safety guidelines, as well as technical, environmental and health standards.\nFor this reason, safety at a power plant is verified and controlled at maximum possible levels in all of its phases (project design, authorization, construction, operation, decommissioning and final dismantlement), using procedures that have been elaborated exclusively for the needs of this specific sector.\nIn addition, the realization of a nuclear power station is subordinated by a particularly complex authorization process comprising two different components: authorization from a nuclear safety standpoint, and in terms of its environmental impact (EIA – Environmental Impact Assessment).\nIn concrete terms, safety in the electronuclear industry is entrusted to a set of technical, organizational and human measures enacted throughout all stages of an installation's lifetime, with the aim of protecting citizens and the surrounding environment from a release of radioactive material under any circumstances.\n\nNuclear power plants emit no greenhouse gas to the atmosphere, in this way NPPs annually contribute to CO2 emission reduction by 15 million tonnes in Slovakia.\nNuclear power plants hence contribute significantly to the obligation to reduce emissions of harmful greenhouse gas to the atmosphere.\nMochovce NPP meets all international requirements and that the operation impact is minimal.\nWater required for cooling is taken from a water dam built on the nearby Hron river, which ensures sufficient supply of water even in extremely dry climate conditions. The impact of the discharged waters on the quality of the Hron river water, fauna and flora is negligible.\nEmissions to the atmosphere and effluents to the hydrosphere are regularly measured and assessed in the 15-km area around the plant. There are 25 monitoring stations of the tele-dosimetry system, which continuously monitor the dose rate of gamma radiation, activity of aerosols and radioactive iodine in the air, soil, water and food chain (feed, milk, agricultural products). The volume of radioactive substances contained in liquid and gaseous discharges is considerably lower than the limits set out by authorities.\n\nFor radiation protection of the power plant staff and population, the ALARA principle is applied. This principle ensures that the radiation exposure inside and outside the power plant is As Low As Reasonably Achievable and well below the limits set by legislation. The impact of the NPP operation on the environment and human health is negligible with respect to other radiation sources present in everyday life. There are 24 monitoring stations of the tele-dosimetry system in the 20 km radius around the power plant, which continuously monitors the dose rate of gamma radiation, volume activity of aerosols and radioactive iodine in the air, soil, water and food chain (feed, milk, agricultural produces). The volume of radioactive substances contained in liquid and gaseous discharges is considerably lower than the limits set out by authorities.\n\nImmediately after the Fukushima accident, European politicians, representatives of the nuclear industry and regulatory bodies agreed on the undertaking of power-plant safety reviews. All 15 member states of the EU operating nuclear power plants were involved. The testing of the two Bohunice NPP V2 units and all four Mochovce NPP units was carried out mainly through engineering analyses, calculations and reports.\nStress tests analysed extraordinary external events – earthquakes, floods, and impacts of other events that might result in the multiple loss of power-plant safety functions. The combination of events, including loss of power supply, long-term water supply breakdown, as well as loss of power supply due to extreme climate conditions were also assessed.\nStress tests revealed no deficiencies requiring immediate action; the further safe operation of neither the operating units nor the units under construction was put in doubt. Identified measures would further increase nuclear safety, for example by adding mobile diesel-generator for recharging of back-up batteries.\n\n\n"}
{"id": "1912329", "url": "https://en.wikipedia.org/wiki?curid=1912329", "title": "Neodymium-doped yttrium orthovanadate", "text": "Neodymium-doped yttrium orthovanadate\n\nNeodymium-doped yttrium orthovanadate (Nd:YVO) is a crystalline material formed by adding neodymium ions to yttrium orthovanadate. It is commonly used as an active laser medium for diode-pumped solid-state lasers. It comes as a transparent blue-tinted material. It is birefringent, therefore rods made of it are usually rectangular.\n\nAs in all neodymium-doped laser crystals, the lasing action of Nd:YVO is due to its content of neodymium ions, which may be excited by visible or infrared light, and undergo an electronic transition resulting in emission of coherent infrared light at a lower frequency, usually at 1064 nm (other transitions in Nd are available, and can be selected for by external optics).\n\n\n\n\n"}
{"id": "1810132", "url": "https://en.wikipedia.org/wiki?curid=1810132", "title": "Nitroguanidine", "text": "Nitroguanidine\n\nNitroguanidine is an organic compound with the formula (NH)CNNO. It is a colorless, crystalline solid that melts at 232 °C and decomposes at 250 °C. It is very flammable; but has a low impact sensitivity; however, its detonation velocity is high. It is used as a propellant (air bags), fertilizer, and for other purposes.\n\nNitroguanidine is produced commercially by a two step process starting with the reaction of calcium cyanamide with ammonium nitrate. Via the intermediacy of biguanidine, this ammonolysis step affords the \"salt\" guanidinium nitrate. In the second step, the nitrate salt is treated with sulfuric acid, a process that dehydrates the salt and forms the N-N bond.\n\nNitroguanidine can also be generated by treatment of urea with ammonium nitrate. Owing to problems of reliability and safety, this process has not been commerciallized despite its attractive economic features.\n\nNitroguanidine is used as an explosive propellant, notably in \"triple-base\" smokeless powder. The nitroguanidine reduces the propellant's flash and flame temperature without sacrificing chamber pressure. These are typically used in large bore guns where barrel erosion and flash are particularly important to avoid.\n\nNitroguanidine's explosive decomposition is given by the following equation:\nHNCO → 2 HO + 2 N + C\n\nNitroguanidine derivatives are used as insecticides, having a comparable effect to nicotine. Derivatives include clothianidin, dinotefuran, imidacloprid, and thiamethoxam.\n\nThe nitrosoylated derivative nitrosoguanidine is often used to mutagenize bacterial cells for biochemical studies.\n\nNitroguanidine exists in two tautomeric forms, as a nitroimine (left) or a nitroamine (right). In solution and in the solid state, the nitroimine form predominates (resonance stabilized).\n"}
{"id": "12121068", "url": "https://en.wikipedia.org/wiki?curid=12121068", "title": "Open cluster remnant", "text": "Open cluster remnant\n\nIn astronomy, an open cluster remnant (OCR) is the final stage in the evolution of an open star cluster.\n\nViktor Ambartsumian (1938) and Lyman Spitzer (1940) showed that, from a theoretical point of view, it was impossible for a star cluster to evaporate completely; furthermore, Spitzer pointed out two possible final results for the evolution of a star cluster: evaporation provokes physical collisions between stars, or evaporation proceeds until a stable binary or higher multiplicity system is produced.\n\nUsing objective-prism plates, Lodén (1987, 1988, 1993) has investigated the possible population of open cluster remnants in our Galaxy under the assumption that the stars in these clusters should have similar luminosity and spectral type. He found that about 30% of the objects in his sample could be catalogued as a possible type of cluster remnant. The membership for these objects is ≥ 15. The typical age of these systems is about 150 Myr with a range of 50-200 Myr. They show a significant density of binaries and a large number of optical binaries. The stars of these OCRs have a trend to be massive and hence early-type (A-F) stars although this observational method includes a noticeable selection effect because bright early-type spectra are easier to detect than fainter and later ones. In fact, almost no stars with spectral type later than F appear among his objects. On the other hand, his results were not fully conclusive because there are known regions in the sky with many stars of the same spectral type but in which it is difficult to find two stars with the same proper motions or radial velocity. A striking example of this fact is Upgren 1; initially, it was suggested that this small group of seven F stars was the remnant of an old cluster (Upgren & Rubin 1965) but later, Gatewood et al. (1988) concluded that Upgren 1 is only a chance alignment of F stars resulting from the close passage of members of two dynamically different sets of stars. Very recently, Stefanik et al. (1997) have shown that one of the sets is formed by 5 stars including a long-period binary and an unusual triple system.\n\nRegarding numerical simulations, for systems with some 25 to 250 stars, von Hoerner (1960, 1963), Aarseth (1968) and van Albada (1968) suggested that the final outcome of the evolution of an open cluster is one or more tightly bound binaries (or even a hierarchical triple system). Van Albada pointed out several observational candidates (σ Ori, ADS 12696, ρ Oph, 1 Cas, 8 Lac and 67 Oph) as being OCRs and Wielen (1975) indicated another one, the Ursa Major moving group (Collinder 285).\n\n\n"}
{"id": "2077685", "url": "https://en.wikipedia.org/wiki?curid=2077685", "title": "Particle-beam weapon", "text": "Particle-beam weapon\n\nA particle-beam weapon uses a high-energy beam of atomic or subatomic particles to damage the target by disrupting its atomic and/or molecular structure. A particle-beam weapon is a type of directed-energy weapon, which directs energy in a particular and focused direction using particles with miniscule mass. Some particle-beam weapons have potential practical applications, e.g. as an antiballistic missile defense system for the United States and its cancelled Strategic Defense Initiative. The vast majority, however, are science fiction and are among the most common weapon types of the genre. They have been known by myriad names: phasers, particle accelerator guns, ion cannons, proton beams, lightning rays, rayguns, etc.\n\nThe concept of particle-beam weapons comes from sound scientific principles and experiments currently underway around the world. One effective process to cause damage to or destroy a target is to simply overheat it until it is no longer operational. However, after decades of R&D, particle-beam weapons are still very much at the research stage and it remains to be seen if or when they will be deployed as practical, high-performance military weapons.\n\nParticle accelerators are a well-developed technology used in scientific research for decades. They use electromagnetic fields to accelerate and direct charged particles along a predetermined path, and electrostatic \"lenses\" to focus these streams for collisions. The cathode ray tube in many twentieth-century televisions and computer monitors is a very simple type of particle accelerator. More powerful versions include synchrotrons and cyclotrons used in nuclear research. A particle-beam weapon is a weaponized version of this technology. It accelerates charged particles (in most cases electrons, positrons, protons, or ionized atoms, but very advanced versions can accelerate other particles such as mercury nuclei) to near-light speed and then shoots them at a target. These particles have tremendous kinetic energy which they impart to matter in the target, inducing near-instantaneous and catastrophic superheating at the surface, and when penetrating deeper, ionization effects which can be especially detrimental to electronics in the target.\n\nCharged particle beams diverge rapidly due to mutual repulsion, so neutral particle beams are more commonly proposed. A neutral-particle-beam weapon ionizes atoms by either stripping an electron off of each atom, or by allowing each atom to capture an extra electron. The charged particles are then accelerated, and neutralized again by adding or removing electrons afterwards.\n\n\"Cyclotron particle accelerators\", \"linear particle accelerators\", and \"Synchrotron particle accelerators\" can accelerate positively charged hydrogen ions until their velocity approaches the speed of light, and each individual ion has a kinetic energy range of 100 MeV to 1000 MeV or more. Then the resulting high energy protons can capture electrons from electron emitter electrodes, and be thus electrically neutralized. This creates an electrically neutral beam of high energy hydrogen atoms, that can proceed in a straight line at near the speed of light to smash into its target and damage it.\n\nThe pulsed particle beam emitted by such a weapon may contain 1 gigajoule of kinetic energy or more. The speed of a beam approaching that of light (299,792,458 m/s in a vacuum) in combination with the energy created by the weapon would negate any realistic means of defending a target against the beam. Target hardening through shielding or materials selection would be impractical or ineffective, especially if the beam could be maintained at full power and precisely focused on the target.\n\nThe U.S. Defense Strategic Defense Initiative put into development the technology of a neutral particle beam to be used as a weapon in outer space. Neutral beam accelerator technology was developed at Los Alamos National Laboratory. A prototype neutral hydrogen beam weapon was launched aboard a suborbital sounding rocket from White Sands Missile Range in July 1989 as part of the Beam Experiments Aboard Rocket (BEAR) project. It reached a maximum altitude of 124 miles, and successfully operated in space for 4 minutes before returning to earth intact. In 2006, the recovered experimental device was transferred from Los Alamos to the Smithsonian Air and Space Museum in Washington, DC.\n\n\n"}
{"id": "25061776", "url": "https://en.wikipedia.org/wiki?curid=25061776", "title": "Simon–Glatzel equation", "text": "Simon–Glatzel equation\n\nThe Simon–Glatzel equation is an empirical correlation describing the pressure dependence of the melting temperature of a solid. The pressure dependence of the melting temperature is small for small pressure changes because the volume change during fusion or melting is rather small. However, at very high pressures higher melting temperatures are generally observed as the liquid usually occupies a larger volume than the solid making melting more thermodynamically unfavorable at elevated pressure. If the liquid has a smaller volume than the solid (as for ice and liquid water) a higher pressure leads to a lower melting point.\n\nformula_1\n\nT and P are normally the temperature and the pressure of the triple point, but the normal melting temperature at atmospheric pressure are also commonly used as reference point because the normal melting point is much more easily accessible. Typically P is then set to 0. \"a\" and \"c\" are adjustable and component specific parameters.\n\nFor Methanol the following parameters can be obtained:\n\nThe reference temperature has been T = T = 174.61 K and the reference pressure P has been set to 0 kPa.\n\nMethanol is a component where the Simon-Glatzel works well in the given validity range. The Simon–Glatzel equation cannot be used if the melting curve is falling or has maximums.\n"}
{"id": "7188776", "url": "https://en.wikipedia.org/wiki?curid=7188776", "title": "South Bay Pumping Plant", "text": "South Bay Pumping Plant\n\nThe South Bay Pumping Plant is located 4 miles (6 km) southwest of the Clifton Court Forebay and 10.3 miles (17 km) northeast of Livermore, CA. The plant is the only main line pumping plant for the 42.9 mile (69 km) long South Bay Aqueduct. The plant pumps from the Bethany Reservoir which is part of the California Aqueduct.\n\n\nDel Valle Pumping Plant is another pumping plant on the South Bay Aqueduct which pumps water into Lake Del Valle for storage.\n"}
{"id": "432630", "url": "https://en.wikipedia.org/wiki?curid=432630", "title": "Superspace", "text": "Superspace\n\nSuperspace is the coordinate space of a theory exhibiting supersymmetry. In such a formulation, along with ordinary space dimensions \"x\", \"y\", \"z\", ..., there are also \"anticommuting\" dimensions whose coordinates are labeled in Grassmann numbers rather than real numbers. The ordinary space dimensions correspond to bosonic degrees of freedom, the anticommuting dimensions to fermionic degrees of freedom.\n\nThe word \"superspace\" was first used by John Wheeler in an unrelated sense to describe the configuration space of general relativity; for example, this usage may be seen in his 1973 textbook \"Gravitation\".\n\nThere are several similar, but not equivalent, definitions of superspace that have been used, and continue to be used in the mathematical and physics literature. One such usage is as a synonym for super Minkowski space. In this case, one takes ordinary Minkowski space, and extends it with anti-commuting fermionic degrees of freedom, taken to be anti-commuting Weyl spinors from the Clifford algebra associated to the Lorentz group. Equivalently, the super Minkowski space can be understood as the quotient of the super Poincaré algebra modulo the algebra of the Lorentz group. A typical notation for the coordinates on such a space is formula_1 with the overline being the give-away that super Minkowski space is the intended space.\n\nSuperspace is also commonly used as a synonym for the super vector space. This is taken to be an ordinary vector space, together with additional coordinates taken from the Grassmann algebra, i.e. coordinate directions that are Grassmann numbers. There are several conventions for constructing a super vector space in use; two of these are described by Rogers and DeWitt.\n\nA third usage of the term \"superspace\" is as a synonym for a supermanifold: a supersymmetric generalization of a manifold. Note that both super Minkowski spaces and super vector spaces can be taken as special cases of supermanifolds.\n\nA fourth, and completely unrelated meaning saw a brief usage in general relativity; this is discussed in greater detail at the bottom.\n\nSeveral examples are given below. The first few assume a definition of superspace as a super vector space. This is denoted as R, the Z-graded vector space with R as the even subspace and R as the odd subspace. The same definition applies to C.\n\nThe four-dimensional examples take superspace to be super Minkowski space. Although similar to a vector space, this has many important differences: First of all, it is an affine space, having no special point denoting the origin. Next, the fermionic coordinates are taken to be anti-commuting Weyl spinors from the Clifford algebra, rather than being Grassmann numbers. The difference here is that the Clifford algebra has a considerably richer and more subtle structure than the Grassmann numbers. So, the Grassmann numbers are elements of the exterior algebra, and the Clifford algebra has an isomorphism to the exterior algebra, but its relation to the orthogonal group and the spin group, used to construct the spin representations, give it a deep geometric significance. (For example, the spin groups form a normal part of the study of Riemannian geometry, quite outside the ordinary bounds and concerns of physics.)\n\nThe smallest superspace is a point which contains neither bosonic nor fermionic directions. Other trivial examples include the \"n\"-dimensional real plane R, which is a vector space extending in \"n\" real, bosonic directions and no fermionic directions. The vector space R, which is the \"n\"-dimensional real Grassmann algebra. The space R of one even and one odd direction is known as the space of dual numbers, introduced by William Clifford in 1873.\n\nSupersymmetric quantum mechanics with \"N\" supercharges is often formulated in the superspace R, which contains one real direction \"t\" identified with time and \"N\" complex Grassmann directions which are spanned by Θ and Θ, where \"i\" runs from 1 to \"N\".\n\nConsider the special case \"N\" = 1. The superspace R is a 3-dimensional vector space. A given coordinate therefore may be written as a triple (\"t\", Θ, Θ). The coordinates form a Lie superalgebra, in which the gradation degree of \"t\" is even and that of Θ and Θ is odd. This means that a bracket may be defined between any two elements of this vector space, and that this bracket reduces to the commutator on two even coordinates and on one even and one odd coordinate while it is an anticommutator on two odd coordinates. This superspace is an abelian Lie superalgebra, which means that all of the aforementioned brackets vanish\n\nwhere formula_3 is the commutator of \"a\" and \"b\" and formula_4 is the anticommutator of \"a\" and \"b\".\n\nOne may define functions from this vector space to itself, which are called superfields. The above algebraic relations imply that, if we expand our superfield as a power series in Θ and Θ, then we will only find terms at the zeroeth and first orders, because Θ = Θ = 0. Therefore, superfields may be written as arbitrary functions of \"t\" multiplied by the zeroeth and first order terms in the two Grassmann coordinates\n\nSuperfields, which are representations of the supersymmetry of superspace, generalize the notion of tensors, which are representations of the rotation group of a bosonic space.\n\nOne may then define derivatives in the Grassmann directions, which take the first order term in the expansion of a superfield to the zeroeth order term and annihilate the zeroeth order term. One can choose sign conventions such that the derivatives satisfy the anticommutation relations\n\nThese derivatives may be assembled into supercharges\n\nwhose anticommutators identify them as the fermionic generators of a supersymmetry algebra\n\nwhere \"i\" times the time derivative is the Hamiltonian operator in quantum mechanics. Both \"Q\" and its adjoint anticommute with themselves. The supersymmetry variation with supersymmetry parameter ε of a superfield Φ is defined to be\n\nWe can evaluate this variation using the action of \"Q\" on the superfields\n\nSimilarly one may define covariant derivatives on superspace\n\nwhich anticommute with the supercharges and satisfy a wrong sign supersymmetry algebra\n\nThe fact that the covariant derivatives anticommute with the supercharges means the supersymmetry transformation of a covariant derivative of a superfield is equal to the covariant derivative of the same supersymmetry transformation of the same superfield. Thus, generalizing the covariant derivative in bosonic geometry which constructs tensors from tensors, the superspace covariant derivative constructs superfields from superfields.\n\nPerhaps the most popular superspace in physics is \"d\"=4 \"N\"=1 super Minkowski space R, which is the direct sum of four real bosonic dimensions and four real Grassmann dimensions (also known as fermionic dimensions). In supersymmetric quantum field theories one is interested in superspaces which furnish representations of a Lie superalgebra called a supersymmetry algebra. The bosonic part of the supersymmetry algebra is the Poincaré algebra, while the fermionic part is constructed using spinors of Grassmann numbers.\n\nFor this reason, in physical applications one considers an action of the supersymmetry algebra on the four fermionic directions of R such that they transform as a spinor under the Poincaré subalgebra. In four dimensions there are three distinct irreducible 4-component spinors. There is the Majorana spinor, the left-handed Weyl spinor and the right-handed Weyl spinor. The CPT theorem implies that in a unitary, Poincaré invariant theory, which is a theory in which the S-matrix is a unitary matrix and the same Poincaré generators act on the asymptotic in-states as on the asymptotic out-states, the supersymmetry algebra must contain an equal number of left-handed and right-handed Weyl spinors. However, since each Weyl spinor has four components, this means that if one includes any Weyl spinors one must have 8 fermionic directions. Such a theory is said to have extended supersymmetry, and such models have received a lot of attention. For example, supersymmetric gauge theories with eight supercharges and fundamental matter have been solved by Nathan Seiberg and Edward Witten, see Seiberg–Witten gauge theory. However, in this subsection we are considering the superspace with four fermionic components and so no Weyl spinors are consistent with the CPT theorem.\n\n\"Note\": There are many sign conventions in use and this is only one of them.\n\nThis leaves us with one possibility, the four fermionic directions transform as a Majorana spinor θ. We can also form a conjugate spinor\n\nwhere \"C\" is the charge conjugation matrix, which is defined by the property that when it conjugates a gamma matrix, the gamma matrix is negated and transposed. The first equality is the definition of while the second is a consequence of the Majorana spinor condition θ = iγCθ. The conjugate spinor plays a role similar to that of θ in the superspace R, except that the Majorana condition, as manifested in the above equation, imposes that θ and θ are not independent.\n\nIn particular we may construct the supercharges\n\nwhich satisfy the supersymmetry algebra\n\nwhere formula_16 is the 4-momentum operator. Again the covariant derivative is defined like the supercharge but with the second term negated and it anticommutes with the supercharges. Thus the covariant derivative of a supermultiplet is another supermultiplet.\n\nThe word \"superspace\" is also used in a completely different and unrelated sense, in the book Gravitation by Misner, Thorne and Wheeler. There, it refers to the configuration space of general relativity, and, in particular, the view of gravitation as geometrodynamics, an interpretation of general relativity as a form of dynamical geometry. In modern terms, this particular idea of \"superspace\" is captured in one of several different formalisms used in solving the Einstein equations in a variety of settings, both theoretical and practical, such as in numerical simulations. This includes primarily the ADM formalism, as well as ideas surrounding the Hamilton–Jacobi–Einstein equation and the Wheeler–DeWitt equation.\n\n\n"}
{"id": "28878398", "url": "https://en.wikipedia.org/wiki?curid=28878398", "title": "Surge chamber", "text": "Surge chamber\n\nIn hydropower, a surge chamber is a large pressurized underground chamber creating a free surface in the waterway to improve the dynamic abilities of the power plant waterways. It is generally used for long waterways when a surge shaft can not be created to fulfill the same purpose . \n\nSurge tank\n\n"}
{"id": "498137", "url": "https://en.wikipedia.org/wiki?curid=498137", "title": "Synthetic diamond", "text": "Synthetic diamond\n\nA synthetic diamond (also known as a laboratory-grown diamond, cultured diamond, or cultivated diamond) is a diamond produced by a controlled process, as contrasted with a natural diamond created by geological processes or an imitation diamond made of non-diamond material that appears similar to a diamond. Synthetic diamond is also widely known as HPHT diamond or CVD diamond, after the two common production methods (referring to the high-pressure high-temperature and chemical vapor deposition crystal formation methods, respectively). While the term \"synthetic\" may sometimes be associated by consumers with imitation products, synthetic diamonds are made of the same material as natural diamonds—pure carbon, crystallized in an isotropic 3D form. In the United States, the Federal Trade Commission has indicated that the terms \"laboratory-grown\", \"laboratory-created\", and \"[manufacturer-name]-created\" \"would more clearly communicate the nature of the stone\".\n\nNumerous claims of diamond synthesis were documented between 1879 and 1928; most of those attempts were carefully analyzed but none were confirmed. In the 1940s, systematic research began in the United States, Sweden and the Soviet Union to grow diamonds using CVD and HPHT processes. The first reproducible synthesis was reported around 1955. Those two processes still dominate the production of synthetic diamond. A third method, known as detonation synthesis, entered the diamond market in the late 1990s. In this process, nanometer-sized diamond grains are created in a detonation of carbon-containing explosives. A fourth method, treating graphite with high-power ultrasound, has been demonstrated in the laboratory, but currently has no commercial application.\n\nThe properties of synthetic diamond depend on the details of the manufacturing processes; however, some synthetic diamonds (whether formed by HPHT or CVD) have properties such as hardness, thermal conductivity and electron mobility that are superior to those of most naturally formed diamonds. Synthetic diamond is widely used in abrasives, in cutting and polishing tools and in heat sinks. Electronic applications of synthetic diamond are being developed, including high-power switches at power stations, high-frequency field-effect transistors and light-emitting diodes. Synthetic diamond detectors of ultraviolet (UV) light or high-energy particles are used at high-energy research facilities and are available commercially. Because of its unique combination of thermal and chemical stability, low thermal expansion and high optical transparency in a wide spectral range, synthetic diamond is becoming the most popular material for optical windows in high-power CO lasers and gyrotrons. It is estimated that 98% of industrial grade diamond demand is supplied with synthetic diamonds.\n\nBoth CVD and HPHT diamonds can be cut into gems and various colors can be produced: clear white, yellow, brown, blue, green and orange. The appearance of synthetic gems on the market created major concerns in the diamond trading business, as a result of which special spectroscopic devices and techniques have been developed to distinguish synthetic and natural diamonds.\n\nAfter the 1797 discovery that diamond was pure carbon, many attempts were made to convert various cheap forms of carbon into diamond. The earliest successes were reported by James Ballantyne Hannay in 1879 and by Ferdinand Frédéric Henri Moissan in 1893. Their method involved heating charcoal at up to 3500 °C with iron inside a carbon crucible in a furnace. Whereas Hannay used a flame-heated tube, Moissan applied his newly developed electric arc furnace, in which an electric arc was struck between carbon rods inside blocks of lime. The molten iron was then rapidly cooled by immersion in water. The contraction generated by the cooling supposedly produced the high pressure required to transform graphite into diamond. Moissan published his work in a series of articles in the 1890s.\n\nMany other scientists tried to replicate his experiments. Sir William Crookes claimed success in 1909. Otto Ruff claimed in 1917 to have produced diamonds up to 7 mm in diameter, but later retracted his statement. In 1926, Dr. J Willard Hershey of McPherson College replicated Moissan's and Ruff's experiments, producing a synthetic diamond; that specimen is on display at the McPherson Museum in Kansas. Despite the claims of Moissan, Ruff, and Hershey, other experimenters were unable to reproduce their synthesis.\n\nThe most definitive replication attempts were performed by Sir Charles Algernon Parsons. A prominent scientist and engineer known for his invention of the steam turbine, he spent about 40 years (1882–1922) and a considerable part of his fortune trying to reproduce the experiments of Moissan and Hannay, but also adapted processes of his own. Parsons was known for his painstakingly accurate approach and methodical record keeping; all his resulting samples were preserved for further analysis by an independent party. He wrote a number of articles—some of the earliest on HPHT diamond—in which he claimed to have produced small diamonds. However, in 1928, he authorized Dr. C.H. Desch to publish an article in which he stated his belief that no synthetic diamonds (including those of Moissan and others) had been produced up to that date. He suggested that most diamonds that had been produced up to that point were likely synthetic spinel.\n\nIn 1941, an agreement was made between the General Electric (GE), Norton and Carborundum companies to further develop diamond synthesis. They were able to heat carbon to about under a pressure of for a few seconds. Soon thereafter, the Second World War interrupted the project. It was resumed in 1951 at the Schenectady Laboratories of GE, and a high-pressure diamond group was formed with Francis P. Bundy and H.M. Strong. Tracy Hall and others joined this project shortly thereafter.\n\nThe Schenectady group improved on the anvils designed by Percy Bridgman, who received a Nobel Prize for his work in 1946. Bundy and Strong made the first improvements, then more were made by Hall. The GE team used tungsten carbide anvils within a hydraulic press to squeeze the carbonaceous sample held in a catlinite container, the finished grit being squeezed out of the container into a gasket. The team recorded diamond synthesis on one occasion, but the experiment could not be reproduced because of uncertain synthesis conditions, and the diamond was later shown to have been a natural diamond used as a seed.\n\nHall achieved the first commercially successful synthesis of diamond on December 16, 1954, and this was announced on February 15, 1955. His breakthrough was using a \"belt\" press, which was capable of producing pressures above and temperatures above . The press used a pyrophyllite container in which graphite was dissolved within molten nickel, cobalt or iron. Those metals acted as a \"solvent-catalyst\", which both dissolved carbon and accelerated its conversion into diamond. The largest diamond he produced was across; it was too small and visually imperfect for jewelry, but usable in industrial abrasives. Hall's co-workers were able to replicate his work, and the discovery was published in the major journal \"Nature\". He was the first person to grow a synthetic diamond with a reproducible, verifiable and well-documented process. He left GE in 1955, and three years later developed a new apparatus for the synthesis of diamond—a tetrahedral press with four anvils—to avoid violating a U.S. Department of Commerce secrecy order on the GE patent applications. Hall received the American Chemical Society Award for Creative Invention for his work in diamond synthesis.\n\nAn independent diamond synthesis was achieved on February 16, 1953 in Stockholm by ASEA (Allmänna Svenska Elektriska Aktiebolaget), one of Sweden's major electrical manufacturing companies. Starting in 1949, ASEA employed a team of five scientists and engineers as part of a top-secret diamond-making project code-named QUINTUS. The team used a bulky split-sphere apparatus designed by Baltzar von Platen and Anders Kämpe. Pressure was maintained within the device at an estimated 8.4 GPa for an hour. A few small diamonds were produced, but not of gem quality or size. The work was not reported until the 1980s. During the 1980s, a new competitor emerged in Korea, a company named Iljin Diamond; it was followed by hundreds of Chinese enterprises. Iljin Diamond allegedly accomplished diamond synthesis in 1988 by misappropriating trade secrets from GE via a Korean former GE employee.\nSynthetic gem-quality diamond crystals were first produced in 1970 by GE, then reported in 1971. The first successes used a pyrophyllite tube seeded at each end with thin pieces of diamond. The graphite feed material was placed in the center and the metal solvent (nickel) between the graphite and the seeds. The container was heated and the pressure was raised to about 5.5 GPa. The crystals grow as they flow from the center to the ends of the tube, and extending the length of the process produces larger crystals. Initially, a week-long growth process produced gem-quality stones of around 5 mm (1 carat or 0.2 g), and the process conditions had to be as stable as possible. The graphite feed was soon replaced by diamond grit because that allowed much better control of the shape of the final crystal.\n\nThe first gem-quality stones were always yellow to brown in color because of contamination with nitrogen. Inclusions were common, especially \"plate-like\" ones from the nickel. Removing all nitrogen from the process by adding aluminium or titanium produced colorless \"white\" stones, and removing the nitrogen and adding boron produced blue ones. Removing nitrogen also slowed the growth process and reduced the crystalline quality, so the process was normally run with nitrogen present.\n\nAlthough the GE stones and natural diamonds were chemically identical, their physical properties were not the same. The colorless stones produced strong fluorescence and phosphorescence under short-wavelength ultraviolet light, but were inert under long-wave UV. Among natural diamonds, only the rarer blue gems exhibit these properties. Unlike natural diamonds, all the GE stones showed strong yellow fluorescence under X-rays. The De Beers Diamond Research Laboratory has grown stones of up to for research purposes. Stable HPHT conditions were kept for six weeks to grow high-quality diamonds of this size. For economic reasons, the growth of most synthetic diamonds is terminated when they reach a mass of to .\n\nIn the 1950s, research started in the Soviet Union and the US on the growth of diamond by pyrolysis of hydrocarbon gases at the relatively low temperature of 800 °C. This low-pressure process is known as chemical vapor deposition (CVD). William G. Eversole reportedly achieved vapor deposition of diamond over diamond substrate in 1953, but it was not reported until 1962. Diamond film deposition was independently reproduced by Angus and coworkers in 1968 and by Deryagin and Fedoseev in 1970. Whereas Eversole and Angus used large, expensive, single-crystal diamonds as substrates, Deryagin and Fedoseev succeeded in making diamond films on non-diamond materials (silicon and metals), which led to massive research on inexpensive diamond coatings in the 1980s.\n\nFrom 2013 reports emerged of a rise in undisclosed synthetic melee diamonds (small round diamonds typically used to frame a central diamond or embellish a band) being found in set jewelry and within diamond parcels sold in the trade. Due to the relatively inexpensive cost of diamond melee, as well as relative lack of universal knowledge for identifying large quantities of melee efficiently, not all dealers have made an effort to test diamond melee to correctly identify whether it is of natural or man-made origin. However, international laboratories are now beginning to tackle the issue head-on, with significant improvements in synthetic melee identification being made.\n\nThere are several methods used to produce synthetic diamonds. The original method uses high pressure and high temperature (HPHT) and is still widely used because of its relatively low cost. The process involves large presses that can weigh hundreds of tons to produce a pressure of 5 GPa at 1500 °C. The second method, using chemical vapor deposition (CVD), creates a carbon plasma over a substrate onto which the carbon atoms deposit to form diamond. Other methods include explosive formation (forming detonation nanodiamonds) and sonication of graphite solutions.\n\nIn the HPHT method, there are three main press designs used to supply the pressure and temperature necessary to produce synthetic diamond: the belt press, the cubic press and the split-sphere (BARS) press. Diamond seeds are placed at the bottom of the press.\nThe internal part of press is heated above 1400 °C and melts the solvent metal. The molten metal dissolves the high purity carbon source, which is then transported to the small diamond seeds and precipitates, forming a large synthetic diamond.\n\nThe original GE invention by Tracy Hall uses the belt press wherein the upper and lower anvils supply the pressure load to a cylindrical inner cell. This internal pressure is confined radially by a belt of pre-stressed steel bands. The anvils also serve as electrodes providing electric current to the compressed cell. A variation of the belt press uses hydraulic pressure, rather than steel belts, to confine the internal pressure. Belt presses are still used today, but they are built on a much larger scale than those of the original design.\n\nThe second type of press design is the cubic press. A cubic press has six anvils which provide pressure simultaneously onto all faces of a cube-shaped volume. The first multi-anvil press design was a tetrahedral press, using four anvils to converge upon a tetrahedron-shaped volume. The cubic press was created shortly thereafter to increase the volume to which pressure could be applied. A cubic press is typically smaller than a belt press and can more rapidly achieve the pressure and temperature necessary to create synthetic diamond. However, cubic presses cannot be easily scaled up to larger volumes: the pressurized volume can be increased by using larger anvils, but this also increases the amount of force needed on the anvils to achieve the same pressure. An alternative is to decrease the surface area to volume ratio of the pressurized volume, by using more anvils to converge upon a higher-order platonic solid, such as a dodecahedron. However, such a press would be complex and difficult to manufacture.\nThe BARS apparatus is claimed to be the most compact, efficient, and economical of all the diamond-producing presses. In the center of a BARS device, there is a ceramic cylindrical \"synthesis capsule\" of about 2 cm in size. The cell is placed into a cube of pressure-transmitting material, such as pyrophyllite ceramics, which is pressed by inner anvils made from cemented carbide (e.g., tungsten carbide or VK10 hard alloy). The outer octahedral cavity is pressed by 8 steel outer anvils. After mounting, the whole assembly is locked in a disc-type barrel with a diameter about 1 meter. The barrel is filled with oil, which pressurizes upon heating, and the oil pressure is transferred to the central cell. The synthesis capsule is heated up by a coaxial graphite heater and the temperature is measured with a thermocouple.\n\nChemical vapor deposition is a method by which diamond can be grown from a hydrocarbon gas mixture. Since the early 1980s, this method has been the subject of intensive worldwide research. Whereas the mass-production of high-quality diamond crystals make the HPHT process the more suitable choice for industrial applications, the flexibility and simplicity of CVD setups explain the popularity of CVD growth in laboratory research. The advantages of CVD diamond growth include the ability to grow diamond over large areas and on various substrates, and the fine control over the chemical impurities and thus properties of the diamond produced. Unlike HPHT, CVD process does not require high pressures, as the growth typically occurs at pressures under 27 kPa.\n\nThe CVD growth involves substrate preparation, feeding varying amounts of gases into a chamber and energizing them. The substrate preparation includes choosing an appropriate material and its crystallographic orientation; cleaning it, often with a diamond powder to abrade a non-diamond substrate; and optimizing the substrate temperature (about ) during the growth through a series of test runs. The gases always include a carbon source, typically methane, and hydrogen with a typical ratio of 1:99. Hydrogen is essential because it selectively etches off non-diamond carbon. The gases are ionized into chemically active radicals in the growth chamber using microwave power, a hot filament, an arc discharge, a welding torch, a laser, an electron beam, or other means.\n\nDuring the growth, the chamber materials are etched off by the plasma and can incorporate into the growing diamond. In particular, CVD diamond is often contaminated by silicon originating from the silica windows of the growth chamber or from the silicon substrate. Therefore, silica windows are either avoided or moved away from the substrate. Boron-containing species in the chamber, even at very low trace levels, also make it unsuitable for the growth of pure diamond.\n\nDiamond nanocrystals (5 nm in diameter) can be formed by detonating certain carbon-containing explosives in a metal chamber. These nanocrystals are called \"detonation nanodiamond\". During the explosion, the pressure and temperature in the chamber become high enough to convert the carbon of the explosives into diamond. Being immersed in water, the chamber cools rapidly after the explosion, suppressing conversion of newly produced diamond into more stable graphite. In a variation of this technique, a metal tube filled with graphite powder is placed in the detonation chamber. The explosion heats and compresses the graphite to an extent sufficient for its conversion into diamond. The product is always rich in graphite and other non-diamond carbon forms and requires prolonged boiling in hot nitric acid (about 1 day at 250 °C) to dissolve them. The recovered nanodiamond powder is used primarily in polishing applications. It is mainly produced in China, Russia and Belarus and started reaching the market in bulk quantities by the early 2000s.\n\nMicron-sized diamond crystals can be synthesized from a suspension of graphite in organic liquid at atmospheric pressure and room temperature using ultrasonic cavitation. The diamond yield is about 10% of the initial graphite weight. The estimated cost of diamond produced by this method is comparable to that of the HPHT method; the crystalline perfection of the product is significantly worse for the ultrasonic synthesis. This technique requires relatively simple equipment and procedures, but it has only been reported by two research groups, and has no industrial use . Numerous process parameters, such as preparation of the initial graphite powder, the choice of ultrasonic power, synthesis time and the solvent, are not yet optimized, leaving a window for potential improvement of the efficiency and reduction of the cost of the ultrasonic synthesis.\n\nTraditionally, the absence of crystal flaws is considered to be the most important quality of a diamond. Purity and high crystalline perfection make diamonds transparent and clear, whereas its hardness, optical dispersion (luster), and chemical stability (combined with marketing), make it a popular gemstone. High thermal conductivity is also important for technical applications. Whereas high optical dispersion is an intrinsic property of all diamonds, their other properties vary depending on how the diamond was created.\n\nDiamond can be one single, continuous crystal or it can be made up of many smaller crystals (polycrystal). Large, clear and transparent single-crystal diamonds are typically used in gemstones. Polycrystalline diamond (PCD) consists of numerous small grains, which are easily seen by the naked eye through strong light absorption and scattering; it is unsuitable for gems and is used for industrial applications such as mining and cutting tools. Polycrystalline diamond is often described by the average size (or \"grain size\") of the crystals that make it up. Grain sizes range from nanometers to hundreds of micrometers, usually referred to as \"nanocrystalline\" and \"microcrystalline\" diamond, respectively.\n\nSynthetic diamond is the hardest known material, where hardness is defined as resistance to indentation. The hardness of synthetic diamond depends on its purity, crystalline perfection and orientation: hardness is higher for flawless, pure crystals oriented to the <nowiki>[</nowiki>111<nowiki>]</nowiki> direction (along the longest diagonal of the cubic diamond lattice). Nanocrystalline diamond produced through CVD diamond growth can have a hardness ranging from 30% to 75% of that of single crystal diamond, and the hardness can be controlled for specific applications. Some synthetic single-crystal diamonds and HPHT nanocrystalline diamonds (see hyperdiamond) are harder than any known natural diamond.\n\nEvery diamond contains atoms other than carbon in concentrations detectable by analytical techniques. Those atoms can aggregate into macroscopic phases called inclusions. Impurities are generally avoided, but can be introduced intentionally as a way to control certain properties of the diamond. Growth processes of synthetic diamond, using solvent-catalysts, generally lead to formation of a number of impurity-related complex centers, involving transition metal atoms (such as nickel, cobalt or iron), which affect the electronic properties of the material.\n\nFor instance, pure diamond is an electrical insulator, but diamond with boron added is an electrical conductor (and, in some cases, a superconductor), allowing it to be used in electronic applications. Nitrogen impurities hinder movement of lattice dislocations (defects within the crystal structure) and put the lattice under compressive stress, thereby increasing hardness and toughness.\n\nUnlike most electrical insulators, pure diamond is a good conductor of heat because of the strong covalent bonding within the crystal. The thermal conductivity of pure diamond is the highest of any known solid. Single crystals of synthetic diamond enriched in (99.9%), isotopically pure diamond, have the highest thermal conductivity of any material, 30 W/cm·K at room temperature, 7.5 times higher than copper. Natural diamond's conductivity is reduced by 1.1% by the naturally present, which acts as an inhomogeneity in the lattice.\n\nDiamond's thermal conductivity is made use of by jewelers and gemologists who may employ an electronic thermal probe to separate diamonds from their imitations. These probes consist of a pair of battery-powered thermistors mounted in a fine copper tip. One thermistor functions as a heating device while the other measures the temperature of the copper tip: if the stone being tested is a diamond, it will conduct the tip's thermal energy rapidly enough to produce a measurable temperature drop. This test takes about 2–3 seconds.\n\nMost industrial applications of synthetic diamond have long been associated with their hardness; this property makes diamond the ideal material for machine tools and cutting tools. As the hardest known naturally occurring material, diamond can be used to polish, cut, or wear away any material, including other diamonds. Common industrial applications of this ability include diamond-tipped drill bits and saws, and the use of diamond powder as an abrasive. These are by far the largest industrial applications of synthetic diamond. While natural diamond is also used for these purposes, synthetic HPHT diamond is more popular, mostly because of better reproducibility of its mechanical properties. Diamond is not suitable for machining ferrous alloys at high speeds, as carbon is soluble in iron at the high temperatures created by high-speed machining, leading to greatly increased wear on diamond tools compared to alternatives.\n\nThe usual form of diamond in cutting tools is micron-sized grains dispersed in a metal matrix (usually cobalt) sintered onto the tool. This is typically referred to in industry as polycrystalline diamond (PCD). PCD-tipped tools can be found in mining and cutting applications. For the past fifteen years, work has been done to coat metallic tools with CVD diamond, and though the work still shows promise it has not significantly replaced traditional PCD tools.\n\nMost materials with high thermal conductivity are also electrically conductive, such as metals. In contrast, pure synthetic diamond has high thermal conductivity, but negligible electrical conductivity. This combination is invaluable for electronics where diamond is used as a heat sink for high-power laser diodes, laser arrays and high-power transistors. Efficient heat dissipation prolongs the lifetime of those electronic devices, and the devices' high replacement costs justify the use of efficient, though relatively expensive, diamond heat sinks. In semiconductor technology, synthetic diamond heat spreaders prevent silicon and other semiconducting materials from overheating.\n\nDiamond is hard, chemically inert, and has high thermal conductivity and a low coefficient of thermal expansion. These properties make diamond superior to any other existing window material used for transmitting infrared and microwave radiation. Therefore, synthetic diamond is starting to replace zinc selenide as the output window of high-power CO lasers and gyrotrons. Those synthetic polycrystalline diamond windows are shaped as disks of large diameters (about 10 cm for gyrotrons) and small thicknesses (to reduce absorption) and can only be produced with the CVD technique. Single crystal slabs of dimensions of length up to approximately 10 mm are becoming increasingly important in several areas of optics including heatspreaders inside laser cavities, diffractive optics and as the optical gain medium in Raman lasers. Recent advances in the HPHT and CVD synthesis techniques have improved the purity and crystallographic structure perfection of single-crystalline diamond enough to replace silicon as a diffraction grating and window material in high-power radiation sources, such as synchrotrons. Both the CVD and HPHT processes are also used to create designer optically transparent diamond anvils as a tool for measuring electric and magnetic properties of materials at ultra high pressures using a diamond anvil cell.\n\nSynthetic diamond has potential uses as a semiconductor, because it can be doped with impurities like boron and phosphorus. Since these elements contain one more or one less valence electron than carbon, they turn synthetic diamond into p-type or n-type semiconductor. Making a p–n junction by sequential doping of synthetic diamond with boron and phosphorus produces light-emitting diodes (LEDs) producing UV light of 235 nm. Another useful property of synthetic diamond for electronics is high carrier mobility, which reaches 4500 cm/(V·s) for electrons in single-crystal CVD diamond. High mobility is favourable for high-frequency operation and field-effect transistors made from diamond have already demonstrated promising high-frequency performance above 50 GHz. The wide band gap of diamond (5.5 eV) gives it excellent dielectric properties. Combined with the high mechanical stability of diamond, those properties are being used in prototype high-power switches for power stations.\n\nSynthetic diamond transistors have been produced in the laboratory. They are functional at much higher temperatures than silicon devices, and are resistant to chemical and radiation damage. While no diamond transistors have yet been successfully integrated into commercial electronics, they are promising for use in exceptionally high-power situations and hostile non-oxidizing environments.\n\nSynthetic diamond is already used as radiation detection device. It is radiation hard and has a wide bandgap of 5.5 eV (at room temperature). Diamond is also distinguished from most other semiconductors by the lack of a stable native oxide. This makes it difficult to fabricate surface MOS devices, but it does create the potential for UV radiation to gain access to the active semiconductor without absorption in a surface layer. Because of these properties, it is employed in applications such as the BaBar detector at the Stanford Linear Accelerator and BOLD (Blind to the Optical Light Detectors for VUV solar observations). A diamond VUV detector recently was used in the European LYRA program.\n\nConductive CVD diamond is a useful electrode under many circumstances. Photochemical methods have been developed for covalently linking DNA to the surface of polycrystalline diamond films produced through CVD. Such DNA modified films can be used for detecting various biomolecules, which would interact with DNA thereby changing electrical conductivity of the diamond film. In addition, diamonds can be used to detect redox reactions that cannot ordinarily be studied and in some cases degrade redox-reactive organic contaminants in water supplies. Because diamond is mechanically and chemically stable, it can be used as an electrode under conditions that would destroy traditional materials. As an electrode, synthetic diamond can be used in waste water treatment of organic effluents and the production of strong oxidants.\n\nSynthetic diamonds for use as gemstones are grown by HPHT or CVD methods, and represented approximately 2% of the gem-quality diamond market as of 2013. However, there are indications that the market share of synthetic jewelry-quality diamonds may grow as advances in technology allows for larger higher-quality synthetic production on a more economic scale. They are available in yellow and blue, and to a lesser extent colorless (or white). The yellow color comes from nitrogen impurities in the manufacturing process, while the blue color comes from boron. Other colors, such as pink or green, are achievable after synthesis using irradiation. Several companies also offer memorial diamonds grown using cremated remains.\n\nGem-quality diamonds grown in a lab can be chemically, physically and optically identical to naturally occurring ones. The mined diamond industry has undertaken legal, marketing and distribution countermeasures to protect its market from the emerging presence of synthetic diamonds. Synthetic diamonds can be distinguished by spectroscopy in the infrared, ultraviolet, or X-ray wavelengths. The DiamondView tester from De Beers uses UV fluorescence to detect trace impurities of nitrogen, nickel or other metals in HPHT or CVD diamonds.\n\nAt least one maker of laboratory-grown diamonds has made public statements about being \"committed to disclosure\" of the nature of its diamonds, and laser-inscribes serial numbers on all of its gemstones. The company web site shows an example of the lettering of one of its laser inscriptions, which includes both the words \"Gemesis created\" and the serial number prefix \"LG\" (laboratory grown).\n\nIn May 2015, a record was set for an HPHT colorless diamond at 10.02 carats. The faceted jewel was cut from a 32.2-carat stone that was grown within 300 hours.\n\nTraditional diamond mining has led to human-rights abuses in Africa and elsewhere. The 2006 Hollywood movie \"Blood Diamond\" helped to publicize the problem. Consumer demand for synthetic diamonds has been increasing, albeit from a small base, as customers look for stones which are ethically sound, and are cheaper.\n\nAccording to a report from the Gem & Jewellery Export Promotional Council, synthetic diamonds accounted for 0.28% of diamond produced for use as gem stones in 2014. Lab diamond jewellery is sold in the United States by brands including Pure Grown Diamonds (formerly known as Gemesis) and Lab Diamonds Direct; and in the UK by Nightingale online jewellers.\n\nAround 2016, the price of synthetic diamond gemstones (e.g., 1 carat stones) began dropping \"precipitously\", by roughly 30% in one year, and became clearly lower than that for mined diamonds.\n\nIn May 2018, the large worldwide diamond company De Beers announced that they would introduce a new jewelry brand called \"Lightbox\" that features synthetic diamonds.\n\nIn July 2018, the U.S. Federal Trade Commission approved a substantial revision to its Jewelry Guides, with changes that impose new rules on how the trade can describe diamonds and diamond simulants. The revised guides were substantially contrary to what had been advocated in 2016 by De Beers. The new guidelines remove the word \"natural\" from the definition of \"diamond\", thus including lab-grown diamonds within the scope of the definition of \"diamond\". The revised guide further states that \"If a marketer uses 'synthetic' to imply that a competitor's lab-grown diamond is not an actual diamond, ... this would be deceptive.\"\n\nThe De Beers Lightbox brand entered the market starting in September 2018. De Beers had previously limited its synthetic diamond production to industrial applications. As of November 2018, the brand's website describes the diamonds as costing $200 for a quarter carat stone, $400 for a half carat, and $800 for a full carat. These prices are far lower than most previous offerings about one-tenth of the price of similar mined diamonds and less than one-fourth the price of synthetic diamonds offered for sale in May 2018 by another producer, Diamond Foundry. However, Lightbox does not offer stones for sale without them being mounted in a setting (which adds somewhat to the price), and the brand only offers relatively low quality settings (sterling silver, rose gold plated, or 10K gold settings, not high-carat solid gold or platinum) and only offers settings for earrings and necklaces, not rings. The website emphasizes pink and blue stones, although colorless stones are also offered. The website's FAQ page says the lab-grown diamonds are \"neither as valuable or precious\" as natural stones. The Lightbox branded jewelry is promoted as being \"for lighter moods and lighter moments, like birthdays and beach days and just because days\", and the items are provided in what \"The New York Times\" called \"candy-colored cardboard gift boxes\". The Lightbox jewelry is offered for sale only directly through the website, although the site says that some partner sales locations will be added in 2019.\n\n\n\n"}
{"id": "49620777", "url": "https://en.wikipedia.org/wiki?curid=49620777", "title": "The Battle for Oil", "text": "The Battle for Oil\n\nThe Battle for Oil (aka Battle for Oil) is a 19-minute 1942 Canadian documentary film, made by the National Film Board of Canada (NFB) as part of the wartime \"Canada Carries On\" series. The film was produced by Raymond Spottiswoode and directed by Stuart Legg.\"The Battle for Oil\" describes the strategic value of oil in modern warfare. The film's French version title was \"La Bataille du pétrole\". \n\nIn 1942, consumers on the North American home front have to contend with sacrifices made in order to support the war effort. One of the restrictions is on the use of petroleum and lubricants, which results in commuters being rigidly constricted in their use of privately-owned vehicles.\n\nThese wartime rations of oil will allow the Aliies to preserve one of the most important strategic materials. From the great oilfields of Texas, Mexico, Venezuela and the Caribbean basin, comes the oil that feeds the Allied war effort. Military forces are deployed to protect this vital resource. The Royal Navy warships range far out to sea protect the convoys that are lifelines to the Allied cause, but are highly reliant on the oil reserves located throughout the world.\n\nThe mechanized Nazi war machine also relies on oil, and with limited natural supplies, Germany must manufacture costly synthetic fuels produced in 25 plants on a nonstop schedule that are constantly subjected to Royal Air Force aerial attacks. In distant battlefields, oil is the prize with Nazi armies threatening the rich oilfields of Baku, with the Soviet Union desperately fighting to retain the region. The German High Command knows that the \"Wehrmacht\" can stall if oil reserves are not secured.\n\nIn the Middle East, where the world's largest oil deposits are found, other oil wells and refineries are also under threat by the advancing Axis powers. The pipelines to Haifa and Tripoli and the storage tanks in Palestine where British and Soviet forces are standing guard, are under constant bombardment.\n\nIn the Far East, Japan, also dependent on foreign oil, is advancing relentlessly on the oil reserves in the Dutch East Indies and India. Singapore has become the fortress which the British Empire relies on to face the imminent onslaught of Japanese forces.\n\nAs the Allies turn to offence, oil is the lifeblood of the war effort. When the German battleship Bismarck makes a break for open sea, the oil-fuelled British battleships and destroyers run down and destroy the German battleship. Nightly RAF air raids also begin to take effect, but for every bombing raid to Berlin, 3,800 tons of high-test fuel is needed for each heavy bomber.\n\nGlobal oil production continues with a new discovery in the Turner Valley oilfields in Alberta becoming significant with 1/6 of the oil in Canada coming from the new oilfield. The construction of the Portland–Montreal Pipeline from Portland, Maine to Montreal refineries brings oil to Eastern Canada where ocean-going oil tankers will then set off in escorted convoys dodging the formidable German U-boats to delivery the precious oil to the European battle front.\n\nTypical of the NFB's \"Canada Carries On\" series of documentary short films, \"The Battle for Oil\" was a morale-boosting propaganda film made in cooperation with the Director of Public Information, Herbert Lash. Using the format of a compilation documentary, the film, edited by Stuart Legg to provide a coherent story, relied heavily on newsreel material including \"enemy\" footage, in order to provide the background to the dialogue. A NFB film crew was also dispatched to the Turner Valley oilfields in Canada.\n\nThe deep baritone voice of stage actor Lorne Greene was featured in the narration of \"The Battle for Oil\". Greene, known for his work on both radio broadcasts as a news announcer at \"CBC\" as well as narrating many of the \"Canada Carries On\" series. His sonorous recitation led to his nickname, \"The Voice of Canada\", and to some observers, the \"voice-of-God\". When reading grim battle statistics or narrating a particularly serious topic, he was known as \"The Voice of Doom\".\n\nAs part of the \"Canada Carries On\" series, \"The Battle for Oil\" was produced in 35 mm for the theatrical market. Each film was shown over a six-month period as part of the shorts or newsreel segments in approximately 800 theatres across Canada. The NFB had an arrangement with Famous Players theatres to ensure that Canadians from coast-to-coast could see them, with further distribution by Columbia Pictures.\n\nAfter the six-month theatrical tour ended, individual films were made available on 16 mm to schools, libraries, churches and factories, extending the life of these films for another year or two. They were also made available to film libraries operated by university and provincial authorities. A total of 199 films were produced before the series was canceled in 1959.\n\nIn a contemporary review of \"The Battle for Oil\", the \"Educational Film Library Association\" notes: \"... the strategic value of oil in the present conflict is emphasized until it appears as the greatest asset any nation can possess in these days of long-range warfare.\" The review also describes the effective use of images, stating the oil derricks make \"... a vivid impression ... riding like a defiant crown against the western sky.\"\n\nThe subject of oil was later profiled in a number of NFB documentaries including \"The Story of Oil\" (1946), \"Struggle for Oil\" (1951) and \"Oil\" (1953) as well as many other more recent productions.\n\n"}
{"id": "22892245", "url": "https://en.wikipedia.org/wiki?curid=22892245", "title": "The Earth After Us", "text": "The Earth After Us\n\nThe Earth After Us is a 2008 non-fiction book by Jan Zalasiewicz about the geological legacy that humans might one day leave behind them.\n\n\n"}
{"id": "420737", "url": "https://en.wikipedia.org/wiki?curid=420737", "title": "Tin pest", "text": "Tin pest\n\nTin pest is an autocatalytic, allotropic transformation of the element tin, which causes deterioration of tin objects at low temperatures. Tin pest has also been called \"tin disease\", \"tin blight\" or \"tin leprosy\" (\"lèpre d'étain\").\n\nIt was observed in medieval Europe that the pipes of pipe organs were affected in cool climates. As soon as the tin began decomposing, the process accelerated.\n\nWith the adoption of the Restriction of Hazardous Substances Directive (RoHS) regulations in Europe, and similar regulations elsewhere, traditional lead/tin solder alloys in electronic devices have been replaced by nearly pure tin, introducing tin pest and related problems such as tin whiskers.\n\nAt 13.2 °C (about 56 °F) and below, pure tin transforms from the silvery, ductile metallic allotrope of β-form \"white tin\" to the brittle, nonmetallic, α-form \"grey tin\" with a diamond cubic structure. The transformation is slow to initiate due to a high activation energy but the presence of germanium (or crystal structures of similar form and size) or very low temperatures of roughly −30 °C aids the initiation. There is also a large volume increase of about 27% associated with the phase change to the nonmetallic low temperature allotrope. This frequently makes tin objects (like buttons) decompose into powder during the transformation, hence the name \"tin pest\".\n\nThe decomposition will catalyze itself, which is why the reaction speeds up once it starts; the mere presence of tin pest leads to tin pest. Tin objects at low temperatures will simply disintegrate.\n\nIn 1910 British polar explorer Robert Scott hoped to be the first to reach the South Pole, but was beaten by Norwegian explorer Roald Amundsen. On foot, the expedition trudged through the frozen deserts of the Antarctic, marching for caches of food and kerosene deposited on the way in. In early 1912, at the first cache, there was no kerosene; the cans — soldered with tin — were empty. The cause of the empty tins could have been related to tin pest. Some observers blame poor quality soldering, as tin cans over eighty years old have been discovered in Antarctic buildings with the soldering in good condition.\n\nThe story is often told of Napoleon's men freezing in the bitter Russian Winter, their clothes falling apart as tin pest ate the buttons. This clearly appears to be an urban legend, and there is no evidence that there existed any failing buttons at all, and thus they cannot have been a contributing factor in the failure of the invasion. Critics of the theory point out that the tin used would have been quite impure and thus more tolerant of low temperatures. Laboratory tests provide evidence that the time required for unalloyed tin to develop significant tin pest damage at lowered temperatures is about 18 months, which is more than twice the length of the invasion. It is clear though that some of the regiments employed in the campaign had tin buttons and that the temperature reached sufficiently low values (below −40 °C or F). However, none of the many survivors' tales mention problems with buttons and it has been suggested that the legend is an amalgamation of a case of disintegrating Russian tin buttons in an army warehouse in the 1860s and the utterly desperate state of Napoleon's army turning soldiers into ragged beggars.\n\nWith the adoption of the Restriction of Hazardous Substances Directive (RoHS) regulations in Europe and California banning most uses of lead, and similar regulations elsewhere, the problem of tin pest has returned, since some manufacturers who previously used tin/lead alloys, now use pure tin. For example, the leads of some electrical and electronic components are plated with pure tin. In cold environments, this can change to α-modification \"grey tin\", which is not electrically conductive, and falls off the leads. After reheating, it changes back to β-modification \"white tin\", which is electrically conductive, and can cause electrical short circuits and failure of equipment. Such problems can be intermittent as the powdered particles of tin move around. Tin pest can be avoided by alloying with small amounts of electropositive metals or semimetals soluble in tin's solid phase e.g. antimony or bismuth, which prevent the decomposition.\n\n\n"}
{"id": "48556598", "url": "https://en.wikipedia.org/wiki?curid=48556598", "title": "Tree baler", "text": "Tree baler\n\nA tree baler is a machine that wraps trees to allow for easier shipment and storage. Wrapped trees take up much less space and are less likely to be damaged during shipment. The aim of the device is to replace work that previously required hand-tying individual trees, most often used at commercial nurseries.\n\nTree balers utilize a funnel, through which the tree is forced, thereby compressing the branches tight to the trunk. The baler then wraps or ties the branches to maintain a compressed shape for shipment and storage. Simple machines are operated manually, usually requiring one person to push the tree through the funnel and another to wrap it. A mechanized baler does both operations and can bale over 100 trees per hour.\n\nAccording to the \"Berks-Mont News\", the motorized christmas tree baler was invented in Pennsylvania in 1944.\n\nA manual baler may also keep the branches compressed by encasing the tree in a plastic netting, rather than wrapping them with twine. This type of baler is often used in retail sales of christmas trees to package the tree for transport by the customer. Manual balers do not compress trees as tightly as mechanized ones.\n\n"}
{"id": "305927", "url": "https://en.wikipedia.org/wiki?curid=305927", "title": "Vitrification", "text": "Vitrification\n\nVitrification (from Latin \"vitreum\", \"glass\" via French \"vitrifier\") is the transformation of a substance into a glass, that is to say a non-crystalline amorphous solid. In the production of ceramics, vitrification is responsible for its impermeability to water.\n\nVitrification is usually achieved by heating materials until they liquidize, then cooling the liquid, often rapidly, so that it passes through the glass transition to form a vitrified solid. Certain chemical reactions also result in glasses. \n\nIn a different sense of the word, the embedding of material inside a glassy matrix is also called \"vitrification\". An important application is the vitrification of radioactive waste to obtain a stable compound that is suitable for ultimate disposal.\n\nIn terms of chemistry, vitrification is characteristic for amorphous materials or disordered systems and occurs when bonding between elementary particles (atoms, molecules, forming blocks) becomes higher than a certain threshold value. Thermal fluctuations break the bonds; therefore, the lower the temperature, the higher the degree of connectivity. Because of that, amorphous materials have a characteristic threshold temperature termed glass transition temperature (T): below T amorphous materials are glassy whereas above T they are molten.\n\nThe most common applications are in the making of pottery, glass, and some types of food, but there are many others, such as the vitrification of an antifreeze-like liquid in cryopreservation.\n\nVitrification is the progressive partial fusion of a clay, or of a body, as a result of a firing process. As vitrification proceeds the proportion of glassy bond increases and the apparent porosity of the fired product becomes progressively lower. Vitreous bodies have open porosity, and may be either opaque or translucent. In this context 'zero porosity'; may be defined as less than 1% water absorption. However, various standard procedures define the conditions of water absorption. An example is by ASTM, who state \"The term vitreous generally signifies less than 0.5% absorption. except for floor and wall tile and low-voltage electrical insulators which are considered vitreous up to 3% water absorption.\"\n\nGlazing alone does not make pottery impermeable to water. Porcelain, bone china and sanitaryware are examples of vitrified pottery, and are impermeable even without glaze. Stoneware may be vitrified or semi-vitrified, the latter type would not be impermeable without glaze.\n\nWhen sucrose is cooled slowly it results in crystal sugar (or rock candy), but when cooled rapidly it can form syrupy cotton candy (candyfloss).\n\nVitrification can also occur when starting with a liquid such as water, usually through very rapid cooling or the introduction of agents that suppress the formation of ice crystals. This is in contrast to ordinary freezing which results in ice crystal formation. Additives used in cryobiology or produced naturally by organisms living in polar regions are called cryoprotectants. Vitrification is used in cryo-electron microscopy to cool samples so quickly that they can be imaged with an electron microscope without damage. In 2017, the Nobel prize for chemistry was awarded for the development of this technology, which can be used to image objects such as proteins or virus particles.\n\nCold-adapted frogs and some other ectotherms naturally produce glycerol (e.g., southern brown tree frog) or glucose (e.g., wood frog) in their livers to reduce ice formation. When glucose is used as a cryoprotectant by arctic frogs, massive amounts of glucose are released at low temperature and a special form of insulin allows for this extra glucose to enter the cells. When the frog rewarms during spring, the extra glucose must be rapidly eliminated, but stored. Arctic insects also use sugars as cryoprotectants. Arctic fish use antifreeze proteins, sometimes appended with sugars, as cryoprotectants.\n\nOrdinary soda-lime glass, used in windows and drinking containers, is created by the addition of sodium carbonate and lime (calcium oxide) to silicon dioxide. Without these additives silicon dioxide will require very high temperature to obtain a melt and subsequently (with slow cooling) a glass.\n\nVitrification is a proven technique in the disposal and long-term storage of nuclear waste or other hazardous wastes in a method called geomelting. Waste is mixed with glass-forming chemicals in a furnace to form molten glass that then solidifies in canisters, thereby immobilizing the waste. The final waste form resembles obsidian and is a non-leaching, durable material that effectively traps the waste inside. The waste can be stored for relatively long periods in this form without concern for air or groundwater contamination. Bulk vitrification uses electrodes to melt soil and wastes where they lay buried. The hardened waste may then be disinterred with less danger of widespread contamination. According to the Pacific Northwest National Labs, \"Vitrification locks dangerous materials into a stable glass form that will last for thousands of years.\"\n\nVitrification in cryopreservation is used as a common method to preserve, for example, human egg cells (oocytes) (in oocyte cryopreservation) and embryos (in embryo cryopreservation). For years, glycerol has been used in cryobiology as a cryoprotectant for blood cells and bull sperm, allowing storage at liquid nitrogen temperatures. However, glycerol cannot be used to protect whole organs from damage. Instead, many biotechnology companies are researching the development of other cryoprotectants more suitable for such uses. A successful discovery may eventually make possible the bulk cryogenic storage (or \"banking\") of transplantable human and xenobiotic organs. A substantial step in that direction has already occurred. Twenty-First Century Medicine has vitrified a rabbit kidney to -135 °C with their proprietary vitrification cocktail. Upon rewarming, the kidney was successfully transplanted into a rabbit, with complete functionality and viability, able to sustain the rabbit indefinitely as the sole functioning kidney.\n\nCurrently, vitrification techniques have only been applied to brains (neurovitrification) by Alcor and to the upper body by the Cryonics Institute, but research is in progress by both organizations to apply vitrification to the whole body.\n\n\n"}
{"id": "38665620", "url": "https://en.wikipedia.org/wiki?curid=38665620", "title": "Westmill Solar Co-operative", "text": "Westmill Solar Co-operative\n\nWestmill Solar Co-operative is the industrial and provident society that owns the Westmill Solar Park in Oxfordshire, England, believed to be the largest community-owned photovoltaic power station in the world.\n\nThe project was originally conceived by Adam Twine, the pioneer behind the neighbouring Westmill Wind Farm Co-operative, built in 2011 and registered for the UK feed-in tariffs. It has a capacity of 5 MW.\n\nWestmill Solar Co-operative acquired the solar park in October 2012, under an option agreement with the original developers. It raised the necessary finance through public and private share offers and a senior debt bond with a pension fund.\n\nWestmill solar park is in the United Kingdom located on a site of 30 acres near to Watchfield, on the Wiltshire/Oxfordshire border, just off the A420. The site adjoins that of the Westmill Wind Farm Co-operative.\n\nThe solar power plant has over 21,000 solar panels and has been operational since July 2011.\n\nWhen acquired by the Co-operative, it was the largest community-owned photovoltaic power station in the world.\n\nThe Co-operative invited members through a public share offer, launched in June 2012. Despite the tight timetable caused by the expiry date on the option, the issue was 50% over-subscribed, when it closed at the end of July. This raised £4m from c. 1,650 members. \n\nAn additional c. £2m was raised by a private placement of B-Shares, mainly from members who had subscribed to the original issue.\n\nThe Co-operative raised the balance of funding required in the form of a 23½-year senior debt bond. This was arranged with the Lancashire County Council Pension Fund, in what was believed to be the first major funding by a local authority in community-owned energy infrastructure. \n\n\n"}
{"id": "2751871", "url": "https://en.wikipedia.org/wiki?curid=2751871", "title": "Yankee Gale", "text": "Yankee Gale\n\nThe Yankee Gale was a major storm in the Gulf of St. Lawrence near Prince Edward Island, Canada, that began on the night of October 3, 1851 and continued for two days. In addition to local ships, the storm wrecked much of the New England fishing fleet that was working in the waters, giving the gale its name. At least 74 ships were destroyed, and 150 crew were killed.\n\nA brassy appearance was witnessed in the northwest sky on the afternoon of October 3. Long swells started moving into the Gulf of St. Lawrence from the southeast, as the wind began to pick up out of the same quarter. The wind backed to the northeast that evening, increasing to gale force. Surface pressures across the region begin to fall rapidly, with one barometer on Nova Scotia falling 31 hPa or mb/nearly 1 inch within 23 hours. The gales continued for nearly two days, according to one of the sea captain's logs from the area.\n\nOn land the cyclone had minimal effect, but out at sea the storm was significantly more damaging. Ships at sea realized a storm was approaching, and tried to sail east of Prince Edward Island. Northeast gales blocked their progress. Ships attempted to hold position, but their sails tore away in the wind and they drifted towards the island. Ships at anchor either sank at anchor or were capsized by other ships ramming them during the cyclone. Most of the American fishing fleet fell victim to the storm. At least 74 ships and 150 lives were lost. (Rousmaniere)\n\n\n"}
{"id": "54063615", "url": "https://en.wikipedia.org/wiki?curid=54063615", "title": "Zel'dovich number", "text": "Zel'dovich number\n\nThe Zel'dovich number is a dimensionless number which provides a quantitative measure for the activation energy of a chemical reaction which appears in the Arrhenius exponent, named after the Russian scientist Yakov Borisovich Zel'dovich, who along with David A. Frank-Kamenetskii, first introduced in their paper in 1938. In 1983 ICDERS meeting at Poitiers, it was decided to name after Zel'dovich.\n\nIt is defined as\n\nwhere \n\nIn terms of heat release parameter formula_6, it is given by\n\nFor typical combustion phenomena, the value for Zel'dovich number lies in the range formula_8. Activation energy asymptotics uses this number as the large parameter of expansion.\n"}
{"id": "49415014", "url": "https://en.wikipedia.org/wiki?curid=49415014", "title": "Zero Days", "text": "Zero Days\n\nZero Days is a 2016 American documentary film directed by Alex Gibney. It was selected to compete for the Golden Bear at the 66th Berlin International Film Festival.\n\n\"Zero Days\" covers the phenomenon surrounding the Stuxnet computer virus and the development of the malware software known as \"Olympic Games.\" It concludes with discussion over follow-up cyber plan Nitro Zeus and the Iran Nuclear Deal.\n\n\nReview aggregator Rotten Tomatoes collected 66 reviews as of May 6, 2017, of which 91% were positive. The site's consensus states: \"Factors beyond Gibney's control prevent Zero Days from offering a comprehensive look at its subject, but the partial picture that emerges remains as frightening as it is impossible to ignore.\" Metacritic gave the film a score of 77/100 based on 23 critics.\n\nWriting for RogerEbert.com, Godfrey Cheshire praised \"Zero Days\" as \"Easily the most important film anyone has released this year, it is a documentary that deserves to be seen by every sentient citizen of this country—and indeed the world.\"\n\n\"Zero Days\" was among 15 films shortlisted for the Academy Award for Best Feature Documentary, but ultimately did not receive an Oscar nomination. The film won a documentary film Peabody Award in 2017.\n\n\"Zero Days\" was released digitally on Amazon Video and iTunes on December 6, 2016, broadcast on BBC Four in the \"Storyville\" strand in the UK on January 16, 2017, and DVD on January 17, 2017.\n\n"}
{"id": "37378098", "url": "https://en.wikipedia.org/wiki?curid=37378098", "title": "Çetin Dam", "text": "Çetin Dam\n\nThe Çetin Dam is a rock-fill dam with an asphalt-concrete core, currently being constructed on the Botan River in Siirt Province, Turkey. The dam will be located directly downstream of the Botan and Büyük River confluence and have a height of . The primary purpose of the dam is hydroelectric power generation. The dam's power plant will house three 135 MW Francis turbine-generators. Also part of the Çetin project is a smaller dam downstream with in height. The Çetin lower dam will regulate outflows from the Çetin main dam and also produce hydroelectric power with a 112 MW capacity via two 56 MW Kaplan turbine-generators.\n\nThe owner was the Norwegian, Statkraft, at a cost of US $678 million. The Contractor was Yϋksel-iLci Joint Venture. Veidekke Industri AS was the subcontractor for Cetin dam. Of the six dams to be built on the Botan River, it will have the largest power plant. Construction on the main dam began in December 2011 and the power plant was expected to be complete in 2015. The project has experienced delays due to Kurdistan Workers' Party attacks on the construction site and equipment. In February 2016, Statkraft suspended construction.\nThe type is changed. More information will be added. \n\n"}
