{"id": "54722182", "url": "https://en.wikipedia.org/wiki?curid=54722182", "title": "2-Hydroxy-3-morpholinopropanesulfonic acid", "text": "2-Hydroxy-3-morpholinopropanesulfonic acid\n\nMOPSO is a zwitterionic organic chemical buffering agent; one of Good's buffers. MOPSO and MOPS (3-morpholinopropanesulfonic acid) are chemically similar, differing only in the presence of a hydroxyl group on the C-2 of the propane moiety. It has a useful pH range of 6.5-7.9 in the physiological range, making it useful for cell culture work. It has a pKa of 6.9 with ΔpKa/°C of -0.015 and a solubility in water at 0°C of 0.75 M.\n\nMOPSO has been used as a buffer component for:\n\n"}
{"id": "26508821", "url": "https://en.wikipedia.org/wiki?curid=26508821", "title": "2012: Time for Change", "text": "2012: Time for Change\n\n2012: Time for Change is a 2010 feature-length documentary film directed by João G. Amorim. Based in part on the books of Daniel Pinchbeck, it premiered on April 9, 2010 at the Lumiere Theater in San Francisco. The film presents a positive alternative to apocalyptic doom and gloom, and features, among others, David Lynch, Sting, Ellen Page, Gilberto Gil, Barbara Marx Hubbard, Michael Dorsey and Paul Stamets.\n\n"}
{"id": "28395508", "url": "https://en.wikipedia.org/wiki?curid=28395508", "title": "Arnprior Solar Generating Station", "text": "Arnprior Solar Generating Station\n\nThe Arnprior Solar Project is a 23.4 MW solar farm located near the town of near Arnprior, Ontario, Canada. It was developed and is owned by EDF EN Canada and is operated by EDF Renewable Services, both units of EDF Energies Nouvelles. Construction began in May 2009 and operation commenced in December 2009.\n\nThe project consists of 312,000 PV thin film solar panels made by First Solar and mounted on 13,000 fixed-tilt racks in a field.\n\n\n"}
{"id": "657", "url": "https://en.wikipedia.org/wiki?curid=657", "title": "Asphalt", "text": "Asphalt\n\nAsphalt, also known as bitumen (, ), is a sticky, black, and highly viscous liquid or semi-solid form of petroleum. It may be found in natural deposits or may be a refined product, and is classed as a pitch. Before the 20th century, the term asphaltum was also used. The word is derived from the Ancient Greek ἄσφαλτος \"ásphaltos\".\n\nThe primary use (70%) of asphalt is in road construction, where it is used as the glue or binder mixed with aggregate particles to create asphalt concrete. Its other main uses are for bituminous waterproofing products, including production of roofing felt and for sealing flat roofs.\n\nThe terms \"asphalt\" and \"bitumen\" are often used interchangeably to mean both natural and manufactured forms of the substance. In American English, \"asphalt\" (or \"asphalt cement\") is commonly used for a refined residue from the distillation process of selected crude oils. Outside the United States, the product is often called \"bitumen\", and geologists worldwide often prefer the term for the naturally occurring variety. Common colloquial usage often refers to various forms of asphalt as \"tar\", as in the name of the La Brea Tar Pits.\n\nNaturally occurring asphalt is sometimes specified by the term \"crude bitumen\". Its viscosity is similar to that of cold molasses while the material obtained from the fractional distillation of crude oil boiling at is sometimes referred to as \"refined bitumen\". The Canadian province of Alberta has most of the world's reserves of natural asphalt in the Athabasca oil sands, which cover , an area larger than England.\n\nThe word \"asphalt\" is derived from the late Middle English, in turn from French \"asphalte\", based on Late Latin \"asphalton\", \"asphaltum\", which is the latinisation of the Greek ἄσφαλτος (\"ásphaltos\", \"ásphalton\"), a word meaning \"asphalt/bitumen/pitch\", which perhaps derives from ἀ-, \"without\" and σφάλλω (\"sfallō\"), \"make fall\". The first use of asphalt by the ancients was in the nature of a cement for securing or joining together various objects, and it thus seems likely that the name itself was expressive of this application. Specifically, Herodotus mentioned that bitumen was brought to Babylon to build its gigantic fortification wall. From the Greek, the word passed into late Latin, and thence into French (\"asphalte\") and English (\"asphaltum\" and \"asphalt\"). In French, the term \"asphalte\" is used for naturally occurring asphalt-soaked limestone deposits, and for specialised manufactured products with fewer voids or greater bitumen content than the \"asphaltic concrete\" used to pave roads.\n\nThe expression \"bitumen\" originated in the Sanskrit words \"jatu\", meaning \"pitch\", and \"jatu-krit\", meaning \"pitch creating\" or \"pitch producing\" (referring to coniferous or resinous trees). The Latin equivalent is claimed by some to be originally \"gwitu-men\" (pertaining to pitch), and by others, \"pixtumens\" (exuding or bubbling pitch), which was subsequently shortened to \"bitumen\", thence passing via French into English. From the same root is derived the Anglo-Saxon word \"cwidu\" (mastix), the German word \"Kitt\" (cement or mastic) and the old Norse word \"kvada\".\n\nIn British English, \"bitumen\" is used instead of \"asphalt\". The word \"asphalt\" is instead used to refer to asphalt concrete, a mixture of construction aggregate and asphalt itself (also called \"tarmac\" in common parlance). Bitumen mixed with clay was usually called \"asphaltum\", but the term is less commonly used today.\n\nIn Australian English, the word \"asphalt\" is used to describe a mix of construction aggregate. \"Bitumen\" refers to the liquid derived from the heavy-residues from crude oil distillation.\n\nIn American English, \"asphalt\" is equivalent to the British \"bitumen\". However, \"asphalt\" is also commonly used as a shortened form of \"asphalt concrete\" (therefore equivalent to the British \"asphalt\" or \"tarmac\").\n\nIn Canadian English, the word \"bitumen\" is used to refer to the vast Canadian deposits of extremely heavy crude oil, while \"asphalt\" is used for the oil refinery product. Diluted bitumen (diluted with naphtha to make it flow in pipelines) is known as \"dilbit\" in the Canadian petroleum industry, while bitumen \"upgraded\" to synthetic crude oil is known as \"syncrude\", and syncrude blended with bitumen is called \"synbit\".\n\n\"Bitumen\" is still the preferred geological term for naturally occurring deposits of the solid or semi-solid form of petroleum. \"Bituminous rock\" is a form of sandstone impregnated with bitumen. The oil sands of Alberta, Canada are a similar material.\n\nNeither of the terms \"asphalt\" or \"bitumen\" should be confused with tar or coal tars.\n\nThe components of asphalt include four main classes of compounds: \n\nThe naphthene aromatics and polar aromatics are typically the majority components. Most natural bitumens also contain organosulfur compounds, resulting in an overall sulfur content of up to 4%. Nickel and vanadium are found at <10 parts per million, as is typical of some petroleum.\n\nThe substance is soluble in carbon disulfide. It is commonly modelled as a colloid, with asphaltenes as the dispersed phase and maltenes as the continuous phase. \"It is almost impossible to separate and identify all the different molecules of asphalt, because the number of molecules with different chemical structure is extremely large\".\n\nAsphalt may be confused with coal tar, which is a visually similar black, thermoplastic material produced by the destructive distillation of coal. During the early and mid-20th century, when town gas was produced, coal tar was a readily available byproduct and extensively used as the binder for road aggregates. The addition of coal tar to macadam roads led to the word \"tarmac\", which is now used in common parlance to refer to road-making materials. However, since the 1970s, when natural gas succeeded town gas, asphalt has completely overtaken the use of coal tar in these applications. Other examples of this confusion include the La Brea Tar Pits and the Canadian oil sands, both of which actually contain natural bitumen rather than tar. \"Pitch\" is another term sometimes informally used at times to refer to asphalt, as in Pitch Lake.\n\nFor economic and other reasons, asphalt is sometimes sold combined with other materials, often without being labeled as anything other than simply \"asphalt\".\n\nOf particular note, in the 21st century, is the use of re-refined engine oil bottoms -- \"REOB\" or \"REOBs\"—the residue of recycled autmotive engine oil, collected from the bottoms of re-refining vacuum distillation towers. It contains the various non-refined elements and compounds in recycled engine oil, leftover from the re-refining process—both additives to the original oil, and materials accumulating from its circulation in the engine (typically iron and copper). Some research has indicated a correlation between this contamination of asphalt and poorer-performing pavement.\n\nThe majority of asphalt used commercially is obtained from petroleum. Nonetheless, large amounts of asphalt occur in concentrated form in nature. Naturally occurring deposits of bitumen are formed from the remains of ancient, microscopic algae (diatoms) and other once-living things. These remains were deposited in the mud on the bottom of the ocean or lake where the organisms lived. Under the heat (above 50 °C) and pressure of burial deep in the earth, the remains were transformed into materials such as bitumen, kerogen, or petroleum.\n\nNatural deposits of bitumen include lakes such as the Pitch Lake in Trinidad and Tobago and Lake Bermudez in Venezuela. Natural seeps occur in the La Brea Tar Pits and in the Dead Sea.\n\nBitumen also occurs in unconsolidated sandstones known as \"oil sands\" in Alberta, Canada, and the similar \"tar sands\" in Utah, US.\nThe Canadian province of Alberta has most of the world's reserves, in three huge deposits covering , an area larger than England or New York state. These bituminous sands contain of commercially established oil reserves, giving Canada the third largest oil reserves in the world. Although historically it was used without refining to pave roads, nearly all of the output is now used as raw material for oil refineries in Canada and the United States.\n\nThe world's largest deposit of natural bitumen, known as the Athabasca oil sands, is located in the McMurray Formation of Northern Alberta. This formation is from the early Cretaceous, and is composed of numerous lenses of oil-bearing sand with up to 20% oil. Isotopic studies show the oil deposits to be about 110 million years old. Two smaller but still very large formations occur in the Peace River oil sands and the Cold Lake oil sands, to the west and southeast of the Athabasca oil sands, respectively. Of the Alberta deposits, only parts of the Athabasca oil sands are shallow enough to be suitable for surface mining. The other 80% has to be produced by oil wells using enhanced oil recovery techniques like steam-assisted gravity drainage.\n\nMuch smaller heavy oil or bitumen deposits also occur in the Uinta Basin in Utah, US. The Tar Sand Triangle deposit, for example, is roughly 6% bitumen.\n\nBitumen may occur in hydrothermal veins. An example of this is within the Uinta Basin of Utah, in the US, where there is a swarm of laterally and vertically extensive veins composed of a solid hydrocarbon termed Gilsonite. These veins formed by the polymerization and solidification of hydrocarbons that were mobilized from the deeper oil shales of the Green River Formation during burial and diagenesis.\n\nBitumen is similar to the organic matter in carbonaceous meteorites. However, detailed studies have shown these materials to be distinct. The vast Alberta bitumen resources are considered to have started out as living material from marine plants and animals, mainly algae, that died millions of years ago when an ancient ocean covered Alberta. They were covered by mud, buried deeply over time, and gently cooked into oil by geothermal heat at a temperature of . Due to pressure from the rising of the Rocky Mountains in southwestern Alberta, 80 to 55 million years ago, the oil was driven northeast hundreds of kilometres and trapped into underground sand deposits left behind by ancient river beds and ocean beaches, thus forming the oil sands.\n\nThe use of natural bitumen for waterproofing, and as an adhesive dates at least to the fifth millennium BC, with a crop storage basket discovered in Mehrgarh, of the Indus Valley Civilization, lined with it. By the 3rd millennia BC refined rock asphalt was in use, in the region, and was used to waterproof the Great Bath, Mohenjo-daro.\n\nIn the ancient Middle East, the Sumerians used natural bitumen deposits for mortar between bricks and stones, to cement parts of carvings, such as eyes, into place, for ship caulking, and for waterproofing. The Greek historian Herodotus said hot bitumen was used as mortar in the walls of Babylon.\n\nThe long Euphrates Tunnel beneath the river Euphrates at Babylon in the time of Queen Semiramis (ca. 800 BC) was reportedly constructed of burnt bricks covered with bitumen as a waterproofing agent.\n\nBitumen was used by ancient Egyptians to embalm mummies. The Persian word for asphalt is \"moom\", which is related to the English word mummy. The Egyptians' primary source of bitumen was the Dead Sea, which the Romans knew as \"Palus Asphaltites\" (Asphalt Lake).\n\nApproximately 40 AD, Dioscorides described the Dead Sea material as \"Judaicum bitumen\", and noted other places in the region where it could be found.\nThe Sidon bitumen is thought to refer to material found at Hasbeya. Pliny refers also to bitumen being found in Epirus. It was a valuable strategic resource, the object of the first known battle for a hydrocarbon deposit—between the Seleucids and the Nabateans in 312 BC.\n\nIn the ancient Far East, natural bitumen was slowly boiled to get rid of the higher fractions, leaving a thermoplastic material of higher molecular weight that when layered on objects became quite hard upon cooling. This was used to cover objects that needed waterproofing, such as scabbards and other items. Statuettes of household deities were also cast with this type of material in Japan, and probably also in China.\n\nIn North America, archaeological recovery has indicated bitumen was sometimes used to adhere stone projectile points to wooden shafts. In Canada, aboriginal people used bitumen seeping out of the banks of the Athabasca and other rivers to waterproof birch bark canoes, and also heated it in smudge pots to ward off mosquitoes in the summer.\n\nIn 1553, Pierre Belon described in his work \"Observations\" that \"pissasphalto\", a mixture of pitch and bitumen, was used in the Republic of Ragusa (now Dubrovnik, Croatia) for tarring of ships.\n\nAn 1838 edition of \"Mechanics Magazine\" cites an early use of asphalt in France. A pamphlet dated 1621, by \"a certain Monsieur d'Eyrinys, states that he had discovered the existence (of asphaltum) in large quantities in the vicinity of Neufchatel\", and that he proposed to use it in a variety of ways – \"principally in the construction of air-proof granaries, and in protecting, by means of the arches, the water-courses in the city of Paris from the intrusion of dirt and filth\", which at that time made the water unusable. \"He expatiates also on the excellence of this material for forming level and durable terraces\" in palaces, \"the notion of forming such terraces in the streets not one likely to cross the brain of a Parisian of that generation\".\n\nBut the substance was generally neglected in France until the revolution of 1830. In the 1830s there was a surge of interest, and asphalt became widely used \"for pavements, flat roofs, and the lining of cisterns, and in England, some use of it had been made of it for similar purposes\". Its rise in Europe was \"a sudden phenomenon\", after natural deposits were found \"in France at Osbann (Bas-Rhin), the Parc (Ain) and the Puy-de-la-Poix (Puy-de-Dôme)\", although it could also be made artificially. One of the earliest uses in France was the laying of about 24,000 square yards of Seyssel asphalt at the Place de la Concorde in 1835.\n\nAmong the earlier uses of bitumen in the United Kingdom was for etching. William Salmon's \"Polygraphice\" (1673) provides a recipe for varnish used in etching, consisting of three ounces of virgin wax, two ounces of mastic, and one ounce of asphaltum. By the fifth edition in 1685, he had included more asphaltum recipes from other sources.\n\nThe first British patent for the use of asphalt was \"Cassell's patent asphalte or bitumen\" in 1834. Then on 25 November 1837, Richard Tappin Claridge patented the use of Seyssel asphalt (patent #7849), for use in asphalte pavement, having seen it employed in France and Belgium when visiting with Frederick Walter Simms, who worked with him on the introduction of asphalt to Britain. Dr T. Lamb Phipson writes that his father, Samuel Ryland Phipson, a friend of Claridge, was also \"instrumental in introducing the asphalte pavement (in 1836)\". Indeed, mastic pavements had been previously employed at Vauxhall by a competitor of Claridge, but without success.\n\nClaridge obtained a patent in Scotland on 27 March 1838, and obtained a patent in Ireland on 23 April 1838. In 1851, extensions for the 1837 patent and for both 1838 patents were sought by the trustees of a company previously formed by Claridge. \"Claridge's Patent Asphalte Company\"—formed in 1838 for the purpose of introducing to Britain \"Asphalte in its natural state from the mine at Pyrimont Seysell in France\",—\"laid one of the first asphalt pavements in Whitehall\". Trials were made of the pavement in 1838 on the footway in Whitehall, the stable at Knightsbridge Barracks, \"and subsequently on the space at the bottom of the steps leading from Waterloo Place to St. James Park\". \"The formation in 1838 of Claridge's Patent Asphalte Company (with a distinguished list of aristocratic patrons, and Marc and Isambard Brunel as, respectively, a trustee and consulting engineer), gave an enormous impetus to the development of a British asphalt industry\". \"By the end of 1838, at least two other companies, Robinson's and the Bastenne company, were in production\", with asphalt being laid as paving at Brighton, Herne Bay, Canterbury, Kensington, the Strand, and a large floor area in Bunhill-row, while meantime Claridge's Whitehall paving \"continue(d) in good order\".\n\nIn 1838, there was a flurry of entrepreneurial activity involving asphalt, which had uses beyond paving. For example, asphalt could also be used for flooring, damp proofing in buildings, and for waterproofing of various types of pools and baths, both of which were also proliferating in the 19th century. On the London stockmarket, there were various claims as to the exclusivity of asphalt quality from France, Germany and England. And numerous patents were granted in France, with similar numbers of patent applications being denied in England due to their similarity to each other. In England, \"Claridge's was the type most used in the 1840s and 50s\".\n\nIn 1914, Claridge's Company entered into a joint venture to produce tar-bound macadam, with materials manufactured through a subsidiary company called Clarmac Roads Ltd. Two products resulted, namely \"Clarmac\", and \"Clarphalte\", with the former being manufactured by Clarmac Roads and the latter by Claridge's Patent Asphalte Co., although \"Clarmac\" was more widely used. However, the First World War ruined the Clarmac Company, which entered into liquidation in 1915. The failure of Clarmac Roads Ltd had a flow-on effect to Claridge's Company, which was itself compulsorily wound up, ceasing operations in 1917, having invested a substantial amount of funds into the new venture, both at the outset and in a subsequent attempt to save the Clarmac Company.\n\nThe first use of bitumen in the New World was by indigenous peoples. On the west coast, as early as the 13th century, the Tongva, Luiseño and Chumash peoples collected the naturally occurring bitumen that seeped to the surface above underlying petroleum deposits. All three groups used the substance as an adhesive. It is found on many different artifacts of tools and ceremonial items. For example, it was used on rattles to adhere gourds or turtle shells to rattle handles. It was also used in decorations. Small round shell beads were often set in asphaltum to provide decorations. It was used as a sealant on baskets to make them watertight for carrying water, possibly poisoning those who drank the water. Asphalt was used also to seal the planks on ocean-going canoes.\n\nAsphalt was first used to pave streets in the 1870s. At first naturally occurring \"bituminous rock\" was used, such as at Ritchie Mines in Macfarlan in Ritchie County, West Virginia from 1852 to 1873. In 1876, asphalt-based paving was used to pave Pennsylvania Avenue in Washington DC, in time for the celebration of the national centennial. In the horse-drawn era, streets were unpaved and covered with dirt or gravel. However, that produced uneven wear, opened new hazards for pedestrians and made for dangerous potholes for bicycles and for motor vehicles. Manhattan alone had 130,000 horses in 1900, pulling streetcars, wagons, and carriages, and leaving their waste behind. They were not fast, and pedestrians could dodge and scramble their way across the crowded streets. Small towns continued to rely on dirt and gravel, but larger cities wanted much better streets. They looked to wood or granite blocks by the 1850s. In 1890, a third of Chicago's 2000 miles of streets were paved, chiefly with wooden blocks, which gave better traction than mud. Brick surfacing was a good compromise, but even better was asphalt paving, which was easy to install and to cut through to get at sewers. With London and Paris serving as models, Washington laid 400,000 square yards of asphalt paving by 1882; it became the model for Buffalo, Philadelphia and elsewhere. By the end of the century, American cities boasted 30 million square yards of asphalt paving, well ahead of brick. The streets became faster and more dangerous so electric traffic lights were installed. Electric trolleys (at 12 miles per hour) became the main transportation service for middle class shoppers and office workers until they bought automobiles after 1945 and commuted from more distant suburbs in privacy and comfort on asphalt highways.\n\nCanada has the world's largest deposit of natural bitumen in the Athabasca oil sands, and Canadian First Nations along the Athabasca River had long used it to waterproof their canoes. In 1719, a Cree named Wa-Pa-Su brought a sample for trade to Henry Kelsey of the Hudson’s Bay Company, who was the first recorded European to see it. However, it wasn't until 1787 that fur trader and explorer Alexander MacKenzie saw the Athabasca oil sands and said, \"At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance.\"\n\nThe value of the deposit was obvious from the start, but the means of extracting the bitumen was not. The nearest town, Fort McMurray, Alberta, was a small fur trading post, other markets were far away, and transportation costs were too high to ship the raw bituminous sand for paving. In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the product to pave 600 feet of road in Edmonton, Alberta. Other roads in Alberta were paved with material extracted from oil sands, but it was generally not economic. During the 1920s Dr. Karl A. Clark of the Alberta Research Council patented a hot water oil separation process and entrepreneur Robert C. Fitzsimmons built the Bitumount oil separation plant, which between 1925 and 1958 produced up to per day of bitumen using Dr. Clark's method. Most of the bitumen was used for waterproofing roofs, but other uses included fuels, lubrication oils, printers ink, medicines, rust- and acid-proof paints, fireproof roofing, street paving, patent leather, and fence post preservatives. Eventually Fitzsimmons ran out of money and the plant was taken over by the Alberta government. Today the Bitumount plant is a Provincial Historic Site.\n\nBitumen was used in early photographic technology. In 1826 or 1827, it was used by French scientist Joseph Nicéphore Niépce to make the oldest surviving photograph from nature. The bitumen was thinly coated onto a pewter plate which was then exposed in a camera. Exposure to light hardened the bitumen and made it insoluble, so that when it was subsequently rinsed with a solvent only the sufficiently light-struck areas remained. Many hours of exposure in the camera were required, making bitumen impractical for ordinary photography, but from the 1850s to the 1920s it was in common use as a photoresist in the production of printing plates for various photomechanical printing processes.\n\nBitumen was the nemesis of many artists during the 19th century. Although widely used for a time, it ultimately proved unstable for use in oil painting, especially when mixed with the most common diluents, such as linseed oil, varnish and turpentine. Unless thoroughly diluted, bitumen never fully solidifies and will in time corrupt the other pigments with which it comes into contact. The use of bitumen as a glaze to set in shadow or mixed with other colors to render a darker tone resulted in the eventual deterioration of many paintings, for instance those of Delacroix. Perhaps the most famous example of the destructiveness of bitumen is Théodore Géricault's Raft of the Medusa (1818–1819), where his use of bitumen caused the brilliant colors to degenerate into dark greens and blacks and the paint and canvas to buckle.\n\nThe vast majority of refined asphalt is used in construction: primarily as a constituent of products used in paving and roofing applications. According to the requirements of the end use, asphalt is produced to specification. This is achieved either by refining or blending. It is estimated that the current world use of asphalt is approximately 102 million tonnes per year. Approximately 85% of all the asphalt produced is used as the binder in asphalt concrete for roads. It is also used in other paved areas such as airport runways, car parks and footways. Typically, the production of asphalt concrete involves mixing fine and coarse aggregates such as sand, gravel and crushed rock with asphalt, which acts as the binding agent. Other materials, such as recycled polymers (e.g., rubber tyres), may be added to the asphalt to modify its properties according to the application for which the asphalt is ultimately intended.\nA further 10% of global asphalt production is used in roofing applications, where its waterproofing qualities are invaluable.\nThe remaining 5% of asphalt is used mainly for sealing and insulating purposes in a variety of building materials, such as pipe coatings, carpet tile backing and paint.\nAsphalt is applied in the construction and maintenance of many structures, systems, and components, such as the following:\n\nThe largest use of asphalt is for making asphalt concrete for road surfaces; this accounts for approximately 85% of the asphalt consumed in the United States. There are about 4,000 asphalt concrete mixing plants in the US, and a similar number in Europe.\n\nAsphalt concrete pavement mixes are typically composed of 5% asphalt cement and 95% aggregates (stone, sand, and gravel). Due to its highly viscous nature, asphalt cement must be heated so it can be mixed with the aggregates at the asphalt mixing facility. The temperature required varies depending upon characteristics of the asphalt and the aggregates, but warm-mix asphalt technologies allow producers to reduce the temperature required.\n\nThe weight of an asphalt pavement depends upon the aggregate type, the asphalt, and the air void content. An average example in the United States is about 112 pounds per square yard, per inch of pavement thickness.\n\nWhen maintenance is performed on asphalt pavements, such as milling to remove a worn or damaged surface, the removed material can be returned to a facility for processing into new pavement mixtures. The asphalt in the removed material can be reactivated and put back to use in new pavement mixes. With some 95% of paved roads being constructed of or surfaced with asphalt, a substantial amount of asphalt pavement material is reclaimed each year. According to industry surveys conducted annually by the Federal Highway Administration and the National Asphalt Pavement Association, more than 99% of the asphalt removed each year from road surfaces during widening and resurfacing projects is reused as part of new pavements, roadbeds, shoulders and embankments.\n\nAsphalt concrete paving is widely used in airports around the world. Due to the sturdiness and ability to be repaired quickly, it is widely used for runways.\n\nMastic asphalt is a type of asphalt that differs from dense graded asphalt (asphalt concrete) in that it has a higher asphalt (binder) content, usually around 7–10% of the whole aggregate mix, as opposed to rolled asphalt concrete, which has only around 5% asphalt. This thermoplastic substance is widely used in the building industry for waterproofing flat roofs and tanking underground. Mastic asphalt is heated to a temperature of and is spread in layers to form an impervious barrier about thick.\n\nA number of technologies allow asphalt to be mixed at much lower temperatures. These involve mixing with petroleum solvents to form \"cutbacks\" with reduced melting point or mixing with water to turn the asphalt into an emulsion. Asphalt emulsions contain up to 70% asphalt and typically less than 1.5% chemical additives. There are two main types of emulsions with different affinity for aggregates, cationic and anionic. Asphalt emulsions are used in a wide variety of applications. Chipseal involves spraying the road surface with asphalt emulsion followed by a layer of crushed rock, gravel or crushed slag. Slurry seal involves the creation of a mixture of asphalt emulsion and fine crushed aggregate that is spread on the surface of a road. Cold-mixed asphalt can also be made from asphalt emulsion to create pavements similar to hot-mixed asphalt, several inches in depth, and asphalt emulsions are also blended into recycled hot-mix asphalt to create low-cost pavements.\n\nSynthetic crude oil, also known as syncrude, is the output from a bitumen upgrader facility used in connection with oil sand production in Canada. Bituminous sands are mined using enormous (100 ton capacity) power shovels and loaded into even larger (400 ton capacity) dump trucks for movement to an upgrading facility. The process used to extract the bitumen from the sand is a hot water process originally developed by Dr. Karl Clark of the University of Alberta during the 1920s. After extraction from the sand, the bitumen is fed into a bitumen upgrader which converts it into a light crude oil equivalent. This synthetic substance is fluid enough to be transferred through conventional oil pipelines and can be fed into conventional oil refineries without any further treatment. By 2015 Canadian bitumen upgraders were producing over per day of synthetic crude oil, of which 75% was exported to oil refineries in the United States.\n\nIn Alberta, five bitumen upgraders produce synthetic crude oil and a variety of other products: The Suncor Energy upgrader near Fort McMurray, Alberta produces synthetic crude oil plus diesel fuel; the Syncrude Canada, Canadian Natural Resources, and Nexen upgraders near Fort McMurray produce synthetic crude oil; and the Shell Scotford Upgrader near Edmonton produces synthetic crude oil plus an intermediate feedstock for the nearby Shell Oil Refinery. A sixth upgrader, under construction in 2015 near Redwater, Alberta, will upgrade half of its crude bitumen directly to diesel fuel, with the remainder of the output being sold as feedstock to nearby oil refineries and petrochemical plants.\n\nCanadian bitumen does not differ substantially from oils such as Venezuelan extra-heavy and Mexican heavy oil in chemical composition, and the real difficulty is moving the extremely viscous bitumen through oil pipelines to the refinery. Many modern oil refineries are extremely sophisticated and can process non-upgraded bitumen directly into products such as gasoline, diesel fuel, and refined asphalt without any preprocessing. This is particularly common in areas such as the US Gulf coast, where refineries were designed to process Venezuelan and Mexican oil, and in areas such as the US Midwest where refineries were rebuilt to process heavy oil as domestic light oil production declined. Given the choice, such heavy oil refineries usually prefer to buy bitumen rather than synthetic oil because the cost is lower, and in some cases because they prefer to produce more diesel fuel and less gasoline. By 2015 Canadian production and exports of non-upgraded bitumen exceeded that of synthetic crude oil at over per day, of which about 65% was exported to the United States.\n\nBecause of the difficulty of moving crude bitumen through pipelines, non-upgraded bitumen is usually diluted with natural-gas condensate in a form called dilbit or with synthetic crude oil, called synbit. However, to meet international competition, much non-upgraded bitumen is now sold as a blend of multiple grades of bitumen, conventional crude oil, synthetic crude oil, and condensate in a standardized benchmark product such as Western Canadian Select. This sour, heavy crude oil blend is designed to have uniform refining characteristics to compete with internationally marketed heavy oils such as Mexican Mayan or Arabian Dubai Crude.\n\nAsphalt was used starting in the 1960s as an hydrophobic matrix aiming to encapsulate radioactive waste such as medium-activity salts (mainly soluble sodium nitrate and sodium sulfate) produced by the reprocessing of spent nuclear fuels or radioactive sludges from sedimentation ponds. Bituminised radioactive waste containing highly radiotoxic alpha-emitting transuranic elements from nuclear reprocessing plants have been produced at industrial scale in France, Belgium and Japan, but this type of waste conditioning has been abandoned because operational safety issues (risks of fire, as occurred in a bituminisation plant at Tokai Works in Japan) and long-term stability problems related to their geological disposal in deep rock formations. One of the main problem is the swelling of asphalt exposed to radiation and to water. Asphalt swelling is first induced by radiation because of the presence of hydrogen gas bubbles generated by alpha and gamma radiolysis. A second mechanism is the matrix swelling when the encapsulated hygroscopic salts exposed to water or moisture start to rehydrate and to dissolve. The high concentration of salt in the pore solution inside the bituminised matrix is then responsible for osmotic effects inside the bituminised matrix. The water moves in the direction of the concentrated salts, the asphalt acting as a semi-permeable membrane. This also causes the matrix to swell. The swelling pressure due to osmotic effect under constant volume can be as high as 200 bar. If not properly managed, this high pressure can cause fractures in the near field of a disposal gallery of bituminised medium-level waste. When the bituminised matrix has been altered by swelling, encapsulated radionuclides are easily leached by the contact of ground water and released in the geosphere. The high ionic strength of the concentrated saline solution also favours the migration of radionuclides in clay host rocks. The presence of chemically reactive nitrate can also affect the redox conditions prevailing in the host rock by establishing oxidizing conditions, preventing the reduction of redox-sensitive radionuclides. Under their higher valences, radionuclides of elements such as selenium, technetium, uranium, neptunium and plutonium have a higher solubility and are also often present in water as non-retarded anions. This makes the disposal of medium-level bituminised waste very challenging.\n\nDifferent type of asphalt have been used: blown bitumen (partly oxidized with air oxygen at high temperature after distillation, and harder) and direct distillation bitumen (softer). Blown bitumens like Mexphalte, with a high content of saturated hydrocarbons, are more easily biodegraded by microorganisms than direct distillation bitumen, with a low content of saturated hydrocarbons and a high content of aromatic hydrocarbons.\n\nConcrete encapsulation of radwaste is presently considered a safer alternative by the nuclear industry and the waste management organisations.\n\nRoofing shingles account for most of the remaining asphalt consumption. Other uses include cattle sprays, fence-post treatments, and waterproofing for fabrics. Asphalt is used to make Japan black, a lacquer known especially for its use on iron and steel, and it is also used in paint and marker inks by some exterior paint supply companies to increase the weather resistance and permanence of the paint or ink, and to make the color darker. Asphalt is also used to seal some alkaline batteries during the manufacturing process.\n\nAbout 40,000,000 tons were produced in 1984. It is obtained as the \"heavy\" (i.e., difficult to distill) fraction. Material with a boiling point greater than around 500 °C is considered asphalt. Vacuum distillation separates it from the other components in crude oil (such as naphtha, gasoline and diesel). The resulting material is typically further treated to extract small but valuable amounts of lubricants and to adjust the properties of the material to suit applications. In a de-asphalting unit, the crude asphalt is treated with either propane or butane in a supercritical phase to extract the lighter molecules, which are then separated. Further processing is possible by \"blowing\" the product: namely reacting it with oxygen. This step makes the product harder and more viscous.\n\nAsphalt is typically stored and transported at temperatures around . Sometimes diesel oil or kerosene are mixed in before shipping to retain liquidity; upon delivery, these lighter materials are separated out of the mixture. This mixture is often called \"bitumen feedstock\", or BFS. Some dump trucks route the hot engine exhaust through pipes in the dump body to keep the material warm. The backs of tippers carrying asphalt, as well as some handling equipment, are also commonly sprayed with a releasing agent before filling to aid release. Diesel oil is no longer used as a release agent due to environmental concerns.\n\nNaturally occurring crude bitumen impregnated in sedimentary rock is the prime feed stock for petroleum production from \"oil sands\", currently under development in Alberta, Canada. Canada has most of the world's supply of natural bitumen, covering 140,000 square kilometres (an area larger than England), giving it the second-largest proven oil reserves in the world. The Athabasca oil sands are the largest bitumen deposit in Canada and the only one accessible to surface mining, although recent technological breakthroughs have resulted in deeper deposits becoming producible by \"in situ\" methods. Because of oil price increases after 2003, producing bitumen became highly profitable, but as a result of the decline after 2014 it became uneconomic to build new plants again. By 2014, Canadian crude bitumen production averaged about per day and was projected to rise to per day by 2020. The total amount of crude bitumen in Alberta that could be extracted is estimated to be about , which at a rate of would last about 200 years.\n\nAlthough uncompetitive economically, asphalt can be made from nonpetroleum-based renewable resources such as sugar, molasses and rice, corn and potato starches. Asphalt can also be made from waste material by fractional distillation of used motor oil, which is sometimes otherwise disposed of by burning or dumping into landfills. Use of motor oil may cause premature cracking in colder climates, resulting in roads that need to be repaved more frequently.\n\nNonpetroleum-based asphalt binders can be made light-colored. Lighter-colored roads absorb less heat from solar radiation, reducing their contribution to the urban heat island effect. Parking lots that use asphalt alternatives are called green parking lots.\n\nSelenizza is a naturally occurring solid hydrocarbon bitumen found in native deposits in Selenice, in Albania, the only European asphalt mine still in use. The bitumen is found in the form of veins, filling cracks in a more or less horizontal direction. The bitumen content varies from 83% to 92% (soluble in carbon disulphide), with a penetration value near to zero and a softening point (ring and ball) around 120 °C. The insoluble matter, consisting mainly of silica ore, ranges from 8% to 17%.\nAlbanian bitumen extraction has a long history and was practiced in an organized way by the Romans. After centuries of silence, the first mentions of Albanian bitumen appeared only in 1868, when the Frenchman Coquand published the first geological description of the deposits of Albanian bitumen. In 1875, the exploitation rights were granted to the Ottoman government and in 1912, they were transferred to the Italian company Simsa. Since 1945, the mine was exploited by the Albanian government and from 2001 to date, the management passed to a French company, which organized the mining process for the manufacture of the natural bitumen on an industrial scale.\n\nToday the mine is predominantly exploited in an open pit quarry but several of the many underground mines (deep and extending over several km) still remain viable. Selenizza is produced primarily in granular form, after melting the bitumen pieces selected in the mine.\nSelenizza is mainly used as an additive in the road construction sector. It is mixed with traditional asphalt to improve both the viscoelastic properties and the resistance to ageing. It may be blended with the hot asphalt in tanks, but its granular form allows it to be fed in the mixer or in the recycling ring of normal asphalt plants. Other typical applications include the production of mastic asphalts for sidewalks, bridges, car-parks and urban roads as well as drilling fluid additives for the oil and gas industry. Selenizza is available in powder or in granular material of various particle sizes and is packaged in sacks or in thermal fusible polyethylene bags.\n\nA life-cycle assessment study of the natural selenizza compared with petroleum asphalt has shown that the environmental impact of the selenizza is about half the impact of the road asphalt produced in oil refineries in terms of carbon dioxide emission.\n\nAlthough asphalt typically makes up only 4 to 5 percent (by weight) of the pavement mixture, as the pavement’s binder, it is also the most expensive part of the cost of the road-paving material.\n\nDuring asphalt's early use in modern paving, oil refiners gave it away. However, asphalt is, today, a highly traded commodity. Its prices increased substantially in the early 21st Century. A U.S. government report states:\n\nThe report indicates that an \"average\" 1-mile (1.6-kilometer)-long, four-lane highway would include \"300 tons of asphalt,\" which, \"in 2002 would have cost around $48,000. By 2006 this would have increased to $96,000 and by 2012 to $183,000... an increase of about $135,000 for every mile of highway in just 10 years.\"\n\nPeople can be exposed to asphalt in the workplace by breathing in fumes or skin absorption. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit of 5 mg/m over a 15-minute period.\n\nAsphalt is basically an inert material that must be heated or diluted to a point where it becomes workable for the production of materials for paving, roofing, and other applications. In examining the potential health hazards associated with asphalt, the International Agency for Research on Cancer (IARC) determined that it is the application parameters, predominantly temperature, that affect occupational exposure and the potential bioavailable carcinogenic hazard/risk of the asphalt emissions. In particular, temperatures greater than 199 °C (390 °F), were shown to produce a greater exposure risk than when asphalt was heated to lower temperatures, such as those typically used in asphalt pavement mix production and placement. IARC has classified asphalt as a Class 2B possible carcinogen. \n\n\n"}
{"id": "2423723", "url": "https://en.wikipedia.org/wiki?curid=2423723", "title": "Bond length", "text": "Bond length\n\nIn molecular geometry, bond length or bond distance is the average distance between nuclei of two bonded atoms in a molecule. It is a transferable property of a bond between atoms of fixed types, relatively independent of the rest of the molecule.\n\nBond length is related to bond order: when more electrons participate in bond formation the bond is shorter. Bond length is also inversely related to bond strength and the bond dissociation energy: all other factors being equal, a stronger bond will be shorter. In a bond between two identical atoms, half the bond distance is equal to the covalent radius.\n\nBond lengths are measured in the solid phase by means of X-ray diffraction, or approximated in the gas phase by microwave spectroscopy. A bond between a given pair of atoms may vary between different molecules. For example, the carbon to hydrogen bonds in methane are different from those in methyl chloride. It is however possible to make generalizations when the general structure is the same.\n\nA table with experimental single bonds for carbon to other elements is given below. Bond lengths are given in picometers. By approximation the bond distance between two different atoms is the sum of the individual covalent radii (these are given in the chemical element articles for each element). As a general trend, bond distances \"decrease\" across the \"row\" in the periodic table and \"increase\" down a \"group\". This trend is identical to that of the atomic radius.\n\nThe bond length between two atoms in a molecule depends not only on the atoms but also on such factors as the orbital hybridization and the electronic and steric nature of the substituents. The carbon–carbon (C–C) bond length in diamond is 154 pm, which is also the largest bond length that exists for ordinary carbon covalent bonds. Since one atomic unit of length(i.e., a Bohr radius) is 52.9177 pm, the C–C bond length is 2.91 atomic units, or approximately three Bohr radii long.\n\nUnusually long bond lengths do exist. In one compound, tricyclobutabenzene, a bond length of 160 pm is reported. The current record holder is another cyclobutabenzene with length 174 pm based on X-ray crystallography. In this type of compound the cyclobutane ring would force 90° angles on the carbon atoms connected to the benzene ring where they ordinarily have angles of 120°.\n\nThe existence of a very long C–C bond length of up to 290 pm is claimed in a dimer of two tetracyanoethylene dianions, although this concerns a 2-electron-4-center bond. This type of bonding has also been observed in neutral phenalenyl dimers. The bond lengths of these so-called \"pancake bonds\" are up to 305 pm.\n\nShorter than average C–C bond distances are also possible: alkenes and alkynes have bond lengths of respectively 133 and 120 pm due to increased s-character of the sigma bond. In benzene all bonds have the same length: 139 pm. Carbon–carbon single bonds increased s-character is also notable in the central bond of diacetylene (137 pm) and that of a certain tetrahedrane dimer (144 pm).\n\nIn propionitrile the cyano group withdraws electrons, also resulting in a reduced bond length (144 pm). Squeezing a C–C bond is also possible by application of strain. An unusual organic compound exists called In-methylcyclophane with a very short bond distance of 147 pm for the methyl group being squeezed between a triptycene and a phenyl group. In an in silico experiment a bond distance of 136 pm was estimated for neopentane locked up in fullerene. The smallest theoretical C–C single bond obtained in this study is 131 pm for a hypothetical tetrahedrane derivative.\n\nThe same study also estimated that stretching or squeezing the C–C bond in an ethane molecule by 5 pm required 2.8 or 3.5 kJ/mol, respectively. Stretching or squeezing the same bond by 15 pm required an estimated 21.9 or 37.7 kJ/mol.\n\n"}
{"id": "25249816", "url": "https://en.wikipedia.org/wiki?curid=25249816", "title": "Cavan (unit)", "text": "Cavan (unit)\n\nCavan, sometimes spelled Caban or Kaban) is Filipino unit of mass and also a unit of volume or dry measure.\n\n\"Cavan\" was defined in the 19th century by the government of the Spanish East Indies as being equivalent to 75 litres. Though officially the Philippines became entirely metric in 1860, this value was still in use well into the 20th century.\n\n\"Cavan\" was reported in the late 19th century as a measure for rice equivalent to 98.28 litres. Various references from the same period describe it as a unit of mass: for rice, 133 lb (about 60.33 kg); for cocoa, 83.5 lb, (about 37.87 kg) one source says on the average 60 kg for rice and 38 kg for cacao). Other sources claim it was the equivalent of 58.2 kg. \n\nIn all likelihood, this is a case in which some commodities began to be traded by weight instead of volume, and a “caban of rice” became a certain mass rather than a certain volume. One source states that before 1973 a cavan of any type of rice weighed 50 kg. One source says that after 1973 a cavan of rough rice weighed 44 kg and a cavan of milled rice weighed 56 kg (the significance of the 1973 date is unclear).\n\nUsage example: \"At present, owing to the late scarcity of rice in Camarines and Leyte, the price of paddy at Iloilo has risen to 10 rials per province cavan, which is equal to one and a half of the measure () used at Manila.\"\n"}
{"id": "3301582", "url": "https://en.wikipedia.org/wiki?curid=3301582", "title": "Charactron", "text": "Charactron\n\nCharactron was a U.S. registered trademark (number 0585950, 23 February 1954) of Consolidated Vultee Aircraft Corporation (Convair) for its shaped electron beam cathode ray tube. Charactron CRTs performed functions of both a display device and a read-only memory storing multiple characters and fonts. The similar Typotron was a U.S. registered trademark (23 November 1953) of Hughes Aircraft Corporation for its type of shaped electron beam storage tube with a direct-view bistable storage screen.\n\nThe Charactron CRT used an electron beam to flood a specially patterned perforated anode that contained the stencil patterns for each of the characters that it could form. The first deflection positioning of the electron beam steered the beam to pass through one of the (typically 64 or 116) characters and symbols that could be formed. The beam, which then had the cross-section of the desired character, was re-centered along the axis of the tube and deflected to the desired position of the screen for display. Alternately, as in the accompanying image, the entire matrix was filled with the electron beam then deflected through a selection aperture to isolate one character.\n\nThe term Charactron is sometimes mistakenly applied to another type of CRT properly called a monoscope which generates an electrical signal by scanning an electron beam of uniform cross section across a printed pattern on an internal target electrode.\n\nThere were two basic types/uses of Charactrons:\n\nThe technical expertise, and trademarks, for the Charactron ultimately passed to Stromberg-Carlson, General Dynamics, Stromberg DatagraphiX, Anacomp, and finally Lexel Imaging Systems. \n\n\n\n"}
{"id": "12253521", "url": "https://en.wikipedia.org/wiki?curid=12253521", "title": "Convective storm detection", "text": "Convective storm detection\n\nConvective storm detection is the meteorological observation of deep, moist convection (DMC) and consists of detection, monitoring, and short-term prediction. Convective storms can produce tornadoes as well as large hail, strong winds, and heavy rain leading to flash flooding. The detection of convective storms relies on direct eyewitness observations, for example from storm spotters; and on remote sensing, especially weather radar. Some in situ measurements are used for direct detection as well, notably, wind speed reports from surface observation stations. It is part of the \"integrated warning system\", consisting of prediction, detection, and dissemination of information on severe weather to users such as emergency management, storm spotters and chasers, the media, and the general public.\n\nRigorous attempts to warn of tornadoes began in the United States in the mid-20th century. Before the 1950s, the only method of detecting a tornado was by someone seeing it on the ground. Often, news of a tornado would reach a local weather office after the storm.\n\nHowever, with the advent of weather radar, areas near a local office could get advance warning of severe weather. The first public tornado warnings were issued in 1950 and the first tornado watches and convective outlooks in 1952. In 1953 it was confirmed that hook echoes are associated with tornadoes. By recognizing these radar signatures, meteorologists could detect thunderstorms likely producing tornadoes from dozens of miles away.\n\nIn the mid-1970s, the US National Weather Service (NWS) increased its efforts to train storm spotters to identify and report key features of storms which indicate severe hail, damaging winds, and tornadoes, as well as damage itself and flash flooding. The program was called Skywarn, and the spotters were local sheriff's deputies, state troopers, firefighters, ambulance drivers, amateur radio operators, civil defense (now emergency management) spotters, storm chasers, and ordinary citizens. When severe weather is anticipated, local weather service offices request that these spotters look out for severe weather, and report any tornadoes immediately, so that the office can issue a timely warning.\n\nUsually, spotters are trained by the NWS on behalf of their respective organizations, and they report to them. The organizations activate public warning systems such as sirens and the Emergency Alert System, and forward the reports to the NWS, which does directly disseminate information and warnings through its NOAA Weather Radio All Hazards network. There are more than 230,000 trained Skywarn weather spotters across the United States.\n\nIn Canada, a similar network of volunteer weather watchers, called Canwarn, helps spot severe weather, with more than 1,000 volunteers. \n\nIn Europe, several nations are organizing spotter networks under the auspices of Skywarn Europe and the Tornado and Storm Research Organisation (TORRO) has maintained a network of spotters in the United Kingdom since the 1970s.\n\nStorm spotters are needed because radar systems such as NEXRAD, and satellite images, do not detect tornadoes or hail, only indications that the storm has the potential. Radar and satellite data interpretation will usually give a warning before there is any visual evidence of such events, but ground truth from an observer can either verify the threat or determine it is not imminent. The spotter's ability to see what these remote sensing devices cannot is especially important as distance from a radar site increases, because the radar beam becomes progressively higher in altitude further away from the radar, due to curvature of Earth and the spread of the beam with distance. Therefore, when far from a radar, only precipitations and velocities high in the storm are observed. The important areas might not then be sampled or the resolution of the data might be poor. Also, some meteorological situations leading to tornadogenesis are not readily detectable by radar and on occasion tornado development may occur more quickly than radar can complete a scan and send the batch of data.\n\nStorm spotters are trained to discern whether a storm seen from a distance is a supercell. They typically look to its rear, the main region of updraft and inflow. Under the updraft is a rain-free base, and the next step of tornadogenesis is the formation of a rotating wall cloud. The vast majority of intense tornadoes occur with a wall cloud on the backside of a supercell.\n\nEvidence of a supercell comes from the storm's shape and structure, and cloud tower features such as a hard and vigorous updraft tower, a persistent and/or large overshooting top, a hard anvil (especially when backsheared against strong upper level winds), and a corkscrew look or striations. Under the storm and closer to where most tornadoes are found, evidence of a supercell and likelihood of a tornado includes inflow bands (particularly when curved) such as a \"beaver tail\", and other clues such as strength of inflow, warmth and moistness of inflow air, how outflow- or inflow-dominant a storm appears, and how far is the forward flank precipitation core from the wall cloud. Tornadogenesis is most likely at the interface of the updraft and forward flank downdraft, and requires a \"balance\" between the outflow and inflow.\n\nOnly wall clouds that rotate spawn tornadoes, and usually precede the tornado by five to thirty minutes. Rotating wall clouds are the visual manifestation of a mesocyclone. Barring a low-level boundary, tornadogenesis is highly unlikely unless a rear flank downdraft occurs, which is usually visibly evidenced by evaporation of cloud adjacent to a corner of a wall cloud. A tornado often occurs as this happens or shortly after; first, a funnel cloud dips and in nearly all cases by the time it reaches halfway down, a surface swirl has already developed, signifying a tornado is on the ground before condensation connects the surface circulation to the storm. Tornadoes may also occur without wall clouds, under flanking lines, and on the leading edge. Spotters monitor all areas of a storm and their surroundings.\n\nToday, most developed countries have a network of weather radars, which remains the main method of detecting signatures likely associated with tornadoes and other severe phenomenons as hail and downbursts. Radar is always available, in places and times where spotters are not, and can also see features that spotters cannot, in the darkness of night and processes hidden within the cloud as well as invisible processes outside the cloud.\n\nIn short-term prediction and detection of tornadoes, meteorologists integrate radar data with reports from the field and knowledge of the meteorological environment. Radar analysis is augmented by automated detection systems called algorithms. Meteorologists first look at the atmospheric environment as well as changes thereof, and once storms develop, storm motion and interaction with the environment.\n\nAn early step in a storm organizing into a tornado producer is the formation of a weak echo region (WER) with a tilted updraft. This is an area within the thunderstorm where precipitation should be occurring but is \"pulled\" aloft by a very strong updraft. The weak echo region is characterized by weak reflectivity with a sharp gradient to strong reflectivity above it and partially surrounding the sides. The region of the precipitation lofted above the WER is the echo overhang consisting of precipitation particles diverging from the storm's summit that descend as they are carried downwind. Within this area, a bounded weak echo region (BWER) may then form above and enclosing the WER. A BWER is found near the top of the updraft and nearly or completely surrounded by strong reflectivity, and is indicative of a supercell capable of cyclic tornadogenesis. A mesocyclone may descend or a tornado may form in the lower level of the storm simultaneously as the mesocyclone forms.\n\nIn reflectivity (precipitation intensity) data, a tight echo gradient (particularly on the inflow area) and a fan shape generally indicate a supercell. A V-notch or \"flying eagle echo\" tend to be most pronounced with intense classic supercells, the type of supercell that produces most of the strongest, largest, and longest lived tornadoes. This is not to be confused with an inflow notch; which is a lower level indentation in the precipitation where there is little to no reflectivity, indicative of strong, organized inflow and a severe storm that is most likely a supercell. The rear inflow notch (or weak echo channel) occurs to the east or north of a mesocyclone and hook echo. Forward inflow notches also occur, particularly on high-precipitation supercells (HP) and quasi-linear convective systems (QLCS).\n\nIn the United States and a few other countries, Doppler capable weather radar stations are used. These devices are capable of measuring the radial velocity, including radial direction (towards or away from the radar) of the winds in a storm, and so can spot evidence of rotation in storms from more than a hundred miles (160 km) away. A supercell is characterized by a mesocyclone, which is usually first observed in velocity data as a tight, cyclonic structure in the middle levels of the thunderstorm. If it meets certain requirements of strength, duration, and vorticity, it may trip the mesocyclone detection algorithm (MDA). Tornadic signatures are indicated by a cyclonic inbound-outbound velocity couplet, where strong winds flowing in one direction and strong winds flowing in the opposite direction are occurring in very close proximity. The algorithm for this is the tornadic vortex signature (TVS) or the tornado detection algorithm (TDA). TVS is then an extremely strong mesocyclone found at very low level and extending over a deep layer of the thunderstorm, not the actual tornadic circulation. The TVS is, however, indicative of a likely tornado or an incipient tornado. The couplet and TVS typically precede tornado formation by 10–30 minutes but may occur at nearly the same time or precede the tornado by 45 minutes or more. Polarimetric radar can discern meteorological and nonmeteorological and other characteristics of hydrometeors that are helpful to tornado detection and nowcasting. Nonmeteorological reflectors co-located with a couplet, can confirm that a tornado has likely occurred and lofted debris. An area of high reflectivity, or debris ball, may also be visible on the end of the hook. Either the polarimetric data or debris ball are formally known as the tornado debris signature (TDS). The hook echo feature is formed as the RFD occludes precipitation around the mesocyclone and is also indicative of a probable tornado (tornadogenesis usually ensues shortly after the RFD reaches the surface).\n\nAfter the implementation of the WSR-88D network in the U.S., the probability of detection of tornadoes increased substantially, the average lead time rose from four minutes to thirteen minutes, and a 2005 NOAA report estimates that as a result of improved warnings that there are 45 percent fewer fatalities and 40 percent fewer injuries annually. Dual-polarization radar, being implemented to the US NEXRAD network, may provide enhanced warning of tornadoes and severe winds and hail associated with the hook echo due to distinct precipitation drop characteristics. Polarimetric radar boosts precipitation observation and prediction, especially rainfall rates, hail detection, and distinguishing precipitation types. Proposed radar technologies, such as phased array and CASA, would further improve observations and forecasts by increasing the temporal and spatial resolution of scans in the former as well as providing low-level radar data over a wide area in the latter.\n\nIn certain atmospheric environments, wind profilers may also provide detection capabilities for tornadic activity.\n\nHail forms in a very intense updraft in a supercell or a multicellular thunderstorm. As for tornadoes, BWER detection and a tilted updraft are indicative of that updraft but does not lead to predict hail. The presence of a hail spike in the reflectivity pattern is an important clue. It is an area of weak reflectivity extending away from the radar immediately behind a thunderstorm with hail. It is caused by radiation from the radar bouncing from hailstone to hailstone or the ground before being reflected back to the radar. The time delay between the backscattered radiation from the storm and the one with multiple paths causes the reflectivity from the hail to appear to come from a farther range than the actual storm. However, this artefact is visible mostly for extremely large hail.\n\nWhat is needed is a knowledge of the water content in the thunderstorm, the freezing level and the height of the summit of the precipitation. One way of calculating the water content is to transform the reflectivities in rain rate at all levels in the clouds and to sum it up. This is done by an algorithm called \"Vertically integrated liquid\", or VIL. This value represent the total amount of liquid water in the cloud that is available. If the cloud would rain out completely, it would be the amount of rain falling on the ground and one can estimate with VIL the potential for flash flood.\n\nHowever, the reflectivities are greatly enhanced by hail and VIL is greatly overestimating the rain potential in presence of hail. On the other hand, National Weather Service meteorologists have found that the VIL density, that is to say VIL divided by the maximum height of the 18 dBZ in the cloud, is a good indicator of the presence of hail when it reach 3.5. This is a crude yes/no index and other algorithms have been developed involving VIL and the freezing level height. More recently, dual polarization of weather radar have shown promising direct detection of hail.\n\nVIL can be used to estimate the potential for downburst, too. A convective downdraft is linked to three forces in the vertical, namely perturbation pressure gradient force, buoyancy force and precipitation loading. The pressure gradient force was neglected as it has significant effect only on the updraft in supercells. With this assumption and other simplifications (e.g. requiring the environment of the air parcel to be static on the time scale of the downdraft). The resulting momentum equation is integrated over height to yield the kinetic energy of the parcel on descending to the surface and is found to be the negative CAPE of a dry air parcel injected into the storm, plus de motion of the convective cell. S. R. Stewart, from NWS, has published in 1991 an equation relating VIL and the echo tops that give the potential for surface gust using this concept. This is a predictive result that gives a certain lead time. With the Doppler velocity data, the meteorologist can see the downdraft and gust fronts happening, but since this a small scale feature, detection algorithms have been developed to point convergence and divergence areas under a thunderstorm on the radar display.\n\nMost populated areas of the earth are now well covered by weather satellites, which aid in the nowcasting of severe convective and tornadic storms. These images are available in the visible and infrared domains. The infrared (IR: 10-13 µm) images permit estimation of the top height of the clouds, according to the air mass soundings of the day, and the visible (VIS: 0.5-1.1 µm) ones will show the shape of the storms by its brightness and shadow produced. Meteorologists can extract information about the development stage and subsequent traits of thunderstorms by recognizing specific signatures in both domains. Visible imagery permits the most detailed imagery whereas infrared imagery has the advantage of availability at night. Sensors on satellites can also detect emissions from water vapor (WV: 6-7 µm), but mostly in the middle to upper levels of the troposphere, so thunderstorms are only seen after being well developed. It is, however, useful in convective storm prediction, as it illustrates the placement and movement of air masses and of moisture, as well as shortwaves and areas of vorticity and lift.\n\nSevere storms have a very strong updraft. The rising air parcels in that column accelerate and will overshoot the equilibrium level before being pulled back by negative buoyancy. This mean the cloud tops will reach higher levels than the surrounding cloud in the updraft region. This overshooting top will be noticeable by a colder temperature region in the thunderstorm on infrared images. Another signature associated with this situation is the Enhanced-V feature where the cold cloud tops forming at the overshooting top fan out in a V shape as cloud matter is blown downwind at that level. Both features can be seen on visible satellite imagery, during daytime, by the shadows they cast on surrounding clouds.\n\nIn multicellular storms and squall lines, the mid-level jet stream is often intersecting the line and its dry air introduced into the cloud is negatively unstable. This results in a drying of the cloudy air in the region where the jet plunge groundward. On the back edge of the line, this shows as clear notches where one can find stronger downdrafts at the surface. These kind of lines will have a very characteristic undulating pattern caused by the interference of the gusts fronts coming from different parts of the line.\n\nFinally, in any type of thunderstorms, the surface cold pool of air associated the downdraft will stabilize the air and form a cloud free area which will end along the gust front. This mesoscale front, when moving into a warm and unstable air mass, will lift it and cumulus clouds appear on satellite pictures. This line is likely the point of further convection and storms. One can notice it at the leading edge of a squall line, in the southeastern quadrant of a typical supercell (in the northern hemisphere), or different regions around other thunderstorms. They may also be visible as an outflow boundary hours or days after convection and can pinpoint areas of favored thunderstorm development, possible direction of movement, and even likelihood for tornadoes. The speed of forward movement of the outflow boundary or gust front to some degree modulates the likelihood of tornadoes and helps determine whether a storm will be enhanced by its presence or the inflow be choked off thus weakening and possibly killing the storm. Thunderstorms may move along slow moving or stationary outflow boundaries and tornadoes are more likely; whereas fast moving gust fronts in many cases weaken thunderstorms after impact and are less likely to produce tornadoes—although brief tornadoes may occur at the time of impact. Fast moving gust fronts may eventually decelerate and become slow moving or stationary outflow boundaries with the characteristic \"agitated area\" of cumulus fields previously mentioned.\n\nUsually in conjunction with data sources such as weather radar and satellites, lightning detection systems are sometimes utilized to pinpoint where thunderstorms are occurring (and to identify lightning hazard). Currently, most lightning data provided in real-time is from terrestrial sources, specifically, networks of ground-based sensors, although airborne sensors are also in operation. Most of these only provide latitude & longitude, time, and polarity of cloud-to-ground strikes within a limited range. Increasing in sophistication and availability, and affording data for a very wide area, are satellite-based lightning detectors which initially included optical sensors indicating flash rates and horizontal location but now radio frequency receivers that can identify intra-cloud flashes with the addition of altitude, as well.\n\nLightning data is useful in suggesting intensity and organization of convective cells as well trends in thunderstorm activity (particularly growth, and to a lesser degree, decay). It is also useful in the early stages of thunderstorm development. This was especially true when visible and infrared satellite data was delayed, but continues to be useful in detecting thunderstorms in stages of development before there is a substantial radar signature or for areas where radar data is lacking. Coming advances in research and observations should improve forecasts of severe weather and increase warning time.\n\nPersonal lightning detection systems are also available, which may provide strike time, azimuth, and distance. In addition, lightning prediction systems are available and used mostly by parks and other outdoor recreational facilities, or meteorologists contracted to provide weather information for them.\n\n\n\n"}
{"id": "1138578", "url": "https://en.wikipedia.org/wiki?curid=1138578", "title": "Crystal field theory", "text": "Crystal field theory\n\nCrystal Field Theory (CFT) is a model that describes the breaking of degeneracies of electron orbital states, usually d or f orbitals, due to a static electric field produced by a surrounding charge distribution (anion neighbors). This theory has been used to describe various spectroscopies of transition metal coordination complexes, in particular optical spectra (colors). CFT successfully accounts for some magnetic properties, colors, hydration enthalpies, and spinel structures of transition metal complexes, but it does not attempt to describe bonding. CFT was developed by physicists Hans Bethe and John Hasbrouck van Vleck in the 1930s. CFT was subsequently combined with molecular orbital theory to form the more realistic and complex ligand field theory (LFT), which delivers insight into the process of chemical bonding in transition metal complexes.\n\nAccording to crystal field theory, the interaction between a transition metal and ligands arises from the attraction between the positively charged metal cation and negative charge on the non-bonding electrons of the ligand. The theory is developed by considering energy changes of the five degenerate \"d\"-orbitals upon being surrounded by an array of point charges consisting of the ligands. As a ligand approaches the metal ion, the electrons from the ligand will be closer to some of the \"d\"-orbitals and farther away from others, causing a loss of degeneracy. The electrons in the \"d\"-orbitals and those in the ligand repel each other due to repulsion between like charges. Thus the d-electrons closer to the ligands will have a higher energy than those further away which results in the \"d\"-orbitals splitting in energy. This splitting is affected by the following factors:\n\n\nThe most common type of complex is octahedral; here six ligands form an octahedron around the metal ion. In octahedral symmetry the \"d\"-orbitals split into two sets with an energy difference, Δ (the crystal-field splitting parameter) where the \"d\", \"d\" and \"d\" orbitals will be lower in energy than the \"d\" and \"d\", which will have higher energy, because the former group is farther from the ligands than the latter and therefore experiences less repulsion. The three lower-energy orbitals are collectively referred to as t, and the two higher-energy orbitals as e. (These labels are based on the theory of molecular symmetry). Typical orbital energy diagrams are given below in the section High-spin and low-spin.\n\nTetrahedral complexes are the second most common type; here four ligands form a tetrahedron around the metal ion. In a tetrahedral crystal field splitting, the \"d\"-orbitals again split into two groups, with an energy difference of Δ. The lower energy orbitals will be \"d\" and \"d\", and the higher energy orbitals will be \"d\", \"d\" and \"d\" - opposite to the octahedral case. Furthermore, since the ligand electrons in tetrahedral symmetry are not oriented directly towards the \"d\"-orbitals, the energy splitting will be lower than in the octahedral case. Square planar and other complex geometries can also be described by CFT.\n\nThe size of the gap Δ between the two or more sets of orbitals depends on several factors, including the ligands and geometry of the complex. Some ligands always produce a small value of Δ, while others always give a large splitting. The reasons behind this can be explained by ligand field theory. The spectrochemical series is an empirically-derived list of ligands ordered by the size of the splitting Δ that they produce (small Δ to large Δ; see also this table):\n\nI < Br < S < SCN (S–bonded) < Cl < NO < N < F < OH < CO < HO < NCS (N–bonded) < CHCN < py < NH < en < 2,2'-bipyridine < phen < NO < PPh < CN < CO.\n\nIt is useful to note that the ligands producing the most splitting are those that can engage in metal to ligand back-bonding.\n\nThe oxidation state of the metal also contributes to the size of Δ between the high and low energy levels. As the oxidation state increases for a given metal, the magnitude of Δ increases. A V complex will have a larger Δ than a V complex for a given set of ligands, as the difference in charge density allows the ligands to be closer to a V ion than to a V ion. The smaller distance between the ligand and the metal ion results in a larger Δ, because the ligand and metal electrons are closer together and therefore repel more.\n\nLigands which cause a large splitting Δ of the \"d\"-orbitals are referred to as strong-field ligands, such as CN and CO from the spectrochemical series. In complexes with these ligands, it is unfavourable to put electrons into the high energy orbitals. Therefore, the lower energy orbitals are completely filled before population of the upper sets starts according to the Aufbau principle. Complexes such as this are called \"low spin\". For example, NO is a strong-field ligand and produces a large Δ. The octahedral ion [Fe(NO)], which has 5 \"d\"-electrons, would have the octahedral splitting diagram shown at right with all five electrons in the \"t\" level. The low spin state therefore does not follow Hund's rule.\n\nConversely, ligands (like I and Br) which cause a small splitting Δ of the \"d\"-orbitals are referred to as weak-field ligands. In this case, it is easier to put electrons into the higher energy set of orbitals than it is to put two into the same low-energy orbital, because two electrons in the same orbital repel each other. So, one electron is put into each of the five \"d\"-orbitals before any pairing occurs in accord with Hund's rule and \"high spin\" complexes are formed. For example, Br is a weak-field ligand and produces a small Δ. So, the ion [FeBr], again with five \"d\"-electrons, would have an octahedral splitting diagram where all five orbitals are singly occupied.\n\nIn order for low spin splitting to occur, the energy cost of placing an electron into an already singly occupied orbital must be less than the cost of placing the additional electron into an e orbital at an energy cost of Δ. As noted above, e refers to the\n\"d\" and \"d\" which are higher in energy than the t in octahedral complexes. If the energy required to pair two electrons is greater than Δ, the energy cost of placing an electron in an e, high spin splitting occurs.\n\nThe crystal field splitting energy for tetrahedral metal complexes (four ligands) is referred to as Δ, and is roughly equal to 4/9Δ (for the same metal and same ligands). Therefore, the energy required to pair two electrons is typically higher than the energy required for placing electrons in the higher energy orbitals. Thus, tetrahedral complexes are usually high-spin.\n\nThe use of these splitting diagrams can aid in the prediction of magnetic properties of coordination compounds. A compound that has unpaired electrons in its splitting diagram will be paramagnetic and will be attracted by magnetic fields, while a compound that lacks unpaired electrons in its splitting diagram will be diamagnetic and will be weakly repelled by a magnetic field.\n\nThe crystal field stabilization energy (CFSE) is the stability that results from placing a transition metal ion in the crystal field generated by a set of ligands. It arises due to the fact that when the \"d\"-orbitals are split in a ligand field (as described above), some of them become lower in energy than before with respect to a spherical field known as the barycenter in which all five \"d\"-orbitals are degenerate. For example, in an octahedral case, the \"t\" set becomes lower in energy than the orbitals in the barycenter. As a result of this, if there are any electrons occupying these orbitals, the metal ion is more stable in the ligand field relative to the barycenter by an amount known as the CFSE. Conversely, the \"e\" orbitals (in the octahedral case) are higher in energy than in the barycenter, so putting electrons in these reduces the amount of CFSE.\n\nIf the splitting of the \"d\"-orbitals in an octahedral field is Δ, the three \"t\" orbitals are stabilized relative to the barycenter by / Δ, and the \"e\" orbitals are destabilized by / Δ. As examples, consider the two \"d\" configurations shown further up the page. The low-spin (top) example has five electrons in the \"t\" orbitals, so the total CFSE is 5 x / Δ = 2Δ. In the high-spin (lower) example, the CFSE is (3 x / Δ) - (2 x / Δ) = 0 - in this case, the stabilization generated by the electrons in the lower orbitals is canceled out by the destabilizing effect of the electrons in the upper orbitals.\n\nThe optical properties (details of absorption and emission spectra) of many coordination complexes can be explained by Crystal Field Theory. Often, however, the deeper colors of metal complexes arise from more intense charge-transfer excitations.\n\n\n\n"}
{"id": "39636980", "url": "https://en.wikipedia.org/wiki?curid=39636980", "title": "Cutthroat flume", "text": "Cutthroat flume\n\nThe Cutthroat flume is a class of flow measurement flume developed during 1966/1967 that is used to measure the flow of surface waters, sewage flows, and industrial discharges. Like other flumes, the Cutthroat flume is a fixed hydraulic structure. Using vertical sidewalls throughout, the flume accelerates flow through a contraction of sidewalls until the flow reaches the \"throat\" of the flume, where the flow is then expanded. Unlike the Parshall flume, the Cutthroat flume lacks a parallel-walled throat section and maintains a flat floor throughout the flume.\n\nThe design of the Cutthroat flume is standardized but not covered by a national or international standard (unlike the Parshall flume). The flumes are not patented and the discharge tables are not copyright protected.\n\nA total of 16 standard sizes of Cutthroat flumes have been developed, covering flow ranges from 0.3536 gpm [0.0223 l/s] to 54,801 gpm [3,458 l/s].\n\nUnder free-flow conditions the depth of water at specified location upstream of the flume throat can be converted to a rate of flow.\n\nThe free-flow discharge can be summarized as \n\nWhere\n\nBoth “K” and “n” vary by flume length alone.\n\nTable 1\n\nSubmergence transitions for Cutthroat flumes varies by flume length:\n\nThe submergence transition values for Cutthroat flumes are generally better than those for similarly sized Parshall flumes – an advantage in flat gradient channels where downstream hydraulics may increase the submergence ratio in the flume.\n\nUnlike the Parshall flume, the secondary point of measurement, Hb, in the Cutthroat flume is located away from the throat section, making the determination of the level relatively easy.\n\nThe Cutthroat flume was developed during the 1966-67s at the Utah Water Research Laboratory, Utah State, Logan, Utah by Skogerboe,Hyatt, Anderson, and Eggleston. The result of these efforts was a flume that is simple in form and construction and that is well suited for use in flat gradient (low slope) applications.\n\nCutthroat flumes lack a parallel-wall throat section (hence the name) and has a flat-bottom to allow for installation in flat gradient channels. From the top, the Cutthroat flume has an hourglass look similar to the Parshall flume, with which it is sometimes confused.\n\nThe walls of a Cutthroat flume are vertical, like Parshall and HS / H / HL flumes. The approach section walls contract uniformly at a 3:1 ratio, while the discharge section walls expand at a 6:1 ratio. The point at with the approach and discharge section walls meet is termed the “throat” of the Cutthroat flume.\n\nThe primary point of measurement, Ha, occurs at a point upstream of the flume throat and can be determined by the equation\n\nWhere \"L\" is flume length.\n\nThe secondary point of measurement, Hb, occurs at a point downstream of the flume throat and can be determined by the equation\n\nWhere \"L\" is flume length.\n\n\n\nFour standard lengths of the Cutthroat flume have been developed, with four throat widths for each length.\n\nBelow are the standard flume lengths with their respective standard throat widths.\n\n\nFor a given length, Cutthroat flumes of intermediate throat widths can be developed without the need for laboratory testing.\n\nWhere\n\nAs with the Parshall flume, the initial applications for Cutthroat flumes were envisioned to be measuring flows in irrigation channels and other surface waters.\n\nAgain, like the Parshall flume, the Cutthroat flume has proven to be applicable to a range of open channel flows including:\n\n\n\n"}
{"id": "15263017", "url": "https://en.wikipedia.org/wiki?curid=15263017", "title": "Deben (unit)", "text": "Deben (unit)\n\nThe deben was an ancient Egyptian weight unit.\n\nStone weights from the Old Kingdom have been found, weighing about 13.6 grams. Similar weights from the Middle Kingdom were discovered at Lisht. From the Middle Kingdom date also \"deben\" weight units used for metals, referred to as copper deben and gold deben, the former being about twice as heavy (c. 23.7 grams) as the latter.\n\nFrom the New Kingdom one \"deben\" was equal to about 91 grams. It was divided into ten \"kidet\" (alt. \"kit\", \"kite\" or \"qedet\"), or into what is referred to by Egyptologists as 'pieces', one twelfth of a \"deben\" weighing 7.6 grams. It was frequently used to denote value of goods, by comparing their worth to a weight of metal, generally silver or copper.\n\nIt has been speculated that pieces of metal weighing a deben were kept in boxes, taken to markets, and were used as a means of exchange. Archaeologists have been unable to find any such standardized pieces of precious metal. On the other hand, it is documented that \"debens\" served to compare values. In the 19th Dynasty, a slave girl priced four deben and one kite of silver was paid for with various goods: 6 bronze vessels, 10 \"deben\" of copper, 15 linen garments, a shroud, a blanket, and a pot of honey.\n\nDebens appeared in the computer game \"Pharaoh\" as its currency (in the form of gold).\n\n"}
{"id": "29526900", "url": "https://en.wikipedia.org/wiki?curid=29526900", "title": "Dolna Odra Power Station", "text": "Dolna Odra Power Station\n\nDolna Odra Power Station is a coal-fired power station at Nowe Czarnowo near Gryfino in West Pomeranian Voivodeship, Poland. It consists of 8 units, 2 with 220 MW and 6 with 232 MW, which went in service between 1974 and 5 to 8 were awarded in 1977. Since 1993, Dolna Odra has gone through modernization process. In 2000, units 7 and 8 were equipped by new controls systems from Pavilion Technologies and ABB. Starting from October 2002, the flue gas from the units 5–8 is cleaned at the flue-gas desulfurization plant, built by Lurgi Lentjes. PGE also plans to build two new natural gas-fired units with capacity of 432 MW each.\n\nDolna Odra Power Station has three flue gas stacks: one with a height of , one with a height of , and one with a height of . A second tall flue gas stack was demolished in a non-explosive way after construction of a smoke cleaning facility, which uses the mentioned tall stack.\n\nOn 24 January 2010, an explosion of coal dust killed one employee and injured three. Part of a plant's coal supply system \nhas been damaged.\n\n"}
{"id": "52875549", "url": "https://en.wikipedia.org/wiki?curid=52875549", "title": "Drumlohan Souterrain and Ogham Stones", "text": "Drumlohan Souterrain and Ogham Stones\n\nDrumlohan souterrain and ogham stones, known locally as the Ogham Cave, is a souterrain with ogham stones forming a National Monument located in County Waterford, Ireland.\n\nDrumlohan souterrain and ogham stones are located in farmland 4 km (2½ mi) east of Lemybrien.\n\nThe ogham stones were carved between 400 and 700 AD.\n\nThe souterrain is believed to have been constructed around the 9th century AD and is aligned WSW, facing the setting sun. Souterrains were storage sites and places of refuge.\n\nIn July/August 1867 a local farmer rediscovered the souterrain and ogham stones. In 1936 part of the souterrain was dismantled and some of the ogham stones re-erected above ground.\n\nThis souterrain gallery is about 4.9 m (16 ft) long and 1.3 m (4 ft) wide, with a roof height of up to 1.2 m (4 ft). It is constructed of orthostats roofed with lintels, and ten ogham stones were used as lintels and sidestones (some of them being installed upside-down). One of the roofstones bears cup marks.\n\nThe stones (CIIC 272–281) vary in size. All are greenschist, except for two of slate and one of conglomerate. The inscriptions are:\n"}
{"id": "6584577", "url": "https://en.wikipedia.org/wiki?curid=6584577", "title": "Ebonol (material)", "text": "Ebonol (material)\n\nEbonol is a synthetic material whose name derives from its similarity in appearance, hardness, and stability to ebony wood. Ebonol is used as a substitute for ebony in the construction of stringed and woodwind instruments (specifically clarinets). The material is particularly well suited for the fingerboards of fretless bass.\n\nEbonol is technically known as \"XXX Paper Phenolic\" and is a high-pressure laminate made from layers of black paper and phenolic resin. \n"}
{"id": "3025876", "url": "https://en.wikipedia.org/wiki?curid=3025876", "title": "Electric bicycle", "text": "Electric bicycle\n\nAn electric bicycle, also known as an e-bike, powerbike or booster bike, is a bicycle with an integrated electric motor which can be used for propulsion. Many kinds of e-bikes are available worldwide, from e-bikes that only have a small motor to assist the rider's pedal-power (i.e., pedelecs) to somewhat more powerful e-bikes which tend closer to moped-style functionality: all, however, retain the ability to be pedalled by the rider and are therefore not electric motorcycles. \n\nE-bikes use rechargeable batteries and the lighter ones can travel up to , depending on local laws, while the more high-powered varieties can often do in excess of . In some markets, such as Germany , they are gaining in popularity and taking some market share away from conventional bicycles, while in others, such as China , they are replacing fossil fuel-powered mopeds and small motorcycles.\n\nDepending on local laws, many e-bikes (e.g., \"pedelecs\") are legally classified as bicycles rather than mopeds or motorcycles. This exempts them from the more stringent laws regarding the certification and operation of more powerful two-wheelers which are often classed as electric motorcycles. E-bikes can also be defined separately and treated under distinct Electric bicycle laws.\n\nE-bikes are the electric motor-powered versions of motorized bicycles, which have been in use since the late 19th century. Some bicycle-sharing systems use them.\n\nIn the 1890s, electric bicycles were documented within various U.S. patents. For example, on 31 December 1895, Ogden Bolton Jr. was granted for a battery-powered bicycle with \"6-pole brush-and-commutator direct current (DC) hub motor mounted in the rear wheel\". There were no gears and the motor could draw up to 100 amperes (A) from a 10-volt battery.\n\nTwo years later, in 1897, Hosea W. Libbey of Boston invented an electric bicycle () that was propelled by a \"double electric motor\". The motor was designed within the hub of the crankset axle. This model was later re-invented and imitated in the late 1990s by Giant Lafree e-bikes.\n\nBy 1898 a rear-wheel drive electric bicycle, which used a driving belt along the outside edge of the wheel, was patented by Mathew J. Steffens. Also, the 1899 by John Schnepf depicted a rear-wheel friction “roller-wheel” style drive electric bicycle. Schnepf's invention was later re-examined and expanded in 1969 by G.A. Wood Jr. with his . Wood’s device used 4 fractional horsepower motors; connected through a series of gears.\n\nTorque sensors and power controls were developed in the late 1990s. For example, Takada Yutky of Japan filed a patent in 1997 for such a device. In 1992 Vector Services Limited offered and sold an e-bike dubbed Zike. The bicycle included NiCd batteries that were built into a frame member and included an 850 g permanent-magnet motor. Despite the Zike, in 1992 hardly any commercial e-bikes were available.\n\nProduction grew from 1993 to 2004 by an estimated 35%. By contrast, according to Gardner, in 1995 regular bicycle production decreased from its peak 107 million units.\n\nSome of the less expensive e-bikes used bulky lead acid batteries, whereas newer models generally used NiMH, NiCd, and/or Li-ion batteries, which offered lighter, denser capacity batteries. Performance varies; however, in general there is an increase in range and speed with the latter battery types.\n\nBy 2001 the terms e-bike, power bike, \"pedelec\", pedal-assisted, and power-assisted bicycle were commonly used to refer to e-bikes. The terms \"electric motorbike\" or \"e-motorbike\" refer to more powerful models that attain up to .\n\nIn a parallel hybrid motorized bicycle, such as the aforementioned 1897 invention by Hosea W. Libbey, human and motor inputs are mechanically coupled either in the bottom bracket, the rear wheel, or the front wheel, whereas in a (mechanical) series hybrid cycle, the human and motor inputs are coupled through differential gearing. In an (electronic) series hybrid cycle, human power is converted into electricity and is fed directly into the motor and mostly additional electricity is supplied from a battery.\n\nBy 2007 e-bikes were thought to make up 10 to 20 percent of all two-wheeled vehicles on the streets of many major Chinese cities. A typical unit requires 8 hours to charge the battery, which provides the range of , at the speed of around 20 km/h.\n\nAs of 2017, electric bicycles sales in the United States have slowed. This is due primarily to lower gas prices.\n\nE-bikes are classed according to the power that their electric motor can deliver and the control system, i.e., when and how the power from the motor is applied. Also the classification of e-bikes is complicated as much of the definition is due to legal reasons of what constitutes a bicycle and what constitutes a moped or motorcycle. As such, the classification of these e-bikes varies greatly across countries and local jurisdictions.\n\nDespite these legal complications, the classification of e-bikes is mainly decided by whether the e-bike's motor assists the rider using a \"pedal-assist\" system or by a \"power-on-demand\" one. Definitions of these are as follows:\n\nTherefore, very broadly, e-bikes can be classed as:\n\nE-bikes with pedal-assist only are usually called \"pedelecs\" but can be broadly classified into pedelecs proper and the more powerful S-Pedelecs.\n\nThe term \"pedelec\" (from pedal electric cycle) refers to a \"pedal-assist\" e-bike with a relatively low-powered electric motor and a decent but not excessive top speed. Pedelecs are legally classed as bicycles rather than low-powered motorcycles or mopeds.\n\nThe most influential definition of pedelecs and which are not comes from the EU. EU directive (EN15194 standard) for motor vehicles considers a bicycle to be a pedelec if:\nAn e-bike conforming to these conditions is considered to be a pedelec in the EU and is legally classed as a bicycle. The EN15194 standard is valid across the whole of the EU and has also been adopted by some non-EU European nations and also some non-European jurisdictions (such as the state of Victoria in Australia).\n\nPedelecs are much like conventional bicycles in use and function — the electric motor only provides assistance, for example, when the rider is climbing or struggling against a headwind. Pedelecs are therefore especially useful for people in hilly areas where riding a bike would prove too strenuous for many to consider taking up cycling as a daily means of transport. They are also useful for riders who more generally need some assistance, e.g. for people with heart, leg muscle or knee joint issues.\n\nMore powerful pedelecs which are not legally classed as bicycles are dubbed S-Pedelecs (short for \"Schnell-Pedelecs\", i.e. Speedy-Pedelecs) in Germany. These have a motor more powerful than 250 watts and less limited, or unlimited, pedal-assist, i.e. the motor does not stop assisting the rider once 25 km/h has been reached. S-Pedelec class e-bikes are therefore usually classified as mopeds or motorcycles rather than as bicycles and therefore may (depending on the jurisdiction) need to be registered and insured, the rider may need some sort of driver's license (either car or motorcycle) and motorcycle helmets may have to be worn. In the United States, many states have adopted S-Pedelecs into the Class 3 category. Class 3 ebikes are limited to <=750 watts of power and 28 mph.\n\nSome e-bikes combine both pedal-assist sensors as well as a throttle. An example of these is the \"eZee Torq\" and Adventure 24+ by BMEBIKES. The motor on this type of e-bike is activated by pushing the throttle or by pedaling.\n\nSome e-bikes have an electric motor that operates on a power-on-demand basis only. In this case, the electric motor is engaged and operated manually using a throttle, which is usually on the handgrip just like the ones on a motorbike or scooter. These sorts of e-bikes often, but not always, have more powerful motors than pedelecs do.\n\nWith \"power-on-demand only\" e-bikes the rider can:\n\nSome power-on-demand only e-bikes can hardly be confused with, let alone categorised as, bicycles. For example, the \"Noped\" is a term used by the Ministry of Transportation of Ontario for e-bikes which do not have pedals or in which the pedals have been removed from their motorised bicycle. These are better categorised as electric mopeds or electric motorcycles.\n\n(*) Allowed on bike paths when electric systems are turned off\n(**) E-bikes are illegal in this region\n(***) Some regions have special regulations, see corresponding entry under Electric bicycle laws.\n\nE-bike usage worldwide has experienced rapid growth since 1998. In 2016 there were 210 million electric bikes worldwide used daily. It is estimated that there were roughly 120 million e-bikes in China in early 2010, and sales are expanding rapidly in India, the United States of America, Germany, the Netherlands, and Switzerland. A total of 700,000 e-bikes were sold in Europe in 2010, up from 200,000 in 2007 and 500,000 units in 2009.\n\nToday, China is the world's leading producer of e-bikes. According to the data of the China Bicycle Association, a government-chartered industry group, in 2004 China's manufacturers sold 7.5 million e-bikes nationwide, which was almost twice the year 2003 sales; domestic sales reached 10 million in 2005, and 16 to 18 million in 2006.\n\nThe two most common types of hub motors used in electric bicycles are brushed and brushless. Many configurations are available, varying in cost and complexity; direct-drive and geared motor units are both used. An electric power-assist system may be added to almost any pedal cycle using chain drive, belt drive, hub motors or friction drive. BLDC hub motors are a common modern design. The motor is built into the wheel hub itself, and the stator fixed solidly to the axle, and the magnets attached to and rotating with the wheel. The bicycle wheel hub is the motor. The power levels of motors used are influenced by available legal categories and are often, but not always limited to under 750 watts.\n\nAnother type of electric assist motor, often referred to as the mid-drive system, is increasing in popularity. With this system, the electric motor is not built into the wheel but is usually mounted near (often under) the bottom bracket shell. In more typical configurations, a cog or wheel on the motor drives a belt or chain that engages with a pulley or sprocket fixed to one of the arms of the bicycle's crankset. Thus the propulsion is provided at the pedals rather than at the wheel, being eventually applied to the wheel via the bicycle's standard drive train.\n\nBecause the power is applied through the chain and sprocket, power is typically limited to around 250–500 watts to protect against fast wear on the drivetrain. An electric mid-drive combined with an internal gear hub at the back hub may require care due to the lack of a clutch mechanism to soften the shock to the gears at the moment of re-engagement. A continuously variable transmission or a fully automatic internal gear hub may reduce the shocks due to the viscosity of oils used for liquid coupling instead of the mechanical couplings of the conventional internal gear hubs.\n\nE-bikes use rechargeable batteries, electric motors and some form of control. Battery systems in use include sealed lead-acid (SLA), nickel-cadmium (NiCad), nickel-metal hydride (NiMH) or lithium-ion polymer (Li-ion). Batteries vary according to the voltage, total charge capacity (amp hours), weight, the number of charging cycles before performance degrades, and ability to handle over-voltage charging conditions. The energy costs of operating e-bikes are small, but there can be considerable battery replacement costs. The lifespan of a battery pack varies depending on the type of usage. Shallow discharge/recharge cycles will help extend the overall battery life.\n\nRange is a key consideration with e-bikes, and is affected by factors such as motor efficiency, battery capacity, efficiency of the driving electronics, aerodynamics, hills and weight of the bike and rider. Some manufacturers, such as the Canadian BionX or American Vintage Electric Bikes, have the option of using regenerative braking, the motor acts as a generator to slow the bike down prior to the brake pads engaging. This is useful for extending the range and the life of brake pads and wheel rims. There are also experiments using fuel cells. e.g. the PHB.\nSome experiments have also been undertaken with super capacitors to supplement or replace batteries for cars and some SUVS.\nE-bikes developed in Switzerland in the late 1980s for the Tour de Sol solar vehicle race came with solar charging stations but these were later fixed on roofs and connected so as to feed into the electric mains. The bicycles were then charged from the mains, as is common today. While ebike batteries were produced mainly by bigger companies in past, many small to medium companies have started using innovative new methods for creating more durable batteries. State of the art, custom built automated precision CNC spot welding machines created 18650 battery packs are commonly used among Do-it-yourself ebike makers.\n\nThere are two distinct types of controllers designed to match either a brushed motor or brushless motor. Brushless motors are becoming more common as the cost of controllers continues to decrease. (See the page on DC motors which covers the differences between these two types.)\n\n\"Controllers for brushless motors\": E-bikes require high initial torque and therefore models that use brushless motors typically have Hall sensor commutation for speed and angle measurement. An electronic controller provides assistance as a function of the sensor inputs, the vehicle speed and the required force. The controllers generally allow input by means of potentiometer or Hall Effect twist grip (or thumb-operated lever throttle), closed-loop speed control for precise speed regulation, protection logic for over-voltage, over-current and thermal protection. Bikes with a pedal assist function typically have a disc on the crank shaft featuring a ring of magnets coupled with a Hall sensor giving rise to a series of pulses, the frequency of which is proportional to pedaling speed. The controller uses pulse width modulation to regulate the power to the motor. Sometimes support is provided for regenerative braking but infrequent braking and the low mass of bicycles limits recovered energy. An implementation is described in an application note for a 200 W, 24 V Brushless DC (BLDC) motor.\n\n\"Controllers for brushed motors\": Brushed motors are also used in e-bikes but are becoming less common due to their intrinsic lower efficiency. Controllers for brushed motors however are much simpler and cheaper due to the fact they don't require hall sensor feedback and are typically designed to be open-loop controllers. Some controllers can handle multiple voltages.\n\nNot all e-bikes take the form of conventional push-bikes with an incorporated motor, such as the Cytronex bicycles which use a small battery disguised as a water bottle.\nSome are designed to take the appearance of low capacity motorcycles, but smaller in size and consisting of an electric motor rather than a petrol engine. For example, the \"Sakura\" e-bike incorporates a 200 W motor found on standard e-bikes, but also includes plastic cladding, front and rear lights, and a speedometer. It is styled as a modern moped, and is often mistaken for one.\n\nConverting a non-electric bicycle to its electric equivalent can be complicated but numerous 'replace a wheel' solutions are now available on the market.\n\nAn Electric Pusher Trailer is an e-bike design which incorporates a motor and battery into a trailer that pushes any bicycle. One such trailer is the two-wheeled Ridekick.\n\nOther, rarer designs include that of a 'chopper' styled e-bike, which are designed as more of a 'fun' or 'novelty' e-bike than as a purposeful mobility aid or mode of transport.\n\nElectric cargo bikes allow the rider to carry large, heavy items which would be difficult to transport without electric power supplementing the human power input.\n\nVarious designs (including those mentioned above) are designed to fit inside most area laws, and the ones that contain pedals can be used on roads in the United Kingdom, among other countries.\n\nFolding e-bikes are also available.\n\nElectric self-balancing unicycles do not conform to e-bike legislation in most countries and therefore cannot be used on the road, but can be utilized in the sidewalk. They are the cheapest electric cycles and used by the last mile commuters, for urban use and to be combined with public transport, including buses.\n\nElectric trikes have also been produced that conform to the e-bike legislation. These have the benefit of additional low speed stability and are often favored by people with disabilities. Cargo carrying tricycles are also gaining acceptance, with a small but growing number of couriers using them for package deliveries in city centres. Latest designs of these trikes resemble a cross-between a pedal cycle and a small van.\n\nE-bikes can be a useful part of cardiac rehabilitation programmes, since health professionals will often recommend a stationary bike be used in the early stages of these. Exercise-based cardiac rehabilitation programmes can reduce deaths in people with coronary heart disease by around 27%; and a patient may feel safer progressing from stationary bikes to e-bikes. They require less cardiac exertion for those who have experienced heart problems.\n\nE-bikes can also provide a source of exercise for individuals who have trouble exercising for an extended time (due to injury or excessive weight, for example) as the bike can allow the rider to take short breaks from pedaling and also provide confidence to the rider that they'll be able to complete the selected path without becoming too fatigued or without having forced their knee joints too hard (people who need to use their knee joints without wearing them out unnecessarily may in some electric bikes adjust the level of motor assistance according to the terrain). A University of Tennessee study provides evidence that energy expenditure (EE) and oxygen consumption (VO2) for e-bikes are 24% lower than that for conventional bicycles, and 64% lower than for walking. Further, the study notes that the difference between e-bikes and bicycles are most pronounced on the uphill segments. Reaching VO2 Max, can really help your body as a whole. Professor Janet Lord of Birmingham University in the UK published a study that looked at older cyclists, ““The study looked at muscle mass, blood cholesterol, their VO2 Max, lung function, and in many of those measures we found they didn’t age! No loss of muscle, their bones were a little thin (but nothing like the general population), their blood pressure didn’t go up.\n\nThere are individuals who claim to have lost considerable amounts of weight by using an electric bike. A recent prospective cohort study however found that people using e-bikes have a higher BMI. By making the biking terrain less of an issue, people who wouldn't otherwise consider biking can use the electric assistance when needed and otherwise pedal as they are able. This means people of lower fitness levels or who haven't cycled in many years can start enjoying the many health benefits E-bikes have to offer. \n\nE-bikes are zero-emissions vehicles, as they emit no combustion by-products. However, the environmental effects of electricity generation and power distribution and of manufacturing and disposing of (limited life) high storage density batteries must be taken into account. Even with these issues considered, e-bikes are claimed to have a significantly lower environmental impact than conventional automobiles, and are generally seen as environmentally desirable in an urban environment.\n\nThe environmental effects involved in recharging the batteries can of course be reduced. The small size of the battery pack on an e-bike, relative to the larger pack used in an electric car, makes them very good candidates for charging via solar power or other renewable energy resources. Sanyo capitalized on this benefit when it set up \"solar parking lots\", in which e-bike riders can charge their vehicles while parked under photovoltaic panels.\n\nThe environmental credentials of e-bikes, and electric / human powered hybrids generally, have led some municipal authorities to use them, such as Little Rock, Arkansas with their \"Wavecrest\" electric power-assisted bicycles or Cloverdale, California police with \"Zap\" e-bikes. China’s e-bike manufacturers, such as Xinri, are now partnering with universities in a bid to improve their technology in line with international environmental standards, backed by the Chinese government who is keen to improve the export potential of the Chinese manufactured e-bikes.\n\nBoth land management regulators and mountain bike trail access advocates have argued for bans of electric bicycles on outdoor trails that are accessible to mountain bikes, citing potential safety hazards as well as the potential for electric bikes to damage trails. A study conducted by the International Mountain Bicycling Association, however, found that the physical impacts of low-powered pedal-assist electric mountain bikes may be similar to traditional mountain bikes.\n\nA recent study on the environment impact of e-bikes vs other forms of transportation found that e-bikes are:\n\nOne major concern is disposal of used lead batteries, which can cause environmental contamination if not recycled.\n\nThere are strict shipping regulations for lithium-ion batteries, due to safety concerns. In this regard, lithium iron phosphate batteries are safer than lithium cobalt oxide batteries.\n\nChina's experience, as the leading e-bike world market, has raised concerns about road traffic safety and several cities have considered banning them from bicycle lanes. As the number of e-bikes increased and more powerful motors are used, capable of reaching up to , the number of traffic accidents have risen significantly in China. E-bike riders are more likely than a car driver to be killed or injured in a collision, and because e-bikers use conventional bicycle lanes they mix with slower-moving bicycles and pedestrians, increasing the risk of traffic collisions.\n\nChina has experienced an explosive growth of sales of non-assisted e-bikes including scooter type, with annual sales jumping from 56,000 units in 1998 to over 21 million in 2008, and reaching an estimated fleet of 120 million e-bikes in early 2010. This boom was triggered by Chinese local governments' efforts to restrict motorcycles in city centers to avoid traffic disruption and accidents. By late 2009 motorcycles are banned or restricted in over ninety major Chinese cities. Users began replacing traditional bicycles and motorcycles and e-bike became an alternative to commuting by car. Nevertheless, road safety concerns continue as around 2,500 e-bike related deaths were registered in 2007. By late 2009 ten cities had also banned or imposed restrictions on e-bikes on the same grounds as motorcycles. Among these cities were Guangzhou, Shenzhen, Changsha, Foshan, Changzhou, and Dongguang.\n\nChina is the world's leading manufacturer of e-bikes, with 22.2 million units produced in 2009. Some of the biggest manufacturers of E-bikes in the world are BYD and Geoby. Production is concentrated in five regions, Tianjin, Zhejiang, Jiangsu, Shandong, and Shanghai. China exported 370,000 e-bikes in 2009.\n\nThe first pedal-assisted bicycles appeared in India in 1993. In 2008, the sales of e-bike sales exceeded moped sales. In recent years, 2-passenger and even 3-passenger (two adults and a child) e-bikes were introduced in India.\n\nThe Netherlands has a fleet of 18 million bicycles. E-bikes have reached a market share of 10% by 2009, as e-bikes sales quadrupled from 40,000 units to 153,000 between 2006 and 2009, and the electric-powered models represented 25% of the total bicycle sales revenue in that year. By early 2010 one in every eight bicycles sold in the country is electric-powered despite the fact that on average an e-bike is three times more expensive than a regular bicycle.\n\nA 2008 market survey showed that the average distance traveled in the Netherlands by commuters on a standard bicycle is while with an e-bike this distance increases to . This survey also showed that e-bike ownership is particularly popular among people aged 65 and over, but limited among commuters. The e-bike is used in particular for recreational bicycle trips, shopping and errands.\n\nIn 2009 the U.S. had an estimated fleet of 200,000 e-bikes. In 2012 they were increasingly favored in New York as food delivery vehicles.\n\nIn 2012, two e-bike advocates completed the first transcontinental e-bike tour from New York to San Francisco to advocate for e-bikes in major cities across the U.S.\nPedego Electric Bikes is the best selling brand in the U.S.\nMany e-bikes in the United States are standard bicycles converted using a kit. In general, the kits include the motor (the majority of which are hub motors built into the front or rear wheel), a speed controller, throttle (usually twist-grip or thumb throttle), necessary wiring and connectors, and a battery. Several U.S. companies offer conversion kits which also offer advanced lithium battery packs. Major manufacturers also offer complete e-bikes. Trek offers a line of e-bikes using the Bionx system in which the rider programs the controller to determine how much effort the motor will give in response to rider effort, from 25% up to 200% of the rider's power. This system ensures a minimum level of rider participation and is also used to comply with many European laws mandating partial human effort before the motor engages.\n\nOne of the biggest headwinds facing electric bicycles in the United States has been lower fuel prices since the end of the 2000s commodities boom.\n"}
{"id": "2150057", "url": "https://en.wikipedia.org/wiki?curid=2150057", "title": "Emergency Action Message", "text": "Emergency Action Message\n\nIn the U.S. military's strategic nuclear weapon nuclear command and control (NC2) system, an Emergency Action Message (EAM) is a preformatted message that directs nuclear-capable forces to execute specific Major Attack Options (MAOs) or Limited Attack Options (LAOs) in a nuclear war. Individual countries or specific regions may be included or withheld in the EAM, as specified in the Single Integrated Operational Plan (SIOP). The SIOP was updated annually until February 2003, when it was replaced by Operations Plan (OPLAN) 8044. Since July 2012, the US nuclear war plan has been OPLAN 8010-12, \"Strategic Deterrence and Force Employment\".\n\nEAMs use cryptographic protocols (including such methods as digital signatures) to authenticate the messages, thereby ensuring that they cannot be forged or altered.\n\nIn the United States, the EAM will be issued from the National Military Command Center (NMCC) at the Pentagon or, if it has been destroyed by an enemy first strike, by the Alternate National Military Command Center - Site R at Raven Rock, Pennsylvania or by the Boeing E-4B National Airborne Operations Center (NAOC).\n\nThe messages are sent in digital format to nuclear-capable major commands via the secure Automatic Digital Network (AUTODIN). The messages are then relayed to aircraft that are on alert by the U.S. Strategic Command at Offutt Air Force Base in Omaha, Nebraska, via single-sideband modulation radio transmitters of the High Frequency Global Communications System (formerly known as the Global High Frequency Service). The EAM is relayed to missile-firing nuclear submarines via special transmitters designed for communication with submarines. The transmitters include those designed to operate at Very Low Frequency (VLF). The submarines pick up the message via special antennas. Nuclear-capable forces will then be expected to carry out an EAM without fail. Manned bombers may be recalled, but missiles fired from land-based silos or from submarines cannot be recalled.\n\nSkyking messages are also read on the same network as EAMs, also known as \"Foxtrot Broadcasts\". These messages will interrupt an EAM if needed to be read. They contain a higher priority and time-sensitive code for orders that need immediate attention.\n\nThe EAM system was featured extensively and used as one of the primary plot devices in the feature film \"Crimson Tide\".\n\n"}
{"id": "16542019", "url": "https://en.wikipedia.org/wiki?curid=16542019", "title": "Energy cannibalism", "text": "Energy cannibalism\n\nEnergy cannibalism refers to an effect where rapid growth of a specific energy producing industry creates a need for energy that uses (or cannibalizes) the energy of existing power plants. Thus during rapid growth the industry as a whole produces no new energy because it is used to fuel the embodied energy of future power plants.\n\nIn order for an “emission free” power plant to have a net negative impact on the greenhouse gas emissions of the energy supply it must produce enough emission-less electricity to offset both greenhouse gas emissions that it is directly responsible for (e.g. from concrete used to construct a nuclear power plant) and to offset the greenhouse gas emissions from electricity generated for its construction (e.g. if coal is used to generate electricity while constructing a nuclear power plant). This can become challenging during rapid growth of the “emission free” technology because it may require the construction of additional power plants of the older technology simply to power the construction of the new “emission free” technology.\n\nFirst, all the individual power plants of a specific type can be viewed as a single aggregate plant or ensemble and can be observed for its ability to mitigate emissions as it grows. This ability is first dependent on the energy payback time of the plant. Aggregate plants with a total installed capacity of formula_1 (in GW) produces:\n\nof electricity, where formula_2 (in hours per year) is the fraction of time the plant is running at full capacity, formula_3 is the capacity of individual power plants and formula_4 is the total number of plants. If we assume that the energy industry grows at a rate, formula_5, (in units of 1/year, e.g. 10% growth = 0.1/year) it will produce additional capacity at a rate (in GW/year) of \n\nAfter one year, the electricity produced would be \n\nThe time that the individual power plant takes to pay for itself in terms of energy it needs over its life cycle, or the energy payback time, is given by the principal energy invested (over the entire life cycle), formula_6, divided by energy produced (or fossil fuel energy saved) per year, formula_7. Thus if the energy payback time of a plant type is formula_8, (in years,) the energy investment rate needed for the sustained growth of the entire power plant ensemble is given by the cannibalistic energy, formula_9:\n\nThe power plant ensemble will not produce any net energy if the cannibalistic energy is equivalent to the total energy produced. So by setting equation () equal to () the following results:\n\nand by doing some simple algebra it simplifies to:\n\nSo if one over the growth rate is equal to the energy payback time, the aggregate type of energy plant produces no net energy until growth slows down.\n\nThis analysis was for energy but the same analysis is true for greenhouse gas emissions. The principle greenhouse gas emissions emitted in order to provide for the power plant divided by the emissions offset every year must be equal to one over the growth rate of type of power to break even.\n\nFor example, if the energy payback is 5 years and the capacity growth is 20%, no net energy is produced and no greenhouse gas emissions are offset if the only power input to the growth is fossil during the growth period.\n\nIn the article “Thermodynamic Limitations to Nuclear Energy Deployment as a Greenhouse Gas Mitigation Technology” the necessary growth rate, r, of the nuclear power industry was calculated to be 10.5%. This growth rate is very similar to the 10% limit due to energy payback example for the nuclear power industry in the United States calculated in the same article from a life cycle analysis for energy.\n\nThese results indicate that any energy policies with the intention of driving down greenhouse gas emissions with deployment of additional nuclear reactors will not be effective unless the nuclear energy industry in the U.S. improves its efficiency. \n\nSome of the energy input into nuclear power plants occurs as production of concrete, which consumes little electricity from power plants.\n\nAs with nuclear power plants, hydroelectric dams are built with large amounts of concrete, which equate to considerable CO2 emissions, but little power usage. The long lifespan of hydroplants then contribute to a positive power ratio for a longer time than most other power plants.\n\nFor the environmental impact of solar power, the energy payback time of a power generating system is the time required to generate as much energy as was consumed during production of the system. In 2000 the energy payback time of PV systems was estimated as 8 to 11 years and in 2006 this was estimated to be 1.5 to 3.5 years for crystalline silicon PV systems and 1-1.5 years for thin film technologies (S. Europe). Similarly, the energy return on investment (EROI) is to be considered.\n\nFor wind power, energy payback is around one year.\n"}
{"id": "520676", "url": "https://en.wikipedia.org/wiki?curid=520676", "title": "Francis Chichester", "text": "Francis Chichester\n\nSir Francis Charles Chichester KBE (17 September 1901 – 26 August 1972) was a pioneering aviator and solo sailor.\n\nHe was knighted by Queen Elizabeth II for becoming the first person to sail single-handed around the world by the clipper route and the fastest circumnavigator, in nine months and one day overall in 1966–67.\n\nChichester was born in the rectory at Shirwell near Barnstaple in Devon, England, the son of a Church of England clergyman, Charles Chichester, himself the seventh son of Sir Arthur Chichester, 8th Baronet. His mother was Emily Annie, daughter of Samuel Page. At the age of six he was sent as a boarder to The Old Ride Preparatory School for boys, then attended Marlborough College during World War I. At the age of eighteen Chichester emigrated to New Zealand where in ten years he built up a prosperous business in forestry, mining and property development, only to suffer severe losses in the Great Depression.\n\nAfter returning to England in 1929 to visit his family, Chichester took flying lessons at Brooklands, Surrey, and qualified as a pilot. He then took delivery of a de Havilland Gipsy Moth aircraft, which he intended to fly to New Zealand, hoping to break Bert Hinkler's record solo flight back to Australia on the way. While mechanical problems meant that the record eluded him, he completed the trip in 41 days. The aircraft was then shipped to New Zealand. Finding that he was unable to carry enough fuel to cross the Tasman Sea directly, Chichester had his Gipsy Moth fitted with floats borrowed from the New Zealand Permanent Air Force, and went on to make the first solo flight across the Tasman Sea from East to West (New Zealand to Australia). He was the first to land an aircraft at Norfolk Island and Lord Howe Island. Again, the trip was delayed: after his aircraft was severely damaged at Lord Howe, he had to rebuild it himself with the help of islanders.\n\nThough the concept of \"off-course navigation\" (steering to one side so you know which way the error is) is probably as old as navigation, Chichester was the first to use it in a methodical manner in an aircraft. His only method of fixing his position was to take sun sights with a sextant. As a solo pilot, this was a difficult thing to do in a moving aircraft, as he needed to fly the aircraft at the same time. After the sun sight was taken, he had to make calculations by long-hand. As all this could be unreliable, Chichester needed an alternative. When he reached a point at which the sun was at a calculated altitude above the horizon, the pilot then made a 90-degree turn to the left (or right as calculated) and then flew along this line until the destination was reached. Since he did not know in advance when he would arrive at a line of position passing through his destination, he calculated a table or graph of the Sun's altitude and azimuth at his destination for a range of times bracketing his ETA. The advantage of this method was that the effects of drift were reduced to errors in distance travelled, usually much smaller. Since Chichester arrived at Lord Howe Island in the afternoon, the Sun was to his north-west when he made his turn. Some hours before making his turn, close to local noon when the Sun was to his north, Chichester made two observations with his sextant to check his dead-reckoning course.\n\nThe general principle was, when the Sun is to the right or left of one's course one can check the course but not distance to the destination. When the Sun is ahead or behind one's course, the distance to one's destination can be checked but not one's course. Chichester planned his final approach to follow a line of position directly to his destination. This technique allowed him to find tiny islands in the Pacific. He was awarded the inaugural Guild of Air Pilots and Air Navigators Johnston Memorial Trophy for this trip. Chichester then decided to circumnavigate the world solo; he made it to Japan but at Katsuura, Chiba he collided with an overhead cable, sustaining serious injuries.\n\nUnable to join the Royal Air Force at the outbreak of the Second World War due to age and eyesight, he was not granted a commission until 14 March 1941 when he joined the Royal Air Force Volunteer Reserve for the duration of hostilities. His civil occupation was listed as Air Navigation Specialist. His first posting was to the Air Ministry in the Navigation section of the Directorate of Air Member Training, where he served until August 1942. In July 1943 he was sent to the Empire Central Flying School where he instructed in navigation until released in September 1945. He wrote the navigation manual that allowed the pilots of single-handed fighter aircraft to navigate across Europe and back using kneeboard navigation similar to that which he had used in the Pacific.\n\nAt the end of the war, he stayed in the United Kingdom. He purchased 15,000 surplus Air Ministry maps, initially pasting them onto boards and making jigsaw puzzles out of them, and later founded his own successful map-making company.\n\nIn 1958, Chichester was diagnosed with terminal lung cancer. (This might have been a misdiagnosis; David Lewis, a London doctor, who competed against Chichester in the first solo trans-Atlantic race, reviewed his case and called Chichester's abnormality a \"lung abscess\".) His wife Sheila put him on a strict vegetarian diet (now considered to be a macrobiotic diet) and his cancer went into remission. Chichester then turned to long-distance yachting.\n\nIn 1960, he entered and won the first single-handed transatlantic yacht race, which had been founded by 'Blondie' Hasler, in the yacht \"Gipsy Moth III\". He came second in the second race four years later.\n\nOn 27 August 1966 Chichester sailed his ketch \"Gipsy Moth IV\" from Plymouth in the United Kingdom and returned there after 226 days of sailing on 28 May 1967, having circumnavigated the globe, with one stop (in Sydney). By doing so, he became the first person to achieve a true circumnavigation of the world solo from West to East via the great Capes. The voyage was also a race against the clock, as Chichester wanted to beat the typical times achieved by the fastest fully crewed clipper ships during the heyday of commercial sail in the 19th century.\n\nIn July 1967, a few weeks after his solo circumnavigation, Chichester was knighted, being appointed a Knight Commander of the Order of the British Empire for \"individual achievement and sustained endeavour in the navigation and seamanship of small craft\". For the ceremony, the Queen used the sword used by her predecessor Queen Elizabeth I to knight the adventurer Sir Francis Drake, the first Englishman with his crew to complete a circumnavigation. \"Gipsy Moth IV\" was preserved alongside the \"Cutty Sark\" at Greenwich.\n\nChichester was also honoured in 1967 by a newly issued 1/9d (one shilling and nine pence) postage stamp, which showed him aboard \"Gipsy Moth IV\". This went against an unwritten tradition of the General Post Office, because Chichester was neither a member of the royal family nor dead when the stamp was issued.\n\nIn 1968, when Donald Crowhurst was trying to win the Sunday Times Golden Globe Race, a single-handed round-the-world event, it was Chichester who dismissed Crowhurst's wildly exaggerated reports of his own progress, which had fooled many enthusiastic supporters.\n\nIn 1970, Chichester attempted to sail 4,000 miles in twenty days, in \"Gipsy Moth V\", but failed by one day.\n\nFrancis Chichester died of cancer in Plymouth, Devon, on 26 August 1972, and was buried in the church of his ancestors, St Peter's Church in Shirwell, near Barnstaple. His widow died in 1989 and is buried with him.\n\nSir Francis Chichester had two sons, George and Giles. His younger son, Giles Chichester, was for many years a British politician, and Conservative Member of the European Parliament for South West England and Gibraltar.\n\nBy the early 2000s, the condition of \"Gipsy Moth IV\", even though it continued to rest in a Greenwich dry dock hoist, had seriously deteriorated. Admirers of the vessel knew that she required a complete restoration if her life was to be extended. A campaign was launched in 2003 by Paul Gelder, editor of \"Yachting Monthly\" magazine, to sail the yacht around the world a second time in observance of the 40th anniversary of Chichester's epic voyage (and coincidentally the 100th birthday of the magazine).\n\nIn November 2004 she was lifted out of Greenwich dry dock (which some had called the yacht's \"grave\") after being sold to the UK Sailing Academy in Cowes, Isle of Wight, for a token £1 and a gin and tonic, said to be Chichester's favourite tipple. The UKSA, \"Yachting Monthly\" and the Maritime Trust were the three major project partners in the bold campaign to save the yacht. She was taken by road back to Camper & Nicholson, her original builders in Gosport.\n\nOn 20 June 2005 \"Gipsy Moth IV\" was relaunched after a £400,000 refit with money raised by donations from the public, and equipment and services given by the British marine industry. In September 2005 she embarked on a 21-month educational round-the-world voyage with the Blue Water Round the World Rally, via the trade wind route and the Panama and Suez Canals (not the Capes as had been followed in its first circumnavigation). In spring 2006, she ran aground on an atoll in the South Pacific. An extensive restoration in Auckland was required to repair the yacht, which was successfully refloated in June 2006. After being accompanied into Plymouth by a flotilla of boats, \"Gipsy Moth IV\" docked at West Hoe Pier on 28 May 2007, as she had done exactly 40 years earlier, to complete her journey round the world.\n\nThe yacht's restoration and the second circumnavigation are described in Paul Gelder's 2007 book, \"Gipsy Moth IV: A Legend Sails Again\".\n\nThe English rock group Dire Straits pay tribute to the achievements of Sir Francis in their album track entitled \"Single Handed Sailor\", which is track No.8 (of 9) of their 1979 album, \"Communiqué\".\n\nNorfolk Island issued a stamp, in 1981, commemorating the first landing of an aircraft on the Island, Chichester's Gipsy Moth \"Mme Elijah\", at Cascade Bay on 28 March 1931. Another stamp (14 cents) was issued by Norfolk Island at a later date showing Chichester's seaplane.\n\nA memorial plaque to Chichester was unveiled at the family home at 9 St James's Place, SW1 in September 1993.\n\n\n"}
{"id": "43523277", "url": "https://en.wikipedia.org/wiki?curid=43523277", "title": "G. W. C. Tait", "text": "G. W. C. Tait\n\nG.W.C. Tait, also known as George William Campbell Tait (April 28, 1912 – March 9, 1993), was an inventor and pioneer in the field of health physics in Canada and at the Chalk River Project.\n\nTait lived in Canada and Vienna. His residences included Deep River, Ontario, Canada, and Gibsons, British Columbia, Canada and Vienna, Austria.\n1951-1958, Head of Radiation Hazard Control, National Research Council, Chalk River Project, Atomic Energy of Canada Limited.\n1958-1960, Director, Division of Health Safety and Waste Disposal, International Atomic Energy Agency, Vienna, Austria.\nTait was involved in early work at the Chalk River Project and during his productive career produced at least two patents that involved gas sampling equipment.\n\nTait was a key member in the formation of the Health Physics Society and represented Canada.\n\n\n"}
{"id": "6042624", "url": "https://en.wikipedia.org/wiki?curid=6042624", "title": "Genus-two surface", "text": "Genus-two surface\n\nIn mathematics, a genus-two surface (also known as a double torus or two-holed torus) is a surface formed by the connected sum of two tori. That is to say, from each of two tori the interior of a disk is removed, and the boundaries of the two disks are identified (glued together), forming a double torus. \n\nThis is the simplest case of the connected sum of \"n\" tori. A connected sum of tori is an example of a two-dimensional manifold. According to the classification theorem for 2-manifolds, every compact connected 2-manifold is either a sphere, a connected sum of tori, or a connected sum of real projective planes.\n\nDouble torus knots are studied in knot theory.\n\nThe Bolza surface is the most symmetric Riemann surface of genus 2.\n\n\n"}
{"id": "6503797", "url": "https://en.wikipedia.org/wiki?curid=6503797", "title": "Ground granulated blast-furnace slag", "text": "Ground granulated blast-furnace slag\n\nGround-granulated blast-furnace slag (GGBS or GGBFS) is obtained by quenching molten iron slag (a by-product of iron and steel-making) from a blast furnace in water or steam, to produce a glassy, granular product that is then dried and ground into a fine powder.\n\nThe chemical composition of a slag varies considerably depending on the composition of the raw materials in the iron production process. Silicate and aluminate impurities from the ore and coke are combined in the blast furnace with a flux which lowers the viscosity of the slag. In the case of pig iron production the flux consists mostly of a mixture of limestone and forsterite or in some cases dolomite. In the blast furnace the slag floats on top of the iron and is decanted for separation. Slow cooling of slag melts results in an unreactive crystalline material consisting of an assemblage of Ca-Al-Mg silicates. To obtain a good slag reactivity or hydraulicity, the slag melt needs to be rapidly cooled or quenched below 800 °C in order to prevent the crystallization of merwinite and melilite. To cool and fragment the slag a granulation process can be applied in which molten slag is subjected to jet streams of water or air under pressure. Alternatively, in the pelletization process the liquid slag is partially cooled with water and subsequently projected into the air by a rotating drum. In order to obtain a suitable reactivity, the obtained fragments are ground to reach the same fineness as Portland cement.\n\nThe main components of blast furnace slag are CaO (30-50%), SiO (28-38%), AlO (8-24%), and MgO (1-18%). In general increasing the CaO content of the slag results in raised slag basicity and an increase in compressive strength. The MgO and AlO content show the same trend up to respectively 10-12% and 14%, beyond which no further improvement can be obtained. Several compositional ratios or so-called hydraulic indices have been used to correlate slag composition with hydraulic activity; the latter being mostly expressed as the binder compressive strength.\n\nThe glass content of slags suitable for blending with Portland cement typically varies between 90-100% and depends on the cooling method and the temperature at which cooling is initiated. The glass structure of the quenched glass largely depends on the proportions of network-forming elements such as Si and Al over network-modifiers such as Ca, Mg and to a lesser extent Al. Increased amounts of network-modifiers lead to higher degrees of network depolymerization and reactivity.\n\nCommon crystalline constituents of blast-furnace slags are merwinite and melilite. Other minor components which can form during progressive crystallization are belite, monticellite, rankinite, wollastonite and forsterite. Minor amounts of reduced sulphur are commonly encountered as oldhamite.\n\nGGBS is used to make durable concrete structures in combination with ordinary portland cement and/or other pozzolanic materials. GGBS has been widely used in Europe, and increasingly in the United States and in Asia (particularly in Japan and Singapore) for its superiority in concrete durability, extending the lifespan of buildings from fifty years to a hundred years.\n\nTwo major uses of GGBS are in the production of quality-improved slag cement, namely Portland Blastfurnace cement (PBFC) and high-slag blast-furnace cement (HSBFC), with GGBS content ranging typically from 30 to 70%; and in the production of ready-mixed or site-batched durable concrete.\n\nConcrete made with GGBS cement sets more slowly than concrete made with ordinary Portland cement, depending on the amount of GGBS in the cementitious material, but also continues to gain strength over a longer period in production conditions. This results in lower heat of hydration and lower temperature rises, and makes avoiding cold joints easier, but may also affect construction schedules where quick setting is required.\n\nUse of GGBS significantly reduces the risk of damages caused by alkali–silica reaction (ASR), provides higher resistance to chloride ingress — reducing the risk of reinforcement corrosion — and provides higher resistance to attacks by sulfate and other chemicals.\n\nGGBS cement can be added to concrete in the concrete manufacturer's batching plant, along with Portland cement, aggregates and water. The normal ratios of aggregates and water to cementitious material in the mix remain unchanged. GGBS is used as a direct replacement for Portland cement, on a one-to-one basis by weight. Replacement levels for GGBS vary from 30% to up to 85%. Typically 40 to 50% is used in most instances.\n\nThe use of GGBS in addition to Portland cement in concrete in Europe is covered in the concrete standard EN 206:2013. This standard establishes two categories of additions to concrete along with ordinary Portland cement: nearly inert additions (Type I) and pozzolanic or latent hydraulic additions (Type II). GGBS cement falls in the latter category. As GGBS cement is slightly less expensive than Portland cement, concrete made with GGBS cement will be similarly priced to that made with ordinary Portland cement.\n\nIt is used partially as per mix ratio.\n\nGGBS cement is routinely specified in concrete to provide protection against both sulphate attack and chloride attack. GGBS has now effectively replaced sulfate-resisting Portland cement (SRPC) on the market for sulfate resistance because of its superior performance and greatly reduced cost compared to SRPC. Most projects in Dublin's Docklands, including Spencer Dock, are using GGBS in subsurface concrete for sulfate resistance.\n\nTo protect against chloride attack, GGBS is used at a replacement level of 50% in concrete. Instances of chloride attack occur in reinforced concrete in marine environments and in road bridges where the concrete is exposed to splashing from road de-icing salts. In most NRA projects in Ireland GGBS is now specified in structural concrete for bridge piers and abutments for protection against chloride attack. The use of GGBS in such instances will increase the life of the structure by up to 50% had only Portland cement been used, and precludes the need for more expensive stainless steel reinforcing.\n\nGGBS is also routinely used to limit the temperature rise in large concrete pours. The more gradual hydration of GGBS cement generates both lower peak and less total overall heat than Portland cement. This reduces thermal gradients in the concrete, which prevents the occurrence of microcracking which can weaken the concrete and reduce its durability, and was used for this purpose in the construction of the Jack Lynch Tunnel in Cork.\n\nIn contrast to the stony grey of concrete made with Portland cement, the near-white color of GGBS cement permits architects to achieve a lighter colour for exposed fair-faced concrete finishes, at no extra cost. To achieve a lighter colour finish, GGBS is usually specified at between 50% to 70% replacement levels, although levels as high as 85% can be used. GGBS cement also produces a smoother, more defect free surface, due to the fineness of the GGBS particles. Dirt does not adhere to GGBS concrete as easily as concrete made with Portland cement, reducing maintenance costs. GGBS cement prevents the occurrence of efflorescence, the staining of concrete surfaces by calcium carbonate deposits. Due to its much lower lime content and lower permeability, GGBS is effective in preventing efflorescence when used at replacement levels of 50% to 60%.\n\nConcrete containing GGBS cement has a higher ultimate strength than concrete made with Portland cement. It has a higher proportion of the strength-enhancing calcium silicate hydrates (CSH) than concrete made with Portland cement only, and a reduced content of free lime, which does not contribute to concrete strength. Concrete made with GGBS continues to gain strength over time, and has been shown to double its 28-day strength over periods of 10 to 12 years.\n\nSince GGBS is a by-product of steel manufacturing process, its use in concrete is recognized by LEED etc. as improving the sustainability of the project and will therefore add points towards LEED certification. In this respect, GGBS can also be used for superstructure in addition to the cases where the concrete is in contact with chlorides and sulfates. This is provided that the slower setting time for casting of the superstructure is justified.\n\n\n"}
{"id": "32588510", "url": "https://en.wikipedia.org/wiki?curid=32588510", "title": "Gundlakamma Reservoir Project", "text": "Gundlakamma Reservoir Project\n\nGundlakamma Reservoir Project is an irrigation project located in Prakasam district in Andhra Pradesh, India. The dam with 56 million cubic meters live storage capacity is located across the Gundlakamma River. It now supplies 100 acres out of the total envisaged area of 80,000. This reservoir also supplies drinking water to the nearby Ongole city.\n"}
{"id": "42254726", "url": "https://en.wikipedia.org/wiki?curid=42254726", "title": "Hasanain Juaini", "text": "Hasanain Juaini\n\nHasanain Juaini is an Indonesian environmental conservationist, educator and winner of the Ramon Magsaysay Award.\n"}
{"id": "2240319", "url": "https://en.wikipedia.org/wiki?curid=2240319", "title": "Heat-Ray", "text": "Heat-Ray\n\nThe Heat-Ray is the primary offensive weapon used by the Martians in H. G. Wells' classic science fiction novel \"The War of the Worlds\" and its offshoots.\n\nThe Heat-Ray is essentially a directed-energy weapon, albeit that the name of \"Heat-Ray\" is more commonly applied to the destructive energy it projects than to the weapon itself; the latter described as a box-like or camera-like case mounted on larger machines, including Tripod fighting-machines, whereas the Ray was credited with striking targets at distances of at least two miles.\n\nThe novel explains:\n\n\"in some way they are able to generate an intense heat in a chamber of practically absolute non-conductivity. This intense heat they project in a parallel beam against any object they choose, by means of a polished parabolic mirror of unknown composition, much as the parabolic mirror of a lighthouse projects a beam of light... it is certain that a beam of heat is the essence of the matter. Heat, and invisible, instead of visible, light. Whatever is combustible flashes into flame at its touch, lead runs like water, it softens iron, cracks and melts glass, and when it falls upon water, incontinently that explodes into steam.\nThe only visible element of the ray was a flash emitted from the chamber, in which respect Wells' description is consistent with experimental directed-energy weapons of later years (such as a powerful CO Laser).\n\nThe Heat-Ray is a feature of virtually every adaptation of the story. Many adaptations adhere to the characteristics given in the novel, such as the 1938 CBS radio adaptation; even reciting near-verbatim descriptions.\n\nThe Heat-Ray is described in \"Jeff Wayne's Musical Version of The War of the Worlds\" and depicted on the album artwork painted by Michael Trim as well as the art \"Panic in the Streets\" by Geoff Taylor, wherein it emanates from a proboscis in the cupola of the tripod.\n\nThe Classics Illustrated comic book adaptation of \"The War of the Worlds\" portrays the Heat-Ray as a visible light yellowish/gold ray.\n\nIn the early science fiction novel \"Edison's Conquest of Mars\", as an answer to the Martian Heat-Ray, Thomas Edison designs a disintegrator ray for use by human invasion forces. This is the first appearance of such a device in science fiction.\n\nIn \"Sherlock Holmes's War of the Worlds\", the Heat-Ray is described as based on nuclear energy, and projected from a pink-hued, multifaceted focusing crystal.\n\nIn the spoof film \"Scary Movie 4\", the Heat-Ray's effects are given a comical treatment; the intersection scene shows a lady push another into the path of the Heat-Ray to get her clothes. In one scene, the President of the United States, played by Leslie Nielsen, announces to the United Nations that America has managed to capture a Heat-Ray. Its effects have been reversed, comically destroying the clothes of everyone in the room, with the President barely aware.\n\nIn Stephen Baxter's sequel, \"The Massacre of Mankind\", the Heat-Ray is said to be an infrared laser with a beam temperature of 1500 degrees. The parabolic mirror is merely an aiming scope. A memorial to Heat-Ray victims, the \"Tomb of the Vanished Warrior\", is established in 1908, a year after the invasion; at the event, Prime Minister Henry Campbell-Bannerman was assassinated by members of the Women's Social and Political Union. \n\nFor the 1953 film version, the Martians manta ray-shaped war machines use a combination of three rays: one Heat-Ray on long necks atop of their machines, which fire red sparks, and two disintegrator rays on their wing tips, which are shown as green bolts. These two can only be pointed in the direction their ships face, while the Heat-Ray on top can be pointed in any direction. Most of their targets glow and vanish, sometimes leaving a stain or pile of ash; the Heat-Ray differs from the disintegrators as the former sets the surrounding environment ablaze as well as vaporizing the target within a few seconds. Although the Heat-Ray does have a simple destructive effect on certain objects, at other times, the ray would set objects ablaze or cause them to explode (as shown when the machines reach Los Angeles). In one scene, General Mann states that it is likely the Martians generate radiation without using heavy screening to power their rays. The book's poisonous \"Black Smoke\" chemical weapon is replaced by the \"skeleton beam\". Doctor Clayton Forrester explained how these skeleton beams worked as such:\n\nAs a sequel to the 1953 film, the Heat-Ray's use in the \"War of the Worlds\" TV series is rather notable. Aside from their employment in the first episode (its destruction replayed in the opening credit sequence in subsequent episodes of the season), the main Heat-Ray is put to more attention in an episode in which the aliens are unable to unearth a buried warship from a recon mission and are forced to remove the gooseneck device from the ship and strap it atop a hearse. The aliens' mission in this episode is put to end when the Heat-Ray hits them after being reflected off a makeshift parabolic mirror.\n\nThe series also offered a Heat-Ray not employed by a war machine, but as a personal weapon. \"The Second Seal\" episode deals with the discovery of archives that contain remnants of the 1953 invasion. Among the material found is a boomerang-shaped weapon that fires Heat-Rays. These rays are the kind of green blobs fired from the tips of their warships, and are similarly shot from the ends of the object. \n\nThe Heat-Rays featured in the series mirror the same power of the film. This includes the variation between their ability to visibly destroy something as well as simply making a target disappear. Although the word \"Heat-Ray\" is never applied in the series as it is in the 1953 or updated 2005 film, the term used in one episode is \"Death-Ray\".\n\nIn Steven Spielberg's \"War of the Worlds\" adaptation of Wells' novel, the Heat-Ray is portrayed as two bluish-white rays having a desiccating effect on living objects, such as animals, and a 'disruptive' effect on other objects; but a bridge in one scene is thrown from its pylons when hit by the ray, as if physically struck, and in an earlier scene, brick-and-wood buildings are either destroyed or catch fire. Later in the movie, an army of tripods destroying a city is shown, with Heat-Rays collapsing targeted buildings in a similar fashion to the destruction of the bridge. Curiously, human clothing does not seem to be affected by the Heat-Rays, which is used to effect in one of the film's scenes when the clothes of slaughtered humans are floating along a river.\n\nIn The Asylum's 2005 film \"H.G Wells' War of the Worlds\" (also entitled \"Invasion\"), the Heat-Ray is seen as a bluish-green light and is built inside the \"walkers\", which are not tripods, but six-legged machines resembling crabs. The ray fires from the walker's single \"eye\". When the ray hits humans, they are instantly burned to the bone. The ray can also be used to destroy buildings. A character named Pvt. Kerry Williams compares the ray's effect on a human to a fly zapper's effect on a fly.\n\nIn the film's sequel, \"\", a kind of Heat-ray can be seen firing from the 'squid-walkers', a living race of flying cybernetic tripods; but the effects of this weapon are largely unknown, as the ray itself is used infrequently. It is most notably used to devastate London and Paris. Another kind of ray is also attached to the machines, but it is made to transport living humans into the mothership.\n\nIn the lesser-known 2005 low-budget, direct-to-video film adaptation from Pendragon Pictures, \"H.G. Wells' The War of the Worlds\", the generator is held by a small arm that extends from the hood of the machine (not one of the many visible tentacles/arms they use to capture humans). Three metallic fingers hold a rapidly spinning disc, generating the Heat-Ray; and when it touches flesh, the victim is reduced to bones.\n\nThere are Heat-Rays used by both humans and martians in this 2012 animated film, wherein human Heat-Rays are a reddish-orange color and the Martian Heat-Rays light green.\n\n"}
{"id": "28686205", "url": "https://en.wikipedia.org/wiki?curid=28686205", "title": "Hoya de Los Vicentes Solar Plant", "text": "Hoya de Los Vicentes Solar Plant\n\nThe Hoya de Los Vicentes Solar Plant () is a photovoltaic power station in Jumilla, Murcia in Spain. The solar park covers area of some and comprises a group of 200 photovoltaic arrays with a total capacity of 23 MW. A total of 120,000 solar panels have been installed in the facility. The project generates energy equivalent to the annual consumption of 20,000 households.\n\n"}
{"id": "47684331", "url": "https://en.wikipedia.org/wiki?curid=47684331", "title": "Imidazolate", "text": "Imidazolate\n\nImidazolate (CHN) is the conjugate base of imidazole. It is a nucleophile and a strong base. The free anion has C symmetry. Imidazole has a p\"K\" of 14.05, so the deprotonation of imidazole (CHNH) requires a strong base.\n\nImidazolate is a common bridging ligand in coordination chemistry. In the zeolitic imidazolate frameworks, the metals are interconnected via imidazolates. In the enzyme superoxide dismutase, imidazolate links copper and zinc centers.\n"}
{"id": "4364816", "url": "https://en.wikipedia.org/wiki?curid=4364816", "title": "Inert waste", "text": "Inert waste\n\nMedical waste Relevance\nInert waste is waste which is neither chemically nor biologically reactive and will not decompose. Examples of this are sand and concrete. This has particular relevance to landfills as inert waste typically requires lower disposal fees than biodegradable waste or hazardous waste.\nThis type of waste coagulates in the environment and can cause serious problem, much like filling up of areas, but they are also relatively less harmful than plastics as they don't give out harmful residues after hundreds of years.\n\n"}
{"id": "19856087", "url": "https://en.wikipedia.org/wiki?curid=19856087", "title": "Intensity (heat transfer)", "text": "Intensity (heat transfer)\n\nIn the field of heat transfer, intensity of radiation formula_1 is a measure of the distribution of radiant heat flux per unit area and solid angle, in a particular direction, defined according to\n\nwhere\n\n\nTypical units of intensity are W·m·sr.\n\nIntensity can sometimes be called radiance, especially in other fields of study.\n\nThe emissive power of a surface can be determined by integrating the intensity of emitted radiation over a hemisphere surrounding the surface:\n\nFor diffuse emitters, the emitted radiation intensity is the same in all directions, with the result that\n\nThe factor formula_11 (which really should have the units of steradians) is a result of the fact that intensity is defined to exclude the effect of reduced view factor at large values formula_8; note that the solid angle corresponding to a hemisphere is equal to formula_13 steradians.\n\nSpectral intensity formula_14 is the corresponding spectral measurement of intensity; in other words, the intensity as a function of wavelength.\n\n\n"}
{"id": "10041349", "url": "https://en.wikipedia.org/wiki?curid=10041349", "title": "Joseph Henry Keenan", "text": "Joseph Henry Keenan\n\nJoseph Henry Keenan (August 24, 1900 – July 17, 1977) was an American thermodynamicist and mechanical engineer noted for his work in the calculation of steam tables, research in jet-rocket propulsion, and his work in furthering the development in the understanding of the laws of thermodynamics in the mid 20th century. His classic 1941 textbook \"Thermodynamics\" served as a fundamental teaching tool in various engineering curricula during the 1940s and 1950s.\n\nHe earned a bachelor's degree in naval architecture and marine engineering at the Massachusetts Institute of Technology in 1922. After working as a design engineer on steam turbines for General Electric Company, Keenan became an assistant professor of mechanical engineering at the Stevens Institute of Technology in 1928. In 1934, he became an associate professor of mechanical engineering at the Massachusetts Institute of Technology. He was promoted to professor in 1939. He served as Head of the Department of Mechanical Engineering from 1958 to 1961.\n\nA major portion of Keenan’s career was devoted to the development of accurate tables of the properties of steam, which are vital to the electric power industry. In 1929, he was appointed the U.S. delegate to the First International Conference on the Properties of Steam; he served as delegate in all successive conferences on this subject through the eighth in 1974.\n\nIn 1965, he published the classic textbook \"Principles of General Thermodynamics\" with George Hatsopoulos which was major turning point in thermodynamics since Gilbert N. Lewis and Merle Randall with their 1923 \"Thermodynamics\" textbook. Their now famous version of the second law of thermodynamics is:\n\nThis shows that the second law of thermodynamics can be stated in terms of the existence of stable equilibrium states.\n\nHe was a fellow of the American Academy of Arts and Sciences and the American Society of Mechanical Engineers. He was award the ASME Worcester Reed Warner Medal in 1955 for work on thermodynamics and the properties of steam. He was elected to the National Academy of Engineering in 1976.\n\nIn 2007, an International Thermodynamics Symposium called “meeting the entropy challenge” was organized in M.I.T. in Honor and Memory of Professor Joseph Henry Keenan.\n\n\n"}
{"id": "22613078", "url": "https://en.wikipedia.org/wiki?curid=22613078", "title": "Lindhard theory", "text": "Lindhard theory\n\nLindhard theory, named after Danish professor Jens Lindhard, is a method of calculating the effects of electric field screening by electrons in a solid. It is based on quantum mechanics (first-order perturbation theory) and the random phase approximation. \n\nThomas–Fermi screening can be derived as a special case of the more general Lindhard formula. In particular, Thomas–Fermi screening is the limit of the Lindhard formula when the wavevector (the reciprocal of the length-scale of interest) is much smaller than the Fermi wavevector, i.e. the long-distance limit.\n\nThis article uses cgs-Gaussian units.\n\nThe Lindhard formula for the longitudinal dielectric function is given by\nHere, formula_1 is a positive infinitesimal constant, formula_2 is formula_3 and formula_4 is the carrier distribution function which is the Fermi–Dirac distribution function for electrons in thermodynamic equilibrium.\nHowever this Lindhard formula is valid also for nonequilibrium distribution functions.\n\nTo understand the Lindhard formula, let's consider some limiting cases in 2 and 3 dimensions. The 1-dimensional case is also considered in other ways.\n\nFirst, let's consider the long wavelength limit (formula_5).\n\nFor the denominator of the Lindhard formula, we get\n\nand for the numerator of the Lindhard formula, we get\n\nInserting these into the Lindhard formula and taking the formula_8 limit, we obtain\n\nwhere we used formula_10, formula_11 and formula_12.\n\nThis result is the same as the classical dielectric function.\n\nSecond, consider the static limit (formula_15).\nThe Lindhard formula becomes\n\nInserting the above equalities for the denominator and numerator, we obtain\n\nAssuming a thermal equilibrium Fermi–Dirac carrier distribution, we get\nhere, we used formula_19 and formula_20.\n\nTherefore, \n\nHere, formula_22 is the 3D screening wave number (3D inverse screening length) defined as \nformula_23.\n\nThen, the 3D statically screened Coulomb potential is given by\n\nAnd the Fourier transformation of this result gives\nknown as the Yukawa potential. Note that in this Fourier transformation, which is basically a sum over \"all\" formula_26, we used the expression for small formula_27 for \"every\" value of formula_26 which is not correct.\n\nFor a degenerated Fermi gas (\"T\"=0), Fermi energy is given by\nSo the density is \n\nAt T=0, formula_31, so formula_32.\n\nInserting this into the above 3D screening wave number equation, we obtain\n\nThis is the 3D Thomas–Fermi screening wave number.\n\nFor reference, Debye-Hückel screening describes the nondegenerate limit case.\n\nThe result is formula_33, 3D Debye-Hückel screening wave number.\n\nFirst, consider the long wavelength limit (formula_5).\n\nFor the denominator of the Lindhard formula,\n\nand for the numerator,\n\nInserting these into the Lindhard formula and taking the limit of formula_8, we obtain\n\nwhere we used formula_39, formula_40 and formula_41.\n\nSecond, consider the static limit (formula_15).\nThe Lindhard formula becomes\n\nInserting the above equalities for the denominator and numerator, we obtain\n\nAssuming a thermal equilibrium Fermi–Dirac carrier distribution, we get\nhere, we used formula_19 and formula_20.\n\nTherefore, \n\nformula_22 is 2D screening wave number(2D inverse screening length) defined as \nformula_50.\n\nThen, the 2D statically screened Coulomb potential is given by\n\nIt is known that the chemical potential of the 2-dimensional Fermi gas is given by\n\nand formula_53.\n\nSo, the 2D screening wave number is \n\nNote that this result is independent of \"n\".\n\nThis time, let's consider some generalized case for lowering the dimension.\nThe lower the dimension is, the weaker the screening effect.\nIn lower dimension, some of the field lines pass through the barrier material wherein the screening has no effect.\nFor the 1-dimensional case, we can guess that the screening affects only the field lines which are very close to the wire axis.\n\nIn real experiment, we should also take the 3D bulk screening effect into account even though we deal with 1D case like the single filament. The Thomas–Fermi screening has been applied to an electron gas confined to a filament and a coaxial cylinder. For a KPt(CN)Cl·2.6H0 filament, it was found that the potential within the region between the filament and cylinder varies as formula_54 and its effective screening length is about 10 times that of metallic platinum.\n\n"}
{"id": "30013500", "url": "https://en.wikipedia.org/wiki?curid=30013500", "title": "List of modern production plug-in electric vehicles", "text": "List of modern production plug-in electric vehicles\n\nThis is a list of mass production highway-capable plug-in electric vehicles (PEVs), and also includes those plug-ins at an advanced stage of development or being tested in demonstration programs. A PEV is any motor vehicle that can be recharged from any external source of electricity, and a subcategory of electric vehicles that includes all-electric or battery electric vehicles (BEVs), plug-in hybrid vehicles, (PHEVs), and plug-in conversions of hybrid electric vehicles depending on battery size and their all-electric range. Conventional hybrid electric vehicles (HEVs) are not included.\n\nElectric bicycles can be separated into different categories. The amount just of Pedelec models currently and past available in Europe is about 12000. \n\n\n\n"}
{"id": "14457415", "url": "https://en.wikipedia.org/wiki?curid=14457415", "title": "Manufacturer's empty weight", "text": "Manufacturer's empty weight\n\nIn aviation, manufacturer's empty weight (MEW) (also known as manufacturer's weight empty (MWE)) is the weight of the aircraft \"as built\" and includes the weight of the structure, power plant, furnishings, installations, systems and other equipment that are considered an integral part of an aircraft before additional operator items are added for operation. \n\n\"Basic aircraft empty weight\" is essentially the same and excludes any baggage, passengers, or usable fuel. Some manufacturers define this empty weight as including optional equipment, i.e. GPS units, cargo baskets, or spotlights.\n\nThis is the MEW quoted in the manufacturer's standard specification documents and is the aircraft standard basic dry weight upon which all other standard specifications and aircraft performance are based by the manufacturer.\n\nThe Specification MEW includes the weight of:\n\n\nFor small aircraft, the MEW may include unusable fuel and oil.\n\nThe Specification MEW excludes the weight of:\n\n\nFor small aircraft, the specification MEW is known as the \"standard empty weight\" (or \"standard weight empty\").\n\n\n"}
{"id": "25944940", "url": "https://en.wikipedia.org/wiki?curid=25944940", "title": "Medical grade silicone", "text": "Medical grade silicone\n\nMedical grade silicones are silicones tested for biocompatibility and are appropriate to be used for medical applications. In the United States, the Food and Drug Administration (FDA) Center for Devices and Radiological Health (CDRH) regulates devices implanted into the body. It does not regulate materials other than certain dental materials. The FDA regulate silicones used in food contact under the auspices of the Center for Food Safety and Nutrition (CFSAN) and for use in pharmaceuticals under the auspices of the Center for Drug Evaluation and Research (CDER).\n\nMedical grade silicones are generally grouped into three categories: non implantable, short term implantable, and long-term implantable. Materials approved as Class V and VI can be considered medical grade. Most medical grade silicones are at least Class VI certified. Silicone suppliers and some silicone prototyping companies provide guidelines for material use.\n\n\nSilicone rubber applications such as catheters are widespread in medicine, but have several limitations. For example, they exhibit poor tear strength and poor resistance to fatigue. Brittle fracture can occur from defects within sections owing to poor control of vulcanization. It resulted in high failure rates for breast implants, and much subsequent litigation in the USA, as well as elsewhere in the world. It led to a crisis of confidence in the US, with many manufacturers being forced out of the business entirely, and others to manufacture under FDA control.\n\n"}
{"id": "55778036", "url": "https://en.wikipedia.org/wiki?curid=55778036", "title": "Metal peroxide", "text": "Metal peroxide\n\nMetal peroxides are metal-containing compounds with ionically- or covalently-bonded peroxide () groups. This large family of compounds can be divided into ionic and covalent peroxide. The first class mostly contains the peroxides of the alkali and alkaline earth metals whereas the covalent peroxides are represented by such compounds as hydrogen peroxide and peroxymonosulfuric acid (HSO). In contrast to the purely ionic character of alkali metal peroxides, peroxides of transition metals have a more covalent character.\n\nThe peroxide ion is composed of two oxygen atoms that are linked by a single bond. The molecular orbital diagram of the peroxide dianion predicts a doubly occupied antibonding π* orbital and a bond order of one. The bond length is 149 pm, which is larger than in the ground state (triplet oxygen) of the oxygen molecule (O, 121 pm). This translates into the smaller force constant of the bond (2.8 N/cm vs. 11.4 N/cm for O) and the lower frequency of the molecular vibration (770 cm vs. 1555 cm for O).\n\nThe peroxide ion can be compared with superoxide , which is a radical, and dioxygen, a diradical.\n\nMost alkali metal peroxides can be synthesized directly by oxygenation of the elements. Lithium peroxide is formed upon treating lithium hydroxide with hydrogen peroxide:\n\nBarium peroxide is prepared by oxygenation of barium oxide at elevated temperature and pressure.\n\nBarium peroxide was once used to produce pure oxygen from air. This process relies on the temperature-dependent chemical balance between barium oxide and peroxide: the reaction of barium oxide with air at 500 °C results in barium peroxide, which upon heating to above 700 °C in oxygen decomposes back to barium oxide releasing pure oxygen.. The lighter alkaline earth metals calcium and magnesium also form peroxides, whichcate used commercially as oxygen sources or oxidizers.\n\nFew reactions are generally formulated for peroxide salt. In excess of dilute acids or water they release hydrogen peroxide.\n\nUpon heating, the reaction with water leads to the release of oxygen. Upon exposure to air, alkali metal peroxides absorb CO to give peroxycarbonates.\n\nUnlike the alkali metal and alkaline earth metal peroxides, binary transition metal peroxides, i.e., compounds containing only metal cations and peroxide anions, are rare. Metal dioxides are pervasive, e.g. MnO and rutile (TiO), but these are oxides, not peroxides. Well characterized examples include the d metal cations, Zinc peroxide (ZnO), two polymorphs, both explosive, of mercury peroxide (HgO), and cadmium peroxide (CdO).\n\nPeroxide is a common ligand in metal complexes. Within the area of transition metal dioxygen complexes, functions as a bidentate ligand. Some complexes have only peroxide ligands, e.g., chromium(VI) oxide peroxide (). Similarly, molybdate reacts in alkaline media with peroxide to form red peroxomolybdate . The reaction of hydrogen peroxide with aqueous titanium(IV) gives a brightly colored peroxy complex that is a useful test for titanium as well as hydrogen peroxide. Many transition metal dioxygen complexes are best described as adducts of peroxide.\n\nMany inorganic peroxides are used for bleaching textiles and paper and as a bleaching additive to detergents and cleaning products. The increasing environmental concerns resulted in the preference of peroxides over chlorine-based compounds and a sharp increase in the peroxide production. The past use of perborates as additives to detergents and cleaning products has been largely replaced by percarbonates. The use of peroxide compounds in detergents is often reflected in their trade names; for example, Persil is a combination of the words \"per\"borate and \"sil\"icate.\n\nSome peroxide salts release oxygen upon reaction with carbon dioxide. This reaction is used in generation of oxygen from exhaled carbon dioxide on submarines and spaceships. Sodium or lithium peroxides are preferred in space applications because of their lower molar mass and therefore higher oxygen yield per unit weight.\n\nAlkali metal peroxides can be used for the synthesis of organic peroxides. One example is the conversion of benzoyl chloride with sodium peroxide to dibenzoyl peroxide.\n\nAlexander von Humboldt synthesized barium peroxide in 1799 as a byproduct of his attempts to decompose air.\n\nNineteen years later Louis Jacques Thénard recognized that this compound could be used for the preparation of hydrogen peroxide. Thénard and Joseph Louis Gay-Lussac synthesized sodium peroxide in 1811. The bleaching effect of peroxides and their salts on natural dyes became known around that time, but early attempts of industrial production of peroxides failed, and the first plant producing hydrogen peroxide was built in 1873 in Berlin.\n"}
{"id": "39313159", "url": "https://en.wikipedia.org/wiki?curid=39313159", "title": "Nanofluids in solar collectors", "text": "Nanofluids in solar collectors\n\nNanofluid-based direct solar collectors are solar thermal collectors where nanoparticles in a liquid medium can scatter and absorb solar radiation. They have recently received interest to efficiently distribute solar energy. Nanofluid-based solar collector have the potential to harness solar radiant energy more efficiently compared to conventional solar collectors.\nNanofluids have recently found relevance in applications requiring quick and effective heat transfer such as industrial applications, cooling of microchips, microscopic fluidic applications, etc. Moreover, in contrast to conventional heat transfer (for solar thermal applications) like water, ethylene glycol, and molten salts, nanofluids are not transparent to solar radiant energy; instead, they absorb and scatter significantly the solar irradiance passing through them.\nTypical solar collectors use a black-surface absorber to collect the sun's heat energy which is then transferred to a fluid running in tubes embedded within. Various limitations have been discovered with these configuration and alternative concepts have been addressed. Among these, the use of nanoparticles suspended in a liquid is the subject of research. Nanoparticle materials including aluminium, copper, carbon nanotubes and carbon-nanohorns have been added to different base fluids and characterized in terms of their performance for improving heat transfer efficiency.\n\nDispersing trace amounts of nanoparticles into common base fluids has a significant impact on the optical as well as thermo physical properties of base fluid. This characteristic can be used to effectively capture and transport solar radiation. Enhancement of the solar irradiance absorption capacity leads to a higher heat transfer resulting in more efficient heat transfer as shown in figure 2.\nThe efficiency of a solar thermal system is reliant on several energy conversion steps, which are in turn governed by the effectiveness of the heat transfer processes. While higher conversion efficiency of solar to thermal energy is possible, the key components that need to be improved are the solar collector. An ideal solar collector will absorb the concentrated solar radiation, convert that incident solar radiation into heat and transfer the heat to the heat transfer fluid. Higher the heat transfer to fluid, higher is the outlet temperature and higher temp lead to improved conversion efficiency in the power cycle.\nnanoparticles have several orders of magnitude higher heat transfer coefficient when transferring heat immediately to the surrounding fluid. This is simply due to the small size of nanoparticle.\n\nWe know that thermal conductivity of solids is greater than liquids. Commonly used fluids in heat transfer applications such as water, ethylene glycol and engine oil have low thermal conductivity when compared to thermal conductivity of solids, especially metals. So, addition of solid particles in a fluid can increase the conductivity of liquids .But we cannot add large solid particles due to main problems:\nDue to these drawbacks, usage of solid particles have not become practically feasible.\nRecent improvements in nanotechnology made it possible to introduce small solid particles with diameter smaller than 10 nm. Liquids, thus obtained have higher thermal conductivity and are known as Nanofluids. As can be clearly seen from figure 4 that carbon nanotubes have highest thermal conductivity as compared to other materials.\nMaxwel model \n\nPak and Choi model\n\nKoo and Kleinstreuer model\n\nUdawattha and Narayana model\n\nwhere\n\nKeblinski et al. had named four main possible mechanisms for the anomalous increase in nanofluids heat transfer which are :\n\nDue to Brownian motion particles randomly move through liquid. And hence better transport of heat.\nBrownian motion increased mode of heat transfer.\n\nLiquid molecules can form a layer around the solid particles and there by enhance the local ordering of the atomic structure at the interface region.hence, the atomic structure of such liquid layer is more ordered than that of the bulk liquid.\n\nThe effective volume of a cluster is considered much larger than the volume of the particles due to the lower packing fraction of the cluster. Since, heat can be transferred rapidly within the such clusters, the volume fraction of the highly conductive phase is larger than the volume of solid, thus increasing its thermal conductivity\n\nIn the last ten years, many experiments have been conducted numerically and analytically to validate the importance of nanofluids.\n\nFrom the table 1 it is clear that nanofluid-based collector have a higher efficiency than a conventional collector. So, it is clear that we can improve conventional collector simply by adding trace amounts of nano-particles.\nIt has also been observed through numerical simulation that mean outlet temperature increase by increasing volume fraction of nanoparticles, length of tube and decreases by decreasing velocity.\n\nNanofluids poses the following advantages as compared to conventional fluids which makes them suitable for use in solar collectors:\nThe fundamental difference between the conventional and nanofluid-based collector lies in the mode of heating of the working fluid. In the former case the sunlight is absorbed by a surface, where as in the latter case the sunlight is directly absorbed by the working fluid (through radiative transfer). On reaching the receiver the solar radiations transfer energy to the nanofluid via scattering and absorption.\n\n\n"}
{"id": "19179639", "url": "https://en.wikipedia.org/wiki?curid=19179639", "title": "Nuclear Weapons: The Road to Zero", "text": "Nuclear Weapons: The Road to Zero\n\nNuclear Weapons: The Road to Zero is a 1998 book edited by Joseph Rotblat. The book is based on the Pugwash Conferences on Science and World Affairs, and in particular on a detailed international study published in 1993 on the importance of, and practical mechanisms to, eliminate nuclear weapons. This monograph is a series of essays that describe the many complex technical, economic, legal and political issues involved. Contrary to the approach of nuclear powers -- that these weapons are needed for national security -- is the \"no longer fanciful dream\" of a nuclear-weapon-free world. Rotblat suggests that this is \"a sound and practical objective, which could be realized in the foreseeable future.\"\n\n"}
{"id": "33638007", "url": "https://en.wikipedia.org/wiki?curid=33638007", "title": "Pagami Creek Fire", "text": "Pagami Creek Fire\n\nThe Pagami Creek Fire was a wildfire in Northern Minnesota, United States, that began with a lightning strike on August 18, 2011. After weeks of slow growth, the wildfire quickly spread to over during several days of hot, dry, windy weather in mid-September. The fire spread beyond the Boundary Waters Canoe Area Wilderness to threaten homes and businesses. Smoke from the fires drifted east and south as far as the Upper Peninsula of Michigan, Ontario, and Chicago\n"}
{"id": "37671005", "url": "https://en.wikipedia.org/wiki?curid=37671005", "title": "Powder snow avalanche", "text": "Powder snow avalanche\n\nA powder snow avalanche is a type of avalanche where the snow grains are largely or wholly suspended by fluid turbulence. They are particle-laden gravity currents and closely related to turbidity currents, pyroclastic flows from volcanoes and dust storms in the desert. The turbulence is typically generated by the forward motion of the current along the lower boundary of the domain, the motion being in turn driven by the action of gravity on the density difference between the particle-fluid mixture and the ambient fluid. The ambient fluid is generally of similar composition to (and miscible with) the interstitial fluid, and is water for turbidity currents and air for avalanches. These flows are non-conservative in that they may exchange particles at the lower boundary by deposition or suspension, and may exchange fluid with the ambient by entrainment or detrainment. Such flows dissipate when the turbulence can no longer hold the particles in suspension and they are deposited on the lower boundary. When the turbulence is strong enough to suspend new material from the bed or the underlying dense flow then current is said to be auto-suspending. Particle concentrations in the suspension cloud are usually sufficiently low (0.1-7% by volume) that particle-particle interactions play a small or negligible role in maintaining the suspension. In powder snow avalanches, even at these low concentrations, the extra density of the suspended particles is large relative to that of air, so the Boussinesq approximation, where density differences are considered negligible in inertia terms, is invalid, so that the snow grains carry most of the flows momentum. This is in contrast to turbidity currents and laboratory experiments in water where the extra inertia of the particles can usually be neglected. Nonetheless, due to the extreme difficulty in estimating particle concentrations in natural flows there remains considerable uncertainty—and debate—concerning the particle loading in large submarine turbidity currents and the validity of the Boussinesq approximation.\n"}
{"id": "1244347", "url": "https://en.wikipedia.org/wiki?curid=1244347", "title": "Proton pack", "text": "Proton pack\n\nThe proton pack is an energy weapon used for weakening ghosts and aiding in capturing them within the \"Ghostbusters\" universe. First depicted in the film \"Ghostbusters\", it has a hand-held wand (\"Neutrona Wand\" or particle thrower) connected to a backpack-sized particle accelerator. It fires a stream of highly focused and radially polarized protons that electrostatically traps the negatively charged energy of a ghost, allowing it to be held in the stream.\n\nThe proton pack, designed by Dr. Egon Spengler, is a man-portable particle accelerator system that is used to create a charged particle beam—composed of protons—that is fired by the proton gun (also referred to as the \"neutrona wand\"). Described in the first movie as a \"positron collider\", it functions by colliding high-energy positrons to generate its proton beam. The beam allows a Ghostbuster to contain and hold \"negatively charged ectoplasmic entities\". This containment ability allows the wielder to position a ghost above a trap for capture. The name \"proton pack\" is not used in the original movie at all, and is not used until the subway tunnel scene in \"Ghostbusters II\", when Egon says that they should get their proton packs. The doorman to the Mayor's mansion also uses the term \"proton pack\", asking the Ghostbusters if he can buy one from them for his little brother. Egon replies that \"A proton pack is not a toy\".\n\nIn the 1984 novelization of \"Ghostbusters\" the Proton Packs are mention eg. “Stantz hauls out the formidable weaponry from the back of the Ectomobile and the four men kit themselves up, buckling on their proton packs…”\n\nWhile the Ghostbusters' dialogue indicates that the accelerator system operates similarly to a cyclotron (and indeed Dr. Peter Venkman refers to the proton packs in one scene as \"unlicensed nuclear accelerators\"), modern particle accelerators produce well collimated particle beams. This is far different from the beam from a proton pack, which tends to undulate wildly (though it still stays within the general area at which the user is aiming). The proton stream is quite destructive to physical objects, and can cause extensive property damage.\n\nIn the , Ray explains how the proton pack works early in the game; the energy emitted by the proton stream helps to dissipate psychokinetic (PK) energy which ghosts use to manifest themselves. Draining them of their PK energy weakens them, allowing them to be captured in their portable ghost traps.\n\nAccording to a line spoken by Egon in \"Ghostbusters II\", each pack's energy cell has a half-life of 5,000 years. Knobs on the main stock of the proton pack can perform various functions to customize the proton stream, including adjustments for stream intensity, length, and degrees of polarization. In the cartoon series \"The Real Ghostbusters\", the maximum power setting for the proton packs is \"500,000 MHz,\" which possibly refers to the rate of positron collisions occurring within the pack's accelerator system. In the cartoon the packs also have a self-destruct mechanism capable of affecting at least a half-mile radius. \"The Real Ghostbusters\" and the \"Extreme Ghostbusters\" also made proton packs less efficient with power cells, allowing them to run out of energy when appropriate for dramatic tension; in the latter show, the proton packs require replaceable power cartridges.\n\nThe IDW monthly \"Ghostbusters\" comic storyline has shown the movie pack, a boson dart capable pack and the Extreme Ghostbusters pack in use. The IDW comic also shows a proton pistol attachment to the movie pack being used by Winston while hunting down Slimer.\n\nCrossing the streams was initially discouraged, as Egon believed that \"total protonic reversal\" would occur: this effect would have catastrophic results (see quote above). However, in a desperate effort to stop the powerful Gozer the Gozerian, Egon noted that the door to Gozer's temple \"swings both ways\" and that by crossing the streams, they may be able to create enough force to close the door on Gozer and its control. As the Ghostbusters cross the streams, the combination of that much energy closes the door to Gozer's dimension and severs its ties to our world. The resulting blast destroys a good portion of the roof and blows up the Stay Puft Marshmallow Man.\n\nIn \"\", the Ghostbusters mention that \"crossing the streams\" during the Gozer Incident (events of the first \"Ghostbusters\" film) only worked due to the presence of a cross-dimension portal (a tactic which is referred to as the \"Gozer gambit\" by Ray) and should only be used as a last resort. During the game's climax, the Ghostbusters are pulled into Ivo Shandor's ghostly realm and come face-to-face with Shandor's Destructor form, forcing them to resort to \"crossing the streams\" to defeat Shandor. The resulting blast not only destroys Shandor but also sends the team flying back to their dimension. During gameplay, it is possible for the player to cross the streams with another Ghostbuster, but this will only cause a burst of energy to travel down the stream and deal a massive amount of damage to the player, also knocking them off their feet for a short time, due to a new \"safety\" that was installed on the neutrona wand.\n\nThe game features a modified version of the proton pack (an experimental prototype) which is given to the player (the Ghostbuster's new \"experimental equipment technician\"/guinea pig) for field testing. This new proton pack is equipped with other features (and upgrades) besides the standard proton stream.\n\nThe props representing proton packs were originally thought to have been made by the prop department of Columbia Pictures. Recent information coming from the auction of a hero proton pack in July 2012 revealed that the hero proton packs could have been made by Boss Film Studios, a prop studio started by ILM veteran Richard Edlund. They are made of molded fiberglass shells on aluminium backplates (or \"motherboards\") bolted to military surplus A.L.I.C.E. frames. The basic shape was sculpted from foam; later, a rubber mold was made of it, from which fiberglass shells were pulled. The \"wand\" had an extending barrel mechanism and the electronics were quite advanced for the time. They were then finished with various surplus 1960s resistors, pneumatic fittings, hoses and ribbon cable, and surplus warning labels and custom-made metal fittings. The overall weight of these props is said to be around . These \"hero\" props were substituted in stunt scenes by flimsy foam rubber pulls from the same mould. The proton packs have a lightbar with 15 blue scrolling lights in a box on the left-hand side and 4 red lights in the circular \"cyclotron\" portion of the bottom of the prop that light up in rotation. The \"wand\" also featured numerous light features; the most elaborate versions had fluorescent bargraphs, incandescent bulbs, and strobing flashes in the tip for the visual effects crew to synchronize the 'streams' to.\n\nThe \"GB1\" Hero proton packs were fiberglass shells mounted on aluminum motherboards with LC-1 ALICE Frames and straps. These packs had many aluminum parts on them, including: aluminum Ion Arm and cap, booster tube, injector tubes, HGA, vacuum line, PPD, Beamline and filler tube, as well as the N-Filter. The wands for these packs were also fully aluminum, minus the resin grips. All these external parts were pop riveted to the shell, which was then, in turn, mounted to the motherboard via L-Brackets and bolts at the four corners of the pack.\n\nThe \"GB1\" stunt packs were packs that were cast in foam to be worn during physical stunts performed in the film. Many of the attached pieces of the hero packs are cast on to these packs in foam. These packs also featured static lights and were attached to plywood motherboards with different straps compared to the \"GB1\" heroes. Other than the bumper, wand and hosing/ribbon cable, everything was cast into the foam shell of the pack.\n\nSome packs from\" Ghostbusters\" were used in the follow-up \"Ghostbusters II\"; these packs were slightly redressed with a black crank knob and thinner ribbon cable. The angle of the gun, or \"wand\" mount was changed to pitch forward slightly, in order to make the prop easier for the actor to use. In addition to these redressed props, one of the originals was hastily cast as a buck to produce basic lightweight \"midgrade\" props (as a solution to complaints by the actors about the weight of the original prop). These midgrade pieces featured many details cast in as part of the mould, instead of separate fittings. The electronics and mechanisms were also cut down greatly, reducing the total weight. The original \"GB1\" props would appear in close-ups, the midgrade in all other scenes, and new rubber \"stunt\" packs were made for whenever the actor needed to take a fall.\n\nThe proton pack props from the \"Ghostbusters\" movies are some of the most wanted and collectible props ever made. Several \"GB2\" packs have surfaced for auction. In September 2004, one rubber stunt and one fiberglass midgrade prop were auctioned by Profiles in History. The midgrade prop fetched well over $13,000. The \"semi-hero\" pack, affectionately known as \"Number Four\", was bought from the auction winner by Ken Heugel for cash and a Sean Bishop replica pack. The auction winner kept the Certificate of Authenticity. The \"Number 4\" pack was dissected and documented by Bishop and Heugel. The examination revealed much of the hurried and shoddy casting and assembly techniques used in its creation. The measurements of the proton pack taken during the examination are still used today by prop builders making their own proton packs. Unfortunately, \"Number 4\" was lost in 2005 during Heugel's move to Romania. The case that held the pack arrived in Romania empty. This theft was classified as \"an airline baggage mishap\". In July 2006, another \"Ghostbusters II\" mid-grade \"semi-hero\" pack with a Certificate of Authenticity was placed on auction by Profiles in History. This proton pack was later pulled when it was determined to be the Bishop replica pack matched with the real Certificate of Authenticity of the \"Number 4\". This replica pack is distinct for its unusually rusty paint job. In 2011 this pack was recovered when author Ernie Cline (\"Fanboys\", \"Ready Player One\") purchased it, returning it to Bishop's shop for a makeover prior to Cline's use of it on his 2011-2012 book signing tour.\n\nAll three variations of the \"GB2\" pack have been displayed at various Planet Hollywood restaurants around the U.S. The Hero packs in \"GB2\" were the same hero packs from \"GB1\" but retrofitted so they had matching parts with the new packs they had to construct for filming. For all intents and purposes, these packs are built the same way, only with some cosmetic differences, such as different ribbon cables, different colored crank knob, the use of nycoil banjos on the wands, etc. Other than that, they are just beat up versions of their former selves from the first film.\n\nHere are some obvious and subtle modifications done to the hero packs for \"GB2\":\n\nThe Semi-Hero packs in \"GB2\" were a new addition to the packs used in filming. These packs were stripped-down versions of the heroes, merging constructional concepts from both the heroes and the stunt packs into a mid grade pack for wide shots. These packs were also constructed lighter for the actors. many of the metal parts from the Hero packs were cast in resin and attached to the shell, such as the HGA, injector tubes, beamline and filler tube. Other metal parts, such as the Ion Arm, Booster Tube (and frame), PPD, and N-Filter, were actually cast into the shell, much like the foam stunt packs from the first film. These packs also had fiberglass cast wands, instead of the metal ones from the first film.\n\nThe \"GB2\" foam stunt packs seem to have been built somewhat better than the \"GB1\" foam stunt packs. The foam packs seem to be constructed the same way as the semi-heroes but instead of being cast as fiberglass, they were cast in foam instead. Once again, the Ion Arm, Booster Tube (and frame), PPD and N-Filter were cast onto the shell and part of the mold.\n\nBesides the video game, there is one more instance of Columbia/Sony having packs produced for production. That would be the packs made for the Universal Studios Florida stage show. As to how much lineage these packs actually have is still up in the air. If the master for these mold was retooled from a production mold, it was heavily modified. There is much extra material added around the EDA, both where the beamline and filler tubes go, as well as between the injector tubes and the EDA. At least the bottom half of the Booster tube and frame are cast into the shell as well as the N-Filter. There also seems to be much warpage at this point. These packs were also mounted on what seems like MDF motherboards and LC-2 ALICE frames.\n\nThe proton pack worn by Bill Murray during his acceptance speech at the 2010 Scream Awards was a reproduction semi-hero pack made by Bishop.\n\nIn July 2012, Profiles in History auctioned a \"Ghostbusters\" hero pack. This proton pack was reportedly worn by Harold Ramis. The source of the pack is suspected to be Sony. This \"Harold Pack\" shows some modifications made to the hero packs from \"Ghostbusters\" to \"Ghostbusters II\". The Harold Pack also showed some damaged and missing parts, most notably a missing crank knob and a broken N-Filter. There is also a clear tube from the \"Ghostbusters\" uniform shown with the Harold Pack. It is unknown if the electronics inside the pack are in working order. The \"Harold Pack\" sold for $130,000. The same Profiles in History auction also features the \"Ghostbusters\" ghost trap and pedal used when Slimer is captured in the hotel ballroom. This \"Slimer Trap\" is supposed to have a working pedal mechanism, but the operational status is also unknown. The \"Slimer Trap\" sold for $60,000.\n\nMany movie prop replica communities (prop forums) have sprung up regarding proton pack research and contain various methods and plans for constructing a replica proton pack. Plans, methods, parts and advice can be obtained from the members of these prop forums to help build a proton pack out of simple cardboard to movie accurate aluminum and fiberglass (shell) hero-pack replicas. Some of the more complex proton pack replicas being built have with lights and sound. These prop forums also have information on other \"Ghostbusters\" props from the ghost trap, to the slime blower, to the Ectomobile. Some of these prop forums also have members that sell replica shells, motherboards and other parts necessary to make a replica proton pack.\n\nEarly script descriptions of the proton pack stated each pack had two neutrona wands, strapped to the wrists, rather than one held in a fashion similar to an assault rifle. Toy proton packs were formerly made by Kenner and became available in toy shops. They consisted of a plastic hollow pack and gun, with a yellow foam cylinder attached to the front of the gun to represent the beam.\n"}
{"id": "48707385", "url": "https://en.wikipedia.org/wiki?curid=48707385", "title": "Proton tunneling", "text": "Proton tunneling\n\nProton tunneling is a type of quantum tunneling involving the instantaneous disappearance of a proton in one site and the appearance of the same proton at an adjacent site separated by a potential barrier. The two available sites are bounded by a double well potential of which its shape, width and height are determined by a set of boundary conditions. According to the WKB approximation, the probability for a particle to tunnel is inversely proportional to its mass and the width of the potential barrier. Electron tunneling is well-known. A proton is about 2000 times more massive than an electron, so it has a much lower probability of tunneling; nevertheless, proton tunneling still occurs especially at low temperatures and high pressures where the width of the potential barrier is decreased.\n\nProton tunneling is usually associated to hydrogen bonds. In many molecules that contain hydrogen, the hydrogen atoms are linked to two non-hydrogen atoms via a hydrogen bond at one end and a covalent bond at the other. A hydrogen atom without its electron is reduced to being a proton. Since the electron is no longer bound to the hydrogen atom in a hydrogen bond, this is equivalent to a proton resting in one of the wells of a double well potential as described above. When proton tunneling occurs, the hydrogen bond and covalent bonds are switched. Once proton tunneling occurs, the same proton has the same probability of tunneling back to its original site provided the double well potential is symmetrical.\n\nThe base pairs of a DNA strand are connected by hydrogen bonds. In essence, the genetic code is contained by a unique arrangement of hydrogen bonds. It is believed that upon the replication of a DNA strand there is a probability for proton tunneling to occur which changes the hydrogen bond configuration; this leads to a slight alteration of the hereditary code which is the basis of mutations. Likewise, proton tunneling is also believed to be responsible for the occurrence of the dysfunction of cells (tumors and cancer) and ageing.\n\nProton tunneling occurs in many hydrogen based molecular crystals such as ice. It is believed that the phase transition between the hexagonal (ice Ih) and orthorhombic (ice XI) phases of ice is enabled by proton tunneling. The occurrence of correlated proton tunneling in clusters of ice has also been reported recently.\n\n"}
{"id": "3777779", "url": "https://en.wikipedia.org/wiki?curid=3777779", "title": "Reactor building", "text": "Reactor building\n\nA reactor building is a general term for a building that houses a reactor of some type. In particular, it often refers to a building containing a nuclear reactor. This can also be used to refer to pressure sealed buildings containing nuclear reactors, though it would be more correct to call them containment buildings.\n\nAlthough some buildings containing nuclear reactors are containment buildings, not all are. For instance, the Soviet RBMK reactors were not housed in containment buildings. This greatly increased the severity of the Chernobyl accident on April 26, 1986.\n\n"}
{"id": "13473605", "url": "https://en.wikipedia.org/wiki?curid=13473605", "title": "Renewable electricity in New Zealand", "text": "Renewable electricity in New Zealand\n\nRenewable electricity in New Zealand is primarily from hydropower. In 2015, 83% of the electricity generated in New Zealand came from renewable sources. In September 2007, former Prime Minister Helen Clark announced a national target of 90% renewable electricity by 2025, with wind energy to make up much of that increase.\n\nThe resource consent process has favoured non-renewable energy projects. This century, thermal projects have received consents (Rodney, Stratford) while several renewable energy projects have had consent applications declined (Awhitu, Mokau hydro, Mount Cass, Project Hayes, Te Waka, Waitahora).\n\nGeothermal power is a significant part of the electrical energy generation capacity of the country, providing approximately 16% of the country's electricity with installed capacity of 971 MW. New Zealand, like only a small number of other countries worldwide, has numerous geothermal sites that could be developed for exploitation, and also boasts some of the earliest large-scale use of geothermal energy in the world.\n\nAs of December 2014, New Zealand had an installed wind generation capacity of 683 MW. The first generation of Wind power into the national grid was in 1992 and it now provides approximately 5% of the country's electricity demand. Consents have been granted for wind farms with a further capacity of 2,200 MW.\n\nNew Zealand has large ocean energy resources but does not yet generate any power from them. TVNZ reported in 2007 that over 20 wave and tidal power projects are currently under development. However, not a lot of public information is available about these projects. The Aotearoa Wave and Tidal Energy Association was established in 2006 to \"promote the uptake of marine energy in New Zealand\". According to their latest newsletter, they have 59 members. However the association doesn't list these members or provide any details of projects.\nFrom 2008 to 2011, the government Energy Efficiency and Conservation Authority is allocating $2 million each year from a Marine Energy Deployment Fund, set up to encourage the utilisation of this resource.\n\nThe greater Cook Strait and Kaipara Harbour seem to offer the most promising sites for using underwater turbines. Two resource consents have been granted for pilot projects in Cook Strait itself and in the Tory Channel, and consent has been granted for up to 200 tidal turbines at the Kaipara Tidal Power Station. Other potential locations include the Manukau and Hokianga Harbours, and French Pass. The harbours produce currents up to 6 knots with tidal flows up to 100,000 cubic metres a second. These tidal volumes are 12 times greater than the flows in the largest New Zealand rivers.\n\nA glance at these figures shows that for the stated target to be attained, current thermal generation will have to be decommissioned and not replaced. Even if the annual total consumption doubles from the 2005 figure ( as it did from 1975 to 2005 ), with all additional generation met by non-thermal means, a static thermal generation of 14,305 on a total of 83,340 is still 17 percent. The most likely candidate for decommissioning would be the 1000 MW Huntly power station, a coal and gas power station that was commissioned in 1983.\n\nWhile there are some non-technical barriers to the widespread use of renewables, global warming concerns coupled with high oil prices and increasing government support are driving increasing growth in the renewable energy industries.\n\nTo boost the renewables ratio, geothermal power is counted as renewable, although the geothermal heat reservoirs are eventually depleted (observably so at Wairakei, the longest-running) and they are not emissions-free. For example, the Ngawha geothermal field emits an unusually high amount of CO (350 tonnes CO per GWh ), and the geothermal fields plus natural hot springs draining to the Waikato river deliver sufficient arsenic to render the water unsafe to drink without special treatment. Re-injection of the waste geothermal fluid can reduce these problems (even extending the life of the field), but involves additional expense.\n\n"}
{"id": "1241487", "url": "https://en.wikipedia.org/wiki?curid=1241487", "title": "Silverstone (plastic)", "text": "Silverstone (plastic)\n\nSilverstone is a non-stick plastic coating made by DuPont. Released in 1976, this three-coat (primer/midcoat/topcoat) fluoropolymer system formulated with PTFE and PFA produces a more durable finish than Teflon coating.\n\nA reinforced version with higher scratch and abrasion resistance is also available. Maximum continuous use temperature is 290°C (550°F).\n\nTeflon is DuPont's registered trademark for its non-stick coatings. Teflon coatings are specially formulated finishes that are based on PTFE, PFA, FEP, and ETFE fluorocarbon resins. Teflon-S is a related family of fluorocarbon coatings containing binding resins which provide increased hardness, abrasion resistance, and other desirable properties.\n\n"}
{"id": "215108", "url": "https://en.wikipedia.org/wiki?curid=215108", "title": "Slinky", "text": "Slinky\n\nA Slinky is a toy precompressed helical spring invented by Richard James in the early 1940s. It can perform a number of tricks, including travelling down a flight of steps end-over-end as it stretches and re-forms itself with the aid of gravity and its own momentum, or appear to levitate for a period of time after it has been dropped. These interesting characteristics have contributed to its success as a toy in its home country of the United States, resulting in many popular toys with slinky components in a wide range of countries.\n\nAndy Davis' in 1943 and demonstrated at Gimbels department store in Philadelphia in November 1945. The toy was a hit, selling its entire inventory of 400 units in ninety minutes. James and his wife Betty formed James Industries in Clifton Heights, Pennsylvania to manufacture Slinky and several related toys such as the Slinky Dog and Suzie, the Slinky Worm. In 1960, James's wife Betty became president of James Industries, and, in 1964, moved the operation back to Hollidaysburg, Pennsylvania. In 1998, Betty James sold the company to Poof Products, Inc.\n\nSlinky was originally priced at $1, but many paid much more due to price increases of spring steel throughout the state of Pennsylvania; it has, however, remained modestly priced throughout its history as a result of Betty James' concern about the toy's affordability for poor customers. Slinky has been used other than as a toy in the playroom: it has appeared in the classroom as a teaching tool, in wartime as a radio antenna, and in physics experiments with NASA. Slinky was inducted into the National Toy Hall of Fame at The Strong in Rochester, New York, in 2000. In 2002, Slinky became Pennsylvania's official state toy, and, in 2003, was named to the Toy Industry Association's \"Century of Toys List.\" In its first 60 years Slinky sold 300 million units.\n\nIn 1943, Richard James, a naval mechanical engineer stationed at the William Cramp & Sons shipyards in Philadelphia, was developing springs that could support and stabilize sensitive instruments aboard ships in rough seas. James accidentally knocked one of the springs from a shelf, and watched as the spring \"stepped\" in a series of arcs to a stack of books, to a tabletop, and to the floor, where it re-coiled itself and stood upright. James's wife Betty later recalled, \"He came home and said, 'I think if I got the right property of steel and the right tension; I could make it walk.'\" James experimented with different types of steel wire over the next year, and finally found a spring that would walk. Betty was dubious at first, but changed her mind after the toy was fine-tuned and neighborhood children expressed an excited interest in it. She dubbed the toy Slinky (meaning \"sleek and graceful\"), after finding the word in a dictionary, and deciding that the word aptly described the sound of a metal spring expanding and collapsing.\n\nWith a US$500 loan, the couple formed James Industries (originally James Spring & Wire Company), had 400 Slinky units made by a local machine shop, hand-wrapped each in yellow paper, and priced them at $1 a piece. Each was 2\" tall, and included 98 coils of high-grade blue-black Swedish steel. The Jameses had difficulty selling Slinky to toy stores but, in November 1945, they were granted permission to set up an inclined plane in the toy section of Gimbels department store in Philadelphia to demonstrate the toy. Slinky was a hit, and the first 400 units were sold within ninety minutes. In 1946, Slinky was introduced at the American Toy Fair.\n\nRichard James opened shop in Albany, New York, after developing a machine that could produce a Slinky within seconds. The toy was packaged in a black-lettered box, and advertising saturated America. James often appeared on television shows to promote Slinky. In 1952, the Slinky Dog debuted. Other Slinky toys introduced in the 1950s included the Slinky train Loco, the Slinky worm Suzie, and the Slinky Crazy Eyes, a pair of glasses that uses Slinkys over the eyeholes attached to plastic eyeballs. James Industries licensed the patent to several other manufacturers including Wilkening Mfg. Co. of Philadelphia and Toronto which produced spring-centered toys such as Mr. Wiggle's Leap Frog and Mr. Wiggle's Cowboy. In its first 2 years, James Industries sold 100 million Slinkys (At $1 apiece, that would be the equivalent to 6 Billion, adjusted for inflation, in gross revenue over those 5 years).\n\nIn 1960 Richard James left the company after his wife filed for divorce and he became an evangelical missionary in Bolivia with Wycliffe Bible Translators. Betty James managed the company, juggled creditors, and in 1964 moved the company to Hollidaysburg, Pennsylvania. Richard James died in 1974. The company and its product line expanded under Betty James’s leadership. In 1995, she explained the toy's success to the Associated Press by saying, \"It's the simplicity of it.\"\n\nBetty James insisted upon keeping the original Slinky affordable. In 1996, when the price ranged from $1.89 to $2.69, she told \"The New York Times\": “So many children can’t have expensive toys, and I feel a real obligation to them. I’m appalled when I go Christmas shopping and $60 to $80 for a toy is nothing.” In 2008, Slinkys cost $4 to $5, and Slinky Dogs about $20.\n\nIn 1998 James Industries was sold to Poof Products, Inc. of Plymouth, Michigan, a manufacturer of foam sports balls. Slinky continued production in Hollidaysburg. In 2003, James Industries merged with Poof Products, Inc., to create Poof-Slinky, Inc.\n\nBetty James died of congestive heart failure in November 2008, age 90, after having served as president of James Industries from 1960 to 1998. Over 300 million Slinkys have been sold between 1945 and 2005, and the original Slinky is still a bestseller.\n\nThe rules that govern the mechanics of a slinky are Hooke's law and the effects of gravitation.\n\nDue to simple harmonic motion the period of oscillation of a dangling slinky is\nWhere \"T\" is the time of the period of oscillation, \"m\" is the mass of the slinky and \"k\" is the spring constant of the slinky.\n\nIn the state of equilibrium of a slinky, all net force is cancelled throughout the entire slinky. This results in a stationary slinky with zero velocity. As the positions of each part of the slinky is governed by the slinky's mass, the force of gravity and the spring constant, various other properties of the slinky may be induced. The length of an idealized slinky extended under its own weight, assuming the fully compressed length is negligible, is\nWhere \"L\" is the length of the slinky, \"W\" is the weight of the slinky, and \"k\" is the spring constant of the slinky.\n\nDue to the effect of gravity, the slinky appears bunched up towards the bottom end, as governed by the equation\nWhere \"n\" is a dimensionless variable, 0 ≤ \"n\" ≤ 1, with \"n\" = 0 corresponding to the top of the slinky and \"n\" = 1 being the bottom. Each intermediate value of \"n\" corresponds to the proportion of the slinky's mass above that point \"n\", and \"p\"(\"n\") gives the position that \"n\" is above the bottom of the slinky.\n\nThis quadratic equation means that rather than the center of mass being at the middle of the slinky, it lies one quarter of the length above the bottom end,\n\nWhen set in motion on a stepped platform such as a stairway, the slinky transfers energy along its length in a longitudinal wave. The whole spring descends end over end in a periodical motion, as if it were somersaulting down one step at a time.\n\nWhen the top end of the Slinky is dropped, the information of the tension change must propagate to the bottom end before both sides begin to fall; the top of an extended Slinky will drop while the bottom initially remains in its original position, compressing the spring. This creates a suspension time of ~0.3 s for an original Slinky, but has potential to create a much larger suspension time.\nA suspended Slinky's center of mass is accelerating downward at 16 feet per second (i.e. g); when released - the lower portion moves up toward the top portion with an equivalent, constant upward acceleration as the tension is relieved. As the spring contracts, every point along its length will accelerate downward with gravity and tension, and experience a decrease in overall downward acceleration related to height along the spring due to the spring force changing with extension- at the bottom of the spring the upward initial acceleration reduces in accordance with Hooke's law as the spring contracts, but the centre toward which it is moving gets closer- meaning the base will have been displaced sufficiently toward the centre of inertial mass for it to appear to have hung still. Should this phenomenology extend to very light strings with heavy suspended masses (which have approximately linear tension distributions), different mathematics would be needed to explain the phenomenon.\n\nThe famous jingle for the Slinky television commercial was created in Columbia, South Carolina in 1962 with Johnny McCullough and Homer Fesperman writing the music and Charles Weagly penning the lyrics. It became the longest-running jingle in advertising history.\n\nThe jingle has itself been parodied and referenced in popular culture. It is seen in the \"Log\" commercial on \"The Ren & Stimpy Show\" and sung by actor Jim Carrey in \"\". The song is also referenced in the movie \"Lords of Dogtown\", where it is sung in full by Emile Hirsch, and is sung by Eddie Murphy as part of the final routine in the stand-up comedy film \"Eddie Murphy Raw\".\n\nEarly in the history of James Industries, Helen (Herrick) Malsed of Washington State sent the company a letter and drawings for developing Slinky pull-toys. The company liked her ideas, and Slinky Dog and Slinky Train were added to the company's product line. Slinky Dog, a small plastic dog whose front and rear ends were joined by a metal Slinky, debuted in 1952. Malsed received royalties of $60,000 to $70,000 annually for 17 years on her patent for the Slinky pull-toy idea, but never visited the plant.\n\nIn 1995 the Slinky Dog was redesigned for all of Pixar’s \"Toy Story\". James Industries had discontinued their Slinky Dog a few years previously. Betty James approved of the new Slinky Dog, telling the press, \"The earlier Slinky Dog wasn’t nearly as cute as this one.\" The molds used in manufacturing the new toy created problems for James Industries, so the plastic front and rear ends were manufactured in China with James Industries doing the assembly and packaging. The entire run of 825,000 redesigned Slinky Dogs sold out well before Christmas 1995.\n\nPlastic Slinkys are also available. They can be made in different colors. Many of them are made with the colors of the rainbow in rainbow order. They were marketed in the 1970s as a safer alternative to metal slinkys as they did not present a hazard when inserted into electrical sockets. The plastic spring toy, known as the Plastic Slinky was invented by Donald James Reum Sr. of Master Mark Plastics in Albany, Minnesota. Reum came up with the idea as he was playing with different techniques to produce a spiral hose for watering plants. However, as it came off the assembly line, according to his children, it looked more like a \"Slinky.\" He worked at it until it came out perfectly and then went to Betty James with his prototype. Reum manufactured the Plastic Slinky for Betty James for several years. Eventually Betty James decided to manufacture the product exclusively through James manufacturing, effectively ending the production of the toy by the small Minnesota company. Reum’s patent number, 4120929 was filed on Dec 28, 1976 and issued by the US Patent Office on Oct 17, 1978.\n\nIn 1999, the United States Postal Service issued a Slinky postage stamp. The Slinky was inducted into the National Toy Hall of Fame in 2000 in their Celebrate the Century stamp series. A bill to nominate the slinky as the state toy of Pennsylvania was introduced by Richard Geist in 2001 but not enacted. The same year, Betty James was inducted into the Toy Industry Association's Hall of Fame. In 2003, Slinky was named to the Toy Industry Association's \"Century of Toys List\", a roll call of the 100 most memorable and most creative toys of the twentieth century.\n\nHigh school teachers and college professors have used Slinkys to simulate the properties of waves, United States troops in the Vietnam War used them as mobile radio antennas (as have amateur radio operators), and NASA has used them in zero-gravity physics experiments in the Space Shuttle.\n\nSlinkys and similar springs can be used to create a 'laser gun' like sound effect.\nThis is done by holding up a slinky in the air and striking one end, resulting in a metallic tone which sharply lowers in pitch. The effect can be amplified by attaching a plastic cup to one end of the Slinky.\n\nIn 1959, John Cage composed an avant garde work called \"Sounds of Venice\" scored for (among other things) a piano, a slab of marble and Venetian broom, a birdcage of canaries, and an amplified Slinky.\n\nIn 1985 in conjunction with the Johnson Space Center and the Houston Museum of Natural Science, Discovery astronauts created a video demonstrating how familiar toys behave in space. \"It won't slink at all,\" Dr. M. Rhea Seddon said of Slinky, \"It sort of droops.\" The video was prepared to stimulate interest in school children about the basic principles of physics and the phenomenon of weightlessness.\n\nIn 1992, the Bishop Museum in Honolulu, Hawaii, hosted an interactive traveling exhibit developed by the Franklin Institute of Philadelphia, called \"What Makes Music?\" Among other things, visitors could examine what makes musical sound by creating waves on an eight-foot-long version of a Slinky toy.\n\n\n"}
{"id": "22302668", "url": "https://en.wikipedia.org/wiki?curid=22302668", "title": "State Energy Program (United States)", "text": "State Energy Program (United States)\n\nThe United States Department of Energy's State Energy Program (SEP) provides grants to states and directs funding to state energy offices from technology programs in Office of Energy Efficiency and Renewable Energy. States use grants to address their energy priorities and program funding to adopt emerging renewable energy and energy efficiency technologies. \n\nThe State Energy Program $3 billion funding will be used to provide rebates to consumers for home energy audits or other energy-saving improvements; to develop renewable energy; to promote Energy Star products; to upgrade the energy efficiency of state and local government buildings; and other innovative state efforts to help families save money on their energy bills. The energy efficiency upgrades are to be available for families making up to 200% of the federal poverty level.\n\n"}
{"id": "39498136", "url": "https://en.wikipedia.org/wiki?curid=39498136", "title": "Trifluoromethylsulfur pentafluoride", "text": "Trifluoromethylsulfur pentafluoride\n\nTrifluoromethylsulfur pentafluoride, CFSF, is a rare industrial greenhouse gas. It was first identified in the atmosphere in 2000. Trifluoromethylsulfur pentafluoride is considered to be one of the several \"super-greenhouse gases\". On a per molecule basis, it is considered to be the most potent greenhouse gas present in Earth's atmosphere, having a global warming potential of about 18,000 times that of carbon dioxide. The chemical is predicted to have a lifetime of 800 years in the atmosphere. However, the current concentration of trifluoromethylsulfur pentafluoride remains at a level that is unlikely to measurably contribute to global warming. The presence of the gas in the atmosphere is attributed to anthropogenic sources, possibly a by-product of the manufacture of fluorochemicals, originating from reactions of SF with fluoropolymers used in electronic devices and in microchips, or the formation can be associated with high voltage equipment created from SF (a breakdown product of high voltage equipment) reacting with CF to form the CFSF molecule. The chemistry of this compound is similar to that of sulfur hexafluoride (SF).\n"}
{"id": "7206824", "url": "https://en.wikipedia.org/wiki?curid=7206824", "title": "Truck scale", "text": "Truck scale\n\nA Truck scale (US), weighbridge (non-US) or railroad scale is a large set of scales, usually mounted permanently on a concrete foundation, that is used to weigh entire rail or road vehicles and their contents. By weighing the vehicle both empty and when loaded, the load carried by the vehicle can be calculated.\n\nThe key component that uses a weighbridge in order to make the weigh measurement is load cells.\n\nCommercial scales have to be National Type Evaluation Program (NTEP) approved or certified. The certification is issued by the National Conference on Weights and Measures (NCWM), in accordance to the National Institute of Standards and Technology (NIST), \"Handbook 44\" specifications and tolerances, through Conformity Assessment and the Verified Conformity Assessment Program (VCAP)\n\nHandbook 44: General Code paragraph G-A.1.; and the NIST Handbook 130 (Uniform Weights and Measures Law; Section 1.13.) define Commercial Weighing and Measuring Equipment as follows;\n\nNTEP approved scales are generally considered those scales which are intended by the manufacturer for use in commercial\napplications where products are sold by weight. NTEP Approved is also known as \"Legal for Trade\" or complies with Handbook 44. NTEP scales are commonly used for applications ranging from weighing coldcuts at the deli, to fruit at the roadside farm stand, shipping centers for determining shipping cost to weighing gold and silver and more.\n\n\nTruck scales can be surface mounted with a ramp leading up a short distance and the weighing equipment underneath or they can be pit mounted with the weighing equipment and platform in a pit so that the weighing surface is level with the road. They are typically built from steel or concrete and by nature are extremely robust.\n\nIn earlier versions the bridge is installed over a rectangular pit that contains levers that ultimately connect to a balance mechanism. The most complex portion of this type is the arrangement of levers underneath the weighbridge since the response of the scale must be independent of the distribution of the load. Modern devices use multiple load cells that connect to an electronic equipment to totalize the sensor inputs. In either type of semi-permanent scale the weight readings are typically recorded in a nearby hut or office.\n\nMany weighbridges are now linked to a PC which runs truck scale software capable of printing tickets and providing reporting features.\n\nTruck scales can be used for two main purposes:\n\n\nThey are used in industries that manufacture or move bulk items, such as in mines or quarries, garbage dumps / recycling centers, bulk liquid and powder movement, household goods, and electrical equipment. Since the weight of the vehicle carrying the goods is known (and can be ascertained quickly if it is not known by the simple expedient of weighing the empty vehicle) they are a quick and easy way to measure the flow of bulk goods in and out of different locations.\n\nA single axle truck scale or axle weighing system can be used to check individual axle weights and gross vehicle weights to determine whether the vehicle is safe to travel on the public highway without being stopped and fined by the authorities for being overloaded. Similar to the full size truck scale these systems can be pit mounted with the weighing surface flush to the level of the roadway or surface mounted.\n\nFor many uses (such as at police over the road truck weigh stations or temporary road intercepts) weighbridges have been largely supplanted by simple and thin electronic weigh cells, over which a vehicle is slowly driven. A computer records the output of the cell and accumulates the total vehicle weight. By weighing the force of each axle it can be assured that the vehicle is within statutory limits, which typically will impose a total vehicle weight, a maximum weight within an axle span limit and an individual axle limit. The former two limits ensure the safety of bridges while the latter protects the road surface.\n\nPortable truck scales can also be found in use around the world. A portable truck scale will have lower frame work that can be placed on non-typical surfaces such as dirt. These scales retain the same level of accuracy as a pit-type scale, with accuracy of up to + or - 1%. The first portable truck scale record in the US was units operated by the \"Weight Patrol\" of the Los Angeles Motor Patrol in 1929. Four such weighing units were used with one under each of the trucks wheels. Each unit could record up to .\n\n\nTypes of Weighbridge & images of different type of weighbridges"}
{"id": "20916812", "url": "https://en.wikipedia.org/wiki?curid=20916812", "title": "Turkish Airlines Flight 278", "text": "Turkish Airlines Flight 278\n\nTurkish Airlines Flight 278, operated by a Boeing 737-4Y0 registered TC-JES and named \"Mersin\", was a domestic scheduled flight from Ankara Esenboğa Airport to Van Ferit Melen Airport in eastern Turkey that crashed on 29 December 1994 during its final approach to land in driving snow. Five of the seven crew and 52 of the 69 passengers lost their lives, while two crew members and 17 passengers survived with serious injuries.\n\nThe aircraft, a Boeing 737-4Y0 with two CFMI CFM56-3C1 jet engines, was built by Boeing with manufacturer serial number 26074/2376, and made its first flight on 25 September 1992.\n\nAt 15:30 EET (13:30 UTC), TK278 struck a hill near Edremit district of Van Province at AMSL around from Van Airport while on a third VOR-DME approach to the Runway 03 in bad weather despite a warning from air traffic control not to attempt any more approaches in a snowstorm. The visibility was reducing to in heavy driving snow.\n\nIt was the worst aviation accident involving a Boeing 737-400 at that time. It was subsequently surpassed by Adam Air Flight 574 which crashed on 1 January 2007 with 102 fatalities, and fourth worst aircraft accident in Turkey at that time.\n\nThe aircraft had a crew of 7 and 69 passengers including two babies. Two of the crew and 17 passengers survived the crash with serious injuries.\n\n"}
