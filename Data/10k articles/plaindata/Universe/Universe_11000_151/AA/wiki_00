{"id": "55295948", "url": "https://en.wikipedia.org/wiki?curid=55295948", "title": "1,1-Dimethyldiborane", "text": "1,1-Dimethyldiborane\n\n1,1-Dimethyldiborane is the organoboron compound with the formula (CH)B(μ-H)BH. A pair of related 1,2-dimethyldiboranes are also known. It is a colorless gas that ignites in air.\n\nThe methylboranes were first prepared by H. I. Schlesinger and A. O. Walker in the 1930s. Methylboranes are formed by the reaction of diborane and trimethylborane. This reaction produces four different substitution of methyl with hydrogen on diborane. Produced are 1-methyldiborane, 1,1-dimethyldborane, 1,1,2-trimethyldiborane, and 1,1,2,2-tetramethyldiborane. \n\nTetramethyl lead reacts with diborane in a 1,2-dimethoxyethane solvent at room temperature to make a range of methyl substituted diboranes, ending up at trimethylborane, but including 1,1-dimethyldiborane, and trimethyldiborane. The other outputs of the reaction are hydrogen gas and lead metal.\n\nOther methods to form methyldiboranes include heating trimethylborane with hydrogen. Alternatively trimethylborane reacts with borohydride salts with in the presence of hydrogen chloride, aluminium chloride, or boron trichloride. If the borohydride is sodium borohydride, then methane is a side product. If the metal is lithium then no methane is produced. dimethylchloroborane and methyldichloroborane are also produced as gaseous products.\n\nWhen CpZr(CH) reacts with borane dissolved in tetrahydrofuran, a borohydro group inserts into the zirconium carbon bond, and methyl diboranes are produced.\n\nIn ether dimethylcalcium reacts with diborane to produce dimethyldiborane and calcium borohydride:\n\n1,2-dimethyldiborane slowly converts on standing to 1,1-dimethyldiborane.\n\nGas chromatography can be used to determine the amounts of the methyl boranes in a mixture. The order they elute are diborane, monomethyldiborane, trimethylborane, 1,1-dimethyldiborane, 1,2-dimethyldiborane, trimethyldiborane, and last tetramethyldiborane.\n\n1,1-Dimethyldiborane has a dipole moment of 0.87 d. The predicted heat of formation for the liquid is ΔH=-31 kcal/mol, and for the gas -25 kcal/mol. Heat of vapourisation was measured at 5.5 kcal/mol.\n\nAt -78.5 °C methyldiborane disproportionates slowly first to diborane and 1,1-dimethyldiborane. In solution methylborane is more stable against disproportionation than dimethylborane.\n\nTrimethyldiborane partially disproportionates over a period of hours at room temperature to yield tetramethyldiborane and 1,2-dimethyldiborane. Over a period of weeks 1,1-dimethyldiborane appears as well.\n\nGentler oxidation of 1,1-dimethyldiborane at 80 °C yields 2,5-dimethyl-1,3,4-trioxadiboralane, a volatile liquid that contains a ring of two boron and three oxygen atoms. An intermediate in this reaction is two molecules of dimethylborylhydroperoxide (CH)BOOH. (CAS 41557-62-5) When methyldiborane is oxidised around 150 °C a similar substance methyltrioxadiboralane is produced. At the same time dimethyltrioxadiboralane and trimethylboroxine are also formed, and also hydrocarbons, diborane, hydrogen, and dimethoxyborane (dimethyl methylboronic ester).\n"}
{"id": "35291341", "url": "https://en.wikipedia.org/wiki?curid=35291341", "title": "7199", "text": "7199\n\nThe 7199 is a vacuum tube, combining a pentode and triode. Typically, the pentode was used for the input stage, and the triode as a phase inverter. The tube was used in a number of American guitar amplifiers; the Gibson Guitar Corporation, for instance, used the 7199 in 1961's Falcon for the reverb circuit. Ampeg also used the 7199 extensively. Notable is the Dynaco ST-70 stereo amplifier introduced in 1959 which used a 7199 tube in the driver section of each channel. Over the next decade more than 350,000 of these amplifiers were produced. American 7199 production ended sometime in the 1980s while the Soviet tube company Sovtek produced one until roughly 2007. As a result the tube is becoming increasingly scarce. Another tube of the same type found in far more plentiful supply is the 6U8A, which is electrically identical, but with a different pinout configuration. The 6U8A can be substituted for a 7199 with a slight modification to the wiring of its tube socket.\n"}
{"id": "1363", "url": "https://en.wikipedia.org/wiki?curid=1363", "title": "André-Marie Ampère", "text": "André-Marie Ampère\n\nAndré-Marie Ampère (; ; 20 January 177510 June 1836) was a French physicist and mathematician who was one of the founders of the science of classical electromagnetism, which he referred to as \"electrodynamics\". He is also the inventor of numerous applications, such as the solenoid (a term coined by him) and the electrical telegraph. An autodidact, Ampère was a member of the French Academy of Sciences and professor at the École polytechnique and the Collège de France.\n\nThe SI unit of measurement of electric current, the ampere, is named after him. His name is also one of the 72 names inscribed on the Eiffel Tower.\n\nAndré-Marie Ampère was born on 20 January 1775 to Jean-Jacques Ampère, a prosperous businessman, and Jeanne Antoinette Desutières-Sarcey Ampère, during the height of the French Enlightenment. He spent his childhood and adolescence at the family property at Poleymieux-au-Mont-d'Or near Lyon. Jean-Jacques Ampère, a successful merchant, was an admirer of the philosophy of Jean-Jacques Rousseau, whose theories of education (as outlined in his treatise Émile) were the basis of Ampère's education. Rousseau believed that young boys should avoid formal schooling and pursue instead an \"education direct from nature.\" Ampère's father actualized this ideal by allowing his son to educate himself within the walls of his well-stocked library. French Enlightenment masterpieces such as Georges-Louis Leclerc, comte de Buffon's \"Histoire naturelle, générale et particulière\" (begun in 1749) and Denis Diderot and Jean le Rond d'Alembert's \"Encyclopédie\" (volumes added between 1751 and 1772) thus became Ampère's schoolmasters. The young Ampère, however, soon resumed his Latin lessons, which enabled him to master the works of Leonhard Euler and Daniel Bernoulli.\n\nIn addition, Ampère used his access to the latest books to begin teaching himself advanced mathematics at age 12. In later life Ampère claimed that he knew as much about mathematics and science when he was eighteen as ever he knew; but, a polymath, his reading embraced history, travels, poetry, philosophy, and the natural sciences. His mother was a devout woman, so Ampère was also initiated into the Catholic faith along with Enlightenment science. The French Revolution (1789–99) that began during his youth was also influential: Ampère's father was called into public service by the new revolutionary government, becoming a justice of the peace in a small town near Lyon. When the Jacobin faction seized control of the Revolutionary government in 1792, his father Jean-Jacques Ampère resisted the new political tides, and he was guillotined on 24 November 1793, as part of the Jacobin purges of the period.\n\nIn 1796 Ampère met Julie Carron, and in 1799 they were married. André-Marie Ampère took his first regular job in 1799 as a mathematics teacher, which gave him the financial security to marry Carron and father his first child, Jean-Jacques (named after his father), the next year. (Jean-Jacques Ampère eventually achieved his own fame as a scholar of languages). Ampère's maturation corresponded with the transition to the Napoleonic regime in France, and the young father and teacher found new opportunities for success within the technocratic structures favoured by the new French First Consul. In 1802 Ampère was appointed a professor of physics and chemistry at the École Centrale in Bourg-en-Bresse, leaving his ailing wife and infant son Jean-Jacques Antoine Ampère in Lyon. He used his time in Bourg to research mathematics, producing \"Considérations sur la théorie mathématique de jeu\" (1802; \"Considerations on the Mathematical Theory of Games\"), a treatise on mathematical probability that he sent to the Paris Academy of Sciences in 1803.\n\nAfter the death of his wife in July 1803, Ampère moved to Paris, where he began a tutoring post at the new École Polytechnique in 1804. Despite his lack of formal qualifications, Ampère was appointed a professor of mathematics at the school in 1809. As well as holding positions at this school until 1828, in 1819 and 1820 Ampère offered courses in philosophy and astronomy, respectively, at the University of Paris, and in 1824 he was elected to the prestigious chair in experimental physics at the Collège de France. In 1814 Ampère was invited to join the class of mathematicians in the new \"Institut Impérial\", the umbrella under which the reformed state Academy of Sciences would sit.\n\nAmpère engaged in a diverse array of scientific inquiries during the years leading up to his election to the academy—writing papers and engaging in topics from mathematics and philosophy to chemistry and astronomy, which was customary among the leading scientific intellectuals of the day. Ampère claimed that \"at eighteen years he found three culminating points in his life, his First Communion, the reading of Antoine Leonard Thomas's \"Eulogy of Descartes\", and the Taking of the Bastille. On the day of his wife's death he wrote two verses from the Psalms, and the prayer, 'O Lord, God of Mercy, unite me in Heaven with those whom you have permitted me to love on earth.' In times of duress he would take refuge in the reading of the Bible and the Fathers of the Church.\"\n\nFor a time he took into his family the young student Frédéric Ozanam (1813–1853), one of the founders of the Conference of Charity, later known as the Society of Saint Vincent de Paul. Through Ampère, Ozanam had contact with leaders of the neo-Catholic movement, such as François-René de Chateaubriand, Jean-Baptiste Henri Lacordaire, and Charles Forbes René de Montalembert. Ozanam was beatified by Pope John Paul II in 1998.\n\nIn September 1820, Ampère's friend and eventual eulogist François Arago showed the members of the French Academy of Sciences the surprising discovery of Danish physicist Hans Christian Ørsted that a magnetic needle is deflected by an adjacent electric current. Ampère began developing a mathematical and physical theory to understand the relationship between electricity and magnetism. Furthering Ørsted's experimental work, Ampère showed that two parallel wires carrying electric currents attract or repel each other, depending on whether the currents flow in the same or opposite directions, respectively - this laid the foundation of electrodynamics. He also applied mathematics in generalizing physical laws from these experimental results. The most important of these was the principle that came to be called Ampère's law, which states that the mutual action of two lengths of current-carrying wire is proportional to their lengths and to the intensities of their currents. Ampère also applied this same principle to magnetism, showing the harmony between his law and French physicist Charles Augustin de Coulomb's law of magnetic action. Ampère's devotion to, and skill with, experimental techniques anchored his science within the emerging fields of experimental physics.\n\nAmpère also provided a physical understanding of the electromagnetic relationship, theorizing the existence of an \"electrodynamic molecule\" (the forerunner of the idea of the electron) that served as the component element of both electricity and magnetism. Using this physical explanation of electromagnetic motion, Ampère developed a physical account of electromagnetic phenomena that was both empirically demonstrable and mathematically predictive. In 1827 Ampère published his magnum opus, \"Mémoire sur la théorie mathématique des phénomènes électrodynamiques uniquement déduite de l’experience\" (Memoir on the Mathematical Theory of Electrodynamic Phenomena, Uniquely Deduced from Experience), the work that coined the name of his new science, \"electrodynamics\", and became known ever after as its founding treatise.\n\nIn 1827 Ampère was elected a Foreign Member of the Royal Society and in 1828, a foreign member of the Royal Swedish Academy of Science.\n\n\nIn recognition of his contribution to the creation of modern electrical science, an international convention, signed at the 1881 International Exposition of Electricity, established the ampere as a standard unit of electrical measurement, along with the coulomb, volt, ohm, and watt, which are named, respectively, after Ampère's contemporaries Charles-Augustin de Coulomb of France, Alessandro Volta of Italy, Georg Ohm of Germany, and James Watt of Scotland. Ampère's name is one of the 72 names inscribed on the Eiffel Tower.\n\nSeveral items are named after Ampère; many streets and squares, schools, a Lyon metro station, and an electric ferry in Norway.\n\n\n\n"}
{"id": "44621719", "url": "https://en.wikipedia.org/wiki?curid=44621719", "title": "Bismuth Indium", "text": "Bismuth Indium\n\nThe elements bismuth and indium have relatively low melting points when compared to other metals, and their alloy Bismuth Indium is classified as a fusible alloy. It has a melting point lower than the eutectic point of the tin lead alloy. The most common application of the alloy is as a low temperature solder, which can also contain, besides Bismuth and Indium, lead, cadmium and tin.\n\nBismuth has characteristics which are uncommon in the majority of the metals. As with gallium, it expands when solidified, approximately 3.32%. Its electrical resistance is higher in the solid state than in the liquid (in a half proportion). The thermal conductivity is low, only bigger than the mercury one (just considering pure metals). It is fragile, highly diamagnetic (higher value of metals) with magnetic susceptibility of -1,68x10mks. Bismuth is used as catalyst in the production of plastics and cosmetics, as additives in steel alloys, aluminum and in electronics. It has a rhombohedral (Biα) structure, with an atomic radius of 1.54 Å, electronegativity of 1.83, and valence of +3 and +5.\n\nIndium is a metal softer than lead (hardness of 0.9 HB), permitting it to be scratched by a nail. It is also malleable, ductile and has a thermal conductivity value of 0.78 W/m°C (85 °C). It also has the capacity of wetting glass, quartz and other ceramic materials. It maintains the plasticity and ductility when exposed to cryogenic environments and has a big gap between the melting point and the boiling point (156.6 °C and 2080 °C respectively). Under compression, it has high plasticity that allows almost unlimited deformation (2.14 MPa of compression resistance) and under tensile it has low elongation (4 MPa of tensile resistance). Indium is used in dental alloys, semiconductor components, nuclear reactor panels, sodium lamps, strengthening factor in lead-based welds and low melting temperature welds. The metal has a body centered tetragonal structure, atomic radius of 1.63 Å, electronegativity of 1.81 and valence of +3 or +5, being the trivalent the more common.\n\nThe most common application of this alloy is as a solder, with the composition of 95wt% of In and 5wt% of Bi. The liquidus line of this composition occurs at 423K (150 °C; 302 °F), and the solidus line at 398K (125 °C; 257 °F), being the first solid phase to be formed during the cooling process In, with Bi as a substitutional solid solution.\n\nWith a smaller application area, due to difficulties on the process of synthesizing, is the alloy composed by 33 wt% of In and 67wt% of Bi. This alloy presents a eutectic temperature of 382K (109 °C; 228.2 °F). The resistance to thermal fatigue of this material is higher, but the quantity of slag when compared to the alloy between tin and lead.\n\nThere is, on the market a solder composed by 49 wt% of Bi, 21 wt% of In, 18 wt% of Pb, and 12 wt% of Sn, called commercially solder 136. This alloy presents a density of 8.58g/cm, tensile strength of 43 MPa, toughness of 14HB, eutectic temperature of 331k (58 °C; 136.4 °F), thermic coefficient of expansion of 12,8 10/K. It is used to parts where precision is necessary, as in inspections, and fusible cores to wax patterns compounds.\n\nAnother alloy also on the market is the solder 117, composted by 44.7 wt% of Bi, 22.60 wt% of Pb, 19 wt% of In, 8.30 wt% of Sn, and 5.30 wt% of Cd. The density of this alloy is 8.86g/cm, tensile strength of 37 MPa, toughness of 12HB, eutectic temperature of 320K (47 °C; 116.6 °F). It is also used to parts on inspection equipment, spindles for machining (polishing), molds for development of prosthesis and dental molds.\n\nOther compositions founded on the market:\n\n- Solder 174: 26 wt% of In, 17 wt% of Sn, and 57 wt% of Bi, presenting a eutectic temperature of 352K (79 °C; 174.2 °F).\n\n- 32.5 wt% of Bi, 16.5 wt% of Sn, and 51 wt% of In, presenting a eutectic temperature of 333K (60 °C; 140 °F).\n\n- 48 wt% of Bi, 25.63 wt% of Pb, 12.77 wt% of Sn, 9.6 wt% of Cd and 4 wt% of In, present a liquids temperature of 338K (65 °C; 149 °F), and a solidus temperature of 334K (61 °C; 141.8 °F).\n\nThe influence of each element\n\nAntimony: is added to increase the strength, but not changing the wettability.\n\nBismuth: it significantly improves the wettability of the solder. When the composition is more than 47% Bi, it will expand upon cooling.\n\nCadmium: It oxidizes fast, causes tarnishes, and slow pace of solder. It improves the mechanical properties of the alloys.\n\nIndium: Each 1 wt% of In added on the alloy of the melting point reduces 1.45 °C. It easily oxidizes, has a very high cost, enables soldering for cryogenic applications, and allows soldering between nonmetals. It makes easier the fabrication process if compared with Bi.\n\nLead: With presence of In it forms a compound that have a phase change at 387K (114 °C; 237.20 °F).\n\nThe phase diagram of the alloy between Bi and In, in room temperature can form three intermetallic, being them: Bi(α), BiIn; BiIn; BiIn. Above the room temperature there is another phase named ε.\n\nSolubility of the elements\n\nThe solubility of the basic elements is not too high, being de 0 - 0,005 wt% of In, on the Bi structure; and ~0 -14 wt% of Bi, on the In structure. This percentage of solubility can be explained by the Hume-Rothery rules, where the crystalline structure must to be the same, the atomic radius must differ 15% or less, the valency must to be the same and the electronegativity of the two components must to be similar.\n\nMain points on the equilibrium diagram.\n\nWhen the two elements are mixed together, the alloy between Bi an In presents three eutectic points, being:\n\nWhen cooled from the melt, it will form a lamellar structure.\n\nThere is one eutectoid point on the diagram, at 83 wt% of In. The eutectoid temperature is 322K (49 °C; 120.20 °F). In the cooling process the phase ε will form BiIn and In.\n\nIn the Peritectic point, with the composition of 86 wt% of In, the liquid and the already formed In will result in the phase ε.\n\nThere are three intermetallic formed in the equilibrium, being:\n\n- BiIn (from 0,005 to 35.4 wt% of In), presenting a structure tetragonal, with 2 atoms per unit cell.\n\n- BiIn (from 47.5 to 97.97 wt% of In), with a tetragonal structure and 4 atoms per unit cell.\n\n- Bi In (from 52.5 to 53.5 wt% of In), having a hexagonal structure, with 2 atoms per unit cell.\n\nThere is regions on the diagram with were determinate thermodynamically due the process of formation take too much time or difficulties on the visualization of the phase.\n\nThe lowest fusion value found for the alloy is at 345.7K (72,7 °C; 162.86 °F), on the composition of 66,7 wt% of In. In a cooling process the phases that will be formed is the Bi In and ε.\n\nThis alloy also present a metastable phases BiIn, occurring at 62 wt% of In.\n\nFusible alloys present a precipitation hardening (aging), so the mechanic properties will be dependent of the melting conditions, solidification rate, time since the melting, and the conditions in which the alloy will be used.\n\nWe can attribute advantages of the Bi In alloy, when compared to the traditional ones (Sn Pb), as bigger thermal fatigue resistance, and lower melting point. Some of the disadvantages are that they are less ductile and they will produce more slag.\n"}
{"id": "8018219", "url": "https://en.wikipedia.org/wiki?curid=8018219", "title": "Borirane", "text": "Borirane\n\nBorirane is a heterocyclic organic compound with the formula CHBH. This colourless, flammable gas is the simplest borirane, a three-membered ring consisting of two carbon and one boron atom. It can be viewed as a structural analog of aziridine, with boron replacing the nitrogen atom of aziridine. Borirane is isomeric with ethylideneborane.\n\nThis compound has five isomers\n"}
{"id": "5017744", "url": "https://en.wikipedia.org/wiki?curid=5017744", "title": "Bulk soil", "text": "Bulk soil\n\nBulk soil is soil outside the rhizosphere. Bulk soil is not penetrated by plant roots. Natural organic compounds are much lower in bulk soil than in the rhizosphere. Microbial populations are typically lower in bulk soil than in rhizospheric soil. Furthermore, bulk soil inhabitants are generally smaller than identical species in the rhizosphere.\n"}
{"id": "3127774", "url": "https://en.wikipedia.org/wiki?curid=3127774", "title": "Caprylic acid", "text": "Caprylic acid\n\nCaprylic acid is the common name for the eight-carbon saturated fatty acid known by the systematic name octanoic acid. Its compounds are found naturally in the milk of various mammals, and as a minor constituent of coconut oil and palm kernel oil. It is an oily liquid that is minimally soluble in water with a slightly unpleasant rancid-like smell and taste.\n\nTwo other acids are named after goats via the Latin word \"capra\": caproic acid (C6) and capric acid (C10). Along with caprylic acid (C8) these total 15% in goat milk fat.\n\nCaprylic acid is used commercially in the production of esters used in perfumery and also in the manufacture of dyes.\n\nCaprylic acid is an antimicrobial pesticide used as a food contact surface sanitizer in commercial food handling establishments on dairy equipment, food processing equipment, breweries, wineries, and beverage processing plants. It is also used as disinfectant in health care facilities, schools/colleges, animal care/veterinary facilities, industrial facilities, office buildings, recreational facilities, retail and wholesale establishments, livestock premises, restaurants, and hotels/motels. In addition, caprylic acid is used as an algaecide, bactericide, fungicide, and herbicide in nurseries, greenhouses, garden centers, and interiorscapes on ornamentals. Products containing caprylic acid are formulated as soluble concentrate/liquids and ready-to-use liquids.\n\nFor ghrelin to have a hunger-stimulating action on a hypothalamus, caprylic acid must be linked to a serine residue at the 3-position of ghrelin. To cause hunger, it must acylate an -OH group. Other fatty acids in the same position have similar effects on hunger.\n\nThe acid chloride of caprylic acid is used in the synthesis of perfluorooctanoic acid.\n\nCaprylic acid is taken as a dietary supplement.\n\nSome studies have shown that medium-chain triglycerides (MCTs) can help in the process of excess calorie burning, and thus weight loss; however, a systematic review of the evidence concluded that the overall results are inconclusive. Also, interest in MCTs has been shown by endurance athletes and the bodybuilding community, but MCTs are not beneficial to improved exercise performance.\n\n"}
{"id": "2023977", "url": "https://en.wikipedia.org/wiki?curid=2023977", "title": "Chalcogenide glass", "text": "Chalcogenide glass\n\nChalcogenide glass (pronounced hard \"ch\" as in \"chemistry\") is a glass containing one or more chalcogens (sulfur, selenium and tellurium, but excluding oxygen). Such glasses are covalently bonded materials and may be classified as covalent network solids. Polonium is also a chalcogen but is not used because of its strong radioactivity. Chalcogenide materials behave rather differently from oxides, in particular their lower band gaps contribute to very dissimilar optical and electrical properties.\n\nThe classical chalcogenide glasses (mainly sulfur-based ones such as As-S or Ge-S) are strong glass-formers and possess glasses within large concentration regions. Glass-forming abilities decrease with increasing molar weight of constituent elements; i.e., S > Se > Te.\n\nChalcogenide compounds such as AgInSbTe and GeSbTe are used in rewritable optical disks and phase-change memory devices. They are fragile glass-formers: by controlling heating and annealing (cooling), they can be switched between an amorphous (glassy) and a crystalline state, thereby changing their optical and electrical properties and allowing the storage of information.\n\nMost stable binary chalcogenide glasses are compounds of a chalcogen and a group 14 or 15 element and may be formed in a wide range of atomic ratios. Ternary glasses are also known.\n\nNot all chalcogenide compositions exist in glassy form, though it is possible to find materials with which these non-glass-forming compositions can be alloyed in order to form a glass. An example of this is gallium sulphide-based glasses. Gallium(III) sulphide on its own is not a known glass former; however, with sodium or lanthanum sulphides it forms a glass, gallium lanthanum sulphide (GLS).\n\nUses include infrared detectors, mouldable infrared optics such as lenses, and infrared optical fibers, with the main advantage being that these materials transmit across a wide range of the infrared electromagnetic spectrum.\n\nThe physical properties of chalcogenide glasses (high refractive index, low phonon energy, high nonlinearity) also make them ideal for incorporation into lasers, planar optics, photonic integrated circuits, and other active devices especially if doped with rare-earth element ions. Some chalcogenide glasses exhibit several non-linear optical effects such as photon-induced refraction, and electron-induced permittivity modification\n\nSome chalcogenide materials experience thermally driven amorphous-to-crystalline phase changes. This makes them useful for encoding binary information on thin films of chalcogenides and forms the basis of rewritable optical discs and non-volatile memory devices such as PRAM. Examples of such phase change materials are GeSbTe and AgInSbTe. In optical discs, the phase change layer is usually sandwiched between dielectric layers of ZnS-SiO, sometimes with a layer of a crystallization promoting film. Other less commonly used such materials are InSe, SbSe, SbTe, InSbSe, InSbTe, GeSbSe, GeSbTeSe and AgInSbSeTe.\n\nIntel claims that its chalcogenide-based 3D XPoint memory technology achieves throughput and write durability 1,000 times higher than flash memory.\n\nElectrical switching in chalcogenide semiconductors emerged in the 1960s, when the amorphous chalcogenide TeAsSiGe was found to exhibit sharp, reversible transitions in electrical resistance above a threshold voltage. If current is allowed to persist in the non-crystalline material, it heats up and changes to crystalline form. This is equivalent to information being written on it. A crystalline region may be melted by exposure to a brief, intense pulse of heat. Subsequent rapid cooling then sends the melted region back through the glass transition. Conversely, a lower-intensity heat pulse of longer duration will crystallize an amorphous region. Attempts to induce the glassy–crystal transformation of chalcogenides by electrical means form the basis of phase-change random-access memory (PC-RAM). This technology has been developed to near commercial use by ECD Ovonics. For write operations, an electric current supplies the heat pulse. The read process is performed at sub-threshold voltages by utilizing the relatively large difference in electrical resistance between the glassy and crystalline states. Examples of such phase change materials are GeSbTe and AgInSbTe.\n\nThe semiconducting properties of chalcogenide glasses were revealed in 1955 by B.T. Kolomiets and N.A. Gorunova from Ioffe Institute, USSR.\n\nAlthough the electronic structural transitions relevant to both optical discs and PC-RAM were featured strongly, contributions from ions were not considered—even though amorphous chalcogenides can have significant ionic conductivities. At Euromat 2005, however, it was shown that ionic transport can also be useful for data storage in a solid chalcogenide electrolyte. At the nanoscale, this electrolyte consists of crystalline metallic islands of silver selenide (AgSe) dispersed in an amorphous semiconducting matrix of germanium selenide (GeSe).\n\nThe electronic applications of chalcogenide glasses have been an active topic of research throughout the second half of the twentieth century and beyond For example, the migration of dissolved ions is required in the electrolytic case, but could limit the performance of a phase-change device. Diffusion of both electrons and ions participate in electromigration—widely studied as a degradation mechanism of the electrical conductors used in modern integrated circuits. Thus, a unified approach to the study of chalcogenides, assessing the collective roles of atoms, ions and electrons, may prove essential for both device performance and reliability.\n\n"}
{"id": "1897245", "url": "https://en.wikipedia.org/wiki?curid=1897245", "title": "Coating", "text": "Coating\n\nA coating is a covering that is applied to the surface of an object, usually referred to as the substrate. The purpose of applying the coating may be decorative, functional, or both. The coating itself may be an all-over coating, completely covering the substrate, or it may only cover parts of the substrate. An example of all of these types of coating is a product label on many drinks bottles- one side has an all-over functional coating (the adhesive) and the other side has one or more decorative coatings in an appropriate pattern (the printing) to form the words and images.\n\nPaints and lacquers are coatings that mostly have dual uses of protecting the substrate and being decorative, although some artists paints are only for decoration, and the paint on large industrial pipes is presumably only for the function of preventing corrosion.\n\nFunctional coatings may be applied to change the surface properties of the substrate, such as adhesion, wettability, corrosion resistance, or wear resistance. In other cases, e.g. semiconductor device fabrication (where the substrate is a wafer), the coating adds a completely new property, such as a magnetic response or electrical conductivity, and forms an essential part of the finished product.\n\nA major consideration for most coating processes is that the coating is to be applied at a controlled thickness, and a number of different processes are in use to achieve this control, ranging from a simple brush for painting a wall, to some very expensive machinery applying coatings in the electronics industry. A further consideration for 'non-all-over' coatings is that control is needed as to where the coating is to be applied. A number of these non-all-over coating processes are printing processes.\n\nMany industrial coating processes involve the application of a thin film of functional material to a substrate, such as paper, fabric, film, foil, or sheet stock. If the substrate starts and ends the process wound up in a roll, the process may be termed \"roll-to-roll\" or \"web-based\" coating. A roll of substrate, when wound through the coating machine, is typically called a web.\n\nCoatings may be applied as liquids, gases or solids.\n\n\nCoating processes may be classified as follows:\n\n\n\n\n\nCommon roll-to-roll coating processes include:\n\n\n"}
{"id": "39092774", "url": "https://en.wikipedia.org/wiki?curid=39092774", "title": "Concrete fracture analysis", "text": "Concrete fracture analysis\n\nConcrete is widely used construction material all over the world. It is composed of aggregate, cement and water. Composition of concrete varies to suit for different applications desired. Even size of the aggregate can influence mechanical properties of concrete to a great extent.\n\nConcrete is strong in compression but weak in tension. When tensile loads are applied, concrete undergoes fracture easily. The reason behind this phenomenon can be explained as follows. The aggregates in concrete are capable of taking compressive stresses so that concrete withstands compressive loading. But during tensile loading cracks are formed which separates the cement particles which hold the aggregates together. This separation of cement particles causes the entire structure to fail as crack propagates. This problem in concrete is resolved by the introduction of reinforcing components such as metallic bars, ceramic fibres etc. These components act as a skeleton of the entire structure and are capable of holding aggregates under tensile loading. This is known as \"Reinforcement of Concrete\".\n\nConcrete may be referred to as a brittle material. This is because concrete's behaviour under loading is completely different from that of ductile materials like steel. But actually concrete differs from ideal brittle materials in many aspects. In modern fracture mechanics concrete is considered as a quasi-brittle material. Quasi-brittle materials possess considerable hardness which is similar to ceramic hardness, so often it is called ceramic hardness. The reason for ceramic hardness can be explained on the basis of subcritical cracking that happens during loading of concrete. Subcritical cracking in concrete which precedes ultimate failure, results in nonlinear StressStrain response and Rcurve behaviour. So concrete obtains hardness from subcritical failure.\nAlso concrete has a heterogeneous structure due to uneven composition of ingredients in it. This also complicates the analysis of concrete by producing misleading results.\n\nLinear Elastic Fracture Mechanics yields reliable results in the field of ductile materials like steel. Most of the experiments and theories in fracture mechanics are formulated taking ductile materials as object of interest. But if we compare the salient features in LEFM with results derived from the testing of concrete, we may find it irrelevant and sometimes trivial. For example, LEFM permits infinite stress at crack tip. This makes no sense in real analysis of concrete where the stress at crack tip is fixed. And LEFM fails to calculate stress at crack tip precisely. So we need some other ways to find out what is stress at crack tip and distribution stress near crack tip.\n\nLEFM cannot answer many phenomenon exhibited by concrete. Some examples are \n\nIn LEFMPA, during cracking, no specific region is mentioned in between the area which is cracked and that which is not. But it is evident that in concrete, there is some intermediate space between cracked and uncracked portion. This region is defined as the \"Fracture Process Zone (FPZ)\". FPZ consists of micro cracks which are minute individual cracks situated nearer to crack tip. As the crack propagates these micro cracks merge and becomes a single structure to give continuity to the already existing crack. So indeed, FPZ acts as a bridging zone between cracked region and uncracked region. Analysis of this zone deserves special notice because it is very helpful to predict the propagation of crack and ultimate failure in concrete.\nIn steel (ductile) FPZ is very small and therefore strain hardening dominates over strain softening. Also due to small FPZ, crack tip can easily be distinguished from uncracked metal. And in ductile materials FPZ is a yielding zone.\n\nWhen we consider FPZ in concrete, we find that FPZ is sufficiently large and contains micro cracks. And cohesive pressure still remains in the region. So strain softening is prevalent in this region. Due to the presence of comparatively large FPZ, locating a precise crack tip is not possible in concrete.\n\nIf we plot stress (\"Pascal\") vs. strain (\"percentage deformation\") characteristics of a material, the maximum stress up to which the material can be loaded is known as peak value (formula_1). The behaviour of concrete and steel can be compared to understand the difference in their fracture characteristics.\nFor this a strain controlled loading of un-notched specimen of each materials can be done. From the observations we can draw these conclusions:\n\n\"Pre-peak\"\n\n\n\"Post-peak\"\n\n\nFracture energy is defined as the energy required to open unit area of crack surface. It is a material property and does not depend on size of structure. This can be well understood from the definition that it is defined for a unit area and thus influence of size is removed.\n\nFracture energy can be expressed as the sum of surface creation energy and surface separation energy. Fracture energy found to be increasing as we approach crack tip.\n\nFracture energy is a function of displacement and not strain. Fracture energy deserves prime role in determining ultimate stress at crack tip.\n\nIn Finite Element Method analysis of concrete, if mesh size is varied, then entire result varies according to it. This is called mesh size dependence. If mesh size is higher, then the structure can withstand more stress. But such results obtained from FEM analysis contradict real case.\n\nIn classical Fracture Mechanics, critical stress value is considered as a material property. So it is same for a particular material of any shape and size. But in practice, it is observed that, in some materials like plain concrete size has a strong influence on critical stress value. So fracture mechanics of concrete consider critical stress value a material property as well as a size dependent parameter.\n\nwhere\n\nThis clearly proves that material size and even the component size like aggregate size can influence cracking of concrete.\n\nBecause of the heterogeneous nature of concrete, it responds to already existing crack testing models \"anomaly\". And it is evident that alteration of existing models was required to answer the unique fracture mechanics characteristics of concrete.\n\n\n\nThe main drawback of both these models was negligence of concept of fracture energy.\n\nThe model proposed by Hillerborg in 1976, was the first model to analyse concrete fracture making use of the fracture energy concept. In this model, Hillerborg describes two crack regions namely,\n\nIn this zone at crack tip, we have peak stress = tensile strength of concrete.\n\nAlong the FPZ stress is continuous and displacement is discontinuous.\n\nCrack propagation in FPZ starts when critical stress is equal to tensile strength of concrete and as crack starts propagating, stress does not become zero. Using the plot of fracture energy versus crack width, we can calculate critical stress at any point including crack tip. So one of the major drawbacks of LEFM is overcome using fracture energy approach. Direction of crack propagation can also be determined by identifying the direction of maximum energy release rate.\n\n\nwhere\n\nHillerborg characteristic length can be used to predict brittleness of a material. As magnitude of characteristic length decreases brittle nature dominates and vice versa.\n\nProposed by Bazant and Oh in 1983, this theory can well attribute materials whose homogeneous nature changes over a certain range randomly. So we select any particular more or less homogeneous volume for the purpose of analysis. Hence we can determine the stresses and strains. The size of this region should be several times that of maximum aggregate. Otherwise the data obtained will be of no physical significance.\nFracture Process Zone is modelled with bands of smeared crack. And to overcome the Finite Element Method unobjectivity, we use cracking criterion of fracture energy.\n\nCrack width is estimated as the product of crack band width and element strain. \nIn finite element analysis, the crack band width is the element size of fracture process path.\n\n\n"}
{"id": "1103407", "url": "https://en.wikipedia.org/wiki?curid=1103407", "title": "DAMA/NaI", "text": "DAMA/NaI\n\nThe experimental set-up was located deep underground in the Laboratori Nazionali del Gran Sasso in Italy.\n\nThe experimental set-up was made by nine 9.70 kg low-radioactivity scintillating thallium-doped sodium iodide crystals [NaI(Tl)]. Each crystal was faced by two low-background photomultipliers through 10 cm light guides. The detectors were installed inside a sealed copper box flushed with highly pure nitrogen in order to insulate the detectors from air that contains trace amounts of radon, a radioactive gas. To reduce the natural environmental background the copper box is enclosed inside a multicomponent multi-ton passive shield made of copper, lead, polyethylene/paraffin, cadmium foil. A plexiglas box encloses the whole shield and is also kept in a highly pure nitrogen atmosphere. A concrete neutron moderator 1 m thick largely surrounds the set-up.\n\nThe DAMA/NaI set-up observed the annual modulation signature over 7 annual cycles (1995-2002). The presence of a model independent positive evidence in the data of DAMA/NaI was first reported by the DAMA collaboration in fall 1997 and published beginning of 1998. The final paper with the full results was published in 2003 after the end of experiment in July 2002. Various corollary investigations are continuing and have also been published.\n\nThe model-independent evidence is compatible with a wide set of scenarios regarding the nature of the dark matter candidate and related astrophysical, nuclear and particle physics For example: neutralinos, inelastic dark matter, Self-interacting dark matter, and heavy 4th generation neutrinos\n\nA careful quantitative investigation of possible sources of systematic and side reactions has been regularly carried out and published at the time of each data release. No systematic effect or side reaction able to account for the observed modulation amplitude and to simultaneously satisfy all the requirements of the signature has been found.\n\nThe experiment has also obtained and published many results on other processes and/or approaches.\n\nNegative results from the XENON Dark Matter Search Experiment seem to contradict DAMA/Nal's results.\n\nDAMA/NaI has been replaced by the new generation experiment, DAMA/LIBRA. These experiments are carried out by Italian and Chinese researchers.\n\n"}
{"id": "23021779", "url": "https://en.wikipedia.org/wiki?curid=23021779", "title": "Elemental chlorine free", "text": "Elemental chlorine free\n\nElemental chlorine free (ECF) is a technique that uses chlorine dioxide for the bleaching of wood pulp. It does not use elemental chlorine gas during the bleaching process and prevents the formation of dioxins and dioxin-like compounds, carcinogens. The traditional ECF sequence is DEopDEpD using the common letter symbols for bleaching stages, though many improved sequences are available.\n\nTotally chlorine free (TCF) is paper that does not use any chlorine compounds for wood pulp bleaching.\n\n\n"}
{"id": "19090226", "url": "https://en.wikipedia.org/wiki?curid=19090226", "title": "Emerson Cavitation Tunnel", "text": "Emerson Cavitation Tunnel\n\nThe Emerson Cavitation Tunnel is a propeller testing facility that is part of the School of Engineering at Newcastle University. \nThe Emerson Cavitation Tunnel consists of a water circuit which flows in the vertical plane, within which propellers and other propulsion devices can be tested. The system is powered by a pump, with a four-bladed impeller and can produce a maximum water velocity of . The test area has a cross sectional area of allowing model propellers of up to in diameter to be tested. The pressure range of the tunnel can vary from a minimum of 7.6 kN/m to a maximum of 106 kN/m. Cavitation numbers of 0.5 (minimum) to 23 (maximum) can be accommodated for. Measurements can be taken using a 3 Watt, water-cooled, Argon-ion laser, a hydrophone, and two dynamometers. A high-speed video camera is also attached with an imaging frequency of 1–10,000 frames per second. \n\nFunding for the tunnel's equipment is raised by numerous organisations, including the Engineering and Physical Sciences Research Council (EPSRC) and the Scottish Universities of Glasgow and Strathclyde.\n\nThe tunnel was first established at the University in 1949 after being disassembled and transported from Pelzerhaken, Germany after the Second World War. The tunnel arrived at the University in 1947 and over the following few years the tunnel was heavily modified. The tunnel - which was originally designed to be operated in the horizontal plane - was converted into a vertical loop tunnel and the length was reduced by half. The original observation window was modified and two more added. Because of damage, a new impeller was constructed and numerous pieces of measuring equipment were added. This equipment included pitot tubes, a tachometer, stroboscopic lighting equipment, contact meters and a vacuum pump. The tunnel was connected to an electrical supply in 1949 and entered service late in 1950, after technical problems called for recalibration of some of the instruments. The Cavitation Tunnel was housed in Newcastle University's old boiler house, where it was originally reconstructed. That location was on King's Road in the middle of the University's city centre campus between the Armstrong building, the Student Union, the Arches and the Bedson building.\n\nIn 2016, the tunnel was moved from the Newcastle University city centre campus and taken to Poland, where it was fully refurbished before being brought back to the North East and installed in a new purpose-built research centre, Marine campus at Blyth.\n\nThe first research grant of £8,000 was awarded in 1950 for the testing of a new series of propellers, and was awarded by the Department of Scientific and Industrial Research (DSIR).\n\nIn the 1970s and 1980s, the tunnel was extensively modified and upgraded in order to improve the range of propellers that could be tested. The tunnel was also renamed to its current name, the \"Emerson Cavitation Tunnel\" after Dr Arnold Emerson, who was the tunnel superintendent and the driving force behind the upgrades.\n\nModifications were made to the tunnel during the 1980s. New computer-based data collection, interpretation and analysis technology has been added to aid with computational fluid dynamics. Data is also collected with the help of laser doppler anemometry (LDA) and phase doppler anemometry (PDA).\n\nThe tunnel is now located at Newcastle University's Marine Campus at Blyth, Northumberland.\n\n\n"}
{"id": "2563319", "url": "https://en.wikipedia.org/wiki?curid=2563319", "title": "Exchange bias", "text": "Exchange bias\n\nExchange bias or exchange anisotropy occurs in bilayers (or multilayers) of magnetic materials where the hard magnetization behavior of an antiferromagnetic thin film causes a shift in the soft magnetization curve of a ferromagnetic film. The exchange bias phenomenon is of tremendous utility in magnetic recording, where it is used to pin the state of the readback heads of hard disk drives at exactly their point of maximum sensitivity; hence the term \"bias.\"\n\nThe essential physics underlying the phenomenon is the exchange interaction between the antiferromagnet and ferromagnet at their interface. Since antiferromagnets have a small or no net magnetization, their spin orientation is only weakly influenced by an externally applied magnetic field. A soft ferromagnetic film which is strongly exchange-coupled to the antiferromagnet will have its interfacial spins pinned. Reversal of the ferromagnet's moment will have an added energetic cost corresponding to the energy necessary to create a Néel domain wall within the antiferromagnetic film. The added energy term implies a shift in the switching field of the ferromagnet. Thus the magnetization curve of an exchange-biased ferromagnetic film looks like that of the normal ferromagnet except that is shifted away from the H=0 axis by an amount H.\n\nIn most well-studied ferromagnet/antiferromagnet bilayers, the Curie temperature of the ferromagnet is larger than the Néel temperature T of the antiferromagnet. This inequality means that the direction of the exchange bias can be set by cooling through T in the presence of an applied magnetic field. The moment of the magnetically ordered ferromagnet will apply an effective field to the antiferromagnet as it orders, breaking the symmetry and influencing the formation of domains.\n\nThe exchange bias effect is attributed to a ferromagnetic unidirectional anisotropy formed at the interface between different magnetic phases. Generally, the process of field cooling from higher temperature is used to obtain ferromagnetic unidirectional anisotropy in different exchange bias systems. In 2011, a large exchange bias has been realized after zero-field cooling from an unmagnetized state, which was attributed to the newly formed interface between different magnetic phases during the initial magnetization process.\n\nExchange anisotropy has long been poorly understood due to the difficulty of studying the dynamics of domain walls in thin antiferromagnetic films. A naive approach to the problem would suggest the following expression for energy per unit area:\n\nformula_1\n\nwhere \"n\" is the number of interfacial spins interactions per unit area, J is the exchange constant at the interface, S refers to the spin vector, M refers to the magnetization, t refers to film thickness and H is the external field. The subscript F describes the properties of the ferromagnet and AF to the antiferromagnet. The expression omits magnetocrystalline anisotropy, which is unaffected by the presence of the antiferromagnet. At the switching field of the ferromagnet, the pinning energy represented by the first term and the Zeeman dipole coupling represented by the second term will exactly balance. The equation then predicts that the exchange bias shift H will be given by the expression\n\nformula_2\n\nMany experimental findings regarding the exchange bias contradict this simple model. For example, the magnitude of measured H values is typically 100 times less than that predicted by the equation for reasonable values of the parameters. The amount of hysteresis shift H is not correlated with the density \"n\" of uncompensated spins in the plane of the antiferromagnet that appears at the interface. In addition, the exchange bias effect tends to be smaller in epitaxial bilayers than in polycrystalline ones, suggesting an important role for defects. In recent years progress in fundamental understanding has been made via synchrotron radiation based element-specific magnetic linear dichroism experiments that can image antiferromagnetic domains and frequency-dependent magnetic susceptibility measurements that can probe the dynamics. Experiments on the Fe/FeF and Fe/MnF model systems have been particularly fruitful.\n\nExchange bias was initially used to stabilize the magnetization of soft ferromagnetic layers in readback heads based on the anisotropic magnetoresistance (AMR) effect. Without the stabilization, the magnetic domain state of the head could be unpredictable, leading to reliability problems. Currently exchange bias is used to pin the harder reference layer in spin valve readback heads and MRAM memory circuits that utilize the giant magnetoresistance or magnetic tunneling effect. Similarly the most advanced disk media are antiferromagnetically coupled, making use of interfacial exchange to effectively increase the stability of small magnetic particles whose behavior would otherwise be superparamagnetic.\n\nDesirable properties for an exchange bias material include a high Néel temperature, a large magnetocrystalline anisotropy and good chemical and structural compatibility with NiFe and Co, the most important ferromagnetic films. The most technologically significant exchange bias materials have been the rocksalt-structure antiferromagnetic oxides like NiO, CoO and their alloys and the rocksalt-structure intermetallics like FeMn, NiMn, IrMn and their alloys.\n\nExchange anisotropy was discovered by Meiklejohn and Bean of General Electric in 1956. The first commercial device to employ the exchange bias was IBM's anisotropic magnetoresistance (AMR) disk drive recording head, which was based on a design by Hunt in the 1970s but which didn't fully displace the inductive readback head until the early 1990s. By the mid-1990s, the spin valve head using an exchange-bias layer was well on its way to displacing the AMR head.\n\n"}
{"id": "23513354", "url": "https://en.wikipedia.org/wiki?curid=23513354", "title": "Fishpaper", "text": "Fishpaper\n\nFish paper or fishpaper is a strong, flexible, fibrous dielectric paper. It resists moderate heat and mechanical injury, and is often used for wrapping coils and insulating stove-top parts. It is hygroscopic and so must be treated with paraffin for use in moist environments. Some fish papers incorporate mica layers to increase the dielectric strength giving good mechanical strength.\n\nFish paper is a durable, flexible, electrical insulator that's made of vulcanized fiber and used with motor windings and for gasket insulation. A lightweight dielectric paper, this electrical grade material is also a good choice for oil-filled transformers. Moisture absorption can cause fish paper to swell or curl.\n\n"}
{"id": "34957810", "url": "https://en.wikipedia.org/wiki?curid=34957810", "title": "Ford F-Series (tenth generation)", "text": "Ford F-Series (tenth generation)\n\nThe tenth generation of the Ford F-Series is a line of pickup trucks produced by Ford from 1995 to 2004; it was sold from model years 1997 to 2004. In a major product shift in the Ford truck lineup, the F-250 and F-350 were split from the F-150. Beginning production in early-1998 (model year 1999) the newly branded Super Duty trucks had a distinct body and chassis, while still branded as F-Series trucks.\n\nThis generation of the F-Series was also sold by Lincoln as the Blackwood for the model year 2002 (2002-2003 in Mexico). In Mexico, the F-150 was rebranded as the Ford Lobo from 2004 to 2010, when it was replaced by the twelfth-generation model.\n\nIn late 1989, during mid-stage development of a facelift due in late-1991 for model year 1992, Ford commenced the PN-96 program on a new truck platform and designated Thomas Baughman as chief engineer. In mid-1990, Andrew Jacobson was designated as design director for the PN-96 truck program. By 1991, designers had developed clay models indicative of car like styling, based on a new design theme. Despite the disapproval from focus groups towards \"softer\" styling, during 1991 and 1992 in concept design clinics, Ford management backed the \"aero\" design philosophy. The end result by Bob Aikins reached in November 1992 and frozen for production in February 1993, took the aero styling further with a rounded nose on the new F-series. The PN-96 mules went into testing 1993, with prototypes running from early 1994. Pilot production began in 1995.\n\nBeing the F-150's first major redesign since 1979, the redesigned truck went on a nationwide 87-stop tour to Ford plants and the external part suppliers in October 1995, prior to its release. To build anticipation for the redesigned truck, the 1997 model was released in January 1996 with the first ad campaigns airing during Super Bowl XXX. Because of the radical styling, Ford predicted from marketing clinics that traditional truck buyers would not receive the radical and car-like 1997 well, so it continued to produce and sell the previous 1996 model alongside the redesigned 1997 model for a few months.\n\nA wide variety of body options were available: the 2-3 passenger regular cab and the 5-6 passenger SuperCab, and beds, and a choice of Styleside or Flareside beds on models. A new Lightning was introduced in March 1999, and Harley-Davidson and King Ranch versions were also created for the 2000 and 2001 model years, respectively. In 2000, the SuperCrew cab was introduced with four full-size doors for the 2001 model year. A Sport 4x4 model was introduced in 2000 as well. It featured the 5.4L Triton V8 and color-matched bumpers and mirror housings, and was available in regular cab and SuperCab in four colors: white, red, black, and silver. In 2002, an FX4 model was introduced, which came with skid plates, a carbon steel frame, Rancho shock absorbers, and unique 17\" aluminum wheels along with more standard features that were optional on XLT. In 2003, a sporty STX trim package was introduced, aimed at younger truck buyers. The STX package featured color-keyed front/rear bumpers along with clear lens headlights and integrated round fog lamps. The package also featured chrome step rails, 17\" chrome wheels, and a Kenwood Z828 stereo was installed in place of the standard Ford radio. Also in 2003, a special trim package \"Heritage Edition\" version with special badging was produced to mark the 100th anniversary of Ford trucks, available only in the 139\" wheelbase SuperCab model.\n\nSales of the F-150 surged in the tenth generation from 750,000 to over 900,000 in 2001 as products from General Motors and Chrysler lagged. Ford's sales dropped, however, for the final years of this generation as the redesigned Dodge Ram and refreshed Chevrolet Silverado were released.\n\nThe new F-150 was \"Motor Trend\" magazine's Truck of the Year in 1997. A minor facelift was introduced September 1998, with minor interior updates for 1999 models. In February 2000, the SuperCrew was added to the lineup early in the 2001 MY (Model Year), entering production on December 13, 1999. Ford also manufactured a limited run of \"Heritage\" (differentiated from the 2003 \"Heritage Edition\") F-150s of the 2003 body style to August 2004 as 2004 models to finish out production.\n\nThis generation of F-150 was sold in Mexico alongside the new 11th generation F-Series through 2008. It was only available as a Regular Cab and in XL trim, while the newer model was available in more trims, SuperCab and SuperCrew configurations and the new model was badged as Lobo, while the older model retained the F-150 name.\n\nA new lineup of improved efficiency engines were offered beginning for 1997. A 4.2 L OHV V6, based on Ford's 3.8 L Essex V6, replaced the 4.9 L OHV I6, while 4.6 and 5.4 liter SOHC V8s replaced the 5.0 and 5.8 liter OHV V8s. The 4.6 and 5.4 liter V8s were marketed under the name Triton and mark the first use of Ford's Modular Single Overhead Cam (SOHC) engines in the F-series pickups. Ford's own 8.8 IFS replaced the Dana 44 front end, while the Ford 8.8 rear remained. The Ford Sterling 9.75 axle was also optioned in heavy-duty versions. In 2000, the Sterling 10.25 axle became an option.\n\nEngines:\n\nThis generation F-150 received an overall \"Poor\" rating by the Insurance Institute for Highway Safety (IIHS) in the frontal offset test, and was ranked the \"2nd Worst Performing Vehicle\" behind the 1997–2005 GM U-platform minivans.\n\nFord has found that the cruise control system in many of their trucks could catch fire, because the switch system could corrode over time, overheat and ignite. Ignition was later blamed on spillage from the adjacent master cylinder. On March 5, 2007 Ford recalled 155,000 2003 full-size pickups and full-size SUVs for the defective part. During the previous two years Ford had recalled 5.8 million vehicles in because of the defective cruise control systems in trucks, SUVs and vans. That recall, one of the largest in history, covered vehicles from the 1994–2002 model years.\n\nAt its January 1996 launch, the 1997 PN96 F-Series was only offered as a F-150; the F-250 and F-350 were produced as 1996 models on the previous-generation chassis. To bridge the gap between the F-150 and the heavier-duty pickups, a PN96 version of the F-250 was introduced nearly a year later (though also a 1997 model), slotted between the F-150 and the F-250HD of the previous-generation chassis. While nearly externally indentical to the F-150, the F-250 gained increased load capability from a heavy-duty rear axle and load-leveling rear suspension; the F-250 was distinguished by 7-bolt wheel rims. \n\nThe PN-96 F-250 was marketed in 1997 and 1998, with Ford offering two generations of the vehicle under the same nameplate. For 1999, the F-250HD and F-350 were replaced by the Super Duty F-Series; the suspension components of the PN-96 F-250 continued as a \"7700\" option package from 2000 to 2003.\n\nThe Ford SVT Lightning is a sports/performance version of the F-150, developed by the SVT (Special Vehicle Team) division of Ford. For 1999, the second generation of the Lightning was released using the PN96 platform, after a three year hiatus of the model line. As with its 1993-1995 predecessor, the Lightning was based on the F-150; all versions were produced with a regular cab, rear-wheel drive, and a 6½-foot bed length. In sharp contrast to its predecessor, the 1999-2002 Lightning was given a payload of 800 pounds (half the payload of a Ranger); for 2003, the figure was raised to 1,350 pounds. \n\nWhile the first-generation Lightning chassis was a hybrid of the F-150 and F-250, to save weight and lower its cost, the second-generation adopted the stock F-150 frame. To improve handling, while the stock short/long arm front suspension configuration was used, the Lighting was lowered one inch with a 31mm stabilizer bar; the rear solid axle with leaf springs was lowered two inches, using a 23mm stabilizer bar. Monroe shocks were used from 1999 to 2001; Bilstein shocks were used from 2002 to 2004. In place of the 17-inch wheels of its predecessor, the second-generation Lightning was given 18-inch wheels with Goodyear Eagle F1 directional tires developed for the truck. \n\nThe second-generation Lightning was powered by a 5.4 L Triton SOHC V8 equipped with an Eaton M112 supercharger. At its launch, the Lightning produced and of torque, increased to and of torque (under 8 psi boost) in 2001. The supercharged V8 was paired with a 4-speed Ford 4R100 overdrive automatic transmission (shared with the 5.4L V8, 6.8L V10, and 7.3L diesel). From 1999 to 2000, the rear axle ratio was 3.55:1, shortened to 3.75:1 in 2001. The same year, a 4.5-inch aluminum driveshaft replaced a 3.5-inch steel unit. \n\nFollowing the 2001 drivetrain revisions, \"Car and Driver\" magazine tested a Lightning, accelerating from 0- in 5.2 seconds. \n\nDuring its production, the 1999-2004 was offered in a limited variety of colors. Initially produced in Bright Red, Black, and White, for 2000, Silver was introduced. For 2002, True Blue (a very dark blue) was introduced, but was replaced by a lighter Sonic Blue for 2003, along with Dark Shadow Gray. \n\nThe Ford SVT Lightning was manufactured by Ford of Canada at its Ontario Truck facility in Oakville, Ontario; in was closed in 2004. Special features specific to the Lightning included:\n\n\nFor 2002, the Lincoln-Mercury division of Ford introduced the Lincoln Blackwood, the first pickup truck ever sold by the Lincoln brand. Brought into production after a positive reception to a 1999 concept vehicle, the Blackwood was a variant of the Ford F-150 SuperCrew introduced for 2001. \n\nStyled with the front fascia of the Lincoln Navigator SUV, the Blackwood diverged from the F-150 in terms of functionality. In place of a pickup bed, the Blackwood was given a stainless-steel cargo area lined with carpet covered with a power-operated tonneau; the plastic body panels of the pickup bed were styled as black wood with pinstripes. To match the simulated wood design of the pickup bed, Lincoln offered black as the only body color for the Blackwood. Sharing its interior with the Navigator, the Blackwood was fitted with four seats (with a center console between the rear seats).\n\nEquipped only with rear-wheel drive, the Blackwood shared its 300 hp 5.4L 32-valve V8 with the Navigator. \n"}
{"id": "4894035", "url": "https://en.wikipedia.org/wiki?curid=4894035", "title": "Gas dynamic laser", "text": "Gas dynamic laser\n\nA gas dynamic laser (GDL) is a laser based on differences in relaxation velocities of molecular vibrational states. The lasing medium gas has such properties that an energetically lower vibrational state relaxes faster than a higher vibrational state, and so a population inversion is achieved in a particular time. It was invented by Edward Gerry and Arthur Kantrowitz at Avco Everett Research Laboratory in 1966.\n\nPure gas dynamic lasers usually use a combustion chamber, supersonic expansion nozzle, and CO, in a mixture with nitrogen or helium, as the laser medium.\n\nGas dynamic lasers can be pumped by combustion or adiabatic expansion of gas. Any hot and compressed gas with appropriate vibrational structure could be utilized.\n\nThe explosively pumped gas dynamic laser is a version of GDL pumped by expansion of explosion products. Hexanitrobenzene and/or tetranitromethane with metal powder is the preferred explosive. This device could have very high pulsed peak power output suitable for laser weapons.\n\n\nAlmost any chemical laser uses gas-dynamic processes to increase its efficiency. \nHigh energy efficiency (as high as 30%) and very high power output make GDL suitable for some (especially military) applications.\n\n\n"}
{"id": "19635896", "url": "https://en.wikipedia.org/wiki?curid=19635896", "title": "Georgian Bay Land Trust", "text": "Georgian Bay Land Trust\n\nThe Georgian Bay Land Trust (GBLT) is a not-for-profit, registered charity, whose goal is to protect the uniqueness of the eastern shore and North Channel (Ontario) of Georgian Bay through the securement and ongoing stewardship of land that has ecological, geological and historical importance.\n\nThrough this work, the GBLT works to educate and promote the importance of this area. The GBLT is one of 45 land trusts working in Ontario to protect important private land from development and loss of native habitats and flora and fauna. As one of the first land trusts in Ontario, GBLT has been a leader in many facets, helping to promote the land trust movement and support other land trusts in their interests and activities in other areas throughout the province.\n\nSince forming in 1991, the GBLT has secured and currently stewards over that have come to the Trust via donations, purchases, conservation easements and a lease. Currently, both independently and in partnership with the Nature Conservancy of Canada, our land protection committee is actively involved in discovering new opportunities and assessing these opportunities with the goals and objectives of the Trust and the funds available to purchase, if necessary and steward these potential projects.\n\n\n"}
{"id": "3777622", "url": "https://en.wikipedia.org/wiki?curid=3777622", "title": "Grid friendly", "text": "Grid friendly\n\nElectrical devices are considered grid friendly if they operate in a manner that supports electrical power grid reliability. Basic grid-friendly devices may incorporate features that work to offset short-term undesirable changes in line frequency or voltage; more sophisticated devices may alter their operating profile based on the current market price for electricity, reducing load when prices are at a peak. Grid-friendly devices can include major appliances found in homes, commercial building systems such as HVAC, and many industrial systems.\n\nMost electric systems use alternating current with a nominal frequency of 50 or 60 Hz (hertz) to deliver energy produced by electrical generators to the electricity consumers. When the amount of electric power produced by the generators exceeds the power used by the customers, the frequency of the electricity rises. Conversely, when the amount of electric power produced is less than what is consumed, the frequency drops. Therefore frequency is an accurate indicator of the system-wide (called \"global\") balance between supply and demand. Without grid-friendly frequency response, the rate at which the frequency changes is dependent principally on the system's total inertia (which is not very controllable) and the aggregate response of the generators' control systems (which can only be controlled relatively slowly). In contrast, grid-friendly devices can act very quickly.\n\nA grid-friendly device can respond to changes in frequency by reducing or interrupting the demand for electric power (called \"load\") when the frequency drops below a certain threshold, and/or increasing load when the frequency rises. Although a single grid-friendly device may be a very small load, the fraction of the total load that can be controlled by frequency at any time is usually sufficient to provide under-frequency protection to the system before more drastic measures like black-outs are required.\n\nThe advantage of grid-friendly frequency response is that frequency is ubiquitous on an electric system. When a generator shuts down in one part of the system, all the loads everywhere in the system can simultaneously detect the change and respond instantly and appropriately without the need for a control system to detect the problem, a control center to make a decision, or a telecommunications network to deliver commands to millions of devices. This type of behavior changes frequency from a simple electrodynamic and control systems input to an emergent property. While there is still some controversy on the subject, it is believed that complex systems utilizing self-regulation through emergence are generally more resilient and flexible than are simpler top-down command and control systems.\n\nIn contrast to frequency, voltage varies widely throughout electric systems, because it is the voltage difference between two devices that largely determines the direction and magnitude of the current (hence the power) that flows between them. Therefore voltage is a more local phenomenon, and grid-friendly devices that respond to voltage will support more local aspects of the electric delivery system. However, load types such as thermally protected induction motors and power electronics can respond poorly to significant voltage changes. When a sufficient fraction of the power demand in a region is composed of such loads, their collective response can lead to fault-induced delayed voltage recovery behavior, which may have adverse effects on transmission system reliability and may require mitigation to avoid initiating system outages.\n\nWhile frequency and voltage respond to physical phenomena on the electric system, grid-friendly price response is designed to address economic phenomena. With the increasing application of electricity markets to manage the efficient distribution of electric power, more consumers are exposed to electricity prices that change over time, rather than fixed for months or years. In general, higher prices occur at times when the electric system is running short of supply. The purpose of grid-friendly price response is to promote demand response among electricity consumers. Demand response is one means of reducing the market power of electricity suppliers when production runs short. Grid-friendly response to price also allows consumers to reduce their energy costs by using less electricity when prices are high, and more electricity when prices are low.\n\nA demonstration of grid-friendly technology was conducted for the United States Department of Energy in 2006 and 2007 in the Northwest region of the United States. Participants included local utilities, residential and commercial customers, industrial loads belonging to municipalities, and a number of vendors and researchers. The grid-friendly technology demonstration showed that common residential appliances did automatically detect grid problems expressed as frequency deviations and reduced energy consumption at critical moments. The Olympic Peninsula demonstration showed that residential, commercial, and industrial loads did adjust their consumption patterns based on price signals emanating from a distribution-level market operated as a double action. Both of these projects showed how grid-friendly technologies can and do reduce pressure on the electric grid during time of peak demand.\n\n\n"}
{"id": "15107340", "url": "https://en.wikipedia.org/wiki?curid=15107340", "title": "Guest star (astronomy)", "text": "Guest star (astronomy)\n\nIn Chinese astronomy, a guest star () is a star which has suddenly appeared in a place where no star had previously been observed and becomes invisible again after some time. The term is a literal translation from ancient Chinese astronomical records.\n\nModern astronomy recognizes that guest stars are manifestations of cataclysmic variable stars: novae and supernovae. The term \"guest star\" is used in the context of ancient records, since the exact classification of an astronomical event in question is based on interpretations of old records, including inference, rather than on direct observations. \n\nIn ancient Chinese astronomy, guest stars were one of the three types of highly transient objects (bright heavenly bodies); the other two (彗星, \"huixing\", “broom star”, a comet with a tail; and \"xing bo\", “fuzzy star”, a comet without a tail) being comets in modern understanding. The earliest Chinese record of guest stars is contained in \"Han Shu\" (漢書), the history of Han Dynasty (206 BCE – 220 CE), and all subsequent dynastic histories had such records. These contain one of the clearest early descriptions consistent with a supernova, posited to be left over by object SN 185, thus identified as a supernova remnant of the exact year 185 CE. \nChronicles of the contemporary Ancient Europeans are more vague when consulted for supernovae candidates. Whether due to weather or other reasons for lack of observation, astronomers have questioned why the notable remnant attributed to Chinese observations of a guest star in 1054 AD (see SN 1054), is missing from the European records.\n"}
{"id": "55645318", "url": "https://en.wikipedia.org/wiki?curid=55645318", "title": "Ian Duncan (businessman)", "text": "Ian Duncan (businessman)\n\nDr Ian Duncan is a businessman active in the Australian resources sector. He is a past president of operations at the Olympic Dam mine in South Australia under Western Mining Corporation. He was Chairman of the London-based Uranium Institute (now the World Nuclear Association) in 1995-1996. From the 1990s to the present, Duncan has advocated for nuclear industrial development in Australia, specifically the development of facilities to store and dispose of nuclear waste and the legalization and development of nuclear power plants for the generation of electricity. He is a Fellow of the Australian Academy of Technology, Science and Engineering (ATSE), the Australasian Institute of Mining and Metallurgy (AusIMM), and Engineers Australia.\n\nDuncan joined Western Mining Corporation (WMC Resources) in 1971 and remained with the company for 27 years. He commenced as operations manager of the company's exploration division, and rose to General Manager of the Olympic Dam mine. \n\nDuring the 2000s and 2010s, Duncan advocated for the development of nuclear power in Australia. \n\nIn 2005, Duncan described the status of breeder reactors as seeing \"little advancement\". He told the ABC that \"There is abundant uranium to meet all future requirements for light water reactors that are planned around the world.\" That year he also became a non-executive director of Perth-based company, Energy Ventures Ltd. As of 2017, the company owns various uranium and energy exploration and development projects in Australia and internationally. The most advanced of these is the Aurora Uranium project in Oregon, USA.\n\nIn 2006, Duncan described opponents of nuclear power as often using the subject of nuclear waste as a \"tool\". He argued that \"due to global progress and example, the disposal of nuclear waste need not be a showstopper for nuclear power in Australia.\" \n\nIn 2009, Duncan told \"The Age\" that he thought it was \"time to seriously think about nuclear power as part of the baseload electricity generation. It's time that we moved along from the caveman attitude of just picking up and burning things. We should move to a higher order of source of energy.\" He also referred to nuclear power as clean electricity with virtually no greenhouse gas emissions. He estimated that Australia's first nuclear power station could cost $6-8 billion and take ten years to design, build and commission.\n\nIn April 2010, Duncan spoke at an event in Western Australia hosted by CEDA entitled \"Assessing the Prospects of Nuclear Power in Australia\". Michael Angwin of the Australian Uranium Association and Daniela Stehlik of the National Academies Forum also spoke at the event.\n\nIn September 2010, Duncan told an Australian Institute of Energy Syposium held by the Perth Branch that he predicted that Australia's first nuclear reactors would be light-water reactors with a 600–1200 MWe capacity each, built in pairs, with sea water cooled condensers. They would be fueled with enriched uranium and spent nuclear fuel would ultimately be disposed of into ancient and stable underground rock storage facilities.\n\nFollowing the 2011 Fukushima nuclear disaster, Duncan remained optimistic about the prospect of nuclear power in Australia.\n\nIn 2013, he wrote that \"if the economics for electricity generation is impacted by a carbon tax or a compulsory carbon capture and sequestration, then nuclear generation will be economically competitive.\" \n\nAt a conference entitled \"Nuclear Power for Australia?\" held by ATSE in July of that year, he argued that if Australia were to aspire to produce electricity with nuclear power plants, a new Commonwealth agency 'inspectorate' with regulatory control over the choice of technology, siting, construction and operation should be established by 2016. He proposed the working title Nuclear Installations Regulator for Australia (NIRA) and presented a detailed timeline of potential milestones to achieve between 2013 and criticality for the first reactor in 2030. His presentation flagged \"Restart nuclear debate\" as a first step, during the period 2014-2017. \n\nIn 2015, a Nuclear Fuel Cycle Royal Commission commenced in South Australia tasked with investigating the opportunities and risks associated with South Australia's future role in the nuclear fuel cycle. Duncan's submission to the Royal Commission identified areas of the state's coastline he believed were potentially suitable for the siting of nuclear power plants. \n\nDuncan was a Member of the SYNROC Steering Committee (whose work was based on research and development undertaken by ANSTO and the ANU). \n\nIn 2002, after his retirement, Duncan completed a doctorate at Oxford University considering the \"interface between society and the disposal of radioactive waste\". His publication was entitled \"Radioactive Waste: Risk, Reward, Space and Time Dynamics\" and he followed it with opinion pieces and media commentary on the subject during the early 2000s. In 2003 he anticipated that \"by far the biggest advancement will come from a better understanding of the public psyche by industry and not by a better understanding of the industry by the public.\"\n\nIn 2003, he made the claim that \"the Premier that supports the siting of a national repository will probably be remembered as the statesman who cleaned up Australia!\"\n\nIn the mid-2000s Duncan was actively consulting in the area, and was consulted during the Uranium Mining, Processing and Nuclear Energy Review (UMPNER) in 2006.\n\nIn 2006, Duncan maintained the view that Australia had an obligation to appropriately manage its own, domestically produced nuclear waste. In an article published in \"Focus\", the magazine of ATSE, he wrote:\"There is no justification for the importation of other countries’ radioactive waste, nor for participation in any so called ‘international attempts’ at nuclear waste disposal. Our moral obligation is to properly dispose of our own waste and that is achievable.\"In 2015 Duncan was appointed to the Independent Advisory Panel of the National Radioactive Waste Management Project for the Australian Government. \n\nIn September 2016, Duncan gave a presentation on the work of the Royal Commission and the National Radioactive Waste Management Project to members of the Minerals, Processing and Extractive Metallurgy Division of the Institute of Materials, Minerals and Mining. During his talk he mentioned that 40 years had transpired since the Flower's Report was published in the UK, which prompted environmental consideration of the fate of nuclear wastes and their future management.\n"}
{"id": "42394237", "url": "https://en.wikipedia.org/wiki?curid=42394237", "title": "Integrated manure utilization system", "text": "Integrated manure utilization system\n\nIMUS (also known as integrated manure utilization system) is an anaerobic digestion technology that converts organic material into biogas that is used to produce electricity, heat and nutrients. The technology uses waste such as municipal waste, cow manure, sand laden feed lot waste, and food processing waste. The technology can be integrated with other industrial process, such as municipal facilities, open pen feedlots, food processing, and ethanol plants. The technology was developed in 1999 by Himark BioGas.\n"}
{"id": "27005851", "url": "https://en.wikipedia.org/wiki?curid=27005851", "title": "Khersan-3 Dam", "text": "Khersan-3 Dam\n\nKhersan-3 dam is an arch dam currently under construction on the Khersan River, a tributary of the Karun River, in Iran. When complete it will have an installed capacity of 400 MW. It is situated near Atashgah in Chaharmahal and Bakhtiari Province and is a complimentary dam to Khersan project along with the proposed Khersan-1 and Khersan-2 Dams. Construction began in 2007 and it is expected to become operational in 2015.\n\n"}
{"id": "5545401", "url": "https://en.wikipedia.org/wiki?curid=5545401", "title": "Kislaya Guba", "text": "Kislaya Guba\n\nKislaya Guba (meaning \"sour bay\" in Russian) is a fjord on the Kola Peninsula near Murmansk, Russia. The fjord is connected to the Barents Sea to the north and is primarily known as the site of the experimental tidal power project, Kislaya Guba Tidal Power Station.\n"}
{"id": "51853411", "url": "https://en.wikipedia.org/wiki?curid=51853411", "title": "Lamu Wind Power Station", "text": "Lamu Wind Power Station\n\nLamu Wind Power Station, also Lamu Wind Farm, is a planned wind-powered power station in Kenya.\n\nThe power station would be located in Baharini Village, near the town of Mpeketoni, in Lamu County, approximately west of the location of Port Lamu. This is approximately , by road, south-east of Nairobi, the capital of Kenya.\n\nAs part of efforts to diversify the energy sources in Kenya, Kenwinds Holdings, a private company, plans to establish a 90 MW wind farm in Mpeketoni Division at the Kenyan coast in Lamu County. The planned wind station will sit on of land and consist of 38 wind turbines. The power generated will be evacuated via a new , 220 kV power line from Lamu to Rabai, where it will be integrated into the national grid.\n\nThe power station will be owned and operated by Kenwind Holdings Limited, a Kenyan corporation. Kenwind Holdings is a subsidiary of Electrawinds, a Belgian energy company, which is collaborating on the project. The International Finance Corporation, a branch of the World Bank is providing a portion of the budgeted US$$235 million financing.\n\nIn March 2017, Cordisons International Limited, an American wind-energy developer, went to court to challenge Kenwind Holdings Limited's right to the piece of property on which the development will sit. In May 2018, Kenwind Holdings Limited, the Belgian company, prevailed in court and retained the rights to develop this power station.\n\n\n"}
{"id": "8312245", "url": "https://en.wikipedia.org/wiki?curid=8312245", "title": "Land disposal unit", "text": "Land disposal unit\n\nA land disposal unit, or LDU, is a site in which hazardous waste is remedied through natural and man-made processes.\n\nTypes of LDUs for hazardous waste disposal (see RCRA Section 3004(k)):\n\n\n"}
{"id": "934064", "url": "https://en.wikipedia.org/wiki?curid=934064", "title": "Low-κ dielectric", "text": "Low-κ dielectric\n\nIn semiconductor manufacturing, a low-κ is a material with a small relative dielectric constant relative to silicon dioxide. Although the proper symbol for the relative dielectric constant is the Greek letter κ (kappa), in conversation such materials are referred to as being \"low-k\" (low-kay) rather than \"low-κ\" (low-kappa). Low-κ dielectric material implementation is one of several strategies used to allow continued scaling of microelectronic devices, colloquially referred to as extending Moore's law. In digital circuits, insulating dielectrics separate the conducting parts (wire interconnects and transistors) from one another. As components have scaled and transistors have gotten closer together, the insulating dielectrics have thinned to the point where charge build up and crosstalk adversely affect the performance of the device. Replacing the silicon dioxide with a low-κ dielectric of the same thickness reduces parasitic capacitance, enabling faster switching speeds and lower heat dissipation.\n\nThe relative dielectric constant of SiO, the insulating material still used in silicon chips, is 3.9. This number is the ratio of the permittivity of SiO divided by permittivity of vacuum, ε/ε,where ε = 8.854×10 pF/μm. There are many materials with lower relative dielectric constants but few of them can be suitably integrated into a manufacturing process. Development efforts have focused primarily on the following classes of materials:\n\nBy doping SiO with fluorine to produce fluorinated silica glass, the relative dielectric constant is lowered from 3.9 to 3.5. Fluorine-doped oxide materials were used for the 180 nm and 130 nm technology nodes.\n\nBy doping SiO with carbon, one can lower the relative dielectric constant to 3.0, the density to 1.4 g/cm and the thermal conductivity to 0.39 W/(m*K). The semiconductor industry has been using the organosilicate glass dielectrics since the 90 nm technology node.\n\nVarious methods may be employed to create voids or pores in a silicon dioxide dielectric. Voids can have a relative dielectric constant of nearly 1, thus the dielectric constant of the porous material may be reduced by increasing the porosity of the film. Relative dielectric constants lower than 2.0 have been reported. Integration difficulties related to porous silicon dioxide implementation include low mechanical strength and difficult integration with etch and polish processes.\n\nPorous organosilicate materials are usually obtained by a two-step procedure where the first step consists of the co-deposition of a labile organic phase (known as porogen) together with an organosilicate phase resulting in an organic-inorganic hybrid material. In the second step, the organic phase is decomposed by UV curing or annealing at a temperature of up to 400°C, leaving behind pores in the organosilicate low-κ materials. Porous organosilicate glasses have been employed since the 45 nm technology node. \n\nPolymeric dielectrics are generally deposited by a spin-on approach, which is traditionally used for the deposition of photoresist materials, rather than chemical vapor deposition. Integration difficulties include low mechanical strength, coefficient of thermal expansion (CTE) mismatch and thermal stability. Some examples of spin-on organic low-κ polymers are polyimide, polynorbornenes, benzocyclobutene, and PTFE.\n\nThere are two kinds of silicon based polymeric dielectric materials, hydrogen silsesquioxane (HSQ) and methylsilsesquioxane (MSQ).\n\n\n"}
{"id": "18899", "url": "https://en.wikipedia.org/wiki?curid=18899", "title": "Mendelevium", "text": "Mendelevium\n\nMendelevium is a synthetic element with chemical symbol Md (formerly Mv) and atomic number 101. A metallic radioactive transuranic element in the actinide series, it is the first element that currently cannot be produced in macroscopic quantities through neutron bombardment of lighter elements. It is the third-to-last actinide and the ninth transuranic element. It can only be produced in particle accelerators by bombarding lighter elements with charged particles. A total of sixteen mendelevium isotopes are known, the most stable being Md with a half-life of 51 days; nevertheless, the shorter-lived Md (half-life 1.17 hours) is most commonly used in chemistry because it can be produced on a larger scale.\n\nMendelevium was discovered by bombarding einsteinium with alpha particles in 1955, the same method still used to produce it today. It was named after Dmitri Mendeleev, father of the periodic table of the chemical elements. Using available microgram quantities of the isotope einsteinium-253, over a million mendelevium atoms may be produced each hour. The chemistry of mendelevium is typical for the late actinides, with a preponderance of the +3 oxidation state but also an accessible +2 oxidation state. Owing to the small amounts of produced mendelevium and all of its isotopes having relatively short half-lives, there are currently no uses for it outside basic scientific research.\n\nMendelevium was the ninth transuranic element to be synthesized. It was first synthesized by Albert Ghiorso, Glenn T. Seaborg, Gregory Robert Choppin, Bernard G. Harvey, and team leader Stanley G. Thompson in early 1955 at the University of California, Berkeley. The team produced Md (half-life of 77 minutes) when they bombarded an Es target consisting of only a billion (10) einsteinium atoms with alpha particles (helium nuclei) in the Berkeley Radiation Laboratory's 60-inch cyclotron, thus increasing the target's atomic number by two. Md thus became the first isotope of any element to be synthesized one atom at a time. In total, seventeen mendelevium atoms were produced. This discovery was part of a program, begun in 1952, that irradiated plutonium with neutrons to transmute it into heavier actinides. This method was necessary as the previous method used to synthesize transuranic elements, neutron capture, could not work because of a lack of known beta decaying isotopes of fermium that would produce isotopes of the next element, mendelevium, and also due to the very short half-life to spontaneous fission of Fm that thus constituted a hard limit to the success of the neutron capture process.\n\nTo predict if the production of mendelevium would be possible, the team made use of a rough calculation. The number of atoms that would be produced would be approximately equal to the product of the number of atoms of target material, the target's cross section, the ion beam intensity, and the time of bombardment; this last factor was related to the half-life of the product when bombarding for a time on the order of its half-life. This gave one atom per experiment. Thus under optimum conditions, the preparation of only one atom of element 101 per experiment could be expected. This calculation demonstrated that it was feasible to go ahead with the experiment. The target material, einsteinium-253, could be produced readily from irradiating plutonium: one year of irradiation would give a billion atoms, and its three-week half-life meant that the element 101 experiments could be conducted in one week after the produced einsteinium was separated and purified to make the target. However, it was necessary to upgrade the cyclotron to obtain the needed intensity of 10 alpha particles per second; Seaborg applied for the necessary funds.\nWhile Seaborg applied for funding, Harvey worked on the einsteinium target, while Thomson and Choppin focused on methods for chemical isolation. Choppin suggested using α-hydroxyisobutyric acid to separate the mendelevium atoms from those of the lighter actinides. The actual synthesis was done by a recoil technique, introduced by Albert Ghiorso. In this technique, the einsteinium was placed on the opposite side of the target from the beam, so that the recoiling mendelevium atoms would get enough momentum to leave the target and be caught on a catcher foil made of gold. This recoil target was made by an electroplating technique, developed by Alfred Chetham-Strode. This technique gave a very high yield, which was absolutely necessary when working with such a rare and valuable product as the einsteinium target material. The recoil target consisted of 10 atoms of Es which were deposited electrolytically on a thin gold foil. It was bombarded by 41 MeV alpha particles in the Berkeley cyclotron with a very high beam density of 6×10 particles per second over an area of 0.05 cm. The target was cooled by water or liquid helium, and the foil could be replaced.\n\nInitial experiments were carried out in September 1954. No alpha decay was seen from mendelevium atoms; thus, Ghiorso suggested that the mendelevium had all decayed by electron capture to fermium and that the experiment should be repeated to search instead for spontaneous fission events. The repetition of the experiment happened in February 1955.\nOn the day of discovery, 19 February, alpha irradiation of the einsteinium target occurred in three three-hour sessions. The cyclotron was in the University of California campus, while the Radiation Laboratory was on the next hill. To deal with this situation, a complex procedure was used: Ghiorso took the catcher foils (there were three targets and three foils) from the cyclotron to Harvey, who would use aqua regia to dissolve it and pass it through an anion-exchange resin column to separate out the transuranium elements from the gold and other products. The resultant drops entered a test tube, which Choppin and Ghiorso took in a car to get to the Radiation Laboratory as soon as possible. There Thompson and Choppin used a cation-exchange resin column and the α-hydroxyisobutyric acid. The solution drops were collected on platinum disks and dried under heat lamps. The three disks were expected to contain respectively the fermium, no new elements, and the mendelevium. Finally, they were placed in their own counters, which were connected to recorders such that spontaneous fission events would be recorded as huge deflections in a graph showing the number and time of the decays. There thus was no direct detection, but by observation of spontaneous fission events arising from its electron-capture daughter Fm. The first one was identified with a \"hooray\" followed by a \"double hooray\" and a \"triple hooray\". The fourth one eventually officially proved the chemical identification of the 101st element, mendelevium. In total, five decays were reported up till 4 a.m. Seaborg was notified and the team left to sleep. Additional analysis and further experimentation showed the produced mendelevium isotope to have mass 256 and to decay by electron capture to fermium-256 with a half-life of 1.5 h.\n\nBeing the first of the second hundred of the chemical elements, it was decided that the element would be named \"mendelevium\" after the Russian chemist Dmitri Mendeleev, father of the periodic table. Because this discovery came during the Cold War, Seaborg had to request permission of the government of the United States to propose that the element be named for a Russian, but it was granted. The name \"mendelevium\" was accepted by the International Union of Pure and Applied Chemistry (IUPAC) in 1955 with symbol \"Mv\", which was changed to \"Md\" in the next IUPAC General Assembly (Paris, 1957).\n\nIn the periodic table, mendelevium is located to the right of the actinide fermium, to the left of the actinide nobelium, and below the lanthanide thulium. Mendelevium metal has not yet been prepared in bulk quantities, and bulk preparation is currently impossible. Nevertheless, a number of predictions and some preliminary experimental results have been done regarding its properties.\n\nThe lanthanides and actinides, in the metallic state, can exist as either divalent (such as europium and ytterbium) or trivalent (most other lanthanides) metals. The former have fds configurations, whereas the latter have fs configurations. In 1975, Johansson and Rosengren examined the measured and predicted values for the cohesive energies (enthalpies of crystallization) of the metallic lanthanides and actinides, both as divalent and trivalent metals. The conclusion was that the increased binding energy of the [Rn]5f6d7s configuration over the [Rn]5f7s configuration for mendelevium was not enough to compensate for the energy needed to promote one 5f electron to 6d, as is true also for the very late actinides: thus einsteinium, fermium, mendelevium, and nobelium were expected to be divalent metals. The increasing predominance of the divalent state well before the actinide series concludes is attributed to the relativistic stabilization of the 5f electrons, which increases with increasing atomic number. Thermochromatographic studies with trace quantities of mendelevium by Zvara and Hübener from 1976 to 1982 confirmed this prediction. In 1990, Haire and Gibson estimated mendelevium metal to have an enthalpy of sublimation between 134 and 142 kJ/mol. Divalent mendelevium metal should have a metallic radius of around . Like the other divalent late actinides (except the once again trivalent lawrencium), metallic mendelevium should assume a face-centered cubic crystal structure. Mendelevium's melting point has been estimated at 827 °C, the same value as that predicted for the neighboring element nobelium. Its density is predicted to be around .\n\nThe chemistry of mendelevium is mostly known only in solution, in which it can take on the +3 or +2 oxidation states. The +1 state has also been reported, but has not yet been confirmed.\n\nBefore mendelevium's discovery, Seaborg and Katz predicted that it should be predominantly trivalent in aqueous solution and hence should behave similarly to other tripositive lanthanides and actinides. After the synthesis of mendelevium in 1955, these predictions were confirmed, first in the observation at its discovery that it eluted just after fermium in the trivalent actinide elution sequence from a cation-exchange column of resin, and later the 1967 observation that mendelevium could form insoluble hydroxides and fluorides that coprecipitated with trivalent lanthanide salts. Cation-exchange and solvent extraction studies led to the conclusion that mendelevium was a trivalent actinide with an ionic radius somewhat smaller than that of the previous actinide, fermium. Mendelevium can form coordination complexes with 1,2-cyclohexanedinitrilotetraacetic acid (DCTA).\n\nIn reducing conditions, mendelevium(III) can be easily reduced to mendelevium(II), which is stable in aqueous solution. The standard reduction potential of the \"E\"°(Md→Md) couple was variously estimated in 1967 as −0.10 V or −0.20 V: later 2013 experiments established the value as . In comparison, \"E\"°(Md→Md) should be around −1.74 V, and \"E\"°(Md→Md) should be around −2.5 V. Mendelevium(II)'s elution behavior has been compared with that of strontium(II) and europium(II).\n\nIn 1973, mendelevium(I) was reported to have been produced by Russian scientists, who obtained it by reducing higher oxidation states of mendelevium with samarium(II). It was found to be stable in neutral water–ethanol solution and be homologous to caesium(I). However, later experiments found no evidence for mendelevium(I) and found that mendelevium behaved like divalent elements when reduced, not like the monovalent alkali metals. Nevertheless, the Russian team conducted further studies on the thermodynamics of cocrystallizing mendelevium with alkali metal chlorides, and concluded that mendelevium(I) had formed and could form mixed crystals with divalent elements, thus cocrystallizing with them. The status of the +1 oxidation state is still tentative.\n\nAlthough \"E\"°(Md→Md) was predicted in 1975 to be +5.4 V, suggesting that mendelevium(III) could be oxidized to mendelevium(IV), 1967 experiments with the strong oxidizing agent sodium bismuthate were unable to oxidize mendelevium(III) to mendelevium(IV).\n\nA mendelevium atom has 101 electrons, of which at least three (and perhaps four) can act as valence electrons. They are expected to be arranged in the configuration [Rn]5f7s (ground state term symbol F), although experimental verification of this electron configuration had not yet been made as of 2006. In forming compounds, three valence electrons may be lost, leaving behind a [Rn]5f core: this conforms to the trend set by the other actinides with their [Rn] 5f electron configurations in the tripositive state. The first ionization potential of mendelevium was measured to be at most (6.58 ± 0.07) eV in 1974, based on the assumption that the 7s electrons would ionize before the 5f ones; this value has since not yet been refined further due to mendelevium's scarcity and high radioactivity. The ionic radius of hexacoordinate Md had been preliminarily estimated in 1978 to be around 91.2 pm; 1988 calculations based on the logarithmic trend between distribution coefficients and ionic radius produced a value of 89.6 pm, as well as an enthalpy of hydration of . Md should have an ionic radius of 115 pm and hydration enthalpy −1413 kJ/mol; Md should have ionic radius 117 pm.\n\nSixteen isotopes of mendelevium are known, with mass numbers from 245 to 260; all are radioactive. Additionally, five nuclear isomers are known: Md, Md, Md, Md, and Md. Of these, the longest-lived isotope is Md with a half-life of 51.5 days, and the longest-lived isomer is Md with a half-life of 58.0 minutes. Nevertheless, the slightly shorter-lived Md (half-life 1.17 hours) is more often used in chemical experimentation because it can be produced in larger quantities from alpha particle irradiation of einsteinium. After Md, the next most stable mendelevium isotopes are Md with a half-life of 31.8 days, Md with a half-life of 5.52 hours, Md with a half-life of 1.60 hours, and Md with a half-life of 1.17 hours. All of the remaining mendelevium isotopes have half-lives that are less than an hour, and the majority of these have half-lives that are less than 5 minutes.\n\nThe half-lives of mendelevium isotopes mostly increase smoothly from Md onwards, reaching a maximum at Md. Experiments and predictions suggest that the half-lives will then decrease, apart from Md with a half-life of 31.8 days, as spontaneous fission becomes the dominant decay mode due to the mutual repulsion of the protons posing a limit to the island of relative stability of long-lived nuclei in the actinide series.\n\nMendelevium-256, the chemically most important isotope of mendelevium, decays through electron capture 90.7% of the time and alpha decay 9.9% of the time. It is most easily detected through the spontaneous fission of its electron-capture daughter fermium-256, but in the presence of other nuclides that undergo spontaneous fission, alpha decays at the characteristic energies for mendelevium-256 (7.205 and 7.139 MeV) can provide more useful identification.\n\nThe lightest mendelevium isotopes (Md to Md) are mostly produced through bombardment of bismuth targets with heavy argon ions, while slightly heavier ones (Md to Md) are produced by bombarding plutonium and americium targets with lighter ions of carbon and nitrogen. The most important and most stable isotopes are in the range from Md to Md and are produced through bombardment of einsteinium isotopes with alpha particles: einsteinium-253, -254, and -255 can all be used. Md is produced as a daughter of No, and Md can be produced in a transfer reaction between einsteinium-254 and oxygen-18. Typically, the most commonly used isotope Md is produced by bombarding either einsteinium-253 or -254 with alpha particles: einsteinium-254 is preferred when available because it has a longer half-life and therefore can be used as a target for longer. Using available microgram quantities of einsteinium, femtogram quantities of mendelevium-256 may be produced.\n\nThe recoil momentum of the produced mendelevium-256 atoms is used to bring them physically far away from the einsteinium target from which they are produced, bringing them onto a thin foil of metal (usually beryllium, aluminium, platinum, or gold) just behind the target in a vacuum. This eliminates the need for immediate chemical separation, which is both costly and prevents reusing of the expensive einsteinium target. The mendelevium atoms are then trapped in a gas atmosphere (frequently helium), and a gas jet from a small opening in the reaction chamber carries the mendelevium along. Using a long capillary tube, and including potassium chloride aerosols in the helium gas, the mendelevium atoms can be transported over tens of meters to be chemically analyzed and have their quantity determined. The mendelevium can then be separated from the foil material and other fission products by applying acid to the foil and then coprecipitating the mendelevium with lanthanum fluoride, then using a cation-exchange resin column with a 10% ethanol solution saturated with hydrochloric acid, acting as an eluant. However, if the foil is made of gold and thin enough, it is enough to simply dissolve the gold in aqua regia before separating the trivalent actinides from the gold using anion-exchange chromatography, the eluant being 6 M hydrochloric acid.\n\nMendelevium can finally be separated from the other trivalent actinides using selective elution from a cation-exchange resin column, the eluant being ammonia α-HIB. Using the gas-jet method often renders the first two steps unnecessary. The above procedure is the most commonly used one for the separation of transeinsteinium elements.\n\nAnother possible way to separate the trivalent actinides is via solvent extraction chromatography using bis-(2-ethylhexyl) phosphoric acid (abbreviated as HDEHP) as the stationary organic phase and nitric acid as the mobile aqueous phase. The actinide elution sequence is reversed from that of the cation-exchange resin column, so that the heavier actinides elute later. The mendelevium separated by this method has the advantage of being free of organic complexing agent compared to the resin column; the disadvantage is that mendelevium then elutes very late in the elution sequence, after fermium.\n\nAnother method to isolate mendelevium exploits the distinct elution properties of Md from those of Es and Fm. The initial steps are the same as above, and employs HDEHP for extraction chromatography, but coprecipitates the mendelevium with terbium fluoride instead of lanthanum fluoride. Then, 50 mg of chromium is added to the mendelevium to reduce it to the +2 state in 0.1 M hydrochloric acid with zinc or mercury. The solvent extraction then proceeds, and while the trivalent and tetravalent lanthanides and actinides remain on the column, mendelevium(II) does not and stays in the hydrochloric acid. It is then reoxidized to the +3 state using hydrogen peroxide and then isolated by selective elution with 2 M hydrochloric acid (to remove impurities, including chromium) and finally 6 M hydrochloric acid (to remove the mendelevium). It is also possible to use a column of cationite and zinc amalgam, using 1 M hydrochloric acid as an eluant, reducing Md(III) to Md(II) where it behaves like the alkaline earth metals. Thermochromatographic chemical isolation could be achieved using the volatile mendelevium hexafluoroacetylacetonate: the analogous fermium compound is also known and is also volatile.\n\nAlthough few people come in contact with mendelevium, the International Commission on Radiological Protection has set annual exposure limits for the most stable isotope. For mendelevium-258, the ingestion limit was set at 9×10 becquerels (1 Bq is equivalent to one decay per second), and the inhalation limit at 6000 Bq.\n\n\n"}
{"id": "22774009", "url": "https://en.wikipedia.org/wiki?curid=22774009", "title": "Multi-mission radioisotope thermoelectric generator", "text": "Multi-mission radioisotope thermoelectric generator\n\nThe multi-mission radioisotope thermoelectric generator (MMRTG) is a type of radioisotope thermoelectric generator developed for NASA space missions such as the Mars Science Laboratory (MSL), under the jurisdiction of the United States Department of Energy's Office of Space and Defense Power Systems within the Office of Nuclear Energy. The MMRTG was developed by an industry team of Aerojet Rocketdyne and Teledyne Energy Systems.\n\nSpace exploration missions require safe, reliable, long-lived power systems to provide electricity and heat to spacecraft and their science instruments. A uniquely capable source of power is the radioisotope thermoelectric generator (RTG) – essentially a nuclear battery that reliably converts heat into electricity. Radioisotope power has been used on eight Earth orbiting missions, eight missions travelling to each of the outer planets as well as each of Apollo missions following 11 to Earth's moon. Some of the outer Solar System missions are the Pioneer, Voyager, Ulysses, \"Galileo\", \"Cassini\" and \"New Horizons\" missions. The RTGs on \"Voyager 1\" and \"Voyager 2\" have been operating since 1977. Similarly, Radioisotope Heat Units (RHUs) were used to provide heat to critical components on Apollo 11 as well as the first two generations of Mars rovers. In total, over the last four decades, 26 missions and 45 RTGs have been launched by the United States.\n\nRTGs convert the heat from the natural decay of a radioisotope into electricity. The MMRTG's heat source is plutonium-238 dioxide. Solid-state thermoelectric couples convert the heat to electricity. Unlike solar arrays, the RTGs are not dependent upon the Sun, so they can be used for deep space missions.\n\nIn June 2003, the Department of Energy (DOE) awarded the MMRTG contract to a team led by Aerojet Rocketdyne. Aerojet Rocketdyne and Teledyne Energy Systems collaborated on an MMRTG design concept based on a previous thermoelectric converter design, SNAP-19, developed by Teledyne for previous space exploration missions. SNAP-19s powered \"Pioneer 10\" and \"Pioneer 11\" missions as well as the Viking 1 and Viking 2 landers.\n\nThe MMRTG is powered by 8 Pu-238 dioxide general-purpose heat source (GPHS) modules, provided by the Department of Energy. Initially, these eight GPHS modules generate about 2 kW thermal power.\n\nThe MMRTG design incorporates PbTe/TAGS thermoelectric couples (from Teledyne Energy Systems), where the TAGS material is a material incorporating Tellurium (Te), Silver (Ag), Germanium (Ge) and Antimony (Sb). The MMRTG is designed to produce 125 W electrical power at the start of mission, falling to about 100 W after 14 years. With a mass of 45 kg the MMRTG provides about 2.8 W/kg of electrical power at beginning of life.\n\nThe MMRTG design is capable of operating both in the vacuum of space and in planetary atmospheres, such as on the surface of Mars. Design goals for the MMRTG included ensuring a high degree of safety, optimizing power levels over a minimum lifetime of 14 years, and minimizing weight.\n\n\"Curiosity\", the MSL rover that was successfully landed in Gale Crater on August 6, 2012, uses one MMRTG to supply heat and electricity for its components and science instruments. Reliable power from the MMRTG will allow it to operate for several years.\n\nIn February 20, 2015, a NASA official reported that there is enough plutonium available to NASA to fuel three more MMRTG like the one used by the \"Curiosity\" rover. One is already committed to the Mars 2020 rover. The other two have not been assigned to any specific mission or program, and could be available by late 2021.\n\n\n"}
{"id": "6064049", "url": "https://en.wikipedia.org/wiki?curid=6064049", "title": "Nanofoam", "text": "Nanofoam\n\nNanofoams are a class of nanostructured, porous materials (foams) containing a significant population of pores with diameters less than 100 nm. Aerogels are one example of nanofoam.\n\nIn 2006, researchers produced metal nanofoams by igniting pellets of energetic metal bis(tetrazolato)amine complexes. Nanofoams of iron, cobalt, nickel, copper, silver, and palladium have been prepared through this technique. These materials exhibit densities as low as 11 mg/cm, and surface areas as high as 258 m/g. These foams are effective catalysts.\n\nCarbon nanofoam is an allotrope of carbon discovered in 1997. It consists of a cluster-assembly of carbon atoms strung together in a loose three-dimensional web. The material has a density of 2–10 mg/cm (0.0012 lb/ft).\n\nIn 2014, researchers also fabricated glass nanofoam via femtosecond laser ablation. Their work consisted of raster scanning femtosecond laser pulses over the surface of glass to produce glass nanofoam with ~70 nm diameter wires.\n\n"}
{"id": "4365725", "url": "https://en.wikipedia.org/wiki?curid=4365725", "title": "National Compact Stellarator Experiment", "text": "National Compact Stellarator Experiment\n\nThe National Compact Stellarator Experiment (NCSX) was a magnetic fusion energy experiment based on the stellarator design being constructed at the Princeton Plasma Physics Laboratory (PPPL). NCSX was one of a number of new stellarator designs from the 1990s that arose after studies illustrated new geometries that offered better performance than the simpler machines of the 1950s and 1960s. Compared to the more common tokamak, these were much more difficult to design and build, but produced far more stable plasma, the main problem with successful fusion. \n\nHowever, the design proved to be too difficult to build, repeatedly running over its budget and timelines. The project was eventually cancelled on 22 May 2008, having spent over $70 M.\n\nStellarators are one of the first fusion power concepts, originally designed by Princeton astrophysics Lyman Spitzer in 1952 while riding the chairlifts at Aspen. Spitzer, considering the motion of plasmas in the stars, realized that any simple arrangements of magnets would not confine a plasma inside a machine - the plasma would drift across the fields and eventually strike the vessel. His solution was very simple; by bending the machine through a 180 degree twist, forming a figure-eight instead of a donut, the plasma would alternately find itself on the inside or outside of the vessel, drifting in opposite directions. The cancellation of net drift would not be perfect, but on paper it appeared that the delay in drift rates was more than enough to allow the plasma to reach fusion conditions.\n\nIn practice this proved not to be. A problem seen in all fusion reactor designs of the era was that the plasma ions were drifting much faster than classical theory predicted, hundreds to thousands of times faster. Designs that suggested stability on the order of seconds turned into machines that were stable for microseconds at best. By the mid-1960s the entire fusion energy field appeared stalled. It was only the 1968 introduction of the tokamak design that rescued the field; Soviet machines were performing at least an order of magnitude better than western designs, although still far short of practical values. The improvement was so dramatic that work on other designs largely ended as teams around the world began to study the tokamak approach. This included the latest stellarator designs; the Model C had only recently started working, and was rapidly converted into the Symmetric Tokamak.\n\nBy the late 1980s it was clear that while the tokamak was a great step forward, it also introduced new problems. In particular, the plasma current the tokamak used for stabilization and heating was itself a source of instabilities as the current grew. Much of the subsequent 30 years of tokamak development has focused on ways to increase this current to the levels required to sustain useful fusion while ensuring that same current does not cause the plasma to break up.\n\nAs the magnitude of the problem with the tokamak became evident, fusion teams around the world began to take a fresh look at other design concepts. Among a number of ideas noted during this process, the stellarator in particular appeared to have a number of potential changes that would greatly improve its performance.\n\nThe basic idea of the stellarator was to use the layout of the magnets to cancel out soon drift, but the simple designs of the 1950s did not do this to the degree needed. A greater problem were the instabilities and collisional effects that greatly increased the diffusion rates. In the 1980s it was noted that one way to improve tokamak performance was to use non-circular cross-sections for the plasma confinement area; ions moving in these non-uniform areas would mix and break up the formation of large-scale instabilities. Applying the same logic to the stellarator appeared to offer the same advantages. Yet, as the stellarator lacked, or lowered, the plasma current, the plasma would be more stable from the start.\n\nWhen one considers the magnet layout needed to achieve both goals, a twisted path around the circumference of the device as well as many smaller twists and mixes along the way, the design becomes extremely complex, well beyond the abilities of conventional design tools. It was only through the use of massively parallel computers that the designs could be studied in depth, and suitable magnet designs created. The result was a very compact device, significantly smaller outside than a classical design for any given volume of plasma, with a low aspect ratio. Lower aspect ratios are highly desirable, because they allow a machine of any given power to be smaller, which lowers construction costs.\n\nBy the late 1990s the studies into new stellarator designs had reached a suitable point for the construction of a machine using these concepts. In comparison to the stellarators of the 1960s, the new machines could use superconducting magnets for much higher field strengths, be only slightly larger than the Model C yet have far larger plasma volume, and have a plasma area inside that varied from circular to planar and back while twisting several times.\n\n\n<br>The 18 modular coils have a complicated 3D shape, ~ 9 different curves in different planes. Some of the coils would need 15 minutes to re-cool between high It plasma runs. \n\n\nBaseline total project cost of $102M for completion date of July 2009.\n\nFirst contracts placed in 2004.\n\nWith the design largely complete, the PPPL began the process of building such a machine, the NCSX, which would test all of these concepts. The design used eighteen fantastically complicated hand-wound magnets, which then had to be assembled into a machine where the maximum variation from the perfect placement was no more than across the entire device. The vacuum vessel surrounding all of this was likewise very complex, with the added complication of carrying all of the wiring to feed power to the magnets.\n\nThe assembly tolerances were very tight and required state of the art use of metrology systems including Laser Tracker and photogrammetry equipment. $50 million of additional funding was needed, spread over the next 3 years, to complete the assembly within tolerance requirements. Components for the Stellarator were measured with 3d laser scanning, and inspected to design models at multiple stages in the manufacturing process.\n\nThe required tolerances could not be achieved; As the modules were assembled, parts were found to be in contact, would sag once installed, and other unexpected effects made alignment very difficult. Fixes were worked into the design, but each one further delayed the completion and required more funding. (The 2008 cost estimate was $170M with an August 2013 scheduled completion.) Eventually a go/no-go condition was imposed, and when the goal was not met on budget, the project was cancelled.\n\n\n"}
{"id": "7760071", "url": "https://en.wikipedia.org/wiki?curid=7760071", "title": "Oil Patch Hotline", "text": "Oil Patch Hotline\n\nOil Patch Hotline is a semi-monthly newsletter about the oil and gas production industry in North Dakota, Montana, and Wyoming. It is published in Williston, North Dakota. The parent company is based out of Plymouth, Florida.\n\nPublication began in 1978 as the domestic oil industry flourished with rising energy prices. The cycle of high crude oil prices ended in the early 1980s and with it much of the oil exploration activity in the United States. As a result, the newsletter suspended publication in 1986.\n\nPublication of \"Oil Patch Hotline\" resumed in June 2006 as worldwide energy demand generated renewed interest in domestic oil production.\n\n"}
{"id": "11278795", "url": "https://en.wikipedia.org/wiki?curid=11278795", "title": "Origami paper", "text": "Origami paper\n\nOrigami paper is used to fold origami, the art of paper folding. The only real requirement of the folding medium is that it must be able to hold a crease, but should ideally also be thinner than regular paper for convenience when multiple folds over the same small paper area are required (e.g. such as would be the case if creating an origami bird's \"legs\", \"feet\", and \"beak\").\n\nKami, or koiy paper, is the cheapest paper made specifically for origami, and the most widely available. It was developed for use in schools. The word \"kami\" is simply Japanese for \"paper\", but it has acquired this specific meaning.\n\nKami is thin and easy to fold. It is usually printed only on one side, with a solid color or pattern. These patterns can be as simple as a gradation from red to blue, or as complex as a multi-colored kimono pattern of flowers and cranes with gold foil embellishments. Kami comes in several sizes, but standard sizes include 75 × 75 mm (about 3 × 3 inches), 6-inch squares and 10-inch squares.\n\nThis medium is a slightly more expensive, flashier, paper that is good for retaining creases called paper-backed foil paper, Japanese foil, or simply foil. Foil paper is composed of a thin layer of foil adhered to an extremely thin sheet of paper. The most common colors are silver and gold, but any color is possible in foil paper including bright pink, blue and copper. In many multi-color packs, one sheet each of silver and gold paper is included. These are usually placed on the bottom end of the string if used in a thousand origami cranes.\n\nWashi (wa = Japanese and shi = paper · washi = Japanese paper) is traditionally a fine handmade thin paper coveted by artists and craftspeople. Washi is made with renewable long-fibered crops and is very strong even when thin. Some washi does not hold a sharp fold due to the extremely long and thick fibers of the paper. Occasionally you will find strands of the long fibers (often kozo) in washi. Washi is also accepting of ink, making it easy to print on as it holds very fine detail. Printed washi has a unique and occasionally transparent texture. Washi paper is not as commonly used as kami paper in origami.\n\nChiyogami refers to Japanese hand-screened decorative kozo washi / paper consisting of repetitive patterns. In Japan “Chiyo” means 1,000 Generations & “-Gami” means paper. Originally the design was applied to handmade kozo paper with wood blocks, but today most chiyogami is produced with silkscreen techniques.\n\nPaper was one of the major materials used in making toys and dolls. The brightly printed chino-gami of Edo (Tokyo) and Kyoto and the crisp strength of newly laid kozo paper were fully utilized. Many urban housewives and girls with the spare time for a hobby have taken up the making of sophisticated paper dolls and figurines – both traditional and modern – and provide a steady and major market for the makers of highly decorative colored papers.\n\nBanknotes may be used to fold models as well. Banknotes are common media for folding as the subject in the obverse of the banknote can make a striking appearance on the finished model.\n"}
{"id": "37431028", "url": "https://en.wikipedia.org/wiki?curid=37431028", "title": "Oxygen diffusion-enhancing compound", "text": "Oxygen diffusion-enhancing compound\n\nAn oxygen diffusion-enhancing compound is any substance that increases the availability of oxygen in body tissues by influencing the molecular structure of water in blood plasma and thereby promoting the movement (diffusion) of oxygen through plasma. Oxygen diffusion-enhancing compounds have shown promise in the treatment of conditions associated with hypoxia (a lack of oxygen in tissues) and ischemia (a lack of oxygen in the circulating blood supply). Such conditions include hemorrhagic shock, myocardial infarction (heart attack), and stroke.\n\nOne of the first substances that was reported to produce an oxygen diffusion-enhancing effect was crocetin, a carotenoid that occurs naturally in plants such as \"crocus sativus\", and is related to another carotenoid, saffron. Saffron has been used culturally (e.g., as a dye) and medicinally since ancient times.\n\nTrans sodium crocetinate (TSC), a synthetic drug containing the carotenoid structure of trans crocetin has been extensively investigated in animal disease models and in human clinical trials. Clinical trials of TSC have focused on testing the compound's effectiveness in sensitizing hypoxic cancer cells to radiation therapy in patients with glioblastoma, an aggressive form of brain cancer.\n\nTSC, which is being developed by Diffusion Pharmaceuticals, has been shown to enhance the oxygenation of hypoxic tumor tissue and belongs to a subclass of oxygen diffusion-enhancing compounds known as bipolar trans carotenoid salts.\n\nOxygen diffusion-enhancing compounds are thought to act by exerting hydrophobic forces that interact with water molecules. These interactions result in greater hydrogen bonding among water molecules, which constitute the majority of the blood plasma medium. As hydrogen bonding increases, the overall molecular structure of water in the plasma becomes more lattice-like, a phenomenon known as structure building. Structure building reduces resistance to the movement of oxygen through plasma via diffusion. Since blood plasma offers the major barrier for oxygen to move from the red blood cells and into the tissues, the more structured character of water imparted by the oxygen diffusion-enhancing compound will enhance movement into tissues.\n\nComputer simulations have shown that TSC specifically can increase the transport of oxygen through water by as much as 30 percent.\n"}
{"id": "44121355", "url": "https://en.wikipedia.org/wiki?curid=44121355", "title": "PandaX", "text": "PandaX\n\nThe Particle and Astrophysical Xenon Detector, or PandaX, is a dark matter detection experiment at China Jinping Underground Laboratory (CJPL) in Sichuan, China. The experiment occupies the deepest underground laboratory in the world, and is among the largest of its kind.\n\nThe experiment is run by an international team of about 40 scientists, led by researchers at China's Shanghai Jiao Tong University. The project began in 2009 with researchers from Shanghai Jiao Tong University, Shandong University, the Shanghai Institute of Applied Physics , and the Chinese Academy of Sciences. Researchers from the University of Maryland, Peking University, and the University of Michigan joined two years later. The PandaX team also includes members from the Ertan Hydropower Development Company. Scientists from University of Science and Technology of China, China Institute of Atomic Energy and Sun Yat-Sen University joined PandaX in 2015.\n\nPandaX is a direct-detection experiment, consisting of a dual-phase xenon time projection chamber (TPC) detector. The use of both liquid and gaseous phases of xenon, similarly to the XENON and LUX experiments, allows the location of events to be determined, and gamma ray events to be vetoed. In addition to searching for dark matter events, PandaX is designed to detect Xe-136 double beta decay.\n\nPandaX is located at China Jinping Underground Laboratory (CJPL), the deepest underground laboratory in the world at more than below ground. The depth of the laboratory means the experiment is better shielded from cosmic ray interference than similar detectors, allowing the instrument to be scaled up more easily. The muon flux at CJPL is 66 events per square meter per year, compared with 950 events/m/year at the Sanford Underground Research Facility, home of the LUX experiment, and 8,030 events/m/year at the Gran Sasso lab in Italy, home to the XENON detector. The marble at Jinping is also less radioactive than the rock at Homestake and Gran Sasso, further reducing the frequency of false detections. Wolfgang Lorenzon, a collaborating researcher from the University of Michigan, has commented that \"the big advantage is that PandaX is much cheaper and doesn't need as much shielding material\" as similar detectors.\n\nLike most low-background physics, the experiment is constructing multiple generations of detectors, each serving as a prototype for the next. A larger size allows greater sensitivity, but this is only useful if unwanted \"background events\" can be kept from swamping the desired ones; ever more stringent limits on radioactive contamination are also required. Lessons learned in earlier generations are used to construct later ones.\n\nThe first generation, PandaX-I, operated until late November, 2014. It used of xenon (of which served as a fiducial mass) to probe the low-mass regime (<10 GeV) and verify dark matter signals reported by other detector experiments. PandaX-I was the first dark matter experiment in China to use more than 100 kg of xenon in its detector, and its size was second only to the LUX experiment in the United States.\n\nPandaX-II, completed in March 2015 and currently operational, uses of xenon (approximately 300 kg fiducual) to probe the 10–1,000 GeV regime. TPandsX-II reuses the shield, outer vessel, cryogenics, purification hardware, and general infrastructure from the first version, but uses a much larger time projection chamber, inner vessel of higher purity (much less radioactive Co) stainless steel, and cryostat\n\nThe construction cost of PandaX is estimated at US$15 million, with an initial cost of $8 million for the first stage.\n\nPandaX-II produced some preliminary physics results from a brief commissioning run in late 2015 (November 21 to December 14) before the main physics run currently underway through 2018.\n\nPandaX-II is significantly more sensitive than both the 100-kg XENON100 and 250-kg LUX detectors. XENON100, in Italy has, in the three to four years prior to 2014, produced the highest sensitivities over a wide range of WIMP masses, but was leapfrogged by PandaX-II, which holds the record until XENON1T comes on line.The most recent results on the spin-independent WIMP-nucleon scattering cross-section of PandaX-II were published in 2017. \n\nThe next stages of PandaX are called PandaX-xT. An intermediate stage with a four-ton target (PandaX-4T) is under construction in the second-phase CJPL-II laboratory. The ultimate goal is to build a third generation dark matter detector, which will contain thirty tons of xenon in the sensitive region.\n\nThe majority of the PandaX experimental equipment was transported from Shanghai Jiao Tong University to China Jinping Underground Laboratory in August 2012, and two engineering test runs were conducted in 2013. The initial data-collection run (PandaX-I) began in May 2014. Results from this run were reported in September 2014 in the journal \"Science China Physics, Mechanics & Astronomy\". In the initial run, about 4 million raw events were recorded, with around 10,000 in the expected energy region for WIMP dark matter. Of these, only 46 events were recorded in the quiet inner core of the xenon target. These events were consistent with background radiation, rather than dark matter. The lack of an observed dark-matter signal in the PandaX-I run places strong constraints on previously-reported dark matter signals from similar experiments.\n\nStefan Funk of the SLAC National Accelerator Laboratory has questioned the wisdom of having many separate direct-detection dark matter experiments in different countries, commenting that \"spending all our money on different direct-detection experiments is not worth it.\" Xiangdong Ji, spokesperson for PandaX and a physicist at Shanghai Jiao Tong University, concedes that the international community is unlikely to support more than two multi-tonne detectors, but argues that having many groups working will lead to faster improvement in detection technology. Richard Gaitskell, a spokesperson for the LUX experiment and a physics professor at Brown University, commented, \"I'm excited about seeing China developing a fundamental physics program.\"\n"}
{"id": "2690065", "url": "https://en.wikipedia.org/wiki?curid=2690065", "title": "Pendulum rocket fallacy", "text": "Pendulum rocket fallacy\n\nThe pendulum rocket fallacy is a common fundamental misunderstanding of the mechanics of rocket flight and how rockets remain on a stable trajectory. The first liquid-fuel rocket, constructed by Robert Goddard in 1926, differed significantly from modern rockets in that the rocket engine was at the top and the fuel tank at the bottom of the rocket. It was believed that, in flight, the rocket would \"hang\" from the engine like a pendulum from a pivot, and the weight of the fuel tank would be all that was needed to keep the rocket flying straight up. This belief is incorrect. In actuality, the stability of such a rocket is dependent on other factors. Basic Newtonian mechanics shows that Goddard's rocket is just as stable (or unstable) as it would be if the engine had been mounted below the fuel tank (as it is in most modern rockets).\n\nIf the engine kept pushing the nose straight up even as the rest of the rocket swung like a pendulum under it, the rocket would have a possibility to get pulled straight and be stable. However, the engines are fixed to the rest of the rocket, so when the rocket tilts, the engine won't apply an upward, stabilising force, but simply push forward in whatever direction the rocket happens to be pointing, not steering it one way or the other at all.\n"}
{"id": "35305536", "url": "https://en.wikipedia.org/wiki?curid=35305536", "title": "Petroleum in Uruguay", "text": "Petroleum in Uruguay\n\nUruguay is a petroleum-importing country. For decades, Uruguay has been searching for petroleum reserves on its territory. One of the first such attempts was in 1957, when ANCAP drilled in the north of the country.\n\nIn 2012, Uruguay engaged on a bidding process for the exploration of hydrocarbons. The following companies are interested: BP, BG Group, Total S.A., and Tullow Oil. Soon will begin exploration in offshore platforms, with an investment of over $1.6 billion in 3 years.\n\n"}
{"id": "42777505", "url": "https://en.wikipedia.org/wiki?curid=42777505", "title": "Protectin D1", "text": "Protectin D1\n\nProtectin D1 also known as neuroprotectin D1 (when it acts in the nervous system) and abbreviated most commonly as PD1 or NPD1 is a member of the class of specialized proresolving mediators. Like other members of this class of polyunsaturated fatty acid metabolites, it possesses strong anti-inflammatory, anti-apoptotic and neuroprotective activity. PD1 is an aliphatic acyclic alkene 22 carbons in length with two hydroxyl groups at the 10 and 17 carbon positions and one carboxylic acid group at the one carbon position.\n\nSpecifically, PD1 is an endogenous stereoselective lipid mediator classified as an autocoid protectin. Autacoids are enzymatically derived chemical mediators with distinct biological activities and molecular structures. Protectins are signaling molecules that are produced enzymatically from unsaturated fatty acids. Their molecular structure is characterized by the presence of a conjugated system of double bonds. PD1, like other protectins, is produced by the oxygenation of the ω-3 polyunsaturated fatty acid docosahexaenoic acid (DHA) and it is found in many tissues, such as the retina, the lungs and the nervous system.\n\nPD1 has a significant role as an anti-inflammatory, anti-apoptotic and neuroprotective molecule. Studies in Alzheimer's disease animal models, in stroke patients and in human retina pigment epithelial cells (RPE) have shown that PD1 can potentially reduce inflammation induced by oxidative stress and inhibit the pro-apoptotic signal, thereby preventing cellular degeneration. Finally, recent studies examining the pathogenicity of influenza viruses, including the avian flu (H5N1), have suggested that PD1 can potentially halt the proliferation of the virus, thus protecting respiratory cells from lethal viral infections.\n\nIn vivo, PD1 is mainly produced as a response to inflammatory signals and it is found in various tissues, such as the retina pigment epithelial cells, lung epithelial cells, peripheral blood mononuclear cells (PBMC) and neural tissues. Studies in PBMC have shown that endogenous DHA, the main precursor of PD1, is released by the activity of phospholipase A2. According to these studies, PD1 is preferentially synthesized in PBMC cells skewed to the Type 2 T helper cell phenotype (T2). This suggests that T-cell differentiation plays an important role in the activation of the PD1 biosynthetic pathway. The interaction of PBMC with interleukin 4 (IL-4), a potent inflammatory signal, leads to the differentiation of PBMC to T2 type lymphocytes. In addition, activated T2 cells further release IL-4, leading to the up-regulation of the enzyme 15-lipoxygenase-1 (15-LO-1). 15-LO-1 is a non-heme iron-carrying dioxygenase that adds oxygen atoms in a stereospecific manner on free and esterified ω-3 polyunsaturated fatty acids like DHA. Overall, the biosynthesis of PD1 proceeds through three distinct steps throughout which the activity of 15-LO-1 is essential. In the first step of the biosynthetic pathway, the binding of 15-LO-1 to its substrate (DHA) leads to the formation of the (17\"S\")-hydro(peroxy)-DHA intermediate. This intermediate is rapidly processed to form a 16(17)-epoxide-containing molecule, which is the second intermediate. Finally, in the third step of the pathway, enzymatic hydrolysis of the 16(17)-epoxide-containing intermediate leads to the formation of PD1.\n\nIn general, PD1 in vivo exhibits a potent anti-apoptotic and anti-inflammatory activity in the tissues in which it is localized. DHA, the main PD1 precursor, is mostly found in tissues such as the retinal synapses, photoreceptors, the lungs and the brain, suggesting that these tissues are more likely to be benefited from the protecting activity of PD1.\n\nRPE are essential in the survival and renewal of the photoreceptors in the retina. These cells exhibit a potent phagocytic activity that ensures the proper function of the retina. Therefore, oxidative stress can potentially damage the RPE cells and cause vision impairment. Studies in human RPE cells have suggested that the presence of oxidative stress triggering molecules, such as HO causes the fragmentation of the DNA that in turn triggers apoptosis. These studies have proposed that PD1 acts as a signaling molecule and through its ligand-receptor interaction down-regulates the expression of genes, such as the transcription factor NF-κB. The inhibition of NF-κB results in the down-regulation of the pro-inflammatory gene COX-2 (cyclooxygenase-2) which is responsible for the release of prostaglandins, a potent pro-inflammatory mediator. In addition, PD1 has an important role in regulating the expression of the Bcl-2 family proteins (Bcl-2, Bcl-x, Bax and Bad) that precedes the release of the cytochrome c complex from the mitochondria and the formation of the apoptosome. The presence of PD1 up-regulates the expression of the anti-apoptotic proteins Bcl-2 and Bcl-x, while it inhibits the expression of the pro-apoptotic proteins Bax and Bad. Specifically, PD1 regulates this protein family by promoting the dephosphorylation of Bcl-x by protein phosphatase 2A (PP2A) at residue Ser-62 which in turn heterodimerizes with the pro-apoptotic protein Bax and inactivates it. Consequently, the activity of the Bcl-2 family proteins results in the inhibition of the caspase 3 enzyme, thus preventing apoptosis and promoting RPE cell survival.\n\nAmong others, Alzheimer's disease is characterized by the reduced concentration of PD1 and by the increased concentration of the amyloid-β peptide (Aβ42) that is responsible for the formation of senile plaques and also induces inflammation and apoptosis in neuronal tissues. Aβ42 is generated by the enzymatic cleavage of the β-amyloid precursor protein (βΑPP) through β- and γ- secretases. Like other pro-inflammatory mediators, Aβ42 induces inflammation through the activation of the pro-inflammatory enzyme COX-2 and the release of prostaglandins. Moreover, the release of Aβ42 down-regulates the anti-apoptotic proteins Bcl-2 and Bcl-x and up-regulates the pro-apoptotic proteins Bax and Bad that ultimately lead to the formation of the apoptosome. PD1 in human neuronal glial cells (HNG) has been shown to trigger the down-regulation of βΑPP, thus decreasing the Aβ42 content in neuronal tissues and reducing inflammation and apoptosis. Specifically, PD1 in Alzheimer's disease models has been shown to respond to the increased concentration of the pro-inflammatory molecule Aβ42 by binding and activating the peroxisome proliferator-activated receptor gamma (PPARγ) either directly or via other mechanisms. According to some models the activation of PPARγ leads to increased ubiquitination and degradation of βAPP, thus reducing the release of Aβ42. Furthermore, PD1 inhibits the production of Aβ42 peptide by down-regulating β-secretase-1 (BACE1), while up-regulating the α-secretase ADAM10 and the secreted amyloid precursor protein-α (sAPPα). Overall, the above mechanism leads to the cleavage of βAPP protein though a non-amyloidogenic pathway that halts the formation of Aβ42 and prevents the premature neuronal degeneration.\n\nStudies in cultured human lung epithelial cells infected with the influenza virus H1N1 or H5N1 have found that endogenous production of PD1 decreases dramatically during infection due to the inhibition of 15-LO-1. Furthermore, the same studies have shown that in vivo administration of PD1 to H1N1 infected mice can potentially inhibit both the proliferation of the virus and the inflammation caused by the infection, thus increasing survival. PD1 protects against viral infections by disrupting the virus life cycle. Specifically, PD1 inhibits the binding of viral RNA to specific nuclear export factors in the host cells, thus blocking the export of viral RNA from the nucleus to the cytosol. The nuclear RNA export factor 1(NXF1) is of particular interest in the attenuation of viral infections via the activity of PD1. Specifically, the NXF1 transporter through its middle and C-terminal domains binds to the phenylalanine/glycine repeats in the nucleoporins (Nups) that line the nuclear pore. In the absence of PD1, influenza viral RNA binds to the NXF1 transporter that later binds specifically to Nup62 nucleoporin and exports the viral RNA into the cytosol. However, the administration of PD1 has shown that this lipid mediator specifically inhibits the binding of the viral RNA to NXF1, thus disrupting the proliferation of the virus.\n\nThe large scale industrial production of PD1 is of great interest for pharmaceutical companies in order to harvest the potent anti-inflammatory and anti-apoptotic activities of this lipid mediator. So far, very few stereoselective laboratory syntheses of PD1 have been reported, but with a relatively low yield.\n\nAccording to one method, PD1 is synthesized in 15% yield through an 8-step convergent stereoselective process. Initially, the alkyne, (\"Z\")-3-tertbutyldimethylsiloxy-oct-5-en-1-yne reacts with bromo-\"E\",\"E\",\"Z\",\"Z\"-tetraene ester in a Sonogashira cross-coupling reaction at room temperature in the presence of Pd-(PPh) and CuI using diethylamine as a solvent which produces the bis-hydroxyl-protected methyl ester. Removal of the two tert-butyldimethylsilyl ethers (TBS-protecting groups) is attained with an excess of TBAF in THF at 0 °C which produces a diol containing a conjugated alkyne. The conjugated alkyne is reduced to the methyl ester. In addition, the diol is hydrogenated via a Lindlar catalyst to produce a highly stereoselective triene, while water is eliminated from the diol through a Boland reduction. Finally, the methyl ester undergoes saponification at 0 °C with dilute LiOH (aq.) in methanol followed by acidic work-up with NaHPO (aq.) in order to produce PD1.\n\nAlternatively, PD1 laboratory synthesis proceeds through a different stereoselective method. Initially, hydroboration of a TBS-protected acetylene with SiaBH produces a TBS-protected vinylborane. The TBS-protected vinylborane reacts with vinyliodide in the presence of a Pd-catalyst, sodium hydroxide (NaOH) and THF to produce a TBS-protected alcohol. Later treatment of the TBS-protected alcohol with TBAF removes the protecting group and produces a diol. Finally, the diol is hydrolyzed with LiOH in THF (aq.) to produce PD1.\n\n22-hydroxy-PD1 (22-OH-PD1; i.e. 10\"R\",17\"S\",20-trihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid) is an omega oxidation product of PD1 probably formed in cells by the action of an unidentified Cytochrome P450 omega hydroxylase (see specialized proresolving mediators#Protectins/neuroprotectins). While the omega oxidation of many bioactive fatty acid metabolites such as leukotriene B4, 5-HETE, 5-oxo-eicosatetraenoic acid (i.e. 5-oxo-ETE) results in a ~100-fold fall in their activity, the omega oxidized product of PD1 has been shown to possess potent ease exhibits potent anti-inflammatory and proresolving actions by inhibiting PMN chemotaxis in vivo and in vitro and decreased pro-inflammatory mediator levels in inflammatory exudates of an animal model at levels comparable to PD1.\n\nProtectin DX (PDX; i.e. 10\"S\",17\"S\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"Z\",15\"E\",19\"Z\"-docosahexaenoic acid) is the 13\"Z\",15\"E\",19\"Z\" isomer of NPD1 (which has the 13\"E\",15\"Z\",19\"Z\" double bond configuration)(see specialized proresolving mediators#Protectins/neuroprotectins). An early study mistakenly used PDX instead of PD1 in attributing anti-replicative and clinically beneficial effects in viral influenza disease in a mouse model to PD1. PDX also inhibits influx of circulating leukocytes into the peritoneum in a mouse model of inflammation. PDX has other anti-inflammatory actions. It inhibits COX-1 and COX-2 thereby blocking the formation of pro-inflammatory prostaglandins; it also inhibits the platelet-aggregating action of thromboxane A2 thereby blocking the platelet aggregations responses to agents that depend on platelets to release thromboxane A2.\n\nAspirin-triggered PD1 (AT-PD1 or 17-epi-PD1: i.e. 10\"R\",17\"R\"-dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid) is the 10\"R\"-hydroxy isomer of PD1 (which has the 10\"S\" hydroxy residue) (see specialized proresolving mediators#Protectins/neuroprotectins). AT-PD1 has been shown to a) reduce the infiltration of neutrophils into the peritoneum in a mouse model of inflammatory disease; b) stimulate the Efferocytosis (i.e. engulfment and removal) of neutrophils; and c) reduce brain infarction and stroke in a rodent model.\n\n10-Epi-PD1 (ent-AT-NPD1: i.e. 10\"S\",17\"S\"-Dihydroxy-4\"Z\",7\"Z\",11\"E\",13\"E\",15\"Z\",19\"Z\"-docosahexaenoic acid) is the 10\"S\"-hydroxy isomer of AT-PD1 (which has a 10\"R\"-hydroxy residue) (see specialized proresolving mediators#Protectins/neuroprotectins). 10-Epi-PD1 was detected in only a small amount in human PMN extracts but was more potent than PD1 or PDX in blocking the inflammatory response to zymosan A-induced murine acute peritonitis.\n\n"}
{"id": "48630521", "url": "https://en.wikipedia.org/wiki?curid=48630521", "title": "Quantum photoelectrochemistry", "text": "Quantum photoelectrochemistry\n\nQuantum photoelectrochemistry is the investigation of the quantum mechanical nature of photoelectrochemistry, the subfield of study within physical chemistry concerned with the interaction of light with electrochemical systems, typically through the application of quantum chemical calculations. Quantum photoelectrochemistry provides an expansion of quantum electrochemistry to processes involving also the interaction with light (photons). It therefore also includes essential elements of photochemistry. Key aspects of quantum photoelectrochemistry are calculations of optical excitations, photoinduced electron and energy transfer processes, excited state evolution, as well as interfacial charge separation and charge transport in nanoscale energy conversion systems.\n\nQuantum photoelectrochemistry in particular provides fundamental insight into basic light-harvesting and photoinduced electro-optical processes in several emerging solar energy conversion technologies for generation of both electricity (photovoltaics) and solar fuels. Examples of such applications where quantum photoelectrochemistry provides insight into fundamental processes include photoelectrochemical cells, semiconductor photochemistry, as well as light-driven electrocatalysis in general, and artificial photosynthesis in particular.\n\nQuantum photoelectrochemistry constitutes an active line of current research, with several publications appearing in recent years that relate to several different types of materials and processes, including light-harvesting complexes, light-harvesting polymers, as well as nanocrystalline semiconductor materials.\n\n"}
{"id": "7339616", "url": "https://en.wikipedia.org/wiki?curid=7339616", "title": "Rajagopala Chidambaram", "text": "Rajagopala Chidambaram\n\nRajagopala Chidambaram (born 12 November 1936) is an Indian Physicist who is known for his integral role in India's nuclear weapons program; he coordinated test preparation for the Pokhran-I (1975) and Pokhran-II (1998).\n\nPreviously served as the principal scientific adviser to the federal Government of India, Chidambaram previously served as the director of the Bhabha Atomic Research Centre (BARC)— and later as chairman, Atomic Energy Commission of the Government of India and he contributed in providing national defence and energy security to India. Chidambaram was chairman of the board of Governors of the International Atomic Energy Agency (IAEA) during 1994–95. He was also a member of the Commission of Eminent Persons appointed by the Director-General, IAEA, in 2008 to prepare a report on \"The Role of the IAEA to 2020 and Beyond\".\n\nThroughout his career, Chidambaram played a key role in developing India's nuclear weapons, being a part of the team conducting the first Indian nuclear test (Smiling Buddha) at Pokhran Test Range in 1974. He gained international fame when he led and represented the team of the Department of Atomic Energy (DAE) while observing and leading efforts to conduct the second nuclear tests in May 1998.\n\nChidambaram completed his early education in Meerut and Chennai, completing his B.Sc. with honors in physics, having stood first rank at the departmental and the university level of the Madras University in 1956. After enrolling in master's program, Chidambaram taught introductory physics laboratory courses and obtained M.Sc. in physics, writing a fundamental thesis on analog computers from the same institution, in 1958.\nHe was accepted for the doctoral programme of the Indian Institute of Science (IISc), and was awarded the PhD in 1962. His thesis contained the research work on the development of Nuclear Magnetic Resonance, and was conferred with the \"Martin Forster Medal\" for the best doctoral thesis submitted to the Indian Institute of Science. Chidambaram is a versatile scholar, interest first in physics. After graduating, his interest in nuclear physics diminished and his research interest in physics did not keep him motivated to contribute in his field. Instead, Chidambaram found himself interested in crystallography and condensed matter physics, writing scientific articles which later played an influential role in the development of modern materials science. His contribution to the enhancement of condensed matter physics and material science led him to be conferred with a D.Sc., in physics by the IISc after submitting his doctoral thesis on experiments which he conducted at IISc. He has been conferred doctoral degrees in physics by eight Indian universities.\nAfter the test of the nuclear device at Pokharan in 1974, Chidambaram started ‘open research' in the area of high pressure physics. For this a complete range of instrumentation such as diamond anvil cells, and gas-gun for launching projectiles were indigenously built. He also laid the foundation of theoretical high-pressure research for calculation of equation of state and phase stability of materials by first principles techniques. The papers published by his high pressure group are also well cited. The one on ‘Omega Phase in Materials’ is considered a textbook by researchers in Condensed Matter Physics/ Materials Science. He is an excellent orator, brightest mind with understanding of diverse subjects. A scientific visionary of India.\n\nAfter receiving his doctorate in physics, Chidambaram joined the Bhabha Atomic Research Centre (BARC). He served as the director of the physics group initiating research on physical aspects of nuclear weapons.\nAt BARC, he rose to become one of the senior nuclear scientists involved in various classified projects, and was one of the central figures building the nuclear programme. In 1967, Chidambaram joined the nuclear weapon designing effort along with his fellow scientists in constructing and building the metallurgical and physical aspects of the nuclear weapons. He and his colleagues worked out the equation of state of plutonium, which is still classified by all nuclear weapon states. He chose the implosion method and initiated research at BARC in very close interaction with the Terminal Ballistics Research Laboratory (TBRL) of the Defence Research and Development Organisation (DRDO) to achieve this. Chidambaram also assisted the Indian Army to construct a nuclear test site at long-constructed Indian Army base, Pokhran Test Range in Rajasthan, Chidambaram was part of a team of scientists who participated in and supervised India's first nuclear test, codename Smiling Buddha, and was one of the scientists who were honoured by Indian Premier Indira Gandhi. Finally, in 1990, Chidambaram became Director of the BARC . His key participation in the design and successful execution of Operation Smiling Buddha saw him leading the DAE team of Operation Shakti in 1998. As the director of BARC, he initiated the development of super-computers, which now have multi-teraflop speed capability. During his chairmanship of the Atomic Energy Commission, he accelerated the development of nuclear power. \nUpset by the secret manner in which the test was conducted, and given his instrumental role in the test, Chidambaram was not positively reciprocated when he approached the US for a visa to attend the 1998 annual conference of the International Union of Crystallography, of which was the vice-president, which was followed by his withdrawal of the visa application.\n\nDr. R. Chidambaram was Principal Scientific Advisor to the Government of India (until Dr. Krishnasamy VijayRaghavan replaced him in March 2018) and Chairman of the Scientific Advisory to the Cabinet of the Federal Government. Some of his initiatives as Principal Scientific Adviser, including the setting up of the Core Advisory Group for R&D in the Automotive Sector (CAR) to increase academia-industry interaction, the creation of RuTAGs (Rural Technology Action Groups) for effective need based technology delivery in rural areas, the establishment of SETS (Society for Electronic Transactions and Security), are making significant impact. During the last few years, he along with National Informatic Center helped conceptualise and supervise the setting up of the high-speed 'National Knowledge Network' to connect about 1,500 educational and research institutions in India. He has emphasized the need for 'Coherent Synergy' (a phrase he has coined) in India's Science & Technology (S&T) efforts to take India on a sustained fast-growth path. He has also focused on the importance of 'Directed Basic Research' as an addition to (not a substitute for) self-directed basic research.\n\nChidambaram is the recipient a number of awards and honours. The Indian Government acknowledged his contribution to the successful nuclear tests by awarding the Padma Shri, the fourth highest Civilian honour of the nation, in 1975 and the Padma Vibushan, the second highest civilian honour, in 1999. His other prominent awards are the Distinguished Alumnus Award of the Indian Institute of Science (1991), the C.V. Raman Birth Centenary Award of the Indian Science Congress Association (1995), the Distinguished Materials Scientist of the Year Award of the Materials Research Society of India (1996), the R.D. Birla Award of the Indian Physics Association (1996), the H. K. Forodia Award for Excellence in S & T (1998), the Hari Om Prerit Senior Scientist Award (2000), the Meghnad Saha Medal of the Indian National Science Academy (2002), the INS Homi Bhabha Lifetime Achievement Award of the Indian Nuclear Society (2006), the Life Time Contribution Award in Engineering (2009) from Indian National Academy of Engineering, the C.V. Raman Medal of the Indian National Science Academy. \nHe has been awarded D.Sc. degrees (Honoris Causa) by more than twenty universities in India and abroad. Chidambaram is a Fellow of all the science Academies in India and the Third World Academy of Science (TWAS), Trieste (Italy). \nHe has also served as a member, chairman and president of a number of organizations which, among others, include IIT-Madras, IIT-Bombay, the Materials Research Society of India, the Council of Scientific and Industrial Research (CSIR), and the International Union of Crystallography. In early 2008, the IAEA invited Chidambaram to be a member of the \"Commission of Eminent Persons\", for making recommendations to the Board of Governors, regarding long-term priorities and funding.\n\n"}
{"id": "11405579", "url": "https://en.wikipedia.org/wiki?curid=11405579", "title": "Spindle (furniture)", "text": "Spindle (furniture)\n\nA spindle, in furniture, is a cylindrically symmetric shaft, usually made of wood. A spindle is usually made of a single piece of wood and typically has decoration (also axially symmetric) fashioned by hand or with a lathe. The spindle was common at least as early as the 17th century in Western Europe as an element of chair and table legs, stretchers, candlesticks, balusters, and other pieces of cabinetry. By definition, the axis of a spindle is straight; hence, for example, a spindle-legged chair is a straight-legged design, even though cylindrical symmetry allows decoration of elaborate notches or bulbs, so long as the cylindrical symmetry is preserved.\n\nThe spindle leg design is characteristic of many Victorian and earlier Nursing chairs, exposed wood armchairs and a variety of cabinets and tables. In French furniture, the spindle leg may be found on Fauteuils,\" chairs, a variety of tables and other pieces.\n\n"}
{"id": "2864985", "url": "https://en.wikipedia.org/wiki?curid=2864985", "title": "Staggered fermion", "text": "Staggered fermion\n\nStaggered fermion is a technical subtlety that arises when fermionic fields are included in lattice gauge theory. When one does so, many new unphysical fermionic excitations corresponding to alternating fermionic fields occur in the spectrum. This is known as the fermion doubling problem. A particular way to resolve this problem, first proposed by Leonard Susskind and John Kogut ( Kogut-Susskind fermion), is the staggered fermion approach where a new nonlocal action is constructed where the Dirac operator is treated as a square root.\n"}
{"id": "1281863", "url": "https://en.wikipedia.org/wiki?curid=1281863", "title": "Tantalum carbide", "text": "Tantalum carbide\n\nTantalum carbides form a family of binary chemical compounds of tantalum and carbon with the empirical formula TaC, where \"x\" usually varies between 0.4 and 1. They are extremely hard, brittle, refractory ceramic materials with metallic electrical conductivity. They appear as brown-gray powders, which are usually processed by sintering. Being important cermet materials, tantalum carbides are commercially used in tool bits for cutting applications and are sometimes added to tungsten carbide alloys. The melting points of tantalum carbides peak at about 3880 °C depending on the purity and measurement conditions; this value is among the highest for binary compounds. Only tantalum hafnium carbide may have a slightly higher melting point of about 3942 °C, whereas the melting point of hafnium carbide is comparable to that of TaC.\n\nTaC powders of desired composition are prepared by heating a mixture of tantalum and graphite powders in vacuum or inert-gas atmosphere (argon). The heating is performed at temperature of about 2000 °C using a furnace or an arc-melting setup. An alternative technique is reduction of tantalum pentoxide by carbon in vacuum or hydrogen atmosphere at a temperature of 1500–1700 °C. This method was used to obtain tantalum carbide in 1876, but it lacks control over the stoichiometry of the product. Production of TaC directly from the elements has been reported through self-propagating high-temperature synthesis.\n\nTaC compounds have a cubic (rock-salt) crystal structure for \"x\" = 0.7–1.0; the lattice parameter increases with \"x\". TaC has two major crystalline forms. The more stable one has an anti-cadmium iodide-type trigonal structure, which transforms upon heating to about 2000 °C into a hexagonal lattice with no long-range order for the carbon atoms.\n\nHere \"Z\" is the number of formula units per unit cell, \"ρ\" is the density calculated from lattice parameters.\n\nThe bonding between tantalum and carbon atoms in tantalum carbides is a complex mixture of ionic, metallic and covalent contributions, and because of the strong covalent component, these carbides are very hard and brittle materials. For example, TaC has a microhardness of 1600–2000 kg/mm (~9 Mohs) and an elastic modulus of 285 GPa, whereas the corresponding values for tantalum are 110 kg/mm and 186 GPa. The hardness, yield stress and shear stress increase with the carbon content in TaC. Tantalum carbides have metallic electrical conductivity, both in terms of its magnitude and temperature dependence. TaC is a superconductor with a relatively high transition temperature of \"T\" = 10.35 K.\n\nThe magnetic properties of TaC change from diamagnetic for \"x\" ≤ 0.9 to paramagnetic at larger \"x\". An inverse behavior (para-diamagnetic transition with increasing \"x\") is observed for HfC, despite that it has the same crystal structure as TaC.\n\nTantalcarbide is a natural form of tantalum carbide. It is cubic, extremely rare mineral.\n\n"}
{"id": "308683", "url": "https://en.wikipedia.org/wiki?curid=308683", "title": "Tonnage", "text": "Tonnage\n\nTonnage is a measure of the cargo-carrying capacity of a ship. The term derives from the taxation paid on \"tuns\" or casks of wine. In modern maritime usage, \"tonnage\" specifically refers to a calculation of the volume or cargo volume of a ship. Tonnage should not be confused with displacement, which refers to the actual weight of the vessel. Tonnage is commonly used to assess fees on commercial shipping.\n\nTonnage measurements are governed by an IMO Convention (International Convention on Tonnage Measurement of Ships, 1969 (London-Rules)), which applies to all ships built after July 1982.\n\nGross tonnage (GT) is a function of the volume of all of a ship's enclosed spaces (from keel to funnel) measured to the outside of the hull framing. The numerical value for a ship's GT is always[*] smaller than the numerical values of \"gross register tonnage (GRT).\" Gross tonnage is therefore a kind of capacity-derived index that is used to rank a ship for purposes of determining manning, safety, and other statutory requirements and is expressed simply as GT, which is a unitless entity, even though it derives from the cubic feet of volumetric capacity. \n\nNet tonnage (NT) is based on a calculation of the volume of all cargo spaces of the ship. It indicates a vessel's earning space and is a function of the moulded volume of all cargo spaces of the ship.\n\nA commonly defined measurement system is important, since a ship's registration fee, harbour dues, safety and manning rules, and the like may be based on its gross tonnage (GT) or net tonnage (NT).\n\nGross register tonnage (GRT) represents the total internal volume of a vessel, where one register ton is equal to a volume of 100 cubic feet (2.83168 m); a volume that, if filled with fresh water, would weigh around 2,800 kg or 2.8 tonnes. The definition (and calculation) of the internal volume is complex; for instance, a ship's hold may be assessed for bulk grain (accounting for all the air space in the hold) or for bales (omitting the spaces into which bulk, but not baled cargo, would spill). Gross register tonnage was replaced by \"gross tonnage\" in 1982 under the Tonnage Measurement convention of 1969, with all ships measured in GRT either scrapped or re-measured in GT by 1994.\n\nNet register tonnage (NRT) is the volume of cargo the vessel can carry—that is, the gross register tonnage less the volume of spaces that do not hold cargo (e.g., engine compartment, helm station, and crew spaces, again with differences depending on which port or country does the calculations). It represents the volume of the ship available for transporting freight or passengers. It was replaced by \"net tonnage\" in 1994, under the Tonnage Measurement convention of 1969.\n\nThe Panama Canal/Universal Measurement System (PC/UMS) is based on \"net tonnage\", modified for Panama Canal purposes. PC/UMS is based on a mathematical formula to calculate a vessel's total volume; one PC/UMS net ton is equivalent to 100 cubic feet of capacity.\n\nSuez Canal Net Tonnage (SCNT) is derived with a number of modifications from the former net register tonnage of the Moorsom System and was established by the International Commission of Constantinople in its Protocol of 18 December 1873. It is still in use, as amended by the \"Rules of Navigation\" of the Suez Canal Authority, and is registered in the Suez Canal Tonnage Certificate.\n\nThames measurement tonnage (TM) is another volumetric system, generally used for small vessels such as yachts; it uses a formula based on the vessel's length and beam.\n\nWhile not tonnage in the proper sense, the following methods of ship measurement are often incorrectly referred to as such:\n\nLightship or lightweight measures the actual weight of the ship with no fuel, passengers, cargo, water, and the like on board.\n\nDeadweight tonnage (often abbreviated as DWT, for deadweight tonnes) is the displacement at any loaded condition minus the lightship weight. It includes the crew, passengers, cargo, fuel, water, and stores. Like displacement, it is often expressed in long tons or in metric tons.\n\nMetric tonnes per centimetre immersion (usually abbreviated to TPC or TPCMI) is the number of metric tonnes (1,000 kg) that need to be loaded on the ship for the salt water draft (draught) to increase by one centimetre. The TPCMI is used to calculate the draft of the vessel with a given deadweight tonnage of cargo loaded. For a typical Panamax bulk carrier with a TPCMI of 80, the ship will sink (i.e., its draft will increase) by one centimetre for every 80 tonnes of cargo loaded. \n\nImperial tons per inch immersion (usually abbreviated to TPI) is the number of imperial long tons (2,240 lb) that need to be loaded on a vessel for the draft to increase by one inch. Old imperial TPI measurements are still occasionally used within the United States and the Panama Canal. As no ship has been measured by a classification society since the 1950s using imperial measures, modern TPI figures are therefore a conversion from the original metric measurements and should not be relied upon to be accurate.\n\nTonnage can refer to the quantity of a mineral or the mineral ore extracted from a mine. It may refer to the production of any commodity that is normally expressed in tons or tonnes. The term can also apply to the total weight drawn by a railway locomotive, or the total weight of freight passing over a railway line or road. \n\nThe tonnage may be expressed in short tons (2,000 lb), metric tons or tonnes (1,000 kg), or in long tons (2,240 lb). Often this distinction is not of any importance, however sometimes it is critical to define the exact units in which the tonnage is expressed.\n\nHistorically, tonnage was the tax on \"tuns\" (casks) of wine that held 954 litres (252 gallons) of wine and weighed 1016 kilograms (2,240 pounds). This suggests that the unit of weight measurement, the long ton (1,016 kg or 2,240 lb), and tonnage share the same etymology. The confusion between weight-based terms (deadweight and displacement) stems from this common source and the eventual decision to assess dues based on a ship's deadweight rather than counting the tuns of wine. In 1720 the Builder's Old Measurement Rule was adopted to estimate deadweight from the length of keel and maximum breadth or beam of a ship. This overly simplistic system was replaced by the Moorsom System in 1854 and calculated internal volume, not weight. This system evolved into the current set of internationally accepted rules and regulations.\n\nWhen steamships came into being, they could carry less cargo, size for size, than could sailing ships. In addition to spaces taken up by boilers and steam engines, steamships carried extra fresh water for the boilers and coal for the engines. Thus, to move the same volume of cargo as a sailing ship, a steamship would be considerably larger than a sailing ship.\n\nHarbour dues are based on tonnage. In order to prevent steamships operating at a disadvantage, various tonnage calculations were established to minimize the disadvantage presented by the extra space requirements of steamships. Rather than charging by length, displacement, or the like, charges were calculated based on the viable cargo space. As commercial cargo sailing ships are now largely extinct, gross tonnage is becoming the universal method of calculating ships' dues, and is also a more straightforward and transparent method of assessment.\n\n\n"}
{"id": "16693478", "url": "https://en.wikipedia.org/wiki?curid=16693478", "title": "Water scoop (hydropower)", "text": "Water scoop (hydropower)\n\nA water scoop is a simple hydropower machine–that is, a machine used to extract power from the flow of water. Unlike a water wheel it operates intermittently, like a seesaw: A container (a bucket or cup) at the end of a lever is filled with water in the upper position. The container side becomes heavier, and so the lever with the filled container moves downward, which may be used to operate a machine drive. In the lower position the container is emptied, and the lever moves back into the upward position. \n\nBecause of their inferior efficiency compared to a water mill, water scoops are less common, and have been used in the past mostly for applications where linear motion is required rather than rotation, for example hammers in smitheries, saws in sawmills, and stamp mills in mining. They are also used for fulling and, nowadays, to operate animated sculptures in fountains.\n\n \n"}
{"id": "2252985", "url": "https://en.wikipedia.org/wiki?curid=2252985", "title": "Weatherhead", "text": "Weatherhead\n\nA weatherhead, also called a weathercap, service head, service entrance cap, or gooseneck (slang) is a weatherproof service drop entry point where overhead power or telephone wires enter a building, or where wires transition between overhead and underground cables. At a building the wires enter a conduit, a protective metal pipe, and the weatherhead is a waterproof cap on the end of the conduit that allows the wires to enter without letting in water. It is shaped like a hood, with the surface where the wires enter facing down at an angle of at least 45°, to shield it from precipitation. A rubberized gasket makes for a tight seal against the wires. Before they enter the weatherhead, a \"drip loop\" is left in the overhead wires, which permits rain water that collects on the wires to drip off before reaching the weatherhead.\n\nA weatherhead termination is only used at low voltages (up to 600 volts), since higher distribution voltages require more insulation between conductors and metal enclosures. Higher voltage connections are made through a pothead. \n\nWeatherheads are required by electrical codes or building codes. They are also used on utility poles where overhead power lines enter a conduit to pass underground.\n"}
{"id": "1425340", "url": "https://en.wikipedia.org/wiki?curid=1425340", "title": "Winged bean", "text": "Winged bean\n\nThe winged bean (Psophocarpus tetragonolobus), also known as the Goa bean, four-angled bean, four-cornered bean, Manila bean, and dragon bean, is a tropical legume plant native to New Guinea.\n\nIt grows abundantly in the hot, humid equatorial countries of South and Southeast Asia. In Southeast Asia and Papua New Guinea it is widely known, but only cultivated on a small scale. Winged bean is widely recognised by farmers and consumers in southern Asia for its variety of uses and disease resistance. Winged bean is nutrient-rich, and all parts of the plant are edible. Leaves can be eaten like spinach, flowers can be used in salads, tubers can be eaten raw or cooked, seeds can be used in similar ways as the soybean. The winged bean is an underutilised species but has the potential to become a major multi-use food crop in the tropics of Asia, Africa, and Latin America.\n\nThe winged bean species belongs to the genus \"Psophocarpus\", which is part of the legume family, Fabaceae. Species in the Psophocarpus genus are perennial herbs grown as annuals. \"Psophocarpus\" species have tuberous roots and pods with wings. They can climb by twining their stems around a support.\n\nThe winged bean plant grows as a vine with climbing stems and leaves, 3–4 m in height. It is an herbaceous perennial, but can be grown as an annual. It is generally taller and notably larger than the common bean. The bean pod is typically long and has four wings with frilly edges running lengthwise. The skin is waxy and the flesh partially translucent in the young pods. When the pod is fully ripe, it turns an ash-brown color and splits open to release the seeds. The large flower is a pale blue. The beans themselves are similar to soybeans in both use and nutritional content (being 29.8% to 39% protein).\n\nThe appearance of the winged bean varies abundantly. The shape of its leaves ranges from ovate to deltoid, ovate-lanceolate, lanceolate, and long lanceolate. The green tone of the leaves also varies. The stem, most commonly, is green, but sometimes boasts purple.\n\nThe pods are usually rectangular, but sometimes appear flat. Pods may be coloured cream, green, pink, or purple. Pods may be smooth are appear smooth or rough, depending on the genotype. Seed shape is often round; oval and rectangular seeds also occur. Seeds may appear white, cream, dark tan, or brown, depending on growing and storage conditions.\n\nThe entire winged bean plant is edible. The leaves, flowers, roots, and bean pods can be eaten raw or cooked; the pods are edible even when raw and unripe. The seeds are edible after cooking. Each of these parts contains vitamin A, vitamin C, calcium, and iron, among other nutrients.\n\nThe tender pods, which are the most widely eaten part of the plant, are best when eaten before they exceed in length. They are ready for harvest within three months of planting. The flowers are used to colour rice and pastry. The young leaves can be picked and prepared as a leaf vegetable, similar to spinach.\n\nThe nutrient-rich, tuberous roots have a nutty flavour. They are about 20% protein; winged bean roots have more protein than many other root vegetables. The leaves and flowers are also high in protein (10–15%).\n\nThe seeds are about 35% protein and 18% fat. They require cooking for two to three hours to destroy the trypsin inhibitors and hemagglutinins that inhibit digestion. They can be eaten dried or roasted. Dried and ground seeds make a useful flour, and can be brewed to make a coffee-like drink.\n\nThe beans are rich not only in protein, but in tocopherols (antioxidants that facilitate vitamin A utilisation in the body). They can be made into milk when blended with water and an emulsifier. Winged bean milk is similar to soy milk, but without the bean-rich flavour. The flavour of raw beans is not unlike that of asparagus. In Malaya, winged beans are used as an effective remedy for smallpox, and as a cure for vertigo.\n\nSmoked pods, dried seeds, tubers (cooked and uncooked), and leaves have been sold in domestic markets in South East and South Asia. Mature seeds can command a high price.\n\nWinged bean is a potential food source for ruminants, poultry, fish, and other livestock.\n\nFor commercial fish feed, winged bean is a potentially lower-cost protein source. In Africa, fish meal is especially scarce and expensive. The African sharptooth catfish, a highly valued food fish in Africa, can eat winged bean.\nIn Papua New Guinea highlands region where winged beans thrive, the husks are fed to the domesticated pigs as part of the dietary supplements.\n\nWinged bean is a self-pollinating plant but mutations and occasional outcrossing, may produce variations in the species. The pretreatment of winged bean seeds is not required in tropical climate, but scarification of seeds has shown to enhance the germination rate of seedlings. Seed soaking may also increase speed to germination, as is typical, and may be used in conjunction with scarification. Seedlings under natural field conditions have been reported to emerge between 5–7 days.\n\nWinged bean can grow at least as fast as comparable legumes, including soy.[16] Plants flower 40 to 140 days after sowing. Pods reach full-length about two weeks after pollination. Three weeks after pollination, the pod becomes fibrous; after six weeks, mature seeds are ready for harvest. Tuber development and flower production vary according to genotype and environmental factors. Some winged bean varieties do not produce tuberous roots. The winged bean is a tropical plant, and will only flower when the day length is shorter than 12 hours, although some varieties have been reported to be day-length neutral. All varieties of winged bean grow on a vine and must grow over a support. Some examples of support systems include: growing against exterior walls of houses, huts, buildings; supporting against larger perennial trees; stakes placed in the ground vertically; and structures made from posts and wires.\n\nBecause the early growth of winged bean is slow, it is important to maintain weeds. Slow early growth makes winged bean susceptible to weed competition in the first 4–6 weeks of development. Khan (1982) recommends weeding by hand or animal drawn tractor two times before the support system of the winged bean is established.\n\nWinged bean can be grown without added fertiliser as the plant has a bacterium on the nodules of the roots that fixes nitrogen and allows the plant to absorb nitrogen. Factors that influence nitrogen fixation include, Rhizobium strain, interactions between strain and host genotype, available nutrients and soil pH.\n\nAlthough winged bean thrives in hot weather and favours humidity, it is adaptable. The plant's ability to grow in heavy rainfall makes it a candidate for the people of the African tropics.\n\nWinged bean production is optimal in humidity, but the species is susceptible to moisture stress and waterlogging. Ideal growing temperature is 25 °C. Lower temperatures suppress germination, and extremely high temperatures inhibit yield.\n\nEven moderate variations in the growing climate can affect yield. Growing winged bean in lower temperatures can increase tuber production. Leaf expansion rate is higher in a warmer climate. For the highest yields, the soil should remain moist throughout the plant's life cycle. Although the plant is tropical, it can flourish in a dry climate if irrigated. If the plant matures during the drier part of the growing season, yields are higher.\n\nWinged bean is an effective cover crop; planting it uniform with the ground suppresses weed growth. As a restorative crop, winged bean can improve nutrient-poor soil with nitrogen when it is turned over into the soil.\n\n"}
