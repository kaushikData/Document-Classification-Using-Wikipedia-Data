{"id": "23113479", "url": "https://en.wikipedia.org/wiki?curid=23113479", "title": "1990 Clinic of Zaragoza radiotherapy accident", "text": "1990 Clinic of Zaragoza radiotherapy accident\n\nThe 1990 Clinic of Zaragoza radiotherapy accident was a radiological accident that occurred from December 10–20, 1990, at the Clinic of Zaragoza, in Spain.\n\nIn the accident, at least 27 patients were injured, and 11 of them died, according to International Atomic Energy Agency (IAEA). All of the injured were cancer patients receiving radiotherapy.\n\nOn December 7, 1990, a technician performed maintenance on an electron accelerator at the Clinic of Zaragoza. On December 10, it returned to service after the repairs. On December 19, the Spanish Nuclear Safety Board was scheduled to make its annual review to the device, but due to bureaucratic reasons this review was delayed. The Spanish Nuclear Safety Board found the electron accelerator power was too high. On December 20, 1990, the unit was stopped, and was restarted on March 8, 1991.\n\nAffected patients immediately suffered burns on the skin of the irradiated area, as well as inflammation of the internal organs and bone marrow. The first patient died on February 16, 1991, two months after irradiation. Fatalities increased until, on December 25, 1991, the last of a total of 25 patients died. However, the IAEA established that eleven of the deaths were due to the faulty maintenance.\n\nThe number affected might have been higher, because 31 other cancer patients were receiving treatment with the accelerator, but the other unit at the clinic was in perfect working condition.\n\nThe radiotherapy unit was repaired without following the correct instructions. The unit, in service 14 years at the time of the failure, had a breakdown in the electron beam accelerator control system ('deviator'). Repairs incorrectly increased output power, so patients that should have received therapy at 7 MeV were instead treated at 40 MeV.\n\nInitially, the hospital was thought responsible for the accident, and specifically, the management of the radiological unit. The manager of the hospital said that the maintenance technician was responsible, and the Health Minister blamed General Electric (GE), the makers of the radiological unit, who had contracted out the maintenance.\n\nFinally, on April 6, 1993, the hospital, its staff, and the Spanish National Institute of Health were acquitted. The court found the technician who performed the repair guilty, and secondarily, found General Electric guilty. GE had to compensate the affected families with 400 million pesetas (around 2.4 million euros).\n\nThe device continued working until December 1996, when it was switched off and scrapped. This was done discreetly to avoid publicity.\n\n"}
{"id": "3026353", "url": "https://en.wikipedia.org/wiki?curid=3026353", "title": "Adverse yaw", "text": "Adverse yaw\n\nAdverse yaw is the natural and undesirable tendency for an aircraft to yaw in the opposite direction of a roll. It is caused by the difference in lift and drag of each wing. The effect can be greatly minimized with ailerons deliberately designed to create drag when deflected upward and/or mechanisms which automatically apply some amount of coordinated rudder. As the major causes of adverse yaw vary with lift, any fixed-ratio mechanism will fail to fully solve the problem across all flight conditions and thus any manually operated aircraft will require some amount of rudder input from the pilot in order to maintain coordinated flight.\n\nAdverse yaw was first experienced in 1901 and later described by Orville Wright, following their unability to control a turn with their glider lacking directionnal control.\n\nAdverse yaw is a secondary effect of the inclination of the lift vectors on the wing due to its rolling velocity and of the application of the ailerons. Some pilot training manuals focus mainly on the additional drag caused by the downward-deflected aileron \nand make only brief or indirect mentions of roll effects. In fact the rolling of the wings usually causes a greater effect than the ailerons. Assuming a roll rate to the right, as in the diagram, the causes are explained as follows:\n\nBy definition, lift is perpendicular to the oncoming flow. As the left wing moves up, its effective angle of attack is decreased, so its lift vector tilts back. Conversely, as the right wing descends, its lift vector tilts forward. The result is an adverse yaw moment to the left, opposite to the intended right turn.\n\nInitiating a roll to the right requires a briefly greater lift on the left than the right. This also causes a greater induced drag on the left than the right, which further adds to the adverse yaw, but only briefly. Once a steady roll rate is established the left/right lift imbalance dwindles, while the other mechanisms described above persist.\n\nThe downward aileron deflection on the left increases the airfoil camber, which will typically increase the profile drag. Conversely, the upward aileron deflection on the right will decrease the camber and profile drag. The profile drag imbalance adds to the adverse yaw. A Frise aileron reduces this imbalance drag, as described further below.\nThere are a number of aircraft design characteristics which can be used to reduce adverse yaw to ease the pilot workload:\n\nA strong directional stability is the first way to reduce adverse yaw. This is influenced by the vertical tail moment (area and lever arm about gravity center).\n\nAs the tilting of the left/right lift vectors is the major cause to adverse yaw, an important parameter is the magnitude of these lift vectors, or the aircraft's lift coefficient to be more specific. Flight at low lift coefficient (or high speed compared to minimum speed) produces less adverse yaw.\nAs intended, the rudder is the most powerful and efficient means of managing yaw but mechanically coupling it to the ailerons is impractical. Electronic coupling is commonplace in fly-by-wire aircraft.\n\nThe geometry of most aileron linkages can be configured so as to bias the travel further upward than downward. By excessively deflecting the upward aileron, profile drag is increased rather than reduced and separation drag further aids in producing drag on the inside wing, producing a yaw force in the direction of the turn. Though not as efficient as rudder mixing, aileron differential is very easy to implement on almost any airplane and offers the significant advantage of reducing the tendency for the wing to stall at the tip first by limiting the downward aileron deflection and its associated effective increase in angle of attack.\n\nMost airplanes use this method of adverse yaw mitigation — particularly noticeable on one of the first well-known aircraft to ever use them, the de Havilland Tiger Moth training biplane of the 1930s — due to the simple implementation and safety benefits.\n\nFrise ailerons are designed so that when up aileron is applied, some of the forward edge of the aileron will protrude downward into the airflow, causing increased drag on this (down-going) wing. This will counter the drag produced by the other aileron, thus reducing adverse yaw.\n\nUnfortunately, as well as reducing adverse yaw, Frise ailerons will increase the overall drag of the aircraft much more than applying rudder correction. Therefore, they are less popular in aircraft where minimizing drag is important (e.g. in a glider).\n\nNote: Frise ailerons were primarily designed to reduce roll control forces. Contrary to the illustration, the aileron leading edge is in fact rounded to prevent flow separation and flutter at negative deflections. That prevents important differential drag forces.\n\nOn large aircraft where rudder use is inappropriate at high speeds or ailerons are too small at low speeds, roll spoilers (also called spoilerons) can be used to minimise adverse yaw or increase roll moment. To function as a lateral control, the spoiler is raised on the down-going wing (up aileron) and remains retracted on the other wing. The raised spoiler increases the drag, and so the yaw is in the same direction as the roll.\n\nCollection of balanced-aileron test data, F.M. Rogallo, Naca WR-L 419\n"}
{"id": "34678771", "url": "https://en.wikipedia.org/wiki?curid=34678771", "title": "Alexander Bathgate", "text": "Alexander Bathgate\n\nAlexander Bathgate (4 August 1845–9 September 1930) was a New Zealand lawyer, company director, writer and conservationist. He was born in Peebles, Peeblesshire, Scotland on 4 August 1845. He was the son of John Bathgate. When Alexander was 18 years old, and was studying at the University of Edinburgh he migrated to Dunedin with his parents, brother and sisters. He was admitted as a barrister and solicitor in 1872. In 1909 he retired. \n\n"}
{"id": "4624890", "url": "https://en.wikipedia.org/wiki?curid=4624890", "title": "Allogenic succession", "text": "Allogenic succession\n\nIn ecology, allogenic succession is succession driven by the abiotic components of an ecosystem. In contrast, autogenic succession is driven by the biotic components of the ecosystem. An allogenic succession can be brought about in a number of ways which can include:\n\nAllogenic succession can happen on a time scale that is proportionate with the disturbance. For example, allogenic succession that is the result of climate change can happen over thousands of years. \n"}
{"id": "1291730", "url": "https://en.wikipedia.org/wiki?curid=1291730", "title": "Backfeeding", "text": "Backfeeding\n\nBackfeeding is flow of electrical energy in the reverse direction from its normal flow. For example, backfeeding may occur when electrical power is injected into the local power grid from a source other than a utility company generator. \n\nBy definition, backfeeding causes electrical power to flow in the opposite direction from its usual flow. When studying backfeeding, engineers must understand the \"transfer of electrical power\", and not confuse this with momentary AC voltages or current flows viewed in isolation from the overall situation.\n\nPower grid \"generators\" normally pump energy into the grid, making it available for others to use. A power station will typically backfeed (and thus \"consume\" power) when it is shut down, due to its own local loads (e.g. lights or repair equipment). \n\nPower grid \"loads\" may backfeed if they also have distributed generation installed, such as a grid-connected photovoltaic solar power system or a microturbine-based power generator. It is also possible for an electric motor to temporarily backfeed if it is mechanically driven (see regenerative braking).\n\nFor cost reasons, many of the circuit (overcurrent) protection and power quality control (voltage regulation) devices used by electric utility companies are designed with the assumption that power always flows in one direction. An interconnection agreement can be arranged for equipment designed to backfeed between an electric utility customer with distributed generation and their power company. This type of interconnection can involve nontrivial engineering, and possibly equipment upgrade costs to keep the distribution circuit properly protected. Such costs may be minimized by limiting distributed generation capacity to less than is locally consumed, and guaranteeing this condition by installing a reverse-power cutoff relay that opens if backfeeding occurs.\n\nBecause it involves transfer of significant amounts of energy, backfeeding must be carefully controlled and monitored. Personnel working on equipment subject to backfeeding must be aware of all possible power sources, and follow systematic protocols to ensure that equipment is fully de-energized before commencing work, or use special equipment and techniques suitable for working on live equipment.\n"}
{"id": "3378", "url": "https://en.wikipedia.org/wiki?curid=3378", "title": "Beryllium", "text": "Beryllium\n\nBeryllium is a chemical element with symbol Be and atomic number 4. It is a relatively rare element in the universe, usually occurring as a product of the spallation of larger atomic nuclei that have collided with cosmic rays. Within the cores of stars beryllium is depleted as it is fused and creates larger elements. It is a divalent element which occurs naturally only in combination with other elements in minerals. Notable gemstones which contain beryllium include beryl (aquamarine, emerald) and chrysoberyl. As a free element it is a steel-gray, strong, lightweight and brittle alkaline earth metal.\n\nBeryllium improves many physical properties when added as an alloying element to aluminium, copper (notably the alloy beryllium copper), iron and nickel. Beryllium does not form oxides until it reaches very high temperatures. Tools made of beryllium copper alloys are strong and hard and do not create sparks when they strike a steel surface. In structural applications, the combination of high flexural rigidity, thermal stability, thermal conductivity and low density (1.85 times that of water) make beryllium metal a desirable aerospace material for aircraft components, missiles, spacecraft, and satellites. Because of its low density and atomic mass, beryllium is relatively transparent to X-rays and other forms of ionizing radiation; therefore, it is the most common window material for X-ray equipment and components of particle detectors. The high thermal conductivities of beryllium and beryllium oxide have led to their use in thermal management applications.\n\nThe commercial use of beryllium requires the use of appropriate dust control equipment and industrial controls at all times because of the toxicity of inhaled beryllium-containing dusts that can cause a chronic life-threatening allergic disease in some people called berylliosis.\n\nBeryllium is a steel gray and hard metal that is brittle at room temperature and has a close-packed hexagonal crystal structure. It has exceptional stiffness (Young's modulus 287 GPa) and a reasonably high melting point. The modulus of elasticity of beryllium is approximately 50% greater than that of steel. The combination of this modulus and a relatively low density results in an unusually fast sound conduction speed in beryllium – about 12.9 km/s at ambient conditions. Other significant properties are high specific heat (1925 J·kg·K) and thermal conductivity (216 W·m·K), which make beryllium the metal with the best heat dissipation characteristics per unit weight. In combination with the relatively low coefficient of linear thermal expansion (11.4×10 K), these characteristics result in a unique stability under conditions of thermal loading.\n\nNaturally occurring beryllium, save for slight contamination by the cosmogenic radioisotopes, is isotopically pure beryllium-9, which has a nuclear spin of . Beryllium has a large scattering cross section for high-energy neutrons, about 6 barns for energies above approximately 10 keV. Therefore, it works as a neutron reflector and neutron moderator, effectively slowing the neutrons to the thermal energy range of below 0.03 eV, where the total cross section is at least an order of magnitude lower – exact value strongly depends on the purity and size of the crystallites in the material.\n\nThe single primordial beryllium isotope Be also undergoes a (n,2n) neutron reaction with neutron energies over about 1.9 MeV, to produce Be, which almost immediately breaks into two alpha particles. Thus, for high-energy neutrons, beryllium is a neutron multiplier, releasing more neutrons than it absorbs. This nuclear reaction is:\n\nNeutrons are liberated when beryllium nuclei are struck by energetic alpha particles producing the nuclear reaction\nBeryllium also releases neutrons under bombardment by gamma rays. Thus, natural beryllium bombarded either by alphas or gammas from a suitable radioisotope is a key component of most radioisotope-powered nuclear reaction neutron sources for the laboratory production of free neutrons.\n\nSmall amounts of tritium are liberated when nuclei absorb low energy neutrons in the three-step nuclear reaction\nNote that has a half-life of only 0.8 seconds, β is an electron, and has a high neutron absorption cross-section. Tritium is a radioisotope of concern in nuclear reactor waste streams.\n\nAs a metal, beryllium is transparent to most wavelengths of X-rays and gamma rays, making it useful for the output windows of X-ray tubes and other such apparatus.\n\nBoth stable and unstable isotopes of beryllium are created in stars, but the radioisotopes do not last long. It is believed that most of the stable beryllium in the universe was originally created in the interstellar medium when cosmic rays induced fission in heavier elements found in interstellar gas and dust. Primordial beryllium contains only one stable isotope, Be, and therefore beryllium is a monoisotopic element.\nRadioactive cosmogenic Be is produced in the atmosphere of the Earth by the cosmic ray spallation of oxygen. Be accumulates at the soil surface, where its relatively long half-life (1.36 million years) permits a long residence time before decaying to boron-10. Thus, Be and its daughter products are used to examine natural soil erosion, soil formation and the development of lateritic soils, and as a proxy for measurement of the variations in solar activity and the age of ice cores. The production of Be is inversely proportional to solar activity, because increased solar wind during periods of high solar activity decreases the flux of galactic cosmic rays that reach the Earth. Nuclear explosions also form Be by the reaction of fast neutrons with C in the carbon dioxide in air. This is one of the indicators of past activity at nuclear weapon test sites.\nThe isotope Be (half-life 53 days) is also cosmogenic, and shows an atmospheric abundance linked to sunspots, much like Be.\n\nBe has a very short half-life of about 7 s that contributes to its significant cosmological role, as elements heavier than beryllium could not have been produced by nuclear fusion in the Big Bang. This is due to the lack of sufficient time during the Big Bang's nucleosynthesis phase to produce carbon by the fusion of He nuclei and the very low concentrations of available beryllium-8. The British astronomer Sir Fred Hoyle first showed that the energy levels of Be and C allow carbon production by the so-called triple-alpha process in helium-fueled stars where more nucleosynthesis time is available. This process allows carbon to be produced in stars, but not in the Big Bang. Star-created carbon (the basis of carbon-based life) is thus a component in the elements in the gas and dust ejected by AGB stars and supernovae (see also Big Bang nucleosynthesis), as well as the creation of all other elements with atomic numbers larger than that of carbon.\n\nThe 2s electrons of beryllium may contribute to chemical bonding. Therefore, when Be decays by L-electron capture, it does so by taking electrons from its atomic orbitals that may be participating in bonding. This makes its decay rate dependent to a measurable degree upon its chemical surroundings – a rare occurrence in nuclear decay.\n\nThe shortest-lived known isotope of beryllium is Be which decays through neutron emission. It has a half-life of 2.7 × 10 s. Be is also very short-lived with a half-life of 5.0 × 10 s. The exotic isotopes Be and Be are known to exhibit a nuclear halo. This phenomenon can be understood as the nuclei of Be and Be have, respectively, 1 and 4 neutrons orbiting substantially outside the classical Fermi 'waterdrop' model of the nucleus.\n\nThe Sun has a concentration of 0.1 parts per billion (ppb) of beryllium. Beryllium has a concentration of 2 to 6 parts per million (ppm) in the Earth's crust. It is most concentrated in the soils, 6 ppm. Trace amounts of Be are found in the Earth's atmosphere. The concentration of beryllium in sea water is 0.2–0.6 parts per trillion. In stream water, however, beryllium is more abundant with a concentration of 0.1 ppb.\n\nBeryllium is found in over 100 minerals, but most are uncommon to rare. The more common beryllium containing minerals include: bertrandite (BeSiO(OH)), beryl (AlBeSiO), chrysoberyl (AlBeO) and phenakite (BeSiO). Precious forms of beryl are aquamarine, red beryl and emerald. \nThe green color in gem-quality forms of beryl comes from varying amounts of chromium (about 2% for emerald).\n\nThe two main ores of beryllium, beryl and bertrandite, are found in Argentina, Brazil, India, Madagascar, Russia and the United States. Total world reserves of beryllium ore are greater than 400,000 tonnes.\n\nThe extraction of beryllium from its compounds is a difficult process due to its high affinity for oxygen at elevated temperatures, and its ability to reduce water when its oxide film is removed. The United States, China and Kazakhstan are the only three countries involved in the industrial-scale extraction of beryllium. Beryllium production technology is in early stages of development in Russia after a 20-year hiatus.\n\nBeryllium is most commonly extracted from the mineral beryl, which is either sintered using an extraction agent or melted into a soluble mixture. The sintering process involves mixing beryl with sodium fluorosilicate and soda at to form sodium fluoroberyllate, aluminium oxide and silicon dioxide. Beryllium hydroxide is precipitated from a solution of sodium fluoroberyllate and sodium hydroxide in water. Extraction of beryllium using the melt method involves grinding beryl into a powder and heating it to . The melt is quickly cooled with water and then reheated in concentrated sulfuric acid, mostly yielding beryllium sulfate and aluminium sulfate. Aqueous ammonia is then used to remove the aluminium and sulfur, leaving beryllium hydroxide.\n\nBeryllium hydroxide created using either the sinter or melt method is then converted into beryllium fluoride or beryllium chloride. To form the fluoride, aqueous ammonium hydrogen fluoride is added to beryllium hydroxide to yield a precipitate of ammonium tetrafluoroberyllate, which is heated to to form beryllium fluoride. Heating the fluoride to with magnesium forms finely divided beryllium, and additional heating to creates the compact metal. Heating beryllium hydroxide forms the oxide, which becomes beryllium chloride when combined with carbon and chlorine. Electrolysis of molten beryllium chloride is then used to obtain the metal.\nBeryllium's chemical behavior is largely a result of its small atomic and ionic radii. It thus has very high ionization potentials and strong polarization while bonded to other atoms, which is why all of its compounds are covalent. It is more chemically similar to aluminium than its close neighbors in the periodic table due to having a similar charge-to-radius ratio.\nAn oxide layer forms around beryllium that prevents further reactions with air unless heated above 1000 °C. Once ignited, beryllium burns brilliantly forming a mixture of beryllium oxide and beryllium nitride. Beryllium dissolves readily in non-oxidizing acids, such as HCl and diluted HSO, but not in nitric acid or water as this forms the oxide. This behavior is similar to that of aluminium metal. Beryllium also dissolves in alkali solutions.\nThe beryllium atom has the electronic configuration [He] 2s. The two valence electrons give beryllium a +2 oxidation state and thus the ability to form two covalent bonds; the only evidence of lower valence of beryllium is in the solubility of the metal in BeCl, and in two neutral beryllium bis(carbene) compounds in which the Be center bears a formal oxidation state of zero. Due to the octet rule, atoms tend to seek a valence of 8 in order to resemble a noble gas. Beryllium tries to achieve a coordination number of 4 because its two covalent bonds fill half of this octet. Tetracoordination allows beryllium compounds, such as the fluoride or chloride, to form polymers.\n\nThis characteristic is employed in analytical techniques using EDTA as a ligand. EDTA preferentially forms octahedral complexes – thus absorbing other cations such as Al which might interfere – for example, in the solvent extraction of a complex formed between Be and acetylacetone. Beryllium(II) readily forms complexes with strong donating ligands such as phosphine oxides and arsine oxides. There have been extensive studies of these complexes which show the stability of the O-Be bond.\n\nSolutions of beryllium salts, e.g. beryllium sulfate and beryllium nitrate, are acidic because of hydrolysis of the [Be(HO)] ion.\nOther products of hydrolysis include the trimeric ion [Be(OH)(HO)]. Beryllium hydroxide, Be(OH), is insoluble even in acidic solutions with pH less than 6, that is at biological pH. It is amphoteric and dissolves in strongly alkaline solutions.\n\nBeryllium forms binary compounds with many non-metals. Anhydrous halides are known for F, Cl, Br and I. BeF has a silica-like structure with corner-shared BeF tetrahedra. BeCl and BeBr have chain structures with edge-shared tetrahedra. All beryllium halides have a linear monomeric molecular structure in the gas phase.\n\nBeryllium difluoride, BeF, is different than the other difluorides. In general, beryllium has a tendency to bond covalently, much more so than the other alkaline earths and its fluoride is partially covalent (although still more ionic than its other halides). BeF has many similarities to SiO (quartz) a mostly covalently bonded network solid. BeF has tetrahedrally coordinated metal and forms glasses (is difficult to crystallize). When crystalline, beryllium fluoride has the same room temperature crystal structure as quartz and shares many higher temperature structures also. Beryllium difluoride is very soluble in water, unlike the other alkaline earth difluorides. (Although they are strongly ionic, they do not dissolve because of the especially strong lattice energy of the fluorite structure.) However, BeF has much lower electrical conductivity when in solution or when molten than would be expected if it were fully ionic.\n\nBeryllium oxide, BeO, is a white refractory solid, which has the wurtzite crystal structure and a thermal conductivity as high as in some metals. BeO is amphoteric. Salts of beryllium can be produced by treating Be(OH) with acid. Beryllium sulfide, selenide and telluride are known, all having the zincblende structure.\n\nBeryllium nitride, BeN is a high-melting-point compound which is readily hydrolyzed. Beryllium azide, BeN is known and beryllium phosphide, BeP has a similar structure to BeN. Basic beryllium nitrate and basic beryllium acetate have similar tetrahedral structures with four beryllium atoms coordinated to a central oxide ion. A number of beryllium borides are known, such as BeB, BeB, BeB, BeB, BeB and BeB. Beryllium carbide, BeC, is a refractory brick-red compound that reacts with water to give methane. No beryllium silicide has been identified.\n\nThe mineral beryl, which contains beryllium, has been used at least since the Ptolemaic dynasty of Egypt. In the first century CE, Roman naturalist Pliny the Elder mentioned in his encyclopedia \"Natural History\" that beryl and emerald (\"smaragdus\") were similar. The Papyrus Graecus Holmiensis, written in the third or fourth century CE, contains notes on how to prepare artificial emerald and beryl.\nEarly analyses of emeralds and beryls by Martin Heinrich Klaproth, Torbern Olof Bergman, Franz Karl Achard, and Johann Jakob Bindheim always yielded similar elements, leading to the fallacious conclusion that both substances are aluminium silicates. Mineralogist René Just Haüy discovered that both crystals are geometrically identical, and he asked chemist Louis-Nicolas Vauquelin for a chemical analysis.\n\nIn a 1798 paper read before the Institut de France, Vauquelin reported that he found a new \"earth\" by dissolving aluminium hydroxide from emerald and beryl in an additional alkali. The editors of the journal \"Annales de Chimie et de Physique\" named the new earth \"glucine\" for the sweet taste of some of its compounds. Klaproth preferred the name \"beryllina\" due to the fact that yttria also formed sweet salts. The name \"beryllium\" was first used by Wöhler in 1828.\nFriedrich Wöhler and Antoine Bussy independently isolated beryllium in 1828 by the chemical reaction of metallic potassium with beryllium chloride, as follows:\nUsing an alcohol lamp, Wöhler heated alternating layers of beryllium chloride and potassium in a wired-shut platinum crucible. The above reaction immediately took place and caused the crucible to become white hot. Upon cooling and washing the resulting gray-black powder he saw that it was made of fine particles with a dark metallic luster. The highly reactive potassium had been produced by the electrolysis of its compounds, a process discovered 21 years before. The chemical method using potassium yielded only small grains of beryllium from which no ingot of metal could be cast or hammered.\n\nThe direct electrolysis of a molten mixture of beryllium fluoride and sodium fluoride by Paul Lebeau in 1898 resulted in the first pure (99.5 to 99.8%) samples of beryllium. However, industrial production started only after the First World War. The original industrial involvement included subsidiaries and scientists related to the Union Carbide and Carbon Corporation in Cleveland OH and Siemens & Halske AG in Berlin. In the US, the process was ruled by Hugh S. Cooper, director of The Kemet Laboratories Company. In Germany, the first commercially successful process for producing beryllium was developed in 1921 by Alfred Stock and Hans Goldschmidt.\n\nA sample of beryllium was bombarded with alpha rays from the decay of radium in a 1932 experiment by James Chadwick that uncovered the existence of the neutron. This same method is used in one class of radioisotope-based laboratory neutron sources that produce 30 neutrons for every million α particles.\n\nBeryllium production saw a rapid increase during World War II, due to the rising demand for hard beryllium-copper alloys and phosphors for fluorescent lights. Most early fluorescent lamps used zinc orthosilicate with varying content of beryllium to emit greenish light. Small additions of magnesium tungstate improved the blue part of the spectrum to yield an acceptable white light. Halophosphate-based phosphors replaced beryllium-based phosphors after beryllium was found to be toxic.\n\nElectrolysis of a mixture of beryllium fluoride and sodium fluoride was used to isolate beryllium during the 19th century. The metal's high melting point makes this process more energy-consuming than corresponding processes used for the alkali metals. Early in the 20th century, the production of beryllium by the thermal decomposition of beryllium iodide was investigated following the success of a similar process for the production of zirconium, but this process proved to be uneconomical for volume production.\n\nPure beryllium metal did not become readily available until 1957, even though it had been used as an alloying metal to harden and toughen copper much earlier. Beryllium could be produced by reducing beryllium compounds such as beryllium chloride with metallic potassium or sodium. Currently most beryllium is produced by reducing beryllium fluoride with purified magnesium. The price on the American market for vacuum-cast beryllium ingots was about $338 per pound ($745 per kilogram) in 2001.\n\nBetween 1998 and 2008, the world's production of beryllium had decreased from 343 to about 200 tonnes, of which 176 tonnes (88%) came from the United States.\n\nEarly precursors of the word \"beryllium\" can be traced to many languages, including Latin ; French ; Ancient Greek , , 'beryl'; Prakrit (); Pāli (), () or () – \"to become pale\", in reference to the pale semiprecious gemstone beryl. The original source is probably the Sanskrit word (), which is of South Indian origin and could be related to the name of the modern city of Belur. For about 160 years, beryllium was also known as glucinum or glucinium (with the accompanying chemical symbol \"Gl\", or \"G\" ), the name coming from the Ancient Greek word for sweet: , due to the sweet taste of beryllium salts.\n\nBecause of its low atomic number and very low absorption for X-rays, the oldest and still one of the most important applications of beryllium is in radiation windows for X-ray tubes. Extreme demands are placed on purity and cleanliness of beryllium to avoid artifacts in the X-ray images. Thin beryllium foils are used as radiation windows for X-ray detectors, and the extremely low absorption minimizes the heating effects caused by high intensity, low energy X-rays typical of synchrotron radiation. Vacuum-tight windows and beam-tubes for radiation experiments on synchrotrons are manufactured exclusively from beryllium. In scientific setups for various X-ray emission studies (e.g., energy-dispersive X-ray spectroscopy) the sample holder is usually made of beryllium because its emitted X-rays have much lower energies (~100 eV) than X-rays from most studied materials.\n\nLow atomic number also makes beryllium relatively transparent to energetic particles. Therefore, it is used to build the beam pipe around the collision region in particle physics setups, such as all four main detector experiments at the Large Hadron Collider (ALICE, ATLAS, CMS, LHCb), the Tevatron and the SLAC. The low density of beryllium allows collision products to reach the surrounding detectors without significant interaction, its stiffness allows a powerful vacuum to be produced within the pipe to minimize interaction with gases, its thermal stability allows it to function correctly at temperatures of only a few degrees above absolute zero, and its diamagnetic nature keeps it from interfering with the complex multipole magnet systems used to steer and focus the particle beams.\n\nBecause of its stiffness, light weight and dimensional stability over a wide temperature range, beryllium metal is used for lightweight structural components in the defense and aerospace industries in high-speed aircraft, guided missiles, spacecraft, and satellites. Several liquid-fuel rockets have used rocket nozzles made of pure beryllium. Beryllium powder was itself studied as a rocket fuel, but this use has never materialized. A small number of extreme high-end bicycle frames have been built with beryllium. From 1998 to 2000, the McLaren Formula One team used Mercedes-Benz engines with beryllium-aluminium-alloy pistons. The use of beryllium engine components was banned following a protest by Scuderia Ferrari.\n\nMixing about 2.0% beryllium into copper forms an alloy called beryllium copper that is six times stronger than copper alone. Beryllium alloys are used in many applications because of their combination of elasticity, high electrical conductivity and thermal conductivity, high strength and hardness, nonmagnetic properties, as well as good corrosion and fatigue resistance. These applications include non-sparking tools that are used near flammable gases (beryllium nickel), in springs and membranes (beryllium nickel and beryllium iron) used in surgical instruments and high temperature devices. As little as 50 parts per million of beryllium alloyed with liquid magnesium leads to a significant increase in oxidation resistance and decrease in flammability.\nThe high elastic stiffness of beryllium has led to its extensive use in precision instrumentation, e.g. in inertial guidance systems and in the support mechanisms for optical systems. Beryllium-copper alloys were also applied as a hardening agent in \"Jason pistols\", which were used to strip the paint from the hulls of ships.\n\nBeryllium was also used for cantilevers in high performance phonograph cartridge styli, where its extreme stiffness and low density allowed for tracking weights to be reduced to 1 gram, yet still track high frequency passages with minimal distortion.\n\nAn earlier major application of beryllium was in brakes for military airplanes because of its hardness, high melting point, and exceptional ability to dissipate heat. Environmental considerations have led to substitution by other materials.\n\nTo reduce costs, beryllium can be alloyed with significant amounts of aluminium, resulting in the AlBeMet alloy (a trade name). This blend is cheaper than pure beryllium, while still retaining many desirable properties.\n\nBeryllium mirrors are of particular interest. Large-area mirrors, frequently with a honeycomb support structure, are used, for example, in meteorological satellites where low weight and long-term dimensional stability are critical. Smaller beryllium mirrors are used in optical guidance systems and in fire-control systems, e.g. in the German-made Leopard 1 and Leopard 2 main battle tanks. In these systems, very rapid movement of the mirror is required which again dictates low mass and high rigidity. Usually the beryllium mirror is coated with hard electroless nickel plating which can be more easily polished to a finer optical finish than beryllium. In some applications, though, the beryllium blank is polished without any coating. This is particularly applicable to cryogenic operation where thermal expansion mismatch can cause the coating to buckle.\n\nThe James Webb Space Telescope will have 18 hexagonal beryllium sections for its mirrors. Because JWST will face a temperature of 33 K, the mirror is made of gold-plated beryllium, capable of handling extreme cold better than glass. Beryllium contracts and deforms less than glass – and remains more uniform – in such temperatures. For the same reason, the optics of the Spitzer Space Telescope are entirely built of beryllium metal.\n\nBeryllium is non-magnetic. Therefore, tools fabricated out of beryllium-based materials are used by naval or military explosive ordnance disposal teams for work on or near naval mines, since these mines commonly have magnetic fuzes. They are also found in maintenance and construction materials near magnetic resonance imaging (MRI) machines because of the high magnetic fields generated. In the fields of radio communications and powerful (usually military) radars, hand tools made of beryllium are used to tune the highly magnetic klystrons, magnetrons, traveling wave tubes, etc., that are used for generating high levels of microwave power in the transmitters.\n\nThin plates or foils of beryllium are sometimes used in nuclear weapon designs as the very outer layer of the plutonium pits in the primary stages of thermonuclear bombs, placed to surround the fissile material. These layers of beryllium are good \"pushers\" for the implosion of the plutonium-239, and they are good neutron reflectors, just as in beryllium-moderated nuclear reactors.\n\nBeryllium is also commonly used in some neutron sources in laboratory devices in which relatively few neutrons are needed (rather than having to use a nuclear reactor, or a particle accelerator-powered neutron generator). For this purpose, a target of beryllium-9 is bombarded with energetic alpha particles from a radioisotope such as polonium-210, radium-226, plutonium-238, or americium-241. In the nuclear reaction that occurs, a beryllium nucleus is transmuted into carbon-12, and one free neutron is emitted, traveling in about the same direction as the alpha particle was heading. Such alpha decay driven beryllium neutron sources, named \"urchin\" neutron initiators, were used in some early atomic bombs. Neutron sources in which beryllium is bombarded with gamma rays from a gamma decay radioisotope, are also used to produce laboratory neutrons.\nBeryllium is also used in fuel fabrication for CANDU reactors. The fuel elements have small appendages that are resistance brazed to the fuel cladding using an induction brazing process with Be as the braze filler material. Bearing pads are brazed in place to prevent fuel bundle to pressure tube contact, and inter-element spacer pads are brazed on to prevent element to element contact.\n\nBeryllium is also used at the Joint European Torus nuclear-fusion research laboratory, and it will be used in the more advanced ITER to condition the components which face the plasma. Beryllium has also been proposed as a cladding material for nuclear fuel rods, because of its good combination of mechanical, chemical, and nuclear properties. Beryllium fluoride is one of the constituent salts of the eutectic salt mixture FLiBe, which is used as a solvent, moderator and coolant in many hypothetical molten salt reactor designs, including the liquid fluoride thorium reactor (LFTR).\n\nThe low weight and high rigidity of beryllium make it useful as a material for high-frequency speaker drivers. Because beryllium is expensive (many times more than titanium), hard to shape due to its brittleness, and toxic if mishandled, beryllium tweeters are limited to high-end home, pro audio, and public address applications. Some high-fidelity products have been fraudulently claimed to be made of the material.\n\nSome high-end phonograph cartridges used beryllium cantilevers to improve tracking by reducing mass.\n\nBeryllium is a p-type dopant in III-V compound semiconductors. It is widely used in materials such as GaAs, AlGaAs, InGaAs and InAlAs grown by molecular beam epitaxy (MBE). Cross-rolled beryllium sheet is an excellent structural support for printed circuit boards in surface-mount technology. In critical electronic applications, beryllium is both a structural support and heat sink. The application also requires a coefficient of thermal expansion that is well matched to the alumina and polyimide-glass substrates. The beryllium-beryllium oxide composite \"E-Materials\" have been specially designed for these electronic applications and have the additional advantage that the thermal expansion coefficient can be tailored to match diverse substrate materials.\n\nBeryllium oxide is useful for many applications that require the combined properties of an electrical insulator and an excellent heat conductor, with high strength and hardness, and a very high melting point. Beryllium oxide is frequently used as an insulator base plate in high-power transistors in radio frequency transmitters for telecommunications. Beryllium oxide is also being studied for use in increasing the thermal conductivity of uranium dioxide nuclear fuel pellets. Beryllium compounds were used in fluorescent lighting tubes, but this use was discontinued because of the disease berylliosis which developed in the workers who were making the tubes.\n\nBeryllium is a component of several dental alloys.\n\nBeryllium is a health and safety issue for workers. Exposure to beryllium in the workplace can lead to a sensitization immune response and can over time develop chronic beryllium disease (CBD). The National Institute for Occupational Safety and Health (NIOSH) in the United States researches these effects in collaboration with a major manufacturer of beryllium products. The goal of this research is to prevent sensitization and CBD by developing a better understanding of the work processes and exposures that may present a potential risk for workers, and to develop effective interventions that will reduce the risk for adverse health effects. NIOSH also conducts genetic research on sensitization and CBD, independently of this collaboration. The NIOSH Manual of Analytical Methods contains methods for measuring occupational exposures to beryllium.\n\nApproximately 35 micrograms of beryllium is found in the average human body, an amount not considered harmful. Beryllium is chemically similar to magnesium and therefore can displace it from enzymes, which causes them to malfunction. Because Be is a highly charged and small ion, it can easily get into many tissues and cells, where it specifically targets cell nuclei, inhibiting many enzymes, including those used for synthesizing DNA. Its toxicity is exacerbated by the fact that the body has no means to control beryllium levels, and once inside the body the beryllium cannot be removed. Chronic berylliosis is a pulmonary and systemic granulomatous disease caused by inhalation of dust or fumes contaminated with beryllium; either large amounts over a short time or small amounts over a long time can lead to this ailment. Symptoms of the disease can take up to five years to develop; about a third of patients with it die and the survivors are left disabled. The International Agency for Research on Cancer (IARC) lists beryllium and beryllium compounds as Category 1 carcinogens. In the US, the Occupational Safety and Health Administration (OSHA) has designated a permissible exposure limit (PEL) in the workplace with a time-weighted average (TWA) 0.002 mg/m and a constant exposure limit of 0.005 mg/m over 30 minutes, with a maximum peak limit of 0.025 mg/m. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of constant 0.0005 mg/m. The IDLH (immediately dangerous to life and health) value is 4 mg/m.\n\nThe toxicity of finely divided beryllium (dust or powder, mainly encountered in industrial settings where beryllium is produced or machined) is very well-documented. Solid beryllium metal does not carry the same hazards as airborne inhaled dust, but any hazard associated with physical contact is poorly documented. Workers handling finished beryllium pieces are routinely advised to handle them with gloves, both as a precaution and because many if not most applications of beryllium cannot tolerate residue of skin contact such as fingerprints.\n\nAcute beryllium disease in the form of chemical pneumonitis was first reported in Europe in 1933 and in the United States in 1943. A survey found that about 5% of workers in plants manufacturing fluorescent lamps in 1949 in the United States had beryllium-related lung diseases. Chronic berylliosis resembles sarcoidosis in many respects, and the differential diagnosis is often difficult. It killed some early workers in nuclear weapons design, such as Herbert L. Anderson.\n\nBeryllium may be found in coal slag. When the slag is formulated into an abrasive agent for blasting paint and rust from hard surfaces, the beryllium can become airborne and become a source of exposure.\n\nEarly researchers tasted beryllium and its various compounds for sweetness in order to verify its presence. Modern diagnostic equipment no longer necessitates this highly risky procedure and no attempt should be made to ingest this highly toxic substance. Beryllium and its compounds should be handled with great care and special precautions must be taken when carrying out any activity which could result in the release of beryllium dust (lung cancer is a possible result of prolonged exposure to beryllium-laden dust). Although the use of beryllium compounds in fluorescent lighting tubes was discontinued in 1949, potential for exposure to beryllium exists in the nuclear and aerospace industries and in the refining of beryllium metal and melting of beryllium-containing alloys, the manufacturing of electronic devices, and the handling of other beryllium-containing material.\n\nA successful test for beryllium in air and on surfaces has been recently developed and published as an international voluntary consensus standard ASTM D7202. The procedure uses dilute ammonium bifluoride for dissolution and fluorescence detection with beryllium bound to sulfonated hydroxybenzoquinoline, allowing up to 100 times more sensitive detection than the recommended limit for beryllium concentration in the workplace. Fluorescence increases with increasing beryllium concentration. The new procedure has been successfully tested on a variety of surfaces and is effective for the dissolution and ultratrace detection of refractory beryllium oxide and siliceous beryllium (ASTM D7458).\n\n\n\n"}
{"id": "20995293", "url": "https://en.wikipedia.org/wiki?curid=20995293", "title": "Bicine", "text": "Bicine\n\nBicine is an organic compound used as a buffering agent. It is one of Good's buffers and has a pKa of 8.35 at 20 °C. It is prepared by the reaction of glycine with ethylene oxide, followed by hydrolysis of the resultant lactone.\n\nBicine is a contaminant in amine systems used for gas sweetening. It is formed by amine degradation in the presence of O, SO, HS or Thiosulfate.\n\n"}
{"id": "5937388", "url": "https://en.wikipedia.org/wiki?curid=5937388", "title": "Crude oil engine", "text": "Crude oil engine\n\nThe crude oil engine is a type of internal combustion engine similar to the hot bulb engine. A crude oil engine could be driven by all sorts of oils such as engine waste oil and vegetable oils. Even peanut oil and butter could be used as fuel if necessary. Like hot bulb engines, crude oil engines were mostly used as stationary engines or in boats/ships. They can run for a very long time; for instance, at the world fair in Milan in 1906, a FRAM engine was started and ran until the exhibition was over one month later. A crude oil engine is a low RPM engine dimensioned for constant running and can last for a very long time if maintained properly. It was later replaced by the diesel engine.\n\nMany modern diesel engines are capable of running on pure crude oil. They are often used in the oil and gas exploration and production industries. Additionally, many large diesel engines, such as those used on large container ships, can also run directly off crude oil.\n\n"}
{"id": "4303453", "url": "https://en.wikipedia.org/wiki?curid=4303453", "title": "Cumulonimbus incus", "text": "Cumulonimbus incus\n\nA cumulonimbus incus (Latin \"incus\", \"anvil\") also known as an anvil cloud is a cumulonimbus cloud which has reached the level of stratospheric stability and has formed the characteristic flat, anvil-top shape. It signifies the thunderstorm in its mature stage, succeeding the cumulonimbus calvus stage. Cumulonimbus incus is a sub-form of Cumulonimbus capillatus.\n\nA cumulonimbus incus is a mature thunderstorm cloud generating many dangerous elements.\n\n\nCumulonimbus clouds can be powerful. If the correct atmospheric conditions are met, they can grow into a supercell storm. This cloud may be a single-cell thunderstorm or one cell in a multicellular thunderstorm. They are capable of producing severe storm conditions for a short amount of time.\n"}
{"id": "17683243", "url": "https://en.wikipedia.org/wiki?curid=17683243", "title": "Deadly Deception: General Electric, Nuclear Weapons and Our Environment", "text": "Deadly Deception: General Electric, Nuclear Weapons and Our Environment\n\nDeadly Deception: General Electric, Nuclear Weapons and Our Environment is a 1991 American short documentary film directed by Debra Chasnoff. It won an Oscar at the 64th Academy Awards in 1992 for Documentary Short Subject.\n\n\n"}
{"id": "504902", "url": "https://en.wikipedia.org/wiki?curid=504902", "title": "Electric field gradient", "text": "Electric field gradient\n\nIn atomic, molecular, and solid-state physics, the electric field gradient (EFG) measures the rate of change of the electric field at an atomic nucleus generated by the electronic charge distribution and the other nuclei. The EFG couples with the nuclear electric quadrupole moment of quadrupolar nuclei (those with spin quantum number greater than one-half) to generate an effect which can be measured using several spectroscopic methods, such as nuclear magnetic resonance (NMR), microwave spectroscopy, electron paramagnetic resonance (EPR, ESR), nuclear quadrupole resonance (NQR), Mössbauer spectroscopy or perturbed angular correlation (PAC). The EFG is non-zero only if the charges surrounding the nucleus violate cubic symmetry and therefore generate an inhomogeneous electric field at the position of the nucleus.\n\nEFGs are highly sensitive to the electronic density in the immediate vicinity of a nucleus. This is because the EFG operator scales as \"r\", where \"r\" is the distance from a nucleus. This sensitivity has been used to study effects on charge distribution resulting from substitution, weak interactions, and charge transfer.\n\nA given charge distribution of electrons and nuclei, \"ρ\"(r), generates an electrostatic potential \"V\"(r). The derivative of this potential is the negative of the electric field generated. The first derivatives of the field, or the second derivatives of the potential, is the electric field gradient. The nine components of the EFG are thus defined as the second partial derivatives of the electrostatic potential, evaluated at the position of a nucleus:\n\nFor each nucleus, the components \"V\" are combined as a symmetric 3 × 3 matrix. Under the assumption that the charge distribution generating the electrostatic potential is external to the nucleus, the matrix is traceless, for in that situation Laplace's equation, ∇\"V\"(r) = 0, holds. Relaxing this assumption, a more general form of the EFG tensor which retains the symmetry and traceless character is\n\nwhere ∇\"V\"(r) is evaluated at a given nucleus.\n\nAs \"V\" (and \"φ\") is symmetric it can be diagonalized. The principal tensor components are usually denoted \"V\", \"V\" and \"V\" in order of decreasing modulus. Given the traceless character, only two of the principal components are independent. Typically these are described by \"V\" and the asymmetry parameter, \"η\", defined as\n\nwith formula_4 and formula_5, thus formula_6.\n\nElectric field gradient as well as the asymmetry parameter can be evaluated numerically for large electric systems as shown in.\n"}
{"id": "27295200", "url": "https://en.wikipedia.org/wiki?curid=27295200", "title": "Energy conservation measure", "text": "Energy conservation measure\n\nAn Energy conservation measure (ECM) is any type of project conducted, or technology implemented, to reduce the consumption of energy in a building. The types of projects implemented can be in a variety of forms but usually are designed to reduce utility costs: water, electricity and gas being the main three for industrial and commercial enterprises. The aim of an ECM should be to achieve a savings, reducing the amount of energy used by a particular process, technology or facility.\n\nEnergy conservation measures are often combined into larger guaranteed Energy Savings Performance Contracts to maximize energy savings while minimizing disruption to building occupants by coordinating renovations. Some ECMs cost less to implement yet return a higher energy savings. Traditionally, lighting projects were a good example of “low hanging fruit” that could be used to drive implementation of more substantial upgrades to HVAC systems in large facilities. Smaller buildings might combine window replacement with modern insulation using advanced building foams to improve energy performance. Energy dashboard projects are a new kind of ECM which relies on the behavioral change of building occupants to save energy. When implemented as part of a program, case studies (such as that for the DC Schools) report energy savings up 30%. Under the right circumstances, open energy dashboards can even be implemented for free to improve upon these savings even more.\n\nOn a global basis energy efficiency works behind the scenes to improve our energy security, lower our energy bills and move us closer to reaching our climate goals. According to the IEA, some 40% of the global energy efficiency market is financed with debt and equity. Energy Performance Investment are one financing mechanism by which ECMs can be implemented now and paid for by the savings realized over the life of the project. While all 50 states, Puerto Rico and Washington, D.C., have statutes allowing companies to offer energy savings performance contracts, success varies because of variations in the approach, the state’s degree of involvement and other factors. Homes and businesses are implementing energy-efficiency measures that include low-energy lighting, insulation and even high tech energy dashboards to cut bills by avoiding waste and boosting productivity.\n\nBusinesses implementing ECMs in their commercial buildings often employ Energy Service Companies (ESCOs) experienced in energy performance contracting . This industry has been around since the 1970s and is more prevalent than ever today. The US-based organization EVO (Efficiency Valuation Organization) has created a set of guidelines for ESCOs to adhere to in evaluating the savings achieved by ECMs. These guidelines are called the International Performance Measurement and Verification Protocol (IPMVP).\n\nHomeowners implementing ECMs in their residential buildings often start with an energy audit. This is a way homeowners look at what areas of their homes are using, and possibly losing energy. Residential energy auditors are accredited by the Building Performance Institute (BPI) or the Residential Energy Services Network (RESNET). Homeowners can hire a professional or do it themselves or use a smartphone to help do an audit.\n\nEnergy Dashboards combine smart metering and Internet technologies to provide real-time data on energy use. Their success is based on the premise that real-time feedback drives behavior change and improves operational efficiency. Energy Dashboards are used to enable energy reduction competitions, showcase real-time building performance and green building features, and empower occupants to become active participants in energy management. Sustainability teams from major corporations, governments, universities and K–12 public schools use energy dashboards as an effective communication tool that creates transparency.\n\n\nInsulation decreases thermal losses in cold climates and thermal gains in hot climates thus reducing HVAC loads.\n\nOne of the simplest ways consumers save a copious amount of energy is switching incandescent light bulb to a compact fluorescent lamp (CFL). A 15W CFL is capable of providing just as much light as a 60W incandescent, while consuming just one fourth of the amount of energy.\n\nThe average US homes wastes thousands of gallons of water a year. There are many water saving solutions that also save energy.\n\nWindows may be one of the biggest contributing factors to energy loss and uncomfortable spaces.\nIndividuals might find some ECMs related to windows more cost effective than others such as thermal curtains, films, or Smart windows.\n\nPlease add more\n"}
{"id": "13567584", "url": "https://en.wikipedia.org/wiki?curid=13567584", "title": "Energy efficient transformer", "text": "Energy efficient transformer\n\nIn a typical power distribution grid, electric transformer power loss typically contributes about 40-50% of the total transmission and distribution loss. Energy efficient transformers are therefore an important means to reduce transmission and distribution loss. With the improvement of electrical steel (silicon steel) properties, the losses of a transformer in 2010 can be half that of a similar transformer in the 1970s. With new magnetic materials, it is possible to achieve even higher efficiency. The amorphous metal transformer is a modern example.\n\n"}
{"id": "12638406", "url": "https://en.wikipedia.org/wiki?curid=12638406", "title": "Feed-in tariff", "text": "Feed-in tariff\n\nA feed-in tariff (FIT, FiT, standard offer contract, advanced renewable tariff, or renewable energy payments) is a policy mechanism designed to accelerate investment in renewable energy technologies. It achieves this by offering long-term contracts to renewable energy producers, typically based on the cost of generation of each technology. Rather than pay an equal amount for energy, however generated, technologies such as wind power and solar PV, for instance, are awarded a lower per-kWh price, while technologies such as tidal power are offered a higher price, reflecting costs that are higher at the moment.\n\nIn addition, feed-in tariffs often include \"tariff degression\", a mechanism according to which the price (or tariff) ratchets down over time. This is done in order to track and encourage technological cost reductions. The goal of feed-in tariffs is to offer cost-based compensation to renewable energy producers, providing price certainty and long-term contracts that help finance renewable energy investments.\nFITs typically include three key provisions:\n\nUnder a feed-in tariff, eligible renewable electricity generators, including homeowners, business owners, farmers and private investors, are paid a cost-based price for the renewable electricity they supply to the grid. This enables diverse technologies (wind, solar, biogas, etc.) to be developed and provides investors a reasonable return. This principle was explained in Germany's 2000 Renewable Energy Sources Act:\n\nAs a result, the tariff (or rate) may differ by technology, location (e.g. rooftop or ground-mounted for solar PV projects), size (residential or commercial scale) and region. The tariffs are typically designed to decline over time to track and encourage technological change.\n\nFITs typically offer a guaranteed purchase agreement for long (15–25 year) periods.\n\nPerformance-based rates give incentives to producers to maximize the output and efficiency of their project.\n\n, feed-in tariff policies had been enacted in over 50 countries, including Algeria, Australia, Austria, Belgium, Brazil, Canada, China, Cyprus, the Czech Republic, Denmark, Estonia, France, Germany, Greece, Hungary, Iran, Republic of Ireland, Israel, Italy, Kenya, the Republic of Korea, Lithuania, Luxembourg, the Netherlands, Pakistan, Portugal, South Africa, Spain, Switzerland, Tanzania, Thailand, Turkey and the United Kingdom. In early 2012 in Spain, the Rajoy administration suspended the feed-in tariff for new projects.\n\nIn 2008, a detailed analysis by the European Commission concluded that \"well-adapted feed-in tariff regimes are generally the most efficient and effective support schemes for promoting renewable electricity\". This conclusion was supported by other analyses, including by the International Energy Agency, the European Federation for Renewable Energy, as well as by Deutsche Bank.\n\nA feed-in tariff can differentiate on the basis of marginal cost. This is a theoretical alternative which is based on the concept of price differentiation (Finon). Under such a policy the tariff price ranges from some level slightly above the spot rate to the price required to obtain the optimal level of production determined by the government. Firms with lower marginal costs receive prices on the lower end of the spectrum that increase their revenue but not by as much as under the uniform feed-in tariff. The more marginal producers face the higher tariff price. This version of the policy has two objectives. The first is to reduce the profitability of certain production sites.\n\nMany renewable sources are highly dependent on their location. For example, windmills are most profitable in windy locations, and solar plants are best at sunny locations. This means that generators tend to be concentrated at these most profitable sites. The differentiated tariff seeks to make less naturally productive sites more profitable and so spread out the generators which many consider to be an undesirable good in the area (Finon). Imagine cutting down all the forests to build wind farms; this would not be good for the environment. This, however, leads to a less cost-effective production of renewable electricity as the most efficient sites are under-utilized. The other goal of tariffs differentiated by marginal cost is to reduce the cost of the program (Finon). Under the uniform tariff all producers receive the same price which is at times in gross excess of the price needed to incentivize them to produce. The additional revenue translates into profit. Thus, the differentiated tariff attempts to give each producer what it requires to maintain production so that the optimal market quantity of renewable energy production can be reached (Finon).\n\nOverall, and in light of incipient globalization, feed-in tariffs are posing increasing problems from the point of view of trade, as their implementation in one country can easily affect industries and policies of others, thus requiring an ideally global coordination of treatment and imposition of such policy instrument, which could be reached at the World Trade Organization.\n\nThere are three methods of compensation.\n\nThe first form of feed-in tariff (under another name) was implemented in the US in 1978 under President Jimmy Carter, who signed the National Energy Act (NEA). This law included five separate Acts, one of which was the Public Utility Regulatory Policies Act (PURPA). The purpose of the National Energy Act was to encourage energy conservation and develop new energy resources, including renewables such as wind, solar and geothermal power.\n\nWithin PURPA was a provision that required utilities to purchase electricity generated from qualifying independent power producers at rates not to exceed their avoided cost. Avoided costs were designed to reflect the cost that a utility would incur to provide that same electrical generation. Different interpretations of PURPA prevailed in the 1980s: some utilities and state utility commissions interpreted avoided costs narrowly to mean avoided fuel costs, while others chose to define \"avoided costs\" as the \"avoided long-run marginal cost\" of generation. The long-run costs referred to the anticipated cost of electricity in the years ahead. This last approach was adopted by California in its Standard Offer Contract No. 4. Another provision included in the PURPA law was that utilities were prevented from owning more than 50% of projects, to encourage new entrants.\n\nTo comply with PURPA, some states began offering Standard Offer Contracts to producers. California's Public Utility Commission established a number of Standard Offer Contracts, including Standard Offer No.4 (SO4), which made use of fixed prices, based on the expected long-run cost of generation. The long-run estimates of electricity costs were based on the belief (widely held at the time) that oil and gas prices would continue to increase. This led to an escalating schedule of fixed purchase prices, designed to reflect the long-run avoided costs of new electrical generation. By 1992, private power producers had installed approximately 1,700 MW of wind capacity in California, some of which is still in service today. The adoption of PURPA also led to significant renewable energy generation in states such as Florida, and Maine.\n\nThis notwithstanding, PURPA retains negative connotations in the U.S. electricity industry. When oil and gas prices plummeted in the late 1980s, the Standard Offer Contracts that were signed to encourage new renewable energy development seemed high by comparison. As a result, PURPA contracts came to be seen as an expensive burden on electricity ratepayers.\n\nAnother source of opposition to PURPA stemmed from the fact that it was designed to encourage non-utility generation. This was interpreted as a threat by many large utilities, particularly monopolistic suppliers. As a result of its encouragement of non-utility generation, PURPA has also been interpreted as an important step toward increasing competition.\n\nIn 1990, Germany adopted its \"Stromeinspeisungsgesetz\" (StrEG), or \"Law on Feeding Electricity into the Grid\". The StrEG required utilities to purchase electricity generated from renewable energy suppliers at a percentage of the prevailing retail price of electricity. The percentage offered to solar and wind power was set at 90% of the residential electricity price, while other technologies such as hydro power and biomass sources were offered percentages ranging from 65–80%. A project cap of 5 MW was included.\n\nWhile Germany's StrEG was insufficient to encourage costlier technologies such as photovoltaics, it proved relatively effective at encouraging lower-cost technologies such as wind, leading to the deployment of 4,400 MW of new wind capacity between 1991 and 1999, representing approximately one third of the global capacity at the time.\n\nAn additional challenge that StrEG addressed was the right to interconnect to the grid. The StrEG guaranteed renewable electricity producers grid access. Similar percentage-based feed-in laws were adopted in Spain, as well as in Denmark in the 1990s.\n\nGermany's feed-in law underwent a major restructuring in 2000 to become the Renewable Energy Sources Act (2000) ( or \"EEG\"). The long title is an act on granting priority to renewable energy sources. In its new form, the act proved to be a highly effective policy framework for accelerating the deployment of renewables. Important changes included:\n\n\nSince it was very successful, the German policy (amended in 2004, 2009, and 2012) was often used as the benchmark against which other feed-in tariff policies were considered. Other countries followed the German approach. Long-term contracts are typically offered in a non-discriminatory manner to all renewable energy producers. Because purchase prices are based on costs, efficiently operated projects yield a reasonable rate of return.\nThis principle was stated in the act:\n\nFeed-in tariff policies typically target a 5–10% return. The success of photovoltaics in Germany resulted in a drop in electricity prices of up to 40% during peak output times, with savings between €520 million and €840 million for consumers. Savings for consumers have meant conversely reductions in the profit margin of big electric power companies, who reacted by lobbying the German government, which reduced subsidies in 2012. The increase in the solar energy share in Germany also had the effect of closing gas and coal-fired generation plants.\n\nOften all power produced is fed to the grid, which makes the system work rather like a PPA according to the disambiguation above, however, there is no need for a purchase agreement with a utility, but the feed-in tariff is state-administered, so the term \"feed-in tariff\" (German \"Einspeisetarif\") is usually used. Since around 2012, other types of contracts became more usual, because PPAs were supported and for small-scale solar projects, direct use of power became more attractive when the feed-in tariff became lower than prices for power bought.\n\nOn 1 August 2014, a revised Renewable Energy Sources Act entered into force. Specific deployment corridors now stipulate the extent to which renewable energy is to be expanded in the future and the funding rates (feed-in tariffs) for new capacity will gradually no longer be set by the government, but will be determined by auction; starting with ground-mounted solar plant. This represented a major change in policy and will be further extended as of 2017 with tender processes for onshore and offshore wind.\n\nFiTs have both increased and decreased electricity prices.\n\nIncreases in electricity rates occurred when the funding for the feed-in tariff scheme is provided by ratepayers via a surcharge in their electricity bills. In Germany, this approach to funding the feed-in tariff added 6.88 cEUR per kWh to the electricity rate for residential consumers in 2017. However, renewable energy can reduce spot market prices via the merit order effect, the practice of using higher-cost fossil fuel facilities only when demand exceeds the capacity of lower cost facilities. This has led to electricity price reductions in Spain, Denmark and Germany.\n\nGrid parity occurs when the cost of an alternative technology for electricity production matches the existing average for the area. Parity can vary both in time (i.e. during the course of the day and over the course of years) and in space (i.e. geographically). The price of electricity from the grid varies widely from high-cost areas such as Hawaii and California, to lower-cost areas such as Wyoming and Idaho. In areas with time-of-day pricing, rates vary over the course of the day, rising during high-demand hours (e.g. 11 AM–8 PM) and declining during low-demand hours.\n\nIn some areas wind power, landfill gas and biomass generation are already lower-cost than grid electricity. Parity has already been achieved in areas that use feed-in tariffs. For example, generation cost from landfill gas systems in Germany are currently lower than the average electricity spot market price. In remote areas, electricity from solar photovoltaics can be cheaper than building new distribution lines to connect to the transmission grid.\n\nRenewable Portfolio Standards (RPS) and subsidies create protected markets for renewable energy. RPS require utilities to obtain a minimum percentage of their energy from renewable sources. In some states, utilities can purchase Renewable Energy Certificates (USA), Renewable Energy Certificate System (EU), Renewable Energy Certificates Registry (AUS) to meet this requirement. These certificates are issued to renewable energy producers based on the amount of energy they feed into the grid. Selling the certificates is another way for the renewable producer to supplement its revenues.\n\nCertificate prices fluctuate based on overall energy demand and competition among renewable producers. If the amount of renewable energy produced exceeds the required amount, certificate prices may crash, as happened with carbon trading in Europe. This can damage the economic viability of the renewable producers.\n\nQuota systems favor large, vertically integrated generators and multinational electric utilities, if only because certificates are generally denominated in units of one megawatt-hour. They are also more difficult to design and implement than an FIT.\n\nMandating dynamic tariffs for customer initiated meter upgrades (including for distributed energy uptake) may be a more cost-effective way to accelerate the development of renewable energy.\n\nFeed-in tariff laws were in place in 46 jurisdictions globally by 2007.\nInformation about solar tariffs may be found in a consolidated form, however not all of the countries are listed in this source.\n\nTo cover the additional costs of producing electricity from renewables and for the costs of diversification, producers of electricity from renewables receive a bonus for each kWh produced, marketed or consumed. For electricity generated from solar or radiant heat only, the bonus is 300% of the price per kWh of electricity produced by the market operator defined by Law 02-01 of 22 Dhu El Kaada 1422 corresponding to 5 February 2002 until the minimum contribution of solar energy represents 25% of all primary energy. For electricity generated from facilities using solar thermal systems solar-gas hybrid, the bonus is 200% of the price per kWh.\n\nFor contributions of solar energy below 25%, said bonus is paid in the following conditions:\nThe price of electricity is fixed by the CREG (Gas and Electricity Regulatory Commission). According to the last decision which fixed it, the consumer pays his electricity as below:\n\n\nThe other consumers (industry, agriculture...etc.), they pay 4.17 DZD/kWh.\n\nThe feed-in tariff provides bonuses for electricity generated by cogeneration of 160%, taking into account thermal energy use of 20% of all primary energy used. The bonuses for solar generated electricity and cogeneration are cumulative. Remuneration of the generated electricity is guaranteed over the whole plant lifetime.\n\nFeed-in tariffs were introduced in 2008 in South Australia and Queensland, 2009 in the Australian Capital Territory and Victoria and 2010 in New South Wales, Tasmania and Western Australia. The Northern Territory offers only local feed-in tariff schemes. A uniform federal scheme to supersede all State schemes was proposed by Tasmanian Greens Senator Christine Milne, but not enacted. By mid-2011, Feed-in tariff in NSW and ACT had been closed to new generators, as the installed capacity cap had been reached. In NSW, both the Feed-in tariff and the cap were cut, due to the overly generous original settings. The new conservative Victorian government replaced the original Feed-in tariff with a less generous transitional Feed-in tariff of 25 cents per kilowatt-hour for any power generated excess to the generator's usage, pending the outcome of an inquiry by the Victorian Competition and Efficiency Commission. This does not meet the normal definition and has been referred to as a \"fake feed-in tariff\". It is actually net metering with a payment for any kilowatt credit, instead of the normal roll over.\n\nOntario introduced a feed-in tariff in 2006, revised in 2009 and 2010, increasing from 42¢/kWh to 80.2¢/kWh for micro-scale (≤10 kW) grid-tied photovoltaic projects, and decreasing to 64.2¢/kWh for applications received after 2 July 2010. Applications received prior to then had until 31 May 2011 to install the system to receive the higher rate. Ontario's FiT program includes a tariff schedule for larger projects up to and including 10MW solar farms at a reduced rate. As of April 2010, several hundred projects have been approved, including 184 large scale projects, worth $8 billion. By April 2012, 12,000 systems had been installed and the rate decreased to 54.9¢/kWh, for applications received after 1 September 2011. The price schedule as 2013 revised solar prices down to 28-38¢/kWh.\nAs of August 2011 a national solar tariff was issued at about US$0.15 per kWh.\n\nChina set a tariff for new onshore wind power plants in a move to help struggling project operators to realise profits. The National Development and Reform Commission (NDRC), the country's economic planning agency, announced four categories of onshore wind projects, which according to region will be able to apply for the tariffs. Areas with better wind resources will have lower tariffs, while those with lower outputs will be able to access more generous tariffs.\n\nThe tariffs are set at 0.51 yuan (US 0.075, GBP 0.05), 0.54 yuan, 0.58 yuan and 0.61 yuan. These represent a significant premium on the average rate of 0.34 yuan per kilowatt-hour paid to coal-fired electricity generators.\n\nCzech Republic introduced a tariff with law no. 180/2005 in 2005. The tariff is guaranteed for 15–30 years (depending on source). Supported sources are small hydropower (up to 10 MW), biomass, biogas, wind and photovoltaics. the highest tariff was 12.25 CZK/kWh for small photovoltaic. In 2010 over 1200 MW of photovoltaics were installed, but at the end of the year the FiT was eliminated for larger systems, and reduced by 50% for smaller systems. In 2011, no photovoltaic systems were installed.\n\nOn 20 September 2014, The Ministry of Electricity announced the new feed-in tariff (FIT) pricing for electricity generated from new and renewable energy sources for households and private sector companies. The FIT will be applied in two phases, the official date for applying the first phase is 27 October 2014 and the second phase to be applied after two years from the first phase (which was launched on 28 October 2016).\n\nThe energy tariff during the first phase has been divided into five categories; The purchase price per kilowatt-hour (KWh) for residential solar generation is EGP 0.848. For non-residential installations of less than 200 kilowatts of installed generation capacity, the price rises to 0.901 EGP/KWh. The third category, between 200 and 500 kilowatts, will be paid 0.973 EGP/KWh. The fourth and fifth categories of non-residential installations are paid in USD, to attract foreign investments, with the fourth category, ranging from 500 kilowatts to 20 megawatts, paid 0.136 USD/KWh (with 15% of tariff pegged at the exchange rate of 7.15 EGP per USD). The last category, which stretches between 20-50MW, will be paid 0.1434 USD/KWh. On the other hand, the purchase price for power generated from wind is based on the number of operating hours and is more elaborate than the solar tariff. It covers operating hours ranging from 2500 up through 4000 hours, with decreasing purchase rates ranging from 0.1148 USD/KWh down to 0.046 USD/KWh.\n\nIn the second phase, the categories of solar generation were reduced to four, with the residential category tariff increased to 1.0288 EGP/KWh. The second category, non-residential installations of less than 500 KW has a purchase price of 1.0858 EGP/KWh. The third and fourth categories, non-residential installations between 500 KW and 20 MW and between 20 MW and 50 MW, have a purchase tariff of 0.0788 USD/KWh and 0.084 USD/KWh, respectively (with 30% of tariff pegged at the exchange rate of 8.88 EGP per USD).\n\nThe government will purchase the electricity generated by investors, taking inflation into account, while consumption will be paid in local currency and depreciation rates reviewed after two years. The Ministry of Finance will provide concessional subsidized bank financing for households and institutions using less than 200 KW at a rate of 4%, and 8% for 200-500KW. The government is preparing a law that would allow for state-owned lands to be made available for new energy production projects under a usufruct system in exchange for 2% of the energy produced. The electricity companies will be obligated to purchase and transport the energy. The new tariff system also includes a reduction in customs on new and renewable energy production supplies by 2% while the proportion of bank financing has been set at 40-60%.\nThe government hopes for new and renewable energy to account for 20% Egypt’s total energy mix by 2020.\n\nThe European Union does not operate or necessarily encourage feed-in tariff schemes, this being a matter for member countries.\n\nHowever feed-in tariff schemes in Europe have been challenged under European law for constituting illegal state aid. PreussenElektra brought a case concerning the German Electricity Feed-in Act (\"Stromeinspeisungsgesetz\"). In 2001, the European Court of Justice (ECJ) ruled that the German arrangements did not constitute state aid. The court concluded that:\n\nThe proposed Transatlantic Trade and Investment Partnership (TTIP) trade agreement now threatens to overturn feed-in tariff schemes throughout the Europe Union. The draft energy chapter of the TTIP, leaked to The Guardian in July 2016, mandates that operators of energy networks grant access to gas and electricity \"on commercial terms that are reasonable, transparent and non-discriminatory, including as between types of energy\". This would open feed-in tariff schemes to commercial challenge, including that used by Germany. The Green MEP Claude Turmes stated: \"These [TTIP] proposals are completely unacceptable. They would sabotage EU legislators' ability to privilege renewables and energy efficiency over unsustainable fossil fuels. This is an attempt to undermine democracy in Europe.\"\n\nThe administrative procedure for ground-mounted PV systems was significantly modified in late 2009. The distinction between segments was essentially based on capacity, which determines the complexity of the administrative process. A call for tenders for PV projects above 250 kW was launched on 15 September 2011. The projects were to be analysed on multiple criteria, including the tariff rate requested by the applicant.\n\nFirst introduced in 2000, the Renewable Energy Sources Act () is reviewed on a regular basis. Its predecessor was the 1991 \"Stromeinspeisegesetz\". As of May 2008, the cost of the program added about €1.01 (USD1.69) to each monthly residential electric bill.\nIn 2012 the costs rose to €0.03592/kWh. Nonetheless, for the first time in more than ten years, electricity prices for household customers fell at the beginning of 2015.\n\nTariff rates for PV electricity vary depending on system size and location. In 2009, tariffs were raised for electricity immediately consumed rather than supplied to the grid with increasing returns if more than 30% of overall production is consumed on-site. This is to incentivise demand-side management and help develop solutions to the intermittency of solar power.\nTariff duration is usually 20 calendar years plus the year of installation. Systems receive the tariff in effect at the time of installation for the entire period.\n\nThe feed-in tariff, in force since 1 August 2004, was modified in 2008.\nIn view of the unexpectedly high growth rates, the depreciation was accelerated and a new category (>1000 kW) was created with a lower tariff. The facade premium was abolished. In July 2010, the Renewable Energy Sources Act was again amended to reduce the tariffs by a further 16% in addition to the normal annual depreciation, as the prices for PV panels had dropped sharply in 2009. The contract duration is 20 years.\n\nThe PV Feed-in tariffs for 2013 are:\n\nIndia inaugurated its latest solar power program to date on 9 January 2010. The Jawaharlal Nehru National Solar Mission (JNNSM) was officially announced by Prime Minister of India on 12 January 2010. This program aimed to install 20 GW of solar power by 2022. The first phase of this program targeted 1000 MW, by paying a tariff fixed by the Central Electricity Regulatory Commission (CERC) of India. While in spirit this is a feed in tariff, several conditions affect project size and commissioning date. The tariff for solar PV projects is fixed at Rs. 17.90 (USD 0.397)/kWh. Tariff for solar thermal projects is fixed Rs. 15.40 (USD 0.342/kWh). Tariff will be reviewed periodically by the CERC. In 2015, the feed in tariff is about Rs. 7.50 (USD 0.125)/kWh and is mostly applicable at utility level. The feed-in tariff for roof top PV plants is still not applicable..\n\nThe Indonesian government, operating mainly through the State Electricity Corporation (\"Perusahaan Listrik Negara, or PLN\"), encouraged independent power producers (IPPs) to invest in the electric power sector. Numerous IPPs are investing in large plants (over 500 MW) and many smaller plants (such as 200 MW and smaller). To support this investment, power purchase agreement (PPA) arrangements are agreed with the PLN. Prices vary widely from relatively low prices for large coal-based plants such as the Cirebon coal plant which began operations in late 2012 to higher prices for smaller geothermal plants producing more expensive power from distant locations such as the Wayang Windu geothermal plant in West Java. Indonesia has made a range of different FIT Regulations for different forms of renewable electricity generation, for example geothermal energy and solar photovoltaic electricity generation. These regulations mandate the price that should be paid by PLN to the IPP in various different circumstances, provided that preconditions are met.\n\nThe Renewable Energy Organization of Iran (SUNA; سانا) first introduced a feed-in tariff in 2008. A purchase price of 1300 Rials/kWh (900 Rial/kWh for 4 night-time hours) was set for electricity from all types of renewable resources. In 2013 the Ministry of Energy introduced new feed-in tariffs, which was set at 4442 Rials/kWh (0.15 USD). The government-set conditions are getting better and there are high feed-in tariffs [FiTs]. FiTs were recently raised and are now set at a reasonable US$0.18 per kWh for wind. The FiTs for solar panels (below 10 MW) has been decreased by 27% from 4/2016. It is now 4900 Rls/kWh= $0.14/kWh. In 2016, Governments modified the tariff and differentiate tariff for each type of renewable technology.\n\nREFIT III supports the medium and large scale production of Electricity from bioenergy sources such as Biomass, Biomass CHP and Anaerobic Digestion CHP. The REFIT scheme is administered by the Department of Communications Energy and Natural Resources (DCENR). The scheme was put in place following extensive lobbying by industrial representative bodies such as the Irish BioEnergy Association and the Micro Energy Generation Association.\n\nResidential and Micro scale Solar, Wind, Hydro and CHP receives no grant aid, no subsidy and no tax deductions are available. No Feed-In tariffs are available for these customers and net-metering is similarly unavailable. Co-operative and privately shared electricity between separate properties is illegal. A 9c/kWh Feed-In tariff was available from Electric Ireland until December 2014, when it was withdrawn without replacement. Income from this feed-in tariff was subject to income tax at up to 58%. No other Micro-scale Feed-In tariffs are available.\n\nHomeowners with grid connected micro-generation systems are charged a €9.45 per billing cycle \"low-usage surcharge\" for importing less than 2kWh per day or being a net exporter of energy in a billing period.\n\nOn 2 June 2008, the Israeli Public Utility Authority approved a \"feed-in tariff\" for solar plants. The tariff is limited to a total installation of 50MW during 7 years, whichever is reached first, with a maximum of 15 kW installation for residential and a maximum of 50 kW for commercial. Bank Hapoalim offered 10 year loans for the installation of solar panels. The National Infrastructures Ministry announced that it would expand the \"feed-in tariff\" scheme to include medium-sized solar-power stations ranging from 50 kilowatts to 5 megawatts. The new tariff scheme caused solar company Sunday Solar Energy to announce that it would invest $133 million to install photovoltaic solar arrays on kibbutzim, which are social communities that divide revenues amongst their members.\n\nItaly introduced a feed-in tariff in February 2007. By 2011 Italy installed 7128 MW, behind only Germany (7500 MW), and reduced the FiT.\n\nAn FiT of ¥42 (US$0.525) per kWh for 10 years for systems less than 10 kW, and ¥40 (US$0.50) for larger systems, but for 20 years, began on 1 July 2012. The rate was to be reviewed annually, for subsequently connected systems.\n\nTo secure the second round price of 37.8 yen/kWh for a 20-year PPA term, foreign investors must complete the following actions by 31 March 2014:\n\nProjects that complete the above steps by 31 March 2014 will be eligible to enter into a 20-year PPA with the relevant electricity utility at a price of 37.8 yen/kWh for 20 years.\n\nThe Dutch Cabinet agreed on 27 March 2009 to implement some parts of a feed-in tariff in response to the global financial crisis. The proposed regulation may adjust the quota incentive system.\nAs of the summer of 2009, The Netherlands operated a subsidy system. The subsidy budget has a quota for diverse types of energy, at several tens of million euros. The wind budget for wind was hardly used, because the tariffs are too low. The 2009 budget for Wind on Land was 900 MW (incl unused 400 MW from 2008); only 2.5 MW was used. Dutch utilities have no obligation to buy energy from windparks. The tariffs change annually. This created uncertain investment conditions. The subsidy system was introduced in 2008. The previous 2003 subsidy scheme \"Ministeriële regeling milieukwaliteit elektriciteitsproductie\" (Ministerial regulation for environmental electricity production) which was funded by charging 100 euro per household annually on top of energy taxes stopped in 2006 because it was seen as too expensive. In 2009, Dutch wind parks were still being built with grants from the old scheme. The old and new subsidy scheme was funded from the general budget.\n\nA feed-in tariff was briefly adopted in 2011, but ended a month later, in February.\n\nUnder the Portuguese energy policy, feed-in tariffs are offered to renewable sources (except large hydro) as well as micro distributed generation (e.g. solar PV, wind), waste and co-generation, and CHP generation from renewable and non-renewable sources, with the oldest tariffs dating back to 1998. The highest feed-in tariff is for photovoltaics, starting at over 500 €/MWh in 2003, and later decreasing to 300 €/MWh; most of the other tariffs have steadily increased and stabilized at between 80 and 120 €/MWh. The Portuguese policy was found to have positive impacts over the period 2000-2010, with a reduction in emissions of 7.2 MtCO2eq, an increase in GDP of 1557M€, and a creation of 160 thousand job-years. Long term impacts are yet to be evaluated as tariffs have not yet expired for the earliest installations.\n\nUnder the Renewable Energy Act of 2008, the Philippine Energy Regulatory Commission can \"(guarantee) fixed rate per kilowatt-hour – the FIT rates – for power producers harnessing renewable energy under the FIT system.\" In February 2015, the ERC agreed to give a FIT rate of P8.69 per kilowatt hour for 20 years to the Burgos Wind Farm of the Energy Development Corporation.\n\nSouth Africa's National Energy Regulator (NERSA) announced 31 March 2009 a system of feed-in tariffs designed to produce 10 tw-h of electricity per year by 2013. The tariffs were substantially higher than those in NERSA's original proposal. The tariffs, differentiated by technology, were to be paid for 20 years.\n\nNERSA said in its release that the tariffs were based on the cost of generation plus a reasonable profit. The tariffs for wind energy and concentrating solar power were among the most attractive worldwide.\n\nThe tariff for wind energy, 1.25 ZAR/kWh (€0.104/kWh) was greater than that offered in Germany and more than proposed in Ontario, Canada.\n\nThe tariff for concentrating solar, 2.10 ZAR/kWh, was less than that in Spain. NERSA's revised program followed extensive public consultation.\n\nStefan Gsänger, Secretary General of the World Wind Energy Association said, \"South Africa is the first African country to introduce a feed-in tariff for wind energy. Many small and big investors will now be able to contribute to the take-off of the wind industry in the country. Such decentralised investment will enable South Africa to overcome its current energy crisis. It will also help many South African communities to invest in wind farms and generate electricity, new jobs and new income. We are especially pleased as this decision comes shortly after the first North American feed-in law has been proposed by the Government of the Canadian Province of Ontario\".\n\nHowever, the tariff was abandoned before it began in favor of a competitive bidding process launched on 3 August 2011. Under this bidding process, the South African government planned to procure 3,750MW of renewable energy: 1,850MW of onshore wind, 1,450MW of solar PV, 200MW of CSP, 75MW of small hydro, 25MW of landfill gas, 12.5MW of biogas, 12.5MW of biomass and 100MW of small projects. The bidding process comprised two steps:\n\nThe first round of bids was due on 4 November 2011. PPA's were expected to be in place by June 2012. Projects should be commissioned by June 2014, except CSP projects expected by June 2015.\n\nSpanish feed-in legislation was set by Royal decree 1578/2008 (\"Real Decreto 1578/2008\"), for photovoltaic installations, and Royal decree 661/2007 for other renewable technologies injecting electricity to the public grid. Originally under the 661/2007, photovoltaic tariffs were developed under a separate law due to its rapid growth.\n\nThe decree 1578/2008 categorized installations in two main groups with differentiated tariffs:\n\nFor other technologies decree 661/2007 setd up:\nOn 27 January 2012 the Spanish government temporarily stopped accepting applications for projects beginning operation after January 2013. Construction and operation of existing projects was not affected. The country's electrical system had a €24 billion deficit. FiT payments did not contribute significantly to that deficit. In 2008 the FiT was expected to result in 400 MW of solar being installed. However, it was so high that over 2600 MW was installed. Utilities in Spain reported that they had no way to pass on cost increases to consumers by increasing rates and instead accrued deficits, although this is under dispute.\n\nSwitzerland introduced the so-called \"Cost-covering remuneration for feed-in to the electricity grid (CRF)\" on 1 May 2008.\n\nCRF applies to hydropower (up to 10 megawatts), photovoltaics, wind energy, geothermal energy, biomass and waste material from biomass and will be applicable for 20 and 25 years, depending on the technology. The implementation is done through the national grid operator SWISSGRID.\n\nWhile high by appearance, CRF has had little effect, as the total amount of \"extra\" cost to the system was capped. Since about 2009, no more projects could be financed. About 15'000 projects awaited allocation of monies. If all those projects were implemented, Switzerland could mothball all its nuclear power plants, which currently supply 40% of its power.\n\nIn 2011, after Fukushima, some local power companies, mostly owned by villages and cantons/provinces, selectively started offering their own tariff, thereby creating a mini-boom.\n\nAs of March 2012 the KEV-FIT for Solar PV had been lowered several times to CHF 0.30-0.40/kWh (USD 0.33-0.44/kWh) depending on size, but was higher than in Germany and most of the rest of the world.\n\nThe feed-in tariff for renewable energy generation in Taiwan is set by the Bureau of Energy. It applies to most of the renewable energy sources, namely solar, wind, hydraulic, geothermal, biomass, waste etc.\n\nIn 2006, the Thai government enacted a tariff paid on top of utility avoided costs, differentiated by technology type and generator size and guaranteed for 7–10 years. Solar received the highest amount, 8 baht/kWh (about US cents 27/kWh). Large biomass projects received the lowest at 0.3 baht/kWh (at about 1 US cent per kWh). Additional per-kWh subsidies were provided for projects that offset diesel use in remote areas. As of 2010 March 1364 MW of private sector renewable energy was online with an additional 4104 MW in the pipeline with signed PPAs. Biomass made up the bulk of this capacity: 1292 MW (online) and 2119 MW (PPA only). Solar electricity was second but growing more rapidly, with 78 MW online and signed PPAs for an additional 1759 MW.\n\nUganda launched a tariff in 2011. The Uganda Electricity Transmission Company Limited held the transmission license in the country and was mandated by the Electricity Regulatory Authority to provide the following FiT for small-scale projects ranging from 0.5MW to 20MW.\n\nUkraine introduced the law 'On feed-in tariff' on 25 September 2008. The law guaranteed grid access for renewable energy producers (small hydro up to 10 MW, wind, biomass, photovoltaic and geothermal). The tariffs for renewable power producers are set by the national regulator. the following tariffs per kWh were applied: biomass – UAH 1.3446 (EUR 0.13), wind – UAH 1.2277 (EUR 0.12), small hydro – UAH 0.8418 (EUR 0.08), solar - UAH 5.0509 (EUR 0.48). In case of significant fluctuations of the national currency against Euro the feed-in tariff adjusts.\n\nIn October 2008 the United Kingdom announced that Britain would implement a scheme by 2010, in addition to its current renewable energy quota scheme (ROCS). In July 2009 Britain's then-Secretary of State for Energy and Climate Change, Ed Miliband, presented details of the scheme, which began in early April 2010.\n\nLess than a year into the scheme, in March 2011 the new coalition government announced that support for large-scale photovoltaic installations (greater than 50 kW) would be cut. This was in response to European speculators lining up to establish huge solar farms in the West Country that would have absorbed disproportionate amounts of the fund.\n\nOn 9 June 2011, DECC confirmed tariff cuts for solar PV systems above 50 KW after 1 August 2011. Many were disappointed with DECC's decision. It was believed that the total subsidies for solar PV industry were unchanged, but that tariffs for large systems would be cut to benefit smaller systems. The fast track review was based on the long term plan to reach an annual installation of 1.9GW in 2020.\n\nIn October 2011 DECC announced dramatic cuts of around 55% to tariff rates, with additional reductions for community or group schemes. The cuts were to be effective from 12 December 2011, with a consultation exercise to end on 23 December 2011. This was successfully challenged in the high court by an application for judicial review, jointly made by environmental pressure group Friends of the Earth (FoE) and two solar companies - Solarcentury and HomeSun. The judgment, made by Mr Justice Mitting after a two-day court hearing, was hailed as a major victory by green campaigners and the solar industry. Lawyers for the Department of Energy and Climate Change immediately moved to appeal the ruling. The appeal was unanimously rejected by the Supreme Court, allowing anyone who installed their systems before 3 March 2012 to receive the higher rate of 43.3 p/kWh.\n\nThe 30.7 p/kWh rate was available for solar systems up to 5 MW, and consequently no larger systems were built. Feed-In-Tariff Payments are Tax-Free in the United Kingdom.\n\nAs of April 2012, 263,274 systems, totaling 1,152.835 MW, were receiving FiT payments. Of these, 260,041 were solar photovoltaic, totaling 1,057.344 MW. Payments are for 25 years. A typical photovoltaic system costing £7,500 pays for itself in 7 years 8 months, and generates £23,610 over 25 years.\n\nIn April 2009, 11 state legislatures were considering adopting a FiT as a complement to their renewable electricity mandates.\n\nThe California Public Utilities Commission (CPUC) approved a feed-in tariff on 31 January 2008 effective immediately.\n\nIn 2010, Marin Energy Authority launched the first Community Choice Aggregate Feed-in Tariff program. The program was updated in November 2012, and now offers 20-year fixed-price contracts, with prices varying by energy source (peak, base-load, intermittent) and progress towards the current program cap of 10-MW.\n\nMunicipal utility companies enacted feed in tariff pilot programs in Palo Alto and Los Angeles:\nPalo Alto CLEAN (Clean Local Energy Accessible Now) is a program to purchase up to 4MW of electricity generated by solar electric systems located in CPAU's service territory. In 2012 the minimum project size was 100 kW. Rates of purchase are between 12.360 ¢/kWh to 14.003 ¢/kWh depending on the length of the contract. The City began accepting applications on 2 April 2012.\n\nOn 17 April 2012, Los Angeles Department of Water and Power's Board of Water and Power Commissioners approved a 10MW FiT Demonstration Program.\n\nAs of 1 January 2010 state laws allowed homeowners to sell excess power to the utility. Previously the homeowner would get no credit for over-production over the course of the year. In order to get the California Solar Initiative (CSI) rebate the customer was not allowed to install a system that deliberately over-produces thereby, encouraging efficiency measures to be installed after solar installation. This over-production credit was not available to certain municipal utility customers namely Los Angeles Water and Power.\n\nIn February 2009, city commissioners in Gainesville, Florida, approved the nation's first solar feed-in tariff. The program was capped at 4 MW per year. As of 2011, Gainesville had increased solar generated electricity from 328 kW to 7,391 kW, approximately 1.2% of peak load energy (610 MW). The program was suspended in 2014 after more than 18 MW of capacity had been installed.\n\nIn September 2009 the Hawaii Public Utilities Commission required Hawaiian Electric Company (HECO & MECO & HELCO) to pay above-market prices for renewable energy fed into the electric grid. The policy offers projects a set price and standard 20-year contract. The PUC planned to review the initial feed-in tariff two years after the program started and every three years thereafter.\n\nProject size was limited to five megawatts (MW) for the island of Oahu and 2.72 MW for Maui and Hawaii island. The Commission's decision capped the total amount of feed-in tariff projects brought onto the electricity grid at 5% of the system peak on Oahu, Maui, and Hawaii Island for the first two years. Tier 3 was still pending a Decision and Order based on the findings of the Reliability Standards Working Group (a \"docket within the docket\").\n\nTier 2 and 3 project size caps varied by island and by technology. Tier 2 includes larger systems that are less than or equal to: 100 kW-AC for on-shore wind and in-line hydropower on all islands; 100 kW-AC for PV and CSP on Lanai and Molokai; 250 kW-AC for PV on Maui and Hawaii; 500 kW-AC for CSP on Maui and Hawaii; and 500 kW-AC for PV and CSP on Oahu. Tier 3 covers systems larger than the Tier 2 caps.\n\nIn 2009 a \"Feed-In\" Tariff bill failed to pass.\nIn June 2009 a pilot program was initiated however, and was available for projects up to 10MW in size. On 24 April 2013, the Maine Utility and Energy Committee was to consider a new bill: LD1085 \"An Act to Establish the Renewable Energy Feed-in Tariff\".\n\nThe Long Island Power Authority (LIPA) adopted a feed-in tariff on 16 July 2012, for systems from 50 kW (AC) to 20 MW (AC), and was limited to 50 MW (AC). As customers cannot use their own electricity, it is actually a 20-year fixed rate power purchase agreement and LIPA retains the SRECs. The 2012 NY legislature failed to pass legislation which would have opened a New York market for SRECs starting in 2013. Payment is 22.5¢/kWh, less than what LIPA paid for peak generation at various times. At an estimated avoided cost of $0.075/kWh, the program added about $0.44/month to the average household electric bill.\n\nIn June 2009, Oregon established a pilot solar volumetric incentive rate and payment program. Under this incentive program, systems are paid for the kilowatt-hours (kWh) generated over a 15-year period, at a rate set at the time a system is enrolled in the program. The Oregon Public Utility Commission (PUC) established rates and rules in May 2010. This program was offered by the three investor-owned utilities in Oregon and administered by the utilities. The PUC planned to periodically re-evaluate rates. Program costs were recoverable in utility rates and utility-owned systems were not eligible for the incentive.\n\nThe pilot program installation cap was limited to an aggregate cap of 25 megawatts (MW) of solar photovoltaics (PV), with a maximum system size cap of 500 kilowatts (kW). The aggregate program cap was to be spread equally over four years, with 6.25 MW of capacity being eligible to receive the incentive each year. The aggregate cap was divided, based on 2008 retail sales revenue. PGE had a cap of 14.9 MW, Pacific Power 9.8 MW, and Idaho Power 0.4 MW. Idaho Power's program was limited to residential installations. Rates differed by system size and geographic zone. Small- and medium-scale systems participated in a program modeled after net metering. Larger-scale systems were competitively bid. Participating PV systems must be grid-connected, metered and meet all applicable codes and regulations. Systems must be \"permanently installed\".\n\nSystems sized 100 kW or less could participate based on net metering. Generating capacity of 20 MW of the aggregate cap was reserved for the net metering portion, with 12 MW available for residential and 8 MW available for small commercial systems. These residential and small commercial systems were paid for the amount of electricity generated, up to the amount of electricity consumed. In essence, customers were paid for the amount of utility electric load consumption that is offset by onsite generation. Unlike typical feed-in tariffs, customers can consume the electricity generated on-site and receive a production incentive – or a volumetric incentive payment – for the amount of electricity generated and consumed. To remove a perverse incentive to increase electricity consumption to receive a greater payment, the system had to be appropriately sized to meet average electricity consumption. Rates were determined by the PUC based on annual system cost and annual energy output, differentiated by geographic zones. The cost estimates were based on installation data from Energy Trust of Oregon. The actual rates paid to the customer-generator were the volumetric incentive rate minus the retail rate. The volumetric incentive rates were to be re-evaluated every six months. The rates for the performance-based incentive program ranged from $0.25/kWh to $0.411/kWh.\n\nVermont adopted feed-in tariffs on 27 May 2009 as part of the Vermont Energy Act of 2009. Generators must possess a capacity of no more than 2.2 MW, and participation is limited to 50 MW in 2012, a limit that increased by 5 to 10 MW/year to a total of 127.5 MW in 2022. Payments were 24¢/kWh for solar, which was increased to 27.1¢/kWh in March 2012, and 11.8¢/kWh for wind over 100 kW and 25.3¢/kWh for wind turbines up to 100 kW. Other qualifying technologies included methane, hydro and biomass. Vermont's SPEED program called for 20% renewable energy by 2017 and 75% by 2032. The program was fully subscribed in 2012. Payments are for 25 years.\n\nThe territory operated a net metering program that paid the energy fed back to the grid at the retail rate. The rate varied monthly around 23 cents per kilowatt. The program credited the provider's account each month rather than making actual payments. At the end of the fiscal year (June) any excess was paid at a fixed 10 cents per KW of which 25% was retained for public schools. To participate in the program insurance and means for disconnecting the system accessible outside of the building and specific brands of equipment dictated by the government were required.\n\n"}
{"id": "10832464", "url": "https://en.wikipedia.org/wiki?curid=10832464", "title": "Fineness ratio", "text": "Fineness ratio\n\nIn naval architecture and aerospace engineering, the fineness ratio is the ratio of the length of a body to its maximum width. Shapes that are short and wide have a low fineness ratio, those that are long and narrow have high fineness ratios. Aircraft that spend time at supersonic speeds, e.g. the Concorde, generally have high fineness ratios.\n\nAt speeds below critical mach, one of the primary forms of drag is skin friction. As the name implies, this is drag caused by the interaction of the airflow with the aircraft's skin. To minimize this drag, the aircraft should be designed to minimize the exposed skin area, or \"wetted surface\". One solution to this problem is constructing an \"egg shaped\" fuselage, for example as used on the home-built Questair Venture.\n\nTheoretical ideal fineness ratios in subsonic aircraft are typically found at about 6:1, however this may be compromised by other design considerations such as seating or freight size requirements. Because a higher fineness fuselage can have reduced tail surfaces, this ideal ratio can practically be increased to 8:1.\n\nMost aircraft have fineness ratios significantly greater than this, however. This is often due to the competing need to place the tail control surfaces at the end of a longer moment arm to increase their effectiveness. Reducing the length of the fuselage would require larger controls, which would offset the drag savings from using the ideal fineness ratio. An example of a high-performance design with an imperfect fineness ratio is the Lancair. In other cases, the designer is forced to use a non-ideal design due to outside factors such as seating arrangements or cargo pallet sizes. Modern airliners often have fineness ratios much higher than ideal, a side effect of their cylindrical cross-section which is selected for strength, as well as providing a single width to simplify seating layout.\n\nAs an aircraft approaches the speed of sound, shock waves form on areas of greater curvature. These shock waves radiate away energy that the engines must supply, energy that does not go into making the aircraft go faster. This appears to be a new form of drag—referred to as wave drag—which peaks at about three times the drag at speeds even slightly below the critical mach. In order to minimize the wave drag, the curvature of the aircraft should be kept to a minimum, which implies much higher fineness ratios. This is why high-speed aircraft have long pointed noses and tails, and cockpit canopies that are flush to the fuselage line.\n\nMore technically, the best possible performance for a supersonic design is typified by two \"perfect shapes\", the Sears-Haack body which is pointed at both ends, or the von Kármán ogive, which has a blunt tail. Examples of the latter design include the Concorde, F-104 Starfighter and XB-70 Valkyrie, although to some degree practically every post-World War II interceptor aircraft featured such a design. Missile designers are even less interested in low-speed performance, and missiles generally have higher fineness ratios than most aircraft.\n\nThe introduction of aircraft with higher fineness ratios also introduced a new form of instability, inertial coupling. As the engines and cockpit moved away from the aircraft's center of mass, the roll inertia of these masses grew to be able to overwhelm the power of the aerodynamic surfaces. A variety of methods are used to combat this effect, including oversized controls and stability augmentation systems.\n\n"}
{"id": "36703965", "url": "https://en.wikipedia.org/wiki?curid=36703965", "title": "Finnish Nature Heritage Foundation", "text": "Finnish Nature Heritage Foundation\n\nFinnish Nature Heritage Foundation is a foundation founded in 1995 by Finnish ecological activist Pentti Linkola. It has founded to preserve the remaining old-growth forests. The foundation gets donations from private individuals and companies, and then buys the forest areas seemed unique enough to deserve the protection. The protection of forests is double guaranteed - the forests are protected by Finnish nature protection laws, but also by being legally in ownership of Finnish Nature Heritage Foundation.\n"}
{"id": "221400", "url": "https://en.wikipedia.org/wiki?curid=221400", "title": "Flight dynamics (fixed-wing aircraft)", "text": "Flight dynamics (fixed-wing aircraft)\n\nFlight dynamics is the science of air vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation in three dimensions about the vehicle's center of mass, known as \"pitch\", \"roll\" and \"yaw\".\n\nAerospace engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the aerodynamic center of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft, causing the aircraft to pitch up or down.\n\nRoll, pitch and yaw refer to rotations about the respective axes starting from a defined steady flight equilibrium state. The equilibrium roll angle is known as wings level or zero bank angle, equivalent to a level heeling angle on a ship. Yaw is known as \"heading\". The equilibrium pitch angle in submarine and airship parlance is known as \"trim\", but in aircraft, \"trim\" usually refers to the equilibrium angle of attack, rather than orientation. However, common usage ignores this distinction between equilibrium and dynamic cases.\n\nThe most common aeronautical convention defines the roll as acting about the longitudinal axis, positive with the starboard (right) wing down. The yaw is about the vertical body axis, positive with the nose to starboard. Pitch is about an axis perpendicular to the longitudinal plane of symmetry, positive nose up.\n\nA fixed-wing aircraft increases or decreases the lift generated by the wings when it pitches nose up or down by increasing or decreasing the angle of attack (AOA). The roll angle is also known as bank angle on a fixed-wing aircraft, which usually \"banks\" to change the horizontal direction of flight. An aircraft is usually streamlined from nose to tail to reduce drag making it typically advantageous to keep the sideslip angle near zero, though there are instances when an aircraft may be deliberately \"sideslipped\" for example a slip in a fixed-wing aircraft.\n\nThree right-handed, Cartesian coordinate systems see frequent use in flight dynamics. The first coordinate system has an origin fixed in the reference frame of the Earth:\nIn many flight dynamics applications, the Earth frame is assumed to be inertial with a flat \"x\",\"y\"-plane, though the Earth frame can also be considered a spherical coordinate system with origin at the center of the Earth.\n\nThe other two reference frames are body-fixed, with origins moving along with the aircraft, typically at the center of gravity. For an aircraft that is symmetric from right-to-left, the frames can be defined as:\n\nAsymmetric aircraft have analogous body-fixed frames, but different conventions must be used to choose the precise directions of the \"x\" and \"z\" axes.\n\nThe Earth frame is a convenient frame to express aircraft translational and rotational kinematics. The Earth frame is also useful in that, under certain assumptions, it can be approximated as inertial. Additionally, one force acting on the aircraft, weight, is fixed in the +\"z\" direction.\n\nThe body frame is often of interest because the origin and the axes remain fixed relative to the aircraft. This means that the relative orientation of the Earth and body frames describes the aircraft attitude. Also, the direction of the force of thrust is generally fixed in the body frame, though some aircraft can vary this direction, for example by thrust vectoring.\n\nThe wind frame is a convenient frame to express the aerodynamic forces and moments acting on an aircraft. In particular, the net aerodynamic force can be divided into components along the wind frame axes, with the drag force in the −\"x\" direction and the lift force in the −\"z\" direction.\n\nIn addition to defining the reference frames, the relative orientation of the reference frames can be determined. The relative orientation can be expressed in a variety of forms, including:\nThe various Euler angles relating the three reference frames are important to flight dynamics. Many Euler angle conventions exist, but all of the rotation sequences presented below use the \"z-y'-x\"\" convention. This convention corresponds to a type of Tait-Bryan angles, which are commonly referred to as Euler angles. This convention is described in detail below for the roll, pitch, and yaw Euler angles that describe the body frame orientation relative to the Earth frame. The other sets of Euler angles are described below by analogy.\n\nTo transform from the Earth frame to the body frame using Euler angles, the following rotations are done in the order prescribed. First, rotate the Earth frame axes \"x\" and \"y\" around the \"z\" axis by the yaw angle \"ψ\". This results in an intermediate reference frame with axes denoted \"x\",y\",z\", where \"z'=z\". Second, rotate the \"x\" and \"z\" axes around the \"y\" axis by the pitch angle \"θ\". This results in another intermediate reference frame with axes denoted \"x\",y\",z\"\", where \"y\"=y\". Finally, rotate the \"y\"\" and \"z\"\" axes around the \"x\"\" axis by the roll angle \"φ\". The reference frame that results after the three rotations is the body frame.\n\nBased on the rotations and axes conventions above, the yaw angle \"ψ\" is the angle between north and the projection of the aircraft longitudinal axis onto the horizontal plane, the pitch angle \"θ\" is the angle between the aircraft longitudinal axis and horizontal, and the roll angle \"φ\" represents a rotation around the aircraft longitudinal axis after rotating by yaw and pitch.\n\nTo transform from the Earth frame to the wind frame, the three Euler angles are the bank angle \"μ\", the flight path angle \"γ\", and the heading angle \"σ\". When performing the rotations described above to obtain the wind frame from the Earth frame, (\"μ\",\"γ\",\"σ\") are analogous to (\"φ\",\"θ\",\"ψ\"), respectively. The heading angle \"σ\" is the angle between north and the horizontal component of the velocity vector, which describes which direction the aircraft is moving relative to cardinal directions. The flight path angle \"γ\" is the angle between horizontal and the velocity vector, which describes whether the aircraft is climbing or descending. The bank angle \"μ\" represents a rotation of the lift force around the velocity vector, which may indicate whether the airplane is turning.\n\nTo transform from the wind frame to the body frame, the two Euler angles are the angle of attack \"α\" and the sideslip angle \"β\". When performing the rotations described earlier to obtain the body frame from the wind frame, (\"α\",\"β\") are analogous to (\"θ\",\"ψ\"), respectively; the angle analogous to \"φ\" in this transformation is always zero. The sideslip angle \"β\" is the angle between the velocity vector and the projection of the aircraft longitudinal axis onto the \"x\",\"y\"-plane, which describes whether there is a lateral component to the aircraft velocity, also known as sideslip. The angle of attack \"α\" is the angle between the \"x\",\"y\"-plane and the aircraft longitudinal axis and, among other things, is an important variable in determining the magnitude of the force of lift.\n\nIn analyzing the stability of an aircraft, it is usual to consider perturbations about a nominal steady flight state. So the analysis would be applied, for example, assuming:\n\nThe speed, height and trim angle of attack are different for each flight condition, in addition, the aircraft will be configured differently, e.g. at low speed flaps may be deployed and the undercarriage may be down.\n\nExcept for asymmetric designs (or symmetric designs at significant sideslip), the longitudinal equations of motion (involving pitch and lift forces) may be treated independently of the lateral motion (involving roll and yaw).\n\nThe following considers perturbations about a nominal straight and level flight path.\n\nTo keep the analysis (relatively) simple, the control surfaces are assumed fixed throughout the motion, this is stick-fixed stability. Stick-free analysis requires the further complication of taking the motion of the control surfaces into account.\n\nFurthermore, the flight is assumed to take place in still air, and the aircraft is treated as a rigid body.\n\nThree forces act on an aircraft in flight: weight, thrust, and the aerodynamic force.\n\nThe expression to calculate the aerodynamic force is:\n\nwhere:\n\nprojected on wind axes we obtain:\n\nwhere:\n\nDynamic pressure of the free current formula_10\n\nProper reference surface (wing surface, in case of planes) formula_11\n\nPressure coefficient formula_12\n\nFriction coefficient formula_13\n\nDrag coefficient formula_14\n\nLateral force coefficient formula_15\n\nLift coefficient formula_16\n\nIt is necessary to know C and C in every point on the considered surface.\n\nIn absence of thermal effects, there are three remarkable dimensionless numbers:\n\n\nwhere:\n\nAccording to λ there are three possible rarefaction grades and their corresponding motions are called:\n\nThe motion of a body through a flow is considered, in flight dynamics, as continuum current. In the outer layer of the space that surrounds the body viscosity will be negligible. However viscosity effects will have to be considered when analysing the flow in the nearness of the boundary layer.\n\nDepending on the compressibility of the flow, different kinds of currents can be considered:\n\nIf the geometry of the body is fixed and in case of symmetric flight (β=0 and Q=0), pressure and friction coefficients are functions depending on:\nwhere:\n\nUnder these conditions, drag and lift coefficient are functions depending exclusively on the angle of attack of the body and Mach and Reynolds numbers. Aerodynamic efficiency, defined as the relation between lift and drag coefficients, will depend on those parameters as well.\n\nIt is also possible to get the dependency of the drag coefficient respect to the lift coefficient. This relation is known as the drag coefficient equation:\nThe aerodynamic efficiency has a maximum value, E, respect to C where the tangent line from the coordinate origin touches the drag coefficient equation plot.\n\nThe drag coefficient, C, can be decomposed in two ways. First typical decomposition separates pressure and friction effects:\n\nThere's a second typical decomposition taking into account the definition of the drag coefficient equation. This decomposition separates the effect of the lift coefficient in the equation, obtaining two terms C and C. C is known as the parasitic drag coefficient and it is the base draft coefficient at zero lift. C is known as the induced drag coefficient and it is produced by the body lift.\n\nA good attempt for the induced drag coefficient is to assume a parabolic dependency of the lift\n\nAerodynamic efficiency is now calculated as:\n\nIf the configuration of the plane is symmetrical respect to the XY plane, minimum drag coefficient equals to the parasitic drag of the plane.\n\nIn case the configuration is asymmetrical respect to the XY plane, however, minimum flag differs from the parasitic drag. On these cases, a new approximate parabolic drag equation can be traced leaving the minimum drag value at zero lift value.\n\nThe Coefficient of pressure varies with Mach number by the relation given below:\n\nwhere\nThis relation is reasonably accurate for 0.3 < M < 0.7 and when \"M = 1\" it becomes ∞ which is impossible physical situation and is called Prandtl–Glauert singularity.\n\nsee Aerodynamic force\n\nsee Longitudinal static stability\n\nDirectional or weathercock stability is concerned with the static stability of the airplane about the z axis. Just as in the case of longitudinal stability it is desirable that the aircraft should tend to return to an equilibrium condition when subjected to some form of yawing disturbance. For this the slope of the yawing moment curve must be positive.\nAn airplane possessing this mode of stability will always point towards the relative wind, hence the name weathercock stability.\n\nIt is common practice to derive a fourth order characteristic equation to describe the longitudinal motion, and then factorize it approximately into a high frequency mode and a low frequency mode. The approach adopted here is using qualitative knowledge of aircraft behavior to simplify the equations from the outset, reaching the result by a more accessible route.\n\nThe two longitudinal motions (modes) are called the short period pitch oscillation (SPPO), and the phugoid.\n\nA short input (in control systems terminology an impulse) in pitch (generally via the elevator in a standard configuration fixed-wing aircraft) will generally lead to overshoots about the trimmed condition. The transition is characterized by a damped simple harmonic motion about the new trim. There is very little change in the trajectory over the time it takes for the oscillation to damp out.\n\nGenerally this oscillation is high frequency (hence short period) and is damped over a period of a few seconds. A real-world example would involve a pilot selecting a new climb attitude, for example 5° nose up from the original attitude. A short, sharp pull back on the control column may be used, and will generally lead to oscillations about the new trim condition. If the oscillations are poorly damped the aircraft will take a long period of time to settle at the new condition, potentially leading to Pilot-induced oscillation. If the short period mode is unstable it will generally be impossible for the pilot to safely control the aircraft for any period of time.\n\nThis damped harmonic motion is called the short period pitch oscillation, it arises from the tendency of a stable aircraft to point in the general direction of flight. It is very similar in nature to the weathercock mode of missile or rocket configurations. The motion involves mainly the pitch attitude formula_46 (theta) and incidence formula_47 (alpha). The direction of the velocity vector, relative to inertial axes is formula_48. The velocity vector is:\n\nwhere formula_51,formula_52 are the inertial axes components of velocity. According to Newton's Second Law, the accelerations are proportional to the forces, so the forces in inertial axes are:\n\nwhere \"m\" is the mass.\nBy the nature of the motion, the speed variation formula_55 is negligible over the period of the oscillation, so:\n\nBut the forces are generated by the pressure distribution on the body, and are referred to the velocity vector. But the velocity (wind) axes set is not an inertial frame so we must resolve the fixed axes forces into wind axes. Also, we are only concerned with the force along the z-axis:\n\nOr:\n\nIn words, the wind axes force is equal to the centripetal acceleration.\n\nThe moment equation is the time derivative of the angular momentum:\nwhere M is the pitching moment, and B is the moment of inertia about the pitch axis.\nLet: formula_61, the pitch rate.\nThe equations of motion, with all forces and moments referred to wind axes are, therefore:\nWe are only concerned with perturbations in forces and moments, due to perturbations in the states formula_47 and q, and their time derivatives. These are characterized by stability derivatives determined from the flight condition. The possible stability derivatives are:\n\nSince the tail is operating in the flowfield of the wing, changes in the wing incidence cause changes in the downwash, but there is a delay for the change in wing flowfield to affect the tail lift, this is represented as a moment proportional to the rate of change of incidence:\n\nThe delayed downwash effect gives the tail more lift and produces a nose down moment, so formula_70 is expected to be negative.\n\nThe equations of motion, with small perturbation forces and moments become:\n\nThese may be manipulated to yield as second order linear differential equation in formula_47:\n\nThis represents a damped simple harmonic motion.\n\nWe should expect formula_76 to be small compared with unity, so the coefficient of formula_47 (the 'stiffness' term) will be positive, provided formula_78. This expression is dominated by formula_68, which defines the longitudinal static stability of the aircraft, it must be negative for stability. The damping term is reduced by the downwash effect, and it is difficult to design an aircraft with both rapid natural response and heavy damping. Usually, the response is underdamped but stable.\n\nIf the stick is held fixed, the aircraft will not maintain straight and level flight, but will start to dive, level out and climb again. It will repeat this cycle until the pilot intervenes. This long period oscillation in speed and height is called the phugoid mode. This is analyzed by assuming that the SSPO performs its proper function and maintains the angle of attack near its nominal value. The two states which are mainly affected are the climb angle formula_80 (gamma) and speed. The small perturbation equations of motion are:\n\nwhich means the centripetal force is equal to the perturbation in lift force.\n\nFor the speed, resolving along the trajectory:\n\nwhere g is the acceleration due to gravity at the earths surface. The acceleration along the trajectory is equal to the net x-wise force minus the component of weight. We should not expect significant aerodynamic derivatives to depend on the climb angle, so only formula_83 and formula_84 need be considered. formula_83 is the drag increment with increased speed, it is negative, likewise formula_84 is the lift increment due to speed increment, it is also negative because lift acts in the opposite sense to the z-axis.\n\nThe equations of motion become:\n\nThese may be expressed as a second order equation in climb angle or speed perturbation:\nNow lift is very nearly equal to weight:\nwhere formula_91 is the air density, formula_92 is the wing area, W the weight and formula_93 is the lift coefficient (assumed constant because the incidence is constant), we have, approximately:\n\nThe period of the phugoid, T, is obtained from the coefficient of u:\n\nOr:\n\nSince the lift is very much greater than the drag, the phugoid is at best lightly damped. A propeller with fixed speed would help. Heavy damping of the pitch rotation or a large rotational inertia increase the coupling between short period and phugoid modes, so that these will modify the phugoid.\n\nWith a symmetrical rocket or missile, the directional stability in yaw is the same as the pitch stability; it resembles the short period pitch oscillation, with yaw plane equivalents to the pitch plane stability derivatives. For this reason, pitch and yaw directional stability are collectively known as the \"weathercock\" stability of the missile.\n\nAircraft lack the symmetry between pitch and yaw, so that directional stability in yaw is derived from a different set of stability derivatives. The yaw plane equivalent to the short period pitch oscillation, which describes yaw plane directional stability is called Dutch roll. Unlike pitch plane motions, the lateral modes involve both roll and yaw motion.\n\nIt is customary to derive the equations of motion by formal manipulation in what, to the engineer, amounts to a piece of mathematical sleight of hand. The current approach follows the pitch plane analysis in formulating the equations in terms of concepts which are reasonably familiar.\n\nApplying an impulse via the rudder pedals should induce Dutch roll, which is the oscillation in roll and yaw, with the roll motion lagging yaw by a quarter cycle, so that the wing tips follow elliptical paths with respect to the aircraft.\n\nThe yaw plane translational equation, as in the pitch plane, equates the centripetal acceleration to the side force.\n\nwhere formula_98 (beta) is the sideslip angle, Y the side force and r the yaw rate.\n\nThe moment equations are a bit trickier. The trim condition is with the aircraft at an angle of attack with respect to the airflow. The body x-axis does not align with the velocity vector, which is the reference direction for wind axes. In other words, wind axes are not principal axes (the mass is not distributed symmetrically about the yaw and roll axes). Consider the motion of an element of mass in position -z, x in the direction of the y-axis, i.e. into the plane of the paper.\nIf the roll rate is p, the velocity of the particle is:\n\nMade up of two terms, the force on this particle is first the proportional to rate of v change, the second is due to the change in direction of this component of velocity as the body moves. The latter terms gives rise to cross products of small quantities (pq, pr,qr), which are later discarded. In this analysis, they are discarded from the outset for the sake of clarity. In effect, we assume that the direction of the velocity of the particle due to the simultaneous roll and yaw rates does not change significantly throughout the motion. With this simplifying assumption, the acceleration of the particle becomes:\n\nThe yawing moment is given by:\n\nThere is an additional yawing moment due to the offset of the particle in the y direction:formula_102\n\nThe yawing moment is found by summing over all particles of the body:\n\nwhere N is the yawing moment, E is a product of inertia, and C is the moment of inertia about the yaw axis.\nA similar reasoning yields the roll equation:\n\nwhere L is the rolling moment and A the roll moment of inertia.\n\nThe states are formula_98 (sideslip), r (yaw rate) and p (roll rate), with moments N (yaw) and L (roll), and force Y (sideways). There are nine stability derivatives relevant to this motion, the following explains how they originate. However a better intuitive understanding is to be gained by simply playing with a model airplane, and considering how the forces on each component are affected by changes in sideslip and angular velocity:\n\nSideslip generates a sideforce from the fin and the fuselage. In addition, if the wing has dihedral, side slip at a positive roll angle increases incidence on the starboard wing and reduces it on the port side, resulting in a net force component directly opposite to the sideslip direction. Sweep back of the wings has the same effect on incidence, but since the wings are not inclined in the vertical plane, backsweep alone does not affect formula_106. However, anhedral may be used with high backsweep angles in high performance aircraft to offset the wing incidence effects of sideslip. Oddly enough this does not reverse the sign of the wing configuration's contribution to formula_106 (compared to the dihedral case).\n\nRoll rate causes incidence at the fin, which generates a corresponding side force. Also, positive roll (starboard wing down) increases the lift on the starboard wing and reduces it on the port. If the wing has dihedral, this will result in a side force momentarily opposing the resultant sideslip tendency. Anhedral wing and or stabilizer configurations can cause the sign of the side force to invert if the fin effect is swamped.\n\nYawing generates side forces due to incidence at the rudder, fin and fuselage.\n\nSideslip in the absence of rudder input causes incidence on the fuselage and empennage, thus creating a yawing moment counteracted only by the directional stiffness which would tend to point the aircraft's nose back into the wind in horizontal flight conditions. Under sideslip conditions at a given roll angle formula_111 will tend to point the nose into the sideslip direction even without rudder input, causing a downward spiraling flight.\n\nRoll rate generates fin lift causing a yawing moment and also differentially alters the lift on the wings, thus affecting the induced drag contribution of each wing, causing a (small) yawing moment contribution. Positive roll generally causes positive formula_113 values unless the empennage is anhedral or fin is below the roll axis. Lateral force components resulting from dihedral or anhedral wing lift differences has little effect on formula_113 because the wing axis is normally closely aligned with the center of gravity.\n\nYaw rate input at any roll angle generates rudder, fin and fuselage force vectors which dominate the resultant yawing moment. Yawing also increases the speed of the outboard wing whilst slowing down the inboard wing, with corresponding changes in drag causing a (small) opposing yaw moment. formula_116 opposes the inherent directional stiffness which tends to point the aircraft's nose back into the wind and always matches the sign of the yaw rate input.\n\nA positive sideslip angle generates empennage incidence which can cause positive or negative roll moment depending on its configuration. For any non-zero sideslip angle dihedral wings causes a rolling moment which tends to return the aircraft to the horizontal, as does back swept wings. With highly swept wings the resultant rolling moment may be excessive for all stability requirements and anhedral could be used to offset the effect of wing sweep induced rolling moment.\n\nYaw increases the speed of the outboard wing whilst reducing speed of the inboard one, causing a rolling moment to the inboard side. The contribution of the fin normally supports this inward rolling effect unless offset by anhedral stabilizer above the roll axis (or dihedral below the roll axis).\n\nRoll creates counter rotational forces on both starboard and port wings whilst also generating such forces at the empennage. These opposing rolling moment effects have to be overcome by the aileron input in order to sustain the roll rate. If the roll is stopped at a non-zero roll angle the formula_118 \"upward\" rolling moment induced by the ensuing sideslip should return the aircraft to the horizontal unless exceeded in turn by the \"downward\" formula_119 rolling moment resulting from sideslip induced yaw rate. Longitudinal stability could be ensured or improved by minimizing the latter effect.\n\nSince Dutch roll is a handling mode, analogous to the short period pitch oscillation, any effect it might have on the trajectory may be ignored. The body rate \"r\" is made up of the rate of change of sideslip angle and the rate of turn. Taking the latter as zero, assuming no effect on the trajectory, for the limited purpose of studying the Dutch roll:\n\nThe yaw and roll equations, with the stability derivatives become:\n\nThe inertial moment due to the roll acceleration is considered small compared with the aerodynamic terms, so the equations become:\n\nThis becomes a second order equation governing either roll rate or sideslip:\n\nThe equation for roll rate is identical. But the roll angle, \"formula_129\" (phi) is given by:\n\nIf \"p\" is a damped simple harmonic motion, so is \"formula_129\", but the roll must be in quadrature with the roll rate, and hence also with the sideslip. The motion consists of oscillations in roll and yaw, with the roll motion lagging 90 degrees behind the yaw. The wing tips trace out elliptical paths.\n\nStability requires the \"stiffness\" and \"damping\" terms to be positive. These are:\n\nThe denominator is dominated by formula_120, the roll damping derivative, which is always negative, so the denominators of these two expressions will be positive.\n\nConsidering the \"stiffness\" term: formula_135 will be positive because formula_120 is always negative and formula_111 is positive by design. formula_118 is usually negative, whilst formula_113 is positive. Excessive dihedral can destabilize the Dutch roll, so configurations with highly swept wings require anhedral to offset the wing sweep contribution to formula_118.\n\nThe damping term is dominated by the product of the roll damping and the yaw damping derivatives, these are both negative, so their product is positive. The Dutch roll should therefore be damped.\n\nThe motion is accompanied by slight lateral motion of the center of gravity and a more \"exact\" analysis will introduce terms in formula_106 etc. In view of the accuracy with which stability derivatives can be calculated, this is an unnecessary pedantry, which serves to obscure the relationship between aircraft geometry and handling, which is the fundamental objective of this article.\n\nJerking the stick sideways and returning it to center causes a net change in roll orientation.\n\nThe roll motion is characterized by an absence of natural stability, there are no stability derivatives which generate moments in response to the inertial roll angle. A roll disturbance induces a roll rate which is only canceled by pilot or autopilot intervention. This takes place with insignificant changes in sideslip or yaw rate, so the equation of motion reduces to:\n\nformula_120 is negative, so the roll rate will decay with time. The roll rate reduces to zero, but there is no direct control over the roll angle.\n\nSimply holding the stick still, when starting with the wings near level, an aircraft will usually have a tendency to gradually veer off to one side of the straight flightpath. This is the (slightly unstable) spiral mode.\n\nIn studying the trajectory, it is the direction of the velocity vector, rather than that of the body, which is of interest. The direction of the velocity vector when projected on to the horizontal will be called the track, denoted \"formula_144\" (mu). The body orientation is called the heading, denoted \"formula_145\" (psi). The force equation of motion includes a component of weight:\n\nwhere \"g\" is the gravitational acceleration, and \"U\" is the speed.\n\nIncluding the stability derivatives:\n\nRoll rates and yaw rates are expected to be small, so the contributions of formula_110 and formula_109 will be ignored.\n\nThe sideslip and roll rate vary gradually, so their time derivatives are ignored. The yaw and roll equations reduce to:\n\nSolving for \"formula_98\" and \"p\":\n\nSubstituting for sideslip and roll rate in the force equation results in a first order equation in roll angle:\n\nThis is an exponential growth or decay, depending on whether the coefficient of \"formula_129\" is positive or negative. The denominator is usually negative, which requires formula_157 (both products are positive). This is in direct conflict with the Dutch roll stability requirement, and it is difficult to design an aircraft for which both the Dutch roll and spiral mode are inherently stable.\n\nSince the spiral mode has a long time constant, the pilot can intervene to effectively stabilize it, but an aircraft with an unstable Dutch roll would be difficult to fly. It is usual to design the aircraft with a stable Dutch roll mode, but slightly unstable spiral mode.\n\n\n\n"}
{"id": "35987728", "url": "https://en.wikipedia.org/wiki?curid=35987728", "title": "Francisco Pineda (environmentalist)", "text": "Francisco Pineda (environmentalist)\n\nFrancisco Pineda is a Salvadoran environmentalist. He was awarded the Goldman Environmental Prize in 2011, for his efforts on protection of water resources in El Salvador against pollution from mining projects.\n"}
{"id": "20135224", "url": "https://en.wikipedia.org/wiki?curid=20135224", "title": "Fuel (film)", "text": "Fuel (film)\n\nFuel (previously called Fields of Fuel) is a 2008 documentary film directed by Josh Tickell and produced by Greg Reitman, Dale Rosenbloom, Daniel Assael, Darius Fisher, and Rebecca Harrell Tickell.\n\nIt won the audience award at the 2008 Sundance Film Festival. The DVD was released on June 22, 2010.\n\nAccording to director Josh Tickell, since its appearance at the Sundance Film Festival, the film has gone through major editing changes and additions. The name was changed from \"Fields of Fuel\" to \"Fuel\". This edited film is a re-cut of the same film with 45 minutes of new material in its total 112-minute running time.\n\n\n"}
{"id": "20392947", "url": "https://en.wikipedia.org/wiki?curid=20392947", "title": "Geothermal Energy Association", "text": "Geothermal Energy Association\n\nThe Geothermal Energy Association (GEA) is a U.S. trade organization composed of U.S. companies who support the expanded use of geothermal energy and are developing geothermal resources worldwide for electrical power generation and direct-heat uses. GEA advocates for public policies that will promote the development and utilization of geothermal resources, provides a forum for the industry to discuss issues and problems, encourages research and development to improve geothermal technologies, presents industry views to governmental organizations, provides assistance for the export of geothermal goods and services, compiles statistical data about the geothermal industry, and conducts education and outreach projects.\n\nHeadquartered in Washington, D.C., GEA represents the U.S. geothermal energy industry in congressional lobbying efforts, compiles and publishes data relating the geothermal installations and the state of the industry, represents the industry to the media and the general public, and hosts conferences and workshops for its member companies.\n\nThe Executive Director of GEA is Karl Gawell.\n\n\n"}
{"id": "867106", "url": "https://en.wikipedia.org/wiki?curid=867106", "title": "Ghislenghien", "text": "Ghislenghien\n\nGhislenghien () is a small town near Ath in Hainaut province, part of the Francophone Walloon region of Belgium. It has about 3000 inhabitants.\n\nOn July 30, 2004 a high-pressure natural gas pipeline operated at a pressure of 70 bar ruptured following recent third party damage. Twenty-four people died as a result and 150 survivors were hospitalised, most with severe burns.\n\nIt is thought that damage to the pipeline occurred during the final stages of a car park construction project. Notice of the work had been given to the pipeline operator, Fluxys, and one of their operatives had regularly attended the site through the course of the project. Damage to the pipeline probably occurred as a mechanical soil stabiliser was driven over it or near by. This resulted in several evenly spaced (but not full depth) gouges in the steel wall of the pipeline. Two weeks after the completion of the car park gas pressure was increased in the pipeline, which then ruptured with the fault centred on a 350 mm long gouge. Other contributing factors to the accident may have been a reduced cover over the pipeline as a result of levelling, the way information was passed down the sub-contracting chain to workers and the frequency and adequacy of supervision by the pipeline operator at the site.\n"}
{"id": "21103187", "url": "https://en.wikipedia.org/wiki?curid=21103187", "title": "Glidcop", "text": "Glidcop\n\nGlidcop is the registered trademark name of North American Höganäs, that refers to a family of copper-based metal matrix composite (MMC) alloys mixed primarily with aluminum oxide ceramic particles. The addition of small amounts of aluminum oxide has minuscule effects on the performance of the copper at room temperature (such as a small decrease in thermal and electrical conductivity), but greatly increases the copper's resistance to thermal softening and enhances high elevated temperature strength. The addition of aluminum oxide also increases resistance to radiation damage. As such, the alloy has found use in applications where high thermal conductivity or electrical conductivity is required while also maintaining strength at elevated temperatures or radiation levels.\n\nGlidcop is available in several grades which have varying amounts of aluminum oxide content.\n\nAdditional materials and elements can be added if lower thermal expansion is required, or higher room temperature and elevated temperature strengths. The hardness can also be increased. A composite material of Glidcop AL-60 and 10 % Niobium provides high strength and high conductivity. The hardness is comparable to many copper-beryllium and copper-tungsten alloys, while the electrical conductivity is comparable to RWMA Class 2 alloy. Other additives for specialized applications include molybdenum, tungsten, Kovar, and Alloy 42.\n\nAt elevated temperatures, Glidcop maintains its strength much better than oxygen-free copper. The aluminum oxide particles in the copper block dislocation movement, which retards recrystallization and prevents grain growth. At Glidcop AL-15 has a yield strength of over 29 ksi (200 MPa). Glidcop also has exceptional elevated temperature stress rupture strength when compared to oxygen-free copper.\n\nGlidcop also has excellent resistance to softening after exposure to elevated temperatures.\n\nGlidcop is resistant to degradation by neutron irradiation. For samples irradiated by neutrons at and cooled to room temperature, the tensile strengths and electrical conductivity were greater than that of pure copper while the swelling was lower. For samples irradiated from 0 to 150 dpm (displacements per atom), the tensile strength was nearly consistent, while the pure copper experienced a linear decrease in tensile strength on the range from 0 to 50 dpm. For sample swelling, the Glidcop had no noticeable swelling to 150 dpm while the pure copper had a linear growth to approximately 50 dpm, where swelling was 30 % of the original. For electrical conductivity, both the pure copper and Glidcop experienced linear drops in performance, though the Glidcop was less affected by the radiation.\n\nGlidcop material is often acquired with a layer of cladding, typically 10-15 % of the cross-sectional area of the stock piece, though this varies depending on the production process. The cladding, which is a remnant of the extrusion process often used with Glidcop, must be machined (usually by milling or grinding) off the stock piece in order to take full advantage of the Glidcop properties. After the cladding is removed, machining and working with Glidcop is similar to that of pure copper.\n\nJoining Glidcop material through brazing can be somewhat difficult. Brazing with silver based braze alloys can lead to problems due to the excessive diffusion of silver along grain boundaries. This is often circumvented by first electroplating the Glidcop part with either copper or nickel. The copper plating is often done in a copper cyanide solution since other solutions were found to be problematic. Brazing alloys used include 3565 AuCu and 5050 AuCu, which are used in a dry hydrogen atmosphere.\n\nGlidcop also has excellent cold workability. Cold working by drawing, cold heading, or cold forming increase strength while reducing ductility.\n\nGlidcop has been successfully applied to resistance welding electrodes to reduce stick to galvanized and other coated steels, and in incandescent light bulb leads by resisting softening after exposure to high temperatures. Likewise, Glidcop has found used in relay blades and contactor supports. The alloy's ability to maintain strength after high temperature brazing has led to use in hybrid circuit packages. Furthermore, it has found use in other high temperature applications such as x-ray tube components, and heat exchanger sections for fusion power and synchrotron units. Other uses include high field magnetic coils, sliding electrical contacts, arc welder electrodes, electronic leadframes, MIG contact tips, commutators, high speed motor and generator components, and microwave power tube components.\n\nOne of the more intensive uses of Glidcop has been in particle accelerator components, where the alloy may be subjected to high temperatures and high radiation simultaneously. Examples include Radio Frequency Quadrupoles (RFQs) and Compact Absorbers for High-Heat-Load X-ray Undulator Beamlines.\n\n\n"}
{"id": "696449", "url": "https://en.wikipedia.org/wiki?curid=696449", "title": "Granular material", "text": "Granular material\n\nA granular material is a conglomeration of discrete solid, macroscopic particles characterized by a loss of energy whenever the particles interact (the most common example would be friction when grains collide). The constituents that compose granular material are large enough such that they are not subject to thermal motion fluctuations. Thus, the lower size limit for grains in granular material is about 1 µm. On the upper size limit, the physics of granular materials may be applied to ice floes where the individual grains are icebergs and to asteroid belts of the Solar System with individual grains being asteroids.\n\nSome examples of granular materials are snow, nuts, coal, sand, rice, coffee, corn flakes, fertilizer, and bearing balls. Research into granular materials is thus directly applicable and goes back at least to Charles-Augustin de Coulomb, whose law of friction was originally stated for granular materials. Granular materials are commercially important in applications as diverse as pharmaceutical industry, agriculture, and energy production.\n\nPowders are a special class of granular material due to their small particle size, which makes them more cohesive and more easily suspended in a gas. \n\nThe soldier/physicist Brigadier Ralph Alger Bagnold was an early pioneer of the physics of granular matter and whose book \"The Physics of Blown Sand and Desert Dunes\" remains an important reference to this day. According to material scientist Patrick Richard, \"Granular materials are ubiquitous in nature and are the second-most manipulated material in industry (the first one is water)\".\n\nIn some sense, granular materials do not constitute a single phase of matter but have characteristics reminiscent of solids, liquids, or gases depending on the average energy per grain. However, in each of these states granular materials also exhibit properties which are unique.\n\nGranular materials also exhibit a wide range of pattern forming behaviors when excited (e.g. vibrated or allowed to flow). As such granular materials under excitation can be thought of as an example of a complex system.\n\nWhen the average energy of the individual grains is low and the grains are fairly stationary relative to each other, the granular material acts like a solid. In general, stress in a granular solid is not distributed uniformly but is conducted away along so-called force chains which are networks of grains resting on one another. Between these chains are regions of low stress whose grains are shielded for the effects of the grains above by vaulting and arching.\n\nIf the granular material is driven harder such that contacts between the grains become highly infrequent, the material enters a gaseous state. Correspondingly, one can define a granular temperature equal to the root mean square of grain velocity fluctuations that is analogous to thermodynamic temperature.\nUnlike conventional gases, granular materials will tend to cluster and clump due to the dissipative nature of the collisions between grains. This clustering has some interesting consequences. For example, if a partially partitioned box of granular materials is vigorously shaken then grains will over time tend to collect in one of the partitions rather than spread evenly into both partitions as would happen in a conventional gas. This effect, known as the granular Maxwell's demon, does not violate any thermodynamics principles since energy is constantly being lost from the system in the process.\n\nGranular systems are known to exhibit jamming and undergo a jamming transition which is thought of as a thermodynamic phase transition to a jammed state.\nThe Lubachevsky-Stillinger algorithm of jamming allows one to produce simulated jammed granular configurations.\nExcited granular matter is a rich pattern-forming system. Some of the pattern-forming behaviours seen in granular materials are:\nSome of the pattern-forming behaviours have been possible to reproduce in computer simulations.\n\nThere are two main computational approaches to such simulations, time-stepped and event-driven, the former being the most efficient for a higher density of the material and the motions of a lower intensity, and the latter for a lower density of the material and the motions of a higher intensity.\n\nSome beach sands, such as those of the aptly named Squeaky Beach, exhibit squeaking when walked upon. Some desert dunes are known to exhibit booming during avalanching or when their surface is otherwise disturbed. Granular materials discharged from silos produce loud acoustic emissions in a process known as silo honking.\n\nGranulation is the act or process in which primary powder particles are made to adhere to form larger, multiparticle entities called \"granules.\"\n\nSeveral methods are available for modeling of granular materials. Most of these methods consist of the statistical methods by which various statistical properties, derived from either point data or an image, are extracted and used to generate stochastic models of the granular medium. A recent and comprehensive review of such methods is available in Tahmasebi and other (2017).\n\nAnother alternative for building a pack of granular particles that recently has been presented is based on the level-set algorithm by which the real shape of the particle can be captured and reproduced through the extracted statistics for particles' morphologies.\n\n\n"}
{"id": "47791995", "url": "https://en.wikipedia.org/wiki?curid=47791995", "title": "Green Machine / Blue Space", "text": "Green Machine / Blue Space\n\nGreen Machine / Blue Space is the first solar hydrogen home. It was developed by NYIT and US Merchant Marine Academy. It is currently housed in US Merchant Marine Academy. It was built using a modified shipping container. It placed fifth in the Solar Decathlon.\n\n"}
{"id": "14329892", "url": "https://en.wikipedia.org/wiki?curid=14329892", "title": "Horná Orava Protected Landscape Area", "text": "Horná Orava Protected Landscape Area\n\nHorná Orava Protected Landscape Area () is one of the 14 protected landscape areas in Slovakia. It is situated in the Námestovo and Tvrdošín districts, within the Orava region.\n\nThe park was created on 24 September 1979 and the law creating it was amended on 29 September 2003.\n\nIt is made of Oravská Magura, Podbeskydská vrchovina and Oravské Beskydy mountains, and the Orava Basin, as well as the Orava Dam, in the northernmost Slovakia.\nMuch of the PLA's territory is made from sandstone mountain ranges. More than half of the area is covered by forests. Beech and fir trees grow in the area, along with strong presence of the spruce monoculture. Exceptions are the areas under Babia hora, Paráč and Pilsko mountains, with old growth spruce with rowan trees being represented. A specific phenomenon of the park are the peat bogs, represented by the pine woods, providing shelter for many threatened species. The Orava Dam is a nesting place for many rare species of birds. The highest mountain is Babia hora at the border with Poland at \n\n"}
{"id": "269980", "url": "https://en.wikipedia.org/wiki?curid=269980", "title": "Julian Schwinger", "text": "Julian Schwinger\n\nJulian Seymour Schwinger (; February 12, 1918 – July 16, 1994) was a Nobel Prize winning American theoretical physicist. He is best known for his work on the theory of quantum electrodynamics (QED), in particular for developing a relativistically invariant perturbation theory, and for renormalizing QED to one loop order. Schwinger was a physics professor at several universities.\n\nSchwinger is recognized as one of the greatest physicists of the twentieth century, responsible for much of modern quantum field theory, including a variational approach, and the equations of motion for quantum fields. He developed the first electroweak model, and the first example of confinement in 1+1 dimensions. He is responsible for the theory of multiple neutrinos, Schwinger terms, and the theory of the spin 3/2 field.\n\nJulian Seymour Schwinger was born in New York City, to Jewish parents originally from Poland, Belle (née Rosenfeld) and Benjamin Schwinger, a garment manufacturer, who had migrated to America. Both his father and his mother's parents were prosperous clothing manufacturers, although the family business declined after the Wall Street Crash of 1929. The family followed the Orthodox Jewish tradition. Schwinger attended the Townsend Harris High School and then the City College of New York as an undergraduate before transferring to Columbia University, where he received his B.A. in 1936 and his Ph.D. (overseen by Isidor Isaac Rabi) in 1939 at the age of 21. He worked at the University of California, Berkeley (under J. Robert Oppenheimer), and was later appointed to a position at Purdue University.\n\nAfter having worked with Oppenheimer, Schwinger's first regular academic appointment was at Purdue University in 1941. While on leave from Purdue, he worked at the Radiation Laboratory at MIT instead of at the Los Alamos National Laboratory during World War II. He provided theoretical support for the development of radar. After the war, Schwinger left Purdue for Harvard University, where he taught from 1945 to 1974. In 1966 he became the Eugene Higgins professor of physics at Harvard.\n\nSchwinger developed an affinity for Green's functions from his radar work, and he used these methods to formulate quantum field theory in terms of local Green's functions in a relativistically invariant way. This allowed him to calculate unambiguously the first corrections to the electron magnetic moment in quantum electrodynamics. Earlier non-covariant work had arrived at infinite answers, but the extra symmetry in his methods allowed Schwinger to isolate the correct finite corrections.\n\nSchwinger developed renormalization, formulating quantum electrodynamics unambiguously to one-loop order.\n\nIn the same era, he introduced non-perturbative methods into quantum field theory, by calculating the rate at which electron-positron pairs are created by tunneling in an electric field, a process now known as the \"Schwinger effect\". This effect could not be seen in any finite order in perturbation theory.\n\nSchwinger's foundational work on quantum field theory constructed the modern framework of field correlation functions and their equations of motion. His approach started with a quantum action and allowed bosons and fermions to be treated equally for the first time, using a differential form of Grassman integration. He gave elegant proofs for the spin-statistics theorem and the CPT theorem, and noted that the field algebra led to anomalous Schwinger terms in various classical identities, because of short distance singularities. These were foundational results in field theory, instrumental for the proper understanding of anomalies.\n\nIn other notable early work, Rarita and Schwinger formulated the abstract Pauli and Fierz theory of the spin 3/2 field in a concrete form, as a vector of Dirac spinors. In order for the spin-3/2 field to interact consistently, some form of supersymmetry is required, and Schwinger later regretted that he had not followed up on this work far enough to discover supersymmetry.\n\nSchwinger discovered that neutrinos come in multiple varieties, one for the electron and one for the muon. Nowadays there are known to be three light neutrinos; the third is the partner of the tau lepton.\n\nIn the 1960s, Schwinger formulated and analyzed what is now known as the Schwinger model, quantum electrodynamics in one space and one time dimension, the first example of a confining theory. He was also the first to suggest an electroweak gauge theory, an SU(2) gauge group spontaneously broken to electromagnetic U(1) at long distances. This was extended by his student Sheldon Glashow into the accepted pattern of electroweak unification. He attempted to formulate a theory of quantum electrodynamics with point magnetic monopoles, a program which met with limited success because monopoles are strongly interacting when the quantum of charge is small.\n\nHaving supervised 73 doctoral dissertations\n, Schwinger is known as one of the most prolific graduate advisors in physics. Four of his students won Nobel prizes: Roy Glauber, Benjamin Roy Mottelson, Sheldon Glashow and Walter Kohn (in chemistry).\n\nSchwinger had a mixed relationship with his colleagues, because he always pursued independent research, different from mainstream fashion. In particular, Schwinger developed the source theory, a phenomenological theory for the physics of elementary particles, which is a predecessor of the modern effective field theory. It treats quantum fields as long-distance phenomena and uses auxiliary 'sources' that resemble currents in classical field theories. The source theory is a mathematically consistent field theory with clearly derived phenomenological results. The criticisms by his Harvard colleagues led Schwinger to leave the faculty in 1972 for UCLA. It is a story widely told that Steven Weinberg, who inherited Schwinger's paneled office in Lyman Laboratory, there found a pair of old shoes, with the implied message, \"think you can fill these?\". At UCLA, and for the rest of his career, Schwinger continued to develop the source theory and its various applications.\n\nAfter 1989 Schwinger took a keen interest in the non-mainstream research of cold fusion. He wrote eight theory papers about it. He resigned from the American Physical Society after their refusal to publish his papers. He felt that cold fusion research was being suppressed and academic freedom violated. He wrote: \"The pressure for conformity is enormous. I have experienced it in editors' rejection of submitted papers, based on venomous criticism of anonymous referees. The replacement of impartial reviewing by censorship will be the death of science.\"\n\nIn his last publications, Schwinger proposed a theory of sonoluminescence as a long distance quantum radiative phenomenon associated not with atoms, but with fast-moving surfaces in the collapsing bubble, where there are discontinuities in the dielectric constant. The mechanism of sonoluminescence now supported by experiments focuses on superheated gas inside the bubble as the source of the light.\n\nSchwinger was jointly awarded the Nobel Prize in Physics in 1965 for his work on quantum electrodynamics (QED), along with Richard Feynman and Shin'ichirō Tomonaga. Schwinger's awards and honors were numerous even before his Nobel win. They include the first Albert Einstein Award (1951), the U.S. National Medal of Science (1964), honorary D.Sc. degrees from Purdue University (1961) and Harvard University (1962), and the Nature of Light Award of the U.S. National Academy of Sciences (1949).\n\nAs a famous physicist, Schwinger was often compared to another legendary physicist of his generation, Richard Feynman. Schwinger was more formally inclined and favored symbolic manipulations in quantum field theory. He worked with local field operators, and found relations between them, and he felt that physicists should understand the algebra of local fields, no matter how paradoxical it was. By contrast, Feynman was more intuitive, believing that the physics could be extracted entirely from the Feynman diagrams, which gave a particle picture. Schwinger commented on Feynman diagrams in the following way,\n\nSchwinger disliked Feynman diagrams because he felt that they made the student focus on the particles and forget about local fields, which in his view inhibited understanding. He went so far as to ban them altogether from his class, although he understood them perfectly well. The true difference is however deeper, and it was expressed by Schwinger in the following passage, \n\nDespite sharing the Nobel Prize, Schwinger and Feynman had a different approach to quantum electrodynamics and to quantum field theory in general. Feynman used a regulator, while Schwinger was able to formally renormalize to one loop without an explicit regulator. Schwinger believed in the formalism of local fields, while Feynman had faith in the particle paths. They followed each other's work closely, and each respected the other. On Feynman's death, Schwinger described him as\n\nSchwinger died of pancreatic cancer. He is buried at Mount Auburn Cemetery; formula_1 is engraved above his name on his tombstone. These symbols refer to his calculation of the correction (\"anomalous\") to the magnetic moment of the electron.\n\nList of things named after Julian Schwinger\n\n\n\n"}
{"id": "52758243", "url": "https://en.wikipedia.org/wiki?curid=52758243", "title": "Kinetic energy metamorphosis", "text": "Kinetic energy metamorphosis\n\nKinetic energy metamorphosis (KEM) is a recently discovered tribological process of gradual crystal re-orientation and foliation of component minerals in certain rocks. It is caused by very high, localized application of kinetic energy. The required energy may be provided by prolonged battery of fluvially propelled bed load of cobbles, by glacial abrasion, tectonic deformation, and even by human action. It can result in the formation of laminae on specific metamorphic rocks that, while being chemically similar to the protolith, differ significantly in appearance and in their resistance to weathering or deformation. These tectonite layers are of whitish color and tend to survive granular or mass exfoliation much longer than the surrounding protolith.\n\nThe products of KEM were first identified in 2015 in cupules, a form of rock art consisting of spherical cap or dome-shaped depressions created by percussion with hammer-stones. KEM laminae, caused by solid state re-metamorphosis of metamorphic rock, have been observed in cupules on three rock types:\n\n\nReplication has established that cupules produced on very hard rocks, such as quartzite, require many tens of thousands of blows with hammer-stones to make. Therefore, the cumulative force applied to very small surface areas (<15 cm2) is in the order of tens of kN (kilo Newtons). In one extreme case, the KEM lamina has been developed to a thickness of c. 10 mm, but the most commonly observed thickness is about 1–2 mm. The tectonite layer is always thickest in the central part of the cupule, i.e. where the greatest amount of energy was applied.\n\nThese phenomena have since also been observed in geological contexts, generally of three types:\n\n\nKinetic energy metamorphosis products are tribological phenomena, caused by very focused, localized cumulative effect of kinetic energy on the syntaxial silica (and the voids it contains) that forms the cement of such rocks as sandstones and quartzites. The conversion to tectonite does not appear to be reversible, and the high resistance of that product to weathering processes protects the parent rock it conceals from both granular and mass exfoliation. Its susceptibility to dating techniques needs to be explored.\n"}
{"id": "12840716", "url": "https://en.wikipedia.org/wiki?curid=12840716", "title": "Lanthanum strontium manganite", "text": "Lanthanum strontium manganite\n\nLanthanum strontium manganite (LSM or LSMO) is an oxide ceramic material with the general formula LaSrMnO, where \"x\" describes the doping level.\n\nIt has a perovskite-based crystal structure, which has the general form ABO. In the crystal, the 'A' sites are occupied by lanthanum and strontium atoms, and the 'B' sites are occupied by the smaller manganese atoms. In other words, the material consists of lanthanum manganite with some of the lanthanum atoms substitutionally doped with strontium atoms. The strontium (valence 2+) doping on lanthanum (valence 3+) introduces extra holes in the valence band and thus increases electronic conductivity.\n\nLSMO has a rich electronic phase diagram, including a doping-dependent metal-insulator transition, paramagnetism and ferromagnetism. The existence of a Griffith phase has been reported as well.\n\nLSM is black in color and has a density of approximately 6.5 g/cm. The actual density will vary depending on the processing method and actual stoichiometry. LSM is primarily an electronic conductor, with transference number close to 1.\n\nThis material is commonly used as a cathode material in commercially produced solid oxide fuel cells (SOFCs) because it has a high electrical conductivity at higher temperatures, and its thermal expansion coefficient is well matched with yttria-stabilized zirconia (YSZ), a common material for SOFC electrolytes.\n\nIn research, LSM is one of the perovskite manganites that show the colossal magnetoresistance (CMR) effect, and is also an observed half-metal for compositions around \"x\"=0.3.\n\nLSM behaves like a half-metal, suggesting its possible use in spintronics. It displays a colossal magnetoresistance effect. Above its Curie temperature (about 350 K) Jahn-Teller polarons are formed; the material's ability to conduct electricity depends on the presence of the polarons.\n\n"}
{"id": "35943373", "url": "https://en.wikipedia.org/wiki?curid=35943373", "title": "LiquiGlide", "text": "LiquiGlide\n\nLiquiGlide is a platform technology which creates slippery, liquid-impregnated surfaces that was developed at the Varanasi Research Group at Massachusetts Institute of Technology by Prof. Kripa Varanasi and his team of students and post doctorals Dave Smith, Rajeev Dhiman, Adam Paxson, Brian Solomon, and Chris Love. Possible applications include improving the flow rate of condiment bottles to avoid food waste, and preventing clogs in gas and oil tubes. The inventors released videos of LiquiGlide being used in ketchup, mayonnaise, jelly, and mustard bottles made of both plastic and glass. The project came in second place in the Business Plan Contest and won the Audience Choice Award at the 2012 MIT $100K Entrepreneurship Competition.\n\nIn March 2015, LiquiGlide signed a deal with Elmer's Products, the first company to use the technology.\n\n"}
{"id": "26728680", "url": "https://en.wikipedia.org/wiki?curid=26728680", "title": "List of ecoregions in the Czech Republic", "text": "List of ecoregions in the Czech Republic\n\nAccording to the World Wide Fund for Nature, the territory of the Czech Republic can be subdivided into four ecoregions:\n\nCentral European mixed forests\nPannonian mixed forests\nWestern European broadleaf forests\nCarpathian montane conifer forests\n"}
{"id": "20749313", "url": "https://en.wikipedia.org/wiki?curid=20749313", "title": "List of hydrogen internal combustion engine vehicles", "text": "List of hydrogen internal combustion engine vehicles\n\nA hydrogen internal combustion engine vehicle (HICEV) is a vehicle powered by a hydrogen-fueled internal combustion engine. Some versions are hydrogen-gasoline hybrids.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "38557038", "url": "https://en.wikipedia.org/wiki?curid=38557038", "title": "Massangis Solar Park", "text": "Massangis Solar Park\n\nThe Massangis Solar Park is a 56 megawatt (MW) photovoltaic power station in France. It uses about 700,000 thin-film CdTe-panels made by First Solar. Commissioned in stages beginning in the spring of 2012.\n\n"}
{"id": "18526033", "url": "https://en.wikipedia.org/wiki?curid=18526033", "title": "Monocrystalline silicon", "text": "Monocrystalline silicon\n\nMonocrystalline silicon (also called \"single-crystal silicon\", \"single-crystal Si\", \"mono c-Si\", or mono-Si) is the base material for silicon chips used in virtually all electronic equipment today. Mono-Si also serves as a photovoltaic, light-absorbing material in the manufacture of solar cells.\n\nIt consists of silicon in which the crystal lattice of the entire solid is continuous, unbroken to its edges, and free of any grain boundaries. Mono-Si can be prepared as an intrinsic semiconductor that consists only of exceedingly pure silicon, or it can be doped by the addition of other elements such as boron or phosphorus to make p-type or n-type silicon. Due to its semiconducting properties, single-crystal silicon is perhaps the most important technological material of the last few decades—the \"silicon era\", because its availability at an affordable cost has been essential for the development of the electronic devices on which the present-day electronics and IT revolution is based.\n\nMonocrystalline silicon differs from other allotropic forms, such as non-crystalline amorphous silicon—used in thin-film solar cells—and polycrystalline silicon, which consists of small crystals also known as crystallites.\n\nMonocrystalline silicon is generally created by one of several methods that involve melting high-purity, semiconductor-grade silicon (only a few parts per million of impurities) and the use of a seed to initiate the formation of a continuous single crystal. This process is normally performed in an inert atmosphere, such as argon, and in an inert crucible, such as quartz, to avoid impurities that would affect the crystal uniformity.\n\nThe most common production method is the Czochralski process, which dips a precisely oriented rod-mounted seed crystal into the molten silicon. The rod is then slowly pulled upwards and rotated simultaneously, allowing the pulled material to solidify into a monocrystalline cylindrical ingot up to 2 meters in length and weighing several hundred kilograms. Magnetic fields may also be applied to control and suppress turbulent flow, further improving the uniformity of the crystallization. Other methods are float-zone growth, which passes a polycrystalline silicon rod through a radiofrequency heating coil that creates a localized molten zone, from which a seed crystal ingot grows, and Bridgman techniques, which move the crucible through a temperature gradient to cool it from the end of the container containing the seed. The solidified ingots are then sliced into thin wafers for further processing.\n\nCompared to the casting of polycrystalline ingots, the production of monocrystalline silicon is very slow and expensive. However, the demand for mono-Si continues to rise due to the superior electronic properties—the lack of grain boundaries allows better charge carrier flow and prevents electron recombination—allowing improved performance of integrated circuits and photovoltaics.\n\nThe primary application of monocrystalline silicon is as mechanical support for integrated circuits. Ingots made from the Czochralski process are sliced into wafers about 0.75 mm thick and polished to obtain a regular, flat substrate, onto which microelectronic devices are built through various microfabrication processes, such as doping or ion implantation, etching, deposition of various materials, and photolithographic patterning.\n\nA single continuous crystal is critical for electronics, since grain boundaries, impurities, and crystallographic defects can significantly impact the local electronic properties of the material, which in turn affects device performance by interfering with circuit paths. For example, without crystalline perfection, it would be virtually impossible to build very large-scale integration (VLSI) devices, in which billions of transistor-based circuits, all of which must function reliably, are combined into a single chip to form a microprocessor. As such, the electronics industry has invested heavily in facilities to produce large single crystals of silicon.\n\nMonocrystalline silicon is also used for high-performance photovoltaic (PV) devices. Since there are less stringent demands on structural imperfections compared to microelectronics applications, lower-quality solar-grade silicon (Sog-Si) is often used for solar cells. Despite this, the monocrystalline-silicon photovoltaic industry has benefitted greatly from the development of faster mono-Si production methods for the electronics industry.\n\nBeing the second most common form of PV technology, monocrystalline silicon is ranked behind only its sister, polycrystalline silicon. Due to the significantly higher production rate and steadily decreasing costs of poly-silicon, the market share of mono-Si has been decreasing: in 2013, monocrystalline solar cells had a market share of 36%, which translated into the production of 12.6 GW of photovoltaic capacity, but the market share had dropped below 25% by 2016. Despite the lowered market share, the equivalent mono-Si PV capacity produced in 2016 was 20.2 GW, indicating a significant increase in the overall production of photovoltaic technologies.\n\nWith a recorded single-junction cell lab efficiency of 26.7%, monocrystalline silicon has the highest confirmed conversion efficiency out of all commercial PV technologies, ahead of poly-Si (22.3%) and established thin-film technologies, such as CIGS cells (21.7%), CdTe cells (21.0%), and a-Si cells (10.2%). Solar module efficiencies for mono-Si—which are always lower than those of their corresponding cells—finally crossed the 20% mark for in 2012 and hit 24.4% in 2016. The high efficiency is largely attributable to the lack of recombination sites in the single crystal and better absorption of photons due to its black color, as compared to the characteristic blue hue of poly-silicon. Since they are more expensive than their polycrystalline counterparts, mono-Si cells are useful for applications where the main considerations are limitations on weight or available area, such as in spacecraft or satellites powered by solar energy, where efficiency can be further improved through combination with other technologies, such as multi-layer solar cells.\n\nBesides the low production rate, there are also concerns over wasted material in the manufacturing process. Creating space-efficient solar panels requires cutting the circular wafers (a product of the cylindrical ingots formed through the Czochralski process) into octagonal cells that can be packed closely together. The leftover material is not used to create PV cells and is either discarded or recycled by going back to ingot production for melting. Furthermore, even though mono-Si cells can absorb the majority of photons within 20 μm of the incident surface, limitations on the ingot sawing process mean commercial wafer thickness are generally around 200 μm. However, advances in technology are expected to reduce wafer thicknesses to 140 μm by 2026.\n\nOther manufacturing method are being researched, such as direct wafer epitaxial growth, which involves growing gaseous layers on reusable silicon substrates. Newer processes may allow growth of square crystals that can then be processed into thinner wafers without compromising quality or efficiency, thereby eliminating the waste from traditional ingot sawing and cutting methods.\n\n"}
{"id": "17346957", "url": "https://en.wikipedia.org/wiki?curid=17346957", "title": "Nuclear holocaust", "text": "Nuclear holocaust\n\nA nuclear holocaust, nuclear apocalypse or atomic holocaust is a theoretical scenario involving widespread destruction and radioactive fallout causing the collapse of civilization, through the use of nuclear weapons. Under such a scenario, some of the Earth is made uninhabitable by nuclear warfare in future world wars.\n\nBesides the immediate destruction of cities by nuclear blasts, the potential aftermath of a nuclear war could involve firestorms, a nuclear winter, widespread radiation sickness from fallout, and/or the temporary loss of much modern technology due to electromagnetic pulses. Some scientists, such as Alan Robock, have speculated that a thermonuclear war could result in the end of modern civilization on Earth, in part due to a long-lasting nuclear winter. In one model, the average temperature of Earth following a full thermonuclear war falls for several years by 7 to 8 degrees Celsius on average.\n\nEarly Cold War-era studies suggested that billions of humans would nonetheless survive the immediate effects of nuclear blasts and radiation following a global thermonuclear war. Some scholars argue that nuclear war could indirectly contribute to human extinction via secondary effects, including environmental consequences, societal breakdown, and economic collapse. Additionally, it has been argued that even a relatively small-scale nuclear exchange between India and Pakistan involving 100 Hiroshima yield (15 kilotons) weapons, could cause a nuclear winter and kill more than a billion people.\n\nSince 1947, the Doomsday Clock of the Bulletin of the Atomic Scientists has visualized how close the world is to a nuclear war.\n\nThe threat of a nuclear holocaust plays an important role in the popular perception of nuclear weapons. It features in the security concept of mutually assured destruction (MAD) and is a common scenario in survivalism. Nuclear holocaust is a common feature in literature and film, especially in speculative genres such as science fiction, dystopian and post-apocalyptic fiction.\n\nThe English word \"holocaust\", derived from the Greek term \"holokaustos\" meaning \"completely burnt\", refers to great destruction and loss of life, especially by fire.\n\nOne early use of the word \"holocaust\" to describe an imagined nuclear destruction appears in Reginald Glossop's 1926 novel \"The Orphan of Space\": \"Moscow ... beneath them ... a crash like a crack of Doom! The echoes of this Holocaust rumbled and rolled ... a distinct smell of sulphur ... atomic destruction.\" In the novel, an atomic weapon is planted in the office of the Soviet dictator, who, with German help and Chinese mercenaries, is preparing the takeover of Western Europe.\n\nReferences to nuclear destruction often speak of \"atomic holocaust\" or \"nuclear holocaust.” For instance, U.S. President Bush stated in August 2007: \"Iran's active pursuit of technology that could lead to nuclear weapons threatens to put a region already known for instability and violence under the shadow of a nuclear holocaust\".\n\nAs of 2016, humanity has about 15,000 nuclear weapons, thousands of which are on hair-trigger alert. While stockpiles have been on the decline following the end of the Cold War, every nuclear country is currently undergoing modernization of its nuclear arsenal. Some experts believe this modernization may increase the risk of nuclear proliferation, nuclear terrorism, and accidental nuclear war.\n\nJohn F. Kennedy estimated the probability of the Cuban Missile Crisis escalating to nuclear conflict as between 33% and 50%.\n\nIn a poll of experts at the Global Catastrophic Risk Conference in Oxford (17‐20 July 2008), the Future of Humanity Institute estimated the probability of complete human extinction by nuclear weapons at 1% within the century, the probability of 1 billion dead at 10% and the probability of 1 million dead at 30%. These results reflect the median opinions of a group of experts, rather than a probabilistic model; the actual values may be much lower or higher.\n\nScientists have argued that even a small-scale nuclear war between two countries could have devastating global consequences and such local conflicts are more likely than full-scale nuclear war.\n\nIn his book \"Reasons and Persons\", philosopher Derek Parfit posed the following question:\nCompare three outcomes:\n(2) would be worse than (1), and (3) would be worse than (2). Which is the greater of these two differences?\nHe continues that \"Most people believe that the greater difference is between (1) and (2). I believe that the difference between (2) and (3) is \"very much\" greater.\" Thus, he argues, even if it would be bad if massive numbers of humans died, human extinction would itself be much worse because it prevents the existence of all future generations. And given the magnitude of the calamity were the human race to become extinct, Nick Bostrom argues that there is an overwhelming moral imperative to reduce even small risks of human extinction.\n\nMany scholars have posited that a global thermonuclear war with Cold War-era stockpiles, or even with the current smaller stockpiles, may lead to human extinction. This position was bolstered when nuclear winter was first conceptualized and modelled in 1983. However, models from the past decade consider total extinction very unlikely, and suggest parts of the world would remain habitable. Technically the risk may not be zero, as the climactic effects of nuclear war are uncertain and could theoretically be larger than current models suggest, just as they could theoretically be smaller than current models suggest. There could also be indirect risks, such as a societal collapse following nuclear war that can make humanity much more vulnerable to other existential threats.\n\nA related area of inquiry is: if a future nuclear arms race someday leads to larger stockpiles or more dangerous nuclear weapons than existed at the height of the Cold War, at what point could a war with such weapons result in human extinction? Physicist Leo Szilard warned in the 1950s that a deliberate \"doomsday device\" could be constructed by surrounding powerful hydrogen bombs with a massive amount of cobalt. Cobalt has a half-life of five years, and its global fallout might, some physicists have posited, be able to clear out all human life via lethal radiation intensity. The main motivation for building a cobalt bomb in this scenario is its reduced expense compared with the arsenals possessed by superpowers; such a doomsday device does not need to be launched before detonation, and thus does not require expensive missile delivery systems, and the hydrogen bombs do not need to be miniaturized for delivery via missile. The system for triggering it might have to be completely automated, in order for the deterrent to be effective. A modern twist might be to also lace the bombs with aerosols designed to exacerbate nuclear winter. A major caveat is that nuclear fallout transfer between the northern and southern hemispheres is expected to be small; unless a bomb detonates in each hemisphere, the effect of a bomb detonated in one hemisphere on the other is diminished.\n\nHistorically, it has been difficult to estimate the total number of deaths resulting from a global nuclear exchange because scientists are continually discovering new effects of nuclear weapons, and also revising existing models.\n\nEarly reports considered direct effects from nuclear blast and radiation and indirect effects from economic, social, and political disruption. In a 1979 report for the U.S. Senate, the Office of Technology Assessment estimated casualties under different scenarios. For a full-scale countervalue/counterforce nuclear exchange between the U.S. and the Soviet Union, they predicted U.S. deaths from 35 to 77 percent (70 million to 160 million dead at the time), and Soviet deaths from 20 to 40 percent of the population.\n\nAlthough this report was made when nuclear stockpiles were at much higher levels than they are today, it also was made before the risk of nuclear winter was discovered in the early 1980s. Additionally, it did not consider other secondary effects, such electromagnetic pulses (EMP), and the ramifications they would have on modern technology and industry.\n\nIn the early 1980s, scientists began to consider the effects of smoke and soot arising from burning wood, plastics, and petroleum fuels in nuclear-devastated cities. It was speculated that the intense heat would carry these particulates to extremely high altitudes where they could drift for weeks and block out all but a fraction of the sun's light. A landmark 1983 study by the so-called TTAPS team (Richard P. Turco, Owen Toon, Thomas P. Ackerman, James B. Pollack and Carl Sagan) was the first to model these effects and coined the term \"nuclear winter.\"\n\nMore recent studies make use of modern global circulation models and far greater computer power than was available for the 1980s studies. A 2007 study examined consequences of a global nuclear war involving moderate to large portions of the current global arsenal. The study found cooling by about 12–20 °C in much of the core farming regions of the US, Europe, Russia and China and as much as 35 °C in parts of Russia for the first two summer growing seasons. The changes they found were also much longer lasting than previously thought, because their new model better represented entry of soot aerosols in the upper stratosphere, where precipitation does not occur, and therefore clearance was on the order of 10 years. In addition, they found that global cooling caused a weakening of the global hydrological cycle, reducing global precipitation by about 45%.\n\nThe authors did not discuss the implications for agriculture in depth, but noted that a 1986 study which assumed no food production for a year projected that \"most of the people on the planet would run out of food and starve to death by then\" and commented that their own results show that, \"This period of no food production needs to be extended by many years, making the impacts of nuclear winter even worse than previously thought.\"\n\nIn contrast to the above investigations of global nuclear conflicts, studies have shown that even small-scale, regional nuclear conflicts could disrupt the global climate for a decade or more. In a regional nuclear conflict scenario where two opposing nations in the subtropics would each use 50 Hiroshima-sized nuclear weapons (about 15 kiloton each) on major populated centres, the researchers estimated as much as five million tons of soot would be released, which would produce a cooling of several degrees over large areas of North America and Eurasia, including most of the grain-growing regions. The cooling would last for years, and according to the research, could be \"catastrophic\". Additionally, the analysis showed a 10% drop in average global precipitation, with the largest losses in the low latitudes due to failure of the monsoons.\n\nRegional nuclear conflicts could also inflict significant damage to the ozone layer. A 2008 study found that a regional nuclear weapons exchange could create a near-global ozone hole, triggering human health problems and impacting agriculture for at least a decade. This effect on the ozone would result from heat absorption by soot in the upper stratosphere, which would modify wind currents and draw in ozone-destroying nitrogen oxides. These high temperatures and nitrogen oxides would reduce ozone to the same dangerous levels we now experience below the ozone hole above Antarctica every spring.\n\nIt is difficult to estimate the number of casualties that would result from nuclear winter, but it is likely that the primary effect would be global famine (known as Nuclear Famine), wherein mass starvation occurs due to disrupted agricultural production and distribution. In a 2013 report, the International Physicians for the Prevention of Nuclear War (IPPNW) concluded that more than two billion people, about a third of the world's population, would be at risk of starvation in the event of a regional nuclear exchange between India and Pakistan, or by the use of even a small proportion of nuclear arms held by the U.S. and Russia. Several independent studies show corroborated conclusions that agricultural outputs will be significantly reduced for years by climatic changes driven by nuclear wars. Reduction of food supply will be further exacerbated by rising food prices, affecting hundreds of millions of vulnerable people, especially in the poorest nations of the world.\n\nAn electromagnetic pulse (EMP) is a burst of electromagnetic radiation. Nuclear explosions create a pulse of electromagnetic radiation called a nuclear EMP or NEMP. Such EMP interference is known to be generally disruptive or damaging to electronic equipment. If a single nuclear weapon \"designed to emit EMP were detonated 250 to 300 miles up over the middle of the country it would disable the electronics in the entire United States.\"\n\nGiven that many of the comforts and necessities we enjoy in the 21st century are predicated on electronics and their functioning, an EMP would disable hospitals, water treatment facilities, food storage facilities, and all electronic forms of communication. An EMP blast threatens the foundation which supports the existence of the modern human condition. Certain EMP attacks could lead to large loss of power for months or years. Currently, failures of the power grid are dealt with using support from the outside. In the event of an EMP attack, such support would not exist and all damaged components, devices, and electronics would need to be completely replaced.\n\nIn 2013, the US House of Representatives considered the \"Secure High-voltage Infrastructure for Electricity from Lethal Damage Act\" that would provide surge protection for some 300 large transformers around the country. The problem of protecting civilian infrastructure from electromagnetic pulse has also been intensively studied throughout the European Union, and in particular by the United Kingdom. While precautions have been taken, James Woolsey and the EMP Commission suggested that an EMP is the most significant threat to the U.S. The greatest threat to human survival in the aftermath of an EMP blast would be the inability to access clean drinking water. For comparison, in the aftermath of the 2010 Haitian earthquake, the water infrastructure had been devastated and led to at least 3,333 deaths from cholera in the first few months after the earthquake. Other countries would similarly see the resurgence of previously non-existent diseases as clean water becomes increasingly scarce.\n\nThe risk of an EMP, either through solar or atmospheric activity or enemy attack, while not dismissed, was suggested to be overblown by the news media in a commentary in \"Physics Today\". Instead, the weapons from rogue states were still too small and uncoordinated to cause a massive EMP, underground infrastructure is sufficiently protected, and there will be enough warning time from continuous solar observatories like SOHO to protect surface transformers should a devastating solar storm be detected.\n\nNuclear fallout is the residual radioactive dust and ash propelled into the upper atmosphere following a nuclear explosion. Fallout is usually limited to the immediate area, and can only spread for hundreds of miles from the explosion site if the explosion is high enough in the atmosphere. Fallout may get entrained with the products of a pyrocumulus cloud and fall as black rain (rain darkened by soot and other particulates).\n\nThis radioactive dust, usually consisting of fission products mixed with bystanding atoms that are neutron activated by exposure, is a highly dangerous kind of radioactive contamination. The main radiation hazard from fallout is due to short-lived radionuclides external to the body. While most of the particles carried by nuclear fallout decay rapidly, some radioactive particles will have half-lives of seconds to a few months. Some radioactive isotopes, like strontium 90 and cesium 137, are very long lived and will create radioactive hot spots for up to 5 years after the initial explosion. Fallout and black rain may contaminate waterways, agriculture, and soil. Contact with radioactive materials can lead to radiation poisoning through external exposure or accidental consumption. In acute doses over a short amount of time radiation will lead to prodromal syndrome, bone marrow death, central nervous system death and gastrointestinal death. Over longer periods of exposure to radiation, cancer becomes the main health risk. Long term radiation exposure can also lead to in utero effects on human development and transgenerational genetic damage.\n\nAs a result of the extensive nuclear fallout of the 1954 Castle Bravo nuclear detonation, author Nevil Shute wrote the popular novel \"On the Beach\" which was released in 1957, in this novel so much fallout is generated in a nuclear war that all human life is extinguished. However the premise that all of humanity would die following a nuclear war and only the \"cockroaches would survive\" is critically dealt with in the 1988 book \"Would the Insects Inherit the Earth and Other Subjects of Concern to Those Who Worry About Nuclear War\" by nuclear weapons expert Philip J. Dolan.\n\nIn 1982 nuclear disarmament activist Jonathan Schell published \"The Fate of the Earth\", which is regarded by many to be the first carefully argued presentation that concluded that extinction is a significant possibility from nuclear war. However, the assumptions made in this book have been thoroughly analyzed and determined to be \"quite dubious\". The impetus for Schell's work, according to physicist Brian Martin, was to argue that \"if the thought of 500 million people dying in a nuclear war is not enough to stimulate action, then the thought of extinction will. Indeed, Schell explicitly advocates use of the fear of extinction as the basis for inspiring the \"complete rearrangement of world politics\".\n\nThe belief in \"overkill\" is also commonly encountered, with an example being the following statement made by nuclear disarmament activist Philip Noel-Baker in 1971 – \"Both the US and the Soviet Union now possess nuclear stockpiles large enough to exterminate mankind three or four – some say ten – times over\". Brian Martin suggested that the origin of this belief was from \"crude linear extrapolations\", and when analyzed it has no basis in reality. Similarly, it is common to see stated that the combined explosive energy released in the entirety of World War II was about 3 megatons, while a nuclear war with warhead stockpiles at Cold War highs would release 6000 WWII's of explosive energy. An estimate for the necessary amount of fallout to begin to have the potential of causing human extinction is regarded by physicist and disarmament activist Joseph Rotblat to be 10 to 100 times the megatonnage in nuclear arsenals as they stood in 1976; however, with the world megatonnage decreasing since the Cold War ended this possibility remains hypothetical.\n\nAccording to the 1980 United Nations report \"General and Complete Disarmament: Comprehensive Study on Nuclear Weapons: Report of the Secretary-General\", it was estimated that there were a total of about 40,000 nuclear warheads in existence at that time, with a potential combined explosive yield of approximately 13,000 megatons.\n\nBy comparison, in the Timeline of volcanism on Earth when the volcano Mount Tambora erupted in 1815 – turning 1816 into the Year Without A Summer due to the levels of global dimming sulfate aerosols and ash expelled – it exploded with a force of roughly 800 to 1,000 megatons, and ejected of mostly rock/tephra, which included 120 million tonnes of sulfur dioxide as an upper estimate. A larger eruption, approximately 74,000 years ago, in Mount Toba produced of tephra, forming lake Toba, and produced an estimated of sulfur dioxide. The explosive energy of the eruption may have been as high as equivalent to 20,000,000 megatons (Mt) of TNT, while the Chicxulub impact, connected with the extinction of the dinosaurs, corresponds to at least 70,000,000 Mt of energy, which is roughly 7000 times the maximum arsenal of the US and Soviet Union.\n\nHowever, it must be noted that comparisons with supervolcanos are more misleading than helpful due to the different aerosols released, the likely air burst fuzing height of nuclear weapons and the globally scattered location of these potential nuclear detonations all being in contrast to the singular and subterranean nature of a supervolcanic eruption. Moreover, assuming the entire world stockpile of weapons were grouped together, it would be difficult due to the nuclear fratricide effect to ensure the individual weapons would detonate all at once. Nonetheless, many people believe that a full-scale nuclear war would result, through the nuclear winter effect, in the extinction of the human species, though not all analysts agree on the assumptions inputted into these nuclear winter models.\n\n\n"}
{"id": "46736435", "url": "https://en.wikipedia.org/wiki?curid=46736435", "title": "Poincaré–Miranda theorem", "text": "Poincaré–Miranda theorem\n\nIn mathematics, the Poincaré–Miranda theorem is a generalization of the intermediate value theorem, from a single function in a single dimension, to functions in dimensions. It says as follows:\n\nThe theorem is named after Henri Poincaré, who conjectured it in 1883, and Carlo Miranda, who in 1940 showed that it is equivalent to the Brouwer fixed-point theorem.\n\nThe picture on the right shows an illustration of the Poincaré–Miranda theorem for functions. Consider a couple of functions whose domain of definition is the square. The function is negative on the left boundary and positive on the right boundary (green sides of the square), while the function is negative on the lower boundary and positive on the upper boundary (red sides of the square). When we go from left to right along \"any\" path, we must go through a point in which is . Therefore, there must be a \"wall\" separating the left from the right, along which is (green curve inside the square). Similarly, there must be a \"wall\" separating the top from the bottom, along which is (red curve inside the square). These walls must intersect in a point in which both functions are (blue point inside the square).\n\nThe simplest generalization, as a matter of fact a corollary, of this theorem is the following one. \nFor every variable , let be any value in the range\nThen there is a point in the unit cube in which for all : \n\nThe this statement can be reduced to the original one by a simple translation of axes,\nwhere\n\n"}
{"id": "17672389", "url": "https://en.wikipedia.org/wiki?curid=17672389", "title": "Pontiac Hispanic History Preservation Project", "text": "Pontiac Hispanic History Preservation Project\n\nThe Pontiac Hispanic History Project is the result of a committee’s effort to gather and catalog the Latino history of the city of Pontiac, Michigan. The committee was begun by Martha Padilla and is co-chaired by her and Willie Martinez.\n\nThe project actively documents and researches the earliest Puerto Rican and Mexican immigrants that came to Pontiac in the 1940s and 1920s respectively. They maintain a growing library of newspaper clippings, personal photographs, and audio and video recordings, as well as acquisitions from the defunct Pontiac Latin Affairs Office. Plans encompass cataloging the archives of the Azteca Boxing Center, a local boxing training facility.\n\nMartha says she was prompted to begin the project after trying to organize her husband’s old papers. Hector Padilla, a native Puerto Rican, had passed on and she felt that the history contained in papers from his tenure as the first Latino city commissioner, as well as lobbying for migrant worker conditions and his efforts in forming the Hispanic Congressional Caucus, were important to the community and needed to be put somewhere.\n\nThe effort of Martha was expanded as she realized that others in the Latino community deserve recognition. Fear that the records of who has done what would be lost spurred Willie Martinez and others to join Padilla in gathering stories and history of the Pontiac Latino community at large.\n\nThe findings will be presented to the Burton Historical Collection at the Detroit Public Library on 08 Oct. 2008. Emphasis was given to making sure any findings were collected and presented to Burton Collection rather than being \"...stored in someone's garage or basement\".\n"}
{"id": "2691899", "url": "https://en.wikipedia.org/wiki?curid=2691899", "title": "Red clump", "text": "Red clump\n\nThe red clump is a clustering of red giants in the Hertzsprung–Russell diagram at around 5,000 K and absolute magnitude (M) +0.5, slightly hotter than most red-giant-branch stars of the same luminosity. It is visible as a more dense region of the red giant branch or a bulge towards hotter temperatures. It is most distinct in many, but not all, galactic open clusters, but it is also noticeable in many intermediate-age globular clusters and in nearby field stars (e.g. the Hipparcos stars).\n\nThe red clump giants are cool horizontal branch stars, stars originally similar to the Sun which have undergone a helium flash and are now fusing helium in their cores.\n\nRed clump stellar properties vary depending on their origin, most notably on the metallicity of the stars, but typically they have early K spectral types and effective temperatures around 5,000 K. The absolute visual magnitude of red clump giants near the sun has been measured at an average of +0.81 with metallicities between −0.6 and +0.4 dex.\n\nThere is a considerable spread in the properties of red clump stars even within a single population of similar stars such as an open cluster. This is partly due to the natural variation in temperatures and luminosities of horizontal branch stars when they form and as they evolve, and partly due to the presence of other stars with similar properties. Although red clump stars are generally hotter than red-giant-branch stars, the two regions overlap and the status of individual stars can only be assigned with a detailed chemical abundance study.\n\nModelling of the horizontal branch has shown that stars have a strong tendency to cluster at the cool end of the zero age horizontal branch (ZAHB). This tendency is weaker in low metallicity stars, so the red clump is usually more prominent in metal-rich clusters. However, there are other effects, and there are well-populated red clumps in some metal-poor globular clusters.\n\nStars with a similar mass to the sun evolve towards the tip of the red giant branch with a degenerate helium core. More massive stars leave the red giant branch early and perform a blue loop, but all stars with a degenerate core reach the tip with very similar core masses, temperatures, and luminosities. After the helium flash they lie along the ZAHB, all with helium cores just under and their properties determined mostly by the size of the hydrogen envelope outside the core. Lower envelope masses result in weaker hydrogen shell fusion and give hotter and slightly less luminous stars strung along the horizontal branch. Different initial masses and natural variations in mass loss rates on the red giant branch cause the variations in the envelope masses even though the helium cores are all the same size. Low-metallicity stars are more sensitive to the size of the hydrogen envelope, so with the same envelope masses they are spread further along the horizontal branch and fewer fall in the red clump.\n\nAlthough red clump stars lie consistently to the hot side of the red giant branch that they evolved from, red clump and red-giant-branch stars from different populations can overlap. This occurs in ω Centauri where metal-poor red-giant-branch stars have the same or hotter temperatures as more metal-rich red clump giants.\n\nOther stars, not strictly horizontal branch stars, can lie in the same region of the H-R diagram. Stars too massive to develop a degenerate helium core on the red giant branch will ignite helium before the tip of the red giant branch and perform a blue loop. For stars only a little more massive than the sun, around , the blue loop is very short and at a luminosity similar to the red clump giants. These stars are an order of magnitude less common than sun-like stars, even rarer compared to the sub-solar stars that can form red clump giants, and the duration of the blue loop is far less than the time spent by a red clump giant on the horizontal branch. This means that these imposters are much less common in the H-R diagram, but still detectable.\n\nStars with will also pass through the red clump as they evolve along the subgiant branch. This is again a very rapid phase of evolution, but stars such as OU Andromedae are found in the red clump region (5,500 K and ) even though it is thought to be a subgiant crossing the Hertzsprung gap.\n\nIn theory, the absolute luminosities of stars in the red clump are fairly independent of stellar composition or age so that consequently they make good standard candles for estimating astronomical distances both within our galaxy and to nearby galaxies and clusters. Variations due to metallicity, mass, age, and extinctions affect visual observations too much for them to be useful, but the effects are much smaller in the infrared. Near infrared I band observations in particular have been used to establish red clump distances. Absolute magnitudes for the red clump at solar metallicity have been measured at −0.22 in the I band and −1.54 in the K band. The distance to the galactic centre has been measured in this way, giving a result of 7.52 kpc in agreement with other methods.\n\nThe red clump should not be confused with the \"red bump\" or red-giant-branch bump, which is a less noticeable clustering of giants partway along the red giant branch, caused as stars ascending the red giant branch temporarily decrease in luminosity because of internal convection.\n\nMany of the bright \"red giants\" visible in the sky are actually early K class red-clump stars:\n\nArcturus has sometimes been thought to be a clump giant, but is now more commonly considered to be on the red giant branch, somewhat cooler and more luminous than a red-clump star.\n\n"}
{"id": "1104494", "url": "https://en.wikipedia.org/wiki?curid=1104494", "title": "Salsabil", "text": "Salsabil\n\nSalsabil ( \"\") is an Islamic Arabic term referring to a spring in paradise (Jannah). It can also mean river or fountain in paradise. The sole Qur'anic reference is in sura Al-Insaan. Salsabil can also be written as Salsabiil or even Salsabeel but \"Salsabiil\" is the most common name.\n\nThe verse may be in reference to the previous verse concerning the drink provided to those who enter paradise.\n\nSalsabil is also the name of one of the old neighborhoods in Tehran, Iran.\n"}
{"id": "42324964", "url": "https://en.wikipedia.org/wiki?curid=42324964", "title": "Shmidtivske gas field", "text": "Shmidtivske gas field\n\nThe Shmidtivske gas field natural gas field located on the continental shelf of the Black Sea. It was discovered in 1974 and developed by Chornomornaftogaz. It started commercial production in 1975. The total proven reserves of the Shmidtivske gas field are around , and production is slated to be around in 2015.\n"}
{"id": "53362118", "url": "https://en.wikipedia.org/wiki?curid=53362118", "title": "Songs about nuclear war", "text": "Songs about nuclear war\n\n"}
{"id": "8761903", "url": "https://en.wikipedia.org/wiki?curid=8761903", "title": "Strain engineering", "text": "Strain engineering\n\nStrain engineering refers to a general strategy employed in semiconductor manufacturing to enhance device performance. Performance benefits are achieved by modulating strain in the transistor channel, which enhances electron mobility (or hole mobility) and thereby conductivity through the channel.\n\nThe use of various strain engineering techniques has been reported by many prominent microprocessor manufacturers, including AMD, IBM, and Intel, primarily with regards to sub-130 nm technologies. One key consideration in using strain engineering in CMOS technologies is that PMOS and NMOS respond differently to different types of strain. Specifically, PMOS performance is best served by applying compressive strain to the channel, whereas NMOS receives benefit from tensile strain. Many approaches to strain engineering induce strain locally, allowing both n-channel and p-channel strain to be modulated independently.\n\nOne prominent approach involves the use of a strain-inducing capping layer. CVD silicon nitride is a common choice for a strained capping layer, in that the magnitude and type of strain (e.g. tensile vs compressive) may be adjusted by modulating the deposition conditions, especially temperature. Standard lithography patterning techniques can be used to selectively deposit strain-inducing capping layers, to deposit a compressive film over only the PMOS, for example.\n\nCapping layers are key to the \"Dual Stress Liner\" (DSL) approach reported by IBM-AMD. In the DSL process, standard patterning and lithography techniques are used to selectively deposit a tensile silicon nitride film over the NMOS and a compressive silicon nitride film over the PMOS.\n\nA second prominent approach involves the use of a silicon-rich solid solution, especially silicon-germanium, to modulate channel strain. One manufacturing method involves epitaxial growth of silicon on top of a relaxed silicon-germanium underlayer. Tensile strain is induced in the silicon as the lattice of the silicon layer is stretched to mimic the larger lattice constant of the underlying silicon-germanium. Conversely, compressive strain could be induced by using a solid solution with a smaller lattice constant, such as silicon-carbon. See, e.g., U.S. Patent No. 7,023,018. Another closely related method involves replacing the source and drain region of a MOSFET with silicon-germanium.\nEpitaxial strain in thin films generally arises due to lattice mismatch between the film and its substrate, and can arise either during film growth or due to thermal expansion mismatch. Tuning this epitaxial strain can be used to moderate the properties of thin films and induce phase transitions. The misfit parameter (formula_1) is given by the equation below:\n\nformula_2\n\nwhere formula_3 is the lattice parameter of the epitaxial film and formula_4 is the lattice parameter of the substrate. After some critical film thickness, it becomes energetically favorable to relieve some mismatch strain through the formation of misfit dislocations or microtwins. Misfit dislocations can be interpreted as a dangling bond at an interface between layers with different lattice constants. This critical thickness (formula_5) was computed by Mathews and Blakeslee to be:\n\nformula_6\n\nwhere formula_7 is the length of the Burgers vector, formula_8 is the Poisson ratio, formula_9 is the angle between the Burgers vector and misfit dislocation line, and formula_10 is the angle between the Burgers vector and the vector normal to the dislocation's glide plane. The equilibrium in-plane strain for a thin film with a thickness (formula_11) that exceeds formula_5 is then given by the expression:\n\nformula_13\n\nStrain relaxation at thin film interfaces via misfit dislocation nucleation and multiplication occurs in three stages which are distinguishable based on the relaxation rate. The first stage is dominated by glide of pre-existing dislocations and is characterized by a slow relaxation rate. The second stage has a faster relaxation rate, which depends on the mechanisms for dislocation nucleation in the material. Finally, the last stage represents a saturation in strain relaxation due to strain hardening.\n\nStrain engineering has been well-studied in complex oxide systems, in which epitaxial strain can strongly influence the coupling between the spin, charge, and orbital degrees of freedom, and thereby impact the electrical and magnetic properties. Epitaxial strain has been shown to induce metal-insulator transitions and shift the Curie temperature for the antiferromagnetic-to-ferromagnetic transition in <chem>La_{1-x}Sr_{x}MnO_{3}</chem>. In alloy thin films, epitaxial strain has been observed to impact the spinodal instability, and therefore impact the driving force for phase separation. This is explained as a coupling between the imposed epitaxial strain and the system’s composition-dependent elastic properties. Researchers recently achieved very large strain in thick oxide films by incorporating nanowires/nanopillars in film matrix. Additionally, in two dimensional materials such as strain has been shown to induce conversion from an indirect semiconductor to a direct semiconductor allowing a hundred-fold increase in the light emission rate.\n\nBi-axial strain has been used to reduce the switching energy in interfacial phase change memory (iPCM) materials. Phase change memory materials have been commercially used in non-volatile memory cells. Interfacial phase change materials are a superlattice of Sb2Te3 and GeTe. The average superlattice composition can be Ge2Sb2Te5, which is a well studied phase change alloy. There is a large change in the materials electrical resistance when atoms at the interface diffussively disorder. In contrast to the Ge2Sb2Te5 alloy, which needs to amorphise to switch, the strained iPCM materials partially disorder at the interface. When the GeTe layers are bi-axially strained, there is more room for atomic transitions and the activation energy for switching is lowered. And when these materials are included in phase change memory devices, the switching energy lowered, the switching voltage is lowered, and the switching time is shortened. In short strain considerably improves the memory cell performance.\n\nStrained silicon\n"}
{"id": "27381487", "url": "https://en.wikipedia.org/wiki?curid=27381487", "title": "Subansiri Lower Dam", "text": "Subansiri Lower Dam\n\nThe Subansiri Lower Dam, officially named Subansiri Lower Hydroelectric Project (SLHEP), is an under construction gravity dam on the Subansiri River in NorthEastern India. It is located upstream of Gerukamukh village in Dhemaji District and Lower Subansiri District on the border of Assam and Arunachal Pradesh. Described as a run-of-the-river project by NHPC Limited, the Project is expected to supply 2,000 MW of power when completed. The project has experienced several problems during construction to include landslides, re-design and opposition. It was expected to be complete in 2018. It is notable that, if completed as planned, it will be the largest hydroelectric project in India.\n\nThe concrete gravity dam is designed to be tall, measured from the river bed and from foundation. Its length will be and the dam will have a structural volume of . The reservoir created by the dam will have a gross storage capacity of , of which can be used for power generation or irrigation. At normal level, the reservoir's surface will cover . The surface powerhouse, located on the left bank, will contain eight 250 MW Francis turbine generators.\n\nThere will be eight horse shoe shaped head race tunnels, each being in diameter and having a length from . There will be eight horse Shoe shaped surge tunnels, each being in diameter and having length from . There will be eight horse shoe/circular shaped penstocks with varying diameters of and lengths of. The tail race channel, which will transfer water discharged by the turbines back to the river, is wide and long.\n\nConstruction of Subansiri Lower Project involves many challenges. These include land not being available when construction was scheduled to commence, a limited annual construction time because of monsoons (from mid-April to mid-October), the need to handle high flood flows and poor rock conditions. The design of the dam has undergone drastic and repeated revisions that have affected the schedule and planning of the construction work.\n\nIn December 2003 the contract to build the dam and its associated structures was awarded to a consortium of Boguchandgesstroy, Soyuzgidrosptsstry and Soma Enterprise Ltd. Due to difficulties acquiring land around the site, construction could not begin in earnest until 13 months after the contract was awarded. Unexpected geological conditions at the dam site led to landslides and slower tunnel excavation. By November 2007, the river was successfully diverted and in April of the next year, the foundation was clear for construction. Before the foundation was fully prepared it was discovered that bedrock was reached sooner than expected. This led to an alteration in the dam's design for stability. While the dam was being re-designed, concrete was placed over the foundation to protect it from the upcoming monsoon floods as the cofferdams stood a good chance of not protecting the foundation from the strong floods. The re-design was completed in October 2008 and soon after the foundation was once again cleared. In May 2009, work was suspended because of the monsoon season and re-commenced in November of that year.\n\nAs of November 2011, the dam reached an elevation of , just below the spillway elevation of . On 16 December 2011, construction equipment was halted by protests.\n\nThe construction cost has gone up by about 1,200 crore owing to forced suspension of work since December 2011. NHPC has already spent about 6,600 crore, according to a status report prepared by the company.\n\nCommissioning of the first generator is loosely expected to occur in 2016 with the final generator in 2018.\n\nSome environmental impacts unique to very large dams will result from completion of the Subansiri Project, both upstream and downstream of the dam site. These impacts will include ecosystem damage and loss of land.\n\nThe reservoir of the Subansiri Project will submerge a length of the Subansiri river and occupy which includes Himalayan subtropical pine forests, Himalayan subtropical broadleaf forests, part of the Tale Valley Wildlife Sanctuary, an elephant corridor and some subsistence agriculture fields.\n\nThirty eight families will be displaced if the dam is completed, according to official data.\nWater flow downstream will be regulated by the dam which is expected to result in low releases (6 m/s) during winter and very high releases (2,560 m/s) when energy is being generated.\n\nThe project has met stiff resistance from several groups including All Assam Students’ Union and the Krishak Mukti Sangram Samiti, who are apprehensive about safety and the project’s downstream impact.\n"}
{"id": "190919", "url": "https://en.wikipedia.org/wiki?curid=190919", "title": "Sunrise", "text": "Sunrise\n\nSunrise or sun up is the moment when the upper limb of the Sun appears on the horizon in the morning. The term can also refer to the entire process of the solar disk crossing the horizon and its accompanying atmospheric effects.\n\nAlthough the Sun appears to \"rise\" from the horizon, it is actually the \"Earth's\" motion that causes the Sun to appear. The illusion of a moving Sun results from Earth observers being in a rotating reference frame; this apparent motion is so convincing that most cultures had mythologies and religions built around the geocentric model, which prevailed until astronomer Nicolaus Copernicus first formulated the heliocentric model in the 16th century.\n\nArchitect Buckminster Fuller proposed the terms \"sunsight\" and \"sunclipse\" to better represent the heliocentric model, though the terms have not entered into common language.\n\nAstronomically, sunrise occurs for only an instant: the moment at which the upper limb of the Sun appears tangent to the horizon. However, the term \"sunrise\" commonly refers to periods of time both before and after this point:\n\nSunrise actually occurs \"before\" the Sun truly reaches the horizon because Earth's atmosphere refracts the Sun's image. At the horizon, the average amount of refraction is 34 arcminutes, though this amount varies based on atmospheric conditions.\n\nAlso, unlike most other solar measurements, sunrise occurs when the Sun's \"upper limb\", rather than its center, appears to cross the horizon. The apparent radius of the Sun at the horizon is 16 arcminutes.\n\nThese two angles combine to define sunrise to occur when the Sun's center is 50 arcminutes below the horizon, or 90.83° from the zenith.\n\nThe timing of sunrise varies throughout the year and is also affected by the viewer's longitude and latitude, altitude, and time zone. These changes are driven by the axial tilt of Earth, daily rotation of the Earth, the planet's movement in its annual elliptical orbit around the Sun, and the Earth and Moon's paired revolutions around each other. The analemma can be used to make approximate predictions of the time of sunrise.\n\nIn late winter and spring, sunrise as seen from temperate latitudes occurs earlier each day, reaching its earliest time near the summer solstice; although the exact date varies by latitude. After this point, the time of sunrise gets later each day, reaching its latest sometime around the winter solstice. The offset between the dates of the solstice and the earliest or latest sunrise time is caused by the eccentricity of Earth's orbit and the tilt of its axis, and is described by the analemma, which can be used to predict the dates.\n\nVariations in atmospheric refraction can alter the time of sunrise by changing its apparent position. Near the poles, the time-of-day variation is exaggerated, since the Sun crosses the horizon at a very shallow angle and thus rises more slowly.\n\nAccounting for atmospheric refraction and measuring from the leading edge slightly increases the average duration of day relative to night. The sunrise equation, however, which is used to derive the time of sunrise and sunset, uses the Sun's physical center for calculation, neglecting atmospheric refraction and the non-zero angle subtended by the solar disc.\n\nNeglecting the effects of refraction and the Sun's non-zero size, whenever sunrise occurs, in temperate regions it is always in the northeast quadrant from the March equinox to the September equinox and in the southeast quadrant from the September equinox to the March equinox. Sunrises occur approximately due east on the March and September equinoxes for all viewers on Earth. Exact calculations of the azimuths of sunrise on other dates are complex, but they can be estimated with reasonable accuracy by using the analemma.\nAir molecules and airborne particles scatter white sunlight as it passes through the Earth's atmosphere. This is done by a combination of Rayleigh scattering and Mie scattering.\n\nAs a ray of white sunlight travels through the atmosphere to an observer, some of the colors are scattered out of the beam by air molecules and airborne particles, changing the final color of the beam the viewer sees.\nBecause the shorter wavelength components, such as blue and green, scatter more strongly,\nthese colors are preferentially removed from the beam.\n\nAt sunrise and sunset, when the path through the atmosphere is longer, the blue and green components are removed almost completely leaving the longer wavelength orange and red hues seen at those times. The remaining reddened sunlight can then be scattered by cloud droplets and other relatively large particles to light up the horizon red and orange. The removal of the shorter wavelengths of light is due to Rayleigh scattering by air molecules and particles much smaller than the wavelength of visible light (less than 50 nm in diameter). The scattering by cloud droplets and other particles with diameters comparable to or larger than the sunlight's wavelengths (more than 600 nm) is due to Mie scattering and is not strongly wavelength-dependent. Mie scattering is responsible for the light scattered by clouds, and also for the daytime halo of white light around the Sun (forward scattering of white light).\n\nSunset colors are typically more brilliant than sunrise colors, because the evening air contains more particles than morning air.\nAsh from volcanic eruptions, trapped within the troposphere, tends to mute sunset and sunrise colors, while volcanic ejecta that is instead lofted into the stratosphere (as thin clouds of tiny sulfuric acid droplets), can yield beautiful post-sunset colors called afterglows and pre-sunrise glows. A number of eruptions, including those of Mount Pinatubo in 1991 and Krakatoa in 1883, have produced sufficiently high stratospheric sulfuric acid clouds to yield remarkable sunset afterglows (and pre-sunrise glows) around the world. The high altitude clouds serve to reflect strongly reddened sunlight still striking the stratosphere after sunset, down to the surface.\n\n"}
{"id": "50899185", "url": "https://en.wikipedia.org/wiki?curid=50899185", "title": "Tornadoes of 1965", "text": "Tornadoes of 1965\n\nThis page documents the tornadoes and tornado outbreaks of 1965, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes.\n"}
{"id": "2337355", "url": "https://en.wikipedia.org/wiki?curid=2337355", "title": "Trigatron", "text": "Trigatron\n\nA trigatron is a type of triggerable spark gap switch designed for high current and high voltage, (usually 10-100 kV and 20-100 kA, though devices in the mega-ampere range exist as well). It has very simple construction and in many cases is the lowest cost high energy switching option. It may operate in open air, it may be sealed, or it may be filled with a dielectric gas other than air or a liquid dielectric. The dielectric gas may be pressurized, or a liquid dielectric (e.g. mineral oil) may be substituted to further extend the operating voltage. Trigatrons may be rated for repeated use (over 10,000 switching cycles), or they may be single-shot, destroyed in a single use.\n\nA trigatron has three electrodes. The heavy main electrodes are for the high current switching path, and a smaller third electrode serves as the trigger. During normal operation, the voltage between the main electrodes is somewhat lower than the breakdown voltage corresponding to their distance and the dielectric between them (usually air, argon-oxygen, nitrogen, hydrogen, or sulfur hexafluoride). To switch the device, a high voltage pulse is delivered to the triggering electrode. This ionizes the medium between it and one of the main electrodes, creating a spark which shortens the thickness of non-ionized medium between the electrodes. The triggering spark also generates ultraviolet light and free electrons in the main gap. These lead to the rapid electrical breakdown of the main gap, culminating in a low resistance electric arc between the main electrodes. The arc will continue to conduct until current flow drops sufficiently to extinguish it.\n\nThe triggering electrode is most often mounted through a hole in the center of the positive main electrode. The undrilled main electrode is the negative electrode. When switching high currents, the electrodes undergo considerable heat stress, as they are directly involved in the electric arc. This causes the surfaces to undergo gradual vaporization, so some designs incorporate methods to easily adjust the distance between the electrodes or to actually replace the electrodes. The main electrodes are typically fabricated from brass, or alloys of copper and tungsten for longer electrode life. \n\nGlass trigatrons are often enclosed in a woven wire mesh, to provide protection from fragmentation if the device explodes due to internal overpressure.\n\nTrigatrons find many different uses in pulsed power applications. For example, they were used in early radar modulators to feed the high-power pulses into the magnetrons, for use with slapper detonators, or for triggering a Marx generator.\n\n\n"}
{"id": "44818034", "url": "https://en.wikipedia.org/wiki?curid=44818034", "title": "Truss (unit)", "text": "Truss (unit)\n\nA truss is a tight bundle of hay or straw. It would usually be cuboid, for storage or shipping, and would either be harvested into such bundles or cut from a large rick.\n\nHay and straw were important commodities in the pre-industrial era. Hay was required as fodder for animals, especially horses, and straw was used for a variety of purposes including bedding. In London, there were established markets for hay at Smithfield, Whitechapel and by the village of Charing, which is still now called the Haymarket. The weight of trusses was regulated by law and statutes were passed in the reigns of William III and Mary II, George II and George III. The latter act of 1796 established the weights as follows:In summary then, the standard weights of a truss were:\n\n\nand 36 trusses made up a load.\n\nA detailed description was provided in \"British Husbandry\", sponsored by the Society for the Diffusion of Useful Knowledge,\n\nThe London hay-cart may have been purpose-made to carry a load of 36 trusses. John French Burke wrote in 1834,\n\nBritish army regulations in 1799 specified standard rations of trusses. These were one truss of straw for each two soldiers, to stuff their palliasses. Half a truss was provided after sixteen days to refresh this and the whole was then changed after 32 days. Five trusses of straw were provided for each company every sixteen days for the batmen and washerwomen, who did not have palliasses. Thirty trusses of straw were provided per company when they took the field to thatch the huts of the washerwomen.\n"}
{"id": "21894711", "url": "https://en.wikipedia.org/wiki?curid=21894711", "title": "Vertical penetration", "text": "Vertical penetration\n\nVertical penetration is a scalar measurement of distance, of the maximum altitude an object, most often an aircraft can gain at any particular moment in time, thereby converting all of its energy from kinetic to gravitational potential.\n\nIt is worth noting that when listing an aircraft's technical specifications, \"vertical penetration\" can be appropriately used in substitution of \"maximum vertical penetration\".\n\nAssume an aircraft has the same mechanical energy at two separate points in its flight - one when the aircraft is straight and level, holding a constant airspeed, and negligible altitude (\"near\" zero potential energy) and the other, following a sudden 90 degree pitch increase in attitude, when the aircraft has noticeable altitude, and negligible airspeed (zero kinetic energy). Assuming the aircraft can 100% efficiently convert all of its kinetic energy to potential (theoretically have a radius of 0 in the turn, refer to centripetal force), the aircraft has the same mechanical energy in both points, the kinetic energy of the first point equaling the potential energy of the second:\n\nformula_1\n\nwhere \"m\" is the mass, \"v\" is the speed, \"h\" is the height of the body, and \"g\" is standard gravity. In SI units (used for most modern scientific work), mass is measured in kilograms, speed in metres per second, height is in metres, standard gravity in Metre per second squared, and the resulting energy is in joules.\n\nNotice that neither mass nor standard gravity were given subscripts indicating which point they correspond to. This is because both are assumed constant. This is mildly incorrect for both. As an aircraft operates, it consumes fuel, oil, etc., which slightly decreases mass. And since the standard gravity is inversely proportional to the distance between the body and Earth, and the aircraft is gaining altitude (increasing said distance), slightly decreasing standard gravity.\n\nBy dividing by standard gravity and mass, and rewriting formula_2 as formula_3, as formula_4 shows that:\n\nformula_5\n\nby canceling out mass, and rewriting formula_6 as simply formula_7 as point two has no velocity, and moving the formula_8 to the denominator, the formula for vertical penetration has yielded:\n\nformula_9\n"}
{"id": "40388671", "url": "https://en.wikipedia.org/wiki?curid=40388671", "title": "Viriome", "text": "Viriome\n\nThe viriome of a habitat or environment is the total virus content within it. A viriome may relate to the viruses that inhabit a multicellular organism as well as the phages that are residing inside bacteria and archaea.\n\nThis term exists in contrast to the virome, which more commonly refers to the collection of nucleic acids contained by viruses in a microbiome. \n\n"}
{"id": "157946", "url": "https://en.wikipedia.org/wiki?curid=157946", "title": "Winter of Discontent", "text": "Winter of Discontent\n\nThe Winter of Discontent was the winter of 1978–79 in the United Kingdom, during which there were widespread strikes by public sector trade unions demanding larger pay rises, following the ongoing pay caps of the Labour Party government led by James Callaghan against Trades Union Congress opposition to control inflation, during the coldest winter for 16 years.\n\nThe strikes were a result of the Labour government's attempt to control inflation by a forced departure from their social contract with the unions by imposing rules on the public sector that pay rises be kept below 5%, to control inflation in itself and as an example to the private sector. However, some employees' unions conducted their negotiations within mutually agreed limits above this limit with employers. While the strikes were largely over by February 1979, the government's inability to contain the strikes earlier helped lead to Margaret Thatcher's Conservative victory in the 1979 general election and legislation to restrict unions. Public sector employee strike actions included an unofficial strike by gravediggers working in Liverpool and Tameside, and strikes by refuse collectors. Additionally, NHS ancillary workers formed picket lines to blockade hospital entrances with the result that many hospitals were reduced to taking emergency patients only.\n\nThe phrase \"Winter of Discontent\" is from the opening line of William Shakespeare's \"Richard III\": \"Now is the winter of our discontent / Made glorious summer by this sun [or son] of York\", and was first applied to the events of the winter by Robin Chater, a writer at \"Incomes Data Report\". It was subsequently used in a speech by James Callaghan and translated to define a crisis by tabloids – including \"The Sun\".\n\nThe weather turned very cold in the early months of 1979 with blizzards and deep snow, the coldest since 1962–63, rendering some jobs impossible, reducing retail spending and worsening the economy.\n\nIn 1969, Labour politician James Callaghan led a cabinet revolt which led to the abandonment of a proposed reform of trade union law outlined in a Barbara Castle white paper called \"In Place of Strife\"; had Castle's white paper been implemented, most of the action during the Winter of Discontent would have been illegal.\n\nBritain's economy during the 1970s was so weak that Callaghan warned his fellow Cabinet members in 1974 of the possibility of \"a breakdown of democracy\", telling them that \"If I were a young man, I would emigrate.\" The Labour governments of Harold Wilson and Callaghan continued a fight begun in 1972 against inflation upon election in February 1974. Inflation had peaked at 26.9% in the 12 months to August 1975, but while demonstrating to markets fiscal responsibility they wished to avoid large increases in unemployment. As part of the campaign to bring down inflation, the government had agreed a 'social contract' with the Trades Union Congress which allowed for a voluntary incomes policy in which the pay rises for workers were held down to limits set by the government. Previous governments had brought in incomes policies backed by Acts of Parliament, but the social contract agreed that this would not happen.\n\nPhase I of the pay policy was announced on 11 July 1975 with a white paper entitled \"The Attack on Inflation\". This proposed a limit on wage rises of £6 per week for all earning below £8,500 annually. The TUC General Council had accepted these proposals by 19 votes to 13. On 5 May 1976 the TUC accepted a new policy for 1976 increases, beginning 1 August, of between £2.50 and £4 per week with further years outlined. At the Annual Congress on 8 September 1976 the TUC rejected a motion which called for a return to free collective bargaining (which meant no incomes policy at all) once Phase I expired on 1 August 1977. This new policy was Phase II of the incomes policy.\n\nOn 15 July 1977, the Chancellor of the Exchequer Denis Healey announced Phase III of the incomes policy in which there was to be a phased return to free collective bargaining, without \"a free-for-all.\" After prolonged negotiations, the TUC agreed to continue with the modest increases recommended for 1977–78 under Phase II limits and not to try to reopen pay agreements made under the previous policy, while the Government agreed not to intervene in pay negotiations. The Conservative Party criticised the power of the unions and lack of any stronger policy to cover the period from the summer of 1978. The inflation rate continued to fall through 1977 and by 1978 the annual rate fell below 10%.\n\nHaving prepared for the imminent end of the incomes policy, global inflation supervened and was coming towards record levels during the 1978–82 period on 21 July 1978 Denis Healey introduced a new White Paper which set a guideline for pay rises of 5% in the year from 1 August. The TUC voted overwhelmingly on 26 July to reject the limit and insist on a return to free collective bargaining as they were promised. Unexpectedly, on 7 September, Prime Minister James Callaghan announced that he would not be calling a general election that autumn but seeking to go through the winter with continued pay restraint so that the economy would be in a better state in preparation for a spring election. The pay limit was officially termed 'Phase IV' but most referred to it as 'the 5% limit.' Although the government did not make the 5% limit a legal requirement, it decided to impose penalties on private and public government contractors who broke the limit.\n\nAlthough not an official guideline, the pay rise set by Ford of Britain was accepted throughout private industry as a benchmark for negotiations. Ford had enjoyed a good year, and could afford to offer a large pay rise to its workers. The company was, however, also a major government contractor. Ford's management therefore made a pay offer within the 5% guidelines. In response, 15,000 Ford workers, mostly from the Transport and General Workers Union (TGWU), began an unofficial strike on 22 September, which subsequently became an official TGWU action on 5 October. The number of participants grew to 57,000.\n\nDuring the strike, Vauxhall Motors employees accepted an 8.5% rise. After long negotiation in which they weighed the chances of suffering from government sanctions against the continued damage of the strike, Ford eventually revised their offer to 17% and decided to accept the sanctions; Ford workers accepted the rise on 22 November.\n\nAs the Ford strike was starting, the Labour Party conference began at Blackpool. Terry Duffy, the delegate from Liverpool Wavertree Constituency Labour Party and a supporter of the Militant group, moved a motion on 2 October which demanded \"that the Government immediately cease intervening in wage negotiations\". Despite a plea from Michael Foot not to put the motion to the vote, the resolution was carried by 4,017,000 to 1,924,000. The next day, the Prime Minister accepted the fact of defeat by saying \"I think it was a lesson in democracy yesterday\", but insisted that he would not let up on the fight against inflation.\n\nMeanwhile, the government's situation in the House of Commons was increasingly difficult; through by-elections it had lost its majority of 3 in 1976 and had been forced to put together a pact with the Liberal Party in 1977 in order to keep winning votes on legislation; the pact lapsed in July 1978. A decision to grant extra Parliamentary seats to Northern Ireland afforded temporary support from the Ulster Unionist Party, but the Unionists were clear that this support would be withdrawn immediately after the Bill to grant extra seats had been passed – it was through the Ulster Unionists agreeing to abstain that the government defeated a motion of no confidence by 312 to 300 on 9 November.\n\nBy the middle of November it was clear that Ford would offer an increase substantially over the 5% limit. The government subsequently entered into intense negotiations with the TUC, hoping to produce an agreement on pay policy that would prevent disputes and show political unity in the run-up to the general election. A limited and weak formula was eventually worked out and put to the General Council of the TUC on 14 November, but its General Council vote was tied 14–14, with the formula being rejected on the Chair's casting vote. One important personality on the TUC General Council had changed earlier in 1978 with Moss Evans replacing Jack Jones at the TGWU. Evans proved a weak leader of his union, although it is doubtful whether Jones could have restrained the actions of some of the TGWU shop stewards.\n\nAfter Ford settled, the government announced on 28 November that sanctions would be imposed on Ford, along with 220 other companies, for breach of the pay policy. The announcement of actual sanctions produced an immediate protest from the Confederation of British Industry which announced that it would challenge their legality. The Conservatives put down a motion in the House of Commons to revoke the sanctions. A co-ordinated protest by left-wing Labour MPs over spending on defence forced the debate set for 7 December to be postponed; however on 13 December an anti-sanctions amendment was passed by 285 to 279. The substantive motion as amended was then passed by 285 to 283. James Callaghan put down a further motion of confidence for the next day, which the government won by 10 votes (300 to 290), but accepted that his government could not use sanctions. In effect this deprived the government of any means of enforcing the 5% limit on private industry.\n\nWith the government having no way of enforcing its pay policy, unions which had not yet put in pay claims began to increase their aim. The first to take extreme action were lorry drivers, members of the TGWU. Large numbers of the lorry drivers worked on oil tankers, and drivers working for BP and Esso began an overtime ban in support of rises of up to 40% on 18 December. With supplies of oil being disrupted, the Cabinet Office prepared 'Operation Drumstick', by which the Army were put on standby to take over from the tanker drivers. However, the Operation would need the declaration of a state of emergency in order to allow conscription of the assets of the oil companies, and the government drew back from such a step. Before the situation developed into a crisis the oil companies settled on wage rises of around 15%.\n\nFrom 3 January 1979 an unofficial strike of all TGWU lorry drivers began. With petrol distribution held up, petrol stations closed across the country. The strikers also picketed the main ports. The strikes were made official on 11 January by the TGWU and 12 January by the United Road Transport Union. With 80% of the nation's goods transported by road, essential supplies were put in danger as striking drivers picketed those firms that continued to work. While the oil tanker drivers were working, the main refineries were also targeted and the tanker drivers let the strikers know where they were going, allowing for flying pickets to turn them back at their destination. More than 1,000,000 UK workers were laid off temporarily during the disputes.\nA further plan was drawn up to call a state of emergency and safeguard essential supplies through the Army, regarding which the government warned the TGWU leadership, which resulted in the union accepting (12 January 1979) a list of emergency supplies which were officially exempt from action. In practice, what counted as an emergency was left up to local officials of the TGWU to determine, and practice across the country varied according to the views of the local shop stewards who established 'dispensation committees' to decide. When strikers in Hull did not allow the correct mix of animal feed through to local farms, the farmers dropped the bodies of dead piglets and chickens outside the union offices; the union contended that the farmer had actually wrung the chicken's necks to kill them, and the piglets had been killed when the sow rolled over and crushed them.\n\nOn 29 January, lorry drivers in the south west accepted a deal awarded by an Arbitration Panel of a rise of up to 20%, just £1 per week less than the union had been striking for; this settlement proved a model which was accepted throughout the country.\n\nOn 10 January, James Callaghan arrived back from a summit in Guadeloupe in the middle of the lorry drivers' strike. Having been tipped off that the press were present, his press secretary Tom McCaffrey advised him to say nothing and return immediately to work, but his political adviser Tom McNally thought that the image of Callaghan returning and declaring his intent to take control of the situation would be reassuring. Callaghan therefore decided to give a press conference at Heathrow Airport. To McNally's dismay Callaghan was jocular and referred to having had a swim in the Caribbean during the summit. He was then asked (by a reporter from the \"Evening Standard\") \"What is your general approach, in view of the mounting chaos in the country at the moment?\" and replied:\n\nThe next day's edition of \"The Sun\" featured the famous headline \"Crisis? What crisis?\" with a subheading \"Rail, lorry, jobs chaos – and Jim blames Press\", condemning Callaghan as being \"out of touch\" with British society.\n\n22 January 1979 was the biggest individual day of strike action since the General Strike of 1926, and many workers stayed out indefinitely afterwards. With many in the private sector having achieved substantial rises, the public sector unions became increasingly concerned to keep pace in terms of pay. The government had already announced a slight weakening of the policy on 16 January, which gave the unions cause for hope that they might win and use free collective bargaining. Train drivers belonging to ASLEF and the National Union of Railwaymen had already begun a series of 24-hour strikes, and the Royal College of Nursing conference on 18 January decided to ask that the pay of nurses be increased to the same level in real terms as 1974, which would mean a 25% average rise. The public sector unions labelled the date the \"Day of Action\", in which they held a 24-hour strike and marched to demand a £60 per week minimum wage.\n\nWith the succession of strikes having been called and then won, many groups of workers began to take unofficial action – often without the consent or support of the union leaderships. Ambulance drivers began to take strike action in mid-January, and in parts of the country (London, West Midlands, Cardiff, Glasgow and the west of Scotland) their action included refusing to attend 999 emergency calls. In these areas, the Army was drafted in to provide a skeleton service. Ancillary hospital staff also went on strike. On 30 January, the Secretary of State for Social Services David Ennals announced that 1,100 of 2,300 NHS hospitals were only treating emergencies, that practically no ambulance service was operating normally, and that the ancillary health service workers were deciding which cases merited treatment. The media reported with scorn that cancer patients were being prevented from getting essential treatment.\n\nA notorious industrial action during the winter, and one which was later frequently referred to by Conservative politicians, was the strike by gravediggers, members of the GMWU in Liverpool and in Tameside near Manchester. Eighty gravediggers being on strike, Liverpool City Council hired a factory in Speke to store the corpses until they could be buried. The Department of Environment noted that there were 150 bodies stored at the factory at one point, with 25 more added every day. The reports of unburied bodies caused concern with the public. On 1 February a persistent journalist asked the Medical Officer of Health for Liverpool, Dr Duncan Bolton, what would be done if the strike continued for months, Bolton speculated that burial at sea would be considered. Although his response was hypothetical, in the circumstances it caused great alarm. Other alternatives were considered, including allowing the bereaved to dig their own funeral's graves, deploying troops, and engaging private contractors to inter the bodies. The main concerns were said to be aesthetic because bodies could be safely stored in heat-sealed bags for up to six weeks. Bolton later reported being 'horrified' by the sensationalised reportage of the strike in the mass media. The gravediggers eventually settled for a 14% rise after a fortnight's strike.\n\nWith many collectors having been on strike since 22 January, local authorities began to run out of space for storing waste and used local parks under their control. The Conservative controlled Westminster City Council used Leicester Square in the heart of London's West End for piles of rubbish and, as the \"Evening Standard\" reported, this attracted rats.\n\nOn 21 February, a settlement of the local authority workers' dispute was agreed, whereby workers got an 11% rise, plus £1 per week, with the possibility of extra rises, should a pay comparability study recommend them. Some left-wing local authorities, among them the London Borough of Camden, conceded the union demands in full (known as the 'Camden surplus') and then saw an investigation by the District Auditor, which eventually ruled it a breach of fiduciary duty and therefore illegal. Camden Borough councillors, among them Ken Livingstone, avoided surcharge. Livingstone was Leader of the Greater London Council at the time the decision not to impose a surcharge was made.\n\nPrior to the \"Winter of Discontent\", the Callaghan government had sought (in 1976) an International Monetary Fund loan of £2.3bn to combat the rampant inflation at that time. Although the media reported this as a humiliation for a former imperial power working to rebuild its economy after World War II and the nadir of the post-war economic period, this in itself led to only a narrow lead for Margaret Thatcher's party in October 1978 in opinion polls.\n\nStrikes by essential services dismayed many senior ministers in the Labour government who had been close to the trade union movement, who had thought it unlikely that trade unionists would take such action. Among these was Prime Minister James Callaghan himself, who had built his political career on his connection to the trade unions, and had practically founded one, the Inland Revenue Staff Federation.\n\nThe government was negotiating with the senior union leaders and on 11 February came to agreement on a proposal to be put to the TUC General Council. On 14 February the General Council agreed the concordat, published under the title 'The Economy, the Government, and Trade Union Responsibilities'. By this stage union executives had limited control over their members and strikes did not immediately cease, although they began to wind down from this point. In total in 1979, 29,474,000 working days were lost in industrial disputes, compared with 9,306,000 in 1978.\n\nIn the summer before the Winter of Discontent, the minority Labour government's fortunes in the opinion polls had been improving and suggested that they could gain an overall majority in the event of a general election being held. However, on 7 September 1978, Callaghan announced that no general election would be held that year. Callaghan's failure to call an election would ultimately prove to be a costly mistake for his government.\nThe strikes appeared to have a profound effect on voting intention. According to Gallup, Labour had a lead of 5 percentage points over the Conservatives in November 1978, which turned to a Conservative lead of 7.5 percentage points in January 1979, and of 20 percentage points in February. On 1 March, referendums on devolution to Scotland and Wales were held. That in Wales went strongly against devolution; that in Scotland produced a small majority in favour which did not reach the threshold set by Parliament of 40% of the electorate. The government's decision not to press ahead with devolution immediately led the Scottish National Party to withdraw support from the government and on 28 March in a motion of no confidence the government lost by one vote, precipitating a general election.\n\nConservative Party leader Margaret Thatcher had already outlined her proposals for restricting trade union power in a party political broadcast on 17 January in the middle of the lorry drivers' strike. During the election campaign the Conservative Party made extensive use of the disruption caused during the strike. One broadcast on 23 April began with the Sun's headline \"Crisis? What Crisis?\" being shown and read out by an increasingly desperate voiceover interspersed with film footage of piles of rubbish, closed factories, picketed hospitals and locked graveyards. The scale of the Conservatives' victory in the general election has often been ascribed to the effect of the strikes, as well as their Labour Isn't Working campaign, and the party used film of the events of the winter in election campaigns for years to come.\n\nFollowing Thatcher's election win, she brought the post-war consensus to a halt and made drastic changes to trade union laws (most notably the regulation that unions had to hold a ballot among members before calling strikes) and as a result strikes were at their lowest level for 30 years by the time of the 1983 general election, which the Conservatives won by a landslide.\n\n\n\n\n"}
{"id": "32359413", "url": "https://en.wikipedia.org/wiki?curid=32359413", "title": "Łódź Heat Power Stations", "text": "Łódź Heat Power Stations\n\nŁódź Heat Power Stations (), also known under the company name of Veolia Energia Łódź S.A. are combined heat and power stations in Łódź, Poland. They are operated by Veolia Energia Łódź.\n\nThey are 2 operating heat and power stations in Łódź.\n\n! EC-3\n! EC-4\n"}
