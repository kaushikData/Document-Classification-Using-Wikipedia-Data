{"id": "33393735", "url": "https://en.wikipedia.org/wiki?curid=33393735", "title": "1835 New Brunswick, New Jersey tornado", "text": "1835 New Brunswick, New Jersey tornado\n\nThe 1835 New Brunswick, New Jersey, tornado of Friday, June 19, 1835, was the deadliest recorded in New Jersey history. It struck what is now part of downtown New Brunswick, along a path that stretched through or near what is now Robert Wood Johnson University Hospital property, rambled towards the site of Monument Square, and went downhill to the river from there, after wreaking havoc on George Street. The twister then destroyed all but two of the twelve houses in Piscataway, New Jersey.\n\nOn June 20, 1835, the \"New York Evening Star\" reported: \"About 5 o'clock yesterday afternoon, a tornado passed over the town of Piscataway, about two miles from New Brunswick, which destroyed every house but two. The current proceeded towards the City of New Brunswick, and made dreadful havoc in that place, destroying and injuring nearly one hundred and fifty houses in Liberty, Richmond and Schureman streets. The most melancholy part of the accident is the death of several persons.There were 5 deaths, and here are 4 of them. A widow lady by the name of Van Arsdale, a man called Henry Boorsem, formerly a midshipman in the Navy, who was killed in the street, and a boy named Bayard.\"\n\n"}
{"id": "44729903", "url": "https://en.wikipedia.org/wiki?curid=44729903", "title": "2014 Sundarbans oil spill", "text": "2014 Sundarbans oil spill\n\nThe 2014 Sundarbans oil spill was an oil spill that occurred on 9 December 2014 at the Shela River in Sundarbans, Bangladesh, a UNESCO World Heritage site. The spill occurred when an oil-tanker named \"Southern Star VII\", carrying of furnace oil, was in collision with a cargo vessel and sank in the river. By December 17, the oil had spread over a area. The oil spread to a second river and a network of canals in Sundarbans, which blackened the shoreline. The spill threatened trees, plankton, and vast populations of small fish and dolphins. The spill occurred at a protected mangrove area, home to rare Irrawaddy and Ganges dolphins. By 12 January 2015, of oil had been cleaned up by local residents, the Bangladesh Navy, and the government of Bangladesh.\n\nThe collision between the oil tanker and the cargo vessel occurred at the Shela River in Sundarbans, Khulna Division, Bangladesh. The site is near Mongla Port, and is about from Kolkata Port. The oil tanker was at anchor at the confluence between the Shela River and the Passur River in dense fog when the cargo vessel collided with it at 5am on December 9, 2014. The site is in a protected mangrove area, home to rare Irrawaddy and Ganges dolphins.\n\nSeven crew members of the sunken ship managed to swim ashore, but the captain of the ship, Mokhlesur Rahman, died, and his body was recovered a few kilometers away from the spot where the ship sank. Experts estimated that was lost as a result of the sinking of the oil-tanker. The residents of the surrounding area are at a health risk. \nThe government of Bangladesh told the local residents to collect the oil and sell it to the Bangladesh Petroleum Corporation. The local oil collectors faced health risks and various skin diseases, including hair fallout if furnace oil mixed with water contacts a person's face or hair.\n\nEnvironmentalists warned that the event was an ecological \"catastrophe\", as the spill occurred at a protected area where rare dolphins were present. Experts expressed concerns that the oil spill will hamper the well-being of the aquatic organisms in the area. Wildlife near the river are at a risk of death, because the smell of oil makes breathing difficult. Some images indicate that the disaster killed some animals.\n\nOn 13 December 2014, a dead Irrawaddy dolphin was seen floating on the Harintana-Tembulbunia channel of the Shela River. However, it was not confirmed that the dolphin's death was caused by the oil spill. According to the local residents, few dolphins have been seen in the area since the oil spill.\nOriental small-clawed otter (\"Amblonyx cinereus\") aquatic carnivore animal lives in the aquatic ecosystem of Sundarbans. On 18 December 2014, two dead otters were recovered from the Shela River by forest department workers. An autopsy of the two otters confirmed that they had died from ingesting oil. The otter is considered to be an endangered species, and has been included in the International Union for Conservation of Nature endangered list.\n\nA team of forest department workers saw crocodiles, monitor lizards and many other animals smeared with oil at the Chandpai range of the Sundarbans.\n\nThe oil spill is also posing a major threat to the forest's food cycle. Reports from various sources showed that the microorganisms, the primary level of the food cycle, are dying. The United Nations expressed deep concern over the oil spill, urging the government of Bangladesh to impose a \"complete ban\" on commercial vessels moving through the forest.\n\nShajahan Khan, the shipping minister of Bangladesh, told Bangladesh Sangbad Sangstha that he has talked with environmental experts, and they said that there will likely be no major damage as a result of the oil spill. Researchers said that Khan's claim was \"unscientific and misleading\".\n\nTen species that are at risk as a result of the spill have been listed by The National Geographic Traveler. These species are the Irrawady dolphins, Bengal tigers, leopards, great egrets, rhesus macaques, northern river terrapins, black-capped kingfishers, chitals, saltwater crocodiles, and horseshoe crabs.\nThe presence of White Rumped Vulture (\"Gyps bengalensis\") over the Sundarbans sky also indicated the large scale death of wild fauna. The secondary\neffect of oil spill is always much greater than the primary effects.\n\nAfter the spill, primarily Bangladeshi fishermen began to clean up the oil using sponges and sacks. Padma Oil Company, a government-owned corporation, bought the oil at a price of 30 takas per litre. The Bangladeshi navy initially sent four ships to deal with the spill, and planned to use chemicals to disperse the oil, but the plan was canceled, as there were concerns that dispersing the oil would further damage the ecology and biodiversity of the mangrove forest. The government of Bangladesh closed the Shela River to all vessels. The owner of the sunken oil tanker, MS Harun & Co., began salvage efforts, assisted by three private rescue vessels. The Bangladesh Forest Department filed a 1 billion taka lawsuit against the owners of the two cargo ships involved in the collision. By 12 January 2015, the government, with the help of local residents, the Bangladesh Navy, and the owner of the oil tanker, had collected of oil. The Bangladesh government said that they do not have the capacity to manage oil spills.\n\nThe Economic Relations Division of the Bangladesh government sent a letter to the United Nation's Bangladesh office on 15 December 2014, asking for help in the oil collection efforts. The United Nations accepted the request, and a team from the UNEP and OCHA went to the site of the spill. A team from the United Nations Disaster Assessment and Coordination arrived in the Dhaka to support the cleanup efforts.\n\n\n"}
{"id": "40333670", "url": "https://en.wikipedia.org/wiki?curid=40333670", "title": "3-Hydroxyoctanoic acid", "text": "3-Hydroxyoctanoic acid\n\n3-Hydroxyoctanoic acid is a beta-hydroxy acid that is naturally produced in humans, other animals, and plants. \n\n3-Hydroxyoctanoic acid is the primary endogenous agonist of hydroxycarboxylic acid receptor 3 (HCA), a G protein-coupled receptor protein which is encoded by the human gene \"HCAR3\". In plants, signalling chemical emitted by the orchid \"Cymbidium floribundum\" and recognized by Japanese honeybees (\"Apis cerana japonica\").\n"}
{"id": "37584586", "url": "https://en.wikipedia.org/wiki?curid=37584586", "title": "Abductin", "text": "Abductin\n\nAbductin is a natural elastic protein that is found in the hinge ligament of bivalve mollusks. It is unique as it is the only protein in nature with compressible elasticity. It is similar to elastin and resilin, but amino acid analysis reveals that it has high concentrations of glycine and methionine. \n\nAbductin is made up of three prominent amino acids, glycine, methionine, and phenylalanine, which are arranged in a repeating pentapeptide sequence throughout the molecule.\n\nIt has been proposed that the protein could have uses in drug delivery or tissue engineering.\n"}
{"id": "42075644", "url": "https://en.wikipedia.org/wiki?curid=42075644", "title": "Adsorbed natural gas", "text": "Adsorbed natural gas\n\nNatural gas burns cleanly as a fuel, making it useful in many vehicles and applications such as cooking, heating or running generators. It contains mostly methane and ethane. These light gases have very high vapor pressure at ambient temperatures, and their storage requires either high-pressure compression, adsorbent systems or an extreme reduction of temperature.\n\nRecently, a process has been introduced to store natural gas called adsorbed natural gas (ANG). In this process, natural gas adsorbs to a porous adsorbent at relatively low pressure (100 to 900 psi) and ambient temperature, solving both the high-pressure and low-temperature problems. If a suitable adsorbent is used, it is possible to store more gas in an adsorbent-filled vessel than in an empty vessel at the same pressure. The amount of adsorbed gas depends on pressure, temperature and adsorbent type. Since this adsorption process is exothermic, an increase or decrease of pressure or an increase or decrease of temperature enhances the efficiency of the adsorption and desorption process. Activated carbon is an adsorbent with high surface area that can be used in ANG storage tanks. Currently, researchers are developing new adsorbents with higher adsorption ratio to optimize this process.\n\nANG technology companies, such as Cenergy Solutions, Inc. (www.cenergysolutions.com), Adsorbed Natural Gas Products (ANGP) (www.angpinc.com), Ingevity Corporation (NYSE: NGVT) (www.ingevity.com), has deployed commercially viable ANG tank technology in the USA for CNG vehicles and bio-methane facility storage. Cenergy Solutions received a US patent on July 10, 2018 that helps the adsorption, desorption and filtering of the adsorbents so they do not escape from the cylinder or tank. Cenergy Solutions was also awarded the Clean Cities, Clean Air Champion Award for 2017 and ANGP was awarded the 2016 Frost & Sullivan New Product Innovation Award for Gas Storage for Natural Gas Vehicles for their work in commercializing ANG technology. In March 2016, ANGP's Generation 1 ANG storage system proved to be the first NGV2 compliant ANG storage system available to the market. The system has been demonstrated on a 2014 Ford F150, with an engine system capable of low-pressure natural gas operation. Cenergy Solutions introduced the first conformable ANG tank on a 2016 Ford F150 in May 2016 with their low pressure EPA approved bi-fuel conversion system. Validation ANG tank prototypes have been operational on an 8.1-liter Chevrolet Suburban; and the first ANG system to be used commercially was delivered in December 2015 on an OEM dedicated Dodge Ram 2500 with a 5.3-liter engine and has been in commercial operation since.\n\nANG technology is a significant step toward solving the “chicken and egg” challenge for broad and rapid adoption of CNG. The ANG system creates the low pressure, conformable CNG storage solution for full-scale, affordable NGV-hybrid deployment in the US. Moreover, the lower compression pressures required to “fill” ANG tanks also allows for the more rapid growth of a ubiquitous CNG refueling network. Cenergy Solutions, Inc. introduced the first low pressure ANG compressor that is CSA and UL certified to be used as a natural gas appliance. This compressor will solve the natural gas fueling infrastructure problem. Due to the significantly lower capital investment associated with lower compression requirements, CNG infrastructure will be more easily and cost-effectively added to the network of gas stations and truck stop.\n\nANG technology also creates a market for compact, affordable, low-pressure CNG Home Refueling Appliances (HRA’s) that will allow consumers to fill their vehicles at home connecting directly to their utility gas line. Cenergy Solutions has introduced this home and business compressor to the US, Canadian and Mexican market. \n\nFord Motor Company’s Fleet Sustainability and Technology Manager is quoted as saying, “The highest cost of a CNG filling station is the compressor, because compressing to 3,600 psi takes a tremendous amount of energy. By reducing that compression to 1,000 psi, or even further to the “Holy Grail” of 500 psi, this is game-changing technology because it enables home refueling, and you have 53 million potential fill points at every residence in the country that has natural gas running into it.” \n\nANG is also being tested for other methane storage applications, including CNG fueling station storage, virtual pipelines, and fugitive/ wellhead methane capture systems. Other significant opportunities for ANG include home heating and cooking applications in developing countries and in Southeast Asia.\n\n7. http://www.angpinc.com\n"}
{"id": "20561282", "url": "https://en.wikipedia.org/wiki?curid=20561282", "title": "Alpha–beta transformation", "text": "Alpha–beta transformation\n\nIn electrical engineering, the alpha-beta (formula_1) transformation (also known as the Clarke transformation) is a mathematical transformation employed to simplify the analysis of three-phase circuits. Conceptually it is similar to the dq0 transformation. One very useful application of the formula_1 transformation is the generation of the reference signal used for space vector modulation control of three-phase inverters.\n\nThe formula_1 transform applied to three-phase currents, as used by Edith Clarke, is\n\nwhere formula_5 is a generic three-phase current sequence and formula_6 is the corresponding current sequence given by the transformation formula_7.\nThe inverse transform is:\n\nThe above Clarke's transformation preserves the amplitude of the electrical variables which it is applied to. Indeed, consider a three-phase symmetric, direct, current sequence\nwhere formula_10 is the RMS of formula_11, formula_12, formula_13 and formula_14 is the generic time-varying angle that can also be set to formula_15 without loss of generality. Then, by applying formula_7 to the current sequence, it results\nwhere the last equation holds since we have considered balanced currents. As it is shown in the above, the amplitudes of the currents in the formula_1 reference frame are the same of that in the natural reference frame.\n\nThe active and reactive powers computed in the Clark's domain with the transformation shown above are not the same of those computed in the standard reference frame. This happens because formula_7 is not unitary. In order to preserve the active and reactive powers one has, instead, to consider\nwhich is a unitary matrix and the inverse coincides with its transpose.\nIn this case the amplitudes of the transformed currents are not the same of those in the standard reference frame, that is\nFinally, the inverse transformation in this case is\n\nSince in a balanced system formula_23 and thus formula_24 one can also consider the simplified transform\n\nwhich is simply the original Clarke's transformation with the 3rd equation thrown away, and\n\nThe formula_1 transformation can be thought of as the projection of the three phase quantities (voltages or currents) onto two stationary axes, the alpha axis and the beta axis.\n\nThe formula_28 transform is conceptually similar to the formula_1 transform. Whereas the formula_28 transform is the projection of the phase quantities onto a rotating two-axis reference frame, the formula_1 transform can be thought of as the projection of the phase quantities onto a stationary two-axis reference frame.\n\n"}
{"id": "40021115", "url": "https://en.wikipedia.org/wiki?curid=40021115", "title": "California Independent System Operator", "text": "California Independent System Operator\n\nThe California Independent System Operator (CAISO) is a non-profit Independent System Operator (ISO) serving California. It oversees the operation of California's bulk electric power system, transmission lines, and electricity market generated and transmitted by its member utilities. The primary stated mission of CAISO is to \"operate the grid reliably and efficiently, provide fair and open transmission access, promote environmental stewardship, and facilitate effective markets and promote infrastructure development.\" The CAISO is one of the largest ISOs in the world, delivering 300 million megawatt-hours of electricity each year and managing about 80% of California's electric flow.\n\nThe California legislature created the CAISO in 1998 as part of the state restructuring of electricity markets. The legislature was responding to Federal Energy Regulatory Commission (FERC) recommendations following the passage of the federal Energy Policy Act of 1992, which removed barriers to competition in the wholesale generation of the electricity business. FERC regulates CAISO because interstate transmission lines fall under the jurisdiction of federal commerce laws. CAISO is governed by a five member governing board appointed by the Governor of California.\n\nCalifornia Independent System Operator settled with the Federal Energy Regulatory Commission and the North American Electric Reliability Corporation for $6 million for violations of standards related to the 2011 Southwest blackout.\n\n\n"}
{"id": "17279558", "url": "https://en.wikipedia.org/wiki?curid=17279558", "title": "Carbon nanotubes in photovoltaics", "text": "Carbon nanotubes in photovoltaics\n\nOrganic photovoltaic devices (OPVs) are fabricated from thin films of organic semiconductors, such as polymers and small-molecule compounds, and are typically on the order of 100 nm thick. Because polymer based OPVs can be made using a coating process such as spin coating or inkjet printing, they are an attractive option for inexpensively covering large areas as well as flexible plastic surfaces. A promising low cost alternative to conventional solar cells made of crystalline silicon, there is a large amount of research being dedicated throughout industry and academia towards developing OPVs and increasing their power conversion efficiency.\n\nSingle wall carbon nanotubes possess a wide range of direct bandgaps matching the solar spectrum, strong photoabsorption, from infrared to ultraviolet, and high carrier mobility and reduced carrier transport scattering, which make themselves ideal photovoltaic material. Photovoltaic effect can be achieved in ideal single wall carbon nanotube (SWNT) diodes. Individual SWNTs can form ideal p-n junction diodes. An ideal behavior is the theoretical limit of performance for any diode, a highly sought after goal in all electronic materials development. Under illumination, SWNT diodes show significant power conversion efficiencies owing to enhanced properties of an ideal diode.\n\nRecently, SWNTs were directly configured as energy conversion materials to fabricate thin-film solar cells, with nanotubes serving as both photogeneration sites and a charge carriers collecting/transport layer. The solar cells consist of a semitransparent thin film of nanotubes conformally coated on a n-type crystalline silicon substrate to create high-density p-n heterojunctions between nanotubes and n-Si to favor charge separation and extract electrons (through n-Si) and holes (through nanotubes). Initial tests have shown a power conversion efficiency of >1%, proving that CNTs-on-Si is a potentially suitable configuration for making solar cells. For the first time, Zhongrui Li demonstrated that SOCl2 treatment of SWNT boosts the power conversion efficiency of SWNT/n-Si heterojunction solar cells by more than 60%. Later on the acid doping approach is widely adopted in the later published CNT/Si works. \nEven higher efficiency can be achieved if acid liquid is kept inside the void space of nanotube network. Acid infiltration of nanotube networks significantly boosts the cell efficiency to 13.8%,as reported by Yi Jia, by reducing the internal resistance that improves fill factor, and by forming photoelectrochemical units that enhance charge separation and transport. \nThe wet acid induced problems can be avoided by using aligned CNT film. In aligned CNT film, the transport distance is shortened, and the exciton quenching rate is also reduced. Additionally aligned nanotube film has much smaller void space, and better contact with substrate. So, plus strong acid doping, using aligned single wall carbon nanotube film can further improve power conversion efficiency (a record-high power-conversion-efficiency of >11% was achieved by Yeonwoong Jung).\n\nZhongrui Li also made the first n-SWNT/p-Si photovoltaic device by tuning SWNTs from p-type to n-type through polyethylene imine functionalization.\n\nCombining the physical and chemical characteristics of conjugated polymers with the high conductivity along the tube axis of carbon nanotubes (CNTs) provides a great deal of incentive to disperse CNTs into the photoactive layer in order to obtain more efficient OPV devices. The interpenetrating bulk donor–acceptor heterojunction in these devices can achieve charge separation and collection because of the existence of a bicontinuous network. Along this network, electrons and holes can travel toward their respective contacts through the electron acceptor and the polymer hole donor. Photovoltaic efficiency enhancement is proposed to be due to the introduction of internal polymer/nanotube junctions within the polymer matrix. The high electric field at these junctions can split up the excitons, while the single-walled carbon nanotube (SWCNT) can act as a pathway for the electrons.\n\nThe dispersion of CNTs in a solution of an electron donating conjugated polymer is perhaps the most common strategy to implement CNT materials into OPVs. Generally poly(3-hexylthiophene) (P3HT) or poly(3-octylthiophene) (P3OT) are used for this purpose. These blends are then spin coated onto a transparent conductive electrode with thicknesses that vary from 60 to 120 nm. These conductive electrodes are usually glass covered with indium tin oxide (ITO) and a 40 nm sublayer of poly(3,4-ethylenedioxythiophene) (PEDOT) and poly(styrenesulfonate) (PSS). PEDOT and PSS help to smooth the ITO surface, decreasing the density of pinholes and stifling current leakage that occurs along shunting paths. Through thermal evaporation or sputter coating, a 20 to 70 nm thick layer of aluminum and sometimes an intermediate layer of lithium fluoride are then applied onto the photoactive material. Multiple research investigations with both multi-walled carbon nanotubes (MWCNTs) and single-walled carbon nanotubes (SWCNTs) integrated into the photoactive material have been completed.\n\nEnhancements of more than two orders of magnitude have been observed in the photocurrent from adding SWCNTs to the P3OT matrix. Improvements were speculated to be due to charge separation at polymer–SWCNT connections and more efficient electron transport through the SWCNTs. However, a rather low power conversion efficiency of 0.04% under 100 mW/cm white illumination was observed for the device suggesting incomplete exciton dissociation at low CNT concentrations of 1.0% wt. Because the lengths of the SWCNTs were similar to the thickness of photovoltaic films, doping a higher percentage of SWCNTs into the polymer matrix was believed to cause short circuits. To supply additional dissociation sites, other researchers have physically blended functionalized MWCNTs into P3HT polymer to create a P3HT-MWCNT with fullerene C double-layered device. However, the power efficiency was still relatively low at 0.01% under 100 mW/cm white illumination. Weak exciton diffusion toward the donor–acceptor interface in the bilayer structure may have been the cause in addition to the fullerene C layer possibly experiencing poor electron transport.\n\nMore recently, a polymer photovoltaic device from C-modified SWCNTs and P3HT has been fabricated. Microwave irradiating a mixture of aqueous SWCNT solution and C solution in toluene was the first step in making these polymer-SWCNT composites. Conjugated polymer P3HT was then added resulting in a power conversion efficiency of 0.57% under simulated solar irradiation (95 mW/cm). It was concluded that improved short circuit current density was a direct result of the addition of SWCNTs into the composite causing faster electron transport via the network of SWCNTs. It was also concluded that the morphology change led to an improved fill factor. Overall, the main result was improved power conversion efficiency with the addition of SWCNTs, compared to cells without SWCNTs; however, further optimization was thought to be possible.\n\nAdditionally, it has been found that heating to the point beyond the glass transition temperature of either P3HT or P3OT after construction can be beneficial for manipulating the phase separation of the blend. This heating also affects the ordering of the polymeric chains because the polymers are microcrystalline systems and it improves charge transfer, charge transport, and charge collection throughout the OPV device. The hole mobility and power efficiency of the polymer-CNT device also increased significantly as a result of this ordering.\n\nEmerging as another valuable approach for deposition, the use of tetraoctylammonium bromide in tetrahydrofuran has also been the subject of investigation to assist in suspension by exposing SWCNTs to an electrophoretic field. In fact, photoconversion efficiencies of 1.5% and 1.3% were achieved when SWCNTs were deposited in combination with light harvesting cadmium sulfide (CdS) quantum dots and porphyrins, respectively.\n\nAmong the best power conversions achieved to date using CNTs were obtained by depositing a SWCNT layer between the ITO and the PEDOT : PSS or between the PEDOT : PSS and the photoactive blend in a modified ITO/PEDOT : PSS/ P3HT : (6,6)-phenyl-C-butyric acid methyl ester (PCBM)/Al solar cell. By dip-coating from a hydrophilic suspension, SWCNT were deposited after an initially exposing the surface to an argon plasma to achieve a power conversion efficiency of 4.9%, compared to 4% without CNTs.\n\nHowever, even though CNTs have shown potential in the photoactive layer, they have not resulted in a solar cell with a power conversion efficiency greater than the best tandem organic cells (6.5% efficiency). But, it has been shown in most of the previous investigations that the control over a uniform blending of the electron donating conjugated polymer and the electron accepting CNT is one of the most difficult as well as crucial aspects in creating efficient photocurrent collection in CNT-based OPV devices. Therefore, using CNTs in the photoactive layer of OPV devices is still in the initial research stages and there is still room for novel methods to better take advantage of the beneficial properties of CNTs.\n\nOne issue with utilizing SWCNTs for the photoactive layer of PV devices is the mixed purity when synthesized (about 1/3 metallic and 2/3 semiconducting). Metallic SWCNTs (m-SWCNTs) can cause exciton recombination between the electron and hole pairs, and the junction between metallic and semiconducting SWCNTs (s-SWCNTs) form Schottky barriers that reduce the hole transmission probability. The discrepancy in electronic structure of synthesized CNTs requires electronic sorting to separate and remove the m-SWCNTs to optimize the semiconducting performance. This may be accomplished through diameter and electronic sorting of CNTs through a density gradient ultracentrifugation (DGU) process, involving a gradient of surfactants that can separate the CNTs by diameter, chirality, and electronic type. This sorting method enables the separation of m-SWCNTs and the precise collection of multiple chiralities of s-SWCNTs, each chirality able to absorb a unique wavelength of light. \nThe multiple chiralities of s-SWCNTs are used as the hole transport material along with the fullerene component PC71BM to fabricate heterojunctions for the PV active layer. The polychiral s-SWCNTs enable a wide range optical absorption from visible to near-infrared (NIR) light, increasing the photo current relative to using single chirality nanotubes. To maximize light absorption, the inverted device structure was used with a zinc oxide nanowire layer penetrating the active layer to minimize collection length. Molybdenum oxide (MoOx) was utilized as a high work function hole transport layer to maximize voltage.\n\nCells fabricated with this architecture have achieved record power conversion efficiencies of 3.1%, higher than any other solar cell materials that utilize CNTs in the active layer. This design also has exceptional stability, with the PCE remaining at around 90% over a period of 30 days. The exceptional chemical stability of carbon nanomaterials enables excellent environmental stability compared to most organic photovoltaics that must be encapsulated to reduce degradation.\n\nRelative to the best of polymer-fullerene heterojunction solar cells that have PCEs of about 10%, polychiral nanotube and fullerene solar cells are still a far ways off. Nevertheless, these findings push the achievable limits of CNT technology in solar cells. The ability for polychiral nanotubes to absorb in the NIR regime is a technology that can be utilized to improve the efficiencies of future of multi-junction tandem solar cells along with increasing the lifetime and durability of future noncrystalline solar cells.\n\nITO is currently the most popular material used for the transparent electrodes in OPV devices; however, it has a number of deficiencies. For one, it is not very compatible with polymeric substrates due to its high deposition temperature of around 600 °C. Traditional ITO also has unfavorable mechanical properties such as being relatively fragile. In addition, the combination of costly layer deposition in vacuum and a limited supply of indium results in high quality ITO transparent electrodes being very expensive. Therefore, developing and commercializing a replacement for ITO is a major focus of OPV research and development.\n\nConductive CNT coatings have recently become a prospective substitute based on wide range of methods including spraying, spin coating, casting, layer-by-layer, and Langmuir–Blodgett deposition. The transfer from a filter membrane to the transparent support using a solvent or in the form of an adhesive film is another method for attaining flexible and optically transparent CNT films. Other research efforts have shown that films made of arc-discharge CNT can result in a high conductivity and transparency. Furthermore, the work function of SWCNT networks is in the 4.8 to 4.9 eV range (compared to ITO which has a lower work function of 4.7 eV) leading to the expectation that the SWCNT work function should be high enough to assure efficient hole collection. Another benefit is that SWCNT films exhibit a high optical transparency in a broad spectral range from the UV-visible to the near-infrared range. Only a few materials retain reasonable transparency in the infrared spectrum while maintaining transparency in the visible part of the spectrum as well as acceptable overall electrical conductivity. SWCNT films are highly flexible, do not creep, do not crack after bending, theoretically have high thermal conductivities to tolerate heat dissipation, and have high radiation resistance. However, the electrical sheet resistance of ITO is an order of magnitude less than the sheet resistance measured for SWCNT films. Nonetheless, initial research studies demonstrate SWCNT thin films can be used as conducting, transparent electrodes for hole collection in OPV devices with efficiencies between 1% and 2.5% confirming that they are comparable to devices fabricated using ITO. Thus, possibilities exist for advancing this research to develop CNT-based transparent electrodes that exceed the performance of traditional ITO materials.\n\nDue to the simple fabrication process, low production cost, and high efficiency, there is significant interest in dye-sensitized solar cells (DSSCs). Thus, improving DSSC efficiency has been the subject of a variety of research investigations because it has the potential to be manufactured economically enough to compete with other solar cell technologies. Titanium dioxide nanoparticles have been widely used as a working electrode for DSSCs because they provide a high efficiency, more than any other metal oxide semiconductor investigated. Yet the highest conversion efficiency under air mass (AM) 1.5 (100 mW/cm) irradiation reported for this device to date is about 11%. Despite this initial success, the effort to further enhance efficiency has not produced any major results. The transport of electrons across the particle network has been a key problem in achieving higher photoconversion efficiency in nanostructured electrodes. Because electrons encounter many grain boundaries during the transit and experience a random path, the probability of their recombination with oxidized sensitizer is increased. Therefore, it is not adequate to enlarge the oxide electrode surface area to increase efficiency because photo-generated charge recombination should be prevented. Promoting electron transfer through film electrodes and blocking interface states lying below the edge of the conduction band are some of the non-CNT based strategies to enhance efficiency that have been employed.\n\nWith recent progress in CNT development and fabrication, there is promise to use various CNT based nanocomposites and nanostructures to direct the flow of photogenerated electrons and assist in charge injection and extraction. To assist the electron transport to the collecting electrode surface in a DSSC, a popular concept is to utilize CNT networks as support to anchor light harvesting semiconductor particles. Research efforts along these lines include organizing CdS quantum dots on SWCNTs. Charge injection from excited CdS into SWCNTs was documented upon excitation of CdS nanoparticles. Other varieties of semiconductor particles including CdSe and CdTe can induce charge-transfer processes under visible light irradiation when attached to CNTs. Including porphyrin and C fullerene, organization of photoactive donor polymer and acceptor fullerene on electrode surfaces has also been shown to offer considerable improvement in the photoconversion efficiency of solar cells. Therefore, there is an opportunity to facilitate electron transport and increase the photoconversion efficiency of DSSCs utilizing the electron-accepting ability of semiconducting SWCNTs.\n\nOther researchers fabricated DSSCs using the sol-gel method to obtain titanium dioxide coated MWCNTs for use as an electrode. Because pristine MWCNTs have a hydrophobic surface and poor dispersion stability, pretreatment was necessary for this application. A relatively low-destruction method for removing impurities, HO treatment was used to generate carboxylic acid groups by oxidation of MWCNTs. Another positive aspect was the fact that the reaction gases including and HO were non-toxic and could be released safely during the oxidation process. As a result of treatment, HO exposed MWCNTs have a hydrophilic surface and the carboxylic acid groups on the surface have polar covalent bonding. Also, the negatively charged surface of the MWCNTs improved the stability of dispersion. By then entirely surrounding the MWCNTs with titanium dioxide nanoparticles using the sol-gel method, an increase in the conversion efficiency of about 50% compared to a conventional titanium dioxide cell was achieved. The enhanced interconnectivity between the titanium dioxide particles and the MWCNTs in the porous titanium dioxide film was concluded to be the cause of the improvement in short circuit current density. Here again, the addition of MWCNTs was thought to provide more efficient electron transfer through film in the DSSC.\n\nOne issue with utilizing SWCNTs for the photoactive layer of PV devices is the mixed purity when synthesized (about 1/3 metallic and 2/3 semiconducting). Metallic SWCNTs (m-SWCNTs) can cause exciton recombination between the electron and hole pairs, and the junction between metallic and semiconducting SWCNTs (s-SWCNTs) form Schottky barriers that reduce the hole transmission probability. The discrepancy in electronic structure of synthesized CNTs requires electronic sorting to separate and remove the m-SWCNTs to optimize the semiconducting performance. This may be accomplished through diameter and electronic sorting of CNTs through a density gradient ultracentrifugation (DGU) process, involving a gradient of surfactants that can separate the CNTs by diameter, chirality, and electronic type. This sorting method enables the separation of m-SWCNTs and the precise collection of multiple chiralities of s-SWCNTs, each chirality able to absorb a unique wavelength of light. \nThe multiple chiralities of s-SWCNTs are used as the hole transport material along with the fullerene component PC71BM to fabricate heterojunctions for the PV active layer. The polychiral s-SWCNTs enable a wide range optical absorption from visible to near-infrared (NIR) light, increasing the photo current relative to using single chirality nanotubes. To maximize light absorption, the inverted device structure was used with a zinc oxide nanowire layer penetrating the active layer to minimize collection length. Molybdenum oxide (MoOx) was utilized as a high work function hole transport layer to maximize voltage.\n\nCells fabricated with this architecture have achieved record power conversion efficiencies of 3.1%, higher than any other solar cell materials that utilize CNTs in the active layer. This design also has exceptionally stability, with the PCE remaining at around 90% over a period of 30 days. The exceptional chemical stability of carbon nanomaterials enables excellent environmental stability compared to most organic photovoltaics that must be encapsulated to reduce degradation.\n\nRelative to the best of polymer-fullerene heterojunction solar cells that have PCEs of about 10%, polychiral nanotube and fullerene solar cells are still a far ways off. Nevertheless, these findings push the achievable limits of CNT technology in solar cells. The ability for polychiral nanotubes to absorb in the NIR regime is a technology that can be utilized to improve the efficiencies of future of multi-junction tandem solar cells along with increasing the lifetime and durability of future noncrystalline solar cells.\n\n"}
{"id": "28544609", "url": "https://en.wikipedia.org/wiki?curid=28544609", "title": "Cheekspot blenny", "text": "Cheekspot blenny\n\nThe cheekspot blenny (\"Parablennius opercularis\") is a species of combtooth blenny found in the western Indian ocean. This species reaches a length of from TL.\n"}
{"id": "24993728", "url": "https://en.wikipedia.org/wiki?curid=24993728", "title": "Cocamidopropyl hydroxysultaine", "text": "Cocamidopropyl hydroxysultaine\n\nCocamidopropyl hydroxysultaine (CAHS) is a synthetic amphoteric surfactant from the hydroxysultaine group. It is found in personal care products (soaps, shampoos, lotions etc.). It has uses as a foam booster, viscosity builder, and an antistatic agent.\n\n\n"}
{"id": "11958464", "url": "https://en.wikipedia.org/wiki?curid=11958464", "title": "Community Choice Aggregation", "text": "Community Choice Aggregation\n\nCommunity Choice Aggregation, abbreviated CCA, also known as Community Choice Energy (CCE), municipal aggregation, governmental aggregation, electricity aggregation, and community aggregation, is an alternative to the investor owned utility energy supply system in which local entities in the United States aggregate the buying power of individual customers within a defined jurisdiction in order to secure alternative energy supply contracts. The CCA chooses the power generation source on behalf of the consumers. By aggregating purchasing power, they are able to create large contracts with generators, something individual buyers may be unable to do. The main goals of CCAs have been to either lower costs for consumers or to allow consumers greater control of their energy mix, mainly by offering \"greener\" generation portfolios than local utilities. Currently CCAS are possible in the United States states of Massachusetts, Ohio, California, Illinois, New Jersey, New York, and Rhode Island, and served nearly 5% of Americans in over 1300 municipalities as of 2014.\n\nCCAs are local, not-for-profit, public agencies that take on the decision-making role about sources of energy for electricity generation. Once established, CCAs become the default service provider for the \"power mix\" delivered to customers. In a CCA service territory, the incumbent utility continues to own and maintain the transmission and distribution infrastructure, metering, and billing. In some states, CCAs may be considered \"de facto\" public utilities of a new form that aggregate regional energy demand and negotiate with competitive suppliers and developers, rather than the traditional utility business model based on monopolizing energy supply.\n\nCCAs have set a number of national green power and climate protection records while reducing power bills, a rare combination that has won National Renewable Energy Laboratory (NREL) and Environmental Protection Agency (EPA) recognition for achieving significantly higher renewable energy portfolios while maintaining rates that are competitive with conventional fossil and nuclear-based utility power. Several major U.S. population centers under CCA have switched to energy portfolios that are an order of magnitude greener than local utilities or other direct access providers, but charge no premium above utility or direct access rates. CCAs are therefore already conspicuous leaders in green power innovation, receiving U.S. Environmental Protection Agency’s “green power leadership awards” for achievements in renewable energy (MCE Clean Energy; Oak Park, IL, Cincinnati, OH). Newer CCAs in California like Sonoma Clean Power and San Francisco's CleanPowerSF are increasingly focused on using the policy as a platform for financing and integrating a transition to local renewable energy sources rather than grid power.\n\nIn the US, CCAs are permitted in seven states: Massachusetts, Ohio, California, Illinois, New Jersey, New York, and Rhode Island, but are only present in the first six. States must first pass legislation allowing for the formation of CCAs before an aggregate can form. Currently, only states with electric deregulation have passed such legislation. This is a natural progression as electricity deregulation separates the functions of electricity generation from transmission and distribution allowing consumers to choose their electricity generator. This separation then allows for CCAs to choose the electricity generation mix on behalf of consumers without having to establish the infrastructure to move electricity. However, only 17 states and the District of Columbia have deregulated markets. The remaining 33 states are considered regulated, where utilities retain a monopoly on generation, transmission, and distribution of electric power.\nIn Massachusetts, where the nation's first CCA bill (Senate 447, Montigny) was first drafted by Massachusetts senate energy committee director Paul Douglas Fenn in 1995 and enacted in 1997, the locales of Cape Cod and Martha's Vineyard formed the Cape Light Compact and successfully lobbied for passage of the seminal CCA legislation. Two of the Cape Light Compact founders, Falmouth Selectman Matthew Patrick and Barnstable County Commissioner Rob O'Leary, were subsequently elected to the Massachusetts House of Representatives and Senate respectively. Between 1995 and 2000, Fenn formed the American Local Power Project and worked with Patrick to draft and pass similar laws in Ohio, New Jersey, and other states.\n\nFormer FERC Commissioner Nora Brownell has called Community Choice Aggregations “the only great exceptions to the failure of electric deregulation in the U.S.” With every CCA yet formed still in operation and charging ratepayers less per kilowatt hour than their Investor-Owned-Utilities, CCAs have proven to be reliable and capable of delivering greener power at competitive prices. Ohio’s Office of the Consumer’s Council has said that CCA is “the greatest success story” in Ohio’s competitive market, and new legislation to re-regulate utility rates in Ohio will preserve CCA even if other forms of competition are eliminated. In Massachusetts, the success of the Cape Light Compact has led to the formation of new CCAs used in towns such as Marlborough, Massachusetts.\n\nThe nation's first CCA, the Cape Light Compact, currently serves 200,000 customers, running aggressive and transparent energy efficiency programs and installing solar installations on Cape Cod schools, fire stations and libraries.\n\nMany other towns and cities have now formed CCAs or are working to complete the initial process.\n\nIn Ohio, the nation's largest CCA was formed shortly after 1999 when the state legislature adopted a CCA law - the Northeast Ohio Public Energy Council (NOPEC), made up of approximately 500,000 customers in 138 cities and towns across eight counties, procured a power supply contract that switched electric generation fuel supply from a mix of coal and nuclear power to a mix of natural gas and a small percentage of renewably powered electricity, announcing a 70% air pollution reduction in the region's power mix. The contract also included solar photovoltaic demonstration projects in each of the eight counties. NOPEC's contracting process was led by Scott Ridley, an energy consultant who had worked with Fenn to develop Community Choice Aggregation in Massachusetts and was a consultant for the Cape Light Compact.\n\nIn 2002, California State Legislature passed Assembly Bill 117, enabling CCA. The Bill allowed CCAs, and mandated that customers be automatically enrolled in their local CCA, with an option to opt out. The law also makes the clarification that, in California, CCAs are by legal definition not utilities, and are legally defined in California law as electric service providers.[2]\n\nIn the early days of the California energy crisis, Paul Fenn, the Massachusetts Senate Energy Committee director who conducted the legal research and drafting of the original CCA legislation, formed Local Power Inc. and drafted new CCA legislation for California. In a campaign organized by Local Power, the City and County of San Francisco led Oakland, Berkeley, Marin County, and a group of Los Angeles municipalities in adopting resolutions asking for a state CCA law in response to the failure of California's deregulated electricity market. Fenn's bill was sponsored by then Assembly Member Carole Migden (D-San Francisco) in 2001, and the bill became law (AB117) in September, 2002.\n\nCCA formation in California was delayed by initial political opposition by the state's investor-owned utilities. In June 2010, Pacific Gas & Electric sponsored a proposition, Proposition 16, to make it more difficult for local entities to form either municipal utilities or CCAs by requiring a two-thirds vote of the electorate rather than a simple majority, for a public agency to enter the retail power business. Although PG&E contributed over $46 million in an effort to pass the initiative (Prop 16's opponents, led by Local Power Inc. and The Utility Reform Network, had access to less than $100,000), Proposition 16 was defeated.\n\nSan Francisco adopted a CCA Ordinance drafted by Fenn (86-04, Tom Ammiano) in 2004, creating a CCA program to build 360 Megawatts (MW) of solar, green distributed generation, wind generation, and energy efficiency and demand response to serve San Francisco ratepayers using solar bonds. Specifically, the ordinance combined the power purchasing authority of CCA with a revenue bond authority also developed by Fenn to expand the power of CCA, known as the H Bond Authority (San Francisco Charter Section 9.107.8, Ammiano), to allow the CCA to finance new green power infrastructure, worth approximately $1 Billion. In 2007 the City adopted a detailed CCA Plan also written primarily by Fenn (Ordinance 447-07, Ammiano and Mirkarimi), which established a 51% Renewable Portfolio Standard by 2017 for San Francisco. Over the following decade, Sonoma and San Francisco worked with Fenn's company, Local Power Inc. on program designs focused on achieving energy localization through renewables and energy efficiency.\n\nInspired by Climate Protection efforts, CCA has spread to cities throughout the Bay Area and the state. In 2007, 40 California local governments were in the process of exploring CCA, virtually all of them seeking to double, triple or quadruple the green power levels (Renewable Portfolio Standard, or \"RPS) of the state's three Investor-Owned Utilities.\n\nIn April, 2014, Assemblymember Steve Bradford (D-Gardena) introduced legislation (AB 2145) that would sharply limit the ability of CCAs to enroll customers. CCA advocates and a broad coalition of local governments, business, and environmental organization rose up in opposition and defeated AB 2145. AB 2145 passed in the California Assembly but died in the Senate on August 30, 2014 when the Senate's legislative session was ended without it coming up for a vote. Regulators hold a hearing in 2017.\n\nMarin County launched California's first CCA program, Marin Clean Energy, on May 7, 2010, offering 50%-100% renewable energy at competitive prices. Marin Clean Energy (MCE) now serves approximately 255,000 customers in Marin County, Napa County and the cities of Benicia, El Cerrito, Lafayette, San Pablo, Richmond, American Canyon, Calistoga, Lafayette, Napa, St. Helena, Walnut Creek, Yountville, Concord, Danville, Martinez, Moraga, Oakley, Pinole, Pittsburg, and San Ramon.\n\nAs California's first CCA program, MCE is charting the course for a new, highly innovative approach to electricity service in the Bay Area. The organization's mission is to reduce energy-related greenhouse gas emissions by expanding access to affordable, renewable energy and energy efficiency programs while creating local economic and workforce benefits.\n\nThe Sonoma County-based Center for Climate Protection formally introduced the idea of pursuing CCA in Sonoma County in the 2008 Community Climate Action Plan. In 2011, the Sonoma County Water Agency funded the production of a feasibility study to study the question. The feasibility study was favorable and after much public review and the formation of a Joint Powers Authority to administer the agency, Sonoma Clean Power launched service on May 1, 2014 offering power that is both greener and more locally sourced, at a lower cost than incumbent utility PG&E. The County and all eight eligible cities in the county eventually joined. This includes Cloverdale, Cotati, Petaluma, Rohnert Park, Santa Rosa, Sebastopol, Sonoma, and Windsor.\n\nIn 2016 Mendocino County voted to join Sonoma Clean Power and the Sonoma Clean Power governing board voted to accept Mendocino County and the cities of Fort Bragg, Willits, and Point Arena into the Joint Powers Authority.\n\nSilicon Valley Clean Energy (SVCE) launched its operation on April 3, 2017 by providing 100% GHG free electricity to 12 Silicon Valley communities, including Campbell, Cupertino, Gilroy, Los Altos, Los Altos Hills, Los Gatos, Monte Sereno, Morgan Hill, Mountain View, Saratoga, Sunnyvale and unincorporated County of Santa Clara. https://www.svcleanenergy.org/about-us\n\nLancaster Choice Energy (LCE) began providing renewable power to municipal accounts in May 2015 with broad public enrollment beginning in October. So far the city of Lancaster, California has offset almost 70% of its peak load (147 Megawatts) with renewable sources of energy. Lancaster aims to become the first net-zero city in the U.S., Lancaster is determined to generate more clean energy than it consumes, along with several private-sector partners. The City has established new rules for building more efficient, sustainable structures.\n\nAs of the end of its first full year of operations in 2016 Lancaster Choice Energy had 55,000 accounts in the City of Lancaster. LCE customers receive a minimum of 36% renewable energy through the standard Clear Choice product, with many opting up to 100% renewable Smart Choice. In addition, LCE’s first solar energy plant is now live. Built by sPower, the plant provides 10 MW of power produced in Lancaster directly for Lancaster residents and is enough to power approximately 1,800 homes.\n\nPeninsula Clean Energy\n\nPeninsula Clean Energy (PCE) (https://www.peninsulacleanenergy.com/) was formed in February 2016 by unanimous votes of the County of San Mateo and all 20 incorporated cities and towns in the County. It began supplying power to customers in the Fall of 2016 and is currently the largest community choice energy program in California.\n\nAs of June 2017 it was offering its customers a baseline product that is both cleaner (at least 50% renewable and 75% greenhouse gas free) and at a lower cost than the incumbent utility, PG&E. As of June 2017 it was also offering its customers a 100% renewable product that was significantly less expensive than PG&E’s 100% renewable product.\n\nEast Bay Community Energy, also abbreviated EBCE, ebce.org, was formed in October 2016 by Alameda County and the cities of Albany, Berkeley, Dublin, Emeryville, Fremont, Hayward, Livermore, Oakland, Piedmont, San Leandro, and Union City. East Bay Community Energy plans to begin providing electricity in June 2018 for commercial and municipal customers and November 2018 for residential customers. \n\nAs of April 2018, EBCE plans to offer customers two energy services. Their Bright Choice service intends to use approximately the same amount of clean energy sources as Pacific Gas and Electric Company (PG&E) but it costs less. Their Brilliant 100 option service is greener and uses more renewable energy than PG&E electricity at the same price. A third service is planned to be launched in November 2018 and hopes to use 100& renewable energy and offered at a cost just above the PG&E rate.\n\nIn 2016 the six existing Community Choice agencies: MCE Clean Energy, Sonoma Clean Power, Lancaster Choice Energy, CleanPowerSF, Peninsula Clean Energy, and Silicon Valley Clean Energy formed a 501(c)(6) non-profit trade association, the California Community Choice Association, Cal-CCA. Cal-CCA held its first meeting in San Francisco on October 20, 2016.\n\nAccording to the Clean Power Exchange, a project of the Center for Climate Protection that tracks Community Choice expansion in California, by the close of 2016, 26 of the 58 counties in California either had operating CCAs, were on schedule to launch service, or were at some earlier stage of evaluation. Over 300 cities are similarly engaged in operational or emerging CCAs.\n\nOn May 16, 2017 San Jose City council approved the creation of San Jose Clean Energy, making San Jose the largest city in California to adopt a CCA.\n\nMonterey Bay Community Power (MBCP) procures carbon-free electricity for Monterey, San Benito, and Santa Cruz Counties, including all cities except Del Rey Oaks and King City which chose not to participate. MBCP began serving commercial customers in March 2018, with residential service beginning July 2018. MBCP matches PG&E rates and by default offers a 3% rebate on generation charges. MBCP is the only multi-county CCA in California, and it is geographically the largest. https://www.mbcommunitypower.org/\n\nMultiple cities across the state of California are considering the implementation of Community Choice aggregation programs across their districts, and many will begin to launch their program in 2018. California has many cities who are anticipated to launch CCAs in 2018, this includes Los Angeles County, Placer County, and Alameda County. There are also other cities who are exploring and in the process of implementing CCA, this includes San Diego County, Fresno County, and San Luis Obispo County.\n\nThe state of Illinois adopted a CCA law in 2009, which has led to an increase of communities providing electricity services to over 2/3 of the state's population as of 2014, including the city of Chicago, whose mayor Rahm Emanuel is focusing the program on reducing coal power production and increasing renewable energy.\n\nAs of October 2013, 671 Illinois cities and towns (representing 80% of the state's residential electricity market) have utilized CCA.\n\nBy the end of 2013, 91 local governments in Illinois (representing 1.7 million state residents) used the state's 2009 CCA law to purchase 100% renewable electricity for their communities.\n\nNew Jersey adopted a CCA law in 2003, but did not see active formation of aggregations until 2013, when Bergen County, Passaic County, and fifteen other cities and counties started CCA programs, focused on both lowering electric bills and in some cases greening their power supply, or both.\n\nThe New York State Public Service Commission (PSC) has identified CCA as consistent with the stated goals of the regulatory reform \"Reforming the Energy Vision” (REV), and has stated that local energy planning helps municipalities benefit from distributed energy resources enabled by REV. CCA legislation had been filed in the New York State Assembly in February 2014, followed by Governor Andrew Cuomo's order directing the PSC to implement CCA directly under its own authority in December 2014.\n\nIn December 2014, non-profit organization Sustainable Westchester submitted a petition to the PSC on behalf of its member municipalities to implement a CCA demonstration program in Westchester County. The PSC granted the Order on February 26, 2015 authorizing Sustainable Westchester to put out an RFP and award contracts for both electric and natural gas supply for residents and small businesses within municipalities in the county that pass a resolution to join the CCA: \"The Sustainable Westchester pilot is expected to provide valuable experience on CCA design and outcomes that, in addition to the many comments in that proceeding, will assist the Commission in making a determination on statewide implementation of CCA.\"\n\nThe program launched in 2015, becoming the first operational CCA in New York State. Similar local CCA organizing efforts are underway in Ulster County, Sullivan County, Hudson Highlands, and other communities.\n\nThe Utility Restructuring Act of 1996 deregulated the utility market within Rhode Island, allowing consumers to choose their electricity generation supplier and CCAs to form. While this act allowed for the creation of CCAs, there are currently no residential or small business CCAs available for private consumers to join. The only CCA option is for municipal facilities.\n\nRhode Island Energy Aggregation Program (REAP)\n\nThe Reap program “is operated by the Rhode Island League of Cities & Towns and serves 36 of Rhode Island’s 39 municipalities and four school districts”. The Reap program facilitated the purchase of electricity by the municipal entity by submitting requests for proposals, reviewing bids from approved electricity generators, and selecting companies that they believe will be the ideal provider for each municipality. The program reported in 2012 it had achieved cost savings of 20-30% over the standard offer.\n\nThere are advantages and disadvantages associated with the implementation of Community Choice Aggregation across different localities. CCA provides benefits like providing a customer choice, reduced energy costs, renewable energy, and environmental benefits. \n\nBy providing customer choice, customers have the ability to be enrolled in CCA or maintain their current utility provider. Customers are automatically enrolled in the program but they can choose to opt out of it. CCAs reduce energy costs, lowering rates for customers. This also increases the use of affordable renewable energy, provided through wind, solar, and geothermal steam. This provides environmental benefits for communities because it reduces natural gas consumption and greenhouse gas emissions.\n\nThere are also disadvantages associated with the implementation of CCAs. Potential issues associated with the implementation include political and financial obstacles. CCAs can encounter groups lobbying against its implementation, setbacks from IOUs, exit fees, and even disadvantages associated with the opt out choices. \n\nOn the political level, local government can be opposed by groups and organizations. An example of this is when the IOU Pacific Gas and Electric Company opposed the creation of CCA by supporting California Proposition 16 in 2010, which would make it difficult for California to implement CCAs across the state. Another utility provider who took action was San Diego Gas & Electric who attempted to stop local government from implementing Community choice aggregation programs. SDG&E created a separate entity, that would allow them to lobby against CCAs in San Diego County. \n\nOther issues that can arise from the development of community choice aggregation include the development of exit fees, specifically in the state of California. This is an issue for CCAs in the state of California because it allows Investor-owned utility (IOU) to raise prices through a Power Charge Indifference Adjustment (PCIA) , making it more expensive for customers to join CCA programs because there will be a fee to customers when they choose to stop using bundled services provided by their utility provider and start using a CCA program. The main issues revolving PCIA is the transparency of the program, accountability of the agencies, and proper valuation of costs associated with the exit fees. Exit fees are put on CCA users to compensate for the cost that would be put on those remaining with IOU services.\n\nCosts such as the exit fees and increased rates can be a disadvantage as CCAs can raise prices for customers. Rate setters and local officials can set CCA prices which can be a disadvantage if local government acts on their own self-interest or if local government lacks the knowledge to make decisions about CCA and prices in their localities. \nThe choice to opt out can be a benefit for customer choice but it can also be a risk for CCA programs because if there are many customers choosing to opt out of the services, this can result in financial instability among the CCA. \n\n"}
{"id": "618169", "url": "https://en.wikipedia.org/wiki?curid=618169", "title": "DC-to-DC converter", "text": "DC-to-DC converter\n\nA DC-to-DC converter is an electronic circuit or electromechanical device that converts a source of direct current (DC) from one voltage level to another. It is a type of electric power converter. Power levels range from very low (small batteries) to very high (high-voltage power transmission).\n\nBefore the development of power semiconductors and allied technologies, one way to convert the voltage of a DC supply to a higher voltage, for low-power applications, was to convert it to AC by using a vibrator, followed by a step-up transformer and rectifier. For higher power an electric motor was used to drive a generator of the desired voltage (sometimes combined into a single \"dynamotor\" unit, a motor and generator combined into one unit, with one winding driving the motor and the other generating the output voltage). These were relatively inefficient and expensive procedures used only when there was no alternative, as to power a car radio (which then used thermionic valves/tubes requiring much higher voltages than available from a 6 or 12 V car battery). The introduction of power semiconductors and integrated circuits made it economically viable to use techniques as described below, for example to convert the DC power supply to high-frequency AC, use a transformer—small, light, and cheap due to the high frequency—to change the voltage, and rectify back to DC. Although by 1976 transistor car radio receivers did not require high voltages, some amateur radio operators continued to use vibrator supplies and dynamotors for mobile transceivers requiring high voltages, although transistorised power supplies were available.\n\nWhile it was possible to derive a \"lower\" voltage from a higher with a linear electronic circuit, or even a resistor, these methods dissipated the excess as heat; energy-efficient conversion only became possible with solid-state switch-mode circuits.\n\nDC to DC converters are used in portable electronic devices such as cellular phones and laptop computers, which are supplied with power from batteries primarily. Such electronic devices often contain several sub-circuits, each with its own voltage level requirement different from that supplied by the battery or an external supply (sometimes higher or lower than the supply voltage). Additionally, the battery voltage declines as its stored energy is drained. Switched DC to DC converters offer a method to increase voltage from a partially lowered battery voltage thereby saving space instead of using multiple batteries to accomplish the same thing.\n\nMost DC to DC converter circuits also regulate the output voltage. Some exceptions include high-efficiency LED power sources, which are a kind of DC to DC converter that regulates the current through the LEDs, and simple charge pumps which double or triple the output voltage.\n\nDC to DC converters developed to maximize the energy harvest for photovoltaic systems and for wind turbines are called power optimizers.\n\nTransformers used for voltage conversion at mains frequencies of 50–60 Hz must be large and heavy for powers exceeding a few watts. This makes them expensive, and they are subject to energy losses in their windings and due to eddy currents in their cores. DC-to-DC techniques that use transformers or inductors work at much higher frequencies, requiring only much smaller, lighter, and cheaper wound components. Consequently these techniques are used even where a mains transformer could be used; for example, for domestic electronic appliances it is preferable to rectify mains voltage to DC, use switch-mode techniques to convert it to high-frequency AC at the desired voltage, then, usually, rectify to DC. The entire complex circuit is cheaper and more efficient than a simple mains transformer circuit of the same output.\n\nPractical electronic converters use switching techniques. Switched-mode DC-to-DC converters convert one DC voltage level to another, which may be higher or lower, by storing the input energy temporarily and then releasing that energy to the output at a different voltage. The storage may be in either magnetic field storage components (inductors, transformers) or electric field storage components (capacitors). This conversion method can increase or decrease voltage. Switching conversion is more power efficient (often 75% to 98%) than linear voltage regulation, which dissipates unwanted power as heat. Fast semiconductor device rise and fall times are required for efficiency; however, these fast transitions combine with layout parasitic effects to make circuit design challenging. The higher efficiency of a switched-mode converter reduces the heatsinking needed, and increases battery endurance of portable equipment. Efficiency has improved since the late 1980s due to the use of power FETs, which are able to switch more efficiently with lower switching losses at higher frequencies than power bipolar transistors, and use less complex drive circuitry.\nAnother important improvement in DC-DC converters is replacing the flywheel diode by synchronous rectification using a power FET, whose \"on resistance\" is much lower, reducing switching losses. Before the wide availability of power semiconductors, low-power DC-to-DC synchronous converters consisted of an electro-mechanical vibrator followed by a voltage step-up transformer feeding a vacuum tube or semiconductor rectifier, or synchronous rectifier contacts on the vibrator.\n\nMost DC-to-DC converters are designed to move power in only one direction, from dedicated input to output. However, all switching regulator topologies can be made bidirectional and able to move power in either direction by replacing all diodes with independently controlled active rectification. A bidirectional converter is useful, for example, in applications requiring regenerative braking of vehicles, where power is supplied \"to\" the wheels while driving, but supplied \"by\" the wheels when braking.\n\nAlthough they require few components, switching converters are electronically complex. Like all high-frequency circuits, their components must be carefully specified and physically arranged to achieve stable operation and to keep switching noise (EMI / RFI) at acceptable levels. Their cost is higher than linear regulators in voltage-dropping applications, but their cost has been decreasing with advances in chip design.\n\nDC-to-DC converters are available as integrated circuits (ICs) requiring few additional components. Converters are also available as complete hybrid circuit modules, ready for use within an electronic assembly.\n\nLinear regulators which are used to output a stable DC independent of input voltage and output load from a higher but less stable input by dissipating excess volt-amperes as heat, could be described literally as DC-to-DC converters, but this is not usual usage. (The same could be said of a simple voltage dropper resistor, whether or not stabilised by a following voltage regulator or Zener diode.)\n\nThere are also simple capacitive voltage doubler and Dickson multiplier circuits using diodes and capacitors to multiply a DC voltage by an integer value, typically delivering only a small current.\n\nIn these DC-to-DC converters, energy is periodically stored within and released from a magnetic field in an inductor or a transformer, typically within a frequency range of 300 kHz to 10 MHz. By adjusting the duty cycle of the charging voltage (that is, the ratio of the on/off times), the amount of power transferred to a load can be more easily controlled, though this control can also be applied to the input current, the output current, or to maintain constant power. Transformer-based converters may provide isolation between input and output. In general, the term \"DC-to-DC converter\" refers to one of these switching converters. These circuits are the heart of a switched-mode power supply. Many topologies exist. This table shows the most common ones.\n\nIn addition, each topology may be:\n\nMagnetic DC-to-DC converters may be operated in two modes, according to the current in its main magnetic component (inductor or transformer):\nA converter may be designed to operate in continuous mode at high power, and in discontinuous mode at low power.\n\nThe half bridge and flyback topologies are similar in that energy stored in the magnetic core needs to be dissipated so that the core does not saturate. Power transmission in a flyback circuit is limited by the amount of energy that can be stored in the core, while forward circuits are usually limited by the I/V characteristics of the switches.\n\nAlthough MOSFET switches can tolerate simultaneous full current and voltage (although thermal stress and electromigration can shorten the MTBF), bipolar switches generally can't so require the use of a snubber (or two).\n\nHigh-current systems often use multiphase converters, also called interleaved converters.\nMultiphase regulators can have better ripple and better response times than single-phase regulators.\n\nMany laptop and desktop motherboards include interleaved buck regulators, sometimes as a voltage regulator module.\n\nSwitched capacitor converters rely on alternately connecting capacitors to the input and output in differing topologies. For example, a switched-capacitor reducing converter might charge two capacitors in series and then discharge them in parallel. This would produce the same output power (less that lost to efficiency of under 100%) at, ideally, half the input voltage and twice the current. Because they operate on discrete quantities of charge, these are also sometimes referred to as charge pump converters. They are typically used in applications requiring relatively small currents, as at higher currents the increased efficiency and smaller size of switch-mode converters makes them a better choice. They are also used at extremely high voltages, as magnetics would break down at such voltages.\n\nA motor-generator set, mainly of historical interest, consists of an electric motor and generator coupled together. A \"dynamotor\" combines both functions into a single unit with coils for both the motor and the generator functions wound around a single rotor; both coils share the same outer field coils or magnets. Typically the motor coils are driven from a commutator on one end of the shaft, when the generator coils output to another commutator on the other end of the shaft. The entire rotor and shaft assembly is smaller in size than a pair of machines, and may not have any exposed drive shafts.\n\nMotor-generators can convert between any combination of DC and AC voltage and phase standards. Large motor-generator sets were widely used to convert industrial amounts of power while smaller units were used to convert battery power (6, 12 or 24 V DC) to a high DC voltage, which was required to operate vacuum tube (thermionic valve) equipment.\n\nFor lower-power requirements at voltages higher than supplied by a vehicle battery, vibrator or \"buzzer\" power supplies were used. The vibrator oscillated mechanically, with contacts that switched the polarity of the battery many times per second, effectively converting DC to square wave AC, which could then be fed to a transformer of the required output voltage(s). It made a characteristic buzzing noise.\n\nA further means of DC to DC conversion in the kilowatts to megawatts range is presented by using redox flow batteries such as the vanadium redox battery.\n\nDC-to-DC converters are subject to different types of chaotic dynamics such as bifurcation, crisis, and intermittency.\n\n\n\n"}
{"id": "176266", "url": "https://en.wikipedia.org/wiki?curid=176266", "title": "Electric unicycle", "text": "Electric unicycle\n\nAn electric unicycle is a self-balancing personal transporter with a single wheel. The rider controls the speed by leaning forwards or backwards, and steers by twisting the unit using their feet. The self-balancing mechanism uses gyroscopes, accelerometers in a similar way to that used by the Segway PT.\n\nMost commercial units are self-balancing in the direction of travel only (single axis) with lateral stability being provided by the rider; more complex fully self-balancing dual-axis devices also need to self-balance from side to side. The control mechanisms of both use control moment gyroscopes, reaction wheels and/or auxiliary pendulums and can be considered to be inverted pendulum.\n\nA hand-power monowheel was patented in 1869 by Richard C. Hemming with a pedal-power unit patented in 1885. Various motorized monowheels were developed and demonstrated during the 1930s without commercial success and Charles F Taylor was granted a patent for a 'vehicle having a single supporting and driving wheel' in 1964 after some 25 years of experimentation.\n\nIn 2003, Bombardier announced a conceptual design for such a device used as a sport vehicle, the Embrio. In September 2004 Trevor Blackwell demonstrated a functional self-balancing unicycle, using the control-mechanism similar to that used by the Segway PT and published the designs as the Eunicycle. In November 2006 Janick and Marc Simeray filed a US patent for a compact seatless device. In 2008 RYNO Motors demonstrated their prototype unit. In January 2009 Focus Designs demonstrates electric unicycle to Segway inventor. In Oct 2010 Focus Designs published a video of an electric unicycle with hub motor and a seat.\n\nShane Chen of Inventist launched the compact seatless 'Solowheel' in February 2011 and in the following month concluded a licensing agreement with the Simeray brothers and filed a patent relating to the device which was challenged by the Simeray brothers in a related patent application filed in 2015.\n\nLate in 2015, the Ford Motor Company patented a \"self-propelled unicycle engagable with vehicle\", intended for last-mile commuters.\n\n\n\n"}
{"id": "30030151", "url": "https://en.wikipedia.org/wiki?curid=30030151", "title": "Flight dynamics", "text": "Flight dynamics\n\nFlight dynamics is the study of the performance, stability, and control of vehicles flying through the air or in outer space. It is concerned with how forces acting on the vehicle influence its speed and attitude with respect to time.\n\nIn fixed-wing aircraft, the changing orientation of the vehicle with respect to the local air flow is represented by two critical parameters, angle of attack (\"alpha\") and angle of sideslip (\"beta\"). These angles describe the vector direction of airspeed, important because they are the principal source of modulations in the aerodynamic forces and moments applied to the aircraft.\n\nSpacecraft flight dynamics involve three forces: propulsive (rocket engine), gravitational, and lift and drag (when traveling through the earth's or any other atmosphere). Because aerodynamic forces involved with spacecraft flight are very small, this leaves gravity as the dominant force.\n\nAircraft and spacecraft share a critical interest in their orientation with respect to the earth horizon and heading, and this is represented by another set of angles, \"yaw\", \"pitch\", and \"roll\", which angles match their colloquial meaning, but also have formal definition as an Euler sequence. These angles are the product of the rotational equations of motion, where orientation responds to torque, just as the velocity of a vehicle responds to forces. For all flight vehicles, these two sets of dynamics, rotational and translational, operate simultaneously and in a coupled fashion to evolve the vehicle's state (orientation and velocity) trajectory.\n\nFlight dynamics is the science of air-vehicle orientation and control in three dimensions. The critical flight dynamics parameters are the angles of rotation with respect to the three aircraft's principal axes about its center of mass, known as \"roll\", \"pitch\" and \"yaw\"\n\nAircraft engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the center of gravity of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the center of gravity of the aircraft, causing the aircraft to pitch up or down.\n\nRoll, pitch and yaw refer, in this context, to rotations about the respective axes starting from a defined equilibrium state. The equilibrium roll angle is known as wings level or zero bank angle, equivalent to a level heeling angle on a ship. Yaw is known as \"heading\".\n\nA fixed-wing aircraft increases or decreases the lift generated by the wings when it pitches nose up or down by increasing or decreasing the angle of attack (AOA). The roll angle is also known as bank angle on a fixed-wing aircraft, which usually \"banks\" to change the horizontal direction of flight. An aircraft is usually streamlined from nose to tail to reduce drag making it typically advantageous to keep the sideslip angle near zero, though there are instances when an aircraft may be deliberately \"sideslipped\" for example a slip in a fixed-wing aircraft.\n\nThe forces acting on spacecraft are of three types: propulsive force (usually provided by the vehicle's engine thrust); gravitational force exerted by the Earth and other celestial bodies; and aerodynamic lift and drag (when flying in the atmosphere of the Earth or another body, such as Mars or Venus). The vehicle's attitude must be taken into account because of its effect on the aerodynamic and propulsive forces. There are other reasons, unrelated to flight dynamics, for controlling the vehicle's attitude in non-powered flight (e.g., thermal control, solar power generation, communications, or astronomical observation).\n\nThe flight dynamics of spacecraft differ from those of aircraft in that the aerodynamic forces are of very small, or vanishingly small effect for most of the vehicle's flight, and cannot be used for attitude control during that time. Also, most of a spacecraft's flight time is usually unpowered, leaving gravity as the dominant force.\n"}
{"id": "40010097", "url": "https://en.wikipedia.org/wiki?curid=40010097", "title": "Formosa Airlines Flight 7601", "text": "Formosa Airlines Flight 7601\n\nFormosa Airlines Flight 7601 was an aviation accident that killed 16 people on 10 August 1997 in Beigan, Matsu Islands, Fujian, Republic of China.\n\nFormosa Airlines Flight 7601, a Dornier Do 228 took off from Taipei Sungshan Airport at 07:37 local time with 14 passengers and two pilots on board for a flight to Matsu Beigan Airport. Rain and high winds caused the plane to miss its first approach to Matsu Beigan Airport. During its go around, the pilot turned right instead of left. The aircraft crashed and caught fire after it struck a military water tower approximately one kilometer from the airport. All 16 passengers and crew on board were killed.\n\nShortly after the crash, a local weather official tried to commit suicide. The Director of the Civil Aeronautics Administration, Tsai Tui, said the suicide attempt stemmed from public outcry over the crash.\n\n"}
{"id": "32758599", "url": "https://en.wikipedia.org/wiki?curid=32758599", "title": "Fortun Hydroelectric Power Station", "text": "Fortun Hydroelectric Power Station\n\nThe Fortun Power Station is a hydroelectric power station located in the municipality Luster in Sogn og Fjordane, Norway. The facility operates at an installed capacity of . The average annual production is 1,375 GWh. \n"}
{"id": "43331290", "url": "https://en.wikipedia.org/wiki?curid=43331290", "title": "Halichoeres maculipinna", "text": "Halichoeres maculipinna\n\nHalichoeres maculipinna (common name clown wrasse) is a species of tropical fish that lives throughout the Caribbean Sea and adjacent parts of the western Atlantic Ocean. It is a carnivorous, multi-colored wrasse that is common throughout its range.\n\n\"Halichoeres maculipinna\" is generally less than long. The fish is slightly elongated with a nearly symmetrical upper and lower body. It has a pointed snout and rows of small teeth in its upper and lower jaws with two sets of canines in each (at the front and corners of its mouth). Its pectoral fin has fourteen rays, its dorsal fin has eleven rays and nine spines, and its anal fin has eleven rays and three spines.\n\nIts dorsal side is yellow and is separated from its white ventral side by a black band. It has three red lines across the top of its head, and it may have a dark spot on its dorsal fin.\n\nThe fish lives in the northwestern Atlantic Ocean. Its range extends from the state of North Carolina in the United States, to the island of Bermuda and as far south as Colombia. It is also endemic to Caribbean islands such as Cuba and the Cayman Islands as well as Central American countries such as Belize. The fish was once believed to live in Brazil, but a study conducted in 2004 demonstrated that these fish belonged to a different species.\n\n\"Halichoeres maculipinna\" lives on the tops of coral reefs and in rocky areas. The fish is generally found beneath the surface. It has also been reported to live within Venezuelan \"Sargassum\" beds.\n\nThe fish is a carnivore. It primarily consumes invertebrates and ray-finned fish.\n\nLike many other wrasses, the fish is a sequential hermaphrodite. It can change its sex from male to female. It mates through lek mating. During this process, males are noted to be particularly territorial. Reproduction occurs through spawning.\n\nWhile a quantitative assessment of the population of \"Halichoeres maculipinna\" has not been performed, it is widespread and fairly common throughout its range. The species faces no major threats beyond occasional collection for the aquarium trade.\n"}
{"id": "48252496", "url": "https://en.wikipedia.org/wiki?curid=48252496", "title": "Hume-Bennett Lumber Company", "text": "Hume-Bennett Lumber Company\n\nThe Hume-Bennett Lumber Company was a logging operation located in the Sequoia National Forest .\n\nIn 1878, Congress passed the Timber and Stone Act to encourage private ownership of timber land and facilitate logging. At this time in American history, resources such as timber were largely viewed as unlimited resources that could best be used for commercial gain and economic growth. Despite a growing human presence in other regions of California, the Sierra Nevada was still a relatively uncharted and virgin land up until the late 1860s due to its formidable and rugged terrain. The Timber and Stone Act facilitated commercial exploitation of these mountain areas, requiring a modest fee and filing in order to transfer complete and unrestrained ownership of federal land to any individual. Tracts were sold in parcels to applicants, who at most times were illegally recruited and paid by corporate interests to file claims then transfer their ownership to lumber companies. As a result of this practice, large tracts of old growth forest passed from the federal government to lumber companies in relatively short order during the late 19th century.\n\nIn 1888 Hiram T. Smith and Austin D. Moore purchased 30,000 acres of land in the Sierra Nevada Mountains and founded the Kings River Lumber Company. A terminus was needed and the town of Sanger offered the company 65 acres adjacent to railroad access. On August 29, 1889 the company agreed to the purchase and started construction. Smith originally planned to build a railroad to transport lumber down the mountains but instead built a log flume. It was constructed by between the years of 1889 and 1890. On September 3, 1890, 2500 people attended \"Flumeopolis\", a celebration and barbecue to mark the completion of the flume. The log flume was 62 miles (100 km) in length, as tall as 300 feet. The flume transported lumber from three mills in the Sierra Nevada Mountains parallel to the Kings River to a lumber yard in Sanger, California. Along the route 11 \"flume houses\" were built where people known as \"flume herders\" kept the flume clear of blockages and reported any damage to the flume. Two of the mills were at the headwaters of the Kings River and one was in Millwood, in the upper Kings River region. The three mills operated ten hours a day employing 300 men and had a capacity of 3 million feet of lumber per month. An additional 200 men worked for the company in Sanger at a box factory, door and sash factory, planing mill and drying yard. The company operated rail lines for transporting lumber as well as oxen known as \"Heart Bulls\". Their first locomotive, Shay No. 1 named \"Sequoia\", was a narrow gauge locomotive built by Lima Locomotive & Machine Works in 1891. The Company operated until going bankrupt during the Depression of 1892.\n\nMoore and Smith reopened after two years of loan negotiations as the Sanger Lumber Company. In 1894 Moore & Smith founded The Sanger Lumber Company with 24,000 shares of stock totaling $600,000. Their creditors exchanged their liens for stock in the Sanger Lumber Company. For a sum of $100,000 The Moore & Smith Lumber Company mortgaged their Port Discovery mill, lumber lands in Washington State, three $56,250 notes of the Kings River Lumber Company, one $1,490 note of the Pacific Pine Lumber Company and a loan of $10,000. Due to continuing financial struggles in 1895 creditors foreclosed on the company and in 1896 moved operations to the Converse Basin named for Charles P. Converse. During this period destructive lumbering techniques were practiced and an estimated 191 Million board feet of giant sequoia trees were cut while only using an estimated 20%.\n\nThe Hume-Bennett Lumber Company was formed in 1905 when Thomas Hume and Ira B. Bennett purchased the Sanger Lumber Company. The purchase included 22,240 acres ranging in elevation from 5,000 to 6200 feet. While 16,960 acres had been cut 5,280 acres of timber remained. The company also purchased two additional standard gauge Shay locomotives.\n\nA fire destroyed the Converse basin sawmill in November 1905. Sparks from rail lines often caused fi so teams of two to six men patrolled during fire season. The mill was replaced and operated for two years. By 1908 logging of giant sequoia ranging from ten to twenty feet in diameter was common, but logging in the Converse Basin eventually became uneconomical.\n\nThe base of operations was moved from Millwood to hume and the flume was extended 17.5 miles at a cost of $100,000. The company began communication with John Samuel Eastwood regarding building a dam at Hume Lake. Eastwood saw the project as an opportunity to perpetuate his work with the logging industry as well as advance his work on hydro electric dams. Eastwood pushed for a multiple arch design. Hume had reservations about the proposal but eventually agreed. Final location surveys were carried out by Eastwood on June 20, 1908 and a few weeks later on June 26, 1908 construction started on a 61-foot concrete dam. The dam was completed in late November at a cost of approximately $35,000 and Eastwood received a fixed salary of $1,770. Chinese American laborers were used to complete the dam. It required 2,207 cubic yards of concrete and eight miles of steel cable to construct. Supplied by twenty five square miles of runoff in June 1909 the damming of Ten Mile Creek filled the valley and formed an 87 acres reservoir named Hume Lake. The 677 foot long dam is the first multiple arch dam built and stands to this day.\n\nThe town of Hume was founded and production began in 1909. Around 1909 a fire destroyed 80 -100 acres of land half a mile from Hume, but by June 1, 1910 the mill was running day and night, producing nearly 100,000 feet of lumber each day. Thomas Hume's son George was now president, Ira Bennett was vice president and T. W. Decker was general manager.\n\nAfter 29 years with the company Bennett was bought out of his share, and replaced by Hume's son George, in the fall of 1912. On December 5, 1912 he bought a competitor, The Fresno Flume and Lumber Company, for nearly $950,000. By 1914 He was in such financial trouble that he left the logging industry.\n\nThe Recession of 1913–1914 forced the company to return to logging the more profitable giant sequoia. In order to accommodate such large trees the rail lines were converted from narrow(3 feet) to standard gauge(4 feet 8.5 Inches) and 50 additional logging cars were purchased. In February 1917 the company incorporated and changed its name back to the Sanger Lumber Company and later to the Sanger Lumber Company of Michigan. After the close of season on November 3, 1917 a fire destroyed the original mill at Hume and the surrounding buildings at a cost of $500,000. Much of the steel from the mill was salvaged for the war effort of World War I. The mill was replaced by a smaller mill and operated on the site into the 1920s, but after constant financial losses The Hume-Bennett Lumber Company closed in 1924. It was the last logging company to log giant sequoia.\n\nOn June 13, 1914 C. M. Mooney was injured by debris from an explosion intended uproot a tree. The company was ordered to pay disability of $104.67.\n\nOn October 20, 1914 Orlando V. Carter a night watchman fell to his death through an opening in a machine room. The company was ordered to pay a death benefit to the widow of $2,490.\n\nIn 1926 a forest fire destroyed 7 miles of the log flume and in 1927 George Hume sold some of the company's assets. On April 8, 1935 he sold the holdings of the company, including the dam and 20,000 acres of land, to the U.S. National Forest Service. The U.S. National Forest Service has incorporated the land into the Sequoia National Forest.\n\nOn January 9, 1946, 320 acres of land adjacent to Hume Lake was sold for $140,000 to Walter Warkentin and partners. The sale included the Hume Lake Hotel, store, service station, post office, 22 cottages, 22 boats, a saloon and a brothel. The land was converted into Hume Lake Christian Camps and serves as a summer camp and conference center for worship and religious studies.\n"}
{"id": "26149056", "url": "https://en.wikipedia.org/wiki?curid=26149056", "title": "IEC 62351", "text": "IEC 62351\n\nIEC 62351 is a standard developed by WG15 of IEC TC57. This is developed for handling the security of TC 57 series of protocols including IEC 60870-5 series, IEC 60870-6 series, IEC 61850 series, IEC 61970 series & IEC 61968 series. The different security objectives include authentication of data transfer through digital signatures, ensuring only authenticated access, prevention of eavesdropping, prevention of playback and spoofing, and intrusion detection.\n\n\n\n"}
{"id": "44847430", "url": "https://en.wikipedia.org/wiki?curid=44847430", "title": "Interatomic potential", "text": "Interatomic potential\n\nInteratomic potentials are mathematical functions for calculating the potential energy of a system of atoms with given positions in space. Interatomic potentials are widely used as the physical basis of molecular mechanics and molecular dynamics simulations in chemistry, molecular physics and materials physics, sometimes in connection with such effects as cohesion, thermal expansion and elastic properties of materials.\n\nInteratomic potentials can be written as a series expansion of\nfunctional terms that depend on the position of one, two, three, etc.\natoms at a time. Then the total energy of the system V can\nbe written as \n\nHere formula_2 is the one-body term, formula_3 the two-body term, formula_4 the\nthree body term, formula_5 the number of atoms in the system,\nformula_6 the position of atom i, etc. i, j and k are indices\nthat loop over atom positions.\n\nNote that in case the pair potential is given per atom pair, in the two-body\nterm the potential should be multiplied by 1/2 as otherwise each bond is counted\ntwice, and similarly the three-body term by 1/6. Alternatively,\nthe summation of the pair term can be restricted to cases formula_7\nand similarly for the three-body term formula_8, if\nthe potential form is such that it is symmetric with respect to exchange\nof the j and k indices (this may not be the case for potentials\nfor multielemental systems).\n\nThe one-body term is only meaningful if the atoms are in an external\nfield (e.g. an electric field). In the absence of external fields,\nthe potential V should not depend on the absolute position of\natoms, but only on the relative positions. This means\nthat the functional form can be rewritten as a function\nof interatomic distances formula_9\nand angles between the bonds\n(vectors to neighbours) formula_10.\nThen, in the absence of external forces, the general\nform becomes\n\nIn the three-body term formula_4 the\ninteratomic distance formula_13 is not needed\nsince the three terms formula_14\nare sufficient to give the relative positions of three atoms\ni,j,k in three-dimensional space. Any terms of order higher than\n2 are also called \"many-body potentials\".\nIn some interatomic potentials the manybody interactions are \nembedded into the terms of a pair potential (see discussion on\nEAM-like and bond order potentials below).\n\nIn principle the sums in the expressions run over all N atoms.\nHowever, if the range of the interatomic potential is finite,\ni.e. the potentials formula_15 above\nsome cutoff distance formula_16,\nthe summing can be restricted to atoms within the cutoff\ndistance of each other. By also using a cellular method\nfor finding the neighbours, the MD algorithm can be\nan O(N) algorithm. Potentials with an infinite\nrange can be summed up efficiently by Ewald summation\nand its further developments.\n\nThe forces acting between atoms can be obtained by differentiation of\nthe total energy with respect to atom positions. That is,\nto get the force on atom i one should take the three-dimensional\nderivative (gradient) with respect to the position of atom i:\n\nFor two-body potentials this gradient reduces, thanks to the\nsymmetry with respect to ij in the potential form, to straightforward\ndifferentiation with respect to the interatomic distances\nformula_18. However, for many-body\npotentials (three-body, four-body, etc.) the differentiation\nbecomes considerably more complex \nsince the potential may not be any longer symmetric with respect to ij exchange.\nIn other words, also the energy\nof atoms k that are not direct neighbours of i can depend on the position formula_19\nbecause of angular and other many-body terms, and hence contribute to the gradient\nformula_20.\n\nInteratomic potentials come in many different varieties, with\ndifferent physical motivations. Even for single well-known elements such as silicon, \na wide variety of potentials quite different in functional form and motivation have been developed.\nThe true interatomic interactions\nare quantum mechanical in nature, and there is no known\nway in which the true interactions described by\nthe Schrödinger equation or Dirac equation for\nall electrons and nuclei could be cast into an analytical\nfunctional form. Hence all analytical interatomic\npotentials are by necessity approximations.\n\nThe arguably simplest widely used interatomic interaction model is the Lennard-Jones potential \nwhere formula_22 is the depth of the potential well\nand formula_23 is the distance at which the potential crosses zero.\nThe term proportional to formula_24in the potential can be motivated from a classical or quantum mechanical\ndescription of the interaction between induced electric dipoles. This\npotential seems to be quite accurate for noble gases, and is widely\nused for systems where dipole interactions are significant, including \nin chemistry force fields to describe intermolecular interactions.\n\nAnother simple and widely used pair potential is the\nMorse potential, which consists simply of a sum of two exponentials.\nHere formula_26 is the equilibrium bond energy and\nformula_27 the bond distance. The Morse\npotential has been applied to studies of molecular vibrations and solids \n, and although rarely used anymore, inspired the functional form\nof more modern potentials such as the bond-order potentials.\n\nIonic materials are often described by a sum of a \nshort-range repulsive term, such as the\nBuckingham pair potential, and a long-range Coulomb potential\ngiving the ionic interactions between the ions forming the material. The short-range\nterm for ionic materials can also be of many-body character\n\nPair potentials have some inherent limitations, like the inability\nto describe all 3 elastic constants of\ncubic metals. Hence modern molecular dynamics simulations \nare to a large extent carried out with different kinds of many-body potentials.\n\nThe Stilinger-Weber potential is a potential that has a \ntwo-body and three-body terms of the standard form \nwhere the three-body term describes how the potential energy changes with bond bending.\nIt was originally developed for pure Si, but has been extended to many other\nelements and compounds\nand also formed the basis for other Si potentials.\nMetals are very commonly described with what can be called\n\"EAM-like\" potentials, i.e. potentials that share\nthe same functional form as the embedded atom model.\nIn these potentials, the total potential energy is written\n\nwhere formula_30 is a so-called embedding function\n(not to be confused with the force formula_31) that is a function of the sum of the so-called electron density\nformula_32. formula_33\nis a pair potential that usually is purely repulsive. In the original\nformulation the electron\ndensity function formula_32 was obtained\nfrom true atomic electron densities, and the embedding function\nwas motivated from density-functional theory as the energy needed\nto 'embed' an atom into the electron density. \nHowever, many other potentials used for metals share the same functional\nform but motivate the terms differently, e.g. based\non tight-binding theory\nor other motivations\n\nEAM-like potentials are usually implemented as numerical tables.\nA collection of tables is available at the interatomic\npotential repository at NIST \n\nCovalently bonded materials are often described by \nbond order potentials, sometimes also called\nTersoff-like or Brenner-like potentials.\n\nThese have in general a form that resembles a pair potential:\n\nwhere the repulsive and attractive part are simple exponential\nfunctions similar to those in the Morse potential.\nHowever, the strength is modified by the environment of the atom formula_36 via the formula_37term. If implemented without\nan explicit angular dependence, these potentials\ncan be shown to be mathematically equivalent to \nsome varieties of EAM-like potentials\nThanks to this equivalence, the bond-order potential formalism has been implemented also for many metal-covalent mixed materials.\n\nFor very short interatomic separations, important in radiation material science,\nthe interactions can be described quite accurately with screened Coulomb potentials which have the general form\nhere φ(r) → 1 when r → 0. Here formula_39 and formula_40 are the charges of the interacting nuclei, and \"a\" is the so-called screening parameter.\nA widely used popular screening function is the \"Universal ZBL\" one.\nand more accurate ones can be obtained from all-electron quantum chemistry calculations\n\nIn binary collision approximation simulations this kind of potential can be used\nto describe the nuclear stopping power.\n\nSince the interatomic potentials are approximations, they by necessity all involve\nparameters that need to be adjusted to some reference values. In simple\npotentials such as the Lennard-Jones and Morse ones, the parameters can be\nset directly to match e.g. the equilibrium bond length and bond strength\nof a dimer molecule or the cohesive energy of a solid\n. However, many-body\npotentials often contain tens or even hundreds of adjustable parameters.\nThese can be fit into a larger set of experimental data, or materials\nproperties derived from more fundamental simulation models such as\ndensity-functional theory. For solids, a well-constructed many-body potential\ncan often describe at least the equilibrium crystal structure cohesion and \nlattice constant, linear elastic constants, and\nbasic point defect properties of all the elements and stable compounds well.\n\nThe aim of most potential construction and fitting is to make the potential\n\"transferable\", i.e. that it can describe materials properties that are clearly\ndifferent from those it was fitted to (for examples of potentials explicitly aiming for this,\nsee e.g.). As an example of demonstrated\npartial transferability, a review of interatomic potentials\nof Si found that for instance the Stillinger-Weber and Tersoff III potentials for Si\nare indeed able to describe several (but certainly not all) materials properties they were not fitted to\n\nThe NIST interatomic potential repository provides\na collection of fitted interatomic potentials, either as fitted parameter values or numerical\ntables of the potential functions.\n\nClassical interatomic potentials cannot reproduce all phenomena. Sometimes quantum description is necessary. Density functional theory is used to overcome this limitation.\n\n\n"}
{"id": "1268745", "url": "https://en.wikipedia.org/wiki?curid=1268745", "title": "International Brotherhood of Electrical Workers", "text": "International Brotherhood of Electrical Workers\n\nThe International Brotherhood of Electrical Workers (IBEW) is a labor union that represents nearly 750,000 workers and retirees in the electrical industry in the United States, Canada, Panama, Guam, and several Caribbean island nations; particularly electricians, or inside wiremen, in the construction industry and linemen and other employees of public utilities. The union also represents some workers in the computer, telecommunications, broadcasting, and other fields related to electrical work.\n\nIt was founded in 1891, two years before George Westinghouse won the electric current wars by lighting up the Chicago worlds fair with AC current, and before homes and businesses in the United States began receiving electricity. It is an international organization, based on the principle of collective bargaining. Its international president is Lonnie R. Stephenson, and is affiliated with the AFL-CIO.\n\nThe beginnings of the IBEW were in the Electrical Wiremen and Linemen's Union No. 5221, founded in St. Louis, Missouri in 1890. By 1891, after sufficient interest was shown in a national union, a convention was held on November 21, 1891 in St. Louis. At the convention, the IBEW, then known as the National Brotherhood of Electrical Workers (NBEW), was officially formed. The American Federation of Labor gave the NBEW a charter as an AFL affiliate on December 7, 1891. The union's official journal, \"The Electrical Worker\", was first published on January 15, 1893, and has been published ever since. At the 1899 convention in Pittsburgh, Pennsylvania, the union's name was officially changed to the International Brotherhood of Electrical Workers.\n\nThe union went through lean times in its early years, then struggled through six years of schism during the 1910s, when two rival groups each claimed to be the duly elected leaders of the union. In 1919, as many employers were trying to drive unions out of the workplace through a national open shop campaign, the union agreed to form the Council on Industrial Relations, a bipartite body made up of equal numbers of management and union representatives with the power to resolve any collective bargaining disputes. That body still functions today, and has largely resolved strikes in the IBEW's jurisdiction in the construction industry.\n\nIn September 1941, the National Apprenticeship Standards for the Electrical Construction Industry, a joint effort among the IBEW, the National Electrical Contractors Association, and the Federal Committee on Apprenticeship, were established. The IBEW added additional training programs and courses as needed to keep up with new technologies, including an industrial electronics course in 1959 and an industrial nuclear power course in 1966.\n\nToday, the IBEW conducts apprenticeship programs for electricians, linemen, and VDV (voice, data, and video) installers (who install low-voltage wiring such as computer networks), in conjunction with the National Electrical Contractors Association, under the auspices of the National Joint Apprenticeship and Training Committee (NJATC), which allows apprentices to \"earn while you learn.\" In Canadian jurisdictions, the IBEW does not deliver apprenticeship training, but does conduct supplemental training for government trained apprentices and journeypersons, often at little or no cost to its members. The IBEW local 353 Toronto requires all apprentices to be registered with the JAC (Joint Apprenticeship Council) for a number of safety courses, pre-apprenticeship training, pre-trade school courses, supplementary training, and pre-exam courses.\n\nThe IBEW's membership peaked in 1972 at approximately 1 million members. The membership numbers were in a slow decline throughout the rest of the 1970s and the 1980s, but have since stabilized. One major loss of membership for the IBEW came about because of the court-ordered breakup at the end of 1982 of AT&T, where the IBEW was heavily organized among both telephone workers and in AT&T's manufacturing facilities. Membership as of 2013 stands at about 750,000, according to their official website.\n\n\n\n"}
{"id": "1532171", "url": "https://en.wikipedia.org/wiki?curid=1532171", "title": "Ixtoc I oil spill", "text": "Ixtoc I oil spill\n\nIxtoc I was an exploratory oil well being drilled by the semi-submersible drilling rig \"Sedco 135\" in the Bay of Campeche of the Gulf of Mexico, about northwest of Ciudad del Carmen, Campeche in waters deep. On 3 June 1979, the well suffered a blowout resulting in one of the largest oil spills in history.\n\nMexico's state-owned oil company Pemex (Petróleos Mexicanos) was drilling a deep oil well when the drilling rig \"Sedco 135\" lost drilling mud circulation.\n\nIn modern rotary drilling, mud is circulated down the drill pipe and back up the well bore to the surface. The goal is to equalize the pressure through the shaft and to monitor the returning mud for gas. Without the counter-pressure provided by the circulating mud, the pressure in the formation allowed oil to fill the well column, blowing out the well. The oil caught fire, and \"Sedco 135\" burned and collapsed into the sea.\n\nAt the time of the accident \"Sedco 135\" was drilling at a depth of about below the seafloor. The day before Ixtoc suffered the blowout and resulting fire that caused her to sink, the drill bit hit a region of soft strata. Subsequently, the circulation of drilling mud was lost resulting in a loss of hydrostatic pressure. Rather than returning to the surface, the drilling mud was escaping into fractures that had formed in the rock at the bottom of the hole. Pemex officials decided to remove the bit, run the drill pipe back into the hole and pump materials down this open-ended drill pipe in an effort to seal off the fractures that were causing the loss of circulation.\n\nDuring the removal of the pipe on \"Sedco 135\", the drilling mud suddenly began to flow up towards the surface; by removing the drill-string the well was swabbed (an effect observed when mud must flow down the annulus to replace displaced drill pipe volume below the bit) leading to a kick. Normally, this flow can be stopped by activating shear rams contained in the blowout preventer (BOP). These rams are designed to sever and seal off the well on the ocean floor; however in this case the drill collars had been brought in line with the BOP and the BOP rams were not able to sever the thick steel walls of the drill collars leading to a catastrophic blowout.\n\nThe drilling mud was followed by a large quantity of oil and gas at an increasing flow rate. The oil and gas fumes exploded on contact with the operating pump motors, starting a fire which led to the collapse of the \"Sedco 135\" drilling tower. The collapse caused damage to underlying well structures. The damage to the well structures led to the release of significant quantities of oil into the Gulf.\n\nIn the initial stages of the spill, an estimated of oil per day were flowing from the well. In July 1979, the pumping of mud into the well reduced the flow to per day, and early in August the pumping of nearly 100,000 steel, iron, and lead balls into the well reduced the flow to per day. Pemex claimed that half of the released oil burned when it reached the surface, a third of it evaporated, and the rest was contained or dispersed. Mexican authorities also drilled two relief wells into the main well to lower the pressure of the blowout, however the oil continued to flow for three months following the completion of the first relief well.\n\nPemex contracted Conair Aviation to spray the chemical dispersant Corexit 9527 on the oil. A total of 493 aerial missions were flown, treating of oil slick. Dispersants were not used in the U.S. area of the spill because of the dispersant's inability to treat weathered oil. Eventually the on-scene coordinator (OSC) requested that Mexico stop using dispersants north of 25°N.\n\nIn Texas, an emphasis was placed on coastal countermeasures protecting the bays and lagoons formed by the barrier islands. Impacts of oil to the barrier island beaches were ranked as second in importance to protecting inlets to the bays and lagoons. This was done with the placement of skimmers and booms. Efforts were concentrated on the Brazos-Santiago Pass, Port Mansfield Channel, Aransas Pass, and Cedar Bayou which during the course of the spill was sealed with sand. Economically and environmentally sensitive barrier island beaches were cleaned daily. Laborers used rakes and shovels to clean beaches rather than heavier equipment which removed too much sand. Ultimately, of oil impacted of U.S. beaches, and over of oiled material were removed.\n\nIn the next nine months, experts and divers including Red Adair were brought in to contain and cap the oil well. An average of approximately per day were discharged into the Gulf until it was finally capped on 23 March 1980, nearly 10 months later. In similarity to the Deepwater Horizon oil spill 31 years later, the list of methods attempted to remediate the leak included lowering a cap over the well, plugging the leak with mud and \"junk\", use of dispersants, and spending months attempting to drill relief wells.\n\nPrevailing currents carried the oil towards the Texas coastline. The US government had two months to prepare booms to protect major inlets. Pemex spent $100 million to clean up the spill and avoided most compensation claims by asserting sovereign immunity as a state-run company.\n\nThe oil slick surrounded Rancho Nuevo, in the Mexican state of Tamaulipas, which is one of the few nesting sites for Kemp's Ridley sea turtles. Thousands of baby sea turtles were airlifted to a clean portion of the Gulf of Mexico to help save the rare species.\n\nThe oil that was lost during the blow-out polluted a considerable part of the offshore region in the Gulf of Mexico as well as much of the coastal zone, which consists primarily of sandy beaches and barrier islands often enclosing extensive shallow lagoons.\n\nThe oil on Mexican beaches that the authors observed in early September was calculated to be about 6000 metric tons. Based on reports from various groups and individuals, five times that figure is thought to represent a fair estimate of what had landed on Mexican beaches. Investigations along the Texas coast show that approximately 4000 metric tons of oil or less than 1 percent was deposited there. The rest of the oil, about 120,000 metric tons or 25 percent, sank to the bottom of the Gulf.\n\nThe oil had a severe impact on the littoral crab and mollusk fauna of the beaches which were contaminated. The populations of crabs, e.g. the ghost crab Ocypode quadrata, were almost totally eliminated over a wide area. The crab populations on coral islands along the coast were also reduced to only a few percent of normal about nine months after the spill.\n\nOne study found that the most persistent issues were pollution of estuaries and coastal lagoons lining the bay, and especially the effects on breeding and growth of several food fish species.\n\nThe oil washed ashore, 30 cm (1 ft.) deep in some places, as it was pushed north by prevailing winds and currents until it crossed the Texas border two months later and eventually coated almost of US beaches. The beach that caused most international concern in Mexico was Rancho Nuevo, a key nesting ground for critically endangered Kemp's Ridley sea turtles which had already moved inland in their hundreds to lay eggs. By the time the eggs hatched, the oil had reached the shore.\n\nFishing was banned or restricted by Mexican authorities in contaminated areas north and south of the well. Fish and octopus catches dropped by 50 to 70% from the 1978 levels. Some larger species with longer life spans took years to recover from the Ixtoc spill. It wasn't until the late-1980s that the population of Kemp's Ridley turtles, which lay a couple of hundred eggs a year, as opposed to the millions produced by shrimp, started recovering. The immediate losses from an oil spill continue to affect larger species for generations.\n\nThere is much less information on the impact of the Ixtoc I spill on benthic species (bottom dwellers). The best studies were on the Texas coast over 1000 km from the spill. Massive kills can occur when oil reaches the benthos in sufficient quantity. The only indication of a massive kill may be the remains of the dead organisms, but if they lack hard parts there will be little evidence.\n\nA report prepared for the US Bureau of Land Management concluded with respect to the spill's effect on US waters:\n\nIn spite of a massive intrusion of petroleum hydrocarbon pollutants from the Ixtoc I event into the study region of the South Texas Outer Continental Shelf during 1979-1980, no definitive damage can be associated with this or other known spillage events (e .g ., Burmah Agate ) on either the epibenthic commercial shrimp population (based on chemical evidence) or the benthic infaunal community. Such conclusions have no bearing on intertidal or littoral communities, which were not the subject of this study.\n\n\n"}
{"id": "187477", "url": "https://en.wikipedia.org/wiki?curid=187477", "title": "Joseph Henry", "text": "Joseph Henry\n\nJoseph Henry (December 17, 1797 – May 13, 1878) was an American scientist who served as the first Secretary of the Smithsonian Institution. He was the secretary for the National Institute for the Promotion of Science, a precursor of the Smithsonian Institution. He was highly regarded during his lifetime. While building electromagnets, Henry discovered the electromagnetic phenomenon of self-inductance. He also discovered mutual inductance independently of Michael Faraday, though Faraday was the first to make the discovery and publish his results. Henry developed the electromagnet into a practical device. He invented a precursor to the electric doorbell (specifically a bell that could be rung at a distance via an electric wire, 1831) and electric relay (1835). The SI unit of inductance, the henry, is named in his honor. Henry's work on the electromagnetic relay was the basis of the practical electrical telegraph, invented by Samuel F. B. Morse and Sir Charles Wheatstone, separately.\n\nHenry was born in Albany, New York, to Scottish immigrants Ann Alexander Henry and William Henry. His parents were poor, and Henry's father died while he was still young. For the rest of his childhood, Henry lived with his grandmother in Galway, New York. He attended a school which would later be named the \"Joseph Henry Elementary School\" in his honor. After school, he worked at a general store, and at the age of thirteen became an apprentice watchmaker and silversmith. Joseph's first love was theater and he came close to becoming a professional actor. His interest in science was sparked at the age of sixteen by a book of lectures on scientific topics titled \"Popular Lectures on Experimental Philosophy\". In 1819 he entered The Albany Academy, where he was given free tuition. Even with free tuition he was so poor that he had to support himself with teaching and private tutoring positions. He intended to go into medicine, but in 1824 he was appointed an assistant engineer for the survey of the State road being constructed between the Hudson River and Lake Erie. From then on, he was inspired to a career in either civil or mechanical engineering.\n\nHenry excelled at his studies (so much so, that he would often help his teachers teach science) and in 1826 was appointed Professor of Mathematics and Natural Philosophy at The Albany Academy by Principal T. Romeyn Beck. Some of his most important research was conducted in this new position. His curiosity about terrestrial magnetism led him to experiment with magnetism in general. He was the first to coil insulated wire tightly around an iron core in order to make a more powerful electromagnet, improving on William Sturgeon's electromagnet which used loosely coiled uninsulated wire. Using this technique, he built the strongest electromagnet at the time for Yale. He also showed that, when making an electromagnet using just two electrodes attached to a battery, it is best to wind several coils of wire in parallel, but when using a set-up with multiple batteries, there should be only one single long coil. The latter made the telegraph feasible. Because of his early experiments in electromagnetism some historians credit Henry with discoveries pre-dating Faraday and Hertz, however, Henry is not credited due to not publishing his work.\nUsing his newly developed electromagnetic principle, Henry in 1831 created one of the first machines to use electromagnetism for motion. This was the earliest ancestor of modern DC motor. It did not make use of rotating motion, but was merely an electromagnet perched on a pole, rocking back and forth. The rocking motion was caused by one of the two leads on both ends of the magnet rocker touching one of the two battery cells, causing a polarity change, and rocking the opposite direction until the other two leads hit the other battery.\n\nThis apparatus allowed Henry to recognize the property of self inductance. British scientist Michael Faraday also recognized this property around the same time. Since Faraday published his results first, he became the officially recognized discoverer of the phenomenon.\n\nFrom 1832 to 1846, Henry served as the first Chair of Natural History at the College of New Jersey (now Princeton University). While in Princeton, he taught a wide range of courses including natural history, chemistry, and architecture, and ran a laboratory on campus. Decades later, Henry wrote that he made \"several thousand original investigations on electricity, magnetism, and electro-magnetism\" while on the Princeton faculty. Henry relied heavily on an African American research assistant, Sam Parker, in his laboratory and experiments. Parker was a free black man hired by the Princeton trustees to assist Henry. In an 1841 letter to mathematician Elias Loomis, Henry wrote:\n\nThe Trustees have however furnished me with an article which I now find indispensible namely with a coloured servant whom I have taught to manage my batteries and who now relieves me from all the dirty work of the laboratory.\n\nIn his letters, Henry described Parker providing materials for experiments, fixing technical issues with Henry's equipment, and at times being used as a test subject in electrical experiments in which Henry and his students would shock Parker in classroom demonstrations. In 1842, when Parker fell ill, Henry's experiments stopped completely until he recovered.\n\nHenry was appointed the first Secretary of the Smithsonian Institution in 1846, and served in this capacity until 1878. In 1848, while Secretary, Henry worked in conjunction with Professor Stephen Alexander to determine the relative temperatures for different parts of the solar disk. They used a thermopile to determine that sunspots were cooler than the surrounding regions. This work was shown to the astronomer Angelo Secchi who extended it, but with some question as to whether Henry was given proper credit for his earlier work.\n\nIn late 1861 and early 1862, during the American Civil War, Henry oversaw a series of lectures by prominent abolitionists at the Smithsonian Institution. Speakers included white clergymen, politicians, and activists such as Wendell Phillips, Horace Greeley, Henry Ward Beecher, and Ralph Waldo Emerson. Famous orator and former fugitive slave Frederick Douglass was scheduled as the final speaker; Henry, however, refused to allow him to attend, stating: \"I would not let the lecture of the coloured man be given in the rooms of the Smithsonian.\"\n\nIn the fall of 2014 history author Jeremy T.K. Farley released \"The Civil War Out My Window: Diary of Mary Henry.\" The 262-page book featured the diary of Henry's daughter Mary, from the years of 1855 to 1878. Throughout the diary, Henry is repeatedly mentioned by his daughter, who showed a keen affection to her father.\n\nProf. Henry was introduced to Prof. Thaddeus Lowe, a balloonist from New Hampshire who had taken interest in the phenomenon of lighter-than-air gases, and exploits into meteorology, in particular, the high winds which we call the Jet stream today. It was Lowe's intent to make a transatlantic crossing by utilizing an enormous gas-inflated aerostat. Henry took a great interest in Lowe's endeavors, promoting him among some of the more prominent scientists and institutions of the day.\n\nIn June 1860, Lowe had made a successful test flight with his gigantic balloon, first named the \"City of New York\" and later renamed \"The Great Western\", flying from Philadelphia to Medford, New York. Lowe would not be able to attempt a transatlantic flight until late Spring of the 1861, so Henry convinced him to take his balloon to a point more West and fly the balloon back to the eastern seaboard, an exercise that would keep his investors interested.\n\nLowe took several smaller balloons to Cincinnati, Ohio in March 1861. On 19 April, he launched on a fateful flight that landed him in Confederate South Carolina. With the Southern States seceding from the Union, during that winter and spring of 1861, and the onset of Civil War, Lowe abandoned further attempts at a trans-Atlantic crossing and, with Henry's endorsement, went to Washington, D.C. to offer his services as an aeronaut to the Federal government. Henry submitted a letter to U.S. Secretary of War at the time Simon Cameron of Pennsylvania which carried Henry's endorsement:\n\nOn Henry's recommendation Lowe went on to form the United States Army/\"Union Army\" Balloon Corps and served two years with the Army of the Potomac as a Civil War \"Aeronaut\".\n\nAs a famous scientist and director of the Smithsonian Institution, Henry received visits from other scientists and inventors who sought his advice. Henry was patient, kindly, self-controlled, and gently humorous. One such visitor was Alexander Graham Bell, who on 1 March 1875 carried a letter of introduction to Henry. Henry showed an interest in seeing Bell's experimental apparatus, and Bell returned the following day. After the demonstration, Bell mentioned his untested theory on how to transmit human speech electrically by means of a \"harp apparatus\" which would have several steel reeds tuned to different frequencies to cover the voice spectrum. Henry said Bell had \"the germ of a great invention\". Henry advised Bell not to publish his ideas until he had perfected the invention. When Bell objected that he lacked the necessary knowledge, Henry firmly advised: \"Get it!\"\n\nOn 25 June 1876, Bell's experimental telephone (using a different design) was demonstrated at the Centennial Exhibition in Philadelphia where Henry was one of the judges for electrical exhibits. On 13 January 1877, Bell demonstrated his instruments to Henry at the Smithsonian Institution and Henry invited Bell to demonstrate them again that night at the Washington Philosophical Society. Henry praised \"the value and astonishing character of Mr. Bell's discovery and invention.\"\n\nHenry died on 13 May 1878, and was buried in Oak Hill Cemetery in the Georgetown section of northwest Washington, D.C. John Phillips Sousa wrote the Transit of Venus March for the unveiling of the Joseph Henry statue in front of the Smithsonian Castle.\n\nHenry was a member of the United States Lighthouse Board from 1852 until his death. He was appointed chairman in 1871 and served in that position the remainder of his life. He was the only civilian to serve as chairman. The United States Coast Guard honored Henry for his work on lighthouses and fog signal acoustics by naming a cutter after him. The \"Joseph Henry\", usually referred to as the \"Joe Henry\", was launched in 1880 and was active until 1904.\n\nIn 1915 Henry was inducted into the Hall of Fame for Great Americans in the Bronx, New York.\n\nBronze statues of Henry and Isaac Newton represent science on the balustrade of the galleries of the Main Reading Room in the Thomas Jefferson Building of the Library of Congress on Capitol Hill in Washington, D.C. They are two of the 16 historical figures depicted in the reading room, each pair representing one of the 8 pillars of civilization.\n\nIn 1872 John Wesley Powell named a mountain range in southeastern Utah after Henry. The Henry Mountains were the last mountain range to be added to the map of the 48 contiguous U.S. states.\n\nAt Princeton, the Joseph Henry Laboratories and the Joseph Henry House are named for him.\n\nAfter the Albany Academy moved out of its downtown building in the early 1930s, its old building in Academy Park was renamed Joseph Henry Memorial, with a statue of him out front. It is now the main offices of the Albany City School District. In 1971 it was listed on the National Register of Historic Places; later it was included as a contributing property when the Lafayette Park Historic District was listed on the Register.\n\n\nElected a member of the American Antiquarian Society in 1851.\n\nThe District of Columbia named a school, built in 1878–80, on P Street between 6th and 7th the Joseph Henry School. It was demolished at some point after 1932.\n\nThe Henry Mountains (Utah) had been so named by geologist Almon Thompson in his honour.\n\n\n\n"}
{"id": "47707761", "url": "https://en.wikipedia.org/wiki?curid=47707761", "title": "Larry Paul Kelley", "text": "Larry Paul Kelley\n\nBorn in 1944 and raised in Hart, Michigan, Kelly was influenced by his father, Laurence, who worked on repairing auto bodies. His father had a set of Popular Science magazines which he read through often with much interest as a boy. As a 12-year-old boy Kelley did an experiment one day based on one of these magazine articles. He understood that ruby was a chemical mixture of aluminum oxide and chromium oxide. He observed that some sandpaper that his father used had aluminum oxide dust. Kelley took some of this dust and placed onto a firebrick. He put a chip of chrome with the aluminum oxide sandpaper dust, melted them with high heat, and serendipitously created his first ruby, albeit of poor quality.\n\nAs a teen, Kelley fostered his interest in science and experimentation. As a thirteen year old he shot homemade rockets over the local lake. He got into trouble with local police authorities when permission was denied to shoot these rockets so close to a heavily populated area.\n\nKelley and his brother Arthur made some small wooden workshops in 1961 to do experiments with chemicals. In 1962 when they blew up these workshops after mixing sulfur potassium chlorate with aluminum oxide dust the boys were seriously injured but survived.\n\nKelley enrolled at the University of Michigan in the late 1960s. There he took up the study of science, chemistry, and physics. One of his studies was working as an assistant chemist at Dow Chemical. At Dow he worked with high-temperature furnaces and processing of materials using heat. At that time there was a high demand for ruby for solid-state lasers. After graduating from U of M he went into the business of making equipment to produce ruby laser rods.\n\nKelley later learned how to cut the ruby laser rods into little chunks and then shape them into \"gems\" using a faceting machine. He then sold these synthetic artificial man-made gems for just under a hundred dollars each. He progressed from making rubies to manufacturing other man-made gems including an imitation diamond. Kelley turned this into a business of making man-made artificial gems for all the traditional birthstones of each of the twelve months.\n\nKelley experimented with several formulas and processes for making synthetic artificial gems. One method involved using temperature difference techniques to create crystals in a way water icicles are formed. One of these man-made gemstones made this way was a transparent clear crystal resembling glass. It looked to the naked eye like a real diamond when faceted correctly and no glass was involved in its manufacture.\n\nA second technique involves small pieces (\"seeds\") of mined gems put into a heated up chemical liquid. The liquid then is slowly cooled, and recrystallized to form larger pieces of the gemstone. Man-made gemstones grow from these \"seeds.\" Kelley's technical success in growing crystals led to the founding of the Shelby Gem Factory in 1970.\n\nIn 1978 Kelley invented a unique method to create thin semiconductor films, especially useful in solar cells. Semiconductors have an absorptive graphite substrate and a layer of silicon grid-work is plated onto it. Kelley's process uses a rotating drum. Surface tension is overcome by the whirling motion of the rotating drum to spread out the molten material. The centrifuge invention spreads out the crystalline thin film material evenly and causes the interface to have a low amount of silicon carbide. The idea is to have the thin film layers be single crystals.\n\nThe title to the patent is \"Centrifugal forming thin films and semiconductors and semiconductor devices\" and is United States patent #04101925. Craig A Hardy is shown as co-inventor. Kelley's invention is cited for reference in 7 other later patents for similarity.\n\n"}
{"id": "20078573", "url": "https://en.wikipedia.org/wiki?curid=20078573", "title": "Manuel Pino", "text": "Manuel Pino\n\nManuel Pino is a professor at Scottsdale Community College in Arizona, who comes from a village of the Tewa people west of Albuquerque, New Mexico. Opposition to uranium mining has played a central role in Pino's life. The theme for his sociology dissertation was \"The Destructive Impact of Uranium Mining on Native American Culture\". He has spoken at many international conferences, including the 1992 World Uranium Hearing in Salzburg, Austria, about victims of uranium mining, depleted uranium, and associated cancer deaths. In 2008, Manuel Pino received the Nuclear-Free Future Award.\n\n\n"}
{"id": "25658445", "url": "https://en.wikipedia.org/wiki?curid=25658445", "title": "Mass concentration (chemistry)", "text": "Mass concentration (chemistry)\n\nIn chemistry, the mass concentration (or ) is defined as the mass of a constituent divided by the volume of the mixture :\n\nFor a pure chemical the mass concentration equals its density (mass divided by volume); thus the mass concentration of a component in a mixture can be called the density of a component in a mixture. This explains the usage of (the lower case Greek letter rho), the symbol most often used for density.\n\nThe volume in the definition refers to the volume of the solution, \"not\" the volume of the solvent. One liter of a solution usually contains either slightly more or slightly less than 1 liter of solvent because the process of dissolution causes volume of liquid to increase or decrease. Sometimes the mass concentration is called titer.\n\nThe notation common with mass density underlines the connection between the two quantities (the mass concentration being the mass density of a component in the solution), but it can be a source of confusion especially when they appear in the same formula undifferentiated by an additional symbol (like a star superscript, a bolded symbol or varrho).\n\nMass concentration depends on the variation of the volume of the solution due mainly to thermal expansion. On small intervals of temperature the dependence is :\n\nwhere is the mass concentration at a reference temperature, is the thermal expansion coefficient of the mixture.\n\nThe sum of the mass concentrations of all components (including the solvent) gives the density of the solution:\n\nThus, for pure component the mass concentration equals the density of the pure component.\n\nThe sum of products between these quantities equals one.\n\nThe SI-unit for mass concentration is kg/m (kilogram/cubic metre). \nThis is the same as mg/mL and g/L. Another commonly used unit is g/(100 mL), which is identical to g/dL (gram/decilitre).\n\nIn biology, the \"%\" symbol is sometimes incorrectly used to denote mass concentration, also called \"mass/volume percentage.\" A solution with 1 g of solute dissolved in a final volume of 100 mL of solution would be labeled as \"1%\" or \"1% m/v\" (mass/volume). The notation is mathematically flawed because the unit \"%\" can only be used for dimensionless quantities. \"Percent solution\" or \"percentage solution\" are thus terms best reserved for \"mass percent solutions\" (m/m = m% = mass solute/mass total solution after mixing), or \"volume percent solutions\" (v/v = v% = volume solute per volume of total solution after mixing). The very ambiguous terms \"percent solution\" and \"percentage solutions\" with no other qualifiers, continue to occasionally be encountered. \n\nThis common usage of % to mean m/v in biology is because of many biological solutions being dilute and water-based or an aqueous solution. Liquid water has a density of approximately 1 g/cm (1 g/mL) (water density). Thus 100 mL of water is equal to approximately 100 g. Therefore, a solution with 1 g of solute dissolved in final volume of 100 mL aqueous solution may also be considered 1% m/m (1 g solute in 99 g water). This approximation breaks down as the solute concentration is increased. For an example, refer to the densities of water–NaCl mixtures (density of water with dissolved NaCl). High solute concentrations are often not physiologically relevant, but are occasionally encountered in pharmacology, where the mass per volume notation is still sometimes encountered. An extreme example is saturated solution of potassium iodide (SSKI) which attains 100 \"%\" m/v potassium iodide mass concentration (1 gram KI per 1 mL solution) only because the solubility of the dense salt KI is extremely high in water, and the resulting solution is very dense (1.72 times as dense as water). \n\nAlthough there are examples to the contrary, it should be stressed that the commonly used \"units\" of % w/v are grams/milliliters (g/mL). 1%  m/v solutions are sometimes thought of as being gram/100 mL but this detracts from the fact that % m/v is g/mL; 1 g of water has a volume of approximately 1 mL (at standard temperature and pressure) and the mass concentration is said to be 100%. To make 10 mL of an aqueous 1% cholate solution, 0.1 grams of cholate are dissolved in 10 mL of water. Volumetric flasks are the most appropriate piece of glassware for this procedure as deviations from ideal solution behavior can occur with high solute concentrations.\n\nIn solutions, mass concentration is commonly encountered as the ratio of mass/[volume solution], or m/v. In water solutions containing relatively small quantities of dissolved solute (as in biology), such figures may be \"percentivized\" by multiplying by 100 a ratio of grams solute per mL solution. The result is given as \"mass/volume percentage\". Such a convention expresses mass concentration of 1 gram of solute in 100 mL of solution, as \"1 m/v %.\"\n\nThe relation between mass concentration and density of a pure component (mass concentration of single component mixtures) is:\nwhere is the density of the pure component, the volume of the pure component before mixing.\n\nSpecific volume is the inverse of mass concentration only in the case of pure substances, for which mass concentration is the same as the density of the pure-substance:\n\nThe conversion to molar concentration is given by:\n\nwhere is the molar mass of constituent .\n\nThe conversion to mass fraction is given by:\n\nThe conversion to mole fraction is given by:\n\nwhere is the average molar mass of the mixture.\n\nFor binary mixtures, the conversion to molality is given by:\n\nThe values of (mass and molar) concentration different in space triggers the phenomenon of diffusion.\n"}
{"id": "7613285", "url": "https://en.wikipedia.org/wiki?curid=7613285", "title": "Megalithic architectural elements", "text": "Megalithic architectural elements\n\nThis article describes several characteristic architectural elements typical of European megalithic (Stone Age) structures.\n\nIn archaeology, a forecourt is the name given to the area in front of certain types of chamber tomb. Forecourts were probably the venue for ritual practices connected with the burial and commemoration of the dead in the past societies that built these types of tombs.\n\nIn European megalithic architecture, forecourts are curved in plan with the entrance to the tomb at the apex of the open semicircle enclosure that the forecourt creates. The sides were built up by either large upright stones or walls of smaller stones laid atop one another.\n\nSome also had paved floors and some had blocking stones erected in front of them to seal the tomb such as at West Kennet Long Barrow. Their shape, which suggests an attempt to focus attention on the tomb itself may mean that they were used ceremonially as a kind of open air auditorium during ceremonies. Excavation within some forecourts has recovered animal bone, pottery and evidence of burning suggesting that they served as locations for votive offerings or feasting dedicated to the dead.\n\nIn archaeology, kerb or peristalith is the name for a stone ring built to enclose and sometimes revet the cairn or barrow built over a chamber tomb.\n\nEuropean dolmens especially hunebed and dyss burials often provide examples of the use of kerbs in megalithic architecture but they were also added to other kinds of chamber tomb. Kerbs may be built in a dry stone wall method employing small blocks or more commonly using larger stones set in the ground. When larger stones are employed, peristalith is the term more properly used. Often, when the earth barrow has been weathered away, the surviving kerb can give the impression of being a stone circle although these monuments date from considerably later. \nExcavation of barrows without stone rings such as Fussell's Lodge in Wiltshire suggests that, in these examples, timber or turf was used to define a kerb instead.\n\nIn the British Isles, the enclosing nature of kerbs has been suggested to be analogous to later Neolithic and Bronze Age stone and timber circles and henges which also demonstrate an attempt to demarcate a distinct, round area for ritual or funerary purposes. Famous sites with kerbs include Newgrange where many of the stones are etched with megalithic art. An example of the dry stone wall type of kerb can be seen at Parc le Breos in Wales.\n\nAn orthostat is a large stone with a more or less slab-like shape that has been artificially set upright (so a cube-shaped block is not an orthostat). Menhirs and other standing stones are technically orthostats although the term is used by archaeologists only to describe individual prehistoric stones that constitute part of larger structures. Common examples include the walls of chamber tombs and other megalithic monuments and the vertical elements of the trilithons at Stonehenge. Especially later, orthostats may be carved with decoration in relief, a common feature of Hittite architecture and Assyrian sculpture among other styles. In the latter case, orthostats are large thin slabs of gypsum neatly and carefully formed, for use as a wall-facing secured by metal fixings and carrying reliefs, which were then painted. \n\nMany orthostats were a focus for megalithic art, as at Knowth in Ireland.\n\nIn megalithic archaeology a port-hole slab is the name of an orthostat with a hole in it sometimes found forming the entrance to a chamber tomb. The hole is usually circular but square examples or those made from two adjoining slabs each with a notch cut in it are known. They are common in the gallery graves of the Seine-Oise-Marne culture.\n\nPortal stones are a pair of Megalithic orthostats, usually flanking the entrance to a chamber tomb. They are commonly found in dolmens. Examples may be seen at Bohonagh and Knocknakilla.\n\nA trilithon (or trilith) is a structure consisting of two large vertical stones supporting a third stone set horizontally across the top. Commonly used in the context of megalithic monuments, the most famous trilithons are those at Stonehenge and those found in the Megalithic Temples of Malta.\n\nThe word \"trilithon\" is derived from the Greek 'having three stones' (Tri - \"three\", lithos - \"stone\") and was first used by William Stukeley. The term also describes the groups of three stones in the Hunebed tombs of the Netherlands and the three massive stones forming part of the wall of the Roman Temple of Jupiter at Baalbek, Lebanon.\n\n\n\n"}
{"id": "6966559", "url": "https://en.wikipedia.org/wiki?curid=6966559", "title": "Nanocrystalline material", "text": "Nanocrystalline material\n\nA nanocrystalline (NC) material is a polycrystalline material with a crystallite size of only a few nanometers. These materials fill the gap between amorphous materials without any long range order and conventional coarse-grained materials. Definitions vary, but nanocrystalline material is commonly defined as a crystallite (grain) size below 100 nm. Grain sizes from 100–500 nm are typically considered \"ultrafine\" grains. \n\nThe grain size of a NC sample can be estimated using x-ray diffraction. In materials with very small grain sizes, the diffraction peaks will be broadened. This broadening can be related to a crystallite size using the Scherrer equation (applicable up to ~50 nm), a Williamson-Hall plot, or more sophisticated methods such as the Warren-Averbach method or computer modeling of the diffraction pattern. The crystallite size can be measured directly using transmission electron microscopy.\n\nNanocrystalline materials can be prepared in several ways. Methods are typically categorized based on the phase of matter the material transitions through before forming the nanocrystalline final product.\n\nSolid-state processes do not involve melting or evaporating the material and are typically done at relatively low temperatures. Examples of solid state processes include mechanical alloying using a high-energy ball mill and certain types of severe plastic deformation processes. \n\nNanocrystalline metals can be produced by rapid solidification from the liquid using a process such as melt spinning. This often produces an amorphous metal, which can be transformed into an nanocrystalline metal by annealing above the crystallization temperature.\n\nThin films of nanocrystalline materials can be produced using vapor deposition processes such as MOCVD. \n\nSome metals, particularly nickel and nickel alloys, can be made into nanocrystalline foils using electrodeposition. \n"}
{"id": "229745", "url": "https://en.wikipedia.org/wiki?curid=229745", "title": "Orders of magnitude (power)", "text": "Orders of magnitude (power)\n\nThis page lists examples of the power in watts produced by various sources of energy. They are grouped by orders of magnitude.\n\nThe productive capacity of electrical generators operated by utility companies is often measured in MW. Few things can sustain the transfer or consumption of energy on this scale; some of these events or entities include: lightning strikes, naval craft (such as aircraft carriers and submarines), engineering hardware, and some scientific research equipment (such as supercolliders and large lasers).\n\nFor reference, about 10,000 100-watt lightbulbs or 5,000 computer systems would be needed to draw 1 MW. Also, 1 MW is approximately 1360 horsepower. Modern high-power diesel-electric locomotives typically have a peak power of 3–5 MW, while a typical modern nuclear power plant produces on the order of 500–2000 MW peak output.\n\n"}
{"id": "35930284", "url": "https://en.wikipedia.org/wiki?curid=35930284", "title": "Outline of solar energy", "text": "Outline of solar energy\n\nThe following outline is provided as an overview of and topical guide to solar energy:\n\nSolar energy – radiant light and heat from the sun. It has been harnessed by humans since ancient times using a range of ever-evolving technologies. Solar energy technologies include solar heating, solar photovoltaics, solar thermal electricity and solar architecture, which can make considerable contributions to solving some of the most urgent problems the world now faces.\nSolar energy can be described as all of the following:\n\n\n\nSolar power – the conversion of sunlight into electricity, either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).\n\nSolar thermal energy (STE) – technology for harnessing solar energy for thermal energy (heat).\n\nConcentrated solar power – a system that uses mirrors or lenses to concentrate a large area of sunlight, or solar thermal energy, onto a small area. Electrical power is produced when the concentrated light is converted to heat, which drives a heat engine (usually a steam turbine) connected to an electrical power generator.\n\nPhotovoltaics –\n\nPhotovoltaic system –\n\n\n\n\nPhotovoltaic power station –\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "29684671", "url": "https://en.wikipedia.org/wiki?curid=29684671", "title": "Photovoltaic keyboard", "text": "Photovoltaic keyboard\n\nA photovoltaic keyboard is a wireless computer keyboard that charges its batteries from a light source such as the sun or interior lighting, addressing a major drawback of wireless computer peripherals that otherwise require regular replacement of discharged batteries.\n\nThe Logitech K750 has a set of photovoltaic cells on the top edge, charges in sunlight or under a standard bulb, can work up to three months in total darkness, and includes software to display battery charging status. It is a fullsized keyboard, including the usual movement keys and NumPad section on the right side, with low-profile keys much like a laptop. There are two models, compatible with Windows or Macintosh operating systems.\n\nEven though the keyboard is not officially supported in Linux, a third party application named Solaar provides functionality akin to the original Logitech software, such as battery and connection status indications, and allows device pairing/unpairing.\n\nAnother Logitech keyboard, the K760, is also PV powered, is highly useful. It is smaller and communicates with the computer via Bluetooth.\n\n"}
{"id": "1685182", "url": "https://en.wikipedia.org/wiki?curid=1685182", "title": "Quarkonium", "text": "Quarkonium\n\nIn particle physics, quarkonium (from quark and -onium, pl. quarkonia) designates a flavorless meson whose constituents are a heavy quark and its own antiquark, making it a neutral particle and the antiparticle of themselves.\n\nLight quarks (up, down, and strange) are much less massive than the heavier quarks, and so the physical states actually seen in experiments (η, η′, and π mesons) are quantum mechanical mixtures of the light quark states. The much larger mass differences between the charm and bottom quarks and the lighter quarks results in states that are well defined in terms of a quark–antiquark pair of a given flavor.\n\nExamples of quarkonia are the J/ψ meson (the ground state of charmonium, ) and the meson (bottomonium, ). Because of the high mass of the top quark, toponium does not exist, since the top quark decays through the electroweak interaction before a bound state can form (a rare example of a weak process proceeding more quickly than a strong process). Usually, the word \"quarkonium\" refers only to charmonium and bottomonium, and not to any of the lighter quark–antiquark states.\n\nIn the following table, the same particle can be named with the spectroscopic notation or with its mass. In some cases excitation series are used: Ψ' is the first excitation of Ψ (for historical reasons, this one is called \"J/ψ\" particle); Ψ\" is a second excitation, and so on. That is, names in the same cell are synonymous.\n\nSome of the states are predicted, but have not been identified; others are unconfirmed. The quantum numbers of the X(3872) particle have been measured recently by the LHCb experiment at CERN\n. This measurement shed some light on its identity, excluding the third option among the three envised, which are :\n\nIn 2005, the BaBar experiment announced the discovery of a new state: Y(4260). CLEO and Belle have since corroborated these observations. At first, Y(4260) was thought to be a charmonium state, but the evidence suggests more exotic explanations, such as a D \"molecule\", a 4-quark construct, or a hybrid meson.\n\nIn the following table, the same particle can be named with the spectroscopic notation or with its mass.\n\nSome of the states are predicted, but have not been identified; others are unconfirmed.\n\nThe Υ(1S) state was discovered by the E288 experiment team, headed by Leon Lederman, at Fermilab in 1977, and was the first particle containing a bottom quark to be discovered. The χ (3P) state was the first particle discovered in the Large Hadron Collider. The article about this discovery was first submitted to arXiv on 21 December 2011. On April 2012, Tevatron's DØ experiment confirms the result in a paper published in \"Phys. Rev. D\".\n\nThe theta meson is not expected to be physically observable, as top quarks decay too fast to form mesons. \n\nThe computation of the properties of mesons in Quantum chromodynamics (QCD) is a fully non-perturbative one. As a result, the only general method available is a direct computation using lattice QCD (LQCD) techniques. However, other techniques are effective for heavy quarkonia as well.\n\nThe light quarks in a meson move at relativistic speeds, since the mass of the bound state is much larger than the mass of the quark. However, the speed of the charm and the bottom quarks in their respective quarkonia is sufficiently smaller, so that relativistic effects affect these states much less. It is estimated that the speed, v, is roughly 0.3 times the speed of light for charmonia and roughly 0.1 times the speed of light for bottomonia. The computation can then be approximated by an expansion in powers of v/c and v/c. This technique is called non-relativistic QCD (NRQCD).\n\nNRQCD has also been quantized as a lattice gauge theory, which provides another technique for LQCD calculations to use. Good agreement with the bottomonium masses has been found, and this provides one of the best non-perturbative tests of LQCD. For charmonium masses the agreement is not as good, but the LQCD community is actively working on improving their techniques. Work is also being done on calculations of such properties as widths of quarkonia states and transition rates between the states.\n\nAn early, but still effective, technique uses models of the \"effective\" potential to calculate masses of quarkonia states. In this technique, one uses the fact that the motion of the quarks that comprise the quarkonium state is non-relativistic to assume that they move in a static potential, much like non-relativistic models of the hydrogen atom. One of the most popular potential models is the so-called \"Cornell\" (or \"funnel\") \"potential\"\n\nwhere formula_3 is the effective radius of the quarkonium state, formula_4 and formula_5 are parameters. This potential has two parts. The first part, formula_6 corresponds to the potential induced by one-gluon exchange between the quark and its anti-quark, and is known as the \"Coulombic\" part of the potential, since its formula_7 form is identical to the well-known Coulombic potential induced by the electromagnetic force. The second part, formula_8, is known as the \"confinement\" part of the potential, and parameterizes the poorly understood non-perturbative effects of QCD. Generally, when using this approach, a convenient form for the wave function of the quarks is taken, and then formula_4 and formula_5 are determined by fitting the results of the calculations to the masses of well-measured quarkonium states. Relativistic and other effects can be incorporated into this approach by adding extra terms to the potential, much in the same way that they are for the hydrogen atom in non-relativistic quantum mechanics. This form has been derived from QCD up to formula_11 by Y. Sumino in 2003. It is popular because it allows for accurate predictions of quarkonia parameters without a lengthy lattice computation, and provides a separation between the short-distance \"Coulombic\" effects and the long-distance \"confinement\" effects that can be useful in understanding the quark/anti-quark force generated by QCD.\n\nQuarkonia have been suggested as a diagnostic tool of the formation of the quark–gluon plasma: both disappearance and enhancement of their formation depending on the yield of heavy quarks in plasma can occur.\n\n"}
{"id": "41006269", "url": "https://en.wikipedia.org/wiki?curid=41006269", "title": "Regasification", "text": "Regasification\n\nRegasification is a process of converting liquefied natural gas (LNG) at −162 °C (−260 °F) temperature back to natural gas at atmospheric temperature. LNG gasification plants can be located on land as well as on floating barges. Floating barge mounted plants have the advantage that they can be towed to new offshore locations for better usage in response to changes in the business environment. In a conventional , LNG is heated by sea water to convert it to natural gas / methane gas.\n\nIn addition to regasification, many valuable industrial byproducts can be produced using cold energy of LNG. Cold energy of LNG utilisation for extracting liquid oxygen and nitrogen gas from air, makes LNG-regasification plants more viable when they are located near integrated steel plants and/or urea plants. Cold energy of LNG usage in lieu of massive and energy intensive cryogenic refrigeration units in natural-gas processing plants is also more viable economically. The natural gas processed with cold energy of LNG and the imported LNG can be readily injected into a conventional natural gas distribution system to reach the ultimate consumers.\n\nThe cold energy of LNG can be used for cooling the exhaust fluid of the gas turbine which is working in closed joule cycle with Argon gas as fluid. Thus near 100% conversion efficiency to electricity is achieved for the LNG/natural gas consumed by the gas turbine as its exhaust heat is fully used/absorbed for the gasification of LNG.\n\nHowever, the abundant availability of natural gas, and the mature technology and its acceptability in using the LNG directly (without regasification) in road and rail vehicles would lead to lesser demand for LNG regasification plants.\n\n\n"}
{"id": "35093960", "url": "https://en.wikipedia.org/wiki?curid=35093960", "title": "River of Renewal", "text": "River of Renewal\n\nRiver of Renewal: Myth and History in the Klamath Basin is a 2006 book by Stephen Most detailing the challenges in balancing economic and ecological concerns in the Klamath Basin region of the United States. The book shows clashes between federal and state government agencies, American Indian tribes, hydroelectric dam operators and the farming and commercial fishery industries, detailing challenges and controversies around the irrigation of farmland and the preservation of the wild salmon population.\nThe book also traces the history of the Klamath Basin, including the Yurok, Hupa, and Karuk tribal populations, the secessionist State of Jefferson movement, and regional Bigfoot legends.\n\nIt was adapted into a 2008 non-fiction film, River of Renewal, that received the Best Documentary Award at the American Indian Film Festival. The film was also broadcast on PBS.\n\n"}
{"id": "34564410", "url": "https://en.wikipedia.org/wiki?curid=34564410", "title": "Ronald James McLean", "text": "Ronald James McLean\n\nRonald James McLean (22 January 1914–26 May 1980) was a New Zealand farmer, aviator, community leader and environmental campaigner. He was born in Wyndham, Southland, New Zealand on 22 January 1914.\n"}
{"id": "27558", "url": "https://en.wikipedia.org/wiki?curid=27558", "title": "Salt (chemistry)", "text": "Salt (chemistry)\n\nIn chemistry, a salt is an ionic compound that can be formed by the neutralization reaction of an acid and a base. Salts are composed of related numbers of cations (positively charged ions) and anions (negative ions) so that the product is electrically neutral (without a net charge). These component ions can be inorganic, such as chloride (Cl), or organic, such as acetate (); and can be monatomic, such as fluoride (F), or polyatomic, such as sulphate ().\n\nSalts can be classified in a variety of ways. Salts that produce hydroxide ions when dissolved in water are called alkali salts. Salts that produce acidic solutions are \"acidic salts\". \"Neutral salts\" are those salts that are neither acidic nor basic. Zwitterions contain an anionic and a cationic centres in the same molecule, but are not considered to be salts. Examples of zwitterions include amino acids, many metabolites, peptides, and proteins.\n\nSolid salts tend to be transparent as illustrated by sodium chloride. In many cases, the apparent opacity or transparency are only related to the difference in size of the individual monocrystals. Since light reflects from the grain boundaries (boundaries between crystallites), larger crystals tend to be transparent, while the polycrystalline aggregates look like white powders.\n\nSalts exist in many different colors, which arise either from the anions or cations. For example:\n\nFew minerals are salts because they would be solubilized by water. Similarly inorganic pigments tend not to be salts, because insolubility is required for fastness. Some organic dyes are salts, but they are virtually insoluble in water.\n\nDifferent salts can elicit all five basic tastes, e.g., salty (sodium chloride), sweet (lead diacetate, which will cause lead poisoning if ingested), sour (potassium bitartrate), bitter (magnesium sulfate), and umami or savory (monosodium glutamate).\n\nSalts of strong acids and strong bases (\"strong salts\") are non-volatile and often odorless, whereas salts of either weak acids or weak bases (\"weak salts\") may smell like the conjugate acid (e.g., acetates like acetic acid (vinegar) and cyanides like hydrogen cyanide (almonds)) or the conjugate base (e.g., ammonium salts like ammonia) of the component ions. That slow, partial decomposition is usually accelerated by the presence of water, since hydrolysis is the other half of the reversible reaction equation of formation of weak salts.\n\nMany ionic compounds exhibit significant solubility in water or other polar solvents. Unlike molecular compounds, salts dissociate in solution into anionic and cationic components. \nThe lattice energy, the cohesive forces between these ions within a solid, determines the solubility. The solubility is dependent on how well each ion interacts with the solvent, so certain patterns become apparent. For example, salts of sodium, potassium and ammonium are usually soluble in water. Notable exceptions include ammonium hexachloroplatinate and potassium cobaltinitrite. Most nitrates and many sulfates are water-soluble. Exceptions include barium sulfate, calcium sulfate (sparingly soluble), and lead(II) sulfate, where the 2+/2− pairing leads to high lattice energies. For similar reasons, most alkali metal carbonates are not soluble in water. Some soluble carbonate salts are: sodium carbonate, potassium carbonate and ammonium carbonate.\n\nSalts are characteristically insulators. Molten salts or solutions of salts conduct electricity. For this reason, liquified (molten) salts and solutions containing dissolved salts (e.g., sodium chloride in water) are called electrolytes.\n\nSalts characteristically have high melting points. For example, sodium chloride melts at 801 °C. Some salts with low lattice energies are liquid at or near room temperature. These include molten salts, which are usually mixtures of salts, and ionic liquids, which usually contain organic cations. These liquids exhibit unusual properties as solvents.\n\nThe name of a salt starts with the name of the cation (e.g., \"sodium\" or \"ammonium\") followed by the name of the anion (e.g., \"chloride\" or \"acetate\"). Salts are often referred to only by the name of the cation (e.g., \"sodium salt\" or \"ammonium salt\") or by the name of the anion (e.g., \"chloride salt\" or \"acetate salt\").\n\nCommon salt-forming cations include:\n\n\nCommon salt-forming anions (parent acids in parentheses where available) include:\n\n\nSalts with varying number of hydrogen atoms, with respect to the parent acid, replaced by cations can be referred to as \"monobasic\", \"dibasic\" or \"tribasic\" salts (\"polybasic\" salts refer to those with more than one hydrogen atom replaced):\n\n\nSalts are formed by a chemical reaction between:\n\n\nStrong salts or strong electrolyte salts are chemical salts composed of strong electrolytes. These ionic compounds dissociate completely in water. They are generally odourless and nonvolatile.\n\nStrong salts start with Na__, K__, NH__, or they end with __NO, __ClO, or __CHCOO. Most group 1 and 2 metals form strong salts. Strong salts are especially useful when creating conductive compounds as their constituent ions allow for greater conductivity.\n\nWeak salts or \"weak electrolyte salts\" are, as the name suggests, composed of weak electrolytes. They are generally more volatile than strong salts. They may be similar in odor to the acid or base they are derived from. For example, sodium acetate, NaCHCOO, smells similar to acetic acid CHCOOH.\n\n"}
{"id": "29469", "url": "https://en.wikipedia.org/wiki?curid=29469", "title": "Sapphire", "text": "Sapphire\n\nSapphire is a precious gemstone, a variety of the mineral corundum, consisting of aluminium oxide () with trace amounts of elements such as iron, titanium, chromium, copper, or magnesium. It is typically blue, but natural \"fancy\" sapphires also occur in yellow, purple, orange, and green colors; \"parti sapphires\" show two or more colors. The only color that sapphire cannot be is red – as red colored corundum is called ruby, another corundum variety. Pink colored corundum may be either classified as ruby or sapphire depending on locale. \nCommonly, natural sapphires are cut and polished into gemstones and worn in jewelry. They also may be created synthetically in laboratories for industrial or decorative purposes in large crystal boules. Because of the remarkable hardness of sapphires – 9 on the Mohs scale (the third hardest mineral, after diamond at 10 and moissanite at 9.5) – sapphires are also used in some non-ornamental applications, such as infrared optical components, high-durability windows, wristwatch crystals and movement bearings, and very thin electronic wafers, which are used as the insulating substrates of very special-purpose solid-state electronics (especially integrated circuits and GaN-based LEDs).\n\nSapphire is the birthstone for September and the gem of the 45th anniversary. A sapphire jubilee occurs after 65 years.\n\nSapphire is one of the two gem-varieties of corundum, the other being ruby (defined as corundum in a shade of red). Although blue is the best-known sapphire color, they occur in other colors, including gray and black, and they can be colorless. A pinkish orange variety of sapphire is called padparadscha.\n\nSignificant sapphire deposits are found in Eastern Australia, Thailand, Sri Lanka, China (Shandong), Madagascar, East Africa, and in North America in a few locations, mostly in Montana. Sapphire and rubies are often found in the same geological setting.\n\nEvery sapphire mine produces a wide range of quality, and origin is not a guarantee of quality. For sapphire, Kashmir receives the highest premium, although Burma, Sri Lanka, and Madagascar also produce large quantities of fine quality gems.\n\nThe cost of natural sapphires varies depending on their color, clarity, size, cut, and overall quality. For gems of exceptional quality, an independent determination from a respected laboratory such as the GIA, AGL or Gubelin of origin often adds to value.\n\nGemstone color can be described in terms of hue, saturation, and tone. Hue is commonly understood as the \"color\" of the gemstone. Saturation refers to the vividness or brightness of the hue, and tone is the lightness to darkness of the hue. Blue sapphire exists in various mixtures of its primary (blue) and secondary hues, various tonal levels (shades) and at various levels of saturation (vividness).\n\nBlue sapphires are evaluated based upon the purity of their primary hue. Purple, violet, and green are the most common secondary hues found in blue sapphires. Violet and purple can contribute to the overall beauty of the color, while green is considered to be distinctly negative. Blue sapphires with up to 15% violet or purple are generally said to be of fine quality. Gray is the normal saturation modifier or mask found in blue sapphires. Gray reduces the saturation or brightness of the hue, and therefore has a distinctly negative effect.\n\nThe color of fine blue sapphires may be described as a vivid medium dark violet to purplish blue where the primary blue hue is at least 85% and the secondary hue no more than 15%, without the least admixture of a green secondary hue or a gray mask.\n\nThe Logan sapphire in the National Museum of Natural History, in Washington, D.C., is one of the largest faceted gem-quality blue sapphires in existence.\n\nSapphires in colors other than blue are called \"fancy\" or \"parti colored\" sapphires.\n\nFancy sapphires are often found in yellow, orange, green, brown, purple and violet hues.\n\nParticolored sapphires are those stones which exhibit two or more colors within a single stone. Australia is the largest source of particolored sapphires; they are not commonly used in mainstream jewelry and remain relatively unknown. Particolored sapphires cannot be created synthetically and only occur naturally.\n\nColorless sapphires have historically been used as diamond substitutes in jewelry.\n\nPink sapphires occur in shades from light to dark pink, and deepen in color as the quantity of chromium increases. The deeper the pink color, the higher their monetary value. In the United States, a minimum color saturation must be met to be called a ruby, otherwise the stone is referred to as a \"pink sapphire\".\n\n\"Padparadscha\" is a delicate, light to medium toned, pink-orange to orange-pink hued corundum, originally found in Sri Lanka, but also found in deposits in Vietnam and parts of East Africa. Padparadscha sapphires are rare; the rarest of all is the totally natural variety, with no sign of artificial treatment.\n\nThe name is derived from the Sanskrit \"padma ranga\" (padma = lotus; ranga = color), a color akin to the lotus flower (\"Nelumbo nucifera\").\n\nNatural padparadscha sapphires often draw higher prices than many of even the finest blue sapphires. Recently, more sapphires of this color have appeared on the market as a result of a new artificial treatment method called \"lattice diffusion\".\n\nA \"star sapphire\" is a type of sapphire that exhibits a star-like phenomenon known as asterism; red stones are known as \"star rubies\". Star sapphires contain intersecting needle-like inclusions following the underlying crystal structure that causes the appearance of a six-rayed \"star\"-shaped pattern when viewed with a single overhead light source. The inclusion is often the mineral rutile, a mineral composed primarily of titanium dioxide. The stones are cut \"en cabochon\", typically with the center of the star near the top of the dome. Occasionally, twelve-rayed stars are found, typically because two different sets of inclusions are found within the same stone, such as a combination of fine needles of rutile with small platelets of hematite; the first results in a whitish star and the second results in a golden-colored star. During crystallisation, the two types of inclusions become preferentially oriented in different directions within the crystal, thereby forming two six-rayed stars that are superimposed upon each other to form a twelve-rayed star. Misshapen stars or 12-rayed stars may also form as a result of twinning.\nThe inclusions can alternatively produce a \"cat's eye\" effect if the 'face-up' direction of the cabochon's dome is oriented perpendicular to the crystal's c-axis rather than parallel to it. If the dome is oriented in between these two directions, an 'off-center' star will be visible, offset away from the high point of the dome.\n\nThe Star of Adam is the largest blue star sapphire which weighs 1404.49 carats. The gem was mined in the city of Ratnapura, southern Sri Lanka. The Black Star of Queensland, the second largest gem-quality star sapphire in the world, weighs 733 carats. The Star of India mined in Sri Lanka and weighing 563.4 carats is thought to be the third-largest star sapphire, and is currently on display at the American Museum of Natural History in New York City. The 182-carat Star of Bombay, mined in Sri Lanka and located in the National Museum of Natural History in Washington, D.C., is another example of a large blue star sapphire. The value of a star sapphire depends not only on the weight of the stone, but also the body color, visibility, and intensity of the asterism.\n\nA rare variety of natural sapphire, known as color-change sapphire, exhibits different colors in different light. Color change sapphires are blue in outdoor light and purple under incandescent indoor light, or green to gray-green in daylight and pink to reddish-violet in incandescent light. Color change sapphires come from a variety of locations, including Thailand and Tanzania. The color-change effect is caused by the interaction of the sapphire, which absorbs specific wavelengths of light, and the light-source, whose spectral output varies depending upon the illuminant. Transition-metal impurities in the sapphire, such as chromium and vanadium, are responsible for the color change.\n\nCertain synthetic color-change sapphires have a similar color change to the natural gemstone alexandrite and they are sometimes marketed as \"alexandrium\" or \"synthetic alexandrite\". However, the latter term is a misnomer: synthetic color-change sapphires are, technically, not synthetic alexandrites but rather alexandrite \"simulants\". This is because genuine alexandrite is a variety of chrysoberyl: not sapphire, but an entirely different mineral.\n\nRubies are corundum which contain chromium impurities that absorb yellow-green light and result in deeper ruby red color with increasing content. Purple sapphires contain trace amounts of vanadium and come in a variety of shades. Corundum that contains ~0.01% of titanium is colorless. If trace amounts of iron are present, a very pale yellow to green color may be seen. However, if both titanium and iron impurities are present together, and in the correct valence states, the result is a deep-blue color.\n\nUnlike localized (\"intra-atomic\") absorption of light which causes color for chromium and vanadium impurities, blue color in sapphires comes from intervalence charge transfer, which is the transfer of an electron from one transition-metal ion to another via the conduction or valence band. The iron can take the form Fe or Fe, while titanium generally takes the form Ti. If Fe and Ti ions are substituted for Al, localized areas of charge imbalance are created. An electron transfer from Fe and Ti can cause a change in the valence state of both. Because of the valence change there is a specific change in energy for the electron, and electromagnetic energy is absorbed. The wavelength of the energy absorbed corresponds to yellow light. When this light is subtracted from incident white light, the complementary color blue results. Sometimes when atomic spacing is different in different directions there is resulting blue-green dichroism.\n\nIntervalence charge transfer is a process that produces a strong colored appearance at a low percentage of impurity. While at least 1% chromium must be present in corundum before the deep red ruby color is seen, sapphire blue is apparent with the presence of only 0.01% of titanium and iron.\n\nSapphires can be treated by several methods to enhance and improve their clarity and color. It is common practice to heat natural sapphires to improve or enhance color. This is done by heating the sapphires in furnaces to temperatures between for several hours, or by heating in a nitrogen-deficient atmosphere oven for seven days or more. Upon heating, the stone becomes more blue in color, but loses some of the rutile inclusions (silk). When high temperatures are used, the stone loses all silk (inclusions) and it becomes clear under magnification. The inclusions in natural stones are easily seen with a jeweler's loupe. Evidence of sapphire and other gemstones being subjected to heating goes back at least to Roman times. Un-heated natural stones are somewhat rare and will often be sold accompanied by a certificate from an independent gemological laboratory attesting to \"no evidence of heat treatment\".\nYogo sapphires sometimes do not need heat treating because their cornflower blue coloring is uniform and deep, they are generally free of the characteristic inclusions, and they have high uniform clarity. When Intergem Limited began marketing the Yogo in the 1980s as the world's only guaranteed untreated sapphire, heat treatment was not commonly disclosed; by 1982 the heat treatment became a major issue. At that time, 95% of all the world's sapphires were being heated to enhance their natural color. Intergem's marketing of guaranteed untreated Yogos set them against many in the gem industry. This issue appeared as a front-page story in the \"Wall Street Journal\" on 29 August 1984 in an article by Bill Richards, \"Carats and Schticks: Sapphire Marketer Upsets The Gem Industry\".\n\nDiffusion treatments are used to add impurities to the sapphire to enhance color. Typically beryllium is diffused into a sapphire under very high heat, just below the melting point of the sapphire. Initially (\"c.\" 2000) orange sapphires were created, although now the process has been advanced and many colors of sapphire are often treated with beryllium. The colored layer can be removed when stones chip or are repolished or refaceted, depending on the depth of the impurity layer. Treated padparadschas may be very difficult to detect, and many stones are certified by gemological labs (\"e.g.\", Gubelin, SSEF, AGTA).\n\nAccording to United States Federal Trade Commission guidelines, disclosure is required of any mode of enhancement that has a significant effect on the gem's value.\n\nThere are several ways of treating sapphire. Heat-treatment in a reducing or oxidising atmosphere (but without the use of any other added impurities) is commonly used to improve the color of sapphires, and this process is sometimes known as \"heating only\" in the gem trade. In contrast, however, heat treatment combined with the deliberate addition of certain specific impurities (e.g. beryllium, titanium, iron, chromium or nickel, which are absorbed into the crystal structure of the sapphire) is also commonly performed, and this process can be known as \"diffusion\" in the gem trade. However, despite what the terms \"heating only\" and \"diffusion\" might suggest, both of these categories of treatment actually involve diffusion processes.\n\nSapphires are mined from alluvial deposits or from primary underground workings. Commercial mining locations for sapphire and ruby include (but are not limited to) the following countries: Afghanistan, Australia, Myanmar/Burma, Cambodia, China, Colombia, India, Kenya, Laos, Madagascar, Malawi, Nepal, Nigeria, Pakistan, Sri Lanka, Tajikistan, Tanzania, Thailand, United States, and Vietnam. Sapphires from different geographic locations may have different appearances or chemical-impurity concentrations, and tend to contain different types of microscopic inclusions. Because of this, sapphires can be divided into three broad categories: classic metamorphic, non-classic metamorphic or magmatic, and classic magmatic.\n\nSapphires from certain locations, or of certain categories, may be more commercially appealing than others, particularly classic metamorphic sapphires from Kashmir, Burma, or Sri Lanka that have not been subjected to heat-treatment.\n\nThe Logan sapphire, the Star of India, and the Star of Bombay originate from Sri Lankan mines. Madagascar is the world leader in sapphire production (as of 2007) specifically its deposits in and around the town of Ilakaka. Prior to the opening of the Ilakaka mines, Australia was the largest producer of sapphires (such as in 1987). In 1991 a new source of sapphires was discovered in Andranondambo, southern Madagascar. That area has been exploited for its sapphires started in 1993, but it was practically abandoned just a few years later—because of the difficulties in recovering sapphires in their bedrock.\n\nIn North America, sapphires have been mined mostly from deposits in Montana: fancies along the Missouri River near Helena, Montana, Dry Cottonwood Creek near Deer Lodge, Montana, and Rock Creek near Philipsburg, Montana. Fine blue Yogo sapphires are found at Yogo Gulch west of Lewistown, Montana. A few gem-grade sapphires and rubies have also been found in the area of Franklin, North Carolina.\n\nThe sapphire deposits of Kashmir are well known in the gem industry, although their peak production took place in a relatively short period at the end of the nineteenth and early twentieth centuries. They have a superior cornflower blue hue to them with a mysterious and almost sleepy quality, described by some gem enthusiasts as ‘blue velvet”. Kashmir-origin contributes meaningfully to the value of a sapphire, and most corundum of Kashmir origin can be readily identified by its characteristic silky appearance and exceptional hue. The unique blue appears lustrous under any kind of light, unlike non-Kashmir sapphires which may appear purplish or grayish in comparison. Sotheby's has been in the forefront overseeing record-breaking sales of Kashmir sapphires worldwide. In October 2014, Sotheby’s Hong Kong achieved consecutive per-carat price records for Kashmir sapphires - first with the 12.00 carat Cartier sapphire ring at US$193,975 per carat, then with a 17.16 carat sapphire at US$236,404, and again in June 2015 when the per-carat auction record was set at US$240,205. At present, the world record price-per-carat for sapphire at auction is held by a sapphire from Kashmir in a ring, which sold in October 2015 for approximately US$242,000 per carat (HK$52,280,000 in total, including buyer's premium, or more than US$6.74 million).\n\nIn 1902, the French chemist Auguste Verneuil developed a process for producing synthetic sapphire crystals. In the Verneuil process, named after him, fine alumina powder is added to an oxyhydrogen flame, and this is directed downward against a mantle. The alumina in the flame is slowly deposited, creating a teardrop shaped \"boule\" of sapphire material. Chemical dopants can be added to create artificial versions of the ruby, and all the other natural colors of sapphire, and in addition, other colors never seen in geological samples. Artificial sapphire material is identical to natural sapphire, except it can be made without the flaws that are found in natural stones. The disadvantage of Verneuil process is that the grown crystals have high internal strains. Many methods of manufacturing sapphire today are variations of the Czochralski process, which was invented in 1916 by Polish chemist Jan Czochralski. In this process, a tiny sapphire seed crystal is dipped into a crucible made of the precious metal iridium or molybdenum, containing molten alumina, and then slowly withdrawn upward at a rate of 1 to 100 mm per hour. The alumina crystallizes on the end, creating long carrot-shaped boules of large size up to 200 kg in mass.\n\nSynthetic sapphire is also produced industrially from agglomerated aluminium oxide, sintered and fused (such as by hot isostatic pressing) in an inert atmosphere, yielding a transparent but slightly porous polycrystalline product.\n\nIn 2003, the world's production of synthetic sapphire was 250 tons (1.25 × 10 carats), mostly by the United States and Russia. The availability of cheap synthetic sapphire unlocked many industrial uses for this unique material.\n\nThe first laser was made with a rod of synthetic ruby. Titanium-sapphire lasers are popular due to their relatively rare capacity to be tuned to various wavelengths in the red and near-infrared region of the electromagnetic spectrum. They can also be easily mode-locked. In these lasers a synthetically produced sapphire crystal with chromium or titanium impurities is irradiated with intense light from a special lamp, or another laser, to create stimulated emission.\n\nSynthetic sapphire—sometimes referred to as \"sapphire glass\"—is commonly used as a window material, because it is both highly transparent to wavelengths of light between 150 nm (UV) and 5500 nm (IR) (the visible spectrum extends about 380 nm to 750 nm), and extraordinarily scratch-resistant.\n\nThe key benefits of sapphire windows are:\nSome sapphire-glass windows are made from pure sapphire boules that have been grown in a specific crystal orientation, typically along the optical axis, the c-axis, for minimum birefringence for the application.\n\nThe boules are sliced up into the desired window thickness and finally polished to the desired surface finish. Sapphire optical windows can be polished to a wide range of surface finishes due to its crystal structure and its hardness. The surface finishes of optical windows are normally called out by the scratch-dig specifications in accordance with the globally adopted MIL-O-13830 specification.\n\nThe sapphire windows are used in both high pressure and vacuum chambers for spectroscopy, crystals in various watches, and windows in grocery store barcode scanners since the material's exceptional hardness and toughness makes it very resistant to scratching.\n\nIt is used for end windows on some high-powered laser tubes as its wide-band transparency and thermal conductivity allow it to handle very high power densities in the infra-red or UV spectrum without degrading due to heating.\n\nAlong with zirconia and aluminium oxynitride, synthetic sapphire is used for shatter resistant windows in armored vehicles and various military body armor suits, in association with composites.\n\nOne type of xenon arc lamp – originally called the \"Cermax\" and now known generically as the \"ceramic body xenon lamp\" – uses sapphire crystal output windows. This product tolerates higher thermal loads and thus higher output powers when compared with conventional Xe lamps with pure silica window.\n\nThin sapphire wafers were the first successful use of an insulating substrate upon which to deposit silicon to make the integrated circuits known as silicon on sapphire or \"SOS\"; now other substrates can also be used for the class of circuits known more generally as silicon on insulator. Besides its excellent electrical insulating properties, sapphire has high thermal conductivity. CMOS chips on sapphire are especially useful for high-power radio-frequency (RF) applications such as those found in cellular telephones, public-safety band radios, and satellite communication systems. \"SOS\" also allows for the monolithic integration of both digital and analog circuitry all on one IC chip, and the construction of extremely low power circuits.\n\nIn one process, after single crystal sapphire boules are grown, they are core-drilled into cylindrical rods, and wafers are then sliced from these cores.\n\nWafers of single-crystal sapphire are also used in the semiconductor industry as substrates for the growth of devices based on gallium nitride (GaN). The use of sapphire significantly reduces the cost, because it has about one-seventh the cost of germanium. Gallium nitride on sapphire is commonly used in blue light-emitting diodes (LEDs).\n\nMonocrystalline sapphire is fairly biocompatible and the exceptionally low wear of sapphire–metal pairs has led to the introduction (in Ukraine) of sapphire monocrystals for hip \njoint endoprostheses.\n\n\n\n\n \n"}
{"id": "28222911", "url": "https://en.wikipedia.org/wiki?curid=28222911", "title": "Schikorr reaction", "text": "Schikorr reaction\n\nThe Schikorr reaction formally describes the conversion of the iron(II) hydroxide (Fe(OH)) into iron(II,III) oxide (FeO).\n\nThe basis of this transformation reaction was first studied by Gerhard Schikorr, a German specialist of iron corrosion, in his early works (~1928-1933) on iron(II) and iron(III) hydroxides. The global reaction that Schikorr proposed to explain his observations onto the iron hydroxides conversion, and which later received his name, can be written as follows:\n\nThe \"Schikorr reaction\" involves two distinct processes:\n\n\nThe global reaction can thus be decomposed in half redox reactions as follows:\n\nto give:\n\nAdding to this reaction one intact iron(II) ion for each two oxidized iron(II) ions leads to:\n\nElectroneutrality requires the iron cations on both sides of the equation to be counterbalanced by 6 hydroxyl anions (OH):\n\nFor completing the main reaction, two companion reactions have still to be taken into account:\n\nThe autoprotolysis of the hydroxyl anions; a proton exchange between two OH, like in a classical acid-base reaction:\n\nit is then possible to reorganize the global reaction as:\n\nConsidering then the formation reaction of iron(II,III) oxide:\n\nit is possible to write the balanced global reaction:\n\nin its final form, known as the \"Schikorr reaction\":\n\nThe Schikorr reaction can occur in the process of anaerobic corrosion of iron and carbon steel in various conditions.\n\nAnaerobic corrosion of metallic iron to give iron(II) hydroxide and hydrogen:\n\nfollowed by the Schikorr reaction:\n\ngive the following global reaction:\n\nAt low temperature, the anaerobic corrosion of iron can give rise to the formation of \"green rust\" (fougerite) an unstable layered double hydroxide (LDH). In function of the geochemical conditions prevailing in the environment of the corroding steel, iron(II) hydroxide and green rust can progressively transform in iron(II,III) oxide, or if bicarbonate ions are present in solution, they can also evolve towards more stable carbonate phases such as iron carbonate (FeCO), or iron(II) hydroxycarbonate (Fe(OH)(CO), chukanovite) isomorphic to copper(II) hydroxycarbonate (Cu(OH)(CO), malachite) in the copper system.\n\n oxidation of iron and steel commonly finds place in oxygen-depleted environments, such as in permanently water-saturated soils, peat bogs or wetlands in which archaeological iron artefacts are often found.\n\nAnaerobic oxidation of carbon steel of canisters and overpacks is also expected to occur in deep geological formations in which high-level radioactive waste and spent fuels should be ultimately disposed. Nowadays, in the frame of the corrosion studies related to HLW disposal, anaerobic corrosion of steel is receiving a renewed and continued attention. Indeed, it is essential to understand this process to guarantee the total containment of HLW waste in an engineered barrier during the first centuries or millennia when the radiotoxicity of the waste is high and when it emits a significant quantity of heat.\n\nThe question is also relevant for the corrosion of the reinforcement bars (rebars) in concrete (Aligizaki \"et al.\", 2000). This deals then with the service life of concrete structures, amongst others the near-surface vaults intended for hosting low-level radioactive waste.\n\nThe slow but continuous production of hydrogen in deep low-permeability argillaceous formations could represent a problem for the long-term disposal of radioactive waste (Ortiz \"et al.\", 2001; Nagra, 2008; recent Nagra NTB reports). Indeed, a gas pressure build-up could occur if the rate of hydrogen production by the anaerobic corrosion of carbon-steel and by the subsequent transformation of green rust into magnetite should exceed the rate of diffusion of dissolved H in the pore water of the formation. The question is presently the object of many studies (King, 2008; King and Kolar, 2009; Nagra Technical Reports 2000–2009) in the countries (Belgium, Switzerland, France, Canada) envisaging the option of disposal in clay formation.\n\nWhen nascent hydrogen is produced by anaerobic corrosion of iron by the protons of water, the atomic hydrogen can diffuse into the metal crystal lattice because of the existing concentration gradient. After diffusion, hydrogen atoms can recombine into molecular hydrogen giving rise to the formation of high-pressure micro-bubbles of H in the metallic lattice. The trends to expansion of H bubbles and the resulting tensile stress can generate cracks in the metallic alloys sensitive to this effect also known as hydrogen embrittlement. Several recent studies (Turnbull, 2009; King, 2008; King and Kolar, 2009) address this question in the frame of the radioactive waste disposal in Switzerland and Canada.\n\n\nFor detailed reports on iron corrosion issues related to high-level waste disposal, see the following links:\n"}
{"id": "28272", "url": "https://en.wikipedia.org/wiki?curid=28272", "title": "Shinto", "text": "Shinto\n\nShinto practices were first recorded and codified in the written historical records of the \"Kojiki\" and \"Nihon Shoki\" in the 8th century. Still, these earliest Japanese writings do not refer to a unified religion, but rather to a collection of native beliefs and mythology. Shinto today is the religion of public shrines devoted to the worship of a multitude of \"spirits\", \"essences\" (\"kami\"), suited to various purposes such as war memorials and harvest festivals, and applies as well to various sectarian organizations. Practitioners express their diverse beliefs through a standard language and practice, adopting a similar style in dress and ritual, dating from around the time of the Nara and Heian periods (8th–12th century).\n\nThe word \"Shinto\" (\"Way of the Gods\") was adopted, originally as \"Jindō\" or \"Shindō\", from the written Chinese \"Shendao\" (神道, ), combining two \"kanji\": , meaning \"spirit\" or \"kami\"; and , \"path\", meaning a philosophical path or study (from the Chinese word \"dào\"). The oldest recorded usage of the word \"Shindo\" is from the second half of the 6th century. \"Kami\" is rendered in English as \"spirits\", \"essences\", or \"gods\", and refers to the energy generating the phenomena. Since the Japanese language does not distinguish between singular and plural, \"kami\" also refers to the singular divinity, or sacred essence, that manifests in multiple forms: rocks, trees, rivers, animals, places, and even people can be said to possess the nature of \"kami\". Kami and people are not separate; they exist within the same world and share its interrelated complexity.\n\nAs much as nearly 80% of the population in Japan participates in Shinto practices or rituals, but only a small percentage of these identify themselves as \"Shintoists\" in surveys. This is because \"Shinto\" has different meanings in Japan. Most of the Japanese attend Shinto shrines and beseech \"kami\" without belonging to an institutional Shinto religion. There are no formal rituals to become a practitioner of \"folk Shinto\". Thus, \"Shinto membership\" is often estimated counting only those who do join organised Shinto sects. Shinto has about 81,000 shrines and about 85,000 priests in the country. According to surveys carried out in 2006 and 2008, less than 40% of the population of Japan identifies with an organised religion: around 35% are Buddhists, 3% to 4% are members of Shinto sects and derived religions. In 2008, 26% of the participants reported often visiting Shinto shrines, while only 16.2% expressed belief in the existence of a god or gods (神) in general.\n\nAccording to Inoue (2003): \"In modern scholarship, the term is often used with reference to kami worship and related theologies, rituals and practices. In these contexts, 'Shinto' takes on the meaning of 'Japan's traditional religion', as opposed to foreign religions such as Christianity, Buddhism, Islam and so forth.\"\n\nShinto religious expressions have been distinguished by scholars into a series of categories:\n\nMany other sects and schools can be distinguished. is a grouping of Japanese new religions developed since the second half of the 20th century that have significantly departed from traditional Shinto and are not always regarded as part of it.\n\n\"Kami\", \"shin\", or, archaically, \"jin\" (神) is defined in English as \"god\", \"spirit\", or \"spiritual essence\", all these terms meaning \"the energy generating a thing\". Since the Japanese language does not distinguish between singular and plural, \"kami\" refers to the divinity, or sacred essence, that manifests in multiple forms. Rocks, trees, rivers, animals, places, and even people can be said to possess the nature of \"kami\". Kami and people exist within the same world and share its interrelated complexity.\n\nEarly anthropologists called Shinto \"animistic\" in which animate and inanimate things have spirits or souls that are worshipped. The concept of animism in Shinto is no longer current, however.\nShinto gods are collectively called , an expression literally meaning \"eight million kami\", but interpreted as meaning \"myriad\", although it can be translated as \"many kami\". There is a phonetic variation, \"kamu\", and a similar word in the Ainu language, \"kamui\". An analogous word is \"mi-koto\".\n\n\"Kami\" refers particularly to the power of phenomena that inspire a sense of wonder and awe in the beholder (the sacred), testifying to the divinity of such a phenomenon. It is comparable to what Rudolf Otto described as the \"mysterium tremendum et fascinans\", which translates as \"fearful and fascinating mystery\".\n\nThe \"kami\" reside in all things, but certain objects and places are designated for the interface of people and kami: \"yorishiro\", \"shintai\", shrines, and \"kamidana\". There are natural places considered to have an unusually sacred spirit about them and are objects of worship. They are frequently mountains, trees, unusual rocks, rivers, waterfalls, and other natural things. In most cases they are on or near a shrine grounds. The shrine is a building in which the kami is enshrined (housed). It is a sacred space, creating a separation from the \"ordinary\" world. The \"kamidana\" is a household shrine that acts as a substitute for a large shrine on a daily basis. In each case the object of worship is considered a sacred space inside which the kami spirit actually dwells, being treated with the utmost respect.\n\nIn Shinto, \"kannagara\" (惟神 or 随神), meaning \"way [path] of [expression] of the \"kami\"\", refers to the law of the natural order. It is the sense of the terms \"michi\" or \"to\", \"way\", in the terms \"kami-no-michi\" or \"Shinto\". Those who understand \"kannagara\" know the divine, the human, and how people should live. From this knowledge stems the ethical dimension of Shinto, focusing on sincerity (\"makoto\"), honesty (\"tadashii\") and purity.\n\nAccording to the \"Kojiki\", \"Amenominakanushi\" (天御中主 \"All-Father of the Originating Hub\", or 天之御中主神 \"Heavenly Ancestral God of the Originating Heart of the Universe\") is the first \"kami\", and the concept of the source of the universe according to theologies. In mythology he is described as a \"god who came into being alone\" (\"hitorigami\"), the first of the \"zōka sanshin\"(\"three \"kami\" of creation\"), and one of the five \"kotoamatsukami\" (\"distinguished heavenly gods\").\n\nAmenominakanushi had been considered a concept developed under the influence of Chinese thought, but now most scholars believe otherwise. With the flourishing of \"kokugaku\" the concept was studied by scholars. The theologian Hirata Atsutane identified Amenominakanushi as the spirit of the North Star, master of the seven stars of the Big Dipper. The god was emphasised by the \"Daikyōin\" in the Meiji period, and worshiped by some Shinto sects.\n\nThe god manifests in a duality, a male and a female function, respectively \"Takamimusubi\" (高御産巣日神) and \"Kamimusubi\" (神産巣日神). In other mythical accounts the originating \"kami\" is called \"Umashiashikabihikoji\" (宇摩志阿斯訶備比古遅神 \"God of the \"Ashi\" [Reed]\") or \"Kuninotokotachi\" (国之常立神 in Kojiki, 国常立尊 in Nihonshoki; \"Kunitokotachi-no-Kami\" or \"Kuninotokotachi-no-Kami\"; the \"God Founder of the Nation\"), the latter used in the \"Nihon Shoki\".\n\nThe generation of the Japanese archipelago is expressed mythologically as the action of two gods: Izanagi (\"He-who-invites\") and Izanami (\"She-who-is-invited\"). The interaction of these two principles begets the islands of Japan and a further group of \"kami\".\n\nThe events are described in the \"Kojiki\" as follows:\n\nIn the myth, the birth of the god of fire (\"Kagu-Tsuchi\") causes the death of Izanami, who descends into \"Yomi-no-kuni\", the netherworld. Izanagi chases her there, but runs away when he finds the dead figure of his spouse. As he returns to the land of the living, Amaterasu (the sun goddess) is born from his left eye, Tsukiyomi (the moon deity) from his right eye, and Susanoo (the storm deity) is born from Izanagi's nose.\n\nShinto teaches that certain deeds create a kind of ritual impurity that one should want cleansed for one's own peace of mind and good fortune rather than because impurity is wrong. Wrong deeds are called , which is opposed to . Normal days are called \"day\" (\"ke\"), and festive days are called \"sunny\" or, simply, \"good\" (\"hare\").\n\nThose who are killed without being shown gratitude for their sacrifice will hold a (grudge) and become powerful and evil \"kami\" who seek revenge (\"aragami\"). Additionally, if anyone is injured on the grounds of a shrine, the area must be ritually purified.\n\nPurification rites called Harae are a vital part of Shinto. They are done on a daily, weekly, seasonal, lunar, and annual basis. These rituals are the lifeblood of the practice of Shinto. Such ceremonies have also been adapted to modern life. New buildings made in Japan are frequently blessed by a Shinto priest called during the groundbreaking ceremony (Jichinsai 地鎮祭), and many cars made in Japan have been blessed as part of the assembly process. Moreover, many Japanese businesses built outside Japan have a Shinto priest perform ceremonies. On occasion priests visit annually to re-purify.\n\nIt is common for families to participate in ceremonies for children at a shrine, yet have a Buddhist funeral at the time of death. In old Japanese legends, it is often claimed that the dead go to a place called \"yomi\" (黄泉), a gloomy underground realm with a river separating the living from the dead mentioned in the legend of Izanami and Izanagi. This \"yomi\" very closely resembles the Greek Hades; however, later myths include notions of resurrection and even Elysium-like descriptions such as in the legend of Okuninushi and Susanoo. Shinto tends to hold negative views on death and corpses as a source of pollution called \"kegare\". However, death is also viewed as a path towards apotheosis in Shinto as can be evidenced by how legendary individuals become enshrined after death. Perhaps the most famous would be Emperor Ojin who was enshrined as Hachiman the God of War after his death.\n\nUnlike many religions, one does not need to publicly profess belief in Shinto to be a believer. Whenever a child is born in Japan, a local Shinto shrine adds the child's name to a list kept at the shrine and declares him or her a . After death an \"ujiko\" becomes a . One may choose to have one's name added to another list when moving and then be listed at both places. Names can be added to the list without consent and regardless of the beliefs of the person added to the list. This is not considered an imposition of belief, but a sign of being welcomed by the local \"kami\", with the promise of addition to the pantheon of \"kami\" after death.\n\nShinto funerals were established during the Tokugawa period and focused on two themes: concern for the fate of the corpse and maintenance of the relationship between the living and the dead. There are at least twenty steps involved in burying the dead. Mourners wear solid black in a day of mourning called Kichu-fuda and a Shinto priest will perform various rituals. People will give monetary gifts to the deceased's family called Koden, and Kotsuge is the gathering of the deceased's ashes. Some of the ashes are taken by family members to put in their home shrines at the step known as Bunkotsu.\n\nThe principal worship of \"kami\" is done at public shrines or worship at small home shrines called \"kamidana\" (神棚, lit. \"god-shelf\"). The public shrine is a building or place that functions as a conduit for \"kami\". A fewer number of shrines are also natural places called \"mori\". The most common of the \"mori\" are sacred groves of trees, or mountains, or waterfalls. All shrines are open to the public at some times or throughout the year.\n\nWhile many of the public shrines are elaborate structures, all are characteristic Japanese architectural styles of different periods depending on their age. Shrines are fronted by a distinctive Japanese gate (鳥居, \"torii\") made of two uprights and two crossbars denoting the separation between common space and sacred space. The \"torii\" have 20 styles and matching buildings based on the enshrined kami and lineage.\n\nThere are a number of symbolic and real barriers that exist between the normal world and the shrine grounds including: statues of protection, gates, fences, ropes, and other delineations of ordinary to sacred space. Usually there will be only one or sometimes two approaches to the Shrine for the public and all will have the \"torii\" over the way. In shrine compounds, there are a \"haiden\" (拝殿) or public hall of worship, \"heiden\" (幣殿) or hall of offerings and the \"honden\" (本殿) or the main hall. The innermost precinct of the grounds is the \"honden\" or worship hall, which is entered only by the high priest, or worshippers on certain occasions. The \"honden\" houses the symbol of the enshrined \"kami\".\n\nThe heart of the shrine is periodic rituals, spiritual events in parishioners' lives, and festivals. All of this is organized by priests who are both spiritual conduits and administrators. Shrines are private institutions, and are supported financially by the congregation and visitors. Some shrines may have festivals that attract hundreds of thousands, especially in the New Year season.\n\nOf the 80,000 Shinto shrines:\n\nAny person may visit a shrine and one need not be Shinto to do this. Doing so is called \"Omairi\".\nTypically there are a few basic steps to visiting a shrine.\n\nThe rite of ritual purification, usually done daily at a shrine, is a ceremony of offerings and prayers of several forms. Shinsen (food offerings of fruit, fish, and vegetables), tamagushi (sakaki tree branches), shio (salt), gohan (rice), mochi (rice cake), and sake (rice wine) are all typical offerings. On holidays and other special occasions the inner shrine doors may be opened and special offerings made.\n\nMisogi means purification. Misogi harai or Misogi Shūhō (禊修法) is the term for water purification.\n\nThe practice of purification by ritual use of water while reciting prayers is typically done daily by regular practitioners, and when possible by lay practitioners. There is a defined set of prayers and physical activities that precede and occur during the ritual. This will usually be performed at a shrine, in a natural setting, but can be done anywhere there is clean running water.\n\nThe basic performance of this is the hand and mouth washing (Temizu 手水) done at the entrance to a shrine. The more dedicated believer may perform misogi by standing beneath a waterfall or performing the ritual ablutions in a river. This practice comes from Shinto history, when the kami Izanagi-no-Mikoto first performed misogi after returning from the land of Yomi, where he was made impure by Izanami-no-Mikoto after her death.\n\nAnother form of ritual cleanliness is avoidance, which means that a taboo is placed upon certain persons or acts. To illustrate, one would not visit a shrine if a close relative in the household had died recently. Killing is generally unclean and is to be avoided. When one is performing acts that harm the land or other living things, prayers and rituals are performed to placate the Kami of the area. This type of cleanliness is usually performed to prevent ill outcomes.\n\nEma are small wooden plaques that wishes or desires are written upon and left at a place in the shrine grounds so that one may get a wish or desire fulfilled. They have a picture on them and are frequently associated with the larger Shrines.\n\n\"Ofuda\" are talismans—made of paper, wood, or metal—that are issued at shrines. They are inscribed with the names of kami and are used for protection in the home. They are typically placed in the home at a kamidana. Ofuda may be kept anywhere, as long as they are in their protective pouches, but there are several rules about the proper placement of kamidana. They are also renewed annually.\n\n\"Omamori\" are personal-protection amulets that are sold by shrines. They are frequently used to ward off bad luck and to gain better health. More recently, there are also amulets to promote good driving, good business, and success at school. Their history lies with Buddhist practice of selling amulets. They are generally replaced once a year, and old omamori are brought to a shrine so they can be properly disposed of through burning by a priest.\n\n\"Omikuji\" are paper lots upon which personal fortunes are written. The fortunes can range from \"daikichi\" (大吉), meaning \"great good luck,\" to \"daikyou\" (大凶), meaning \"great bad luck.\"\n\nA \"daruma\" is a round, paper doll of the Indian monk, Bodhidharma. The recipient makes a wish and paints one eye; when the goal is accomplished, the recipient paints the other eye. While this is a Buddhist practice, darumas can be found at shrines, as well. These dolls are very common.\n\nOther protective items include \"dorei\", which are earthenware bells that are used to pray for good fortune. These bells are usually in the shapes of the zodiacal animals: \"hamaya\", which are symbolic arrows for the fight against evil and bad luck; and \"Inuhariko\", which are paper dogs that are used to induce and to bless good births.\n\n\"Kagura\" is the ancient Shinto ritual dance of shamanic origin. The word \"kagura\" is thought to be a contracted form of \"kami no kura\" or \"seat of the kami\" or the \"site where the kami is received.\" There is a mythological tale of how \"kagura\" dance came into existence. The sun goddess Amaterasu became very upset at her brother so she hid in a cave. All of the other gods and goddesses were concerned and wanted her to come outside. Ame-no-uzeme began to dance and create a noisy commotion in order to entice Amaterasu to come out. The kami (gods) tricked Amaterasu by telling her there was a better sun goddess in the heavens. Amaterasu came out and light returned to the universe.\n\nMusic plays a very important role in the \"kagura\" performance. Everything from the setup of the instruments to the most subtle sounds and the arrangement of the music is crucial to encouraging the kami to come down and dance. The songs are used as magical devices to summon the gods and as prayers for blessings. Rhythm patterns of five and seven are common, possibly relating to the Shinto belief of the twelve generations of heavenly and earthly deities. There is also vocal accompaniment called \"kami uta\" in which the drummer sings sacred songs to the gods. Often the vocal accompaniment is overshadowed by the drumming and instruments, reinforcing that the vocal aspect of the music is more for incantation rather than aesthetics.\n\nIn both ancient Japanese collections, the Nihongi and Kojiki, Ame-no-uzeme’s dance is described as \"asobi\", which in old Japanese language means a ceremony that is designed to appease the spirits of the departed, and which was conducted at funeral ceremonies. Therefore, \"kagura\" is a rite of \"tama shizume\", of pacifying the spirits of the departed. In the Heian period (8th–12th centuries) this was one of the important rites at the Imperial Court and had found its fixed place in the \"tama shizume\" festival in the eleventh month. At this festival people sing as accompaniment to the dance: \"Depart! Depart! Be cleansed and go! Be purified and leave!\"\nThis rite of purification is also known as \"chinkon\". It was used for securing and strengthening the soul of a dying person. It was closely related to the ritual of \"tama furi\" (shaking the spirit), to call back the departed soul of the dead or to energize a weakened spirit. Spirit pacification and rejuvenation were usually achieved by songs and dances, also called \"asobi\". The ritual of \"chinkon\" continued to be performed on the emperors of Japan, thought to be descendents of Amaterasu. It is possible that this ritual is connected with the ritual to revive the sun goddess during the low point of the winter solstice.\n\nThere is a division between the \"kagura\" that is performed at the Imperial palace and the shrines related to it, and the \"kagura\" that is performed in the countryside. Folk \"kagura\", or \"kagura\" from the countryside is divided according to region. The following descriptions relate to \"sato kagura\", \"kagura\" that is from the countryside. The main types are: \"miko kagura\", \"Ise kagura\", \"Izumo kagura\", and \"shishi kagura\".\n\"Miko kagura\" is the oldest type of \"kagura\" and is danced by women in Shinto shrines and during folk festivals. The ancient miko were shamanesses, but are now considered priestesses in the service of the Shinto Shrines. \"Miko kagura\" originally was a shamanic trance dance, but later, it became an art and was interpreted as a prayer dance. It is performed in many of the larger Shinto shrines and is characterized by slow, elegant, circular movements, by emphasis on the four directions and by the central use of torimono (objects dancers carry in their hands), especially the fan and bells.\n\n\"Ise kagura\" is a collective name for rituals that are based upon the \"yudate\" (boiling water rites of Shugendō origin) ritual. It includes \"miko\" dances as well as dancing of the \"torimono\" type. The \"kami\" are believed to be present in the pot of boiling water, so the dancers dip their \"torimono\" in the water and sprinkle it in the four directions and on the observers for purification and blessing.\n\n\"Izumo kagura\" is centered in the Sada shrine of Izumo, Shimane prefecture. It has two types: \"torimono ma\", unmasked dances that include held objects, and \"shinno\" (sacred No), dramatic masked dances based on myths. \"Izumo kagura\" appears to be the most popular type of \"kagura\".\n\n\"Shishi kagura\" also known as the Shugen-No tradition, uses the dance of a \"shishi\" (lion or mountain animal) mask as the image and presence of the deity. It includes the \"Ise daikagura\" group and the \"yamabushi kagura\" and \"bangaku\" groups of the Tohoku area (Northeastern Japan). \"Ise daikagura\" employs a large red Chinese type of lion head which can move its ears. The lion head of the \"yamabushi kagura\" schools is black and can click its teeth. Unlike other \"kagura\" types in which the \"kami\" appear only temporarily, during the \"shishi kagura\" the \"kami\" is constantly present in the shishi head mask. During the Edo period, the lion dances became showy and acrobatic losing its touch with spirituality. However, the \"yamabushi kagura\" tradition has retained its ritualistic and religious nature.\n\nOriginally, the practice of \"kagura\" involved authentic possession by the \"kami\" invoked. In modern-day Japan it appears to be difficult to find authentic ritual possession, called \"kamigakari\", in \"kagura\" dance. However, it is common to see choreographed possession in the dances. Actual possession is not taking place but elements of possession such as losing control and high jumps are applied in the dance.\n\nThere is no core sacred text in Shinto, as the Bible is in Christianity or Qu'ran is in Islam. Instead there are books of lore and history which provide stories and background to many Shinto beliefs.\n\nShinto has very ancient roots in the Japanese islands. The recorded history dates to the Kojiki (712) and Nihon Shoki (720), but archeological records date back significantly further. Both are compilations of prior oral traditions. The Kojiki establishes the Japanese imperial family as the foundation of Japanese culture, being the descendants of Amaterasu Omikami. There is also a creation myth and a genealogy of the gods. The Nihonshoki was more interested in creating a structural system of government, foreign policy, religious hierarchy, and domestic social order.\n\nThere is an internal system of historical Shinto development that configures the relationships between Shinto and other religious practices over its long history; the inside and outside Kami (spirits). The inside or \"ujigami\" (\"uji\" meaning clan) Kami roles that supports cohesion and continuation of established roles and patterns; and the \"hitogami\" or outside Kami, bringing innovation, new beliefs, new messages, and some instability.\n\nJōmon peoples of Japan used natural housing, predated rice farming, and frequently were hunter-gatherers; the physical evidence for ritual practices are difficult to document. There are many locations of stone ritual structures, refined burial practices and early Torii that lend to the continuity of primal Shinto. The Jōmon had a clan-based tribal system developed similar to much of the world's indigenous people. In the context of this clan based system, local beliefs developed naturally and when assimilation between clans occurred, they also took on some beliefs of the neighboring tribes. At some point there was a recognition that the ancestors created the current generations and the reverence of ancestors (\"tama\") took shape. There was some trade amongst the indigenous peoples within Japanese islands and the mainland, as well as some varying migrations. The trade and interchange of people helped the growth and complexity of the peoples spirituality by exposure to new beliefs. The natural spirituality of the people appeared to be based on the worship of nature forces or \"mono\", and the natural elements to which they all depended.\n\nThe gradual introduction of methodical religious and government organizations from mainland Asia starting around 300 BCE seeded the reactive changes in primal Shinto over the next 700 years to a more formalized system. These changes were directed internally by the various clans frequently as a syncratic cultural event to outside influences. Eventually as the Yamato gained power a formalization process began. The genesis of the Imperial household and subsequent creation of the Kojiki helped facilitate the continuity needed for this long term development through modern history. There is today a balance between outside influences of Buddhist, Confucian, Taoist, Abrahamic, Hindu and secular beliefs. In more modern times Shinto has developed new branches and forms on a regular basis, including leaving Japan.\n\nBy the end of the Jōmon period, a dramatic shift had taken place according to archaeological studies. New arrivals from the continent seem to have invaded Japan from the West, bringing with them new technologies such as rice farming and metallurgy. The settlements of the new arrivals seem to have coexisted with those of the Jōmon for some time. Under these influences, the incipient cultivation of the Jōmon evolved into sophisticated rice-paddy farming and government control. Many other elements of Japanese culture also may date from this period and reflect a mingled migration from the northern Asian continent and the southern Pacific areas. Among these elements are Shinto mythology, marriage customs, architectural styles, and technological developments such as lacquerware, textiles, laminated bows, metalworking, and glass making. The Jōmon is succeeded by the Yayoi period.\n\nJapanese culture begins to develop in no small part due to influences from mainland trade and immigration from China. During this time in the pre-writing historical period, objects from the mainland start appearing in large amounts, specifically mirrors, swords, and jewels. All three of these have a direct connection to the imperial divine status as they are the symbols of imperial divinity and are Shinto honorary objects. Also the rice culture begins to blossom throughout Japan and this leads to the settlement of society, and seasonal reliance of crops. Both of these changes are highly influential on the Japanese people's relationship to the natural world, and likely development of a more complex system of religion. This is also the period that is referenced as the beginning of the divine imperial family. The Yayoi culture was a clan based culture that lived in compounds with a defined leader who was the chief and head priest. They were responsible for the relationship with their \"gods\" Kami and if one clan conquered another, their \"god\" would be assimilated. The earliest records of Japanese culture were written by Chinese traders who described this land as \"Wa\". This time period led to the creation of the Yamato culture and development of formal Shinto practices.\n\nThe development of \"niiname\" or the (now) Shinto harvest festival is attributed to this period as offerings for good harvests of similar format (typically rice) become common.\n\nThe great bells and drums, Kofun burial mounds, and the founding of the imperial family are important to this period. This is the period of the development of the feudal state, and the Yamato and Izumo cultures. Both of these dominant cultures have a large and central shrine which still exists today, Ise Shrine in the North East and Izumo Taisha in the South West. This time period is defined by the increase of central power in Naniwa, now Osaka, of the feudal lord system. Also there was an increasing influence of Chinese culture which profoundly changed the practices of government structure, social structure, burial practices, and warfare. The Japanese also held close alliance and trade with the Gaya confederacy which was in the south of the peninsula. The Paekche in the Three Kingdoms of Korea had political alliances with Yamato, and in the 5th century imported the Chinese writing system to record Japanese names and events for trade and political records. In 513 they sent a Confucian scholar to the court to assist in the teachings of Confucian thought. In 552 or 538 a Buddha image was given to the Yamato leader which profoundly changed the course of Japanese religious history, especially in relation to the undeveloped native religious conglomeration that was Shinto. In the latter 6th century, there was a breakdown of the alliances between Japan and Paekche but the influence led to the codification of Shinto as the native religion in opposition to the extreme outside influences of the mainland. Up to this time Shinto had been largely a clan ('uji') based religious practice, exclusive to each clan.\n\nThe Theory of Five Elements in Yin and Yang philosophy of Taoism and the esoteric Buddhism had a profound impact on the development of a unified system of Shinto beliefs. In the early Nara period, the \"Kojiki\" and the \"Nihon Shoki\" were written by compiling existing myths and legends into a unified account of Japanese mythology. These accounts were written with two purposes in mind: the introduction of Taoist, Confucian, and Buddhist themes into Japanese religion; and garnering support for the legitimacy of the Imperial house, based on its lineage from the sun goddess, Amaterasu. Much of modern Japan was under only fragmentary control by the Imperial family, and rival ethnic groups. The mythological anthologies, along with other poetry anthologies like the \"Collection of Ten Thousand Leaves\" (\"Man'yōshū\") and others, were intended to impress others with the worthiness of the Imperial family and their divine mandate to rule.\n\nIn particular the Asuka rulers of 552–645 saw disputes between the more major families of the clan Shinto families. There were disputes about who would ascend to power and support the imperial family between the Soga and Mononobe/Nakatomi Shinto families. The Soga family eventually prevailed and supported Empress Suiko and Prince Shōtoku, who helped impress Buddhist faith into Japan. However, it was not until the Hakuho ruling period of 645–710 that Shinto was installed as the imperial faith along with the Fujiwara Clan and reforms that followed.\n\nBeginning with Emperor Tenmu (672–686), continuing through Empress Jitō (686–697) and Emperor Monmu (697–707) Court Shinto rites are strengthened and made parallel to Buddhist beliefs in court life. Prior to this time clan Shinto had dominated and a codification of \"Imperial Shinto\" did not exist as such. The Nakatomi family are made the chief court Shinto chaplains and chief priests at Ise Daijingū which held until 1892. Also the practice of sending imperial princesses to the Ise shrine begins. This marks the rise of Ise Daijingū as the main imperial shrine historically. Due to increasing influence from Buddhism and mainland Asian thought, codification of the \"Japanese\" way of religion and laws begins in earnest. This culminates in three major outcomes: Taihō Code (701 but started earlier), the \"Kojiki\" (712), and the \"Nihon Shoki\" (720).\n\nThe Taiho Code also called Ritsuryō (律令) was an attempt to create a bulwark to dynamic external influences and stabilize the society through imperial power. It was a liturgy of rules and codifications, primarily focused on regulation of religion, government structure, land codes, criminal and civil law. All priests, monks, and nuns were required to be registered, as were temples. The Shinto rites of the imperial line were codified, especially seasonal cycles, lunar calendar rituals, harvest festivals, and purification rites. The creation of the imperial Jingi-kan or Shinto Shrine office was completed.\n\nThis period hosted many changes to the country, government, and religion. The capital is moved again to Heijō-kyō, or Nara, in AD 710 by Empress Genmei due to the death of the Emperor. This practice was necessary due to the Shinto belief in the impurity of death and the need to avoid this pollution. However, this practice of moving the capital due to \"death impurity\" is then abolished by the Taihō Code and rise in Buddhist influence. The establishment of the imperial city in partnership with Taihō Code is important to Shinto as the office of the Shinto rites becomes more powerful in assimilating local clan shrines into the imperial fold. New shrines are built and assimilated each time the city is moved. All of the grand shrines are regulated under Taihō and are required to account for incomes, priests, and practices due to their national contributions.\n\nDuring this time, Buddhism becomes structurally established within Japan by Emperor Shōmu (reign 724–749), and several large building projects are undertaken. The Emperor lays out plans for the Buddha Dainichi (Great Sun Buddha), at Tōdai-ji assisted by the Priest Gyogi (or Gyoki) Bosatsu. The priest Gyogi went to Ise Daijingu Shrine for blessings to build the Buddha Dainichi. They identified the statue of Viarocana with Amaterasu (the sun goddess) as the manifestation of the supreme expression of universality.\n\nThe priest Gyogi is known for his belief in assimilation of Shinto Kami and Buddhas. Shinto kami are commonly being seen by Buddhist clergy as guardians of manifestation, guardians, or pupils of Buddhas and bodhisattvas. The priest Gyogi conferred boddhisattva precepts on the Emperor in 749 effectively making the Imperial line the head of state and divine to Shinto while beholden to Buddhism.\n\nWith the introduction of Buddhism and its rapid adoption by the court in the 6th century, it was necessary to explain the apparent differences between native Japanese beliefs and Buddhist teachings. One Buddhist explanation saw the \"kami\" as supernatural beings still caught in the cycle of birth and rebirth (reincarnation). The \"kami\" are born, live, die, and are reborn like all other beings in the karmic cycle. However, the \"kami\" played a special role in protecting Buddhism and allowing its teachings of compassion to flourish.\n\nThis explanation was later challenged by Kūkai (空海, 774–835), who saw the \"kami\" as different embodiments of the Buddhas themselves (\"honji suijaku\" theory). For example, he linked Amaterasu (the sun goddess and ancestor of the Imperial family) with Dainichi Nyorai, a central manifestation of the Buddhists, whose name means literally \"Great Sun Buddha\". In his view, the \"kami\" were just Buddhas by another name.\n\nBuddhism and Shinto coexisted and were amalgamated in the \"shinbutsu shūgō\" and Kūkai's syncretic view held wide sway up until the end of the Edo period. There was no theological study that could be called \"Shinto\" during medieval and early modern Japanese history, and a mixture of Buddhist and popular beliefs proliferated. At that time, there was a renewed interest in \"Japanese studies\" (\"kokugaku\"), perhaps as a result of the closed country policy.\n\nIn the 18th century, various Japanese scholars, in particular Motoori Norinaga (本居 宣長, 1730–1801), tried to tear apart the \"real\" Shinto from various foreign influences. The attempt was largely unsuccessful, since as early as the \"Nihon Shoki\" parts of the mythology were explicitly borrowed from Taoism doctrines. For example, the co-creator deities Izanami and Izanagi are explicitly compared to yin and yang. However, the attempt did set the stage for the arrival of state Shinto, following the Meiji Restoration (c.1868), when Shinto and Buddhism were separated (\"shinbutsu bunri\").\n\nFridell argues that scholars call the period 1868–1945 the \"State Shinto period\" because, \"during these decades, Shinto elements came under a great deal of overt state influence and control as the Japanese government systematically utilized shrine worship as a major force for mobilizing imperial loyalties on behalf of modern nation-building.\" However, the government had already been treating shrines as an extension of government before Meiji; see for example the Tenpō Reforms. Moreover, according to the scholar Jason Ānanda Josephson, It is inaccurate to describe shrines as constituting a \"state religion\" or a \"theocracy\" during this period since they had neither organization, nor doctrine, and were uninterested in conversion.\n\nThe Meiji Restoration reasserted the importance of the emperor and the ancient chronicles to establish the Empire of Japan, and in 1868 the government attempted to recreate the ancient imperial Shinto by separating shrines from the temples that housed them. During this period, numerous scholars of \"kokugaku\" believed that this national Shinto could be the unifying agent of the country around the Emperor while the process of modernization was undertaken with all possible speed. The psychological shock of the Western \"Black Ships\" and the subsequent collapse of the shogunate convinced many that the nation needed to unify in order to resist being colonized by outside forces.\n\nIn 1871, a Ministry of Rites (\"jingi-kan)\" was formed and Shinto shrines were divided into twelve levels with the Ise Shrine (dedicated to Amaterasu, and thus symbolic of the legitimacy of the Imperial family) at the peak and small sanctuaries of humble towns at the base. The following year, the ministry was replaced with a new Ministry of Religion, charged with leading instruction in \"shushin\" (moral courses). Priests were officially nominated and organized by the state, and they instructed the youth in a form of Shinto theology based on the official dogma of the divinity of Japan's national origins and its Emperor. However, this propaganda did not take, and the unpopular Ministry of Rites was dissolved in the mid-1870s.\n\nAlthough the government sponsorship of shrines declined, Japanese nationalism remained closely linked to the legends of foundation and emperors, as developed by the \"kokugaku\" scholars. In 1890, the Imperial Rescript on Education was issued, and students were required to ritually recite its oath to \"offer yourselves courageously to the State\" as well as to protect the Imperial family. Such processes continued to deepen throughout the early Shōwa period, coming to an abrupt end in August 1945 when Japan lost the war in the Pacific. On 1 January 1946, Emperor Shōwa issued the Ningen-sengen, in which he quoted the Five Charter Oath of Emperor Meiji and declared that he was not an \"akitsumikami\" (a deity in human form).\n\nThe imperial era came to an abrupt close with the end of World War II, when Americans declared that Japanese nationalism had been informed by something called \"State Shinto\", which they attempted to define with the Shinto Directive. The meaning of \"State Shinto\" has been a matter of debate ever since.\n\nIn the post-war period, numerous \"New Religions\" cropped up, many of them ostensibly based on Shinto, but on the whole, Japanese religiosity may have decreased. However, the concept of religion in Japan is a complex one. A survey conducted in the mid-1970s indicated that of those participants who claimed not to believe in religion, one-third had a Buddhist or Shinto altar in their home, and about one quarter carried an \"omamori\" (an amulet to gain protection by \"kami\") on their person. Following the war, Shinto shrines tended to focus on helping ordinary people gain better fortunes for themselves through maintaining good relations with their ancestors and other \"kami\". The number of Japanese citizens identifying their religious beliefs as Shinto has declined a great deal, yet the general practice of Shinto rituals has not decreased in proportion, and many practices have persisted as general cultural beliefs (such as ancestor worship), and community festivals (\"matsuri\")—focusing more on religious practices. The explanation generally given for this anomaly is that, following the demise of State Shinto, modern Shinto has reverted to its more traditional position as a traditional religion which is culturally ingrained, rather than enforced. In any case, Shinto and its values continue to be a fundamental component of the Japanese cultural mindset.\n\nShinto has also spread abroad to a limited extent, and a few non-Japanese Shinto priests have been ordained. A relatively small number of people practice Shinto in America. There are several Shinto shrines in America. Shrines were also established in Taiwan and Korea during the period of Japanese imperial rule, but following the war, they were either destroyed or converted into some other use.\n\nWithin Shinto, there are a variety of sects which are not a part of Shrine Shinto and the officially defunct State Shinto. Sect Shinto, like Izumo Taishakyo Mission of Hawaii and Konkokyo, have unique practices which originated alongside older Shinto practices before the classification and separation of Shinto practices of the Meiji era in 1868.\n\n"}
{"id": "27838889", "url": "https://en.wikipedia.org/wiki?curid=27838889", "title": "Siemens Wind Power", "text": "Siemens Wind Power\n\nSiemens Wind Power, (formerly Danregn Vindkraft A/S and Bonus Energy A/S) is a wind turbine manufacturer established in 1980 as Danregn Vindkraft. Bonus Energy was acquired by Siemens of Germany in 2004. The organisation became a separate division of Siemens in 2011, with headquarters established in Hamburg, Germany.\n\nIn 2015 Siemens Wind had a combined market share of 63% of European offshore wind turbines (nearly 75% in 2009 by capacity and number). In 2011, Siemens Wind Power had 6.3% share of the world wind turbine market, and was the second largest in 2014.\n\nIn 2016 Siemens Wind and Gamesa agreed a 59:41 merger of their wind businesses.\n\nHistory of the company started in 1980, when Danish irrigation system manufacturer Danregn, diversified into the windturbine business; its first wind turbines were machines with rotor diameters of around with generator powers of . In 1981 the wind activities were separated into newly established company Danregn Vindkraft A/S, established by Peter Stubkjær Sørensen and Egon Kristensen in Brande, Denmark, with a capital of 300,000 kroner; the company's product was a , blade diameter turbine.\n\nBetween 1982 and 1987 the company exported wind turbines to the USA in collaboration with Difko AS, in response to a wind farm building boom promoted by government subsidies; the company changed its name from Danregn Vindkraft to Bonus Energy in 1983, an easier name for the English speaking North American market.\n\nIn 1991, eleven 450 kW Bonus turbines were installed in the Vindeby Offshore Wind Farm (Denmark), the first offshore wind farm in the world.\n\nThe company sourced its first blades from Viborg based company \"Økær Vind Energi\". Later it sourced blades from LM Wind Power. In the late 1990s Bonus began to develop its own blades, beginning production in the early 2000s in Aalborg.\n\nBonus AS was sold to Siemens in 2004 for an undisclosed amount, but before the sale the value was assessed to be somewhere between DKK 1.5 (USD 240 million) and 2.5 billion (USD 400 million). The sales and project management headquarters moved to Hamburg, Germany in May 2009.\nBetween 2004 and 2011, Siemens grew wind power from 0.5% to 5% of the combined Siemens turnover, with employees growing from 800 to 7,800, of which 5,200 are in Denmark, and 1,000 in Germany. The growth included the expansion of production, warehousing and offices at its Brande site in 2005/6; acquisition in 2006 of a former LM Glasfiber wind turbine blade factory in Engesvang, Denmark; construction of a blade factory in Fort Madison, Iowa, USA in 2007; a hub factory in Ølgod began production in 2008; and a nacelle manufacturing plant was established in Hutchinson, Kansas, USA between 2009 and 2010, opening in December 2010. Additionally Bonus Energy sales and service partner company \"AN Windenergie GmbH\" in Bremen (Germany) was acquired in 2005.\n\nIn mid-2008 the company began testing of development prototypes of direct drive wind turbines; units based on the geared SWT-3.6–107 were installed in 2008 with a permanent magnet generator directly replacing the gearbox and alternator; Successful tests led to development of a new production design by 2009. A prototype of the new direct drive design, an IEC 61400 wind class IA, 3 MW machine (SWT 3.0–101 DD) was installed near Brande, Denmark in 2009. The 3 MW design was launched as a product in April 2010 and significantly reduced complexity (half the components) and lower nacelle weight than earlier 2.3 MW designs. A 2.3 MW version for lower wind speeds (SWT-2.3–113) was launched in 2011.\n\nIn 2010 Siemens Wind Power acquired 49% of A2SEA (an offshore wind farm installation company) from DONG Energy for a price of DKK 860m.\n\nA factory established in Linggang (\" Siemens Wind Power Blades (Shanghai) Co., Ltd.\") near the Yangshan Deep Water Port began production in 2010. Additionally in December 2010 Siemens announced it would install a blade factory at an existing unused facility in Tillsonburg, Ontario, Canada. In early 2011 Siemens and ABP announced the development of a £210 million turbine assembly plant, and dock development at Alexandra Dock, in Kingston upon Hull, UK.\n\nIn 2011 Siemens' wind power operations were split into a separate division, 'Wind Power'; with its other renewable energy activities place into a 'Solar & Hydro' division, the divisions headquarters were established in Hamburg on 1 October 2011, the European offshore wind headquarters remained in Brande, Denmark.\n\nIn May 2011 testing began of a prototype 6 MW direct drive design with a rotor, the design was launched as a product in November 2011. In 2013 Siemens announced a development of its 3.6 MW design, the SWT 4.0–130 which used a rotor of diameter 130m with 4 MW rated power. At the same time the company introduced new product platform codes for its products, with 'G' indicating geared drive, and 'D' indicating direct drive, suffixed by a number indicating an approximate power class. The four initial product ranges were Siemens G2, G4, D3 and D6.\n\nIn July 2012, the company agreed to supply Dong Energy with 300 direct drive, 75m blade, 6 MW SWT-6.0–154 turbines for the English offshore market from 2014. Two turbines are to be installed for testing at the Gunfleet Sands offshore wind farm. The value of the contract was estimated at over £2 billion. Prototype 6 MW machines were installed at the Gunfleet Sands 2 wind farm in 2013; with the first full scale commercial installation of 6 MW machines at the 210 MW Westernmost Rough wind farm in 2014.\nIn September 2012 Siemens Wind announced the lay off of 615 of a workforce of around 1650 workers in the United States, citing reduced demand for wind turbines due to uncertainty concerning future tax break incentives in the USA for wind power. (see United States Wind Energy Policy.)\n\nIn March 2014 Siemens and Associated British Ports (ABP) finalised the 2011 MOU to build a turbine factory in Hull, UK ('Green Port Hull'), and announced an additional facility near Paull, East Riding of Yorkshire, east of Hull which would manufacture rotor blades for turbines. In 2014 the planned factory at Paull was abandoned, with all production to be concentrated at the Alexandra dock site. Revised plans for the site submitted April 2015 included only a blade manufacturing factory at the site with no nacelle production.\n\nIn 2015 Siemens upgraded its 6 MW offshore design to a rated 7 MW power with a larger permanent magnet generator, and further to 8 MW in 2016. The first order for the 7 MW design was awarded in October 2015 for 47 turbines in the Walney 3 offshore.\n\nIn early 2015 Siemens announced it had reached agreements to build 2 GW of wind turbines in Egypt, and to construct a blade factory in that country, as part of a larger power generation agreement. The €8 billion, 16.4 GW energy development deal was signed in June 2015, including an approximate 1000 worker blade factory in Ain Soukhna and 12 wind farms (600 turbine, 2 GW) in the Gulf of Suez and west Nile areas of Egypt.\n\nIn August 2015 Siemens announced it was to construct a new nacelle manufacturing plant at Cuxhaven, Germany, an investment of £200 million. The plant was expected to become operational mid 2017, and employ 1000 people. A €100 million blade plant to be built in the Tanger Automotive City (near Tanger-Med port) in Morocco was announced in early 2016.\n\nOn 17 June 2016 Siemens and Gamesa (Spain) announced they were to merge their wind businesses, with the two operations forming 59% (Siemens Wind) and 41% (Gamesa) of the resulting company's shareholding, with Siemens offering €3.75 per Gamesa share. The resultant company was to be headquartered in Spain, with an offshore operations headquartered in Hamburg, Germany and Vejle, Denmark. Siemens was reported to have paid €1 billion ($1.13 billion) cash for Gamesa shares. Cost savings between duplicated functions in the two businesses was expected by Gamesa to save c. €230 million in the first year of operation. The combined business would be the largest wind turbine manufacturer worldwide by installed capacity (c. 69 GW), exceeding Vestas and GE.\n\nThird party analysis (Feng Zhou, FTI Consulting) suggested that Gamesa's strength in China and India and west pacific markets as a strategic asset for Siemens.\n\nAn agreement was reached between Areva Wind and Gamesa on their joint venture Adwen, whereby Areva surrendered contractual obligations with Gamesa, and Siemens/Gamesa gave Areva an option to either sell or acquire the jv; Adwen was said to have been a source of contention during negotiations, as Siemens was reluctant to fund factories and development of an 8 MW turbine in France.\n\nIn February 2017 Siemens announced the closure of the Engesvang blade factory (Denmark), with the loss of 430 jobs, citing the plants inability to produce larger size blades.\n\nAlong with Bonus in 2004, Siemens acquired \"AN Windenergie GmbH\" in Bremen (Germany) in 2005; the sales and service partner company of Bonus Energy.\n\nIn 2010 Siemens Wind Power acquired 49% of A2SEA (an offshore wind farm installation company) from DONG Energy for a price of DKK 860m.\n\nIn December 2011 Siemens signed a strategic collaboration agreement with Shanghai Electric for wind power supply in China.\n\nSiemens Wind has R&D, and production facilities in Brande, Denmark. Blade production is located in Aalborg and Engesvang (Denmark), Linggang (China), Fort Madison, Iowa (USA) and Tillsonburg, Ontario (Canada); with factories under construction or planned (2016) for Kingston upon Hull (UK), Tanger Automotive City (Morocco) and Ain Soukhna (Egypt).\n\nOther established production sites included nacelle manufacture at Hutchinson, Kansas (USA, 1.6 GW) and hub production at Ølgod (Denmark). As of 2016 a new nacelle plant is under development at Cuxhaven (Germany).\n\nSiemens acquired the first of two Roll-on/roll-off turbine transport ships in 2016, converted from a container ship, to reduce logistics costs. A telescopic roof also allows Lift-on/lift-off with cranes.\n\nAs of 2016 Siemens Wind products include 2.3, 3.6 and 4.0 MW geared turbines; and 3.0, 3.2, 3.3, 3.4, 6.0 and 7.0 MW direct drive turbines.\n\nBy 2010 Siemens Wind Power had filed 242 wind turbine patents on the United Kingdom Intellectual Property Office database (UK-IPO), while Vestas has filed 787 and General Electric has 666.\n\nIn 2008, a R&D center was opened in Boulder, Colorado, United States, stating that it could recruit higher quality aerodynamicists in the location than in Denmark.\n\nIn 2009 Siemens supplied a special SWT-2.3–82 turbine installed on \"Hywind\", the first large capacity floating wind turbine in the world, developed by Statoil.\n\nIn around 2010 Siemens has a goal of reducing the cost per kilowatt-hour to €0.05 for onshore windpower and to €0.10 for offshore wind by 2020, many of the cost saving mechanisms were based on practice originating in the auto industry. Potential cost reductions included: automation/robotisation of blade manufacture and tailor woven glass fibre mats to reduce to simplify the blade manufacturing process; use of standardised components across product ranges to reduce overall component costs; elimination of geared generator drives to reduce maintenance cost; and modularisation of nacelle design, splitting generator and power conversion into separate modules, with the aim of additional flexibility in manufacture, and reduced transportation costs due reduced weight of the modules. The company also offered shorter length bolted tower sections allowing container transportation, and simplified mass production.\n\nIn a Life-cycle assessment, SWP calculates that the energy for manufacturing a 6 MW direct drive wind turbine is made back in under 10 months depending on circumstances, roughly the same as independent research suggests.\n\nSWP develops artificial neural networks for machine learning to predict and diagnose potential problems in 9,000 wind turbines with 400 sensors each, sending data several times a second.\n\nSiemens tests its turbines at LORC, its own test sites and Østerild Wind Turbine Test Field. In 2017, Siemens installed a low-wind 3.15 MW turbine with a 142-meter rotor.\n\n\n"}
{"id": "3267910", "url": "https://en.wikipedia.org/wiki?curid=3267910", "title": "Solar chemical", "text": "Solar chemical\n\nSolar chemical refers to a number of possible processes that harness solar energy by absorbing sunlight in a chemical reaction. The idea is conceptually similar to photosynthesis in plants, which converts solar energy into the chemical bonds of glucose molecules, but without using living organisms, which is why it is also called artificial photosynthesis.\n\nA promising approach is to use focused sunlight to provide the energy needed to split water into its constituent hydrogen and oxygen in the presence of a metallic catalyst such as zinc. This is normally done in a two-step process so that hydrogen and oxygen are not produced in the same chamber, which creates an explosion hazard. Another approach involves taking the hydrogen created in this process and combining it with carbon dioxide to create methane. The benefit of this approach is that there is an established infrastructure for transporting and burning methane for power generation, which is not true for hydrogen. One main drawback to both of these approaches is common to most methods of energy storage: adding an extra step between energy collection and electricity production drastically decreases the efficiency of the overall process.\n\nIt is also possible to use solar light to directly drive industrial chemical reactions and applications, eliminating the need to burn fossil fuels for energy.\n\nAs early as 1909, the dimerization of anthracene into dianthracene was investigated as a means of storing solar energy, as well as the photodimerization of the naphthalene series. In the 70’s and 80’s a fuel had been made from another reversible chemical, the norbornadiene to quadricyclane transformation cycle, but this failed because the reversal process had a low potential. Ruthenium-based molecules were also attempted, but this was dismissed because ruthenium is both rare and too heavy of a material. In the past decade, a new hybrid nanostructure was theorized as a new approach to this previously known concept of solar energy storage.\n\nPhotodimerization is the light induced formation of dimers and Photoisomerization is the light induced formation of isomers. While photodimerization stores the energy from sunlight in new chemical bonds, Photoisomerization stores solar energy by reorienting existing chemical bonds into a higher energy configuration.\n\nIn order for an isomer to store energy then, it must be metastable as shown above. This results in a trade-off between the stability of the fuel isomer and how much energy must be put in to reverse the reaction when it is time to use the fuel. The isomer stores energy as strain energy in its bonds. The more strained the bonds are the more energy they can store, but the less stable the molecule is. The activation energy, Ea, is used to characterize how easy or hard it is for the reaction to proceed. If the activation energy is too small the fuel will tend to spontaneously move to the more stable state, providing limited usefulness as a storage medium. However, if the activation energy is very large, the energy expended to extract the energy from the fuel will effectively reduce the amount of energy that the fuel can store. Finding a useful molecule for a solar fuel requires finding the proper balance between the yield, the light absorption of the molecule, the stability of the molecule in the metastable state, and how many times the molecule can be cycled without degrading.\n\nVarious ketones, azepines and norbornadienes among other compounds, such as azobenzene and its derivates, have been investigated as potential energy storing isomers. The norbornadiene-quadricyclane couple and its derivates have been extensively investigated for solar energy storage processes. Norbornadiene is converted to quadricyclane using energy extracted from sunlight, and the controlled release of the strain energy stored in quadricyclane (about 110 k J/mole) as it relaxes back to norbornadiene allows the energy to be extracted again for use later.\n\nResearch into both the azobenzene and norbonadiene-quadricyclane systems was abandoned in the 1980s as unpractical due to problems with degradation, instability, low energy density, and cost. With recent advances in computing power though, there has been renewed interest in finding materials for solar thermal fuels. In 2011, researchers at MIT used time-dependent density functional theory, which models systems at an atomic level, to design a system composed of azobenzene molecules bonded to carbon nanotube (CNT) templates. The CNT substrates will allow customizable interactions between neighboring molecules which greatly helps in fine tuning the properties of the fuel, for example an increase in the amount of energy stored. Through experimental procedures, researchers were able to get the first proof of principle that the hybrid nanostructure works as a functional thermal fuel. Azobenzenes have the advantage of absorbing wavelengths that are very abundant in sunlight, when this happens the molecule transforms from a trans-isomer to a cis-isomer which has a higher energy state of about 0.6 eV. To bring the molecule back down to its original state, i.e. release the energy it had collected, there are a few options. The first is to apply heat but that is associated with a cost which, relative to the amount of heat that will be produced from the release, is not cost efficient. The second, more effective option is to use a catalyst that lowers the thermal barrier and allows the heat to be released, almost like a switch. The transition back from cis to trans can also be triggered by blue visible light.\n\nThis system provides an energy density comparable to lithium-ion batteries, while simultaneously increasing the stability of the activated fuel from several minutes to more than a year and allowing for large numbers of cycles without significant degradation. Further research is being done in search of even more improvement by examining different possible combinations of substrates and photoactive molecules.\n\nThere are a wide variety of both potential and current applications for solar chemical fuels. One of the major pros of this technology is its scalability. Since the energy can be stored and then later converted to heat when needed, it is ideal for smaller on the go units. These range from portable stoves or small personal heaters that can be charged in the sun to providing medical sanitation in off-grid areas, and plans are even in the works to use the system developed at MIT as a window de-icing system in automobiles. It also has the ability to be scaled up and heat larger homes or buildings or even heat bodies of water. A solar thermal fuel would ideally be able to cycle indefinitely without degradation, making it ideal for larger scale implementations that generally would need more replacements of other forms of storage.\n\n"}
{"id": "8438905", "url": "https://en.wikipedia.org/wiki?curid=8438905", "title": "Steam bus", "text": "Steam bus\n\nA steam bus is a bus powered by a steam engine. Early steam-powered vehicles designed for carrying passengers were more usually known as steam carriages, although this term was sometimes used to describe other early experimental vehicles too.\n\nRegular intercity bus services by steam-powered buses were pioneered in England in the 1830s by Walter Hancock and by associates of Sir Goldsworthy Gurney among others, running reliable services over road conditions which were too hazardous for horse-drawn transportation. Steam carriages were much less likely to overturn, and did not \"run away with\" the customer as horses sometimes did. They travelled faster than horse-drawn carriages (24 mph over four miles and an average of 12 mph over longer distances). They could run at a half to a third of the cost of horse-drawn carriages. Their brakes did not lock and drag like horse-drawn transport (a phenomenon that increased damage to roads). According to engineers, steam carriages caused one-third the damage to the road surface as that caused by the action of horses' feet. Indeed, the wide tires of the steam carriages (designed for better traction) caused virtually no damage to the streets, whereas the narrow wheels of the horse-drawn carriages (designed to reduce the effort required of horses) tended to cause rutting.\n\nHowever, the heavy road tolls imposed by the Turnpike Acts discouraged steam road vehicles and left the way clear for the horse bus companies, and from 1861 onwards, harsh legislation virtually eliminated mechanically propelled vehicles altogether from the roads of Great Britain for 30 years, the Locomotive Act of that year imposing restrictive speed limits on \"road locomotives\" of 5 mph in towns and cities, and 10 mph in the country.\n\nIn 1865 the Locomotives Act of that year (the famous Red Flag Act) further reduced the speed limits to 4 mph in the country and just 2 mph in towns and cities, additionally requiring a man bearing a red flag to precede every vehicle. At the same time, the act gave local authorities the power to specify the hours during which any such vehicle might use the roads. The sole exceptions were street trams which from 1879 onwards were authorised under licence from the Board of Trade.\n\nIn 1881, the engineer John Inshaw built a steam carriage for use in Aston, Birmingham, UK. Capable of carrying ten people at speeds of up to 12 mph, Inshaw discontinued his experiments due to the legislation then in force.\nThe Red Flag Act was repealed in 1896, and experimental steam buses again operated in various places in England and around the empire.\n\nLiquid Fuel Engineering Co. (Lifu) of Cowes built steam buses from 1897 to 1901. Lifu buses ran at Mansfield from 1 July 1898, between Dover and Deal in 1899, Fairford and Cirencester for the Midland and South Western Junction Railway in 1898 and 1899.\n\nIn 1899 a 24-seat (14 on top) bus of E. Gillett & Co (Hounslow) was licensed, but ran no regular service.\n\nStraker buses were run by Potteries Electric Traction from 1901.\n\nLondon Road Car Co ran a Hammersmith-Shepherd's Bush-Oxford Circus route with a Thornycroft 36-seater (12 inside) in 1902.\n\nThomas Clarkson produced steam buses at Moulsham Works, Chelmsford and exhibited at 1903 and 1905 Motor Shows.\n\nTorquay had steam buses from 1903 to 1923. In May 1903 a Chelmsford steam bus demonstration resulted in the formation of the \"Torquay & District Motor Omnibus Co Ltd\" on 23 July 1903. The company's prospectus said, \"The Chelmsford motor omnibuses are steam propelled, and, what is of importance in a town of the character of Torquay, are entirely free from smell, noise, and vibration.\" 3 single-deck 14-seat (12 inside and 2 with the driver) Chelmsford steam buses were ordered in May, built in August, but got stuck in mud between Salisbury and Exeter and didn't start a Strand to Chelston service until 2 November. There were 2,828 passengers in the first week. 2 more steam buses arrived in January 1904 and 8 were working by Easter 1905, the last 3 being 20 seaters. They had two-cylinder horizontal engines, with tubular boilers, and a working pressure of 150 to 250psi. They averaged a gallon of paraffin for 3.8 miles. 1904 costs were under 8½d per mile, made up of lubricants 0.2d., wages and fuel 5d. and repairs, establishment, and other charges 3.3d. Fares were 1d. to 4d., with 15 minute frequencies on two routes, and half-hourly on the others. The company declared a 7½% dividend in its first year. All the Torquay buses were sold to \"Harrogate Road Car Company\" just before \"Torquay Tramways\" opened in April 1907, but the \"Torquay Road Car Co\" took over the garage and bought 6 Chelmsfords, 3 from \"Vale of Llangollen\", 2 from Eastbourne Corporation and 1 from \"Great Western Railway\". The new company was liquidated on 24 December 1908 and 3 buses sold to \"Bargoed Motor Service Company\" in 1909. The Torquay-Chelston Steam Car Company Ltd was formed on 28 March 1911 and took over the 3 unsold Chelmsfords and a new 25-seater Clarkson type IV steam bus from August 1911. The steam buses were replaced by petrol in 1923, when Devon General started to compete on the Chelston route. \nIn London Chelmsfords had mixed fortunes. Single-deck Chelmsfords were used by \"London General Omnibus Co\" and \"London Road-Car Co\", but the last were withdrawn in 1905 because of heavy losses. However, double deck Chelmsfords with improved boilers were delivered to \"London Road-Car\" in 1905 and in 1909 \"National Steam Car Co. Ltd\" also started to run double deck Chelmsfords in London. By 1914 \"National Steam\" had 184, but post war replacements were petrol and its last steam bus ran in London on 18 November 1919.Chelmsford buses reached many other places too, but weren't successful. Crewe to Nantwich had double deckers in 1905. \"London and South Western Railway\" ran them between Lyndhurst and Milford and Ambleside had 2. India was another destination. Four (or 6) Chelmsford chassis were imported by the \"New South Wales Railways\" in 1905. The bodies were constructed in Sydney and the vehicles placed in service on the streets of that city. Even further from their Chelmsford factory, two were imported for a Devonport-Takapuna service in 1904, but were unsuccessful and transferred to Hamilton in 1906. They probably failed to be profitable there too, as horses were again the motive power by 1910. \nDarracq-Serpollet steam buses were run by the \"Metropolitan Steam Omnibus Co Ltd\" from 5 October 1907 to 16 October 1912. When \"London General\" took over its main rivals on 1 July 1908 it had 1066 motor buses, 35 of them steam. A year later \"London General\" had abandoned steam.\n\nSteam power for road transportation saw a modest revival in the 1920s. It was economical to use, with prices of fuel oil (such as kerosene) being about one-third that of gasoline, with comparable fuel consumption to contemporary gasoline-engined vehicles. Additionally, startup times vis-a-vis gasoline-powered vehicles and safety issues from vaporized fuel had been solved, with steam cars such as the Doble requiring a mere 40 seconds to start from cold. In 1931 Doble was employed as a consultant by A & G Price of Thames, New Zealand to construct a steam engine for buses. Four were built in 1931, but plans were shelved as the depression deepened and the Auckland bus was too heavy for the roads. However, two 1932 reports described progress with construction.\n\nThe Canadian company Brooks Steam Motors of Toronto, Ontario produced steam city buses in the 1920s. More recently, in 1972, the American inventor Bill Lear introduced a steam bus intended for trials in San Francisco.\n\nA steam bus, Elizabeth operates in the English seaside town of Whitby.\n\n\n"}
{"id": "42622953", "url": "https://en.wikipedia.org/wiki?curid=42622953", "title": "Tumarín Dam", "text": "Tumarín Dam\n\nThe Tumarín Dam is a 60 meter tall, concrete gravity dam planned for construction on the Río Grande de Matagalpa just upstream of the town of Tumarín in the South Caribbean Coast Autonomous Region, Nicaragua. It is located about east of San Pedro del Norte, where the Río Grande de Matagalpa meets the Tuma River. First announced in March 2010, construction was expected to take four years, but there have been many delays. Preliminary construction (roads, bridges and foundation) was to begin in 2011 and main works were expected to begin in February 2015. Completion is scheduled for 2019. Brazil's Eletrobras will fund the US$1.1 billion under a 20 to 30 year build–operate–transfer (BOT) agreement. The project is being developed by Centrales Hidroelectricas de Nicaragua (CHN). The power station located at the base of the dam will house three 84.33 MW Kaplan turbine-generators for an installed capacity of 253 MW. The dam will create a reservoir covering 40 square kilometers.\n"}
{"id": "2973881", "url": "https://en.wikipedia.org/wiki?curid=2973881", "title": "Tunnel injection", "text": "Tunnel injection\n\nTunnel injection is a field electron emission effect; specifically a quantum process called Fowler–Nordheim tunneling, whereby charge carriers are injected to an electric conductor through a thin layer of an electric insulator.\n\nIt is used to program NAND flash memory. The process used for erasing is called tunnel release.\n\nAn alternative to tunnel injection is the spin injection.\n\n"}
{"id": "2799888", "url": "https://en.wikipedia.org/wiki?curid=2799888", "title": "Turbulator", "text": "Turbulator\n\nA turbulator is a device that turns a laminar flow into a turbulent flow. Turbulent flow can be desired on parts of the surface of an aircraft wing (airfoil) or in industrial applications such as heat exchangers and the mixing of fluids. The term “turbulator” is applied to a variety of applications and is used as a derivative of the word turbulent. However, the word has no commonly accepted technical or scientific meaning. As such, it has been approved as a trademark in the U.S. and other countries in conjunction with machine parts used within rotating drums, sterilizers, heat transfer ovens, mixing and pelletizing machines, and air destratification fans for horticultural and agricultural uses, among others.\n\nIn gliders the turbulator is often a thin zig-zag strip that is placed on the lower side of the wing and sometimes on the vertical stabilizer.\n\n"}
{"id": "38578380", "url": "https://en.wikipedia.org/wiki?curid=38578380", "title": "Yangbajing Solar Park", "text": "Yangbajing Solar Park\n\nThe Yangbajing Solar Park is a 30 MWp photovoltaic power station located in Yangbajain, Tibet.\n\n"}
{"id": "40133655", "url": "https://en.wikipedia.org/wiki?curid=40133655", "title": "Çocuktepe Dam", "text": "Çocuktepe Dam\n\nThe Çocuktepe Dam is a gravity dam under construction on the Güzeldere River (a tributary of the Great Zab) in Çukurca district of Hakkâri Province, southeast Turkey. Under contract from Turkey's State Hydraulic Works, İnelsan İnşaat began construction on the dam in 2008 and a completion date has not been announced. Construction on the Gölgeliyamaç Dam immediately upstream began in 2008 as well but was cancelled due to poor geology.\n\nThe reported purpose of the dam is water storage and it can also support a hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of PKK militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Hakkâri are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River. In Şırnak there is the Silopi Dam on the Hezil River and the Şırnak, Uludere, Balli, Kavşaktepe, Musatepe and Çetintepe Dams on the Ortasu River.\n\n"}
