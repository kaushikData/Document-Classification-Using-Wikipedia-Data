{"id": "148135", "url": "https://en.wikipedia.org/wiki?curid=148135", "title": "Alaska Permanent Fund", "text": "Alaska Permanent Fund\n\nThe Alaska Permanent Fund is a constitutionally established permanent fund managed by a state-owned corporation, the Alaska Permanent Fund Corporation (APFC). It was established in Alaska in 1976 by Article 9, Section 15 of the Alaska State Constitution under Governor Jay Hammond. From February 1976 until April 1980, the Department of Revenue Treasury Division managed the state's Permanent Fund assets, until, in 1980, the Alaska State Legislature created the APFC. As of the end of 2016, the fund is worth nearly $55 billion that has been funded by oil revenues.\n\nShortly after the oil from Alaska's North Slope began flowing to market through the Trans-Alaska Pipeline System, the Permanent Fund was created by an amendment to the Alaska Constitution. It was designed to be an investment where at least 25% of the oil money would be put into a dedicated fund for future generations, who would no longer have oil as a resource. This does not mean the fund is solely funded by oil revenue. The Fund includes neither property taxes on oil company property nor income tax from oil corporations, so the minimum 25% deposit is closer to 11% if those sources were also considered. The Alaska Permanent Fund sets aside a certain share of oil revenues to continue benefiting current and all future generations of Alaskans. Many citizens also believed that the legislature too quickly and too inefficiently spent the $900 million bonus the state got in 1969 after leasing out the oil fields. This belief spurred a desire to put some oil revenues out of direct political control.\n\nThe Alaska Permanent Fund Corporation manages the assets of both the Permanent Fund and other state investments, but spending Fund income is up to the Legislature. The Corporation is to manage for maximum prudent return, and not—as some Alaskans at first wanted—as a development bank for in-state projects. The Fund grew from an initial investment of $734,000 in 1977 to approximately $53.7 billion as of July 9, 2015. Some growth was due to good management, some to inflationary re-investment, and some via legislative decisions to deposit extra income during boom years. Each year, the fund's realized earnings are split between inflation-proofing, operating expenses, and the annual Permanent Fund Dividend.\n\nThe fund is a member of the International Forum of Sovereign Wealth Funds and has therefore signed up to the Santiago Principles on best practice in managing sovereign wealth funds. The Fund's current chief investment officer is Marcus Framption.\n\nThe Alaska Permanent Fund Corporation is a government instrumentality of the State of Alaska created to manage and invest the assets of the Alaska Permanent Fund and other funds designated by law.\n\nThe Board of Trustees are governor-appointed\n\nThe Permanent Fund Dividend [PFD] is a dividend paid to Alaska residents that have lived within the state for a full calendar year (January 1 – December 31), and intend to remain an Alaska resident indefinitely. This means if residency is taken on January 2, the \"calendar year\" wouldn't start until next January 1.\n\nHowever, an individual is not eligible for a PFD for a dividend year when:\n\nThe amount of each payment is based upon a five-year average of the Permanent Fund's performance and varies widely depending on the stock market and many other factors. The PFD is calculated by the following steps:\n\n\nThe lowest individual dividend payout was $331.29 in 1984 and the highest was $2,072 in 2015. However, in 2008 Governor Sarah Palin signed Senate Bill 4002 that used revenues generated from the state's natural resources and provided a one-time special payment of $1,200 to every Alaskan eligible for the PFD.\n\nAlthough the principal or corpus of the Fund is constitutionally protected, income earned by the Fund, like nearly all State income, is constitutionally defined as general fund money.\n\nThe first dividend plan would have paid Alaskans $50 for each year of residency up to 20 years, but the U.S. Supreme Court in disapproved the $50 per year formula as an invidious distinction burdening interstate travel. As a result, each qualified resident now receives the same annual amount, regardless of age or years of residency.\n\nPayments from the fund are subject to federal income tax. Alaska has no state income tax, but part-year residents who leave the state may be taxed on them by their new state of residence.\n\nThis is the fund's history of annual individual payouts, in nominal USD.\n\nThe Constitutional Budget Reserve is a companion fund to the Permanent Fund which was established in 1991 to ease problems from the variability of oil revenue, which vary depending upon the price of oil in the market. Deposits into the CBR consist of settlements of back taxes and other revenues owed to the state. Draws from the CBR into the general fund require a 3/4 vote of each house of the legislature and must be repaid. To date, the general fund has amassed a debt of approximately $4 billion to the CBR to maintain a stable level of public spending.\n\nThe size of the debt owed to the CBR has raised doubts over repayment. The CBR is based on the assumption that the general fund deficit will remain constant over time (allowing paybacks to balance draws). Believing this to be mistaken, critics allege the state uses resources from the CBR to avoid reducing the budget, acknowledging debt, or increasing taxes. According to them, falling oil revenues and growing spending requirements will leave paybacks consistently lower than draws, causing the CBR to fail.\n\nFormer state senator Dave Donley (R-Anchorage) recognized that the high vote requirement to spend CBR money (¾ of each house) had a perverse and unintended consequence. The high vote requirement was meant to ensure that draws from the CBR would be rare, but in fact such draws are common. Donley explained that the high vote requirement really empowers the minority party (in the 2000–07 era, the Democratic Party), who can then get what they want in a Christmas tree bill (presents for everyone, both majority and minority) in exchange for their votes (which minority votes would not be needed with the usual 51% voting rule). Donley thus explains why both parties can and do use the higher voting rule requirement to more frequently spend from the CBR.\n\nWhile the Permanent Fund generally generated large surpluses even after payment of the Dividend [PFD], the state general fund operated at a substantial deficit. However, the consolidated account of both General and Permanent Funds usually shows a surplus. The Funds' ultimate uses were never clearly spelled out at its inception, leaving no current consensus over what role Fund earning should play in the current and expected state budget shortfalls. However, some people argue that the original intent was to fund state government after the temporary oil riches ceased, while others note that the Fund's intent changed from its 1976 origin when in 1982 the Dividend program began. Public opinion strongly favors the Dividend program. Indeed, in 1999, with oil prices going as low as $9 per barrel and Alaska's oil consultant Daniel Yergin forecasting low prices \"for the foreseeable future\", the State put an advisory vote before Alaskans, asking if government could spend \"some\" part of Permanent Fund earning for government purposes. Gov. Knowles, Lt. Gov. Ulmer, and many other elected officials urged a \"yes\" vote. Campaign spending greatly favored the \"yes\" side. The public voted \"no\" by nearly 84%. (Oil prices rose dramatically, starting about two weeks after Yergin's prediction, to above $60 per barrel, though the quantity produced continues to fall.) Perceived support of the dividend program is so universally strong that it ensures the dividend's continuity and the protection of the Fund's principal, since any measure characterized as negatively impacting dividend payouts represents a loss to the entire populace. That is, legislators willing to appropriate the Fund's annual earnings are constrained by the politically suicidal nature of any decrease in the public's dividend.\n\nIn 2000, the APFC Board of Trustees proposed changing the Permanent Fund's management system to a Percent of Market Value (PoMV) approach which would require an amendment to the state constitution. The PoMV proposal would limit withdrawals to five percent of the fund's value each year, to be spent at the discretion of the Legislature. Currently the Legislature has authority to appropriate all of the fund's realized earnings. Tentative, unapproved proposals indicate that half of this five percent withdrawal would go to the dividend and half to government spending—but POMV died in the Legislature because most there saw POMV as unambiguously tied to such politically unpopular spending proposals. Most Alaskans (84% in 1999) disapprove of allowing the government to tamper with the fund, especially if that means government might spend Fund income.\n\nAgain in 2015–2017, a POMV approach was considered. The market price for North Slope oil fell from an average $107.57 per barrel in FY2014 to $50.05 per barrel in FY2017. This price shift caused an 80 percent decline in state revenue and resulted in a multibillion-dollar budget gap. Both bodies of the legislature have passed a bill that provides for an annual draw of 5.25% of the average balance of the Permanent Fund (average of the first 5 of the last six years). Since the formula is based on an average, rather than a single year, the effective draw is only about 4.2%—enough to preserve the real value of the fund considering that the fund has returned close to 9% annually. The legislature carefully vetted this percentage over the course of two sessions and has come to a consensus. This draw is projected to produce $2.7 billion in FY2019 and grow with the balance of the Permanent Fund. The major point of disagreement, however, is the size of the dividend: The House of Representatives version of the bill uses 5.25% draw for government (33% of for Dividends and 67% for government services) and an additional 0.25% draw for Permanent Fund inflation proofing. This produces $2.7 billion ($1.8 billion for government use, net of a $900.9 million dividend—about $1,250.00 per Alaskan—growing with the value of the fund). The Senate version of the bill uses the same 5.25% draw as the House, but directs only 25% of the draw to dividends. This produces the same $2.7 billion but government services receive $2.0 billion while the dividend receives just under $700 million—about $1,000.00 per person—growing with the value of the fund.\n\nOil revenues are forecast (by the state Department of Revenue) to remain stagnant through FY2027, and traditional budget reserves may be empty by FY2019 but with a Permanent Fund value in excess of $60.0 billion, the budget gap can be reduced significantly. Since this POMV proposal does not close the gap entirely, members of the legislature are considering a tax bill as well.\n\nA 2018 paper found that the Alaska Permanent Fund \"dividend had no effect on employment, and increased part-time work by 1.8 percentage points (17 percent)... our results suggest that a universal and permanent cash transfer does not significantly decrease aggregate employment.\"\n\n\n"}
{"id": "11108071", "url": "https://en.wikipedia.org/wiki?curid=11108071", "title": "Alaska Wildlife Alliance", "text": "Alaska Wildlife Alliance\n\nThe Alaska Wildlife Alliance is a non-profit organization that was founded in 1978 in Anchorage, Alaska. It has dedicated its efforts and funds to protect Alaskan wildlife for its intrinsic value and to benefit the present and future generations. \n"}
{"id": "40765543", "url": "https://en.wikipedia.org/wiki?curid=40765543", "title": "Aleta Baun", "text": "Aleta Baun\n\nAleta Baun is an award-winning Indonesian environmental activist. She has been described as the Indonesian Avatar.\n\nShe won the 2013 Goldman Environmental Prize for organizing hundreds of local villagers to peacefully occupy marble mining sites in “weaving protests,” to stop destruction of sacred forest land on Mutis Mountain on the island of Timor.\n\nA leader of indigenous Mollo people, she was born to a family of farmers. Having lost her mother at a young age, she was raised by other women and elders in the village who taught her to respect the environment as a source of their spiritual identity and livelihood. As a community leader sharing traditional knowledge, she eventually became known as “Mama Aleta.” \n\nMama Aleta’s work made her a target for the mining interests and local authorities, who put a price on her head. After surviving an assassination attempt, Mama Aleta went into hiding in the forest with her baby.\n\nDespite intimidation, Mama Aleta grew the movement to include hundreds of villagers. It culminated in a weaving occupation where 150 women spent a year sitting on the marble rocks at the mining site, quietly weaving their traditional cloth in protest. Because women were traditionally responsible for getting food, dye and medicine from the mountains, mining in these mountains would have directly impacted their livelihoods. While the women protested at the mine, the men provided domestic support at home, cooking, cleaning and caring for the children.\n\nIn the face of the villagers’ peaceful and sustained presence, marble mining became an increasingly untenable endeavor for the companies involved. Public awareness of the weaving occupation was growing, and Indonesian government officials took notice. By 2010, the mining companies, reacting to the pressure, halted mining at all four sites within the Mollo territories and abandoned their operations.\n\nMama Aleta now helps communities across West Timor to map their traditional forests. She works in water security and indigenous peoples natural resource management and land rights. She said to the Jakarta Post:\n“We especially want to conserve the upstream region of our territory because it is a watershed for the entire island. We are considering a joint title for our three communities and placing the land under collective ownership of the communities”. To help with this, she obtained a law degree in 2011 from Universitas Tritunggal Surabaya.\n"}
{"id": "13144103", "url": "https://en.wikipedia.org/wiki?curid=13144103", "title": "Arbel (automobile)", "text": "Arbel (automobile)\n\nThe Arbel (also called Loubières, Loubière, Symétric, Symétric-Paris, or Arbel-Symétric) was a fiber-glass bodied hybrid petrol-electric vehicle produced by the Compagnie Normande d'Etudes pour l'Application de Procédés Mécaniques made as individual models from 1951 to 1953 and again 1957 to 1959. Only a limited number were made throughout the period, mainly for experimentation or prototyping.\n\nThe first Arbel was designed and made by Casimir (Casi) André Loubière, then a 43-year-old car salesman, and financed by his brother Maurice, the owner of an air-transport business, COSARA (Société Transatlantique Aérienne en Extrême Orient), in Indo-China. They initially traded as \"Les Freres Loubière\". \n\nThe car was described as an eight seater powered by a Simca four cylinder 1100cc petrol engine that powered four electric motors fitted to the inside of the wheel hubs. It was built of glass and light alloy, weighed 1,870lb, and had windows that were curved and slid into the roof. The brothers stated that they intended the car to be mass-produced and sell for ₤295. It was first shown as the \"Symétrie\" at the Geneva Motor Show in 1951, but created little interest.\n\nThe car was redesigned as a roadster with a plastic body and re-shown during the Paris Grand Palais 40th Salon d'Automobile and Cycle Show on 22 October 1953. At that time it was stated as being of interest to the French War Department. Light, cheap, fast electric car; The Central Queensland Herald; Rockhampton, Queensland; 24 December 1953; p 25</ref> It had one pedal, a handbrake for emergencies, and no gears. The body was fibreglass. Normal braking was automatic when the accelerator was not pressed. The Loubière's drove the car in Paris, where it reached 60 mph. It was said to be able to attain a top speed of 95 mph and petrol consumption was 32 mpg under normal driving. The stopping distance at 60 mph was said to be less than 40 yards, compared with the 60 to 80 yards for cars of the period. With a proposed annual production of 1,000 cars, the \"Symetric\" was estimated to cost £435.\n\nThe second version was designed between 1957 and 1958. It was displayed at the March 1958 Geneva Motor Show.\n\nCalled the Arbel-Symétric, alternative power plants were considered, the first was a static gas generator fueled by diesel oil and the second in 1958, possibly taking the idea from the Ford Nucleon, was to be powered by a \"genestatom\", a 40-KW nuclear thermal generator using radioactive cartridges made of nuclear waste. The French government did not approve the use of nuclear fuel and no development took place.\n\nThe 1958 Geneva show also had the Studebaker-Packard Astral, another concept car with the idea of nuclear propulsion, on display.\n\nThe company went into debt and never recovered. In 1963 the Loubières patented a rotary engine, although this may have related to their engine patented on 13 June 1950.\n\n\n\n\n"}
{"id": "4427464", "url": "https://en.wikipedia.org/wiki?curid=4427464", "title": "Bayamo (wind)", "text": "Bayamo (wind)\n\nA bayamo is a violent wind blowing from the land on the south coast of Cuba, especially near the Bight of Bayamo.\nIt is also the namesake for Giddings and Webster's most popular tuba mouthpiece.\n\n"}
{"id": "34042159", "url": "https://en.wikipedia.org/wiki?curid=34042159", "title": "Billionth Barrel Monument", "text": "Billionth Barrel Monument\n\nThe Billionth Barrel Monument is a monument located in Seria, Brunei.\n\nThe monument was built in 1991. It was commemorated by Hassanal Bolkiah on 18 July 1991. The monument commemorates the production of the billionth barrel of oil produced in the onshore oil field in Seria.\n\nThe monument is located near the S-1, the first well discovered in Seria Oil Field. The well was discovered by Brunei Shell Petroleum in 1929. The monument is located 38 meters from S-1, which is now located on the beach. The only marker that remains of S-1 is a corroded steel bar.\n\nThe monument was designed by a local architect.\n"}
{"id": "15100744", "url": "https://en.wikipedia.org/wiki?curid=15100744", "title": "Compressed hydrogen tube trailer", "text": "Compressed hydrogen tube trailer\n\nHydrogen tube trailers are semi-trailers that consist of 10 to 36 cluster high-pressure hydrogen tanks varying in length from for small tubes to on jumbo tube trailers. They are part of the hydrogen highway and usually precede a local hydrogen station.\n\nModular tube trailer range from 8 to 54 tubes.\n\nIntermediate tubes are assembled in banks of 5 tubes in lengths of and provide mobile or stationary storage.\n\nA trailer with 10 tubes and a chassis, operating with pressures in excess of .\n\nAs of September 2013, The Linde Group has introduced a tube trailer operating at utilizing new, lighter storage materials to more than double the amount of compressed gaseous hydrogen to , or a normal of hydrogen gas. The new trailers can be filled and emptied in less than 60 minutes.\n\nAs on July 2016, Nishal Group has multiple cascade configurations in the form of cascade banks operating at 200 bar. \n\n\n"}
{"id": "48542637", "url": "https://en.wikipedia.org/wiki?curid=48542637", "title": "Cotton production in Pakistan", "text": "Cotton production in Pakistan\n\nCotton production in Pakistan is integral to the economic development of the country. The nation is largely dependent on the cotton industry and its related textile sector, and the crop has been given a principal status in the country. Cotton is grown as an industrial crop in 15% of the nation's land during the monsoon months of May to August, known as the kharif period, and is grown at a smaller scale between February and April. Record production of cotton was reported at 15 million bales of each in the form of \"phutti\" (seed cotton) during 2014–15, which was an 11% rise compared to the previous season (2013–14). Production-wise, as of 2012–13, Pakistan occupied the fourth position among the cotton growers of the world, the first three being China, India and the United States, in that order. In respect of exports of raw cotton, Pakistan holds third position, and is the fourth in consumption (about 30 and 40 per cent of its production). It is the largest exporter of cotton yarn.\n\nThe earliest known historical traces of cotton were found at Mehrgarh near the city of Quetta, making Pakistan one of the first regions of cotton cultivation. Cotton was discovered in threads on a copper bead at a burial site dated to the Neolithic period (6000 BC). The mineralised threads were subject to metallurgical analysis with a combination of a reflected-light microscope and a scanning electron microscope, revealing that they were of cotton (genus \"Gossypium\"). Cotton cultivation became more widespread during the Indus Valley Civilisation, which covered parts of present day eastern Pakistan and northwestern India. Archaeobotanical evidence of seeds has been traced to 5000 BC in Mehrgarh, though it is not clear if they belonged to a wild or cultivated variety. Use of cotton cloth in the Indus Valley cities of Mohenjo-daro and Harappa dates to 2,500 BC. Cotton pollen has been recorded at Balakot. At Harappa (Mature Harappan period 2500-2000 BC), evidence of cotton threads has been found tied to the handle of a mirror, an antiquity from a female burial site, and around a copper razor. There is also much other evidence of cotton in some form, such as Malavaceae (flowering plant) pollen type, similar to \"Gossypium\" in Balakot (Mature Harappan period, 2500-2000 BC); as seeds at Banawali (Mature Harappan, 2200-1900 BC), Sanghol (Late Harappan, 1900-1400 BC), Kanmer, Kacchh (Late Harappan, 2,000-1,700 BC), Imlidhi Khurd and Gorakhpur (1300-800 BC); as fibres in Late Ochre-Coloured Pottery at Sringaverapura (1200-700 BC); and in Hallur as seeds and fragments of the Early Iron Age (950-900 BC).\n\nCotton is purely a cellulose fibre crop, one of the four major crops in the country, and is known by popular epithets as \"King cotton\" and \"white gold\". It forms the primary input for the textile industry of Pakistan.\nCotton is integral to Pakistan's economy. According to an analysis in the USDA Foreign Agricultural Service report of 2015, it is grown as an industrial crop in 15% of the nation's land. It is grown during the monsoon months of May to August, known as the kharif period. It is also grown on a smaller scale between February and April.\n\nCotton is grown mostly in the two provinces of Punjab and Sindh, with the former accounting for 79% and the latter for 20% of the nation's cotton growing land. It is also grown in Khyber Pakhtoon Khawah (KPK) and Balochistan provinces. The total land area of cotton cultivation was reported as during the 2014–15 growing season. Generally, small farmers with land holdings less than in size form the largest group of growers; farmers holding less than account for 50% of the farms. Land holdings with under cotton cultivation form less than 2% of farms. According to a 2013 estimate, there were 1.6 million farmers (out of a total of 5 million in all sectors) engaged in cotton farming, growing more than 3 million hectares.\n\nFarmers have widely adopted \"Bacillus thuringiensis\" (Bt) cotton since its first trial in Sindh province in 2002. It is now used in 95% of the area. The Punjab Seed Council has approved the use of 18 Bt cotton and non-Bt varieties for cultivation. These are: 12 BT varieties FH-114, CIM-598, SITARA-009, A-one, BH-167, MIAD-852, CIM-573, SLH-317, TARZAN-1, NS-141, IR-NIBGE-3, MNH-886, and six non-BT varieties NIBGE −115, FH-941, FH-942, IR-1524, Ali Akbar-802 and NEELAM-121. In Sindh province, local Sindh varieties of cotton are also grown in about 40% of the area. They are generally planted from April to July, and harvested during August–December.\n\nCotton serves as the base for the nation's industrial sector. Production of cotton was reported at a record high of 15 million bales of 470 lbs each in the form of \"phutti\" (seed cotton) during 2014–15; this was an 11% increase compared to the previous season (2013–14). Its phenomenal growth was from 1.38 million bales in 1961 to 11.138 million bales in 2014, with the estimated 2014–15 figures showing a further increase to 15 million bales. Between 1980–81 and 1990–91, the growth in production was rapid, with production rising from 0.70 million to 2.2 million tonnes, which was called the \"magic year\" of Pakistan's cotton industry. This was attributed to better pest control measures, use of improved seed types and increased use of fertilisers. The cotton and textile industries are integrated and account for 1,000 ginneries, 425 textile mills, and 300 cottonseed crushers and oil refiners. Cotton hybrids, created by crossing the Bt gene into traditional varieties, have been developed by local firms dealing with seeds. In Sindh province cotton is grown in more than one million acres in the districts of Benazirabad, Hyderabad, Jamshoro, Mirpur Khas, Naushero Feroz, Sanghar, Badin, Sukkar, Ghotki, Tharparkar, Thatta and Umar Kot.\n\nIn terms of production, Pakistan is at the fourth position among the cotton growers of the world; the first three are China, India and the United States, in that order Raw cotton exported from Pakistan holds third position in the world as per records of 2012-13. Consumption-wise it holds the fourth position (about 30 and 40 per cent of its production). It is the largest exporter of cotton yarn.\n\nCotton produced within the country is of medium staple. Hence long staple cotton is imported to produce quality fabrics for export. Medium staple cotton, also called standard medium-staple cotton is American Upland type with staple length varying from about . Long-staple cottons have relatively longer fibre, are expensive and used mostly to make fine fabrics, yarns, and hosiery. The country's economic development is largely dependent on the cotton industry and its related textile sector, and this has given a principal status to cotton in the country. Apart from use in textiles in the form of cotton lint, yarn, thread, cloth, and garments, its seeds are used for oil extraction.\n\nViruses and pests affect yield of Bt cotton varieties. Cotton leaf curl virus, which is a plant pathogenic virus of the family Geminiviridae, stunts plant growth seriously affecting yield. Pests like White Fly, Mealy Bugs, Aphids, Pink Boll Worm infect the plants reducing yield.\n\n474,091 bales of 470 lbs each were exported during the 2014–15 season, an increase from 382,006 bales in 2014–15. The cotton and textile industries play a dominant role in exports; cotton accounts for 55 percent of country's export earnings, and Pakistan has a 14% share of the world's cloth exports. The European Union (EU) granted Generalized System of Preferences \"Plus\" status to Pakistan in 2013, which has promoted textile exports to the EU.\n\nThough a bio-safety regulatory system was part of the 18th Amendment to the Constitution of Pakistan that \"devolved\" several functions to the provinces, the system is still unclear with regard to regulators who can oversee the approval of new seed technologies. In this context the three regulatory acts which are under approval stage are the Plant Breeders’ Rights Act, Amendments to the 1976 Seed Act, and the Biosafety Law. According to the USDA Foreign Agricultural Service report of 2015, passage of these laws is crucial to the introduction of new biotech events.\n"}
{"id": "1639303", "url": "https://en.wikipedia.org/wiki?curid=1639303", "title": "Critical Mach number", "text": "Critical Mach number\n\nIn aerodynamics, the critical Mach number (Mcr or M* ) of an aircraft is the lowest Mach number at which the airflow over some point of the aircraft reaches the speed of sound, but does not exceed it. At the lower critical Mach number, airflow around the entire aircraft is subsonic. At the upper critical Mach number, airflow around the entire aircraft is supersonic.\n\nFor an aircraft in flight, the speed of the airflow around the aircraft differs considerably in places from the airspeed of the aircraft; this is due to the airflow having to speed up and slow down as it travels around the aircraft's structure. When the aircraft's airspeed reaches the critical Mach number, the speed of the airflow in some areas near the airframe reaches the speed of sound, even though the aircraft itself has an airspeed lower than Mach 1.0. This creates a weak shock wave. As the aircraft exceeds the critical Mach number, its drag coefficient increases suddenly, causing dramatically increased drag, and, in an aircraft not designed for transonic or supersonic speeds, changes to the airflow over the flight control surfaces lead to deterioration in control of the aircraft.\n\nIn aircraft not designed to fly at or above the critical Mach number, the shock waves that form in the airflow over the wing and tailplane are sufficient to stall the wing, render the control surfaces ineffective, or lead to loss of control of the aircraft (such as Mach tuck, when shock waves in the airflow over the elevator send the aircraft into an uncontrollable dive). These problematic phenomena appearing at or above the critical Mach number became known as compressibility. Compressibility led to a number of accidents involving high-speed military and experimental aircraft in the 1930s and 1940s.\n\nAlthough unknown at the time, compressibility was the cause of the phenomenon known as the sound barrier. 1940s-era military subsonic aircraft, such as the Supermarine Spitfire, Bf 109, P-51 Mustang, Gloster Meteor, He 162, and P-80, have relatively thick, unswept wings, and are incapable of reaching Mach 1.0 in controlled flight. In 1947, Chuck Yeager flew the Bell X-1 (also with an unswept wing, but a much thinner one), reaching Mach 1.06 and beyond, and the sound barrier was finally broken.\n\nEarly transonic military aircraft, such as the Hawker Hunter and F-86 Sabre, were designed to fly satisfactorily even at speeds greater than their critical Mach number. They did not possess sufficient engine thrust to break the sound barrier in level flight, but could exceed Mach 1.0 in a dive while remaining controllable. Modern jet airliners, such as Airbus and Boeing aircraft, have maximum operating Mach numbers slower than Mach 1.0.\n\nSupersonic aircraft, such as Concorde, Tu-144, the English Electric Lightning, Lockheed F-104, Dassault Mirage III, and MiG 21, are designed to exceed Mach 1.0 in level flight, and, therefore, are designed with very thin wings. Their critical Mach numbers are higher than those of subsonic and transonic aircraft, but are still less than Mach 1.0.\n\nThe actual critical Mach number varies from wing to wing. In general, a thicker wing will have a lower critical Mach number, because a thicker wing deflects the airflow passing around it more than a thinner wing does, and thus accelerates the airflow to a faster speed. For instance, the fairly-thick wing on the P-38 Lightning has a critical Mach number of about .69. The aircraft could occasionally reach this speed in dives, leading to a number of crashes. The Supermarine Spitfire's much thinner wing gave it a considerably higher critical Mach number (about 0.89).\n\n\n"}
{"id": "7191704", "url": "https://en.wikipedia.org/wiki?curid=7191704", "title": "Cryo", "text": "Cryo\n\nCryo- is from Ancient Greek κρύος (krúos, “icy cold, chill, frost”). Uses of the prefix Cryo- include:\n\n\n\n"}
{"id": "26592825", "url": "https://en.wikipedia.org/wiki?curid=26592825", "title": "De Groot dual", "text": "De Groot dual\n\nIn mathematics, in particular in topology, the de Groot dual (after Johannes de Groot) of a topology \"τ\" on a set \"X\" is the topology \"τ\"* whose closed sets are generated by compact saturated subsets of (\"X\", \"τ\").\n\n"}
{"id": "371968", "url": "https://en.wikipedia.org/wiki?curid=371968", "title": "Debye model", "text": "Debye model\n\nIn thermodynamics and solid state physics, the Debye model is a method developed by Peter Debye in 1912 for estimating the phonon contribution to the specific heat (heat capacity) in a solid. It treats the vibrations of the atomic lattice (heat) as phonons in a box, in contrast to the Einstein model, which treats the solid as many individual, non-interacting quantum harmonic oscillators. The Debye model correctly predicts the low temperature dependence of the heat capacity, which is proportional to formula_1 – the Debye T law. Just like the Einstein model, it also recovers the Dulong–Petit law at high temperatures. But due to simplifying assumptions, its accuracy suffers at intermediate temperatures.\n\nSee M. Shubin and T. Sunada for a rigorous treatment of the Debye model.\n\nThe Debye model is a solid-state equivalent of Planck's law of black body radiation, where one treats electromagnetic radiation as a photon gas. The Debye model treats atomic vibrations as phonons in a box (the box being the solid). Most of the calculation steps are identical as both are examples of a massless Bose gas with linear dispersion relation.\n\nConsider a cube of side formula_2. From the particle in a box article, the resonating modes of the sonic disturbances inside the box (considering for now only those aligned with one axis) have wavelengths given by\n\nwhere formula_4 is an integer. The energy of a phonon is\n\nwhere formula_6 is Planck's constant and formula_7 is the frequency of the phonon. Making the approximation that the frequency is inversely proportional to the wavelength, we have:\n\nin which formula_9 is the speed of sound inside the solid.\nIn three dimensions we will use:\n\nin which formula_11 is the magnitude of the three-dimensional momentum of the phonon.\n\nThe approximation that the frequency is inversely proportional to the wavelength (giving a constant speed of sound) is good for low-energy phonons but not for high-energy phonons (see the article on phonons.) This disagreement is one of the limitations of the Debye model, and corresponds to incorrectness of the results at intermediate temperatures, whereas both at low temperatures and also at high temperatures they are exact.\n\nLet's now compute the total energy in the box,\n\nwhere formula_13 is the number of phonons in the box with energy formula_14. In other words, the total energy is equal to the sum of energy multiplied by the number of phonons with that energy (in one dimension). In 3 dimensions we have:\n\nHere, the Debye model and Planck's law of black body radiation differ. Unlike electromagnetic radiation in a box, there is a finite number of phonon energy states because a phonon cannot have arbitrarily high frequencies. Its frequency is bounded by the medium of its propagation—the atomic lattice of the solid. Consider an illustration of a transverse phonon below.\n\nIt is reasonable to assume that the minimum wavelength of a phonon is twice the atom separation, as shown in the lower figure. There are formula_16 atoms in a solid. Our solid is a cube, which means there are formula_17 atoms per edge. Atom separation is then given by formula_18, and the minimum wavelength is\n\nmaking the maximum mode number formula_4 (infinite for photons)\n\nThis number bounds the upper limit of the triple energy sum\n\nFor slowly varying, well-behaved functions, a sum can be replaced with an integral (also known as Thomas-Fermi approximation)\n\nSo far, there has been no mention of formula_24, the number of phonons with energy formula_25 Phonons obey Bose–Einstein statistics. Their distribution is given by the famous Bose–Einstein formula\n\nBecause a phonon has three possible polarization states (one longitudinal, and two transverse which approximately do not affect its\nenergy) the formula above must be multiplied by 3,\n\nSubstituting into the energy integral yields\n\nThe ease with which these integrals are evaluated for photons is due to the fact that light's frequency, at least semi-classically, is unbound. As the figure above illustrates, which is not true for phonons. In order to approximate this triple integral, Debye used spherical coordinates\nand approximated the cube by an eighth of a sphere\n\nwhere formula_35 is the radius of this sphere, which is found by conserving the number of particles in the cube and in the eighth of a sphere. The volume of the cube is formula_16 unit-cell volumes,\n\nso we get:\n\nThe substitution of integration over a sphere for the correct integral introduces another source of inaccuracy into the model.\n\nThe energy integral becomes\n\nChanging the integration variable to formula_40,\n\nTo simplify the appearance\nof this expression, define the Debye temperature formula_29\n\nMany references describe the Debye temperature as merely shorthand for some constants and material-dependent variables. However, as shown below, formula_44 is roughly equal to the phonon energy of the minimum wavelength mode, and so we can interpret the Debye temperature as the temperature at which the highest-frequency mode (and hence every mode) is excited.\n\nContinuing, we then have the specific internal energy:\n\nwhere formula_46 is the (third) Debye function.\n\nDifferentiating with respect to formula_47 we get the dimensionless heat capacity:\n\nThese formulae treat the Debye model at all temperatures. The more elementary formulae given further down give the asymptotic behavior in the limit of low and high temperatures. As already mentioned, this behaviour is exact, in contrast to the intermediate behaviour. The essential reason for the exactness at low and high energies, respectively, is that the Debye model gives (i) the exact \"dispersion relation\" formula_49 at low frequencies, and (ii) corresponds to the exact \"density of states\" formula_50concerning the number of vibrations per frequency interval.\n\nActually, Debye derived his equation somewhat differently and more simply. Using continuum mechanics, he found that the number of vibrational states with a frequency less than a particular value was asymptotic to\n\nin which formula_52 is the volume and formula_53 is a factor which he calculated from elasticity coefficients and density. Combining this formula with the expected energy of a harmonic oscillator at temperature T (already used by Einstein in his model) would give an energy of\n\nif the vibrational frequencies continued to infinity. This form gives the formula_1 behavior which is correct at low temperatures. But Debye realized that there could not be more than formula_56 vibrational states for N atoms. He made the assumption that in an atomic solid, the spectrum of frequencies of the vibrational states would continue to follow the above rule, up to a maximum frequency formula_57chosen so that the total number of states is formula_56:\n\nDebye knew that this assumption was not really correct (the higher frequencies are more closely spaced than assumed), but it guarantees the proper behavior at high temperature (the Dulong–Petit law). The energy is then given by:\n\nwhere formula_66 is the function later given the name of third-order Debye function.\n\nIn other words simply define that \nE= 3/5N (πkT)^4 (1/hv)^3\nEnergy term of debye's theory\n\nFirst we derive the vibrational frequency distribution; the following derivation is based on Appendix VI from. Consider a three-dimensional isotropic elastic solid with N atoms in the shape of a rectangular parallelepiped with side-lengths formula_67. The elastic wave will obey the wave equation and will be plane waves; consider the wave vector formula_68 and define formula_69. Note that we have\n\nSolutions to the wave equation are\n\nand with the boundary conditions formula_71 at formula_72, we have\n\nwhere formula_73 are positive integers. Substituting () into () and also using the dispersion relation formula_74, we have\n\nThe above equation, for fixed frequency formula_76, describes an eighth of an ellipse in \"mode space\" (an eighth because formula_73 are positive). The number of modes with frequency less than formula_76 is thus the number of integral points inside the ellipse, which, in the limit of formula_79 (i.e. for a very large parallelepiped) can be approximated to the volume of the ellipse. Hence, the number of modes formula_80 with frequency in the range formula_81 is\nwhere formula_82 is the volume of the parallelepiped. Note that the wave speed in the longitudinal direction is different from the transverse direction and that the waves can be polarised one way in the longitudinal direction and two ways in the transverse direction; thus we define formula_83.\n\nFollowing the derivation from, we define an upper limit to the frequency of vibration formula_84; since there are N atoms in the solid, there are 3N quantum harmonic oscillators (3 for each x-, y-, z- direction) oscillating over the range of frequencies formula_85. Hence we can determine formula_86 like so:\n\nBy defining formula_87, where k is Boltzmann's constant and h is Planck's constant, and substituting () into (), we get\n\nThe energy contribution for oscillators with frequency formula_76 is then\n\nBy noting that formula_94 (because there are formula_95 modes oscillating with frequency formula_76), we have\n\nFrom above, we can get an expression for 1/A; substituting it into (), we have\n\nformula_99\n\nIntegrating with respect to ν yields\n\nThe temperature of a Debye solid is said to be low if formula_102, leading to\n\nThis definite integral can be evaluated exactly:\n\nIn the low temperature limit, the limitations of the Debye model mentioned above do not apply, and it gives a correct relationship between (phononic) heat capacity, temperature, the elastic coefficients, and the volume per atom (the latter quantities being contained in the Debye temperature).\n\nThe temperature of a Debye solid is said to be high if formula_105. Using formula_106 if formula_107leads to\n\nThis is the Dulong–Petit law, and is fairly accurate although it does not take into account anharmonicity, which causes the heat capacity to rise further. The total heat capacity of the solid, if it is a conductor or semiconductor, may also contain a non-negligible contribution from the electrons.\n\nSo how closely do the Debye and Einstein models correspond to experiment? Surprisingly close, but Debye is correct at low temperatures whereas Einstein is not.\n\nHow different are the models? To answer that question one would naturally plot the two on the same set of axes... except one can't. Both the Einstein model and the Debye model provide a \"functional form\" for the heat capacity. They are \"models\", and no model is without a scale. A scale relates the model to its real-world counterpart. One can see that the scale of the Einstein model, which is given by\n\nis formula_111. And the scale of the Debye model is formula_29, the Debye temperature. Both are usually found by fitting the models to the experimental data. (The Debye temperature can theoretically be calculated from the speed of sound and crystal dimensions.) Because the two methods approach the problem from different directions and different geometries, Einstein and Debye scales are not the same, that is to say\n\nwhich means that plotting them on the same set of axes makes no sense. They are two models of the same thing, but of different scales. If one defines Einstein temperature as\n\nthen one can say\n\nand, to relate the two, we must seek the ratio\n\nThe Einstein solid is composed of single-frequency quantum harmonic oscillators, formula_117. That frequency, if it indeed existed, would be related to the speed of sound in the solid. If one imagines the propagation of sound as a sequence of atoms \"hitting\" one another, then it becomes obvious that the frequency of oscillation must correspond to the minimum wavelength sustainable by the atomic lattice, formula_118.\n\nwhich makes the Einstein temperature\n\nand the sought ratio is therefore\n\nNow both models can be plotted on the same graph. Note that this ratio is the cube root of the ratio of the volume of one octant of a 3-dimensional sphere to the volume of the cube that contains it, which is just the correction factor used by Debye when approximating the energy integral above.\n\nAlternately the ratio of the 2 temperatures can be seen to be the ratio of Einstein's single frequency at which all oscillators oscillate and Debye's maximum frequency. Einstein's single frequency can then be seen to be a mean of the frequencies available to the Debye model.\n\nEven though the Debye model is not completely correct, it gives a good approximation for the low temperature heat capacity of insulating, crystalline solids where other contributions (such as highly mobile conduction electrons) are negligible. For metals, the electron contribution to the heat is proportional to formula_47, which at low temperatures dominates the Debye formula_1 result for lattice vibrations. In this case, the Debye model can only be said to approximate for the lattice \"contribution\" to the specific heat. The following table lists Debye temperatures for several pure elements and Sapphire:\nThe Debye model's fit to experimental data is often phenomenologically improved by allowing the Debye temperature to become temperature dependent; for example, the value for water ice increases from about 222 K to 300 K as the temperature goes from Absolute zero to about 100 K.\n\nFor other bosonic quasi-particles, e.g., for magnons (quantized spin waves) in ferromagnets instead of the phonons (quantized sound waves) one easily derives analogous results. In this case at low frequencies one has different dispersion relations, e.g., formula_124 in the case of magnons, instead of formula_125 for phonons (with formula_126). One also has different density of states (e.g., formula_127). As a consequence, in ferromagnets one gets a magnon contribution\nto the heat capacity, formula_128, which dominates at sufficiently low temperatures the phonon contribution, formula_129. In metals, in contrast, the main low-temperature contribution to the heat capacity, formula_130, comes from the electrons. It is fermionic, and is calculated by different methods going back to Sommerfeld's free electron model.\n\nIt was long thought that phonon theory is not able to explain the heat capacity of liquids, since liquids only sustain longitudinal, but not transverse phonons, which in solids are responsible for 2/3 of the heat capacity. However, Brillouin scattering experiments with neutrons and with X-rays, confirming an intuition of Yakov Frenkel, have shown that transverse phonons do exist in liquids, albeit restricted to frequencies above a threshold called the Frenkel frequency. Since most energy is contained in these high-frequency modes, a simple modification of the Debye model is sufficient to yield a good approximation to experimental heat capacities of simple liquids.\n\n\n\n"}
{"id": "46600473", "url": "https://en.wikipedia.org/wiki?curid=46600473", "title": "Descent along torsors", "text": "Descent along torsors\n\nIn mathematics, given a \"G\"-torsor \"X\" → \"Y\" and a stack \"F\", the descent along torsors says there is a canonical equivalence between \"F\"(\"Y\"), the category of \"Y\"-points and \"F\"(\"X\"), the category of \"G\"-equivariant \"X\"-points. It is a basic example of descent, since it says the \"equivariant data\" (which is an additional data) allows one to \"descend\" from \"X\" to \"Y\".\n\nWhen \"G\" is the Galois group of a finite Galois extension \"L\"/\"K\", for the \"G\"-torsor formula_1, this generalizes classical Galois descent (cf. field of definition).\n\nFor example, one can take \"F\" to be the stack of quasi-coherent sheaves (in an appropriate topology). Then \"F\"(\"X\") consists of equivariant sheaves on \"X\"; thus, the descent in this case says that to give an equivariant sheaf on \"X\" is to give a sheaf on the quotient \"X\"/\"G\".\n\n\n"}
{"id": "4414414", "url": "https://en.wikipedia.org/wiki?curid=4414414", "title": "Electride", "text": "Electride\n\nAn electride is a ionic compound in which an electron is the anion. Solutions of alkali metals in ammonia are electride salts. In the case of sodium, these blue solutions consist of [Na(NH)] and solvated electrons: \nThe cation [Na(NH)] is an octahedral coordination complex.\n\nAddition of a complexant like crown ether or 2,2,2-cryptand to a solution of [Na(NH)]e affords [Na(crown ether)]e or [Na(2,2,2-crypt)]e. Evaporation of these solutions yields a blue-black paramagnetic salt with the formula [Na(2,2,2-crypt)]e.\n\nMost solid electride salts decompose above 240 K, although [CaAlO](e) is stable at room temperature. In these salts, the electron is delocalized between the cations. Electrides are paramagnetic and Mott insulators. Properties of these salts have been analyzed.\n\nSolutions of electride salts are powerful reducing agents, as demonstrated by their use in the Birch reduction. Evaporation of these blue solutions affords a mirror of Na. Such solutions slowly lose their colour as the electrons reduce ammonia:\nThis conversion is catalyzed by various metals. An electride, [Na(NH)]e, is formed as a reaction intermediate.\n\nTheoretical evidence supports electride behaviour in insulating high-pressure forms of potassium, sodium, and lithium. Here the isolated electron is stabilized by efficient packing, which reduces enthalpy under external pressure. The electride is identified by a maximum in the electron localization function, which distinguishes the electride from pressure-induced metallization. Electride phases are typically semiconducting or have very low conductivity, usually with a complex optical response.\n\n\n"}
{"id": "39208945", "url": "https://en.wikipedia.org/wiki?curid=39208945", "title": "Energy transition", "text": "Energy transition\n\nEnergy transition is generally defined as a long-term structural change in energy systems. These have occurred in the past, and still occur worldwide. Historic energy transitions are most broadly described by Vaclav Smil. Contemporary energy transitions differ in terms of motivation and objectives, drivers and governance.\n\nThe layout of the world’s energy systems have changed significantly over time. Until the 1950s, the economic mechanism behind energy systems was local rather than global. As development progressed, different national systems became more and more integrated becoming the large, international systems seen today. Historical transition rates of energy systems have been extensively studied. While historical energy transitions were generally protracted affairs, unfolding over many decades, this does not necessarily hold true for the present energy transition, which is unfolding under very different policy and technological conditions.\n\nSolving the global warming problem is regarded as the most important challenge facing humankind in the 21st century. The capacity of the earth system to absorb greenhouse gas emissions is already exhausted, and under the Paris climate agreement, current emissions must be fully stopped until 2040 or 2050. Barring a breakthrough in carbon sequestration technologies, this requires an energy transition away from fossil fuels such as oil, natural gas, lignite, and coal. This energy transition is also known as the decarbonization of the energy system. Available technologies are nuclear fuel (uranium) and the renewable energy sources wind, hydropower, solar power, geothermal, and Marine energy.\n\nA timely implementation for the energy transition requires multiple approaches in parallel. Energy conservation and improvements in energy efficiency thus play a major role. Smart electric meters can schedule energy consumption for times when electricity is available abundantly, reducing consumption at times when the more volatile renewable energy sources are scarce (night time and lack of wind).\n\nAfter a transitional period, renewable energy production is expected to make up most of the world's energy production. The risk management firm, DNV GL, forecasts that, by 2050, the world's primary energy mix will be split equally between fossil and non-fossil sources. A 2011 projection by the International Energy Agency expects solar PV to supply more than half of the world's electricity by 2060, dramatically reducing the emissions of greenhouse gases.\n\nAn example of transition toward sustainable energy, is the shift by Germany () and Switzerland, to decentralised renewable energy, and energy efficiency. Although so far these shifts have been replacing nuclear energy, their declared goal 2012 was the abolishment of coal, reducing non-renewable energy sources and the creation of an energy system based on 60% renewable energy by 2050. As of 2018, the 2030 coalition goals are to achieve 65% renewables in electricity production until 2030 in Germany.\n\nAn 'energy transition' designates a significant change for an energy system that could be related to one or a combination of system structure, scale, economics, and energy policy. An 'energy transition' is usefully defined as a change in the state of an energy system as opposed to a change in an individual energy technology or fuel source. A prime example is the change from a pre-industrial system relying on traditional biomass and other renewable power sources (wind, water, and muscle power) to an industrial system characterized by pervasive mechanization (steam power) and the use of coal. Market shares reaching pre-specified thresholds are typically used to characterize the speed of transition (e.g. coal versus traditional biomass) and typical market share thresholds in the literature are 1%, 10% for the initial shares and 50%, 90% and 99% for outcome shares following a transition.\n\nFor energy systems, many lessons can be learned from history. The need for large amounts of firewood in early industrial processes in combination with prohibitive costs for overland transportation led to a scarcity of accessible (e.g. affordable) wood and it has been found that eighteenth century glass-works “operated like a forest clearing enterprise. When Britain had to resort to coal after largely having run out of wood, the resulting fuel crisis triggered a chain of events that two centuries later culminated in the Industrial Revolution. Similarly, increased use of peat and coal was vital elements paving the way for the Dutch Golden Age roughly spanning the entire 17th century. Another example where resource depletion triggered technological innovation and a shift to new energy sources in 19th Century whaling and how whale oil eventually became replaced by kerosene and other petroleum-derived products.\n\nTechnology has been identified as an important but difficult-to-predict driver of change within energy systems. Published forecasts have systematically tended to overestimate the potential of new energy and conversion technologies and underestimated the inertia in energy systems and energy infrastructure (e.g. power plants, once built, characteristically operate for many decades). The history of large technical systems is very useful for enriching debates about energy infrastructures by detailing many of their long-term implications. The speed at which a transition in the energy sector needs to take place will be historically rapid. Moreover, the underlying technological, political and economic structures will need to change radically — a process one author calls regime shift.\n\nThe term 'energy transition' could also encompasses a reorientation of policy and this is often the case in public debate about energy policy. For example, this could imply a rebalance of demand to supply and a shift from centralized to distributed generation (for example, producing heat and power in very small cogeneration units), which should replace overproduction and avoidable energy consumption with energy-saving measures and increased efficiency. In a broader sense the energy transition could also entail a democratization of energy or a move towards increased sustainability.\n\nIn June 2018, at their G20 Summit in Argentina, the G20 Energy Ministers ‘welcome(d) the approach of Argentina’s G20 Presidency, which recognises that there are different possible national paths to achieve cleaner energy systems - while promoting sustainability, resilience and energy security - under the term “transitions” (in plural). This view reflects the fact that each G20 member - according to its stage of development - has a unique and diverse energy system as starting point, with different energy resources, demand dynamics, technologies, stock of capital, geographies and cultures.’\n\nAustria embarked on its energy transition (\"Energiewende\") some decades ago. Due to geographical conditions, energy production in Austria relies heavily on renewable energies, notably hydropower. 78.4% of domestic production in 2013 came from renewable energy, 9.2% from natural gas and 7.2% from petroleum. (rest: waste). On the basis of the Federal Constitutional Law for a Nuclear-Free Austria, no nuclear power stations are in operation in Austria.\nBut domestic energy production makes up only 36% of Austria's total energy consumption, which among other things encompasses transport, electricity production, and heating. In 2013, oil accounts for about 36.2% of total energy consumption, renewable energies 29.8%, gas 20.6%, and coal 9.7%. In the past 20 years, the structure of gross domestic energy consumption has shifted from coal and oil to new renewables, in particular between 2005 and 2013 (plus 60%). The EU target for Austria require a renewables share of 34% by 2020 (gross final energy consumption). Austria is on a good way to achieve this target (32.5% in 2013).\nEnergy transition in Austria can be also seen on the local level, in some villages, towns and regions. For example, the town of Güssing in the state of Burgenland is a pioneer in independent and sustainable energy production. Since 2005, Güssing has already produced significantly more heating (58 gigawatt hours) and electricity (14 GWh) from renewable resources than the city itself needs.\n\nDenmark, as a country reliant on imported oil, was impacted particularly hard by the 1973 oil crisis. This roused public discussions on building nuclear power stations to diversify energy supply. A strong anti-nuclear movement developed, which fiercely criticized nuclear power plans taken up by the government, and this ultimately led to a 1985 resolution not to build any nuclear power stations in Denmark. The country instead opted for renewable energy, focusing primarily on wind power. Wind turbines for power generation already had a long history in Denmark, as far back as the late 1800s. As early as 1974 a panel of experts declared \"that it should be possible to satisfy 10% of Danish electricity demand with wind power, without causing special technical problems for the public grid.\" Denmark undertook the development of large wind power stations — though at first with little success (like with the Growian project in Germany).\n\nSmall facilities prevailed instead, often sold to private owners such as farms. Government policies promoted their construction; at the same time, positive geographical factors favored their spread, such as good wind power density and Denmark's decentralized patterns of settlement. A lack of administrative obstacles also played a role. Small and robust systems came on line, at first in the power range of only 50-60 kilowatts — using 1940s technology and sometimes hand-crafted by very small businesses. In the late seventies and the eighties a brisk export trade to the United States developed, where wind energy also experienced an early boom. In 1986 Denmark already had about 1200 wind power turbines, though they still accounted for just barely 1% of Denmark's electricity. This share increased significantly over time. In 2011, renewable energies covered 41% of electricity consumption, and wind power facilities alone accounted for 28%. The government aims to increase wind energy's share of power generation to 50% by 2020, while at the same time reducing carbon dioxide emissions by 40%.\nOn 22 March 2012, the Danish Ministry of Climate, Energy and Building published a four-page paper titled \"DK Energy Agreement,\" outlining long-term principles for Danish energy policy.\n\nThe installation of oil and gas heating is banned in newly constructed buildings from the start of 2013; beginning in 2016 this will also apply to existing buildings. At the same time an assistance program for heater replacement was launched. Denmark's goal is to reduce the use of fossil fuels 33% by 2020. The country is scheduled to attain complete independence from petroleum and natural gas by 2050.\n\nSince 2012, political discussions have been developing in France about the energy transition and how the French economy might profit from it.\n\nIn September 2012, Minister of the Environment Delphine Batho coined the term \"ecological patriotism.\" The government began a work plan to consider starting the energy transition in France. This plan should address the following questions by June 2013:\n\nThe Environmental Conference on Sustainable Development on 14 and 15 September 2012 treated the issue of the environmental and energy transition as its main theme.\n\nOn 8 July 2013, the national debate leaders submits some proposals to the government. Among them, there were environmental taxation, and smart grid development.\n\nIn 2015, the National Assembly has adopted legislation for the transition to low emission vehicles.\n\nFrance is second only to Denmark as having the worlds lowest carbon emissions in relation to gross domestic product.\n\nThe key policy document outlining the \"Energiewende\" was published by the German government in September 2010, some six months before the Fukushima nuclear accident.\nLegislative support was passed in September 2010. Important aspects include:\n\nIn addition, there will be an associated research and development drive.\n\nThe policy has been embraced by the German federal government and has resulted in a huge expansion of renewables, particularly wind power. Germanys share of renewables has increased from around 5% in 1999 to 17% in 2010, reaching close to the OECD average of 18% usage of renewables. Producers have been guaranteed a fixed feed-in tariff for 20 years, guaranteeing a fixed income. Energy co-operatives have been created, and efforts were made to decentralize control and profits. The large energy companies have a disproportionately small share of the renewables market. Nuclear power stations were closed, and the existing nine stations will close earlier than necessary, in 2022.\n\nThe reduction of reliance on nuclear stations has had the consequence of increased reliance on fossil fuels. One factor that has inhibited efficient employment of new renewable energy has been the lack of an accompanying investment in power infrastructure to bring the power to market. It is believed 8300 km of power lines must be built or upgraded.\n\nDifferent Länder have varying attitudes to the construction of new power lines. Industry has had their rates frozen and so the increased costs of the \"Energiewende\" have been passed on to consumers, who have had rising electricity bills. Germans in 2013 had some of the highest electricity costs in Europe. Nonetheless, for the first time in more than ten years, electricity prices for household customers fell at the beginning of 2015.\n\nOn 14 September 2012 the Japanese government decided at a ministerial meeting in Tokyo to phase out nuclear power by the 2030s, or 2040 at the very latest. The government said that it would take \"all possible measures\" to achieve this goal. A few days later the government retrenched the planned nuclear phaseout after the industry pushed for reconsideration. Arguments cited were that a nuclear phaseout would burden the economy, and that imports of oil, coal, and gas would bring high added costs. The government then approved the energy transition, but left open the time-frame for decommissioning the nuclear power stations.\n\nThe United Kingdom is mainly focusing on wind power, both onshore and offshore, and in particular is strongly promoting the establishment of offshore wind power. With an installed capacity of 18.8 GW at the end of 2017, Britain is one of the worldwide leaders taking the sixth place, after China, the United States, Germany, India, and Spain. It was initially promoted with a quota system, but expansion targets were missed repeatedly. This led the government to implement a feed-in tariff instead.\n\nThe Obama administration made a large push for green jobs, particularly in his first term.\n\nIn the United States, the share of renewable energy (excluding hydropower) in electricity generation has grown from 3.3 percent (1990) to 5.5 percent (2013). Oil use will decline in the USA owing to the increasing efficiency of the vehicle fleet and replacement of crude oil by natural gas as a feedstock for the petrochemical sector. One forecast is that the rapid uptake of electric vehicles will reduce oil demand drastically, to the point where it is 80% lower in 2050 compared with today.\n\nIn December 2016, Block Island Wind Farm became the first commercial US offshore wind farm. It consists of five 6 MW turbines (together 30 MW) located \"near-shore\" ( from Block Island, Rhode Island) in the Atlantic Ocean.\nAt the same time, Norway-based oil major Statoil laid down nearly $42.5 million on a bid to lease a large offshore area off the coast of New York.\n\n\n\n"}
{"id": "7898117", "url": "https://en.wikipedia.org/wiki?curid=7898117", "title": "Environmental Forum for Action", "text": "Environmental Forum for Action\n\nThe Environmental Forum for Action (ENFORAC) is a coalition of 16 environmental non-governmental organisations, community groups and academic institutions that have come together as a united voice to protect and advocate for Sierra Leone’s natural resources.\n\nFormed in 2004, it was not officially launched until April 2006.\n\n\nBetween 2009-2014 The Environmental Forum for Action was Implementing Partner in the EU funded project:\n\"Conserving the Western Area Peninsula Forest and its Watersheds\"-WAPFOR.\nGerman-Agro Action, later known as Welthungerhilfe was project holder. The action resulted in the declaration of the Western Area Peninsula Forest Reserve as a National Park and tentative scoping steps towards qualification of The Western Area National Park as a candidate for REDD+ carbon credit and UNESCO World Heritage Site declaration. 55,000 indigenous forest fringe residents were educated about forest benefits and provided facilities and equipment to provide incentive for alternative livelihoods for forest destruction activities such as logging, charcoal production and hunting.\n\nCarried through in partnership with the Government of Sierra Leone Forestry Division, the previous forest was re-demarcated with GIS from 17,000 hectares to 18,000 hectares.\n\nA National Protected Area Authority is assuming the mantle of responsibility for this National Park and others in Sierra Leone."}
{"id": "28638765", "url": "https://en.wikipedia.org/wiki?curid=28638765", "title": "Flying Eagle Preserve", "text": "Flying Eagle Preserve\n\nFlying Eagle Preserve is located in Inverness, Florida in Citrus County, Florida and managed as part of the Southwest Florida Water Management District. The park is located at 11080 East Moccasin Slough Road in Inverness, Florida and bounded on the east by the Withlacoochee River and is surrounded, in large part, by the Tsala Apopka Chain of Lakes. The preserve provides the setting for various aquatic and sporting activities.\n\nMcGregor Smith Scout Reservation was part of the preserve. The preserve also includes Withlapopka Community Park.\n\nWithlapopka Community Park is a 50-acre area that was used by Citrus County as a dumping site for spoil dredged from canals. It includes an unpaved loop trail (about a mile long) benches, picnic pavilion, picnic tables, and grills. It is used for frisbee golf (chipping and driving), horseshoes, tetherball and volleyball. There is also a swing and teeter-totter. It is located at 10390 East Gobbler Drive.\n"}
{"id": "1403749", "url": "https://en.wikipedia.org/wiki?curid=1403749", "title": "Force density", "text": "Force density\n\nIn fluid mechanics, the force density is the negative gradient of pressure. It has the physical dimensions of force per unit volume. Force density is a vector field representing the flux density of the hydrostatic force within the bulk of a fluid. Force density is represented by the symbol f \n, and given by the following equation, where \"P\" is the pressure:\n\nThe net force on a differential volume element \"dV\" of the fluid is:\n\nForce density acts in different ways which is caused by the boundary conditions. There is stick-slip boundary conditions and stick boundary conditions which effect force density.\n\nIn a sphere placed in an arbitrary non-stationary flow field of viscous incompressible fluid for stick boundary conditions where the force density’s calculations leads to show the generalization of Faxen's theorem to force multipole moments of arbitrary order.\n\nIn a sphere moving in an incompressible fluid in a non-stationary flow with mixed stick-slip boundary condition where the force of density shows an expression of the Faxén type for the total force, but the total torque and the symmetric force-dipole moment.\n\nThe force density at a point in a fluid, divided by the density, is the acceleration of the fluid at that point.\n\nThe force density F is defined as the force per unit volume, so that:\n\nThe force density in an electromagnetic field is given in cgs by:\n\nWhere p is the charge density, E is the electric field, J is the current density, c is the speed of light, and B is the magnetic field.\n\n"}
{"id": "31058221", "url": "https://en.wikipedia.org/wiki?curid=31058221", "title": "Ford Ecostar", "text": "Ford Ecostar\n\nThe Ford Ecostar is an experimental electrically-powered small delivery van that was built by Ford Europe. A sodium-sulfur battery in the floor of the cargo area stored power for a electric motor under the front hood. The Ecostar introduced the road-and-leaf logo now used on a number of Ford products.\n\nJust over 100 Ecostars were produced, and used in fleet tests between 1992 and 1996 with over driven. The Ecostar averaged on a full charge, and demonstrated range in one test. However, on several occasions the battery burst into flame during recharging. For this, and several other reasons, Ford lost interest in the sodium-sulfur battery and turned to fuel cell concepts instead.\n\nThe product niche appeared to be a useful one and has led to a number of similar designs. While the 1998 Citroën Berlingo électrique was almost identical in performance and range, it just replaced the older 1991 C15 électrique. Ford is re-entering the market as well, with an electric version of the Transit Connect.\n\nFord developed the sodium-sulfur battery technology in 1965, but had not developed it commercially. Development was later picked up in Europe.\n\nThe Ecostar was introduced as a purely experimental effort, to help develop all aspects of electric vehicle design from engineering to supplier development to market development. A \"prototype-of-the-prototypes\" was completed with lead acid batteries in 1992, and introduced with the comment that future models would include the new battery technology. Several similar models followed and were lent out for test drives with favorable results.\n\nA total of 80 to 105 \"production-prototype\" sulfur-powered Ecostars were hand-built starting in 1993, and used in fleet trials with a number of customers starting the next year. At the time, the cost of the battery was a significant $46,000. However, the vehicles were hand-built at a cost of $250,000, so the battery cost was not representative of a production version.\n\nThe tests ran for 30 months. Throughout the tests there were problems with the system, including two vehicles that burst into flame while charging. The sulfur in the battery was flammable, a serious safety risk. ABB introduced a new version of the battery, but was unwilling to guarantee performance beyond one year.\n\nIn late 1997, Ford announced a partnership with Daimler-Benz and Ballard Power Systems to introduce car-ready fuel cells, and their experiments with the sodium-sulfur batteries ended.\n\nThe Ecostar's chassis was based on the Escort Van produced by Ford's Halewood Body & Assembly factory outside Liverpool. This was essentially a European Escort Mk V with a raised cargo area forming a 2-door panel van. Fully equipped, it weighed .\n\nA significant portion of that weight was its sodium-sulfur battery, which stored 37 kWh. Using the Federal Urban Driving Schedule, this gave the Ecostar a range of . The sodium-sulfur technology was invented by Ford in the 1960s, but the battery for the Ecostar was built by ABB Group in Heidelberg, Germany. In order for the battery to work, it had to be maintained at a temperature of , which keeps the sulfur molten.\n\nTo avoid heat loss, and to insulate the cabin from its heat, the battery was stored inside a double-walled stainless steel vacuum bottle. This allowed it to stay warm overnight, and keep its temperature constant even during cold weather, when traditional batteries are less responsive. The container was mounted below the floor plan of the cargo area, which gave the vehicle a low center of gravity that give it well-liked handling. One complaint, however, was the lack of power steering.\n\nThe battery powered a three-phase electric motor located under the hood in the area normally used for the gasoline engine. The motor delivered only 75 hp, low for a vehicle of this size, but up to 143 foot-pounds of torque, numbers typical of a much more powerful gasoline engine. This was better torque than the Ford Escort GT, for instance, whose 1.8 litre four-cylinder engine delivered 127 hp but only 114 ft-lb.\n\nOne drawback of the sodium-sulfur battery is its relatively slow discharge rates, which limited the amount of power that could be drawn from the battery during acceleration. Drivers described it as \"sedate\", while Ford estimated its 0-60 mph (0-100 km/h) acceleration at about 16.5 seconds. While slow by car standards, this is similar to other small European delivery vans of the era, like the Volkswagen Eurovan.\n\nWhile cruising the motor used only 8 kW to maintain speed, about . This speaks to the motor-to-wheel efficiency of the electric drivetrain, as well as the 50 psi low-drag tires. However, the long quoted ranges referred only to driving, not accessory systems. These added considerably to the load; the electric heater burned 5 kW, and the air conditioner 6 kW. In real-world driving, using either would significantly impact range.\n\n\n"}
{"id": "996410", "url": "https://en.wikipedia.org/wiki?curid=996410", "title": "Forwarder", "text": "Forwarder\n\nA forwarder is a forestry vehicle that carries big felled logs from the stump to a roadside landing. Unlike a skidder, a forwarder carries logs clear of the ground, which can reduce soil impacts but tends to limit the size of the logs it can move. Forwarders are typically employed together with harvesters in cut-to-length logging operations.\n\nForwarders are commonly categorised on their load carrying capabilities. The smallest are trailers designed for towing behind all-terrain vehicles which can carry a load between 1 and 3 tonnes. Agricultural self-loading trailers designed to be towed by farm tractors can handle load weights up to around 12 to 15 tonnes. Light weight purpose-built machines utilised in commercial logging and early thinning operations can handle payloads of up to 8 tonnes. Medium-sized forwarders used in clearfells and later thinnings carry between 12 and 16 tonnes. The largest class specialized for clearfells handles up to 25 tonnes. Forwarders also carry their load at least 2 feet above the ground.\n\n"}
{"id": "12703710", "url": "https://en.wikipedia.org/wiki?curid=12703710", "title": "Gas composition", "text": "Gas composition\n\nThe Gas composition of any gas can be characterised by listing the pure substances it contains, and stating for each substance its proportion of the gas mixture's molecule count.\n\nTo give a familiar example, air has a composition of:\nStandard Dry Air is the agreed-upon gas composition for air from which all water vapour has been removed. There are various standards bodies which publish documents that define a dry air gas composition. Each standard provides a list of constituent concentrations, a gas density at standard conditions and a molar mass.\n\nIt is extremely unlikely that the actual composition of any specific sample of air will completely agree with any definition for standard dry air. While the various definitions for standard dry air all attempt to provide realistic information about the constituents of air, the definitions are important in and of themselves because they establish a standard which can be cited in legal contracts and publications documenting measurement calculation methodologies or equations of state.\n\nThe standards below are two examples of commonly used and cited publications that provide a composition for standard dry air:\n\n\n"}
{"id": "7073521", "url": "https://en.wikipedia.org/wiki?curid=7073521", "title": "Hardwood timber production", "text": "Hardwood timber production\n\nHardwood timber production is the process of managing stands of deciduous trees to maximize woody output. The production process is not linear because other factors must be considered, including marketable and non-marketable goods, financial benefits, management practices, and the environmental implications ,of those management practices.\n\nForests include market and non-market products. Marketable products include goods that have a market price. Timber is the main one, with prices that range from a few hundred dollars per thousand board feet (MBF) to several thousand dollars for a veneer log. Others include grazing/fodder, specialty crops such as mushrooms or berries, usage fees for recreation or hunting, and biomass. Forests also provide some non-market values which have no current market price. Examples of non-market goods would be improving water quality, air quality, aesthetics, and carbon sequestration. \n\nThe more biodiverse the hardwood-forest ecosystem, the more challenges and opportunities its managers face. Managers aim for sustainable forest management to keep their cash crop renewing itself, using silvicultural practices that include growing, selling, controlling insects and most diseases, providing manure, applying herbicle treatments, and thinning. Fertilization can stop the growth rate and amount of plant material, thus possibly increasing the number of wildlife that can inhabit a site. Invasive species control maintains an area's structure and native composition.\n\nBut management can also harm the ecosystem; for example, machinery used in a timber harvest can compact the soil, stress the root system, reduce tree growth, lengthen the time needed for a stand to mature to harvestability. Machinery can also damage the understory, disturbing wildlife habitat and prevent regeneration.\n\n\n"}
{"id": "25044351", "url": "https://en.wikipedia.org/wiki?curid=25044351", "title": "Holmside Hall Wind Farm", "text": "Holmside Hall Wind Farm\n\nHolmside Hall Wind Farm is a wind farm near Stanley, County Durham, England. Owned and operated by E.ON UK, the farm has a nameplate capacity of 5.5MW, containing two NM80 turbines each rated at 2.75 MW.\n\nAt the time of construction, which was delayed due to high winds, the turbines were the largest and most powerful in the UK.\n"}
{"id": "26655421", "url": "https://en.wikipedia.org/wiki?curid=26655421", "title": "Hwange Thermal Power Station", "text": "Hwange Thermal Power Station\n\nThe Hwange Thermal Power Station is the biggest power plant in Zimbabwe with an installed capacity of 920 MW. It is owned and driven by the national electricity company ZESA Holdings (Pvt) LTD.\n\nIt was built in two stages and consists of 4 units of 120 MW each and 2 units of 220 MW each. Engineering Consultants, Merz & McLellan, were employed for the design and supervision of the construction of the power station. Construction of Stage 1 commenced in 1973, but was suspended in 1975 due to economic sanctions imposed on Rhodesia. Stage 1's units were commissioned from 1983 to 1986 with Stage 2's units following in 1986/87.\n\nA reliable source of water lies further north, in the Zambezi River. From there, through a 44 kilometre long pipeline, water for the Boilers and Cooling Towers is drawn by both high and low lift pumps to a storage reservoir located adjacent to the station and conveyed by gravity to the station. About 107,000 cubic meters of raw water can be provided per day; while the demineralisation plant has a capacity of 5,420 cubic metres per day.\n\nA 3.5 kilometre conveyor belt brings about 1,750 tonnes of coal per hour from the nearby Wankie colliery open cast mine, and 250,000 tonnes of coal are stockpiled on site. Coal reserves estimated to support 1,200 MW for an estimated 30 years are concealed beneath the vast expanse of the coal mine.\n\nTechnical problems due to neglect of maintenance, part replacement and upgrading make the plant prone to frequent production stops. In 2009, Namibia's NamPower made agreements to help ZESA to revive the plant’s capacity in exchange for power deliveries. The extensive problems are however continuing and have even led the government to considering a full close-down of the plant. In April 2008, Chadha Power of India secured a contract to refurbish four units at the power plant.\n\nIn December 2015 China agreed to provide a $1.2 billion loan to add 600 MW of generating capacity to the Hwange station.\n\n"}
{"id": "3117601", "url": "https://en.wikipedia.org/wiki?curid=3117601", "title": "Hydreliox", "text": "Hydreliox\n\nHydreliox is an exotic breathing gas mixture of helium, oxygen and hydrogen. For the Hydra VIII mission at 50 atmospheres of ambient pressure, the mixture used was 49% hydrogen, 50.2% helium, and 0.8% oxygen.\n\nIt is used primarily for research and scientific deep diving, usually below . Below this depth, extended breathing of heliox gas mixtures may cause high pressure nervous syndrome (HPNS). Two gas mixtures exist that attempt to combat this problem: trimix and hydreliox. Like trimix, hydreliox contains helium and oxygen and a third gas to counteract HPNS. The third gas in trimix is nitrogen and the third gas in hydreliox is hydrogen. Because hydrogen is the lightest element, it is easier to breathe than nitrogen under high pressure. To avoid the risk of explosion, as a rule of thumb hydrogen is only considered for use in breathing mixtures if the proportion of oxygen in the mixture is less than 5%. However, the pressure during the dive must be such that the partial pressure of 5% oxygen is sufficient to sustain the diver. (The flammability of the mixture also depends to some degree on the pressure)\n\nHydreliox has been tested to simulated depths in excess of in a chamber by COMEX S.A., a French diving company. Although breathing hydreliox improves the symptoms seen in HPNS, tests have shown that hydrogen narcosis becomes a factor at depths of .\n\n"}
{"id": "14749", "url": "https://en.wikipedia.org/wiki?curid=14749", "title": "Indium", "text": "Indium\n\nIndium is a chemical element with symbol In and atomic number 49. It is a post-transition metal that makes up 0.21 parts per million of the Earth's crust. Very soft and malleable, indium has a melting point higher than sodium and gallium, but lower than lithium and tin. Chemically, indium is similar to gallium and thallium, and it is largely intermediate between the two in terms of its properties. Indium was discovered in 1863 by Ferdinand Reich and Hieronymous Theodor Richter by spectroscopic methods. They named it for the indigo blue line in its spectrum. Indium was isolated the next year.\n\nIndium is a minor component in zinc sulfide ores and is produced as a byproduct of zinc refinement. It is most notably used in the semiconductor industry, in low-melting-point metal alloys such as solders, in soft-metal high-vacuum seals, and in the production of transparent conductive coatings of indium tin oxide (ITO) on glass.\n\nIndium has no biological role, though its compounds are somewhat toxic when injected into the bloodstream. Most occupational exposure is through ingestion, from which indium compounds are not absorbed well, and inhalation, from which they are moderately absorbed.\n\nIndium is a silvery-white, highly ductile post-transition metal with a bright luster. It is so soft (Mohs hardness 1.2) that like sodium, it can be cut with a knife. It also leaves a visible line on paper. It is a member of group 13 on the periodic table and its properties are mostly intermediate between its vertical neighbours gallium and thallium. Like tin, a high-pitched cry is heard when indium is bent – a crackling sound due to crystal twinning. Like gallium, indium is able to wet glass. Like both, indium has a low melting point, 156.60 °C (313.88 °F); higher than its lighter homologue, gallium, but lower than its heavier homologue, thallium, and lower than tin. The boiling point is 2072 °C (3762 °F), higher than that of thallium, but lower than gallium, conversely to the general trend of melting points, but similarly to the trends down the other post-transition metal groups because of the weakness of the metallic bonding with few electrons delocalized.\n\nThe density of indium, 7.31 g/cm, is also greater than gallium, but lower than thallium. Below the critical temperature, 3.41 K, indium becomes a superconductor. At standard temperature and pressure, indium crystallizes in the face-centered tetragonal crystal system in the space group \"I\"4/\"mmm\" (lattice parameters: \"a\" = 325 pm, \"c\" = 495 pm): this is a slightly distorted face-centered cubic structure, where each indium atom has four neighbours at 324 pm distance and eight neighbours slightly further (336 pm). Indium displays a ductile viscoplastic response, found to be size-independent in tension and compression. However it does have a size effect in bending and indentation, associated to a length-scale of order 50–100 µm, significantly large when compared with other metals.\n\nIndium has 49 electrons, with an electronic configuration of [Kr]4d5s5p. In compounds, indium most commonly donates the three outermost electrons to become indium(III), In. In some cases, the pair of 5s-electrons are not donated, resulting in indium(I), In. The stabilization of the monovalent state is attributed to the inert pair effect, in which relativistic effects stabilize the 5s-orbital, observed in heavier elements. Thallium (indium's heavier homolog) shows an even stronger effect, causing oxidation to thallium(I) to be more probable than to thallium(III), whereas gallium (indium's lighter homolog) commonly shows only the +3 oxidation state. Thus, although thallium(III) is a moderately strong oxidizing agent, indium(III) is not, and many indium(I) compounds are powerful reducing agents. While the energy required to include the s-electrons in chemical bonding is lowest for indium among the group 13 metals, bond energies decrease down the group so that by indium, the energy released in forming two additional bonds and attaining the +3 state is not always enough to outweigh the energy needed to involve the 5s-electrons. Indium(I) oxide and hydroxide are more basic and indium(III) oxide and hydroxide are more acidic.\n\nA number of standard electrode potentials, depending on the reaction under study, are reported for indium, reflecting the decreased stability of the +3 oxidation state:\nIndium metal does not react with water, but it is oxidized by stronger oxidizing agents such as halogens to give indium(III) compounds. It does not form a boride, silicide, or carbide, and the hydride InH has at best a transitory existence in ethereal solutions at low temperatures, being unstable enough to spontaneously polymerize without coordination. Indium is rather basic in aqueous solution, showing only slight amphoteric characteristics, and unlike its lighter homologs aluminium and gallium, it is insoluble in aqueous alkaline solutions.\n\nIndium has 39 known isotopes, ranging in mass number from 97 to 135. Only two isotopes occur naturally as primordial nuclides: indium-113, the only stable isotope, and indium-115, which has a half-life of 4.41 years, four orders of magnitude greater than the age of the universe and nearly 30,000 times greater than that of natural thorium. The half-life of In is very long because the beta decay to Sn is spin-forbidden. Indium-115 makes up 95.7% of all indium. Indium is one of three known elements (the others being tellurium and rhenium) of which the stable isotope is less abundant in nature than the long-lived primordial radioisotopes.\n\nThe stablest artificial isotope is indium-111, with a half-life of approximately 2.8 days. All other isotopes have half-lives shorter than 5 hours. Indium also has 47 meta states, among which indium-114m1 (half-life about 49.51 days) is the most stable, more stable than the ground state of any indium isotope other than the primordial. All decay by isomeric transition. The indium isotopes lighter than In predominantly decay through electron capture or positron emission to form cadmium isotopes, while the other indium isotopes from In and greater predominantly decay through beta-minus decay to form tin isotopes.\n\nIndium(III) oxide, InO, forms when indium metal is burned in air or when the hydroxide or nitrate is heated. InO adopts a structure like alumina and is amphoteric, that is able to react with both acids and bases. Indium reacts with water to reproduce soluble indium(III) hydroxide, which is also amphoteric; with alkalis to produce indates(III); and with acids to produce indium(III) salts:\n\nThe analogous sesquichalcogenides with sulfur, selenium, and tellurium are also known. Indium forms the expected trihalides. Chlorination, bromination, and iodination of In produce colorless InCl, InBr, and yellow InI. The compounds are Lewis acids, somewhat akin to the better known aluminium trihalides. Again like the related aluminium compound, InF is polymeric.\n\nDirect reaction of indium with the pnictogens produces the gray or semimetallic III–V semiconductors. Many of them slowly decompose in moist air, necessitating careful storage of semiconductor compounds to prevent contact with the atmosphere. Indium nitride is readily attacked by acids and alkalis.\n\nIndium(I) compounds are not common. The chloride, bromide, and iodide are deeply colored, unlike the parent trihalides from which they are prepared. The fluoride is known only as an unstable gaseous compound. Indium(I) oxide black powder is produced when indium(III) oxide decomposes upon heating to 700 °C.\n\nLess frequently, indium forms compounds in oxidation state +2 and even fractional oxidation states. Usually such materials feature In–In bonding, most notably in the halides InX and [InX], and various subchalcogenides such as InSe. Several other compounds are known to combine indium(I) and indium(III), such as In(InCl)Cl, In(InBr)(InBr), InInBr.\n\nOrganoindium compounds feature In–C bonds. Most are In(III) derivatives, but cyclopentadienylindium(I) is an exception. It was the first known organoindium(I) compound, and is polymeric, consisting of zigzag chains of alternating indium atoms and cyclopentadienyl complexes. Perhaps the best-known organoindium compound is trimethylindium, In(CH), used to prepare certain semiconducting materials.\n\nIn 1863, the German chemists Ferdinand Reich and Hieronymous Theodor Richter were testing ores from the mines around Freiberg, Saxony. They dissolved the minerals pyrite, arsenopyrite, galena and sphalerite in hydrochloric acid and distilled raw zinc chloride. Reich, who was color-blind, employed Richter as an assistant for detecting the colored spectral lines. Knowing that ores from that region sometimes contain thallium, they searched for the green thallium emission spectrum lines. Instead, they found a bright blue line. Because that blue line did not match any known element, they hypothesized a new element was present in the minerals. They named the element indium, from the indigo color seen in its spectrum, after the Latin \"indicum\", meaning 'of India'.\n\nRichter went on to isolate the metal in 1864. An ingot of was presented at the World Fair 1867. Reich and Richter later fell out when the latter claimed to be the sole discoverer.\n\nIndium is created by the long-lasting (up to thousands of years) s-process (slow neutron capture) in low-to-medium-mass stars (which range in mass between 0.6 and 10 solar masses). When a silver-109 atom (the isotope that comprises approximately half of all silver in existence) catches a neutron, it undergoes a beta decay to become cadmium-110. Capturing further neutrons, it becomes cadmium-115, which decays to indium-115 by another beta decay. This explains why the radioactive isotope is more abundant than the stable one. The stable indium isotope, indium-113, is one of the p-nuclei, the origin of which is not fully understood; although indium-113 is known to be made directly in the s- and r-processes (rapid neutron capture), and also as the daughter of very long-lived cadmium-113, which has a half-life of about eight quadrillion years, this cannot account for all indium-113.\n\nIndium is the 68th most abundant element in Earth's crust at approximately 50 ppb. This is similar to the crustal abundance of silver, bismuth and mercury. It very rarely forms its own minerals, or occurs in elemental form. Fewer than 10 indium minerals such as roquesite (CuInS) are known, and none occur at sufficient concentrations for economic extraction. Instead, indium is usually a trace constituent of more common ore minerals, such as sphalerite and chalcopyrite. From these, it can be extracted as a by-product during smelting. While the enrichment of indium in these deposits is high relative to its crustal abundance, it is insufficient, at current prices, to support extraction of indium as the main product.\n\nDifferent estimates exist of the amounts of indium contained within the ores of other metals. However, these amounts are not extractable without mining of the host materials (see Production and availability). Thus, the availability of indium is fundamentally determined by the \"rate\" at which these ores are extracted, and not their absolute amount. This is an aspect that is often forgotten in the current debate, e.g. by the Graedel group at Yale in their criticality assessments, explaining the paradoxically low depletion times some studies cite.\n\nIndium is produced exclusively as a by-product during the processing of the ores of other metals. Its main source material are sulfidic zinc ores, where it is mostly hosted by sphalerite. Minor amounts are probably also extracted from sulfidic copper ores. During the roast-leach-electrowinning process of zinc smelting, indium accumulates in the iron-rich residues. From these, it can be extracted in different ways. It may also be recovered directly from the process solutions. Further purification is done by electrolysis. The exact process varies with the mode of operation of the smelter.\n\nIts by-product status means that indium production is constrained by the amount of sulfidic zinc (and copper) ores extracted each year. Therefore, its availability needs to be discussed in terms of supply potential. The supply potential of a by-product is defined as that amount which is economically extractable from its host materials \"per year\" under current market conditions (i.e. technology and price). Reserves and resources are not relevant for by-products, since they \"cannot\" be extracted independently from the main-products. Recent estimates put the supply potential of indium at a minimum of 1,300 t/yr from sulfidic zinc ores and 20 t/yr from sulfidic copper ores. These figures are significantly greater than current production (655 t in 2016). Thus, major future increases in the by-product production of indium will be possible without significant increases in production costs or price. The average indium price in 2016 was 240/kg, down from 705/kg in 2014.\n\nChina is a leading producer of indium (290 tonnes in 2016), followed by South Korea (195 t), Japan (70 t) and Canada (65 t). The Teck Resources refinery in Trail, British Columbia, is a large single-source indium producer, with an output of 32.5 tonnes in 2005, 41.8 tonnes in 2004 and 36.1 tonnes in 2003.\n\nThe primary consumption of indium worldwide is LCD production. Demand rose rapidly from the late 1990s to 2010 with the popularity of LCD computer monitors and television sets, which now account for 50% of indium consumption. Increased manufacturing efficiency and recycling (especially in Japan) maintain a balance between demand and supply. According to the UNEP, indium's end-of-life recycling rate is less than 1%.\n\nIn 1924, indium was found to have a valued property of stabilizing non-ferrous metals, and that became the first significant use for the element. The first large-scale application for indium was coating bearings in high-performance aircraft engines during World War II, to protect against damage and corrosion; this is no longer a major use of the element. New uses were found in fusible alloys, solders, and electronics. In the 1950s, tiny beads of indium were used for the emitters and collectors of PNP alloy-junction transistors. In the middle and late 1980s, the development of indium phosphide semiconductors and indium tin oxide thin films for liquid-crystal displays (LCD) aroused much interest. By 1992, the thin-film application had become the largest end use.\n\nIndium(III) oxide and indium tin oxide (ITO) are used as a transparent conductive coating on glass substrates in electroluminescent panels. Indium tin oxide is used as a light filter in low-pressure sodium-vapor lamps. The infrared radiation is reflected back into the lamp, which increases the temperature within the tube and improves the performance of the lamp.\n\nIndium has many semiconductor-related applications. Some indium compounds, such as indium antimonide and indium phosphide, are semiconductors with useful properties: one precursor is usually trimethylindium (TMI), which is also used as the semiconductor dopant in II–VI compound semiconductors. InAs and InSb are used for low-temperature transistors and InP for high-temperature transistors. The compound semiconductors InGaN and InGaP are used in light-emitting diodes (LEDs) and laser diodes. Indium is used in photovoltaics as the semiconductor copper indium gallium selenide (CIGS), also called CIGS solar cells, a type of second-generation thin-film solar cell. Indium is used in PNP bipolar junction transistors with germanium: when soldered at low temperature, indium does not stress the germanium.\nIndium wire is used as a vacuum seal and a thermal conductor in cryogenics and ultra-high-vacuum applications, in such manufacturing applications as gaskets that deform to fill gaps. Indium is an ingredient in the gallium–indium–tin alloy galinstan, which is liquid at room temperature and replaces mercury in some thermometers. Other alloys of indium with bismuth, cadmium, lead, and tin, which have higher but still low melting points (between 50 and 100 °C), are used in fire sprinkler systems and heat regulators.\n\nIndium is one of many substitutes for mercury in alkaline batteries to prevent the zinc from corroding and releasing hydrogen gas. Indium is added to some dental amalgam alloys to decrease the surface tension of the mercury and allow for less mercury and easier amalgamation.\n\nIndium's high neutron-capture cross-section for thermal neutrons makes it suitable for use in control rods for nuclear reactors, typically in an alloy of 80% silver, 15% indium, and 5% cadmium. In nuclear engineering, the (n,n') reactions of In and In are used to determine magnitudes of neutron fluxes.\n\nIndium has no metabolic role in any organism. In a similar way to aluminium salts, indium(III) ions can be toxic to the kidney when given by injection. Indium tin oxide and indium phosphide harm the pulmonary and immune systems, predominantly through ionic indium, though hydrated indium oxide is more than forty times as toxic when injected, measured by the quantity of indium introduced. Radioactive indium-111 (in very small amounts on a chemical basis) is used in nuclear medicine tests, as a radiotracer to follow the movement of labeled proteins and white blood cells in the body. Indium compounds are mostly not absorbed upon ingestion and are only moderately absorbed on inhalation; they tend to be stored temporarily in the muscles, skin, and bones before being excreted, and the biological half-life of indium is about two weeks in humans.\n\nPeople can be exposed to indium in the workplace by inhalation, ingestion, skin contact, and eye contact. The National Institute for Occupational Safety and Health has set a recommended exposure limit (REL) of 0.1 mg/m over an eight-hour workday.\n\n"}
{"id": "14990791", "url": "https://en.wikipedia.org/wiki?curid=14990791", "title": "Institute of Hydrology, Meteorology and Environmental Studies (Colombia)", "text": "Institute of Hydrology, Meteorology and Environmental Studies (Colombia)\n\nThe Institute of Hydrology, Meteorology and Environmental Studies (), also known by its acronym in Spanish, , is a government agency of the Ministry of Environment and Sustainable Development of Colombia. It is in charge of producing and managing the scientific and technical information on the environment of Colombia, and its territorial composition. The IDEAM also serves as the Colombian institute of meteorology and studies the climate of Colombia.\n\nThe IDEAM is charged with obtaining, analyzing, processing and divulging information pertaining to hydrology, hydrogeology, meteorology, and geography of biophysical, geomorphological aspects, and the vegetation and land area to improve the use and care of the biophysical resources or the country.\n\nIt was created on December 22, 1993, when Congress passed \"Law 99 of 1993\", replacing the Colombian Institute of Hydrology, Meteorology, and Land Management (\"Instituto Colombiano de Hidrología, Meteorología y Adecuación de Tierras\" - HIMAT), and it officially started functioning on March 1, 1995.\n"}
{"id": "14655770", "url": "https://en.wikipedia.org/wiki?curid=14655770", "title": "Lobosillo Solar Park", "text": "Lobosillo Solar Park\n\nLobosillo Solar Park is a 12.7 MW-peak photovoltaic power plant located in Lobosillo, Murcia, Spain, making it at the time the third largest photovoltaic (PV) power plant in the world. \n\nThe plant was constructed by Ecostream and uses Sputnik Engineering's SolarMax solar inverters .\n\nIt was opened for operation in September, 2007 \n\n"}
{"id": "36526205", "url": "https://en.wikipedia.org/wiki?curid=36526205", "title": "Local distribution company", "text": "Local distribution company\n\nA local distribution company, or LDC, is a distribution company that maintains the portion of the utility supply grid that is closest to the residential and small commercial consumer. There are many alternative terms depending on the type of utility and its location; in Ontario, Canada, the term is used for both the electrical grid and natural gas supply, while within the UK's National Grid they are known as distribution network operators (DNOs).\n\nLDCs normally buy their power from larger companies, sometimes ones dedicated solely to wholesale supply. They re-sell it to the smaller customer. Larger customers typically buy their power directly from the wholesaler, and do not use the LDC.\n"}
{"id": "8851414", "url": "https://en.wikipedia.org/wiki?curid=8851414", "title": "Low-temperature thermal desorption", "text": "Low-temperature thermal desorption\n\nLow-temperature thermal desorption (LTTD), also known as low-temperature thermal volatilization, thermal stripping, and soil roasting, is an ex-situ remedial technology that uses heat to physically separate petroleum hydrocarbons from excavated soils. Thermal desorbers are designed to heat soils to temperatures sufficient to cause constituents to volatilize and desorb (physically separate) from the soil. Although they are not designed to decompose organic constituents, thermal desorbers can, depending upon the specific organics present and the temperature of the desorber system, cause some organic constituents to completely or partially decompose. The vaporized hydrocarbons are generally treated in a secondary treatment unit (e.g., an afterburner, catalytic oxidation chamber, condenser, or carbon adsorption unit) prior to discharge to the atmosphere. Afterburners and oxidizers destroy the organic constituents. Condensers and carbon adsorption units trap organic compounds for subsequent treatment or disposal.\n\nSome preprocessing and postprocessing of soil is necessary when using LTTD. Excavated soils are first screened to remove large (greater than 2 inches in diameter) objects. These may be sized (e.g., crushed or shredded) and then introduced back into the feed material. After leaving the desorber, soils are cooled, re-moistened to control dust, and stabilized (if necessary) to prepare them for disposal or reuse. Treated soil may be redeposited onsite, used as cover in landfills, or incorporated into asphalt.\n\nLTTD has proven very effective in reducing concentrations of petroleum products including gasoline, jet fuels, kerosene, diesel fuel, heating oils, and lubricating oils. LTTD is applicable to constituents that are volatile at temperatures up to 1,200 °F. Most desorbers operate at temperatures between 300 °F to 1,000 °F. Desorbers constructed of special alloys can operate at temperatures up to 1,200 °F. More volatile products (e.g. gasoline) can be desorbed at the lower operating range, while semivolatile products (e.g. kerosene, diesel fuel) generally need temperatures over 700 °F, and relatively nonvolatile products (e.g., heating oil, lubricating oils) need even higher temperatures. Essentially all soil types are amenable for treatment by LTTD systems. However, different soils may require varying degrees and types of pretreatment. For example, coarse-grained soils (e.g. gravel and cobbles) may require crushing; fine-grained soils that are excessively cohesive (e.g. clay) may require shredding.\n\nState and local regulations specify that petroleum-contaminated soils must be pilot tested, by some soil from the site being processed through the LTTD system (a \"test burn\"). The results of preliminary testing of soil samples should identify the relevant constituent properties, and examination of the machine's performance records should indicate how effective the system will be in treating the soil. However, it should be noted that the proven effectiveness of a particular system for a specific site or waste does not ensure that it will be effective at all sites or that the treatment efficiencies achieved will be acceptable at other sites. If a test burn is conducted, it is important to ensure that the soil tested is representative of average conditions and that enough samples are analyzed before and after treatment to confidently determine whether LTTD will be effective.\n\nOperation of LTTD units requires various permits and demonstration of compliance with permit requirements. Monitoring requirements for LTTD systems are by their nature different from monitoring required at a UST site. Monitoring of LTTD system waste streams (e.g. concentrations of particulates, volatiles, and carbon monoxide in stack gas) are required by the agency or agencies issuing the permits for operation of the facility. The LTTD facility owner/operator is responsible for complying with limits specified by the permits and for other LTTD system operating parameters (e.g. desorber temperature, soil feed rate, afterburner temperature).\n\nThe decision as to whether or not LTTD is a practical remedial alternative depends upon site-specific characteristics (e.g. the location and volume of contaminated soils, site layout). Practicability is also determined by regulatory, logistical, and economic considerations. The economics of LTTD as a remedial option are highly site-specific. Economic factors include:-\n\nThermal desorption systems fall into two general classes—stationary facilities and mobile units. Contaminated soils are excavated and transported to stationary facilities; mobile units can be operated directly onsite. Desorption units are available in a variety of process configurations including rotary desorbers, asphalt plant aggregate dryers, thermal screws, and conveyor furnaces.\n\nThe plasticity of the soil is a measure of its ability to deform without shearing and is to some extent a function of water content. Plastic soils tend to stick to screens and other equipment, and agglomerate into large clumps. In addition to slowing down the feed rate, plastic soils are difficult to treat. Heating plastic soils requires higher temperatures because of the low surface area to volume ratio and increased moisture content. Also, because plastic soils tend to be very fine-grained, organic compounds tend to be tightly sorbed. Thermal treatment of highly plastic soils requires pretreatment, such as shredding or blending with more friable soils or other amendments (e.g. gypsum).\n\nMaterial larger than 2 inches in diameter will need to be crushed or removed. Crushed material is recycled back into the feed to be processed. Coarser-grained soils tend to be free-flowing and do not agglomerate into clumps. They typically do not retain excessive moisture, therefore, contaminants are easily desorbed. Finer-grained soils tend to retain soil moisture and agglomerate into clumps. When dry, they may yield large amounts of particulates that may require recycling after being intercepted in the baghouse.\n\nThe solids processing capacity of a thermal desorption system is inversely proportional to the moisture content of the feed material. The presence of moisture in the excavated soils to be treated in the LTTD unit will determine the residence time required and heating requirements for effective removal of contaminants. In order for desorption of petroleum constituents to occur, most of the soil moisture must be evaporated in the desorber. This process can require significant additional thermal input to the desorber and excessive residence time for the soil in the desorber. Moisture content also influences plasticity which affects handling of the soil. Soils with excessive moisture content (> 20%) must be dewatered. Typical dewatering methods include air drying (if storage space is available to spread the soils), mixing with drier soils, or mechanical dewatering.\n\nThe presence of metals in soil can have two implications:\nAt normal LTTD operating temperatures, heavy metals are not likely to be significantly separated from soils.\n\nHigh concentrations of petroleum products in soil can result in high soil heating values. Heat released from soils can result in overheating and damage to the desorber. Soils with heating values greater than 2,000 Btu/lb require blending with cleaner soils to dilute the high concentration of hydrocarbons. High hydrocarbon concentrations in the offgas may exceed the thermal capacity of the afterburner and potentially result in the release of untreated vapors into the atmosphere. Excessive constituent levels in soil could also potentially result in the generation of vapors in the desorber at concentrations exceeding the lower explosive limit (LEL). If the LEL is exceeded there is a potential for explosion.\n\nThe term \"thermal desorber\" describes the primary treatment operation that heats petroleum-contaminated materials and desorbs organic materials into a purge gas. Mechanical design features and process operating conditions vary considerably among the various types of LTTD systems. Desorption units are: available in four configurations:\n\nAlthough all LTTD systems use heat to separate (desorb) organic contaminants from the soil matrix, each system has a different configuration with its own set of advantages and disadvantages. The decision to use one system over another depends on the nature of the contaminants as well as machine availability, system performance, and economic considerations. System performance may be evaluated on the basis of pilot tests (e.g., test burns) or examination of historical machine performance records. Pilot tests to develop treatment conditions are generally not necessary for petroleum-contaminated soils.\n\nRotary dryer systems use a cylindrical metal reactor (drum) that is inclined slightly from the horizontal. A burner located at one end provides heat to raise the temperature of the soil sufficiently to desorb organic contaminants. The flow of soil may be either cocurrent with or countercurrent to the direction of the purge gas flow. As the drum rotates, soil is conveyed through the drum. Lifters raise the soil, carrying it to near the top of the drum before allowing it to fall through the heated purge gas. Mixing in a rotary dryer enhances heat transfer by convection and allow soils to be rapidly heated. Rotary desorber units are manufactured for a wide range of treatment capacities; these units may be either stationary or mobile.\n\nThe maximum soil temperature that can be obtained in a rotary dryer depends on the composition of the dryer shell. The soil discharge temperature of carbon steel drums is typically 300 to 600 degrees F. Alloy drums are available that can increase the soil discharge temperature to 1,200 degrees F. Most rotary dryers that are used to treat petroleum contaminated soil are made of carbon steel. After the treated soil exits the rotary dryer, it enters a cooling conveyor where water is sprayed on the soil for cooling and dust control. Water addition may be conducted in either a screw conveyor or a pugmill.\n\nBesides the direction of purge gas flow relative to soil feed direction, there is one major difference in configuration between countercurrent and cocurrent rotary dryers. The purge gas from a countercurrent rotary dryer is typically only 350 °F to 500 °F and does not require cooling before entering the baghouse where fine particles are trapped. A disadvantage is that these particles may not have been decontaminated and are typically recycled to the dryer. Countercurrent dryers have several advantages over cocurrent systems. They are more efficient in transferring heat from purge gas to contaminated soil, and the volume and temperature of exit gas are lower, allowing the gas to go directly to a baghouse without needing to be cooled. The cooler exit gas temperature and smaller volume eliminates the need for a cooling unit, which allows downstream processing equipment to be smaller. Countercurrent systems are effective on petroleum products with molecular weights lower than No.2 fuel oil.\n\nIn cocurrent systems, the purge gas is 50 °F to 100 °F hotter than the soil discharge temperature. The result is that the purge gas exit temperature may range from 400 °F to 1,000 °F and cannot go directly to the baghouse. Purge gas first enters an afterburner to decontaminate the fine particles, then goes into a cooling unit prior to introduction into the baghouse. Because of the higher temperature and volume of the purge gas, the baghouse and all other downstream processing equipment must be larger than in a countercurrent system. Cocurrent systems do have several advantages over countercurrent systems: The afterburner is located upstream of the baghouse ensuring that fine particles are decontaminated; and because the heated purge gas is introduced at the same end of the drum as the feed soil, the soil is heated faster, resulting in a longer residence time. Higher temperatures and longer residence time mean that cocurrent systems can be used to treat soils contaminated with heavier petroleum products. Cocurrent systems are effective for light and heavy petroleum products including No. 6 fuel oil, crude oil, motor oil, and lubricating oil.\n\nHot-mix asphalt plants use aggregate that has been processed in a dryer before it is mixed with liquid asphalt. The use of petroleum contaminated soils for aggregate material is widespread. Aggregate dryers may either be stationary or mobile. Soil treatment capacities range from 25-150 tons per hour. The soil may be incorporated into the asphalt as a recycling process or the treated soil may be used for other purposes.\n\nAsphalt rotary dryers are normally constructed of carbon steel and have a soil discharge temperature of 300 °F to 600 °F. Typically, asphalt plant aggregate dryers are identical to the countercurrent rotary desorbers described above and are effective on the same types of contaminants. The primary difference is that an afterburner is not required for incorporation of clean aggregate into the asphalt mix. In some areas, asphalt plants that use petroleum-contaminated soil for aggregate may be required to be equipped with an afterburner.\n\nA thermal screw desorber typically consists of a series of 1-4 augers. The auger system conveys, mixes, and heats contaminated soils to volatilize moisture and organic contaminants into a purge gas stream. Augers can be arranged in series to increase the soil residence time, or they can be configured in parallel to increase throughput capacity. Most thermal screw systems circulate a hot heat-transfer oil through the hollow flights of the auger and return the hot oil through the shaft to the heat transfer fluid heating system. The heated oil is also circulated through the jacketed trough in which each auger rotates. Thermal screws can also be steam-heated. Systems heated with oil can achieve soil temperatures of up to 500 °F, and steam-heated systems can heat soil to approximately 350 °F.\n\nMost of the gas generated during heating of the heat-transfer oil does not come into contact the waste material and can be discharged directly to the atmosphere without emission controls. The remainder of the flue gas maintains the thermal screw purge gas exit temperature above 300 degrees F. This ensures that volatilized organics and moisture do not condense. In addition, the recycled flue gas has a low oxygen content (less than 2% by volume) which minimizes oxidation of the organics and reduces the explosion hazard. If pretreatment analytical data indicates a high organic content (greater than 4 percent), use of a thermal screw is recommended. After the treated soil exits the thermal screw, water is sprayed on the soil for cooling and dust control. Thermal screws are available with soil treatment capacities ranging from 3-15 tons per hour.\n\nSince thermal screws are indirectly heated, the volume of purge gas from the primary thermal treatment unit is less than one half of the volume from a directly heated system with an equivalent soil processing capacity. Therefore, offgas treatment systems consist of relatively small unit operations that are well suited to mobile applications. Indirect heating also allows thermal screws to process materials with high organic contents since the recycled flue gas is inert, thereby reducing the explosion hazard.\n\nA conveyor furnace uses a flexible metal belt to convey soil through the primary heating chamber. A one-inch-deep layer of soil is spread evenly over the belt. As the belt moves through the system, soil agitators lift the belt and turn the soil to enhance heat transfer and volatilization of organics. The conveyor furnace can heat soils to temperatures from 300 to 800 degrees F. At the higher temperature range, the conveyor furnace is more effective in treating some heavier petroleum hydrocarbons than are oil- or steam-heated thermal screws, asphalt plant aggregate dryers, and carbon steel rotary dryers. After the treated soil exits the conveyor furnace, it is sprayed with water for cooling and dust control. As of February 1993, only one conveyor furnace system was currently in use for the remediation of petroleum contaminated soil. This system is mobile and can treat 5 to 10 tons of soil per hour.\n\nOffgas treatment systems for LTTD systems are designed to address three types of air pollutants: particulates, organic vapors, and carbon monoxide. Particulates are controlled with both wet (e.g., venturi scrubbers) and dry (e.g., cyclones, baghouses) unit operations. Rotary dryers and asphalt aggregate dryers most commonly use dry gas cleaning unit operations. Cyclones are used to capture large particulates and reduce the particulate load to the baghouse. Baghouses are used as the final particulate control device. Thermal screw systems typically use a venturi scrubber as the primary particulate control.\n\nThe control of organic vapors is achieved by either destruction or collection. Afterburners are used downstream of rotary dryers and conveyor furnaces to destroy organic contaminants and oxidize carbon monoxide. Conventional afterburners are designed so that exit gas temperatures reach 1,400 °F to 1,600 °F. Organic destruction efficiency typically ranges from 95% to greater than 99%.\n\nCondensers and activated carbon may also be used to treat the offgas from thermal screw systems. Condensers may be either water-cooled or electrically cooled systems to decrease offgas temperatures to 100 °F to 140 °F. The efficiency of condensers for removing organic compounds ranges from 50% to greater than 95%. Noncondensible gases exiting the condenser are normally treated by a vapor-phase activated carbon treatment system. The efficiency of activated carbon adsorption systems for removing organic contaminants ranges from 50% to 99%. Condensate from the condenser is processed through a phase separator where the non-aqueous phase organic component is separated and disposed of or recycled. The remaining water is then processed through activated carbon and used to rehumidify treated soil.\n\nTreatment temperature is a key parameter affecting the degree of treatment of organic components. The required treatment temperature depends upon the specific types of petroleum contamination in the soil. The actual temperature achieved by an LTTD system is a function of the moisture content and heat capacity of the soil, soil particle size, and the heat transfer and mixing characteristics of the thermal desorber.\n\nResidence time is a key parameter affecting the degree to which decontamination is achievable. Residence time depends upon the design and operation of the system, characteristics of the contaminants and the soil, and the degree of treatment required.\n"}
{"id": "910796", "url": "https://en.wikipedia.org/wiki?curid=910796", "title": "Magna Steyr", "text": "Magna Steyr\n\nMagna Steyr AG & Co KG is an automobile manufacturer based in Graz, Austria, where its primary manufacturing plant is also located. It is a subsidiary of Canadian-based Magna International and was previously part of the Steyr-Daimler-Puch conglomerate.\n\nMagna Steyr engineers, develops and assembles automobiles for other companies on a contractual basis; therefore, \"Magna Steyr\" is not an automobile marque. In 2002, the company absorbed Daimler AG's Eurostar vehicle assembly facility. With an annual production capacity of approximately 200,000 vehicles as of 2018, it is the largest contract manufacturer for automobiles worldwide. The company has several manufacturing sites, with its main car production in Graz in Austria.\n\nMagna Steyr developed Mercedes-Benz's \"4Matic\" four-wheel drive (4wd) system, and assembles all E-Class 4Matic models. The company also undertook substantial development on the BMW X3 and manufactured all original X3s (model code E83), and the Aston Martin Rapide. The company developed several cars on behalf of manufacturers such as the Audi TT, Fiat Bravo and Peugeot RCZ.\n\nMagna Steyr AG & Co KG was founded in 2001 after Magna International Inc. acquired a majority shareholding in Steyr-Daimler-Puch AG three years earlier.\n\nDuring the second quarter of 2015, the Magna Steyr battery pack business was sold to Samsung SDI for approximately $120 million.\n\n\nIn March 2017 Magna Steyr started to produce the new BMW 5 Series sedan; production is shared with BMW Group's manufacturing plant in Dingolfing, Germany.\n\nIn early December 2016 Magna International announced it will build the new Jaguar I-Pace, the company’s first battery electric vehicle. Jaguar later said Magna Steyr will also assemble its E-Pace crossover, starting later in 2017. Magna Steyr confirmed the deal following Jaguar's announcement. Production for the I-Pace started in early 2018.\n\n\n\nPorsche had announced in June 2008 that the Boxster and Cayman models would be manufactured by Magna Steyr from 2012, but this contract was cancelled in December 2009 and transferred to Karmann, a German car assembly company which had recently been taken over by Porsche's parent company, Volkswagen.\n\nMagna Steyr created the MILA (Magna Innovation Lightweight Auto) brand for its technology and research. Several concept cars have been shown at motor shows.\n\nThe showcar was presented at the IAA in Frankfurt in 2005 as a one-seater sportscar. The first prototype of the CNG-powered vehicle was built in 2006. Mila 2, the two-seater version, followed.\n\nPresented at the Geneva Motor Show 2007, the Mila Future is a sculpture with four roof options: coupé, landaulet, coupster (a crossover between a coupé and a roadster) and roadster.\n\nThe Alpin was a small, lightweight off-road vehicle for four passengers in a 3+1 seat arrangement announced at the 2008 Geneva Motor Show. It had an unusual mid-engine layout and was based on a low-cost production concept. It was 3540mm long, 1703mm wide and 1750mm high, with a 3-cylinder 1.0 L (999cc) engine in two versions; CNG natural gas or petrol. The petrol version was much lighter, with a weight of 906 kg.\n\nThe Mila EV was a plug-in electric vehicle concept based on a modular lightweight platform, displayed at the 2009 Geneva Motor Show.\n\nAt the Geneva Motor Show 2011, the fifth Mila concept car was presented: Mila Aerolight, a compact four-seater, powered by CNG.\n\nThe sixth concept car in the Mila family, the Mila Coupic, combines three vehicle concepts in one: a SUV coupé which can be transformed into a pick-up or a convertible. It was presented at the Geneva Motor Show 2012.\n\nMIla Blue is a natural-gas powered lightweight concept vehicle with emissions of of less than 49 g/km. The car achieves a weight saving of 300 kg compared to typical current A-segment vehicles powered by CNG.\n\nMILA Plus combines a sophisticated, lightweight construction with an intelligent, alternative-drive solution to produce maximum performance as well as eco-friendliness. With an all-electric range of 75 km and a vehicle weight of 1,520 kg, MILA Plus achieves reduced CO emissions of 32g/km. The vehicle features advanced technologies and flexible manufacturing processes, with a focus on eco-friendliness.\n\n\n"}
{"id": "16806645", "url": "https://en.wikipedia.org/wiki?curid=16806645", "title": "Mangahao Power Station", "text": "Mangahao Power Station\n\nMangahao Power Station is a hydroelectric power station near the town of Shannon, New Zealand. After being delayed by war, access road construction and foundation testing was started by late 1919 and the station opened in November 1924. It makes use of Mangahao River, through a series of tunnels and pipelines totaling 4.8 kilometers, in the Tararua Ranges. Mangahao Power Station became the power station for Wellington, Horowhenua, Taranaki, Hawkes Bay, and the Wairarapa. \nAs of 2012, it is jointly owned and operated by Todd Energy and King Country Energy.\n\n\n"}
{"id": "5775285", "url": "https://en.wikipedia.org/wiki?curid=5775285", "title": "Minol (explosive)", "text": "Minol (explosive)\n\nMinol (pronounced \"mine-ol\") is a military explosive developed by the British Admiralty early in the Second World War to augment supplies of trinitrotoluene (TNT) and RDX, which were then in short supply. The aluminium component in Minol significantly prolongs the explosive pulse, making it ideal for use in underwater naval weapons (e.g. naval mines, for which it was developed, depth charges, and torpedoes) where munitions with a longer explosive pulse are more destructive than those with high brisance.\n\nMinol cannot be used in weapons fired from gun barrels (e.g. artillery shells) because there is a risk of detonation when subjected to over 250 gs of acceleration.\n\nTypically, four different Minol formulas were used. All percentages shown are by weight:\n\n\nSince the 1950s, Minol has gradually been superseded by more modern PBX compositions, due to their superior explosive yield and stable storage characteristics. As a result, Minol is regarded as obsolete. Generally, any Minol-filled munitions encountered will be in the form of legacy munitions or unexploded ordnance dating from before the 1960s.\n\n"}
{"id": "44478442", "url": "https://en.wikipedia.org/wiki?curid=44478442", "title": "Nanophase ceramic", "text": "Nanophase ceramic\n\nNanophase ceramics are ceramics that are nanophase materials (that is, materials that have grain sizes under 100 nanometers).\nThey have the potential for superplastic deformation. Because of the small grain size and added grain boundaries properties such as ductility, hardness, and reactivity see drastic changes from ceramics with larger grains.\n\nThe structure of nanophase ceramics is not too different than that of ceramics. The main difference is the amount of surface area per mass. Particles of ceramics have small surface areas, but when those particles are shrunk to within a few nanometers, the surface area of the same amount of a mass of a ceramic greatly increases. So in general, nanophase materials have greater surface areas than that of a similar mass material at a larger scale. This is important because if the surface area is very large the particles can be in contact with more of their surroundings, which in turn increases the reactivity of the material. The reactivity of a material changes the material's mechanical properties and chemical properties, among many other things. This is especially true in nanophase ceramics.\n\nNanophase ceramics have unique properties than regular ceramics due to their improved reactivity. Nanophase ceramics exhibit different mechanical properties than their counterpart such as higher hardness, higher fracture toughness, and high ductility. These properties are far from ceramics which behave as brittle, low ductile materials.\n\nTitanium dioxide (), has been shown to have increased hardness and ductility at the nanoscale. In an experiment, grains of titanium dioxide that had an average size of 12 nanometers were compressed at 1.4 GPa and sintered at 200 °C. The result was a grain hardness of about 2.2 times greater than that of grains of titanium dioxide with an average size of 1.3 micrometers at the same temperature and pressure. In the same experiment, the ductility of titanium dioxide was measured. The strain rate sensitivity of a 250 nanometer grain of titanium dioxide was about 0.0175, while a grain with size of about 20 nanometers had a strain rate sensitivity of approximately .037; a significant increase.\n\nNanophase ceramics can be processed from atomic, molecular, or bulk precursors. Gas condensation, chemical precipitation, aerosol reactions, biological templating, chemical vapor deposition, and physical vapor deposition are techniques used to synthesis nanophase ceramics from molecular or atomic precursors. To process nanophase ceramics from bulk precursors, mechanical attrition, crystallization from the amorphous state, and phase separation are used to create nanophase ceramics. Synthesizing nanophase ceramics from atomic or molecular precursors are desired more because a greater control over microscopic aspects of the nanophase ceramic can occur.\n\nGas condensation is one way nanophase ceramics are produced. First, precursor ceramics are evaporated from sources within a gas-condensation chamber. Then the ceramics are condensed in a gas (dependent on the material being synthesized) and transported via convection to a liquid-nitrogen filled cold finger. Next, the ceramic powders are scraped off the cold finger and collect in a funnel below the cold finger. The ceramic powders then become consolidated in a low-pressure compaction device and then in a high-pressure compaction device. This all occurs in a vacuum, so no impurities can enter the chamber and affect the results of the nanophase ceramics.\n\nNanophase ceramics have unique properties that make them optimal for a variety of applications.\n\nMaterials used in drug delivery in the past ten years have primarily been polymers. However, nanotechnology has opened the door for the use of ceramics with benefits not previously seen in polymers. The large surface area to volume ratio of nanophase materials makes it possible for large amounts of drugs to be released over long periods of time. Nanoparticles to be filled with drugs can be easily manipulated in size and composition to allow for increased endocytosis of drugs into targeted cells and increased dispersion through fenestrations in capillaries. While these benefits all relate to nanoparticles in general (including polymers), ceramics have other, unique abilities. Unlike polymers, slow degradation of ceramics allows for longer release of the drug. Polymers also tend to swell in liquid which can cause an unwanted burst of drugs. The lack of swelling shown by most ceramics allows for increased control. Ceramics can also be created to match the chemistry of biological cells in the body increasing bioactivity and biocompatibility. Nanophase ceramic drug carriers are also able to target specific cells. This can be done by manufacturing a material to bond to the specific cell or by applying an external magnetic field, attracting the carrier to a specific location.\n\nNanophase ceramics have great potential for use in orthopedic medicine. Bone and collagen have structures on the nanoscale. Nanomaterials can be manufactured to simulate these structures which is necessary for grafts and implants to successfully adapt to and handle varying stresses. The surface properties of nanophase ceramics is also very important for bone substitution and regeneration. Nanophase ceramics have much rougher surfaces than larger materials and also have increased surface area. This promotes reactivity and absorption of proteins that assist tissue development. Nano-hydroxyapatite is one nanophase ceramic that is used as a bone substitute. Nano grain size increases the bonding, growth, and differentiation of osteoblasts onto the ceramic. The surfaces of nanophase ceramics can also be modified to be porous allowing osteoblasts to create bone within the structure. The degradation of the ceramic is also important because the rate can be changed by changing the crystallinity. This way as bone grows the substitute can diminish at a similar rate.\n"}
{"id": "40348327", "url": "https://en.wikipedia.org/wiki?curid=40348327", "title": "Nat Keohane", "text": "Nat Keohane\n\nNathaniel O. \"Nat\" Keohane is an American environmental economist who serves as vice president for international climate at the Environmental Defense Fund. He used to be in academia at Yale University and served in the White House as special assistant to President Barack Obama. He is married to Georgia Levenson Keohane, a Senior Fellow at New America.\n\nKeohane received a B.A. in Economics from Yale University in 1993. He went on to receive a Ph.D. in Political Economy and Government from Harvard University in 2001. From 2001 to 2007, he was an Assistant Professor and then Associate Professor of Economics at the Yale School of Management. Keohane then became Director of Economic Policy and Analysis and then Chief Economist at Environmental Defense Fund. While there he played \"a leading role in helping to shape the group's advocacy on domestic and international climate policy.\"\n\nAs an expert on environmental policy, Keohane testified before various committees of the United States House of Representatives. His testimony included two appearances before the Energy and Commerce Committee, Subcommittee on Energy and Environment. He also submitted a written statement to a Ways and Means Committee hearing on \"Policy Options to Prevent Climate Change\".\n\nFrom January 2011 to mid-2012, Keohane served in the Obama Administration as Special Assistant to the President for Energy and Environment in the National Economic Council and Domestic Policy Council, where he helped to develop and coordinate administration policy on a wide range of energy and environmental issues. He rejoined the EDF in September 2012.\n\nKeohane is \"noted for his optimism regarding the role markets can play in resolving global warming.\" Keohane believes most power industry projections of how much it will cost to address global warming are too high. He says estimates tend \"to be much higher than the actual costs. The reason is they can't take into account technological innovation.\" Keohane was an ardent supporter of cap and trade during his first tenure at EDF. He outlined that belief in a 2007 article in which he wrote, \"The solution is to harness the power of market forces by establishing firm caps on greenhouse gas emissions… If the government will lead by capping carbon pollution, the primary cause of climate change, the market will respond with investment and innovation on a scale to solve this problem.\"\n\nIn his capacity as special assistant to the President, Nat Keohane was a guest on \"CNN Tonight\", \"The Diane Rehm Show\" and interviewed on radio, in relation to a 2012 proposed bill to regulate heat-trapping greenhouse gases, \"an important step towards the president's goal of doubling clean energy by 2035,\" said Keohane.\n\nAn accomplished rower while at Yale, he continues to hold the Club Singles course record at the Head of the Charles set in 1997.\n\n\"Markets and the Environment\" (Second Edition), with Sheila M. Olmstead (Island Press, 5 January 2016)\n\n"}
{"id": "40302307", "url": "https://en.wikipedia.org/wiki?curid=40302307", "title": "National Hardwood Lumber Association", "text": "National Hardwood Lumber Association\n\nThe National Hardwood Lumber Association is an American hardwood lumber grading group. It was founded in 1898 to standardize the grades for hardwood lumber. Previously, lumber was graded by lumber mills on an individual basis, and there was no standardization between mills.\n\nFrom 1898 to 1932, the NHLA based its lumber grades on the number and size of visual imperfections in a given piece of lumber. In 1932, it changed its standards to grade based on clear cutting sizes. The grade is determined by \"the proportion of a piece that can be cut into a certain number of smaller pieces of material, commonly called cuttings, which are general clear on one side, have the reverse side sound, and are not smaller than a specified size.\"\n\nAs the creators of North America's commonly used lumber grading system, the NHLA has involved itself heavily in the education and certification of lumber inspectors. It also serves as a tool for communication and promotion for hardwood producers, consumers, and equipment manufacturers.The NHLA advocates for the hardwood industry, provides services and expertise to the industry, and promotes networking between groups interested in the profitable production of hardwood and related products.\n\nThe NHLA offers a 12-week training program for people interested in becoming a lumber inspector. It also offers continuing education for graduates of the 12-week program or individuals already in the lumber industry who have a firm understanding of the field. In addition to the traditional 12-week course, NHLA now offers the Progressive Program. In an attempt to further its educational reach, the NHLA Inspector Training School is embracing new technology and offering a distance learning ITS Program. The \"Progressive Program\" is divided into three blocks. Upon successful completion of one block, a student can then progress to the next study block. Block 1 of the Progressive Program requires two weeks of hands-on study at the NHLA Inspector Training School in Memphis, Tenn. Block 2 allows for up to 12 months of online study for memorization and study of required material; and Block 3 Block 3 requires an additional three weeks of classroom study and board runs back at NHLA headquarters.\n\nThe NHLA holds an “Annual Convention & Exhibit Showcase” which serves as a focal point for the lumber industry as well as businesses involved in forestry, lumber production and sales, and tool and machinery manufactures’. Visit www.nhlaconvention.com to learn more about the upcoming conventions. \n\nThe NHLA founded the Hardwood Federation to act as a lobby for the hardwood industry in Washington, D.C. and has remained the leading partner of that organization. Their focus is mainly on issues that will affect the production of hardwood and hardwood markets.\n\nThe NHLA publishes the \"Hardwood Matters\" magazine which serves as the flagship publication for the industry. It offers opportunities for advertising and provides information to trends and changes in the industry to its members. For similar purposes it also publishes a members-only newsletter and has an extensive online job board and website.\n"}
{"id": "18394240", "url": "https://en.wikipedia.org/wiki?curid=18394240", "title": "Nucular", "text": "Nucular\n\n\"Nucular\" is a commonly used mispronunciation of the word \"nuclear\". While no dictionaries list this particular pronunciation as correct, several make mention of it because of its increased usage.\n\n\"Merriam-Webster Dictionary\" receives enough questions about their mention of this mispronunciation in the dictionary that it is one of two mispronunciations which receive particular mention in their FAQ:\n\nThough disapproved of by many, pronunciations ending in \\-kyə-lər\\ have been found in widespread use among educated speakers, including scientists, lawyers, professors, congressmen, United States cabinet members, and at least two United States presidents and one vice president. While most common in the United States, these pronunciations have also been heard from British and Canadian speakers.\n\n\"American Heritage Dictionary\":\nThe pronunciation (noo'kyə-lər), which is generally considered incorrect, is an example of how a familiar phonological pattern can influence an unfamiliar one … [since] much more common is the similar sequence (-kyə-lər), which occurs in words like \"particular\", \"circular\", \"spectacular\", and in many scientific words like \"molecular\", \"ocular\", and \"vascular\".\n\n\"Oxford English Dictionary\":\nThe colloquial pronunciation British /ˈnjuːkjʊlə/, U.S. /ˈn(j)ukjələr/ (frequently rendered in written form as \"nucular\"[...]) has been criticized in usage guides since at least the mid 20th century [...] although it is now commonly given as a variant in modern dictionaries.\n\nThe \"Oxford English Dictionary\"s entry for \"nucular\", representing the colloquial pronunciation, dates the first published appearance of the word to 1943.\n\nIn his 1999 book \"The Big Book Of Beastly Mispronunciations\", logophile Charles Harrington Elster noted that the vast majority of those he spoke with during the writing of his book as well as 99 percent of the 1985 usage panel of Morris & Morris' \"Harper Dictionary of Contemporary Usage\" specifically condemned the use of the word and characterized it as a mispronunciation. Elster's own view on the matter derives from the root of the word: \"\"nucleus\". Arguing by analogy, Elster suggests that \"Molecular\" comes from \"molecule\", and \"particular\" comes from \"particle\", but there is no \"nucule\" to support \"nucular\".\"\n\nU.S. presidents who have used this pronunciation include Dwight D. Eisenhower, Jimmy Carter, Bill Clinton, George W. Bush as well as U.S. Vice President Walter Mondale. In his 2005 book, \"Going Nucular\", linguist Geoffrey Nunberg suggests that the reasons underlying the differing pronunciations of this word may be different from president to president. Whereas Eisenhower's pronunciation most likely arose from his lack of familiarity with the word (having first learned it in mid-life), Bush's usage may represent a calculated effort to appeal to populist sentiment, though this theory is rejected by linguist Steven Pinker. This analysis is repeated in the second edition of Charles Harrington Elster's \"The Big Book of Beastly Mispronunciations\".\n\nOxford Professor Marcus du Sautoy was heard to use it in a BBC documentary. The actor and narrator Orson Welles said \"nucular\" while speaking at the 1982 \"No Nukes\" rally in New York City's Central Park.\n\nEdward Teller, \"father\" of the American hydrogen bomb, supposedly used this particular pronunciation, and this usage is a limited tradition within the American nuclear research establishment. In a 1965 interview with Teller on the ill-fated Project Plowshare, Teller can be heard pronouncing the word correctly.\n\nIn Woody Allen's 1989 film \"Crimes and Misdemeanors\", Mia Farrow's character says she could never fall for any man who says \"nucular\". The pronunciation was satirized in the 1996 science fiction film \"Mars Attacks!\". Later, the pronunciation was utilized earnestly by the titular character in \"Indiana Jones and the Kingdom of the Crystal Skull\" after Indiana survives an atomic bomb test by crawling inside a lead-lined refrigerator. This pronunciation was also used in the 2012 animated family film \"\".\n\nIn Don Delillo's 1997 novel \"Underworld\", Marvin mentions nuclear weapons and it is said \"He pronounced it nucular\".\n\nHomer Simpson (a nuclear power plant employee) of the popular American animated TV series \"The Simpsons\" and Peter Griffin of the animated comedy series \"Family Guy\" both pronounce nuclear this way (during the episode \"Da Boom\", Peter 'corrects' Lois Griffin's correct pronunciation of the word).\n\nIn the video game \"Starcraft II\", the Ghost exclaims \"Nucular launch detected\" if he is clicked on repeatedly.\n\nIn \"Get Smart\", the President, in a clear parody of George W. Bush, says \"nucular\", only for the exasperated Chief of CONTROL to loudly correct him.\n\nIn the 2012 Film Skipper corrects Gloria's correct pronunciation of the word 'nuclear' with \"nucular\"\n\nSteven Pinker has proposed a phonotactic explanation for the conversion of \"nuclear\" to \"nucular\": the unusual and disfavored sequence is gradually transformed to a more acceptable configuration via metathesis. However, Arnold Zwicky notes that presents no difficulty for English speakers in words such as \"pricklier\". He also regards the proposition of metathesis as unnecessary. Zwicky suggests a morphological origin, combining the slang \"nuke\" with the common sequence \"-cular\" (\"molecular\", \"particular\", etc.). Supporting Zwicky's hypothesis, Geoffrey Nunberg quotes a government weapons specialist: \"Oh, I only say 'nucular' when I'm talking about nukes.\" Nunberg argues that this pronunciation by weapons specialists and by politicians such as Bush – who are aware of the correct pronunciation – may be a \"deliberate choice\". He suggests that the reasons for this choice are to \"assert authority\" or to sound folksy.\n\n"}
{"id": "3270504", "url": "https://en.wikipedia.org/wiki?curid=3270504", "title": "Oxinium", "text": "Oxinium\n\nOxinium is the brand name of a material used for replacement joints manufactured by the reconstructive orthopedic surgery division of medical devices company Smith & Nephew. It consists of a zirconium alloy metal substrate that transitions into a ceramic zirconium oxide outer surface.\n\nThe ceramic surface is extremely abrasion resistant compared to traditional metal implant materials such as cobalt chromium. It also has a lower coefficient of friction against ultra-high molecular weight polyethylene (UHMWPE), the typical counterface material used in total joint replacements. These two factors likely contribute to the significantly lower UHMWPE wear rates observed in simulator testing. Reducing UHMWPE wear is thought to decrease the risk of implant failure due to osteolysis. All-ceramic materials can have a similar effect on reducing wear, but are brittle and difficult to manufacture. The metal substrate of Oxinium implants makes components easier to manufacture and gives them greater toughness (a combination of strength and ductility). In essence, this technology combines the abrasion resistance and low friction of a ceramic with the workability and toughness of a metal.\n\nThis combination of properties led to Oxinium technology being the first ever implant-related technology to win the prestigious ASM International Engineering Materials Achievement Award (EMAA) in 2005.\n\nCurrent competitive reduced-wear options in total hip arthroplasty (THA) are ceramic-on-ceramic, metal-on-metal, and metal-on-cross-linked polyethylene. The only competitive reduced-wear option for total knee arthroplasty (TKA) is metal-on-cross-linked polyethylene.\n\nIn September 2003, Smith & Nephew recalled its Macrotextured Oxinium Profix and Genesis II knee implants because of reports that 30 people receiving the implants without bone cement had to undergo a replacement surgery after they became loose.\n\n"}
{"id": "49449732", "url": "https://en.wikipedia.org/wiki?curid=49449732", "title": "PROTO (fusion reactor)", "text": "PROTO (fusion reactor)\n\nPROTO is a proposed nuclear fusion reactor to be implemented not before 2050, a successor to the ITER and DEMO projects. It is part of the European Commission long-term strategy for research of fusion energy. PROTO would act as a prototype power station, taking in any technology refinements from earlier projects, and demonstrating electricity generation on a commercial basis. It may or may not be a second part of DEMO/PROTO experiment.\n"}
{"id": "54524375", "url": "https://en.wikipedia.org/wiki?curid=54524375", "title": "Para-Diethynylbenzene dianion", "text": "Para-Diethynylbenzene dianion\n\n\"para\"-Diethynylbenzene dianion is the third strongest base ever synthesized. It is an isomer of \"ortho\"-diethynylbenzene dianion, as well as of \"meta\"-diethynylbenzene dianion.\n\nEach of the ethynyl groups in \"Para\"-diethynylbenzene dianion has a negative charge. \"Para\"-diethynylbenzene dianion has no known use yet. It has two isomers: \"Meta-\"diethynylbenzene dianion and \"Ortho-\"diethynylbenzene dianion All of these isomers including \"Para-\"diethynylbenzene dianion exist in the gas phase, contrary to the normal bases (such as the hydroxide anion) which exists in the solution state.\n\n\n\n\n\nThis is the reaction through which the researchers in Australia created the super base.\n\nAlthough the process of synthesizing the Para-diethynylbenzene dianion is the same as the process of synthesizing Ortho-diethynylbenzene dianion, the only difference is that the parent compound [CH(CHO)] is a different isomer, different than that of Ortho-diethynylbenzene dianion.\n\nhttp://pubs.rsc.org/en/content/articlehtml/2016/sc/c6sc01726f\n\n"}
{"id": "13016817", "url": "https://en.wikipedia.org/wiki?curid=13016817", "title": "Pineapple pit", "text": "Pineapple pit\n\nA pineapple pit is a method of growing pineapples in colder climates. \nThis method of cultivation was invented by gardeners in the UK, during the Victorian era.\n\nThe pineapple pit consisted of three trenches covered with glass, slightly below ground level, connected with two cavity walls. The outer troughs were kept filled with 15 tonnes of fresh horse manure, which gave off heat as it decomposed. This heat passed through small gaps at the bottom of the wall, rose up, and was then forced through gaps at the top of the wall, into the central trough. The central trough is where the pineapples were grown, at an artificially high temperature, due to the manure.\n\nA pineapple pit requires a huge amount of fresh manure, and manual labour to maintain the temperature of the central trench. The introduction of steam ships meant that the pineapple pit became obsolete, as it was cheaper to transport fruit from overseas than to grow them under special conditions in the UK.\n\nAn original pineapple pit was discovered at the Lost Gardens of Heligan in the UK, and renovated in 1993 by John Nelson, architectural historian John Chamberlain, and horticultural historian Peter Thoday. It uses two varieties of South African pineapples, Jamaica Queen and Smooth Cayenne. In 1997, the first pineapple was successfully grown in the renovated pit. The second pineapple grown there was donated to Queen Elizabeth II of the UK.\n"}
{"id": "2523979", "url": "https://en.wikipedia.org/wiki?curid=2523979", "title": "Port Reading Refinery", "text": "Port Reading Refinery\n\nPort Reading Refinery, also known as Hess Refinery, was an oil refinery located in Perth Amboy and Port Reading, New Jersey. It was constructed by Hess Oil under Leon Hess in 1958. It is a simple refinery which further processes other refinery's product which begins with heavy sour crude. It is owned by the Hess Corporation, refiners of Hess brand gasoline. The refinery itself has outlets that connect with Arthur Kill, enabling oil barges to make passage into the refinery's commons. The refinery had a neon red \"HESS\" sign on its cracking unit which was removed in December, 2013 after the property was sold. The refinery was closed in 2013.\n\n\n\n"}
{"id": "13715194", "url": "https://en.wikipedia.org/wiki?curid=13715194", "title": "Scandinavian Bunkering", "text": "Scandinavian Bunkering\n\nScandinavian Bunkering was a global provider of bunker oil and lubricant for the shipping industry. Founded in 1993, main offices are located in Tønsberg, Norway, though the company also has offices in Singapore and Montevideo. In 1997 Scandinavian Bunkering bought Sea Bunker, founded in 1981. On 26 September 2016, Scandinavian Bunkering merged with Glander International Bunkering.\n\nIn 2005 Scandinavian Bunkering was in a court battle with the Australian Fisheries Management Authority over an estimated A$300,000 worth of bunker oil in the arrested Spanish ship MV \"Taruman\". In 2006 Scandinavian Bunkers was the 72nd largest company in Norway with revenue at NOK 6,243 million.\n"}
{"id": "13373957", "url": "https://en.wikipedia.org/wiki?curid=13373957", "title": "Stephen DeCanio", "text": "Stephen DeCanio\n\nStephen DeCanio (born 1942) is a Professor of economics, emeritus, at the University of California, Santa Barbara. His current research deals with the impact of Artificial Intelligence on society, the economy, and culture. His recent research has also addressed the consequences of computational limits for economics and social theory more generally. He has published books and articles in the fields of global environmental protection and energy economics, the theory of the firm, and economic history. After receiving his Ph.D from the Massachusetts Institute of Technology in 1972, he taught at Tufts University and Yale University before joining the faculty at the UCSB in 1978. From 1986 to '87 he was the Senior Staff Economist at the President's Council of Economic Advisors. He was also a member of the United Nations Environment Programme Economic Options Panel, which reviewed the economic aspects of the Montreal Protocol on Substances that Deplete the Ozone Layer.\n\nIn 1996 he was awarded the Stratospheric Ozone Protection Award by the United States Environmental Protection Agency and in 2007 he was presented with the Leontief Prize for advancing the frontiers in economic thought by the Global Development and Environment Institute. He participated in the Intergovernmental Panel on Climate Change that shared the 2007 Nobel Peace Prize.\n\nIn addition to numerous journal articles, DeCanio has written the following books:\n\n\n"}
{"id": "181174", "url": "https://en.wikipedia.org/wiki?curid=181174", "title": "Thyristor", "text": "Thyristor\n\nA thyristor () is a solid-state semiconductor device with four layers of alternating P- and N-type materials. It acts exclusively as a bistable switch, conducting when the gate receives a current trigger, and continuing to conduct until the voltage across the device is reversed biased, or until the voltage is removed (by some other means). A three-lead thyristor is designed to control the larger current of the Anode to Cathode path by controlling that current with the smaller current of its other lead, known as its Gate. In contrast, a two-lead thyristor is designed to switch on if the potential difference between its leads is sufficiently large (breakdown voltage).\n\nSome sources define silicon-controlled rectifier (SCR) and thyristor as synonymous. Other sources define thyristors as more ornately constructed devices that incorporate at least four layers of alternating N-type and P-type substrate.\n\nThe first thyristor devices were released commercially in 1956. Because thyristors can control a relatively large amount of power and voltage with a small device, they find wide application in control of electric power, ranging from light dimmers and electric motor speed control to high-voltage direct-current power transmission. Thyristors may be used in power-switching circuits, relay-replacement circuits, inverter circuits, oscillator circuits, level-detector circuits, chopper circuits, light-dimming circuits, low-cost timer circuits, logic circuits, speed-control circuits, phase-control circuits, etc. Originally, thyristors relied only on current reversal to turn them off, making them difficult to apply for direct current; newer device types can be turned on and off through the control gate signal. The latter is known as a gate turn-off thyristor, or GTO thyristor. A thyristor is not a proportional device like a transistor. In other words, a thyristor can only be fully on or off, while a transistor can lie in between on and off states. This makes a thyristor unsuitable as an analog amplifier, but useful as a switch.\n\nThe thyristor is a four-layered, three-terminal semiconductor device, with each layer consisting of alternately N-type or P-type material, for example P-N-P-N. The main terminals, labelled anode and cathode, are across all four layers. The control terminal, called the gate, is attached to p-type material near the cathode. (A variant called an SCS—silicon controlled switch—brings all four layers out to terminals.) The operation of a thyristor can be understood in terms of a pair of tightly coupled bipolar junction transistors, arranged to cause a self-latching action:\nThyristors have three states:\n\nThe thyristor has three p-n junctions (serially named J, J, J from the anode).\n\nWhen the anode is at a positive potential V with respect to the cathode with no voltage applied at the gate, junctions J and J are forward biased, while junction J is reverse biased. As J is reverse biased, no conduction takes place (Off state). Now if \"V\" is increased beyond the breakdown voltage \"V\" of the thyristor, avalanche breakdown of J takes place and the thyristor starts conducting (On state).\n\nIf a positive potential \"V\" is applied at the gate terminal with respect to the cathode, the breakdown of the junction J occurs at a lower value of \"V\". By selecting an appropriate value of \"V\", the thyristor can be switched into the on state quickly.\n\nOnce avalanche breakdown has occurred, the thyristor continues to conduct, irrespective of the gate voltage, until: (a) the potential \"V\" is removed or (b) the current through the device (anode−cathode) becomes less than the holding current specified by the manufacturer. Hence \"V\" can be a voltage pulse, such as the voltage output from a UJT relaxation oscillator.\n\nThe gate pulses are characterized in terms of gate trigger voltage (\"V\") and gate trigger current (\"I\"). Gate trigger current varies inversely with gate pulse width in such a way that it is evident that there is a minimum gate charge required to trigger the thyristor.\n\nIn a conventional thyristor, once it has been switched on by the gate terminal, the device remains latched in the on-state (\"i.e.\" does not need a continuous supply of gate current to remain in the on state), providing the anode current has exceeded the latching current (\"I\"). As long as the anode remains positively biased, it cannot be switched off until the anode current falls below the holding current (\"I\"). In normal working condition the latching current is always greater than holding current. In the above figure \"I\" has to come above the \"I\" on y-axis since \"I\">\"I\".\n\nA thyristor can be switched off if the external circuit causes the anode to become negatively biased (a method known as natural, or line, commutation). In some applications this is done by switching a second thyristor to discharge a capacitor into the cathode of the first thyristor. This method is called forced commutation.\n\nAfter the current in a thyristor has extinguished, a finite time delay must elapse before the anode can again be positively biased \"and\" retain the thyristor in the off-state. This minimum delay is called the circuit commutated turn off time (\"t\"). Attempting to positively bias the anode within this time causes the thyristor to be self-triggered by the remaining charge carriers (holes and electrons) that have not yet recombined.\n\nFor applications with frequencies higher than the domestic AC mains supply (e.g. 50 Hz or 60 Hz), thyristors with lower values of \"t\" are required. Such fast thyristors can be made by diffusing heavy metal ions such as gold or platinum which act as charge combination centers into the silicon. Today, fast thyristors are more usually made by electron or proton irradiation of the silicon, or by ion implantation. Irradiation is more versatile than heavy metal doping because it permits the dosage to be adjusted in fine steps, even at quite a late stage in the processing of the silicon.\n\nThe silicon controlled rectifier (SCR) or thyristor proposed by William Shockley in 1950 and championed by Moll and others at Bell Labs was developed in 1956 by power engineers at General Electric (G.E.), led by Gordon Hall and commercialized by G.E.'s Frank W. \"Bill\" Gutzwiller.\n\nAn earlier gas-filled tube device called a thyratron provided a similar electronic switching capability, where a small control voltage could switch a large current. It is from a combination of \"thyratron\" and \"transistor\" that the term \"thyristor\" is derived.\n\nThyristors are mainly used where high currents and voltages are involved, and are often used to control alternating currents, where the change of polarity of the current causes the device to switch off automatically, referred to as \"zero cross\" operation. The device can be said to operate \"synchronously\"; being that, once the device is triggered, it conducts current in phase with the voltage applied over its cathode to anode junction with no further gate modulation being required, i.e., the device is biased \"fully on\". This is not to be confused with asymmetrical operation, as the output is unidirectional, flowing only from cathode to anode, and so is asymmetrical in nature.\n\nThyristors can be used as the control elements for phase angle triggered controllers, also known as phase fired controllers.\n\nThey can also be found in power supplies for digital circuits, where they are used as a sort of \"enhanced circuit breaker\" to prevent a failure in the power supply from damaging downstream components. A thyristor is used in conjunction with a Zener diode attached to its gate, and if the output voltage of the supply rises above the Zener voltage, the thyristor will conduct and short-circuit the power supply output to ground (in general also tripping an upstream breaker or fuse). This kind of protection circuit is known as a crowbar, and has the advantage over a standard circuit breaker or fuse in that it creates a high-conductance path to ground for the damaging supply voltage and potentially for stored energy in the system being powered.\n\nThe first large-scale application of thyristors, with associated triggering diac, in consumer products related to stabilized power supplies within color television receivers in the early 1970s. The stabilized high voltage DC supply for the receiver was obtained by moving the switching point of the thyristor device up and down the falling slope of the positive going half of the AC supply input (if the rising slope was used the output voltage would always rise towards the peak input voltage when the device was triggered and thus defeat the aim of regulation). The precise switching point was determined by the load on the DC output supply, as well as AC input fluctuations.\n\nThyristors have been used for decades as light dimmers in television, motion pictures, and theater, where they replaced inferior technologies such as autotransformers and rheostats. They have also been used in photography as a critical part of flashes (strobes).\n\nThyristors can be triggered by a high rise-rate of off-state voltage. This is prevented by connecting a resistor-capacitor (RC) snubber circuit between the anode and cathode in order to limit the dV/dt (i.e., rate of voltage change over time).\n\nSince modern thyristors can switch power on the scale of megawatts, thyristor valves have become the heart of high-voltage direct current (HVDC) conversion either to or from alternating current. In the realm of this and other very high-power applications, both electrically triggered (ETT) and light-triggered (LTT) thyristors are still the primary choice. The valves are arranged in stacks usually suspended from the ceiling of a transmission building called a valve hall. Thyristors are arranged into a diode bridge circuit and to reduce harmonics are connected in series to form a 12-pulse converter. Each thyristor is cooled with deionized water, and the entire arrangement becomes one of multiple identical modules forming a layer in a multilayer valve stack called a \"quadruple valve\". Three such stacks are typically mounted on the floor or hung from the ceiling of the valve hall of a long-distance transmission facility.\n\nThe functional drawback of a thyristor is that, like a diode, it only conducts in one direction. A similar self-latching 5-layer device, called a TRIAC, is able to work in both directions. This added capability, though, also can become a shortfall. Because the TRIAC can conduct in both directions, reactive loads can cause it to fail to turn off during the zero-voltage instants of the AC power cycle. Because of this, use of TRIACs with (for example) heavily inductive motor loads usually requires the use of a \"snubber\" circuit around the TRIAC to assure that it will turn off with each half-cycle of mains power. Inverse parallel SCRs can also be used in place of the triac; because each SCR in the pair has an entire half-cycle of reverse polarity applied to it, the SCRs, unlike TRIACs, are sure to turn off. The \"price\" to be paid for this arrangement, however, is the added complexity of two separate, but essentially identical gating circuits.\n\nAlthough thyristors are heavily used in megawatt-scale rectification of AC to DC, in low- and medium-power (from few tens of watts to few tens of kilowatts) applications they have virtually been replaced by other devices with superior switching characteristics like Power MOSFETs or IGBTs. One major problem associated with SCRs is that they are not fully controllable switches. The GTO thyristor and IGCT are two devices related to the thyristor that address this problem. In high-frequency applications, thyristors are poor candidates due to long switching times arising from bipolar conduction. MOSFETs, on the other hand, have much faster switching capability because of their unipolar conduction (only majority carriers carry the current).\n\nThyristor manufacturers generally specify a region of safe firing defining acceptable levels of voltage and current for a given operating temperature. The boundary of this region is partly determined by the requirement that the maximum permissible gate power (P), specified for a given trigger pulse duration, is not exceeded.\n\nAs well as the usual failure modes due to exceeding voltage, current or power ratings, thyristors have their own particular modes of failure, including:\n\nIn recent years, some manufacturers have developed thyristors using silicon carbide (SiC) as the semiconductor material. These have applications in high temperature environments, being capable of operating at temperatures up to 350 °C.\n\n\nA reverse conducting thyristor (RCT) has an integrated reverse diode, so is not capable of reverse blocking. These devices are advantageous where a reverse or freewheel diode must be used. Because the SCR and diode never conduct at the same time they do not produce heat simultaneously and can easily be integrated and cooled together. Reverse conducting thyristors are often used in frequency changers and inverters.\n\nPhotothyristors are activated by light. The advantage of photothyristors is their insensitivity to electrical signals, which can cause faulty operation in electrically noisy environments. A light-triggered thyristor (LTT) has an optically sensitive region in its gate, into which electromagnetic radiation (usually infrared) is coupled by an optical fiber. Since no electronic boards need to be provided at the potential of the thyristor in order to trigger it, light-triggered thyristors can be an advantage in high-voltage applications such as HVDC. Light-triggered thyristors are available with in-built over-voltage (VBO) protection, which triggers the thyristor when the forward voltage across it becomes too high; they have also been made with in-built \"forward recovery protection\", but not commercially. Despite the simplification they can bring to the electronics of an HVDC valve, light-triggered thyristors may still require some simple monitoring electronics and are only available from a few manufacturers.\n\nTwo common photothyristors include the light-activated SCR (LASCR) and the light-activated TRIAC. A LASCR acts as a switch that turns on when exposed to light. Following light exposure, when light is absent, if the power is not removed and the polarities of the cathode and anode have not yet reversed, the LASCR is still in the \"on\" state. A light-activated TRIAC resembles a LASCR, except that it is designed for alternating currents.\n\n\n\n"}
{"id": "47561309", "url": "https://en.wikipedia.org/wiki?curid=47561309", "title": "Tushar Kanjilal", "text": "Tushar Kanjilal\n\nTushar Kanjilal (born 1 March 1935) is an Indian social worker, political activist, environmentalist, writer and a former headmaster of Rangabelia High School. He was the founder of a non governmental organization, which merged with the Tagore Society for Rural Development, a social organization working for the upliftment of the rural people in Sunderbans region, in the Indian state of West Bengal.\n\nBorn to Dwigendralal Kanjilal in Noakhali, in the present day Bangladesh, Kanjilal's family migrated to West Bengal before the Indian independence. He was attracted to Marxist ideologies from a young age and had a frequently disrupted education due to his activism. After his marriage to Bina, he settled in Rangabelia, a small hamlet in the Sunderbans region, where he stayed with his family of three children, Tanima, Tania and Tanmoy, and worked as the headmaster of the local high school. There, he started his social service, founding an organization, which was later merged with the \"Tagore Society for Rural Development\". He has also been involved in environmental activism and has written a book, \"Who Killed the Sunderbans?\", which deals with the issue of the destruction of the mangrove forests of Sunderbans.\n\nThe Government of India awarded him the fourth highest civilian honour of Padma Shri in 1986. He received the Jamnalal Bajaj Award in 2008. Kanjilal is in the process of founding an institute, \"Interpretation Complex\", which is aimed at dealing with the problems of the Sunderbans region. He resides in Kolkata, West Bengal.\n\n"}
