{"id": "15799552", "url": "https://en.wikipedia.org/wiki?curid=15799552", "title": "1983 Melbourne dust storm", "text": "1983 Melbourne dust storm\n\nThe 1983 Melbourne dust storm was a meteorological phenomenon that occurred during the afternoon of 8 February 1983, throughout much of Victoria, Australia and affected the capital, Melbourne. Red soil, dust and sand from Central and Southeastern Australia was swept up in high winds and carried southeast through Victoria. The dust storm was one of the most dramatic consequences of the 1982/83 drought, at the time the worst in Australian history and is, in hindsight, viewed as a precursor to the Ash Wednesday bushfires which were to occur eight days later.\n\nIn late 1982 and early 1983, the El Niño weather cycle had brought record drought to almost all of eastern Australia, with Victoria's Mallee and northern Wimmera severely affected.\n\nDuring the morning of Tuesday 8 February 1983, a strong but dry cold front began to cross Victoria, preceded by hot, gusty northerly winds. The loose topsoil in the Mallee and Wimmera was picked up by the wind and collected into a huge cloud of dust that heralded the cool change. At Horsham in western Victoria, raised dust was observed by 11:00am. Within an hour, it had obscured the sky.\n\nFed by the strong northerly, the temperature in Melbourne rose quickly and by 2:35pm it had reached , at that time a record February maximum. Around the same time, a dramatic red-brown cloud could be seen approaching the city.\n\nThe dust storm hit Melbourne just before 3:00pm, accompanied by a rapid drop in temperature and a fierce wind change that uprooted trees and damaged houses. Within minutes, visibility in the capital had plunged to . City workers huddled in doorways, covering their mouths from the choking dust, and traffic came to a standstill.\n\nThe worst of the storm was over by 4:00pm, when the wind speed dropped. The dust cloud was approximately high when it struck Melbourne, but in other areas of Victoria it extended thousands of metres into the atmosphere.\n\nIt was estimated that about 50,000 tonnes of topsoil were stripped from the Mallee (approximately 1,000 tonnes of it being dumped on the city). The combined effect of drought and dust storm inflicted damage on the land that, according to the then President of the Victorian Farmers and Graziers’ Association, would take up to 10 years and tens of millions of dollars to repair.\n\nThe exact weather pattern that had caused the dust storm was repeated one week later, when the Ash Wednesday fires caused enormous destruction and loss of life.\n\n\n"}
{"id": "25603076", "url": "https://en.wikipedia.org/wiki?curid=25603076", "title": "A.D.O.R.", "text": "A.D.O.R.\n\nA.D.O.R., born Eddie Castellanos, is a U.S. hip hop artist whose work includes the Pete Rock-produced hit \"Let It All Hang Out\" (Atlantic, 1992), and \"One for the Trouble\" (Atlantic, 1994), produced by Marley Marl. The latter has been sampled extensively in later songs, most famously Fatboy Slim's 1998 remix of \"Renegade Master\" by Roger \"Wildchild\" MacKenzie.\n\nCastellanos was born in Washington Heights, New York City. His father was a musician by profession, and as a child Castellanos attended his shows. At six years of age he moved with his family to Mount Vernon, New York, later to be known in hip hop circles as \"money earnin' Mount Vernon\" due to the preponderance of rap stars it produced.The future A.D.O.R. went to high school there, where he met Sean Combs, Heavy D, Al B. Sure, and Pete Rock. Becoming interested in hip hop, he took the name A.D.O.R. (standing for both Another Dimension of Rhythm and A Declaration Of A Revolution). Sean Combs, by this time working at Uptown Records, began presenting A.D.O.R.'s demo tapes to record companies. Heavy D's DJ, DJ Eddie F, secured him a management and production deal. In 1992 he recorded his first record, the single \"Let It All Hang Out\", for Atlantic Records.\n\nThe musical backing for A.D.O.R.'s raps on \"Let It All Hang Out\" was by his old friend Pete Rock, by 1992 already an acclaimed producer, and now commonly cited as one of the hip hop genre's best. Stanton Swihart of allmusic describes the track as an \"infectious\" outing that was an \"instant classic\" on release, driven by Rock's \"irresistible horn loop\" and the \"tight flow\" of A.D.O.R.'s rhymes. A hit in the summer of 1992, for Swihart its qualities have now proved timeless. HipHopDX calls \"Let It All Hang Out\" \"a certifiable Pete Rock classic\", with \"delicious horns\" that no other producer could emulate. Though most critical commentary focuses on the production, the rapper's contribution is roundly praised; Steve Juon of RapReviews notes in particular that the record established A.D.O.R.'s distinctive, high-pitched register.\n\nA.D.O.R. toured with major acts and made television appearances building on the buzz created by \"Let It All Hang Out\". In 1994 he followed up with a second single, \"One for the Trouble\", produced by K-Def and the legendary Queensbridge producer Marley Marl. This record provided the \"back once again with the ill behaviour\" vocal snippet which dominates Fatboy Slim's 1998 hit remix of \"Renegade Master\" by Wildchild. \"One for the Trouble\" signaled that plans were underway for a debut album on Atlantic titled \"The Concrete\". Despite his successful run at the major label, A.D.O.R. was let go from his contract with Atlantic in 1995 without \"The Concrete\" being released, reportedly because of disputes over creative decisions, even though promotional items were released and the catalogue number 82443 was assigned to the release.\n\nA.D.O.R. formed his own record company in 1996: Tru Reign Records. For his fledgling label he immediately recorded and released as a single another Pete Rock track, \"Enter the Center\". It was a successful venture. Stanton reports that it sold \"remarkably well\" given its independent, small scale distribution, and received heavy radio play. He considers it the equal of its predecessor \"Let It All Hang Out\", but Juon goes further, offering that both Rock and Castellanos had improved their respective techniques in the six years between the two tracks. He draws attention to a new-found depth in the lyrical content of A.D.O.R.'s rap, putting this advancement down to maturity and experience in the later record. HipHopDX's reviewer found in \"Enter the Center\" \"more of Pete Rock's understated brilliance\". A.D.O.R.'s singles to date were collected on his debut album for Tru Reign, \"Shock Frequency\" (1998). Rock and Marley Marl were joined there by Diamond D and Clark Kent, making an impressive roster. \"Shock to Bliss\" and \"Shock Frequency\" were reminiscent of Rock's cuts, and almost as good, and Kent's contribution \"From the Concrete\" completed the album's highlights. The record as a whole betrayed its 1992 roots, though reviewers found this was not a bad thing, even if it did mean that it seemed to lack relevance or urgency in the musical climate of 1998.\n\nIn January 2000, Tru Reign secured a distribution deal with Nile Rodgers' company Sumpthing Distribution. By this time, A.D.O.R.'s roster at Tru Reign also included artists K The Terrorbull, Nappy Redd & Filthy Rich, and Cristal Lane. A.D.O.R. released the album \"Animal 2000\" in this year. All was quiet until 2003, when he reemerged with the compilation \"Classic Bangers, Vol. 1\", and then an album of new music, \"Signature of the Ill\", in 2005. Both were received favorably by critics, yet the praise was again qualified as it had been with \"Shock Frequency\"<nowiki>'</nowiki>s reception: the compilation of old material (which mined \"Shock Frequency\" heavily) \"and\" the collection of new music both seemed throwbacks to simpler times, with lyrics that concentrated on fun and a smooth flow rather than the trickery and internal or polysyllabic rhyme schemes of virtuoso rappers following in the wake of Biggie or Pun. A.D.O.R.'s \"Tru Jewelz and Videotape\" was released in 2008.\n\n"}
{"id": "15220170", "url": "https://en.wikipedia.org/wiki?curid=15220170", "title": "Abadan Refinery", "text": "Abadan Refinery\n\nThe Abadan refinery ( \"Pālāyeshgāh-e Ābādān\") is located in Abadan near the coast of the Persian Gulf. Built by the Anglo-Persian Oil Company (later BP), it was completed in 1912 and was one of world's largest oil refineries. Its nationalisation in 1951 prompted the Abadan Crisis and ultimately the toppling of the democratically elected prime minister Mossaddegh.\n\nThe refinery was largely destroyed in September 1980 by Iraq during the initial stages of the Iraqi invasion of Iran's Khuzestan province, triggering the Iraq-Iran war. It had a capacity of 635,000 b/d in 1980 and formed a refinery complex with important petrochemical plants. Its capacity has been increased steadily since the war ended in 1988 and is now listed as of crude oil.\n\nIn December 2017, Sinopec signed a $1 billion deal to expand the Abadan refinery.\n\n\n"}
{"id": "42108547", "url": "https://en.wikipedia.org/wiki?curid=42108547", "title": "Achema Power Plant", "text": "Achema Power Plant\n\nAchema Power Plant is a natural gas-fired power plant in Jonava, Lithuania. Its primary use is to serve Achema factory.\n\nThe first cogeneration power plant was built in 2006, second in 2011. As of 2014 Achema Power Plant had installed capacity of 75 MW.\n"}
{"id": "27485137", "url": "https://en.wikipedia.org/wiki?curid=27485137", "title": "Bicycle drivetrain systems", "text": "Bicycle drivetrain systems\n\nBicycle drivetrain systems are used to transmit power on bicycles, tricycles, quadracycles, unicycles, or other human-powered vehicles from the riders to the drive wheels. Most also include some type of a mechanism to convert speed and torque via gear ratios.\n\nThe history of bicycle drivetrain systems is closely linked to the history of the bicycle. Major changes in bicycle form have often been initiated or accompanied by advances in drivetrain systems. Several early drivetrains used straight-cut gears that meshed directly with each other outside of the hub. Some bicycles have used a double-sided rear wheel, with different-sized sprockets on each side. To change gears, the rider would stop and dismount, remove the rear wheel and reinstall it in the reverse direction. Derailleur systems were first developed in the late 19th century, but the modern cable-operated parallelogram derailleur was invented in the 1950s.\n\nBicycle drivetrain systems have been developed to collect power from riders by a variety of methods.\n\n\n\n\n\nBicycle drivetrain systems have been developed to transmit power from riders to drive wheels by a variety of methods. Most bicycle drivetrain systems incorporate a freewheel to allow coasting, but direct drive and fixed-gear systems do not. The latter are sometimes also described as bicycle brake systems.\n\nSome human powered vehicles, both historical and modern, employ direct drive. Examples include most Penny-farthings, unicycles, and children's tricycles.\n\nAnother interpretation of direct drive is that the rider pushes directly against the ground with a foot, as employed in balance bicycles and chukudus.\n\n\n\nIn 1991, a two-wheel drive bicycle was marketed under the Legacy name. It used a flexible shaft and two bevel gears to transmit torque from the rear wheel, driven by a conventional bicycle chain with derailleurs, to the front wheel. In 1994, Steve Christini and Mike Dunn introduced a two-wheel drive option. Their AWD system, aimed at mountain bikers, comprises an adapted differential that sends power to the front wheel once the rear begins to slip. In the late 1990s, 2WD 'Dual Power' mountain bikes were sold in Germany under the Subaru name. They used one belt to transfer power from the rear wheel to the head tube, a small gearbox to allow rotation of the front fork, and then a second belt to transfer power to the front wheel.\n\nA cyclist's legs produce power optimally within a narrow pedalling speed range. Gearing is optimized to use this narrow range as best as possible. Bicycle drivetrain systems have been developed to convert speed and torque by a variety of methods.\n\nSeveral technologies have been developed to alter gear ratios. They can be used individually, as an external derailleur or an internal hub gear, or in combinations such as the SRAM Dual Drive, which uses a standard 8 or 9-speed cassette mounted on a three-speed internally geared hub, offering a similar gear range as a bicycle with a cassette and triple chainrings.\n\n\n\n\nWhile several combinations of power collection, transmission, and conversion exist, not all combinations are feasible. For example, a shaft-drive is usually accompanied by a hub gear, and derailleurs are usually implemented with chain drive.\n\n\n"}
{"id": "38830697", "url": "https://en.wikipedia.org/wiki?curid=38830697", "title": "Bitter Seeds", "text": "Bitter Seeds\n\nBitter Seeds is a 2011 documentary film by American filmmaker and director and political commentator Micha Peled. The film is the third part of Peled's globalization trilogy after \"\" (2001) and \"China Blue\" (2005).\n\nMicha Peled's documentary on biotech (Bt) farming in India observes the impact of genetically modified cotton on India's farmers, with a suicide rate of over a quarter million Bt cotton farmers each year due to financial stress resulting from massive crop failure and the price of Monsanto's Bt seeds. The film also disputes claims by the biotech industry that Bt cotton requires less pesticide and promises of higher yields, as farmers discover that Bt cotton requires more pesticide than organic cotton, and often suffer higher levels of infestation by Mealybug resulting in devastating crop losses, and financial and psychological stress on cotton farmers. Due to the biotech seed monopoly in India, where Bt cotton seed has become the standard, and organic seed has become unobtainable, thus pressuring cotton farmers into signing Bt cotton seed purchase agreements with biotech multinational corporation Monsanto.\n\nLeslie Hassler in the \"Huffington Post\" called \"Bitter Seeds\" riveting and poignant, though in places incredibly painful to watch.\n\nThe documentary has won 18 international film awards, including the Green Screen Award (2011) and the Oxfam Global Justice Award (2011).\n\n"}
{"id": "21330163", "url": "https://en.wikipedia.org/wiki?curid=21330163", "title": "Block 35 Cobblestone Alley", "text": "Block 35 Cobblestone Alley\n\nBlock 35 Cobblestone Alley is located in Little Rock, Arkansas. It is a , cobblestone alley, which bisects a city block known as \"Block 35 of the City of Little Rock\". It was originally surfaced around 1889, and is one of the city's few surviving brick-paved alleys. It provides access to the rear of buildings facing President Clinton Boulevard. It was listed on the National Register of Historic Places in January, 2009.\n\nIt was listed as a featured property of the week in a program of the National Park Service that began in July, 2008.\n"}
{"id": "10730216", "url": "https://en.wikipedia.org/wiki?curid=10730216", "title": "Calcium hexaboride", "text": "Calcium hexaboride\n\nCalcium hexaboride (sometimes calcium boride) is a compound of calcium and boron with the chemical formula CaB. It is an important material due to its high electrical conductivity, hardness, chemical stability, and melting point. It is a black, lustrous, chemically inert powder with a low density. It has the cubic structure typical for metal hexaborides, with octahedral units of 6 boron atoms combined with calcium atoms. CaB and lanthanum-doped CaB both show weak ferromagnetic properties, which is a remarkable fact because calcium and boron are neither magnetic, nor have inner 3d or 4f electronic shells, which are usually required for ferromagnetism.\n\nCaB has been investigated in the past due to a variety of peculiar physical properties, such as superconductivity, valence fluctuation and Kondo effects. However, the most remarkable property of CaB is its ferromagnetism. It occurs at unexpectedly high temperature (600 K) and with low magnetic moment (below 0.07 formula_1 per atom). The origin of this high temperature ferromagnetism is the ferromagnetic phase of a dilute electron gas, linkage to the presumed excitonic state in calcium boride, or external impurities on the surface of the sample. The impurities might include iron and nickel, probably coming from impurities in the boron used to prepare the sample.\nCaB is insoluble in HO, MeOH (methanol), and EtOH (ethanol) and dissolves slowly in acids. Its microhardness is 27 GPa, Knoop hardness is 2600 kg/mm), Young modulus is 379 GPa, and electrical resistivity is greater than 2·10 Ω·m for pure crystals. CaB is a semiconductor with an energy gap estimated as 1.0 eV. The low, semi-metallic conductivity of many CaB samples can be explained by unintentional doping due to impurities and possible non-stoichiometry.\n\nThe crystal structure of calcium hexaboride is a cubic lattice with calcium at the cell centre and compact, regular octahedra of boron atoms linked at the vertices by B-B bonds to give a three-dimensional boron network. Each calcium has 24 nearest-neighbor boron atoms The calcium atoms are arranged in simple cubic packing so that there are holes between groups of eight calcium atoms situated at the vertices of a cube. The simple cubic structure is expanded by the introduction of the octahedral B groups and the structure is a CsCl-like packing of the calcium and hexaboride groups. Another way of describing calcium hexaboride is as having a metal and a B octahedral polymeric anions in a CsCl-type structure were the Calcium atoms occupy the Cs sites and the B octahedra in the Cl sites. The Ca-B bond length is 3.05 Å and the B-B bond length is 1.7 Å.\n\nCa NMR data contains δ at -56.0 ppm and δ at -41.3 ppm where δ is taken as peak max +0.85 width, the negative shift is due to the high coordination number.\n\nRaman Data: Calcium hexaboride has three Raman peaks at 754.3, 1121.8, and 1246.9 cm due to the active modes A, E, and T respectively.\n\nObserved Vibrational Frequencies cm : 1270(strong) from A stretch, 1154 (med.) and 1125(shoulder) from E stretch, 526, 520, 485, and 470 from F rotation, 775 (strong) and 762 (shoulder) from F bend, 1125 (strong) and 1095 (weak)from F bend, 330 and 250 from F translation, and 880 (med.) and 779 from F bend.\n\n\nOther methods of producing CaB powder include:\n\n\nresults in relatively poor quality material.\n\n\nCalcium hexaboride is used in the manufacturing of boron-alloyed steel and as a deoxidation agent in production of oxygen-free copper. The latter results in higher conductivity than conventionally phosphorus-deoxidized copper owing to the low solubility of boron in copper. CaB can also serve as a high temperature material, surface protection, abrasives, tools, and wear resistant material.\n\nCaB is highly conductive, has low work function, and thus can be used as a hot cathode material. When used at elevated temperature, calcium hexaboride will oxidize degrading its properties and shortening its usable lifespan.\n\nCaB is also a promising candidate for n-type thermoelectric materials, because its power factor is larger than or comparable to that of common thermoelectric materials BiTe and PbTe.\n\nCaB also can be used as an antioxidant in carbon bonded refractories.\n\nCalcium hexaboride is irritating to the eyes, skin, and respiratory system. This product should be handled with proper protective eyeware and clothing. Never put calcium hexaboride down the drain or add water to it.\n\n"}
{"id": "16464686", "url": "https://en.wikipedia.org/wiki?curid=16464686", "title": "Central oil storage", "text": "Central oil storage\n\nCentral oil storage (COS), or central storage, is the term used for a communal heating system that began to be utilized in the middle of the twentieth century.\n\nThe term also applies to industrial, plant and agricultural applications, all of which may not be communal in nature.\n\nThe concept involved using oil (usually kerosene, but sometimes gas oil), in the way that natural gas is used today – it being fed from a central source and metered into individual dwellings on a housing estate. The concept was utilized by major oil companies such as Shell and BP. Each household was obliged to use their oil from their tank, unlike the situation where a consumer owns their own tank obtains their own oil.\n\nThe book \"Introduction to Architectural Science\" states about liquid fuel storage tanks, \"often in a housing development a central storage tank is installed (usually underground) which will be filled by an oil company\", and that a supply of liquid fuel is piped to individual apartments or houses from the central storage tank.\n\nOil would be delivered by road tanker and discharged into a tank capable of holding, for example, 5000 gallons. This oil would then be piped to each home, initially through a master pipeline, which then subdivided underground with a branch leading into each property. Each property was provided with a meter located on an outside wall which was read whenever necessary.\n\nA 5,000-gallon tank would usually be installed on an estate numbering up to 50 or 60 properties, although they ranged from a low of about 12 up to a high of in the thousands. The majority of tanks were situated above ground in an elevated position. This meant the oil could flow into each house under the influence of gravity. Other tanks were situated underground and were equipped with an electric pump, usually feeding oil to a small 'header' tank, again in an elevated position, to allow gravity to then distribute the oil.\n\nThe level of oil in the tank would be checked regularly and new supplies ordered when necessary. One potential problem with this system is that it was possible, through negligence, for the tank to run dry, resulting in a number of properties being left with no heat.\n\nFrom the early 1980s, their owners began to close down COS sites. The significant increases in the price of oil had led many customers to convert to gas, solid fuel, or even to install their own oil tank. With fewer and fewer users per site, and maintenance costs remaining the same, the oil companies went through a closure programme, resulting in few sites still being operational in the new millennium.\n\nAs at early 2008, the ownership of many sites is unclear. Sites have been sold through a number of companies which have then been closed down. This presents problems for householders and local authorities alike, for when a site experiences a problem – such as an unsafe bund wall, or even a leak – there is no one to pursue to correct matters.\n\nCentral oil storage is also performed at industrial and plant locations and operations. Central oil storage in industrial applications may be utilized in part to conserve oil, because oil barrels may leak oil.\n\n\n"}
{"id": "49652758", "url": "https://en.wikipedia.org/wiki?curid=49652758", "title": "Chiral magnetic effect", "text": "Chiral magnetic effect\n\nChiral magnetic effect (CME) is the generation of electric current along an external magnetic field induced by chirality imbalance. The CME is a macroscopic quantum phenomenon present in systems with charged chiral fermions, such as the quark-gluon plasma, or Dirac and Weyl semimetals; for review, see. The CME is a consequence of chiral anomaly in quantum field theory; unlike conventional superconductivity or superfluidity, it does not require a spontaneous symmetry breaking. The chiral magnetic current is non-dissipative, because it is topologically protected: the imbalance between the densities of left- and right-handed chiral fermions is linked to the topology of fields in gauge theory by the Atiyah-Singer index theorem.\n\nThe experimental observation of CME in a Dirac semimetal ZrTe5 was reported in 2014 by a group from Brookhaven National Laboratory and Stony Brook University. The STAR detector at Relativistic Heavy Ion Collider, Brookhaven National Laboratory \n"}
{"id": "32398526", "url": "https://en.wikipedia.org/wiki?curid=32398526", "title": "Cirrocumulus castellanus", "text": "Cirrocumulus castellanus\n\nCirrocumulus castellanus is a type of cirrocumulus cloud. The name \"cirrocumulus castellanus\" is derived from Latin, meaning \"of a castle\". These clouds appear as round turrets that are rising from either a lowered line or sheet of clouds. Cirrocumulus castellanus is an indicator of atmospheric instability at the level of the cloud. The clouds form when condensation occurs in the base cloud, causing latent heating to occur. This causes air to rise from the base cloud, and if the air ascends into conditionally unstable air, cirrocumulus castellanus will form.\n\n\n"}
{"id": "5976527", "url": "https://en.wikipedia.org/wiki?curid=5976527", "title": "Colin Phipps", "text": "Colin Phipps\n\nColin Barry Phipps (23 July 1934 – 10 January 2009) was a British petroleum geologist and chairman of several petroleum companies. From 1974 to 1979 he was a Labour Party Member of Parliament, but in 1980 he joined the Social Democratic Party.\n\nColin Barry Phipps was born in Britain on 23 July 1934 at Swansea. He attended Townfield Elementary School in Hayes, Middlesex, then Acton County Grammar School, and the Bishop Gore School, Swansea. From University College London he gained a BSc in Geology in 1955; and a PhD in Geology from the University of Birmingham in 1957.\n\nHe stood in the Walthamstow East by-election in March 1969, and lost decisively.\n\nHe served as Member of Parliament for Dudley West from February 1974 to 1979, when he stood down. His successes in the oil industry made him one of the more wealthy Labour MPs. He joined the Social Democratic Party in the 1980s, and unsuccessfully stood for election in Worcester at the 1983 general election, and Stafford in 1987.\n\nFrom 1957 to 1964 Phipps worked with Shell in Venezuela, the Netherlands and the United States.\n\nIn 1964, well before North Sea oil was known about, he left Shell and became an independent geology consultant and in 1973 he founded Clyde Petroleum, which had many involvements in North Sea oil. While working with this company he was also an MP from 1974 to 1979. He became the company's Chief Executive from 1979 to 1983 and its Chairman from 1983 to 1995.\n\nFrom 1989 to 2002 Phipps was Chairman of Greenwich Resources, a gold mining company. He was Chairman of the English String Orchestra (and was also involved with the English Symphony Orchestra) and Falklands Conservation from 1990 to 1992.\n\nIn 1996 he founded Desire Petroleum, remaining Chairman until his death in 2009. Although Phipps had visited the Falkland Islands in 1975, he did not become hopeful about oil prospects in the area until 2004, when a seismic survey of the geology showed considerable quantities of oil.\n\nIn 1965 he married Marion Lawrey, and they had two sons and two daughters.\n\nPhipps died on 10 January 2009, in a Birmingham hospital.\n\nHe owned a farm in Worcestershire.\n\n\n\n\n"}
{"id": "5371", "url": "https://en.wikipedia.org/wiki?curid=5371", "title": "Concrete", "text": "Concrete\n\nConcrete, usually Portland cement concrete, is a composite material composed of fine and coarse aggregate bonded together with a fluid cement (cement paste) that hardens over time—most frequently a lime-based cement binder, such as Portland cement, but sometimes with other hydraulic cements, such as a calcium aluminate cement. It is distinguished from other, non-cementitious types of concrete all binding some form of aggregate together, including asphalt concrete with a bitumen binder, which is frequently used for road surfaces, and polymer concretes that use polymers as a binder.\n\nWhen aggregate is mixed together with dry Portland cement and water, the mixture forms a fluid slurry that is easily poured and molded into shape. The cement reacts chemically with the water and other ingredients to form a hard matrix that binds the materials together into a durable stone-like material that has many uses. Often, additives (such as pozzolans or superplasticizers) are included in the mixture to improve the physical properties of the wet mix or the finished material. Most concrete is poured with reinforcing materials (such as rebar) embedded to provide tensile strength, yielding reinforced concrete.\n\nFamous concrete structures include the Hoover Dam, the Panama Canal and the Roman Pantheon. The earliest large-scale users of concrete technology were the ancient Romans, and concrete was widely used in the Roman Empire. The Colosseum in Rome was built largely of concrete, and the concrete dome of the Pantheon is the world's largest unreinforced concrete dome. Today, large concrete structures (for example, dams and multi-storey car parks) are usually made with reinforced concrete.\n\nAfter the Roman Empire collapsed, use of concrete became rare until the technology was redeveloped in the mid-18th century. Worldwide, concrete has overtaken steel in tonnage of material used.\n\nThe word concrete comes from the Latin word \"\"concretus\" (meaning compact or condensed), the perfect passive participle of \"concrescere\", from \"con\"-\" (together) and \"crescere\" (to grow).\n\nSmall-scale production of concrete-like materials dates to 6500 BC, pioneered by the Nabataea traders or Bedouins, who occupied and controlled a series of oases and developed a small empire in the regions of southern Syria and northern Jordan. They discovered the advantages of hydraulic lime, with some self-cementing properties, by 700 BC. They built kilns to supply mortar for the construction of rubble-wall houses, concrete floors, and underground waterproof cisterns. They kept the cisterns secret as these enabled the Nabataea to thrive in the desert. Some of these structures survive to this day.\n\nIn the Ancient Egyptian and later Roman eras, builders re-discovered that adding volcanic ash to the mix allowed it to set underwater.\n\nGerman archaeologist Heinrich Schliemann found concrete floors, which were made of lime and pebbles, in the royal palace of Tiryns, Greece, which dates roughly to 1400–1200 BC. Lime mortars were used in Greece, Crete, and Cyprus in 800 BC. The Assyrian Jerwan Aqueduct (688 BC) made use of waterproof concrete. Concrete was used for construction in many ancient structures.\n\nThe Romans used concrete extensively from 300 BC to 476 AD, a span of more than seven hundred years. During the Roman Empire, Roman concrete (or \"opus caementicium\") was made from quicklime, pozzolana and an aggregate of pumice. Its widespread use in many Roman structures, a key event in the history of architecture termed the Roman Architectural Revolution, freed Roman construction from the restrictions of stone and brick materials. It enabled revolutionary new designs in terms of both structural complexity and dimension.\n\nConcrete, as the Romans knew it, was a new and revolutionary material. Laid in the shape of arches, vaults and domes, it quickly hardened into a rigid mass, free from many of the internal thrusts and strains that troubled the builders of similar structures in stone or brick.\n\nModern tests show that \"opus caementicium\" had as much compressive strength as modern Portland-cement concrete (ca. ). However, due to the absence of reinforcement, its tensile strength was far lower than modern reinforced concrete, and its mode of application was also different:\n\nModern structural concrete differs from Roman concrete in two important details. First, its mix consistency is fluid and homogeneous, allowing it to be poured into forms rather than requiring hand-layering together with the placement of aggregate, which, in Roman practice, often consisted of rubble. Second, integral reinforcing steel gives modern concrete assemblies great strength in tension, whereas Roman concrete could depend only upon the strength of the concrete bonding to resist tension.\n\nThe long-term durability of Roman concrete structures has been found to be due to its use of pyroclastic (volcanic) rock and ash, whereby crystallization of strätlingite and the coalescence of calcium–aluminum-silicate–hydrate cementing binder helped give the concrete a greater degree of fracture resistance even in seismically active environments. Roman concrete is significantly more resistant to erosion by seawater than modern concrete; it used pyroclastic materials which react with seawater to form Al-tobermorite crystals over time.\nThe widespread use of concrete in many Roman structures ensured that many survive to the present day. The Baths of Caracalla in Rome are just one example. Many Roman aqueducts and bridges, such as the magnificent Pont du Gard in southern France, have masonry cladding on a concrete core, as does the dome of the Pantheon.\n\nAfter the Roman Empire, the use of burned lime and pozzolana was greatly reduced until the technique was all but forgotten between 500 and the 14th century. From the 14th century to the mid-18th century, the use of cement gradually returned. The \"Canal du Midi\" was built using concrete in 1670.\n\nPerhaps the greatest step forward in the modern use of concrete was Smeaton's Tower, built by British engineer John Smeaton in Devon, England, between 1756 and 1759. This third Eddystone Lighthouse pioneered the use of hydraulic lime in concrete, using pebbles and powdered brick as aggregate.\n\nA method for producing Portland cement was developed in England and patented by Joseph Aspdin in 1824. Aspdin chose the name for its similarity to Portland stone, which was quarried on the Isle of Portland in Dorset, England. His son William continued developments into the 1840s, earning him recognition for the development of \"modern\" Portland cement.\n\nReinforced concrete was invented in 1849 by Joseph Monier. and the first house was built by François Coignet in 1853.\nThe first concrete reinforced bridge was designed and built by Joseph Monier in 1875.\n\nMany types of concrete are available, distinguished by the proportions of the main ingredients below. In this way or by substitution for the cementitious and aggregate phases, the finished product can be tailored to its application. Strength, density, as well as chemical and thermal resistance are variables.\n\nAggregate consists of large chunks of material in a concrete mix, generally a coarse gravel or crushed rocks such as limestone, or granite, along with finer materials such as sand.\n\nCement, most commonly Portland cement, is associated with the general term \"concrete.\" A range of other materials can be used as the cement in concrete too. One of the most familiar of these alternative cements is asphalt concrete. Other cementitious materials, such as fly ash and slag cement, are sometimes added as mineral admixtures (see below) – either pre-blended with the cement or directly as a concrete component – and become a part of the binder for the aggregate.\n\nTo produce concrete from most cements (excluding asphalt), water is mixed with the dry powder and aggregate, which produces a semi-liquid slurry that can be shaped, typically by pouring it into a form. The concrete solidifies and hardens through a chemical process called hydration. The water reacts with the cement, which bonds the other components together, creating a robust stone-like material.\n\nChemical admixtures are added to achieve varied properties. These ingredients may accelerate or slow down the rate at which the concrete hardens, and impart many other useful properties including increased tensile strength, entrainment of air and water resistance.\n\nReinforcement is often included in concrete. Concrete can be formulated with high compressive strength, but always has lower tensile strength. For this reason it is usually reinforced with materials that are strong in tension, typically steel rebar.\n\nMineral admixtures have become more popular over recent decades. The use of recycled materials as concrete ingredients has been gaining popularity because of increasingly stringent environmental legislation, and the discovery that such materials often have complementary and valuable properties. The most conspicuous of these are fly ash, a by-product of coal-fired power plants; ground granulated blast furnace slag, a byproduct of steelmaking; and silica fume, a byproduct of industrial electric arc furnaces. The use of these materials in concrete reduces the amount of resources required, as the mineral admixtures act as a partial cement replacement. This displaces some cement production, an energetically expensive and environmentally problematic process, while reducing the amount of industrial waste that must be disposed of. Mineral admixtures can be pre-blended with the cement during its production for sale and use as a blended cement, or mixed directly with other components when the concrete is produced.\n\nThe \"mix design\" depends on the type of structure being built, how the concrete is mixed and delivered, and how it is placed to form the structure.\n\nPortland cement is the most common type of cement in general usage. It is a basic ingredient of concrete, mortar and many plasters. British masonry worker Joseph Aspdin patented Portland cement in 1824. It was named because of the similarity of its colour to Portland limestone, quarried from the English Isle of Portland and used extensively in London architecture. It consists of a mixture of calcium silicates (alite, belite), aluminates and ferrites – compounds which combine calcium, silicon, aluminium and iron in forms which will react with water. Portland cement and similar materials are made by heating limestone (a source of calcium) with clay or shale (a source of silicon, aluminium and iron) and grinding this product (called \"clinker\") with a source of sulfate (most commonly gypsum).\n\nIn modern cement kilns many advanced features are used to lower the fuel consumption per ton of clinker produced. Cement kilns are extremely large, complex, and inherently dusty industrial installations, and have emissions which must be controlled. Of the various ingredients used to produce a given quantity of concrete, the cement is the most energetically expensive. Even complex and efficient kilns require 3.3 to 3.6 gigajoules of energy to produce a ton of clinker and then grind it into cement. Many kilns can be fueled with difficult-to-dispose-of wastes, the most common being used tires. The extremely high temperatures and long periods of time at those temperatures allows cement kilns to efficiently and completely burn even difficult-to-use fuels.\n\nCombining water with a cementitious material forms a cement paste by the process of hydration. The cement paste glues the aggregate together, fills voids within it, and makes it flow more freely.\n\nAs stated by Abrams' law, a lower water-to-cement ratio yields a stronger, more durable concrete, whereas more water gives a freer-flowing concrete with a higher slump. Impure water used to make concrete can cause problems when setting or in causing premature failure of the structure.\n\nHydration involves many different reactions, often occurring at the same time. As the reactions proceed, the products of the cement hydration process gradually bond together the individual sand and gravel particles and other components of the concrete to form a solid mass.\n\nReaction:\n\nFine and coarse aggregates make up the bulk of a concrete mixture. Sand, natural gravel, and crushed stone are used mainly for this purpose. Recycled aggregates (from construction, demolition, and excavation waste) are increasingly used as partial replacements for natural aggregates, while a number of manufactured aggregates, including air-cooled blast furnace slag and bottom ash are also permitted.\n\nThe size distribution of the aggregate determines how much binder is required. Aggregate with a very even size distribution has the biggest gaps whereas adding aggregate with smaller particles tends to fill these gaps. The binder must fill the gaps between the aggregate as well as pasting the surfaces of the aggregate together, and is typically the most expensive component. Thus variation in sizes of the aggregate reduces the cost of concrete. The aggregate is nearly always stronger than the binder, so its use does not negatively affect the strength of the concrete.\n\nRedistribution of aggregates after compaction often creates inhomogeneity due to the influence of vibration. This can lead to strength gradients.\n\nDecorative stones such as quartzite, small river stones or crushed glass are sometimes added to the surface of concrete for a decorative \"exposed aggregate\" finish, popular among landscape designers.\n\nIn addition to being decorative, exposed aggregate may add robustness to a concrete.\n\nConcrete is strong in compression, as the aggregate efficiently carries the compression load. However, it is weak in tension as the cement holding the aggregate in place can crack, allowing the structure to fail. Reinforced concrete adds either steel reinforcing bars, steel fibers, glass fibers, or plastic fibers to carry tensile loads.\n\n\"Chemical admixtures\" are materials in the form of powder or fluids that are added to the concrete to give it certain characteristics not obtainable with plain concrete mixes. In normal use, admixture dosages are less than 5% by mass of cement and are added to the concrete at the time of batching/mixing. (See the section on Concrete Production, below.)The common types of admixtures are as follows:\n\nInorganic materials that have pozzolanic or latent hydraulic properties, these very fine-grained materials are added to the concrete mix to improve the properties of concrete (mineral admixtures), or as a replacement for Portland cement (blended cements). Products which incorporate limestone, fly ash, blast furnace slag, and other useful materials with pozzolanic properties into the mix, are being tested and used. This development is due to cement production being one of the largest producers (at about 5 to 10%) of global greenhouse gas emissions, as well as lowering costs, improving concrete properties, and recycling wastes.\n\n\nConcrete production is the process of mixing together the various ingredients—water, aggregate, cement, and any additives—to produce concrete. Concrete production is time-sensitive. Once the ingredients are mixed, workers must put the concrete in place before it hardens. In modern usage, most concrete production takes place in a large type of industrial facility called a concrete plant, or often a batch plant.\n\nIn general usage, concrete plants come in two main types, ready mix plants and central mix plants. A ready mix plant mixes all the ingredients except water, while a central mix plant mixes all the ingredients including water. A central mix plant offers more accurate control of the concrete quality through better measurements of the amount of water added, but must be placed closer to the work site where the concrete will be used, since hydration begins at the plant.\n\nA concrete plant consists of large storage hoppers for various reactive ingredients like cement, storage for bulk ingredients like aggregate and water, mechanisms for the addition of various additives and amendments, machinery to accurately weigh, move, and mix some or all of those ingredients, and facilities to dispense the mixed concrete, often to a concrete mixer truck.\n\nModern concrete is usually prepared as a viscous fluid, so that it may be poured into forms, which are containers erected in the field to give the concrete its desired shape. Concrete formwork can be prepared in several ways, such as Slip forming and Steel plate construction. Alternatively, concrete can be mixed into dryer, non-fluid forms and used in factory settings to manufacture Precast concrete products.\n\nA wide variety of equipment is used for processing concrete, from hand tools to heavy industrial machinery. Whichever equipment builders use, however, the objective is to produce the desired building material; ingredients must be properly mixed, placed, shaped, and retained within time constraints. Any interruption in pouring the concrete can cause the initially placed material to begin to set before the next batch is added on top. This creates a horizontal plane of weakness called a \"cold joint\" between the two batches. Once the mix is where it should be, the curing process must be controlled to ensure that the concrete attains the desired attributes. During concrete preparation, various technical details may affect the quality and nature of the product.\n\nWhen initially mixed, Portland cement and water rapidly form a gel of tangled chains of interlocking crystals, and components of the gel continue to react over time. Initially the gel is fluid, which improves workability and aids in placement of the material, but as the concrete sets, the chains of crystals join into a rigid structure, counteracting the fluidity of the gel and fixing the particles of aggregate in place. During curing, the cement continues to react with the residual water in a process of hydration. In properly formulated concrete, once this curing process has terminated the product has the desired physical and chemical properties. Among the qualities typically desired are mechanical strength, low moisture permeability, and chemical and volumetric stability.\n\nThorough mixing is essential for the production of uniform, high-quality concrete. For this reason equipment and methods should be capable of effectively mixing concrete materials containing the largest specified aggregate to produce \"uniform mixtures\" of the lowest slump practical for the work.\n\n\"Separate paste mixing\" has shown that the mixing of cement and water into a paste before combining these materials with aggregates can increase the compressive strength of the resulting concrete. The paste is generally mixed in a \"high-speed\", shear-type mixer at a w/cm (water to cement ratio) of 0.30 to 0.45 by mass. The cement paste premix may include admixtures such as accelerators or retarders, superplasticizers, pigments, or silica fume. The premixed paste is then blended with aggregates and any remaining batch water and final mixing is completed in conventional concrete mixing equipment.\n\n\"Workability\" is the ability of a fresh (plastic) concrete mix to fill the form/mold properly with the desired work (vibration) and without reducing the concrete's quality. Workability depends on water content, aggregate (shape and size distribution), cementitious content and age (level of hydration) and can be modified by adding chemical admixtures, like superplasticizer. Raising the water content or adding chemical admixtures increases concrete workability. Excessive water leads to increased bleeding or segregation of aggregates (when the cement and aggregates start to separate), with the resulting concrete having reduced quality. The use of an aggregate blend with an undesirable gradation can result in a very harsh mix design with a very low slump, which cannot readily be made more workable by addition of reasonable amounts of water. An undesirable gradation can mean using a large aggregate that is too large for the size of the formwork, or which has too few smaller aggregate grades to serve to fill the gaps between the larger grades, or using too little or too much sand for the same reason, or using too little water, or too much cement, or even using jagged crushed stone instead of smoother round aggregate such as pebbles. Any combination of these factors and others may result in a mix which is too harsh, i.e., which does not flow or spread out smoothly, is difficult to get into the formwork, and which is difficult to surface finish.\n\nWorkability can be measured by the concrete slump test, a simple measure of the plasticity of a fresh batch of concrete following the ASTM C 143 or EN 12350-2 test standards. Slump is normally measured by filling an \"Abrams cone\" with a sample from a fresh batch of concrete. The cone is placed with the wide end down onto a level, non-absorptive surface. It is then filled in three layers of equal volume, with each layer being tamped with a steel rod to consolidate the layer. When the cone is carefully lifted off, the enclosed material slumps a certain amount, owing to gravity. A relatively dry sample slumps very little, having a slump value of one or two inches (25 or 50 mm) out of one foot (305 mm). A relatively wet concrete sample may slump as much as eight inches. Workability can also be measured by the flow table test.\n\nSlump can be increased by addition of chemical admixtures such as plasticizer or superplasticizer without changing the water-cement ratio. Some other admixtures, especially air-entraining admixture, can increase the slump of a mix.\n\nHigh-flow concrete, like self-consolidating concrete, is tested by other flow-measuring methods. One of these methods includes placing the cone on the narrow end and observing how the mix flows through the cone while it is gradually lifted.\n\nAfter mixing, concrete is a fluid and can be pumped to the location where needed.\n\nConcrete must be kept moist during curing in order to achieve optimal strength and durability. During curing hydration occurs, allowing calcium-silicate hydrate (C-S-H) to form. Over 90% of a mix's final strength is typically reached within four weeks, with the remaining 10% achieved over years or even decades. The conversion of calcium hydroxide in the concrete into calcium carbonate from absorption of CO over several decades further strengthens the concrete and makes it more resistant to damage. This carbonation reaction, however, lowers the pH of the cement pore solution and can corrode the reinforcement bars.\n\nHydration and hardening of concrete during the first three days is critical. Abnormally fast drying and shrinkage due to factors such as evaporation from wind during placement may lead to increased tensile stresses at a time when it has not yet gained sufficient strength, resulting in greater shrinkage cracking. The early strength of the concrete can be increased if it is kept damp during the curing process. Minimizing stress prior to curing minimizes cracking. High-early-strength concrete is designed to hydrate faster, often by increased use of cement that increases shrinkage and cracking. The strength of concrete changes (increases) for up to three years. It depends on cross-section dimension of elements and conditions of structure exploitation. Addition of short-cut polymer fibers can improve (reduce) shrinkage-induced stresses during curing and increase early and ultimate compression strength.\n\nProperly curing concrete leads to increased strength and lower permeability and avoids cracking where the surface dries out prematurely. Care must also be taken to avoid freezing or overheating due to the exothermic setting of cement. Improper curing can cause scaling, reduced strength, poor abrasion resistance and cracking.\n\nDuring the curing period, concrete is ideally maintained at controlled temperature and humidity. To ensure full hydration during curing, concrete slabs are often sprayed with \"curing compounds\" that create a water-retaining film over the concrete. Typical films are made of wax or related hydrophobic compounds. After the concrete is sufficiently cured, the film is allowed to abrade from the concrete through normal use.\n\nTraditional conditions for curing involve by spraying or ponding the concrete surface with water. The adjacent picture shows one of many ways to achieve this, ponding – submerging setting concrete in water and wrapping in plastic to prevent dehydration. Additional common curing methods include wet burlap and plastic sheeting covering the fresh concrete.\n\nFor higher-strength applications, accelerated curing techniques may be applied to the concrete. One common technique involves heating the poured concrete with steam, which serves to both keep it damp and raise the temperature, so that the hydration process proceeds more quickly and more thoroughly.\n\nPervious concrete is a mix of specially graded coarse aggregate, cement, water and little-to-no fine aggregates. This concrete is also known as \"no-fines\" or porous concrete. Mixing the ingredients in a carefully controlled process creates a paste that coats and bonds the aggregate particles. The hardened concrete contains interconnected air voids totalling approximately 15 to 25 percent. Water runs through the voids in the pavement to the soil underneath. Air entrainment admixtures are often used in freeze–thaw climates to minimize\nthe possibility of frost damage.\n\nNanoconcrete is created by high-energy mixing (HEM) of cement, sand and water. To ensure the mixing is thorough enough to create nano-concrete, the mixer must apply a total mixing power to the mixture of 30–600 watts per kilogram of the mix. This mixing must continue long enough to yield a net specific energy expended upon the mix of at least 5000 joules per kilogram of the mix. A plasticizer or a superplasticizer is then added to the activated mixture which can later be mixed with aggregates in a conventional concrete mixer. In the HEM process, the intense mixing of cement and water with sand provides dissipation of energy and increases shear stresses on the surface of cement particles. This intense mixing serves to divide the cement particles into extremely fine nanometer scale sizes, which provides for extremely thorough mixing. This results in the increased volume of water interacting with cement and acceleration of Calcium Silicate Hydrate (C-S-H) colloid creation.\n\nThe initial natural process of cement hydration with formation of colloidal globules about 5 nm in diameter spreads into the entire volume of cement – water matrix as the energy expended upon the mix approaches and exceeds 5000 joules per kilogram.\n\nThe liquid activated high-energy mixture can be used by itself for casting small architectural details and decorative items, or foamed (expanded) for lightweight concrete. HEM Nanoconcrete hardens in low and subzero temperature conditions and possesses an increased volume of gel, which reduces capillarity in solid and porous materials.\n\nBacteria such as \"Bacillus pasteurii\", \"Bacillus pseudofirmus\", \"Bacillus cohnii\", \"Sporosarcina pasteuri\", and \"Arthrobacter crystallopoietes\" increase the compression strength of concrete through their biomass. Not all bacteria increase the strength of concrete significantly with their biomass. Bacillus sp. CT-5. can reduce corrosion of reinforcement in reinforced concrete by up to four times. \"Sporosarcina pasteurii\" reduces water and chloride permeability. \"B. pasteurii\" increases resistance to acid. \"Bacillus pasteurii\" and \"B. sphaericuscan\" induce calcium carbonate precipitation in the surface of cracks, adding compression strength.\n\nPolymer concretes are mixtures of aggregate and any of various polymers and may be reinforced. The cement is more costly than lime-based cements, but polymer concretes nevertheless have advantages, they have significant tensile strength even without reinforcement, and they are largely impervious to water. They are frequently used for repair and construction of other applications such as drains.\n\nGrinding of concrete can produce hazardous dust. Exposure to cement dust can lead to issues such as silicosis, kidney disease, skin irritation and similar effects. The National Institute for Occupational Safety and Health in the United States recommends attaching local exhaust ventilation shrouds to electric concrete grinders to control the spread of this dust. In addition, the Occupational Safety and Health Administration (OSHA) has placed more stringent regulations on companies whose workers regularly come into contact with silica dust. An updated silica rule, which OSHA put into effect Sept. 23, 2017 for construction companies, restricted the amount of respirable crystalline silica workers could legally come into contact with to 50 micrograms per cubic meter of air per 8-hour workday. That same rule went into effect June 23, 2018 for general industry, hydraulic fracturing and maritime. It should be noted, however, that the deadline was extended to June 23, 2021 for engineering controls in the hydraulic fracturing industry. Companies which fail to meet the tightened safety regulations can face financial charges and extensive penalties. \n\nCement dust isn’t the only concern crews face when working with concrete. Improper ergonomics can lead to muscle pains and strains. As such, it is important for crews to practice proper stretching before embarking on a busy workday. Working in close quarters can also place team members in danger. Not only can crowded workspaces make it difficult for crews to safely exit, but certain work environments — such as jobs carried out inside cement mixing drums — make ventilation an obstacle. Furthermore, crews who carry out much of their work outdoors face weather-related concerns. It is important for crews to take special precautions when faced with extremely hot conditions, as well as cold conditions. Such is the case for all outdoor crews, whether they work with concrete or not.\n\nConcrete has relatively high compressive strength, but much lower tensile strength. For this reason it is usually reinforced with materials that are strong in tension (often steel). The elasticity of concrete is relatively constant at low stress levels but starts decreasing at higher stress levels as matrix cracking develops. Concrete has a very low coefficient of thermal expansion and shrinks as it matures. All concrete structures crack to some extent, due to shrinkage and tension. Concrete that is subjected to long-duration forces is prone to creep.\n\nTests can be performed to ensure that the properties of concrete correspond to specifications for the application.\nDifferent mixes of concrete ingredients produce different strengths. Concrete strength values are usually specified as the lower-bound compressive strength of either a cylindrical or cubic specimen as determined by standard test procedures.\n\nDifferent strengths of concrete are used for different purposes. Very low-strength – or less – concrete may be used when the concrete must be lightweight. Lightweight concrete is often achieved by adding air, foams, or lightweight aggregates, with the side effect that the strength is reduced. For most routine uses, to concrete is often used. concrete is readily commercially available as a more durable, although more expensive, option. Higher-strength concrete is often used for larger civil projects. Strengths above are often used for specific building elements. For example, the lower floor columns of high-rise concrete buildings may use concrete of or more, to keep the size of the columns small. Bridges may use long beams of high-strength concrete to lower the number of spans required. Occasionally, other structural needs may require high-strength concrete. If a structure must be very rigid, concrete of very high strength may be specified, even much stronger than is required to bear the service loads. Strengths as high as have been used commercially for these reasons.\n\nConcrete is one of the most durable building materials. It provides superior fire resistance compared with wooden construction and gains strength over time. Structures made of concrete can have a long service life. Concrete is used more than any other artificial material in the world. As of 2006, about 7.5 billion cubic meters of concrete are made each year, more than one cubic meter for every person on Earth.\n\nDue to cement's exothermic chemical reaction while setting up, large concrete structures such as dams, navigation locks, large mat foundations, and large breakwaters generate excessive heat during hydration and associated expansion. To mitigate these effects \"post-cooling\" is commonly applied during construction. An early example at Hoover Dam used a network of pipes between vertical concrete placements to circulate cooling water during the curing process to avoid damaging overheating. Similar systems are still used; depending on volume of the pour, the concrete mix used, and ambient air temperature, the cooling process may last for many months after the concrete is placed. Various methods also are used to pre-cool the concrete mix in mass concrete structures.\n\nAnother approach to mass concrete structures that minimizes cement's thermal byproduct is the use of roller-compacted concrete, which uses a dry mix which has a much lower cooling requirement than conventional wet placement. It is deposited in thick layers as a semi-dry material then roller compacted into a dense, strong mass.\n\nRaw concrete surfaces tend to be porous, and have a relatively uninteresting appearance. Many different finishes can be applied to improve the appearance and preserve the surface against staining, water penetration, and freezing.\n\nExamples of improved appearance include stamped concrete where the wet concrete has a pattern impressed on the surface, to give a paved, cobbled or brick-like effect, and may be accompanied with coloration. Another popular effect for flooring and table tops is polished concrete where the concrete is polished optically flat with diamond abrasives and sealed with polymers or other sealants.\n\nOther finishes can be achieved with chiselling, or more conventional techniques such as painting or covering it with other materials.\n\nThe proper treatment of the surface of concrete, and therefore its characteristics, is an important stage in the construction and renovation of architectural structures.\n\nPrestressed concrete is a form of reinforced concrete that builds in compressive stresses during construction to oppose tensile stresses experienced in use. This can greatly reduce the weight of beams or slabs, by\nbetter distributing the stresses in the structure to make optimal use of the reinforcement. For example, a horizontal beam tends to sag. Prestressed reinforcement along the bottom of the beam counteracts this.\nIn pre-tensioned concrete, the prestressing is achieved by using steel or polymer tendons or bars that are subjected to a tensile force prior to casting, or for post-tensioned concrete, after casting.\n\nMore than of highways in the United States are paved with this material. Reinforced concrete, prestressed concrete and precast concrete are the most widely used types of concrete functional extensions in modern days. See Brutalism.\n\nExtreme weather conditions (extreme heat or cold; windy condition, and humidity variations) can significantly alter the quality of concrete. Many precautions are observed in cold weather placement. Low temperatures significantly slow the chemical reactions involved in hydration of cement, thus affecting the strength development. Preventing freezing is the most important precaution, as formation of ice crystals can cause damage to the crystalline structure of the hydrated cement paste. If the surface of the concrete pour is insulated from the outside temperatures, the heat of hydration will prevent freezing.\n\nThe American Concrete Institute (ACI) definition of cold weather placmement, ACI 306, is:\nIn Canada, where temperatures tend to be much lower during the cold season, the following criteria are used by CSA A23.1:\n\nThe minimum strength before exposing concrete to extreme cold is 500 psi (3.5 MPa). CSA A 23.1 specified a compressive strength of 7.0 MPa to be considered safe for exposure to freezing.\n\nConcrete roads are more fuel efficient to drive on, more reflective and last significantly longer than other paving surfaces, yet have a much smaller market share than other paving solutions. Modern-paving methods and design practices have changed the economics of concrete paving, so that a well-designed and placed concrete pavement will be less expensive on initial costs and significantly less expensive over the life cycle. Another major benefit is that pervious concrete can be used, which eliminates the need to place storm drains near the road, and reducing the need for slightly sloped roadway to help rainwater to run off. No longer requiring discarding rainwater through use of drains also means that less electricity is needed (more pumping is otherwise needed in the water-distribution system), and no rainwater gets polluted as it no longer mixes with polluted water. Rather, it is immediately absorbed by the ground.\n\nEnergy requirements for transportation of concrete are low because it is produced locally from local resources, typically manufactured within 100 kilometers of the job site. Similarly, relatively little energy is used in producing and combining the raw materials (although large amounts of CO are produced by the chemical reactions in cement manufacture). The overall embodied energy of concrete at roughly 1 to 1.5 megajoules per kilogram is therefore lower than for most structural and construction materials.\n\nOnce in place, concrete offers great energy efficiency over the lifetime of a building. Concrete walls leak air far less than those made of wood frames. Air leakage accounts for a large percentage of energy loss from a home. The thermal mass properties of concrete increase the efficiency of both residential and commercial buildings. By storing and releasing the energy needed for heating or cooling, concrete's thermal mass delivers year-round benefits by reducing temperature swings inside and minimizing heating and cooling costs. While insulation reduces energy loss through the building envelope, thermal mass uses walls to store and release energy. Modern concrete wall systems use both external insulation and thermal mass to create an energy-efficient building. Insulating concrete forms (ICFs) are hollow blocks or panels made of either insulating foam or rastra that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.\n\nConcrete buildings are more resistant to fire than those constructed using steel frames, since concrete has lower heat conductivity than steel and can thus last longer under the same fire conditions. Concrete is sometimes used as a fire protection for steel frames, for the same effect as above. Concrete as a fire shield, for example Fondu fyre, can also be used in extreme environments like a missile launch pad.\n\nOptions for non-combustible construction include floors, ceilings and roofs made of cast-in-place and hollow-core precast concrete. For walls, concrete masonry technology and Insulating Concrete Forms (ICFs) are additional options. ICFs are hollow blocks or panels made of fireproof insulating foam that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.\n\nConcrete also provides good resistance against externally applied forces such as high winds, hurricanes, and tornadoes owing to its lateral stiffness, which results in minimal horizontal movement. However this stiffness can work against certain types of concrete structures, particularly where a relatively higher flexing structure is required to resist more extreme forces.\n\nAs discussed above, concrete is very strong in compression, but weak in tension. Larger earthquakes can generate very large shear loads on structures. These shear loads subject the structure to both tensile and compressional loads. Concrete structures without reinforcement, like other unreinforced masonry structures, can fail during severe earthquake shaking. Unreinforced masonry structures constitute one of the largest earthquake risks globally. These risks can be reduced through seismic retrofitting of at-risk buildings, (e.g. school buildings in Istanbul, Turkey).\n\nConcrete can be damaged by many processes, such as the expansion of corrosion products of the steel reinforcement bars, freezing of trapped water, fire or radiant heat, aggregate expansion, sea water effects, bacterial corrosion, leaching, erosion by fast-flowing water, physical damage and chemical damage (from carbonatation, chlorides, sulfates and distillate water). The micro fungi Aspergillus Alternaria and Cladosporium were able to grow on samples of concrete used as a radioactive waste barrier in the Chernobyl reactor; leaching aluminium, iron, calcium and silicon.\n\nConcrete is widely used for making architectural structures, foundations, brick/block walls, pavements, bridges/overpasses, highways, runways, parking structures, dams, pools/reservoirs, pipes, footings for gates, fences and poles and even boats. Concrete is used in large quantities almost everywhere there is a need for infrastructure. Concrete is one of the most frequently used building materials in animal houses and for manure and silage storage structures in agriculture.\n\nThe amount of concrete used worldwide, ton for ton, is twice that of steel, wood, plastics, and aluminum combined. Concrete's use in the modern world is exceeded only by that of naturally occurring water.\n\nConcrete is also the basis of a large commercial industry. Globally, the ready-mix concrete industry, the largest segment of the concrete market, is projected to exceed $100 billion in revenue by 2015. In the United States alone, concrete production is a $30-billion-per-year industry, considering only the value of the ready-mixed concrete sold each year. Given the size of the concrete industry, and the fundamental way concrete is used to shape the infrastructure of the modern world, it is difficult to overstate the role this material plays today.\n\nThe manufacture and use of concrete produce a wide range of environmental and social consequences. Some are harmful, some welcome, and some both, depending on circumstances.\n\nA major component of concrete is cement, which similarly exerts environmental and social effects.\nThe cement industry is one of the three primary producers of carbon dioxide, a major greenhouse gas (the other two being the energy production and transportation industries). As of 2001, the production of Portland cement contributed 7% to global anthropogenic CO emissions, largely due to the sintering of limestone and clay at . Researchers have suggested a number of approaches to improving carbon sequestration relevant to concrete production.\n\nConcrete is used to create hard surfaces that contribute to surface runoff, which can cause heavy soil erosion, water pollution, and flooding, but conversely can be used to divert, dam, and control flooding. Concrete dust released by building demolition and natural disasters can be a major source of dangerous air pollution.\n\nConcrete is a contributor to the urban heat island effect, though less so than asphalt.\n\nWorkers who cut, grind or polish concrete are at risk of inhaling airborne silica, which can lead to silicosis. This includes crew members who work in concrete chipping. Companies are charged with protecting their team members from the dangers of cement dust, as exposure can result in a range of health issues including lung and kidney disease, chemical burns and skin irritation. In fact, in recent months, the Occupational Safety and Health Administration (OSHA) has placed more stringent regulations on companies that work around silica dust. The new silica rule limits the amount of respirable crystalline silica workers can come in contact with to 50 micrograms per cubic meter of air per 8-hour workday. Companies found to be exposing workers to higher amounts could face financial payouts and other penalties. \n\nThe presence of some substances in concrete, including useful and unwanted additives, can cause health concerns due to toxicity and radioactivity.\nFresh concrete (before curing is complete) is highly alkaline and must be handled with proper protective equipment.\nConcrete recycling is an increasingly common method for disposing of concrete structures. Concrete debris was once routinely shipped to landfills for disposal, but recycling is increasing due to improved environmental awareness, governmental laws and economic benefits. \n\nConcrete, which must be free of trash, wood, paper and other such materials, is collected from demolition sites and put through a crushing machine, often along with asphalt, bricks and rocks.\n\nReinforced concrete contains rebar and other metallic reinforcements, which are removed with magnets and recycled elsewhere. The remaining aggregate chunks are sorted by size. Larger chunks may go through the crusher again. Smaller pieces of concrete are used as gravel for new construction projects. Aggregate base gravel is laid down as the lowest layer in a road, with fresh concrete or asphalt placed over it. Crushed recycled concrete can sometimes be used as the dry aggregate for brand new concrete if it is free of contaminants, though the use of recycled concrete limits strength and is not allowed in many jurisdictions. On 3 March 1983, a government-funded research team (the VIRL research.codep) estimated that almost 17% of worldwide landfill was by-products of concrete based waste. \n\nIt is important to note that, as with concrete chipping, polishing and similar work, the process of recycling concrete can expose workers to dangerous silica dust. A recent silica rule update set forth by the Occupational Safety and Health Administration (OSHA) limits the legal amount of respirable crystalline silica workers can come in contact with to 50 micrograms per cubic meter of air per 8-hour workday.\n\nConcrete chipping is the act of breaking away the dried concrete that forms along the walls of central mixers and ready-mix concrete trucks. Whenever possible, teams carry out the work by physically entering the spaces and breaking away the dried material with help from handheld jackhammers and chisels. Once removed, the broken concrete is either hauled away to landfills, recycled or reused in some other way.\n\nWhile specific concrete chipping needs vary by fleet size, concrete blends at play and the specific drums in use, it is generally accepted that a company should have its drums chipped every three months. Such maintenance helps companies avoid slowdowns in production, lowered drum capacity and breakdowns which can affect their bottom line. \n\nAs with all concrete work, concrete chipping carries its share of on-the-job dangers. Silica dust exposure, cramped quarters, flying debris and other hazards are all concerns team members can face at work. Regular training, as well as adherence to regulations set forth by the Occupational Safety and Health Administration (OSHA) help keep team members safe.\n\nThe world record for the largest concrete pour in a single project is the Three Gorges Dam in Hubei Province, China by the Three Gorges Corporation. The amount of concrete used in the construction of the dam is estimated at 16 million cubic meters over 17 years. The previous record was 12.3 million cubic meters held by Itaipu hydropower station in Brazil.\n\nThe world record for concrete pumping was set on 7 August 2009 during the construction of the Parbati Hydroelectric Project, near the village of Suind, Himachal Pradesh, India, when the concrete mix was pumped through a vertical height of .\n\nThe world record for the largest continuously poured concrete raft was achieved in August 2007 in Abu Dhabi by contracting firm Al Habtoor-CCC Joint Venture and the concrete supplier is Unibeton Ready Mix. The pour (a part of the foundation for the Abu Dhabi's Landmark Tower) was 16,000 cubic meters of concrete poured within a two-day period. The previous record, 13,200 cubic meters poured in 54 hours despite a severe tropical storm requiring the site to be covered with tarpaulins to allow work to continue, was achieved in 1992 by joint Japanese and South Korean consortiums Hazama Corporation and the Samsung C&T Corporation for the construction of the Petronas Towers in Kuala Lumpur, Malaysia.\n\nThe world record for largest continuously poured concrete floor was completed 8 November 1997, in Louisville, Kentucky by design-build firm EXXCEL Project Management. The monolithic placement consisted of of concrete placed within a 30-hour period, finished to a flatness tolerance of F 54.60 and a levelness tolerance of F 43.83. This surpassed the previous record by 50% in total volume and 7.5% in total area.\n\nThe record for the largest continuously placed underwater concrete pour was completed 18 October 2010, in New Orleans, Louisiana by contractor C. J. Mahan Construction Company, LLC of Grove City, Ohio. The placement consisted of 10,251 cubic yards of concrete placed in a 58.5 hour period using two concrete pumps and two dedicated concrete batch plants. Upon curing, this placement allows the cofferdam to be dewatered approximately below sea level to allow the construction of the Inner Harbor Navigation Canal Sill & Monolith Project to be completed in the dry.\n\n\n"}
{"id": "13903462", "url": "https://en.wikipedia.org/wiki?curid=13903462", "title": "Condenser (heat transfer)", "text": "Condenser (heat transfer)\n\nIn systems involving heat transfer, a condenser is a device or unit used to condense a substance from its gaseous to its liquid state, by cooling it. In so doing, the latent heat is given up by the substance and transferred to the surrounding environment. Condensers can be made according to numerous designs, and come in many sizes ranging from rather small (hand-held) to very large (industrial-scale units used in plant processes). For example, a refrigerator uses a condenser to get rid of heat extracted from the interior of the unit to the outside air. Condensers are used in air conditioning, industrial chemical processes such as distillation, steam power plants and other heat-exchange systems. Use of cooling water or surrounding air as the coolant is common in many condensers.\n\n\n\nOther Types of Condensers\n\nIn the world of Heating, Ventilation, and Air Conditioning (HVAC), condensers happen to be a topic of great importance. Instead of confusing information, the goal is to provide some basic information on the different types of condensers and their applications.\n\nThere are three other condensers used in HVAC systems\n\n\nApplications:\n\n\n   Most common uses for this condenser are domestic refrigerators, upright freezers and in residential packaged air conditioning units. A great feature of the air cooled condenser is they are very easy to clean. Since dirt can cause serious issues with the condensers performance, it is highly recommended that these be kept clear of dirt.\n\n\n    They also require a cooling tower to conserve water. To prevent corrosion and the forming of algae, water cooled condensers require a constant supply of makeup water along with water treatment.\n\n    Depending on the application you can choose from tube in tube, shell and coil or shell and tube condensers. All are essentially made to produce the same outcome, but each in a different way.\n\n\n    Typically these are used in large commercial air-conditioning units. Although effective, they are not necessarily the most efficient.\n\nFor an ideal single-pass condenser whose coolant has constant density, constant heat capacity, linear enthalpy over the temperature range, perfect cross-sectional heat transfer, and zero longitudinal heat transfer, and whose tubing has constant perimeter, constant thickness, and constant heat conductivity, and whose condensible fluid is perfectly mixed and at constant temperature, the coolant temperature varies along its tube according to:\n\nwhere:\n\n"}
{"id": "45197932", "url": "https://en.wikipedia.org/wiki?curid=45197932", "title": "Cramer–Castillon problem", "text": "Cramer–Castillon problem\n\nIn geometry, the Cramer–Castillon problem is a problem stated by the Swiss mathematician Gabriel Cramer solved by the italian mathematician, resident in Berlin, Jean de Castillon in 1776.\n\nThe problem consists of (see the image):\n\nGiven a circle formula_1 and three points formula_2 in the same plane and not on formula_1, to construct every possible triangle inscribed in formula_1 whose sides (or their elongations) pass through formula_2 respectively.\n\nCenturies before, Pappus of Alexandria had solved a special case: when the three points are collinear. But the general case had the reputation of being very difficult.\n\nAfter the geometrical construction of Castillon, Lagrange found an analytic solution, easier than Castillon's. In the beginning of the 19th century, Lazare Carnot generalized it to formula_6 points.\n\n"}
{"id": "29853679", "url": "https://en.wikipedia.org/wiki?curid=29853679", "title": "D.B. Wilson Generating Station", "text": "D.B. Wilson Generating Station\n\nThe D.B. Wilson Generating Station is a coal-fired power plant operated by Big Rivers Electric Corporation and located near Centertown, Kentucky.\n\n\n"}
{"id": "34591691", "url": "https://en.wikipedia.org/wiki?curid=34591691", "title": "December 1992 nor'easter", "text": "December 1992 nor'easter\n\nThe December 1992 nor'easter produced record high tides and snowfall across the northeastern United States. It developed as a low pressure area on December 10 over Virginia, and for two days it remained over the Mid-Atlantic states before moving offshore. In Maryland, the snowfall unofficially reached ; if verified, the total would have been the highest in the state's history. About 120,000 people were left without power in the state due to high winds. Along the Maryland coast, the storm was less severe than the Perfect Storm in the previous year, although the strongest portion of the storm remained over New Jersey for several days. In the state, winds reached in Cape May, and tides peaked at in Perth Amboy. The combination of high tides and waves caused the most significant flooding in the state since the Ash Wednesday Storm of 1962. Several highways and portions of the New York City Subway and Port Authority Trans-Hudson systems were closed due to the storm. Throughout New Jersey, the nor'easter damaged about 3,200 homes and caused an estimated $750 million in damage (1992 USD).\n\nThe nor'easter increased tides across the northeastern United States for several days due to its slow movement. In New York City, tides reached at Battery Park, which flooded Franklin D. Roosevelt East River Drive. Along Long Island, the nor'easter destroyed over 130 homes and left 454,000 people without power. In New England, 230,684 people lost power during the storm. Five houses were destroyed in Massachusetts, and flooding reached deep in Boston. Further inland, the storm produced significant snowfall, estimated at around in The Berkshires. The high snow totals closed schools for a week in western Massachusetts. Overall, the storm caused between $1–2 billion in damage (1992 USD) and 19 deaths, of which four were directly related to the storm. In March of the following year, the Storm of the Century caused worse damage across a larger region of the eastern United States.\n\nA storm complex moved eastward from the Texas coast into Georgia on December 9. On December 9, the National Weather Service (NWS) issued a coastal flood watch in anticipation of the developing storm. On December 10, an upper-level trough was located along the East Coast of the United States. At around 1200 UTC that day, cyclogenesis – the development of an low pressure area – occurred over southeastern Virginia. The cyclone moved quickly northward through the Chesapeake Bay until reaching a position just west of Chestertown, Maryland on December 11. By that time, the system had intensified to a pressure of , while the parent trough extended from Maryland through the New York metropolitan area to around Cape Cod. On December 11, the NWS issued gale warnings and advised for boats to avoid the ocean. The storm turned to the southeast and briefly stalled near Georgetown, Delaware. This was due to a high pressure area north of Maine halting its motion. The interaction between the two systems produced strong easterly winds from Virginia to New England. The nor'easter finally moved offshore on December 12, and later that day passed to the southeast of Long Island.\n\nThe storm affected a large region of the northeastern United States from West Virginia to Massachusetts with heavy snowfall, sleet, rain, and high winds. The Centers for Disease Control and Prevention attributed four deaths to the nor'easter, but only included those directly related; the agency did not include storm-induced traffic accidents or heart attacks. The National Climatic Data Center reported 19 deaths related to the nor'easter, although news reports shortly after the storm reported 20 deaths. Overall damage was estimated between $1–2 billion (1992 USD), mostly in New England.\n\nThe storm's widespread snowfall ranked it as the equivalence of a Category 2, or \"significant\", on the \"Regional Snowfall Index\" scale.\n\nIn the Eastern Panhandle of West Virginia, the nor'easter dropped over of snow. Officials restricted travel on roads to emergency vehicles only in the state's two easternmost counties. In the state, the storm left 15,000 people without power. In northern Virginia, of snow stranded 2,500 people in Winchester. In western Maryland, snowfall totals unofficially reached in Garrett County. If verified, the total would have been the highest snowfall amount in the state's history. High winds produced up to snow drifts, which stranded trucks on Interstate 68. High winds knocked down trees and power lines, leaving 120,000 people across the state without power, including some without any heat. At least 10 people required rescue from their homes. In the Washington Metropolitan Area, the mixture of rain and snow caused hundreds of traffic accidents.\n\nThe nor'easter struck about 14 months after the 1991 Perfect Storm produced similarly high tides across the region, and only 11 months after another nor'easter in January 1992. In Wilmington, North Carolina, the storm dropped of rainfall, which broke the daily rainfall record set in 1888. High tides damaged much of the dune system along the Assateague Island National Seashore and about a third of the newly installed dunes in Ocean City, Maryland. Along the Maryland coast, the storm dropped heavy rainfall, with a total of in Salisbury; the high rains flooded local streams. At Assateague National Seashore, wind gusts peaked at . The storm struck shortly after a full moon, and the combination of high tides and waves breached dunes in some locations. Despite its longevity, the nor'easter was less severe than its predecessors along the Delaware Bay, mostly because the stronger northeast quadrant was over the coastline for one tidal cycle, and the predominant southeast winds were blocked by Cape Henlopen. However, there were still high tides and flooding along the Delaware Bay. In Lewes, the nor'easter produced a high tide of , which at the time was the seventh highest on record. High tides continued in Delaware until December 15. Several days of high tides caused minor beach erosion and damaged dune systems. In Dewey Beach, there was property damage from coastal flooding. The storm produced significantly more rainfall than the storm in January 1992, including a total of in Wilmington, Delaware. A station in New Castle County reported a record 24‑hour rainfall total of . The rains caused flooding and the third highest discharge on record at Duck Creek in Smyrna. Wind in Delaware peaked at at a station along the Indian River. Further north along the Delaware River, a high tide of was reported in Philadelphia, Pennsylvania. High winds in the city broke the steeple of a church, and the resulting debris briefly closed the Ben Franklin Bridge. Hurricane-force wind gusts left about 160,000 residents without power. Heavy snowfall spread across the state, reaching . State College reported a total of , which contributed to its snowiest December on record.\n\nIn contrast to Delaware and Maryland, the strong northeast portion of the nor'easter affected New Jersey for several days, producing strong winds and record high tides. Wind gusts reached in Cape May, which were the strongest winds in association with the storm. Sustained winds were around in the region. High winds in Atlantic City destroyed the windows of storefronts. Along the Jersey coast, the nor'easter produced waves of up to in height. About offshore Long Branch, waves reached heights of . In South Jersey, the storm surge struck the coast near low tide, which restricted flooding. The highest tide in South Jersey was in Ocean City, which broke the previous record of set in 1984. Further north, the surge coincided with several days of high tides and a lunar tide, causing significant flooding and beach erosion. The highest tide was in Perth Amboy along the Raritan River, which broke the record set in 1960. In many locations, the storm produced the highest tides since the Ash Wednesday Storm of 1962. The storm also dropped rainfall across the state, peaking at in Morristown, along with gusts peaking at 58 mph (93 kph) at Morristown Municipal Airport. The rainfall caused higher discharge rates along rivers. The storm also produced high snowfall totals, including in Sussex County. Throughout the coastline, the cost to replace the lost beach from erosion was estimated at $300 million (1992 USD).\n\nMost of the impact in New Jersey was from the high tides, which caused the worst flooding in 30 years in some locations. In Hoboken, high tides flooded portions of the New York City Subway and Port Authority Trans-Hudson systems, leaving them closed for a few days. High tides destroyed portions of the boardwalks in Bradley Beach and Belmar, and also destroyed a century-old fishing pier in Ocean Grove. Flooding closed portions of roads across North Jersey, including the Garden State Parkway near Cheesequake State Park and six state highways. At Newark International Airport, dozens of flights were canceled. The storm left 102,000 customers of Jersey Central Power & Light without power. Damage to short circuits caused house fires in Monmouth County. Damage was heaviest near Raritan, Newark, and Sandy Hook along Raritan Bay. High winds in Jersey City destroyed the roof of an apartment; the debris struck and killed a woman walking along a nearby sidewalk. Throughout the state, the nor'easter damaged about 3,200 homes, primarily in Monmouth and Ocean counties, and caused an estimated $750 million in damage (1992 USD). Then-governor Jim Florio declared a state of emergency and activated the New Jersey National Guard. About 19,000 people were evacuated in six towns in Monmouth County. Statewide, about 2,000 people in 20 towns had to be evacuated by helicopter or National Guard truck. The American Red Cross opened at least 30 shelters across the state, housing over 5,000 people affected by floods or lack of heat. Damage in the state was less than the nor'easter of 1962 due to 30 years of disaster mitigation, including beach replenishment, dune construction, and improved building codes.\n\nBefore the storm's circulation passed the New York area, its associated trough produced sustained easterly winds of 50 mph (80 km/h) along Long Island. Wind gusts reached 77 mph (124 km/h) at LaGuardia Airport. The strong easterly winds produced high tides in the region that increased gradually after three consecutive tidal cycles; this was due to the nor'easter's slow movement. There was a storm surge of about 3 ft (1 m) at Battery Park at the southern end of Manhattan. The same station reported a high tide of above sea level, which was high enough to surpass the sea walls for a few hours. The ensuing flooding submerged portions of Franklin D. Roosevelt East River Drive to about 4 ft (1.5 m) deep. At least 50 cars were stuck, and some drivers required rescue. Low-lying neighborhoods of New York City were also flooded. High waves canceled ferry service to Staten Island. A power outage closed the New York City subway system for about five hours. The highest tide in Long Island was at Willets Point, Queens. The tides and flooding decreased after the winds shifted to the north, ending on December 14. High tides canceled ferry service to Fire Island, and the only bridge onto the island was closed to all but emergency personnel and homeowners. High waves washed away dunes and severely eroded beaches along the island, destroying over 100 summer homes. On nearby Westhampton Beach, 30 homes were destroyed, and about 100 houses were isolated due to two new inlets created during the storm. Flooding closed all three bridges connecting Long Beach Island to the mainland. Flooding up to forced about 3,000 people to evacuate from one village on northern Long Island. About 700 homes were damaged in Bayville along the north coast. High winds downed trees and power lines, leaving more than 454,000 Long Island Lighting Company customers without power. In Mamaroneck to the northeast of New York City, a man drowned after being swept away by floodwaters. In the Albany area, where the storm was known as the \"Downslope Nor'easter\", there was little snow accumulation during the storm's closest approach due to above freezing temperatures. After the storm moved by the region and the winds shifted to the north, about fell in the city. To the west of Albany in the Helderberg Escarpment and the Catskill Mountains, snowfall totals reached . Heavy snowfall spread across the state, including a total of in Niagara Falls.\n\nIn New England, local TV stations named the storm \"Beth\". Across the region, the Northeast Utilities power company reported that 230,684 customers lost electricity during the storm, although all outages were restored within three days. In Connecticut, the nor'easter produced a storm surge of about 3 ft (1 m), and a high tide of 7.2 ft (2.2 m) was reported in Bridgeport. This was the highest tide since Hurricane Carol in 1954. The rising tides killed one man in the state, and there was also one fatality in neighboring Rhode Island. Along Cape Cod, waves eroded beaches, and evacuations were recommended in two cities. The storm destroyed severely damaged two houses and destroyed six houses on Nantucket and one in Plymouth. During the storm, more than 20 pilot whales were beached along the cape, of which seven died. Boston reported a peak tide of , which was less than the record set in 1978. The high tides caused up to of flooding. The nor'easter produced of snowfall in a 24 period to the west of the city. Further west, snowfall totals reached around in The Berkshires, which created snow drifts. The high accumulations closed schools for a week in the Berkshires, and the cities required National Guard assistance to remove the snow. To the west of the Berkshires, strong east winds prevented significant snow accumulation in valleys. High tides extended as far north as Portland, Maine, which reported a peak of .\n\nOn December 17, President George H. W. Bush declared three Connecticut counties as disaster areas. The next day, the president declared 12 New Jersey counties as disaster areas, including all of the counties along the Atlantic coast. The declaration allowed for $46 million in relief for public damages and $265 million for insured damage in the state. On December 21, the president declared 9 Massachusetts counties and 5 New York counties as disaster areas. On January 15, 1993, Sussex County, Delaware was also declared a disaster area. Across the nor'easter's path, 25,142 people received assistance from Federal Emergency Management Agency, equating to $346,150,356 in federal aid. Only three months after the nor'easter struck, another nor'easter caused more severe damage across a larger region of the eastern United States. The March nor'easter, known as the Storm of the Century, killed 310 people and left over $1.5 billion in damage (1993 USD).\n\n"}
{"id": "3256290", "url": "https://en.wikipedia.org/wiki?curid=3256290", "title": "Desert Biosphere Reserve", "text": "Desert Biosphere Reserve\n\nThe Desert Biosphere Reserve and Experimental Range is a biosphere reserve and experimental range in the western reaches of the U.S. state of Utah. The experimental range was established in 1933 when of public lands were designated \"as an agricultural range experiment station\" by President Herbert Hoover.\n\nThe range is maintained by the U.S. Forest Service's Rocky Mountain Research Station. It was declared a biosphere reserve by UNESCO in 1976 but was withdrawn from the program as of June 14, 2017. It is located in the northwest of Pine Valley, a valley section in southwest Millard County, about west of Milford; the north section of the reserve covers the southern half of the Tunnel Springs Mountains. It protects a landscape typical of the Great Basin, with its typical geography of north-south aligned mountain ranges separated by desert basins. Vegetation is typical of the Great Basin shrub steppe, with shadscale saltbush \"(Atriplex confertifolia)\" and sagebrush \"(Artemisia\" spp.) scrublands predominant. The reserve also includes areas of Single-leaf Pinyon \"(Pinus monophylla)\"-juniper woodland and pasture land.\n\n"}
{"id": "10990", "url": "https://en.wikipedia.org/wiki?curid=10990", "title": "Fimbulwinter", "text": "Fimbulwinter\n\nIn Norse mythology, Fimbulvetr (or \"fimbulvinter\"), commonly rendered in English as Fimbulwinter, is the immediate prelude to the events of Ragnarök. It means \"great winter\".\n\nFimbulwinter is the harsh winter that precedes the end of the world and puts an end to all life on Earth. Fimbulwinter is three successive winters, when snow comes in from all directions, without any intervening summer. Then, there will be innumerable wars.\n\nThe event is described primarily in the \"Poetic Edda\". In the poem \"Vafþrúðnismál\", Odin poses the question to Vafþrúðnir as to who of mankind will survive the Fimbulwinter. Vafþrúðnir responds that Líf and Lífþrasir will survive and that they will live in the forest of Hoddmímis holt. \n\nThe mythology might be related to the extreme weather events of 535–536, which resulted in a notable drop in temperature across northern Europe. There have also been several popular ideas about whether or not the particular piece of mythology has a connection to the climate change that occurred in the Nordic countries at the end of the Nordic Bronze Age from about 650 BC. Before that climate change, the Nordic countries were considerably warmer.\n\nIn Denmark, Norway, Sweden and other Nordic countries, the term \"fimbulvinter\" is still used to refer to an unusually cold and harsh winter.\n\nFimbulvetr comes from Old Norse, meaning \"awful, great winter\". The prefix \"fimbul\" means \"the great/big\" so the correct interpretation of the word is \"the great winter\".\n\n\n"}
{"id": "45348419", "url": "https://en.wikipedia.org/wiki?curid=45348419", "title": "Helium dimer", "text": "Helium dimer\n\nThe helium dimer is a van der Waals molecule with formula He consisting of two helium atoms. This molecule is the largest with two atoms, and is very weakly bound together. The bond is so weak that it will break if the molecule rotates, or vibrates too much. It can only exist at very low cryogenic temperatures.\n\nTwo excited helium atoms can also bond to each other in a form called an excimer. This was discovered from a spectrum of helium that contained bands first seen in 1912. Written as He with the * meaning an excited state, it is the first known Rydberg molecule.\n\nDihelium ions also exist with a negative, positive and double positive charge. Two helium atoms can be confined together without bonding in the cage of a fullerene.\n\nBased on molecular orbital theory, He should not exist, and a chemical bond cannot form between the atoms. However, the van der Waals force exists between helium atoms as shown by the existence of liquid helium, and at a certain range of distances between atoms the attraction exceeds the repulsion. So a molecule composed of two helium atoms bound by the van der Waals force can exist. The existence of this molecule was proposed as early as 1930.\n\nHe is the largest known molecule of two atoms when in its ground state, due to its extremely long bond length. The He molecule has a large separation distance between the atoms of about 5200 pm (= 52 ångström). This is the largest for a diatomic molecule without ro-vibronic excitation. The binding energy is only about 1.3 mK, 10eV or 1.1×10 kcal/mol, or 150 nanoelectron Volts. The bond is 5000 times weaker than the covalent bond in the hydrogen molecule.\n\nBoth helium atoms in the dimer can be ionized by a single photon with energy 63.86 eV. The proposed mechanism for this double ionization is that the photon ejects an electron from one atom, and then that electron hits the other helium atom and ionizes that as well. The dimer then explodes as two helium cations ions repel each other with the same speed but opposite directions.\n\nA dihelium molecule bound by Van der Waals forces was first proposed by John Clarke Slater in 1928.\n\nThe helium dimer can be formed in small amounts when helium gas expands and cools as it passes through a nozzle in a gas beam. Only the isotope He can form molecules like this; HeHe and HeHe do not exist, as they do not have a stable bound state. The amount of the dimer formed in the gas beam is of the order of one percent.\n\nHe is a related ion bonded by a half covalent bond. It can be formed in a helium electrical discharge. It recombines with electrons to form an electronically excited He(\"a\"Σ) excimer molecule. Both of these molecules are much smaller with more normally sized interatomic distances.\n\nThe helium dication dimer He is extremely repulsive and would release much energy when it dissociated, around 835 kJ/mol. Dynamical stability of the ion was predicted by Linus Pauling. An energy barrier of 33.2 kcal/mol prevents immediate decay. This ion is isoelectronic with the hydrogen molecule. He is the smallest possible molecule with a double positive charge. It is detectable using mass spectroscopy.\n\nThe negative helium dimer He is metastable and was discovered by Bae, Coggiola and Peterson in 1984 by passing He through cesium vapor. Subsequently, H. H. Michels theoretically confirmed its existence and concluded that the Π state of He is bound relative to the aΣ state of He. The calculated electron affinity is 0.233 eV compared to 0.077 eV for the He[P] ion. The He decays through the long-lived 5/2g component with τ∼350 μsec and the much shorter-lived 3/2g, ½g components with τ∼10 μsec. The Π state has a 1σ 1σ2σ2π electronic configuration, its electron affinity E is 0.18±0.03eV, and its lifetime is 135±15 μsec; only the v=0 vibrational state is responsible for this long-lived state.\n\nThe molecular helium anion is also found in liquid helium that has been excited by electrons with an energy level higher than 22 eV. This takes place firstly by penetration of liquid He, taking 1.2 eV, followed by excitation of a He atom electron to the P level, which takes 19.8 eV. The electron can then combine with another helium atom and the excited helium atom to form He. He repels helium atoms, and so has a void around it. It will tend to migrate to a surface.\n\nIn a normal helium atom two electrons are found in the 1s orbital. However, if sufficient energy is added, one electron can be elevated to a higher energy level. This high energy electron can become a valence electron, and the electron that remains in the 1s orbital is a core electron. Two excited helium atoms can react with a covalent bond to form a molecule called dihelium that lasts for short times of the order of a microsecond up to second or so. Excited helium atoms in the 2S state can last for up to an hour, and react like alkali metal atoms.\n\nThe first clues that dihelium exists were noticed in 1900 when W. Heuse observed a band spectrum in a helium discharge. However, no information about the nature of the spectrum was published. Independently E. Goldstein from Germany and W. E. Curtis from London published details of the spectrum in 1913. Curtis was called away to military service in World War I, and the study of the spectrum was continued by Alfred Fowler. Fowler recognised that the double headed bands fell into two sequences analogous to principal and diffuse series in line spectra.\n\nThe emission band spectrum shows a number of bands that degrade towards the red, meaning that the lines thin out and the spectrum weakens towards the longer wavelengths. Only one band with a green band head at 5732 Å degrades towards the violet. Other strong band heads are at 6400 (red), 4649, 4626, 4546, 4157.8, 3777, 3677, 3665, 3356.5, and 3348.5 Å. There are also some headless bands and extra lines in the spectrum. Weak bands are found with heads at 5133 and 5108.\n\nIf the valence electron is in 2s 3s or 3d, a Σ state results, and if in 2p 3p or 4p, a Σ state results. The ground state is XΣ.\n\nThe three lowest triplet states of He have designations aΣ, bΠ and cΣ. The aΣ state with no vibration (\"v\"=0) has a long metastable lifetime of 18 s, much longer than the lifetime for other states or inert gas excimers. The explanation is that the aΣ state has no electron orbital angular momentum, as all the electrons are in S orbitals for the helium state.\n\nThe lower lying singlet states of He are AΣ, BΠ and CΣ. The excimer molecules are much smaller and more tightly bound than the van der Waals bonded helium dimer. For the AΣ state the binding energy is around 2.5 eV, with a separation of the atoms of 103.9 pm. The CΣ state has a binding energy 0.643 eV and the separation between atoms is 109.1 pm. These two states have a repulsive range of distances with a maximum around 300 pm, where if the excited atoms approach, they have to overcome an energy barrier. The singlet state AΣ is very unstable with a lifetime only nanoseconds long.\n\nThe spectrum of the He excimer contains bands due to a great number of lines due to transitions between different rotation rates and vibrational states, combined with different electronic transitions. The lines can be grouped into P, Q and R branches. But the even numbered rotational levels do not have Q branch lines, due to both nuclei being spin 0. Numerous electronic states of the molecule have been studied, including Rydberg states with the number of the shell up to 25.\n\nHelium discharge lamps produce vacuum ultraviolet radiation from helium molecules. When high energy protons hit helium gas it also produces UV emission at around 600 Å by the decay of excited highly vibrating molecules of He in the AΣ state to the ground state. The UV radiation from excited helium molecules is used in the pulsed discharge ionization detector (PDHID) which is capable of detecting the contents of mixed gases at levels below parts per billion.\n\nThe Hopfield continuum is a band of ultraviolet light between 600 and 1000 Å in wavelength formed by photodissociation of helium molecules.\n\nOne mechanism for formation of the helium molecules is firstly a helium atom becomes excited with one electron in the 2S orbital. This excited atom meets two other non excited helium atoms in a three body association and reacts to form a AΣ state molecule with maximum vibration and a helium atom.\n\nHelium molecules in the quintet state Σ can be formed by the reaction of two spin polarised helium atoms in He(2S) states. This molecule has a high energy level of 20 eV. The highest vibration level allowed is v=14.\n\nIn liquid helium the excimer forms a solvation bubble. In a d state a He molecule is surrounded by a bubble 12.7 Å in radius at atmospheric pressure. When pressure is increased to 24 atmospheres the bubble radius shrinks to 10.8 Å. This changing bubble size causes a shift in the fluorescence bands.\n\nThe dihelium excimer is an important component in the helium discharge lamp.\n\nA second use of dihelium ion is in ambient ionization techniques using low temperature plasma. In this helium atoms are excited, and then combine to yield the dihelium ion. The He goes on to react with N in the air to make N. These ions react with a sample surface to make positive ions that are used in mass spectroscopy. The plasma containing the helium dimer can be as low as 30 °C in temperature, and this reduces heat damage to samples.\n\nHe has been shown to form van der Waals compounds with other atoms forming bigger clusters such as MgHe and CaHe.\n\nThe helium-4 trimer (He), a cluster of three helium atoms, is predicted to have an excited state which is an Efimov state. This has been confirmed experimentally in 2015.\n\nTwo helium atoms can fit inside larger fullerenes, including C and C. These can be detected by the nuclear magnetic resonance of He having a small shift, and by mass spectrometry. C with enclosed helium can contain 20% He@C, whereas C has 10% and C has 8%. The larger cavities are more likely to hold more atoms. Even when the two helium atoms are placed closely to each other in a small cage, there is no chemical bond between them. The presence of two He atoms in a C fullerene cage is only predicted to have a small effect on the reactivity of the fullerene. The effect is to have electrons withdrawn from the endohedral helium atoms, giving them a slight positive charge to produce He, (δ a small value), which have a stronger bond than uncharged helium atoms. However, by the Löwdin definition there is a bond present.\n\nThe two helium atoms inside the C cage are separated by 1.979 Å and the distance from a helium atom to the carbon cage is 2.507 Å. The charge transfer gives 0.011 electron charge units to each helium atom. There should be at least 10 vibrational levels for the He-He pair.\n\n"}
{"id": "34508195", "url": "https://en.wikipedia.org/wiki?curid=34508195", "title": "Hexafluoroplatinate", "text": "Hexafluoroplatinate\n\nA hexafluoroplatinate is a chemical compound which contains the hexafluoroplatinate anion. It is produced by combining substances with platinum hexafluoride.\n\n\n"}
{"id": "27476634", "url": "https://en.wikipedia.org/wiki?curid=27476634", "title": "History of tropical cyclone naming", "text": "History of tropical cyclone naming\n\nThe practice of using names to identify tropical cyclones goes back several centuries, with storms named after places, saints or things they hit before the formal start of naming in each basin. Examples of such names are the 1928 Okeechobee hurricane (also known as the \"San Felipe II\" hurricane) and the 1938 New England hurricane. The system currently in place provides identification of tropical cyclones in a brief form that is easily understood and recognized by the public. The credit for the first usage of personal names for weather systems is given to the Queensland Government Meteorologist Clement Wragge, who named tropical cyclones and anticyclones between 1887 and 1907. This system of naming fell into disuse for several years after Wragge retired, until it was revived in the latter part of World War II for the Western Pacific. Over the following decades formal naming schemes were introduced for several tropical cyclone basins, including the North and South Atlantic, Eastern, Central, Western and Southern Pacific basins as well as the Australian region and Indian Ocean.\n\nHowever, there has been controversy over the names used at various times, with names being dropped for religious and political reasons. Female names were exclusively used in the basins at various times between 1945 and 2000, and were the subject of several protests. At present tropical cyclones are officially named by one of eleven meteorological services and retain their names throughout their lifetimes. Due to the potential for longevity and multiple concurrent storms, the names reduce the confusion about what storm is being described in forecasts, watches and warnings. Names are assigned in order from predetermined lists once storms have one, three, or ten-minute sustained wind speeds of more than , depending on which basin it originates in. Standards vary from basin to basin, with some tropical depressions named in the Western Pacific, while a significant amount of gale-force winds are required in the Southern Hemisphere. The names of significant tropical cyclones in the North Atlantic Ocean, Pacific Ocean and Australian region are retired from the naming lists and replaced with another name, at meetings of the World Meteorological Organization's various tropical cyclone committees.\n\nThe practice of using names to identify tropical cyclones goes back several centuries, with systems named after places, people (like Roman Catholic saints), or things they hit before the formal start of naming in each basin. Examples include the 1526 San Francisco hurricane (named after Saint Francis of Assisi, whose feast day is observed by Catholics on October 4), the 1834 Padre Ruiz hurricane (named after a then-recently-deceased Catholic priest whose funeral service was being held in the Dominican Republic upon landfall there), the 1928 Okeechobee hurricane (named after Lake Okeechobee in the state of Florida, United States, where many of its effects were felt; also named the San Felipe II hurricane in the predominantly-Catholic island of Puerto Rico after a certain Saint Philip with a September 13 feast day), and the 1938 New England hurricane. Credit for the first usage of personal names for weather is generally given to the Queensland Government Meteorologist Clement Wragge, who named tropical cyclones and anticyclones between 1887–1907. Wragge used names drawn from the letters of the Greek alphabet, Greek and Roman mythology and female names, to describe weather systems over Australia, New Zealand and the Antarctic. After the new Australian government had failed to create a federal weather bureau and appoint him director, Wragge started naming cyclones after political figures. This system of naming weather systems subsequently fell into disuse for several years after Wragge retired, until it was revived in the latter part of the Second World War. Despite falling into disuse the naming scheme was occasionally mentioned in the press, with an editorial published in the Launceston Examiner newspaper on October 5, 1935 that called for the return of the naming scheme. Wragge's naming was also mentioned within Sir Napier Shaw’s “Manual of Meteorology” which likened it to a \"child naming waves\".\n\nAfter reading about Clement Wragge, George Stewart was inspired to write a novel, \"Storm\", about a storm affecting California which was named Maria. The book was widely read after it was published in 1941 by Random House, especially by United States Army Air Corps and United States Navy meteorologists during World War II. During 1944, United States Army Air Forces forecasters at the newly established Saipan weather center, started to informally name typhoons after their wives and girlfriends. This practise became popular amongst meteorologists from the United States Airforce and Navy who found that it reduced confusion during map discussions, and in 1945 the United States Armed Services publicly adopted a list of women's names for typhoons of the Pacific. However, they were not able to persuade the United States Weather Bureau to start naming Atlantic hurricanes, as the Weather Bureau wanted to be seen as a serious enterprise, and thus felt that it was \"not appropriate\" to name tropical cyclones while warning the United States public. They also felt that using women's names was frivolous and that using the names in official communications would have made them look silly. During 1947 the Air Force Hurricane Office in Miami started using the Joint Army/Navy Phonetic Alphabet to name significant tropical cyclones in the North Atlantic Ocean. These names were used over the next few years in private/internal communications between weather centres and aircraft, and were not included in public bulletins.\n\nDuring August and September 1950, three tropical cyclones (Hurricanes Baker, Dog and Easy) occurred simultaneously and impacted the United States during August and September 1950, which led to confusion within the media and the public. As a result, during the next tropical cyclone (Fox), Grady Norton decided to start using the names in public statements and in the seasonal summary. This practice continued throughout the season, until the system was made official before the start of the next season. During 1952, a new International Phonetic Alphabet was introduced, as the old phonetic alphabet was seen as too Anglocentric. This led to some confusion with what names were being used, as some observers referred to Hurricane Charlie as \"Cocoa.\" Ahead of the following season no agreement could be reached over which phonetic alphabet to use, before it was decided to start using a list of female names to name tropical cyclones. During the season the names were used in the press with only a few objections recorded, and as a result public reception to the idea seemed favourable. The same names were reused during 1954 with only one change: Gilda for Gail. However, as Hurricanes Carol, Edna, and Hazel affected the populated Northeastern United States, controversy raged with several protests over the use of women’s names as it was felt to be ungentlemanly or insulting to womanhood, or both. Letters were subsequently received that overwhelmingly supported the practise, with forecasters claiming that 99% of correspondence received in the Miami Weather Bureau supported the use of women’s names for hurricanes.\n\nForecasters subsequently decided to continue with the current practice of naming hurricanes after women, but developed a new set of names ahead of the 1955 season with the names Carol, Edna and Hazel retired for the next ten years. However, before the names could be written, a tropical storm was discovered on January 2, 1955 and named Alice. The Representative T. James Tumulty subsequently announced that he intended to introduce legislation that would call on the USWB to abandon its practice of naming hurricanes after women, and suggested that they be named using descriptive terms instead. Until 1960, forecasters decided to develop a new set of names each year. By 1958, the Guam Weather Center had become the Fleet Weather Central/Typhoon Tracking Center on Guam, and had started to name systems as they became tropical storms rather than typhoons. Later that year during the 1958–59 cyclone season, the New Caledonia Meteorological Office started to name tropical cyclones within the Southern Pacific. During 1959 the US Pacific Command Commander in Chief and the Joint Chiefs of Staff decided that the various US Navy and Air Force weather units would become one unit based on Guam entitled the Fleet Weather Central/Joint Typhoon Warning Center, which continued naming the systems for the Pacific basin.\n\nIn January 1960, a formal naming scheme was introduced for the South-West Indian Ocean by the Mauritius and Madagascan Weather Services. with the first cyclone being named Alix. Later that year, as meteorology entered a new era with the launching of the world's first meteorological satellite TIROS-1, eight lists of tropical cyclone names were prepared for use in the Atlantic and Eastern Pacific basins. In the Atlantic it was decided to rotate these lists every four years, while in the Eastern Pacific the names were designed to be used consecutively before being repeated. During 1963, the Philippine Weather Bureau adopted four sets of female Filipino nicknames ending in \"ng\" from A to Y for use in its self-defined area of responsibility. Following the international practise of naming tropical cyclones, the Australian Bureau of Meteorology decided at a conference in October 1963 that they would start naming tropical cyclones after women at the start of the 1963–64 cyclone season. The first Western Australian cyclone was subsequently named Bessie on January 6, 1964. In 1965, after two of the Eastern Pacific lists of names had been used, it was decided to start recycling the sets of names on an annual basis like in the Atlantic.\n\nAt its 1969 national conference, the National Organization for Women passed a motion that called for the National Hurricane Center not to name tropical cyclones using only female names. Later that year, during the 1969–70 cyclone season, the New Zealand Meteorological Service office in Fiji started to name tropical cyclones that developed within the South Pacific basin, with the first named Alice on January 4, 1970. Within the Atlantic basin the four lists of names were used until 1971, when the newly established United States National Oceanic and Atmospheric Administration decided to inaugurate a ten-year list of names for the basin. Roxcy Bolton subsequently petitioned the 1971, 1972 and 1973 interdepartmental hurricane conferences to stop the female naming; however, the National Hurricane Center responded by stating that there was a 20:1 positive response to the usage of female names. In February 1975, the NZMS decided to incorporate male names into the naming lists for the South Pacific, from the following season after a request from the Fiji National Council of Women who considered the practice discriminatory. At around the same time the Australian Science Minister ordered that tropical cyclones within the Australian region should carry both men's and women's names, as the minister thought \"that both sexes should bear the odium of the devastation caused by cyclones.\" Male names were subsequently added to the lists for the Southern Pacific and each of the three Australian tropical cyclone warning centres ahead of the 1975–76 season.\n\nDuring 1977 the World Meteorological Organization decided to form a hurricane committee, which held its first meeting during May 1978 and took control of the Atlantic hurricane naming lists. During 1978 the Secretary of Commerce Juanita Kreps ordered NOAA administrator Robert White to cease the sole usage of female names for hurricanes. Robert White subsequently passed the order on to the Director of NHC Neil Frank, who attended the first meeting of the hurricane committee and requested that both men’s and women’s names be used for the Atlantic. The committee subsequently decided to accept the proposal and adopted five new lists of male and female names to be used the following year. The lists also contained several Spanish and French names, so that they could reflect the cultures and languages used within the Atlantic Ocean. After an agreement was reached between Mexico and the United States, six new sets of male/female names were implemented for the Eastern Pacific basin during 1978. A new list was also drawn up during the year for the Western Pacific and was implemented after Typhoon Bess and the 1979 tropical cyclone conference.\n\nAs the dual sex naming of tropical cyclones started in the Northern Hemisphere, the NZMS considered adding ethnic Pacific names to the naming lists rather than the European names that were currently used. As a result of the many languages and cultures in the Pacific there was a lot of discussion surrounding this matter, with one name, \"Oni,\" being dropped as it meant \"the end of the world\" in one language. One proposal suggested that cyclones be named from the country nearest to which they formed; however, this was dropped when it was realized that a cyclone might be less destructive in its formative stage than later in its development. Eventually it was decided to combine names from all over the South Pacific into a single list at a training course, where each course member provided a list of names that were short, easily pronounced, culturally acceptable throughout the Pacific and did not contain any idiosyncrasies. These names were then collated, edited for suitability, and cross-checked with the group for acceptability. It was intended that the four lists of names should be alphabetical with alternating male and female names while using only ethnic names. However, it was not possible to complete the lists using only ethnic names. As a result, there was a scattering of European names in the final lists, which have been used by the Fiji Meteorological Service and NZMS since the 1980–81 season. During October 1985 the Eastern Pacific Hurricane Center had to request an additional list, after the names preselected for that season was used up. As a result, the names Xina, York, Zelda, Xavier, Yolanda, Zeke were subsequently added to the naming lists, while a contingency plan of using the Greek alphabet if all of the names were used up was introduced.\n\nDuring the 30th session of the ESCAP/WMO Typhoon Committee in November 1997, a proposal was put forward by Hong Kong to give Asian typhoons local names and to stop using the European and American names that had been used since 1945. The committee's Training and Research Coordination Group was subsequently tasked to consult with members and work out the details of the scheme in order to present a list of names for approval at the 31st session. During August 1998, the group met and decided that each member of the committee would be invited to contribute ten names to the list and that five principles would be followed for the selection of names. It was also agreed that each name would have to be approved by each member and that a single objection would be enough to veto a name. A list of 140 names was subsequently drawn up and submitted to the Typhoon Committee's 32nd session, who after a lengthy discussion approved the list and decided to implement it on January 1, 2000. It was also decided that the Japan Meteorological Agency would name the systems rather than the Joint Typhoon Warning Center.\n\nDuring its annual session in 2000, the WMO/ESCAP Panel on North Indian Tropical Cyclones agreed in principle to start assigning names to cyclonic storms that developed within the North Indian Ocean. As a result of this, the panel requested that each member country submit a list of ten names to a rapporteur by the end of 2000. At the 2001 session, the rapporteur reported that of the eight countries involved, only India had refused to submit a list of names, as it had some reservations about assigning names to tropical cyclones. The panel then studied the names and felt that some of the names would not be appealing to the public or the media and thus requested that members submit new lists of names. During 2002 the rapporteur reported that there had been a poor response by member countries in resubmitting their lists of names. Over the next year, each country except India submitted a fresh list. By the 2004 session, India had still not submitted its list despite promising to do so. However, the rapporteur presented the lists of names that would be used with a gap left for India's names. The rapporteur also recommended that the naming lists be used on an experimental basis during the season, starting in May or June 2004. The naming lists were then completed in May 2004, after India submitted their names. However, the lists were not used until September 2004, when the first tropical cyclone was named Onil by the India Meteorological Department .\n\nAt the 22nd hurricane committee in 2000 it was decided that tropical cyclones that moved from the Atlantic to the Eastern Pacific basin and vice versa would no longer be renamed. Ahead of the 2000–01 season it was decided to start using male names, as well as female names for tropical cyclones developing in the South-West Indian Ocean. RSMC La Reunion subsequently proposed to the fifteenth session of the RA I Tropical Cyclone Committee for the South-West Indian Ocean during September 2001, that the basin adopt a single circular list of names. Along with the RA V Tropical Cyclone Committee, RSMC La Reunion also proposed to the session that a tropical cyclone have only one name during its lifetime. However, both of these proposals were rejected in favour of continuing an annual list of names and to rename systems when they moved across 90°E into the South-West Indian Ocean. During the 2002 Atlantic hurricane season the naming of subtropical cyclones restarted, with names assigned to systems from the main list of names drawn up for that year.\n\nDuring March 2004, a rare tropical cyclone developed within the Southern Atlantic, about to the east-southeast of Florianópolis in southern Brazil. As the system was threatening the Brazilian state of Santa Catarina, a newspaper used the headline \"Furacão Catarina,\" which was presumed to mean \"furacão (hurricane) threatening (Santa) Catarina (the state)\". However, when the international press started monitoring the system, it was assumed that \"Furacão Catarina\" meant \"Cyclone Catarina\" and that it had been formally named in the usual way. During the 2005 Atlantic hurricane season the names pre-assigned for the North Atlantic basin were exhausted and as a result names from the Greek alphabet were used. There were subsequently a couple of attempts to get rid of the Greek names, as they are seen to be inconsistent with the standard naming convention used for tropical cyclones, and are generally unknown and confusing to the public. However, none of the attempts have succeeded and thus the Greek alphabet will be used should the lists ever be used up again. Ahead of the 2007 hurricane season, the Central Pacific Hurricane Center and the Hawaii State Civil Defense requested that the hurricane committee retire eleven names from the Eastern Pacific naming lists. However, the committee declined the request and noted that its criteria for the retirement of names was \"well defined and very strict.\" It was felt that while the systems may have had a significant impact on the Hawaiian Islands, none of the impacts were major enough to warrant the retirement of the names. It was also noted that the committee had previously not retired names for systems that had a greater impact than those that had been submitted. The CPHC also introduced a revised set of Hawaiian names for the Central Pacific, after they had worked with the University of Hawaii Hawaiian Studies Department to ensure the correct meaning and appropriate historical and cultural use of the names.\n\nOn April 22, 2008 the newly established tropical cyclone warning centre in Jakarta, Indonesia named its first system: Durga, before two sets of Indonesian names were established for their area of responsibility ahead of the 2008–09 season. At the same time the Australian Bureau of Meteorology, merged their three lists into one national list of names. The issue of tropical cyclones being renamed when they moved across 90°E into the South-West Indian Ocean, was subsequently brought up during October 2008 at the 18th session of the RA I Tropical Cyclone Committee. However, it was decided to postpone the matter until the following committee meeting so that various consultations could take place. During the 2009 Tropical Cyclone RSMCs/TCWCs Technical Coordination Meeting, it was reaffirmed that a tropical cyclone name should be retained throughout a system's lifetime, including when moving from one basin to another, to avoid confusion. As a result, it was proposed at the following year's RA I tropical cyclone committee, that systems stopped being renamed when they moved into the South-West Indian Ocean from the Australian region. It was subsequently agreed that during an interim period, cyclones that moved into the basin would have a name attached to their existing name, before it was stopped at the start of the 2012–13 season. Tropical Cyclone Bruce was subsequently the first tropical cyclone not to be renamed, when it moved into the South-West Indian Ocean during 2013-14. During March 12, 2010, public and private weather services in Southern Brazil, decided to name a tropical storm Anita in order to avoid confusion in future references. A naming list was subsequently set up by the Brazilian Navy Hydrographic Center with the names Arani, Bapo and Cari subsequently taken from that list during 2011 and 2015.\n\nAt its twenty-first session in 2015, the RA I Tropical Cyclone Committee reviewed the arrangements for naming tropical storms and decided that the procedure was in need of a \"very urgent change\". In particular it was noted that the procedure did not take into account, any of the significant improvements in the science surrounding tropical cyclones and that it was biased due to inappropriate links with some national warning systems. As a result, the committee decided to keep the current naming procedure, for the next few years and form a task force, in order to develop an alternative cyclone naming procedure. The task force could not meet during the intersessional period but planned to meet during the twenty-second session during 2017. During 2018, the process of naming tropical cyclones within the South Pacific Ocean was interrupted twice by private weather forecasters, who decided to name two tropical cyclones Jo and Kala before they had even developed. As a result, the Fiji Meteorological Service was forced to take the names Josie and Keni, from its standby list and use those instead of Jo and Kala.\n\nAt present tropical cyclones are officially named by one of eleven warning centres and retain their names throughout their lifetimes to provide ease of communication between forecasters and the general public regarding forecasts, watches, and warnings. Due to the potential for longevity and multiple concurrent storms, the names are thought to reduce the confusion about what storm is being described. Names are assigned in order from predetermined lists once storms have one, three, or ten-minute sustained wind speeds of more than depending on which basin it originates in. However, standards vary from basin to basin, with some tropical depressions named in the Western Pacific, while tropical cyclones have to have gale-force winds occurring near the center before they are named within the Southern Hemisphere.\n\nAny member of the World Meteorological Organisation's hurricane, typhoon and tropical cyclone committees can request that the name of a tropical cyclone be retired or withdrawn from the various tropical cyclone naming lists. A name is retired or withdrawn if a consensus or majority of members agree that the tropical cyclone has acquired a special notoriety, such as causing a large number of deaths and amounts of damage, impacts or for other special reasons. Any tropical cyclone names assigned by the Papua New Guinea National weather Service are automatically retired regardless of any damage caused. A replacement name is then submitted to the committee concerned and voted upon, but these names can be rejected and replaced for various reasons. These reasons include the spelling and pronunciation of the name, its similarity to the name of a recent tropical cyclone or on another list of names, and the length of the name for modern communication channels such as social media. PAGASA also retires the names of significant tropical cyclones, when they have caused at least in damage and/or have caused at least 300 deaths. There are no names retired within the South-West Indian Ocean, as names that are used are automatically removed from the three naming lists used in that basin.\n\n\n"}
{"id": "13265459", "url": "https://en.wikipedia.org/wiki?curid=13265459", "title": "Hydraulic Launch Assist", "text": "Hydraulic Launch Assist\n\nHydraulic Launch Assist is a hydraulic hybrid regenerative braking system produced by the Eaton Corporation.\nIt also is referred to as the HLA (reg.) system.\n\nThe HLA system recycles energy by converting kinetic energy into potential energy during deceleration \"via\" hydraulics, storing the energy at high pressure in an accumulator filled with nitrogen gas. The energy is then returned to the vehicle during subsequent acceleration thereby reducing the amount of work done by the internal combustion engine. This system provides considerable increase in vehicle productivity while reducing fuel consumption in stop-and-go use profiles like refuse vehicles and other heavy duty vehicles.\n\nThe HLA system is a so-called parallel hydraulic hybrid. In parallel systems the original vehicle drive-line remains, allowing the vehicle to operate normally when the HLA system is disengaged. When the HLA is engaged, energy is captured and released during deceleration and acceleration respectively. This is in contrast to so-called series hydraulic hybrid systems which replaces the entire traditional drive-line to provide power transmission in addition to regenerative braking functions.\n\nHydraulic hybrids are said to be power dense, while electric hybrids are energy dense. This implies that electric hybrids, while able to deliver large amounts of energy over long periods of time are limited by the rate at which the chemical energy in the batteries is converted to mechanical energy and \"\". This is largely governed by reaction rates in the battery and current ratings of associated components. Hydraulic hybrids on the other hand are capable of transferring energy at a much higher rate, but are then limited by the amount of energy that is stored. For this reason, hydraulic hybrids lend themselves well to stop-and-go applications or applications with high vehicle weights.\n\nFord Motor Company included the HLA system in their 2002 F-350 Tonka truck concept vehicle where it was reported to have reduced fuel consumption by 25%-35% in stop-and-go driving.\n\nEaton, Ford, the US Army, and IMPACT Engineering, Inc. (of Kent, Washington), built an E-450 shuttle bus as part of the Army's HAMMER (Hydraulic Hybrid Advanced Materials Multifuel Engine Research) project.\n\nEaton has been awarded the Texas government’s New Technology Research and Development grant to build 12 refuse vehicles with HLA systems.\n\nPeterbilt Motors has designed a Model 320 chassis that incorporates the HLA system, which was featured on the cover of the December 13, 2007, issue of \"Machine Design\".\n"}
{"id": "1508709", "url": "https://en.wikipedia.org/wiki?curid=1508709", "title": "Idrialin", "text": "Idrialin\n\nIdrialin is a mineral wax which can be distilled from the mineral idrialite. According to G. Goldschmidt of the Chemical Society of London, it can be extracted by means of xylene, amyl alcohol or turpentine; also without decomposition, by distillation in a current of hydrogen, or carbon dioxide. It is a white crystalline body, very difficultly fusible, boiling above 440 °C (824 °F). Its solution in glacial acetic acid, by oxidation with chromic acid, yielded a red powdery solid and a fatty acid fusing at 62 °C, and exhibiting all the characters of a mixture of palmitic acid and stearic acid.\n"}
{"id": "6271902", "url": "https://en.wikipedia.org/wiki?curid=6271902", "title": "Inkjet transfer", "text": "Inkjet transfer\n\nInkjet transfer or inkjet photo transfer is a technique to transfer a photograph or graphic, printed with an inkjet printer onto textiles, cups, CDs, glass and other surfaces.\n\nA special transfer sheet, usually ISO A4 size, is printed on with a regular inkjet printer. The photo has to be printed as a mirror image (except for some transfer sheets for dark materials).\n\nIt is often a requirement of \"home made\" garments (adhesive based transfers) that they be washed inside out, only in cold water, sometimes by hand, and not be tumble dried. The heat from washing or drying conventionally, or from ironing over the transfer area, can damage the transfer or cause it to separate. These attributes generally makes them less practical for frequent wear than purchased items.\n\nThe second type of transfer paper is merely a substrate which ink is deposited onto, from which it is sublimated directly into the t-shirt fibers (requires at least 50% polyester fabric). The main advantages of this process are permanence and lack of a rough adhesive on the surface of the fabric. This technique is used commercially, and not easily reproduced at home, since a heat press is required to heat the inks to their sublimation temperature (over 200°F) evenly.\n\nThe transfer sheet is placed ink side down (usually) onto a t-shirt or fabric and ironed (without steam) onto the cloth. Some transfer sheets change color to signal that the transfer is finished. To create a glossy effect with adhesive based transfers, the transfer sheet is removed after it has been cooled down. To create a matte effect, it is peeled off while still hot.\n"}
{"id": "11294679", "url": "https://en.wikipedia.org/wiki?curid=11294679", "title": "Ivano-Frankivsk National Technical University of Oil and Gas", "text": "Ivano-Frankivsk National Technical University of Oil and Gas\n\nIvano-Frankivsk National Technical University of Oil and Gas () is an institution of higher education in Ivano-Frankivsk, Ukraine. \n\nSince its establishment in 1967, the university has been known for preparing qualified and experienced specialists for the oil and gas industries. However, it has expanded to offer education in economics, management and law that is directly related to the needs of energy industry.\n\nIvano-Frankivsk Oil and Gas University has a two-pronged approach toward education of its students: in the classrooms and in the field. In the classroom, students are provided the theoretical knowledge. Then, by working at university \"field-classrooms\" or by holding internships with regional businesses and organizations, the future specialists gain practical experience. \n\n\n\n"}
{"id": "8731428", "url": "https://en.wikipedia.org/wiki?curid=8731428", "title": "Kile (unit)", "text": "Kile (unit)\n\nThe kile () was an Ottoman unit of volume similar to a bushel, like other dry measures also often defined as a specific weight of a particular commodity. Its value varied widely by location, period, and commodity, from 8 to 132 oka. The 'standard' kile was 36 liters or 20 oka.\n\n"}
{"id": "6513914", "url": "https://en.wikipedia.org/wiki?curid=6513914", "title": "Komar mass", "text": "Komar mass\n\nThe Komar mass (named after Arthur Komar) of a system is one of several formal concepts of mass that are used in general relativity. The Komar mass can be defined in any stationary spacetime, which is a spacetime in which all the metric components can be written so that they are independent of time. Alternatively, a stationary spacetime can be defined as a spacetime which possesses a timelike Killing vector field.\n\nThe following discussion is an expanded and simplified version of the motivational treatment in (Wald, 1984, pg 288).\n\nConsider the Schwarzschild metric. Using the Schwarzschild basis, a for the Schwarzschild metric, one can find that the radial acceleration required to hold a test mass stationary at a Schwarzschild coordinate of r is:\n\nformula_1\n\nBecause the metric is static, there is a well-defined meaning to \"holding a particle stationary\".\n\nInterpreting this acceleration as being due to a \"gravitational force\", we can then compute the integral of normal acceleration multiplied by area to get a \"Gauss law\" integral of:\n\nWhile this approaches a constant as r approaches infinity, it is not a constant independent of r. We are therefore motivated to introduce a correction factor to make the above integral independent of the radius r of the enclosing shell. For the Schwarzschild metric, this correction factor is just formula_3, the \"red-shift\" or \"time dilation\" factor at distance r. One may also view this factor as \"correcting\" the local force to the \"force at infinity\", the force that an observer at infinity would need to apply through a string to hold the particle stationary. (Wald, 1984).\n\nTo proceed further, we will write down a line element for a static metric.\n\nwhere g and the quadratic form are functions only of the spatial coordinates x,y,z and are not functions of time. In spite of our choices of variable names, it should not be assumed that our coordinate system is Cartesian. The fact that none of the metric coefficients are functions of time makes the metric stationary: the additional fact that there are no \"cross terms\" involving both time and space components (such as dx dt) make it static.\n\nBecause of the simplifying assumption that some of the metric coefficients are zero, some of our results in this motivational treatment will not be as general as they could be.\n\nIn flat space-time, the proper acceleration required to hold station is formula_5, where u is the 4-velocity of our hovering particle and tau is the proper time. In curved space-time, we must take the covariant derivative. Thus we compute the acceleration vector as:\n\nwhere u is a unit time-like vector such that u u = -1.\n\nThe component of the acceleration vector normal to the surface is \n\nIn a Schwarzschild coordinate system, for example, we find that \nas expected - we have simply re-derived the previous results presented in a frame-field in a coordinate basis.\n\nWe define formula_10 so that in our Schwarzschild example formula_11.\n\nWe can, if we desire, derive the accelerations a and the adjusted \"acceleration at infinity\" ainf from a scalar potential Z, though there is not necessarily any particular advantage in doing so. (Wald 1984, pg 158, problem 4)\n\nformula_12\nformula_13\n\nWe will demonstrate that integrating the normal component of the \"acceleration at infinity\" ainf over a bounding surface will give us a quantity that does not depend on the shape of the enclosing sphere, so that we can calculate the mass enclosed by a sphere by the integral\n\nformula_14\n\nTo make this demonstration, we need to express this surface integral as a volume integral. In flat space-time, we would use Stokes theorem and integrate formula_15 over the volume. In curved space-time, this approach needs to be modified slightly.\n\nUsing the formulas for electromagnetism in curved space-time as a guide, we write instead.\nformula_16\n\nwhere F plays a role similar to the \"Faraday tensor\", in that formula_17 We can then find the value of \"gravitational charge\", i.e. mass, by evaluating\n\nformula_18 and integrating it over the volume of our sphere.\n\nAn alternate approach would be to use differential forms, but the approach above is computationally more convenient as well as not requiring the reader to understand differential forms.\n\nA lengthy, but straightforward (with computer algebra) calculation from our assumed line element shows us that\n\nformula_19\n\nThus we can write\n\nformula_20\n\nIn any vacuum region of space-time, all components of the Ricci tensor must be zero. This demonstrates that enclosing any amount of vacuum will not change our volume integral. It also means that our volume integral will be constant for any enclosing surface, as long as we enclose all of the gravitating mass inside our surface. Because Stokes theorem guarantees that our surface integral is equal to the above volume integral, our surface integral will also be independent of the enclosing surface as long as the surface encloses all of the gravitating mass.\n\nBy using Einstein's Field Equations\n\nletting u=v and summing, we can show that R = -8 π T.\n\nThis allows us to rewrite our mass formula as a volume integral of the stress–energy tensor.\n\nformula_22\n\nTo make the formula for Komar mass work for a general stationary metric, regardless of the choice of coordinates, it must be modified slightly. We will present the applicable result from (Wald, 1984 eq 11.2.10 ) without a formal proof.\n\nformula_23\n\nNote that formula_26 replaces formula_27 in our motivational result.\n\nIf none of the metric coefficients formula_28 are functions of time, formula_29\n\nWhile it is not \"necessary\" to choose coordinates for a stationary space-time such that the metric coefficients are independent of time, it is often \"convenient\".\n\nWhen we chose such coordinates, the time-like Killing vector for our system formula_30 becomes a scalar multiple of a unit coordinate-time vector formula_31, i.e. formula_32. When this is the case, we can rewrite our formula as\n\nformula_33\n\nBecause formula_34 is by definition a unit vector, K is just the length of formula_24, i.e. K = formula_36.\n\nEvaluating the \"red-shift\" factor K based on our knowledge of the components of formula_37, we can see that K = formula_3.\n\nIf we chose our spatial coordinates so that we have a locally Minkowskian metric formula_39 we know that\n\nWith these coordinate choices, we can write our Komar integral as\n\nWhile we can't choose a coordinate system to make a curved space-time globally Minkowskian, the above formula provides some insight into the meaning of the Komar mass formula. Essentially, both energy and pressure contribute to the Komar mass. Furthermore, the contribution of local energy and mass to the system mass is multiplied by the local \"red shift\" factor formula_42\n\nWe also wish to give the general result for expressing the Komar mass as a surface integral.\n\nThe formula for the Komar mass in terms of the metric and its Killing vector is (Wald, 1984, pg 289, formula 11.2.9)\n\nformula_43\n\nThe surface integral above is interpreted as the \"natural\" integral of a two form over a manifold.\n\nAs mentioned previously, if none of the metric coefficients formula_28 are functions of time, formula_29\n\n\n"}
{"id": "12005945", "url": "https://en.wikipedia.org/wiki?curid=12005945", "title": "Kyushu Electric Power", "text": "Kyushu Electric Power\n\nKyushu Electric Power was founded on May 1, 1951. The company began supplying electricity to Hiroshima in November 2005 - the first provider in Japan to supply energy outside its area.\n\n"}
{"id": "5929271", "url": "https://en.wikipedia.org/wiki?curid=5929271", "title": "Midstream", "text": "Midstream\n\nThe oil and gas industry is usually divided into three major components: upstream, midstream and downstream. The midstream sector involves the transportation (by pipeline, rail, barge, oil tanker or truck), storage, and wholesale marketing of crude or refined petroleum products. Pipelines and other transport systems can be used to move crude oil from production sites to refineries and deliver the various refined products to downstream distributors. Natural gas pipeline networks aggregate gas from natural gas purification plants and deliver it to downstream customers, such as local utilities. \n\nThe midstream operations are often taken to include some elements of the upstream and downstream sectors. For example, the midstream sector may include natural gas processing plants that purify the raw natural gas as well as removing and producing elemental sulfur and natural gas liquids (NGL) as finished end-products. \n\n\nISO 20815 defines \"midstream\" in its definition section as: <br>\n3.1.27 midstream <br>\nbusiness category involving the processing and transportation sectors of petroleum industry.\n\n\n"}
{"id": "8674810", "url": "https://en.wikipedia.org/wiki?curid=8674810", "title": "Nobuo Tanaka", "text": "Nobuo Tanaka\n\nIn 1995 he returned to METI where he served as Director for Industrial Finance Division and as Director for Policy Planning and Coordination Division. In 1998–2000 he was posted at the Embassy of Japan in Washington, D.C., as Minister for Energy, Trade and Industry. After returning to Japan in 2000 he took a post of the Executive Vice President for the Research Institute of Economy Trade and Industry, and in 2002–2004 the post of the Director-General for the Multilateral Trade System Department of METI.\n\nFrom 16 August 2004 to 31 August 2007 Nobuo Tanaka was the Director for Science, Technology and Industry at the OECD, and head of the internal OECD Steering Group for the Centre for Entrepreneurship. On 1 September 2007 he succeeded Claude Mandil as the Executive Director of the IEA. On 1 September 2011, he was succeeded in this role by the Former Minister of Economic Affairs of the Netherlands, Maria van der Hoeven.\n\nHe is now Global Associate for Energy Security and Sustainability at the Institute for Energy Economics, Japan (\"Eneken\") in Tokyo. He is also a Professor at the Graduate School of Public Policy, University of Tokyo. He is also a fellow at the Center on Global Energy Policy at Columbia University in New York where he has given lectures on subjects like Post Fukushima energy policy, the Shale revolution and energy security, China energy and sustainability, and the Integral Fast Reactor during frequent weekly visits to the Columbia Morningside campus each academic semester. He is a vocal advocate for advanced nuclear energy for Japan and international cooperation between Japan and the Republic of Korea to build the first commercial Integral Fast Reactor in the world. On May 28, 2014 he hosted the Global Leader Program for Social Design and Management (GSDM) 15th Platform Seminar, The 79th Public Policy Seminar at the University of Tokyo titled \"Peaceful and Safer Use of Nuclear Power: Role of Integral Fast Reactor\". His research paper on the role of the Integral Fast Reactor was scheduled to be presented in the Spring of 2015 in New York.\n\nNobuo Tanaka is married and has two children.\n"}
{"id": "20260582", "url": "https://en.wikipedia.org/wiki?curid=20260582", "title": "Nuclear programme of South Africa", "text": "Nuclear programme of South Africa\n\nAs a member of the nuclear non-proliferation treaty, South Africa uses nuclear science for peaceful means. South Africa's nuclear programme includes both nuclear energy and nuclear medicine. In the past there was also a military component, and South Africa previously possessed nuclear weapons, which were subsequently dismantled.\n\nThe Koeberg nuclear power station is the only nuclear power station in South Africa and contains two uranium pressurised water reactors based on a design by Framatome of France. The station is located 30 km north of Cape Town. The plant is owned and operated by the country's national electricity supplier, Eskom.\n\nThe Pebble bed modular reactor (PBMR) was a particular design of pebble-bed reactor under development by South African company PBMR (Pty) Ltd since 1994. The project entailed the construction of a demonstration power plant at Koeberg near Cape Town and a fuel plant at Pelindaba near Pretoria. Government financing was withdrawn in 2010 because of missed deadlines and lack of customers.\n\nThe South African Nuclear Energy Corporation (NECSA) was established as a public company by the Republic of South Africa Nuclear Energy Act in 1999 and is wholly owned by the State. NECSA replaced the country's Atomic Energy Corporation. The main functions of NECSA are to undertake and promote research and development in the field of nuclear energy and related technologies; to process and store nuclear material and other restricted material; and to co-ordinate with other organisations in matters falling within these spheres.\n\nThe project is currently being dismantled.\n\nThe following South African universities offer courses in nuclear engineering:\n\nSouth Africa built six nuclear bombs in the 1980s, which were subsequently dismantled.\n\n\n"}
{"id": "26968500", "url": "https://en.wikipedia.org/wiki?curid=26968500", "title": "Outline of animal-powered transport", "text": "Outline of animal-powered transport\n\nThe following outline is provided as an overview of and topical guide to animal-powered transport:\n\nAnimal-powered transport – broad category of the human use of non-human working animals (also known as \"beasts of burden\") for the movement of people and goods. Humans may ride some of the larger of these animals directly on their backs, use them as pack animals for carrying goods, or harness them, singly or in teams, to pull (or haul) sleds or wheeled vehicles.\n\n\n\n\n\n"}
{"id": "9184570", "url": "https://en.wikipedia.org/wiki?curid=9184570", "title": "Particle density (packed density)", "text": "Particle density (packed density)\n\nThe particle density or true density of a particulate solid or powder, is the density of the particles that make up the powder, in contrast to the bulk density, which measures the average density of a large volume of the powder in a specific medium (usually air).\n\nThe particle density is a relatively well-defined quantity, as it is not dependent on the degree of compaction of the solid, whereas the bulk density has different values depending on whether it is measured in the freely settled or compacted state (tap density). However, a variety of definitions of particle density are available, which differ in terms of whether pores are included in the particle volume, and whether voids are included.\n\nThe measurement of particle density can be done in a number of ways:\n\nThe powder is placed inside a pycnometer of known volume, and weighed. The Pycnometer is then filled with a fluid of known density, in which the powder is not soluble. The volume of the powder is determined by the difference between the volume as shown by the pycnometer, and the volume of liquid added (i.e. the volume of air displaced). A similar method, which does not include pore volume, is to suspend a known mass of particles in molten wax of known density, allow any bubbles to escape, allow the wax to solidify, and then measure the volume and mass of the wax/particulate brick.\n\nA slurry of the powder in a liquid of known density can also be used with a hydrometer to measure particle density by buoyancy.\n\nAnother method based on buoyancy is to measure the weight of the sample in air, and also in a liquid of known density.\n\nA column of liquid with a density gradient can also be prepared: The column should contain a liquid of continuously varying composition, so that the maximum density (at the bottom) is higher than that of the solid, and the minimum density is lower. If a small sample of powder is allowed to settle in this column, it will come to rest at the point where the liquid density is equal to the particle density.\n\nA gas pycnometer can be used to measure the volume of a powder sample. A sample of known mass is loaded into a chamber of known volume that is connected by a closed valve to a gas reservoir, also of known volume, at a higher pressure than the chamber. After the valve is opened, the final pressure in the system allows the total gas volume to be determined by application of Boyle's law.\n\nA mercury porosimeter is an instrument that allows the total volume of a powder to be determined, as well as the volume of pores of different sizes: A known mass of powder is submerged in mercury. At ambient pressure, the mercury does not invade the interparticle spaces or the pores of the sample. At increasing pressure, the mercury invades smaller and smaller pores, with the relationship between pore diameter and pressure being known. A continuous trace of pressure versus volume can then be generated, which allows for a complete characterization of the sample's porosity.\n\n\n"}
{"id": "4962816", "url": "https://en.wikipedia.org/wiki?curid=4962816", "title": "Picul", "text": "Picul\n\nA picul \nor tam is a traditional Asian unit of weight, defined as \"a shoulder-load, as much as a man can carry on a shoulder-pole\".\n\nThe word \"picul\" appeared as early as the mid 9th century in Javanese. \n\nFollowing Spanish, Portuguese, British and most especially the Dutch colonial maritime trade, the term \"picul\" was both a convenient unit, and a lingua franca unit that was widely understood and employed by other Austronesians (in modern Malaysia and the Philippines) and their centuries-old trading relations with Indians, Chinese and Arabs. It remained a convenient reference unit for many commercial trade journals in the 19th century. One example is \"Hunts Merchant Magazine\" of 1859 giving detailed tables of expected prices of various commodities, such as coffee, e.g. one picul of Javanese coffee could be expected to be bought from 8 to 8.50 Spanish dollars in Batavia and Singapore.\n\nAs for any traditional measurement unit, the exact definition of the picul varied historically and regionally.\nIn imperial China and later, the unit was used for a measure equivalent to 100 catties.\n\nIn 1831, the Dutch East Indies authorities acknowledged local variances in the definition of the pikul.\nIn Hong Kong, one picul was defined in \"Ordinance No. 22 of 1844\" as avoirdupois pounds. The modern definition is exactly 60.478982 kilograms.\nThe measure was and remains used on occasion in Taiwan where it is defined as 60 kg. The last, a measure of rice, was 20 picul, or 1,200 kg.\n\nWhile the character \"石\" (\"stone\") is normally pronounced \"shi\" (Cantonese: \"sek6\"), as a unit of measure it is pronounced \"dàn\" (Cantonese: \"daam3\").\n\nHistorically, during the Qin and Han dynasties, the stone was used as a unit of measurement equal to 120 catties. Government officials at the time were paid in grain, counted in stones. The amount of salary in weight was then used as a ranking system for officials, with the top ministers being paid 2000 stones.\n\nIn the early days of Hong Kong as a British colony, the stone (石, with a Cantonese pronunciation given as \"shik\") was used as a measurement of weight equal to 120 catties or , alongside the picul of 100 catties. It was made obsolete by subsequent overriding legislation in 1885, which included the picul but not the stone.\n"}
{"id": "42982962", "url": "https://en.wikipedia.org/wiki?curid=42982962", "title": "Ramesh Agrawal", "text": "Ramesh Agrawal\n\nRamesh Agrawal is an Indian social worker, internet café owner and grassroots environmentalist from Chhattisgarh. He was awarded the Goldman Environmental Prize in 2014 for his efforts in organizing protests against certain industrialization plans in the region, and in particular informing citizens about environmental and social consequences of projected large-scale coal mining.\n"}
{"id": "607530", "url": "https://en.wikipedia.org/wiki?curid=607530", "title": "Reaction mechanism", "text": "Reaction mechanism\n\nIn chemistry, a reaction mechanism is the step by step sequence of elementary reactions by which overall chemical change occurs. \n\nA chemical mechanism is a theoretical conjecture that tries to describe in detail what takes place at each stage of an overall chemical reaction. The detailed steps of a reaction are not observable in most cases. The conjectured mechanism is chosen because it is thermodynamically feasible, and has experimental support in isolated intermediates (see next section) or other quantitative and qualitative characteristics of the reaction. It also describes each reactive intermediate, activated complex, and transition state, and which bonds are broken (and in what order), and which bonds are formed (and in what order). A complete mechanism must also explain the reason for the reactants and catalyst used, the stereochemistry observed in reactants and products, all products formed and the amount of each. \nThe electron or arrow pushing method is often used in illustrating a reaction mechanism; for example, see the illustration of the mechanism for benzoin condensation in the following examples section.\n\nA reaction mechanism must also account for the order in which molecules react. Often what appears to be a single-step conversion is in fact a multistep reaction.\n\nReaction intermediates are chemical species, often unstable and short-lived (however sometimes can be isolated), which are not reactants or products of the overall chemical reaction, but are temporary products and/or reactants in the mechanism's reaction steps. Reaction intermediates are often free radicals or ions. \n\nThe kinetics (relative rates of the reaction steps and the rate equation for the overall reaction) are explained in terms of the energy needed for the conversion of the reactants to the proposed transition states (molecular states that corresponds to maxima on the reaction coordinates, and to saddle points on the potential energy surface for the reaction). \n\nInformation about the mechanism of a reaction is often provided by the use of chemical kinetics to determine the rate equation and the reaction order in each reactant.\n\nConsider the following reaction for example:\n\nIn this case, experiments have determined that this reaction takes place according to the rate law formula_1. This form suggests that the rate-determining step is a reaction between two molecules of NO. A possible mechanism for the overall reaction that explains the rate law is:\n\nEach step is called an elementary step, and each has its own rate law and molecularity. The elementary steps should add up to the original reaction. (Meaning, if we were to cancel out all the molecules that appear on both sides of the reaction, we would be left with the original reaction.)\n\nWhen determining the overall rate law for a reaction, the slowest step is the step that determines the reaction rate. Because the first step (in the above reaction) is the slowest step, it is the rate-determining step. Because it involves the collision of two NO molecules, it is a bimolecular reaction with a rate law of formula_1.\n\nOther reactions may have mechanisms of several consecutive steps. In organic chemistry, the reaction mechanism for the benzoin condensation, put forward in 1903 by A. J. Lapworth, was one of the first proposed reaction mechanisms.\nA chain reaction is an example of a complex mechanism, in which the propagation steps form a closed cycle.\n\nMany experiments that suggest the possible sequence of steps in a reaction mechanism have been designed, including:\n\n\nA correct reaction mechanism is an important part of accurate predictive modeling. For many combustion and plasma systems, detailed mechanisms are not available or require development.\n\nEven when information is available, identifying and assembling the relevant data from a variety of sources, reconciling discrepant values and extrapolating to different conditions can be a difficult process without expert help. Rate constants or thermochemical data are often not available in the literature, so computational chemistry techniques or group additivity methods must be used to obtain the required parameters.\n\nComputational chemistry methods can also be used to calculate potential energy surfaces for reactions and determine probable mechanisms.\n\nMolecularity in chemistry is the number of colliding molecular entities that are involved in a single reaction step.\n\nIn general, reaction steps involving more than three molecular entities do not occur, because is statistically improbable in terms of Maxwell distribution to find such transition state.\n\n\nL.G.WADE,ORGANIC CHEMISTRY 7TH ED,2010\n\n"}
{"id": "34214588", "url": "https://en.wikipedia.org/wiki?curid=34214588", "title": "Sonoran Solar Project", "text": "Sonoran Solar Project\n\nThe 300 MW Sonoran Solar Project is a proposed solar energy project in the Sonoran Desert, within Maricopa County, Arizona. \n\nIt is a photovoltaic solar power plant, planned by a subsidiary of NextEra Energy Resources. \n\nSecretary of the Interior Ken Salazar of the Obama administration granted approval for the project in December 2011. It would be the first solar project in Arizona to be built on federally owned land.\n\n"}
{"id": "39636406", "url": "https://en.wikipedia.org/wiki?curid=39636406", "title": "Surface chemistry of neural implants", "text": "Surface chemistry of neural implants\n\nAs with any material implanted in the body, it is important to minimize or eliminate foreign body response and maximize effectual integration. Neural implants have the potential to increase the quality of life for patients with such disabilities as Alzheimer's, Parkinson's, epilepsy, depression, and migraines. With the complexity of interfaces between a neural implant and brain tissue, adverse reactions such as fibrous tissue encapsulation that hinder the functionality, occur. Surface modifications to these implants can help improve the tissue-implant interface, increasing the lifetime and effectiveness of the implant.\n\nIntracranial electrodes consist of conductive electrode arrays implanted on a polymer or silicon, or a wire electrode with an exposed tip and insulation everywhere that stimulation or recording is not desired. Biocompatibility is essential for the entire implant, but special attention is paid to the actual electrodes since they are the site producing the desired function.\n\nOne main physiological issue that current long-term implanted electrodes suffer from are fibrous glial encapsulations after implantation. This encapsulation is due to the poor biocompatibility and biostability (integration at the hard electrode and soft tissue interface) of many neural electrodes being used today. The encapsulation causes a reduced signal intensity because of the increased electrical impedance and decreased charge transfer between the electrode and the tissue. The encapsulation causes decreased efficiency, performance, and durability.\n\nElectrical impedance is the opposition to current flow with an applied voltage, usually represented as \"Z\" in units of ohms (Ω). The impedance of an electrode is especially important as it is directly related to its effectiveness. A high impedance causes poor charge transfer and thus poor electrode performance for either stimulating or recording the neural tissue. Electrode impedance is related to surface area at the interface between the electrode and the tissue. At electrode sites, the total impedance is controlled by the double-layer capacitance. The capacitance value is directly related to the surface area. Increasing the surface area at the electrode-tissue interface will increase the capacitance and thus decrease the impedance. The equation below describes the inverse relationship between the capacitance and impedance.\n\nProteins are typically added to the material surface via self-assembled monolayer (SAM) formation.\n"}
{"id": "56832008", "url": "https://en.wikipedia.org/wiki?curid=56832008", "title": "Suswa–Isinya–Rabai High Voltage Power Line", "text": "Suswa–Isinya–Rabai High Voltage Power Line\n\nSuswa–Isinya–Rabai High Voltage Power Line is an operational high voltage (400 kilo Volts) electricity power line connecting the high voltage substation at Suswa, Kenya to another high voltage substation at Rabai, Kenya.\n\nThe power line starts at Suswa, in Narok County, about , by road, north-west of Nairobi, and runs in a south-easterly direction for approximately to the Ketraco Power Substation at Isinya, in Kajiado County.\n\nFrom Isinya, the power line follows a south-easterly course to end at Rabai, in Kilifi County, approximately , away, as the crow flies. The power line measures about .\n\nThe original plan was to build a 220 kilo Volt transmission line. Plans were later revised and the voltage was increased to 400kV. The line serves three main purposes: (a) It transmits power generated from geothermal power stations in the Eastern Rift Valley, to Kenya's coastal region (b) It transmits power from thermal power stations near the coast to the industrial centers in and near Nairobi (c) Through Suswa, the power line connects to Lessos and Tororo, Uganda, allowing the export of electricity to Uganda and Rwanda.\n\nConstruction started in August 2011, with the Rabai–Isinya section. Construction was budgeted at KSh14 billion (US$140 million), funded with loans from (i) the French Development Agency (ii) the European Investment Bank and (iii) the African Development Bank. The Government of Kenya, invested equity in the project. Kalpataru Power Transmission Limited (KPTL) from India, was the lead contractor on this project. Siemens was the substation contractor. After delays, the power line came on-line in the second half of 2017.\n\n\n"}
{"id": "37642759", "url": "https://en.wikipedia.org/wiki?curid=37642759", "title": "Tariric acid", "text": "Tariric acid\n\nTariric acid is an acetylenic fatty acid that can be found in the tallow-wood tree, \"Ximenia americana\".\n\nLéon-Albert Arnaud (1853–1915) was the first scientist to describe the chemical make-up of tariric acid, an extraction from the glucoside of the \"tariri plant\" found in Guatemala.\n\nTariric acid has been found in several oils and fats of plant origin. It was first isolated in 1892 from the seed oil of a species of \"Picramnia\". It appears in \"Picramnia camboita\" from Brazil, \"Picramnia carpinterae\" from Guatemala, and \"Picramnia lindeniana\" from Mexico.\n\nTariric acid is biosynthesised from petroselinic acid; both fatty acids have been found together in \"Picramnia\" and \"Alvaradoa\" species. The occurrence of tariric acid as the major fatty acid is typical for the Picramniaceae.\n\nTariric acid can be synthesised from commercially available petroselinic acid.\n\nIn chemical analysis, tariric acid can be separated from other fatty acids by gas chromatography of methyl esters; additionally, a separation of unsaturated fatty acids is possible by argentation thin-layer chromatography.\n"}
{"id": "16142363", "url": "https://en.wikipedia.org/wiki?curid=16142363", "title": "Techa River", "text": "Techa River\n\nThe Techa River is a river on the eastern flank of the southern Ural Mountains noted for its nuclear contamination. It is about long, and its basin covers . It begins at the formerly secret nuclear-processing town of Ozyorsk, Chelyabinsk Oblast about northwest of Chelyabinsk and flows northeast to Dalmatovo on the Iset River, a tributary of the Tobol River. Its basin is enclosed on the southeast by that of the Miass River, another river that flows northeast into the Iset.\n\nFrom 1949 to 1956 the Mayak complex dumped an estimated of radioactive waste water into the Techa River, a cumulative dispersal of of radioactivity.\n\nAs many as forty villages, with a combined population of about 28,000 residents, lined the river at the time. For 24 of them, the Techa was a major source of water; 23 of them were eventually evacuated. In the past 45 years, about half a million people in the region have been irradiated in one or more of the incidents, exposing them to as much as 20 times the radiation suffered by the Chernobyl disaster victims.\n\n"}
{"id": "140615", "url": "https://en.wikipedia.org/wiki?curid=140615", "title": "Tropopause", "text": "Tropopause\n\nThe tropopause is the boundary in the Earth's atmosphere between the troposphere and the stratosphere. It is a thermodynamic gradient stratification layer, marking the end of troposphere. It lies, on average, at above equatorial regions, and above over the polar regions.\n\nGoing upward from the surface, it is the point where air ceases to cool with height, and becomes almost completely dry. More formally, the tropopause is the region of the atmosphere where the environmental lapse rate changes from positive, as it behaves in the troposphere, to the stratospheric negative one. Following is the exact definition used by the World Meteorological Organization:\n\nThe tropopause as defined above renders as a first-order discontinuity surface, that is, \"temperature\" as a function of height varies continuously through the atmosphere but the \"temperature gradient\" does not.\n\nThe troposphere is the lowest layer of the Earth's atmosphere; it is located right above the planetary boundary layer, and is the layer in which most weather phenomena take place. The troposphere contains the boundary layer, and ranges in height from an average of at the poles, to at the Equator. In the absence of inversions and not considering moisture, the temperature lapse rate for this layer is 6.5 °C per kilometer, on average, according to the \"U.S. Standard Atmosphere\". A measurement of both the tropospheric and the stratospheric lapse rates helps identifying the location of the tropopause, since temperature increases with height in the stratosphere, and hence the lapse rate becomes negative. The tropopause location coincides with the lowest point at which the lapse rate falls below a prescribed threshold.\n\nSince the tropopause responds to the average temperature of the entire layer that lies underneath it, it is at its peak levels over the Equator, and reaches minimum heights over the poles. On account of this, the coolest layer in the atmosphere lies at about 17 km over the equator. Due to the variation in starting height, the tropopause extremes are referred to as the equatorial tropopause and the polar tropopause.\n\nGiven that the lapse rate is not a conservative quantity when the tropopause is considered for stratosphere-troposphere exchanges studies, there exists an alternative definition named \"dynamic tropopause\". It is formed with the aid of potential vorticity, which is defined as the product of the isentropic density, i.e. the density that arises from using potential temperature as the vertical coordinate, and the absolute vorticity, given that this quantity attains quite different values for the troposphere and the stratosphere. Instead of using the vertical temperature gradient as the defining variable, the dynamic tropopause surface is expressed in \"potential vorticity units\" (PVU). Given that the absolute vorticity is positive in the Northern Hemisphere and negative in the Southern Hemisphere, the threshold value should be taken as positive north of the Equator and negative south of it. Theoretically, to define a global tropopause in this way, the two surfaces arising from the positive and negative thresholds need to be matched near the equator using another type of surface such as a constant potential temperature surface. Nevertheless, the dynamic tropopause is useless at equatorial latitudes because the isentropes are almost vertical. For the extratropical tropopause in the Northern Hemisphere the WMO established a value of 1.5 PVU, but greater values ranging between 2 and 3.5 PVU have been traditionally used.\n\nIt is also possible to define the tropopause in terms of chemical composition. For example, the lower stratosphere has much higher ozone concentrations than the upper troposphere, but much lower water vapor concentrations, so appropriate cutoffs can be used.\n\nThe tropopause is not a \"hard\" boundary. Vigorous thunderstorms, for example, particularly those of tropical origin, will overshoot into the lower stratosphere and undergo a brief (hour-order or less) low-frequency vertical oscillation. Such oscillation sets up a low-frequency atmospheric gravity wave capable of affecting both atmospheric and oceanic currents in the region.\n\nMost commercial aircraft are flown in the lower stratosphere, just above the tropopause, where clouds are usually absent, as also are significant weather perturbations.\n\n\n\n"}
{"id": "7056315", "url": "https://en.wikipedia.org/wiki?curid=7056315", "title": "Two-dimensional gas", "text": "Two-dimensional gas\n\nA two-dimensional gas is a collection of objects constrained to move in a planar or other two-dimensional space in a gaseous state. The objects can be: ideal gas elements such as rigid disks undergoing elastic collisions; elementary particles, or any object in physics which obeys laws of motion. The concept of a two-dimensional gas is used either because:\n\nWhile physicists have studied simple two body interactions on a plane for centuries, the attention given to the two-dimensional gas (having many bodies in motion) is a 20th-century pursuit. Applications have led to better understanding of superconductivity, gas thermodynamics, certain solid state problems and several questions in quantum mechanics.\n\nResearch at Princeton University in the early 1960s posed the question of whether the Maxwell–Boltzmann statistics and other thermodynamic laws could be derived from Newtonian laws applied to multi-body systems rather than through the conventional methods of statistical mechanics. While this question appears intractable from a three-dimensional closed form solution, the problem behaves differently in two-dimensional space. In particular an ideal two-dimensional gas was examined from the standpoint of relaxation time to equilibrium velocity distribution given several arbitrary initial conditions of the ideal gas. Relaxation times were shown to be very fast: on the order of mean free time .\n\nIn 1996 a computational approach was taken to the classical mechanics non-equilibrium problem of heat flow within a two-dimensional gas. This simulation work showed that for N>1500, good agreement with continuous systems is obtained.\n\nWhile the principle of the cyclotron to create a two-dimensional array of electrons has existed since 1934, the tool was originally not really used to analyze interactions among the electrons (e.g. two-dimensional gas dynamics). An early research investigation to explore a two-dimensional electron gas with respect to cyclotron resonance behavior and the de Haas–van Alphen effect. The investigator was able to demonstrate for a two-dimensional gas, the de Haas–van Alphen oscillation period is independent of the short-range electron interactions.\n\nIn 1991 a theoretical proof was made that a Bose gas can exist in two dimensions. In the same work an experimental recommendation was made that could verify the hypothesis.\n\nIn general, 2D molecular gases are experimentally observed on weakly interacting surfaces such as metals, graphene etc. at a non-cryogenic temperature and a low surface coverage. As a direct observation of individual molecules is not possible due to fast diffusion of molecules on a surface, experiments are either indirect (observing an interaction of a 2D gas with surroundings, e.g. condensation of a 2D gas) or integral (measuring integral properties of 2D gases, e.g. by diffraction methods).\n\nAn example of the indirect observation of a 2D gas is the study of Stranick et al. who used a scanning tunnelling microscope in ultrahigh vacuum (UHV) to image an interaction of a two-dimensional benzene gas layer in contact with a planar solid interface at 77 kelvins. The experimenters were able to observe mobile benzene molecules on the surface of Cu(111), to which a planar monomolecular film of solid benzene adhered. Thus the scientists could witness the equilibrium of the gas in contact with its solid state.\n\nIntegral methods that are able to characterize a 2D gas usually fall into a category of diffraction (see for example study of Kroger et al.). The exception is the work of Matvija et al. who used a scanning tunneling microscope to directly visualize a local time-averaged density of molecules on a surface. This method is of special importance as it provides an opportunity to probe local properties of 2D gases; for instance it enables to directly visualize a pair correlation function of a 2D molecular gas in a real space.\n\nIf the surface coverage of adsorbates is increased, a 2D liquid is formed, followed by a 2D solid. It was shown that the transition from a 2D gas to a 2D solid state can be controlled by a scanning tunneling microscope which can affect the local density of molecules via an electric field.\n\nA multiplicity of theoretical physics research directions exist for study via a two-dimensional gas. Examples of these are\n\n\n\n"}
{"id": "24218313", "url": "https://en.wikipedia.org/wiki?curid=24218313", "title": "Uttam Ghoshal", "text": "Uttam Ghoshal\n\nUttam Ghoshal is an Indian-American scientist in the field of thermodynamic electrical engineering, notable for cooling green technologies with applications in green refrigeration and solar power generation. He founded Sheetak Inc. which received funding from ARPA-E.\n\nHe received his B.Tech. in Electrical Engineering from the Indian Institute of Technology, Bombay. He received his Ph.D. in Electrical Engineering from the University of California, Berkeley with T. Van Duzer as his advisor. Prior to Sheetak, he was a Vice President at Cypress Semiconductors, was the founder and CTO of nanoCoolers until its closure, and was a Senior Researcher Staff member and Master Inventor at IBM.\n\n"}
{"id": "75876", "url": "https://en.wikipedia.org/wiki?curid=75876", "title": "Walther Nernst", "text": "Walther Nernst\n\nWalther Hermann Nernst, (25 June 1864 – 18 November 1941) was a German chemist known for his work in thermodynamics, physical chemistry, electrochemistry, and solid state physics. His formulation of the Nernst heat theorem helped pave the way for the third law of thermodynamics, for which he won the 1920 Nobel Prize in Chemistry. He is also known for developing the Nernst equation in 1887.\n\nNernst was born in Briesen in West Prussia (now Wąbrzeźno, Poland) to Gustav Nernst (1827–1888) and Ottilie Nerger (1833–1876). His father was a country judge. Nernst had three older sisters and one younger brother. His third sister died of cholera. Nernst went to elementary school at Graudenz. He studied physics and mathematics at the universities of Zürich, Berlin, Graz and Würzburg, where he received his doctorate 1887. In 1889, he finished his habilitation at University of Leipzig.\n\nIt was said that Nernst was mechanically minded in that he was always thinking of ways to apply new discoveries to industry. His hobbies included hunting and fishing. His friend Albert Einstein was amused by \"his childlike vanity and self-complacency\" \"His own study and laboratory always presented aspects of extreme chaos which his coworkers termed appropriately 'the state of maximum entropy'\".\n\nNernst married Emma Lohmeyer in 1892 with whom he had two sons and three daughters. Both of Nernst's sons died fighting in World War I. He was a friend and colleague of Svante Arrhenius, and suggested setting fire to unused coal seams to increase the global temperature. He was a vocal critic of Adolf Hitler and Nazism, and two of his three daughters married Jewish men. After Hitler came to power they emigrated, one to England and the other to Brazil. Nazism also ended Nernst's career as a scientist. Nernst had a severe heart attack in 1939. He died in 1941 and is buried near Max Planck, Otto Hahn and Max von Laue in Göttingen, Germany.\n\nNernst started university at Zurich in 1883, then after an interlude in Berlin, he returned to Zurich. He wrote his thesis at Graz where Boltzmann was professor, though he worked under the direction of Ettinghausen. They discovered the Nernst effect: that a magnetic field applied perpendicular to a metallic conductor in a temperature gradient gives rise to an electrical potential difference. Next, he moved to Würzburg under Kohlrausch where he submitted and defended his thesis. Ostwald recruited him to the first department of physical chemistry at Leipzig. Nernst moved there as an assistant, working on the thermodynamics of electrical currents in solutions. Promoted to lecturer, he taught briefly at Heidelberg and then moved to Göttingen. Three years later, he was offered a professorship in Munich, to keep him in Prussia the government created a chair for him at Göttingen. There, he wrote a celebrated textbook \"Theoretical Chemistry\", which was translated into English, French, and Russian. He also derived the Nernst equation for the electrical potential generated by unequal concentrations of an ion separated by a membrane that is permeable to the ion. His equation is widely used in cell physiology and neurobiology. \n\nThe carbon electric filament lamp then in use was dim and expensive because it required a vacuum in its bulb. Nernst invented a solid-body radiator with a filament of rare-earth oxides, known as the Nernst glower, it is still important in the field of infrared spectroscopy. Continuous ohmic heating of the filament results in conduction. The glower operates best in wavelengths from 2 to 14 micrometers. It gives a bright light but only after a warm-up period. Nernst sold the patent for one million marks, wisely not opting for royalties because soon the tungsten filament lamp filled with inert gas was introduced. With his riches, Nernst in 1898 bought the first of the eighteen automobiles he owned during his lifetime and a country estate of more than a five hundred hectares for hunting. He increased the power of his early automobiles by carrying a cylinder of nitrous oxide that he could inject into the carburetor. After eighteen productive years at Göttingen, investigating osmotic pressure and electrochemistry and presenting a theory of how nerves conduct, he moved to Berlin, and was awarded the title \"Geheimrat\"\n\nIn 1905, he proposed his \"New Heat Theorem\", later known as the Third law of thermodynamics. He showed that as the temperature approached absolute zero, the entropy approaches zero while the free energy remains above zero. This is the work for which he is best remembered, as it enabled chemists to determine free energies (and therefore equilibrium points) of chemical reactions from heat measurements. Theodore Richards claimed that Nernst had stolen his idea, but Nernst is almost universally credited with the discovery. Nernst became friendly with Kaiser Wilhelm, whom he persuaded to found the \"Kaiser Wilhelm Gesellschaft\" for the Advancement of the Sciences with an initial capital of eleven million marks. Nernst's laboratory discovered that at low temperatures specific heats fell markedly and would probably disappear at absolute zero. This fall was predicted for liquids and solids in a 1909 paper of Einstein's on the quantum mechanics of specific heats at cryogenic temperatures. Nernst was so impressed that he traveled all the way to Zurich to visit Einstein, who was relatively unknown in Zurich in 1909, so people said: \"Einstein must be a clever fellow if the great Nernst comes all the way from Berlin to Zurich to talk to him.\" Nernst and Planck lobbied to establish a special professorship in Berlin and Nernst donated to its endowment. In 1913 they traveled to Switzerland to persuade Einstein to accept it; a dream job: a named professorship at the top university in Germany, without teaching duties, leaving him free for research.\n\nIn 1911, Nernst and Max Planck organized the first Solvay Conference in Brussels. In the following year, the impressionist painter Max Liebermann painted his portrait.\n\nIn 1914, the Nernsts were entertaining coworkers and students they had brought to their country estate in a private railway car when they learned that war had been declared. Their two older sons entered the army, while father enlisted in the voluntary driver's corps. He supported the German army against their opponent's charges of barbarism by signing the Manifesto of the Ninety-Three, On 21 August 1914, he drove documents from Berlin to the commander of the German right wing in France, advancing with them for two weeks until he could see the glow of the Paris lights at night. The tide turned at the battle of the Marne. When the stalemate in the trenches began, he returned home. He contacted Colonel Max Bauer, the staff officer responsible for munitions, with the idea of driving the defenders out of their trenches with shells releasing tear gas. When his idea was tried one of the observers was Fritz Haber, who argued that too many shells would be needed, it would be better to release a cloud of heavier-than-air poisonous gas; the first chlorine cloud attack on 22 April 1915 was not supported by a strong infantry thrust, so the chance that gas would break the stalemate was irrevocably gone. Nernst was awarded the Iron Cross second class. As a Staff Scientific Advisor in the Imperial German Army, he directed research on explosives, much of which was done in his laboratory where they developed guanidine perchlorate. Then he worked on the development of trench mortars. He was awarded the Iron Cross first class and later the \"Pour le Mérite\". When the high command was considering unleashing unrestricted submarine warfare, he asked the Kaiser for an opportunity to warn about the enormous potential of the United States as an adversary. They would not listen, Ludendorff shouted him down for \"incompetent nonsense.\" He published his book \"The Foundations of the New Heat Theorem\".\nBoth sons had died at the front.\n\nIn 1918, after studying photochemistry, he proposed the atomic chain reaction theory. It stated that when a reaction in which free atoms are formed that can decompose target molecules into more free atoms would result in a chain reaction. His theory is closely related to the natural process of Nuclear Fission.\n\nIn 1920, he and his family briefly fled abroad because he was one of the scientists on the Allied list of war criminals. Later that year he received the Nobel Prize in chemistry in recognition of his work on thermochemistry. He was elected Rector of Berlin University for 1921-1922. He set up an agency to channel government and private funds to young scientists and declined becoming Ambassador to the United States. For two unhappy years, he was the president of the \"Physikalisch-Technische Reichsanstalt\" (National Physical Laboratory), where he could not cope with the \"mixture of mediocrity and red tape\". In 1924, he became director of the \"Institute of Physical Chemistry\" at Berlin. Although the press release described him as \"completely unmusical\", Nernst developed an electric piano, the \"Neo-Bechstein-Flügel\" in 1930 in association with the Bechstein and Siemens companies, replacing the sounding board with vacuum tube amplifiers. The piano used electromagnetic pickups to produce electronically modified and amplified sound in the same way as an electric guitar. In fact, he was a pianist, sometimes accompanying Einstein's violin.\n\nIn 1927, the decrease in specific heat at low temperatures was extended to gases. He studied the theories of cosmic rays and cosmology.\n\nIn 1933, Nernst learned that a colleague, with whom he had hoped to collaborate, had been dismissed from the department because he was a Jew. Nernst immediately taxied to see Haber to request a position in his Institute, which was not controlled by the government, only to learn that Haber was moving to England. Soon, Nernst was in trouble for declining to fill out a government form on his racial origins. He retired from his professorship but was sacked from the board of the Kaiser Wilhelm Institute. He lived quietly in the country; in 1937 he traveled to Oxford to receive an honorary degree, also visiting his eldest daughter, her husband, and his three grandchildren.\n\n\n\n\n\n"}
{"id": "7241834", "url": "https://en.wikipedia.org/wiki?curid=7241834", "title": "Water sampling stations", "text": "Water sampling stations\n\nTo enhance water quality monitoring in a drinking water network, water sampling stations are installed at various points along the network's route. These sampling stations are typically positioned at street level, where they connect to a local water main, and are designed as enclosed, secured boxes containing a small sink and spigot to aid in sample collection. Collected samples are analyzed for bacteria, chlorine levels, pH, inorganic and organic pollutants, turbidity, odor, and many other water quality indicators.\n\nIn the United States, water sampling stations aid in public infrastructural safety in regards to water quality monitoring, and help municipalities comply with federal and state drinking water regulations. New York City has 965 sampling stations that are distributed based on population density, water pressure zones, proximity to water mains, and accessibility. The stations rise about 4½ feet above the ground and are made of heavy cast iron. Using these stations, the New York City Department of Environmental Protection (DEP) collects more than 1,200 water samples per month from up to 546 locations. \n\n"}
{"id": "3409726", "url": "https://en.wikipedia.org/wiki?curid=3409726", "title": "Yttrium iron garnet", "text": "Yttrium iron garnet\n\nYttrium iron garnet (YIG) is a kind of synthetic garnet, with chemical composition (Fe), or YFeO. It is a ferrimagnetic material with a Curie temperature of 560 K. YIG may also be known as yttrium ferrite garnet, or as iron yttrium oxide or yttrium iron oxide, the latter two names usually associated with powdered forms.\n\nIn YIG, the five iron(III) ions occupy two octahedral and three tetrahedral sites, with the yttrium(III) ions coordinated by eight oxygen ions in an irregular cube. The iron ions in the two coordination sites exhibit different spins, resulting in magnetic behavior. By substituting specific sites with rare earth elements, for example, interesting magnetic properties can be obtained.\n\nYIG has a high Verdet constant which results in the Faraday effect, high Q factor in microwave frequencies, low absorption of infrared wavelengths down to 1200 nm, and very small linewidth in electron spin resonance. These properties make it useful for MOI (magneto optical imaging) applications in superconductors.\n\nYIG is used in microwave, acoustic, optical, and magneto-optical applications, e.g. microwave YIG filters, or acoustic transmitters and transducers. It is transparent for light wavelengths over 600 nm. It also finds use in solid-state lasers in Faraday rotators, in data storage, and in various nonlinear optics applications.\n\n"}
