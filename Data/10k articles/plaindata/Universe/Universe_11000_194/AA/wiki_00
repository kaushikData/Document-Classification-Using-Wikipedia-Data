{"id": "27458074", "url": "https://en.wikipedia.org/wiki?curid=27458074", "title": "2007 Haradh, Saudi Arabia gas pipeline explosion", "text": "2007 Haradh, Saudi Arabia gas pipeline explosion\n\n2007 Haradh, Saudi Arabia, gas pipeline explosion occurred just after midnight on 18 November 2007 at a natural gas pipeline near Haradh gas plant and at a distance of thirty kilometers (eighteen miles) off the Hawiya gas plant in eastern Saudi Arabia. The plant and the pipeline are operated by Saudi Aramco, the country's national oil company, and contract workers were linking a new pipe to the line at the time of the explosion.\n\nAccording to first reports, twenty-eight workers, including five Aramco employees, died in the explosion. Their nationalities were not specified by the company, nor the number of the injured. According to diplomatic sources, most of the dead were Asian workers along with at least one Lebanese national. According to Arab News, King Fahd Hospital in Al-Ahsa had received 10 non-Saudi workers who were injured. However, the next day, as a special technical panel set up by Saudi Aramco continued its probe yesterday to determine the reason for the blast and the subsequent fire, it was announced that the death toll had risen to 38. The death toll was later determined as 34, three of whom were identifiable, while a special fatwa from the Senior Ulema Council of Saudi Arabia had to be obtained to allow the bodies charred beyond recognition, some of whom were Muslims and some of whom non-Muslims, to be buried together.\n\nThere was no immediate suggestion of a terrorist link to the incident and the incident was affirmed as being purely maintenance-related by Aramco.\n\nHaradh and Hawiya plants, opened 2002-2003, are among Saudi Arabia’s major gas-processing plants, in the desert near the Ghawar Field, the world’s largest, south of Dhahran.\n"}
{"id": "26053735", "url": "https://en.wikipedia.org/wiki?curid=26053735", "title": "Active matter", "text": "Active matter\n\nActive matter is composed of large numbers of active \"agents\", each of which consumes energy in order to move or to exert mechanical forces. Due to the energy consumption, these systems are intrinsically out of thermal equilibrium. Examples of active matter are schools of fish, flocks of birds, bacteria, artificial self-propelled particles, and self-organising bio-polymers such as microtubules and actin, both of which are part of the cytoskeleton of living cells. Most examples of active matter are biological in origin; however, a great deal of current experimental work is devoted to synthetic systems. Active matter is a relatively new material classification in soft matter: the most extensively studied model, the Vicsek model, dates from 1995.\n\nResearch in active matter combines analytical techniques, numerical simulations and experiments. Notable analytical approaches include hydrodynamics, kinetic theory, and non-equilibrium statistical physics. Numerical studies mainly involve self-propelled-particles models, making use of agent-based models such as molecular dynamics algorithms as well as computational studies of hydrodynamic equations of active fluids. Experiments on biological systems extend over a wide range of scales, including animal groups (e.g., bird flocks, mammalian herds, fish schools and insect swarms), bacterial colonies, cellular tissues (e.g. epithelial tissue layers, cancer growth and embryogenesis), cytoskeleton components (e.g., \"in vitro\" motility assays, actin-myosin networks and molecular-motor driven filaments). Experiments on synthetic systems include self-propelled colloids (e.g., phoretically propelled particles), driven granular matter (e.g. vibrated monolayers), swarming robots and Quinke rotators.\n\nConcepts in Active matter\n\nActive matter systems\n"}
{"id": "1658929", "url": "https://en.wikipedia.org/wiki?curid=1658929", "title": "Adrian Bejan", "text": "Adrian Bejan\n\nAdrian Bejan is a Romanian American professor who has made contributions to modern thermodynamics and developed what he calls the constructal law. He is J. A. Jones Distinguished Professor of Mechanical Engineering at Duke University and author of the 2016 book \"The Physics of Life: The Evolution of Everything\".\n\nBejan was born in Galaţi, a port town located on the Danube in Romania.\nHis mother, Marioara Bejan (1914–1998), was a pharmacist. His father, Dr. Anghel Bejan (1910–1976), was a veterinarian. Bejan showed an early talent in drawing, and his parents enrolled him in art school. He also excelled in basketball, which earned him a position on the Romanian national basketball team.\n\nAt age 19 Bejan won a scholarship to the United States and entered Massachusetts Institute of Technology in Cambridge, Massachusetts. In 1972 he was awarded BS and MS degrees as a member of the Honors Course in Mechanical Engineering. He graduated in 1975 with a PhD from MIT with a thesis titled \"Improved thermal design of the cryogenic cooling system for a superconducting synchronous generator\". His advisor was Joseph L. Smith Jr.\n\nFrom 1976 to 1978 Bejan was a Miller research fellow in at the University of California Berkeley working with Chang-Lin Tien. In 1978 he moved to Colorado and joined the faculty of the Department of Mechanical Engineering at the University of Colorado in Boulder. In 1982 Bejan published his first book, \"Entropy Generation Through Heat and Fluid Flow\". The book is aimed at practical applications of the second law of thermodynamics, and presented his ideas on irreversibility, availability and exergy analysis in a form for engineers. In 1984 he published \"Convection Heat Transfer\"'. In an era when researchers did heat transfer calculations using numerical methods on supercomputers, the book emphasized new research methods such as intersection of asymptotes, heatlines, and scale analysis to solve problems.\n\nBejan was appointed full professor at Duke University in 1984. In 1988 he published the first edition of his textbook \"Advanced Engineering Thermodynamics\". The book combined thermodynamics theory with engineering heat transfer and fluid mechanics, and introduced entropy generation minimization as a method of optimization. In 1996 the ASME awarded him the Worcester Reed Warner Medal for \"originality, challenges to orthodoxy, and impact on thermodynamics and heat transfer, which were made through his first three books\".\n\nIn 1989 Bejan was appointed the J. A. Jones Distinguished Professor of Mechanical Engineering. In 1988 and 1989, his peers named two dimensionless groups \"Bejan number (Be),\" in two different fields: for the pressure difference group, in heat transfer by forced convection, and for the dimensionless ratio of fluid friction irreversibility divided by heat transfer irreversibility, in thermodynamics. From 1992 to 1996 he published four more books, \"Convection in Porous Media\", \"Heat Transfer\", \"Thermal Design and Optimization\" and \"Entropy Generation Minimization\".\n\nIn 1995 while reviewing entropy generation minimization for a symposium paper and writing another paper on the cooling of electronic components, Bejan formulated his self-described constructal law. Where electronic components are too small for convective cooling, they must be designed for efficient conduction. The paper provides a method for efficiently designing conductive paths, from smaller paths leading to larger ones. The similarity of the solution to the branching structures seen in multiple inanimate and living things led to his statement of what he calls a new law of nature: \"For a finite-size system to persist in time (to live), it must evolve in such a way that it provides easier access to the imposed (global) currents that flow through it.\" To emphasize the coming together of paths he called the theory \"constructal\" from the Latin \"to build\", in contrast with approaches using \"fractal\" geometry, from the Latin \"to break\".\n\nBejan incorporated his constructal law into the second edition of his textbook, \"Advanced Engineering Thermodynamics\" (1997). Since then he has concentrated on constructal law and its applications. In 2004 he published \"Porous and Complex Flow Structures in Modern Technologies\". The same year he and Sylvie Lorente were awarded the Edward F. Obert Award by the ASME for their paper \"Thermodynamic Formulation of the Constructal Law\" In 2008 he published \"Design with Constructal Theory\", a textbook for the course he developed with Lorente at Duke. In 2011 the American Society of Mechanical Engineers presented him with an honorary membership. He was cited for \"an extraordinary record of creative work, including the unification of thermodynamics and heat transfer; the conceptual development of design as a science that unites all fields; legendary contributions to engineering education; and, since 1996, the discovery and continued development of the constructal law.\"\n\nBejan has also written books for the general audience. In 2012 he published \"Design in Nature: How the Constructal Law Governs Evolution in Biology, Technology, and Social Organization\" and 2016 \"The Physics of Life: The Evolution of Everything\". He credits these books for his award of the Ralph Coats Roe Medal from the ASME in 2017. He was cited for \"permanent contributions to the public appreciation of the pivotal role of engineering in an advanced society through outstanding accomplishments as an engineering scientist and educator, renowned communicator and prolific writer\".\n\nIn November 2017 the Franklin Institute of Philadelphia announced that Bejan would be awarded the 2018 Benjamin Franklin Medal in Mechanical Engineering. He was cited for \"his pioneering interdisciplinary contributions in thermodynamics and convection heat transfer that have improved the performance of engineering systems, and for constructal theory, which predicts natural design and its evolution in engineering, scientific, and social systems.\" \n\nBejan has received multiple awards and honorary degrees.\n\n\n\n\n\n"}
{"id": "52103658", "url": "https://en.wikipedia.org/wiki?curid=52103658", "title": "Bangladesh Energy Regulatory Commission", "text": "Bangladesh Energy Regulatory Commission\n\nBangladesh Energy Regulatory Commission or BERC, is a regulatory agency that regulates the gas, electricity and petroleum products in Bangladesh and is located in Dhaka, Bangladesh. \nThe agency was created in 2004 and is responsible for the setting up of gas and electricity prices in Bangladesh. Its also arbitrates disputes in the energy industry. Its approval is need for any changes in the price of electricity. The Energy Security Fund is under this agency.\n"}
{"id": "42459734", "url": "https://en.wikipedia.org/wiki?curid=42459734", "title": "Brighton Energy Co-operative", "text": "Brighton Energy Co-operative\n\nBrighton Energy Co-operative is a cooperative based in Brighton & Hove, United Kingdom that operates renewable energy plants. It is incorporated as a Community Benefit Society under the Industrial and Provident Societies Act. Live operations include 132 kWp of photovoltaic solar panels, installed in 2012, located across three sites in the city: St George's Church in Kemptown, City Coast Church in Portslade and the Hove Enterprise Centre at Shoreham Port.\n\nIn 2013, it launched a second community share offer to raise up to £616,000 to purchase and install up to 565 kWp of photovoltaic capacity, mainly on huge sheds at Shoreham.\n\nSince then more than 400 people have joined BEC and the coop has raised more than £1.5m of community investment. BEC has installed on many buildings in and around Brighton and Hove (see a map here), including:\n\n\"Portslade Aldridge Community Academy (PACA) - 100kWp\"\n\n\"Infinity Foods, Portslade - 90kWp\"\n\n\"University of Brighton Hillbrow Sports Centre - 120kWp\"\n\n\"University of Brighton, Varley Halls - 30KWp\"\n\n\"Hove Enterprise Centre, Hove - 87kWp\"\n\n\"Big Lemon Solar Bus Depot - 30kWp\"\n\n\"Shed 3a, Shoreham Port - 200kWp\"\n\n\"Shed 10, Shoreham Port - 212kWp\"\n\n\"St George's Church, Brighton - 10kWp\"\n\n\"BCMY, Littlehampton - 60kWp\"\n\n\"Park Gate, Hove - 100kWp\"\n\n\"Maidstone United FC - 140kWp\"\n\n\"RT Page & Sons - 100kWp\"\n\n\"City Coast Church, Portslade - 37kWp\"\n\n\"Brighton EarthShip, Brighton - 3kWp\"\n\n\"Refine Metals, Chiddingfold - 177kWp\"\n\n\"Medisort\", Littlehampton, 60kWp\n\n\"University of Brighton Robert Dodd annex - 30kWp\"\n\n\"Moulescoomb Forest Garden - 4kWp\"\n"}
{"id": "7586367", "url": "https://en.wikipedia.org/wiki?curid=7586367", "title": "Caesium cadmium bromide", "text": "Caesium cadmium bromide\n\nCaesium cadmium bromide (CsCdBr) is a synthetic crystalline material. It belongs to the AMX group (where A = alkali metal, M = bivalent metal, X = halogen ion). As a single crystal structure doped with rare-earth ions, it can be used as active laser medium. However, its hygroscopic behaviour limits its use as a laser crystal.\n"}
{"id": "5227275", "url": "https://en.wikipedia.org/wiki?curid=5227275", "title": "Cerium hexaboride", "text": "Cerium hexaboride\n\nCerium hexaboride (CeB, also called cerium boride, CeBix, CEBIX, and (incorrectly) CeB) is an inorganic chemical, a boride of cerium. It is a refractory ceramic material. It has low work function and one of the highest electron emissivity known, and is stable in vacuum. The principal use of cerium hexaboride is a coating of hot cathodes, or hot cathodes made of cerium hexaboride crystals. It usually operates at temperature of 1450 °C.\n\nLanthanum hexaboride (LaB) and cerium hexaboride (CeB) are used as coating of some high-current hot cathodes. Hexaborides show low work function, around 2.5 eV. They are also somewhat resistant to cathode poisoning. Cerium boride cathodes show lower evaporation rate at 1700 K than lanthanum boride, but it becomes equal at 1850 K and higher above that. Cerium boride cathodes have one and half the lifetime of lanthanum boride, due to its higher resistance to carbon contamination. Boride cathodes are about ten times as \"bright\" than the tungsten ones and have 10–15 times longer lifetime. In some laboratory tests, CeB has proven to be more resistant to the negative impact of carbon contamination than LaB. They are used e.g. in electron microscopes, microwave tubes, electron lithography, electron beam welding, X-Ray tubes, and free electron lasers.\n\nCerium hexaboride, like lanthanum hexaboride, slowly evaporates during the cathode operation. In conditions where CeB cathodes are operated under 1850 K, CeB should maintain its optimum shape longer and therefore last longer. While the process is about 30% slower than with lanthanum boride, the cerium boride deposits are reported to be more difficult to remove.\n\nThe Ce heavy fermion compounds have attracted much attention since they show a variety of unusual and interesting macroscopic properties. In particular, interest has been focused on the 4f narrow-band occupancy, and the role of hybridization with the conduction band states which strongly affects the physical properties.\n"}
{"id": "45327132", "url": "https://en.wikipedia.org/wiki?curid=45327132", "title": "Circulating water plant", "text": "Circulating water plant\n\nA circulating water plant or circulating water system is an arrangement of flow of water in fossil-fuel power station, chemical plants and in oil refineries. The system is required because various industrial process plants uses heat exchanger, and also for active fire protection measures. In chemical plants, for example in caustic soda production, water is needed in bulk quantity for preparation of brine. The circulating water system in any plant consists of a circulator pump, which develops an appropriate hydraulic head, and pipelines to circulate the water in the entire plant.\n\nCirculating water systems are normally of the wet pit type, but for sea water circulation, both the wet pit type and the concrete volute type are employed. In some industries, one or two stand-by pumps are also connected parallel to CW pumps. It is recommended that these pumps must be constantly driven by constant speed squirrel cage induction motors. CW pumps are designed as per IS:9137, standards of the Hydraulic Institute, USA or equivalent.\n\nIn the present era, mechanical induced draft–type cooling towers are employed in cooling of water. Performance testing of cooling towers (both IDCT and NDCT) shall be carried\nout as per ATC-105 at a time when the atmospheric conditions are within the permissible limits of deviation from the design conditions. As guidelines of Central Electricity Authority, two mechanical draft cooling towers Or one natural draft cooling tower must be established for each 500 MW unit in power plants. The cooling towers are designed as per Cooling Tower Institute codes.\n\nSome coastal power stations or chemical plants intake water from sea for condenser cooling. They either use closed cycle cooling by using cooling towers or once through cooling. Selection of type of system is based on the thermal pollution effect on sea water and techno-economics based on the distance of power station from the coast and cost of pumping sea water. Due to high salt concentration, it is necessary for circulating water make up.\n\nSource:\n\n\nSource:\n"}
{"id": "2782188", "url": "https://en.wikipedia.org/wiki?curid=2782188", "title": "Conditioners", "text": "Conditioners\n\nConditioners used on leather take many shapes and forms. They are used mostly to keep leather from drying out and deteriorating.\n\nTwo very old and widely used conditioners are dubbin and Leather Honey. Another common conditioner is Mink oil.\n"}
{"id": "165384", "url": "https://en.wikipedia.org/wiki?curid=165384", "title": "Curie temperature", "text": "Curie temperature\n\nIn physics and materials science, the Curie temperature (\"T\"), or Curie point, is the temperature above which certain materials lose their permanent magnetic properties, to be replaced by induced magnetism. The Curie temperature is named after Pierre Curie, who showed that magnetism was lost at a critical temperature.\n\nThe force of magnetism is determined by the magnetic moment, a dipole moment within an atom which originates from the angular momentum and spin of electrons. Materials have different structures of intrinsic magnetic moments that depend on temperature; the Curie temperature is the critical point at which a material's intrinsic magnetic moments change direction.\n\nPermanent magnetism is caused by the alignment of magnetic moments and induced magnetism is created when disordered magnetic moments are forced to align in an applied magnetic field. For example, the ordered magnetic moments (ferromagnetic, Figure 1) change and become disordered (paramagnetic, Figure 2) at the Curie temperature. Higher temperatures make magnets weaker, as spontaneous magnetism only occurs below the Curie temperature. Magnetic susceptibility above the Curie temperature can be calculated from the Curie–Weiss law, which is derived from Curie's law.\n\nIn analogy to ferromagnetic and paramagnetic materials, the Curie temperature can also be used to describe the phase transition between ferroelectricity and paraelectricity. In this context, the order parameter is the \"electric\" polarisation that goes from a finite value to zero when the temperature is increased above the Curie temperature.\n\nMagnetic moments are permanent dipole moments within an atom that comprise electron angular momentum and spin by the relation μ = el/2m, where m is the mass of an electron, μ is the magnetic moment, and l is the angular momentum; this ratio is called the gyromagnetic ratio.\n\nThe electrons in an atom contribute magnetic moments from their own angular momentum and from their orbital momentum around the nucleus. Magnetic moments from the nucleus are insignificant in contrast to the magnetic moments from the electrons. Thermal contributions result in higher energy electrons disrupting the order and the destruction of the alignment between dipoles. \n\nFerromagnetic, paramagnetic, ferrimagnetic and antiferromagnetic materials have different intrinsic magnetic moment structures. At a material's specific Curie temperature, these properties change. The transition from antiferromagnetic to paramagnetic (or vice versa) occurs at the Néel temperature, which is analogous to Curie temperature.\n\nFerromagnetic, paramagnetic, ferrimagnetic and antiferromagnetic structures are made up of intrinsic magnetic moments. If all the electrons within the structure are paired, these moments cancel out due to their opposite spins and angular momenta. Thus, even with an applied magnetic field, these material have different properties and no Curie temperature.\n\nA material is paramagnetic only above its Curie temperature. Paramagnetic materials are non-magnetic when a magnetic field is absent and magnetic when a magnetic field is applied. When a magnetic field is absent, the material has disordered magnetic moments; that is, the atoms are asymmetrical and not aligned. When a magnetic field is present, the magnetic moments are temporarily realigned parallel to the applied field; the atoms are symmetrical and aligned. The magnetic moments being aligned in the same direction are what causes an induced magnetic field.\n\nFor paramagnetism, this response to an applied magnetic field is positive and is known as magnetic susceptibility. The magnetic susceptibility only applies above the Curie temperature for disordered states.\n\nSources of paramagnetism (materials which have Curie temperatures) include:\n\nAbove the Curie temperature, the atoms are excited, and the spin orientations become randomized but can be realigned by an applied field, i.e., the material becomes paramagnetic. Below the Curie temperature, the intrinsic structure has undergone a phase transition, the atoms are ordered and the material is ferromagnetic. The paramagnetic materials' induced magnetic fields are very weak compared with ferromagnetic materials' magnetic fields.\n\nMaterials are only ferromagnetic below their corresponding Curie temperatures. Ferromagnetic materials are magnetic in the absence of an applied magnetic field.\n\nWhen a magnetic field is absent the material has spontaneous magnetization which is a result of the ordered magnetic moments; that is, for ferromagnetism, the atoms are symmetrical and aligned in the same direction creating a permanent magnetic field.\n\nThe magnetic interactions are held together by exchange interactions; otherwise thermal disorder would overcome the weak interactions of magnetic moments. The exchange interaction has a zero probability of parallel electrons occupying the same point in time, implying a preferred parallel alignment in the material. The Boltzmann factor contributes heavily as it prefers interacting particles to be aligned in the same direction. This causes ferromagnets to have strong magnetic fields and high Curie temperatures of around 1000 K.\n\nBelow the Curie temperature, the atoms are aligned and parallel, causing spontaneous magnetism; the material is ferromagnetic. Above the Curie temperature the material is paramagnetic, as the atoms lose their ordered magnetic moments when the material undergoes a phase transition.\n\nMaterials are only ferrimagnetic below their corresponding Curie temperature. Ferrimagnetic materials are magnetic in the absence of an applied magnetic field and are made up of two different ions.\n\nWhen a magnetic field is absent the material has a spontaneous magnetism which is the result of ordered magnetic moments; that is, for ferrimagnetism one ion's magnetic moments are aligned facing in one direction with certain magnitude and the other ion's magnetic moments are aligned facing in the opposite direction with a different magnitude. As the magnetic moments are of different magnitudes in opposite directions there is still a spontaneous magnetism and a magnetic field is present.\n\nSimilar to ferromagnetic materials the magnetic interactions are held together by exchange interactions. The orientations of moments however are anti-parallel which results in a net momentum by subtracting their momentum from one another.\n\nBelow the Curie temperature the atoms of each ion are aligned anti-parallel with different momentums causing a spontaneous magnetism; the material is ferrimagnetic. Above the Curie temperature the material is paramagnetic as the atoms lose their ordered magnetic moments as the material undergoes a phase transition.\n\nMaterials are only antiferromagetic below their corresponding Néel temperature. This is similar to the Curie temperature as above the Néel Temperature the material undergoes a phase transition and becomes paramagnetic.\n\nThe material has equal magnetic moments aligned in opposite directions resulting in a zero magnetic moment and a net magnetism of zero at all temperatures below the Néel temperature. Antiferromagnetic materials are weakly magnetic in the absence or presence of an applied magnetic field.\n\nSimilar to ferromagnetic materials the magnetic interactions are held together by exchange interactions preventing thermal disorder from overcoming the weak interactions of magnetic moments. When disorder occurs it is at the Néel temperature.\n\nThe Curie–Weiss law is an adapted version of Curie's law.\n\nThe Curie–Weiss law is a simple model derived from a mean-field approximation, this means it works well for the materials temperature, , much greater than their corresponding Curie temperature, , i.e. ; however fails to describe the magnetic susceptibility, , in the immediate vicinity of the Curie point because of local fluctuations between atoms.\n\nNeither Curie's law nor the Curie–Weiss law holds for .\n\nCurie's law for a paramagnetic material:\n\nThe Curie–Weiss law is then derived from Curie's law to be:\n\nwhere:\n\nFor full derivation see Curie–Weiss law.\n\nAs the Curie–Weiss law is an approximation, a more accurate model is needed when the temperature, , approaches the material's Curie temperature, .\n\nMagnetic susceptibility occurs above the Curie temperature.\n\nAn accurate model of critical behaviour for magnetic susceptibility with critical exponent :\n\nThe critical exponent differs between materials and for the mean-field model is taken as  = 1.\n\nAs temperature is inversely proportional to magnetic susceptibility, when approaches the denominator tends to zero and the magnetic susceptibility approaches infinity allowing magnetism to occur. This is a spontaneous magnetism which is a property of ferromagnetic and ferrimagnetic materials.\n\nMagnetism depends on temperature and spontaneous magnetism occurs below the Curie temperature. An accurate model of critical behaviour for spontaneous magnetism with critical exponent :\n\nThe critical exponent differs between materials and for the mean-field model as taken as  =  where .\n\nThe spontaneous magnetism approaches zero as the temperature increases towards the materials Curie temperature.\n\nThe spontaneous magnetism, occurring in ferromagnetic, ferrimagnetic and antiferromagnetic materials, approaches zero as the temperature increases towards the material's Curie temperature. Spontaneous magnetism is at its maximum as the temperature approaches 0 K. That is, the magnetic moments are completely aligned and at their strongest magnitude of magnetism due to no thermal disturbance.\n\nIn paramagnetic materials temperature is sufficient to overcome the ordered alignments. As the temperature approaches 0 K, the entropy decreases to zero, that is, the disorder decreases and becomes ordered. This occurs without the presence of an applied magnetic field and obeys the third law of thermodynamics.\n\nBoth Curie's law and the Curie–Weiss law fail as the temperature approaches 0 K. This is because they depend on the magnetic susceptibility which only applies when the state is disordered.\n\nGadolinium sulphate continues to satisfy Curie's law at 1 K. Between 0 and 1 K the law fails to hold and a sudden change in the intrinsic structure occurs at the Curie temperature.\n\nThe Ising model is mathematically based and can analyse the critical points of phase transitions in ferromagnetic order due to spins of electrons having magnitudes of ±. The spins interact with their neighbouring dipole electrons in the structure and here the Ising model can predict their behaviour with each other.\n\nThis model is important for solving and understanding the concepts of phase transitions and hence solving the Curie temperature. As a result, many different dependencies that affect the Curie temperature can be analysed.\n\nFor example, the surface and bulk properties depend on the alignment and magnitude of spins and the Ising model can determine the effects of magnetism in this system.\n\nMaterials structures consist of intrinsic magnetic moments which are separated into domains called Weiss domains. This can result in ferromagnetic materials having no spontaneous magnetism as domains could potentially balance each other out. The position of particles can therefore have different orientations around the surface than the main part (bulk) of the material. This property directly affects the Curie temperature as there can be a bulk Curie temperature and a different surface Curie temperature for a material.\n\nThis allows for the surface Curie temperature to be ferromagnetic above the bulk Curie temperature when the main state is disordered, i.e. Ordered and disordered states occur simultaneously.\n\nThe surface and bulk properties can be predicted by the Ising model and electron capture spectroscopy can be used to detect the electron spins and hence the magnetic moments on the surface of the material. An average total magnetism is taken from the bulk and surface temperatures to calculate the Curie temperature from the material, noting the bulk contributes more.\n\nThe angular momentum of an electron is either + or − due to it having a spin of , which gives a specific size of magnetic moment to the electron; the Bohr magneton. Electrons orbiting around the nucleus in a current loop create a magnetic field which depends on the Bohr Magneton and magnetic quantum number. Therefore, the magnetic moments are related between angular and orbital momentum and affect each other. Angular momentum contributes twice as much to magnetic moments than orbital.\n\nFor terbium which is a rare-earth metal and has a high orbital angular momentum the magnetic moment is strong enough to affect the order above its bulk temperatures. It is said to have a high anisotropy on the surface, that is it is highly directed in one orientation. It remains ferromagnetic on its surface above its Curie temperature while its bulk becomes ferrimagnetic and then at higher temperatures its surface remains ferrimagnetic above its bulk Néel Temperature before becoming completely disordered and paramagnetic with increasing temperature. The anisotropy in the bulk is different from its surface anisotropy just above these phase changes as the magnetic moments will be ordered differently or ordered in paramagnetic materials.\n\nComposite materials, that is, materials composed from other materials with different properties, can change the Curie temperature. For example, a composite which has silver in it can create spaces for oxygen molecules in bonding which decreases the Curie temperature as the crystal lattice will not be as compact.\n\nThe alignment of magnetic moments in the composite material affects the Curie temperature. If the materials moments are parallel with each other the Curie temperature will increase and if perpendicular the Curie temperature will decrease as either more or less thermal energy will be needed to destroy the alignments.\n\nPreparing composite materials through different temperatures can result in different final compositions which will have different Curie temperatures. Doping a material can also affect its Curie temperature.\n\nThe density of nanocomposite materials changes the Curie temperature. Nanocomposites are compact structures on a nano-scale. The structure is built up of high and low bulk Curie temperatures, however will only have one mean-field Curie temperature. A higher density of lower bulk temperatures results in a lower mean-field Curie temperature and a higher density of higher bulk temperature significantly increases the mean-field Curie temperature. In more than one dimension the Curie temperature begins to increase as the magnetic moments will need more thermal energy to overcome the ordered structure.\n\nThe size of particles in a material's crystal lattice changes the Curie temperature. Due to the small size of particles (nanoparticles) the fluctuations of electron spins become more prominent, this results in the Curie temperature drastically decreasing when the size of particles decrease as the fluctuations cause disorder. The size of a particle also affects the anisotropy causing alignment to become less stable and thus lead to disorder in magnetic moments.\n\nThe extreme of this is superparamagnetism which only occurs in small ferromagnetic particles and is where fluctuations are very influential causing magnetic moments to change direction randomly and thus create disorder.\n\nThe Curie temperature of nanoparticles are also affected by the crystal lattice structure, body-centred cubic (bcc), face-centred cubic (fcc) and a hexagonal structure (hcp) all have different Curie temperatures due to magnetic moments reacting to their neighbouring electron spins. fcc and hcp have tighter structures and as a results have higher Curie temperatures than bcc as the magnetic moments have stronger effects when closer together. This is known as the coordination number which is the number of nearest neighbouring particles in a structure. This indicates a lower coordination number at the surface of a material than the bulk which leads to the surface becoming less significant when the temperature is approaching the Curie temperature. In smaller systems the coordination number for the surface is more significant and the magnetic moments have a stronger affect on the system.\n\nAlthough fluctuations in particles can be minuscule, they are heavily dependent on the structure of crystal lattices as they react with their nearest neighbouring particles. Fluctuations are also affected by the exchange interaction as parallel facing magnetic moments are favoured and therefore have less disturbance and disorder, therefore a tighter structure influences a stronger magnetism and therefore a higher Curie temperature.\n\nPressure changes a material's Curie temperature. Increasing pressure on the crystal lattice decreases the volume of the system. Pressure directly affects the kinetic energy in particles as movement increases causing the vibrations to disrupt the order of magnetic moments. This is similar to temperature as it also increases the kinetic energy of particles and destroys the order of magnetic moments and magnetism.\n\nPressure also affects the density of states (DOS). Here the DOS decreases causing the number of electrons available to the system to decrease. This leads to the number of magnetic moments decreasing as they depend on electron spins. It would be expected because of this that the Curie temperature would decrease however it increases. This is the result of the exchange interaction. The exchange interaction favours the aligned parallel magnetic moments due to electrons being unable to occupy the same space in time and as this is increased due to the volume decreasing the Curie temperature increases with pressure. The Curie temperature is made up of a combination of dependencies on kinetic energy and the DOS.\n\nThe concentration of particles also affects the Curie temperature when pressure is being applied and can result in a decrease in Curie temperature when the concentration is above a certain percent.\n\nOrbital ordering changes the Curie temperature of a material. Orbital ordering can be controlled through applied strains. This is a function that determines the wave of a single electron or paired electrons inside the material. Having control over the probability of where the electron will be allows the Curie temperature to be altered. For example, the delocalised electrons can be moved onto the same plane by applied strains within the crystal lattice.\n\nThe Curie temperature is seen to increase greatly due to electrons being packed together in the same plane, they are forced to align due to the exchange interaction and thus increases the strength of the magnetic moments which prevents thermal disorder at lower temperatures.\n\nIn analogy to ferromagnetic and paramagnetic materials, the term Curie temperature () is also applied to the temperature at which a ferroelectric material transitions to being paraelectric. Hence, is the temperature where ferroelectric materials lose their spontaneous polarisation as a first or second order phase change occurs. In case of a second order transition the Curie Weiss temperature which defines the maximum of the dielectric constant is equal to the Curie temperature. However, the Curie temperature can be 10 K higher than in case of a first order transition.\n\nMaterials are only ferroelectric below their corresponding transition temperature . Ferroelectric materials are all pyroelectric and therefore have a spontaneous electric polarisation as the structures are unsymmetrical.\n\nFerroelectric materials' polarization is subject to hysteresis (Figure 4); that is they are dependent on their past state as well as their current state. As an electric field is applied the dipoles are forced to align and polarisation is created, when the electric field is removed polarisation remains. The hysteresis loop depends on temperature and as a result as the temperature is increased and reaches the two curves become one curve as shown in the dielectric polarisation (Figure 5).\n\nA modified version of the Curie–Weiss law applies to the dielectric constant, also known as the relative permittivity:\n\nA heat-induced ferromagnetic-paramagnetic transition is used in magneto-optical storage media, for erasing and writing of new data. Famous examples include the Sony Minidisc format, as well as the now-obsolete CD-MO format. Curie point electro-magnets have been proposed and tested for actuation mechanisms in passive safety systems of fast breeder reactors, where control rods are dropped into the reactor core if the actuation mechanism heats up beyond the material's curie point. Other uses include temperature control in soldering irons, and stabilizing the magnetic field of tachometer generators against temperature variation.\n\n\n"}
{"id": "46790", "url": "https://en.wikipedia.org/wiki?curid=46790", "title": "Desert varnish", "text": "Desert varnish\n\nDesert varnish or rock varnish is an orange-yellow to black coating found on exposed rock surfaces in arid environments. Desert varnish is approximately one micrometer thick and exhibits nanometer-scale layering. Rock rust and desert patina are other terms which are also used for the condition, but less often.\n\nDesert varnish forms only on physically stable rock surfaces that are no longer subject to frequent precipitation, fracturing or wind abrasion. The varnish is primarily composed of particles of clay along with iron and manganese oxides. There is also a host of trace elements and almost always some organic matter. The color of the varnish varies from shades of brown to black.\n\nIt has been suggested that desert varnish should be investigated as a potential candidate for a \"shadow biosphere\".\n\nOriginally scientists thought that the varnish was made from substances drawn out of the rocks it coats. Microscopic and microchemical observations, however, show that a major part of varnish is clay, which could only arrive by wind. Clay, then, acts as a to catch additional substances that chemically react together when the rock reaches high temperatures in the desert sun. Wetting by dew is also important in the process.\n\nAn important characteristic of black desert varnish is that it has an unusually high concentration of manganese. Manganese is relatively rare in the Earth's crust, making up only 0.12% of its weight. In black desert varnish, however, manganese is 50 to 60 times more abundant. One proposal for a mechanism of desert varnish formation is that it is caused by manganese-oxidizing microbes (mixotrophs) which are common in environments poor in organic nutrients. A micro-environment pH above 7.5 is inhospitable for manganese-concentrating microbes. In such conditions, orange varnishes develop, poor in manganese (Mn) but rich in iron (Fe). An alternative hypothesis for Mn/Fe fluctuation has been proposed that considers Mn-rich and Fe-rich varnishes to be related to humid and arid climates, respectively \n\nEven though it contains high concentrations of iron and manganese, there are no significant modern uses of desert varnish. However, some Native American peoples created petroglyphs by scraping or chipping away the dark varnish to expose the lighter rock beneath.\n\nDesert varnish often obscures the identity of the underlying rock, and different rocks have varying abilities to accept and retain varnish. Limestones, for example, typically do not have varnish because they are too water-soluble and therefore do not provide a stable surface for varnish to form. Shiny, dense and black varnishes form on basalt, fine quartzites and metamorphosed shales due to these rocks' relatively high resistance to weathering.\n\n\n"}
{"id": "3233763", "url": "https://en.wikipedia.org/wiki?curid=3233763", "title": "Dusty plasma", "text": "Dusty plasma\n\nA dusty plasma is a plasma containing millimeter (10) to nanometer (10) sized particles suspended in it. Dust particles are charged and the plasma and particles behave as a plasma. Dust particles may form larger particles resulting in \"grain plasmas\". Due to the additional complexity of studying plasmas with charged dust particles, dusty plasmas are also known as complex plasmas.\n\nDusty plasmas are encountered in:\n\n\nDusty plasmas are interesting because the presence of particles significantly alters the charged particle equilibrium leading to different phenomena. It is a field of current research. Electrostatic coupling between the grains can vary over a wide range so that the states of the dusty plasma can change from weakly coupled (gaseous) to crystalline. Such plasmas are of interest as a non-Hamiltonian system of interacting particles and as a means to study generic fundamental physics of self-organization, pattern formation, phase transitions, and scaling.\n\nThe temperature of dust in a plasma may be quite different from its environment. For example:\nThe electric potential of dust particles is typically 1–10 V (positive or negative). The potential is usually negative because the electrons are more mobile than the ions. The physics is essentially that of a Langmuir probe that draws no net current, including formation of a Debye sheath with a thickness of a few times the Debye length. If the electrons charging the dust grains are relativistic, then the dust may charge to several kilovolts. Field electron emission, which tends to reduce the negative potential, can be important due to the small size of the particles. The photoelectric effect and the impact of positive ions may actually result in a positive potential of the dust particles.\n\nInterest in the dynamics of charged dust in plasmas was amplified by the detection of spokes in the rings of Saturn. The motion of solid particles in a plasma follows the following equation:\nwhere terms are for the Lorentz force, the gravitational forces, forces due to radiation pressure, the drag forces and the thermophoretic force respectively.\n\nThe Lorentz force, the contributions from the electric and magnetic force, is given by:\nwhere E is the electric field, v is the velocity and B is the magnetic field. \n\nformula_3 is the sum of all gravitational forces acting on the dust particle, whether it be from planets, satellites or other particles and formula_4 is the force contribution from radiation pressure. This is given as:\nThe direction of the force vector, formula_6 is that of the incident radiation of photon flux formula_7. The radius of the dust particle is formula_8.\n\nFor the drag force there are two major components of interest, those from positive ions-dust particle interactions, and neutral-dust particle interactions. Ion-dust interactions are further divided into three different interactions, through regular collisions, through Debye sheath modifications, and through coulomb collisions.\n\nThe thermophoretic force is the force that arises from the net temperature gradient that may be present in a plasma, and the subsequent pressure imbalance; causing more net momentum to be imparted from collisions from a specific direction.\n\nThen depending in the size of the particle, there are four categories:\n\nDusty plasmas are often studied in laboratory setups. The dust particles can be grown inside the plasma, or microparticles can be inserted. Usually, a low temperature plasma with a low degree of ionization is used. The microparticles then become the dominant component regarding the energy and momentum transport, and they can essentially be regarded as single-species system. This system can exist in all three classical phases, solid, liquid and gaseous, and can be used to study effects such as crystallization, wave and shock propagation, defect propagation, etc.\n\nWhen particles of micrometer-size are used, it is possible to observe the individual particles. Their movement is slow enough to be able to be observed with ordinary cameras, and the kinetics of the system can be studied. However, for micrometer-sized particles, gravity is a dominant force that disturbs the system. Thus, experiments are sometimes performed under microgravity conditions during parabolic flights or on board a space station.\n\nPadma Kant Shukla—coauthor of Introduction to Dusty Plasma Physics\n\n\n"}
{"id": "11259750", "url": "https://en.wikipedia.org/wiki?curid=11259750", "title": "EBOR", "text": "EBOR\n\nThe Experimental Beryllium Oxide Reactor (EBOR) was a 10MWt helium cooled beryllium moderated nuclear reactor at Idaho National Laboratory. It never achieved criticality. The project started on February 17, 1958 as the Maritime Gas-Cooled Reactor. The project started with a contract between the U.S. Atomic Energy Commission and General Dynamics. The Goal of the project was to create a small nuclear reactor for use in merchant shipping or in a medium sized power plant. The main goals for the reactor were a simple design, low maintenance costs and maximum efficiency over a wide range of power settings. In December 1960 the project was authorized to construct a 10-Mw test reactor to determine the characteristics of the Beryllium Oxide gas cooled system. The EBOR was designed to test the basic fuel element and moderator designs for the final reactor. The EBOR used a Helium cooling system and was an intermediate step towards a prototype power plant. The plan was to use a closed cycle turbine or a steam cycle with the reactor to make a small land based or maritime power plant. This plan was abandoned as the reactor never achieved criticality.\n\n\n"}
{"id": "21873573", "url": "https://en.wikipedia.org/wiki?curid=21873573", "title": "Freddy Fox", "text": "Freddy Fox\n\nFreddy Fox () is a hardcover picture book written and illustrated by Ronald J. Meyer. It tells the story of a young fox and his family, following the daily activities of the fox kit, Freddy. Freddy Fox teaches the reader about red foxes, including that they are born shades of gray, how their den is built, what they eat, and family size. Then through a simple segue, Freddy's red fox mother teaches the young fox about other animals and traits attributed to them. By learning his lessons, Freddy will grow up to be a wise fox. All of the lessons, such as the bear eats a balanced diet or the raccoon washes his hands before eating, can easily be translated into lessons for young children.\n\nThe book was first published in 2004. It won the Silver Mom's Choice Award in the \"Animal Kingdom\" category in 2008.\n\nThe author is a wildlife photographer, and he used his wildlife photographs to illustrate the book. \n\n"}
{"id": "53320264", "url": "https://en.wikipedia.org/wiki?curid=53320264", "title": "Garden sanctuary", "text": "Garden sanctuary\n\nGarden Sanctuary is a concept that follows on from the popular understanding of a therapeutic \"Secret Garden\" first imagined by Frances Hodgson Burnett.\n\n\nA Secret Garden, or a Garden Sanctuary, may be:\n\n"}
{"id": "15403146", "url": "https://en.wikipedia.org/wiki?curid=15403146", "title": "Gaspar Makale", "text": "Gaspar Makale\n\nGaspar Makale (1960 circa. to December 2007, Tanzania) was one of the pioneers of solar electrification in the African Great Lakes. During the 1990s, he was the Chief Solar Technician at the KARADEA Solar Training Facility (KSTF) in Karagwe district, Kagera region in Northern Tanzania, situated between Lake Victoria and Rwanda.\n\nKSTF, the first dedicated training centre for solar energy technology in the African Great Lakes, was founded by Oswald Kasazi in 1993. During the following decade KSTF gave regular three-week-long training courses which were attended by people from all over the African Great Lakes region (Tanzania, Kenya, Uganda), as well as from further afield. The courses were held in partnership with Alternative Energy Africa (EAA), run by Mark Hankins and Daniel Kithokoi, who were based in Nairobi, Kenya. Gaspar Makale managed the practical sessions as well as arranging for the field-trips during which course participants installed solar electric domestic systems in the Karagwe district.\n\nKSTF also ran a solar apprentice scheme for which Gaspar Makale was responsible. He was also involved in other solar training courses in Tanzania, such as the one held at Wasso Hospital, situated in Maasailand, Tanzania. Course participants, many of whom later went on to set up solar businesses and work in the growing African Great Lakes solar industry, got their first hands-on experience of installing solar electric systems under Gaspar Makale's experienced and expert guidance. He installed an Ampair Hawk 100 wind turbine at KSTF for charging batteries, the first wind turbine installed in that part of Tanzania. He also ran his own solar business.\n\nWhile working with KSTF, Gaspar Makale installed numerous solar systems in local schools, hospitals, clinic refrigeration systems, two-way radio systems, domestic lighting systems. He also installed systems in the refugee camps that sprung up in Karagwe after the Rwanda genocide in 1994. He made one trip to the USA where he attended solar and wind energy courses at Solar Energy International in Colorado, USA. He also ran a solar-powered disco in his own village where he lived with his family on a small farm.\n\nGaspar Makale also worked closely with Harold Burris of Solar Shamba, one of the very first people to see the potential of solar electricity in the African Great Lakes, and with Frank Jackson from APSO (the Irish state overseas development agency), who worked at KSTF during the 1990s.\n\n"}
{"id": "25072414", "url": "https://en.wikipedia.org/wiki?curid=25072414", "title": "Geography of Tristan da Cunha", "text": "Geography of Tristan da Cunha\n\nTristan da Cunha is an archipelago of five islands in the southern Atlantic Ocean, the largest of which is the island of Tristan da Cunha itself and the second-largest the remote bird haven Gough Island. It forms part of a wider territory called Saint Helena, Ascension and Tristan da Cunha which includes Saint Helena and Ascension Island.\n\nThis archipelago, 1500 miles (2500 km) from the continents of Africa and South America, is one of the most remote inhabited places on earth. It consists of the following islands:\n\nThe main island is quite mountainous; the only flat area is the location of the capital, Edinburgh of the Seven Seas, on the northwest coast. The highest point is a volcano called Queen Mary's Peak ; it is covered by snow in winter and is listed as an ultra prominent peak. Tristan da Cunha is thought to have been formed by a long-lived centre of upwelling magma called the Tristan hotspot.\n\nThe climate is marine cool-temperate with small temperature differences between summer and winter (11.3° - 14.5°) and between day and night. Sandy Point on the east coast is reputed to be the warmest and driest place on the island, being in the lee of the prevailing winds.\n\nEven the smaller islands have some plant cover, with the larger ones dominated by ferns and moss. Flora on the archiplego includes many endemic species and many that have a broad circumpolar distribution in the South Atlantic and South Pacific Oceans. Thus many of the species that occur in Tristan da Cunha occur as far away as New Zealand. For example, the species \"Nertera depressa\" was first collected in Tristan da Cunha, but has since been recorded in occurrence as far distant as New Zealand.\n\nTristan da Cunha is home to ocean-going species including subantarctic fur seal, the southern elephant seal and birds such as northern rockhopper penguins and macaroni penguins. The islands are important for their bird life both those established on the islands and breeding seabirds, of which twenty species nest on Gough Island alone. Important species include Tristan albatross, Tristan thrush, Tristan bunting, Gough bunting, Gough moorhen, Atlantic petrel, and the Inaccessible Island rail. There are no native reptiles, amphibians, freshwater fish, or land mammals.\n\nApart from Tristan da Cunha, which was settled as a base for whaling and sealing in the 18th century, the islands of the group are uninhabited except for a weather station on Gough Island belonging to South Africa. Fishing is still an important economic activity especially for crayfish and octopus but also the Tristan rock lobster \"(Jasus tristani)\". Gough Island has also been used as base for whaling and sealing but only ever temporarily. The islands do receive a small number of tourists.\n\nSheep and cattle have been introduced on Tristan da Cunha and their grazing, along with other human activity has caused damage to the island's ecosystems. Night fishing has caused the deaths of many seabirds as they crash into ships' lights.\n\n"}
{"id": "4606244", "url": "https://en.wikipedia.org/wiki?curid=4606244", "title": "Halonium ion", "text": "Halonium ion\n\nA halonium ion in organic chemistry is any onium compound (ion) containing a halogen atom carrying a positive charge. This cation has the general structure R––R where X is any halogen and R any organic residue and this structure can be cyclic or an open chain molecular structure. Halonium ions formed from fluorine, chlorine, bromine, and iodine are called fluoronium, chloronium, bromonium, and iodonium, respectively. The cyclic variety commonly proposed as intermediates in electrophilic halogenation may be called haliranium ions, using the Hantzch-Widman nomenclature system. \n\nThe simplest halonium ions are of the structure H––H (X = F, Cl, Br, I). Halonium ions often have a three-atom cyclic structure, similar to that of an epoxide, resulting from the formal addition of a halogenium ion X to a C=C double bond, as when a halogen is added to an alkene.\n\nThe propensity to form bridging halonium ions is in the order I > Br > Cl > F. Whereas iodine and bromine readily form bridged iodonium and bromonium ions, fluoronium ions have only recently been characterized in designed systems that force close encounter of the fluorine lone pair and a carbocationic center. In practice, structurally, there is a continuum between a symmetrically bridged halonium, to an unsymmetrical halonium with a long weak bond to one of the carbon centers, to a true β-halocarbocation with no halonium character. The equilibrium structure depends on the ability of the carbon atoms and the halogen to accommodate positive charge. Thus, a bromonium ion that bridges a primary and tertiary carbon will often exhibit a skewed structure, with a weak bond to the tertiary center (with significant carbocation character) and stronger bond to the primary carbon. This is due to the increased stability of tertiary carbons to stabilize positive charge. In the more extreme case, if the tertiary center is doubly benzylic for instance, then the open form may be favored. Similarly, switching from bromine to chlorine also weakens bridging character, due to the higher electronegativity of chlorine and lower propensity to share electron density compared to bromine. \n\nThese ions are usually only short-lived reaction intermediates; they are very reactive, owing to high ring strain in the three-membered ring and the positive charge on the halogen; this positive charge makes them great electrophiles. In almost all cases, the halonium ion is attacked by a nucleophile within a very short time. Even a weak nucleophile, such as water will attack the halonium ion; this is how halohydrins can be made.\n\nOn occasion, a halonium atom will rearrange to a carbocation. This usually occurs only when that carbocation is an allylic or a benzylic carbocation.\n\nHalonium ions were first postulated in 1937 by Roberts and Kimball to account for observed \"anti\" diastereoselectivity in halogen addition reactions to alkenes. They correctly argued that if the initial reaction intermediate in bromination is the open-chain X–C–C, rotation around the C–C single bond would be possible leading to a mixture of equal amounts of dihalogen \"syn\" isomer and \"anti\" isomer, which is not the case. They also asserted that a positively charged halogen atom is isoelectronic with oxygen and that carbon and bromine have comparable ionization potentials. For certain aryl substituted alkenes, the \"anti\" stereospecificity is diminished or lost, as a result of weakened or absent halonium character in the cationic intermediate.\n\nIn 1970 George A. Olah succeeded in preparing and isolating halonium salts by adding a methyl halide such as methyl bromide or methyl chloride in sulfur dioxide at −78 °C to a complex of antimony pentafluoride and tetrafluoromethane in sulfur dioxide. After evaporation of sulfur dioxide this procedure left crystals of [HC–X–CH][SbF], stable at room temperature but not to moisture. A fluoronium ion was recently characterized in solution phase (dissolved in sulfur dioxide or sulfuryl chloride fluoride) at low temperature.\n\nCyclic and acyclic chloronium, bromonium and iodonium ions have been structurally characterised by X-ray crystallography, such as the bi(adamantylidene)-derived bromonium cation shown below.\n\nCompounds containing trivalent or tetravalent halonium ions do not exist but for some hypothetical compounds stability has been computationally tested.\n"}
{"id": "5571591", "url": "https://en.wikipedia.org/wiki?curid=5571591", "title": "Hertzsprung gap", "text": "Hertzsprung gap\n\nThe Hertzsprung Gap is a feature of the Hertzsprung–Russell diagram for a star cluster. It is named after Ejnar Hertzsprung, who first noticed the absence of stars in the region of the Hertzsprung–Russell diagram between A5 and G0 spectral type and between +1 and −3 absolute magnitudes (i.e. between the top of the main sequence and the red giants for stars above roughly 1.5 solar mass. When a star during its evolution crosses the Hertzsprung gap, it means that it has finished core hydrogen burning, but has yet to start hydrogen shell burning.\n\nStars do exist in the Hertzsprung gap region, but because they move through this section of the Hertzsprung–Russell diagram very quickly in comparison to the lifetime of the star (thousands of years, compared to tens of billions of years for the lifetime of the star), that portion of the diagram is less densely populated. Full Hertzsprung–Russell diagrams of the 11,000 Hipparcos mission targets show a handful of stars in that region.\n"}
{"id": "33691271", "url": "https://en.wikipedia.org/wiki?curid=33691271", "title": "Hobby horse (toy)", "text": "Hobby horse (toy)\n\nA hobby horse (or hobby-horse) is a child's toy horse, particularly popular during the days before cars. Children played at riding a wooden hobby horse made of a straight stick with a small horse's head (of wood or stuffed fabric), and perhaps reins, attached to one end. The bottom end of the stick sometimes had a small wheel or wheels attached. This toy was also sometimes known as a cock horse (as in the nursery rhyme \"Ride a cock horse to Banbury Cross\") or stick horse.\n\nHobby horses feature in the worship of Rajasthani folk deity Baba Ramdevji, a reference to a story about his childhood; wooden toy horses are popular offerings at his temple at Ramdevra. They also figured in the public rites of the Romanian Călușari.\n\nA hobby horse is not always a riding-stick like the child's toy; larger hobby horses feature in some traditional seasonal customs (such as Mummers Plays and the Morris dance in England). They vary in size from a costume for one person to large frameworks carried by nine people.\nFrom \"hobby horse\" (see Etymology, below) came the expression \"to ride one's hobby-horse\", meaning \"to follow a favourite pastime\", and in turn, the modern sense of the term \"hobby\".\n\nThe term is also connected to the \"draisine\", a forerunner of the bicycle, invented by Baron Karl von Drais. In 1818, a London coach-maker named Denis Johnson began producing an improved version, which was popularly known as the \"hobby-horse\".\n\nThe artistic movement, Dada, is possibly named after a French child's word for hobby-horse.\n\nThe word \"hobby\" is glossed by the OED as \"a small or middle-sized horse; an ambling or pacing horse; a pony.\" The word is attested in English from the 14th century, as Middle English \"hobyn\". Old French had \"hobin\" or \"haubby\", whence Modern French \"aubin\" and Italian \"ubino\". But the Old French term is apparently adopted from English rather than vice versa. OED connects it to \"the by-name \"Hobin\", \"Hobby\"\", a variant of \"Robin\"\" (compare the abbreviation \"Hob\" for \"Robert\"). This appears to have been a name customarily given to a cart-horse, as attested by White Kennett in his \"Parochial Antiquities \" (1695), who stated that \"Our ploughmen to some one of their cart-horses generally give the name of Hobin, the very word which Phil. Comines uses, Hist. VI. vii.\" Another familiar form of the same Christian name, \"Dobbin\" has also become a generic name for a cart-horse.\n\nSamuel Johnson, \"Dictionary of the English Language\", 1755, glosses \"A strong, active horse, of a middle size, said to have been originally from Ireland; an ambling nag.\"\n\nHoblers or Hovellers were men who kept a light nag that they may give instant information of threatened invasion. (Old French, hober, to move up and down; our hobby, q.v.) In mediæval times their duties were to reconnoitre, to carry intelligence, to harass stragglers, to act as spies, to intercept convoys, and to pursue fugitives. Henry Spelman (d. 1641) derived the word from \"hobby\".\n\nThe Border horses, called hobblers or hobbies, were small and active, and trained to cross the most difficult and boggy country, \"and to get over where our footmen could scarce dare to follow.\" - George MacDonald Fraser, \"The Steel Bonnets, The Story of the Anglo-Scottish Border Reivers\".\n\nHobby Horse Polo uses parts of the polo rules but has its own specialities, as e.g. 'punitive sherries' and is using hobby horses instead of ponies. The Hobby Horse variant started 1998 as a fun sport in south western Germany and lead 2002 to the foundation of the First Kurfürstlich-Kurpfälzisch Polo-Club in Mannheim. In the meantime it gained further interest in other German cities.\n\nNowadays hobby horses are not only ridden by children, but also teenagers and adults. Hobby horses have become a popular hobby in Northern Europe, especially in Finland. Horses used are not mass produced toy horses, but fine, realistic pieces of art usually done completely by hand. Hobbyists give each of their horses names, breeds, genders and personalities. They also compete with hobby horses in same disciplines as real horses, the most popular disciplines being dressage and show jumping.\n\nThe hobby horse has been used numerous times in classic cartoons and in some new ones incorporating a Wild West theme.\n\n\n"}
{"id": "4483084", "url": "https://en.wikipedia.org/wiki?curid=4483084", "title": "IEEE Congress on Evolutionary Computation", "text": "IEEE Congress on Evolutionary Computation\n\nThe IEEE Congress on Evolutionary Computation (CEC) is one of the largest and most important conferences within Evolutionary computation (EC), the other conferences of similar importance being Genetic and Evolutionary Computation Conference (GECCO), Parallel Problem Solving from Nature (PPSN) and European Conference on Genetic Programming (EuroGP).\n\nCEC, which is organized by the IEEE Computational Intelligence Society in cooperation with the Evolutionary Programming Society, covers most subtopics of EC, such as Evolutionary robotics, Multiobjective optimization, Evolvable hardware, Theory of evolutionary computation, Evolutionary design etc. Papers can also be found that deal with topics which are related to rather than part of EC, such Ant colony optimization, Swarm intelligence and Quantum computing.\n\nThe conference usually attracts several hundreds of attendees, as well as hundreds of papers.\n\n"}
{"id": "48599524", "url": "https://en.wikipedia.org/wiki?curid=48599524", "title": "John F. Brady (chemical engineer)", "text": "John F. Brady (chemical engineer)\n\nJohn Francis Brady (born January 8, 1954) is the Chevron Professor of Chemical Engineering and Executive Officer of Chemical Engineering at the California Institute of Technology. He is a fluid mechanician and creator of the Stokesian dynamics method for simulating suspensions of spheres and ellipsoids in low Reynolds number flows.\n\nBrady was educated in chemical engineering at the University of Pennsylvania (B.S. 1975), the University of Cambridge, England (Certificate of Postgraduate Study, 1976), and Stanford University (M.S. 1977 and Ph.D. 1981). He completed his dissertation entitled Inertial effects in closed cavity flows and their influence in drop breakup advised by Professor Andreas Acrivos. Following his Ph.D, Brady was a NATO post-doctoral fellow at the Ecole Superiéure de Physique et de Chimie Industrielles, Paris, France (1980–81).\nFollowing his research in France, Brady joined the faculty in chemical engineering at the Massachusetts Institute of Technology as an assistant professor in 1981. He moved in 1985 to the Division of Chemistry and Chemical Engineering at the California Institute of Technology, which has been his academic home since. \nBrady is an expert in fluid mechanics, rheology and transport phenomena, particularly for suspensions and complex fluids. Among his many accomplishments is the creation of Stokesian dynamics \nwith Georges Bossis. The Stokesian dynamics method allows the accurate and rapid simulation of the dynamics and rheology of suspensions of spherical particles at low Reynolds number.\nThe technique has been used by researchers world-wide to model suspensions and understand a variety of physical systems. Brady was an associate editor of the \"Journal of Fluid Mechanics\" (1990-2004) and the editor of the \"Journal of Rheology\" (2005-2012).\n\nHe has received numerous awards and honors which include:\n\n"}
{"id": "1837480", "url": "https://en.wikipedia.org/wiki?curid=1837480", "title": "Knudsen gas", "text": "Knudsen gas\n\nA Knudsen gas is a gas in a state of such low density that the mean free path of molecules is greater than the diameter of receptacle that contains it. The molecular dynamical regime is then dominated by the collisions of the gas molecules with the walls of the receptacle rather than with each other. \n\nA flow of gas through a pipe is called Knudsen flow when it can be expressed as the difference of two independent flows, which enter the pipe through its different ends.\n\n\n"}
{"id": "35352555", "url": "https://en.wikipedia.org/wiki?curid=35352555", "title": "Lauingen Energy Park", "text": "Lauingen Energy Park\n\nThe Lauingen Energy Park is a 25.7-megawatt (MW) photovoltaic power station, located in Bavarian Swabia, Germany. It covers an area of and was commissioned in June 2010.\n\nThe project was built in three phases:\n\nThe largest solar power station in Swabia was built by the German company Gehrlicher Solar and features the following key figures:\n\n"}
{"id": "600122", "url": "https://en.wikipedia.org/wiki?curid=600122", "title": "Leaf spring", "text": "Leaf spring\n\nA leaf spring is a simple form of spring commonly used for the suspension in wheeled vehicles. Originally called a \"laminated\" or \"carriage spring\", and sometimes referred to as a semi-elliptical spring or cart spring, it is one of the oldest forms of springing, appearing on carriages in England after 1750 and from there migrating to France and Germany. \n\nA leaf spring takes the form of a slender arc-shaped length of spring steel of rectangular cross-section. In the most common configuration, the center of the arc provides location for the axle, while loops formed at either end provide for attaching to the vehicle chassis. For very heavy vehicles, a leaf spring can be made from several leaves stacked on top of each other in several layers, often with progressively shorter leaves. Leaf springs can serve locating and to some extent damping as well as springing functions. While the interleaf friction provides a damping action, it is not well controlled and results in stiction in the motion of the suspension. For this reason, some manufacturers have used mono-leaf springs.\n\nA leaf spring can either be attached directly to the frame at both ends or attached directly at one end, usually the front, with the other end attached through a shackle, a short swinging arm. The shackle takes up the tendency of the leaf spring to elongate when compressed and thus makes for softer springiness. Some springs terminated in a concave end, called a \"spoon end\" (seldom used now), to carry a swiveling member.\n\nThe leaf spring has seen a modern development in cars. The new Volvo XC90 (from 2016 year model and forward) has a transverse leaf spring in high tech composite materials, a solution that is similar to the latest Chevrolet Corvette. This means a straight leaf spring, that is tightly secured to the chassis, and the ends of the spring bolted to the wheel suspension, to allow the spring to work independently on each wheel. This means the suspension is smaller, flatter and lighter than a traditional setup.\n\nThere are a variety of leaf springs, usually employing the word \"elliptical\". \"Elliptical\" or \"full elliptical\" leaf springs referred to two circular arcs linked at their tips. This was joined to the frame at the top center of the upper arc, the bottom center was joined to the \"live\" suspension components, such as a solid front axle. Additional suspension components, such as trailing arms, would usually be needed for this design, but not for \"semi-elliptical\" leaf springs as used in the Hotchkiss drive. That employed the lower arc, hence its name. \"Quarter-elliptic\" springs often had the thickest part of the stack of leaves stuck into the rear end of the side pieces of a short ladder frame, with the free end attached to the differential, as in the Austin Seven of the 1920s. As an example of non-elliptic leaf springs, the Ford Model T had multiple leaf springs over its differential that were curved in the shape of a yoke. As a substitute for dampers (shock absorbers), some manufacturers laid non-metallic sheets in between the metal leaves, such as wood.\n\nLeaf springs were very common on automobiles, right up to the 1970s in Europe and Japan and late 1970s in America when the move to front-wheel drive, and more sophisticated suspension designs saw automobile manufacturers use coil springs instead. Today leaf springs are still used in heavy commercial vehicles such as vans and trucks, SUVs, and railway carriages. For heavy vehicles, they have the advantage of spreading the load more widely over the vehicle's chassis, whereas coil springs transfer it to a single point. Unlike coil springs, leaf springs also locate the rear axle, eliminating the need for trailing arms and a Panhard rod, thereby saving cost and weight in a simple live axle rear suspension. A further advantage of a leaf spring over a helical spring is that the end of the leaf spring may be guided along a definite path.\n\nA more modern implementation is the parabolic leaf spring. This design is characterized by fewer leaves whose thickness varies from centre to ends following a parabolic curve. In this design, inter-leaf friction is unwanted, and therefore there is only contact between the springs at the ends and at the centre where the axle is connected. Spacers prevent contact at other points. Aside from a weight saving, the main advantage of parabolic springs is their greater flexibility, which translates into vehicle ride quality that approaches that of coil springs. There is a trade-off in the form of reduced load carrying capability, however. The characteristic of parabolic springs is better riding comfort and not as \"stiff\" as conventional \"multi-leaf springs\". It is widely used on buses for better comfort. A further development by the British GKN company and by Chevrolet with the Corvette among others, is the move to composite plastic leaf springs. Nevertheless, due to missing inter-leaf friction and internal dampening effects, this type of spring requires more powerful dampers or shock absorbers.\n\nTypically when used in automobile suspension the leaf both supports an axle and locates/ partially locates the axle. This can lead to handling issues (such as 'axle tramp'), as the flexible nature of the spring makes precise control of the unsprung mass of the axle difficult. Some suspension designs use a Watts link (or a Panhard rod) and radius arms to locate the axle and do not have this drawback. Such designs can use softer springs, resulting in better ride. The various Austin-Healey 3000's and Fiat 128's rear suspension are examples.\n\nThe leaf spring acts as a linkage for holding the axle in position and thus separate linkages are not necessary.\nIt makes the construction of the suspension simple and strong.\n\nBecause the positioning of the axle is carried out by the leaf springs, it is disadvantageous to use soft springs\ni.e. springs with low spring constant.\n\nTherefore, this type of suspension does not provide good riding comfort.\nThe inter-leaf friction between the leaf springs affects the riding comfort.\n\nAcceleration and braking torque cause wind-up and vibration. Also wind-up causes rear-end squat and nose-diving.\n\nThe inter-leaf friction damps the spring's motion and reduces rebound, which until shock absorbers were widely adopted was a great advantage over helical springs.\n\nMulti-leaf springs are made as follows:\n\n\nBecause leaf springs are made of relatively high quality steel, they are a favorite material for blacksmiths. In countries such as India, Nepal, Bangladesh, the Philippines and Pakistan, where traditional blacksmiths still produce a large amount of the country's tools, leaf springs from scrapped cars are frequently used to make knives, kukris, and other tools. They are also commonly used by amateur and hobbyist blacksmiths.\n\nLeaf springs have also replaced traditional coil springs in some trampolines (known as soft-edge trampolines), which improves safety for users and reduces risk of concussion. The leaf springs are spaced around the frame as 'legs' that branch from the base frame to suspend the jumping mat, providing flexibility and resilience. \n\nThe \"diaphragm\" common in automotive clutches is a type of leaf spring.\n\n"}
{"id": "39295868", "url": "https://en.wikipedia.org/wiki?curid=39295868", "title": "Lely method", "text": "Lely method\n\nThe Lely method or Lely process is a crystal growth technology used for producing silicon carbide crystals for the semi-conductor industry. The patent for this process was filed in the Netherlands in 1954 and in the United States in 1955 by Jan Anthony Lely of Philips Electronics. The patent was subsequently granted on 30 September 1958, and was refined by D.R. Hamilton et al. in 1960, and by V.P. Novikov and V.I. Ionov in 1968.\n\nThe Lely method produces bulk silicon carbide crystals through the process of sublimation. Silicon carbide powder is loaded into a graphite crucible, which is purged with Argon gas and heated to approximately . The silicon carbide near the outer walls of the crucible sublimes and is deposited on a graphite rod near the center of the crucible, which is at a lower temperature. \n\nSeveral modified versions of the Lely method exist, most commonly the silicon carbide is heated from the bottom end rather than the walls of the crucible, and deposited on the lid. Other modifications include varying the temperature, temperature gradient, Argon pressure, and geometry of the system. Typically, an induction furnace is used to achieve the required temperatures of .\n\n"}
{"id": "14181444", "url": "https://en.wikipedia.org/wiki?curid=14181444", "title": "List of national parks of Uruguay", "text": "List of national parks of Uruguay\n\nThe following is a list of some of the National Parks in Uruguay.\n\n\n\n\n"}
{"id": "199940", "url": "https://en.wikipedia.org/wiki?curid=199940", "title": "Massive compact halo object", "text": "Massive compact halo object\n\nA massive astrophysical compact halo object (MACHO) is any kind of astronomical body that might explain the apparent presence of dark matter in galaxy halos. A MACHO is a body composed of normal baryonic matter that emits little or no radiation and drifts through interstellar space unassociated with any planetary system. Since MACHOs are not luminous, they are hard to detect. MACHOs include black holes or neutron stars as well as brown dwarfs and unassociated planets. White dwarfs and very faint red dwarfs have also been proposed as candidate MACHOs. The term was coined by astrophysicist Kim Griest.\n\nA MACHO may be detected when it passes in front of or nearly in front of a star and the MACHO's gravity bends the light, causing the star to appear brighter in an example of gravitational lensing known as gravitational microlensing. Several groups have searched for MACHOs by searching for the microlensing amplification of light. These groups have ruled out dark matter being explained by MACHOs with mass in the range solar masses (0.3 lunar masses) to 100 solar masses. One group, the MACHO collaboration, claims to have found enough microlensing to predict the existence of many MACHOs with mass of about 0.5 solar masses, enough to make up perhaps 20% of the dark matter in the galaxy.\nThis suggests that MACHOs could be white dwarfs or red dwarfs which have similar masses. However, red and white dwarfs are not completely dark; they do emit some light, and so can be searched for with the Hubble Telescope and with proper motion surveys. These searches have ruled out the possibility that these objects make up a significant fraction of dark matter in our galaxy. Another group, the EROS2 collaboration, does not confirm the signal claims by the MACHO group. They did not find enough microlensing effect with a sensitivity higher by a factor 2. Observations using the Hubble Space Telescope's NICMOS instrument showed that less than one percent of the halo mass is composed of red dwarfs. This corresponds to a negligible fraction of the dark matter halo mass. Therefore, the missing mass problem is not solved by MACHOs.\n\nMACHOs may sometimes be considered to include black holes. Black holes are truly black in that they emit no light and any light shone upon them is absorbed and not reflected. It is thought possible that there is a halo of black holes surrounding the Milky Way.\nA black hole can sometimes be detected by the halo of bright gas and dust that forms around it as an accretion disc being pulled in by the black hole's gravity. Such a disk can generate jets of gas that are shot out away from the black hole because it cannot be absorbed quickly enough. An isolated black hole, however, would not have an accretion disk and would only be detectable by gravitational lensing.\nCosmologists doubt they make up a majority of dark matter because the black holes are at isolated points of the galaxy. The largest contributor to the missing mass must be spread throughout the galaxy to balance the gravity. A minority of physicists, including Chapline and Laughlin, believe that the widely accepted model of the black hole is wrong and needs to be replaced by a new model, the dark-energy star; in the general case for the suggested new model, the cosmological distribution of dark energy would be slightly lumpy and dark-energy stars of primordial type might be a possible candidate for MACHOs.\n\nNeutron stars, differently from black holes, are not heavy enough to collapse completely, and instead forms a material rather like that of an atomic nucleus (sometimes informally called neutronium). After sufficient time these stars could radiate away enough energy to become cold enough that they would be too faint to see. Likewise, old white dwarfs may also become cold and dead, eventually becoming black dwarfs, although the universe is not thought to be old enough for any stars to have reached this stage.\n\nThe next candidate for MACHOs are the brown dwarfs mentioned above. Brown dwarfs are sometimes called \"failed stars\" as they do not have enough mass for nuclear fusion to begin and simply glow a dull brown. Hence, their only source of energy is released through their own gravitational contraction, and may therefore be faintly visible in some circumstances. Brown dwarfs are about thirteen to seventy-five times the mass of Jupiter.\n\nTheoretical work simultaneously also showed that ancient MACHOs are not likely to account for the large amounts of dark matter now thought to be present in the universe. The Big Bang as it is currently understood could not have produced enough baryons and still be consistent with the observed elemental abundances, including the abundance of deuterium. Furthermore, separate observations of baryon acoustic oscillations, both in the cosmic microwave background and large-scale structure of galaxies, set limits on the ratio of baryons to the total amount of matter. These observations show that a large fraction of non-baryonic matter is necessary regardless of the presence or absence of MACHOs.\n\n"}
{"id": "21496735", "url": "https://en.wikipedia.org/wiki?curid=21496735", "title": "Mezimbite Forest Center", "text": "Mezimbite Forest Center\n\nMezimbite Forest Center is a conservation project in Mozambique which aims to eliminates poverty in forest communities by providing the highest valued jobs that protect and restore the forests of Africa.\n"}
{"id": "6395308", "url": "https://en.wikipedia.org/wiki?curid=6395308", "title": "Noisy Cricket", "text": "Noisy Cricket\n\nThe Noisy Cricket is a fictional weapon that appeared in the Men in Black film series. It is the first alien weapon used by one of the protagonists, Agent J, and is his signature weapon in the first installment of the Men in Black series.\n\nA Noisy Cricket is a small pistol-like weapon with a small grip and pointed barrel. It fires a powerful orb of energy, creating the sound of a cricket. It produces a strong recoil that can upend an unsuspecting user. It can also be suppressed to reduce the recoil, but this reduces its power. The spin-off MIB Agent's Handbook explains that Noisy Crickets are deliberately given to rookies, and bets are taken on how far the rookie will fly as a result of the gun's recoil (a practice discouraged under MIB's 'No Hazing' policy). The weapon's concept may have been adapted from an identically-described tiny gun with immense recoil described on the first page of Neal Stephenson's novel Snow Crash.\n\nThe Noisy Cricket was first introduced in the first film. While searching for an appropriate weapon for J, Agent K found and gave him a Noisy Cricket. J mockingly puts it in his pocket and says that he might break it. Yet, he later finds out that the Cricket is very dangerous. Upon firing it the first time, the recoil hurled J into a wall. He tries to shoot it a second time with a proper stance, but it launched him back again, this time into the windshield of a passing taxi. \n\nIt again appeared in the sequel. It was given to a neuralyzed K, who was called Kevin Brown. J jokingly gave Brown a Noisy Cricket. J described the cricket as Brown's favorite weapon. Even though he had no memory of being an MIB agent, K proves his experience with the cricket. He blew Jeeb's head off without being hurt or bothered by the recoil. However, he was lying on the floor at the time, so the recoil presumably had no effect.\n\nThere is no mention or use of the Noisy Cricket in \"Men in Black 3\".\n\nThe Noisy Cricket is the signature weapon of Agent J in the series. It appears in almost every episode as Jay's weapon of choice. Despite his frequent usage of the weapon, the kick is still there and it continues to bother him. He eventually puts a suppressor on it so it no longer kicks, but this suppressor limits the weapon's power.\n\nThe noisy cricket appears in the video game as a weapon in the player's disposal. Like in the films, the cricket fires orbs of energy at the enemy. Although in the gameplay, its power is somewhat reduced, as the main character can fire it off without the conventional recoil seen in the series.\n\nThe Noisy Cricket appeared in a special segment of Sci-Fi Saved My Life entitled \"Men in Black\", discussing the possibilities of it being a reality. It is commonly mistaken that the gun also appears in Will Smith's movie \"Wild Wild West\" (1999) when on the train the outline of the gun rack their appears to be a noisy cricket outline but in fact this is what is called a palm gun.\n\nIt also has been listed on many websites as one of the greatest sci-fi weapons.\n\nA weapon called the Tiny Pistol is featured in Saints Row IV which is found lying on a chair in the hidden secret basement under the Let's Pretend costume store which can accessed by shooting the door with an \"Employees Only\" sign inside the store, which is based on the Noisy Cricket and like the Noisy Cricket it is a powerful weapon that produces a powerful blast of energy, though it also possesses the Noisy Cricket's recoil, as well as a longer recharge time but it can be upgraded to recharge faster. It only weapon skin which is its default skin which is called \"\"Loud Locust\". Its in game description reads \"A miniature pistol of obscure origin and unknown capability\"\".\n\n"}
{"id": "299368", "url": "https://en.wikipedia.org/wiki?curid=299368", "title": "Oil sands", "text": "Oil sands\n\nOil sands, also known as tar sands or crude bitumen, or more technically bituminous sands, are a type of unconventional petroleum deposit. Oil sands are either loose sands or partially consolidated sandstone containing a naturally occurring mixture of sand, clay, and water, saturated with a dense and extremely viscous form of petroleum technically referred to as bitumen (or colloquially as tar due to its superficially similar appearance).\n\nNatural bitumen deposits are reported in many countries, but in particular are found in extremely large quantities in Canada. Other large reserves are located in Kazakhstan, Russia, and Venezuela. The estimated worldwide deposits of oil are more than ; the estimates include deposits that have not been discovered. Proven reserves of bitumen contain approximately 100 billion barrels, and total natural bitumen reserves are estimated at worldwide, of which , or 70.8%, are in Alberta, Canada.\n\nThe crude bitumen contained in the Canadian oil sands is described by the National Energy Board of Canada as \"a highly viscous mixture of hydrocarbons heavier than pentanes which, in its natural state, is not usually recoverable at a commercial rate through a well because it is too thick to flow.\" Crude bitumen is a thick, sticky form of crude oil, so heavy and viscous (thick) that it will not flow unless heated or diluted with lighter hydrocarbons such as light crude oil or natural-gas condensate. At room temperature, it is much like cold molasses. The World Energy Council (WEC) defines natural bitumen as \"oil having a viscosity greater than 10,000 centipoise under reservoir conditions and an API gravity of less than 10° API\". The Orinoco Belt in Venezuela is sometimes described as oil sands, but these deposits are non-bituminous, falling instead into the category of heavy or extra-heavy oil due to their lower viscosity. Natural bitumen and extra-heavy oil differ in the degree by which they have been degraded from the original conventional oils by bacteria. According to the WEC, extra-heavy oil has \"a gravity of less than 10° API and a reservoir viscosity of no more than 10,000 centipoise\".\n\nOil sands have only recently been considered to be part of the world's oil reserves, as historically high oil prices and new technology enabled profitable extraction and processing. Together with other so-called unconventional oil extraction practices, oil sands are implicated in the unburnable carbon debate but also contribute to energy security and counteract the international price cartel OPEC. According to a study ordered by the Government of Alberta, Canada, conducted by Jacobs Engineering Group, carbon emissions from oil-sand crude are 12% higher than from conventional oil.\n\nThe exploitation of bituminous deposits and seeps dates back to Paleolithic times. The earliest known use of bitumen was by Neanderthals, some 40,000 years ago. Bitumen has been found adhering to stone tools used by Neanderthals at sites in Syria. After the arrival of Homo sapiens, humans used bitumen for construction of buildings and waterproofing of reed boats, among other uses. In ancient Egypt, the use of bitumen was important in preparing Egyptian mummies.\n\nIn ancient times, bitumen was primarily a Mesopotamian commodity used by the Sumerians and Babylonians, although it was also found in the Levant and Persia. The area along the Tigris and Euphrates rivers was littered with hundreds of pure bitumen seepages. The Mesopotamians used the bitumen for waterproofing boats and buildings. In Europe, they were extensively mined near the French city of Pechelbronn, where the vapour separation process was in use in 1742.\n\nThe name \"tar sands\" was applied to bituminous sands in the late 19th and early 20th century. People who saw the bituminous sands during this period were familiar with the large amounts of tar residue produced in urban areas as a by-product of the manufacture of coal gas for urban heating and lighting. The word \"tar\" to describe these natural bitumen deposits is really a misnomer, since, chemically speaking, tar is a human-made substance produced by the destructive distillation of organic material, usually coal.\n\nSince then, coal gas has almost completely been replaced by natural gas as a fuel, and coal tar as a material for paving roads has been replaced by the petroleum product asphalt. Naturally occurring bitumen is chemically more similar to asphalt than to coal tar, and the term \"oil sands\" (or oilsands) is more commonly used by industry in the producing areas than \"tar sands\" because synthetic oil is manufactured from the bitumen, and due to the feeling that the terminology of \"tar sands\" is less politically acceptable to the public. Oil sands are now an alternative to conventional crude oil.\n\nIn Canada, the First Nation peoples had used bitumen from seeps along the Athabasca and Clearwater Rivers to waterproof their birch bark canoes from early prehistoric times. The Canadian oil sands first became known to Europeans in 1719 when a Cree native named Wa-Pa-Su brought a sample to Hudsons Bay Company fur trader Henry Kelsey, who commented on it in his journals. Fur trader Peter Pond paddled down the Clearwater River to Athabasca in 1778, saw the deposits and wrote of \"springs of bitumen that flow along the ground.\" In 1787, fur trader and explorer Alexander MacKenzie on his way to the Arctic Ocean saw the Athabasca oil sands, and commented, \"At about 24 miles from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of 20 feet long may be inserted without the least resistance.\"\n\nThe commercial possibilities of Canada's vast oil sands were realized early by Canadian government researchers. In 1884, Robert Bell of the Geological Survey of Canada commented, \"The banks of the Athabasca would furnish an inexhaustible supply of fuel... the material occurs in such enormous quantities that a profitable means of extracting oil...may be found\". In 1915, Sidney Ells of the Federal Mines Branch experimented with separation techniques and used the material to pave of road in Edmonton as well as in other places. In 1920, chemist Karl Clark of the Alberta Research Council began experimenting with methods to extract bitumen from the oil sands and in 1928 he patented the first commercial hot water separation process.\n\nCommercial development began in 1923 when businessman Robert Fitzsimmons began drilling oil wells at Bitumount, north of Fort McMurray but obtained disappointing results with conventional drilling. In 1927 he formed the International Bitumen Company and in 1930 built a small hot-water separation plant based on Clark's design. He produced about of bitumen in 1930 and shipped it by barge and rail to Edmonton. The bitumen from the mine had numerous uses but most of it was used to waterproof roofs. Costs were too high and Fitzsimmons went bankrupt. In 1941 the company was renamed Oil Sands Limited and attempted to iron out technical problems but was never very successful. It went through several changes of ownership and in 1958 closed down permanently. In 1974 Bitumount became an Alberta Provincial Historic Site.\n\nIn 1930 businessman Max Ball formed Canadian Oil Sand Product, Ltd, which later became Abasand Oils. He built a separation plant capable of handling 250 tons of oil sands per day which opened in 1936 and produced an average of of oil. The plant burned down in late 1941 but was rebuilt in 1942 with even larger capacity. In 1943 the Canadian government took control of the Abasand plant under the War Measures Act and planned to expand it further. However in 1945 the plant burned down again and in 1946 the Canadian government abandoned the project because the need for fuel had diminished with the end of the war. The Abasand site is also an Alberta Historic Site.\n\nThe world's largest deposits of oil sands are in Venezuela and Canada. The geology of the deposits in the two countries is generally rather similar. They are vast heavy oil, extra-heavy oil, and/or bitumen deposits with oil heavier than 20°API, found largely in unconsolidated sandstones with similar properties. \"Unconsolidated\" in this context means that the sands have high porosity, no significant cohesion, and a tensile strength close to zero. The sands are saturated with oil which has prevented them from consolidating into hard sandstone.\n\nThe magnitude of the resources in the two countries is on the order of 3.5 to 4 trillion barrels (550 to 650 billion cubic metres) of original oil in place (OOIP). Oil in place is not necessarily oil reserves, and the amount that can be produced depends on technological evolution. Rapid technological developments in Canada in the 1985–2000 period resulted in techniques such as steam-assisted gravity drainage (SAGD) that can recover a much greater percentage of the OOIP than conventional methods. The Alberta government estimates that with current technology, 10% of its bitumen and heavy oil can be recovered, which would give it about 200 billion barrels (32 billion m) of recoverable oil reserves. Venezuela estimates its recoverable oil at 267 billion barrels (42 billion m). This places Canada and Venezuela in the same league as Saudi Arabia, having the three largest oil reserves in the world.\n\nThere are numerous deposits of oil sands in the world, but the biggest and most important are in Canada and Venezuela, with lesser deposits in Kazakhstan and Russia. The total volume of non-conventional oil in the oil sands of these countries exceeds the reserves of conventional oil in all other countries combined. Vast deposits of bitumen – over 350 billion cubic metres (2.2 trillion barrels) of oil in place – exist in the Canadian provinces of Alberta and Saskatchewan. If only 30% of this oil could be extracted, it could supply the entire needs of North America for over 100 years at 2002 consumption levels. These deposits represent plentiful oil, but not cheap oil. They require advanced technology to extract the oil and transport it to oil refineries.\n\nThe oil sands of the Western Canadian Sedimentary Basin (WCSB) are a result of the formation of the Canadian Rocky Mountains by the Pacific Plate overthrusting the North American Plate as it pushed in from the west, carrying the formerly large island chains which now comprise most of British Columbia. The collision compressed the Alberta plains and raised the Rockies above the plains, forming mountain ranges. This mountain building process buried the sedimentary rock layers which underlie most of Alberta to a great depth, creating high subsurface temperatures, and producing a giant pressure cooker effect that converted the kerogen in the deeply buried organic-rich shales to light oil and natural gas. These source rocks were similar to the American so-called oil shales, except the latter have never been buried deep enough to convert the kerogen in them into liquid oil.\n\nThis overthrusting also tilted the pre-Cretaceous sedimentary rock formations underlying most of the sub-surface of Alberta, depressing the rock formations in southwest Alberta up to deep near the Rockies, but to zero depth in the northeast, where they pinched out against the igneous rocks of the Canadian Shield, which outcrop on the surface. This tilting is not apparent on the surface because the resulting trench has been filled in by eroded material from the mountains. The light oil migrated up-dip through hydro-dynamic transport from the Rockies in the southwest toward the Canadian Shield in the northeast following a complex pre-Cretaceous unconformity that exists in the formations under Alberta. The total distance of oil migration southwest to northeast was about . At the shallow depths of sedimentary formations in the northeast, massive microbial biodegradation as the oil approached the surface caused the oil to become highly viscous and immobile. Almost all of the remaining oil is found in the far north of Alberta, in Middle Cretaceous (115 million-year old) sand-silt-shale deposits overlain by thick shales, although large amounts of heavy oil lighter than bitumen are found in the Heavy Oil Belt along the Alberta-Saskatchewan border, extending into Saskatchewan and approaching the Montana border. Note that, although adjacent to Alberta, Saskatchewan has no massive deposits of bitumen, only large reservoirs of heavy oil >10°API.\n\nMost of the Canadian oil sands are in three major deposits in northern Alberta. They are the Athabasca-Wabiskaw oil sands of north northeastern Alberta, the Cold Lake deposits of east northeastern Alberta, and the Peace River deposits of northwestern Alberta. Between them, they cover over —an area larger than England—and contain approximately of crude bitumen in them. About 10% of the oil in place, or , is estimated by the government of Alberta to be recoverable at current prices, using current technology, which amounts to 97% of Canadian oil reserves and 75% of total North American petroleum reserves. Although the Athabasca deposit is the only one in the world which has areas shallow enough to mine from the surface, all three Alberta areas are suitable for production using \"in-situ\" methods, such as cyclic steam stimulation (CSS) and steam assisted gravity drainage (SAGD).\n\nThe largest Canadian oil sands deposit, the Athabasca oil sands is in the McMurray Formation, centered on the city of Fort McMurray, Alberta. It outcrops on the surface (zero burial depth) about north of Fort McMurray, where enormous oil sands mines have been established, but is deep southeast of Fort McMurray. Only 3% of the oil sands area containing about 20% of the recoverable oil can be produced by surface mining, so the remaining 80% will have to be produced using in-situ wells. The other Canadian deposits are between deep and will require in-situ production.\n\nThe Athabasca oil sands lie along the Athabasca River and are the largest natural bitumen deposit in the world, containing about 80% of the Alberta total, and the only one suitable for surface mining. With modern unconventional oil production technology, at least 10% of these deposits, or about are considered to be economically recoverable, making Canada's total proven reserves the third largest in the world, after Saudi Arabia's conventional oil and Venezuela's Orinoco oil sands.\n\nThe Athabasca oil sands are more or less centered around the remote northern city of Fort McMurray. They are by far the largest deposit of bitumen in Canada, probably containing over 150 billion cubic metres (900 billion barrels) of oil in place. The bitumen is highly viscous and is often denser than water (10°API or 1000 kg/m). The oil saturated sands range from thick in places, and the oil saturation in the oil-rich zones is on the order of 90% bitumen by weight.\n\nThe Athabasca River cuts through the heart of the deposit, and traces of the heavy oil are readily observed as black stains on the river banks. Since portions of the Athabasca sands are shallow enough to be surface-mineable, they were the earliest ones to see development. Historically, the bitumen was used by the indigenous Cree and Dene Aboriginal peoples to waterproof their canoes. The Athabasca oil sands first came to the attention of European fur traders in 1719 when Wa-pa-su, a Cree trader, brought a sample of bituminous sands to the Hudson's Bay Company post at York Factory on Hudson Bay.\nIn 1778, Peter Pond, a fur trader for the rival North West Company, was the first European to see the Athabasca deposits. In 1788, fur trader and explorer Alexander Mackenzie from the Hudson Bay Company, who later discovered the Mackenzie River and routes to both the Arctic and Pacific Oceans, described the oil sands in great detail. He said, \"At about from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of long may be inserted without the least resistance. The bitumen is in a fluid state and when mixed with gum, the resinous substance collected from the spruce fir, it serves to gum the Indians' canoes.\"\n\nIn 1883, G.C. Hoffman of the Geological Survey of Canada tried separating the bitumen from oil sand with the use of water and reported that it separated readily. In 1888, Robert Bell of the Geological Survey of Canada reported to a Senate Committee that \"The evidence ... points to the existence in the Athabasca and Mackenzie valleys of the most extensive petroleum field in America, if not the world.\" In 1926, Karl Clark of the University of Alberta patented a hot water separation process which was the forerunner of today's thermal extraction processes. However, it was 1967 before the first large scale commercial operation began with the opening of the Great Canadian Oil Sands mine by the Sun Oil Company of Ohio.\n\nToday its successor company, Suncor Energy (no longer affiliated with Sun Oil), is the largest oil company in Canada. In addition, other companies such as Royal Dutch Shell, ExxonMobil, and various national oil companies are developing the Athabasca oil sands. As a result, Canada is now by far the largest exporter of oil to the United States.\n\nThe smaller Wabasca (or Wabiskaw) oil sands lie above the western edge of the Athabasca oil sands and overlap them. They probably contain over 15 billion cubic metres (90 billion barrels) of oil in place. The deposit is buried from deep and ranges from thick. In many regions the oil-rich Wabasca formation overlies the similarly oil-rich McMurray formation, and as a result the two overlapping oil sands are often treated as one oil sands deposit. However, the two deposits are invariable separated by a minimum of of clay shale and silt. The bitumen in the Wabasca is as highly viscous as that in the Athabasca, but lies too deep to be surface-mined, so in-situ production methods must be used to produce the crude bitumen.\n\nThe Cold Lake oil sands are northeast of Alberta's capital, Edmonton, near the border with Saskatchewan. A small portion of the Cold Lake deposit lies in Saskatchewan. Although smaller than the Athabasca oil sands, the Cold Lake oil sands are important because some of the oil is fluid enough to be extracted by conventional methods. The Cold Lake bitumen contains more alkanes and less asphaltenes than the other major Alberta oil sands and the oil is more fluid. As a result, cyclic steam stimulation (CSS) is commonly used for production.\n\nThe Cold Lake oil sands are of a roughly circular shape, centered around Bonnyville, Alberta. They probably contain over 60 billion cubic metres (370 billion barrels) of extra-heavy oil-in-place. The oil is highly viscous, but considerably less so than the Athabasca oil sands, and is somewhat less sulfurous. The depth of the deposits is and they are from thick. They are too deep to surface mine.\n\nMuch of the oil sands are on Canadian Forces Base Cold Lake. CFB Cold Lake's CF-18 Hornet jet fighters defend the western half of Canadian air space and cover Canada's Arctic territory. Cold Lake Air Weapons Range (CLAWR) is one of the largest live-drop bombing ranges in the world, including testing of cruise missiles. As oil sands production continues to grow, various sectors vie for access to airspace, land, and resources, and this complicates oil well drilling and production significantly.\n\nThe Peace River oil sands located in northwest-central Alberta are the smallest of the three major oil sands deposits in Alberta. The Peace River oil sands lie generally in the watershed of the Peace River, the largest river in Alberta. The Peace and Athabasca rivers, which are by far the largest rivers in Alberta, flow through their respective oil sands and merge at Lake Athabasca to form the Slave River, which flows into the MacKenzie River, one of the largest rivers in the world. All of the water from these rivers flow into the Arctic Ocean.\n\nThe Peace River oil sands probably contain over 30 billion cubic metres (200 billion barrels) of oil-in-place. The thickness of the deposit ranges from and it is buried about deep.\n\nWhereas the Athabasca oil sands lie close enough to the surface that the bitumen can be excavated in open-pit mines, the smaller Peace River deposits are too deep, and must be exploited using in situ methods such as steam-assisted gravity drainage and Cold Heavy Oil Production with Sand (CHOPS).\n\nThe Eastern Venezuelan Basin has a structure similar to the WCSB, but on a shorter scale. The distance the oil has migrated up-dip from the Sierra Orientale mountain front to the Orinoco oil sands where it pinches out against the igneous rocks of the Guyana Shield is only about . The hydrodynamic conditions of oil transport were similar, source rocks buried deep by the rise of the mountains of the Sierra Orientale produced light oil that moved up-dip toward the south until it was gradually immobilized by the viscosity increase caused by biodgradation near the surface. The Orinoco deposits are early Tertiary (50 to 60 million years old) sand-silt-shale sequences overlain by continuous thick shales, much like the Canadian deposits.\n\nIn Venezuela, the Orinoco Belt oil sands range from deep and no surface outcrops exist. The deposit is about long east-to-west and wide north-to-south, much less than the combined area covered by the Canadian deposits. In general, the Canadian deposits are found over a much wider area, have a broader range of properties, and have a broader range of reservoir types than the Venezuelan ones, but the geological structures and mechanisms involved are similar. The main differences is that the oil in the sands in Venezuela is less viscous than in Canada, allowing some of it to be produced by conventional drilling techniques, but none of it approaches the surface as in Canada, meaning none of it can be produced using surface mining. The Canadian deposits will almost all have to be produced by mining or using new non-conventional techniques.\n\nThe Orinoco Belt is a territory in the southern strip of the eastern Orinoco River Basin in Venezuela which overlies one of the world's largest deposits of petroleum. The Orinoco Belt follows the line of the river. It is approximately from east to west, and from north to south, with an area about .\n\nThe oil sands consist of large deposits of extra heavy crude. Venezuela's heavy oil deposits of about of oil in place are estimated to approximately equal the world's reserves of lighter oil. Petróleos de Venezuela S.A. (PDVSA), Venezuela's national oil company, has estimated that the producible reserves of the Orinoco Belt are up to which would make it the largest petroleum reserve in the world.\n\nIn 2009, the US Geological Survey (USGS) increased its estimates of the reserves to of oil which is \"technically recoverable (producible using currently available technology and industry practices).\" No estimate of how much of the oil is economically recoverable was made.\n\nIn addition to the three major Canadian oil sands in Alberta, there is a fourth major oil sands deposit in Canada, the Melville Island oil sands in the Canadian Arctic islands, which are too remote to expect commercial production in the foreseeable future.\n\nApart from the megagiant oil sands deposits in Canada and Venezuela, numerous other countries hold smaller oil sands deposits. In the United States, there are supergiant oil sands resources primarily concentrated in Eastern Utah, with a total of of oil (known and potential) in eight major deposits in Carbon, Garfield, Grand, Uintah, and Wayne counties. In addition to being much smaller than the Canadian oil sands deposits, the US oil sands are hydrocarbon-wet, whereas the Canadian oil sands are water-wet. This requires somewhat different extraction techniques for the Utah oil sands than those used for the Alberta oil sands.\n\nRussia holds oil sands in two main regions. Large resources are present in the Tunguska Basin, East Siberia, with the largest deposits being Olenek and Siligir. Other deposits are located in the Timan-Pechora and Volga-Urals basins (in and around Tatarstan), which is an important but very mature province in terms of conventional oil, holds large amounts of oil sands in a shallow permian formation. In Kazakhstan, large bitumen deposits are located in the North Caspian Basin.\n\nIn Madagascar, Tsimiroro and Bemolanga are two heavy oil sands deposits, with a pilot well already producing small amounts of oil in Tsimiroro. and larger scale exploitation in the early planning phase. In the Republic of the Congo reserves are estimated between .\n\nBituminous sands are a major source of unconventional oil, although only Canada has a large-scale commercial oil sands industry. In 2006, bitumen production in Canada averaged through 81 oil sands projects. 44% of Canadian oil production in 2007 was from oil sands. This proportion was (as of 2008) expected to increase in coming decades as bitumen production grows while conventional oil production declines, although due to the 2008 economic downturn work on new projects has been deferred. Petroleum is not produced from oil sands on a significant level in other countries.\n\nThe Alberta oil sands have been in commercial production since the original Great Canadian Oil Sands (now Suncor Energy) mine began operation in 1967. Despite the increasing levels of production, the process of extraction and processing of oil sands can still be considered to be in its infancy; with new technologies and stakeholders oversight providing an ever-lower environmental footprint. A second mine, operated by the Syncrude consortium, began operation in 1978 and is the biggest mine of any type in the world. The third mine in the Athabasca Oil Sands, the Albian Sands consortium of Shell Canada, Chevron Corporation, and Western Oil Sands Inc. [purchased by Marathon Oil Corporation in 2007] began operation in 2003. Petro-Canada was also developing a $33 billion Fort Hills Project, in partnership with UTS Energy Corporation and Teck Cominco, which lost momentum after the 2009 merger of Petro-Canada into Suncor.\n\nBy 2013 there were nine oil sands mining projects in the Athabasca oil sands deposit: Suncor Energy Inc. (Suncor), Syncrude Canada Limited (Syncrude)'s Mildred Lake and Aurora North, Shell Canada Limited (Shell)'s Muskeg River and Jackpine, Canadian Natural Resources Limited (CNRL), Horizon, Imperial Oil Resources Ventures Limited (Imperial), Kearl Oil Sands Project (KOSP), Total E&P Canada Ltd. Joslyn North Mine and Fort Hills Energy Corporation (FHEC). In 2011 alone they produced over 52 million cubic metres of bitumen.\n\nNo significant development of Venezuela's extra-heavy oil deposits was undertaken before 2000, except for the BITOR operation which produced somewhat less than 100,000 barrels of oil per day (16,000 m/d) of 9°API oil by primary production. This was mostly shipped as an emulsion (Orimulsion) of 70% oil and 30% water with similar characteristics as heavy fuel oil for burning in thermal power plants. However, when a major strike hit the Venezuelan state oil company PDVSA, most of the engineers were fired as punishment. Orimulsion had been the pride of the PDVSA engineers, so Orimulsion fell out of favor with the key political leaders. As a result, the government has been trying to \"Wind Down\" the Orimulsion program.\n\nDespite the fact that the Orinoco oil sands contain extra-heavy oil which is easier to produce than Canada's similarly-sized reserves of bitumen, Venezuela's oil production has been declining in recent years because of the country's political and economic problems, while Canada's has been increasing. As a result, Canadian heavy oil and bitumen exports have been backing Venezuelan heavy and extra-heavy oil out of the US market, and Canada's total exports of oil to the US have become several times as great as Venezuela's.\n\nBy 2016, with the economy of Venezuela in a tailspin and the country experiencing widespread shortages of food, rolling power blackouts, rioting, and anti-government protests, it was unclear how much new oil sands production would occur in the near future.\n\nIn May 2008, the Italian oil company Eni announced a project to develop a small oil sands deposit in the Republic of the Congo. Production is scheduled to commence in 2014 and is estimated to eventually yield a total of .\n\nExcept for a fraction of the extra-heavy oil or bitumen which can be extracted by conventional oil well technology, oil sands must be produced by strip mining or the oil made to flow into wells using sophisticated \"in-situ\" techniques. These methods usually use more water and require larger amounts of energy than conventional oil extraction. While much of Canada's oil sands are being produced using open-pit mining, approximately 90% of Canadian oil sands and all of Venezuela's oil sands are too far below the surface to use surface mining.\n\nConventional crude oil is normally extracted from the ground by drilling oil wells into a petroleum reservoir, allowing oil to flow into them under natural reservoir pressures, although artificial lift and techniques such as horizontal drilling, water flooding and gas injection are often required to maintain production. When primary production is used in the Venezuelan oil sands, where the extra-heavy oil is about 50 degrees Celsius, the typical oil recovery rates are about 8–12%. Canadian oil sands are much colder and more biodegraded, so bitumen recovery rates are usually only about 5–6%. Historically, primary recovery was used in the more fluid areas of Canadian oil sands. However, it recovered only a small fraction of the oil in place, so it is not often used today.\n\nThe Athabasca oil sands are the only major oil sands deposits which are shallow enough to surface mine. In the Athabasca sands there are very large amounts of bitumen covered by little overburden, making surface mining the most efficient method of extracting it. The overburden consists of water-laden muskeg (peat bog) over top of clay and barren sand. The oil sands themselves are typically thick deposits of crude bitumen embedded in unconsolidated sandstone, sitting on top of flat limestone rock. Since Great Canadian Oil Sands (now Suncor Energy) started operation of the first large-scale oil sands mine in 1967, bitumen has been extracted on a commercial scale and the volume has grown at a steady rate ever since.\n\nA large number of oil sands mines are currently in operation and more are in the stages of approval or development. The Syncrude Canada mine was the second to open in 1978, Shell Canada opened its Muskeg River mine (Albian Sands) in 2003 and Canadian Natural Resources Ltd (CNRL) opened its Horizon Oil Sands project in 2009. Newer mines include Shell Canada's Jackpine mine, Imperial Oil's Kearl Oil Sands Project, the Synenco Energy (now owned by Total S.A.) Northern Lights mine, and Suncor's Fort Hills mine.\n\nOil sands tailings ponds are engineered dam and dyke systems that contain salts, suspended solids and other dissolvable chemical compounds such as naphthenic acids, benzene, hydrocarbons residual bitumen, fine silts (mature fine tails MFT), and water. Large volumes of tailings are a byproduct of surface mining of the oil sands and managing these tailings is one of the most difficult environmental challenges facing the oil sands industry. The Government of Alberta reported in 2013 that tailings ponds in the Alberta oil sands covered an area of about . The Syncrude Tailings Dam or Mildred Lake Settling Basin (MLSB) is an embankment dam that is, by volume of construction material, the largest earth structure in the world in 2001.\n\nSome years ago Canadian oil companies discovered that if they removed the sand filters from heavy oil wells and produced as much sand as possible with the oil, production rates improved significantly. This technique became known as Cold Heavy Oil Production with Sand (CHOPS). Further research disclosed that pumping out sand opened \"wormholes\" in the sand formation which allowed more oil to reach the wellbore. The advantage of this method is better production rates and recovery (around 10% versus 5–6% with sand filters in place) and the disadvantage that disposing of the produced sand is a problem. A novel way to do this was spreading it on rural roads, which rural governments liked because the oily sand reduced dust and the oil companies did their road maintenance for them. However, governments have become concerned about the large volume and composition of oil spread on roads. so in recent years disposing of oily sand in underground salt caverns has become more common.\n\nThe use of steam injection to recover heavy oil has been in use in the oil fields of California since the 1950s. The cyclic steam stimulation (CSS) \"huff-and-puff\" method is now widely used in heavy oil production worldwide due to its quick early production rates; however recovery factors are relatively low (10–40% of oil in place) compared to SAGD (60–70% of OIP).\n\nCSS has been in use by Imperial Oil at Cold Lake since 1985 and is also used by Canadian Natural Resources at Primrose and Wolf Lake and by Shell Canada at Peace River. In this method, the well is put through cycles of steam injection, soak, and oil production. First, steam is injected into a well at a temperature of 300 to 340 degrees Celsius for a period of weeks to months; then, the well is allowed to sit for days to weeks to allow heat to soak into the formation; and, later, the hot oil is pumped out of the well for a period of weeks or months. Once the production rate falls off, the well is put through another cycle of injection, soak and production. This process is repeated until the cost of injecting steam becomes higher than the money made from producing oil.\n\nSteam assisted gravity drainage was developed in the 1980s by the Alberta Oil Sands Technology and Research Authority and fortuitously coincided with improvements in directional drilling technology that made it quick and inexpensive to do by the mid 1990s. In SAGD, two horizontal wells are drilled in the oil sands, one at the bottom of the formation and another about 5 metres above it. These wells are typically drilled in groups off central pads and can extend for miles in all directions. In each well pair, steam is injected into the upper well, the heat melts the bitumen, which allows it to flow into the lower well, where it is pumped to the surface.\n\nSAGD has proved to be a major breakthrough in production technology since it is cheaper than CSS, allows very high oil production rates, and recovers up to 60% of the oil in place. Because of its economic feasibility and applicability to a vast area of oil sands, this method alone quadrupled North American oil reserves and allowed Canada to move to second place in world oil reserves after Saudi Arabia. Most major Canadian oil companies now have SAGD projects in production or under construction in Alberta's oil sands areas and in Wyoming. Examples include Japan Canada Oil Sands Ltd's (JACOS) project, Suncor's Firebag project, Nexen's Long Lake project, Suncor's (formerly Petro-Canada's) MacKay River project, Husky Energy's Tucker Lake and Sunrise projects, Shell Canada's Peace River project, Cenovus Energy's Foster Creek and Christina Lake developments, ConocoPhillips' Surmont project, Devon Canada's Jackfish project, and Derek Oil & Gas's LAK Ranch project. Alberta's OSUM Corp has combined proven underground mining technology with SAGD to enable higher recovery rates by running wells underground from within the oil sands deposit, thus also reducing energy requirements compared to traditional SAGD. This particular technology application is in its testing phase.\n\nSeveral methods use solvents, instead of steam, to separate bitumen from sand. Some solvent extraction methods may work better in \"in situ\" production and other in mining. Solvent can be beneficial if it produces more oil while requiring less energy to produce steam.\n\nVapor Extraction Process (VAPEX) is an \"in situ\" technology, similar to SAGD. Instead of steam, hydrocarbon solvents are injected into an upper well to dilute bitumen and enables the diluted bitumen to flow into a lower well. It has the advantage of much better energy efficiency over steam injection, and it does some partial upgrading of bitumen to oil right in the formation. The process has attracted attention from oil companies, who are experimenting with it.\n\nThe above methods are not mutually exclusive. It is becoming common for wells to be put through one CSS injection-soak-production cycle to condition the formation prior to going to SAGD production, and companies are experimenting with combining VAPEX with SAGD to improve recovery rates and lower energy costs.\n\nThis is a very new and experimental method that combines a vertical air injection well with a horizontal production well. The process ignites oil in the reservoir and creates a vertical wall of fire moving from the \"toe\" of the horizontal well toward the \"heel\", which burns the heavier oil components and upgrades some of the heavy bitumen into lighter oil right in the formation. Historically fireflood projects have not worked out well because of difficulty in controlling the flame front and a propensity to set the producing wells on fire. However, some oil companies feel the THAI method will be more controllable and practical, and have the advantage of not requiring energy to create steam.\n\nAdvocates of this method of extraction state that it uses less freshwater, produces 50% less greenhouse gases, and has a smaller footprint than other production techniques.\n\nPetrobank Energy and Resources has reported encouraging results from their test wells in Alberta, with production rates of up to per well, and the oil upgraded from 8 to 12 API degrees.\nThe company hopes to get a further 7-degree upgrade from its CAPRI (controlled atmospheric pressure resin infusion) system, which pulls the oil through a catalyst lining the lower pipe.\n\nAfter several years of production in situ, it has become clear that current THAI methods do not work as planned. Amid steady drops in production from their THAI wells at Kerrobert, Petrobank has written down the value of their THAI patents and the reserves at the facility to zero. They have plans to experiment with a new configuration they call \"multi-THAI,\" involving adding more air injection wells.\n\nThis is an experimental method that employs a number of vertical air injection wells above a horizontal production well located at the base of the bitumen pay zone. An initial Steam Cycle similar to CSS is used to prepare the bitumen for ignition and mobility. Following that cycle, air is injected into the vertical wells, igniting the upper bitumen and mobilizing (through heating) the lower bitumen to flow into the production well. It is expected that COGD will result in water savings of 80% compared to SAGD.\n\nApproximately of energy is needed to extract a barrel of bitumen and upgrade it to synthetic crude. As of 2006, most of this is produced by burning natural gas. Since a barrel of oil equivalent is about , its EROEI is 5–6. That means this extracts about 5 or 6 times as much energy as is consumed. Energy efficiency is expected to improve to an average of of natural gas or of energy per barrel by 2015, giving an EROEI of about 6.5.\n\nAlternatives to natural gas exist and are available in the oil sands area. Bitumen can itself be used as the fuel, consuming about 30–35% of the raw bitumen per produced unit of synthetic crude. Nexen's Long Lake project will use a proprietary deasphalting technology to upgrade the bitumen, using asphaltene residue fed to a gasifier whose syngas will be used by a cogeneration turbine and a hydrogen producing unit, providing all the energy needs of the project: steam, hydrogen, and electricity. Thus, it will produce syncrude without consuming natural gas, but the capital cost is very high.\n\nShortages of natural gas for project fuel were forecast to be a problem for Canadian oil sands production a few years ago, but recent increases in US shale gas production have eliminated much of the problem for North America. With the increasing use of hydraulic fracturing making US largely self-sufficient in natural gas and exporting more natural gas to Eastern Canada to replace Alberta gas, the Alberta government is using its powers under the NAFTA and the Canadian Constitution to reduce shipments of natural gas to the US and Eastern Canada, and divert the gas to domestic Alberta use, particularly for oil sands fuel. The natural gas pipelines to the east and south are being converted to carry increasing oil sands production to these destinations instead of gas. Canada also has huge undeveloped shale gas deposits in addition to those of the US, so natural gas for future oil sands production does not seem to be a serious problem. The low price of natural gas as the result of new production has considerably improved the economics of oil sands production.\n\nThe extra-heavy crude oil or crude bitumen extracted from oil sands is a very viscous semisolid form of oil that does not easily flow at normal temperatures, making it difficult to transport to market by pipeline. To flow through oil pipelines, it must either be upgraded to lighter synthetic crude oil (SCO), blended with diluents to form dilbit, or heated to reduce its vicosity.\n\nIn the Canadian oil sands, bitumen produced by surface mining is generally upgraded on-site and delivered as synthetic crude oil. This makes delivery of oil to market through conventional oil pipelines quite easy. On the other hand, bitumen produced by the in-situ projects is generally not upgraded but delivered to market in raw form. If the agent used to upgrade the bitumen to synthetic crude is not produced on site, it must be sourced elsewhere and transported to the site of upgrading. If the upgraded crude is being transported from the site by pipeline, and additional pipeline will be required to bring in sufficient upgrading agent. The costs of production of the upgrading agent, the pipeline to transport it and the cost to operate the pipeline must be calculated into the production cost of the synthetic crude.\n\nUpon reaching a refinery, the synthetic crude is processed and a significant portion of the upgrading agent will be removed during the refining process. It may be used for other fuel fractions, but the end result is that liquid fuel has to be piped to the upgrading facility simply to make the bitumen transportable by pipeline. If all costs are considered, synthetic crude production and transfer using bitumen and an upgrading agent may prove economically unsustainable.\n\nWhen the first oil sands plants were built over 50 years ago, most oil refineries in their market area were designed to handle light or medium crude oil with lower sulfur content than the 4–7% that is typically found in bitumen. The original oil sands upgraders were designed to produce a high-quality synthetic crude oil (SCO) with lower density and lower sulfur content. These are large, expensive plants which are much like heavy oil refineries. Research is currently being done on designing simpler upgraders which do not produce SCO but simply treat the bitumen to reduce its viscosity, allowing to be transported unblended like conventional heavy oil.\n\nWestern Canadian Select, launched in 2004 as a new heavy oil stream, blended at the Husky Energy terminal in Hardisty, Alberta,\nis the largest crude oil stream coming from the Canadian oil sands and the benchmark for emerging heavy, high TAN (acidic) crudes.\n<ref name=\"TMX/Shorcan Energy Brokers\">\n</ref> \nWCS is traded at Cushing, Oklahoma, a major oil supply hub connecting oil suppliers to the Gulf Coast, which has become the most significant trading hub for crude oil in North America. While its major component is bitumen, it also contains a combination of sweet synthetic and condensate diluents, and 25 existing streams of both conventional and unconventional oil making it a syndilbit— both a dilbit and a synbit.\n\nThe first step in upgrading is vacuum distillation to separate the lighter fractions. After that, de-asphalting is used to separate the asphalt from the feedstock. Cracking is used to break the heavier hydrocarbon molecules down into simpler ones. Since cracking produces products which are rich in sulfur, desulfurization must be done to get the sulfur content below 0.5% and create sweet, light synthetic crude oil.\n\nIn 2012, Alberta produced about of crude bitumen from its three major oil sands deposits, of which about was upgraded to lighter products and the rest sold as raw bitumen. The volume of both upgraded and non-upgraded bitumen is increasing yearly. Alberta has five oil sands upgraders producing a variety of products. These include:\n\nModernized and new large refineries such as are found in the Midwestern United States and on the Gulf Coast of the United States, as well as many in China, can handle upgrading heavy oil themselves, so their demand is for non-upgraded bitumen and extra-heavy oil rather than SCO. The main problem is that the feedstock would be too viscous to flow through pipelines, so unless it is delivered by tanker or rail car, it must be blended with diluent to enable it to flow. This requires mixing the crude bitumen with a lighter hydrocarbon diluent such as condensate from gas wells, pentanes and other light products from oil refineries or gas plants, or synthetic crude oil from oil sands upgraders to allow it to flow through pipelines to market.\n\nTypically, blended bitumen contains about 30% natural gas condensate or other diluents and 70% bitumen. Alternatively, bitumen can also be delivered to market by specially designed railway tank cars, tank trucks, liquid cargo barges, or ocean-going oil tankers. These do not necessarily require the bitumen be blended with diluent since the tanks can be heated to allow the oil to be pumped out.\n\nThe demand for condensate for oil sands diluent is expected to be more than by 2020, double 2012 volumes. Since Western Canada only produces about of condensate, the supply was expected to become a major constraint on bitumen transport. However, the recent huge increase in US tight oil production has largely solved this problem, because much of the production is too light for US refinery use but ideal for diluting bitumen. The surplus American condensate and light oil is being exported to Canada and blended with bitumen, and then re-imported to the US as feedstock for refineries. Since the diluent is simply exported and then immediately re-imported, it is not subject to the US ban on exports of crude oil. Once it is back in the US, refineries separate the diluent and re-export it to Canada, which again bypasses US crude oil export laws since it is now a refinery product. To aid in this process, Kinder Morgan Energy Partners is reversing its Cochin Pipeline, which used to carry propane from Edmonton to Chicago, to transport of condensate from Chicago to Edmonton by mid-2014; and Enbridge is considering the expansion of its Southern Lights pipeline, which currently ships of diluent from the Chicago area to Edmonton, by adding another .\n\nAlthough Venezuelan extra-heavy oil is less viscous than Canadian bitumen, much of the difference is due to temperature. Once the oil comes out of the ground and cools, it has the same difficulty in that it is too viscous to flow through pipelines. Venezuela is now producing more extra heavy crude in the Orinoco oil sands than its four upgraders, which were built by foreign oil companies over a decade ago, can handle. The upgraders have a combined capacity of , which is only half of its production of extra-heavy oil. In addition Venezuela produces insufficient volumes of naphtha to use as diluent to move extra-heavy oil to market. Unlike Canada, Venezuela does not produce much natural gas condensate from its own gas wells, and unlike Canada, it does not have easy access to condensate from new US shale gas production. Since Venezuela also has insufficient refinery capacity to supply its domestic market, supplies of naptha are insufficient to use as pipeline diluent, and it is having to import naptha to fill the gap. Since Venezuela also has financial problems – as a result of the country's economic crisis -, and political disagreements with the US government and oil companies, the situation remains unresolved.\n\nA network of gathering and feeder pipelines collects crude bitumen and SCO from Alberta's northern oil sands deposits (primarily Athabasca, Cold Lake, and Peace River), and feeds them into two main collection points for southbound deliveries: Edmonton, Alberta and Hardisty, Alberta. Most of the feeder pipelines move blended bitumen or SCO southbound and diluent northbound, but a few move product laterally within the oil sands region. In 2012, the capacity of the southbound feeder lines was over 300,000 m³/d (2 million bbl/d) and more capacity was being added. The building of new oil sands feeder pipelines requires only the approval of the Alberta Energy Regulator, an agency that deals with matters entirely within Alberta and is likely to give little consideration to interference from political and environmental interest from outside Alberta.\n\nFrom Edmonton and Hardisty, main transmission pipelines move blended bitumen and SCO, as well as conventional crude oil and various oil and natural productions to market destinations across North America. The main transmission systems include:\n\n\nOverall, the total pipeline capacity for the movement of crude oil from Edmonton and Hardisty to the rest of North America is about . However, other substances such as conventional crude oil and refined petroleum products also share this pipeline network. The rapidly increasing tight oil production from the Bakken formation of North Dakota also competes for space on the Canadian export pipeline system. North Dakota oil producers are using the Canadian pipelines to deliver their oil to US refineries.\n\nIn 2012, the Canadian export pipeline system began to become overloaded with new oil production. As a result, Enbridge implemented pipeline apportionment on its southbound lines, and Kinder Morgan on its westbound line. This rationed pipeline space by reducing the monthly allocation of each shipper to a certain percentage of its requirements. The Chevron Corporation Burnaby Refinery, the last remaining oil refinery on Canada's west coast, applied to the NEB for preferential access to Canadian oil since American refineries in Washington and California were outbidding it for pipeline space, but was denied because it would violate NAFTA equal access to energy rules. Similarly, new North Dakota tight oil production began to block new Canadian production from using the Enbridge, Kinder Morgan, and TransCanada southbound systems.\n\nIn addition, the US oil marketing hub at Cushing was flooded with new oil because most new North American production from Canada, North Dakota, and Texas converged at that point, and there was insufficient capacity to take it from there to refineries on the Gulf Coast, where half of US oil refinery capacity is located. The American pipeline system is designed to take imported oil from the Gulf Coast and Texas to the refineries in the northern US, and the new oil was flowing in the opposite direction, toward the Gulf Coast. The price of West Texas Intermediate delivered at Cushing, which is the main benchmark for US oil prices, fell to unprecedented low levels below other international benchmark oils such as Brent Crude and Dubai Crude. Since the price of WTI at Cushing is usually quoted by US media as \"the price of oil\", this gave many Americans a distorted view of world oil prices as being lower than they were, and the supply being better than it was internationally. Canada used to be in a similar position to the US in that offshore oil was cheaper than domestic oil, so the oil pipelines used to run westward from the east coast to Central Canada, now they are being reversed to carry cheaper domestic oil sands production from Alberta to the east coast.\n\nLack of access to markets, limited export capacity, and oversupply in the US market have been a problem for oil sands producers in recent years. They have caused lower prices to Canadian oil sands producers and reduced royalty and tax revenues to Canadian governments. The pipeline companies have moved forward with a number of solutions to the transportation problems:\n\nWith the main constraint on Canadian oil sands development becoming the availability of export pipeline capacity, pipeline companies have proposed a number of major new transmission pipelines. Many of these became stalled in government regulatory processes, both by the Canadian and American governments. Another factor is competition for pipeline space from rapidly increasing tight oil production from North Dakota, which under NAFTA trade rules has equal access to Canadian pipelines.\nIn addition, there are a large number of new pipelines proposed for Alberta. These will likely be approved rapidly by the Alberta Energy Regulator, so there are likely to be few capacity problems within Alberta.\n\nThe movement of crude oil by rail is far from new, but it is now a rapidly growing market for North American railroads. The growth is driven by several factors. One is that the transmission pipelines from Alberta are operating at or near capacity and companies who cannot get pipeline space have to move oil by rail instead. Another is that many refineries on the east, west, and Gulf coasts of North America are under-served by pipelines since they assumed that they would obtain their oil by ocean tanker. Producers of new oil in Alberta, North Dakota, and West Texas are now shipping oil by rail to coastal refiners who are having difficulty obtaining international oil at prices competitive with those in the interior of North America. In addition, crude bitumen can be loaded directly into tank cars equipped with steam heating coils, avoiding the need for blending it with expensive condensate in order to ship it to market. Tank cars can also be built to transport condensate on the back-haul from refineries to the oil sands to make additional revenue rather than returning empty.\n\nA single-track rail line carrying 10 trains per day, each with 120 tank cars, can move to , which is the capacity of a large transmission pipeline. This would require 300 locomotives and 18,000 tank cars, which is a small part of the fleet of a Class 1 railroad. By comparison, the two Canadian Class 1 railways, Canadian Pacific Railway (CP) and Canadian National Railway (CN), have 2,400 locomotives and 65,000 freight cars between them, and CP moves 30–35 trains per day on its main line to Vancouver. Two US Class 1 railways, Union Pacific Railroad (UP) and BNSF Railway handle more than 100 trains per day on their western corridors. CN Rail has said that it could move of bitumen from Edmonton to the deepwater port of Prince Rupert, BC if the Northern Gateway Pipeline from Edmonton to the port of Kitimat, BC was not approved.\n\nWith many of their lines being underused, railroads find transporting crude oil an attractive source of revenue. With enough new tank cars they could carry all the new oil being produced in North America, albeit at higher prices than pipelines. In the short term, the use of rail will probably continue to grow as producers try to bypass short-term pipeline bottlenecks to take advantage of higher prices in areas with refineries capable of handling heavier crudes. In the long term the growth in rail transport will largely depend on the continued pipeline bottlenecks due to increased production in North America and regulatory delays for new pipelines. At present rail moves over of crude oil, and with continued growth in oil production and building of new terminals, rail movements will probably continue to grow into the foreseeable future.\n\nBy 2013, exports of oil from Canada to the US by rail had increased 9-fold in less than two years, from in early 2012 to in late 2013, mainly because new export pipelines had been held up by regulatory delays. As a result, Canadian farmers suffered an acute shortage of rail capacity to export their grains because so much of Canada's rail capacity was tied up by oil products. The safety of rail transport of oil was being called into question after several derailments, especially after a train with 74 tank cars of oil derailed and caught fire in Lac Megantic, Quebec.\n\nThe ensuing explosion and firestorm burned down 40 buildings in the town center and killed 47 people. The cleanup of the derailment area could take 5 years, and another 160 buildings may need to be demolished. Ironically, the oil was not Canadian bitumen being exported to the United States but Bakken formation light crude oil being imported into Canada from North Dakota to the Irving Oil Refinery in New Brunswick. Although near a huge oil import port on the Atlantic Ocean, the Irving refinery is importing US Bakken oil by rail because oil from outside North America is too expensive to be economic, and there are no pipelines to deliver heavier but cheaper Western Canadian oil to New Brunswick. It was subsequently pointed out that the Bakken light oil was much more flammable than Alberta bitumen, and the rail cars were mislabeled by the North Dakota producers as to their flammability.\n\nBy 2014, the movement of crude by rail had become very profitable to oil companies. Suncor Energy, Canada's largest oil company declared record profits and attributed much of it to transporting oil to market by rail. It was moving about to Cushing, Oklahoma, and putting it into TransCanada's new Gulf Coast pipeline – which was originally going to be the southern leg of the Keystone XL pipeline, before the northern leg across the border from Canada was stalled by US federal government delays.\n\nSuncor has also been moving of Alberta bitumen and North Dakota tight oil by rail to its Montreal Refinery with plans to increase it to . Suncor claimed this saved about $10/bbl off the price of buying offshore oil. However, it was also anticipating the reversal of Enbridge's Line 9 from southwestern Ontario to Montreal to deliver oil even cheaper. Suncor has been considering adding a coker to its Montreal refinery to upgrade heavy oil sands bitumen, which would be cheaper than adding another upgrader to its oil sands operation. It was also shipping marine cargoes on an \"opportunistic basis\" from Texas and Louisiana \"at significant discounts to the international crudes we would typically run in Montreal\", thereby taking advantage of the recent US tight oil glut in addition to increased supplies of cheap Canadian oil sands bitumen.\n\nHeavy crude oil feedstock#crude feedstock needs pre-processing before it is fit for conventional refineries, although heavy oil and bitumen refineries can do the pre-processing themselves. This pre-processing is called 'upgrading', the key components of which are as follows:\n\nAs carbon rejection is very inefficient and wasteful in most cases, catalytic hydrocracking is preferred in most cases. All these processes take large amounts of energy and water, while emitting more carbon dioxide than conventional oil.\n\nCatalytic purification and hydrocracking are together known as hydroprocessing. The big challenge in hydroprocessing is to deal with the impurities found in heavy crude, as they poison the catalysts over time. Many efforts have been made to deal with this to ensure high activity and long life of a catalyst. Catalyst materials and pore size distributions are key parameters that need to be optimized to deal with this challenge and varies from place to place, depending on the kind of feedstock present.\n\nThere are four major oil refineries in Alberta which supply most of Western Canada with petroleum products, but as of 2012 these processed less than 1/4 of the approximately of bitumen and SCO produced in Alberta. Some of the large oil sands upgraders also produced diesel fuel as part of their operations. Some of the oil sands bitumen and SCO went to refineries other provinces, but most of it was exported to the United States. The four major Alberta refineries are:\n\nThe $8.5-billion Sturgeon Refinery, a fifth major Alberta refinery, is under construction near Fort Saskatchewan with a completion date of 2017. The proponents are Alberta Petroleum Marketing Commission, Canadian Natural Resources Limited and North West Upgrading Inc. NWU, which was founded in 2004, is a private, Alberta-based company with headquarters in Calgary. Canadian Natural Resources Limited 50/50 entered into a joint venture partnership with NWU in February 2011 forming North West Redwater Partnership. This is the first oil refinery to be constructed in Alberta in thirty years – the last was Shell’s Scotford refinery which was completed in 1984. The Sturgeon Refinery is the \"first new refinery to be built with a carbon capture and storage system.\" The plant is designed to convert up to of crude bitumen directly to diesel fuel. \"In addition to producing ultra low-sulphur diesel and naphtha, the project will capture carbon dioxide which will be sold to Enhance Energy’s Alberta Carbon Trunk Line for use in enhanced oil recovery.\" The refinery will process bitumen into diesel fuel not SCO so it is more of an upgrader than a refinery. A petroleum coker is required to upgrade the raw product before it can be made into diesel.\"\n\nBy June 2014 the estimated cost of construction had increased from $5.7 billion to $8.5 billion – or $170,000 per barrel of new capacity.\n\nThe Alberta government has guaranteed NWU's loans and signed a firm contract for feedstock deliveries because of some economic issues. Alberta levies royalties on bitumen at \"before payout\" (2%) and \"after payout\" (25%) rates, and accepts payments \"in kind\" rather than \"in cash.\" (BRIK), Alberta will receive 300,000 bpd of bitumen under this BRIK program. With bitumen production expected to reach by 2035, it means that after the projects pay out, the Alberta government will have of bitumen to sell. Since Alberta has a chronic shortage of diesel fuel, the government would prefer to sell diesel fuel rather than bitumen to Alberta and international oil companies.\n\nThe Pacific Future Energy project proposes a new refinery in British Columbia that would bring in Western Canadian bitumen and process it into fuels for Asian and Canadian markets. Pacific Future Energy proposes to transport near-solid bitumen to the refinery using railway tank cars.\n\nCanadian oil exports have increased tenfold since 1980, mostly as the result of new oil sands bitumen and heavy oil output, but at the same time Canadian oil consumption and refining capacity has hardly grown at all. Since the 1970s, the number of oil refineries in Canada has declined from 40 to 19. There hasn't been a new oil refinery (other than oil sands upgraders) built in Canada since 1984.\n\nMost of the Canadian oil refining industry is foreign-owned, and except for Alberta, international companies preferred to build refining capacity elsewhere than in Canada. The result is a serious imbalance between Canadian oil production versus Canadian oil refining. Although Canada produces much more oil than it refines, and exports more oil and refined products than it consumes, most of the new production is heavier than traditional oil and concentrated in the landlocked provinces of Alberta and Saskatchewan. Canadian refineries have pipeline access to and can process only about 25% of the oil produced in Canada. The remainder of Canadian oil production is exported, almost all of it to the US. At the same time Canada imports of crude oil from other countries and exports much of the oil products to other countries, most of it to the US.\n\nCanadian refineries, outside of the major oil producing provinces of Alberta and Saskatchewan, were originally built on the assumption that light and medium crude oil would continue to be cheap in the long term, and that imported oil would be cheaper than oil sands production. With new oil sands production coming on production at lower prices than international oil, market price imbalances have ruined the economics of refineries which could not process it. Most of the Canadian oil refineries which closed were in the oil deficient regions of Quebec, the Atlantic Provinces, and British Columbia where they had no access to cheaper domestic Canadian production. They also were not designed to refine the heavier grades which comprised most new Canadian production. These refinery closures were part of an international trend, since about a dozen refineries in Europe, the Caribbean and along the US east coast have shut down recent years due to sharp increases in the cost of imported oil and declining domestic demand for fuel.\n\nPrior to 2013, when China surpassed it, the United States was the largest oil importer in the world. Unlike Canada, the US has hundreds of oil refineries, many of which have been modified to process heavy oil as US production of light and medium oil declined. The main market for Canadian bitumen as well as Venezuelan extra-heavy oil was assumed to be the US. The United States has historically been Canada’s largest customer for crude oil and products, particularly in recent years. American imports of oil and products from Canada grew from in 1981 to in 2013 as Canada's oil sands produced more and more oil, while in the US, domestic production and imports from other countries declined. However, this relationship is becoming strained due to physical, economic and political influences. Export pipeline capacity is approaching its limits; Canadian oil is selling at a discount to world market prices; and US demand for crude oil and product imports has declined because of US economic problems.\n\nFor the benefit of oil marketers, in 2004 Western Canadian producers created a new benchmark crude oil called Western Canadian Select, (WCS), a bitumen-derived heavy crude oil blend that is similar in its transportation and refining characteristics to California, Mexico Maya, or Venezuela heavy crude oils. This heavy oil has an API gravity of 19–21 and despite containing large amounts of bitumen and synthetic crude oil, flows through pipelines well and is classified as \"conventional heavy oil\" by governments. There are several hundred thousand barrels per day of this blend being imported into the US, in addition to larger amounts of crude bitumen and synthetic crude oil (SCO) from the oil sands.\n\nThe demand from US refineries is increasingly for non-upgraded bitumen rather than SCO. The Canadian National Energy Board (NEB) expects SCO volumes to double to around by 2035, but not keep pace with the total increase in bitumen production. It projects that the portion of oil sands production that is upgraded to SCO to decline from 49% in 2010 to 37% in 2035. This implies that over of bitumen will have to be blended with diluent for delivery to market.\n\nDemand for oil in Asia has been growing much faster than in North America or Europe. In 2013, China replaced the United States as the world's largest importer of crude oil, and its demand continues to grow much faster than its production. The main impediment to Canadian exports to Asia is pipeline capacity – The only pipeline capable of delivering oil sands production to Canada's Pacific Coast is the Trans Mountain Pipeline from Edmonton to Vancouver, which is now operating at its capacity of supplying refineries in B.C. and Washington State. However, once complete, the Northern Gateway pipeline and the Trans Mountain expansion currently undergoing government review are expected to deliver an additional to to tankers on the Pacific coast, from where they could deliver it anywhere in the world. There is sufficient heavy oil refinery capacity in China and India to refine the additional Canadian volume, possibly with some modifications to the refineries. In recent years, Chinese oil companies such as China Petrochemical Corporation (Sinopec), China National Offshore Oil Corporation (CNOOC), and PetroChina have bought over $30 billion in assets in Canadian oil sands projects, so they would probably like to export some of their newly acquired oil to China.\n\nThe world's largest deposits of bitumen are in Canada, although Venezuela's deposits of extra-heavy crude oil are even bigger. Canada has vast energy resources of all types and its oil and natural gas resource base would be large enough to meet Canadian needs for generations if demand was sustained. Abundant hydroelectric resources account for the majority of Canada's electricity production and very little electricity is produced from oil. In a scenario with oil prices above US$100, Canada would have more than enough energy to meet its growing needs, with the excess oil production from its oil sands probably going to export. The major importing country would probably continue to be the United States, although before the developments in 2014, there was increasing demand for oil, particularly heavy oil, from Asian countries such as China and India.\n\nCanada has abundant resources of bitumen and crude oil, with an estimated remaining ultimate resource potential of 54 billion cubic metres (340 billion barrels). Of this, oil sands bitumen accounts for 90 per cent. Alberta currently accounts for all of Canada’s bitumen resources. Resources become reserves only after it is proven that economic recovery can be achieved. At 2013 prices using current technology, Canada had remaining oil reserves of 27 billion m (170 billion bbls), with 98% of this attributed to oil sands bitumen. This put its reserves in third place in the world behind Venezuela and Saudi Arabia. At the much lower prices of 2015, the reserves are much smaller.\n\nThe costs of production and transportation of saleable petroleum from oil sands is typically significantly higher than from conventional global sources. Hence the economic viability of oil sands production is more vulnerable to the price of oil. The price of benchmark West Texas Intermediate (WTI) oil at Cushing, Oklahoma above US$100/bbl that prevailed until late 2014 was sufficient to promote active growth in oil sands production. Major Canadian oil companies had announced expansion plans and foreign companies were investing significant amounts of capital, in many cases forming partnerships with Canadian companies. Investment had been shifting towards in-situ steam assisted gravity drainage (SAGD) projects and away from mining and upgrading projects, as oil sands operators foresee better opportunities from selling bitumen and heavy oil directly to refineries than from upgrading it to synthetic crude oil. Cost estimates for Canada include the effects of the mining when the mines are returned to the environment in \"as good as or better than original condition\". Cleanup of the end products of consumption are the responsibility of the consuming jurisdictions, which are mostly in provinces or countries other than the producing one.\n\nThe Alberta government estimated that in 2012, the supply cost of oil sands new mining operations was $70 to $85 per barrel, whereas the cost of new SAGD projects was $50 to $80 per barrel. These costs included capital and operating costs, royalties and taxes, plus a reasonable profit to the investors. Since the price of WTI rose to $100/bbl beginning in 2011, production from oil sands was then expected to be highly profitable assuming the product could be delivered to markets. The main market was the huge refinery complexes on the US Gulf Coast, which are generally capable of processing Canadian bitumen and Venezuelan extra-heavy oil without upgrading.\n\nThe Canadian Energy Research Institute (CERI) performed an analysis, estimating that in 2012 the average plant gate costs (including 10% profit margin, but excluding blending and transport) of primary recovery was $30.32/bbl, of SAGD was $47.57/bbl, of mining and upgrading was $99.02/bbl, and of mining without upgrading was $68.30/bbl. Thus, all types of oil sands projects except new mining projects with integrated upgraders were expected to be consistently profitable from 2011 onward, provided that global oil prices remained favourable. Since the larger and more sophisticated refineries preferred to buy raw bitumen and heavy oil rather than synthetic crude oil, new oil sands projects avoided the costs of building new upgraders. Although primary recovery such as is done in Venezuela is cheaper than SAGD, it only recovers about 10% of the oil in place versus 60% or more for SAGD and over 99% for mining. Canadian oil companies were in a more competitive market and had access to more capital than in Venezuela, and preferred to spend that extra money on SAGD or mining to recover more oil.\n\nThen in late 2014 the dramatic rise in U.S. production from shale formations, combined with a global economic malaise that reduced demand, caused the price of WTI to drop below $50, where it remained as of late 2015. \nIn 2015, the Canadian Energy Research Institute (CERI) re-estimated the average plant gate costs (again including 10% profit margin) of SAGD to be $58.65/bbl, and 70.18/bbl for mining without upgrading. Including costs of blending and transportation, the WTI equivalent supply costs for delivery to Cushing become US$80.06/bbl for SAGD projects, and US$89.71/bbl for a standalone mine.\nIn this economic environment, plans for further development of production from oil sands have been slowed or deferred, \nProduction of synthetic crude from mining operations continue at a loss because of the costs of shutdown and restart, as well as commitments to supply contracts.\n\nOil sands production forecasts released by the Canadian Association of Petroleum Producers (CAPP), the Alberta Energy Regulator (AER), and the Canadian Energy Research Institute (CERI) are comparable to National Energy Board (NEB) projections, in terms of total bitumen production. None of these forecasts take into account probable international constraints to be imposed on combustion of all hydrocarbons in order to limit global temperature rise, giving rise to a situation denoted by the term \"carbon bubble\". Ignoring such constraints, and also assuming that the price of oil recovers from its collapse in late 2014, the list of currently proposed projects, many of which are in the early planning stages, would suggest that by 2035 Canadian bitumen production could potentially reach as much as 1.3 million m/d (8.3 million barrels per day) if most were to go ahead. Under the same assumptions, a more likely scenario is that by 2035, Canadian oil sands bitumen production would reach 800,000 m/d (5.0 million barrels/day), 2.6 times the production for 2012. The majority of the growth would likely occur in the in-situ category, as in-situ projects usually have better economics than mining projects. Also, 80% of Canada's oil sands reserves are well-suited to in-situ extraction, versus 20% for mining methods.\n\nAn additional assumption is that there would be sufficient pipeline infrastructure to deliver increased Canadian oil production to export markets. If this were a limiting factor, there could be impacts on Canadian crude oil prices, constraining future production growth. Another assumption is that US markets will continue to absorb increased Canadian exports. Rapid growth of tight oil production in the US, Canada's primary oil export market, has greatly reduced US reliance on imported crude. The potential for Canadian oil exports to alternative markets such as Asia is also uncertain. There are increasing political obstacles to building any new pipelines to deliver oil in Canada and the US. In November 2015, U.S. President Barack Obama rejected the proposal to build the Keystone XL pipeline from Alberta to Steele City, Nebraska. \nIn the absence of new pipeline capacity, companies are increasingly shipping bitumen to US markets by railway, river barge, tanker, and other transportation methods. Other than ocean tankers, these alternatives are all more expensive than pipelines.\n\nA shortage of skilled workers in the Canadian oil sands developed during periods of rapid development of new projects. In the absence of other constraints on further development, the oil and gas industry would need to fill tens of thousands of job openings in the next few years as a result of industry activity levels as well as age-related attrition. In the longer term, under a scenario of higher oil and gas prices, the labor shortages would continue to get worse. A potential labor shortage can increase construction costs and slow the pace of oil sands development.\n\nThe skilled worker shortage was much more severe in Venezuela because the government controlled oil company PDVSA fired most of its heavy oil experts after the Venezuelan general strike of 2002–03, and wound down the production of Orimulsion, which was the primary product from its oil sands. Following that, the government re-nationalized the Venezuelan oil industry and increased taxes on it. The result was that foreign companies left Venezuela, as did most of its elite heavy oil technical experts. In recent years, Venezuela's heavy oil production has been falling, and it has consistently been failing to meet its production targets.\n\nAs of late 2015, development of new oil sand projects were deterred by the price of WTI below US$50, which is barely enough to support production from existing operations. Demand recovery was suppressed by economic problems that may continue indefinitely to bedevil both the European Community and China. Low-cost production by OPEC continued at maximum capacity, efficiency of production from U.S. shales continued to improve, and Russian exports were mandated even below cost of production, as their only source of hard currency. There is also the possibility that there will emerge an international agreement to introduce measures to constrain the combustion of hydrocarbons in an effort to limit global temperature rise to the nominal 2 °C that is consensually predicted to limit environmental harm to tolerable levels. Rapid technological progress is being made to reduce the cost of competing renewable sources of energy. Hence there is no consensus about when, if ever, oil prices paid to producers may substantially recover.\n\nA detailed academic study of the consequences for the producers of the various hydrocarbon fuels concluded in early 2015 that a third of global oil reserves, half of gas reserves and over 80% of current coal reserves should remain underground from 2010 to 2050 in order to meet the target of 2 °C. Hence continued exploration or development of reserves would be extraneous to needs. To meet the 2 °C target, strong measures would be needed to suppress demand, such as a substantial carbon tax leaving a lower price for the producers from a smaller market. The impact on producers in Canada would be far larger than in the U.S. Open-pit mining of natural bitumen in Canada would soon drop to negligible levels after 2020 in all scenarios considered because it is considerably less economic than other methods of production.\n\nIn their 2011 commissioned report entitled \"Prudent Development: Realizing the Potential of North America’s Abundant Natural Gas and Oil Resources,\" the National Petroleum Council, an advisory committee to the U.S. Secretary of Energy, acknowledged health and safety concerns regarding the oil sands which include \"volumes of water needed to generate issues of water sourcing; removal of overburden for surface mining can fragment wildlife habitat and increase the risk of soil erosion or surface run-off events to nearby water systems; GHG and other air emissions from production.\"\n\nOil sands extraction can affect the land when the bitumen is initially mined, water resources by its requirement for large quantities of water during separation of the oil and sand, and the air due to the release of carbon dioxide and other emissions. Heavy metals such as vanadium, nickel, lead, cobalt, mercury, chromium, cadmium, arsenic, selenium, copper, manganese, iron and zinc are naturally present in oil sands and may be concentrated by the extraction process. The environmental impact caused by oil sand extraction is frequently criticized by environmental groups such as Greenpeace, Climate Reality Project, Pembina Institute, 350.org, MoveOn.org, League of Conservation Voters, Patagonia, Sierra Club, and Energy Action Coalition. In particular, mercury contamination has been found around oil sands production in Alberta, Canada. The European Union has indicated that it may vote to label oil sands oil as \"highly polluting\". Although oil sands exports to Europe are minimal, the issue has caused friction between the EU and Canada. According to the California-based Jacobs Consultancy, the European Union used inaccurate and incomplete data in assigning a high greenhouse gas rating to gasoline derived from Alberta’s oilsands. Also, Iran, Saudi Arabia, Nigeria and Russia do not provide data on how much natural gas is released via flaring or venting in the oil extraction process. The Jacobs report pointed out that extra carbon emissions from oil-sand crude are 12 percent higher than from regular crude, although it was assigned a GHG rating 22% above the conventional benchmark by EU.\n\nIn 2014 results of a study published in the Proceedings of the National Academy of Sciences showed that official reports on emissions were not high enough. Report authors noted that, \"emissions of organic substances with potential toxicity to humans and the environment are a major concern surrounding the rapid industrial development in the Athabasca oil sands region (AOSR).\" This study found that tailings ponds were an indirect pathway transporting uncontrolled releases of evaporative emissions of three representative polycyclic aromatic hydrocarbon (PAH)s (phenanthrene, pyrene, and benzo(a)pyrene) and that these emissions had been previously unreported.\n\nThe Alberta government computes an Air Quality Health Index (AQHI) from sensors in five communities in the oil sands region, operated by a \"partner\" called the Wood Buffalo Environmental Association (WBEA). Each of their 17 continuously monitoring stations measure 3 to 10 air quality parameters among carbon monoxide (CO), hydrogen sulfide (), total reduced sulfur (TRS), Ammonia (), nitric oxide (NO), nitrogen dioxide (), nitrogen oxides (NO), ozone (), particulate matter (PM2.5), sulfur dioxide (), total hydrocarbons (THC), and methane/non-methane hydrocarbons (/NMHC). These AQHI are said to indicate 'low risk\"'air quality more than 95% of the time. Prior to 2012, air monitoring showed significant increases in exceedances of hydrogen sulfide () both in the Fort McMurray area and near the oil sands upgraders. In 2007, the Alberta government issued an environmental protection order to Suncor in response to numerous occasions when ground level concentration for ) exceeded standards. The Alberta Ambient Air Data Management System (AAADMS) of the Clean Air Strategic Alliance (aka CASA Data Warehouse) records that, during the year ending on 1 November 2015, there were 6 hourly reports of values exceeding the limit of 10 ppb for , and 4 in 2013, down from 11 in 2014, and 73 in 2012.\n\nIn September 2015, the Pembina Institute published a brief report about \"a recent surge of odour and air quality concerns in northern Alberta associated with the expansion of oilsands development\", contrasting the responses to these concerns in Peace River and Fort McKay. In Fort McKay, air quality is actively addressed by stakeholders represented in the WBEA, whereas the Peace River community must rely on the response of the Alberta Energy Regulator. In an effort to identify the sources of the noxious odours in the Fort McKay community, a Fort McKay Air Quality Index was established, extending the provincial Air Quality Health Index to include possible contributors to the problem: , TRS, and THC. Despite these advantages, more progress was made in remediating the odour problems in the Peace River community, although only after some families had already abandoned their homes. The odour concerns in Fort McKay were reported to remain unresolved.\n\nA large part of oil sands mining operations involves clearing trees and brush from a site and removing the overburden— topsoil, muskeg, sand, clay and gravel – that sits atop the oil sands deposit. Approximately 2.5 tons of oil sands are needed to produce one barrel of oil (roughly ⅛ of a ton). \nAs a condition of licensing, projects are required to implement a reclamation plan. The mining industry asserts that the boreal forest will eventually colonize the reclaimed lands, but their operations are massive and work on long-term timeframes. As of 2013, about of land in the oil sands region have been disturbed, and of that land is under reclamation. In March 2008, Alberta issued the first-ever oil sands land reclamation certificate to Syncrude for the parcel of land known as Gateway Hill approximately north of Fort McMurray. Several reclamation certificate applications for oil sands projects are expected within the next 10 years.\n\nBetween 2 and 4.5 volume units of water are used to produce each volume unit of synthetic crude oil in an \"ex-situ\" mining operation. According to Greenpeace, the Canadian oil sands operations use of water, twice the amount of water used by the city of Calgary. However, in SAGD operations, 90–95% of the water is recycled and only about 0.2 volume units of water is used per volume unit of bitumen produced.\n\nFor the Athabasca oil sand operations water is supplied from the Athabasca River, the ninth longest river in Canada. The average flow just downstream of Fort McMurray is with its highest daily average measuring . Oil sands industries water license allocations totals about 1.8% of the Athabasca river flow. Actual use in 2006 was about 0.4%. In addition, according to the Water Management Framework for the Lower Athabasca River, during periods of low river flow water consumption from the Athabasca River is limited to 1.3% of annual average flow.\n\nIn December 2010, the Oil Sands Advisory Panel, commissioned by former environment minister Jim Prentice, found that the system in place for monitoring water quality in the region, including work by the Regional Aquatic Monitoring Program, the Alberta Water Research Institute, the Cumulative Environmental Management Association and others, was piecemeal and should become more comprehensive and coordinated.\n\nThe production of bitumen and synthetic crude oil emits more greenhouse gases than the production of conventional crude oil. A 2009 study by the consulting firm IHS CERA estimated that production from Canada's oil sands emits \"about 5% to 15% more carbon dioxide, over the\n\"well-to-wheels\" (WTW) lifetime analysis of the fuel, than average crude oil.\" Author and investigative journalist David Strahan that same year stated that IEA figures show that carbon dioxide emissions from the oil sands are 20% higher than average emissions from the petroleum production.\n\nA Stanford University study commissioned by the EU in 2011 found that oil sands crude was as much as 22% more carbon intensive than other fuels.\n\nGreenpeace says the oil sands industry has been identified as the largest contributor to greenhouse gas emissions growth in Canada, as it accounts for 40 million tons of emissions per year.\n\nAccording to the Canadian Association of Petroleum Producers and Environment Canada the industrial activity undertaken to produce oil sands make up about 5% of Canada's greenhouse gas emissions, or 0.1% of global greenhouse gas emissions. It predicts the oil sands will grow to make up 8% of Canada's greenhouse gas emissions by 2015. While the production industrial activity emissions per barrel of bitumen produced decreased 26% over the decade 1992–2002, total emissions from production activity were expected to increase due to higher production levels. As of 2006, to produce one barrel of oil from the oil sands released almost of greenhouse gases with total emissions estimated to be per year by 2015. A study by IHS CERA found that fuels made from Canadian oil sands resulted in significantly lower greenhouse gas emissions than many commonly cited estimates. A 2012 study by Swart and Weaver estimated that if only the economically viable reserve of oil sands was burnt, the global mean temperature would increase by 0.02 to 0.05 °C. If the entire oil-in-place of 1.8 trillion barrels were to be burnt, the predicted global mean temperature increase is 0.24 to 0.50 °C. Bergerson et al. found that while the WTW emissions can be higher than crude oil, \"the lower emitting oil sands cases can outperform higher emitting conventional crude cases\".\n\nTo offset greenhouse gas emissions from the oil sands and elsewhere in Alberta, sequestering carbon dioxide emissions inside depleted oil and gas reservoirs has been proposed. This technology is inherited from enhanced oil recovery methods. In July 2008, the Alberta government announced a C$2 billion fund to support sequestration projects in Alberta power plants and oil sands extraction and upgrading facilities.\n\nIn November 2014, Fatih Birol, the chief economist of the International Energy Agency, described additional greenhouse gas emissions from Canada's oil sands as \"extremely low\". The IEA forecasts that in the next 25 years oil sands production in Canada will increase by more than , but Dr. Birol said \"the emissions of this additional production is equal to only 23 hours of emissions of China — not even one day.\" The IEA is charged with responsibility for battling climate change, but Dr. Birol said he spends little time worrying about carbon emissions from oil sands. \"There is a lot of discussion on oil sands projects in Canada and the United States and other parts of the world, but to be frank, the additional CO2 emissions coming from the oil sands is extremely low.\" Dr. Birol acknowledged that there is tremendous difference of opinion on the course of action regarding climate change, but added, \"I hope all these reactions are based on scientific facts and sound analysis.\"\n\nIn 2014, the U.S. Congressional Research Service published a report in preparation for the decision about permitting construction of the Keystone XL pipeline. The report states in part: \"Canadian oil sands crudes are generally more GHG emission-intensive than other crudes they may displace in U.S. refineries, and emit an estimated 17% more GHGs on a life-cycle basis than the average barrel of crude oil refined in the United States\".\n\nThere is conflicting research on the effects of the oil sands development on aquatic life. In 2007, Environment Canada completed a study that shows high deformity rates in fish embryos exposed to the oil sands. David W. Schindler, a limnologist from the University of Alberta, co-authored a study on Alberta's oil sands' contribution of aromatic polycyclic compounds, some of which are known carcinogens, to the Athabasca River and its tributaries. Scientists, local doctors, and residents supported a letter sent to the Prime Minister in September 2010 calling for an independent study of Lake Athabasca (which is downstream of the oil sands) to be initiated due to the rise of deformities and tumors found in fish caught there.\n\nThe bulk of the research that defends the oil sands development is done by the Regional Aquatics Monitoring Program (RAMP). RAMP studies show that deformity rates are normal compared to historical data and the deformity rates in rivers upstream of the oil sands.\n\nIn 2007, it was suggested that wildlife has been negatively affected by the oil sands; for instance, moose were found in a 2006 study to have as high as 453 times the acceptable levels of arsenic in their systems, though later studies lowered this to 17 to 33 times the acceptable level (although below international thresholds for consumption).\n\nConcerns have been raised concerning the negative impacts that the oil sands have on public health, including higher than normal rates of cancer among residents of Fort Chipewyan. However, John O'Connor, the doctor who initially reported the higher cancer rates and linked them to the oil sands development, was subsequently investigated by the Alberta College of Physicians and Surgeons. The College later reported that O'Connor's statements consisted of \"mistruths, inaccuracies and unconfirmed information.\"\n\nIn 2010, the Royal Society of Canada released a report stating that \"there is currently no credible evidence of environmental contaminant exposures from oil sands reaching Fort Chipewyan at levels expected to cause elevated human cancer rates.\"\n\nIn August 2011, the Alberta government initiated a provincial health study to examine whether a link exists between the higher rates of cancer and the oil sands emissions.\n\nIn a report released in 2014, Alberta’s Chief Medical Officer of Health, Dr. James Talbot, stated that \"There isn’t strong evidence for an association between any of these cancers and environmental exposure [to tar sands].\" Rather, Talbot suggested that the cancer rates at Fort Chipewyan, which were slightly higher compared with the provincial average, were likely due to a combination of factors such as high rates of smoking, obesity, diabetes, and alcoholism as well as poor levels of vaccination.\"\n\n"}
{"id": "35282774", "url": "https://en.wikipedia.org/wiki?curid=35282774", "title": "Olympic Airways Flight 954", "text": "Olympic Airways Flight 954\n\nOlympic Airways Flight 954 was a Douglas DC-6B aircraft that crashed into a mountain near Keratea (approx. coordinates 37°48' N, 23°57' E), Greece. All 85 passengers and 5 crew on board died in the crash.\n\nThe flight was a domestic scheduled passenger service from Chania on the island of Crete to Athens. While on approach to Athens and with its undercarriage retracted, the aircraft struck Mount Paneio at an altitude of approximately 2,000 feet. The weather at the time of the crash was characterized by rain and high winds.\n\nThe crash of Flight 954 was the deadliest aviation accident in Greek history at the time it took place, a record it maintained until the crash of Helios Airways Flight 522 nearly thirty-six years later. It is still the deadliest aviation accident involving a Douglas DC-6., and deadliest crash in the history of Olympic Airways.\n\nIt was ruled that the flight crew had deviated from the proper track and descended below the minimum safe altitude while making an ILS approach.\n\n"}
{"id": "41435295", "url": "https://en.wikipedia.org/wiki?curid=41435295", "title": "Plug-in electric vehicles in Sweden", "text": "Plug-in electric vehicles in Sweden\n\nThe adoption of plug-in electric vehicles in Sweden is actively supported by the Government of the Kingdom of Sweden. , a total of 50,304 plug-in electric vehicles have been registered in Sweden since 2011, consisting of 36,405 plug-in hybrids, 12,223 all-electric cars and 1,676 all-electric utility vans. The Swedish plug-in segment is dominated by plug-in hybrids, representing 72.4% of the Swedish light-duty plug-in electric vehicle registrations through December 2016.\n\nSweden has ranked among the world's top ten best-selling plug-in markets for two years running, 2015 and 2016, listed in both years as the ninth largest country market. , the Swedish stock of plug-in cars and vans is the sixth largest in Europe. The market share of plug-in electric vehicles climbed from 0.57% in 2013 to 1.53% of new car sales in the country in 2014. The segment market share reached 2.5% in 2015, rose to 3.5% in 2016, and achieved a record of 5.2% in 2017.\n\nIn September 2011 the Swedish government approved a program, effective starting in January 2012, to provide a subsidy of per car for the purchase of 5,000 electric cars and other \"super green cars\" with ultra-low carbon emissions, defined as those with emissions below 50 grams of carbon dioxide () per km. \nThere is also an exemption from the annual circulation tax for the first five years from the date of their first registration that benefits owners of electric vehicles with an energy consumption of 37 kWh per 100 km or less, and hybrid vehicles with emissions of 120 g/km or less. In addition, for both electric and hybrid vehicles, the taxable value of the car for the purposes of calculating the benefit in kind of a company car under personal income tax is reduced by 40% compared with the corresponding or comparable gasoline or diesel-powered car. \nThe reduction of the taxable value has a cap of per year.\n\nBy July 2014 the program run out of funds as a total of 5,028 new \"super clean cars\" had been registered in the country since January 2012. \nBIL Sweden, the national association for the automobile industry, requested the government an additional to cover the subsidy for another 2,500 registrations of new super clean cars between August and December 2014. \nIn December 2014 the Riksdagen, the Swedish parliament, approved an appropriation of to finance the super clean car subsidies in 2015. The appropriation for 2015, according to the parliamentary decision and subsequent government decision, was to also be used for the retroactive payment of the super green cars registered in 2014 that did not receive the subsidy.\n\nThe Government raised the appropriation for the super green car rebate by for 2015 and by for 2016. \nBeginning in 2016, only zero emissions cars are entitled to receive the full premium, while other super green cars, plug-in hybrids, receive half premium. \nThe exemption for the first five years of ownership from the annual circulation tax is still in place. In 2016, in order to promote the introduction of electricity-powered buses in the market, the Government planned to allocate for 2016 and per year between 2017 and 2019 to introduce an electric bus premium.\n\nTwo alternative proposals are being considered by the Swedish government regarding the introduction of a bonus-malus system. Both proposals entail changes to vehicle and car benefit taxation and the premium system for purchases of new cars. An official inquiry report was due by 29 April 2016. The goal is for the system to enter into force on 1 January 2017.\n\n, a total of 50,304 light-duty plug-in vehicles have been registered since 2011, consisting of 36,405 plug-in hybrids, 12,223 all-electric cars, and 1,676 all-electric vans. The plug-in segment has been dominated by plug-in hybrids, which represent 74.9% of plug-in car registrations through 2017. The market share of plug-in electric vehicles climbed from 0.57% in 2013 to 1.53% of new car sales in the country in 2014. The segment market share reached 2.5% in 2015, rose to 3.5% in 2016, and achieved a record of 5.2% in 2017.\n\nA total of 178 all-electric cars were registered in Sweden in 2011, and registrations of plug-in electric vehicles climbed to 928 units in 2012, led by the Toyota Prius Plug-in Hybrid with 499 units, followed by the Nissan Leaf with 129 units, and the third place was shared by the Volvo C30 Electric and the Opel Ampera with 88 units each. Electric-drive cars reached a market share of 0.33% in 2012. In addition, 265 Renault Kangoo Z.E. utility vans were sold in 2012. During 2013 a total of 1,545 plug-in electric cars were registered in the country out of 269,363 new passenger cars sold, representing a market share of 0.57%. With 1,113 units registered in 2013, plug-in hybrids represented 72.0% of total plug-in electric car registrations. This number includes 10 BMW i3s sold with the range extender option, which in Sweden are classified as plug-in hybrids. The top selling plug-in cars during 2013 were the Volvo V60 PHEV with 601 units, the Prius PHV with 376 and the Nissan Leaf with 317.\nPlug-in electric car sales during 2014 grew significantly. Registrations of super clean cars increased five-fold in July 2014 driven by the end of the quota of 5,000 new cars eligible for the super clean car subsidy. A total of 4,656 plug-in super clean passenger cars were registered in 2014, representing a 1.53% market share of new passenger cars registered in the country in 2014. Registrations of super clean cars were up 201% from 2013, while registrations of new passenger cars increased 12.7%. Super clean cars represented 8.8% of alternative fuel cars sold during 2014. The top selling plug-in electric cars in 2014 were the Mitsubishi Outlander P-HEV with 2,289 units, Volvo V60 PHEV with 745, and the Nissan Leaf with 438 units. The top selling all-electric utility van was the Kangoo Z.E. with 242 units out of a total of 282 electric vans registered.\n\nA total of 8,908 light-duty plug-in electric vehicles were registered in 2015, up 80% from 2014. The registered stock consisted of 5,625 plug-in hybrids, 2,962 all-electric cars and 321 all-electric utility vans. The plug-in segment had a market share of 2.49% of new car sales in 2015. The Mitsubishi Outlander P-HEV was the top selling plug-in car for a second year running with 3,302 units registered in 2015, followed by the Tesla Model S with 996 units. The top selling all-electric utility van was the Nissan e-NV200 with 168 units. , a total of 16,996 plug-in electric vehicles had registered in Sweden since 2011, up from 7,342 at the end of 2014 (131.5%).\n\nRegistrations totaled 13,454 light-duty plug-in electric vehicles in 2016, consisting 10,257 plug-in hybrids, up 16.7% from 2015, 2,924 all-electric cars, up 4.8% year-on-year, and 273 all-electric vans down 15.9% from 2015. Super clean car registrations totaled 12,995 units, up 51.4% from 2015. The plug-in electric car segment achieved a market share of 3.5% of all new cars registered in 2016, the world's third largest after Norway and the Netherlands. Stockholm county registered the highest proportion of super green car registrations during the first nine months of 2016, with 5.6% of the country's total. The proportion during September 2016 was 8.5%. In 2016 the Volkswagen Passat GTE listed as the top selling plug-in car with 3,804 units, followed by the Outlander P-HEV (1,819), Volvo V60 PHEV (1,239), Volvo XC90 T8 (983), Tesla Model S (838), and Nissan Leaf (836). The top selling all-electric utility van was the Renault Kangoo Z.E. with 171 units registered.\n\n, the all-time top selling plug-in electric cars are the Mitsubishi Outlander P-HEV with 7,506 units registered, followed by the Volkswagen Passat GTE (4,075), Volvo V60 PHEV (3,239), Nissan Leaf (2,561) and Tesla Model S (2,099). The Renault Kangoo Z.E. continued as the all-time the leader in the plug-in commercial utility segment with 1,024 units. The following table presents registrations of highway-capable plug-in electric passenger cars by model between January 2011 and September 2016.\n\nA total of 19,981 light-duty plug-in vehicles were registered in 2017, consisting of 15,447 plug-in hybrids, 4,217 all-electric cars, and 317 all-electric vans. Passenger car registrations totaled 379,393 units in 2017, of which, the plug-in segment captured a market share of 5.2%. Registrations of super green cars totaled 19,211 units, up from 12,995 in 2016. The market continues to be dominated by plug-in hybrids, representing 74.9% of plug-in car registrations between 2011 and 2017.\n\nThe top selling model in 2017 was the Volkswagen Passat GTE plug-in hybrid with 4,624 units registered, followed by the Mitsubishi Outlander PHEV with 2,452 units, Kia Optima PHEV with 1,534, the Volvo V60 N PHEV with 1,239, and the Nissan Leaf with 836 units. , the Outlander PHEV continues to rank as the all-time top selling plug-in electric car with 9,957 units registered.\n\n"}
{"id": "31326824", "url": "https://en.wikipedia.org/wiki?curid=31326824", "title": "Powerex (electricity)", "text": "Powerex (electricity)\n\nPowerex Corp. is the wholly owned energy marketing subsidiary of BC Hydro. Powerex buys and sells wholesale electricity, natural gas and environmental energy products and services in Western North America (WECC). In business since 1988, Powerex Corp. is headquartered in Vancouver, British Columbia. Powerex also markets the Canadian Entitlement energy from the Columbia River Treaty.\n\n"}
{"id": "6314549", "url": "https://en.wikipedia.org/wiki?curid=6314549", "title": "Renewable Transport Fuel Obligation", "text": "Renewable Transport Fuel Obligation\n\nThe Renewable Transport Fuel Obligation (RTFO) in the United Kingdom is a requirement on transport fuel suppliers to ensure that 5 percent of all road vehicle fuel is supplied from sustainable renewable sources by 2010. The Government intends to set variable targets for the level of carbon and sustainability performance expected from all transport fuel suppliers claiming certificates for biofuels in the early years of the RTFO.\n\nThe announcement to introduce the Obligation was made on 10 November 2005, using powers included in the Energy Act 2004. It came into force on 15 April 2008. In mid-2005, biofuel made up 0.25% of overall road fuel sales, around 50% of it imported.\n\nIn practice, the Renewable Transport Fuel Obligation will mostly be achieved by blending fossil fuels with bioethanol, biomethanol or biodiesel - derived from sources such as palm oil, oilseed rape, cereals, sugar cane, sugar beet, and reprocessed vegetable oil - or biomethane. Almost all existing vehicles are able to run on a 5% blend in liquid fuels, in some cases with fuel system adjustments. Natural gas vehicles can run on pure biomethane. Consideration should also be given to the effect of biofuel constituents on fuel system parts such as rubber hoses, particularly in older or classic vehicles. Flexible Fuel Vehicles, currently more common in the Americas than in Europe, are able to use blends of up to 85% ethanol in the U.S. and Canada, and up to 100% hydrous ethanol in Brazil.\n\nThe requirement that the biofuel sources should be sustainable is also important. In South America and Asia, the production of biofuels for export has in some cases resulted in significant ecological damage, including the clearing of rainforest.\n\nIn the future, the Department for Transport estimate that up to 1/3 of the fuel in the UK transport sector could be produced from home-grown biofuels. In addition to contributing to cutting carbon emissions, the use of biofuels is seen as widening the country's fuel diversity and increasing fuel security.\n\nThe RTFO will help bring the UK into line with European Union biofuels directive, which sets targets for all EU countries for biofuel usage of 2% by the end of 2005 and 5.75% by the end of 2010.\n\nThe RTFO will be implemented through a certification scheme administered by the Department for Transport. Companies certified as having sold more than the 5% obligation will be able to sell their certificates for the excess to those who sold less.\n\n\n\n"}
{"id": "33216232", "url": "https://en.wikipedia.org/wiki?curid=33216232", "title": "SeaTwirl", "text": "SeaTwirl\n\nSeaTwirl is a vertical floating wind turbine that was tested off the west coast of Sweden in 2011. It is also the name of the company that produces the turbine.\n\nThe design was developed by Daniel Ehrnberg. Chalmers University of Technology built a prototype and tested it at SSPA. A 1/50 scale third-generation anchored prototype rising 3 m above the water and reaching 7 m deep was installed off the west coast of Sweden near Halmstad in August 2011, tested successfully and de-commissioned.\n\nSeaTwirl has a vertical axis, with blades above water and the generator below. It is one of several designs that can float and therefore be positioned far offshore and take advantage of the stronger winds there. The SeaTwirl design uses the seawater itself as a roller bearing and stores energy in a water-filled torus. The company intends to use the design to enable the use of cheaper and heavier materials to lower the cost below conventional turbines, and generate energy even when the wind is not blowing.\n\nIf the design is scaled to 430 m long, the company claims that it could provide 4.5 MW of electricity and store 25 MWh of energy.\n\n\n"}
{"id": "7856656", "url": "https://en.wikipedia.org/wiki?curid=7856656", "title": "Six-stroke engine", "text": "Six-stroke engine\n\nThe term six-stroke engine has been applied to a number of alternative internal combustion engine designs that attempt to improve on traditional two-stroke and four-stroke engines. Claimed advantages may include increased fuel efficiency, reduced mechanical complexity and/or reduced emissions. These engines can be divided into two groups based on the number of pistons that contribute to the six strokes.\n\nIn the single-piston designs, the engine captures the heat lost from the four-stroke Otto cycle or Diesel cycle and uses it to drive an additional power and exhaust stroke of the piston in the same cylinder in an attempt to improve fuel-efficiency and/or assist with engine cooling. The pistons in this type of six-stroke engine go up and down three times for each injection of fuel. These designs use either steam or air as the working fluid for the additional power stroke.\n\nThe designs in which the six strokes are determined by the interactions between two pistons are more diverse. The pistons may be opposed in a single cylinder or may reside in separate cylinders. Usually one cylinder makes two strokes while the other makes four strokes giving six piston movements per cycle. The second piston may be used to replace the valve mechanism of a conventional engine, which may reduce mechanical complexity and enable an increased compression ratio by eliminating hotspots that would otherwise limit compression. The second piston may also be used to increase the expansion ratio, decoupling it from the compression ratio. Increasing the expansion ratio in this way can increase thermodynamic efficiency in a similar manner to the Miller or Atkinson cycle.\n\nThese designs use a single piston per cylinder, like a conventional two- or four-stroke engine. A secondary, non-detonating fluid is injected into the chamber, and the leftover heat from combustion causes it to expand for a second power stroke followed by a second exhaust stroke.\n\nIn 1883, the Bath-based engineer Samuel Griffin was an established maker of steam and gas engines. He wished to produce an internal combustion engine, but without paying the licensing costs of the Otto patents. His solution was to develop a \"patent slide valve\" and a single-acting six-stroke engine using it.\nBy 1886, Scottish steam locomotive maker Dick, Kerr & Co. saw a future in large oil engines and licensed the Griffin patents. These were double-acting, tandem engines and sold under the name \"Kilmarnock\". A major market for the Griffin engine was in electricity generation, where they developed a reputation for happily running light for long periods, then suddenly being able to take up a large demand for power. Their large heavy construction didn't suit them to mobile use, but they were capable of burning heavier and cheaper grades of oil. \nThe key principle of the \"Griffin Simplex\" was a heated exhaust-jacketed external vapouriser, into which the fuel was sprayed. The temperature was held around , sufficient to physically vapourise the oil but not to break it down chemically. This fractional distillation supported the use of heavy oil fuels, the unusable tars and asphalts separating out in the vapouriser.\nHot-bulb ignition was used, which Griffin termed the \"catathermic igniter\", a small isolated cavity connected to the combustion chamber. The spray injector had an adjustable inner nozzle for the air supply, surrounded by an annular casing for the oil, both oil and air entering at pressure, and being regulated by a governor.\nGriffin went out of business in 1923.\nOnly two known examples of a Griffin six-stroke engine survive. One is in the Anson Engine Museum. The other was built in 1885 and for some years was in the Birmingham Museum of Science and Technology, but in 2007 it returned to Bath and the Museum of Bath at Work.\n\nLeonard Dyer invented a six-stroke internal combustion water-injection engine in 1915, very similar to Crower's design (see below). A dozen more similar patents have been issued since.\n\nDyer's six-stroke engine features:\n\nThe Bajulaz six-stroke engine is similar to a regular combustion engine in design. There are, however, modifications to the cylinder head, with two supplementary fixed-capacity chambers: a combustion chamber and an air-preheating chamber above each cylinder. The combustion chamber receives a charge of heated air from the cylinder; the injection of fuel begins an isochoric (constant-volume) burn, which increases the thermal efficiency compared to a burn in the cylinder. The high pressure achieved is then released into the cylinder to work the power or expansion stroke. Meanwhile, a second chamber, which blankets the combustion chamber, has its air content heated to a high degree by heat passing through the cylinder wall. This heated and pressurized air is then used to power an additional stroke of the piston.\n\nThe claimed advantages of the engine include reduction in fuel consumption by at least 40%, two expansion strokes in six strokes, multi-fuel usage capability, and a dramatic reduction in pollution.\n\nThe Bajulaz six-stroke engine was invented in 1989 by Roger Bajulaz of the Bajulaz S.A. company, based in Geneva, Switzerland; it has and .\n\nThe Bajulaz six-stroke engine features claimed are:\n\nIn a Velozeta engine, fresh air is injected into the cylinder during the exhaust stroke, which expands by heat and therefore forces the piston down for an additional stroke. The valve overlaps have been removed, and the two additional strokes using air injection provide for better gas scavenging. The engine seems to show 40% reduction in fuel consumption and dramatic reduction in air pollution. Its Power-to-weight ratio is slightly less than that of a four-stroke gasoline engine. The engine can run on a variety of fuels, ranging from gasoline and diesel fuel to LPG. An altered engine shows a 65% reduction in carbon monoxide pollution when compared with the four-stroke engine from which it was developed. The engine was developed in 2005 by a team of mechanical engineering students, Mr. U Krishnaraj, Mr. Boby Sebastian, Mr. Arun Nair and Mr. Aaron Joseph George of the College of Engineering, Trivandrum.\n\nThis engine was designed, developed and patented by Chanayil Cleetus Anil, of Kochi, India, in 2012. He holds Indian patent number IN252642, granted by IPIndia on May 25, 2012. The name of the engine is taken from the name of his company, NIYKADO Motors. The engine underwent a preliminary round of full-throttle tests at the Automotive Research Association of India (ARAI), Pune. The inventor claims this engine \"is 23 per cent more fuel efficient compared to a standard four-stroke engine\" and that it is \"very low on pollution\".\n\nIn a six-stroke engine prototyped in the United States by Bruce Crower, water is injected into the cylinder after the exhaust stroke and is instantly turned to steam, which expands and forces the piston down for an additional power stroke. Thus, waste heat that requires an air or water cooling system to discharge in most engines is captured and put to use driving the piston. Crower estimated that his design would reduce fuel consumption by 40% by generating the same power output at a lower rotational speed. The weight associated with a cooling system could be eliminated, but that would be balanced by a need for a water tank in addition to the normal fuel tank.\n\nThe Crower six-stroke engine was an experimental design that attracted media attention in 2006 because of an interview given by the 75-year-old American inventor, who has applied for a patent on his design. That patent application was subsequently abandoned.\n\nThese designs use two pistons per cylinder operating at different rates, with combustion occurring between the pistons.\n\nThis design was developed by Malcolm Beare of Australia. The technology combines a four-stroke engine bottom end with an opposed piston in the cylinder head working at half the cyclical rate of the bottom piston. Functionally, the second piston replaces the valve mechanism of a conventional engine. Claimed benefits include a 9% increase in power, and improved thermodynamic efficiency through an increased compression ratio enabled by the elimination of the hot exhaust valve.\n\nThe idea was developed at the Silesian University of Technology, Poland, under the leadership of dr Adam Ciesiołkiewicz. It was granted patent nr 195052 by the Polish Patent Office.\n\nThe M4+2 engines have much in common with the Beare-head engines, combining two opposed pistons in the same cylinder. One piston works at half the cyclical rate of the other, but while the main function of the second piston in a Beare-head engine is to replace the valve mechanism of a conventional four-stroke engine, the M4+2 takes the principle one step further. The double-piston combustion engine's work is based on the cooperation of both modules. The air load change takes place in the two-stroke section of the engine. The piston of the four-stroke section is an air load exchange aiding system, working as a system of valves. The cylinder is filled with air or with an air-fuel mixture. The filling process takes place at overpressure by the slide inlet system. The exhaust gases are removed as in the classical two-stroke engine, by exhaust windows in the cylinder. The fuel is supplied into the cylinder by a fuel-injection system. Ignition is realized by two spark plugs. The effective power output of the double-piston engine is transferred by two crankshafts. The characteristic feature of this engine is an opportunity of continuous change of cylinder capacity and compression rate during engine work by changing the piston's location. The mechanical and thermodynamical models were meant for double-piston engines, which enable to draw up new theoretical thermodynamic cycle for internal combustion double-pistons engine.\n\nThe working principle of the engine is explained in the two- and four-stroke engines article.\n\nIn this engine, similar in design to the Beare head, a \"piston charger\" replaces the valve system. The piston charger charges the main cylinder and simultaneously regulates the inlet and the outlet aperture, leading to no loss of air and fuel in the exhaust. In the main cylinder, combustion takes place every turn as in a two-stroke engine and lubrication as in a four-stroke. Fuel injection can take place in the piston charger, in the gas-transfer channel or in the combustion chamber. It is also possible to charge two working cylinders with one piston charger. The combination of compact design for the combustion chamber together with no loss of air and fuel is claimed to give the engine more torque, more power and better fuel consumption. The benefit of fewer moving parts and design is claimed to lead to lower manufacturing costs. Good for hybrid technology and stationary engines. The engine is claimed to be suited to alternative, fuels since there is no corrosion or deposits left on valves.\nThe six strokes are:\nThis is an invention of Helmut Kottmann from Germany, while working 25 years at MAHLE GmbH piston and cylinder construction. Kottman's US patents 3921608 and 5755191 are listed below.\n\nThis design was invented by Belgian engineer Gerhard Schmitz, and has been prototyped by Ilmor Engineering.\n\nThese designs use two (or 4, 6, 8) cylinders with a conventional Otto four-stroke cycle. An additional piston (in its own cylinder) is shared by the two Otto cycle cylinders. The exhaust from the Otto cycle cylinder is directed into the shared cylinder, where it is expanded generating additional work. This is in some respects similar to the operation of a compound steam engine, with the Otto cycle cylinders being the high-pressure stage and the shared cylinder the low pressure stage. The operation of the engine is thus:\n\nThe designers consider this to be a five-stroke design, regarding the simultaneous HP exhaust stroke and LP expansion stroke as a single stroke. This design provides higher fuel efficiency due to the higher overall expansion ratio of the combined cylinders. Expansion ratios comparable to diesel engines can be achieved, while still using gasoline (petrol) fuel. Five-stroke engines allegedly are lighter and have higher power density than diesel engines.\n\nThe controlled combustion engines, designed by Bradley Howell-Smith of Australian firm Revetec Holdings Pty Ltd, use opposed pairs of pistons to drive a pair of counter-rotating three-lobed cams through bearings. These elements replace the conventional crankshaft and connecting-rods, which enables the motion of the pistons to be purely axial, so that most of the power otherwise wasted on lateral motion of the con-rods is effectively transferred to the output shaft. This gives six power strokes per revolution of the shaft (spread across a pair of pistons). An independent test measured the BSFC of Revetec's X4v2 prototype gasoline engine at 212g/kW-h (corresponding to an energy efficiency of 38.6%). Any even number of pistons can be used, in boxer or X configurations; the three lobes of the cams can be replaced by any other odd number greater than one; and the geometry of the cams can be changed to suit the needs of the target fuels and applications of the engines. Such variants may have ten or more strokes per cycle.\n\n\n\n\n"}
{"id": "3673665", "url": "https://en.wikipedia.org/wiki?curid=3673665", "title": "Splice joint", "text": "Splice joint\n\nA splice joint is a method of joining two members end to end in woodworking. The splice joint is used when the material being joined is not available in the length required. It is an alternative to other joints such as the butt joint and the scarf joint. Splice joints are stronger than unenforced butt joints and have the potential to be stronger than a scarf joint.\n\nSplices are therefore most often used when structural elements are required in longer lengths than the available material. The most common form of the splice joint is the half lap splice, which is common in building construction, where it is used to join shorter lengths of timber into longer beams.\n\n\nThere are four main types of splice joints: Half lap, Bevel lap, Tabled, and Tapered finger.\n\nThe half lap splice joint is the simplest form of the splice joint and is commonly used to join structural members where either great strength is not required or reinforcement, such as mechanical fasteners, are to be used.\n\nThe joint is cut as for a half lap.\n\nThe bevel lap is a variation of the half-lap in which the cheeks of the opposing members are cut at an angle of 5 to 10 degrees, sloping back away from the end of the member, so that some resistance to tension is introduced. This helps to prevent the members from being pulled apart.\n\nThe tabled splice joint is another variation of the half lap. The cheeks are cut with interlocking surfaces so that when brought together the joint resists being pulled apart.\n\nThe tapered finger splice joint requires a series of matching 'fingers' or interlocking prominences to be cut on the ends of opposing members. The joint is brought together and glued, with the fingers providing substantial glue surface.\n\nThis joint is commonly used in the production of building materials from smaller offcuts of timber. It is commonly found in skirting, architrave, and fascia.\n\nThe joint is usually made by machine.\n"}
{"id": "53626840", "url": "https://en.wikipedia.org/wiki?curid=53626840", "title": "Stagnation point flow", "text": "Stagnation point flow\n\nStagnation point flow represents a fluid flow in the immediate neighborhood of solid surface at which fluid approaching the surface divides into different streams or a counterflowing fluid streams encountered in experiments. Although the fluid is stagnant everywhere on the solid surface due to no-slip condition, the name stagnation point refers to the stagnation points of inviscid Euler solutions.\n\nHiemenz formulated the problem and calculated the solution numerically in 1911 and subsequently by Leslie Howarth(1934). The flow in the neighborhood of the stagnation point can be modeled by a flow towards an infinite flat plate, even though the whole body is a curved one(locally curvature effects are negligible). Let the plate be in the formula_1 plane with formula_2 representing the stagnation point. The inviscid stream function formula_3 and velocity formula_4 from Potential flow theory are\n\nwhere formula_6 is an arbitrary constant (represents strain rate in the counter flow setup). For real fluid(including viscous effects), there exists a self-similar solution if one defines\n\nwhere formula_8 is the Kinematic viscosity and formula_9 is a boundary layer thickness but it is constant(vorticity generated at the solid surface is prevented diffusing far away by an opposing convection, similar profiles are Blasius boundary layer with suction, Von Kármán swirling flow etc.,). Then the velocity components and subsequently pressure and the equation for formula_10 using Navier–Stokes equations are\n\nand the boundary condition due to no penetration and no-slip and the free stream condition for formula_13(Note boundary conditions for formula_14 far away from the plate is not specified, because it is part of the solution - a typical boundary layer problem) are\n\nThe asymptotic forms for large formula_16 are\n\nwhere formula_18 is the displacement thickness.\n\nStagnation point flow with moving plate with constant velocity formula_19 can be considered as model for rotating solids near the stagnation points. The stream function is\n\nwhere formula_21 satisfies the equation\n\nand Rott(1956) gave the solution as formula_23\n\nThe previous analyses assumes the flow impinges in normal direction. The inviscid stream function for oblique stagnation point flow is obtained by adding a constant vorticity formula_24.\n\nThe corresponding analysis for viscous fluid is studied by Stuart(1959), Tamada(1979) and Dorrepaal(1986). The self-similar stream function is,\n\nwhere formula_27 satisfies the equation\n\nThe corresponding problem in axisymmetric coordinate is solved by Homann(1936) and this serves a model for flow around near the stagnation point of a sphere. Paul A. Libby(1974)(1976) considered Homann flow with constantly moving plate with velocity formula_19 and also allowed for suction/injection with velocity formula_30 at the surface.\n\nThe self-similar solution is obtained by introducing following transformation for the velocity formula_31 in cylindrical coordinates\n\nand the pressure is given by\n\nTherefore, the Navier–Stokes equations reduce to\n\nwith boundary conditions,\n\nWhen formula_36, the classical Homann problem is recovered.\n\nJets emerging from a slot-jets creates stagnation point in between according to potential theory. The flow near the stagnation point can by studied using self-similar solution. This setup is widely used in combustion experiments. The initial study of impinging stagnation flows are due to C.Y. Wang. \nLet two fluids with constant properties denoted with suffix formula_37 flowing from opposite direction impinge and let's assume the two fluids are immiscible and the interface(located at formula_38) is planar. The velocity is given by\n\nwhere formula_40 are strain rates of the fluids. At the interface, velocities, tangential stress and pressure must be continuous.\nIntroducing the self-similar transformation,\n\nresults equations,\n\nThe no-penetration condition at the interface and free stream condition far away from the stagnation plane become\n\nBut the equations require two more boundary conditions. At formula_46, the tangential velocities formula_47, the tangential stress formula_48 and the pressure formula_49 are continuous. Therefore,\n\nwhere formula_51(from outer inviscid problem) is used. Both formula_52 are not known \"apriori\", but derived from matching conditions. The third equation is determine variation of outer pressure formula_53 due to the effect of viscosity. So there are only two parameters, which governs the flow, which are\n\nthen the boundary conditions become\n\nWhen densities and viscosities of the two impinging jets are same and constant, then the strain rate is also constant formula_56 and the potential flow solution become the solution of the Navier-Stokes equations, i.e.,\n\neverywhere in the flow domain. Kerr and Dold found additional new solutions called as Kerr-Dold vortices of Navier-Stokes equations in 1994 in the form of periodic array of steady vortices superposed on the constant density and constant viscosity counterflowing jets.\n"}
{"id": "1320289", "url": "https://en.wikipedia.org/wiki?curid=1320289", "title": "Steam reforming", "text": "Steam reforming\n\nSteam reforming or steam methane reforming is a chemical synthesis for producing syngas, hydrogen, carbon monoxide from hydrocarbon fuels such as natural gas. This is achieved in a processing device called a reformer which reacts steam at high temperature and pressure with methane in the presence of a nickel catalyst. The steam methane reformer is widely used in industry to make hydrogen. There is also interest in the development of much smaller units based on similar technology to produce hydrogen as a feedstock for fuel cells. Small-scale steam reforming units to supply fuel cells are currently the subject of research and development, typically involving the reforming of methanol, but other fuels are also being considered such as propane, gasoline, autogas, diesel fuel, and ethanol.\n\nSteam reforming of natural gas is the most common method of producing commercial bulk hydrogen at about 95% of the world production of 500 billion m in 1998. Hydrogen is used in the industrial synthesis of ammonia and other chemicals. At high temperatures (700 – 1100 °C) and in the presence of a metal-based catalyst (nickel), steam reacts with methane to yield carbon monoxide and hydrogen. \n\nCatalysts with high surface-area-to-volume ratio are preferred because of diffusion limitations due to high operating temperature. Examples of catalyst shapes used are spoked wheels, gear wheels, and rings with holes. Additionally, these shapes have a low pressure drop which is advantageous for this application.\n\nAdditional hydrogen can be obtained by reacting the CO with water via the water-gas shift reaction.\n\nThe first reaction is strongly endothermic (consumes heat, ΔH= 206 kJ/mol), the second reaction is mildly exothermic (produces heat, ΔH= -41 kJ/mol).\n\nThe United States produces nine million tons of hydrogen per year, mostly with steam reforming of natural gas. The worldwide ammonia production, using hydrogen derived from steam reforming, was 144 million metric tonnes in 2014.\n\nThis steam reforming process is quite different from and not to be confused with catalytic reforming of naphtha, an oil refinery process that also produces significant amounts of hydrogen along with high octane gasoline.\n\nSteam reforming of natural gas is approximately 65–75% efficient.\n\nAutothermal reforming (ATR) uses oxygen and carbon dioxide or steam in a reaction with methane to form syngas. The reaction takes place in a single chamber where the methane is partially oxidized. The reaction is exothermic due to the oxidation. When the ATR uses carbon dioxide the H:CO ratio produced is 1:1; when the ATR uses steam the H:CO ratio produced is 2.5:1\n\nThe reactions can be described in the following equations, using CO:\n\nAnd using steam:\n\nThe outlet temperature of the syngas is between 950-1100 C and outlet pressure can be as high as 100 bar.\n\nThe main difference between SMR and ATR is that SMR only uses oxygen via air for combustion as a heat source to create steam, while ATR directly combusts oxygen. The advantage of ATR is that the H:CO can be varied, this is particularly useful for producing certain second generation biofuels, such as DME which requires a 1:1 H:CO ratio.\n\nPartial oxidation (POX) is a type of chemical reaction. It occurs when a substoichiometric fuel-air mixture is partially combusted in a reformer, creating a hydrogen-rich syngas which can then be put to further use.\n\nThe capital cost of steam reforming plants is prohibitive for small to medium size applications because the technology does not scale down well. Conventional steam reforming plants operate at pressures between 200 and 600 psi with outlet temperatures in the range of 815 to 925 °C. However, analyses have shown that even though it is more costly to construct, a well-designed SMR can produce hydrogen more cost-effectively than an ATR.\n\nFlared gas and vented VOCs are known problems in the offshore industry and in the on-shore oil and gas industry, since both emit unnecessary greenhouse gases into the atmosphere. Reforming for combustion engines utilizes steam reforming technology for converting waste gases into a source of energy.\n\nReforming for combustion engines is based on steam reforming, where non-methane hydrocarbons (NMHCs) of low quality gases are converted to synthesis gas (H + CO) and finally to methane (CH), carbon dioxide (CO) and hydrogen (H) - thereby improving the fuel gas quality (methane number).\n\nIn contrast to conventional steam reforming, the process is operated at lower temperatures and with lower steam supply, allowing a high content of methane (CH) in the produced fuel gas. The main reactions are:\n\nSteam reforming: \n\nMethanation: \n\nWater-gas shift: \n\nSteam reforming of gaseous hydrocarbons is seen as a potential way to provide fuel for fuel cells. The basic idea for vehicle on-board reforming is that for example a methanol tank and a steam reforming unit would replace the bulky pressurized hydrogen tanks that would otherwise be necessary. This might mitigate the distribution problems associated with hydrogen vehicles; however the major market players discarded the approach of on-board reforming as impractical. (At high temperatures see above).\n\nThe reformer–fuel-cell system is still being researched but in the near term, systems would continue to run on existing fuels, such as natural gas or gasoline or diesel. However, there is an active debate about whether using these fuels to make hydrogen is beneficial while global warming is an issue. Fossil fuel reforming does not eliminate carbon dioxide release into the atmosphere but reduces the carbon dioxide emissions and nearly eliminates carbon monoxide emissions as compared to the burning of conventional fuels due to increased efficiency and fuel cell characteristics. However, by turning the release of carbon dioxide into a point source rather than distributed release, carbon capture and storage becomes a possibility, which would prevent the carbon dioxide's release to the atmosphere, while adding to the cost of the process.\n\nThe cost of hydrogen production by reforming fossil fuels depends on the scale at which it is done, the capital cost of the reformer and the efficiency of the unit, so that whilst it may cost only a few dollars per kilogram of hydrogen at industrial scale, it could be more expensive at the smaller scale needed for fuel cells.\n\nHowever, there are several challenges associated with this technology:\n\n"}
{"id": "12461863", "url": "https://en.wikipedia.org/wiki?curid=12461863", "title": "Targeted temperature management", "text": "Targeted temperature management\n\nTargeted temperature management (TTM) previously known as therapeutic hypothermia or protective hypothermia is an active treatment that tries to achieve and maintain a specific body temperature in a person for a specific duration of time in an effort to improve health outcomes during recovery after a period of stopped blood flow to the brain. This is done in an attempt to reduce the risk of tissue injury following lack of blood flow. Periods of poor blood flow may be due to cardiac arrest or the blockage of an artery by a clot as in the case of a stroke.\n\nTargeted temperature management improves survival and brain function following resuscitation from cardiac arrest. Evidence supports its use following certain types of cardiac arrest in which an individual does not regain consciousness. Both and appear to result in similar outcomes. Targeted temperature management following traumatic brain injury has shown mixed results with some studies showing benefits in survival and brain function while others show no clear benefit. While associated with some complications, these are generally mild.\n\nTargeted temperature management is thought to prevent brain injury by several methods including decreasing the brain's oxygen demand, reducing the production of neurotransmitters like glutamate, as well as reducing free radicals that might damage the brain. The lowering of body temperature may be accomplished by many means including the use of cooling blankets, cooling helmets, cooling catheters, ice packs and ice water lavage.\nTargeted temperature management may be used in the following conditions:\n\nThe 2013 ILCOR and 2010 American Heart Association guidelines support the use of cooling following resuscitation from cardiac arrest. These recommendations were largely based on two trials from 2002 which showed improved survival and brain function when cooled to after cardiac arrest.\n\nHowever more recent research suggests that there is no benefit to cooling to when compared with less aggressive cooling only to a near-normal temperature of ; it appears cooling is effective because it prevents fever, a common complication seen after cardiac arrest. There is no difference in long term quality of life following mild compared to more severe cooling.\n\nIn children, following cardiac arrest, cooling does not appear useful as of 2018.\n\nHypothermia therapy for neonatal encephalopathy has been proven to improve outcomes for newborn infants affected by perinatal hypoxia-ischemia, hypoxic ischemic encephalopathy or birth asphyxia. A 2013 Cochrane review found that it is useful in full term babies with encephalopathy. Whole body or selective head cooling to , begun within six hours of birth and continued for 72 hours, reduces mortality and reduces cerebral palsy and neurological deficits in survivors.\n\nPossible complications may include: infection, bleeding, dysrhythmias and high blood sugar. One review found an increased risk of pneumonia and sepsis but not the overall risk of infection. Another review found a trend towards increased bleeding but no increase in severe bleeding. Hypothermia induces a \"cold diuresis\" which can lead to electrolyte abnormalities - specifically hypokalemia, hypomagnesaemia, and hypophosphatemia, as well as hypovolemia.\n\nThe earliest rationale for the effects of hypothermia as a neuroprotectant focused on the slowing of cellular metabolism resulting from a drop in body temperature. For every one degree Celsius drop in body temperature, cellular metabolism slows by 5–7%. Accordingly, most early hypotheses suggested that hypothermia reduces the harmful effects of ischemia by decreasing the body’s need for oxygen. The initial emphasis on cellular metabolism explains why the early studies almost exclusively focused on the application of deep hypothermia, as these researchers believed that the therapeutic effects of hypothermia correlated directly with the extent of temperature decline.\n\nIn the special case of infants suffering perinatal asphyxia it appears that apoptosis is a prominent cause of cell death and that hypothermia therapy for neonatal encephalopathy interrupts the apoptotic pathway. In general, cell death is not directly caused by oxygen deprivation, but occurs indirectly as a result of the cascade of subsequent events. Cells need oxygen to create ATP, a molecule used by cells to store energy, and cells need ATP to regulate intracellular ion levels. ATP is used to fuel both the importation of ions necessary for cellular function and the removal of ions that are harmful to cellular function. Without oxygen, cells cannot manufacture the necessary ATP to regulate ion levels and thus cannot prevent the intracellular environment from approaching the ion concentration of the outside environment. It is not oxygen deprivation itself that precipitates cell death, but rather without oxygen the cell can not make the ATP it needs to regulate ion concentrations and maintain homeostasis.\n\nNotably, even a small drop in temperature encourages cell membrane stability during periods of oxygen deprivation. For this reason, a drop in body temperature helps prevent an influx of unwanted ions during an ischemic insult. By making the cell membrane more impermeable, hypothermia helps prevent the cascade of reactions set off by oxygen deprivation. Even moderate dips in temperature strengthen the cellular membrane, helping to minimize any disruption to the cellular environment. It is by moderating the disruption of homeostasis caused by a blockage of blood flow that many now postulate, results in hypothermia’s ability to minimize the trauma resultant from ischemic injuries.\n\nTargeted temperature management may also help to reduce reperfusion injury, damage caused by oxidative stress when the blood supply is restored to a tissue after a period of ischemia. Various inflammatory immune responses occur during reperfusion. These inflammatory responses cause increased intracranial pressure, which leads to cell injury and in some situations, cell death. Hypothermia has been shown to help moderate intracranial pressure and therefore to minimize the harmful effects of a patient’s inflammatory immune responses during reperfusion. The oxidation that occurs during reperfusion also increases free radical production. Since hypothermia reduces both intracranial pressure and free radical production, this might be yet another mechanism of action for hypothermia's therapeutic effect. Overt activation of N-methyl-D-aspartate (NMDA) receptors following brain injuries can lead to calcium entry which triggers neuronal death via the mechanisms of excitotoxicity.\n\nThere are a number of methods through which hypothermia is induced. These include: cooling catheters, cooling blankets, and application of ice applied around the body among others. As of 2013 it is unclear if one method is any better than the others. While cool intravenous fluid may be given to start the process, further methods are required to keep the person cold.\n\nCore body temperature must be measured (either via the esophagus, rectum, bladder in those who are producing urine, or within the pulmonary artery) to guide cooling. A temperature below should be avoided, as adverse events increase significantly. The person should be kept at the goal temperature plus or minus half a degree Celsius for 24 hours. Rewarming should be done slowly with suggested speeds of per hour.\n\nTargeted temperature management should be started as soon as possible. The goal temperature should be reached before 8 hours. Targeted temperature management remains partially effective even when initiated as long as 6 hours after collapse.\n\nPrior to the induction of targeted temperature management, pharmacological agents to control shivering must be administered. When body temperature drops below a certain threshold—typically around —people may begin to shiver. It appears that regardless of the technique used to induce hypothermia, people begin to shiver when temperature drops below this threshold. Drugs commonly used to prevent and treat shivering in targeted temperature management include acetaminophen, buspirone, opioids including pethidine (meperidine), dexmedetomidine, fentanyl, and/or propofol. If shivering is unable to be controlled with these drugs, patients are often placed under general anesthesia and/or are given paralytic medication like vecuronium. People should be rewarmed slowly and steadily in order to avoid harmful spikes in intracranial pressure.\n\nCooling catheters are inserted into a femoral vein. Cooled saline solution is circulated through either a metal coated tube or a balloon in the catheter. The saline cools the person’s whole body by lowering the temperature of a person’s blood. Catheters reduce temperature at rates ranging from per hour. Through the use of the control unit, catheters can bring body temperature to within of the target level. Furthermore, catheters can raise temperature at a steady rate, which helps to avoid harmful rises in intracranial pressure. A number of studies have demonstrated that targeted temperature management via catheter is safe and effective.\n\nAdverse events associated with this invasive technique include bleeding, infection, vascular puncture, and deep vein thrombosis (DVT). Infection caused by cooling catheters is particularly harmful, as resuscitated people are highly vulnerable to the complications associated with infections. Bleeding represents a significant danger, due to a decreased clotting threshold caused by hypothermia. The risk of deep vein thrombosis may be the most pressing medical complication.\n\nDeep vein thrombosis can be characterized as a medical event whereby a blood clot forms in a deep vein, usually the femoral vein. This condition may become potentially fatal if the clot travels to the lungs and causes a pulmonary embolism. Another potential problem with cooling catheters is the potential to block access to the femoral vein, which is a site normally used for a variety of other medical procedures, including angiography of the venous system and the right side of the heart. However, most cooling catheters are triple lumen catheters, and the majority of people post-arrest will require central venous access. Unlike non-invasive methods which can be administered by nurses, the insertion of cooling catheters must be performed by a physician fully trained and familiar with the procedure. The time delay between identifying a person who might benefit from the procedure and the arrival of an interventional radiologist or other physician to perform the insertion may minimize some of the benefit of invasive methods' more rapid cooling.\n\nTransnasal evaporative cooling is a method of inducing the hypothermia process and provides a means of continuous cooling of a person throughout the early stages of targeted temperature management and during movement throughout the hospital environment. This technique uses two cannulae, inserted into a persons nasal cavity, to deliver a spray of coolant mist that evaporates directly underneath the brain and base of the skull. As blood passes through the cooling area, it reduces the temperature throughout the rest of the body.\n\nThe method is compact enough to be used at the point of cardiac arrest, during ambulance transport, or within the hospital proper. It is intended to reduce rapidly the person's temperature to below while targeting the brain as the first area of cooling. Research into the device has shown cooling rates of per hour in the brain (measured through infrared tympanic measurement) and per hour for core body temperature reduction.\n\nWith these technologies, cold water circulates through a blanket, or torso wraparound vest and leg wraps. To lower temperature with optimal speed, 70% of a person’s surface area should be covered with water blankets. The treatment represents the most well studied means of controlling body temperature. Water blankets lower a person’s temperature exclusively by cooling a person’s skin and accordingly require no invasive procedures.\n\nWater blankets possess several undesirable qualities. They are susceptible to leaking, which may represent an electrical hazard since they are operated in close proximity to electrically powered medical equipment. The Food and Drug Administration also has reported several cases of external cooling blankets causing significant burns to the skin of person. Other problems with external cooling include overshoot of temperature (20% of people will have overshoot), slower induction time versus internal cooling, increased compensatory response, decreased patient access, and discontinuation of cooling for invasive procedures such as the cardiac catheterization.\n\nIf therapy with water blankets is given along with two litres of cold intravenous saline, people can be cooled to in 65 minutes. Most machines now come with core temperature probes. When inserted into the rectum, the core body temperature is monitored and feedback to the machine allows changes in the water blanket to achieve the desired set temperature. In the past some of the models of cooling machines have produced an overshoot in the target temperature and cooled people to levels below , resulting in increased adverse events. They have also rewarmed patients at too fast a rate, leading to spikes in intracranial pressure. Some of the new models have more software that attempt to prevent this overshoot by utilizing warmer water when the target temperature is close and preventing any overshoot. Some of the new machines now also have 3 rates of cooling and warming; a rewarming rate with one of these machines allows a patient to be rewarmed at a very slow rate of just an hour in the \"automatic mode,\" allowing rewarming from to over 24 hours.\n\nThere are a number of non-invasive head cooling caps and helmets designed to target cooling at the brain. A hypothermia cap is typically made of a synthetic material such as neoprene, silicone, or polyurethane and filled with a cooling agent such as ice or gel which is either cooled to a very cold temperature, , before application or continuously cooled by an auxiliary control unit. Their most notable uses are in preventing or reducing alopecia in chemotherapy, and for preventing cerebral palsy in babies born with hypoxic ischemic encephalopathy. In the continuously cooled iteration, coolant is cooled with the aid of a compressor and pumped through the cooling cap. Circulation is regulated by means of valves and temperature sensors in the cap. If the temperature deviates or if other errors are detected, an alarm system is activated. The frozen iteration involves continuous application of caps filled with Crylon gel cooled to to the scalp before, during and after intravenous chemotherapy. As the caps warm on the head, multiple cooled caps must be kept on hand and applied every 20 to 30 minutes.\n\nHypothermia has been applied therapeutically since antiquity. The Greek physician Hippocrates, the namesake of the Hippocratic Oath, advocated the packing of wounded soldiers in snow and ice. Napoleonic surgeon Baron Dominique Jean Larrey recorded that officers who were kept closer to the fire survived less often than the minimally pampered infantrymen. In modern times, the first medical article concerning hypothermia was published in 1945. This study focused on the effects of hypothermia on patients suffering from severe head injury. In the 1950s, hypothermia received its first medical application, being used in intracerebal aneurysm surgery to create a bloodless field. Most of the early research focused on the applications of deep hypothermia, defined as a body temperature of . Such an extreme drop in body temperature brings with it a whole host of side effects, which made the use of deep hypothermia impractical in most clinical situations.\n\nThis period also saw sporadic investigation of more mild forms of hypothermia, with mild hypothermia being defined as a body temperature of . In the 1950s, Doctor Rosomoff demonstrated in dogs the positive effects of mild hypothermia after brain ischemia and traumatic brain injury. In the 1980s further animal studies indicated the ability of mild hypothermia to act as a general neuroprotectant following a blockage of blood flow to the brain. This animal data was supported by two landmark human studies that were published simultaneously in 2002 by the New England Journal of Medicine. Both studies, one occurring in Europe and the other in Australia, demonstrated the positive effects of mild hypothermia applied following cardiac arrest. Responding to this research, in 2003 the American Heart Association (AHA) and the International Liaison Committee on Resuscitation (ILCOR) endorsed the use of targeted temperature management following cardiac arrest. Currently, a growing percentage of hospitals around the world incorporate the AHA/ILCOR guidelines and include hypothermic therapies in their standard package of care for patients suffering from cardiac arrest. Some researchers go so far as to contend that hypothermia represents a better neuroprotectant following a blockage of blood to the brain than any known drug. Over this same period a particularly successful research effort showed that hypothermia is a highly effective treatment when applied to newborn infants following birth asphyxia. Meta-analysis of a number of large randomised controlled trials showed that hypothermia for 72 hours started within 6 hours of birth significantly increased the chance of survival without brain damage.\n\nTTM has been studied in several use scenarios where it has not been found to be helpful, or is still under investigation.\n\nThere is currently no evidence supporting targeted temperature management use in humans and clinical trials have not been completed. Most of the data concerning hypothermia’s effectiveness in treating stroke is limited to animal studies. These studies have focused primarily on ischemic stroke as opposed to hemorrhagic stroke, as hypothermia is associated with a lower clotting threshold. In these animal studies, hypothermia was represented an effective neuroprotectant. The use of hypothermia to control intracranial pressure (ICP) after an ischemic stroke was found to be both safe and practical.\n\nAnimal studies have shown the benefit of targeted temperature management in traumatic central nervous system (CNS) injuries. Clinical trials have shown mixed results with regards to the optimal temperature and delay of cooling. Achieving therapeutic temperatures of is thought to prevent secondary neurological injuries after severe CNS trauma. A systematic review of randomised controlled trials in traumatic brain injury (TBI) suggests there is no evidence that hypothermia is beneficial.\n\nAs of 2015 hypothermia had shown no improvements in neurological outcomes or in mortality in neurosurgery.\n\n\n"}
{"id": "37639191", "url": "https://en.wikipedia.org/wiki?curid=37639191", "title": "Toul-Rosières Solar Park", "text": "Toul-Rosières Solar Park\n\nThe Toul-Rosières Solar Park is a 115 megawatt (MW) solar farm located at the Toul-Rosières Air Base, in France. It is the largest solar power station using photovoltaic technology in France. The project is developed by EDF Énergies Nouvelles (EDF EN).\n\nThe solar park has about 1.4 million thin-film PV panels based on CdTe technology made by the US company First Solar. It covers area of .\n\nIn 2012, the Luxembourg-based Marguerite Fund acquired 36 MW stake in the solar park. 24 MW stake was sold to the independent power producer Sonnedix.\n\n"}
{"id": "12512882", "url": "https://en.wikipedia.org/wiki?curid=12512882", "title": "Trashion", "text": "Trashion\n\nTrashion (a portmanteau of \"trash\" and \"fashion\") is a term for art, jewellery, fashion and objects for the home created from used, thrown-out, found and repurposed elements. The term was first coined in New Zealand in 2004 and gained in usage through 2005. Trashion is a subgenre of found object art, which is basically using objects that already have some other defined purpose, and turning it into art. In this case, trash is used.\n\nInitially trashion was used to describe art-couture costume usually linked to contests or fashion shows; however, as recycling and 'green' fashion become more prevalent, trashion has taken a turn for the more wearable. The term is now widely used in creative circles to describe any wearable item or accessory that is constructed using all or part materials that have been recycled, including clothing that has been thrifted and reconditioned.\n\nTrashion is a philosophy and an ethic encompassing environmentalism and innovation. Making traditional objects out of recycled materials can be trashion, as can making avant-garde fashion from cast-offs or junk. It springs from a desire to make the best use of limited resources. Trashion is similar to upcycling and refashion, although it began with specific fashion aspirations. Like upcycling, trashion generates items that are valued again, but these may be low-cost or high-cost, perhaps depending on the skill of the artist. The environmental aim of trashion is to call attention to the polluting outcome of waste.\n\nIndigenous people throughout the world have used salvaged materials to create new objects for an indeterminate number of years. Africans have made bags from rice and juice packets, Haitians have made sculptural jewelry from old oil cans, and American settlers have made quilts and rugs from cast-off clothing and feed sacks. People were making something from nothing long before the word \"trashion\" was coined; however, Trashion usually refers to \"making something from nothing\" for aesthetic purposes, not for practical use.\n\nIn the 1990s, American artist Ann Wizer began using plastic waste in her wearable art. Working in the Philippines, and titling her line the 'Virus Project,' Wizer created a set of costumes made entirely from post consumer plastic waste to celebrate Earth Day. The first plastic packaging tote bags were accessories to these costumes. There are now numerous small scale poverty intervention projects throughout Southeast Asia in the process of creating similar trashion accessories and other fashion and homeware items. This includes the XSProject, a charity founded by Wizer and based in Jakarta.\n\nThe Trashion movement took off in New York City in late 2004, early 2005 through underground parties on the lower east side of Manhattan at a club called Plan B. Trashion was part of the fashion of Urban Gypsy Circus, which is an interactive art party thrown by multi-media artist Miz Metro. These parties exhibited art made out of recycled materials and included monthly \"TRASHION FACEOFFS\" where 2 designers would compete to be the \"TRASHION QUEEN\". This led to Matthew Namer & Miz Metro curating the TRASHION Art show at Gallery 151 on exhibit during the summer of 2009 on the lower east side of Manhattan.\n\nTrashion has become a very popular style of art. For example, in 2006, Julia Genatossio launched Monsoon Vermont, a trashion design house. The company designs items such as umbrellas, shower curtains and backpacks which are made from Jakarta's scavenged trash. Unlike the XSProject, Monsoon Vermont is a for-profit venture.\n\nTrashion is also the subject of school projects, local fashion shows, community center exhibits, and fundraisers, among other purposes.\n\nSome contemporary trashion artists include Marina DeBris and Nancy Judd.\n"}
{"id": "54209559", "url": "https://en.wikipedia.org/wiki?curid=54209559", "title": "Tropical cyclone tracking chart", "text": "Tropical cyclone tracking chart\n\nA tropical cyclone tracking chart is used by those within hurricane-threatened areas to track tropical cyclones worldwide. In the north Atlantic basin, they are known as hurricane tracking charts. New tropical cyclone information is available at least every six hours in the Northern Hemisphere and at least every twelve hours in the Southern Hemisphere. Charts include maps of the areas where tropical cyclones form and track within the various basins, include name lists for the year, basin-specific tropical cyclone definitions, rules of thumb for hurricane preparedness, emergency contact information, and numbers for figuring out where tropical cyclone shelters are open.\n\nIn paper form originally, computer programs were developed in the 1980s for personal home and use by professional weather forecasters. Those used by weather forecasters saved preparation times, allowing tropical cyclone advisories to be sent an hour earlier. With the advent of the internet in the 1990s, digitally-prepared charts began to include other information along with storm position and past track, including forecast track, areas of wind impact, and related watches and warnings. Geographic information system (GIS) software allows end users to underlay other layered files onto forecast storm tracks to anticipate future impacts.\n\nTropical cyclone tracking charts were initially used for tropical cyclone forecasting and towards the end of the year for post season summaries of the season's activity. Their use led to an north Atlantic-based term still in use today: Cape Verde hurricane. Prior to the early 1940s, the term Cape Verde hurricane referred to August and early September storms that formed to the east of the surface plotting charts in use at the time. By October 1955, charts used for tropical cyclone tracking and forecasting operationally, such as United States Weather Bureau Form 770-17 and National Weather Service Chart HU-1, extended eastward to the African coast.\n\nWithin the United States since at least 1956, during the Atlantic hurricane season, those within threatened states were provided hurricane tracking charts in order to follow tropical storms and hurricanes during the season for situational awareness. This was more popular along sections of the southern Eastern Seaboard and Gulf Coast of the United States than the coast of California due to their increased danger of a landfalling tropical cyclone. The maps would include place names, latitude and longitude lines, names of the storms on that year's list, along with hurricane preparedness information. Newspapers, television stations, radio stations, banks, restaurants, grocery stores, insurance companies, gas stations, the American Red Cross, the Federal Emergency Management Agency, state departments of emergency management, the National Weather Service, and its subagency the National Hurricane Center were the main suppliers of these charts. Companies would distribute these for free as they were considered good advertising. Some would have a table where you could enter data prior to plotting the storm's position, usually using an associated tropical cyclone symbol: open circle for tropical depression, open circle with curved lines on opposite sides of the circle for tropical storms, and a closed circle with curved lines on opposite sides of the circle for hurricanes. The Vanuatu Meteorology and Geo-Hazards Department started preparing special tropical cyclone tracking charts for its archipelago in the 1980s.\n\nInitially, the charts were in paper form. Magnetic charts appeared in 1956. By 1974, laminated paper was used, and by 1977 maps were placed under glass, so that grease pencil, washable marker, or dry erase marker could be used and that the map could be used for multiple seasons. Starting in the 1980s with the increasing popularity of personal computers, programs were available to track the storms digitally, and databases of past storms could be maintained. However, computational space requirements did not allow access to the entire hurricane database for a related basin until the 1990s, with the advent of more powerful computers with megabytes of storage and file quantities became less limited in computer directory structures. Starting in the mid 1990s, with the popularity of the World Wide Web, web sites kept images of old hurricane tracks and interactive web sites allowed you to specify parameters for the storms you wished to display. Ongoing storms have tracking charts with forecast track overlaid. Since 2004, GIS software has been available for hurricane tracking.\n\nHistorically, tropical cyclone tracking charts were used to include the past track and prepare future forecasts at Regional Specialized Meteorological Centers and Tropical Cyclone Warning Centers. The need for a more modernized method for forecasting tropical cyclones had become apparent to operational weather forecasters by the mid-1980s. At that time the United States Department of Defense was using paper maps, acetate, grease pencils, and disparate computer programs to forecast tropical cyclones. The Automated Tropical Cyclone Forecasting System (ATCF) software was developed by the Naval Research Laboratory for the Joint Typhoon Warning Center (JTWC) beginning in 1986, and used since 1988. During 1990 the system was adapted by the National Hurricane Center (NHC) for use at the NHC, National Centers for Environmental Prediction and the Central Pacific Hurricane Center. This provided the NHC with a multitasking software environment which allowed them to improve efficiency and cut the time required to make a forecast by 25% or 1 hour. ATCF was originally developed for use within DOS, before later being adapted to Unix and Linux. Despite ATCF's introduction, into the late 1990s, a National Hurricane Center forecaster stated that the most important tools available were \"a pair of dividers to measure distance, a ruler, a brush for eraser dirt, three sharp pencils colored red, black, and blue, and a large paper plotting chart\".\n\nSymbols used within the charts vary by basin, by center, and by individual preference. Simple dots or circles can be used for each position. The National Hurricane Center uses a variety of symbols composed of overlapping 6's and 9's for tropical storms and hurricanes to emulate their circulation pattern, and a circle for tropical depressions. Other Northern Hemisphere centers used the overlapping 6 and 9 symbols for all tropical cyclones of tropical storm strength, with L's reserved for tropical depressions or general low pressure areas in the tropics. Southern Hemisphere versions would use backward overlapping 6's and 9's. The World Meteorological Organization uses an unfilled symbol to depict tropical storms, a filled symbol to depict systems of cyclone/hurricane/typhoon strength, and a circle to depict a tropical low or tropical convective cluster. Colors of the symbols may be representative of the cyclone's intensity.\n\nLines or dots connecting symbols can be varying colors, solid, dashed, or symbols between the points depending on the intensity and type of the system being tracked. Different colors could also be used to differentiate storms from one other within the same map. If black and white markings are used, tropical depression track portions can be indicated by dots, with tropical storms indicated by dashes, systems of cyclone/hurricane/typhoon strength using a solid line, intermittent triangles for the subtropical cyclone stage, and intermittent plus signs for the extratropical cyclone phase. Systems of category 3 strength or greater on the Saffir–Simpson scale can be depicted with a thicker line.\n\nIn order to use a hurricane tracking chart, one needs access to latitude/longitude pairs of the cyclone's center and maximum sustained wind information in order to know which symbol to depict. New tropical cyclone information is available at least every twelve hours in the Southern Hemisphere and at least every six hours in the Northern Hemisphere from Regional Specialized Meteorological Centers and Tropical Cyclone Warning Centers.\n\nIn decades past, newspaper, television, and radio (including weather radio) were primary sources for this information. Local television stations within threatened markets would advertise tropical cyclone positions within the morning, evening, and nightly news during their weather segments. The Weather Channel includes the information within their tropical updates every hour during the Atlantic and Pacific hurricane seasons. Starting in the mid 1990s, the World Wide Web allowed for the development of ftp and web sites by the Bureau of Meteorology in Australia, Canadian Hurricane Centre, Central Pacific Hurricane Center, the Nadi Tropical Cyclone Centre/Fiji Meteorological Service, Japan Meteorological Agency, Joint Typhoon Warning Center, Météo-France La Réunion, National Hurricane Center, and the Philippine Atmospheric, Geophysical and Astronomical Services Administration which allows the end user to get their information from their official products.\n\nThe maps either use a mercator projection if restricted to the tropics and subtropics, but can use a Lambert conformal conic projection if the maps reach towards the arctic for the North Atlantic basin where tropical cyclones move more poleward. Meteorologists use these maps to estimate a system's initial position based on aircraft, satellite, and surface data within surface weather analyses. The data is then analyzed to determine recent storm motion and create and convey forecast tracks, wind swaths, uncertainty, related watches, and related warnings to end users of tropical cyclone forecasts.\n\nHurricane tracking charts allow people to track ongoing systems to form their own opinions regarding where the storms are going and whether or not they need to prepare for the system being tracked, including possible evacuation. This continues to be encouraged by the National Oceanic and Atmospheric Administration and National Hurricane Center. Some agencies provide track storms in their immediate vicinity, while others cover entire ocean basins. One can choose to track one storm per map, use the map until the table is filled, or use one map per season. Some tracking charts have important contact information in case of an emergency or to locate nearby hurricane shelters. Tracking charts allow tropical cyclones to be better understood by the end user.\nA number of Hurricane tracker apps are also available online to install directly over a smartphone. By using these apps, one can easily track the current activity of the Hurricanes. Red Cross has also launched several applications for this purpose.\n"}
{"id": "5304916", "url": "https://en.wikipedia.org/wiki?curid=5304916", "title": "Voltage converter", "text": "Voltage converter\n\nA voltage converter is an electric power converter which changes the voltage of an electrical power source. It may be combined with other components to create a power supply.\n\nAC voltage conversion uses a transformer. Conversion from one DC voltage to another requires electronic circuitry(electromechanical equipment was required before the development of semiconductor electronics), like a DC-DC converter. Mains power (called household current in the US) is universally AC.\n\nA common use of the voltage converter is for a device that allows appliances made for the mains voltage of one geographical region to operate in an area with different voltage. Such a device may be called a \"voltage converter\", \"power converter\", \"travel adapter\", etc. Most single phase alternating-current electrical outlets in the world supply power at 210–240 V or at 100–120 V. A transformer or autotransformer can be used; (auto)transformers are inherently reversible, so the same transformer can be used to step the voltage up, or step it down by the same ratio. Lighter and smaller devices can be made using electronic circuitry; reducing the voltage electronically is simpler and cheaper than increasing it. Small, inexpensive, \"travel adapters\" suitable for low-power devices such as electric shavers, but not, say, hairdriers, are available; travel adapters usually include plug-end adapters for the different standards used in different countries. A transformer would be used for higher power.\n\nTransformers do not change the frequency of electricity; in many regions with 100–120 V, electricity is supplied at 60 Hz, and 210–240 V regions tend to use 50 Hz. This may affect operation of devices which depend on mains frequency (some audio turntables and mains-only electric clocks, etc., although modern equipment is less likely to depend upon mains frequency). Equipment with high-powered motors or internal transformers designed to operate at 60 Hz may overheat at 50 Hz even if the voltage supplied is correct.\n\nMost mains-powered electrical equipment, though it may specify a single nominal voltage, actually has a range of tolerance above and below that point. Thus, devices usually can be used on either any voltage from approx. 100 to 120 V, or any voltage from approx. 210 to 240 V. In such cases, voltage converters need only be specified to convert any voltage within one range, to a voltage within the other, rather than separate converters being needed for all possible pairs of nominal voltages (110–220, 117–220, 110–230, etc.)\n\nAnother requirement is to provide low-voltage electricity to a device from mains electricity; this would be done by what is usually called a \"power supply\". Most modern electronic devices require between 1.5 and 24 volts DC; lower-powered devices at these voltages can often work either from batteries or mains. Some devices incorporate a power supply and are simply plugged into the mains. Others use an external power supply comprising either a transformer and rectifier, or electronic circuitry. Switched-mode power supplies have become widespread in the early twenty-first century; they are smaller and lighter than the once-universal transformer converters, and are often designed to work from AC mains at any voltage between 100 and 250 V. Additionally, because they are typically rectified to operate at a DC voltage, they are minimally affected by the frequency of the mains (50 vs 60 Hz). Details on operation are given in the article on power supplies.\n\nVoltage converters can be used in vehicles with 12 V DC outlets. A simple voltage dropper can be used to reduce the voltage for low-power devices; if more than 12V is required, or for high-powered devices, a switched-mode power supply is used. The output will usually be DC in the range 1.5–24 V. Power supplies that output either 100–120 V AC or 210–240 V AC are available; they are called inverters, due to the conversion from DC to AC rather than the voltage change. The output frequency and waveform of an inverter may not accurately replicate that supplied by mains electricity, although this is not usually a problem.\n\nA converter to drive equipment should be specified to supply at least the actual watts or amperes used by the equipment; this is usually stated on a label on the equipment. A converter of higher rating is safe to use. Some equipment uses more power or current at startup; a 20% margin is usually adequate, although it may be included in the rated figure.\n\n"}
{"id": "2896717", "url": "https://en.wikipedia.org/wiki?curid=2896717", "title": "Wax sculpture", "text": "Wax sculpture\n\nA wax sculpture is a depiction made using a waxy substance. Often these are effigies, usually of a notable individual, but there are also death masks and scenes with many figures, mostly in relief.\n\nThe properties of beeswax make it an excellent medium for preparing figures and models, either by modeling or by casting in molds. It can easily be cut and shaped at room temperature, melts at a low temperature, mixes with any coloring matter, takes surface tints well, and its texture and consistency may be modified by the addition of earthy matters and oils or fats. When molten, it is highly responsive to impressions from a mold and, once it sets and hardens, its form is relatively resilient against ordinary temperature variations, even when it is cast in thin laminae. These properties have seen wax used for modelling since the Middle Ages and there is testimony for it having been used for making masks (particularly death masks) in ancient Rome. The death masks of illustrious ancestors would be displayed by the elite holding the right of \"ius imaginem.\"\n\nThe cost of making a wax sculpture can be between USD150,000 and 300,000.\n\nThe display of temporary or permanent effigies in wax and other media of the deceased was a common part of the funeral ceremonies of very important people in European historical times. Most of the figures would wear the real clothes of the deceased so they could be made quickly. The museum of Westminster Abbey has a collection of British royal wax effigies, as well as those of figures such as the naval hero Horatio Nelson, and Frances Stewart, Duchess of Richmond, who also had her parrot stuffed and displayed. The effigy of Charles II of England (1680) was displayed over his tomb until the early 19th century, when all were removed from the abbey itself. Nelson's effigy was a pure tourist attraction, commissioned the year after his death in 1805, and his burial in St Paul's Cathedral after a government decision that major public figures should in future be buried there. Concerned for their revenue from visitors, the Abbey decided it needed a rival attraction for admirers of Nelson.\n\nThe practice of wax modelling can be traced through the Middle Ages, when votive offerings of wax figures were made to churches. The memory and lineaments of monarchs and great personages were preserved by means of wax masks.\n\nDuring this period, superstition found expression in the formation of wax images of hated persons, into which long pins were thrust, in the confident expectation that thereby deadly injury would be induced to the person represented. This practice was considered more effective when some portion of the victim's hair or nails were added to the wax figure, thus strengthening the connection with its actual subject. This belief and practice continued until the 17th century, though the superstition survived into the 19th century. In the Scottish Highlands, a clay model of an enemy was found in a stream in 1885, having been placed there in the belief that, as the clay was washed away, so would the health of the hated one decline.\n\nDuring the Italian Renaissance, modeling in wax took a position of high importance, and it was practised by some of the greatest of the early masters. The bronze medallions of Pisanello and of the other famous medalists owe their value to the properties of wax: all early bronzes and metalwork were cast from wax models first.\n\nThere are a number of very high quality wax figures from the 16th and 17th centuries, mostly portrait figures and religious or mythological scenes, often with many figures. Antonio Abondio (1538–91) pioneered the coloured wax portrait miniature in relief, working mainly for the Habsburg and other courts of Northern Europe, and his son Alessandro continued in his footsteps.\n\nTowards the close of the 18th century, modeling of medallion portraits and of relief groups, the latter frequently polychromatic, was in considerable vogue throughout Europe. Many of the artists were women. John Flaxman executed in wax many portraits and other relief figures which Josiah Wedgwood translated into pottery for his Jasperware. The National Portrait Gallery has 40 wax portraits, mostly from this period.\n\nThe famous wax bust attributed to Leonardo da Vinci acquired in 1909 by the Museum of Berlin is the work of an English forger who worked about 1840. The wax model of a head, at the Wicar Museum at Lille, belongs probably to the school of Canova, which robs it of none of its exquisite grace.\n\nWax-works, not intended as fine art, subsequently became popular attractions, consisting principally of images of historical or notorious personages, made up of waxen masks on lay figures in which sometimes mechanism is fitted to give motion to the figure. Such an exhibition of wax-works with mechanical motions was shown in Germany early in the eighteenth century.\n\nThe most famous modern waxwork exhibition is that of Madame Tussauds, where the technology of animatronics brings the wax figures to life.\n\nWaxworks are frequently made presented by contemporary artists who take advantage of its lifelike and uncanny qualities. While the artist often creates a wax self-portrait, there are examples too of imaginary personalities and historical personae. For example, Gavin Turk had his portrait made as Sid Vicious (\"Pop\", Waxwork in vitrine 279 x 115 x 115 cm, 1993), Jan Fabre as a notorious thief (homage to Jacques Mesrine (Bust) II, 2008. Lifesize. Private collection.ta.) Contemporary artists working with wax include Beth B, Berlinde de Bruyckere, Maurizio Cattelan, Peta Coyne, Eleanor Crook, Robert Gober, John Isaacs, Wendy Mayer, Pascale Pollier, Chantal Pollier, Sigrid Sarda, Gil Shachar and Kiki Smith. Techniques include body casting using alginate and silicone rubber moulds, and hand modelling which creates unique forms and distortions.\n\nThe museum of medieval torture instruments in Amsterdam also used wax figures in order to demonstrate the use of machines and tools of their display.\n\nThe modeling of the soft parts of dissections, teaching illustrations of anatomy, was first practised at Florence during the Renaissance. The practice of moulage, or the depiction of human anatomy and different diseases taken from directly casting from the body using (in the early period) gelatine moulds, later alginate or silicone moulds, used wax as its primary material (later to be replaced by latex and rubber). Some moulages were directly cast from the bodies of diseased subjects, others from healthy subjects to which disease features( blisters, sores, growths, rashes) were skilfully applied with wax and pigments. During the 19th century, moulage evolved into three-dimensional, realistic representations of diseased parts of the human body. These can be seen in many European medical museums, notably the Spitzner collection currently in Brussels, the Charite Hospital museum in Berlin and the Gordon Museum of Pathology at Guy's Hospital in London UK. A comprehensive book monograph on moulages is \"Diseases in Wax: the History of Medical Moulage\" by Thomas Schnalke (Author) the director of the Charite Museum and Kathy Spatschek (Translator)\n\nA wax museum or waxworks consists of a collection of wax figures representing famous people from history and contemporary personalities exhibited in lifelike poses. Wax museums often have a special section dubbed the \"chamber of horrors\" in which the more grisly exhibits are displayed.\n\n"}
{"id": "10639161", "url": "https://en.wikipedia.org/wiki?curid=10639161", "title": "Wind power in Estonia", "text": "Wind power in Estonia\n\nWind power in Estonia in 2012 amounted to an installed capacity of 269.4 MW, whilst roughly 1466.5 MW worth of projects are currently being developed. All wind farms as of now are on land, offshore farms are planned on Lake Peipus and in the Baltic Sea near the island of Hiiumaa.\n\nPakri wind farm is located in Paldiski at the tip of the Pakri peninsula near the old light house. It consists of eight wind turbines and has a capacity of 18.4 MWs.\n\nAltogether, three major projects with total capacity of 1490 MW are being planned in Estonia: a 700 MW project near the island of Hiiumaa by Nelja Energia, also a 600 MW project in Gulf of Riga by Eesti Energia and a 190 MW offshore wind farm near the western coast of Estonia by Neugrund OÜ.\n\nInstalled wind power capacity in Estonia and generation in recent years is shown in the table below:\n\n"}
{"id": "23853142", "url": "https://en.wikipedia.org/wiki?curid=23853142", "title": "Wind turbine syndrome", "text": "Wind turbine syndrome\n\nWind turbine syndrome and wind farm syndrome are terms for adverse human health effects that have been ascribed to the proximity of wind turbines.. Proponents have claimed that these effects include death, cancer and congenital abnormality. The distribution of recorded events, however, correlates with media coverage of wind farm syndrome itself, and not with the presence or absence of wind farms. Neither term is recognised by any international disease classification system, nor do they appear in any title or abstract in the United States National Library of Medicine's PubMed database. Wind turbine syndrome has been characterized as pseudoscience.\n\nThe Center for Media and Democracy's \"SourceWatch\" website has identified at least one Australian fossil fuel industry funded astroturfing group as involved in promoting the idea of wind turbine syndrome. An investigation led to the foundation being stripped of its status as a health promotion charity.\n\nSince 2003, 25 reviews have been published of the scientific literature on wind turbines and health. These studies have consistently found no reason to believe that wind turbines are harmful to health.\n\nA panel of experts commissioned by the Massachusetts Department of Environmental Protection concluded in 2012 that \"there is not an association between noise from wind turbines and measures of psychological distress or mental health problems.\" \n\nA 2009 Canadian study found that \"a small minority of those exposed report annoyance and stress associated with noise perception...\" [however] \"Annoyance is not a disease.\" The study group pointed out that similar irritations are produced by local and highway vehicles, as well as from industrial operations and aircraft.\n\nA 2011 literature review found that although wind turbines are associated with some health effects, such as sleep disturbance, the health effects reported by those living near wind turbines were probably caused not by the turbines themselves but rather by \"physical manifestation from an annoyed state.\" A 2013 report for the National Health and Medical Research Council (Australia) elaborated: \"There is consistent evidence that noise from wind turbines is associated with annoyance, and reasonable consistency that it is associated with sleep disturbance and poorer sleep quality and quality of life. However, it is unclear whether the observed associations are due to wind turbine noise or plausible confounders.\"\n\nA meta study published in 2014 reported that among the cross-sectional studies of better quality, no clear or consistent association is seen between wind turbine noise and any reported disease or other indicator of harm to human health. Noise from turbines played a minor role in comparison with other factors in leading people to report annoyance in the context of wind turbines.\n\nIn Ontario, Canada, the Ministry of the Environment created noise guidelines to limit wind turbine noise levels 30 metres away from a dwelling or campsite to 40 dB(A). These regulations also set a minimum distance of for a group of up to five relatively quiet [102 dB(A)] turbines within a radius, rising to for a group of 11 to 25 noisier (106-107 dB(A)) turbines. Larger facilities and noisier turbines would require a noise study.\n\nIn a 2009 report about rural wind farms, a Standing Committee of the Parliament of New South Wales, Australia, recommended a minimum setback of two kilometres between wind turbines and neighbouring houses (which can be waived by the affected neighbour) as a precautionary approach. \n\nDespite the lack of scientific literature demonstrating any health effects from wind turbines, Australia's Turnbull government appointed a wind farm commissioner in October 2015 to address complaints. The 2016 annual report of the Independent Scientific Committee on Wind Turbines was tabled in the Australian Parliament on 8 August 2017. A website is maintained for the National Wind Farm Commissioner, with information about the role's purpose and links to a variety of publications that address wind turbines and their management, from a range of national and international sources.\n\nModern wind turbines produce significantly less noise than older designs. Turbine designers work to minimise noise, as noise reflects lost energy and output. Noise levels at nearby residences may be managed through the siting of turbines, the approvals process for wind farms, and operational management of the wind farm.\n"}
{"id": "2741999", "url": "https://en.wikipedia.org/wiki?curid=2741999", "title": "Wood finishing", "text": "Wood finishing\n\nWood finishing refers to the process of refining or protecting a wooden surface, especially in the production of furniture where typically it represents between 5 and 30% of manufacturing costs.\n\nFinishing is the final step of the manufacturing process that gives wood surfaces desirable characteristics, including enhanced appearance and increased resistance to moisture and other environmental agents. Finishing can also make wood easier to clean and keep it sanitized, sealing pores that can be breeding grounds for bacteria. Finishing can also influence other wood properties, for example tonal qualities of musical instruments and hardness of flooring. In addition, finishing provides a way of giving low-value woods the appearance of ones that are expensive and difficult to obtain.\n\nFinishing of wood requires careful planning to ensure that the finished piece looks attractive, performs well in service and meets safety and environmental requirements. Planning for finishing begins with the design of furniture. Care should be taken to ensure that edges of furniture are rounded so they can be adequately coated and are able to resist wear and cracking. Careful attention should also be given to the design and strength of wooden joints to ensure they do not open-up in service and crack the overlying finish. Care should also be taken to eliminate recesses in furniture, which are difficult to finish with some systems, especially UV-cured finishes.\n\nPlanning for wood finishing also involves thinking about the properties of the wood that you are going to finish, as these can greatly affect the appearance and performance of finishes, and also the type of finishing system that will give the wood the characteristics you are seeking. For example, woods that show great variation in colour between sapwood and heartwood or within heartwood may require a preliminary staining step to reduce colour variation. Alternatively, the wood can be bleached to remove the natural colour of the wood and then stained to the desired colour. Woods that are coarse textured such oaks and other ring-porous hardwoods may need to be filled before they are finished to ensure the coating can bridge the pores and resist cracking. The pores in ring-porous woods preferentially absorb pigmented stain, and advantage can be taken of this to highlight the wood's grain. Some tropical woods, such as rosewood (\"Dalbergia nigra\"), cocobolo (\"Dalbergia retusa\") and African padauk (\"Pterocarpus soyauxii\"), contain extractives such as quinones, which retard the curing of unsaturated polyester and UV-cured acrylate coatings, and so other finishing systems should be used with these species.\nPlanning for wood finishing also involves being aware of how the finishing process influences the end result. Careful handling of the wood is needed to avoid dents, scratches and soiling with dirt. Wood should be marked for cutting using pencil rather than ink; however, avoid hard or soft pencil. HB is recommend for face work and 2H for joint work. Care should be taken to avoid squeeze-out of glue from joints because the glue will reduce absorption of stain and finish. Any excess glue should be carefully removed to avoid further damage to the wood. \n\nWood’s moisture content affects the staining of wood. Changes in wood moisture content can result in swelling and shrinkage of wood which can stress and crack coatings. Both problems can be avoided by storing wood indoors in an environment where it can equilibriate to a recommended moisture content (6 to 8%) that is similar to that of the intended end use of the furniture.\n\nFinally, consideration needs to be given to whether the finished wood will come into contact with food, in which case a food-safe finish should be used, local environmental regulations governing the use of finishes, and recycling of finished wood at the end of its life.\n\nSanding is carried out before finishing to remove defects from the wood surface that will affect the appearance and performance of finishes that are subsequently applied to the wood. These defects include cutter marks and burns, scratches and indentations, small glue spots and raised grain. Sanding should not be used to eliminate larger defects such as gouges, and various forms of discolouration. Other techniques are used to remove these defects (see below). \nThe key to preparing a defect free surface is to develop a sanding schedule that will quickly eliminate defects and leave the surface smooth enough so that tiny scratches produced by sanding cannot be seen when the wood is finished. A sanding schedule usually begins with sandpaper that is coarse enough to remove larger defects (typically 80 or 100 grit, but sometimes higher if the surface is already quite smooth), and progresses through a series of sandpaper grades that gradually remove the sanding scratches created by the previous sanding steps. A typical sanding schedule prior to wood finishing might involve sanding wood along the grain with the following grades of sandpaper, 80, 100, 120, 150 and finishing with 180 and sometimes 220 grit. The precise sanding schedule is a matter of trial and error because the appearance of a sanded surface depends on the wood you are sanding and the finish that will subsequently be applied to the wood. According to Nagyszalanczy, coarse grained woods with large pores such as oak hide sanding scratches better than fine grained wood and hence with such species it may be possible to use 180 or even 150 grit sandpaper as the final step in the sanding schedule. Conversely, sanding scratches are more easily seen in finer grained, harder woods and also end-grain, and hence, they require finer sandpaper (220 grit) during the final sanding stage. The sandpaper selected for the final sanding stage affects the colour of stained wood, and therefore when staining is part of finishing avoid sanding the wood to a very smooth finish. On the other hand, according to Nagyszalanczy if you are using an oil-based finish, it is desirable to sand the wood using higher grit sandpaper (400 grit) because oil tends to highlight sanding scratches. \n\nSanding is very good at removing defects at wood surfaces, but it creates a surface that contains minute scratches in the form of microscopic valleys and ridges, and also slivers of wood cell wall material that are attached to the underlying wood. These sanding ridges and slivers of wood swell and spring-up, respectively, when sanded wood is finished with water-based finishes, creating a rough fuzzy surface. This defect is known as grain raising. It can be eliminated by wetting the surface with water, leaving the wood to dry and then lightly sanding the wood to remove the ‘raised grain’. \n\nLarger defects that interfere with wood finishing include dent, gouges, splits and glue spots and smears. These defects should also be removed before finishing, otherwise they will affect the quality of the finished furniture or object. However, it is difficult to completely eliminate large defects from wood surfaces.\n\nRemoving dents from wood surfaces is quite straightforward as pointed out by Flexner. Add a few droplets of demineralized water to the dent and let it soak in. Then put a clean cloth over the dent and place the tip of a hot iron on the cloth that lies immediately above the dent, taking great care not to burn the wood. The transfer of heat from the iron to the wood will cause compressed fibres in the dent to recover their original dimensions. As a result the dent will diminish in size or even disappear completely, although removal of large dents may require a number of wetting and heating cycles. The wood in the recovered dent should then be dried and sanded smooth to match the surrounding wood.\n\nGouges and holes in wood are more difficult to repair than dents because wood fibres have been cut, torn and removed from the wood. Larger gouges and splits are best repaired by patching the void with a piece of wood that matches the colour and grain orientation of the wood under repair. Patching wood requires skill, but when done properly it is possible to create a repair that is very difficult to see. An alternative to patching is filling (sometimes known as stopping). Numerous coloured fillers (putties and waxes) are produced commercially and are coloured to match different wood species. Successful filling of voids in wood requires the filler to precisely match the colour and grain pattern of the wood around the void, which is difficult to achieve in practice. Furthermore, filled voids do not behave like wood during subsequent finishing steps, and they age differently to wood. Hence, repairs to wood using fillers may noticeable. Therefore filling is best used with opaque finishes rather than semitransparent finishes, which allow the grain of the wood to be seen.\n\nGlue smears and droplets are sometimes present around the joints of furniture. They can be removed using a combination of scraping, scrubbing and sanding. These approaches remove surface glue, but not the glue beneath the wood surface. Sub-surface glue will reduce the absorption of stain by wood, and may alter the scratch pattern created by sanding. Both these effects will influence the way in which the wood colours when stains are used to finish the wood. To overcome this problem it may be necessary to locally stain and touch-up areas previously covered by glue to ensure that the finish on such areas matches that of the surrounding wood.\n\nWood surfaces are occasionally affected by various organic and inorganic stains. Sometimes such stains enhance the colour and appearance of wood. For example, oak wood affected by the beef-steak fungus has a deep rich, attractive, brown colour and there is no reason to remove the stain from the wood prior to finishing. The same applies to spalted wood whose attractive appearance is again caused by fungi. On the other hand some fungal stains and those caused by the reaction of iron with wood can disfigure wood. These stains can be removed from wood using bleach. Bleaches are also occasionally used to reduce the difference in colour between lighter sapwood and heartwood and also colour variation within heartwood. Such bleaching make it easier to achieve a uniformly coloured wood when the wood is subsequently coloured with pigmented stains and dyes (see below). Furthermore, the natural colours of wood fade when wood is exposed to sunlight, and more permanent colours can be created by bleaching wood to remove its natural colour and then re-colouring the wood using artificial, light-fast, stains.\n\nThe bleaches used to remove unwanted fungal stains from wood include two-part peroxide bleach and solutions of sodium hypochlorite. The former is particularly effective at removing the natural colour of wood before it is recoloured with pigmented stains or dyes. Oxalic acid is particularly effective at removing iron stains from wood.\n\nWood can be stained to change its colour or left unstained before application of lacquer, or other types of top-coats. Staining should enhance the appearance of wood by reducing colour variation between and within sapwood and heartwood. It also provides a way of giving bland looking woods such as poplar, the appearance of prized furniture woods such as ebony, mahogany or walnut. Wood can be stained using dyes or pigmented finishes. These finishes are available in a wide variety of colours, many of which are not part of the natural colour palette of wood, for example, blues and greens. Pigmented stains tend to highlight the grain (and also sanding scratches), whereas dyes do not have this effect and are more transparent. Wood can also be coloured by exposing it to chemicals that react with the wood to form coloured compounds. Chemical staining of wood is rarely carried out because it is easier to colour wood using dye or pigmented stain, however, ammonia fuming is a chemical staining method that is still occasionally used to darken woods such as oak that contain a lot of tannins. Staining of wood is difficult to control because some parts of the wood absorb more stain than others, which leads to problems such as blotchiness and streaking. For this reason, as pointed out by Flexner, many people prefer to omit the staining step when finishing wood.\n\nWood finishing starts with sanding either by hand, typically using a sanding block or power sander, scraping, or planing. Imperfections or nail holes on the surface may be filled using wood putty or pores may be filled using wood filler. Often, the wood's color is changed by staining, bleaching, or any of a number of other techniques.\n\nOnce the wood surface is prepared and stained, the finish is applied. It usually consists of several coats of wax, shellac, drying oil, lacquer, varnish, or paint, and each coat is typically followed by sanding.\n\nFinally, the surface may be polished or buffed using steel wool, pumice, rotten stone or other materials, depending on the shine desired. Often, a final coat of wax is applied over the finish to add a degree of protection.\n\nFrench polishing is a finishing method of applying many thin coats of shellac using a rubbing pad, yielding a very fine glossy finish.\n\nAmmonia fuming is a traditional process for darkening and enriching the color of white oak. Ammonia fumes react with the natural tannins in the wood and cause it to change colours. The resulting product is known as \"fumed oak\".\n\nThere are three major types of finish:\n\nWax is an evaporative finish because it is dissolved in turpentine or petroleum distillates to form a soft paste. After these distillates evaporate, a wax residue is left over.\n\nReactive finishes may use solvents such as white spirits and naphtha as a base. Varnishes, linseed oil and tung oil are reactive finishes, meaning they change chemically when they cure, unlike evaporative finishes. This chemical change is typically a polymerisation, and the resultant material is less readily dissolved in solvents .\n\nTung oil and linseed oil are reactive finishes that cure by reacting with oxygen, but do not form a film.\n\nWater based finishes generally fall into the coalescing category.\n\nClear finishes are intended to make wood look good and meet the demands to be placed on the finish. Choosing a clear finish for wood involves trade-offs between appearance, protection, durability, safety, requirements for cleaning, and ease of application. The following table compares the characteristics of different clear finishes. 'Rubbing qualities' indicates the ease with which a finish can be manipulated to deliver the finish desired.\nShellac should be considered in two different ways. It is used thinned with denatured alcohol as a finish and as a way to manipulate the wood's ability to absorb other finishes. The alcohol evaporates almost immediately to yield a finish that will attach to virtually any surface, even glass, and virtually any other finish can be used over it.\n\n\" accentuates visual properties due to differences in wood grain.\"\n\nManufacturers who mass-produce products implement automated flatline finish systems. These systems consist of a series of processing stations that may include sanding, dust removal, staining, sealer and topcoat applications. As the name suggests, the primary part shapes are flat. Liquid wood finishes are applied via automated spray guns in an enclosed environment or spray cabin. The material then can enter an oven or be sanded again depending on the manufacturer’s setup. The material can also be recycled through the line to apply another coat of finish or continue in a system that adds successive coats depending on the layout of the production line. The systems typically used one of two approaches to production.\n\nIn the hangline approach, wood items being finished are hung by carriers or hangers that are attached to a conveyor system that moves the items overhead or above the floor space. The conveyor itself can be ceiling mounted, wall mounted or supported by floor mounts. A simple overhead conveyor system can be designed to move wood products through several wood finishing processes in a continuous loop. The hangline approach to automated wood finishing also allows the option of moving items up to warmer air at the ceiling level to speed up drying process.\n\nThe towline approach to automating wood finishing uses mobile carts that are propelled by conveyors mounted in or on the floor. This approach is useful for moving large, awkward shaped wood products that are difficult or impossible to lift or hang overhead, such as four-legged wood furniture. The mobile carts used in the towline approach can be designed with top platens that rotate either manually or automatically. The rotating top platens allow the operator to have easy access to all sides of the wood item throughout the various wood finishing processes such as sanding, painting and sealing.\n\n\n\n"}
{"id": "171396", "url": "https://en.wikipedia.org/wiki?curid=171396", "title": "Zone melting", "text": "Zone melting\n\nZone melting (or zone refining or floating zone process or travelling melting zone) is a group of similar methods of purifying crystals, in which a narrow region of a crystal is melted, and this molten zone is moved along the crystal. The molten region melts impure solid at its forward edge and leaves a wake of purer material solidified behind it as it moves through the ingot. The impurities concentrate in the melt, and are moved to one end of the ingot. Zone refining was invented by John Desmond Bernal and further developed by William Gardner Pfann in Bell Labs as a method to prepare high purity materials, mainly semiconductors, for manufacturing transistors. Its early use was on germanium for this purpose, but it can be extended to virtually any solute-solvent system having an appreciable concentration difference between solid and liquid phases at equilibrium. This process is also known as the float zone process, particularly in semiconductor materials processing.\n\nThe principle is that the segregation coefficient \"k\" (the ratio of an impurity in the solid phase to that in the liquid phase) is usually less than one. Therefore, at the solid/liquid boundary, the impurity atoms will diffuse to the liquid region. Thus, by passing a crystal boule through a thin section of furnace very slowly, such that only a small region of the boule is molten at any time, the impurities will be segregated at the end of the crystal. Because of the lack of impurities in the leftover regions which solidify, the boule can grow as a perfect single crystal if a seed crystal is placed at the base to initiate a chosen direction of crystal growth. When high purity is required, such as in semiconductor industry, the impure end of the boule is cut off, and the refining is repeated.\n\nIn zone refining, solutes are segregated at one end of the ingot in order to purify the remainder, or to concentrate the impurities. In zone leveling, the objective is to distribute solute evenly throughout the purified material, which may be sought in the form of a single crystal. For example, in the preparation of a transistor or diode semiconductor, an ingot of germanium is first purified by zone refining. Then a small amount of antimony is placed in the molten zone, which is passed through the pure germanium. With the proper choice of rate of heating and other variables, the antimony can be spread evenly through the germanium. This technique is also used for the preparation of silicon for use in computer chips.\n\nA variety of heaters can be used for zone melting, with their most important characteristic being the ability to form short molten zones that move slowly and uniformly through the ingot. Induction coils, ring-wound resistance heaters, or gas flames are common methods. Another method is to pass an electric current directly through the ingot while it is in a magnetic field, with the resulting magnetomotive force carefully set to be just equal to the weight in order to hold the liquid suspended. Optical heaters using high powered halogen or xenon lamps are used extensively in research facilities particularly for the production of insulators, but their use in industry is limited by the relatively low power of the lamps, which limits the size of crystals produced by this method. Zone melting can be done as a batch process, or it can be done continuously, with fresh impure material being continually added at one end and purer material being removed from the other, with impure zone melt being removed at whatever rate is dictated by the impurity of the feed stock.\n\n\"Indirect-heating floating zone\" methods use an induction-heated tungsten ring to heat the ingot radiatively, and are useful when the ingot is of a high-resistivity semiconductor on which classical induction heating is ineffective.\n\nWhen the liquid zone moves by a distance formula_1, the number of impurities in the liquid change. Impurities are incorporated in the melting liquid and freezing solid.\n\nThe number of impurities in the liquid changes in accordance with the expression below during the movement formula_1 of the molten zone\n\nIn solar cells float zone processing is particularly useful because the single crystal silicon grown has desirable properties. The bulk charge carrier lifetime in float-zone silicon is the highest among various manufacturing processes. Float-zone carrier lifetimes are around 1000 microseconds compared to 20-200 microseconds with Czochralski process, and 1–30 microseconds with cast multi-crystalline silicon. A longer bulk lifetime increases the efficiency of solar cells significantly.\n\nAnother related process is zone remelting, in which two solutes are distributed through a pure metal. This is important in the manufacture of semiconductors, where two solutes of opposite conductivity type are used. For example, in germanium, pentavalent elements of group V such as antimony and arsenic produce negative (n-type) conduction and the trivalent elements of group III such as aluminum and boron produce positive (p-type) conduction. By melting a portion of such an ingot and slowly refreezing it, solutes in the molten region become distributed to form the desired n-p and p-n junctions.\n\n\n"}
