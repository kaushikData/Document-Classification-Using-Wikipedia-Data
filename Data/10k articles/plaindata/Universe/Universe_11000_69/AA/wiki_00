{"id": "1240842", "url": "https://en.wikipedia.org/wiki?curid=1240842", "title": "Adjunction space", "text": "Adjunction space\n\nIn mathematics, an adjunction space (or attaching space) is a common construction in topology where one topological space is attached or \"glued\" onto another. Specifically, let \"X\" and \"Y\" be topological spaces with \"A\" a subspace of \"Y\". Let \"f\" : \"A\" → \"X\" be a continuous map (called the attaching map). One forms the adjunction space \"X\" ∪ \"Y\" by taking the disjoint union of \"X\" and \"Y\" and identifying \"y\" with \"f\"(\"y\") for all \"y\" in \"A\". Formally,\n\nSometimes, the adjunction is written as formula_2. Intuitively, one may think of \"Y\" as being glued onto \"X\" via the map \"f\".\n\nAs a set, \"X\" ∪ \"Y\" consists of the disjoint union of \"X\" and (\"Y\" − \"A\"). The topology, however, is specified by the quotient construction. In the case where \"A\" is a closed subspace of \"Y\" one can show that the map \"X\" → \"X\" ∪ \"Y\" is a closed embedding and (\"Y\" − \"A\") → \"X\" ∪ \"Y\" is an open embedding.\n\n\nThe attaching construction is an example of a pushout in the category of topological spaces. That is to say, the adjunction space is universal with respect to following commutative diagram:\n\nHere \"i\" is the inclusion map and φ, φ are the maps obtained by composing the quotient map with the canonical injections into the disjoint union of \"X\" and \"Y\". One can form a more general pushout by replacing \"i\" with an arbitrary continuous map \"g\" — the construction is similar. Conversely, if \"f\" is also an inclusion the attaching construction is to simply glue \"X\" and \"Y\" together along their common subspace.\n\n\n"}
{"id": "26665747", "url": "https://en.wikipedia.org/wiki?curid=26665747", "title": "Ancillary services (electric power)", "text": "Ancillary services (electric power)\n\nThe United States Federal Energy Regulatory Commission (FERC) defines the ancillary services as:\n\"those services necessary to support the transmission of electric power from seller to purchaser given the obligations of control areas and transmitting utilities within those control areas to maintain reliable operations of the interconnected transmission system.\"\n\nAncillary services are the specialty services and functions provided by the electric grid that facilitate and support the continuous flow of electricity so that supply will continually meet demand. The term ancillary services is used to refer to a variety of operations beyond generation and transmission that are required to maintain grid stability and security. These services generally include, frequency control, spinning reserves and operating reserves. Traditionally ancillary services have been provided by generators, however, the integration of intermittent generation and the development of smart grid technologies have prompted a shift in the equipment that can be used to provide ancillary services.\n\nFERC identifies six different kinds of ancillary services:\n\nUsually performed by the independent system operator or transmission system operator, both are services dedicated to the commitment and coordination of the generation and transmission units in order to maintain the reliability of the power grid.\n\nScheduling refers to before-the-fact actions (like scheduling a generator to produce a certain amount of power the next week), while dispatch refers to the real-time control of the available resources.\n\nReactive power can be used to compensate the voltage drops, but must be provided closer to the loads than real power needs (this is because reactive power tend to travel badly through the grid). Notice that voltage can be controlled also using transformer taps and voltage regulators.\n\nFrequency control refers to the need to ensure that the grid frequency stays within a specific range of the nominal frequency. Mismatch between electricity generation and demand causes variations in frequency, so control services are required to bring the frequency back to its nominal value and ensure it does not vary out of range.\n\nAn operating reserve is a generator that can quickly be dispatched to ensure that there is sufficient energy generation to meet load. Spinning reserves are generators that are already online and can rapidly increase their power output to meet fast changes in demand. Spinning reserves are required because demand can vary on short timescales and rapid response is needed. Other operating reserves are generators that can be dispatched by the operator to meet demand, but that cannot respond as quickly as spinning reserves.\n\nThe grid integration of renewable generation simultaneously requires additional ancillary services and has the potential to provide ancillary services to the grid. The inverters that are installed with distributed generation systems and roof top solar systems have the potential to provide many of the ancillary services that are traditionally provided by spinning generators and voltage regulators. These services include reactive power compensation, voltage regulation, flicker control, active power filtering and harmonic cancellation. Wind turbines with variable-speed generators have the potential to add synthetic inertia to the grid and assist in frequency control. Hydro-Québec began requiring synthetic inertia in 2005 as the first grid operator, demanding a temporary 6% power boost when countering frequency drop by combining the power electronics with the rotational inertia of a wind turbine rotor. Similar requirements came into effect in Europe in 2016.\n\nPlug-in electric vehicles have the potential to be utilized to provide ancillary services to the grid, specifically load regulation and spinning reserves. Plug-in electric vehicles can behave like distributed energy storage and have the potential to discharge power back to the grid through bidirectional flow, referred to as vehicle-to-grid (V2G). Plug-in electric vehicles have the ability to supply power at a fast rate which enables them to be used like spinning reserves and provide grid stability with the increased use of intermittent generation such as wind and solar. The technologies to utilize electric vehicles to provide ancillary services are not yet widely implemented, but there is much anticipation of their potential.\n\n\n"}
{"id": "1659281", "url": "https://en.wikipedia.org/wiki?curid=1659281", "title": "Asbestos cement", "text": "Asbestos cement\n\nThe name fibro or fibrolite is short for \"fibrous (or fibre) cement sheet\", more commonly called \"asbestos cement sheet\" or \"AC sheet\". It is a building material in which asbestos fibres are used to reinforce thin rigid cement sheets. Asbestos-cement is a modern product, utilized mainly in industrial work due to the plain surface and lack of stylistic elements on each sheet. The material rose to necessity during World War II to make sturdy, inexpensive military housing, and continued to be used as an affordable substitute for many roofing products following the war. Advertised as a fireproof alternative to other roofing materials such as asphalt, asbestos-cement roofs were popular not only for safety but also for affordability. Due to asbestos-cement’s imitation of more expensive materials such as wood siding and shingles, brick, slate, and stone, the product was marketed as an affordable renovation material. Asbestos-cement faced competition with the aluminum alloy, available in large quantities after WWII, and the reemergence of wood clapboard and vinyl siding in the mid to late twentieth century. \n\nAsbestos-cement is usually formed into flat or corrugated sheets or piping, but can be molded into any shape wet cement can fit. In Europe, many forms were historically used for cement sheets, while the US leaned more conservative in material shapes due to labor and production costs. Although fibro was used in a number of countries, it was in Australia and New Zealand where its use was the most widespread. Predominantly manufactured and sold by James Hardie & Co. until the mid-1980s, fibro in all its forms was a very popular building material, largely due to its durability. The reinforcing fibres involved in construction were almost always asbestos. \n\nThe use of fibro that contains asbestos has been banned in several countries, including Australia. As recently as 2016, the material has been discovered in new components sold for construction projects.\n\nWhen exposed to weather and erosion elements, such as in roofs, the surface corrosion of asbestos cement can be a source of airborne toxic fibres. Asbestos is directly related to a number of life-threatening diseases including, asbestosis, pleural mesothelioma (lung) and peritoneal mesothelioma (abdomen). Fibre cement sheet is still readily available, but the reinforcing fibres are now cellulose rather than asbestos.\nHowever the name \"fibro\" is still applied to it for traditional reasons.\n\n\nSome Australian states, such as Queensland, prohibit the cleaning of fibro with pressure washers, because it can spread the embedded asbestos fibres over a wide area. Safer cleaning methods involve using a fungicide and a sealant.\n\nIn the James Blundell & James Reyne song \"Way Out West\", there is a reference to a fibro cement house, with the original Dingoes' version of the song having a reference to a house of fibre cement. Fibro is also mentioned several times on the Australian TV show \"Housos\".\n\n\n"}
{"id": "4194", "url": "https://en.wikipedia.org/wiki?curid=4194", "title": "Bohrium", "text": "Bohrium\n\nBohrium is a synthetic chemical element with symbol Bh and atomic number 107. It is named after Danish physicist Niels Bohr. As a synthetic element, it can be created in a laboratory but is not found in nature. It is radioactive: its most stable known isotope, Bh, has a half-life of approximately 61 seconds, though the unconfirmed Bh may have a longer half-life of about 690 seconds.\n\nIn the periodic table of the elements, it is a d-block transactinide element. It is a member of the 7th period and belongs to the group 7 elements as the fifth member of the 6d series of transition metals. Chemistry experiments have confirmed that bohrium behaves as the heavier homologue to rhenium in group 7. The chemical properties of bohrium are characterized only partly, but they compare well with the chemistry of the other group 7 elements.\n\nTwo groups claimed discovery of the element. Evidence of bohrium was first reported in 1976 by a Soviet research team led by Yuri Oganessian, in which targets of bismuth-209 and lead-208 were bombarded with accelerated nuclei of chromium-54 and manganese-55 respectively. Two activities, one with a half-life of one to two milliseconds, and the other with an approximately five-second half-life, were seen. Since the ratio of the intensities of these two activities was constant throughout the experiment, it was proposed that the first was from the isotope bohrium-261 and that the second was from its daughter dubnium-257. Later, the dubnium isotope was corrected to dubnium-258, which indeed has a five-second half-life (dubnium-257 has a one-second half-life); however, the half-life observed for its parent is much shorter than the half-lives later observed in the definitive discovery of bohrium at Darmstadt in 1981. The IUPAC/IUPAP Transfermium Working Group (TWG) concluded that while dubnium-258 was probably seen in this experiment, the evidence for the production of its parent bohrium-262 was not convincing enough.\n\nIn 1981, a German research team led by Peter Armbruster and Gottfried Münzenberg at the GSI Helmholtz Centre for Heavy Ion Research (GSI Helmholtzzentrum für Schwerionenforschung) in Darmstadt bombarded a target of bismuth-209 with accelerated nuclei of chromium-54 to produce 5 atoms of the isotope bohrium-262:\n\nThis discovery was further substantiated by their detailed measurements of the alpha decay chain of the produced bohrium atoms to previously known isotopes of fermium and californium. The IUPAC/IUPAP Transfermium Working Group (TWG) recognised the GSI collaboration as official discoverers in their 1992 report.\n\nIn September 1992, the German group suggested the name \"nielsbohrium\" with symbol \"Ns\" to honor the Danish physicist Niels Bohr. The Soviet scientists at the Joint Institute for Nuclear Research in Dubna, Russia had suggested this name be given to element 105 (which was finally called dubnium) and the German team wished to recognise both Bohr and the fact that the Dubna team had been the first to propose the cold fusion reaction to solve the controversial problem of the naming of element 105. The Dubna team agreed with the German group's naming proposal for element 107.\n\nThere was an element naming controversy as to what the elements from 104 to 106 were to be called; the IUPAC adopted \"unnilseptium\" (symbol \"Uns\") as a temporary, systematic element name for this element. In 1994 a committee of IUPAC recommended that element 107 be named \"bohrium\", not \"nielsbohrium\", since there was no precedence for using a scientist's complete name in the naming of an element. This was opposed by the discoverers as there was some concern that the name might be confused with boron and in particular the distinguishing of the names of their respective oxyanions, \"bohrate\" and \"borate\". The matter was handed to the Danish branch of IUPAC which, despite this, voted in favour of the name \"bohrium\", and thus the name \"bohrium\" for element 107 was recognized internationally in 1997; the names of the respective oxyanions of boron and bohrium remain unchanged despite their homophony.\n\nBohrium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes of bohrium have been reported with atomic masses 260–262, 264–267, 270–272, 274, and 278, one of which, bohrium-262, has a known metastable state. All of these but the unconfirmed Bh decay only through alpha decay, although some unknown bohrium isotopes are predicted to undergo spontaneous fission.\n\nThe lighter isotopes usually have shorter half-lives; half-lives of under 100 ms for Bh, Bh, Bh, and Bh were observed. Bh, Bh, Bh, and Bh are more stable at around 1 s, and Bh and Bh have half-lives of about 10 s. The heaviest isotopes are the most stable, with Bh and Bh having measured half-lives of about 61 s and 40 s respectively, and the even heavier unconfirmed isotope Bh appearing to have an even longer half-life of about 690 s. The unknown isotopes Bh and Bh are predicted to have even longer half-lives of around 90 minutes and 40 minutes respectively. Before its discovery, Bh was also predicted to have a half-life of 90 minutes, but it was found to have a half-life of only about 40 seconds.\n\nThe proton-rich isotopes with masses 260, 261, and 262 were directly produced by cold fusion, those with mass 262 and 264 were reported in the decay chains of meitnerium and roentgenium, while the neutron-rich isotopes with masses 265, 266, 267 were created in irradiations of actinide targets. The five most neutron-rich ones with masses 270, 271, 272, 274, and 278 (unconfirmed) appear in the decay chains of Nh, Mc, Mc, Ts, and Fl respectively. These eleven isotopes have half-lives ranging from about ten milliseconds for Bh to about one minute for Bh and Bh, extending to about twelve minutes for the unconfirmed Bh, one of the longest-lived known superheavy nuclides.\n\nBohrium is the fifth member of the 6d series of transition metals and the heaviest member of group 7 in the periodic table, below manganese, technetium and rhenium. All the members of the group readily portray their group oxidation state of +7 and the state becomes more stable as the group is descended. Thus bohrium is expected to form a stable +7 state. Technetium also shows a stable +4 state whilst rhenium exhibits stable +4 and +3 states. Bohrium may therefore show these lower states as well. The higher +7 oxidation state is more likely to exist in oxyanions, such as perbohrate, , analogous to the lighter permanganate, pertechnetate, and perrhenate. Nevertheless, bohrium(VII) is likely to be unstable in aqueous solution, and would probably be easily reduced to the more stable bohrium(IV).\n\nTechnetium and rhenium are known to form volatile heptoxides MO (M = Tc, Re), so bohrium should also form the volatile oxide BhO. The oxide should dissolve in water to form perbohric acid, HBhO.\nRhenium and technetium form a range of oxyhalides from the halogenation of the oxide. The chlorination of the oxide forms the oxychlorides MOCl, so BhOCl should be formed in this reaction. Fluorination results in MOF and MOF for the heavier elements in addition to the rhenium compounds ReOF and ReF. Therefore, oxyfluoride formation for bohrium may help to indicate eka-rhenium properties. Since the oxychlorides are asymmetrical, and they should have increasingly large dipole moments going down the group, they should become less volatile in the order TcOCl > ReOCl > BhOCl: this was experimentally confirmed in 2000 by measuring the enthalpies of adsorption of these three compounds. The values are for TcOCl and ReOCl are −51 kJ/mol and −61 kJ/mol respectively; the experimental value for BhOCl is −77.8 kJ/mol, very close to the theoretically expected value of −78.5 kJ/mol.\n\nBohrium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure (/ = 1.62), similar to its lighter congener rhenium. It should be a very heavy metal with a density of around 37.1 g/cm, which would be the third-highest of any of the 118 known elements, lower than only meitnerium (37.4 g/cm) and hassium (41 g/cm), the two following elements in the periodic table. In comparison, the densest known element that has had its density measured, osmium, has a density of only 22.61 g/cm. This results from bohrium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough bohrium to measure this quantity would be impractical, and the sample would quickly decay.\n\nThe atomic radius of bohrium is expected to be around 128 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Bh ion is predicted to have an electron configuration of [Rn] 5f 6d 7s, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues manganese and technetium. Rhenium, on the other hand, follows its heavier congener bohrium in giving up a 5d electron before a 6s electron, as relativistic effects have become significant by the sixth period, where they cause among other things the yellow color of gold and the low melting point of mercury. The Bh ion is expected to have an electron configuration of [Rn] 5f 6d 7s; in contrast, the Re ion is expected to have a [Xe] 4f 5d configuration, this time analogous to manganese and technetium. The ionic radius of hexacoordinate heptavalent bohrium is expected to be 58 pm (heptavalent manganese, technetium, and rhenium having values of 46, 57, and 53 pm respectively). Pentavalent bohrium should have a larger ionic radius of 83 pm.\n\nIn 1995, the first report on attempted isolation of the element was unsuccessful, prompting new theoretical studies to investigate how best to investigate bohrium (using its lighter homologs technetium and rhenium for comparison) and removing unwanted contaminating elements such as the trivalent actinides, the group 5 elements, and polonium.\n\nIn 2000, it was confirmed that although relativistic effects are important, bohrium behaves like a typical group 7 element. A team at the Paul Scherrer Institute (PSI) conducted a chemistry reaction using six atoms of Bh produced in the reaction between Bk and Ne ions. The resulting atoms were thermalised and reacted with a HCl/O mixture to form a volatile oxychloride. The reaction also produced isotopes of its lighter homologues, technetium (as Tc) and rhenium (as Re). The isothermal adsorption curves were measured and gave strong evidence for the formation of a volatile oxychloride with properties similar to that of rhenium oxychloride. This placed bohrium as a typical member of group 7. The adsorption enthalpies of the oxychlorides of technetium, rhenium, and bohrium were measured in this experiment, agreeing very well with the theoretical predictions and implying a sequence of decreasing oxychloride volatility down group 7 of TcOCl > ReOCl > BhOCl.\n\nThe longer-lived heavy isotopes of bohrium, produced as the daughters of heavier elements, offer advantages for future radiochemical experiments. Although the heavy isotope Bh requires a rare and highly radioactive berkelium target for its product, the isotopes Bh, Bh, and Bh can be readily produced as daughters of more easily produced moscovium and nihonium isotopes.\n\n"}
{"id": "18714420", "url": "https://en.wikipedia.org/wiki?curid=18714420", "title": "Brașov Power Station", "text": "Brașov Power Station\n\nThe Braşov Power Station is a CHP thermal power plant located in Braşov, having 2 generation groups of 50 MW each having a total electricity generation capacity of 100 MW.\n\n"}
{"id": "5181235", "url": "https://en.wikipedia.org/wiki?curid=5181235", "title": "Brian O'Leary", "text": "Brian O'Leary\n\nBrian Todd O'Leary (January 27, 1940 – July 28, 2011) was an American scientist, author, and former NASA astronaut. He was part of NASA Astronaut Group 6, a group of scientist-astronauts chosen with the intention of training for the Apollo Applications Program.\n\nO'Leary was born and raised in Boston, Massachusetts on January 27, 1940. He decided to become an astronaut after visiting Washington, D.C. as a teenager.\n\nO'Leary graduated from Belmont High School in 1957. He received a B.A. in physics from Williams College in 1961, an M.A. in astronomy from Georgetown University in 1964, and a Ph.D. in astronomy from the University of California, Berkeley in 1967.\n\nO'Leary became a Fellow of the American Association for the Advancement of Science in 1975. From 1970–1976, he was the secretary of the American Geophysical Union's Planetology Section. In 1977, he worked on Asteroidal Resources Group, NASA Ames Summer Study on Space Settlements as team leader.\n\nDuring his graduate studies at the University of California, Berkeley, O'Leary published several scientific papers on the atmosphere of Mars. O'Leary's Ph.D. thesis in 1967 was on the Martian surface. Upon finishing his Ph.D., O'Leary was chosen for a possible Human mission to Mars that NASA was planning at the time. O'Leary was the only planetary scientist-astronaut in NASA Astronaut Corps during the Apollo program. In April 1968, O'Leary left the astronaut program.\n\nAfter O'Leary's resignation from NASA, Carl Sagan invited him to lecture at Cornell University in 1968, where he stayed until 1971 as a research associate (1968–1969) and assistant professor (1969–1971) of astronomy. While at Cornell, he studied lunar mascons. During the 1970–1971 academic year, O'Leary was deputy team leader of the Mariner 10 Venus-Mercury TV Science Team as a visiting researcher at the California Institute of Technology. The team received NASA's group achievement award for its participation. He later taught at San Francisco State University (associate professor of astronomy and interdisciplinary sciences; 1971–1972), the UC Berkeley School of Law (visiting associate professor; 1971–1972), Hampshire College (assistant professor of astronomy and science policy assessment; 1972–1975), Princeton University (research staff and lecturer in physics; 1976–1981) and California State University, Long Beach (visiting lecturer in physics; 1986–1987).\n\nAt Princeton, he was involved with Gerard K. O'Neill and the L5 Society's orbiting city plans. He suggested that passing asteroids and the moons of Mars would be the easiest to access resources for space colonies.\n\nO'Leary wrote and edited books on astronomy and astronautics.\n\nO'Leary became politically active early in his career and participated in a demonstration in Washington, D.C. in 1970, to protest the Cambodian Campaign. Richard Nixon administration officials invited O'Leary and his fellow Cornell professors to present their views. In 1975 and 1976, he worked on was Morris Udall's presidential campaign as an energy advisor, as well as for the U.S. House Interior Committee subcommittee on energy and the environment as Udall's special staff consultant on energy. O'Leary worked for U.S. presidential candidates Jesse Jackson, Dennis Kucinich, George McGovern, and Walter Mondale.\n\nDuring those years, he wrote about the Space Shuttle, NASA's lunar landings, and the weaponization of space. O'Leary traveled to the Soviet Union twice in the late 1980s with the aim of promoting peaceful space exploration, including a peace cruise along the Dnieper River.\n\nA remote viewing experience in 1979 and a near-death experience in 1982 initiated O'Leary's departure from orthodox science. After Princeton, O'Leary worked Science Applications International Corporation. He refused to work on military space applications, for which reason he lost his position there in 1987. Beginning in 1987, O'Leary increasingly explored unorthodox ideas, particularly the relationship between consciousness and science, and became widely known for his writings on \"the frontiers of science, space, energy and culture\".\n\nSince the 1980s, he lectured at the Findhorn Foundation, Esalen Institute, Omega Institute for Holistic Studies, Unity Churches, Religious Science churches and Sivananda Yoga Vedanta Centres.\n\nWith Meredith Miller, his artist wife, he co-founded the Montesueños Eco-Retreat in Vilcabamba, Ecuador in 2008, which is devoted to \"peace, sustainability, the arts and new science\".\n\nO'Leary contracted skin cancer in his 60s, which he treated with an alternative methodology involving a substance called Cansema. After having his second heart attack in 2010, he died of intestinal cancer on July 28, 2011, soon after diagnosis, at his home in Vilcabamba.\n\n\n"}
{"id": "926513", "url": "https://en.wikipedia.org/wiki?curid=926513", "title": "Burns' Day Storm", "text": "Burns' Day Storm\n\nThe Burns' Day Storm (also known as Cyclone Daria) was an extremely violent windstorm that took place on 25–26 January 1990 over north-western Europe. It is one of the strongest European windstorms on record. This storm has received different names as there is no official list of such events in Europe. Starting on the birthday of Scottish poet Robert Burns, it caused widespread damage and hurricane-force winds over a wide area. The storm was responsible for 97 deaths (according to the Met Office), although figures have ranged from 89 to over 100.\n\nThe storm began as a cold front over the Northern Atlantic Ocean on 23 January. By the 24th, it had a minimum central pressure of 992 mbar and began to undergo explosive cyclogenesis, sometimes referred to as a weather bomb. It made landfall on the morning of the 25th over Ireland, where 17 died, including 8 on a bus which was struck by a falling tree. It then tracked over to Ayrshire in Scotland. The lowest pressure of 949 mbar was recorded near Edinburgh around 16:00. After hitting the United Kingdom, the storm tracked rapidly east towards Denmark, causing major damage and 30 deaths in the Netherlands and Belgium.\n\nThe strongest sustained winds recorded were between 70 and 75 mph (110–120 km/h), comparable to a weak Category 1 hurricane or Hurricane-force 12 on the Beaufort Scale. Strong gusts of up to 104 mph (170 km/h;) were reported, and it was these which caused the most extensive damage.\n\nThe Burns' Day Storm of 1990 has been given as an example of when the Met Office \"got the prediction right\". The model forecast hinged on observations from two ships in the Atlantic near the developing storm the day before it reached the UK.\n\nDuring the day of the storm the Royal Netherlands Meteorological Institute (KNMI) increased warnings to force 11 and eventually to hurricane force 12. Research conducted by them showed that most of the general public were not able to understand the severity of the warnings. The storm has led to more awareness about the understanding of storminess among the public by the KNMI, who started a teletext page and the introduction of special warnings for extreme weather events in reaction to these findings.\n\nCasualties were much higher than those of the Great Storm of 1987, because the storm hit during the daytime. The storm caused extensive damage, with approximately 3 million trees downed, power disrupted to over 500,000 homes and severe flooding in England and West Germany. The storm cost insurers in the UK £3.37 billion, the UK's most expensive weather event to insurers. Most of the deaths were caused by collapsing buildings or falling debris. In one case in Sussex, a class of children was evacuated just minutes before their school building collapsed. Actor Gorden Kaye was also injured during the storm, when a plank of an advertising board was blown through his car's windscreen.\n\n\n"}
{"id": "24844937", "url": "https://en.wikipedia.org/wiki?curid=24844937", "title": "Buzău Power Station", "text": "Buzău Power Station\n\nThe Buzău Power Station is a large thermal power plant located in Buzău, having 4 generation groups of 45 MW and one group of 27 MW and having a total electricity generation capacity of 207 MW.\n"}
{"id": "53522779", "url": "https://en.wikipedia.org/wiki?curid=53522779", "title": "CC-HOD", "text": "CC-HOD\n\nCC-HOD (Catalytic Carbon - Hydrogen On Demand) is a process splits water into hydrogen and oxygen at low temperatures.\n\nThe CC-HOD method was discovered by Howard Phillips, managing director of Phillips Company, a pharmaceutical manufacturing company.\n\nHydrogen is energy dense and environmentally benign because upon combustion it produces only water vapor. Most hydrogen fuel is produced by electrolysis, a process that requires substantial energy. Phillips proposed that CC-HOD could become an alternative to electrolysis.\n\nCC-HOD requires that water be heated only modestly above ambient temperature, catalytic carbon and scrap metal. If aluminium is submerged in a tank with the catalytic carbon. Pure Hydrogen gas is formed when the water reaches a temperature of about . Oxygen remains in the tank where it bonds with the aluminium to form aluminium hydroxide:\n\nAl + H²O + CC = CC + Al(OH)³ + H²\n\nCC-HOD has mainly been researched by open science projects. LENR Ltd. is a UK based open research group specializing in validating novel energy technologies. They tested CC-HOD for several years and claimed it was reliable.\n\nBesides production of clean energy for transportation and heating CC-HOD can be used for recycling scrap metal. Depending on which type of metal is used, the byproducts from the process could be used for used to manufacture for example electronics, deodorant, paint or ink.\n\nA patent application 'Methods and systems for producing hydrogen WO2013016367 A1' was filed June 24, 2012.\n"}
{"id": "32575604", "url": "https://en.wikipedia.org/wiki?curid=32575604", "title": "Camp Grove Wind Farm", "text": "Camp Grove Wind Farm\n\nThe Camp Grove Wind Farm is a 100-turbine wind farm composed of GE 1.5MW 77 meter rotor and with 80 meter hub height wind turbines in Marshall County and Stark County Illinois, north of the city of Peoria. The owner of the project is Camp Grove Wind Farm LLC, which is owned by subsidiaries of Orion Energy Group LLC (Oakland, CA), Vision Energy, LLC (Cincinnati, OH) and other investors. The project is managed by Orion Energy Group LLC, and, at 1.5 megawatts per General Electric turbine, has a nameplate capacity of 150 megawatts (mW).\n\nThe turbine farm began operations in November 2007. 75 mW of the power generated by Camp Grove, approximately one-half of the nameplate capacity, is sold pursuant to a 20-year contract to the Appalachian Power unit of American Electric Power, a multi-state electrical generation holding company. 60 of the turbines are located in Marshall County, and 40 in the smaller Stark County. The nearest village is Camp Grove, for which the wind farm is named.\n\nOther wind farms developed by Orion and Vision include the Benton County Wind Farm and the Fowler Ridge Wind Farm.\n"}
{"id": "20267236", "url": "https://en.wikipedia.org/wiki?curid=20267236", "title": "Cave of the Crystals", "text": "Cave of the Crystals\n\nCave of the Crystals or Giant Crystal Cave () is a cave connected to the Naica Mine at a depth of , in Naica, Chihuahua, Mexico.\n\nThe main chamber contains giant selenite crystals (gypsum, CaSO·2 HO), some of the largest natural crystals ever found. The cave's largest crystal found to date is in length, in diameter and 55 tons in weight. When it was accessible, the cave was extremely hot, with air temperatures reaching up to with 90 to 99 percent humidity. The cave is relatively unexplored due to these factors. Without proper protection, people could only endure approximately ten minutes of exposure at a time.\n\nThe cave was discovered by the brothers Eloy and Javier Delgado. It has since been allowed to re-flood, with the cavern now filled once more with the water rich in minerals required for the crystals to grow.\n\nA group of scientists known as the \"Naica Project\" have been heavily involved in researching these caverns.\n\nNaica lies on an ancient fault above an underground magma chamber which is approximately below the cave. The magma heated the ground water which was saturated with sulfide ions (S). Cool oxygenated surface water contacted the mineral saturated heated water, but the two did not mix due to the difference in their densities. The oxygen slowly diffused into the heated water and oxidized the sulfides (S) into sulfates (SO). The hydrated sulfate gypsum crystallized at an extremely slow rate over the course of at least 500,000 years, forming the enormous crystals found today.\n\nIn 1910 miners discovered a cavern beneath the Naica mine workings, the Cave of Swords (). It is located at a depth of , above the Cave of the Crystals, and contains spectacular, smaller ( long) crystals. It is speculated that at this level, transition temperatures may have fallen much more rapidly, leading to an end in the growth of the crystals.\n\nGiant Crystal Cave was discovered in April 2000 by miners excavating a new tunnel for the Industrias Peñoles mining company located in Naica, Mexico, while drilling through the Naica fault, which they were concerned would flood the mine. The mining complex in Naica contains substantial deposits of silver, zinc and lead.\n\nThe Cave of Crystals is a horseshoe-shaped cavity in limestone. Its floor is covered with perfectly faceted crystalline blocks. Huge crystal beams jut out from both the blocks and the floor. The crystals deteriorate in air, so the Naica Project attempted to visually document the crystals before they deteriorate further.\n\nTwo other smaller caverns were also discovered in 2000, Queen’s Eye Cave and Candles Cave, and a further chamber was found in a drilling project in 2009. The new cave, named Ice Palace, is deep and is not flooded, but its crystal formations are much smaller, with small \"cauliflower\" formations and fine, threadlike crystals.\n\nAll of the caves discovered are: Cave of Crystals, Queen's Eye, Candles Cave, Ice Palace and Cave of Swords.\n\nA scientific team coordinated by Paolo Forti, specialist of cave minerals and crystallographer at the University of Bologna (Italy) explored the cave in detail in 2006. To survive and to be able to work in the extreme temperature and humid conditions which prevent prolonged incursion in the crystal chamber, they developed their own refrigerated suits and cold breathing systems (respectively dubbed Tolomea suit and Sinusit respirator). Special caving overalls were fitted with a mattress of refrigerating tubes placed all over the body and connected to a backpack weighing about containing a reservoir filled with cold water and ice. The cooling provided by melting ice was sufficient to provide about half an hour of autonomy.\n\nBeside mineralogical and crystallographic studies, biogeochemical and microbial characterization of the gypsum giant crystals were also performed. Stein-Erik Lauritzen (University of Bergen, Norway) performed uranium-thorium dating to determine the maximum age of the giant crystals, about 500,000 years.\n\nPenelope Boston (New Mexico Institute of Mining and Technology), speleologist and geomicrobiologist specialist of extremophile organisms, realized sterile sampling of gypsum drillcores by making small boreholes inside large crystals under aseptic conditions. The aim was to detect the possible presence of ancient bacteria encapsulated inside fluid and solid inclusions present the calcium sulfate matrix from its formation.\n\nSolid inclusions mainly consist of magnesium and iron oxy-hydroxide but no organic matter could be found associated with the solid hydroxides. No DNA from ancient bacteria could be extracted from the solid inclusions and amplified by PCR.\n\nMicrobial studies on fluid inclusions are foreseen to attempt to evidence the presence of ancient micro-organisms in the original fluid solution in which the crystals developed.\n\nOther researches also cover the fields of palynology (pollen study), geochemistry, hydrogeology and the physical conditions prevailing in the Cave of Crystals.\n\nAt the 2017 meeting of the American Association for the Advancement of Science (AAAS), researchers, including Dr. Boston, announced the discovery of bacteria found in inclusions embedded in some of the crystals. Using sterile methods, the researchers were able to extract and reanimate these organisms, which are not closely related to anything in the known genetic databases.\n\nThe cave was featured on the Discovery Channel program \"\" in February 2011. Exploration has given credence to the existence of further chambers, but further exploration would have required significant removal of the crystals. As the cave's accessibility is dependent on the mine's water pumps, when mineral exploitation is ended in the area it was likely pumping would stop and water level allowed to rise again.\n\nIn February 2017, Penelope Boston reported to the BBC that mining operations had ceased and that the caves have re-flooded.\n\n"}
{"id": "27655185", "url": "https://en.wikipedia.org/wiki?curid=27655185", "title": "Communauté Électrique du Bénin", "text": "Communauté Électrique du Bénin\n\nThe Communauté Electrique du Bénin - CEB (Electricity Community of Benin) is an international organisation co-owned by the governments of Bénin and Togo. It is in charge of developing electricity infrastructure in both countries which are strongly dependent on energy imports from Ghana. Most of the energy consumed by Benin and Togo is generated in Ghana.\n\nCEB owns the Nangbeto dam in Togo with an installed capacity of 64 MW. It has three customers: the Société Béninoise d'Énergie Électrique, Togo Electricité and the Togo Phosphates Agency (Office togolais des phosphates).\n\n"}
{"id": "1123493", "url": "https://en.wikipedia.org/wiki?curid=1123493", "title": "Composition C", "text": "Composition C\n\nThe Composition C family is a family of related US-specified plastic explosives consisting primarily of RDX. All can be molded by hand for use in demolition work and packed by hand into shaped charge devices. Variants have different proportions and plasticisers and include composition C-2, composition C-3, and composition C-4.\n\nThe term \"composition\" is used for any explosive material compounded from several ingredients. In particular, in the 1940s the format \"Composition <letter>\" was used for various compositions of the (relatively) novel explosive RDX, such as Composition B and other variants.\n\nThe original material was developed by the British during World War II, and was used in the Gammon bomb. It was standardised as Composition C when introduced to US service. This material consisted of 88.3% RDX and a mineral oil-based plasticiser and phlegmatiser. It suffered from a relatively limited range of serviceable temperatures, and was replaced by Composition C-2 around 1943.\n\nComposition C-1 contained a slightly smaller proportion of RDX, but used an explosive plasticiser, which contained tetryl, nitrocellulose and a mixture of nitroaromatics produced during the manufacture of TNT (containing trinitrotoluene, dinitrotoluene and mononitrotoluene), and a trace of solvent. While Composition C-3 had a much wider serviceable temperature range than Composition C, it could not be stored at elevated temperatures. Consequently, it was replaced around 1944 by Composition C-4.\n\nComposition C-3 was very similar to Composition C-1, but removed the solvent and varied the exact proportions of plasticisers to improve high temperature storage. It is a yellow, putty-like material. It remained a service item through the Korean War, but had marginal plasticity at the very low temperatures encountered in Korean winters, and was significantly toxic, including by vapour and skin absorption. The velocity of detonation is about 7600 m/s (25,000 feet per second.)\n\nComposition C-3 consists of 77%–85% cyclonite (RDX) and 15%–23% gel made out of liquid nitro compounds (e.g. liquid DNT and small amount of NT) and nitrocellulose or butyl phthalate and nitrocellulose.\n\nOne of the first reported and tested compositions of C-3 was very similar to earlier Composition C-2 and contained 77% RDX, 3% tetryl, 4% TNT, 1% NC, 5% NT, and 10% DNT.\nThe last two compounds (they are very poor explosives) are oily liquids and plasticize the mixture. The most important later innovation of C-3 introduced the non-explosive plasticizer butyl phthalate instead of this mixture of nitro compounds. This reduced the toxicity while increasing the concentration of RDX and improving safety of use and storage. It also opened the way to begin study of new non-explosive low-toxicity plasticizers (esters of dicarboxylic acid) and binder (branched polymers).\n\nResearch on a replacement for C-2 was begun prior to 1950, but the new material, new generation of Composition C (number four, C-4), did not begin pilot production until 1956.\n\nSince 1960 the mixture of C-3 has contained:\n\nIt is less volatile than C-4 and has less tendency to harden at low temperature. It has a density 1.48–1.60g/ml, does not become hard even at −55 °C (−67 °F), and does not exude at +77 °C (171 °F).\nC4 has a detonation velocity of 8092 m/s (26550 ft/s) at high density and velocity of 7550 m/s (24770 ft/s) at low density 1.48 g/ml. It is so successful that it remains in army service up to the current time without any significant changes.\n\n"}
{"id": "23403486", "url": "https://en.wikipedia.org/wiki?curid=23403486", "title": "Contract adjustment board", "text": "Contract adjustment board\n\nIn government contracting, a Contract Adjustment Board is a department board at the Secretariat level which deals with disputes and requests for extraordinary relief under Public Law 85-804 of Aug. 28, 1958.\n\nIn brief:\n\nPublic Law No. 85-804, codified at 50 U.S.C. § 1431-35 (Supp. IV 1998), grants to the President the authority to authorize any agency which exercises functions in connection with the national defense to enter into contracts or into amendments or modifications of contracts, and to make advance payments, without regard to other applicable legal provisions whenever such action would facilitate the national defense. 50 U.S.C. § 1431. The legislative history of the statute indicates that it may also be used as the basis for making indemnity payments under certain government contracts, the so-called \"residual powers. \" ECR Current Materials at 1005, 1021. The legislative history explains that \"[t]he need for indemnity clauses in most cases arises from the advent of nuclear power and the use of highly volatile fuels in the missile program. The magnitude of the risks involved under procurement contracts in these areas have rendered commercial insurance either unavailable or limited in coverage.\n\nDuring World War I, a \"Board of Contract Adjustment\" was created to determine \"all claims, doubts and disputes which may arise under departmental contracts\"; it implemented the policies for Liquidation, Cancellation, and Adjustment of contracts.\n\nAccording to Evans Reamer Machine Company v. United States. 386 F.2d 873, \"since the early days of World War II,\" the main defense agencies have been authorized to grant discretionary relief to contractors suffering losses on account of mistakes. The underpinning for the granting of relief must be a finding that such action would facilitate the national defense or prosecution of the war. Title II relief has been referred to variously as \"far-reaching,\" \"extraordinary,\" and \"a snare and a delusion.\"\n\nAccording to U.S. v. Utah Constr. & Mining Co., pursuant to a delegation by the President under Public Law 85—804, government departments and agencies exercising functions in connection with the national defense may, upon a finding that such action would 'facilitate the national defense,' enter into amendments and modifications of contracts without regard to other provisions of law respecting such amendments and modifications. As implemented by the Atomic Energy Commission's procurement regulations, the authority conferred encompasses amendments without consideration, correction of mutual mistakes, and formalization of informal commitments. This authority, which in many respects is analogous to power to settle claims, is delegated to Contract Adjustment Boards established within the departments and agencies concerned separate from the Boards of Contract Appeals. Because the regulations preclude resort to the powers conferred by Public Law 85—804 '(u)nless other legal authority in the Department concerned is deemed to be lacking or inadequate,' the Army Contract Adjustment Board has generally required contractors to exhaust remedies before the ASBCA under the disputes clause. Thus it is quite evident from the administration of Public Law 85—804 and its predecessors that the limitations on the jurisdiction of the Boards of Contract Appeals are well understood by the military procurement departments and Congress.\n\n\n\n\n"}
{"id": "24312735", "url": "https://en.wikipedia.org/wiki?curid=24312735", "title": "Dniproenergo", "text": "Dniproenergo\n\nDniproenergo () is a major electric and thermal energy producing companies in central Ukraine. Until 2012 it was known as Dniproenergo.\n\nIt is a subsidiary of the Government of Ukraine (25%), while its managed by DTEK. The company has an installed capacity of 8,185 MW that represents around 16% of the country's total installed capacity.\n\n\n"}
{"id": "52242724", "url": "https://en.wikipedia.org/wiki?curid=52242724", "title": "ESA Automation", "text": "ESA Automation\n\nESA Automation is a multinational group that manufactures technologies for industrial automation, energy management and for CNC and motion.\n\nESA Automation’s headquarters are in Mariano Comense, Como, Italy, and has branches in Spain, Germany, United States, Turkey, China and India. In Italy it has also two subsidiaries: ESA Elettronica S.p.A. in Pontedera and ESA Energy S.r.l. in Rovereto.\n\n"}
{"id": "842224", "url": "https://en.wikipedia.org/wiki?curid=842224", "title": "Elastomer", "text": "Elastomer\n\nAn elastomer is a polymer with viscoelasticity (i.e., both viscosity and elasticity) and very weak intermolecular forces, and generally low Young's modulus and high failure strain compared with other materials. The term, a portmanteau of \"elastic polymer\", is often used interchangeably with rubber, although the latter is preferred when referring to vulcanisates. Each of the monomers which link to form the polymer is usually a compound of several elements among carbon, hydrogen, oxygen and silicon. Elastomers are amorphous polymers maintained above their glass transition temperature, so that considerable molecular reconformation, without breaking of covalent bonds, is feasible. At ambient temperatures, such rubbers are thus relatively soft (E ≈ 3 MPa) and deformable. Their primary uses are for seals, adhesives and molded flexible parts. Application areas for different types of rubber are manifold and cover segments as diverse as tires, soles for shoes, and damping and insulating elements. The importance of these rubbers can be judged from the fact that global revenues are forecast to rise to US$56 billion in 2020.\n\nIUPAC defines the term \"elastomer\" by \"Polymer that displays rubber-like elasticity.\"\n\nManufacturers of Elastomeric parts include NoProto, PrintForm, 3D Systems, and Afformativ. \n\nRubber-like solids with elastic properties are called elastomers. Polymer chains are held together in these materials by relatively weak intermolecular bonds, which permit the polymers to stretch in response to macroscopic stresses. Natural rubber, neoprene rubber, buna-s and buna-n are all examples of such elastomers.\n\nElastomers are usually thermosets (requiring vulcanization) but may also be thermoplastic (see thermoplastic elastomer). The long polymer chains cross-link during curing, i.e., vulcanizing. The molecular structure of elastomers can be imagined as a 'spaghetti and meatball' structure, with the meatballs signifying cross-links. The elasticity is derived from the ability of the long chains to reconfigure themselves to distribute an applied stress. The covalent cross-linkages ensure that the elastomer will return to its original configuration when the stress is removed. As a result of this extreme flexibility, elastomers can reversibly extend from 5–700%, depending on the specific material. Without the cross-linkages or with short, uneasily reconfigured chains, the applied stress would result in a permanent deformation.\n\nTemperature effects are also present in the demonstrated elasticity of a polymer. Elastomers that have cooled to a glassy or crystalline phase will have less mobile chains, and consequentially less elasticity, than those manipulated at temperatures higher than the glass transition temperature of the polymer.\n\nIt is also possible for a polymer to exhibit elasticity that is not due to covalent cross-links, but instead for thermodynamic reasons.\n\nUnsaturated rubbers that can be cured by sulfur vulcanization:\n(Unsaturated rubbers can also be cured by non-sulfur vulcanization if desired.)\n\nSaturated rubbers that cannot be cured by sulfur vulcanization:\n\n\"The definitions are not authentic as the Rubber which is classified in World Customs Organisation Books in Chapter 40, where as the above definitions stating all rubber and different polymers in same chapter which is classified in Chapter 39 of the World Custom Organisation's Harmonised Commodity for Description and coding system. One should go through all differentiation while editing between Plastics and articles thereof and Rubber and articles thereof.\"\n\nVarious other types of 4S elastomers:\n\n"}
{"id": "6855511", "url": "https://en.wikipedia.org/wiki?curid=6855511", "title": "Electrical room", "text": "Electrical room\n\nAn electrical room is a room or space in a building dedicated to electrical equipment. Its size is usually proportional to the size of the building; large buildings may have a main electrical room and subsidiary electrical rooms. Electrical equipment may be for power distribution equipment, or for communications equipment.\n\nElectrical rooms typically house the following equipment:\nIn large building complexes, the primary electrical room may house an indoor electrical substation.\n\nThe construction features of an electrical room vary depending on the scope of the equipment to be installed. Floors may be reinforced to support heavy transformers and switchgear. Walls and ceilings may have to support a heavy cable tray system or busbars. Additional ventilation or air conditioning may be needed, since electrical apparatus gives off heat but the temperature must not rise beyond the tolerance of equipment. Double doors may be installed to allow for maintenance of large equipment. If utility service entrance equipment and metering is present in the room, special provisions may be made for access by utility personnel. Fire detection and fire suppression systems, such as carbon dioxide, may be installed. \n\nA large electrical room may have extensive provisions for grounding (earthing) and bonding enclosures of electrical equipment to prevent stray voltage and danger of electric shock, even during faults in the electrical system. Lightning protection requires different measures than protection from power-frequency faults. Electrical rooms may have electromagnetic shielding to prevent interference to nearby sensitive audio or video equipment. In large facilities access control systems may control admission to the room.\n\nLayout details and construction of electrical rooms will be controlled by local building code and electrical code regulations. Requirements for an electrical room relate to fire safety and electrical hazards. An electrical room is usually required to be secured from access by unauthorized persons; these rules are especially strict where equipment within the room has exposed live terminals. Regulations may require two separate means of exit from a room where the power rating of circuits exceeds some threshold, to allow for quick exit in an emergency. Rooms containing oil-filled equipment may be required to have fire-resistant construction or active fire suppression equipment in the room and may be designated as an \"electrical vault\". Since power distribution often requires large numbers of electrical cables, special measures for fire resistance of cables and cable trays may be also specified by regulations. \n\nIn industrial buildings that handle flammable gases or liquids, or combustible dusts, special electrical rooms may be prepared that have ventilation and other measures to prevent an explosion hazard that would otherwise exist with electrical equipment in hazardous areas. For large installations, it may be less costly overall to use a special room than to install a large number of devices that are resistant to the hazardous conditions. Similarly, in wet or corrosive environments, electrical equipment may be separated in a room that can be protected from the atmospheric conditions. \n\nBuilding code and electrical code regulations will dictate minimal working space around equipment to allow safe access during maintenance. Practical design of an electrical room will consider layout of the initial equipment and allow for additions over the economic life of the facility.\n\n"}
{"id": "12163566", "url": "https://en.wikipedia.org/wiki?curid=12163566", "title": "Environmental emergency", "text": "Environmental emergency\n\nEnvironmental emergencies are defined as “sudden-onset disasters or accidents resulting from natural, technological or human-induced factors, or a combination of these, that causes or threatens to cause severe environmental damage as well as loss of human lives and property.” (UNEP/GC.22/INF/5, 13 November 2002.)\n\nFollowing a disaster or conflict, an environmental emergency can occur when people’s health and livelihoods are at risk due to the release of hazardous and noxious substances, or because of significant damage to the ecosystem. Examples include fires, oil spills, chemical accidents, toxic waste dumping and groundwater pollution. \n\nThe environmental risks can be acute and life-threatening. According to the International Disaster Database (EM-DAT), between 2003 and 2013, there were 380 industrial accidents reported, affecting 207 668 people and resulting in over US$22 million in losses. Climate change is having an unprecedented effect on the occurrence of natural disasters and the associated risk of environmental emergencies. With climate change already stretching the disaster relief system, future climate-related emergency events will generate increased and more costly demands for assistance.\nAll disasters have some environmental impacts. \n\nSome of these may be immediate and life-threatening – for example, when an earthquake damages an industrial facility, which in turn releases hazardous materials. In such cases these so-called ‘secondary impacts’ may cause as much damage as the initial causal factor. \n\nFor example, Typhoon Haiyan/Yolanda that struck the Philippines in November 2013, caused massive destruction and had a huge human toll but also generated a spill of around 800,000 litres of heavy oil, when a power barge ran aground in Estancia, Iloilo province, at the height of the typhoon. \n\nDisasters may also have longeOÑKL-term impacts. For example, natural disasters may cause long-term waste management or ecosystem damage.\n\nThe Environmental Emergencies Forum is a unique biennial international forum that brings together disaster managers and environmental experts from governments, UN agencies, industries, academies, NGOs and civil society to improve prevention, preparedness, response and overall resilience to environmental emergencies. It also provides guidance for the Joint UNEP/OCHA Environment Unit, which provides a Secretariat to the meeting. The most recent meeting was held in Norway in June 2015. The next meeting will be held in Nairobi in June 2017.\n\nThe Joint United Nations Environment Programme (UNEP)/Office for the Coordination of Humanitarian Affairs (OCHA) Environment Unit (JEU): By pairing the UNEP’s technical expertise with OCHA’s humanitarian response network, the Joint UNEP/OCHA Environment Unit (JEU) mobilizes and coordinates a comprehensive response to environmental emergencies to protect lives, livelihoods, ecosystems and future generations. The JEU can be reached 24 hours/day, seven days/week, all year round and operates at the request of affected countries. The JEU can be called by member states when acute environmental risks to life and health as a result of conflicts, natural disasters and industrial accidents are suspected. \n\nThe JEU hosts the Environmental Emergencies Centre (www.eecentre.org), an online tool designed primarily to provide national responders with a one-stop-shop of all information relevant to the preparedness, prevention and response stages of an environmental emergency. \nWebsite: www.unocha/org/unep; www.eecentre.org\n\nThe biennial Green Star Awards recognize individuals, organizations, governments and companies who demonstrate achievements in prevention, preparedness and response to environmental emergencies. A joint initiative between Green Cross International, the UN Office for the Coordination of Humanitarian Affairs (OCHA) and the UN Environment Programme (UNEP), the Green Star Awards seeks to increase awareness of environmental emergencies by drawing attention to efforts made to prevent, prepare for and respond to such emergencies. \n\n"}
{"id": "17836373", "url": "https://en.wikipedia.org/wiki?curid=17836373", "title": "Estimated pore pressure", "text": "Estimated pore pressure\n\nEstimated pore pressure, as used in the oil industry and mud logging, is an approximation of the amount of force that is being exerted into the borehole by fluids or gases within the formation that has been penetrated.\n\nIn the oil industry, estimated pore pressure is measured in pounds per square inch (psi), but is converted to \"equivalent mud weight\" and measured in pounds per gallon (lb/gal) to more easily determine the amount of mud weight required to prevent the fluid or gas from escaping and causing a blowout or wellbore failure.\n"}
{"id": "21495119", "url": "https://en.wikipedia.org/wiki?curid=21495119", "title": "Eurelectric", "text": "Eurelectric\n\nEurelectric (full name: The Union of the Electricity Industry) is the sector association which represents the common interests of the electricity industry at a European level, plus its affiliates and associates on several other continents. eurelectric covers all major issues affecting the sector, from generation and markets to distribution networks and customer issues. eurelectric was formally established in 1989. The goal at the time was to defend the status quo against European Community proposals to liberalize the power market. Eurelectric hosts an Annual Convention and Conference at which energy policy is debated.\n\nEurelectric is led by a President and two Vice Presidents, who are elected to two-year terms, and a Board of Directors. The Secretary-General leads the Secretariat, which consists of several organizational units employing an international staff of approximately thirty people. There are five professional committees and approximately thirty working groups. \neurelectric is a signatory to the EU Transparency Register.\n"}
{"id": "21727035", "url": "https://en.wikipedia.org/wiki?curid=21727035", "title": "Explosophore", "text": "Explosophore\n\nExplosophores are functional groups in organic chemistry that give organic compounds explosive properties. \n\nThe term was first coined by Russian chemist V. Pletz in 1935. and originally mistranslated in some articles as \"plosophore\". Also of note is an auxoexplose concept (similar to chromophore and auxochrome concept), which is a group that modifies the explosive capability of the molecule. \nThe term \"explosophore\" has been used more frequently after its use in books such as \"Organic Chemistry of Explosives\" by J. Agrawal and R. Hodgson (2007)'.\n\nNitrogen-containing explosophores (groups I, II and II below) are particularly strong because in addition to providing oxygen they react to form molecular nitrogen, which is a very stable molecule, and thus the overall reaction is strongly exothermic. The gas formed also expands, causing the shock wave which is observed. \n\nPletz grouped the explosophores into eight distinct categories. It has been suggested that the current list is insufficient and that several other new categories should be added.\n\nThese represent\nBy far the most commercially used explosives are nitrate and nitrite based. \n\nThe azo and azide groups respectively, connected to organic/inorganic compounds (e.g. AgN, Pb(N), NHN)\n\nThe halogenated nitrogen group X:halogen (for example NI and NCl)\n\nThe fulminate group (example HONC and Hg(ONC))\n\nThe chlorate and perchlorate groups respectively, connected to organics/inorganics (e.g. KClO, FOClO)\n\nThe peroxide and ozonide groups respectively, connected to organics/inorganics (e.g. acetone peroxide, butanone peroxide)\n\nThe acetylide group with its metal derivatives\n\nThis class contains for instance organic compounds of mercury, thallium, and lead.\n\nOther substances have been characterised as explosophores outside of the eight classes as defined by Pletz.\n\nA functional group that is frequently considered as an explosophore is picrate, the salts or ethers of picric acid (2,4,6-trinitrophenol), which gains its explosive capability from the nitrate groups attached to it.\n"}
{"id": "34957798", "url": "https://en.wikipedia.org/wiki?curid=34957798", "title": "Ford F-Series (seventh generation)", "text": "Ford F-Series (seventh generation)\n\nThe seventh generation of the Ford F-Series is a line of pickup trucks and medium-duty commercial trucks that was produced by Ford from 1979 to 1986. For the first time since 1965, the pickup trucks were based upon a completely new chassis and body. Distinguished by its squarer look, sharper lines and flatter panels, the trucks were designed with improved fuel efficiency in mind; to this end, Ford added its new AOD automatic overdrive (four-speed) transmission as an option on light-duty models. The 4-speed manual and 3-speed C6 automatic transmission were retained from previous years. To increase longevity, Ford increased the use of galvanized body panels to fight corrosion. Light Pickups were available in six configurations: Regular Cab, SuperCab (extended cab), or Crew Cab (four full doors), in either 6ft or 8ft bed lengths. They are typically considered to be the last of the \"classic\" Ford trucks, due to features such as sealed beam headlights that would become obsolete in the next body style.\n\nAlong with the body and chassis upgrades, Ford made several changes to the branding of the F-Series over this generation. In 1981, the upscale Ranger trim was discontinued to create the nameplate for the company's all-new compact pickup that replaced the Mazda-built Courier. Largely superseded by the F-150, the F-100 was discontinued after the 1983 model year, but the F-100 nameplate remained in Argentina. In Mexico, there was an F-200 introduced in 1976; this variant remained until 1991.\n\nIn 1979, Ford debuted a brand new, redesigned F-Series pickup truck line, with the goal of maintaining utility while getting better fuel economy than its previous generation. However, drastic measures were taken in reducing weight, including cutting large holes in the frame; this severely weakened frame rails on model year 1980-1981 trucks, causing them to bend or buckle under load. This frame is known by enthusiasts as the \"Swiss cheese frame\". This was remedied by 1981 for the 1982 model year, returning the chassis rigidity to the same toughness and strength as the previous generation. Model year 1980–1981 trucks had a plain grille with \"FORD\" spelled across the front of the hood in chrome lettering, similar to the 1978-1979 models of the previous generation.\n\nThe 1982 model year was marked by a slight but important cosmetic change: 1982–86 models had the \"FORD\" letters above the grille removed, and a Ford oval placed in the center of the grille, with fewer vertical bars in the grille itself. This made the 1982 the first model year to feature the Blue Oval on the front, a trademark of all Ford pickups since, with the exception of the 2010–present F-150 SVT Raptor. The frame was strengthened and the trucks became heavier for 1982; this frame would underpin the F-Series until the 1997 redesign. Grille options included a full chrome grille, a black grille or the standard flat grey plastic grille. The headlight bezels also came in several color options, ranging from light grey, grey, dark grey, and black; with the latter two being the most common.\n\nIntroduced for 1980 models, an optional resettable trip meter was installed on speedometers and the mileage counter was moved to the top of the speedometer as part of the optional Sport Instrumentation Group. The Sport Instrumentation Group also included the optional tachometer in the center of the cluster, as well as oil and ammeter gauges. In 1984, the body moulding and interior trim were updated. In 1985-1986 models, the upper accent mouldings were moved below the front marker. For 1985, the rear tailgate moulding on XLT models was updated and previewed the design of the 1987 model. This molding has become increasingly rare and fetches a high price. A cargo light was available as an option and was included in the Light Group option package. (A Combination Stop/cargo lamp was not required until September 1, 1993 for the 1994 year model.)\n\n17 different colors were available, along with two-tone options and a choice of clearcoat or non-clearcoat paint.\n\nVarious standard equipment included interesting features such as a coat hook on the driver's side, AM radio (AM/FM and AM/FM Cassette were optional), scuff plates and vent windows. The back of the glove compartment door featured coin slots and cup depressions to hold cups and food similar to a food tray on a train. This was a feature only found on this generation and never on later models. It also showed a diagram with lift points as well as other mechanical information. Sliding rear windows were optional as well as cargo lights, under-hood lights, and many others. Ford offered over 150 options for the seventh-generation F-Series.\n\nFor 1981,various special order equipment was offered on all F-100, 81 model pickup truck with a V6 motor 250, and 350 models. These are listed below with their respective descriptions.\n\n\nThis generation saw two different sets of trim levels:\n\nFor 1980 and 1981, there was:\n\nFor 1982–1986:\n\nThe seventh-generation F-Series marked a major transition in the powertrains used by the pickup line. As before, the standard engine was a carbureted 300 cubic-inch inline-6. For 1982, this was supplemented by a 3.8 L V6 borrowed from the Fox platform; it was dropped after 1983 as the result of slow sales. The standard V8 remained the 302 Windsor V8. To further boost fuel efficiency, a downsized 255 cubic-inch version of the 302 Windsor V8 was made an option; it proved unpopular and was dropped after 1983. As Ford streamlined its small-block V8 engine lineup, the 351M was replaced by the 351 Windsor. Initially, the largest engine offered was the 400 V8 carried over from the previous generation; it was available only in F-350 and certain F-250 models. As similar-size engines were discontinued by General Motors and Chrysler during the late 1970s, the 400 was discontinued after 1982.\n\nIn 1982, the 460 V8 made its return as the replacement for the 400 on 1983 models. Coinciding with the reintroduction of the big-block 460, Ford introduced another engine offering for buyers seeking higher-output engines. Largely a response to General Motors, who had offered diesel-engined pickups since 1977, Ford produced their first North American diesel F-Series in 1982, while in Argentina the F-100 carried over the same 3.3L Perkins 4.203 available since the 3rd generation F-series. Rather than develop its own engine (as GM had), the 6.9 L \"IDI\" V8 was the product of a joint venture with International Harvester. The 460 was first offered in 4x4s in 1983 as an option on 1984 models. Up until that point, it had never been offered in 4WD models.\n\nIn 1984, the 302 Windsor was available with electronic fuel injection as an option on 1985 models; a year later, it became standard (an industry first for full-size pickups). The F-150 equipped with the 3-speed manual transmission was the second-to-last American vehicle to have a column-shifted manual transmission; it was discontinued after 1986, a year before similar Chevrolet and GMC models. \n\nThe heavy duty Ford C6 3-speed automatic transmission, marketed as the \"Select-Shift\" automatic, was the standard automatic transmission all years and came paired with most engine options if ordered. Various transfer cases were used, most built by New Process Gear. NP205 and NP208F cases were most common. Each feature a stick-shift 4WD engagement, with 4 speeds: 4 Low, 4 High, Neutral and 2 High. Various Borg-Warner transfer cases were also used.\n\nEngines:\n\n† Only available F-250 HD and F-350 \n†† 1984–85 only available on special ordered F-150 models v.i.n. designation H, as well as HD F-250 and F-350 models, 1986 available all models\n\nThis generation was the first time Ford used Independent suspension on their full size 4x4 trucks, as well as being the first time any of the Big Three (automobile manufacturers) made a 4x4 full size truck without a solid front axle. Ford and Dana Holding Corporation called this the Twin Traction Beam or TTB and used many of Dana Spicer parts. The F-150 used a light duty Dana 44 TTB. From 1979-1984, the rear axle was typically a Ford 9-inch axle, with the Ford 8.8 axle being phased gradually until the 9\" was finally phased out before the 1987 model year. The F-250 used an 8 lug version of the Dana 44 TTB called the Dana 44 TTBHD with the Dana 50 TTB being an option. The rear was a Dana 60 until mid 1985 when Ford phased out that axle for their own Sterling 10.25. Dana 60s could be either full float or semi float and came with a range of gear ratios. Semi float Dana 60s were either c-clip style, which utilize c-clips to hold the axle shafts in, or pressed in bearings which held the axles in with a special wheel bearing that bolted to the outer axle housing inside the brake drum. These were typically used in lighter duty trucks. Up until then, early 1985 models were built with left over 1984 materials, making some parts tough to find. The F-350 used the Dana 50 TTB in front until a mid-year change in 1985, when the F-350 was fitted with the Dana 60 solid front axle. F-250s could be ordered with a Dana 50 TTB if it was a heavier duty model; all other F-250's were equipped with a Dana 44 TTB. These trucks were leaf sprung and used a single gas shock with no coil springs and radius arms like on the F-150. For the rear axle the F-350 trucks used a Dana 60 for the single rear wheel trucks and a Dana 70 for the dual rear wheel trucks until 1985 when Ford once again phased in their own Sterling axle. Factory lifts used 2\" blocks on the rear suspension, or 2\" front and 4\" rear on HD trucks, usually on F250s and higher trims. Heavier duty F150s could be ordered with 2\" blocks.\n\nFor the first time since 1967, the medium-duty version of the F-Series (F-600 and above) were completely redesigned. Again sharing a common cab and interior with the pickup trucks, the medium-duty trucks were distinguished by separate bodywork ahead of the firewall. For the first time since the 1957 \"Big Job\" trucks, the fenders were separate from the hood; although a traditional rear-hinged hood was standard, the optional forward-tilting hood (in the style of the larger L-Series trucks) quickly overtook it in popularity. Alongside the standard two-door cab, the medium-duty F-Series was available with a four-door crew cab.\n\nSharing exterior styling derived from the larger L-Series trucks, the medium-duty F-Series saw few changes through the 1980s. As a running change during 1983, the truck adapted the Ford Blue Oval for the 1984 model year, becoming the last Ford vehicle to do so. After that change, the medium-duty F-Series would not see any exterior change (aside from engine badging) until its 1995 facelift (consisting of a new hood, fenders, and headlights). Although the cab itself was shared with F-Series pickup trucks produced afterward, the MY 1980-1986 interior of the medium-duty F-Series was carried over entirely and used until the medium-duty F-Series was discontinued in 1998.\n\nThe medium-duty F-Series was initially produced with several engines. Two gasoline V8 engines were available, a 370 cubic-inch V8 and a 429 cubic-inch V8; both engines were versions of the 460 V8. In 1991, the 370 was discontinued, with the 7.0L V8 (429) becoming standard. At its launch, two diesels were offered: the Detroit Diesel 8.2L \"Fuel Pincher\" V8 was offered alongside the Caterpillar 3208 V8; Caterpillar-powered vehicles were re-designated Ford \"F-8000\" (adding an extra \"0\" to the model name). During the late 1980s, the Caterpillar and Detroit Diesel engines are replaced by 6.6L and 7.8L inline-6 diesels produced in a joint venture between Ford and New Holland in Brazil. In 1992, the Ford-New Holland engines are both replaced by a Cummins 5.9L inline-6 diesel.\n\n"}
{"id": "25631381", "url": "https://en.wikipedia.org/wiki?curid=25631381", "title": "Friedel oscillations", "text": "Friedel oscillations\n\nFriedel oscillations, named after French physicist Jacques Friedel, arise from localized perturbations in a metallic or semiconductor system caused by a defect in the Fermi gas or Fermi liquid. Friedel oscillations are a quantum mechanical analog to electric charge screening of charged species in a pool of ions. Whereas electrical charge screening utilizes a point entity treatment to describe the make-up of the ion pool, Friedel oscillations describing fermions in a Fermi fluid or Fermi gas require a quasi-particle or a scattering treatment. Such oscillations depict a characteristic exponential decay in the fermionic density near the perturbation followed by an ongoing sinusoidal decay resembling sinc function.\n\nThe electrons that move through a metal or semiconductor behave like free electrons of a Fermi gas with plane wave-like wave function, that is\n\nElectrons in a metal behave differently than particles in a normal gas because electrons are fermions and they obey Fermi–Dirac statistics. This behaviour means that every k-state in the gas can only be occupied by two electrons with opposite spin. The occupied states fill a sphere in the band structure k-space, up to a fixed energy level, the so-called Fermi energy. The radius of the sphere in k-space, \"k\", is called the Fermi wave vector.\n\nIf there is a foreign atom embedded in the metal or semiconductor, a so-called impurity, the electrons that move freely through the solid are scattered by the deviating potential of the impurity. During the scattering process the initial state wave vector k of electron wave function is scattered to a final state wave vector k. Because the electron gas is a Fermi gas only electrons with energies near the Fermi level can participate in the scattering process because there must be empty final states for the scattered states to jump to. Electrons that are too far below the Fermi energy \"E\" can't jump to unoccupied states. The states around the Fermi level that can be scattered occupy a limited range of k-values or wavelengths. So only electrons within a limited wavelength range near the Fermi energy are scattered resulting in a density modulation around the impurity like\n\nIn the classic scenario of electric charge screening, a dampening in the electric field is observed in a mobile charge-carrying fluid upon the presence of a charged object. Since electric charge screening considers the mobile charges in the fluid as point entities, the concentration of these charges with respect to distance away from the point decreases exponentially. This phenomenon is governed by Poisson–Boltzmann equation. The quantum mechanical description of a perturbation in a one-dimensional Fermi fluid is modelled by the Tomonaga-Luttinger liquid. The fermions in the fluid that take part in the screening cannot be considered as a point entity but a wave-vector is required to describe them. Charge density away from the perturbation is not a continuum but fermions arrange themselves at discrete spaces away from the perturbation. This effect is the cause of the circular ripples around the impurity.\n\n\"N.B. Where classically near the charged perturbation an overwhelming number of oppositely charged particles can be observed, in the quantum mechanical scenario of Friedel oscillations periodic arrangements of oppositely charged fermions followed by spaces with same charged regions.\"\nIn the figure to the right, a 2-dimensional Friedel Oscillations has been illustrated with an STM image of a clean surface. As the image is taken on a surface, the regions of low electron density leave the atomic nuclei ‘exposed’ which result in a net positive charge. \n\n"}
{"id": "12477527", "url": "https://en.wikipedia.org/wiki?curid=12477527", "title": "Hotel energy management", "text": "Hotel energy management\n\nHotel Energy Management is the practice of controlling procedures, operations and equipment that contribute to the energy use in a hotel operation. This can include electricity, gas, water or other natural resources. Because hotels can have complicated operations and extensive facilities they utilize many different types of energy resources. Hotel energy usages are tracked and classified by the U.S. Department of Energy and statistics are regularly published in the Energy Information Administration annual reports.\n\nModern practices to control energy usage includes contributions by the guests themselves which has been popularized by information cards requesting guests to save water by letting hotel housekeeping staff know if they would care to re-use towels and bed linens. This reduces the amount of water and/or cleaning substances used by the hotel laundry department which also reduces the expense to the property owner or manager.\n\nRecently consultants have developed entire organizations for advising hotels regarding inefficient usage of energy and optimizing its usage. Some of these consultants participate by providing the products to implement their advice for a share of the cost savings as their conaideration. These companies have proliferated over the years as public and business energy concerns grow and are popularly known as ESCO's (Energy Service COmpanies). Other practices include using Infrared motion sensors, door contacts, and other means to detect occupancy to control the Heating, Ventilating and Air Conditioning systems (HVAC) when guests forget to switch off when they leave their room. Turning off the lights when a room is not occupied is called smart lighting.\n\nHotel facility managers are using cloud-based software nowadays to manage their energy efficiency projects. The Department of Energy (DOE) Software Directory describes EnergyActio software, a cloud based platform designed for this purpose. \n\nEnergy management markets in general have been going through several changes including the shift to service based models or performance contracted models used by Energy Services Companies (ESCOs). Traditionally ESCOs did not address the hotel segment because of the small values associated with hotel energy retrofits and the difficulty in measuring so many potential load sources.\n"}
{"id": "16775728", "url": "https://en.wikipedia.org/wiki?curid=16775728", "title": "LX-14", "text": "LX-14\n\nLX-14 is made of HMX explosive powder (95.5%) and Estane and 5702-Fl plastic binders (4.5%).\n\nLX-14-0 has a density of 1830 kg/m.\n"}
{"id": "45196762", "url": "https://en.wikipedia.org/wiki?curid=45196762", "title": "Lamu Coal Power Station", "text": "Lamu Coal Power Station\n\nThe proposed Lamu Coal Power Station is a potential coal-fired thermal power station in Kenya. The proposed plant would be developed on 865 acres of land and feature a 210 meter tall smoke stack, which would become East Africa's tallest structure.\n\nKenya national government and media have been largely positive about the economic benefits from the coal plant activity. However, community advocates and some local government officials expressed concern over whether the benefits would be well distributed, whether the jobs would really materialise, and the lack of discussion over possible negative effects from the project.\n\nAs of June 2017, recent coverage has centred on the lack of economic viability and need for the proposed Lamu coal plant, citing a range of experts in news and analysis pieces. International accountability organisations also raised concerns in a series of global blog posts.\n\nThe power station would be located on in the Kwasasi area, about north of the town of Lamu in Lamu County, along Kenya's Indian Ocean coast. This is approximately , by air, north-east of Mombasa. The driving distance is approximately .\n\nConstruction was expected to begin in September 2015 and last approximately 21 months. Once constructed, it will be the largest single power station in Kenya. The power generated will be transmitted to Nairobi, the country's capital, via a new , 400 kiloVolt electricity transmission line. In the initial years, the station will utilize imported coal, mainly from South Africa, and later convert to locally sourced coal from the Mui Basin in Kitui County. In September 2016, Kenyan print media indicated that construction was expected to begin in the fourth quarter of 2016 and last 42 months. In June 218, the Energy Regulatory Commission instructed the developers of this power station to reduce its capacity from the original 1,050 megawatts to something smaller, in the 450 to 600 megawatts range.\n\nCommunity activists and some local officials have also raised the need for revenue sharing for the community, and what conditions would be set for the project to ensure it would benefit the host community, citing the fact that community members affected by the earlier oil infrastructure project known as Lamu Port and Lamu-Southern Sudan-Ethiopia Transport Corridor have not been compensated. Community, Kenyan and international activists and one of the world's leading economists Joseph Stiglitz have raised serious concerns about the environmental and human health implications of the fossil fuel plant. The population wants cleaner, more progressive renewable energy.\n\nThe plant will cause massive pollution according to various reports including John Musingi PhD, a senior lecturer and Programme Coordinator of Environment Planning and Management, in the Department of Geography and Environmental Studies, at Nairobi University. There is no demand for the proposed 1,050 Mw of power in Kenya to date or for the foreseeable future as Kenya is sufficient in energy generation at this time. Aside from this Turkana wind Power's 310 MW was still not connected to the national grid as of June 2018; and will supply additional power to an already over-supplied Kenya.\n\nChina is seen to be exporting second hand coal plants to Africa and the expense of the African Nations that take them on. Ernest Niemi, an economist and president of Natural Resource Economics, who has done studies on coal plants across the world for over 40 years, said that operating the AMU Power coal plant to produce electricity will be cheap for the developers but expensive as an energy source to consumers and detrimental to the society in general.\n\nTestifying before the National Environmental Tribunal in Nairobi in June 2017, the US-based consultant said Kenya will incur a massive health burden coupled with deaths with the operation of the plant. Studies across the world, he said, show that the social cost of running coal-fired power plants exceeds the economic benefits.\n\nSave Lamu has tabled numerous malpractices by Kenya National Environmental Management Authority (Nema), in approving the license to Amu Power. The right to comment to the public hearing was for every Kenyan, and the ERC failure to identify an accessible and safe venue for all was a clear plan to keep opposition away from the hearing.\n\nThe Secretary-General of Save Lamu, Walid Ahmed, said that, on several occasions, ERC had tried to lock out Lamu residents and activists from attending their meetings and airing their views concerning the coal plant project.\n\nWalid Ahmed also said that residents have not been provided with enough time to go through the environmental and social impact assessment report that was released by Amu Power Company in order for the residents to give their opinion. \"Even before they approved the project, they had already shown signs of locking us out including changing planned venues for public hearing on the coal project. We will not relent. We will fight till the end. Our fishermen, mangrove harvesters and the population at large have a stake in sustainable environment that is free from pollution. The coal plant is harmful and is likely to affect the marine ecosystem and the livelihoods of our people. We won't accept it,\" said Mr Walid.\n\nSave Lamu Coordinator Khadija Shekue Famau said that despite the concerns raised, there are no proper mitigation measures that the investor has put in place or even explained to the communities and stakeholders how emissions from the plant will be handled.\n\nA former chairman of the Energy Regulatory Commission estimates that capacity charges would be Sh36 billion per year.\n\nIn January 2014, the Government of Kenya sourced for bids from private developers to build, own, and operate the power station. In September 2014, the development rights were awarded to a consortium of the following entities:\n\nTwo of the losing bidders challenged the award in court. An arbitration tribunal, however, upheld the award in January 2015.\n\nThe power station will be built using private funds on a build-own-operate model. The developers will own and operate the plant for 25 years from commissioning. The expected construction costs for the coal plant will be about US$2 billion (KES:180 billion). Of this, approximately US$500 million will be generated internally while the balance will be borrowed.\n\nAmu Power Company, a special purpose vehicle formed by the developers, will develop, own and operate the station as part of the \"Lamu Port and Lamu-Southern Sudan-Ethiopia Transport Corridor\".\n\nIn July 2016, Standard Bank of South Africa and ICBC agreed to jointly fund the $300 million that was needed to close the deal. In October 2017, Standard Bank retracted the support.\n\n\n"}
{"id": "3769587", "url": "https://en.wikipedia.org/wiki?curid=3769587", "title": "Low-Income Home Energy Assistance Program", "text": "Low-Income Home Energy Assistance Program\n\nThe Low Income Home Energy Assistance Program (LIHEAP) is a United States federal social services program first established in 1981 and funded annually through Congressional appropriations. The mission of LIHEAP is to assist low income households, particularly those with the lowest incomes that pay a high proportion of household income for home energy, primarily in meeting their immediate home energy needs. The program, part of the United States Department of Health and Human Services (HHS), is funded by grants appropriated from the federal government.\n\nFunding is distributed to each of the fifty states, U.S. territories and tribal governments through the United States Department of Health and Human Services (HHS). Administration of the program is left up to state, territorial or tribal governments. Congress also provides the President of the United States with limited contingency funds each year, which are reserved for emergency situations and released at the President's discretion.\n\nState legislatures often provide additional appropriations each year to supplement federal LIHEAP funds.\n\nLIHEAP offers one-time financial assistance to qualifying low-income households who require support in paying their home heating or cooling bills. Applicants must have an income less than 150% of federal poverty level or 60% of state median poverty level to be eligible, however some states have expanded their programs to include more households (for example, in Massachusetts, applicants must be within 60% of the estimated State Median Income).\n\nThe Low Income Home Energy Assistance Program (LIHEAP) provides funding assistance to low-income households, targeting those who are truly vulnerable: the disabled, elderly, and families with preschool-age children. Funding is distributed to states or other governmental entities, who administer the program and stems from four sources including: Block grants, the Residential Energy Assistance Challenge Program, Contingency Funds, and Leveraging Incentive Programs. Allocations are based on local climate, economic, and demographic formulas. Additional funds may be available in emergency situations or as match funds when contributions are provided by local governments, private businesses, or non-profit organizations. Final distribution of funds is conducted through the program grantees, which are able to design their programs to meet consumer needs while adhering to the goals and policies of LIHEAP.\n\nIn most states, the program is run on a first come-first served basis. This typically results in a rush to apply and receive assistance, because once the funding pool is empty most energy assistance offices close their doors. In some states, the legislature or governor may make a politically popular gesture of extending eligibility to additional individuals through an emergency bill or executive order, even though this may result in funds being claimed earlier in the winter season.\n\nThe Home Energy Assistance Target (H.E.A.T.) program is the State of Utah's program through which funds are distributed to the target population. This program is specifically administered by the state and various Associations of Governments (AOG). The Mountain land AOG provides H.E.A.T. assistance to persons in Utah, Wasatch, and Summit Counties.\n\nMany state LIHEAP agencies also offer weatherization support, in which contractors are sent to residences to make physical changes to help retain heat or install more fuel-efficient furnaces. Occasionally, acceptance of the weatherization process is mandatory with approval for LIHEAP assistance.\n\nSome states have attempted to enact Percentage of Income Payment (PIP) plans within or in addition to the traditional LIHEAP block grant model. Although overall funding has increased since 2002, future funding may be limited due to the recent trend in cutting the budget based on building codes requiring energy efficiency, modern appliances with low energy use standards, and concerns about federal budget sustainability. As the program moves forward, the budget being cut along with the rise of applicants are the two major challenges it faces.\n\nLIHEAP history began in 1980 when congress created the Low Income Energy Assistance Program (LIEAP), as part of the Crude Oil Windfall Profits Tax Act to answer the concerns of the rising energy prices of the 1970s. In 1981, LIEAP was replaced with LIHEAP as part of the Omnibus Budget Reconciliation Act. In 1984, the Human Services Reauthorization Act added a new goal to provide funds for cooling costs of low-income households. Congress also required the use of more recent population and energy data, which meant the shifting of funds from solely cold-weather states to warm-weather states. In 1988, there was another major change where the National Center for Appropriate Technology (NCAT) started to operate the LIHEAP Clearinghouse.\n\nLIHEAP is administered by the Department of Health and Human Services. Several federal divisions provide oversight and direction to the program. These include the Administration for Children and Family Services, the Office of Community Services, and the Division of Energy Assistance. The National Center for Appropriate Technology is an additional advisory body in the program before funds are given to grantees. \nThe federal government does not provide LIHEAP assistance to the public. Instead, the federal government provides funds to states, federal or state-recognized Indian tribes, tribal organizations, and insular areas to administer.\n\nGroups who receive funds are considered grantees in the program. LIHEAP grantees have flexibility to design their programs, within very broad federal guidelines, to meet the needs of their citizens. Each state may have varying departments or divisions to disseminate funds. Shown above is the program flow from the federal to local level in the State of Utah.\nIn Utah, LIHEAP funding is managed through the Home Energy Assistance Target (HEAT) program, which is handled by the state or Association of Governments (AOG). The Mountainland AOG provides direct oversight for assistance to local government agencies in Utah, Wasatch, and Summit Counties. MAG receives nearly $2.5 Million annually.\nState of Illinois has announced in 2014 that residents can apply at two area agencies; a single-person household can qualify with a monthly income of up to $1,459; a two-person household up to $1,966; a family of three can earn up to $2,474; and a family of four can earn up to $2,981.\n\nLIHEAP is funded by annual appropriations to the Department of Health & Human Services. The funding for the fiscal year 2017 is $3.09 billion.\n\nFunding is distributed to each of the fifty states, U.S. territories and tribal governments through the United States Department of Health and Human Services (HHS). Administration of the program is left up to state, territorial or tribal governments. Congress also provides the President of the United States with limited contingency funds each year, which are reserved for emergency situations and released at the President's discretion.\nState legislatures often provide additional appropriations each year to supplement federal LIHEAP funds.\n\nThe amount of funding available for actual assistance comes from four major sources including: Block grants, the Residential Energy Assistance Challenge Program, Contingency Funds, and Leveraging Incentive Programs. The following is a synopsis of how these sources tie into LIHEAP.\n\nBlock grants account for the majority of dollars distributed for the LIHEAP program. To be granted a block grant, states, territories, Indian tribes, and tribal organizations that wish to assist low-income households in meeting the costs of home energy, may apply for a LIHEAP block grant.\n\nThe Residential Energy Assistance Challenge Program (REACH) \"provides grants that fund demonstration projects to test various approaches to help low-income families reduce their energy usage and become more self-sufficient in meeting their home energy needs.\" Local community-based agencies that implement innovative plans to help LIHEAP eligible households reduce their energy vulnerability can receive these funds.\n\nAs of 2001, 54 REACH grants had been funded creating an annual budget of $6 Million, or one half of a one percent of the total funding for the LIHEAP program. These grants have been used to create weatherization materials, workshops on energy efficiency measures for homes, budget counseling, and have formed consumer cooperatives to purchase home energy. State projects run for three years, and state grantees are required to contract for third-party evaluations and to report after the conclusion of the project on the effectiveness of the approaches that they have tried.\n\nContingency funds are funds that are released to assist with home energy needs due to emergency situations. They may be allocated to one or more grantees, or to all grantees, based on criteria appropriate to the nature of the emergency. Generally, these funds are released in response to extreme weather conditions or energy price increases. In the 1980s, contingency funding was only used twice. In the 1990s, it was used eight times, and since the year 2000 there has been a call for contingency funding every year. \nThe Leveraging Incentive Program is designed to reward those grantees that have acquired non-federal leveraged resources for their LIHEAP programs.\n\nIt encourages grantees to look for ways to add non-federal dollars or other resources to their LIHEAP programs. Additionally, grantees are encouraged to integrate and coordinate with other energy assistance programs to provide non-federal energy assistance to low-income households who meet LIHEAP eligibility criteria. Participation in this program is optional, but if non-federal dollars are reported, the grantee can receive additional LIHEAP funds. \nAn example of this program is to leverage a discount on wood that is negotiated by a tribal LIHEAP coordinator with a wood supplier. The amount of the discount, given to LIHEAP eligible households, would be reported as a leveraged or non-federal resource and be eligible for additional funding to the tribe.\n\nLow Income Home Energy Assistance Programs (LIHEAP) and Weatherization Assistance Programs (WAP) work together to help low-income individuals and families pay energy bills and reduce energy costs. This article gives of overview of each program and describes how they work together. LIHEAP and WAP literature is also examined. Finally, a section detailing literacy in low-income consumers is included.\n\nThe mission of the Low Income Energy Assistance Program (LIEAP) (also known as Low Income Home Energy Assistance Program (LIHEAP)), created in 1981, is to assist low income households, particularly those with the lowest incomes that pay a high proportion of household income for home energy, primarily in meeting their immediate home energy needs. The program, part of the United States Department of Health and Human Services (DHHS), is funded by grants appropriated from the federal government.\n\nLIHEAP pays partial winter energy bills for eligible individuals and families. Payments are usually made directly to local utility companies or vendors. To be eligible, an individual's income level must not be more than 150% of the federal poverty level. The payment amount is figured according to the size and type of your home, as well as type of fuel.\n\nA press release from the Department of Health and Human Services on June 5, 2013, indicates that $187.4 million was released to states to help low-income homeowners and renters with rising energy costs. This funding supplements $3.065 billion in grants made available earlier in the year through The Low-Income Home Energy Assistance Program (LIHEAP).\n\nThe funding serves to help families pay their heating and electricity, as well as make weather-related improvements to their homes. This helps to prevent these families from having service interruptions.\n\nGeorge Shelton, HHS acting assistant secretary for the Administration for Children and Families, stated that high temperatures translate into high energy bills for families who are already struggling to make ends meet.\n\nThe United States Weatherization Assistance Program (WAP) was created in 1976 to help low-income families reduce energy consumption and costs. WAP is governed by various federal regulations designed to help manage and account for the resources provided by the Department of Energy (DOE). WAP funding is derived from annual appropriations from Congress. Each year, the Senate and House Interior Appropriations committees decide how much funding to allocate to the Program.\n\nWAP technicians perform energy audits on a home to help locate efficiency problems. Once an audit is complete, the program can help by insulating walls and windows, replacing broken glass, and testing, repairing, and/or replacing combustion appliances. Like the LIHEAP, an individual's income level must be at or below 150% of the federal poverty level to be eligible for the WAP. Many state WAPs and LIHEAPs work together to provide the best energy services for low-income households.\n\nLIHEAP and WAP are the cornerstones of any public energy assistance program. Often, the two programs not only work together, they automatically encompass each other. At the Montana energy assistance offices, clients are automatically enrolled in the WAP when they are enrolled in the LIHEAP.\n\nThis association is fitting being that the LIHEAP and WAP are usually located in the same office. In Butte, Montana, the Energy Assistance office and Weatherization departments (which are part of the Human Resources Council, District XII) are in charge of LIHEAP and WAP. Each department offers several other programs that assist low-income individuals and families with heating and energy issues.\n\nWhen a client completes and turns in an application for LIHEAP, they are automatically enrolled in WAP. After the application is processed, it usually takes one to two weeks to receive financial assistance for their heating and energy bills. During this time, the applicant is contacted by the WAP department. Before a client's home can be weatherized, an auditor comes to the home to complete an energy audit. The auditor is trained to determine the most cost-efficient weatherization measures for the home.\n\nWeatherization measures may include caulking, weather-stripping, insulation, vent dampers, replacement of broken glass, repair or replacement of primary doors, and furnace tune-ups. Weatherization workers may not be able to install all the materials, but they will do the most important weatherization within the dollar limits allowed.\n\nWhen a home is scheduled for weatherization service, a crew will come to install the necessary materials in the home. After the work is completed, the client will be asked to sign a statement saying the work was done properly and to the client's satisfaction. Surveys have shown that weatherizing a home can significantly decrease winter heating or energy bills.\n\nBesides working together to provide energy assistance, LIHEAP and WAP programs around the country have been emphasizing energy conservation lately. The program literature contains more than just information about the program, the literature offers tips on how to reduce energy consumption. Tips include replacing light bulbs with compact fluorescent light bulbs and using Energy Star appliances.\n\nFuture Issues\n\nFunding Trends\n\nCongress appropriated $3.47 Billion toward the program in 2012. Over the past decade, funding for LIHEAP has trended closely with winter fuel prices, except for the federal fiscal years of 2009-2011, when funding leapt above the winter fuel price index tracked by the Center on Budget and Policy Priorities (CBPP). The President's budget proposal in 2012 significantly reduced allocation amounts to \"return LIHEAP funding to historic levels received for 2008 prior to energy price spikes.\" The two main reasons why the budget has been cut for 2012 is because first, the building codes now require energy efficiency, modern appliances with low energy use standards and second, concerns about federal budget sustainability are causing federal officials to consider reduction of LIHEAP funds. Changes in future funding will need to consider the efficacy of program management, energy costs, the number of recipients, and other potential sources of assistance. \nPerformance Measures and Reporting\n\nANNUAL MEASURE FY TARGET RESULT\n1A. INCREASE THE RECIPIENCY TARGETING INDEX SCORE OF LIHEAP HOUSEHOLDS HAVING AT LEAST ONE MEMBER 60 YEARS OR OLDER. (OUTCOME) 2009 96 Aug-10\nLIHEAP efficacy is monitored on a regular basis through its recipiency targeting index. This index quantifies the extent to which assistance eligible households are receiving LIHEAP assistance. The greatest focus of eligibility is those households with elderly, disabled, or young children. An index score above 100 indicates that LIHEAP is serving a target group of households at a rate higher than the prevalence of LIHEAP income eligible households that are members of that group. The table at right reflects the target scores from Fiscal Years (FY) 2004-2009 for households with an elderly member. The table also shows what the actual national targeting score was for FY 2004-2006.\nEach December, state LIHEAP grantees are required to report on the LIHEAP Grantee Survey the following data for the previous federal fiscal year:\nThe data from the LIHEAP Grantee Survey are included in the LIHEAP Report to Congress. The most recent available data are for FY 2003 LIHEAP and related data are obtained from LIHEAP grantee reports and surveys, national household surveys, and other federal agencies. Much of the data are published in the department's LIHEAP Home Energy Notebook and the LIHEAP Report to Congress.\n\nEnergy Costs\n\nWhile the program administrators use the above measures and reports to understand how the program is working, there is greater concern that not enough funding is being brought into the program to stave off increases in fuel prices. The 2008 spike in funding was needed to address the sharp increase in home heating oil. The recent decrease in allocations prior to 2008 levels may not be enough to handle the continued climbing of heating costs and its effect on many households with low incomes.\nRecipients\n\nNot only are fuel prices continuing to increase, program recipients are on the rise. This may be illustrated in the following chart showing the increase in households served by the program in relation to the amount of LIHEAP funds allocated to the State of Utah. Some statistics of note for the State of Utah include:\nCoordination with Outside Programs\n\nIn addition to providing matching funds through the Leveraging Incentive Program, LIHEAP strives to coordinate efforts with private utility companies and non-profits where federal funding is not available. In the State of Utah, some of these other sources include Rocky Mountain Power's Home Electric Lifeline and Lend-a-Hand Programs, Questar's Energy Assistance Fund and REACH program, Catholic Community Services, American Red Cross, and Murray City Relief Program. H.E.A.T. funding applicants may be referred to these or other private assistance groups if there are not sufficient LIHEAP funds.\n\n\n"}
{"id": "4022565", "url": "https://en.wikipedia.org/wiki?curid=4022565", "title": "Madras Atomic Power Station", "text": "Madras Atomic Power Station\n\nMadras Atomic Power Station (MAPS) located at Kalpakkam about south of Chennai, India, is a comprehensive nuclear power production, fuel reprocessing, and waste treatment facility that includes plutonium fuel fabrication for fast breeder reactors (FBRs). It is also India's first fully indigenously constructed nuclear power station, with two units each generating 220  MW of electricity. The first and second units of the station went critical in 1983 and 1985 respectively. The station has reactors housed in a reactor building with double shell containment improving protection also in the case of a loss-of-coolant accident. An Interim Storage Facility (ISF) is also located in Kalpakkam.\n\nDuring its construction, a total of 3.8 lakh (380,000) railway sleeper (logs) were brought from all over India to lift the 180 ton critical equipment in the first unit, due to lack of proper infrastructure and handling equipment.\n\nThe following month the loading of the 1750 ton liquid sodium coolant were expected to happen in four to five months, with sources in the Department of Atomic Energy reporting that criticality would likely be reached only around May 2017.\n\nThe facility houses two indigenously built Pressurised Heavy-Water Reactors (PHWRs), MAPS-1 and MAPS-2 designed to produce 235 MW of electricity each. MAPS-1 was completed in 1981, but start-up was delayed due to a shortage of heavy water. After procuring the necessary heavy water, MAPS-1 went critical in 1983 and began operating at full power on 27 January 1984. MAPS-2 obtained criticality in 1985 and began full power operations on 21 March 1986.\n\nWith India not being a signatory to the Treaty on the Non-Proliferation of Nuclear Weapons the reactors have since 1985 been delivering their spent fuel to the nuclear reprocessing plant at Tarapur, providing the country with unsafeguarded plutonium.\n\nA beachhead at Kalpakkam also hosts India's first indigenous Pressurised (light) water reactor (PWR). The 80 MW reactor was developed by Bhabha Atomic Research Centre (BARC) as the land-based prototype of the nuclear power unit for India's nuclear submarines. This unit does not come under MAPS.\n\nThe reactors' coolant pipes have been plagued by vibrations and cracking with substantial cracking in the reactor coolant system. This cracking has led to the discovery of Zircaloy pieces in a moderator pump, requiring the power generation to be lowered to 170 MW.\n\nOn 26 March 1999 large amounts of heavy water spilled at MAPS-2, exposing seven technicians to heavy doses of radiation.\n\n\n \n"}
{"id": "37317764", "url": "https://en.wikipedia.org/wiki?curid=37317764", "title": "Minicharged particle", "text": "Minicharged particle\n\nMinicharged particles (or milli-charged particles) are a proposed type of subatomic particle. They are charged, but with a tiny fraction of the charge of the electron. They weakly interact with matter. Minicharged particles are not part of the Standard Model. One proposal to detect them involved photons tunneling through an opaque barrier in the presence of a perpendicular magnetic field, the rationale being that a pair of oppositely charged minicharged particles are produced that curve in opposite directions, and recombine on the other side of the barrier reproducing the photon again.\n\nMinicharged particles would result in vacuum magnetic dichroism, and would cause energy loss in microwave cavities. Photons from the cosmic microwave background would be dissipated by galactic-scale magnetic fields if minicharged particles existed, so this effect could be observable. In fact the dimming observed of remote supernovae that was used to support dark energy could also be explained by the formation of minicharged particles.\n\nTests of Coulomb's law can be applied to set bounds on minicharged particles.\n"}
{"id": "28737206", "url": "https://en.wikipedia.org/wiki?curid=28737206", "title": "National Energy Board", "text": "National Energy Board\n\nThe National Energy Board () is an independent economic regulatory agency created in 1959 by the Government of Canada to oversee \"international and inter-provincial aspects of the oil, gas and electric utility industries\". Its head office is located in Calgary, Alberta.\n\nThe NEB mainly regulates the construction and operation of oil and natural gas pipelines crossing provincial or international borders. The Board approves pipeline traffic, tolls and tariffs under the authority of the \"National Energy Board Act\". It deals with approximately 750 applications annually, through written or oral proceedings.\n\nThe National Energy Board also has jurisdiction over the construction and operation of international power lines, defined as lines built \"for the purpose of transmitting electricity from or to a place in Canada from or to a place outside of Canada\". The NEB authorizes imports of natural gas, and exports of crude oil, natural gas, oil, natural gas liquids (NGLs), refined petroleum products and electricity. The NEB also has jurisdiction over designated inter-provincial power lines, by determination of the federal Cabinet, but no such line has been designated, leaving the regulation of existing interties to provincial regulatory bodies. Recent NEB decisions in favour of petroleum-industry interests have led to increasing controversy.\n\nIn February 2018, the government of Justin Trudeau proposed to supersede the NEB with the Impact Assessment Agency of Canada, which would be better structured to assess health, Indigenous, and socioeconomic concerns in additional to the environmental impact of major Canadian projects. The NEB's regulatory tasks such as regulating pipelines and oil and gas transmission would be taken up by a new Canadian Energy Regulator (CER).\n\nPrime Minister Justin Trudeau (elected in the Canadian federal election, 2015) has strongly condemned the Harper-era process of regulation, and especially the NEB, citing serious conflict of interest and mandate flaws. As of December 2016, no changes to the Board had been announced.\n\nThe NEB's lack of coherence on climate change is a major source of uncertainty. Ontario and Quebec had initially imposed approval conditions on Energy East re \"upstream\" emissions in Alberta, similar to those imposed both upstream and downstream by the EPA and Obama Administration on Keystone XL. Both dropped these climate change concerns in December 2014 despite the Pembina Institute estimate that \"Energy East would cause 32 million tonnes of added greenhouse-gas emissions every year, which would cancel out the emissions reductions Ontario achieved by closing all of its coal-fired power plants\".\n\nAnother complicating factor is the position of Brad Wall, Premier of Saskatchewan, that equalization can be withheld from provinces that do not support raw bitumen export.\n\nA final issue requiring federal clarification is the United Nations Declaration on the Rights of Indigenous Peoples. Under the UNDRIP, indigenous peoples on unceded lands, including those over which Northern Gateway and Energy East would need to pass, appear to most authorities to have a strict veto and not mere \"consultation\" rights as under Canada's 1981 constitution. That constitution was not agreed to by the province of Quebec nor aboriginal authority at any level.\n\nThis especially affects Energy East, as New Brunswick and Quebec Maliseet accordingly have strict authority under UNDRIP to unilaterally reject it, as its proposed route crosses their territory (and those of a total of 180 aboriginal / indigenous groups).\n\nOpposition to Northern Gateway and Energy East is strong among natives. A veto is supported by some Canadian oil extraction corporations such as Suncor and Tembec. Indigenous groups have a long history of winning court challenges in Canada.\n\nWhile the NEB does hear aboriginal oral evidence from 70 specific intervenors the NEB process created under Stephen Harper falls far short of the demands of aboriginal groups themselves.\n\nIn 2014 John Bennett, national program director for the Sierra Club Canada (SCC) criticized the NEB for considering changes in its approach to preventing oil spills in future offshore drilling in the Beaufort Sea. Current policy requires companies working in the Arctic to have the capability to drill a relief well in the same season to release pressure and stop oil flow in case of a blowout such as the one that happened with BP in the Gulf of Mexico. But the NEB has said that other equally effective methods would be considered.\n\nEconomist Robyn Allan, questioned the July 28, 2015 appointment by the Governor in Council on behalf of the Office of the Prime Minister, of Steven J. Kelly — a former Kinder Morgan consultant — as a full-time member of the National Energy Board (Board). In a letter dated August 21, 2015 NEB Board of Drectors chair David Hamiltonand and fellow Members, Alison Scott and Philip Davies wrote a In 1996 Kelly joined Purvin & Gertz, Inc. now IHS Inc. and was vice-president of IHS Global Canada, \"an oil industry consulting firm hired by Kinder Morgan to do an economic study justifying the Trans Mountain pipeline expansion.\"\n\nNEB postponed oral hearings scheduled for August 24 in Calgary to avoid a conflict of interest with Kelly's appointment which is effective October 13. Alternative dates have not yet been provided. Kinder Morgan has already filed copious amounts of IHS evidence with affidavits that was submitted by Steven Kelly in support of Kinder Morgan's application for the proposed pipeline expansion Trans Mountain. According to the NEB, all Kelly's evidence will be struck from the record.\n\nThe NEB process has been sharply criticized and even called a \"farce\" by former public officials objecting to lack of oral cross-examination.\n\n\n"}
{"id": "10249946", "url": "https://en.wikipedia.org/wiki?curid=10249946", "title": "Natural gasoline", "text": "Natural gasoline\n\nNatural gasoline is a natural gas liquid with a vapor pressure intermediate between natural gas condensate (drip gas) and liquefied petroleum gas and has a boiling point within the range of gasoline. The typical gravity of natural gasoline is around 80 API. This hydrocarbon mixture is liquid at ambient pressure and temperature. It is volatile and unstable, but can be blended with other hydrocarbons to produce commercial gasoline. The hydrocarbon mixture of natural gasoline are mostly pentanes and heavier (smaller amounts of C6 and C6+), extracted from natural gas, that meets vapor pressure, end-point, and other specifications for natural gasoline set by the Gas Processors Association. Includes isopentane, which is a saturated branch-chain hydrocarbon (CH), obtained by fractionation of natural gasoline or isomerization of normal pentane.\n\nNatural gasoline is often used as a denaturant for fuel-grade ethanol, where it is commonly added volumetrically between 2.0% and 2.5% to make denatured fuel ethanol (DFE), or E98. This process renders the fuel-grade ethanol undrinkable. It is then transferred to a blender, which will add this E98 to conventional gasoline to make common 87 octane fuels (E10). It can also be added to ethanol in higher volumetric concentrations to produce high-level blends of ethanol, such as E85. Natural gasoline has a lower octane content (RON roughly equal to 70) than conventional commercial distilled gasoline, so it cannot normally be used by itself for fuel for modern automobiles. However, when mixed with higher concentrations of ethanol (RON roughly equal to 113) to produce products such as E85, the octane level of the natural gasoline and ethanol mixture is now within the usable range for flex-fuel vehicles. \n\nIt may be sourced from production of natural-gas wells (see \"drip gas\") or produced by extraction processes in the field, as opposed to refinery cracking of conventional gasoline.\n"}
{"id": "3530751", "url": "https://en.wikipedia.org/wiki?curid=3530751", "title": "Nd:YAB", "text": "Nd:YAB\n\nNd:YAB (Yttrium aluminum borate, YAl(BO), hereafter referred to as YAB), is grown by the flux method with a modified molybdate flux system. And it provides the possibility of realizing diode-pumped visible lasers by self-frequency-doubling (SFD) the main infrared Laser Radiation lines.\n\n"}
{"id": "14335071", "url": "https://en.wikipedia.org/wiki?curid=14335071", "title": "Oil megaprojects", "text": "Oil megaprojects\n\nOil megaprojects are large oil field projects.\n\nA series of project tabulations and analyses by Chris Skrebowski, editor of \"Petroleum Review\", have presented a more pessimistic picture of future oil supply. In a 2004 report, based on an analysis of new projects over , he argued that although ample supply might be available in the near-term, after 2007 \"the volumes of new production for this period are well below likely requirements.\" By 2006, although \"the outlook for future supply appears somewhat brighter than even six months ago\", nonetheless, if \"all the factors reducing new capacity come into play, markets will remain tight and prices high. Only if new capacity flows into the system rather more rapidly than of late, will there be any chance of rebuilding spare capacity and softening prices.\"\n\nThe smallest fields, even in aggregate, do not contribute a large fraction of the total. For example, a relatively small number of giant and super-giant oilfields are providing almost half of the world production.\n\nThe most important variable is the average decline rate for Fields in Production (FIP) which is difficult to assess.\n\n"}
{"id": "54709700", "url": "https://en.wikipedia.org/wiki?curid=54709700", "title": "Oil purification", "text": "Oil purification\n\nOil purification (transformer, turbine, industrial, etc.) removes oil contaminants in order to prolong oil service life.\nContaminants and various impurities get into industrial oils during storage and operation. The most common contaminants are :\nIndustrial oils are purified through sedimentation, filtration, centrifugation, vacuum treatment and adsorption purification .\n\nSedimentation is precipitation of solid particles and water to the bottom of oil tanks under gravity. The main drawback of this process is its longevity .\n\nFiltration is a partial removal of solid particles through filter medium. Oil filtration systems generally use a multistage filtration with coarse and fine filters .\n\nCentrifugation is separation of oil and water, or oil and solid particles by centrifugal forces.\n\nVacuum treatment degasses and dehydrates industrial oil. This method is well suited for removing dispersed and dissolved water, as well as dissolved gases .\n\nAdsorption purification, in contrast to the methods mentioned above, does not remove solid particles and gases, but it shows good results at removing water, oil sludge and aging products. This process uses adsorbents of natural or artificial origin: bleaching clays, synthetic aluminosilicates, silica gels, zeolites, etc .\n\nOften the terms \"oil purification\" and \"oil regeneration\" are used synonymously. Although in fact they are not the same. Oil purification cleans oil from contaminants. it can be used independently or as a part of oil regeneration. Oil regeneration also removes aging products (with the help of adsorbents) and stabilizes oil with additives. Regenerated oil is clean from carcinogenic products of oil aging and stabilized with the help of additives.\n"}
{"id": "58940478", "url": "https://en.wikipedia.org/wiki?curid=58940478", "title": "PEG Africa", "text": "PEG Africa\n\nPEG Africa (also known as PEG) is a for profit, asset financing solar power company operating in West Africa. The company provides loans for pay-as-you-go (PAYG) solar power home systems to households that do not have access to an electrical grid. The loans are repaid by customers in small increments, using mobile payments. The solar power home systems can be used by customers as collateral for loans on further products and services.\n\nPEG Africa is headquartered in Accra, Ghana (PEG Ghana) with additional offices in Abidjan, Ivory Coast (PEG Côte d’Ivoire) and Dakar, Senegal.\n\nThe company was founded in 2013 by Hugh Whalan (CEO) and Nate Heller. Whalan had previously run a solar financing and distribution company called Impact Energies in Ghana , which was acquired in 2013 in the first off-grid solar exit in Africa.\n\nAfter commercial launch in 2015, the company had over 10,000 customers and 29 service centers across Ghana by 2016, and closed its Series A funding round with a total of $7.5 million. In 2017, the company raised a further $13.5 million through a combination of debt and Series B equity financing. Investors across both rounds of funding included Energy Access Ventures (EAV), Blue Haven Initiative, Investisseurs & Partenaires (I&P), ENGIE Rassembleurs d’Energies, Impact Assets, Acumen, PCG Investments, among others. In late 2016, the company extended its operations to the Ivory Coast, and in 2018, the company expanded to Senegal.\n\nSince 2016, PEG Africa has been awarded by B Corporation with ‘\"best for Customers\"’ and ‘\"best for the World\"’ awards. In 2017, the company has introduced free hospital insurance for its customers and in 2018 started a corporate social responsibility (CSR) initiative in Ghana named PEG Boafo. The initiative aims to provide education and healthcare support to remote communities in West Africa. In March 2018 PEG provided lighting systems for a school and a medical facility in the Volta Region. Under the Boafo initiative, the company has also built a learning center and community health facility in Amotare, a rural community near Begoro.\n\nPEG received the International Ashden Award for \"Innovative Finance in 2017\" and an SME Ghana Award in 2018. The company features on the London Stock Exchange Group list of Companies to Inspire Africa 2017 in the Renewable Energy category.\n"}
{"id": "9073820", "url": "https://en.wikipedia.org/wiki?curid=9073820", "title": "PVLAS", "text": "PVLAS\n\nPVLAS (Polarizzazione del Vuoto con LASer, \"polarization of the vacuum with laser\") aims to carry out a test of quantum electrodynamics and possibly detect dark matter at the Department of Physics and National Institute of Nuclear Physics in Ferrara, Italy. It searches for vacuum polarization causing nonlinear optical behavior in magnetic fields. Experiments began in 2001 at the INFN Laboratory in Legnaro (Padua, Italy) and continue today with new equipment.\n\nNonlinear electrodynamic effects in vacuum have been predicted since the earliest days of quantum electrodynamics (QED), a few years after the discovery of positrons. One such effect is vacuum magnetic birefringence, closely connected to elastic light-by-light interaction. The effect is extremely small and has never yet been observed directly.\nAlthough today QED is a very well-tested theory, the importance of detecting light-by-light interaction remains. First, QED has always been tested in the presence of charged particles either in the initial state or the final state. No tests exist in systems with only photons. More generally, no interaction has ever been observed directly with only gauge bosons present in the initial and final states. Second, to date, the evidence for zero-point quantum fluctuations relies entirely on the observation of the Casimir effect, which applies to photons only. PVLAS deals with the fluctuations of virtual charged particle-antiparticle pairs (of any nature, including hypothetical millicharged particles) and therefore the structure of fermionic quantum vacuum: to leading order, it would be a direct detection of loop diagrams. Finally, the observation of light-by-light interaction would be an evidence of the breakdown of the superposition principle and of Maxwell’s equations. One important consequence of a nonlinearity is that the velocity of light would depend on the presence or not of other electromagnetic fields. PVLAS carries out its search by looking at changes in the polarisation state of a linearly polarised laser beam after it passes through a vacuum with an intense magnetic field. The birefringence of the vacuum in quantum electrodynamics by an external field is generally credited to Stephen L. Adler, who presented the first general derivation in Photon splitting and photon dispersion in a strong magnetic field in 1971. Experimental investigation of the photon splitting in atomic field was carried out at the ROKK-1 facility at the Budker institute in 1993-96.\n\nPVLAS uses a high-finesse Fabry-Perot optical cavity. The first setup, used until 2005, sent a linearly polarized laser beam though vacuum with 5T magnetic field from a superconducting magnet to an ellipsometer. After upgrades to avoid fringe fields, several runs were done at 2.3T and 5T, excluding a prior claim of axion detection. It was determined that an optimized optical setup was needed for discovery potential. A prototype with much improved sensitivity was tested in 2010. In 2013 the upgraded apparatus at INFN Ferrara with permanent magnets and horizontal ellipsometer was set up and began data taking in 2014\n\nPVLAS investigated vacuum polarization induced by external magnetic fields. An observation of the rotation of light polarization by the vacuum in a magnetic field was published in 2006. Data taken with an upgraded setup excluded the previous magnetic rotation in 2008 and set limits on photon-photon scattering. An improved limit on nonlinear vacuum effects was set in 2012: A < 2.9·10 T @ 95% C.L.\n\n\n"}
{"id": "902969", "url": "https://en.wikipedia.org/wiki?curid=902969", "title": "Permissible exposure limit", "text": "Permissible exposure limit\n\nThe permissible exposure limit (PEL or OSHA PEL) is a legal limit in the United States for exposure of an employee to a chemical substance or physical agent such as loud noise. Permissible exposure limits are established by the Occupational Safety and Health Administration (OSHA). Most of OSHA’s PELs were issued shortly after adoption of the Occupational Safety and Health (OSH) Act in 1970. \n\nFor chemicals, the chemical regulation is usually expressed in parts per million (ppm), or sometimes in milligrams per cubic meter (mg/m). Units of measure for physical agents such as noise are specific to the agent. \n\nA PEL is usually given as a time-weighted average (TWA), although some are short-term exposure limits (STEL) or ceiling limits. A TWA is the average exposure over a specified period, usually a nominal eight hours. This means that, for limited periods, a worker may be exposed to concentration excursions higher than the PEL, so long as the TWA is not exceeded and any applicable excursion limit is not exceeded. An excursion limit typically means that \"...worker exposure levels may exceed 3 times the PEL-TWA for no more than a total of 30 minutes during a workday, and under no circumstances should they exceed 5 times the PEL-TWA, provided that\nthe PEL-TWA is not exceeded.\" Excursion limits are enforced in some states (for example Oregon) and on the federal level for certain contaminants such as asbestos.\n\nA short-term exposure limit is one that addresses the average exposure over a 15-30 minute period of maximum exposure during a single work shift. A ceiling limit is one that may not be exceeded for any time, and is applied to irritants and other materials that have immediate effects.\n\nThe current PEL for OSHA standards are based on a 5 decibel exchange rate. OSHA’s PEL for noise exposure is 90 decibels (dBA) for an 8-hour TWA. Levels of 90-140 dBA are included in the noise dose. PEL can also be expressed as 100 percent “dose” for noise exposure. When the noise exposure increases by 5 dB, the exposure time is cut in half. According to OSHA, a 95dBA TWA would be a 200 percent dose. PEL is exceeded when TWA > 90 dBA. OSHA requires feasible engineering OR administrative controls, and mandatory hearing protection when the PEL is exceeded.\n\nLike OSHA, Mine Safety and Health Administration(MSHA) also uses the same 5 decibel exchange rate and 90 dBA for an 8-hour TWA for their PEL. Once a miner's noise exposure exceeds the PEL, feasible engineering AND administrative controls must be in place to try to limit the noise exposure of the employees. If a mine operator uses administrative controls, procedures for such controls must be posted on the bulletin board and a copy must be supplied to all affected employees. \nThe National Institute for Occupational Safety and Health(NIOSH) Recommended Exposure Limit (REL) for noise exposure uses a 3 decibel exchange rate. The recommendation for occupational noise exposure is 85 decibles (dBA) for an 8-hour TWA. For every 3dB over 85, the exposure time is cut in half. NIOSH reports exposures above this level are considered hazardous. NIOSH uses a hierarchy of control to reduce or remove hazardous noise.\n\n"}
{"id": "215844", "url": "https://en.wikipedia.org/wiki?curid=215844", "title": "Pulp (paper)", "text": "Pulp (paper)\n\nPulp is a lignocellulosic fibrous material prepared by chemically or mechanically separating cellulose fibres from wood, fiber crops, waste paper, or rags. Many kinds of paper are made from wood with nothing else mixed into them. This includes newspaper, magazines and even toilet paper. Pulp is one of the most abundant raw materials .\n\nPulp for papermaking was produced by macerating mulberry bark as early as the 2nd century in Han dynasty China, where the invention of paper is traditionally attributed to Cai Lun. Lu Ji, in his 3rd century commentary on the \"Classic of Poetry\", mentions that people residing south of the Yangtze River would traditionally pound mulberry bark to make paper or clothing. By the 6th century, the mulberry tree was domesticated by farmers in China specifically for the purpose of producing pulp to be used in the papermaking process. In addition to mulberry, pulp was also made from bamboo, hibiscus bark, blue sandalwood, straw, and cotton. Papermaking using pulp made from hemp and linen fibers from tattered clothing, fishing nets and fabric bags spread to Europe in the 13th century, with an ever-increasing use of rags being central to the manufacture and affordability of rag paper, a factor in the development of printing. By the 1800s, demand often exceeding the available supply of rags, and also the manual labor of papermaking resulted in paper being still a relatively pricey product.\n\nUsing wood pulp to make paper is a fairly recent innovation, that was almost concurrent to the invention of automatic papermaking machines, both together resulting in paper and cardboard becoming an inexpensive commodity in modern times. Although the first use of paper made from wood pulp dates from 1800, as seen in some pages of a book published by Matthias Koops that year in London, large-scale wood paper production began with the development of mechanical pulping in Germany by Friedrich Gottlob Keller in the 1840s, and by the Canadian inventor Charles Fenerty in Nova Scotia, Chemical processes quickly followed, first with J. Roth's use of sulfurous acid to treat wood, then by Benjamin Tilghman's U.S. patent on the use of calcium bisulfite, Ca(HSO), to pulp wood in 1867. Almost a decade later, the first commercial sulfite pulp mill was built, in Sweden. It used magnesium as the counter ion and was based on work by Carl Daniel Ekman. By 1900, sulfite pulping had become the dominant means of producing wood pulp, surpassing mechanical pulping methods. The competing chemical pulping process, the sulfate, or kraft, process, was developed by Carl F. Dahl in 1879; the first kraft mill started, in Sweden, in 1890. The invention of the recovery boiler, by G.H. Tomlinson in the early 1930s, allowed kraft mills to recycle almost all of their pulping chemicals. This, along with the ability of the kraft process to accept a wider variety of types of wood and to produce stronger fibres, made the kraft process the dominant pulping process, starting in the 1940s.\n\nGlobal production of wood pulp in 2006 was 175 million tons (160 million tonnes). In the previous year, 63 million tons (57 million tonnes) of market pulp (not made into paper in the same facility) was sold, with Canada being the largest source at 21 percent of the total, followed by the United States at 16 percent. The wood fiber sources required for pulping are \"45% sawmill residue, 21% logs and chips, and 34% recycled paper\" (Canada, 2014). Chemical pulp made up 93 percent of market pulp.\n\nThe timber resources used to make wood pulp are referred to as pulpwood. While in theory, any tree can be used for pulp-making, coniferous trees are preferred because the cellulose fibers in the pulp of these species are longer, and therefore make stronger paper.\nSome of the most commonly used softwood trees for paper making include spruce, pine, fir, larch and hemlock, and hardwoods such as eucalyptus, aspen and birch. There is also increasing interest in genetically modified tree species (such as GM eucalyptus and GM poplar), because of several major benefits these can provide, such as increased ease of breaking down lignin and increased growth rate.\n\nA pulp mill is a manufacturing facility that converts wood chips or other plant fibre source into a thick fiberboard which can be shipped to a paper mill for further processing. Pulp can be manufactured using mechanical, semi-chemical or fully chemical methods (kraft and sulfite processes). The finished product may be either bleached or non-bleached, depending on the customer requirements.\n\nWood and other plant materials used to make pulp contain three main components (apart from water): cellulose fibers (desired for papermaking), lignin (a three-dimensional polymer that binds the cellulose fibres together) and hemicelluloses, (shorter branched carbohydrate polymers). The aim of pulping is to break down the bulk structure of the fibre source, be it chips, stems or other plant parts, into the constituent fibres.\n\nChemical pulping achieves this by degrading the lignin and hemicellulose into small, water-soluble molecules which can be washed away from the cellulose fibres without depolymerizing the cellulose fibres (chemically depolymerizing the cellulose weakens the fibres). The various mechanical pulping methods, such as groundwood (GW) and refiner mechanical (RMP) pulping, physically tear the cellulose fibres one from another. Much of the lignin remains adhering to the fibres. Strength is impaired because the fibres may be cut. There are a number of related hybrid pulping methods that use a combination of chemical and thermal treatment to begin an abbreviated chemical pulping process, followed immediately by a mechanical treatment to separate the fibres. These hybrid methods include thermomechanical pulping, also known as TMP, and chemithermomechanical pulping, also known as CTMP. The chemical and thermal treatments reduce the amount of energy subsequently required by the mechanical treatment, and also reduce the amount of strength loss suffered by the fibres.\n\nMost pulp mills use good forest management practices in harvesting trees to ensure that they have a sustainable source of raw materials. One of the major complaints about harvesting wood for pulp mills is that it reduces the biodiversity of the harvested forest. Pulp tree plantations account for 16 percent of world pulp production, old-growth forests 9 percent, and second- and third- and more generation forests account for the rest. Reforestation is practiced in most areas, so trees are a renewable resource. The FSC (Forest Stewardship Council), SFI (Sustainable Forestry Initiative), PEFC (Programme for the Endorsement of Forest Certification), and other bodies certify paper made from trees harvested according to guidelines meant to ensure good forestry practices.\n\nThe number of trees consumed depends on whether mechanical processes or chemical processes are used. It has been estimated that based on a mixture of softwoods and hardwoods 12 metres (40 ft) tall and 15–20 centimetres (6–8 in) in diameter, it would take an average of 24 trees to produce 0.9 tonne (1 ton) of printing and writing paper, using the kraft process (chemical pulping). Mechanical pulping is about twice as efficient in using trees, since almost all of the wood is used to make fibre, therefore it takes about 12 trees to make 0.9 tonne (1 ton) of mechanical pulp or newsprint.\n\nThere are roughly two short tons in a cord of wood.\n\nWood chipping is the act and industry of chipping wood for pulp, but also for other processed wood products and mulch. Only the heartwood and sapwood are useful for making pulp. Bark contains relatively few useful fibers and is removed and used as fuel to provide steam for use in the pulp mill. Most pulping processes require that the wood be chipped and screened to provide uniform sized chips.\n\nThere are a number of different processes which can be used to separate the wood fiber:\n\nManufactured grindstones with embedded silicon carbide or aluminum oxide can be used to grind small wood logs called \"bolts\" to make stone pulp (SGW). If the wood is steamed prior to grinding it is known as pressure ground wood pulp (PGW). Most modern mills use chips rather than logs and ridged metal discs called refiner plates instead of grindstones. If the chips are just ground up with the plates, the pulp is called refiner mechanical pulp (RMP) and if the chips are steamed while being refined the pulp is called thermomechanical pulp (TMP). Steam treatment significantly reduces the total energy needed to make the pulp and decreases the damage (cutting) to fibres. Mechanical pulps are used for products that require less strength, such as newsprint and paperboards.\n\nThermomechanical pulp is pulp produced by processing wood chips using heat (thus \"thermo-\") and a mechanical refining movement (thus \"-mechanical\"). It is a two-stage process where the logs are first stripped of their bark and converted into small chips. These chips have a moisture content of around 25–30 percent. A mechanical force is applied to the wood chips in a crushing or grinding action which generates heat and water vapour and softens the lignin thus separating the individual fibres. The pulp is then screened and cleaned, any clumps of fibre are reprocessed. This process gives a high yield of fibre from the timber (around 95 percent) and as the lignin has not been removed, the fibres are hard and rigid.\n\nWood chips can be pretreated with sodium carbonate, sodium hydroxide, sodium sulfite and other chemicals prior to refining with equipment similar to a mechanical mill. The conditions of the chemical treatment are much less vigorous (lower temperature, shorter time, less extreme pH) than in a chemical pulping process since the goal is to make the fibres easier to refine, not to remove lignin as in a fully chemical process. Pulps made using these hybrid processes are known as chemithermomechanical pulps (CTMP).\n\nChemical pulp is produced by combining wood chips and chemicals in large vessels called digesters. There, heat and chemicals break down lignin, which binds cellulose fibres together, without seriously degrading the cellulose fibres. Chemical pulp is used for materials that need to be stronger or combined with mechanical pulps to give a product different characteristics. The kraft process is the dominant chemical pulping method, with the sulfite process second. Historically soda pulping was the first successful chemical pulping method.\n\nRecycled pulp is also called deinked pulp (DIP). DIP is recycled paper which has been processed by chemicals, thus removing printing inks and other unwanted elements and freed the paper fibres. The process is called deinking.\n\nDIP is used as raw material in papermaking. Many newsprint, toilet paper and facial tissue grades commonly contain 100 percent deinked pulp and in many other grades, such as lightweight coated for offset and printing and writing papers for office and home use, DIP makes up a substantial proportion of the furnish.\n\nOrganosolv pulping uses organic solvents at temperatures above 140 °C to break down lignin and hemicellulose into soluble fragments. The pulping liquor is easily recovered by distillation. The reason for using a solvent is to make the lignin more soluble in the cooking liquor. Most common used solvents are methanol, ethanol, formic acid and acetic acid often in combination with water.\n\nResearch is under way to develop biopulping (biological pulping), similar to chemical pulping but using certain species of fungi that are able to break down the unwanted lignin, but not the cellulose fibres. In the biopulping process, the fungal enzyme lignin peroxidase selectively digests lignin to leave remaining cellulose fibres. This could have major environmental benefits in reducing the pollution associated with chemical pulping. The pulp is bleached using chlorine dioxide stage followed by neutralization and calcium hypochlorite. The oxidizing agent in either case oxidizes and destroys the dyes formed from the tannins of the wood and accentuated (reinforced) by sulfides present in it.\n\nSteam exploded fibre is a pulping and extraction technique that has been applied to wood and other fibrous organic material.\n\nThe pulp produced up to this point in the process can be bleached to produce a white paper product. The chemicals used to bleach pulp have been a source of environmental concern, and recently the pulp industry has been using alternatives to chlorine, such as chlorine dioxide, oxygen, ozone and hydrogen peroxide.\n\nNon-wood pulp made from rags, or from linters (short fibers discarded by the textile industry), is still manufactured today mostly as a pricey product perceived as being of better quality, especially for the art market and so-called \"archival\" paper. The modern source fiber is most often cotton, with a much higher value given to paper made from linen, hemp, abaca, kozo or other fibers. 100% cotton, or a combination of cotton and linen pulp is used for certificates, currency, and passports. Abaca pulp has very long, strong fibers, and is used for teabags.\n\nToday, some people and groups advocate using field crop fibre or agricultural residues instead of wood fibre as being more sustainable.\n\nThere is enough straw to meet much of North America's book, magazine, catalogue and copy paper needs. Agricultural-based paper does not come from tree farms. Some agricultural residue pulps take less time to cook than wood pulps. That means agricultural-based paper uses less energy, less water and fewer chemicals. Pulp made from wheat and flax straw has half the ecological footprint of pulp made from forests.\n\nHemp paper is a possible replacement, but processing infrastructure, storage costs and the low usability percentage of the plant means it is not a ready substitute.\n\nHowever, wood is also a renewable resource, with about 90 percent of pulp coming from plantations or reforested areas. Non-wood fibre sources account for about 5–10 percent of global pulp production, for a variety of reasons, including seasonal availability, problems with chemical recovery, brightness of the pulp etc. Non-wood pulp processing requires a high use of water and energy.\n\nNonwovens are in some applications alternatives to paper made from wood pulp, like filter paper or tea bags.\n\nMarket pulp is any variety of pulp that is produced in one location, dried and shipped to another location for further processing. Important quality parameters for pulp not directly related to the fibres are brightness, dirt levels, viscosity and ash content. In 2004 it accounted for about 55 million metric tons of market pulp.\n\nAir dry pulp is the most common form to sell pulp. This is pulp dried to about 10 percent moisture content. It is normally delivered as sheeted bales of 250 kg. The reason to leave 10 percent moisture in the pulp is that this minimizes the fibre to fibre bonding and makes it easier to disperse the pulp in water for further processing to paper.\n\nRoll pulp or \"reel pulp\" is the most common delivery form of pulp to non traditional pulp markets. Fluff pulp is normally shipped on rolls (reels). This pulp is dried to 5–6 percent moisture content. At the customer this is going to a comminution process to prepare for further processing.\n\nSome pulps are flash dried. This is done by pressing the pulp to about 50 percent moisture content and then let it fall through silos that are 15–17 m high. Gas fired hot air is the normal heat source. The temperature is well above the char point of cellulose, but large amount of moisture in the fibre wall and lumen prevents the fibres from being incinerated. It is often not dried down to 10 percent moisture (air dry). The bales are not as densely packed as air dry pulp.\n\nThe major environmental impacts of producing wood pulp come from its impact on forest sources and from its waste products.\n\nThe impact of logging to provide the raw material for wood pulp is an area of intense debate. Modern logging practices, using forest management seek to provide a reliable, renewable source of raw materials for pulp mills. The practice of clear cutting is a particularly sensitive issue since it is a very visible effect of logging. Reforestation, the planting of tree seedlings on logged areas, has also been criticized for decreasing biodiversity because reforested areas are monocultures.\nLogging of old growth forests accounts for less than 10 percent of wood pulp, but is one of the most controversial issues.\n\nThe process effluents are treated in a biological effluent treatment plant, which guarantees that the effluents are not toxic in the recipient.\n\nMechanical pulp is not a major cause for environmental concern since most of the organic material is retained in the pulp, and the chemicals used (hydrogen peroxide and sodium dithionite) produce benign byproducts (water and sodium sulfate (finally), respectively).\n\nChemical pulp mills, especially kraft mills, are energy self-sufficient and very nearly closed cycle with respect to inorganic chemicals.\n\nBleaching with chlorine produces large amounts of organochlorine compounds, including dioxins.\n\nThe kraft pulping reaction in particular releases foul-smelling compounds. The hydrogen sulfide reagent that degrades lignin structure also causes some demethylation to produce methanethiol, dimethyl sulfide and dimethyl disulfide. These same compounds are released during many forms of microbial decay, including the internal microbial action in Camembert cheese, although the kraft process is a chemical one and does not involve any microbial degradation. These compounds have extremely low odor thresholds and disagreeable smells.\n\nThe main applications for pulp are paper and board production. The furnish of pulps used depends on the quality on the finished paper. Important quality parameters are wood furnish, brightness, viscosity, extractives, dirt count and strength.\n\nChemical pulps are used for making nanocellulose.\n\nSpeciality pulp grades have many other applications. Dissolving pulp is used in making regenerated cellulose that is used textile and cellophane production. It is also used to make cellulose derivatives. Fluff pulp is used in diapers, feminine hygiene products and nonwovens.\n\nThe Fourdrinier Machine is the basis for most modern papermaking, and it has been used in some variation since its conception. It accomplishes all the steps needed to transform a source of wood pulp into a final paper product.\n\nIn 2009, NBSK pulp sold for $650/ton in the United States. The price had dropped due to falling demand when newspapers reduced their size, in part, as a result of the recession.\n\n"}
{"id": "32943176", "url": "https://en.wikipedia.org/wiki?curid=32943176", "title": "Pyreliophorus", "text": "Pyreliophorus\n\nThe Pyreliophorus was a device similar to a burning glass, created by the Portuguese priest Manuel António Gomes, also known as \"padre Himalaya\", whose objective was to melt many different types of materials using solar energy. The device used several reflecting mirrors to concentrate the sunlight into a common point. With this device, it was possible to reach a temperature of around 3500 °C, enough to melt many types of metals and rocks. Unlike a common burning glass the Pyreliophorus uses a concentric parabolic array of mirrors to concentrate the sun light into a common point, instead of a lens. The device uses a clock system that makes the mirror array concentric axis to rotate along the sun alignment.\n\nThis device was one of the main attractions on the Saint Louis World's Fair in 1904, and it was awarded with two gold medals and with one silver medal. Himalaya protected his Pyreliophorus using the patent system in many jurisdictions, including the British one on the patent application number GB190116181 or in the American with the patent application US797891\n\nSome figures of the Himalaya's British patent application\n"}
{"id": "14176411", "url": "https://en.wikipedia.org/wiki?curid=14176411", "title": "Registro de Emisiones y Transferencia de Contaminantes", "text": "Registro de Emisiones y Transferencia de Contaminantes\n\nThe Registro de Emisiones y Transferencia de Contaminantes (RETC) is Mexico's Pollutant Release and Transfer Register (PRTR), similar to Canada's National Pollutant Release Inventory and the US Toxics Release Inventory. \n\n"}
{"id": "40962924", "url": "https://en.wikipedia.org/wiki?curid=40962924", "title": "Sandia method", "text": "Sandia method\n\nThe Sandia method (also known as Veers method) is a method for generating a turbulent wind profile that can be used in aero-elastic software to evaluate the fatigue imparted on a turbine in a turbulent environment. That is, it generates time series of wind speeds at a set of points on a surface, say the plane of the rotor of a wind turbine. Analysis is performed initially in the frequency domain, where turbulence can be described quantitatively with more ease than the time domain. Then, the time series are obtained by inverse fast Fourier transforms.\n\nIn its original form, the Sandia method only simulates the u-component of the wind; that is, the wind was modelled as propagating in a direction perpendicular to the plane of the rotor. Work carried out by NREL, specifically Kelley, suggested that a considerable amount of turbulent energy existed in the v-component (the v-component is parallel to both the plane of the rotor and the Earth). As such, the Sandia method was upgraded such that it included the v-component and w-component. Further upgrades have been performed such that the wind profile exhibits cross-axis correlation (turbulent fluctuations in one component being somehow connected to turbulent fluctuations in another). However, these are not considered in this article.\n\nAlthough turbulence leads to unpredictable results in the time domain, it can, to some extent, be characterized in the frequency domain. Turbulent fluctuations are dominated by low frequency components, with higher frequency components having less influence. For further information, see Kolmogorov's theory on turbulence.\n\nSeveral models of frequency domain representations of point wind speeds have been developed: the von Kármán wind turbulence model and Dryden Wind Turbulence Model are examples of such.\n\nA spectrum in its original form is a continuous function. However, computer programmes operate on discrete functions. Thus a modification to whatever type of spectrum, be it Kaimal, von Karman, or some other spectrum, is needed. This is given below:\n\nwhere formula_2 is the discretized spectrum evaluated only at the discrete frequencies formula_3, formula_4 is the continuous spectrum evaluated at formula_5 and formula_6 is the size of the step between consecutive frequencies being considered.\n\nWhen generating a time series of wind speeds for a set of points across a surface, coherence needs to be taken into account. That is, the instantaneous wind speed at some point, A, will bear some resemblance to the wind speed at some other point, B. Clearly, the resemblance is influenced by the separation of points A and B. That is, two points separated by a large distance will show less similarity to each other than two neighbouring points on the surface. \nIn addition, one would expect low frequency components of the wind speeds at points A and B to show more correlation than high frequency components. As such, many coherence functions have been proposed: Davenport, Solari, etc. The Solari coherence spectrum is provided as an example:\n\nwhere formula_8 is a constant, formula_9 is the separation of points formula_10 and formula_11 on the surface, formula_3 is the frequency, and formula_13 and formula_14 are the mean wind speeds at points formula_10 and formula_11 respectively. The indices formula_10 and formula_11 run from 1 to n, and the index formula_19 covers the frequency range. From the coherence function stems the coherence matrix. To cover all relationships between all points, the coherence function must be an formula_20-by-formula_20-by-formula_22 matrix. Clearly, the coherence matrix is symmetric about the main diagonal if we are restricting ourselves to looking at the coherence function at a single frequency. This is because the spatial separation between points A and B is the same as the spatial separation between B and A. That is, for a n-by-n grid, only formula_23 elements need to be calculated for each frequency.\n\nIt is worth noting at this point that whilst the coherence matrix is strictly a 3-dimensional matrix (i, j, and k), computer programmes which implement the Sandia method typically reduce the coherence matrix to a 2-dimensional matrix where the frequency dimension has been 'removed'. This is to ease computational requirements. A 2-dimensional matrix is also required to perform some of the actions on the spectral matrix, such as a Cholesky decomposition, which is mentioned later. Of course, variation in frequency is still applied. However, the following process is carried out in full for one given frequency before proceeding to the next frequency. As such, in the following section, a power spectrum refers to the value of the power spectrum at a given frequency and not the full set of values across the frequency range being used. That is,\n\nand\n\nformula_25\n\nwhere an element of S, formula_26 , would have given the strength of the spectrum at a particular frequency formula_3 at a particular point in space.\n\nPower spectra are needed for each of the formula_20 points on the surface; this encapsulates information about the turbulence intensity for each point. It should be noted that under IEC standards, only one power spectrum is used; that is, all formula_20 points have the same turbulence intensity.\n\nWith the power spectra, the spectral matrix can be formed. This a formula_20-by-formula_20 matrix. The main diagonal of the spectral matrix contains the previously defined spectra for all formula_20 points on the surface. The off-main diagonal elements contain all the cross spectra between the points. The cross spectra are determined by the following function:\n\nDue to the symmetry of the coherence matrix, only formula_23 elements are independent. This property can be exploited to lighten memory requirements when writing a programme to simulate the Sandia method.\n\nThe spectral matrix, formula_2, can be written as the matrix product of a matrix, formula_36, and its transpose. That is,\n\nformula_36 is ultimately needed to obtain the complex Fourier co-efficients of the Fourier transforms of the time series of the wind speeds at all the points on the surface. Note - if the Fourier transform of a time domain function, formula_39, is formula_40, then the resultant spectrum is formula_41; for multiple time domain functions, the Fourier co-efficients can be stored in a matrix, which then means that the above equation is applicable.\n\nObviously, there are an infinite number of solutions to the above expression; consequently, the assumption that formula_36 is a lower triangular matrix is made such that only one solution exists. The solution can be found via a Cholesky decomposition. The resultant formula_36 matrix can be thought of as the weighting factors for the\nlinear combination of N independent, unit-magnitude, white-noise inputs that will\nyield N correlated outputs with the correct spectral matrix.\n\nTo get the complex Fourier co-efficients associated with the Fourier transform of the time series of the wind speeds, a column vector, formula_44 is obtained by multiply the formula_36 matrix by a column vector containing formula_20 values of Gaussian white noise, formula_47, as shown below:\n\nThe column vector gives the Fourier co-efficients for all points on the grid at a given frequency. This is then built up into a two dimensional matrix which covers the complex Fourier co-efficients for all points across all frequencies. Then, an inverse fast Fourier transform is performed to get the time series. That is,\n"}
{"id": "3311418", "url": "https://en.wikipedia.org/wiki?curid=3311418", "title": "Single-ended triode", "text": "Single-ended triode\n\nA single-ended triode (SET) is a vacuum tube electronic amplifier that uses a single triode to produce an output, in contrast to a push-pull amplifier which uses a pair of devices with antiphase inputs to generate an output with the wanted signals added and the distortion components subtracted. Single-ended amplifiers normally operate in Class A; push-pull amplifiers can also operate in Classes AB or B without excessive net distortion, due to cancellation.\n\nThe term single-ended triode amplifier is mainly used for output stages of audio power amplifiers.\nThe phrase directly heated triode single-ended triode amplifier (abbreviated to DHT SET) is used when directly heated triodes are used.\n\nThere are also single-ended tetrode, beam tetrode/beam power tube/kinkless tetrode, and pentode amplifiers with the same functionality and similar circuitry; e.g. this Mullard design.\n\nA typical triode audio power amplifier will have a driver that provides voltage gain, coupled to a triode (like 2A3 and 300B) or a pentode or kinkless tetrode such as EL34 or KT88 connected as a triode, connected to the loudspeaker through an audio transformer in a common cathode arrangement. The triode is biased to Class A operation by applying a suitable negative bias voltage to its input control grid (see diagram), or by raising the cathode potential with biasing components.\n\nIn traditional SET amp, the direct current of output triode (from 30 mA for triode-strapped 6V6 to 250 mA for 6C33C) flows continuously through the primary winding of a transformer. This requires inserting a gap in the transformer core to prevent core saturation by DC current; adding a gap decreases primary inductance and limits bass response; the inductance and bass response can be restored by using a larger transformer than if the DC were not present.\n\nAn alternative schematic, parafeed amplifier, solves bandwidth problem by blocking direct current from output transformer (which does not need to be gapped, thus improving its bass response). Power supply is reconfigured into a constant current source, usually with a massive, high-inductance anode choke (gapped inductor), so there is little, if any, gain in cost and weight of magnetic components.\n\nA stereo class A single-ended design with KT88 kinkless tetrodes which produces 15W of output power per channel, and 5W when triode-connected, is the Antique Sound Lab MG-SI15DT. By comparison a pair of the same tubes in class AB push-pull claim to output about 50W at 1% distortion (higher powers at high distortion are quoted for guitar amplifiers).\n\n\n\nHistorically, negative feedback in single ended pentode amplifiers was quite common (for example, the Mullard 3-3 design built around EL84). Today negative feedback is less popular with SET amplifiers, with many having no overall feedback loop. Their frequency response, limited by transformer passband, is then modulated by irregularities in loudspeaker impedance. This, and the very low attainable power levels (3 Watts for 2A3 to 20 Watts for 6C33C), requires careful matching of amplifier to speakers; selection is usually limited to high-efficiency loudspeakers with a sensitivity exceeding 90 dB/Watt.\n\nSingle-ended triode (SET) amplifiers are considered a classic design among certain audiophiles and have achieved a cult status because of their alleged excellent midband performance (argued to be the most important part of the audio spectrum in music reproduction), \"musicality\" and \"directness.\" This perceived high sonic quality is mainly attributed to the simplicity and minimalistic approach of the circuits involved and the triode amplifying tubes that are typically used.\n\nOn the other hand, the legitimacy of branding single-ended triode amplifiers as adequate for Hi-Fi purposes is debated, as from a technical standpoint, SET amplifiers are considered to be generally far inferior to subsequent (and more common) push-pull tube designs or solid-state amplifiers: SET designs require output transformers which are able to cope with a strong DC component in the signal, which causes them to have worse performance in regard to frequency response, distortion and efficiency (although the latter is not generally a priority for most SET enthusiasts, or audiophiles in general). Furthermore, as SET amplifiers have a relatively high output impedance, it is hard to couple them effectively to a loudspeaker which hasn't been designed especially to be driven by a high output impedance amplifier, as this will cause the amplifier to be much more sensitive to the loudspeaker's impedance characteristics across the spectrum, resulting in coloration.\n\nIn general, the configuration will usually provide higher measured distortion performance compared to high feedback amplifiers. This distortion is predominantly second harmonic which is not unpleasant to the ear (the second harmonic for an A on the musical scale at 440 Hz is 880 Hz which is, obviously, also an A i.e., the same note just an octave higher), but by definition their high THD figures make SET amplifiers inaccurate. In a push pull amplifier this second harmonic distortion is cancelled in the output transformer. Several percent THD is not unusual at full power output, but will be much lower at normal music levels. Some builders and users have concluded that while global feedback reduces distortion across the harmonic range, it also reduces the dramatic dynamics associated with a SET amplifier and the highly efficient speaker needed to enjoy a low power amplifier, but this is also strongly debated.\n\nApart from the field of hi-fi amplification, SET amplifiers are well regarded by some in the guitar world precisely due to their distortion characteristics, which may considered desirable in the context of musical instrument amplification, as their aim isn't accuracy but expressiveness and harmonic complexity.\n\nIn class A, in order to produce a full sine wave, the tube must be exactly half-way on. Therefore, the maximum current swing is +/- 50%. No actual amplifier will ever achieve this.\n\nObtain the following parameters:\n\nCompute maximum theoretical power output by P[ot] = Pa / 2.\n\nWhile the valve can be driven all the way to shut-off fairly easily, the maximum current will be limited by the internal resistance of the valve as the grid reaches the voltage of the cathode and is not impeding electron flow. For this reason, valves that have a low Rp can yield more power in class A1 than other valves with similar Pa ratings.\n\nNext, compute P[o] = P[ot] * (1 - Rp/Rl) to obtain an estimate of the maximum obtainable output power.\n\nDeduct 10% from this figure to account for the relatively heavy distortion encountered as the valve approaches cut-off.\n\nClass A2 amplifiers can overcome a high Rp by driving the grid positive with respect to the cathode. Because this makes the grid a secondary anode, it too will draw current from the cathode while accelerating the remaining electrons towards the plate. Grid currents can place extreme demands on the driving circuitry, sometimes requiring as much as 8 watts input for larger tubes such as the 211.\n\n\n\n"}
{"id": "9374856", "url": "https://en.wikipedia.org/wiki?curid=9374856", "title": "Solar power in Australia", "text": "Solar power in Australia\n\nSolar power in Australia is a growing industry. As of September 2018, Australia had over 10,131 MW of installed photovoltaic (PV) solar power, of which 3,366 MW were installed in the preceding 12 months. In 2017, 23 solar PV projects with a combined installed capacity of 2,034 MW were either under construction, constructed or due to start construction having reached financial closure. PV accounted for 3.8% of Australia's electrical energy production in 2017. \n\nFeed-in tariffs and renewable energy targets designed to assist renewable energy commercialisation in Australia have largely been responsible for the rapid increase. In South Australia, a solar feed-in tariff was introduced for households and an educational program that involved installing PVs on the roofs of major public buildings such as the Adelaide Airport, State Parliament, Museum, Art Gallery and several hundred public schools. In 2018, the Queensland government introduced the Affordable Energy Plan offering interest free loans for solar panels and solar storage in an effort to increase the uptake of solar energy in the state. In 2008 Premier Mike Rann announced funding for $8 million worth of solar panels on the roof of the new Goyder Pavilion at the Royal Adelaide Showgrounds, the largest rooftop solar installation in Australia, qualifying it for official \"power station\" status. South Australia has the highest per capita take up of household solar power in Australia. \n\nThe installed PV capacity in Australia has increased 10-fold between 2009 and 2011, and quadrupled between 2011 and 2016. \nThe first commercial-scale PV power plant, the 1 MW \"Uterne Solar Power Station\", was opened in 2011. \nGreenough River Solar Farm opened in 2012 with a capacity of 10 MW. \nThe price of photovoltaics has been decreasing, and in January 2013, was less than half the cost of using grid electricity in Australia.\n\nAustralia has been internationally criticised for producing very little of its energy from solar power, despite its vast resources, extensive sunshine and overall high potential.\n\nThe combination of Australia's dry climate and latitude give it high benefits and potential for solar energy production. Most of the Australian continent receives in excess of per square metre per day of insolation during winter months, with a region in the north exceeding per square metre per day.\n\nAustralia's insolation greatly exceeds the average values in Europe, Russia, and most of North America. Comparable levels are found in desert areas of northern and southern Africa, south western United States and adjacent area of Mexico, and regions on the Pacific coast of South America. However, the areas of highest insolation are distant to Australia's population centres.\n\nWith an installed photovoltaic capacity of 5,900 MW at the end of 2016, Australia ranks among the world's top ten solar countries. The installed capacity in 2015 was 5,070 MW.\n\nThe Solar Homes and Communities Plan was a rebate provided by the Australian Government of up to A$8,000 for installing solar panels on homes and community use buildings (other than schools). This rebate was phased out on 8 June 2009, to be replaced by the Solar Credits Program, where an installation of a solar system would receive 5 times as many Renewable Energy Certificates for the first 1.5 kilowatts of capacity under the Renewable Energy Target (see below).\n\nSchools were eligible to apply for grants of up to A$50,000 to install 2 kW solar panels and other measures through the National Solar Schools Program beginning on 1 July 2008, which replaced the Green Vouchers for Schools program. Applications for the program ended 21 November 2012. A total of 2,870 schools have installed solar panels. The output of each array can be viewed, and compared with that of up to four other schools.\n\nA number of states have set up schemes to encourage the uptake of solar PV power generation involving households installing solar panels and selling excess electricity to electricity retailers to put into the electricity grid, widely called \"feed-in\". Each scheme involves the setting of feed in tariffs, which can be classified by a number of factors including the price paid, whether it is on a net or gross export basis, the length of time payments are guaranteed, the maximum size of installation allowed and the type of customer allowed to participate. Many Australian state feed-in tariffs were net export tariffs, whereas conservation groups argued for gross feed-in tariffs. In March 2009, the Australian Capital Territory (ACT) started a solar gross feed-in tariff. For systems up to 10 kW the payment was 50.05 cents per kWh. For systems from 10 kW to 30 kW the payment was 40.04 cents per kWh. The payment was revised downward once before an overall capacity cap was reached and the scheme closed. Payments are made quarterly based on energy generated and the payment rate is guaranteed for 20 years. In Germany, a guaranteed PV tariff means that (as of 2006) Germany now has the highest PV capacity per capita – at 10 W for every person in Germany compared to Australia at 2.6 W per capita.\n\nIn 2001, the Australian government introduced a mandatory renewable energy target (MRET) designed to ensure renewable energy achieves a 20% share of electricity supply in Australia by 2020. The MRET was to increase new generation from 9,500 gigawatt-hours to 45,000 gigawatt-hours by 2020. The MRET requires wholesale purchasers of electricity (such as electricity retailers or industrial operations) to purchase renewable energy certificates (RECs), created through the generation of electricity from renewable sources, including wind, hydro, landfill gas and geothermal, as well as solar PV and solar thermal. The objective is to provide a stimulus and additional revenue for these technologies. The scheme was proposed to continue until 2030.\n\nThe Solar Flagships program sets aside $1.6 billion for solar power over six years. The government funding is for 4 new solar plants that produce coal plant scale power (in total up to 1000 MW - coal plants typically produce 500 to 2,000 MW). This subsidy would need additional funding from the plant builders and/or operators. As a comparison Abengoa Solar, a company currently constructing solar thermal plants, put the cost of a 300 MW plant at €1.2 billion in 2007. In 2009, the Arizona state government announced a 200 MW plant for US$1 billion.\n\nProjects with a power rating less than 50 MW are not listed.\n\nA 20 MWp solar power plant has been built on 50 hectares of land in Royalla, a rural part of the Australian Capital Territory south of Canberra. It is powered by 83,000 solar panels, and can power 4,400 homes. It was officially opened on 3 September 2014. It is the first solar plant facility in the Australian capital, and at the time of building the largest such plant in Australia. The facility was built by a Spanish company, Fotowatio Renewable Ventures (FRV).\n\nThere are 30 solar concentrator dishes at three locations in the Northern Territory: Hermannsburg, Yuendumu and Lajamanu. Solar Systems and the Federal government were involved in the projects.\n\nThe solar concentrator dish power stations together generate 720 kW and 1,555,000 kWh per year, representing a saving of 420,000 litres of diesel and 1,550 tonnes of greenhouse gas emissions.\n\nThe solar power stations at these three remote indigenous communities in Australia's Northern Territory are constructed using Solar Systems' CS500 concentrator dish systems. The project cost A$7M, offset by a grant from the Australian and Northern Territory Governments under their Renewable Remote Power Generation Program.\n\nThe project won a prestigious Engineering Excellence award in 2005.\n\nThe Federal Government has funded over 120 innovative small-scale standalone solar systems in remote indigenous communities, designed by Bushlight, incorporating sophisticated demand side management systems with user-friendly interfaces.\n\nOver 2GW of solar farms were completed or under construction in Queensland as of 2018.\n\nThe 100MW Clare Solar Farm, located 35 km southwest of Ayr, in north Queensland begun exporting to the grid in May 2018.\n\nThe 50 MW AC Kidston Solar Project has been built on the site of the Kidston Gold Mine. This is phase 1 of a planned solar energy and pumped storage combination. Kidston is owned by Genex and was constructed by UGL\n\nThe Lilyvale Solar Farm, with a capacity of 130 MWac, is under construction by Spanish companies GRS and Acciona, after an EPC contract was signed with Fotowatio Renewable Ventures (FRV). It will be located in Lilyvale, which is around 50 km northeast of Emerald, and commercial operations are expected to start in late 2018.\n\nThe Hamilton Solar Farm is a 69.0 MW DC single-axis tracking project located a few kilometres north of Collinsville in North Queensland. Its owners are Edify Energy and Wirsol. The solar farm came online in July 2018.\n\nThe Whitsunday Solar Farm is a 69.0 MW DC single-axis tracking project located a few kilometres north of Collinsville in North Queensland. Its owners are Edify Energy and Wirsol. The solar farm came online in July 2018.\n\nThere are 2 more solar projects under construction by Edify Energy in Collinsville due to come on line in late 2018. The Hayman Solar Farm which is a 60.0 MW DC single-axis tracking project and the Daydream Solar Farm which is a 180.0 MW DC single-axis tracking project.\n\nBungala Solar Power Project north of Port Augusta is the first grid-scale facility in South Australia. Stage 1 is rated at 110MW. It has a contract to provide electricity to Origin Energy.\n\nSundrop Farms concentrated solar power plant has a generating capacity of 40 MW, and is the first of its kind to be commissioned in the state. It was completed in 2016. A floating array of solar PV panels is in place at Jamestown wastewater treatment plant, with a generating capacity of 3.5 MW.\n\nThe largest rooftop solar PV array in South Australia was installed in 2017 at Yalumba Wine Company across three Barossa locations. Total generating capacity is 1.39 MW generating approximately 2,000 MWh per annum. Previous significant installations include Adelaide airport, with a generating capacity of 1.17 MW, and the Adelaide Showgrounds, with a generating capacity of 1MW. The showgrounds array was the first PV station in Australia to reach a generating capacity of 1MW and was expected to generate approximately 1,400 Megawatt-hours of electricity annually. \n\nOn 29 November 2017 the state government announced a new round of finance for renewable energy projects which included a Planet Arc Power - Schneider Electric development of a $13.9m solar PV and battery project at a major distribution centre in Adelaide’s North. The project includes a micro-grid management system optimising 5.7MW of solar PV coupled with 2.9MWh of battery storage. The University of South Australia will develop 1.8MW of ground and roof mounted solar PV at its Mawson Lakes campus. At the Heathgate Resources Beverley mine there are plans for a relocatable 1MW of solar PV paired with a 1MW/0.5MWh battery which will be integrated with an existing on-site gas power plant.\n\nThe Aurora Solar Thermal Power Project is proposed for near Port Augusta, on the north side of the town. Auroroa has a contract to supply electricity to state government offices when it is completed in 2020. It is proposed to be a solar thermal facility providing thermal storage to be able to generate while the sun is not shining. Riverland Solar Storage has development approval to establish a photovoltaic solar power farm near Morgan. The developer expected it to begin operations in late 2018, but construction is now expected to begin in 2019.\n\nThe 100 MW PV Mildura Solar Concentrator Power Station, formerly expected to be completed in 2017, is now cancelled. It was expected to be the biggest and most efficient solar photovoltaic power station in the world. The power station was expected to concentrate the sun by 500 times onto the solar cells for ultra high power output. The Victorian power station would have generated electricity directly from the sun to meet the annual needs of over 45,000 homes with on-going zero greenhouse gas emissions.\n\nThe Gannawarra Solar Farm is a 60.0 MW DC single-axis tracking project located west of Kerang in north-west Victoria. It is the first large-scale solar farm to be constructed in Victoria.\n\nWestern Australia's first major large scale solar farm, the Greenough River Solar farm, is at Walkaway, 70 km SE of Geraldton. It was opened in October 2012. The 10MW field has 150,000 solar panels. The 20 MW Emu Downs solar farm became the largest solar farm in WA when opened in March 2018. Emu Downs solar farm is co-located with the Emu Downs Wind Farm. The proposed Asian Renewable Energy Hub would include 3,500MW of solar power along with 7500MW of wind power.\n\n\"Solar Cities\" is a demonstration program designed to promote solar power, smart meters, and energy conservation in urban locations throughout Australia. One such location is Townsville, Queensland.\n\nThe Council of Sydney is attempting to make the city run 100% on renewable energy by 2030. This plan was announced earlier in 2014 with the blueprints made public on their \nwebsite. This ambitious plan was recently awarded the 2014 Eurosolar prize in the category of \"Towns/municipalities, council districts and public utilities\".\n\n\n"}
{"id": "17153924", "url": "https://en.wikipedia.org/wiki?curid=17153924", "title": "Spring pendulum", "text": "Spring pendulum\n\nA spring pendulum (also called elastic pendulum or swinging spring) is a physical system where a piece of mass is connected to a spring so that the resulting motion contains elements of a simple pendulum as well as a spring. The system is much more complex than a simple pendulum, as the properties of the spring add an extra dimension of freedom to the system. For example, when the spring compresses, the shorter radius causes the spring to move faster due to the conservation of angular momentum. It is also possible that the spring has a range that is overtaken by the motion of the pendulum, making it practically neutral to the motion of the pendulum.\n\n"}
{"id": "38577826", "url": "https://en.wikipedia.org/wiki?curid=38577826", "title": "Starokozache Solar Park", "text": "Starokozache Solar Park\n\nThe Starokozache Solar Park is a 42.95 MW photovoltaic power located in the Odessa Oblast in southern Ukraine.\n\n"}
{"id": "11508745", "url": "https://en.wikipedia.org/wiki?curid=11508745", "title": "T Tauri wind", "text": "T Tauri wind\n\nThe T Tauri wind — so named because of the young star currently in this stage—is a phenomenon indicative of the phase of stellar development between the accretion of material from the slowing rotating material of a solar nebula and the ignition of the hydrogen that has agglomerated into the protostar.\n\nThe protostar at first only has about 1% of its final mass. But the envelope of the star continues to grow as infalling material is accreted. After 10,000–100,000 years, thermonuclear fusion begins in its core, then a strong stellar wind is produced which stops the infall of new mass. The protostar is now considered a young star since its mass is fixed, and its future evolution is now set.\n\nInitially there is a random amount of interstellar gaseous matter, mainly hydrogen, containing traces of dusts (ices, carbon, rocks).\n\nThe T Tauri stars, with masses less than twice the mass of our Sun, are thought to follow this process:\n\nThe main portion of emission continuum of \"Classic T Tauri Stars\" is formed outside the accretion shock, what means a great deal of accretion matter falls onto the star in nearly horizontal direction. This gas decelerate in turbulent layer near the star surface. \n\nWe suggest two scenarios to explain such nature of accretion: two-stream accretion (through boundary layer and magnetosphere) and magnetospheric accretion by way of streams, the bulk of matter in which falls onto the star in nearly horizontal direction.\n\nObservations have provided quantitative parameters of disk wind, derived from the analysis of optical and UV spectra of CTTS. The matter outflows observed from a disk region with an outer radius of < 0.5 AU. The outflowing matter initially moves almost along the disk until being accelerated up to \"V\" > 100 km/s and only afterwards begins to collimate. Inner region of the wind is collimated into the jet at a distance <3 AU from the disk mid plain. The \"V\" gas velocity component in the jet decreases with increasing distance from the jet axis. The gas temperature in the jet bottom is less than 20,000 kelvins.\n\n"}
{"id": "24054356", "url": "https://en.wikipedia.org/wiki?curid=24054356", "title": "Tianying", "text": "Tianying\n\nTianying () is a town under the administration of Jieshou, which is in turn administered by the prefecture-level city of Fuyang, in northwestern Anhui Province, China. The town has 26,095 inhabitants according to the 2001 census.\n\nNearly half of China's lead production is located in Tianying and the surrounding area. The use of antiquated technology and lack of proper disposal has led to high levels of lead pollution in the area. While the government closed some of the lead factories in 2003, the Blacksmith Institute still rated the place as one of the world's most polluted in 2007. Tianying, along with the Chinese city of Linfen, are two of the world's most polluted cities according to Time Magazine\n"}
{"id": "200167", "url": "https://en.wikipedia.org/wiki?curid=200167", "title": "Water cycle", "text": "Water cycle\n\nThe water cycle, also known as the hydrological cycle or the hydrologic cycle, describes the continuous movement of water on, above and below the surface of the Earth. The mass of water on Earth remains fairly constant over time but the partitioning of the water into the major reservoirs of ice, fresh water, saline water and atmospheric water is variable depending on a wide range of climatic variables. The water moves from one reservoir to another, such as from river to ocean, or from the ocean to the atmosphere, by the physical processes of evaporation, condensation, precipitation, infiltration, surface runoff, and subsurface flow. In doing so, the water goes through different forms: liquid, solid (ice) and vapor.\n\nThe water cycle involves the exchange of energy, which leads to temperature changes. When water evaporates, it takes up energy from its surroundings and cools the environment. When it condenses, it releases energy and warms the environment. These heat exchanges influence climate.\n\nThe evaporative phase of the cycle purifies water which then replenishes the land with freshwater. The flow of liquid water and ice transports minerals across the globe. It is also involved in reshaping the geological features of the Earth, through processes including erosion and sedimentation. The water cycle is also essential for the maintenance of most life and ecosystems on the planet.\n\nThe sun, which drives the water cycle, heats water in oceans and seas. Water evaporates as water vapor into the air. Some ice and snow sublimates directly into water vapor. Evapotranspiration is water transpired from plants and evaporated from the soil. The water molecule has smaller molecular mass than the major components of the atmosphere, nitrogen and oxygen, and , hence is less dense. Due to the significant difference in density, buoyancy drives humid air higher. As altitude increases, air pressure decreases and the temperature drops (see Gas laws). The lower temperature causes water vapor to condense into tiny liquid water droplets which are heavier than the air, and fall unless supported by an updraft. A huge concentration of these droplets over a large space up in the atmosphere become visible as cloud. Some condensation is near ground level, and called fog.\n\nAtmospheric circulation moves water vapor around the globe, cloud particles collide, grow, and fall out of the upper atmospheric layers as precipitation. Some precipitation falls as snow or hail, sleet, and can accumulate as ice caps and glaciers, which can store frozen water for thousands of years. Most water falls back into the oceans or onto land as rain, where the water flows over the ground as surface runoff. A portion of runoff enters rivers in valleys in the landscape, with streamflow moving water towards the oceans. Runoff and water emerging from the ground (groundwater) may be stored as freshwater in lakes. Not all runoff flows into rivers, much of it soaks into the ground as infiltration. Some water infiltrates deep into the ground and replenishes aquifers, which can store freshwater for long periods of time. Some infiltration stays close to the land surface and can seep back into surface-water bodies (and the ocean) as groundwater discharge. Some groundwater finds openings in the land surface and comes out as freshwater springs. In river valleys and floodplains, there is often continuous water exchange between surface water and ground water in the hyporheic zone. Over time, the water returns to the ocean, to continue the water cycle.\n\nThe water cycle involves many of these processes.\n\nThe \"residence time\" of a reservoir within the hydrologic cycle is the average time a water molecule will spend in that reservoir (\"see adjacent table\"). It is a measure of the average age of the water in that reservoir.\n\nGroundwater can spend over 10,000 years beneath Earth's surface before leaving. Particularly old groundwater is called fossil water. Water stored in the soil remains there very briefly, because it is spread thinly across the Earth, and is readily lost by evaporation, transpiration, stream flow, or groundwater recharge. After evaporating, the residence time in the atmosphere is about 9 days before condensing and falling to the Earth as precipitation.\n\nThe major ice sheets – Antarctica and Greenland – store ice for very long periods. Ice from Antarctica has been reliably dated to 800,000 years before present, though the average residence time is shorter.\n\nIn hydrology, residence times can be estimated in two ways. The more common method relies on the principle of conservation of mass and assumes the amount of water in a given reservoir is roughly constant. With this method, residence times are estimated by dividing the volume of the reservoir by the rate by which water either enters or exits the reservoir. Conceptually, this is equivalent to timing how long it would take the reservoir to become filled from empty if no water were to leave (or how long it would take the reservoir to empty from full if no water were to enter).\n\nAn alternative method to estimate residence times, which is gaining in popularity for dating groundwater, is the use of isotopic techniques. This is done in the subfield of isotope hydrology.\nThe water cycle describes the processes that drive the movement of water throughout the hydrosphere. However, much more water is \"in storage\" for long periods of time than is actually moving through the cycle. The storehouses for the vast majority of all water on Earth are the oceans. It is estimated that of the 332,500,000 mi (1,386,000,000 km) of the world's water supply, about 321,000,000 mi (1,338,000,000 km) is stored in oceans, or about 97%. It is also estimated that the oceans supply about 90% of the evaporated water that goes into the water cycle.\n\nDuring colder climatic periods, more ice caps and glaciers form, and enough of the global water supply accumulates as ice to lessen the amounts in other parts of the water cycle. The reverse is true during warm periods. During the last ice age, glaciers covered almost one-third of Earth's land mass with the result being that the oceans were about 122 m (400 ft) lower than today. During the last global \"warm spell,\" about 125,000 years ago, the seas were about 5.5 m (18 ft) higher than they are now. About three million years ago the oceans could have been up to 50 m (165 ft) higher.\n\nThe scientific consensus expressed in the 2007 Intergovernmental Panel on Climate Change (IPCC) Summary for Policymakers is for the water cycle to continue to intensify throughout the 21st century, though this does not mean that precipitation will increase in all regions. In subtropical land areasplaces that are already relatively dryprecipitation is projected to decrease during the 21st century, increasing the probability of drought. The drying is projected to be strongest near the poleward margins of the subtropics (for example, the Mediterranean Basin, South Africa, southern Australia, and the Southwestern United States). Annual precipitation amounts are expected to increase in near-equatorial regions that tend to be wet in the present climate, and also at high latitudes. These large-scale patterns are present in nearly all of the climate model simulations conducted at several international research centers as part of the 4th Assessment of the IPCC. There is now ample evidence that increased hydrologic variability and change in climate has and will continue to have a profound impact on the water sector through the hydrologic cycle, water availability, water demand, and water allocation at the global, regional, basin, and local levels. Research published in 2012 in \"Science\" based on surface ocean salinity over the period 1950 to 2000 confirm this projection of an intensified global water cycle with salty areas becoming more saline and fresher areas becoming more fresh over the period:Fundamental thermodynamics and climate models suggest that dry regions will become drier and wet regions will become wetter in response to warming. Efforts to detect this long-term response in sparse surface observations of rainfall and evaporation remain ambiguous. We show that ocean salinity patterns express an identifiable fingerprint of an intensifying water cycle. Our 50-year observed global surface salinity changes, combined with changes from global climate models, present robust evidence of an intensified global water cycle at a rate of 8 ± 5% per degree of surface warming. This rate is double the response projected by current-generation climate models and suggests that a substantial (16 to 24%) intensification of the global water cycle will occur in a future 2° to 3° warmer world.\n\nAn instrument carried by the SAC-D satellite Aquarius, launched in June, 2011, measured global sea surface salinity.\n\nGlacial retreat is also an example of a changing water cycle, where the supply of water to glaciers from precipitation cannot keep up with the loss of water from melting and sublimation. Glacial retreat since 1850 has been extensive.\n\nHuman activities that alter the water cycle include:\n\nThe water cycle is powered from solar energy. 86% of the global evaporation occurs from the oceans, reducing their temperature by evaporative cooling. Without the cooling, the effect of evaporation on the greenhouse effect would lead to a much higher surface temperature of , and a warmer planet.\n\nAquifer drawdown or overdrafting and the pumping of fossil water increases the total amount of water in the hydrosphere, and has been postulated to be a contributor to sea-level rise.\n\nWhile the water cycle is itself a biogeochemical cycle, flow of water over and beneath the Earth is a key component of the cycling of other biogeochemicals. Runoff is responsible for almost all of the transport of eroded sediment and phosphorus from land to waterbodies. The salinity of the oceans is derived from erosion and transport of dissolved salts from the land. Cultural eutrophication of lakes is primarily due to phosphorus, applied in excess to agricultural fields in fertilizers, and then transported overland and down rivers. Both runoff and groundwater flow play significant roles in transporting nitrogen from the land to waterbodies. The dead zone at the outlet of the Mississippi River is a consequence of nitrates from fertilizer being carried off agricultural fields and funnelled down the river system to the Gulf of Mexico. Runoff also plays a part in the carbon cycle, again through the transport of eroded rock and soil.\n\nThe hydrodynamic wind within the upper portion of a planet's atmosphere allows light chemical elements such as Hydrogen to move up to the exobase, the lower limit of the exosphere, where the gases can then reach escape velocity, entering outer space without impacting other particles of gas. This type of gas loss from a planet into space is known as planetary wind. Planets with hot lower atmospheres could result in humid upper atmospheres that accelerate the loss of hydrogen.\n\nIn ancient times, it was widely thought that the land mass floated on a body of water, and that most of the water in rivers has its origin under the earth. Examples of this belief can be found in the works of Homer (circa 800 BCE).\n\nIn the ancient near east, Hebrew scholars observed that even though the rivers ran into the sea, the sea never became full. Some scholars conclude that the water cycle was described completely during this time in this passage: \"The wind goeth toward the south, and turneth about unto the north; it whirleth about continually, and the wind returneth again according to its circuits. All the rivers run into the sea, yet the sea is not full; unto the place from whence the rivers come, thither they return again\" (, KJV). Scholars are not in agreement as to the date of Ecclesiastes, though most scholars point to a date during the time of King Solomon, son of David and Bathsheba, \"three thousand years ago, there is some agreement that the time period is 962–922 BCE. Furthermore, it was also observed that when the clouds were full, they emptied rain on the earth (). In addition, during 793–740 BCE a Hebrew prophet, Amos, stated that water comes from the sea and is poured out on the earth (, ).\n\nIn the Biblical Book of Job, dated between 7th and 2nd centuries BCE, there is a description of precipitation in the hydrologic cycle, \"For he maketh small the drops of water: they pour down rain according to the vapour thereof; Which the clouds do drop and distil upon man abundantly\" (, KJV).\n\nIn the Adityahridayam (a devotional hymn to the Sun God) of Ramayana, a Hindu epic dated to the 4th century BCE, it is mentioned in the 22nd verse that the Sun heats up water and sends it down as rain. By roughly 500 BCE, Greek scholars were speculating that much of the water in rivers can be attributed to rain. The origin of rain was also known by then. These scholars maintained the belief, however, that water rising up through the earth contributed a great deal to rivers. Examples of this thinking included Anaximander (570 BCE) (who also speculated about the evolution of land animals from fish) and Xenophanes of Colophon (530 BCE). Chinese scholars such as Chi Ni Tzu (320 BCE) and Lu Shih Ch'un Ch'iu (239 BCE) had similar thoughts. The idea that the water cycle is a closed cycle can be found in the works of Anaxagoras of Clazomenae (460 BCE) and Diogenes of Apollonia (460 BCE). Both Plato (390 BCE) and Aristotle (350 BCE) speculated about percolation as part of the water cycle.\n\nUp to the time of the Renaissance, it was thought that precipitation alone was insufficient to feed rivers, for a complete water cycle, and that underground water pushing upwards from the oceans were the main contributors to river water. Bartholomew of England held this view (1240 CE), as did Leonardo da Vinci (1500 CE) and Athanasius Kircher (1644 CE).\n\nThe first published thinker to assert that rainfall alone was sufficient for the maintenance of rivers was Bernard Palissy (1580 CE), who is often credited as the \"discoverer\" of the modern theory of the water cycle. Palissy's theories were not tested scientifically until 1674, in a study commonly attributed to Pierre Perrault. Even then, these beliefs were not accepted in mainstream science until the early nineteenth century.\n\n\n"}
