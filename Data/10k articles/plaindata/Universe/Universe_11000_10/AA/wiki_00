{"id": "1735557", "url": "https://en.wikipedia.org/wiki?curid=1735557", "title": "1,1,1,2-Tetrafluoroethane", "text": "1,1,1,2-Tetrafluoroethane\n\n1,1,1,2-Tetrafluoroethane (also known as norflurane (INN), R-134a, Freon 134a, Forane 134a, Genetron 134a, Florasol 134a, Suva 134a, or HFC-134a) is a haloalkane refrigerant with thermodynamic properties similar to R-12 (dichlorodifluoromethane) but with insignificant ozone depletion potential and a somewhat lower global warming potential (1,430, compared to R-12's GWP of 10,900). It has the formula CHFCF and a boiling point of −26.3 °C (−15.34 °F) at atmospheric pressure. R-134a cylinders are colored light blue. Attempts at phasing out its use as a refrigerant with substances that have lower global warming potential, such as HFO-1234yf, are underway.\n\n1,1,1,2-Tetrafluoroethane is a non-flammable gas used primarily as a \"high-temperature\" refrigerant for domestic refrigeration and automobile air conditioners. These devices began using 1,1,1,2-tetrafluoroethane in the early 1990s as a replacement for the more environmentally harmful R-12 and retrofit kits are available to convert units that were originally R-12-equipped. Other uses include plastic foam blowing, as a cleaning solvent, a propellant for the delivery of pharmaceuticals (e.g. bronchodilators), wine cork removers, gas dusters, such as Dust-Off, and in air driers for removing the moisture from compressed air. 1,1,1,2-Tetrafluoroethane has also been used to cool computers in some overclocking attempts. It is the refrigerant used in plumbing pipe freeze kits. It is also commonly used as a propellant for airsoft airguns. The gas is often mixed with a silicone-based lubricant.\n1,1,1,2-Tetrafluoroethane is also being considered as an organic solvent suitable for extraction of flavor and fragrance compounds, as a possible alternative to other organic solvents and supercritical carbon dioxide.\nIt can also be used as a solvent in organic chemistry, both in liquid and supercritical fluid.\nIt is used in the resistive plate chamber particle detectors in the Large Hadron Collider. It is also used for other types of particle detectors, e.g. some cryogenic particle detectors. It can be used as an alternative to sulfur hexafluoride in magnesium smelting as a shielding gas.\n\n1,1,1,2-Tetrafluoroethane is also being considered as an alternative to sulfur hexafluoride as a dielectric gas. Its arc-quenching properties are poor, but its dielectric properties are fairly good.\n\nRecently, 1,1,1,2-tetrafluoroethane has been subject to use restrictions due to its contribution to climate change. It has a global warming potential of 1300.\n\nThe Society of Automotive Engineers (SAE) has proposed 1,1,1,2-tetrafluoroethane to be best replaced by a new fluorochemical refrigerant HFO-1234yf (CFCF=CH) in automobile air-conditioning systems. California may also prohibit the sale of canned 1,1,1,2-tetrafluoroethane to individuals to avoid non-professional recharge of air conditioners. A ban had been in place in Wisconsin since October 1994 under ATCP 136 prohibiting sales of container sizes holding less than 15 lbs of 1,1,1,2-tetrafluoroethane, but this restriction applied only when the chemical was intended to be a refrigerant. However, the ban was lifted in Wisconsin in 2012. During the time that it was active, this Wisconsin-specific ban contained loopholes. For example, it was legal for a person to purchase gas duster containers with any amount of the chemical because in that instance the chemical is neither intended to be a refrigerant nor is HFC-134a included in the § 7671a listing of class I and class II substances.\n\n1,1,1,2-Tetrafluoroethane first appeared in the early 1990s as a replacement for dichlorodifluoromethane (R-12), which has ozone depleting properties. 1,1,1,2-Tetrafluoroethane has been atmospherically modeled for its impact on depleting ozone and as a contributor to global warming. Research suggests that over the past 10 years the concentration of 1,1,1,2-tetrafluoroethane has increased significantly in the Earth's atmosphere, with a recent study revealing a doubling in atmospheric concentration between 2001 and 2004. It has insignificant ozone depletion potential (ozone layer), significant global warming potential (100-yr GWP = 1430) and negligible acidification potential (acid rain). Because of its high GWP, 1,1,1,2-tetrafluoroethane has been banned from use in the European Union, starting with cars in 2011 and phasing out completely by 2017.\n\nTetrafluoroethane is typically made by reacting trichloroethylene with hydrogen fluoride:\n\nMixtures with air of the gas 1,1,1,2-tetrafluoroethane are not flammable at atmospheric pressure and temperatures up to 100 °C (212 °F). However, mixtures with high concentrations of air at elevated pressure and/or temperature can be ignited. Contact of 1,1,1,2-tetrafluoroethane with flames or hot surfaces in excess of 250 °C (482 °F) may cause vapor decomposition and the emission of toxic gases including hydrogen fluoride and carbonyl fluoride. 1,1,1,2-Tetrafluoroethane itself has an of 1,500 g/m in rats, making it relatively non-toxic, apart from the dangers inherent to inhalant abuse. Its gaseous form is denser than air and will displace air in the lungs. This can result in asphyxiation if excessively inhaled. This is what contributes to most deaths by inhalant abuse.\n\nAerosol cans containing 1,1,1,2-tetrafluoroethane, when inverted, become effective freeze sprays. Under pressure, 1,1,1,2-tetrafluoroethane is compressed into a liquid, which upon vaporization absorbs a significant amount of thermal energy. As a result, it will greatly lower the temperature of any object it contacts as it evaporates. This can result in frostbite when it contacts skin, as well as blindness upon eye contact.\n\nFor its medical uses, 1,1,1,2-tetrafluoroethane has the generic name norflurane. It is used as propellant for some metered dose inhalers. It is considered safe for this use. In combination with pentafluoropropane, it is used as a topical vapocoolant spray for numbing boils before curettage. It has also been studied as a potential inhalational anesthetic, but it is nonanaesthetic at doses used in inhalers.\n\n\n"}
{"id": "1825226", "url": "https://en.wikipedia.org/wiki?curid=1825226", "title": "2002 European floods", "text": "2002 European floods\n\nIn August 2002 a flood caused by over a week of continuous heavy rains ravaged Europe, killing dozens, dispossessing thousands, and causing damage of billions of euros in Russia, the Czech Republic, Austria, Germany, Slovakia, Poland, Hungary, Romania and Croatia. The flood was of a magnitude expected to occur roughly once a century. there were approximately 250 deaths by this natural disaster. Unprecedented flood heights were recorded and over 110 people died. These were the most costly floods affecting Europe in years. As of December 2002, total economic damage estimates exceeded 15 billion Euro. 15% of this cost was insured. \n\nFlooding resulted from the passage of two Genoa low pressure systems (named Hanne and Ilse by the Free University of Berlin) which brought warm moist air from the Mediterranean northwards. The effects of El Niño are believed to have possibly contributed although others disagree. The floods started with heavy rainfall in the Eastern Alps, which resulted in floods in Northern Italy, Bavaria and the Austrian states of Salzburg and Upper Austria . The floods gradually moved eastwards along the Danube, although the damage in the large cities on its shores was not as severe as in the areas affected by the floods later.\n\nWhen the rainfall moved northeast to the Bohemian Forest and to the source areas of the Elbe and Vltava rivers, the result were catastrophic water levels first in the Austrian areas of Mühlviertel and Waldviertel and later in the Czech Republic, Thuringia and Saxony. Rivers changed their courses in unexpected ways, catching residents off guard . Several villages in Northern Bohemia, Thuringia and Saxony were more or less destroyed by rivers changing their courses or massively overflowing their banks.\n\nThe floods that hit Europe during August 2002 were part of a larger system that was also affecting Asia. Within Europe, however, the areas that sustained significant damage included the Czech Republic, Slovakia, Italy, Spain, Germany, Romania, Bulgaria, Croatia, Hungary, and Ukraine. Several rivers in the region, including the Vltava, Elbe and Danube reached record highs.\n\nPrague received significant damage from what were deemed to be the worst floods ever to hit the capital. The flow of Vltava culminated at 5300 m³/s, 20% more than during the flood of 1845. Among the regions of the capital city most severely affected were: Karlín, Kampa and Holešovice, where there was significant risk of building collapse. Most of Prague's art work was saved due to advance warning of high water levels, however there was significant damage to the Prague Metro subway system, much of which was completely flooded, as well as to the historic Prague pneumatic post, which was forced to cease operation.\n\nPrague's Jewish Quarter also received significant damage, a part of the estimated $30 million in damage to Czech cultural sites including: the Prague Municipal Library, Malá Strana, the National Theatre and Terezín.\n\nThe evacuations before the worst of the flooding have been cited as one of the reasons for relatively little loss of life in the capital. An estimated 40,000 people were evacuated from Prague. One of the most visible victims of the summer's flood was Gaston, a sea lion from the Prague Zoo who swam to Germany following the flooding of his aquarium. For some time, it was believed he would survive, however he died following capture in Dresden.\n\nIn total, 17 people lost their lives in the Czech Republic due to the floods, and damage from the flood was estimated at between 70 and 73 billion CZK. The damage to the Prague metro has been estimated at approximately 7 billion CZK.\n\nGermany was the hardest hit, with over two-thirds of the flood’s total losses. the 10 years of renovation work that had been carried out since reunification in 1990 in the town of Grimma, in the former East Germany, were said to have been destroyed in one night.\n\nDresden received significant damage when the Elbe River reached an all-time high of 9.4 meters (30.8 feet). More than 30,000 people were evacuated from various neighborhoods throughout the city and some of the city's cultural landmarks were considered to be at risk.\n\nDresden's Zwinger Palace, home to a significant number of Europe's artistic treasures including Raphael's Sistine Madonna was at risk from the flooding Elbe, however all of the art works could be saved. The Semper Opera House also suffered damage.\n\nThe Black Sea Coast region was among the most severely hit regions of Russia with significant loss of life due to a tornado that hit the tourist region and destroyed homes. This was after earlier summer floods in southern Russia. All told, damage in the region was calculated at more than $400 million.\n\nAlthough all of Europe was affected to some degree or another from the record rains that fell, some cities were spared the severe flooding that hit Dresden and Prague.\n\nAlthough the Danube reached record highs, both Bratislava and Vienna were spared significant flooding. Bratislava's sparing was due to the city's flood protection measures, which withstood the water, while it was generally believed that Vienna was spared significant damage due the city's engineering, and plans were undertaken to see if such work could be applied to the other cities as well. The floods also hit Budapest, with 1,700 people evacuated.\n\nOnce the water levels returned to normal and residents returned to their home, they faced not only the damage left by the rising waters but also threats of disease due to decaying waste and food. The damage increased due to flooding of sewage treatment plants and the risk of damage to chemical plants.\n\nEven once the waters began to recede, the work in the region was not yet complete. European leaders gathered in Berlin to discuss the effects of the floods and to create a better understanding of how to prevent such disaster in the future. This meeting garnered some criticism as Russia, which had suffered significant damage, was not invited to what was billed as a meeting of EU members and future members. The EU leaders did promise aid to the central European countries that suffered the most under the floods with monies coming from the EU's structural budget and this outreach to non-members was seen as symbolic in an effort to portray a truly united Europe.\n\n\n"}
{"id": "54834563", "url": "https://en.wikipedia.org/wiki?curid=54834563", "title": "ADA (buffer)", "text": "ADA (buffer)\n\nADA is a zwitterionic organic chemical buffering agent; one of Good's buffers. It has a useful pH range of 6.0-7.2 in the physiological range, making it useful for cell culture work. It has a pKa of 6.6 with ΔpKa/°C of -0.011 and is most often prepared in 1 M NaOH where it has a solubility of 160 mg/mL.\n\nADA has been used in protein-free media for chicken embryo fibroblasts, as a chelating agent for H, Ca, and Mg, and for isoelectric focusing in immobilized pH gradients. Its effects on dog kidney Na/K-ATPase and rat brain GABA receptors have also been studied. ADA does, however, alter coloring in bicinchoninic acid assays.\n"}
{"id": "53897245", "url": "https://en.wikipedia.org/wiki?curid=53897245", "title": "ANSI A300", "text": "ANSI A300\n\nANSI A300 is the tree care industry standard of care in the USA. Various industry stakeholders participate in its periodic review and updating. The standard is divided into ten parts:\n\n\nA proposed A300 Part 11, Urban Forest Products, was not adopted because it was deemed to be outside the scope of tree care management standards. \n"}
{"id": "42749266", "url": "https://en.wikipedia.org/wiki?curid=42749266", "title": "Alliance for Water Efficiency", "text": "Alliance for Water Efficiency\n\nThe Alliance for Water Efficiency is a non-profit group that promotes the efficient and sustainable use of water. The group's activities include advocacy, research, and training. The group is supported by the U.S. Environmental Protection Agency.\n\nThe Alliance's network of members includes over 380 stakeholders (including American Rivers, Hunter Industries, Kohler Company, Metropolitan Water District of Southern California, Rain Bird, Southern Nevada Water Authority, and Toto USA) working to improve water efficiency and conservation nationwide.\n\n\n\n"}
{"id": "6982885", "url": "https://en.wikipedia.org/wiki?curid=6982885", "title": "Baywind Energy Co-operative", "text": "Baywind Energy Co-operative\n\nBaywind Energy Co-operative was the first co-operative to own wind turbines in the United Kingdom.\n\nBaywind was modelled on the similar wind turbine cooperatives and other renewable energy co-operatives that are common in Scandinavia, and was founded as an Industrial and Provident Society in 1996. It now has 1,200 members, each with one vote. A proportion of the profits is invested in local community environmental initiatives through the Baywind Energy Community Trust. The organisation won the 2004 National Social Enterprise Award for its innovative approach to community owned renewable energy.\n\nBaywind owned a 2.5 megawatt five-turbine wind farm at Harlock Hill near Ulverston, Cumbria (operational since 29 January 1997), and one of the 600 kilowatt turbines at the Haverigg II wind farm near Millom, Cumbria. The Harlock Hill site has now been re-powered with two 2.3 Mwh Enercon turbines, operational from May 2016. The site is now owned by High Winds, a BenCom created under the new legal rules.\n\nSince Baywind's establishment, further wind energy co-operatives have been established, many with the help of the organisation formed by Baywind for this purpose, Energy4All.\n\n\n"}
{"id": "31108661", "url": "https://en.wikipedia.org/wiki?curid=31108661", "title": "Body adiposity index", "text": "Body adiposity index\n\nThe body adiposity index (BAI) is a method of estimating the amount of body fat in humans. The BAI is calculated without using body weight, unlike the body mass index (BMI). Instead, it uses the size of the hips compared to the person's height.\n\nBased on population studies, the BAI is approximately equal to the percentage of body fat for adult men and women of differing ethnicities.\n\nThe BAI is calculated as:\n\nformula_1\n\nHip circumference (R = 0.602) and height (R = −0.524) are strongly correlated with percentage of body fat. Comparing BAI with \"gold standard\" dual-energy X-ray absorptiometry (DXA) results, the correlation between DXA-derived percentage of adiposity and the BAI in a target population was R = 0.85, with a concordance of C_b = 0.95.\n\nThe BAI could be a good tool to measure adiposity due, at least in part, to the advantages over other more complex mechanical or electrical systems. Probably, the most important advantage of BAI over BMI is that weight is not needed. However, in general it seems that the BAI does not overcome the limitations of BMI.\n\nStated advantages of the BAI are that it approximates body fat percentage, while the widely used BMI is known to be of limited accuracy, and is different for males and females with similar percentage body adiposity; and that it does not involve weighing, so can be used in remote locations with limited access to scales. A detailed study published in 2012 concluded that estimates of body fat percentage based on BAI were not more accurate than those based on BMI, waist circumference, or hip circumference.\n\nAdiposity indexes that include the waist circumference (for example waist-to-height ratio WHtR) may be better than BAI and BMI in evaluating metabolic and cardiovascular risk in both clinical practice and research.\n\n"}
{"id": "44603", "url": "https://en.wikipedia.org/wiki?curid=44603", "title": "Calcite", "text": "Calcite\n\nCalcite is a carbonate mineral and the most stable polymorph of calcium carbonate (CaCO). The Mohs scale of mineral hardness, based on scratch hardness comparison, defines value 3 as \"calcite\".\n\nOther polymorphs of calcium carbonate are the minerals aragonite and vaterite. Aragonite will change to calcite over timescales of days or less at temperatures exceeding 300°C, and vaterite is even less stable.\n\nCalcite is derived from the German \"Calcit\", a term coined in the 19th century from the Latin word for lime, \"calx\" (genitive calcis) with the suffix -ite used to name minerals. It is thus etymologically related to chalk.\n\nWhen applied by archaeologists and stone trade professionals, the term alabaster is used not just as in geology and mineralogy, where it is reserved for a variety of gypsum; but also for a similar-looking, translucent variety of fine-grained banded deposit of calcite.\n\nIn publications, two different sets of Miller indices are used to describe directions in calcite crystals - the hexagonal system with three indices and the rhombohedral system with four indices . To add to the complications, there are also two definitions of unit cell for calcite. One, an older \"morphological\" unit cell, was inferred by measuring angles between faces of crystals and looking for the smallest numbers that fit. Later, a \"structural\" unit cell was determined using X-ray crystallography. The morphological unit cell has approximate dimensions and , while for the structural unit cell they are and . For the same orientation, must be multiplied by 4 to convert from morphological to structural units. As an example, the cleavage is given as \"perfect on {1 0 1}\" in morphological coordinates and \"perfect on {1 0 4}\" in structural units. (In hexagonal indices, these are {1 0 1} and {1 0 4}.) Twinning, cleavage and crystal forms are always given in morphological units.\n\nOver 800 forms of calcite crystals have been identified. Most common are scalenohedra, with faces in the hexagonal {2 1 1} directions (morphological unit cell) or {2 1 4} directions (structural unit cell); and rhombohedral, with faces in the {1 0 1} or {1 0 4} directions (the most common cleavage plane). Habits include acute to obtuse rhombohedra, tabular forms, prisms, or various scalenohedra. Calcite exhibits several twinning types adding to the variety of observed forms. It may occur as fibrous, granular, lamellar, or compact. A fibrous, efflorescent form is known as lublinite. Cleavage is usually in three directions parallel to the rhombohedron form. Its fracture is conchoidal, but difficult to obtain.\n\nScalenohedral faces are chiral and come in pairs with mirror-image symmetry; their growth can be influenced by interaction with chiral biomolecules such as L- and D-amino acids. Rhombohedral faces are achiral.\n\nIt has a defining Mohs hardness of 3, a specific gravity of 2.71, and its luster is vitreous in crystallized varieties. Color is white or none, though shades of gray, red, orange, yellow, green, blue, violet, brown, or even black can occur when the mineral is charged with impurities.\n\nCalcite is transparent to opaque and may occasionally show phosphorescence or fluorescence. A transparent variety called \"Iceland spar\" is used for optical purposes. Acute scalenohedral crystals are sometimes referred to as \"dogtooth spar\" while the rhombohedral form is sometimes referred to as \"nailhead spar\".\n\nSingle calcite crystals display an optical property called birefringence (double refraction). This strong birefringence causes objects viewed through a clear piece of calcite to appear doubled. The birefringent effect (using calcite) was first described by the Danish scientist Rasmus Bartholin in 1669. At a wavelength of ~590 nm calcite has ordinary and extraordinary refractive indices of 1.658 and 1.486, respectively. Between 190 and 1700 nm, the ordinary refractive index varies roughly between 1.9 and 1.5, while the extraordinary refractive index varies between 1.6 and 1.4.\n\nCalcite, like most carbonates, will dissolve with most forms of acid. Calcite can be either dissolved by groundwater or precipitated by groundwater, depending on several factors including the water temperature, pH, and dissolved ion concentrations. Although calcite is fairly insoluble in cold water, acidity can cause dissolution of calcite and release of carbon dioxide gas. Ambient carbon dioxide, due to its acidity, has a slight solubilizing effect on calcite. Calcite exhibits an unusual characteristic called retrograde solubility in which it becomes less soluble in water as the temperature increases. When conditions are right for precipitation, calcite forms mineral coatings that cement the existing rock grains together or it can fill fractures. When conditions are right for dissolution, the removal of calcite can dramatically increase the porosity and permeability of the rock, and if it continues for a long period of time may result in the formation of caves. On a landscape scale, continued dissolution of calcium carbonate-rich rocks can lead to the expansion and eventual collapse of cave systems, resulting in various forms of karst topography.\n\nAncient Egyptians carved many items out of calcite, relating it to their goddess Bast, whose name contributed to the term alabaster because of the close association. Many other cultures have used the material for similar carved objects and applications.\nHigh-grade optical calcite was used in World War II for gun sights, specifically in bomb sights and anti-aircraft weaponry. Also, experiments have been conducted to use calcite for a cloak of invisibility. \n\nMicrobiologically precipitated calcite has a wide range of applications, such as soil remediation, soil stabilization and concrete repair.\n\nCalcite, obtained from an 80 kg sample of Carrara marble, is used as the IAEA-603 isotopic standard in mass spectrometry for the calibration of δO and δC.\n\nCalcite is a common constituent of sedimentary rocks, limestone in particular, much of which is formed from the shells of dead marine organisms. Approximately 10% of sedimentary rock is limestone. It is the primary mineral in metamorphic marble. It also occurs in deposits from hot springs as a vein mineral; in caverns as stalactites and stalagmites; and in volcanic or mantle-derived rocks such as carbonatites, kimberlites, or rarely in peridotites.\n\nCalcite is often the primary constituent of the shells of marine organisms, e.g., plankton (such as coccoliths and planktic foraminifera), the hard parts of red algae, some sponges, brachiopods, echinoderms, some serpulids, most bryozoa, and parts of the shells of some bivalves (such as oysters and rudists). Calcite is found in spectacular form in the Snowy River Cave of New Mexico as mentioned above, where microorganisms are credited with natural formations. Trilobites, which became extinct a quarter billion years ago, had unique compound eyes that used clear calcite crystals to form the lenses.\n\nThe largest documented single crystal of calcite originated from Iceland, measured 7×7×2 m and 6×6×3 m and weighed about 250 tons.\n\nCalcite formation can proceed via several pathways, from the classical terrace ledge kink model to the crystallization of poorly ordered precursor phases (amorphous calcium carbonate, ACC) via an Ostwald ripening process, or via the agglomeration of nanocrystals.\n\nThe crystallization of ACC can occur in two stages: first, the ACC nanoparticles rapidly dehydrate and crystallize to form individual particles of vaterite. Secondly, the vaterite transforms to calcite via a dissolution and reprecipitation mechanism with the reaction rate controlled by the surface area of calcite. The second stage of the reaction is approximately 10 times slower. However, the crystallization of calcite has been observed to be dependent on the starting pH and presence of Mg in solution. A neutral starting pH during mixing promotes the direct transformation of ACC into calcite. Conversely, when ACC forms in a solution that starts with a basic initial pH, the transformation to calcite occurs via metastable vaterite, which forms via a spherulitic growth mechanism. In a second stage this vaterite transforms to calcite via a surface-controlled dissolution and recrystallization mechanism. Mg has a noteworthy effect on both the stability of ACC and its transformation to crystalline CaCO, resulting in the formation of calcite directly from ACC, as this ion destabilizes the structure of vaterite.\n\nCalcite may form in the subsurface in response to activity of microorganisms, such as during sulfate-dependent anaerobic oxidation of methane, where methane is oxidized and sulfate is reduced by a consortium of methane oxidizers and sulfate reducers, leading to precipitation of calcite and pyrite from the produced bicarbonate and sulfide. These processes can be traced by the specific carbon isotope composition of the calcites, which are extremely depleted in the C isotope, by as much as −125 per mil PDB (δC).\n\nCalcite seas existed in Earth history when the primary inorganic precipitate of calcium carbonate in marine waters was low-magnesium calcite (lmc), as opposed to the aragonite and high-magnesium calcite (hmc) precipitated today. Calcite seas alternated with aragonite seas over the Phanerozoic, being most prominent in the Ordovician and Jurassic. Lineages evolved to use whichever morph of calcium carbonate was favourable in the ocean at the time they became mineralised, and retained this mineralogy for the remainder of their evolutionary history. Petrographic evidence for these calcite sea conditions consists of calcitic ooids, lmc cements, hardgrounds, and rapid early seafloor aragonite dissolution. The evolution of marine organisms with calcium carbonate shells may have been affected by the calcite and aragonite sea cycle.\n\nCalcite is one of the minerals that has been shown to catalyze an important biological reaction, the formose reaction, and may have had a role in the origin of life. Interaction of its chiral surfaces (see Form) with aspartic acid molecules results in a slight bias in chirality; this is one possible mechanism for the origin of homochirality in living cells.\n\n\n"}
{"id": "2644038", "url": "https://en.wikipedia.org/wiki?curid=2644038", "title": "Carburetor icing", "text": "Carburetor icing\n\nCarburetor Icing, or carb icing, is an icing condition which can affect any carburetor under certain atmospheric conditions. The problem is most notable in certain realms of aviation.\n\nCarburetor icing is caused by the temperature drop in the carburetor, as an effect of fuel vaporization, and the temperature drop associated with the pressure drop in the venturi. If the temperature drops below freezing, water vapor will freeze onto the throttle valve, and other internal surfaces of the carburetor. The venturi effect can drop the ambient air temperature by 70 absolute degrees Fahrenheit (F), or 38.89 absolute degrees Celsius (C). In other words, air at an outside temperature of 100 degree F, can drop to 30 degrees F in the carburetor. Carburetor icing most often occurs when the outside air temperature is below 70 degrees F (21 degrees C) and the relative humidity is above 80 percent. \n\nUnfortunately, the warm air temperature often causes pilots of aircraft to overlook the possibility of carb icing. The ice will form on the surfaces of the carburetor throat, further restricting it. This may increase the Venturi effect initially, but eventually restricts airflow, perhaps even causing a complete blockage of air to the carburetor. The engine begins to run more rich as ice formation increases. Without intervention (carb heat or leaning) this can only continue until the mixture is outside of the \"chemically correct\" range for combustion. Icing may also cause jamming of the mechanical parts of the carburetor, such as the throttle, typically a butterfly valve.\n\nWhile it applies to all carburetors, carburetor icing is of particular concern in association with piston-powered aircraft, especially small, single-engine, light aircraft. Aircraft powered by carbureted engines are equipped with carburetor heat systems to overcome the icing problem. In cars, carburetor icing can occasionally be a nuisance. The inlet manifold and parts of the carburetor often have warm water from the cooling system or exhaust gas circulating through them to combat this problem. Motorcycles can also suffer from carburetor icing, although some engine designs are more susceptible to it than others. Air-cooled engines may be more prone to icing, due to the absence of warm coolant circulating through the engine.\n"}
{"id": "50461118", "url": "https://en.wikipedia.org/wiki?curid=50461118", "title": "Cold liquor tank", "text": "Cold liquor tank\n\nCold liquor tanks, cold water tanks as they called in the brewery. Cold liquor tank is a buffer vessel and it carries cold water that will be used to cool the bitter wort down to a fermentable temperature field after boiling.\n\nThe first stage is ground water and it running back to HLT to next sparge. Glycol serves as a secondary cooling system and its back to glycol container. Valves on both channels, and a temp exploration right after the heat exchanger. Heats up your water for your next brew without using any extra power, reuses your water you would use from a CLT, and saves you the space of a whole tank.\n\nCold liquor tanks are used for brewing process and CLT’s also called buffer tanks. Buffer tanks contain cold water used for the purpose of cooling the bitter wort temperature down to the fermentable level range after boiling. This process is done by wort cooler.\n\nThe cold storage tank that been a single skin vessel and should be in a room which requires cold temperature. Cold liquid tanks connected with glycol chiller tanks and its impressed inside water.\nFrequency level of the cooling capacity of the beer helps to recommend the appropriate volume and cooling methods. It can contain 1000+ litre and when water can be passed through the heat exchanger and when it run hot liquid tank temperature near about 70 to 80-degree Celsius.It's a typical process and its help continuous brewing without any break and no need waiting for gain actual temperature.\n\n\nThere are several sizes of CLT’s and HLT’s for commercial use purpose but the size of the tanks totally depends on the production and brewing system with water at the quantity and temperature needed. 2.5x is the proper size of CLT’s.SW = Single Wall,J/I = Jacketed and Insulated,\n\nAll tanks remain consistent with stainless steel piping and automated valves the brewer can only recirculate, radiation or pass it to the brewhouse without transmitting the platform. Put the tanks in “auto” mode and they are at temp when they hit the brewery!\n\n"}
{"id": "40800454", "url": "https://en.wikipedia.org/wiki?curid=40800454", "title": "Conical plate centrifuge", "text": "Conical plate centrifuge\n\nA conical plate centrifuge (also known as a disc bowl centrifuge or disc stack separator) is a type of centrifuge that has a series of conical discs which provides a parallel configuration of centrifugation spaces.\n\nThe conical plate centrifuge is used to remove solids (usually impurities) from liquids or to separate two liquid phases from each other by means of an enormously high centrifugal force. The denser solids or liquids which are subjected to these forces move outwards towards the rotating bowl wall while the less dense fluids moves towards the centre. The special plates (known as disc stacks) increase the surface settling area which speeds up the separation process. Different stack designs, arrangements and shapes are used for different processes depending on the type of feed present. The concentrated denser solid or liquid is then removed continuously, manually or intermittently, depending on the design of the conical plate centrifuge. This centrifuge is very suitable for clarifying liquids that have small proportion of suspended solids.\nThe centrifuge works by using the inclined plate settler principle. A set of parallel plates with a tilt angle θ with respect to horizontal plane is installed to reduce the distance of the particle settling. The reason for the tilted angle is to allow the settled solids on the plates to slide down by gravitational force so they do not accumulate and clog the channel formed between adjacent plates.\n\nThis type of centrifuge removes solid matter from liquid feed from the plates. A centrifugal pump creates a pressure to discharge the clear liquid from the centrifuge. The solid is extracted through nozzles continuously, as shown in Figure 2.\nThe quantity of concentrate depends on speed of bowl rotation, number of nozzles per centrifuge, radius of the nozzle’s position and the nozzle diameter. The quality depends on volume of concentrate discharged, and concentration and volume of the liquid fed into the centrifuge. The concentration of the discharge through the nozzle is varied by adjusting the diameter of the nozzle and initial volume of liquid feed.\n\nPre-treatment includes strainers in the feed lines to prevent coarse solid impurities clogging the nozzles. Generally, the diameter of the holes of the pre-treatment filter is about 10% smaller than nozzle diameter.\n\nLighter and heavier liquid exit separately through the top of the centrifuge, while the sludge is discharged intermittently through the nozzles. Another type of self-cleaning centrifuge has a removable chamber bottom. Both methods can be controlled independently or automatically, either time-controlled or depending on quality of discharge through nozzle\n\nDepending on the design, the feed enters through the top or bottom inlet. Once clarified on the conical plates, product is discharged under pressure through the outlet. Separated solids, or sludge, are accumulated in the conical space adjacent to the nozzle. Once full (without exceeding the area of the plates), a piston is set to hydraulically open each nozzle ports, ejecting the sludge. Generally, water is used as the service fluid, acting as a piston to control the nozzle. During sludging, the water is injected to open the nozzle and drained to close it.\n\nThis type of separator is a closed (hermetic) centrifuge; it can be a chamber or conical plate bowl. This centrifuge can accommodate a system with a maximum pressure of 8 bars. The feed and discharge inlet are attached to the rotating bowl. The head of the centrifuge contains a built-in rotary feed and discharge pumps. It is extremely useful for:\n\n\nConical plate centrifuge can be used to remove water, salts and solids to condition fuels for gas turbine. It also removes some heavy phase liquid and fine solids to obtain high purity liquid fuel. On the other hand, the centrifuge is also useful for treating water, an oil and gas by product, by removing oil contaminants before discharging back to the sea, as required by law. Moreover, emulsion of oil and water can be further treated to produce more oil by separating oil, water and impurities using the conical plate centrifuge.\n\nDisc stack centrifuge is also useful in producing biodiesel, as an initiative to alternative energy sources. The centrifuge separates fuel from methanol or water to convert oils from raw materials (such as rape seed) into diesel fuel. Oil additives is used to improve fuel performances, and in order to remove as much contaminants as possible, this type of centrifuge is used to separate excess fine solids such as metal salts and lime.\n\nIn paper coating, the conical plate centrifuge cleans and sorts the ‘kaolin’ (a material which gives paper its glossy look in certain grades of paper) according to its particle size. The separator for this process needs a design that can withstand abrasion caused by kaolin. Additionally, this centrifuge is also used to remove water, impurities and other metal particles from oils and lubricants used for dynamic processes, such as motors, compressors and hydraulics. This treatment is reliable in extending the service life of the equipment.\n\nAs an equipment that is easily sterilised and fully contained, the conical plate centrifuge is an excellent choice for producing vaccines and antibodies in sterile and hygienic conditions.\n\nHermetic cell culture centrifuge is used to harvest cell cultures from mammals. The feed enters the bottom of the liquid—filled centrifuge (ensuring air-free for cell separation) and a hollow spindle prevents instant acceleration of the feed, minimising damage to the sensitive cell membrane. The outlet is air-free to reduce foaming. This centrifuge can also be cleaned and sanitised in place (SIP and CIP systems) without major dismantling to ensure the operations remains hygienic.\n\nIn beer brewing, kieselguhr (an off-white powder known as diatomaceous earth) is used to filter water from the alcoholic beverages. The addition of a conical plate centrifuge will reduce the usage of kieselguhr, be more economical and time-efficient, as well as minimise product loss.\n\nIn olive oil production, high speed separators are extensively used, therefore conical plate centrifuge is highly recommended because it ensures efficient separation with minimal oil heating and oxidation. For cold-pressed lemon oil (etheric oil), it needs a separator specifically designed to handle the delicate nature and value of the oil. A hermetic centrifuge is best for this purpose because it can prevent product contamination and losses.\n\nBoth conical plate centrifuge and tubular bowl centrifuge can be used for liquid/liquid and solid/liquid separation. However, the advantage of conical plate centrifuge over tubular bowl centrifuge is that solid discharge is possible in conical plate but recovery of solids in tubular bowl is difficult and there is limited solid capacity. As the liquid discharge in conical plate centrifuge is under high pressure, this eliminates foaming but foaming is present in tubular bowl centrifuges unless special skimming or centripetal pumps are used. Tubular bowl centrifuges are easier to clean and good sludge dewatering as compared to conical plate centrifuge.\n\nBoth the conical plate centrifuge and chamber bowl centrifuge can be used for liquid/liquid and solid/liquid separation. However, the advantage of conical plate centrifuge over chamber bowl centrifuge is that solid discharge is possible in conical plate. The chamber bowl has a high capacity for solids but there is no solid discharge.\nBowl cooling is possible for both conical plate centrifuge and chamber bowl centrifuge. However, cleaning is easier as well as better sludge dewatering in chamber bowl centrifuge as compared to conical plate centrifuge.\n\nThe following process characteristics are required in a disk stack centrifuge:\n\nA typical angle of 35 to 50° (with respect to vertical axis) of the disk is used in the centrifuge with 50 to 200 numbers of disks. This in turns provide a centrifugal acceleration G in the range between 5000 and 15000 g.\n\nThe efficiency of solid separation can be increased by applying a relatively moderate G-force of 3000 Gs in a centrifuge processing calcium carbonate (with sizes below 8 and 12 micron). This efficiency is reduced at a higher G-force of around 6000 Gs because the high fluid velocities near the wall can flush out the settled coarser particles into the light phase reducing the retention time for particle separation. However this depends on the size of the processed solids.\n\n Typically the spacing between adjacent disk ranges from 0.32 to 1 mm. This depends on the application and feed to be processed as well as feed concentration as shown in the examples below.\n\nAs a rule of thumb, the ratio of the bowl outer diameter to the bowl height must be approximately equal to 1.\n\nThere are two types of feed solids for this centrifuge:\nCommon for biopharmaceutical application, the feed has 2 to 4% v/v (by bulk volume) for mammalian cells. This may increase to 4 to 6% v/v or even higher in the future due to the increase in solids capacity from the upstream processes such as bioreactors. In contrast, the feed has up to 30% v/v bulk volume for yeast.\nThe feed may contain dissolved solids consisting of valuable protein product and other soluble contaminants that requires removal in downstream purification.\n\nThe angle of solid discharge is important as it affects the rate of concentrate being discharged.\nThe steeper the cone angle, the greater the G-force produced to clear the solids off the wall of the cone. Furthermore, a steep cone angle helps in the compaction of the concentrate hence preventing discharge issues, resulting in more concentrate solid being discharged.\n\n For a centrifuge with intermittent discharge mode, determining the discharge frequency is essential to maximize productivity. The time, formula_1, to fill up the disk centrifuge is given by the expression:\n\nformula_2\n\nWhere\nAn initial guess of is implemented otherwise calculated and from calculated formula_1, frequency discharge can be approximated. By monitoring the turbidity of centrate, further fine-tuning can be made.\n\nIn a nozzle disc bowl, centrate (solid output) or effluent liquid can be discharged by centripetal pump or paring pump. This is advantageous in reducing the energy of the discharge stream, allows air contact as well as reduces foaming especially when liquid has dissolved protein.\n\n In process design, many of the important decisions are made based on experience and heuristics. There are many factors affecting process design. Furthermore, processes would be modified in accordance to production, market and environmental demands. A heuristic method available to help design separation process (conical plate centrifuge) is called Douglas (1988) methodology.\n\nDouglas methodology uses a three level hierarchical systematic procedure where heuristics can be applied. The three level process designs are:\nDouglas’ method is suitable for modelling a conical plate centrifugal process as it breaks down a complicated design problem into simple piece.\n\nModification of a specific design in accordance to the heuristics (rule of thumb) for a particular process is one of the effective methods to be applied. Possible heuristics that can be used when designing certain parts of the conical plate centrifuge are:\n\nEquipment\n\nNozzles\n\nFeed\n\n\nProduct\n\nMaterial of construction\n\nConical plate centrifuge produces waste sludge which needs to be treated before it can be disposed. The treatment for sludge is thickening, dewatering, digestion, drying and destruction, which can act as a post treatment to the conical plate centrifugation process. Further elaboration on these post treatment are:\n\nSludge thickening by gravitational settling and dynamic is used to minimize the volume of sludge. Feed with a content of 0.8% solids, can be thickened to a content of 4% solids, which means a fivefold decrease in sludge volume is obtained. This in return helps optimize the following steps by reducing the size of structure and operating costs.\n\nSludge thickening using centrifugal force is amongst the most common process used. There are two types of centrifuge designed for sludge thickening and both methods depends on the same solid-liquid separation principal. The first method is \"solid bowl centrifuge\", a horizontally-arranged helical screw that admits sludge, removes solid in a countercurrent fashion while allowing liquids to pass through. \nThe moving shaft consists of a set of helical scrolls which push the solid waste against the flow of incoming sludge. As solid content starts to build up, it will become too heavy and then drops down to a collection bin. A video illustrates this process.\n\nCentrifuge design depends mostly on solid throughput criteria and solids flux.\n\nSludge dewatering can be achieved by electro-osmosis or centrifugation process. Prior to dewatering, the thickened sludge is first conditioned. This is to increase the particle size and break the cohesion between the matter and the water, for better dewatering process.\n\nThe objective of digestion is to reduce sludge quantity, increase sludge dryness and stabilization of sludge. Furthermore, it provides valuation in green energy by producing biogas. The condition for digestion depends on the quantity and nature of sludge.\n\nSludge drying is necessary to remove remaining water available due to mechanical limitation during sludge dewatering. The thermal drying process is affected by the specific behaviour (depends on the dryness to be reached) of the sludge.\n\nIn sludge destruction, all the organic matter present in the sludge is destroyed and during destruction, vapour and electricity is generated.\n\nHigh separation efficiency of a centrifuge is the result of the combination of centrifugal force and built in pack plate usually a conical disc plate. Therefore, new development usually emphasize on these two areas.\n\nA new semi-hermetic centrifuge was designed so that the feed pressure entering the system can be as low as possible by keeping the outlets open, which reduces the pressure drop across the separator. The stationary paring disc installed at the outlet also allows the process to be operated at low pressure. Another advantage of this design is it produces low noise level due to the rubber-damped assembly, jacketed frame and an outer bowl.\n\nAnother recent development to the conical plate centrifuge is in sludge dewatering. With a wide range of design variants available to change particular designs to fit different applications, baffles have been fitted at the solid discharge end of the decanter. The bowl becomes almost full of solids rather than having a shallow layer at the bowl periphery and the solid discharge is drier.\n\nSociety of Petroleum Engineer (SPE) has designed a new plate pack with vertical plates made of 360 stainless steel plates held by a plate support sleeve. It is arranged in parallel to the centrifuge axis. An increase in the centrifugal efficiency is obtained based on a computer simulation program ran beforehand. This is proven experimentally in which oil concentration in the effluent has decreased by about 25%. Further improvement of the plate pack may lead to a decrease of oil effluent concentration of more than 40% and seems feasible.\n"}
{"id": "1814410", "url": "https://en.wikipedia.org/wiki?curid=1814410", "title": "Coulomb collision", "text": "Coulomb collision\n\nA Coulomb collision is a binary elastic collision between two charged particles interacting through their own electric field. As with any inverse-square law, the resulting trajectories of the colliding particles is a hyperbolic Keplerian orbit. This type of collision is common in plasmas where the typical kinetic energy of the particles is too large to produce a significant deviation from the initial trajectories of the colliding particles, and the cumulative effect of many collisions is considered instead.\n\nIn a plasma a Coulomb collision rarely results in a large deflection. The cumulative effect of the many small angle collisions, however, is often larger than the effect of the few large angle collisions that occur, so it is instructive to consider the collision dynamics in the limit of small deflections.\n\nWe can consider an electron of charge formula_1 and mass formula_2 passing a stationary ion of charge formula_3 and much larger mass at a distance formula_4 with a speed formula_5. The perpendicular force is formula_6 at the closest approach and the duration of the encounter is about formula_7. The product of these expressions divided by the mass is the change in perpendicular velocity:\n\nNote that the deflection angle is proportional to formula_9. Fast particles are \"slippery\" and thus dominate many transport processes. The efficiency of velocity-matched interactions is also the reason that fusion products tend to heat the electrons rather than (as would be desirable) the ions. If an electric field is present, the faster electrons feel less drag and become even faster in a \"run-away\" process.\n\nIn passing through a field of ions with density formula_10, an electron will have many such encounters simultaneously, with various impact parameters (distance to the ion) and directions. The cumulative effect can be described as a diffusion of the perpendicular momentum. The corresponding diffusion constant is found by integrating the squares of the individual changes in momentum. The rate of collisions with impact parameter between formula_4 and formula_12 is formula_13, so the diffusion constant is given by\n\nThe integral of formula_15 thus yields the logarithm of the ratio of the upper and lower cut-offs. This number is known as the Coulomb logarithm and is designated by either formula_16 or formula_17. It is the factor by which small-angle collisions are more effective than large-angle collisions. For many plasmas of interest it takes on values between formula_18 and formula_19. (For convenient formulas, see pages 34 and 35 of the \"NRL Plasma formulary\".) The limits of the impact parameter integral are not sharp, but are uncertain by factors on the order of unity, leading to theoretical uncertainties on the order of formula_20. For this reason it is often justified to simply take the convenient choice formula_21. The analysis here yields the scalings and orders of magnitude.\n\n\n"}
{"id": "2507965", "url": "https://en.wikipedia.org/wiki?curid=2507965", "title": "Crossed-field amplifier", "text": "Crossed-field amplifier\n\nA crossed-field amplifier (CFA) is a specialized vacuum tube, first introduced in the mid-1950s and frequently used as a microwave amplifier in very-high-power transmitters.\n\nRaytheon engineer William C. Brown's work to adapt magnetron principles to create a new broadband amplifier is generally recognized as the first CFA, which he called an Amplitron. Other names that are sometimes used by CFA manufacturers include Platinotron or Stabilotron.\n\nA CFA has lower gain and bandwidth than other microwave amplifier tubes (such as klystrons or traveling-wave tubes); but it is more efficient and capable of much higher output power.\n\nPeak output powers of many megawatts and average power levels of tens of kilowatts can be achieved, with efficiency ratings in excess of 70 percent. Their current use is in ground stations for TVRO broadcasting and Deep Space telecommunications networks.\n\nThe electric and magnetic fields in a CFA are perpendicular to each other (\"crossed fields\"). This is the same type of field interaction used in a magnetron; as a result, the two devices share many characteristics (such as high peak power and efficiency) and they have similar physical appearances. However, a magnetron is an oscillator and a CFA is an amplifier (although a CFA can be driven to oscillate by application of improper low voltages as can any amplifier); a CFA's RF circuit (or slow-wave structure) is similar to that in a coupled-cavity TWT.\n\nThe CFA has the useful property that when power is shut off, the input simply passes to the output with very little loss. This avoids the need for RF bypass switching in the event of failure.\n\nTwo CFAs can be connected sequentially with only one powered; if it fails, power can be removed from the primary tube and applied to the secondary as a backup. This approach with built-in redundancy was used on the S-band downlink transmitter on the Apollo Lunar Module where high efficiency and reliability were needed.\nA large negative voltage is placed on the green electrode in the center, and a large magnetic field is directed perpendicular to the page. This forms a thin spinning disk of electrons with a flow pattern like spinning water as it drains from a sink or toilet. A slow-wave structure is located above and below the spinning disk of electrons. Electrons flow much slower than the speed of light, and the slow wave structure reduces the velocity of the input RF enough to match the electron velocity.\n\nThe RF input is introduced into the slow wave structure. The alternating microwave field causes the electrons to alternately speed up and slow down. These disturbances grow larger as electrons spiral around the device, and electrons slow down as the RF energy grows. This produces amplification.\n\nThere is a small amount of RF feedback from output to input. This creates a slight random phase jitter when the device is pulsed.\n"}
{"id": "31223242", "url": "https://en.wikipedia.org/wiki?curid=31223242", "title": "DarkSide", "text": "DarkSide\n\nThe DarkSide collaboration is an international affiliation of universities and labs seeking to directly detect dark matter in the form of weakly interacting massive particles (WIMPs). The collaboration is building a series of noble liquid time projection chambers (TPCs) that are designed to be employed at the Gran Sasso National Laboratory in Assergi, Italy. The detector is filled with liquid argon from underground sources in order to exclude the radioactive isotope , which is makes up one in every 10 (quadrillion) atoms in atmospheric argon. The Darkside-10 prototype was tested in 2012, and the Darkside-50 experiment has been operating since 2013.\n\nDarkside-50 is operating with 46 kg of argon target mass. A 3-year run is planned and ton-scale expansion has been proposed.\n\nInitial results using a month of running were reported in 2014. Spin-independent limits were set using 1422 kg×days of exposure to atmospheric argon. A cross section limit of for a 100 Gev WIMP was found.\n\nThe following institutions' physics departments include members of DarkSide:\n\n\n"}
{"id": "6211280", "url": "https://en.wikipedia.org/wiki?curid=6211280", "title": "Departure function", "text": "Departure function\n\nIn thermodynamics, a departure function is defined for any thermodynamic property as the difference between the property as computed for an ideal gas and the property of the species as it exists in the real world, for a specified temperature \"T\" and pressure \"P\". Common departure functions include those for enthalpy, entropy, and internal energy.\n\nDeparture functions are used to calculate real fluid extensive properties (i.e. properties which are computed as a difference between two states). A departure function gives the difference between the real state, at a finite volume or non-zero pressure and temperature, and the ideal state, usually at zero pressure or infinite volume and temperature.\n\nFor example, to evaluate enthalpy change between two points h(v,T) and h(v,T) we first compute the enthalpy departure function between the v and infinite volume at T=T, then add to that the ideal gas enthalpy change due to the temperature change from T to T, then subtract the departure function value between v and infinite volume.\n\nDeparture functions are computed by integrating a function which depends on an equation of state and its derivative.\n\nGeneral Expressions for the Enthalpy \"H\", the Entropy \"S\" and the Gibbs Energy \"G\" are given by\n\nThe Peng-Robinson equation of state relates the three interdependent state properties pressure \"P\", temperature \"T\", and molar volume \"V\". From the state properties (\"P\", \"V\", \"T\"), one may compute the departure function for enthalpy per mole (denoted \"h\") and entropy per mole (\"s\"):\n\nWhere formula_6 is defined in the Peng-Robinson equation of state, \"T\" is the reduced temperature, \"P\" is the reduced pressure, \"Z\" is the compressibility factor, and\n\nTypically, one knows two of the three state properties (\"P\", \"V\", \"T\"), and must compute the third directly from the equation of state under consideration. To calculate the third state property, it is necessary to know three constants for the species at hand: the critical temperature \"T\", critical pressure \"P\", and the acentric factor \"ω\". But once these constants are known, it is possible to evaluate all of the above expressions and hence determine the enthalpy and entropy departures.\n\n\n"}
{"id": "2213787", "url": "https://en.wikipedia.org/wiki?curid=2213787", "title": "Drill bit sizes", "text": "Drill bit sizes\n\nDrill bits are the cutting tools of drilling machines. They can be made in any size to order, but standards organizations have defined sets of sizes that are produced routinely by drill bit manufacturers and stocked by distributors.\n\nIn the U.S., fractional inch and gauge drill bit sizes are in common use. In nearly all other countries, metric drill bit sizes are most common, and all others are anachronisms or are reserved for dealing with designs from the US. The British Standards on replacing gauge size drill bits with metric sizes in the UK was first published in 1959.\n\nA comprehensive table for metric, fractional wire and tapping sizes can be found at the drill and tap size chart.\n\nMetric drill bit sizes define the diameter of the bit in terms of standard metric lengths. Standards organizations define sets of sizes that are conventionally manufactured and stocked. For example, British Standard BS 328 defines sizes from 0.2 mm to 25.0 mm.\n\nFrom 0.2 through 0.98 mm, sizes are defined as follows, where \"N\" is an integer from 2 through 9:\n\nFrom 1.0 through 2.95 mm, sizes are defined as follows, where \"N\" is an integer from 10 through 29:\n\nFrom 3.0 through 13.9 mm, sizes are defined as follows, where \"N\" is an integer from 30 through 139:\n\nFrom 14.0 through 25.0 mm, sizes are defined as follows, where \"M\" is an integer from 14 through 25:\n\nIn smaller sizes, bits are available in smaller diameter increments. This reflects both the smaller drilled hole diameter tolerance possible on smaller holes and the wishes of designers to have drill bit sizes available within at most 10% of an arbitrary hole size.\n\nThe price and availability of particular size bits does not change uniformly across the size range. Bits at size increments of 1 mm are most commonly available, and lowest price. Sets of bits in 1 mm increments might be found on a market stall. In 0.5 mm increments, any hardware store. In 0.1 mm increments, any engineers' store. Sets are not commonly available in smaller size increments, except for drill bits below 1 mm diameter. Drill bits of the less routinely used sizes, such as 2.55 mm, would have to be ordered from a specialist drill bit supplier. This subsetting of standard sizes is in contrast to general practice with number gauge drill bits, where it is rare to find a set on the market which does not contain every gauge.\n\nThere are also Renard series sequences of preferred metric drill bits:\n\nMetric dimensioning is routinely used for drill bits of all types, although the details of BS 328 apply only to twist drill bits. For example, a set of Forstner bits may contain 10, 15, 20, 25 and 30 mm diameter cutters.\n\nFractional inch drill bit sizes are still in common use in the United States and in any factory (around the globe) that makes inch-sized products for the U.S. market. \n\nANSI B94.11M-1979 sets size standards for jobber-length straight-shank twist drill bits from 1/64 inch through 1 inch in 1/64 inch increments. For Morse taper-shank drill bits, the standard continues in 1/64 inch increments up to 1¾ inch, then 1/32 inch increments up to 2¼ inch, 1/16 inch increments up to 3 inches, 1/8 inch increments up to 3¼ inches, and a single 1/4 inch increment to 3½ inches. One aspect of this method of sizing is that the size increment between drill bits becomes larger as bit sizes get smaller: 100% for the step from 1/64th to 1/32nd, but a much smaller percentage between 1 47/64th and 1 3/4.\n\nDrill bit sizes are written down on paper and etched onto bits as irreducible fractions. So, instead of 78/64 inch, or 1 14/64 inch, the size is noted as 1 7/32 inch.\n\nBelow is a chart providing the decimal-fraction equivalents that are most relevant to fractional-inch drill bit sizes (that is, 0 to 1 by 64ths). (Decimal places for .25, .5, and .75 are shown to thousandths [.250, .500, .750], which is how machinists usually think about them [\"two-fifty\", \"five hundred\", \"seven-fifty\"]. Machinists generally truncate the decimals after thousandths; for example, a 27/64\" drill bit may be referred to in shop-floor speech as a \"four-twenty-one drill\".)\n\nCharts like this, printed at poster size, are widely posted on machine shop walls for handy reference by machinists as they work. For some tasks it is faster and less annoying to glance down a column of numbers on a poster than it is to punch calculator buttons or do penciled arithmetic. It is also true that with long experience, talented machinists have much of the chart memorized anyway (with decimals to truncated thou). But the poster is there for reference and quick sanity check as needed. Tool distributors often give away such charts as promotional merchandise. \n\nFor anyone looking to create such a poster or to tailor the design and colors to their own preferences, it is useful to realize that spreadsheet software makes this fast and easy to do. The tools that the software provides—formulas, cell formatting with rounding, autofill, and sorting by numerical order—obviate any manual calculation and data entry. For example, one can create the inch nominal rows for the chart—with columns for inch fraction, inch decimal, and corresponding millimeter decimal conversion—using formulas, cell formatting with rounding, and autofill; and one can then create the metric nominal rows below that, with the inch decimal conversions being provided by formula; and then one can sort the whole sheet by size order, automatically interposing the inch nominal and metric nominal rows by size order. Font and color can be chosen to make a poster-size chart that is easy to read from 1 or 2 meters away. Printing can be done on a desktop printer, with the letter-size sheets then being stapled together to make up the larger poster. \n\nNumber drill bit gauge sizes range from size 80 (the smallest) to size 1 (the largest) followed by letter gauge size A (the smallest) to size Z (the largest). Although the ASME B94.11M twist drill standard, for example, lists sizes as small as size 97, sizes smaller than 80 are rarely encountered in practice.\n\nNumber and letter sizes are commonly used for twist drill bits rather than other drill forms, as the range encompass the sizes for which twist drill bits are most often used.\n\nThe gauge-to-diameter ratio is not defined by a formula, but is instead based on, but is not identical to, the Stubs Steel Wire Gauge, which originated in Britain during the 19th century. The accompanying graph, with each step along the horizontal axis being one gauge size, illustrate the change in diameter with change in gauge, as well as the reduction in step size as the gauge size decreases.\n\nNumber and letter gauge drill bits are still in common use in the U.S. and to a lesser extent the UK, where they have largely been superseded by metric sizes. Other countries that formerly used the number series have for the most part also abandoned these in favour of metric sizes.\n\nThe shortest standard-length drills (that is, lowest length-to-diameter ratio) are \"screw-machine-length drills\" (sometimes abbreviated \"S/M\"). They are named for their use in screw machines. Their shorter flute length and shorter overall length compared to a standard jobber bit results in a more rigid drill bit, reducing deflection and breakage. They are rarely available in retail hardware stores or home centers.\n\nJobber-length drills are the most commonly found type of drill. The length of the flutes is between 9 and 14 times the diameter of the drill, depending on the drill size. So a diameter drill will be able to drill a hole deep, since it is 9 times the diameter in length. A diameter drill can drill a hole deep, since it is 13 times the diameter in flute length.\n\nThe term \"jobber\" refers to a wholesale distributor—a person or company that buys from manufacturers and sells to retailers. (It was especially widely used in the 1850-1950 period.) Manufacturers producing drill bits \"for the trade\" (as opposed to for specialized machining applications with particular length and design requirements) made ones of medium length suitable for a wide variety of jobs, because that was the type most desirable for general distribution. Thus, at the time that the name of jobber-length drill bits became common, it reflected the same concept that names like \"general-purpose\" and \"multipurpose\" reflect.\n\nExtended-reach or long-series drills are commonly called aircraft-length from their original use in manufacturing riveted aluminum aircraft. For bits thicker than a minimum size such as 1/8 inch, they are available in fixed lengths such as 6, 8, 12, or 18 inches rather than the progressive lengths of jobber drills. \n\nThe image shows a long-series drill compared to its diametric equivalents, all are in diameter. The equivalent Morse taper drill shown in the middle is of the usual length for a taper-shank drill. The lower drill bit is the \"jobber\" or \"parallel shank\" equivalent.\n\nCenter drills are available with two different included angles; 60 degrees is the standard for drilling centre holes (for example for subsequent centre support in the lathe), but 90 degrees is also common and used when locating holes prior to drilling with twist drills. Center drills are made specifically for drilling lathe centers, but are often used as spotting drills because of their radial stiffness.\n\nSpotting drills are available in a relatively small range of sizes, both metric and imperial, as their purpose is to provide a precise spot for guiding a standard twist drill. Commonly available sizes are 1/8\", 1/4\", 3/8\", 1/2\", 5/8\", 3/4\", 4 mm, 6 mm, 8 mm, 10 mm, 12 mm, 14 mm, 16 mm and 18 mm. The drills are most ordinarily available with either 90° or 120° included angle points.\n\n"}
{"id": "1146172", "url": "https://en.wikipedia.org/wiki?curid=1146172", "title": "Dutch roll", "text": "Dutch roll\n\nDutch roll is a type of aircraft motion, consisting of an out-of-phase combination of \"tail-wagging\" and rocking from side to side. This yaw-roll coupling is one of the basic flight dynamic modes (others include phugoid, short period, and spiral divergence). This motion is normally well damped in most light aircraft, though some aircraft with well-damped Dutch roll modes can experience a degradation in damping as airspeed decreases and altitude increases. Dutch roll stability can be artificially increased by the installation of a yaw damper. Wings placed well above the center of gravity, sweepback (swept wings) and dihedral wings tend to increase the roll restoring force, and therefore increase the Dutch roll tendencies; this is why high-winged aircraft often are slightly anhedral, and transport-category swept-wing aircraft are equipped with yaw dampers.\n\nIn aircraft design, Dutch roll results from relatively weaker positive directional stability as opposed to positive lateral stability. When an aircraft rolls around the longitudinal axis, a sideslip is introduced into the relative wind in the direction of the rolling motion. Strong lateral stability begins to restore the aircraft to level flight. At the same time, somewhat weaker directional stability attempts to correct the sideslip by aligning the aircraft with the perceived relative wind. Since directional stability is weaker than lateral stability for the particular aircraft, the restoring yaw motion lags significantly behind the restoring roll motion. The aircraft passes through level flight as the yawing motion is continuing in the direction of the original roll. At that point, the sideslip is introduced in the opposite direction and the process is reversed.\n\nThe most common mechanism of Dutch roll occurrence is a moment of yawing motion which can be caused by any number of factors. As a swept-wing aircraft yaws (to the right, for instance), the left wing becomes less-swept than the right wing in reference to the relative wind. Because of this, the left wing develops more lift than the right wing causing the aircraft to roll to the right. This motion continues until the yaw angle of the aircraft reaches the point where the vertical stabilizer effectively becomes a wind vane and reverses the yawing motion. As the aircraft yaws back to the left, the right wing then becomes less swept than the left resulting in the right wing developing more lift than the left. The aircraft then rolls to the left as the yaw angle again reaches the point where the aircraft wind-vanes back the other direction and the whole process repeats itself. The average duration of a Dutch roll half-cycle is 2 to 3 seconds.\n\nThe Dutch roll mode can be excited by any use of aileron or rudder, but for flight test purposes it is usually excited with a rudder singlet (short, sharp motions of the rudder to a specified angle, and then back to the centered position) or doublet (a pair of such motions in opposite directions). Some larger aircraft are better excited with aileron inputs. Periods can range from a few seconds for light aircraft to a minute or more for airliners.\n\nDutch roll is also the name (considered by professionals to be a misnomer) given to a coordination maneuver generally taught to student pilots to help them improve their \"stick-and-rudder\" technique. The aircraft is alternately rolled as much as 60 degrees left and right while rudder is applied to keep the nose of the aircraft pointed at a fixed point. More correctly, this is a rudder coordination practice exercise, to teach a student pilot how to correct for the effect known as adverse aileron yaw during roll inputs.\n\nThis coordination technique is better referred to as \"rolling on a heading\", wherein the aircraft is rolled in such a way as to maintain an accurate heading without the nose moving from side-to-side (or yawing). The yaw motion is induced through the use of ailerons alone due to aileron drag, wherein the lifting wing (aileron down) is doing more work than the descending wing (aileron up) and therefore creates more drag, forcing the lifting wing back, yawing the aircraft toward it. This yawing effect produced by rolling motion is known as adverse yaw. This has to be countered precisely by application of rudder \"in the same direction\" as the aileron control (left stick, left rudder - right stick, right rudder). This is known as synchronised controls when done properly, and is difficult to learn and apply well. The correct amount of rudder to apply with aileron is different for each aircraft.\n\nThe origin of the name Dutch roll is uncertain. However, it is likely that this term, describing a lateral asymmetric motion of an airplane, was borrowed from a reference to similar-appearing motion in ice skating. In 1916, aeronautical engineer Jerome Clarke Hunsaker published the following quote:\n\"Dutch roll – the third element in the [lateral] motion [of an airplane] is a yawing to the right and left, combined with rolling. The motion is oscillatory of period for 7 to 12 seconds, which may or may not be damped. The analogy to 'Dutch Roll' or 'Outer Edge' in ice skating is obvious.\"\nIn 1916, Dutch Roll was the term used for skating repetitively to right and left (by analogy to the motion described for the aircraft) on the outer edge of one's skates. By 1916, the term had been imported from skating to aeronautical engineering, perhaps by Hunsaker himself. 1916 was only five years after G. H. Bryan did the first mathematical analysis of lateral motion of aircraft in 1911.\n\n\n\n"}
{"id": "711765", "url": "https://en.wikipedia.org/wiki?curid=711765", "title": "EconSimp", "text": "EconSimp\n\nEconSimp is a bioeconomic management model of the Barents Sea fisheries. It consists of two modules modelling respectively the Barents Sea ecosystem and the Norwegian fleet structure and activity. The ecosystem module is the multispecies model Aggmult developed at the Norwegian Institute of Marine Research and the fleet module is EconMult.\n"}
{"id": "7324758", "url": "https://en.wikipedia.org/wiki?curid=7324758", "title": "Essent", "text": "Essent\n\nEssent N.V., based in 's-Hertogenbosch, Netherlands, is an energy company. It is a public limited liability corporation. Essent is the largest energy company in the Netherlands. Belgium is their second home market. Essent provides customers with gas, electricity, heat and energy services. Essent (including its predecessors) has over 90 years experience of generating, trading, transmitting and supplying electricity. Essent is part of Innogy SE.\n\nEssent has 2.3 million customers for electricity and about 2.0 million for gas.\n\nEssent was formed in 1999 by the merger of PNEM/MEGA Groep and EDON Groep, energy groups based in the south and north of the Netherlands respectively.\n\nUntil 1 October 2009, the shares of Essent were owned by six Dutch provinces of Overijssel (18%), Groningen (6%), Noord-Brabant (30.8%), Drenthe (2.3%), Flevoland (0.02%) and Limburg (16%) and by more than 100 municipalities in these provinces and in the province of Friesland (the remaining 26%). \n\nIn January 2009, German energy major, RWE AG, announced its intention to buy out all outstanding and issued shares of Essent. The deal, valued to be around 9.3 billion euros, would make Essent an operating company of RWE in the Netherlands and Belgium. This deal was objected by socialist legislators, who were not in favor of privatizing a prominent public utility. On 30 September 2009, the deal was closed. Since the split of RWE and Innogy as of 1 April 2016, the power generation business was transferred to RWE Generation NL B.V., while Essent as retail and renewable energy company remained in the Innogy group.\n\nIn 2010 Essent opened the first commercial fast-charge station for electric vehicles in Europe. The charger produced by Epyon charges electric 9-person taxi-vans in 30 minutes.\n\n"}
{"id": "55785489", "url": "https://en.wikipedia.org/wiki?curid=55785489", "title": "Glass melting furnace", "text": "Glass melting furnace\n\nA glass melting furnace is a furnace which is designed to melt raw materials into glass. There are several types of furnace which output a different amount of heat and use different power sources. A glass melting furnace is made from a refractory material.\n"}
{"id": "1277251", "url": "https://en.wikipedia.org/wiki?curid=1277251", "title": "HVDC converter station", "text": "HVDC converter station\n\nAn HVDC converter station (or simply converter station) is a specialised type of substation which forms the terminal equipment for a high-voltage direct current (HVDC) transmission line. It converts direct current to alternating current or the reverse. In addition to the converter, the station usually contains:\n\nThe converter is usually installed in a building called the valve hall. Early HVDC systems used mercury-arc valves, but since the mid-1970s, solid state devices such as thyristors have been used. Converters using thyristors or mercury-arc valves are known as \"line commutated converters\". In thyristor-based converters, many thyristors are connected in series to form a thyristor valve, and each converter normally consists of six or twelve thyristor valves. The thyristor valves are usually grouped in pairs or groups of four and can stand on insulators on the floor or hang from insulators from the ceiling.\n\nLine commutated converters require voltage from the AC network for commutation, but since the late 1990s, voltage sourced converters have started to be used for HVDC. Voltage sourced converters use insulated-gate bipolar transistors instead of thyristors, and these can provide power to a deenergized AC system.\n\nAlmost all converters used for HVDC are intrinsically able to operate with power conversion in either direction. Power conversion from AC to DC is called \"rectification\" and conversion from DC to AC is called \"inversion\".\n\nThe direct current equipment often includes a coil (called a \"reactor\") that adds inductance in series with the DC line to help smooth the direct current. The inductance typically amounts to between 0.1 H and 1 H. The smoothing reactor can have either an air-core or an iron-core. Iron-core coils look like oil-filled high voltage transformers. Air-core smoothing coils resemble, but are considerably larger than, carrier frequency choke coils in high voltage transmission lines and are supported by insulators. Air coils have the advantage of generating less acoustical noise than iron-core coils, they eliminate the potential environmental hazard of spilled oil, and they do not saturate under transient high current fault conditions. This part of the plant will also contain instruments for measurement of direct current and voltage. \n\nSpecial direct current filters are used to eliminate high frequency interference. Such filters are required if the transmission line will use power-line communication techniques for communication and control, or if the overhead line will run through populated areas. These filters can be passive LC filters or active filters, consisting of an amplifier coupled through transformers and protection capacitors, which gives a signal out of phase to the interference signal on the line, thereby cancelling it. Such a system was used on the Baltic Cable HVDC project.\n\nThe converter transformers step up the voltage of the AC supply network. Using a star-to-delta or \"wye-delta\" connection of the transformer windings, the converter can operate with 12 pulses for each cycle in the AC supply, which eliminates numerous harmonic current components. The insulation of the transformer windings must be specially designed to withstand a large DC potential to earth. Converter transformers can be built as large as 300 megavolt-amperes (MW) as a single unit. It is impractical to transport larger transformers, so when larger ratings are required, several individual transformers are connected together. Either two three-phase units or three single-phase units can be used. With the latter variant only one type of transformer is used, making the supply of a spare transformer more economical. \n\nConverter transformers operate with high flux Power Steps In the Four Steps of the Converter per cycle, and so produce more acoustic noise than normal three-phase power transformers. This effect should be considered in the siting of an HVDC converter station. Noise-reducing enclosures may be applied.\n\nWhen line commutated converters are used, the converter station will require between 40% and 60% of its power rating as reactive power. This can be provided by banks of switched capacitors or by synchronous condensers, or if a suitable power generating station is located close to the static inverter plant, the generators in the power station. The demand for reactive power can be reduced if the converter transformers have on-load tap changers with a sufficient range of taps for AC voltage control. Some of the reactive power requirement can be supplied in the harmonic filter components.\n\nVoltage sourced converters can generate or absorb reactive as well as real power, and additional reactive power equipment is generally not needed.\n\nHarmonic filters are necessary for the elimination of the harmonic waves and for the production of the reactive power at line commutated converter stations. At plants with six pulse line commutated converters, complex harmonic filters are necessary because there are odd numbered harmonics of the orders formula_1 and formula_2 produced on the AC side and even harmonics of order formula_3 on the DC side. At 12 pulse converter stations, only harmonic voltages or currents of the order formula_4 and formula_5 (on the AC side) or formula_6 (on the DC side) result. Filters are tuned to the expected harmonic frequencies and consist of series combinations of capacitors and inductors. \n\nVoltage sourced converters generally produce lower intensity harmonics than line commutated converters. As a result, harmonic filters are generally smaller or may be omitted altogether.\n\nBeside the harmonic filters, equipment is also provided to eliminate spurious signals in the frequency range of power-line carrier equipment in the range of 30 kHz to 500 kHz. These filters are usually near the alternating current terminal of the static inverter transformer. They consist of a coil which passes the load current, with a parallel capacitor to form a resonant circuit.\n\nIn special cases, it may be possible to use exclusively machines for generating the reactive power. This is realized at the terminal of HVDC Volgograd-Donbass situated on Volga Hydroelectric Station.\n\nThe three-phase alternating current switch gear of a converter station is similar to that of an AC substation. It will contain circuit breakers for overcurrent protection of the converter transformers, isolating switches, grounding switches, and instrument transformers for control, measurement and protection. The station will also have lightning arresters for protection of the AC equipment from lightning surges on the AC system.\n\nThe area required for a converter station with a transmission rating of 600 megawatts and a transmission voltage of 400 kV is approximately 300 x 300 metres (1000 x 1000 feet). Lower-voltage plants may require somewhat less ground area, since less air space clearance would be required around outdoor high-voltage equipment.\n\nConverter stations produce acoustic noise and radio-frequency interference. Walls may provide noise protection. As with all AC substations, oil from equipment must be prevented from contaminating ground water in case of a spill. Substantial area may be required for overhead transmission lines, but can be reduced if underground cable is used.\n\n"}
{"id": "1123622", "url": "https://en.wikipedia.org/wiki?curid=1123622", "title": "Helmet of Constantine", "text": "Helmet of Constantine\n\nThe Helmet of Constantine was a helmet or form of helmet worn by the Roman Emperor Constantine the Great, now lost, which featured in his imperial iconography. According to a story recorded by Ambrose and others, it included relics gathered in the Holy Land by his mother, empress Helena. Constantine’s conversion to Christianity, which happened around AD 300, was of great importance. In this period it was believed that touching the body of the deceased or even something that came in contact with the person who had died, was said to have special powers. This belief started a huge obsession with finding these relics to protect churches, cities, and even people.\n\nConstantine I was one of these people who was, as they thought, protected because he had a relic in his possession. The helmet that he wore had a piece of the True Cross that Jesus was crucified on. After Constantine was made Caesar, he issued the Edict of Milan (313), sending his mother Helena to find the True Cross and to send back what she found. Helena found the cross and sent three nails that were used in the crucifixion of Jesus back to Rome and her son Constantine.\n\nThe helmet that Constantine wore included one of the Holy Nails that Saint Helena found at the crucifixion site. By making the helmet with one of the nails, it was supposed to protect him from any harm. Not only was it a sign of protection, but it was also said to fulfill the prophecy of Zechariah. The prophecy located in the fourteenth chapter of the Book of Zechariah said that one would come who engraved “Holiness to the Lord” on both the bells of the horses and head piece of the man. At this time, the Roman emperor had the holiest relics known to man.\n\nA rare silver medallion of 315 shows Constantine with a chi-rho symbol as the crest of his helmet, and Eusebius' \"Life of Constantine\" records that he often wore such a helmet in later life, although it is not visible in other bronze coins. Constantine was very aware of this, and let the entire Roman Empire know that he was the owner of this holy helmet. The emperor had coins made with the image of him wearing his helmet on the front. On every VLLP coin that was made, Constantine was shown wearing his helmet. This was one way that Constantine moved the Roman Empire away from the pagan religion and towards Christianity. The coins that were made after Constantine acquired his helmet showed that the nail was not the only thing that made his helmet religious. Constantine also had the Christogram inscribed on the helmet. There were also a few religious symbols that were placed on the helmet. The Christogram was placed on the helmet, which usually represented the cross, but at times it was interpreted as Jesus Christ himself. Another one of these symbols was a labarum piercing a dragon, a serpent. This was significant because the serpent was represented as the Devil. But it was also the symbol for any enemy that the Roman Empire had. Constantine’s helmet stood for the strength that the Roman Empire had through military force, and God. The helmet was put on the coins to let everyone know of this.\nConstantine’s helmet was mainly a crown that was fastened to the head by two clamps. The majority of the helmet was made out of iron and gold. Constantine thought having the chi-roh symbol on the helmet would give him added protection to that of the relic that was already in the helmet. The crown that was made with the nail was placed around a full helmet when the Emperor went into battle, but could easily be taken off. It was called a helmet however he wore it though. This is because of the two clamps that held it in place. If these two clamps were not on the original crown it would not be called a helmet at all. -->\n\n"}
{"id": "46774048", "url": "https://en.wikipedia.org/wiki?curid=46774048", "title": "Hitaveita Suðurnesja", "text": "Hitaveita Suðurnesja\n\nHitaveita Suðurnesja was an Icelandic energy company. The largest shareholder was Reykjanesbær. The company was founded as a geothermal energy firm in the southwest of Iceland in 1974. It built a power plant at Svartsengi to tap the geothermal energy in the area, and was completed in 1976. It was reportedly the first power plant in the world of its kind. It produced and distributed heating and electricity for the entire Sudurnes region. \n\nIn May 2008, Parliament passed Act no. 58/2008, amending some laws on natural resources and energy. As a result, Hitaveita Suðurnesja was divided into two independent companies, HS Utilities Ltd., distributing electricity, and HS Orka, heating and freshwater, which came into effect on July 1, 2008.\n"}
{"id": "46578380", "url": "https://en.wikipedia.org/wiki?curid=46578380", "title": "Ice and the Sky", "text": "Ice and the Sky\n\nIce and the Sky (, also known as Antarctica: Ice and Sky) is a 2015 French documentary film directed by Luc Jacquet about the work of Claude Lorius, who began studying Antarctic ice in 1957, and, in 1965, was the first scientist to be concerned about global warming. The film was selected to close the 2015 Cannes Film Festival.\n\n"}
{"id": "51666458", "url": "https://en.wikipedia.org/wiki?curid=51666458", "title": "Ikeja Electric", "text": "Ikeja Electric\n\nIkeja Electric Plc is the largest Nigerian power distribution company. It is based in Ikeja, capital of the state of Lagos. The company emerged on November 1, 2013 following the handover of the defunct Power Holding Company of Nigeria (PHCN) to NEDC/KEPCO Consortium under the privatization scheme of the Federal Government of Nigeria.\n\nIkeja Electric has over 700,000 customers. Ikeja Electric's introduced a Debt Discount initiative which provides various percentage discount options to enable customers pay off their outstanding bills and meet their financial obligations to the company. \n"}
{"id": "2785135", "url": "https://en.wikipedia.org/wiki?curid=2785135", "title": "Imbibition", "text": "Imbibition\n\nImbibition is a special type of diffusion when water is absorbed by solids-colloids causing an enormous increase in volume. Examples include the absorption of water by seeds and dry wood. If it were not for the pressure due to imbibition, seedlings would not have been able to emerge out of soil into the open; they probably would not have been able to establish.\n\nImbibition is also diffusion since water surface potential movement is along a concentration gradient; the seeds and other such materials have almost no water hence they absorb water easily. Water potential gradient between the absorbent and the liquid imbibed is essential for imbibition. In addition, for any substance to imbibe any liquid, affinity between the adsorbant and the liquid is also a prerequisite.\n\nImbibition occurs when a wetting fluid displaces a non-wetting fluid, contrary to drainage where a non-wetting phase displaces the wetting fluid. The two processes are governed by different mechanisms.\n\nOne example of imbibition that is found in nature is the absorption of water by hydrophilic colloids. Matrix potential contributes significantly to water in such substances. Examples of plant material which exhibit imbibition are dry seeds before germination. Imbibition can also entrain the genetic clock that controls circadian rhythms in Arabidopsis thaliana and (probably) other plants. Another example is that of imbibition in the Amott test.\n\nDifferent types of organic substances have different imbibing capacities. Proteins have a very high imbibing capacity, then starch less and cellulose least. That is why proteinaceous pea seeds swell more on imbibition than starchy wheat seeds.\n\nImbibition of water increases the volume of the imbibant, which results in imbibitional pressure. This pressure can be of tremendous magnitude. This fact can be demonstrated by the splitting of rocks by inserting dry wooden stalks in the crevices of the rocks and soaking them in water, a technique used by early Egyptians to cleave stone blocks.\n\nSkin grafts (split thickness and full thickness) receive oxygenation and nutrition via imbibition, maintaining cellular viability until the processes of inosculation and revascularisation have re-established a new blood supply within these tissues.\n\n"}
{"id": "29360040", "url": "https://en.wikipedia.org/wiki?curid=29360040", "title": "Karlshamn Power Station", "text": "Karlshamn Power Station\n\nKarlshamn Power Station (also known as Stärnö Power Station, ) is an oil-fired thermal power station on Stärnö peninsula west of Karlshamn. It has three units each with a generation capacity of 340 MW, which went in service in 1969, 1971 and 1973. Each unit has its own tall flue gas stack.\nUnit 1 delivers its power to the 130 kV grid, while unit 2 and 3 feed the 400 kV grid. Next to the power station, a converter station of the SwePol HVDC submarine cable is located.\n\n"}
{"id": "8460569", "url": "https://en.wikipedia.org/wiki?curid=8460569", "title": "List of protected areas of Peru", "text": "List of protected areas of Peru\n\nThis is a list of protected areas in Peru.\n\nThe Constitution of Peru of 1993 recognized the natural resources and ecosystem variety of its country as a heritage. In 1900, the National System of Natural Areas that are protected by the Government (SINANPE) was created. This entity depends on the National Service of Protected Areas by the State (SERNANP), Ministry of Environment.\n\nPeru has 75 natural protected areas (15.21% of the country surface area) that are preserved by the National Government: 12 national parks, 9 national sanctuaries, 4 historical sanctuaries, 15 national reserves, 3 wildlife refuges, 2 landscape reserves, 8 communal reserves, 6 protected forests, 2 hunting enclosed lands and 14 reserved zones. A map was also created containing the natural protected areas.\n\nPeru is considered to be among 17 of the most megadiverse countries in the world. With over 1,700 species of birds, it has the world's second most diverse avian community, after Colombia.\n\nNational Parks are places where the wild flora and fauna are protected and preserved. Natural resources exploitation and human settlements are forbidden.\n\n\n\nNational sanctuaries are areas of national importance for the protection of the habitat of specific species of flora and fauna, and natural formations of scientific or scenic interest.\n\n\n\n\nCommunal reserves are conservation areas for flora and fauna, allowing traditional use for the rural populations surrounding the areas. The use and marketing of the natural resources within the communal reserve is conducted by the same rural populations.\n\n\n\n\n\n\n"}
{"id": "9183407", "url": "https://en.wikipedia.org/wiki?curid=9183407", "title": "Lovecraft Biofuels", "text": "Lovecraft Biofuels\n\nLovecraft Biofuels was a Silverlake, California operation that claimed typical diesel engines could run on vegetable oil using what is known as a \"single tank\" conversion. It does not heat the vegetable oil and the kit compromises filtering quality. As a result, the lifespan of vehicles with a Lovecraft system is short.\" \"\n\nIt was featured on the History Channel's show Modern Marvels: Renewable Energy Sources\n\n"}
{"id": "3240412", "url": "https://en.wikipedia.org/wiki?curid=3240412", "title": "Lumicera", "text": "Lumicera\n\nLumicera is a transparent ceramic developed by Murata Manufacturing Co., Ltd. \n\nMurata Manufacturing first developed transparent polycrystalline ceramics in February 2001. This polycrystalline ceramic is a type of dielectric resonator material commonly used in microwaves and millimeter waves. While offering superior electrical properties, high levels of transmissivity, and refractive index, it also has good optical characteristics without birefringence.\n\nNormally, ceramics are opaque because pores are formed at triple points where grains intersect, causing scattering of incident light. Murata has optimized the entire development process of making dense and homogenous ceramics to improve their performance.\n\nUnder recommendations from Casio, the material itself has been refined for use in digital camera optical lenses by endowing it with improved transmission of short wavelength light and by reducing pores inside ceramics that reduce transparency.\n\nLumicera has the same light transmitting qualities as optical glass commonly used in today's conventional camera lenses, however it has refractive index (nd = 2.08 at 587 nm) much greater than that of optical glass (nd = 1.5 – 1.85 ) and offers superior strength. The Lumicera Z variant is described as barium oxide based material, not containing any environmentally hazardous materials (e.g. lead). \n\nLumicera is transparent up to 10 micrometers, making it useful for instruments operating in mid-infrared spectrum.\n\nLumicera is a trademark of Murata Manufacturing Co., Ltd.\n\nLumicera is used in some Casio Exilim cameras, where it allowed 20% reduction of the lens profile.\n"}
{"id": "20258837", "url": "https://en.wikipedia.org/wiki?curid=20258837", "title": "Marine energy management", "text": "Marine energy management\n\nMarine energy management is the application of systematic methods to measure, register and analyze the energy usage of oceangoing vessels in specific. \n\nThe goal of marine energy management is to \nThese are two separated optimization problems.\n\nMarine energy management can both be applied on board and onshore. It is a complex problem, due to the number of inter-related energy systems on board vessels, such as the propulsion, the auxiliary engines, refrigeration systems, HVAC, etc. The weather and sea-state, plus the logistics involved in transporting goods from one port to another, also have big effects. \n\nMarine energy management can be addressed on board through measuring devices, monitoring systems and decision-support systems. It can be addressed onshore through data analysis, leading to change in operation on board.\n\n\n"}
{"id": "28266935", "url": "https://en.wikipedia.org/wiki?curid=28266935", "title": "Matthias Willenbacher", "text": "Matthias Willenbacher\n\nMatthias Willenbacher (born 14 July 1969) is a German entrepreneur and pioneer in the field of renewable energy. He founded juwi together with Fred Jung in 1996.\n\nHe is one of the shown protagonists in the film „Die 4. Revolution – Energy Autonomy“ and with his enterprise one of the main sponsors of the film.\n\nTogether with his partner Fred Jung he was awarded „Greentech Manager des Jahres“ in 2009 by the German magazine Capital.\n\n"}
{"id": "34972929", "url": "https://en.wikipedia.org/wiki?curid=34972929", "title": "Mulayit Wildlife Sanctuary", "text": "Mulayit Wildlife Sanctuary\n\nMulayit Wildlife Sanctuary is a small protected area in Kayin State, Burma. It is located around Mulayit mountain in the Dawna Range.\n\nThe reserve occupies an area of and was established in 1936.\n\nThe white-fronted scops owl, the silver-eared laughingthrush (\"Trochalopteron melanostigma\"), the grey-sided thrush (\"Turdus feae\"), a vulnerable species, and the Tenasserim white-bellied rat (\"Niviventer tenaster\") are found in the Mulayit Taung area.\n\nThe wildlife sanctuary is an insurgency area and wildlife biodiversity is declining. Protection of species is compromised for all the forests of the region have seen unprecedented destruction in recent years.\n\n"}
{"id": "38706734", "url": "https://en.wikipedia.org/wiki?curid=38706734", "title": "Myth of superabundance", "text": "Myth of superabundance\n\nThe myth of superabundance is the belief that earth has more than sufficient natural resources to satisfy humanity's needs, and that no matter how much of these resources humanity uses, the planet will continuously replenish the supply. Although the idea had existed previously among conservationists in the 19th century, it was not given a name until Stewart Udall's 1964 book \"The Quiet Crisis\".\n\nUdall describes the myth as the belief that there was \"so much land, so much water, so much timber, so many birds and beasts\" that man did not envision a time where the planet would not replenish what had been sowed. The myth of superabundance began to circulate during Thomas Jefferson's presidency at the beginning of the nineteenth century and persuaded many Americans to exploit natural resources as they pleased with no thought of long-term consequences. According to historian of the North American west George Colpitts, \"No theme became as integral to western promotion as natural abundance.\" Especially with respect to the west after 1890, promotional literature encouraged migration by invoking the idea that God had provided an abundant environment there such that no man or family would fail if they sought to farm or otherwise live off the land out west. Since at that time environmental science and the study of ecology barely allowed for the possibility of animal extinction and did not provide tools for measuring biomass or the limits of natural resources, many speculators, settlers, and other parties participated in unsustainable practices that led to various extinctions, the Dust Bowl phenomenon, and other environmental catastrophes.\n\nIn 1784, John Filson wrote \"The Discovery, Settlement And present State of Kentucke\", which included the chapter \"The Adventures of Colonel Daniel Boon\". This work represents one of the earliest instance of the myth of superabundance, acting as something of a promotional ad enticing settlers to Kentucky based on the abundance of resources to be found there.\n\nUdall describes many large-scale impacts on natural resources, terming them \"The Big Raid on resources\". The first was the need for lumber in a growing nation for fuel, housing and paper. Udall states that it was with this first big raid on the earth’s natural resources that the myth of superabundance began to show its fallacy. It was only towards the end of the nineteenth century that people were awakened to the empty hillsides and the vastness of blackened woods from the lumber industry. Petroleum followed, as it was widely believed that oil was constantly made inside the earth, and so, like everything else, was inexhaustible. Then came seal hunting, and by 1866 the seal population that originally numbered approximately five million was drastically cut in half. Many of the seals were shot in the water and never recovered, allowing for enormous waste. The Fur Seal Treaty which came about in 1911 saved the seals from becoming the first major marine species to become extinct thanks to the myth of superabundance.\n\nThe passenger pigeon was the largest wildlife species known to humanity in the early nineteenth century, when the bird's population was estimated at about five billion. By the early 20th century, due to overhunting and habitat destruction brought about by the timber industry, the species had become extinct, the last passenger pigeon having died in the Cincinnati Zoo. The passenger pigeon became extinct in under a century and was just one of the many victims of the myth of superabundance.\n\nLikewise, the American buffalo was threatened by the myth of superabundance. They were considered to be the largest and most valuable resource because just about every piece of them was usable. The big kill of the buffalo began at the end of the Civil War when armies wanted the animals killed in order to starve out the Plains Indians. Railroad men wanted them killed in order to supply heavier and profitable loads of hides. Buffalo were killed for their tongues and hides, and some hunters simply wanted them as trophies. Pleas of protection for the buffalo were ignored, nearly wiping out the species.\n\nThe Great Leap Forward in China in 1958 corresponded closely with the myth of superabundance; economic planners reduced the acreage space for planting wheat and grains, trying to force farmers and agricultural labourers into accepting new forms of industry. As a result, production of wheat and grain was slowed dangerously, and floods in the South and droughts in the North struck in 1959, leading China into the record-breaking Great Chinese Famine.\n\nGeorge Perkins Marsh, who wrote \"Man and Nature\" in 1864, rejected the idea that any resource could be exploited without any concerns for the future. Perkins was a witness to natural destruction; he saw that mistakes of the past were destroying the present prosperity. He believed that nature should be second nature to all and should not be used as an exploitation for economics and politics. He was, after all, \"forest born\". Man's role as a catalyst of change in the natural world intrigued him. He believed that progress was entirely possible and necessary, if only men used wisdom in the management of resources. He deflated, but did not destroy the myth of superabundance. He began the spin into doubt, which made way for John Muir in 1874. Muir, who had grown up surrounded by wilderness, believed that wildlife and nature could provide people with heightened sense abilities and experiences of awe that could be found nowhere else. Entering into civilization with a desire to see preservation of some of what he believed to be America’s most beautiful nature, he built upon steps that had been taken by Frederick Law Olmsted, a young landscape architect who designed Central Park in New York City. Olmsted had persuaded Congress to pass a bill preserving much of Yosemite Valley, which President Lincoln had then approved in 1864. In 1872 President Grant signed the Yellowstone Park bill, saving over two million acres of wildlife.\n\nMuir saw overgrazing destruction in Yosemite, in parts of it that were not under protection. It was a result of nearby sheepmen and their herds. In 1876, Muir wrote an article \"God’s First Temples – How Shall We Preserve Our Forests\", which he published in the newspaper, pleading for help with protection of the forests. At first he failed against the overriding ideal of the myth of superabundance, but he did inspire bills in the 1880s that sought to enlarge Yosemite’s reservation. Muir formed the Sierra Club, a group of mountaineers and conservationists like him who had responded to his many articles. The Sierra Club’s first big fight came as a counter-attack on lumbermen and stockmen who wanted to monopolize some of Yosemite County. Yosemite Valley, which was still owned by the state, was mismanaged and natural reserves like the meadows and Mirror Lake, which was dammed for irrigation, were still being destroyed even under supposed protection. In 1895, Muir and the Sierra Club began a battle that would span over ten years, fighting for natural management of Yosemite Valley. Theodore Roosevelt met with Muir in 1903 and was instantly fascinated with Muir’s passion for the wilderness. Roosevelt approved Muir’s argument for Yosemite Valley, and so the Sierra Club took their decade long campaign to Sacramento, where they finally won against California legislature in 1905. With Roosevelt on Muir’s side, Yosemite Valley finally became part of the Yosemite National Park and was allowed natural management.\n\nUdall asserts that the myth of superabundance, once exposed, was replaced in the 20th century by the \"myth of scientific supremacy\": the belief that science can eventually find a solution to any problem. This leads to behaviors which, while recognizing that resources are not infinite, still fail to properly preserve those resources, putting the problem off to future generations to solve through science. \"Present the repair bill to the next generation\" is their silent motto. George Perkins Marsh had said that conservation's greatest enemies were \"greed and shortsightedness\". Men reach a power trip thinking they can manipulate nature the way that they want.\n\nIn order for man to live harmoniously with nature, as Muir and Perkins and many others have fought for, Patsy Hallen in the article, \"The Art of Impurity\" says that an ethics development must occur in which respect for nature and our radical dependency on it can take place. Humans see themselves as superior to nature, and yet we are in a constant state of continuity with it. Hallen argues that humanity cannot afford such an irrational state of mind and ecological denial if it expects to prosper in the future.\n"}
{"id": "41230199", "url": "https://en.wikipedia.org/wiki?curid=41230199", "title": "Oak woodland", "text": "Oak woodland\n\nAn oak woodland is a plant community with a tree canopy dominated by oaks (\"Quercus spp.\"). In terms of canopy closure, oak woodlands are intermediate between oak savanna, which is more open, and oak forest, which is more closed. Although the community is named for the dominance of oak trees, the understory vegetation is often diverse and includes many species of grasses, sedges, forbs, ferns, shrubs, and other plants. \n\n\n"}
{"id": "41846526", "url": "https://en.wikipedia.org/wiki?curid=41846526", "title": "Oatmeal paper", "text": "Oatmeal paper\n\nOatmeal paper is a paper made from oatmeal. In Korea, Oatmeal paper is called as Hwangji (황지, 黃紙, Yellow paper). Jaegaseung (Jurchen people descendants who live in Korea) produced this paper and substitute their tax to oatmeal paper.\n\n"}
{"id": "24236", "url": "https://en.wikipedia.org/wiki?curid=24236", "title": "Power (physics)", "text": "Power (physics)\n\nIn physics, power is the rate of doing work or transferring heat, the amount of energy transferred or converted per unit time. Having no direction, it is a scalar quantity. In the International System of Units, the unit of power is the joule per second (J/s), known as the watt in honour of James Watt, the eighteenth-century developer of the steam engine condenser. Another common and traditional measure is horsepower (comparing to the power of a horse). Being the rate of work, the equation for power can be written:\n\nAs a physical concept, power requires both a change in the physical system and a specified time in which the change occurs. This is distinct from the concept of work, which is only measured in terms of a net change in the state of the physical system. The same amount of work is done when carrying a load up a flight of stairs whether the person carrying it walks or runs, but more power is needed for running because the work is done in a shorter amount of time. \n\nThe output power of an electric motor is the product of the torque that the motor generates and the angular velocity of its output shaft. The power involved in moving a vehicle is the product of the traction force of the wheels and the velocity of the vehicle. The rate at which a light bulb converts electrical energy into light and heat is measured in watts—the higher the wattage, the more power, or equivalently the more electrical energy is used per unit time.\n\nThe dimension of power is energy divided by time. The SI unit of power is the watt (W), which is equal to one joule per second. Other units of power include ergs per second (erg/s), horsepower (hp), metric horsepower (Pferdestärke (PS) or cheval vapeur (CV)), and foot-pounds per minute. One horsepower is equivalent to 33,000 foot-pounds per minute, or the power required to lift 550 pounds by one foot in one second, and is equivalent to about 746 watts. Other units include dBm, a relative logarithmic measure with 1 milliwatt as reference; food calories per hour (often referred to as kilocalories per hour); BTU per hour (BTU/h); and tons of refrigeration (12,000 BTU/h).\n\nPower, as a function of time, is the rate at which work is done, so can be expressed by this equation:\n\nwhere \"P\" is power, \"W\" is work, and \"t\" is time. Because work is a force F applied over a distance r,\n\nfor a constant force, power can be rewritten as:\n\nAs a simple example, burning one kilogram of coal releases much more energy than does detonating a kilogram of TNT, but because the TNT reaction releases energy much more quickly, it delivers far more power than the coal.\nIf Δ\"W\" is the amount of work performed during a period of time of duration Δ\"t\", the average power \"P\" over that period is given by the formula\nIt is the average amount of work done or energy converted per unit of time. The average power is often simply called \"power\" when the context makes it clear.\n\nThe instantaneous power is then the limiting value of the average power as the time interval Δ\"t\" approaches zero.\n\nIn the case of constant power \"P\", the amount of work performed during a period of duration \"T\" is given by:\n\nIn the context of energy conversion, it is more customary to use the symbol \"E\" rather than \"W\".\n\nPower in mechanical systems is the combination of forces and movement. In particular, power is the product of a force on an object and the object's velocity, or the product of a torque on a shaft and the shaft's angular velocity.\n\nMechanical power is also described as the time derivative of work. In mechanics, the work done by a force F on an object that travels along a curve \"C\" is given by the line integral:\nwhere x defines the path \"C\" and v is the velocity along this path.\n\nIf the force F is derivable from a potential (conservative), then applying the gradient theorem (and remembering that force is the negative of the gradient of the potential energy) yields:\n\nwhere \"A\" and \"B\" are the beginning and end of the path along which the work was done.\n\nThe power at any point along the curve \"C\" is the time derivative\n\nIn one dimension, this can be simplified to:\n\nIn rotational systems, power is the product of the torque τ and angular velocity ω,\nwhere ω measured in radians per second. The formula_13 represents scalar product.\n\nIn fluid power systems such as hydraulic actuators, power is given by\nwhere \"p\" is pressure in pascals, or N/m and \"Q\" is volumetric flow rate in m/s in SI units.\n\nIf a mechanical system has no losses, then the input power must equal the output power. This provides a simple formula for the mechanical advantage of the system.\n\nLet the input power to a device be a force \"F\" acting on a point that moves with velocity \"v\" and the output power be a force \"F\" acts on a point that moves with velocity \"v\". If there are no losses in the system, then\nand the mechanical advantage of the system (output force per input force) is given by\n\nThe similar relationship is obtained for rotating systems, where \"T\" and \"ω\" are the torque and angular velocity of the input and \"T\" and \"ω\" are the torque and angular velocity of the output. If there are no losses in the system, then\nwhich yields the mechanical advantage\n\nThese relations are important because they define the maximum performance of a device in terms of velocity ratios determined by its physical dimensions. See for example gear ratios.\n\nThe instantaneous electrical power \"P\" delivered to a component is given by\n\nwhere \n\nIf the component is a resistor with time-invariant voltage to current ratio, then:\n\nwhere\nis the resistance, measured in ohms.\n\nIn the case of a periodic signal formula_22 of period formula_23, like a train of identical pulses, the instantaneous power formula_24 is also a periodic function of period formula_23. The \"peak power\" is simply defined by:\n\nThe peak power is not always readily measurable, however, and the measurement of the average power formula_27 is more commonly performed by an instrument. If one defines the energy per pulse as:\nthen the average power is:\n\nOne may define the pulse length formula_30 such that formula_31 so that the ratios\n\nare equal. These ratios are called the \"duty cycle\" of the pulse train.\n\nPower is related to intensity at a distance formula_33; the power emitted by a source can be written as:\n\n"}
{"id": "18692535", "url": "https://en.wikipedia.org/wiki?curid=18692535", "title": "Power of Siberia", "text": "Power of Siberia\n\nThe Power of Siberia () pipeline (formerly known as Yakutia–Khabarovsk–Vladivostok pipeline) is a natural gas pipeline under construction in Eastern Siberia to transport Yakutia's gas to Primorsky Krai and Far East countries.\n\nOn 29 October 2012 president Vladimir Putin instructed the general manager of Gazprom to start the construction of the pipeline. On 21 May 2014, Russia and China signed a 30-year gas deal which was needed to make the project feasible. Construction was launched on 1 September 2014 in Yakutsk by president Putin and Chinese deputy premier minister Zhang Gaoli. Construction of the pipeline from Vladivostok to China started on 29 June 2015.\n\nThe goal was to complete 1,300 kilometers of the pipeline before the end of 2017. Construction was ahead of schedule, thanks to a warmer-than-expected 2016 winter.\n\nThe pipeline is expected to cost 770 billion roubles and the investment in the gas production is 430 billion roubles. It is expected to be operational by 2019. Capacity of the pipeline would be up to of natural gas. would be supplied to China. The pipeline's working pressure is 9.8 megapascals (1,421 psi).\n\nThe pipeline will be able to withstand temperatures as low as -62 °C (-79.6 °F). Nanocomposite coatings manufactured and engineered by JSC Metaclay will be used to increase the lifetime of the pipeline. Furthermore, the pipeline will be able to withstand earthquakes by incorporating materials that will deform under seismic activity. Internal coatings ensure energy efficiency by reducing the friction of the pipeline’s inner surfaces. The mass of all the pipes used to construct the pipeline is greater than 2.5 megatons.\n\nA section of the pipeline will start from the Chayanda oil and gas field in Yakutia. It will partly run within an integrated corridor with the second stage of Eastern Siberia-Pacific Ocean oil pipeline. In Khabarovsk, it will be connected with the Sakhalin–Khabarovsk–Vladivostok pipeline. Together, these pipelines will feed a planned LNG plant, which will produce liquefied natural gas for export to Japan, and a planned petrochemical complex in Primorsky Krai. Branches toward Northern China are envisaged.\n\nIn addition, the project includes pipeline from Irkutsk to Yakutia.\n\nThe pipeline will be fed from the Chayanda oil and gas field in Yakutia. The gas field is expected to be launched in 2019. Later the Kovykta field, which would come operational by 2021, will be connected to the pipeline. Independent producers may supply up to of natural gas.\n\n\n"}
{"id": "23276709", "url": "https://en.wikipedia.org/wiki?curid=23276709", "title": "Reinforced rubber", "text": "Reinforced rubber\n\nAlthough seldom shared under this name, one of the largest groups of composite materials worldwide is that of the reinforced rubber products. Familiar examples are automobile tyres, hoses and conveyor belts.\n\nReinforced rubber products combine a rubber matrix and a reinforcing material, so high strength to flexibility ratios can be achieved. The reinforcing material, usually a kind of fibre, provides the strength and stiffness. The rubber matrix, with low strength and stiffness, provides air-fluid tightness and supports the reinforcing materials to maintain their relative positions. These positions are of great importance because they influence the resulting mechanical properties.\n\nA composite structure where all fibres are loaded equally everywhere when pressurized, is called an isotropic structure and the type of loading is named an isotensoidal loading. To meet the isotensoidal concept the structure geometry must have an isotensoid meridian profile and the fibres must be positioned following geodesic paths. A geodesic path connects two arbitrary points on a continuous surface by means of the shortest possible way.\n\nTo achieve optimal loading in a straight rubber hose the fibres must be positioned under an angle of approximately 54.7 angular degrees, also referred to as the magic angle. The magic angle of 54.7 exactly balances the internal-pressure-induced longitudinal stress and the hoop (circumferential) stress, as observed in most biological pressurized fiber-wound cylinders, like arteries. If the fiber angle is initially above or below 54.7, it will change under increased internal pressure until it rises to the magic angle where hoop stresses and longitudinal stresses equalize, with concomitant accommodations in hose diameter and hose length. A hose with an initially low fiber angle will rise under pressure to 54.7, inducing a hose diameter increase and a length decrease, whereas a hose with an initially high fiber angle will drop to 54.7, inducing a hose diameter decrease and a length increase. The equilibrium state is a fiber angle of 54.7. In this situation the fibres tend to be loaded purely in tension, so ~100% of their strength resists the forces acting on the hose due to the internal pressure. (The magic angle for cylindrical shapes of 54.7 angular degrees is based on calculations in which the influence of the matrix material is neglected. Therefore, depending on the stiffness of the rubber material used, the actual equilibrium angle can vary a few tenths of degrees from the magic angle.)\n\nWhen the fibres of the reinforcement structure are placed under angles larger than 54.7 angular degrees, the fibres want to relocate to their optimal path when pressurized. This means that the fibres will re-orient themselves until they have reached their force equilibrium. In this case this will lead to an increase in length and a decrease in diameter. With angles smaller than 54.7 degrees the opposite will occur. A product which makes use of this principle is a pneumatic muscle.\n\nFor a cylinder with a constant diameter the reinforcement angle is constant as well and is 54.7º. This also known as the magic angle or neutral angle. The neutral angle is the angle where a wound structure is in equilibrium. For a cylinder this is 54.7º, but for a more complex shape like a bellows which has a varying radius over the length of the product, this neutral angle is different for each radius. In other words, for complex shapes there is not one magic angle but the fibres follow a geodesic path with angles varying with the change in radius. To obtain a reinforcement structure with isotensoidal loading the geometry of the complex shape must follow an isotensoid meridian profile. \nThe mil fabric reinforcement can be applied on the rubber products with different processes. For straight hoses the most used processes are braiding, spiralling, knitting and wrapping. The first three processes have in common that multiple strands of fibres are applied to the product simultaneously on a predetermined pattern in an automated process. The fourth process comprises manual or semi-automated wrapping of rubber sheets reinforced with fabric plies. For the reinforcement of complex shaped rubber products like bellows most manufacturers use these fabric reinforced rubber sheets. These sheets are made by calendering of rubber onto pre-woven fabric plies. The products are manufactured by wrapping (mostly manually) these sheets around a mandrel until enough rubber and reinforcement is applied. However, the disadvantage of using these sheets is that it is impossible to control the positioning of the individual fibres of the fabric when applied on complex shapes. Therefore, no geodesic paths can be achieved and therefore also no isotensoid loading is possible. To obtain isotensoide loading on a complex shape, the shape must have an isotensoideal profile and geodesic positioning of the fibre structure is required. This can be achieved by using automated winding processes like filament winding or spiralling.\n"}
{"id": "11869024", "url": "https://en.wikipedia.org/wiki?curid=11869024", "title": "Short-chain fatty acid", "text": "Short-chain fatty acid\n\nShort-chain fatty acids (SCFAs), also referred to as volatile fatty acids (VFAs), are fatty acids with less than six carbon atoms. Free SCFAs can cross the blood-brain barrier via monocarboxylate transporters.\n\nShort-chain fatty acids are produced when dietary fiber is fermented in the colon. Short-chain fatty acids and medium-chain fatty acids are primarily absorbed through the portal vein during lipid digestion, while long-chain fatty acids are packed into chylomicrons and enter lymphatic capillaries, and enter the blood first at the subclavian vein.\n\nShort-chain fatty acids have diverse physiological roles in body functions. Butyrate is particularly important for colon health because it is the primary energy source for colonic cells.\n\n\n"}
{"id": "2841656", "url": "https://en.wikipedia.org/wiki?curid=2841656", "title": "Stroke ratio", "text": "Stroke ratio\n\nIn a reciprocating piston engine, the stroke ratio, defined by either bore/stroke ratio or stroke/bore ratio, is a term to describe the ratio between cylinder bore diameter and piston stroke. This can be used for either an internal combustion engine, where the fuel is burned within the cylinders of the engine, or external combustion engine, such as a steam engine, where the combustion of the fuel takes place \"outside\" the working cylinders of the engine.\n\nA fairly comprehensive yet understandable study of stroke/bore effects was published in \"Horseless Age\", 1916.\n\nIn a piston engine, there are two different ways of describing the \"stroke ratio\" of its cylinders, namely: \"bore/stroke\" ratio, and \"stroke/bore\" ratio.\n\nBore/stroke is the more commonly used term, with usage in North America, Europe, United Kingdom, Asia, and Australia.\n\nThe diameter of the cylinder bore is divided by the length of the piston stroke to give the ratio.\n\nThe following terms describe the naming conventions for the configurations of the various bore/stroke ratio:\n\nA square engine has equal bore \"and\" stroke dimensions, giving a bore/stroke value of exactly 1:1.\n\n1967 – FIAT 125, 124Sport engine 125A000-90 hp, 125B000-100 hp, 125BC000-110 hp, 1608 ccm, DOHC, bore and stroke.\n\n1970 – Ford 400M had a bore and stroke.\n\n1973 – Kawasaki Z1 and KZ(Z)900 had a bore and stroke.\n\n1973 – British Leyland's Australian division created a 4.4-litre version of the Rover V8 engine, with bore and stroke both measuring 88.9 mm. This engine was exclusively used in the Leyland P76.\n\n1982 - Honda Nighthawk 250 and Honda CMX250C Rebel have a bore and stroke, making it a square engine.\n\n1983 – Mazda FE 2.0L inline four-cylinder engine with a perfectly squared bore and stroke. This engine also features the ideal 1.75:1 rod/stroke ratio.\n\n1987 – The Opel/Vauxhaul 2.0 L GM Family II engines are square at bore and stroke; example as C20XE C20NE C20LET X20A X20XEV X20XER Z20LET Z20LEH Z20LER A20NHT A20NFT.\n\n1989 – Nissan's SR20DE is a square engine, with an bore and stroke.\n\n1990–2010 Saab B234/B235 is a square engine, with a bore and stroke.\n\n1991 – Ford's 4.6 V8 OHC engine has a bore and stroke. It has been the backbone of Ford V8-powered cars and trucks in different power levels and head designs for two decades.\n\n1995 – The BMW M52 engine with a displacement of 2793 cubic centimeters is an example of a perfect square engine with an bore and stroke.\n\n1996 – Jaguar's AJ-V8 engine in 4.0-litre form has an 86.0 mm bore and stroke.\n\n2000 – Mercedes-Benz 4.0-litre () OM628 V8 diesel engine is an example of a square engine – with an bore and stroke.\n\nThe Volkswagen Group's 2005 W16 engine as used in the Bugatti Veyron also using a bore and stroke.\n\nThe Peugeot XU10 engine line – with a displacement of 1998 cubic centimeters – is an example of a perfect square engine with an bore and stroke.\n\nToyota's 2JZ and 4U are square engines, with bore and stroke.\n\nHonda's J30A engine has an bore and stroke.\n\nAn engine is described as oversquare or short-stroke if its cylinders have a greater bore diameter than its stroke length, giving a bore/stroke ratio greater than 1:1.\n\nAn oversquare engine allows for more and larger valves in the head of the cylinder, higher possible rpm by lowering maximum piston ring speed and lower crank stress due to the lower peak piston acceleration for the same engine speed. Due to the increased piston and head surface area, the heat loss increases as the bore/stroke ratio is increased. Thus an excessively high ratio can lead to a decreased thermal efficiency compared to other engine geometries. Because these characteristics favor higher engine speeds, oversquare engines are often tuned to develop peak torque at a relatively high speed. The large size/width of the combustion chamber at ignition can cause increased inhomogeneity in the air/fuel mixture during combustion, resulting in higher emissions.\n\nThe reduced stroke length allows for a shorter cylinder and sometimes a shorter connecting rod, generally making oversquare engines less tall but wider than undersquare engines of similar engine displacement.\n\nOversquare engines (a.k.a. \"short stroke engines\") are very common, as they allow higher rpm (and thus more power), without excessive piston speed.\n\nExamples include both Chevrolet and Ford small-block V8s. The BMW N45 gasoline engine has a bore/stroke ratio of 1.167.\n\nHorizontally opposed, also known as \"Boxer\" or \"flat\", engines typically feature oversquare designs since any increase in stroke length would result in twice the increase in overall engine width. This is particularly so in Subaru’s front-engine layout, where the steering angle of the front wheels is constrained by the width of the engine. Although oversquare engines have a reputation for being high-strung, low-torque machines, the Subaru EJ181 engine develops peak torque at speeds as low as 3200 rpm.\n\nNissan's SR16VE engine found in Nissan Pulsar VZ-R and VZ-R N1 is an oversquare engine with bore and stroke, giving it an impressive but relatively small torque of \n\nExtreme oversquare engines are found in Formula One racing cars, where strict rules limit displacement, thereby necessitating that power be achieved through high engine speeds. Stroke ratios approaching 2.5:1 are allowed, enabling engine speeds of 18,000 rpm while remaining reliable for multiple races.\n\nThe Ducati Panigale motorcycle engine is massively oversquare with a bore/stroke ratio of 1.84:1. It was given the name \"SuperQuadro\" by Ducati, roughly translated as \"super-square\" from Italian.\n\nThe side-valve Belgian D-Motor LF26 aero-engine has a bore/stroke ratio of 1.4:1.\n\nEarly Mercedes-Benz M116 engines had a bore and a stroke for a 3.5 litre V8.\n\nAn engine is described as undersquare or long-stroke if its cylinders have a smaller bore (width, diameter) than its stroke (length of piston travel) - giving a ratio value of less than 1:1.\n\nAt a given engine speed, a longer stroke increases engine friction and increases stress on the crankshaft due to the higher peak piston acceleration. The smaller bore also reduces the area available for valves in the cylinder head, requiring them to be smaller or fewer in number.\n\nUndersquare engines exhibit peak torque at lower rpm than an oversquare engine due to their longer crank throw and high piston speed.\n\nUndersquare engines have become more common lately, as manufacturers push for more and more efficient engines and higher fuel economy. Undersquare engines have a higher volume/surface area ratio, leading to reduced heat loss and higher BSFC.\n\nMany inline engines, particularly those mounted transversely in front-wheel-drive cars, utilize an undersquare design. The smaller bore allows for a shorter engine that increases room available for the front wheels to steer. Examples of this include many Volkswagen, Nissan, Honda, and Mazda engines. The 1KR-FE-engine used in the Toyota Aygo, Citroën C1 and Peugeot 107 amongst others is an example of a modern long-stroke engine widely used in FF layout cars. This engine has a 71 mm bore and 84 mm stroke giving it a bore/stroke ratio of 0.845:1. Some rear-wheel-drive cars that borrow engines from front-wheel-drive cars (such as the Mazda MX-5) use an undersquare design.\n\nBMW's acclaimed S54B32 engine was undersquare ( vs bore), offering a world record torque-per-litre figure () for normally-aspirated production engines at the time; this record stood until Ferrari unveiled the 458 Italia.\n\nMany British automobile companies used undersquare designs until the 1950s, largely because of a motor tax system that taxed cars by their cylinder bore. This includes the BMC A-Series engine, and many Nissan derivatives. The Trojan Car used an undersquare, split piston, two stroke, two cylinder in line engine; this was partly for this tax advantage and partly because its proportions allowed flexing V-shaped connecting rods for the two pistons of each U-shaped cylinder, which was cheaper and simpler than two connecting rods joined with an additional bearing.\n\nThe 225 cu in (3.7 litre) Chrysler Slant-6 engine is undersquare, with a bore and a stroke (bore/stroke ratio = 0.819:1).\n\nThe Ford 5.4L Modular Engine features a cylinder bore of 90.1 mm (3.552 in) and a stroke of 105.8 mm (4.165 in), which makes a bore/stroke ratio of 0.852:1. Since the stroke is significantly longer than the bore, the SOHC 16V (2-valve per cylinder) version of this engine is able to generate a peak torque of 350 lb·ft as low as 2501 rpm.\n\nThe Willys Jeep L134 and F134 engines were undersquare, with a 79.4 mm (3.125 in) bore and 111.1 mm (4.375 in) stroke (bore/stroke ratio = 0.714:1).\n\nThe Dodge Power Wagon used a straight-six Chrysler Flathead engine of 230 cu in (3.8 l) with a bore of and a stroke of , yielding a substantially undersquare bore/stroke ratio of 0.709:1.\n\nThe 4-litre Barra Inline 6 engine from the Australian Ford Falcon, uses a 92.21 mm bore and 99.31 mm stroke, which equates to a 0.929:1 bore-stroke ratio.\n\nThe 292 Chevrolet I6 is also undersquare, with a bore of 3.875 in and a stroke of 4.125 in (bore/stroke ratio = 0.939:1).\n\nMitsubishi's 4G63T engine found primarily in many generations of Mitsubishi Lancer Evolution is an undersquare engine at 85 mm bore x 88 mm stroke.\n\nVirtually all piston engines used in military aircraft were long-stroke engines. The PW R-2800, Wright R-3350, Pratt & Whitney R-4360 Wasp Major, Rolls-Royce Merlin (1650), Allison V-1710, and Hispano-Suiza 12Y-Z are only a few of more than a hundred examples.\n\nAll diesel-powered ships have massively undersquare marine engines. A Wärtsilä two-stroke marine diesel engine has a cylinder bore of 960 mm (37.8 in) and stroke of 2500 mm (98.4 in), (bore/stroke ratio = 0.384:1).\n\nWhile most modern motorcycle engines are square or oversquare, some are undersquare. The Kawasaki Z1300's straight-six engine was made undersquare to minimise engine width, more recently, a new straight-twin engine for the Honda NC700 series used an undersquare design to achieve better combustion efficiency in order to reduce fuel consumption.\n"}
{"id": "11633197", "url": "https://en.wikipedia.org/wiki?curid=11633197", "title": "Tacca leontopetaloides", "text": "Tacca leontopetaloides\n\nTacca leontopetaloides is a species of flowering plant in the yam family Dioscoreaceae, that is native to tropical Africa, South Asia, Southeast Asia, northern Australia, New Guinea, Samoa, Micronesia, and Fiji. As an important food source, it was intentionally taken to tropical Pacific Islands with early human migrations. Common names include Polynesian arrowroot, pia (Hawaii, French Polynesia, Niue, and Cook Islands), masoa (Samoa), mahoaa (Tonga), yabia (Fiji) gapgap (Guam) and taka (Indonesia).\n\nSeveral petioles in length extend from the center of the plant, on which the large leaves ( long and up to wide) are attached. The leaf's upper surface has depressed veins, and the under surface is shiny with bold yellow veins. Flowers are borne on tall stalks in greenish-purple clusters, with long trailing bracts. The plant is usually dormant for part of the year and dies down to the ground. Later, new leaves will arise from the round underground tuber. The tubers are hard and potato-like, with a brown skin and white interior.\n\nThe tubers of Polynesian arrowroot contain starch, making it an important food source for many Pacific Island cultures, primarily for the inhabitants of low islands and atolls. Polynesian arrowroot was prepared into a flour to make a variety of puddings. The tubers were first grated and then allowed to soak in fresh water. The settled starch was rinsed repeatedly to remove the bitterness and then dried. The flour was mixed with mashed taro, breadfruit, or \"Pandanus\" fruit extract and mixed with coconut cream to prepare puddings. In Hawaii, a local favorite is haupia, which was originally made with \"pia\" flour, coconut cream and kō (cane sugar). Today, Polynesian arrowroot has been largely replaced by cornstarch.\n\nThe starch was additionally used to stiffen fabrics, and on some islands, the stem's bast fibres were woven into mats.\n\nIn traditional Hawaiian medicine the raw tubers were eaten to treat stomach ailments. Mixed with water and red clay, the plant was consumed to treat diarrhea and dysentery. This combination was also used to stop internal hemorrhaging in the stomach and colon and applied to wounds to stop bleeding.\n"}
{"id": "11709087", "url": "https://en.wikipedia.org/wiki?curid=11709087", "title": "Turbine engine failure", "text": "Turbine engine failure\n\nTurbine engines in use on today's turbine-powered aircraft are very reliable. Engines operate efficiently with regularly scheduled inspections and maintenance. These units can have lives ranging in the thousands of hours of operation. However, engine malfunctions or failures occasionally occur that require an engine to be shut down in flight. Since multi-engine airplanes are designed to fly with one engine inoperative and flight crews are trained to fly with one engine inoperative, the in-flight shutdown of an engine typically does not constitute a serious safety of flight issue. Following an engine shutdown, a precautionary landing is usually performed with airport fire and rescue equipment positioned near the runway. The prompt landing is a precaution against the risk that another engine will fail later in the flight or that the engine failure that has already occurred may have caused or been caused by other as-yet unknown damage or malfunction of aircraft systems (such as fire or damage to aircraft flight controls) that may pose a continuing risk to the flight. Once the airplane lands, fire department personnel assist with inspecting the airplane to ensure it is safe before it taxis to its parking position. Turboprop-powered aircraft and turboshaft-powered helicopters are also powered by turbine engines and are subject to engine failures for many similar reasons as jet-powered aircraft. In the case of an engine failure in a helicopter, it is often possible for the pilot to enter autorotation, using the unpowered rotor to slow the aircraft's descent and provide a measure of control, usually allowing for a safe emergency landing even without engine power.\n\nMost in-flight shutdowns are harmless and likely to go unnoticed by passengers. For example, it may be prudent for the flight crew to shut down an engine and perform a precautionary landing in the event of a low oil pressure or high oil temperature warning in the cockpit. However, passengers in a jet powered aircraft may become quite alarmed by other engine events such as a compressor surge — a malfunction that is typified by loud bangs and even flames from the engine's inlet and tailpipe. A compressor surge is a disruption of the airflow through a gas turbine jet engine that can be caused by engine deterioration, a crosswind over the engine's inlet, ice accumulation around the engine inlet, ingestion of foreign material, or an internal component failure such as a broken blade. While this situation can be alarming, the engine may recover with no damage.\n\nOther events that can happen with jet engines, such as a fuel control fault, can result in excess fuel in the engine's combustor. This additional fuel can result in flames extending from the engine's exhaust pipe. As alarming as this would appear, at no time is the engine itself actually on fire.\n\nAlso, the failure of certain components in the engine may result in a release of oil into bleed air that can cause an odor or oily mist in the cabin. This is known as a fume event. The dangers of fume events are the subject of debate in both aviation and medicine.\n\nEngine failures can be caused by mechanical problems in the engine itself, such as damage to portions of the turbine or oil leaks, as well as damage outside the engine such as fuel pump problems or fuel contamination. A turbine engine failure can also be caused by entirely external factors, such as volcanic ash, bird strikes or weather conditions like precipitation or icing. Weather risks such as these can sometimes be countered through the usage of supplementary ignition or anti-icing systems.\n\nA turbine-powered aircraft's takeoff procedure is designed around ensuring that an engine failure will not endanger the flight. This is done by planning the takeoff around three critical V speeds, V1, VR and V2. V1 is the critical engine failure recognition speed, the speed at which a takeoff can be continued with an engine failure, and the speed at which stopping distance is no longer guaranteed in the event of a rejected takeoff. VR is the speed at which the nose is lifted off the runway, a process known as rotation. V2 is the single-engine safety speed, the single engine climb speed. The use of these speeds ensure that either sufficient thrust to continue the takeoff, or sufficient stopping distance to reject it will be available at all times.\n\nIn order to allow twin-engined aircraft to fly longer routes that are over an hour from a suitable diversion airport, a set of rules known as ETOPS (Extended Twin-engine Operational Performance Standards) is used to ensure a twin turbine engine powered aircraft is able to safely arrive at a diversionary airport after an engine failure or shutdown, as well as to minimize the risk of a failure. ETOPS includes maintenance requirements, such as frequent and meticulously logged inspections and operation requirements such as flight crew training and ETOPS-specific procedures.\n\nEngine failures may be described as either as \"contained\" or \"uncontained\".\n\n\nEngine cases are not designed to contain failed turbine disks. Instead, the risk of uncontained disk failure is mitigated by designating disks as safety-critical parts, defined as the parts of an engine whose failure is likely to present a direct hazard to the aircraft. Engine manufacturers are required by the FAA to perform blade off tests to ensure containment of shrapnel if blade separation occurs.\n\n"}
{"id": "14633281", "url": "https://en.wikipedia.org/wiki?curid=14633281", "title": "Twig work", "text": "Twig work\n\nTwig-work is the term applied to architectural details constructed of twigs and branches to form decorative motifs in buildings and furniture. Carpentry or woodworking using wood that has not been milled into lumber and is still in its natural shape describes the National park service rustic style.\n\nJoinery on twigs and branches is similar to joinery for lumber. Mortise and tenon joints are strong, but also labor-intensive and time-consuming. Twigs and branches can also be fastened with nails. Where one branch meets another, the ends must be coped, or cut to match the curve.\n\n\n"}
{"id": "17423741", "url": "https://en.wikipedia.org/wiki?curid=17423741", "title": "VM reactor", "text": "VM reactor\n\nThe various types of the VM reactor series are nuclear pressurized water reactors (PWR). They are used singly or in pairs to power the Soviet Navy's submarines. \n\nIt was developed by NIKIET.\n\n"}
{"id": "42044657", "url": "https://en.wikipedia.org/wiki?curid=42044657", "title": "Walker Electric Truck", "text": "Walker Electric Truck\n\nWalker Electric Trucks were battery-powered vehicles built from 1907 to 1942 in Chicago, Illinois and Detroit, Michigan. Initially designed and manufactured by the Walker Vehicle Company (not to be confused with the Walker Motor Car Company) in Chicago, they were bought by the Anderson Electric Car Company of Detroit in 1916, then sold to Commonwealth Edison of Chicago in 1920, and last to York & Towne in 1933. In addition to the trucks the company manufactured the Chicago Electric Car. Several of their trucks were long in service, surviving the brand, in some cases, by decades. A few of them are now on display in museums or, in the case of one exported to New Zealand, still \"in service\" with its original owner Orion New Zealand Limited.\n\nAccording to Jay Christ's Collection of Antique Delivery Trucks and Classic Cars in Manchester, Pennsylvania, that has restored two Walker Electric Trucks—a 1919 Model LA-10, and a 1938 Model 500 Gas-Electric Dynamotive: \nWalker manufactured many different models of trucks and these trucks were sold all across the USA and even to Britain and New Zealand. These trucks had a 3.5 HP electric motor that was powered by many batteries to produce 66 to 80 volts and a maximum of 40 amps. The driving range of these trucks was about 50 miles and the maximum speed was 10 to 12 MPH. The trucks were plugged into a charging station in the evening after the daily deliveries were completed...These trucks were a welcome relief for the general public and the delivery companies. Previously the deliveries were made by horse and wagon. The horses were not the most sanitary because of their daily habits of relieving themselves anywhere they felt like it along the routes. This didn't make the customers too happy. The horses were loyal in the task of pulling the wagons but their pace was slow compared the trucks. The first companies to use this style of delivery were dairies, bakeries, US Mail, retail store and freight companies. Marshall Field and Company had a fleet of 276 electric Walker Trucks in 1925.\n\nThe Iowa Trucking Museum in Walcott, Iowa has a 1911 Walker Electric Truck that had been owned and used by Bowman Dairy and celebrated the truck's 100th birthday at the Walcott Truckers Jamboree in 2011. A 1918 Walker Electric panel van is displayed at the Thor Electric Truck company in Los Angeles.\n\nThe Dwinell-Wright Company in Boston, Massachusetts employed a Walker Electric Truck from 1914 through 1960 to move freight—primarily green coffee beans—from the docks to their South Boston warehouse. Afterward the vehicle was put on display at Edaville Railroad.\n\n"}
{"id": "9978850", "url": "https://en.wikipedia.org/wiki?curid=9978850", "title": "Western Power Corridor", "text": "Western Power Corridor\n\nThe Western Power Corridor (Westcor) was a project to construct and supply energy from two hydroelectric power plants to the Democratic Republic of the Congo, Angola, Namibia, Botswana and South Africa. Originally, the hydro power was to be supplied form the Democratic Republic of the Congo's INGA III project. Later Inga III was supposed to be replaced by Angola's Cuanza River and Cunene River projects or by the new project in the Democratic Republic of Congo.\n\nAn inter-governmental memorandum on the Westcor was signed on 22 October 2004 in Johannesburg, South Africa, followed by the shareholders' agreement signed by the national electricity companies. The pre-feasibility study was completed in 2005.\n\nIn 2009, Democratic Republic of the Congo decided to develop the INGA III project by using private investments and the project was redesigned to involve Cuanza River and Cunene River projects in Angola. In 2010 the Democratic Republic of Congo indicated it will propose a new project involving also Mozambique, Zambia, and Zimbabwe.\n\nUltimately the project was aborted by shareholders when unforeseen changes were proposed to the founding agreements.\n\nThe total length of Westcor will be over and the capacity will be . INGA III will be connected with the Capanda Power Station in Angola via two 400-kilovolt (kV) high voltage alternating current (HVAC) lines. INGA III will be also connected with Kinshasa via another 400 kV HVAC line. From Angola, two multi-terminal high-voltage direct current systems will run to Namibia, Botswana and South Africa. The system will consist of five converter stations: Kuanza in Angola, Auas in Namibia, Goborone in Botswana, and Omega and Pegasus in South Africa. The expected cost of the project is about € 5 billion. \n\nThe Westcor company was registered and incorporated in Botswana in 2003. The shareholders are Empresa Nacional de Electricidade de Angola, Botswana Power Corporation (BPC), the Société nationale d'électricité (SNEL) of the DRC, Eskom of South Africa and NamPower of Namibia. CEO of the company was Pat Naidoo.\n\n"}
{"id": "29784171", "url": "https://en.wikipedia.org/wiki?curid=29784171", "title": "Winter of 2010–11 in Europe", "text": "Winter of 2010–11 in Europe\n\nThe winter of 2010–2011 in Europe began with an unusually cold November caused by a cold weather cycle that started in southern Scandinavia and subsequently moved south and west over both Belgium and the Netherlands on 25 November and into the west of Scotland and north east England on 26 November. This was due to a low pressure zone in the Baltics, with a high pressure over Greenland on 24 November.\n\nFrom 22 November 2010, cold conditions arrived in the United Kingdom, as a cold northerly wind developed and snow began to fall in northern and eastern parts, causing disruption. The winter arrived particularly early for the European climate, with temperatures dropping significantly lower than previous lows for the month of November. On 28 November, Wales recorded its lowest-ever November temperature of in Llysdinam, and Northern Ireland recorded its lowest ever November temperature of in Lough Rea. The UK Met Office issued severe-weather warnings for heavy snow for eastern Scotland and the north-east of England.\n\nFrom January, temperatures were more normal.\n\nThe weather phenomenon was caused by a cold weather cycle that had started in southern Scandinavia and subsequently moved south and west over both Belgium and the Netherlands on 25 November and into the west of Scotland and North East England on 26 November. This was due to a low pressure zone in the Baltics a high pressure over Greenland on 24 November.\n\nA cold front moved out of Siberia on 24 November, and cold spell and snow storms also hit the Alps, on 26 November before hitting the UK on 29 November. Other earlier, but unrelated, storms had dusted Northumberland and Scottish Borders Region on 23–24 November, before being absorbed into the advancing Scandinavian weather system. Ireland was first hit on 26 Nov\n\nHeavy snow caused many problems across the UK and the first disruption of Snowfall occurred on 24 November in the Grampians, Eastern Scotland and Cairngorms, where snow showers blown from a northerly wind caused havoc as accumulations up to in Aviemore made conditions difficult and major roads in Aberdeen had gridlock problems in the rush hour. Further snow disrupted all of Scotland, Southern Wales, Northern Ireland, South West and England and much of the North and East of England as snow accumulated to over in rural settlements in Scotland. The snowfall was the earliest widespread snowfall since 1993. Some forecasters have warned of temperatures dropping to . Temperatures in Carterhouse, Scottish Borders, fell to , and several inches of snow were recorded in Devon and Cornwall.\n\nOn 22 November, forecasters in the Baltic nations saw snow storms expected some in parts of Sweden on the next day, especially in the south of the country. Snow flurries were reported by the afternoon. The Scandinavian low moved southward bringing snow and frost to both The Netherlands and the north west coast of Germany.\n\nThe earliest winter snowfall in the United Kingdom for 17 years was recorded in November 2010.\nA low night temperature of and heavy snow fell over the night of the 25th/26th, which was recorded at Redesdale Camp, Northumberland. A similar quantity of snow fell in Aberdeenshire, and in Durham that night. A Thomsonfly Boeing 737-800 plane with 196 passengers overshot its landing position at Newcastle Airport due to an icy runway. On 26 November, night-time temperatures plummeted well below , with the Welsh towns of Sennybridge and Trawscoed being among the coldest places at . The town of Dalwhinnie in the Scottish Highlands saw the temperatures fall to and Chesham in Buckinghamshire fell to , and Preston, Lancashire recorded . Inverness recorded a nighttime low of with a daytime high of . The Met Office then issued severe weather warnings for almost every part of the UK The Welsh village of Hawarden recorded its coldest November temperature since 1944 with a reading of .\n\nThe thermometers at Llysdinam, near Llandrindod Wells, Powys, recorded a low of , the coldest temperature for the month of November in the UK since 1985, and the coldest November night in Wales on record. Shawbury, in Shropshire was hit with ; Lough Fea, in Northern Ireland was left hopping with ; and Church Fenton in North Yorkshire chilled out with a nocturnal low of .\n\nThe severe winter weather resulted in school closures as Northern Scotland, North East England and parts of North and East Yorkshire were blanketed in up to of snow. Sub-zero overnight temperatures were recorded across the country, with the coldest place being at Carterhouse in the Scottish Borders at while Benson in Oxfordshire fell to . By the middle of 27 November, up to of snow fell in parts of Staffordshire overnight while residents in the Black Country also woke up to a covering today with warnings of way with blizzards expected in the region with a predicted snowfall of over the next few days. Scotland saw the temperature at Loch Glascarnoch fall to , a new record low for November in Scotland.\n\nNorthern Ireland hit a new low of at Lough Fea, Co Tyrone, and Scotland set a November record at Loch Glascarnoch, with as snow fell in Scotland, Northern Ireland and North East England. Topcliffe in North Yorkshire saw a temperature of on 2 December, making it the coldest night recorded in Yorkshire.\n\nThe city of Edinburgh had of snowfall. As a result of the chaos the city's airport was closed. It was also reported that around 3,000 homes in the Tayside and Perth areas were without power.\n\nAA said that 29 November 2010 was one of its busiest times in its entire 105-year history as they were called to more than 200,000 broken down drivers across the UK.\n\nThe morning of 30 November saw about of snow hit Banburyshire and Oxfordshire with snow falling many more parts of the UK, but much less fall in Scotland and Northern Ireland. Over 1,000 schools were closed across the Kingdom, mostly in Scotland and the north of England, with 50 schools in Northumberland, County Durham, and Tyne and Wear closed.\n\nThe evening of 30 November and the whole of 1 December saw extremely heavy snowfall hit Sheffield, one of the worst affected cities in England. All transport was cancelled throughout the city and the surrounding areas and many people were snowed in their homes. On that day, of snow fell on top of a blanket of of snow.\n\nThe evening of 1 December and the morning of 2 December there was extremely heavy snow in Southern England, especially on the South Coast, of snow was recorded throughout East and West Sussex with the South Downs receiving close to of snow.\n\nMore heavy snowfall was reported in Southern England on 17/18 December, with parts of Greater London reporting nearly . Snow even settled in Central London, causing travel disruptions to the city's Underground system and airports.\n\nThis was the coldest December recorded in the UK since 1890.\n\nThe lowest temperature of the winter so far is recorded at Altnaharra in the Scottish Highlands at 10 am on 2 December.\n\nIreland was first hit by the snow on the morning of 27 November 2010. The morning of Saturday, 27 November, saw Ireland freezing in what could be a rather costly cold snap as it emerged that the extreme weather earlier in 2010 had cost a colossal €297m in insurance payouts, due to the snow causing damage was also caused to homes and other buildings all over the country. The Irish Insurance Federation revealed there were 22,450 claims from the public, the vast majority of which involved snow or ice damage to people's homes. Road conditions in the Dublin area were made dangerous after freezing temperatures and snow led to icy surfaces and paths were made Icy by frost. A prolonged freeze disrupted businesses and schools as travel was made hazardous. The DART and the northern commuter and Maynooth commuter lines were not running and Belfast and Rosslare train services out of Dublin were also affected. The main runway at Dublin Airport closed due to snow and ice for most of the day.\n\nThe extreme weather was reminiscent of the winter storms of 2009/2010, which were the worst in recent Irish history. Met Éireann said the areas worst affected by the overnight snow were eastern parts of Leinster, Donegal and Connacht and said the bad weather was expected to last for up to a week, with depths of up to in places. Met Éireann put a weather warning in place and more snow for the counties in the east, north and north west. They also put a gale and small craft warning in effect.\n\nHeavy snowfall was reported across higher parts of the country on 26 November and in lowland Switzerland on the next day. Heavy snow caused Geneva Airport to close. The temperatures hovered between for the next 8 days, with 30 November to 1 December having a nighttime temperature of . Similar weather was predicted for the Austrian Tyrol.\n\nA record was measured 30 November in Orléans, France. This is the lowest temperature in November at sea level in France. Ice and snow led to power outages in Orléans.\n\nTrondheim, Norway's third-biggest city, located in Central Norway, experienced the coldest November since the beginning of recording temperatures in 1788. Especially the last week of November saw temperatures below normal.\n\nSevere blizzards hit southern Sweden and Denmark, affecting flights at Copenhagen Airport. Over of snow fell. Helsinki and Stockholm recorded their coldest November nights on record, at .\n\nAccording to thelocal.se Sweden had its coldest and snowiest start to the winter in 100 years. The east coast of Sweden experienced heavy snowfall in November/December, coming from very cold air influenced by the still open water of the Baltic Sea and the Gulf of Bothnia.\n\nOn 22 December 2010, there were 56 centimeters of snow in Helsinki, Finland. Throughout the winter, the snow had caused a one-minute increase in the average response time of the city's fire and rescue department.\nIn Sweden there was severe snowfall near Christmas, especially in Scania on 23 December, on Öland on 24 December, and in South Norrland coast on 26 December. These were very important Christmas travel days, and caused heavy travel disturbances, especially for trains. Worst hit was Bornholm, Denmark on 23 December, which got all roads unpassable for days, which meant that arriving visitors had to celebrate Christmas in schools or hotels, Bornholm got to 130 cm snow in December 2010, the deepest snow cover on record in Denmark's weather history.\n\nIn January, the temperatures in Sweden were warmer than normal, Malmö had 0 °C, Stockholm −3 °C and Luleå -11 °C on average. South Sweden had 0–1 °C above normal, but south Norrland had 2–4 °C above normal. The snow cover mostly remained in all the country during this month.\n\nIn February, the temperatures in Sweden were again colder than normal, Malmö had −1 °C, Stockholm −5 °C and Luleå -16 °C on average. South Sweden had 0–2 °C below normal, but north Norrland had 3–5 °C below normal. There was a storm on 8 February over Skåne and neighbourhood with a sustained wind of up to 30 m/s. The temperature reached −42.6 °C in Nikkaluokta, the coldest in Sweden this winter.\n\nNovember started with very high temperatures across Estonia, temperatures rose up to 13 °C, but a week later a cold front crossed Estonia bringing storms with up to 20 cm of snowfall, but it melted very fast. End of November temperatures went down to -27 °C. \nOn 9 and 10 December snowstorm Monika brought severe snowfall and blizzards to Estonia. Little town of Väike- Maarja received in 24 hours up to 40 cm of snow. Six hundred cars with people were trapped on the road in Padaoru, Lääne- Nigula. Many of Tallinn airport's flights were cancelled because of heavy snowfall. Schools were closed. Another snowstorm Scarlett brought a very heavy snowfall on 24 December, again many people were trapped and the roads were closed. Between those 2 storms, lake effect brought more snow, so by the end of the December many places in Estonia had up to 50 to 80 cm of snow. The year-lowest temperature was in Jõgeva -33.4 °C, on 18 February.\n\nGermany experienced its coldest December since 1969. The average temperature was −3.7 °C which was 4.5 °C below the long-term mean. This made December 2010 the fourth-coldest of the last 120 years. Only 1890, 1969 and 1933 were colder. The cold was strongest in the northern half of Germany with deviations from the long-term mean between −5 °C and −6 °C. The month was slightly milder in the south, due to two notable periods of thawing and maximum temperatures of up to +15 °C around 8 December and shortly before Christmas.\n\nAdditionally to the cold, about 60% of the German Weather Service's weather stations below 200 metres' elevation saw record snow falls for December. Notably, Aachen, in Germany's west, had 36 cm of snow, breaking the old record of 25 cm. The heavy snowfalls between 22 and 25 December provided for a white Christmas for all of Germany – the first time since 1981. On 26 December, Gera had a snow cover of 70 cm, also a new record. However, these snowfalls and freezing rain caused chaotic conditions for those travelling home for Christmas. Similar conditions had already occurred at the beginning of the month. At Germany's busiest airport in Frankfurt, many flights were cancelled for the second day running on 2 December due to the severe weather conditions. Munich Airport and rail transport were also affected.\n\nEighteen people died from exposure in Poland, where temperatures dropped as low as .\n\nOn 18 December, Belgium had recorded 17 days with snow for the season, an absolute record. A church in Diepenbeek collapsed on 24 December under the weight of the snow.\n\nMany cities in Sweden had their coldest December since the beginning of recording temperatures. The average temperature in Gothenburg was −6.2 °C. which was 2.5 °C colder than the old record. In Örebro the coldest temperature in December since 1886 with −26.6 °C was measured. The coldest temperature that was notified in December in Sweden was −42.1 °C in Nikkaluokta.\n\nTrondheim, Norway's third-biggest city, experienced the coldest November since the beginning of recording temperatures in 1788. Especially the last week of November saw temperatures below normal. The average temperature in Oslo was −1.7 °C in November 2010, the coldest since 1968 which had −2.1 °C. The record low for Norway in November 2010 was measured in Karasjok in Finnmark, the northernmost county, on 27 November, showing .\n\nCastlederg, in western Northern Ireland, recorded a new record-low temperature of on 19 December 2010. The previous Northern Ireland record was .\n\nThe same location then beat this record on the morning of 23 December, reaching .\n\nMuch different temperatures occurred in the beginning of December in the Balkans, where heavy rain caused floodings in Bosnia and Herzegovina, Croatia, Montenegro, and Serbia. The River Neretva reached its highest level in the past 50 years. The rivers Lim and Drina both caused severe flooding. The River Drina reached a high point of around eleven times its normal level, and the Lim flooded around 250 acres of land, as well as around 50 buildings. A state of emergency was declared in parts of Albania because of floodings near the cities of Shkodra and Durrës. In Bulgaria, temperatures in the end of November and beginning of December went to .\n\nIn response to the flooding, upwards of 12,000 people were evacuated in Albania from the areas most affected. An estimated 2,600 houses were flooded, while some 7,500 more damaged. Around 1,400 Albanian military and police personnel were deployed to assist in the evacuations. In addition to domestic resources, NATO dispatched five helicopters from Greece and Turkey, while Italy delivered 25 tonnes of supplies to the country. According to Albanian interior minister Lulzim Basha, the flooding was the worst \"in living memory.\" In Montenegro, upwards of a thousand soldiers were deployed in response to the flooding.\n"}
{"id": "15415145", "url": "https://en.wikipedia.org/wiki?curid=15415145", "title": "Woodchips", "text": "Woodchips\n\nWoodchips are small to medium sized pieces of wood formed by cutting or chipping larger pieces of wood such as trees, branches, logging residues, stumps, roots, and wood waste.Woodchips may be used as a biomass solid fuel and are raw material for producing wood pulp. They may also be used as an organic mulch in gardening, landscaping, restoration ecology, bioreactors for denitrification and as a substrate for mushroom cultivation.\n\nThe process of making woodchips is called wood chipping and is done using a wood chipper. The types of woodchips formed following chipping is dependent on the type of wood chipper used and the material from which they are made. Woodchip varieties include: forest chips (from forested areas), wood residue chips (from untreated wood residues, recycled wood and off-cuts), sawing residue chips (from sawmill residues), and short rotation forestry chips (from energy crops).\n\nThe raw materials of woodchips can be pulpwood, waste wood, and residual wood from agriculture, landscaping, logging, and sawmills. Woodchips can also be produced from remaining forestry materials including tree crowns, branches, unsaleable materials or undersized trees.Forestry operations provide the raw materials needed for woodchip production. Almost any tree can be converted into woodchips, however, the type and quality of the wood used to produce woodchips depends largely on the market. Softwood species, for instance, tend to be more versatile for use as woodchips than hardwood species because they are less dense and faster growing.\n\nA wood chipper is a machine used for cutting wood into smaller pieces (chips). There are several types of wood chippers, each having a different use depending on the type of processing the woodchips will undergo.\n\nWoodchips used for chemical pulp must be relatively uniform in size and free of bark. The optimum size varies with the wood species. It is important to avoid damage to the wood fibres as this is important for the pulp properties. For roundwood it is most common to use disk chippers. A typical size of the disk is 2.0 – 3.5 m in diameter, 10 – 25 cm in thickness and weight is up to 30 tons. The disk is fitted with 4 to 16 knives and driven with motors of ½ - 2 MW. Drum chippers are normally used for wood residuals from saw mills or other wood industry.\n\nThere are four potential methods to move woodchips: pneumatic, conveyor belt, hopper with direct chute, and batch system (manual conveyance).\n\nA disk wood chipper features a flywheel made of steel and chopping blades with slotted disks. The blades slice through the wood as the material is fed through the chute. Knives located in the throat of the chipper cuts the wood in the opposite direction. The design is not as energy efficient as other styles but produces consistent shapes and sizes of woodchips.\n\nA drum wood chipper has a rotating parallel-sided drum attached to the engine with reinforced steel blades attached in a horizontal direction. Wood is drawn into the chute by gravity and the rotation of the drum where it is broken up by the steel blades. The drum type is noisy and creates large uneven chips but are more energy efficient than the disk type.\n\nA screw-type wood chipper contains a conical, screw-shaped blade. The blade rotation is set parallel to the opening so wood is pulled into the chipper by the spiral motion. Screw-type, also called high-torque rollers, are popular for residential use due to being quiet, easy to use and safer than disk and drum types.\n\nWoodchips are used primarily as a raw material for technical wood processing. In industry, processing of bark chips is often separated after peeling the logs due to different chemical properties.\n\nOnly the heartwood and sapwood are useful for making pulp. Bark contains relatively few useful fibres and is removed and used as fuel to provide steam for use in the pulp mill. Most pulping processes require that the wood be chipped and screened to provide uniform sized chips.\n\nWoodchips are also used as landscape and garden mulch, producing benefits such as water conservation, weed control, reducing and preventing soil erosion, and for supporting germination of native seeds and acorns in habitat restoration projects. As the ramial chipped wood decomposes it improves the soil structure, permeability, bioactivity, and nutrient availability of the soil. Woodchips when used as a mulch are at least three inches thick.\n\nWoodchips can be reprocessed into an extremely effective playground surfacing material, or impact-attenuation surface. When used as playground surfacing (soft fall, cushion fall, or play chip, as it is sometimes known), woodchips can be very effective in lessening the impact of falls from playground equipment. When spread to depths of one foot (30 centimetres) playground woodchips can be effective at reducing impacts in falls up to 11 feet (3 meters). Playground woodchips are also an environmentally friendly alternative to rubber type playground surfaces.\n\nWoodchips can also be used to infuse flavor and enhance the smoky taste to barbecued meats and vegetables. Several different species of wood can be used depending on the type of flavor wanted. For a mild, sweet fruity flavor, apple wood can be used while hickory gives a smoky, bacon-like flavor. Other different types of wood used are cherry, hickory, mesquite and pecan.\n\nWoodchip piles at the edge of a field can inhibit nitrates from running off into water tiles. They are a simple measure for farmers to reduce nitrate pollution of the watershed without them having to change their land management practice. A 2011 study showed that most of the nitrate removal was due to heterotrophic denitrification. A 2013 experiment from Ireland showed that after 70 days of startup, a woodchip pile loaded with liquid pig manure at 5 L/m/day removed an average of 90% of nitrate in the form of ammonium after one month. A January 2015 study from Ohio State University showed very low nitrogen gas, i.e. greenhouse gas emissions from nitrate transformation under the anaerobic conditions of the woodchip bioreactor. Scientists constructed a model for water flow and nitrate removal kinetics which can be used to design denitrification beds. It is unknown if other nutrients like phosphorus or pathogens are affected by the bioreactor as well.\n\nWoodchips have been traditionally used as solid fuel for space heating or in energy plants to generate electric power from renewable energy. The main source of forest chips in Europe and in most of the countries have been logging residues. It is expected that the shares of stumps and roundwood will increase in the future. in the EU, the estimates for biomass potential for energy, available under current 2018 conditions including sustainable use of the forest as well as providing wood to the traditional forest sectors, are: 277 million m, for above ground biomass and 585 million m for total biomass.\n\nThe newer fuel systems for heating use either woodchips or wood pellets. The advantage of woodchips is cost, the advantage of wood pellets is the controlled fuel value. The use of woodchips in automated heating systems, is based on a robust technology.\n\nThe size of the woodchips is particularly important when burning woodchip in small plants. Unfortunately there are not many standards to decide the fractions of woodchip. One standard is the GF60 which is commonly used in smaller plants, including small industries, villas, and apartment buildings. \"GF60\" is known as \"Fine, dry, small chips\". The requirements for GF60 are that the moisture is between 10–30% and the fractions of the woodchips are distributed as follows: 0–3.5mm: <8%, 3.5–30mm: <7%, 30–60 mm: 80–100%, 60–100 mm: <3%, 100–120 mm: <2%.\n\nThe energy content in one cubic metre is normally higher than in one cubic metre wood logs, but can vary greatly depending on moisture. The moisture is decided by the handling of the raw material. If the trees are taken down in the winter and left to dry for the summer (with teas in the bark and covered so rain can't reach to them), and is then chipped in the fall, the woodchips' moisture content will be approximately 20–25%. The energy content, then, is approximately 3.5–4.5kWh/kg (~150–250 kg/cubic metre).\n\nCoal power plants have been converted to run on woodchips, which is fairly straightforward to do, since they both use an identical steam turbine heat engine, and the cost of woodchip fuel is comparable to coal.\n\nSolid biomass is an attractive fuel for addressing the concerns of the energy crisis and climate change, since the fuel is affordable, widely available, close to carbon neutral and thus climate-neutral in terms of carbon dioxide (CO2), since in the ideal case only the carbon dioxide which was drawn in during the tree’s growth and stored in the wood is released into the atmosphere again.\n\nCompared to the solid waste disposal problems of coal and nuclear fuels, woodchip fuel's waste disposal problems are less grave; in a study from 2001 fly ash from woodchip combustion had 28.6 mg cadmium/kg dry matter. Compared to fly ash from burning of straw, cadmium was bound more heavily, with only small amounts of cadmium leached. It was speciated as a form of cadmium oxide, cadmium silicate (CdSiO); authors noted that adding it to agricultural or forest soils in the long-term could cause a problem with accumulation of cadmium.\n\nLike coal, wood combustion is a known source of mercury emissions, particularly in northern climates during winter. The mercury is both gaseous as elemental mercury (especially when wood pellets are burned) or mercury oxide, and solid PM2.5 particulate matter when untreated wood is used.\n\nWhen wood burning is used for space heating, indoor emissions of 1,3-butadiene, benzene, formaldehyde and acetaldehyde, which are suspected or known carcinogenic compounds, are elevated. The cancer risk from these after exposure to wood smoke is estimated to be low in developed countries.\n\nCertain techniques for burning woodchips result in the production of biochar – effectively charcoal – which can be either utilised as charcoal, or returned to the soil, since wood ash can be used as a mineral-rich plant fertilizer. The latter method can result in an effectively carbon-negative system, as well as acting as a very effective soil conditioner, enhancing water and nutrient retention in poor soils.\n\nUnlike the smooth, uniform shape of manufactured wood pellets, woodchip sizes vary and are often mixed with twigs and sawdust. This mixture has a higher probability of jamming in small feed mechanisms. Thus, sooner or later, one or more jams is likely to occur. This reduces the reliability of the system, as well as increasing maintenance costs. Despite what some pellet stove manufacturers may say, researchers who are experienced with woodchips, say they are not compatible with the 2 inch (5 cm) auger used in pellet stoves.\n\nWood is occasionally used to power engines, such as steam engines, Stirling engines, and Otto engines running on woodgas. As of 2008, these systems are rare, but as technology and the need for it develops, it is likely to be more common in the future. For the time being, wood can be increasingly used for heating applications. This will reduce the demand for heating oil, and thereby allow a greater percentage of fuel oil to be used for applications such as internal combustion engines, which are less compatible with wood based fuel and other solid biomass fuels. Heating applications generally do not require refined or processed fuels, which are almost always more expensive.\n\nWoodchips are similar to wood pellets, in that the movement and handling is more amenable to automation than cord wood, particularly for smaller systems. Woodchips are less expensive to produce than wood pellets, which must be processed in specialized facilities. While avoiding the costs associated with refinement, the lower density and higher moisture content of woodchips reduces their calorific value, substantially increasing the feedstock needed to generate an equivalent amount of heat. Greater physical volume requirements also increase the expense and emissions impact of trucking, storing and/or shipping the wood.\n\nWoodchips are less expensive than cord wood, because the harvesting is faster and more highly automated. Woodchips are of greater supply, partly because all parts of a tree can be chipped, whereas small limbs and branches can require substantial labor to convert to cord wood. Cord wood generally needs to be \"seasoned\" or \"dry\" before it can be burned cleanly and efficiently. On the other hand, woodchip systems are typically designed to cleanly and efficiently burn \"green chips\" with very high moisture content of 43–47% (wet basis). (see gasification and woodgas)\n\nCompared to conventional timber harvesting, woodchip harvesting has a greater impact on the environment, since a larger proportion of biomass is removed. Increased use of woodchips can have negative effects on the stability and long-term growth of the forests in which they're removed from. For instance, chipping of trees from forests has been shown to increase the removal of plant nutrients and organic matter from an ecosystem, thereby reducing both the nutrients and humus content of the soil. One option to balance the negative effects of woodchip harvesting is to return the woodchip ash to the forest which would restore some of the lost nutrients back into the soil.\n\nIf woodchips are harvested as a by-product of sustainable forestry practices, then this is considered a source of renewable energy. On the other hand, harvesting practices, such as clearcutting large areas, are highly damaging to forest ecosystems.\n\nTheoretically, whole-tree chip harvesting does not have as high a solar energy efficiency compared to short rotation coppice; however, it can be an energy-efficient and low-cost method of harvesting. In some cases, this practice may be controversial when whole-tree harvesting may often be associated with clear cutting and perhaps other questionable forestry practices.\n\nWoodchips and bark chips can be used as bulking agents in industrial composting of municipal biodegradeable waste, particularly biosolids. Woodchip biomass does not have the waste disposal issues of coal and nuclear power, since wood ash can be used directly as a mineral-rich plant fertilizer.\n\nWoodchip harvesting can be used in concert with creating man made firebreaks, which are used as barriers to the spread of wildfire. Undergrowth coppice is ideal for chipping, and larger trees may be left in place to shade the forest floor and reduce the rate of fuel accumulation.\n\nCurrently, domestic or residential sized systems are not available in products for sale on the general market. Homemade devices have been produced, that are small-scale, clean-burning, and efficient for woodchip fuels. Much of the research activity to date, has consisted of small budget projects that are self-funded. The majority of funding for energy research has been for liquid biofuels.\n\nWoodchip costs usually depend on such factors as the distance from the point of delivery, the type of material (such as bark, sawmill residue or whole-tree chips), demand by other markets and how the wood fuel is transported. Chips delivered directly to the (powerplant) station by truck are less expensive than those delivered ... and shipped by railcar. The range of prices is typically between US$18 to US$30 per (wet)-ton delivered.\n\nIn 2006, prices were US$15 and US$30 per wet-ton in the northeast.\n\nIn the 20 years leading up to 2008, prices have fluctuated between US$60–70/oven-dry metric ton (odmt) in the southern states, and between US$60/odmt and US$160/odmt in the Northwest.\n\nWood chips have been used as a source of single-dwelling heating in Canada since the early days of settling but the development of oil and natural gas has dramatically decreased its usage. Most of the wood chip usage is by installations such as schools, hospitals and prisons. Prince Edward Island (PEI) has the most wood-chip plants due to high electricity rates and subsidies from the federal government. Nova Scotia has a 2.5 MW wood chip burning system that provides power to a textile factory as well as systems that provide power to a poultry processing plant, two hospitals and an agricultural college.\n\nThe University of New Brunswick operates a wood chip burning furnace system to supply heat to the university, several industrial buildings, an apartment complex and a hospital. Usage of wood chips for heat is low in Quebec due to low hydroelectricity rates but a small town is using wood chips as an alternative to road salt for icy roads. EMC3 Technologies started producing wood chips coated with magnesium chloride in November 2017 for the town and has claimed it maintains traction in -30 degrees Celsius compared to regular road salt at -15 degrees Celsius. In Ontario, wood chip operations include a college in Brockville, a few secondary schools in Northern Ontario as well as a chip-fired boiler at the National Forestry Institute in Petawawa. In the late 1980’s, the Ontario provincial government in conjunction with the federal government subsidized building three co-generation plants next to sawmills. The first one was constructed in 1987 in Chapleau followed by a plant built in Cochrane in 1989 and the largest one in Kirkland Lake which was built in 1991.\n\nIn several well wooded European countries (e.g. Austria, Finland, Germany, Sweden) woodchips are becoming an alternative fuel for family homes and larger buildings due to the abundant availability of woodchips, which result in low fuel costs. The European Union is promoting woodchips for energy production in the EU Forest action plan 2007–2011. The total long term potential of woodchips in the EU is estimated to be 913 million m.\n\nAfter a long period of negative scores, the demand of woodchips for paper manufacturing started increasing again. Starting in the last quarter of 2013, orders for printing paper and card board increased before the consumption tax increase then by weakening yen, import of papers like copy paper decreases and export of paper increases, which stimulate paper production in Japan. Softwood chip prices from the United States increased by 12% compared to October 2013 and softwood chip prices from Australia increased by 7%.\n\n\n"}
