{"id": "24961570", "url": "https://en.wikipedia.org/wiki?curid=24961570", "title": "AC module", "text": "AC module\n\nAn AC module is a photovoltaic module which has a small AC inverter mounted onto its back side which produces AC power with no external DC connector.\n\nAC modules are defined by Underwriters Laboratories as the smallest and most complete system for harvesting solar energy.\n"}
{"id": "13458964", "url": "https://en.wikipedia.org/wiki?curid=13458964", "title": "ARC21", "text": "ARC21\n\nARC21 is a local government body in Northern Ireland that is tasked with coordination of the waste management and recycling services in the North East of Northern Ireland. The ARC21 waste management region includes the city, borough and district councils of:\n\n\nARC21 is accountable to the Northern Ireland Department for the Environment.\n\n"}
{"id": "32185392", "url": "https://en.wikipedia.org/wiki?curid=32185392", "title": "Alpha factor", "text": "Alpha factor\n\nThe α-factor is used to predict the solid–liquid interface type of a material during solidification.\n\nAccording to John E. Gruzleski in his book Microstructure Development During Metalcasting (1996):\n\nα = (L/k*T)*(η/v)\n\nwhere \nL is latent heat of fusion\nk is Boltzmann’s constant\nT is equilibrium freezing temperature\nη is the number of nearest neighbours an atom has in the interface plane\nv is the number of nearest neighbours in the bulk solid\n\nSince L/T = ΔS\n\nwhere ΔS is the molar entropy of fusion of the material\n\nα = ΔS/k(η/v)\n\nAccording to Martin Glicksman in his book Principles of Solidification : An Introduction to Modern Casting and Crystal Growth Concepts(2011):\n\nα = ΔS/R(η/Z)\n\nwhere R is the universal gas constant,\n(η/Z) is similar to previous, always 1/4<(η/Z)<1\n"}
{"id": "25776751", "url": "https://en.wikipedia.org/wiki?curid=25776751", "title": "Aurora Energy (New Zealand)", "text": "Aurora Energy (New Zealand)\n\nAurora Energy is an electricity distribution company in Otago, New Zealand. Aurora Energy is owned by Dunedin City Holdings Limited on behalf of the Dunedin City Council.\n\nAurora Energy is New Zealand's sixth largest electricity distributor. Over 85,000 premises in Dunedin and Central Otago receive power from Aurora Energy.\n\nAurora Energy's distribution network is supplied from Transpower's national grid at five grid exit points (GXPs): Halfway Bush and South Dunedin for the Dunedin network, and Clyde, Cromwell and Frankton for the Central Otago network. Most of the network's subtransmission is at 33,000 volts with the major exception of the lines feeding Wanaka from Cromwell GXP, which operate at 66,000 volts. Distribution is at 6600 volts in the Dunedin urban area and the Clyde area, and at 11,000 volts elsewhere.\n\n\n"}
{"id": "140459", "url": "https://en.wikipedia.org/wiki?curid=140459", "title": "Base (chemistry)", "text": "Base (chemistry)\n\nIn chemistry, bases are substances that, in aqueous solution, release hydroxide (OH) ions, are slippery to the touch, can taste bitter if an alkali, change the color of indicators (e.g., turn red litmus paper blue), react with acids to form salts, promote certain chemical reactions (base catalysis), accept protons from any proton donor or contain completely or partially displaceable OH ions. Examples of bases are the hydroxides of the alkali metals and the alkaline earth metals (NaOH, Ca(OH), etc., see alkali hydroxide and alkaline earth hydroxide).\n\nThese particular substances produce hydroxide ions (OH) in aqueous solutions, and are thus classified as Arrhenius bases. \nFor a substance to be classified as an Arrhenius base, it must produce hydroxide ions in an aqueous solution. Arrhenius believed that in order to do so, the base must contain hydroxide in the formula. This makes the Arrhenius model limited, as it cannot explain the basic properties of aqueous solutions of ammonia (NH) or its organic derivatives (amines). There are also bases that do not contain a hydroxide ion but nevertheless react with water, resulting in an increase in the concentration of the hydroxide ion. An example of this is the reaction between ammonia and water to produce ammonium and hydroxide. In this reaction ammonia is the base because it accepts a proton from the water molecule. Ammonia and other bases similar to it usually have the ability to form a bond with a proton due to the unshared pair of electrons that they possess. In the more general Brønsted–Lowry acid–base theory, a base is a substance that can accept hydrogen cations (H)—otherwise known as protons. In the Lewis model, a base is an electron pair donor.\n\nIn water, by altering the autoionization equilibrium, bases yield solutions in which the hydrogen ion activity is lower than it is in pure water, i.e., the water has a pH higher than 7.0 at standard conditions. A soluble base is called an alkali if it contains and releases OH ions quantitatively. However, it is important to realize that basicity is not the same as alkalinity. Metal oxides, hydroxides, and especially alkoxides are basic, and counteranions of weak acids are weak bases.\n\nBases can be thought of as the chemical opposite of acids. However, some strong acids are able to act as bases. Bases and acids are seen as opposites because the effect of an acid is to increase the hydronium (HO) concentration in water, whereas bases reduce this concentration. A reaction between an acid and base is called neutralization. In a neutralization reaction, an aqueous solution of a base reacts with an aqueous solution of an acid to produce a solution of water and salt in which the salt separates into its component ions. If the aqueous solution is saturated with a given salt solute, any additional such salt precipitates out of the solution.\n\nThe notion of a base as a concept in chemistry was first introduced by the French chemist Guillaume François Rouelle in 1754. He noted that acids, which at that time were mostly volatile liquids (like acetic acid), turned into solid salts only when combined with specific substances. Rouelle considered that such a substance serves as a \"base\" for the salt, giving the salt a \"concrete or solid form\".\n\nGeneral properties of bases include:\n\n\nThe following reaction represents the general reaction between a base (B) and water to produce a conjugate acid (BH) and a conjugate base (OH):\n\nThe equilibrium constant, K, for this reaction can be found using the following general equation:\n\nIn this equation, both the base (B) and the extremely strong base (the conjugate base) compete with one another for the proton. As a result, bases that react with water have relatively small equilibrium constant values. The base is weaker when it has a lower equilibrium constant value.\n\nBases react with acids to neutralize each other at a fast rate both in water and in alcohol. When dissolved in water, the strong base sodium hydroxide ionizes into hydroxide and sodium ions:\n\nand similarly, in water the acid hydrogen chloride forms hydronium and chloride ions:\n\nWhen the two solutions are mixed, the and ions combine to form water molecules:\n\nIf equal quantities of NaOH and HCl are dissolved, the base and the acid neutralize exactly, leaving only NaCl, effectively table salt, in solution.\n\nWeak bases, such as baking soda or egg white, should be used to neutralize any acid spills. Neutralizing acid spills with strong bases, such as sodium hydroxide or potassium hydroxide, can cause a violent exothermic reaction, and the base itself can cause just as much damage as the original acid spill.\n\nBases are generally compounds that can neutralize an amount of acids. Both sodium carbonate and ammonia are bases, although neither of these substances contains groups. Both compounds accept H when dissolved in protic solvents such as water:\n\nFrom this, a pH, or acidity, can be calculated for aqueous solutions of bases. Bases also directly act as electron-pair donors themselves:\n\nA base is also defined as a molecule that has the ability to accept an electron pair bond by entering another atom's valence shell through its possession of one electron pair. There are a limited number of elements that have atoms with the ability to provide a molecule with basic properties. Carbon can act as a base as well as nitrogen and oxygen. Fluorine and sometimes rare gases possess this ability as well. This occurs typically in compounds such as butyl lithium, alkoxides, and metal amides such as sodium amide. Bases of carbon, nitrogen and oxygen without resonance stabilization are usually very strong, or superbases, which cannot exist in a water solution due to the acidity of water. Resonance stabilization, however, enables weaker bases such as carboxylates; for example, sodium acetate is a weak base.\n\nA strong base is a basic chemical compound that can remove a proton (H) from (or \"deprotonate\") a molecule of even a very weak acid (such as water) in an acid-base reaction. Common examples of strong bases include hydroxides of alkali metals and alkaline earth metals, like NaOH and , respectively. Due to their low solubility, some bases, such as alkaline earth hydroxides, can be used when the solubility factor is not taken into account. One advantage of this low solubility is that \"many antacids were suspensions of metal hydroxides such as aluminum hydroxide and magnesium hydroxide.\" These compounds have low solubility and have the ability to stop an increase in the concentration of the hydroxide ion, preventing the harm of the tissues in the mouth, oesophagus, and stomach. As the reaction continues and the salts dissolve, the stomach acid reacts with the hydroxide produced by the suspensions. Strong bases hydrolyze in water almost completely, resulting in the leveling effect.\" In this process, the water molecule combines with a strong base, due to the water's amphoteric ability; and, a hydroxide ion is released. Very strong bases can even deprotonate very weakly acidic C–H groups in the absence of water. Here is a list of several strong bases:\n\n\nThe cations of these strong bases appear in the first and second groups of the periodic table (alkali and earth alkali metals).\n\nAcids with a p\"K\" of more than about 13 are considered very weak, and their conjugate bases are strong bases.\n\nGroup 1 salts of carbanions, amides, and hydrides tend to be even stronger bases due to the extreme weakness of their conjugate acids, which are stable hydrocarbons, amines, and dihydrogen. Usually these bases are created by adding pure alkali metals such as sodium into the conjugate acid. They are called \"superbases\", and it is impossible to keep them in water solution because they are stronger bases than the hydroxide ion. As such, they deprotonate the conjugate acid water. For example, the ethoxide ion (conjugate base of ethanol) in the presence of water undergoes this reaction.\n\nExamples of superbases are:\n\n\nWhen a neutral base forms a bond with a neutral acid, a condition of electric stress occurs. The acid and the base share the electron pair that formerly only belonged to the base. As a result, a high dipole moment is created, which can only be destroyed by rearranging the molecules.\n\nA weak base is one which does not fully ionize in an aqueous solution, or in which protonation is incomplete.\n\nExamples of solid bases include:\n\nDepending on a solid surface's ability to successfully form a conjugate base by absorbing an electrically neutral acid, the basic strength of the surface is determined. \"The number of basic sites per unit surface area of the solid\" is used to express how much base is found on a solid base catalyst. Scientists have developed two methods to measure the amount of basic sites: titration with benzoic acid using indicators and gaseous acid adsorption. A solid with enough basic strength will absorb an electrically neutral acid indicator and cause the acid indicator's color to change to the color of its conjugate base. When performing the gaseous acid adsorption method, nitric oxide is used. The basic sites are then determined using the amount of carbon dioxide than is absorbed.\n\nBasic substances can be used as insoluble heterogeneous catalysts for chemical reactions. Some examples are metal oxides such as magnesium oxide, calcium oxide, and barium oxide as well as potassium fluoride on alumina and some zeolites. Many transition metals make good catalysts, many of which form basic substances. Basic catalysts have been used for hydrogenations, the migration of double bonds, in the Meerwein-Ponndorf-Verley reduction, the Michael reaction, and many other reactions. Both CaO and BaO can be highly active catalysts if they are treated with high temperature heat.\n\n\nThe number of ionizable hydroxide (OH-) ions present in one molecule of base is called the acidity of bases. On the basis of acidity bases can be classified into three types: monoacidic, diacidic and triacidic.\n\nWhen one molecule of a base via complete ionization produces one hydroxide ion, the base is said to be a monoacidic base. Examples of monoacidic bases are:\n\nSodium hydroxide, potassium hydroxide, silver hydroxide, ammonium hydroxide, etc.\n\nWhen one molecule of base via complete ionization produces two hydroxide ions, the base is said to be diacidic. Examples of diacidic bases are:\n\nBarium hydroxide, magnesium hydroxide, calcium hydroxide, zinc hydroxide, iron(II) hydroxide, tin(II) hydroxide, lead(II) hydroxide, copper(II) hydroxide, etc.\n\nWhen one molecule of base via complete ionization produces three hydroxide ions, the base is said to be triacidic. Examples of triacidic bases are:\n\nAluminium hydroxide, ferrous hydroxide, Gold Trihydroxide,\n\nThe concept of base stems from an older alchemical notion of \"the matrix\":\n\n\n"}
{"id": "9581910", "url": "https://en.wikipedia.org/wiki?curid=9581910", "title": "Bernard S. Baker", "text": "Bernard S. Baker\n\nDr. Bernard S. Baker (June 26, 1936 – June 21, 2004) was born in Philadelphia and a resident of Bethel, Connecticut in the United States. He was a pioneer in the field of electrochemistry and his career spanned 45 years. He was a founder and served as president, chief executive officer and chairman of Energy Research Corporation (now called FuelCell Energy, Inc., in Danbury, Connecticut), developer and manufacturer of direct fuel cells (MCFC) used to generate electric power. Power plants based on his concepts are providing electricity in distributed generation locations throughout the world.\n\nBaker was known worldwide as an expert in electrochemical systems. He directed research in and development of various electrochemical power generation devices, including different types of fuel cells, batteries and hybrid systems. Baker's expertise encompassed fundamental research as well as the technological, engineering, system and marketing aspects of these systems, including \"direct fuel cells\", which can process hydrocarbon fuels such as natural gas without an external reformer.\n\nBaker received his bachelor's and master's degrees in chemical engineering from the University of Pennsylvania and was a post-graduate Fulbright fellow at the Laboratory for Electrochemistry of the University of Amsterdam, before earning a doctorate from the Illinois Institute of Technology in 1969. His doctoral thesis was also on fuel cell heat transfer and internal reforming.\n\nBefore joining Energy Research Corporation, Baker was director of basic sciences at the Institute of Gas Technology in Chicago, where he directed research in the area of energy conversion and fuel cells. Before that, he was senior scientist at Lockheed Aircraft Corporation, Missiles & Space Division, where he was responsible for research on carbonate fuel cell systems and electrochemical kinetic studies.\n\nBaker is a major contributor to the field of fuel cell research, development and commercialization. He was issued 20 U.S. patents relating to fuel cells and other electrochemical systems. He authored more than 100 publications, including technical papers, books and symposia proceedings on the subject of fuel cells.\n\nIn 1999, he received the Grove Medal, which was presented in conjunction with the Sixth Grove Fuel Cell Symposium in London and is awarded to acknowledge an individual or company that has made valuable contributions toward the development and success of fuel cell technology.\n\nAt the time of the award, the chairman of the Grove Symposium Steering Committee said: \"Dr. Bernard S. Baker devoted his entire professional career to develop and promote fuel cells. In the beginning as a scientist, finally as a top manager he dealt with various fuel cells including alkaline, phosphoric acid and molten carbonate technologies. Dr. Baker personifies a remarkable combination of scientific capabilities and management skills. This unique blend enabled him not only to have many patents on fuel cell related inventions and to issue more than 100 fuel cell publications but to build up one of the world wide leading companies in the carbonate fuel cell technology.\"\n\nBaker received the Cecil J. Previdi Award for Entrepreneurial Spirit and Business Leadership in 1995. He was a Ralph E. Peck lecturer at the Illinois Institute of Technology in 1994.\n\n"}
{"id": "8195160", "url": "https://en.wikipedia.org/wiki?curid=8195160", "title": "Blue Norther (weather)", "text": "Blue Norther (weather)\n\nA Blue Norther, also known as a Texas Norther, is a fast moving cold front in the plains area of the U.S. Midwest, marked by a sudden drop in temperature, heavy precipitation, and dark blue skies moving rapidly. The phrase originated within Texas, where the land is very flat, perhaps making the approaching front seem darker and more threatening. The cold front originates from the north, hence the \"norther,\". The interaction between the warm causing cool temperatures and precipitation.\n\nBecause the Midwest lies to the north of the Gulf of Mexico and to the east of the Rocky Mountains in the zone of the westerlies during the winter, temperatures can closely follow the sun, and highs that precede a Texas norther can, reach 85 °F in January and 90 °F under bright sunlight in nearly-calm conditions before the cold front passes through. Winds turn sharply from the north and become very strong. Windchill due to a combination of cold temperatures and strong winds is dangerous to anyone who is caught unaware and unprepared for it. It exists only from November to early March in Texas\n\nThis phenomenon is rare but is most common in November, when the last vestiges of fall are still clinging on. One of the most famous Blue Northers was the Great Blue Norther of November 11, 1911, which spawned tornadoes and dropped temperatures 40 degrees in only 15 minutes and 67 degrees in 10 hours, a world record.\n\n"}
{"id": "25103981", "url": "https://en.wikipedia.org/wiki?curid=25103981", "title": "Campus carbon neutrality", "text": "Campus carbon neutrality\n\nAll across the world, colleges and universities are looking to a sustainable future by working to become carbon neutral. Universities are taking responsibility for their environmental impact and are working to neutralize those effects. To become carbon neutral, universities are working to reduce their emissions of greenhouse gases, cut their use of energy, use more renewable energy, and emphasize the importance of sustainable energy sources. Universities that have committed to becoming carbon neutral have recognized the threat of global warming and are therefore committing to reverse the trend.\n\n\nSecond Nature is a nonprofit organization that works with higher education leaders to incorporate sustainability in with the foundation of curriculum and practice within colleges and universities. Second Nature was among the first professional associations in sustainability, established in 1993 in Boston, Massachusetts. It is the primary supporter for the American College and University Presidents' Climate Commitment (ACUPCC). Its involvement in ACUPCC includes coordinating cooperative action in developing a carbon offset protocol for universities, as well as coordinating outreach for new presidents to make the commitment to carbon neutrality.\n\nA partner with Second Nature, the Association for the Advancement of Sustainability in Higher Education (AASHE), was established in 2006 as the first professional higher education association for the campus sustainability community. AASHE works with all sectors of a campus in integrating sustainability in with campus governance, education, and research. Professional development is also emphasized through conferences, workshops, and networking. Press releases, blogs, and awards are coordinated to promote carbon reduction and sustainable awareness. AASHE facilitates the reporting system for the ACUPCC, which documents all committed tangible actions and as well as greenhouse gas reports, climate action plans, and progress reports.\n\nThe American College and University Presidents' Climate Commitment (ACUPCC or PCC) is a project of Second Nature and AASHE. The organization consists of the text of an institutional commitment and the supporting informational and marketing materials surrounding the commitment. The mission of the organization is to \"provides a framework and support for America’s colleges and universities to implement comprehensive plans in pursuit of climate neutrality.\" The ACUPCC grew from concept to text during late 2006, and by March 2007 consisted of 157 signatories. The organization was officially unveiled in June 2007, and currently numbers 677 signatories.\n\nThe ACUPCC text can be summarized in two sections: the statement, and the plan. The statement is a four-paragraph declaration that outlines the scientific knowledge regarding global climate change that the signatories accept as fact. It goes on to describe the perceived or predicted institutional and social benefits of taking steps to reduce the emissions of greenhouse gases. The text proposes that by acting to limit their carbon footprint, universities and colleges can act as leader institutions in their communities, and similarly better serve their students and their educational needs. It concludes that, in accordance with the many resultant institutional benefits expected, leaders of educational institutions ought to commit to the steps outlined in the plan. The plan is a three-part outline of certain steps signatory institutions should take \"in pursuit of climate neutrality.\" The three parts of the plan involve developing a comprehensive plan, choosing from a menu of 'tangible actions' to initiate while developing a comprehensive plan, and issuing regular reports to AASHE regarding institutional progress. Comprehensive plan development under the ACUPCC follows three milestones. By the two-month milestone, signatories commit to having created formal institutional structures (and where appropriate, departments) for the express purpose of guiding the development of the plan as well as implementation of the other elements of the commitment. By the one-year milestone, signatories commit to having completed a greenhouse gas inventory, in accordance with the GHG protocol, with accordances in place to update the figures on a bi-yearly basis. The two-year milestone builds upon the greenhouse gas inventory, requiring that signatories create a comprehensive action plan for achieving climate neutrality. This action plan must include certain minimum elements: a target date for complete climate neutrality, intermediate institutional goals and target dates, steps to integrate climate neutrality and related concerns with educational curriculum and research, and institutional mechanisms for tracking intermediate goals above and beyond the specific structures outlined by the ACUPCC. The menu of possible actions includes conforming to LEED Silver standards or greater, purchasing Energy Star products, and encouraging and fostering use of public transportation. Finally, the reporting section of the three-part plan requires signatory institutions to make the details of their plans, inventories, and progress publicly available through AASHE. The final one-sentence paragraph of the text mentions the need to spread the knowledge contained in the commitment and recruit more signatories through institutional diplomacy.\n\nIn 2018, American University became the first university in the United States to reach carbon neutral status.\n\nCornell University has signed the President's Climate Commitment and pledged that its Ithaca campus will have net zero carbon-based emissions by 2050.\nCornell has already completed a CO emissions inventory and estimated that its carbon footprint for the year 2008 was 319,000 metric tons of CO-equivalent.\nCornell has created 19 initiatives to help it achieve its goal of net zero emissions which consist of five main categories: green development, energy conservation, alternative transportation, using renewable energy to replace fossil fuels, and offsetting CO emissions.\nAs part of its green development plan, Cornell wants “new buildings design[ed] to limit energy usage to 50 percent of the industry standard baseline.” In addition, Cornell is planning on converting some of its built land back into open space and decreasing the distance vehicles on campus need to travel.\nThe energy conservation part of the plan includes switching to compact fluorescent light bulbs which would “reduce annual electrical usage for campus lighting by 25 percent.\" The university also plans to reduce the amount of coal it uses by replacing its old steam line piping so that it is more efficient.\nIn order to achieve greener transportation, Cornell wants to reduce its employee use of single-occupant vehicles by 25 percent within 15 years. It wants to achieve this goal by increasing bicycle and transit travel. Cornell also wants to reduce carbon emissions by having business meetings conducted via teleconferences instead of having employees travel for business.\nCornell wants to increase its reliance on renewable energy so it can decrease its reliance on fossil fuels. Cornell wants to use wood to replace “10 percent of the coal burned in the two main Cornell solid fuel boilers.”\nCornell is planning to take steps to offset its carbon emissions in order to become net carbon neutral, but it views this measure as a last resort because it would rather focus on reducing the amount of carbon it produces. The university wants to contribute to afforestation by turning some of its unused cropland and pastures into forests that will absorb CO. Cornell wants to turn per year into forest for the next 10 years, which it estimates to offset 3,800 metric tons of CO per year. \nAdditionally, established at Cornell is the Atkinson Center for a Sustainable Future, an organization focused on multidisciplinary research in energy issues, the environment and economic development.\n\nIn October 2006, University of Florida President Bernard Machen became the first college president to sign the President's Climate Commitment. By signing, the University of Florida committed to developing an action plan for carbon neutrality with the ultimate goal of becoming carbon neutral by 2025. In 2007, the University of Florida held the first carbon neutral college football game in NCAA history with the University of Florida versus Florida State rivalry game. To accomplish this feat, scientists calculated the carbon footprint of the game.\" They then offset the carbon emissions by working with foresters to set aside of rural North Florida land and then manage the land as a pine plantation forest for the next ten years. In 2008, the University of Florida partnered with the Neutral Gator Initiative to offset the carbon emissions from all of the home football games. In 2009, the University upped the standard and set out to offset the entire athletics program in order to have a carbon neutral athletics program.\n\nThe Neutral Gator Initiative was created by the organization Earth Givers along with the University of Florida to educate game day fans about sustainability and carbon neutrality as well as to offset the carbon emissions of the University of Florida's entire athletic program. The goal of Neutral Gator is to help the University of Florida achieve its goal of becoming carbon neutral by 2025. Neutral Gator offsets the carbon emissions through various activities including compact fluorescent light bulb exchanges, weatherization efforts in low-income households, and local as well as regional natural area restoration projects.\n\nAfter signing the President's Climate Commitment, the University of Florida was required to create a climate action plan detailing the steps the university will take to reach its goal of becoming carbon neutral by 2025. The plan focuses on the following:\nClean Air-Cool Planet (CA-CP) was a Northeast-based nonprofit organization that was created in 2001. It partnered with colleges and universities (as well as other civil society institutions) to support their efforts toward carbon footprint reductions and ultimately, carbon neutrality. Before the establishment of the ACUPCC, the presidents of a number of leading Northeast schools signed Memorandums of Understanding with CA-CP, committing themselves to 1) conducting campus-wide greenhouse gas inventories; 2) setting reduction targets and timelines; 3) creating climate action plans; 4) implementing the carbon reduction measures contained in their plans; and 5) communicating with their students, faculty, staff and community members to \"institutionalize\" a culture of sustainability on their campuses. These schools, many of which became leading members of the American College and University Presidents Climate Commitment when it was established in 2006 and/or have driven innovation and demonstrated international leadership on the issue of climate change in other ways.\n\nThe CA-CP Campus Climate Action Toolkit — which included the CA-CP Campus Carbon Calculator developed by CA-CP in partnership with the University of NH — was made available to the public in 2004. The Campus Carbon Calculator was downloaded by users at thousands of different institutions, which include not only colleges and universities but also hospitals, science centers, museums and aquaria, government agencies, k-12 schools and businesses, many of which are actively working toward campus carbon neutrality. More than 90% of U.S. colleges and universities, and many outside the US, use the calculator to do their greenhouse gas inventories and create climate action plans.\n\nIn 2014, the University of New Hampshire's Sustainability Institute took over operations of Clean Air-Cool Planet. The CA-CP Campus Carbon Calculator became a part of the Carbon Management and Analysis Platform, or CarbonMAP. CarbonMAP has many resources and tools to help organizations calculate and track their carbon use online.\n\nAlthough the UNHSI Campus Carbon Calculator is still a widely used tool for calculating a campus' carbon footprint, it has received critiques. Some researchers believe that using carbon emissions as a single metric for campus climate change impact is outdated. They suggest that UNHSI would benefit from combining their carbon calculator with a nitrogen footprint calculator to most precisely estimate a campus' climate impact.\n\nUniversity and college leaders have taken action to reduce campus carbon emissions in the following ways: purchasing renewable energy credits, installing micro-wind turbines, retrocommissioning HVAC systems, install geothermal or solar hot water systems, building cogeneration electrical plants on campus, upgrading lighting on campus, establishing campus green teams, providing free public transportation for students, staff, and faculty, providing financial incentives for carpooling, implementing car-sharing and/or bike-sharing programs, and implementing campus shuttle services. Student clubs and organizations are active in raising awareness in various aspects of carbon neutrality. Climate change, sustainability practice, natural resource conservation, clean energy, and environmental policy are common focuses of student groups working toward carbon neutrality.\n\nThe Sustainable Endowments Institute (SEI) is a nonprofit organization created in 2005 that supports sustainable development for campus operation and endowment policy. The SEI focuses its efforts on various climate change initiatives, such as the College Sustainability Report Card and the Billion Dollar Green Challenge Program.\n\nThe Billion Dollar Green Challenge Program was started by the SEI and 15 other sustainability organizations to encourage colleges and universities to invest in green revolving funds. The program requires that institutions involved collaborate and engage with other institutions in order to reach a collective $1 billion of investments. The funds are then reinvested in green initiatives in participating campuses. As of 2016, 60 institutions are participating in the Billion Dollar Green Challenge and have committed $76 million toward green revolving funds.\n\nFrom 2007-2011, SEI released the College Sustainability Report Card, which identified the leading campus sustainability efforts in the US and Canada. Three hundred thirty two campuses were assessed via independent research and voluntary surveys, which included 48 indicators in nine categories: administration, climate change and energy, food and recycling, green building, student involvement, transportation, endowment transparency, investment priorities, and shareholder engagement. Each category was given a grade (A-F), and an overall grade was awarded to the school. Profiles of the colleges and universities were published on the report card website. Each profile includes the overall grade, categorical grades, and a description of efforts in each category. The website aimed to identify leading colleges and universities, as well as provide information for other schools to learn and adopt sustainability policies used elsewhere.\n\nThe Green Report Card was a highly regarded metric for campus sustainability, being used by researchers to assess environmentally-friendly campuses. However, the Green Report Card also garnered some criticism for being redundant with other ratings of sustainability, specifically the Sustainability Tracking, Assessment & Rating System (STARS) by AASHE.\n\nIn March 2012, the Sustainability Endowments Institute suspended the College Sustainability Report Card program to focus on the Billion Dollar Green Challenge Program.\n\nBrown University is a leader when it comes to both campus sustainability and carbon neutrality. The university was one of only 26 universities across the country to receive an A- on the College Sustainability Report Card. Brown University is working with the Sidney E. Frank Foundation to reduce greenhouse gas emissions. The program, named Community Carbon Use Reduction at Brown has received $350,000 and will help to reduce local carbon emissions through projects that will help meet the needs of Providence, Rhode Island neighborhoods while also reducing greenhouse gas emissions. The projects are created with the goals of: “Providing vibrant opportunities for learning for all those involved, engage non-university groups in thinking about how to increase the sustainability of the greater Providence area and its neighborhoods in a way that is responsive to the needs of the community, and lead to a measurable reduction in greenhouse gas emissions.\" Brown University also has a plan to achieve carbon neutrality on campus. This plan entails carbon offset projects to “reduce greenhouse gas emissions in the local community and purchase carbon offset contracts.\" Additionally, Brown’s Energy and Environmental Advisory Committee is working to retrofit lights, motors, and mechanical equipment in existing buildings in order to reduce the university’s environmental impact and its carbon emissions. The University is also a member of the Rhode Island Student Climate Coalition which comprises five universities and nine high schools within the state of Rhode Island. The coalition is working to have a bill passed that would enable an 80 percent reduction in carbon emissions by the year 2050.\n\nThere are many organizations in coordination with Second Nature and AASHE. Some work to establish carbon-neutral goals within colleges and universities. These include the Disciplinary Associations Network for Sustainability (DANS), the Higher Education Associations Sustainability Consortium, and the US Partnership for Education for Sustainable Development, and ecoAmerica.\n\nReducing carbon emissions has received international attention due to the theory that these emissions are one of the causes of global warming. There are national and internationally binding laws to try to reduce such greenhouse gases. If it were not for this legislation, universities would probably not be pushing for carbon neutrality themselves. Therefore, it is important to understand some of the key legislation when looking at how schools are trying to contribute to the solution.\n\nThe Kyoto Protocol is an international agreement that requires countries to commit to reducing the greenhouse gases they emit into the atmosphere. The protocol was created in Kyoto, Japan on December 11, 1997 and put into practice on February 16, 2005. The protocol targets industrialized countries because they are the ones producing most of the greenhouse gases. Specific chemicals that it wants to reduce are carbon dioxide, methane, nitrous oxide, Sulfur hexafluoride, hydrofluorocarbons, and perfluorocarbons. Thirty-seven industrialized countries and parts of Europe have signed and are bound by the agreement. The United States is not one of them.\nThe Kyoto Protocol allots a specific amount of greenhouse gas emissions that each country can produce, however, countries can also trade emissions with other countries, resulting in the cap and trade system. If one country has not used up all of its allotted emissions, it can sell them to other countries that have gone over their limit.\n\nThe Montreal Protocol on Substances that Deplete the Ozone Layer is an international agreement that seeks to phase out chemicals that damage the ozone layer. The Montreal protocol was signed in 1987, went into practice in 1989, and was further amended in London in 1990 and in Copenhagen in 1992. The protocol set a quicker timetable for industrialized nations to phase out chemicals than it did for developing nations. For example, developed countries were to phase-out chlorofluorocarbons by 1995, but developing countries have until 2010 to phase out this chemical. Other chemicals that it wanted to phase out were halons, carbon tetrachloride, methyl chloroform, hydrochloroforlourocarbons, hydrobromofluorocarbons, and methyl bromide.\n\nThe Greenhouse Gas Protocol was created by the Climate Registry to establish a common method of reporting and measuring greenhouse gas emissions. Thirty-four U.S. states, two Canadian provinces, and one American Indian tribe have joined. Mexico, for example, is using the protocol a guideline for reducing its greenhouse gas emissions. As part of its three-step plan, Mexico will have voluntary emissions reporting, then it will have a greenhouse gas cap, and finally it will follow a cap- and-trade system. In addition, “forty-two of the largest corporations follow GHG Protocol standards.”\n\nThe Talloires Declaration is the first official statement of a commitment to environmental sustainability in higher education. The declaration was made in 1990 at an international conference in France. The Talloires Declaration is an action plan consisting of ten points that aim to incorporate sustainability and environmental literacy in teaching, research, operations and outreach at universities. It was written with the premise that universities play a major role in the process of addressing and reversing the current trends of environmental changes because they prepare most of society's leaders and are therefore uniquely positioned to influence the direction that society takes.\nBy signing the Talloires Declaration, a university agrees to:\nThe Climate Neutral Network is “an initiative led by the United Nations Environment Programme to promote global action to de-carbonize our economies and societies.\" The initiative involves universities, colleges, and other academic institutions worldwide and its aim is to work together in order to transform so that they can have a zero emission future. Like the Presidential Climate Commitment, the Climate Neutral Network is designed with the premise that colleges and universities are vital components in addressing climate change \"because they can model climate neutrality on their campuses, and they can teach their students skills and knowledge they need to address the climate crisis.\"\n\nUniversity of Málaga (UMA) announced its participation in UNEP's Climate Neutral Network in July 2009. As an urban campus, Malaga University has focused on building infrastructure and facilities for renewable energy production and fostering low-impact transportation to further its goals of being carbon neutral. In the realm of transit, the University plans on completing a subway line between the local city center and the University center by 2012, as well as facilitating the growth of bicycle and pedestrian portions of traffic. UMA has also planned or constructed installations of solar thermal and photovoltaic generation on its campus, and is currently constructing a photovoltaic installation aimed to produce one megawatt of energy for the school. The school is also investing in co- and tri-generation technologies that incorporate fuel-burning generation and secondary heat capture aided by geothermal heat. The campus' immediate goal is to rely entirely on sustainable sources of power.\n\nTongji University in Shanghai, China was one of the first six schools to join the Climate Neutral Network. Tongji is using technology, management, and education and participation to meet its goal of using fewer resources. Tongji is saving energy by using solar water heating and re-using waste water in its bath house. Additional steps are being taken to save energy in many of the university’s buildings, such as “low-E windows, architectural sun shading, roof greening, and energy-saving lighting.” The Wenyuan building, for example, uses “geo-heat pumps, thermal insulation systems, rainwater collection and recycling systems.”\nThe Tongji Fuel Cell Vehicle Engineering Center is helping to create fuel cell vehicles. These fuel cell powered sedans can travel 150 miles per hour and can “cover up to 300 miles after one hydrogen charging.”\n\nUniversity of the West of England is part of the Climate Neutral Network and with this, it is committed to reducing climate change. This entails reducing greenhouse gas emissions, so to accomplish this, the university developed a carbon management plan in partnership with Carbon Trust. The aim of the university’s carbon management plan is “to use energy more efficiently to progressively reduce dependency on fossil fuels and contribute to achieving the UK target of an 80 percent reduction in carbon dioxide emissions by 2050.\" The critical objectives of the plan are to: \"reduce energy use in buildings, including residential buildings; implement an energy awareness campaign; implement 80 percent of the technical carbon saving measures detailed in the Carbon Management Plan; aim to achieve BREEAM rating of at least “Very Good” for energy aspects of new buildings; and to consider the procurement of a proportion of electricity from renewable sources where economic to do so.\" Additionally, the University of the West of England is a member of the West of England Carbon Challenge. As a member, the university has pledged to reduce its emissions over the course of four years in order to have an overall 10 percent decrease in emissions by 2012.\n"}
{"id": "44163119", "url": "https://en.wikipedia.org/wiki?curid=44163119", "title": "Caribbean Electric Utility Services Corporation", "text": "Caribbean Electric Utility Services Corporation\n\nThe Caribbean Electric Utility Services Corporation (CARILEC) is an association of electric utilities, suppliers, manufacturers and other stakeholders operating in the electricity industry in the Caribbean.\n\nCARILEC's Mission is to enhance the effectiveness of its members by providing industry related services, creating regular networking, training and knowledge sharing opportunities; supporting mutual assistance programs and being an advocate for the industry throughout the Caribbean.\n\nCARILEC was established in 1989 with nine (9) members as part of an electric utilities modernization project funded by USAID and implemented by NRECA under a five-year \"Co-operative Agreement.\"\n\nCurrently, CARILEC comprises a total of ninety two (92) members. This includes thirty four (34) Full Members that are electric utilities and fifty three (53) Associate Members that are companies involved in some aspect of servicing the electric utility business and five (5) Affiliate Members.\n\n\n\nAccording to the CARILEC by-laws provision is made for the appointment of a Board of Directors comprising not more than (15) and not less than (3) members to be responsible for policy governance. A Director shall be the Chief Executive/Operating Officer or a senior professional staff of a member utility. Provision is also made for the Directors to elect a Chairman and Vice Chairman from among them. Currently, the Board of Directors of CARILEC comprises a fifteen (15) member team. Thirteen (13) of the Directors are elected at the annual meeting of members from among the Full Members for a term of three (3) years each, while one (1) Director is elected to serve as a representative of the Associate Members for a period of one (1) year. The Board also comprises an ex-officio Executive Director who is responsible for the day-to-day administration of the Secretariat located in Saint Lucia and serves as the Company Secretary/Treasurer.\n"}
{"id": "12833921", "url": "https://en.wikipedia.org/wiki?curid=12833921", "title": "Chemocline", "text": "Chemocline\n\nA chemocline is a cline caused by a strong, vertical chemistry gradient within a body of water. A chemocline is analogous to a thermocline, the border at which warmer and cooler waters meet in an ocean, sea, lake, or other body of water. (In some cases, the thermocline and chemocline coincide.)\n\nChemoclines most commonly occur where local conditions favor the formations of anoxic bottom water — deep water deficient in oxygen, where only anaerobic forms of life can exist. The Black Sea is the classic example of such a body, though similar bodies of water (classified as meromictic lakes) exist across the globe. Aerobic life is restricted to the region above the chemocline, anaerobic below. Photosynthetic forms of anaerobic bacteria, like green phototrophic and purple sulfur bacteria, cluster at the chemocline, taking advantage of both the sunlight from above and the hydrogen sulfide (HS) produced by the anaerobic bacteria below.\n\nIn any body of water in which oxygen-rich surface waters are well-mixed (holomictic), no chemocline will exist. To cite the most obvious example, the Earth's global ocean has no chemocline.\n\n\n"}
{"id": "31027238", "url": "https://en.wikipedia.org/wiki?curid=31027238", "title": "Community gardening in the United States", "text": "Community gardening in the United States\n\nCommunity gardening in the United States encompasses a wide variety of approaches. Community gardens can function as gathering places for neighbors, promote healthier eating, and showcase art to raise ecological awareness (see Karl Linn). Other gardens resemble European \"allotment\" gardens, with plots where individuals and families can grow vegetables and flowers; including a number which began as \"victory gardens\" during World War II.\n\nSome community gardens are devoted entirely to creating ecological green space or habitat, growing flowers, educational purposes, or providing access to gardening to those who otherwise could not have a garden, such as the elderly, recent immigrants, urban dwellers, or the homeless. Some gardens are worked as community farms with no individual plots at all, similar to urban farms.\n\nCommunity gardens can vary in shape, size, and function, but the goal of bridging the gap between people and nature is central to their creation. These gardens weaken the divide between nature and culture, city and country, and producer and consumer.\n\nThe largest community garden in the nation is reported to be Shiloh Field Community Garden in Denton, TX, measuring at 14.5 acres of land.\n\nA community garden is any piece of land gardened by a group of people. The majority of gardens in community gardening programs are collections of individual garden plots, frequently between and . This holds true whether they are sponsored by public agencies, city departments, large non-profits, or (most commonly) a coalition of different entities and groups.\n\nWhether the garden is run as a co-op by the gardeners themselves (common in New York City, Boston and other East Coast cities) or managed by a public or non-profit agency, plot holders typically are asked to pay a modest fee each year and to abide by a set of rules. Many gardens encourage activities such as work days, fundraisers, and social gatherings. Community garden organizers typically say that \"growing community\" is as important as growing vegetables; or, as the American Community Gardening Association (ACGA) puts it: \"In community gardening, 'community' comes first.\" The ACGA, a non-profit coalition founded in 1979, is the primary advocacy group for community gardening in the US and Canada.\n\nCommunity gardening in the United States overlaps to some extent with the related but distinct movement to encourage local food production, local farmers' markets and community supported agriculture farms (CSAs). Leases and rules prevent some, though not all, community gardeners from selling their produce commercially, although their gardens may donate fresh fruits and vegetables to local food pantries, cooperatives, and homeless members of their community.\n\nHowever, community gardens offer ideal sites for local farmers markets, and gardeners often seek farmers to provide space-intensive crops such as corn or potatoes. They also can hire farmers to provide services such as plowing and providing mulch and manure. In turn, small farmers can reach a wider audience and consumer base by drawing on community gardeners and their contacts. Although the two approaches are distinct, both can be effective ways to produce local food in urban areas, safeguard green space, and contribute to food security. Community gardens also increase environmental aesthetics, promote neighborhood attachment and social involvement.\n\nIn an interesting variant on the practice of reclaiming bombed-out areas for community gardens (also practiced during WWII in the ghettos of Eastern Europe), in American inner-cities, community groups have reclaimed abandoned or vacant lots for garden plots. In these cases, groups have subsequently leased from a municipality that claims the property or claimed squatter's rights or a right to subsistence not currently recognized by the legal system. Two notable cases include the gardens of Manhattan's lower Eastside and the South Central Farm of Los Angeles, California. A lasting legacy of the New York gardens is 'guerrilla gardening', and the historically important \"Green Guerrillas\" founded by Liz Christy. In contrast, The South Central Farm was recently bulldozed in Los Angeles.\n\nCommunity gardens often face pressure due to economic development, rising land values, and decreased city government budgets. In some cases they have responded to the changes by forming nonprofit organizations to provide assistance and by building gardens on city park spaces and school yards.\n\nThe European history of community gardening in the US dates back to the early 18th century, when Moravians created a community garden as part of the community of Bethabara, near modern Winston-Salem, North Carolina – a garden still active and open for visitors today. First Nations peoples also gardened with a community approach (\"Buffalo Bird Woman's Garden\" paints a picture of gardens among the Hidatsa), likely for generations before the arrival of waves of immigrants.\n\nDuring World War I and World War II, some Victory gardens were planted on public land.\n\nAcademic study of American community gardening by T.J. Bassett and more recently Laura Lawson (\"City Bountiful\") suggests that the community gardening \"movement\" is best described as a series of distinct phases each with contrasting ideologies and purposes, even though all resulted in people creating gardens on public or abandoned land. The latest phase began with the alternative politics and culture and dawning ecological activism of the late 1960s.\n\nFrom the mid-1970s through the early 1990s, community gardening in a select number of major American cities enjoyed Federal financial support, though many programs struggled to find funding. The loss of the Federal program increased the challenge of finding funding to support programs. Funding remains a key challenge, along with secure land tenure for garden sites, finding insurance, and helping gardeners develop ways to work together smoothly.\n\nCommunity gardens benefit community food access by enhancing nutrition and physical activity as well as promoting the role of public health. The U.S. Department of Health and Human Services recommends eating more dark green vegetables, orange vegetables, legumes, and fruits; eating less refined grains, fat, and calories; and obtaining 60 minutes of physical activity on most days. Recent public health evaluations show community gardens as a promising approach to promote healthy behaviors. This is particularly important in establishing healthy behaviors among children given the rise of childhood obesity. One recent pilot study in Los Angeles showed a gardening and nutrition intervention improved dietary intake in children and reduced body mass index.\n\nCommunity gardens also benefit community food security by providing residents with safe, culturally acceptable, nutritionally adequate diet through a sustainable food system that maximizes community self-reliance and social justice. Community garden initiatives have inspired cities to enact policies for water use, improved access to produce, strengthened community building skills, and created culturally appropriate education programs that help elevate the community's collective consciousness about public health. In impoverished urban areas especially, produce harvested from community gardens provides a nutritious alternative to what Nancy Janovicek calls \"the industrial diet,\" which consists of cheap and accessible options like fast-food chains.\n\nProfessor Jill Litt and colleagues at University of Colorado School of Public Health evaluated the effects on community gardening in the Denver metro area on social environment, community building and fruit and vegetable intake. Community gardeners were more likely than home gardeners and non-gardeners to meet the national recommendations of fruit and vegetable intake. Semistructured interviews carried out by Teig \"et al.\" revealed that Denver community gardeners felt a high level of trust between members of the garden and a strong sense of community. Furthermore, gardeners were involved in community voluntary efforts and donated surplus produce to populations without access to fresh produce.\n\nCommunity gardens have the potential to positively impact the areas around them. If gardeners employ organic and environmentally conscious techniques, the community gardens can be a step away from chemically dependent and wasteful food systems. Gardens that produce crops and vegetables act to reduce the need for fossil-fuel intensive storage of delivery of food to local community members. As researcher Montenegro de Wit states, sustainable agriculture should not be \"contained to the countryside.\" By bringing these techniques into communities, learning opportunities arise as well as the chance of converting land from an \"emissions-source\" to a \"carbon sink\" as Robert Biel writes.\n\nIn addition to the possible environmental benefits community gardening brings, there are unintended consequences that can result from poor planning and lack of ecological consideration. For example, if most community members have to drive a considerate distance to reach their community garden or farmer's market, the benefits of locally sourced food evens out. The carbon emissions of travel to the community garden, step closer to those of commercial packaging and transportation costs.\n\nCommunity gardens play a part in a larger food systems movements such as food justice, food sovereignty, food security, urban farming, and more. These movements are not only happening in the United States, but transnationally in the Global North and the Global South.\n\nAgricultural activity in communities is a way of promoting self-sufficiency, as well as community empowerment and involvement. Additionally, producing food, helping the environment, and creating green spaces in cities contributes to an overall increase in happiness by helping community members accomplish fundamental human tasks such as growing food. Space in cities and communities reserved for growing vegetables and flowers promotes wellbeing, neighborliness, and the protection of nature.\n\nAs the majority of the United States' farmers reach retirement age, community gardens play an active role in informing and perhaps inspiring a new generation to become involved with and passionate about growing food. Diversifying the food system with community gardens and other methods of urban agriculture will benefit the economy and create competition between product quality and value.\n\nGreen spaces in cities often increase the land value of an area and contribute to gentrification. The perception of organic foods being for an affluent population, plus the perceived notion of eliteness that comes from organic-based market chains like Whole Foods and Trader Joe's work against the goals of most urban agricultural initiatives by creating exclusive spaces. Community gardens based on crop sharing, knowledge sharing, and community building help to promote access to healthy foods by creating accessible spaces.\n\nOne issue faced by lower-income citizens is that they don't have the time or energy needed for becoming active in a community garden, and therefore struggle to receive the benefits they offer. Community gardens that are working for high crop yields are labor-intensive, and some community members may work multiple jobs and have little to no extra time to commit. One way that lower-income citizens can gain access to healthy food is through the SNAP program, which is increasingly being accepted at farmer's markets.\n\nCommunity gardens are a way for a variety of cultures to come together and create a stronger community. Focusing on creating equitable and respectful spaces where farming knowledge can be shared is crucial to creating a just food system for all community members. Communities hold specific knowledge and expertise about their local environment, and therefore, community members have the power to play a central role in the creation of their local food system. Partnerships between academic researchers, farmers/practitioners, advocates, and community members will filter knowledge of healthy foods and farming techniques throughout the community as a whole. All of these benefits will lead, in researcher Montenegro de Wit's opinion, to \"a more egalitarian food system\" that \"will likely emerge from participation by those traditionally excluded from it.\"\n\n. To find a community garden in your area, visit the communitygarden.org website.\n\nIn San Francisco, community gardens are available through various public and private entities. Most community gardens in San Francisco are available through its Recreation and Park Department, which manages over 35 community gardens on City property. These are allotment gardens whereby individuals or groups volunteer to be assigned garden plots. Garden members within their respective gardens democratically organize themselves to set bylaws that are consistent with City policy. These gardeners often self-impose garden dues as a membership requirement to cover common expenses. To standardize the development and management of its community gardens, the Recreation and Park Commission adopted its Community Garden Policy in 2006.\n\nThough not plot-based, the City's Department of Public Works supports communal-style gardening on City property whereby community groups participate in the development and maintenance of public gardens. No one person is responsible for any portion of the site. One group, a community-based and resident-led volunteer group in an underserved neighborhood called Bayview Hunters Point, has created an enclosed food-producing garden on City-owned land, as well as developed many residential urban farms around privately owned homes. This group, the Quesada Gardens Initiative, is one of many organizations in the San Francisco Bay Area working at the nexus of environmental justice, health and wellness and food security, and community-building.\n\nAll of the community gardens of San Francisco are listed on the San Francisco Garden Resource Organization web site with detailed directions and garden pictures of some of the gardens.\n\nThere are over 100 community gardens in the Denver metro area. Gardens are located on vacant land (42%), school grounds (26%), housing facilities (15%), and other location (17%) such as churches and senior centers. Based on land tenure, community gardens in Denver are found on public land (52%), private land (24%), or owned by non-profits (16%) and Denver Urban Gardens (8%). Denver Urban Gardens (DUG), a non-profit organization that assists community members with the design, planning, and construction of neighborhood community gardens. The majority of DUG's community gardens are located in low-to-moderate income areas, and more than 20 are located at Denver public schools. DUG also partners with government and other non-profit agencies to offer gardening and nutrition education.\n\nThe Westwood community partnered with the non-profit Re:Vision to create a system of community gardens in 2010 to increase healthy food access. As of 2015, they planned to expand the initiative and open a food cooperative in the neighborhood.\n\nThe Aurora Mental Health Center, located in Aurora, Colorado, started a community garden in the eastern Aurora area in 2014 to improve community relations. Aurora Mental Health's community garden also provides individual horticulture therapy practices. The Aurora Mental Health Center Community Garden allows for community members to become garden leaders, helping newer members grow their plants.\n\nJasper County Indiana supports a local community garden run by local churches. The garden has given away over half a million pounds of produce to local food shelves since it was founded in 2008.\n\nCommonwealth Gardens is a 501(c)3 organization dedicated to the formation of community gardens and school gardens in the Frankfort and Franklin County area. Commonwealth Gardens also encourages the consumption of locally grown food not only because it tastes better and takes less energy to produce, but also because it is important to support our local farmers and merchants.\n\nCommonwealth Gardens has the support of the Frankfort Department for Parks, Recreation, and Historic Sites, Kentucky Employees Credit Union, Franklin County Cooperative Extension, Pioneering Healthy Communities, The Kentucky Coffeetree Cafe, Earth Tools, Inside Out Design, and many other local businesses and organizations in the Frankfort area.\n\nIn the city of Boston, Massachusetts there are a variety of local and non-profit organizations which own, promote and manage approximately 180 community gardens throughout the city. These organizations include the Boston Natural Areas Network (BNAN), Boston Nature Center of the Massachusetts Audubon Society, Boston Parks and Recreation Department, Boston Urban Gardeners (BUG), MA Department of Conservation and Recreation, Dorchester Gardenlands Preserve, ReVision House, and the South End Lower Roxbury Open Space Land Trust. In 2002, the volunteer-run Boston Community Garden Council was formed as a means of facilitating communication and cooperation between these organizations along with individual gardeners in Boston.\n\nCommunity gardens located in Detroit include the Earthworks Farm, the North Cass Community Garden, and the Woodbridge Community Garden.\n\nSt. Louis is home to Gateway Greening, a unique non-profit organization that works with interested neighborhoods to transform vacant lots into vibrant community gardens. Since 1984, Gateway Greening has grown to support more than 250 community, school and youth gardens throughout St. Louis City and County. This support is provided through the creation of a grant processes which awards much needed materials, tools, and other valuable resources to new and existing community gardens. Additionally, Gateway Greening provides a rich schedule of ongoing community education opportunities at the Bell Community and Demonstration Garden, network of Community Resource Gardens, and the Gateway Greening Urban Farm, a 2.5 acre urban farm located in downtown St. Louis.\n\nAlthough Gateway Greening is a major proponent of community gardening in St. Louis, it is by no means the only group to create or support STL Urban Agriculture. Community gardening and urban agriculture has taken off in St. Louis, Missouri, in recent years in part thanks to the Garden Lease Program which allows residents to lease LRA land for a period of 5 years.\n\nCommunity gardens in New Jersey include the South Orange Community Garden.\n\nIn New York City, there are nearly 600 community gardens located in all five boroughs that are supported by GreenThumb – the community gardening division of NYC Parks. GreenThumb provides technical, material, operational, organizational and property management support to community gardens and another 650 school gardens located throughout the City. Begun in 1978, it is the largest and oldest program of its kind in the United States.\n\nIn Salt Lake City, community gardens are available through the non-profit organization Wasatch Community Gardens. On May 16, 2009 Wasatch Community Gardens, in collaborated with The Redevelopment Agency of Salt Lake City (RAD), launched the first People's Portable Garden in Salt Lake City. The garden is designed to stimulate growth and revitalize different areas of the city. Salt Lake City put $48,000 into the People’s Portable Garden on 900 South. The People's Portable Garden is located at 900 S 200 W, Salt Lake City.\n\nThe Seattle P-Patch program for community garden plots began in the early 70s during an economic downturn known locally as the \"Boeing Bust\" which had resulted in many people without work or money. Darlyn Rundberg Del Boca, a University of Washington student, saw an opportunity to promote children's gardening with a focus on growing for the local Neighbors in Need food bank program, and with the help of a Seattle Councilmember obtained permission to use part of the Picardo family's truck garden in northeast Seattle with the City of Seattle renting the land for the cost of its real estate taxes. The first garden consisted of a large central garden plot planted by children from the nearby elementary school and their parents; families who volunteered to help were offered smaller individual plots around the perimeter of the central plot. The City subsequently purchased the Picardo farm, and the program of renting individual garden plots arising from the first efforts was named 'P-Patch' in honor of the Picardo family's contribution. The P-Patch program continued to grow and currently consists of 1900 plots in 68 locations with a total of of land, with additions planned each year, and the tradition of growing for local food banks resulted in 12.3 tons of food donated in 2008.\n\nIn 2010 the city of Olympia adopted a plan to create up to six community gardens. Currently the city has two gardens: one at Sunrise park and the Yauger Community Garden Project. There are also many private community gardens such Wendell Berry in the Bigelow neighborhood.\n\n\n\n"}
{"id": "4245290", "url": "https://en.wikipedia.org/wiki?curid=4245290", "title": "Danubit", "text": "Danubit\n\nDanubit is an industrial plastic explosive produced by the Slovak company Istrochem.\nIt is intended primarily as a rock blasting explosive for surface and underground mass mining of mineral raw materials although underwater blasting applications are possible as well.\n\nThe producer of Danubit, Istrochem, is a chemical company founded by Alfred Nobel in Bratislava, Slovakia.\n\n\n"}
{"id": "45015620", "url": "https://en.wikipedia.org/wiki?curid=45015620", "title": "Dual-mass flywheel", "text": "Dual-mass flywheel\n\nA dual-mass flywheel (DMF) is a rotating mechanical device that is used to provide continuous energy (rotational energy) in systems where the energy source is not continuous, the same way as a conventional flywheel acts, but damping any violent variation of torque or revolutions that could cause an unwanted vibration. The vibration reduction is achieved by accumulating stored energy in the two flywheel half masses over a period of time but damped by a series of strong springs, doing that at a rate that is compatible with the energy source, and then releasing that energy at a much higher rate over a relatively short time. A compact dual-mass flywheel often includes the whole clutch, including the pressure plate and the friction disc.\n\nDual-mass flywheels were developed to address the escalation of torque and power, especially at low revs. The growing concern for the environment and the adoption of more stringent regulations have marked the development of more efficient new engines, lowering the cylinder number to 3 or even 2 cylinders, and allowing the delivery of more torque and power at low revolutions. The counterpart has been an increase in the level of vibration which traditional clutch discs are unable to absorb. This is where the dual-mass flywheels play a key role, making these mechanical developments more viable.\n\nThe absorption capacity of the vibration depends on the moving parts of the DMF, these parts are subject to wear. Whenever the clutch is replaced, the DMF should be checked for wear. The two key wear characteristics are freeplay and sideplay (rock). These should be measured to determine whether the flywheel is serviceable. The wear limit specifications can be found in vehicle or flywheel manufacturer's published documentation. Other failure modes are severely grooved/damaged clutch mating surface, grease loss, and cracking.\n\nThe main type is called a \"planetary DMF\". The planetary gear and the torsional damper are incorporated into the main flywheel. For this purpose, the main flywheel is divided into primary and secondary pinion-connected masses, and between them there are four different types of bent springs:\n\nThe simplest form of the bent spring is the standard single spring.\n\nThe standard springs are called parallel springs of one phase. These consist of an outer and an inner spring of almost equal lengths and connected in parallel. The individual characteristic curves of the two springs are added to form the characteristic curve of the spring pair.\n\nIn the case of two-stage spring there are two curved parallel springs, one inside the other, but the internal spring is shorter so that it acts later. The characteristic curve of the outer spring is adapted to increase when the engine is started. The softer outer spring only acts to increase the problematic resonance frequency range. When the torque increases, reaching the maximum value, the internal spring also acts. In this second phase, the inner and outer springs work together. The collaboration of both springs thus ensures good acoustic isolation at all engine speeds.\n\nThis curved spring consists of an outer and two inner springs with different elastic characteristics connected in series. This category of bent spring uses the two concepts together: parallel and series connection in order to ensure optimal torsional compensation for each value of torque.\n\n\n"}
{"id": "57932064", "url": "https://en.wikipedia.org/wiki?curid=57932064", "title": "Dynamic line rating for electric utilities", "text": "Dynamic line rating for electric utilities\n\nDynamic line rating (DLR), also known as real-time thermal rating (RTTR), and in German \"Freileitungsmonitoring\" (FLM), is an electric power transmission operation philosophy aiming at maximizing load, when environmental conditions allow it, without compromising safety. Research, prototyping and pilot projects were initiated in the 1990s, but the emergence of the \"smart grid\" stimulated electric utilities, scientists and vendors to develop comprehensive and sustainable solutions.\n\nThe current-carrying capacity, or ampacity, of overhead lines starts with the type of conductor used. The conductor choice provides the physical parameters for dynamic line rating. Transmission ratings are set with a maximum allowable conductor temperature (annealing temperature) and minimum clearance rules to adhere to legislation. Both the temperature and the minimum clearance of a transmission line is affected by the number of amps flowing through the wire, corresponding to the Joule Effect\n\nAmpacity has traditionally been limited by conductor thermal capacity defined in terms of Static Rating (SR), based on a predetermined set of worst-case weather scenarios with high reliability levels (typically: simultaneous occurrence of (i) conservative minimum perpendicular wind speed of about 0.5-0.6 [m/s], (ii) conservative high solar radiation of about 1000 [W/m²] and (iii) conservative high ambient temperature defined by season).\n\nMore often than not, there are unused margins between the limits defined by static ratings and the \"true limits\" measured by a DLR/RTTR system.\nSeveral methods have been developed since the 1990s. Most of them rely upon sensors deployed on overhead lines, measuring parameters in real-time. Other systems utilize weather stations that monitor environmental conditions without contacting the line. Data received from any method is reported to a main computer for processing. Control center operators access usable data (line temperature, ratings, forecasts, historical values) in pseudo-real-time through a human-machine interface (HMI).\n\nWind generation requires an area that has consistent wind conditions. When new wind generation is prospected the grid connection point must also be analyzed. Adding new generation to the grid typically requires an infrastructure buildup that can potentially cost millions of dollars. In some circumstances a phenomenon, known as concurrent cooling, can be utilized to mitigate the expenses. Concurrent cooling is the cooling of transmission lines, thus increasing ampacity, when the wind power is generating. The exact parameters of the system are vital to determine feasibility, however, there has been successful cases \n\nDLR methods and technologies are considered \"mature\" by industry groups like ENTSO-E after field validations for various applications. Transmission utilities in Asia, Europe, North America, and South America have included deployment of DLR in their grid development roadmaps. IEEE and CIGRÉ devised standard thermal modeling of conductors for ampacity calculations. CIGRÉ issued guidelines for DLR.\n\nDeploying DLR consists in equipping circuits likely to benefit from significant capacity gains with sensors and using the resulting capacity increases when required and possible.\nTypical applications:\n\nThese subjects are addressed at load dispatch center level, and by planning and maintenance departments. However, to decide on priorities, simulations based on 3D line profile analysis and weather data are sometimes performed prior to deployment.\n\nThere are two categories of DLR computation methods:\n\nDirect methods give actual conductor state condition with respect to clearance rules and/or maximum conductor temperature. Indirect methods provide estimates based on prevailing, forecast, or measured weather conditions around the line. Computational fluid dynamics models can be used to provide a larger effective area to a single weather station by mapping wind patterns around the terrain Direct methods are strongly recommended by CIGRÉ standards TB 207 and TB 498 - Guidelines for thermal rating and real-time monitoring).\n\nSeveral kinds of ratings are available, depending on input parameters and algorithms. Real-time ratings allow control room engineers to adjust power flows according to normal operational events or contingencies. They are based on \"steady-state (equilibrium) ampacity\" calculations. Emergency ratings are based on transient equations and models: they provide permissible overload ratings for a short and adjustable time (typically 5 to 30 minutes).\nForecasting methods have been developed to determine intraday and day-ahead ampacity forecasts. They combine DLR historical measurements and weather forecasts. These methods are proprietary, they are promoted by expert and industry associations like CIGRÉ's Study Committee B2 and the WATT Coalition.\n\nWhile DLR solutions can be implemented as stand-alone, their destination is system integration in the control room with the electric utility SCADA. Energy Management Solution and Distribution Management Solutions vendors propose this type of integration to enhance their offering.\n\nPioneer companies, now defunct, followed initiatives by the US Department of Energy, electric utiities ONCOR and NYPA, ENTSO-E and the European Commission's Energy Research programme. They contributed in setting the standards and paved the way for more advanced solutions. The most active manufacturers originate in North America and Europe where market stimulation is strongest. Experiments were conducted in order to field-test available technologies and to quantify benefits, in terms of available transit gains and capital expenditure savings. Operation expenditure gains are more delicate to evaluate since they depend on grid codes, local regulation rules, incentives and penalties. However they can be classified as \"additional transit revenues\", \"improved economic dispatch\" and \"avoided penalties\".\nAs of June 2018 around 20 electric utilities in the Americas and Europe use DLR in daily transmission operations. Benefits recorded have attracted the attention of investors, governments and international development aid institutions, who include a DLR share in the scope of their green-field powerline construction projects.\n\n\"An academic view of these technologies, by Jean-Louis Lilien, Honorarium Professor, University of Liège, Belgium:\n\nDynamic (electrical overhead-)line rating is a great opportunity for transmission line operators (any voltage from 15 kV to 735 kV and over). It has been studied for more than 20 years inside CIGRE and IEEE working groups. But technology (sensors, weather forecast), as well as national or supra national rules, didn’t allow to generate a financial return for the owners of the line, which is nowadays (2018) possible in many countries.\nMoreover, the electricity generation mix is strongly changing in many countries due to the large deployment of wind and solar farms. The power flows are therefore changing a lot because of renewables and intercountry exchanges, where potential larger transit (owing to DLR) allows lower market electricity cost.\n\nThere are large opportunities owing to technology miniaturization (accelerometers, digital signal processing for embedded data treatment, telecommunications, etc.) as well as new development in weather forecasts (which are nowadays much more accurate and require less computational power than only a few years ago), including for these last, up to 72 hours ahead. Availability of this equipment and data for reasonable costs help to develop new sensors and systems which may be implemented into the EMS (energy management system) of national or local dispatching. These dynamic line rating sensors may sometimes warranty up to 30% more power transit in the monitored lines, depending on wind speed which is actually and locally evaluated by appropriate sensors using intelligent data processing.\n\nThe original development of sensors, including advanced treatment (reinforced learning for example) help to largely deploy these sensors on power lines.\n\nCapacity forecasting seems to be the new promise of DLR, mixing on-line sensors with reliable weather data forecast, up to two days ahead. In this field, significant social welfare benefits are expected inasmuch as capacity forecasts favour converging zonal prices and a decrease in congestion rents.\n\nDirect measurement methods use field data measured to inform on line condition (conductor temperature, sagging, clearances) and on weather parameters (ambient temperature, solar radiation, wind velocity). Spans deemed \"critical\", i.e. likely to reach thermal or clearance limits first, are monitored in real-time, providing data on their status with respect to safety limits. The state change equation of the conductor, using its thermal features (absorptivity, emissivity) converts data on span geometry into valid temperature data for the \"ruling span\" of the monitored section. If weather conditions are more favorable than those used for the calculation of static limits, a margin probably exists. To be able to provide reliable and safe values, real-time parameters (load current, conductor temperature, weather parameters) are fed into the conductor's thermal model as per IEEE and CIGRÉ guidelines. Measurements, software and specialized algorithms are housed in a dedicated computer set up, forming a \"DLR server\", with communication facilities. After processing and formatting, \"ratings\" are made available to human operators via a user interface (computer displays and data files). Several kinds of ratings are available, from real-time, indicating ampacity immediately available, without time limits should conditions remain identical, to emergency ratings (higher ratings for a limited duration), to same-day or day-ahead forecasts, using sophisticated mathematical techniques and ever-improving Machine Learning algorithms.\n\nNote 1: DLR computations deal with \"near real-time\" data, i.e. information updated more or less every 5 minutes. Shorter time cycles would require uneconomical computing power and would not make sense because of the thermal inertia of power transit phenomena. Furthermore, in current applications, DLR involves human-made decisions in control rooms of electrical utility companies, without automation.\n\nNote 2: Weather parameters affect line ratings in decreasing order, as follows:\n\nStandards published by IEEE and CIGRÉ cover the following subjects, necessary to perform DLR:\n\nDLR translates into benefits when dispatch engineers apply optimized ratings to transit operations from their national or regional control center. Stand-alone DLR solutions provide real-time data for day-to-day operations and historical data for statistical analysis. The ultimate destination of a DLR solution is integration into control room tools and systems. Typically, the DLR server can be configured to send standard telecontrol frames to the electric utility's scada front-end acquisition units. These frames can then be processed for display and calculations by the utility energy management system or distribution management system. Short-term network operation decisions are based on optimized rating information, as well as load-flow calculations and economic dispatch scenarios. The latter also benefit from short-term forecasts in contingency analysis.\n\nInput data (weather parameters, circuit load, infrastructure design, field measurements) are public domain, proprietary if not confidential, and must be managed accordingly. Output data (line condition, ratings and forecasts) are definitely proprietary and confidential. To ensure provisions of the CIA triad, the utility and the vendor implement secured communications with cyphering, access control and restrictions. Industry trends favor deployments in SaaS (Software as a service) mode. Sensitive areas are:\n"}
{"id": "6929549", "url": "https://en.wikipedia.org/wiki?curid=6929549", "title": "Electricity Generating Authority of Thailand", "text": "Electricity Generating Authority of Thailand\n\nThe Electricity Generating Authority of Thailand (EGAT; (; ) is a state enterprise, managed by the Ministry of Energy, responsible for electric power generation and transmission as well as bulk electric energy sales in Thailand. EGAT, established on 1 May 1969, is the largest power producer in Thailand, owning and operating power plants at 45 sites across the country with a total installed capacity of 15,548 MW.\n\nEGAT's monopoly position in Thailand's electrical energy market has been challenged by critics as influential as a former energy minister and other government members are on the board. It has been criticised as inefficient and an impediment to the development of renewable energy sources.\n\nAs stated in EGAT's \"Annual Report 2017\":\n\nEGAT reported revenues of 494,119 million baht in fiscal year 2017 (FY2017: 1 January–31 December 2017). Net income was 59,042 million baht and total assets grew to 986,306 million baht.\n\nEGAT's power generation plants consist of three thermal power plants, six combined cycle power plants, 24 hydropower plants, eight renewable energy plants, and four diesel power plants. EGAT produced 37 percent of Thailand's electricity; independent power producers, 35 percent; small power producers, 19 percent; and electricity imports, 9 percent. Gas-fired generation powers 67 percent of EGAT's electricity generation while coal-fired power plants account for 20 percent. Most of EGAT's electricity is sold to the Metropolitan Electricity Authority (MEA), which supplies the Bangkok region, and the Provincial Electricity Authority (PEA), which supplies the rest of Thailand.\n\nEGAT operates the Mae Moh coal (lignite) mine in Lampang Province and is required by its enabling legislation to sell lignite.\n\nObservers have noted that in some Western countries, the state purchases renewable energy from producers first before purchasing non-renewable energy. If renewables fail to meet the country's energy demand, it is topped up using non-renewable energy sources. In Thailand, this policy is reversed.\n\nEGAT's net profits have declined 3.5 to 4 percent per year for the last several years, in concert with its share of power generation dipping to 36 percent in 2017 from 55 percent over a decade ago. Independent power producers (IPP) ramped up production over the last four years, from a few hundred megawatts to nearly 3,000 MW at the end of 2017. As IPP-supplied power purchases increase, EGAT's profits decline.\n\nAs of May 2016, EGAT employed 22,955 persons. To shore up declining profits, EGAT intends to reduce its staff to 15,000 by 2021, the first staff cuts in its 49 year history.\n\nIn the first half of 2016, Thailand imported 11.13 million tonnes of coal, up 3.5 percent from 10.76 million metric tonnes (mt) in the first half of 2015. Indonesia and Australia supplied 5.6 million tonnes of bituminous coal and 5.5 million tonnes of \"other\" coal. China and Russia supplied 47,395 tonnes of anthracite coal. Thailand produced 6.88 million mt of lignite from January-May 2016, rising 7.5 percent from the same period in 2015. EGAT accounts for \"...most of the country's domestic lignite production, which is mainly supplied to its own power plants.\" Coal-fired power plants consumed 10.31 million mt of coal and lignite from January to May 2016, or 60.9 percent of the total, rising 11.4 percent year on year. \n\nIn 2017, Thailand imported 22.18 million mt of coal, up 2.5 percent from 2016. The nation produced 16.26 million mt of lignite in 2017, down 4.2 percent year on year. The country consumed a total of 39.07 million mt of coal and lignite, up 0.9 percent. Coal-fired power plants consumed 23.73 million mt of the total, down 4.2 percent from 2016. Other industries consumed the remainder. \n\nWhile EGAT pushes forward with plans for coal-fired generating plants, many countries are spurning coal or deferring its use:\n\nGuiding EGAT's efforts is Thailand's Power Development Plan (PDP). The plan, prepared by the Ministry of Energy (MOE) and EGAT, is issued iteratively. The previous edition, PDP2010 Revision 3, covered the years 2012-2030.\n\nAlong with the PDP, the MOE produces several subsidiary plans that roll up into the PDP:\n\nPDP2015 begins with the assumptions that:\n\nPDP2015 projects the following changes in Thailand electrical power generation fuel mix over the period 2014-2036:\n\nPDP2015 projects that Thailand's CO emissions from power generation will rise from 86,998,000 tons in 2015 to 104,075,000 tons in 2036.\n\nThailand's newest power development plan, PDP2018, is expected to be issued in September 2018.\n\nSeveral critics have pointed out that EGAT's in-house generating capacity coupled with its power purchases from other suppliers has resulted in excessive reserve capacity. One such critic has been the editorial board of the \"Bangkok Post\". They point out that in May 2017, Thailand's peak power demand was 28,578 MW. Total EGAT installed and purchased power capacity in May 2017 was 41,903 MW, leaving a reserve power capacity of 13,325 MW, 46 percent of total May demand. Industry standard best practice is that a 15 percent reserve power capacity is sufficient to \"...maintain a stable power supply.\" In July 2017, EGAT generated 16,071 MW and purchased 25,652 MW from other suppliers for a total of 41,723 MW.\n\nUpdated figures reported in June 2018 indicate that Thailand has the capacity to produce or purchase 42,547 megawatts. Peak demand as of 31 April 2018 was recorded at 29,968 megawatts. Thus Thailand has a reserve margin of 58 percent. The \"internationally accepted ideal reserve margin...[is] 15 percent of peak demand.\"\n\nEGAT continues to press forward with plans to construct six new coal-fired power plants by 2025 in spite of institutions such as the World Bank halting funding for new coal projects except in \"rare circumstances\". Rachel Kyte, the World Bank climate change envoy, said continued use of coal was exacting a heavy cost on some of the world's poorest countries, in local health impacts as well as climate change, which is imposing even graver consequences on the developing world. \"In general globally we need to wean ourselves off coal...There is a huge social cost to coal and a huge social cost to fossil fuels...if you want to be able to breathe clean air.\" EGAT \"...has—in TV commercials—ridiculed renewable energy as expensive and insufficient to deal with rising electricity demand.\"\n\nA persistent criticism of EGAT is that it has paid scant attention to the demand side of the energy equation. Rather than build more carbon-powered plants, working to reduce demand and use existing supplies more efficiently has taken a back seat to network expansion. Opportunities for big savings exist: on 29 March 2014, Thailand observed \"Earth Hour.\" For one hour, superfluous lighting was turned off, resulting in a savings of 1,778 megawatts, the energy equivalent of a new power plant, and more than six million baht in power bills.\n\nEGAT's plans for future developments have been dogged by protests by local residents:\n\nIn mid-2015, government plans to build an 800 megawatt coal-fired electricity generating station (EGAT Coal-Fired TH #3) in Krabi Province have generated protests and hunger strikes by those opposed to the plant who say that it would endanger Krabi's relatively pristine environment. EGAT has pushed forward with development despite not having completed an environmental impact study. It intends to start the bidding process without an environmental assessment in order to \"save time\". The Krabi site is one of nine coal-fired plants planned for southern Thailand to be constructed over the next two decades to off-set the depletion of natural gas fields in the Gulf of Thailand. Opponents of the plan say their demands—which include a three-year waiting period to see if the province can produce 100 percent renewable energy—have been ignored.\n\nIn August 2015, the prime minister ordered the formation of a commission composed of state agencies, EGAT, and citizen activists to find solutions to the power plant conflict. Gen Sakon Sajjanit was appointed committee chairman. It was agreed that the government put a hold on consideration of the Environmental Impact Assessment and Environmental Health Impact Assessments; that EGAT postpone bidding for the plant and the seaport; and that all parties allow Krabi to try to produce 100 percent renewable energy for three years with government support. EGAT broke the agreement as it proceeded with the bidding process, won by the Power Construction Corporation of China and Italian-Thai Development PCL. In November 2016, Prime Minister Prayut Chan-o-cha put the project \"on-hold\". According to the \"Bangkok Post\", this is a move to \"buy time\".\n\nIn early-2017, following a series of protests by those opposed to a coal-fired plant in Krabi, Prime Minister Prayut Chan-o-cha ordered that new environmental (EIA) and health impact assessments (EHIA) be conducted for the Krabi project. He directed that the public must be allowed to have its say. \"A new power plant will certainly be built, but how? We have to take a look at what is good, safe and can deal with power shortages in order to ensure power security. There must be a balance between fossil fuels and recyclable energy,\" the prime minister said. The new assessments are expected to take at least two and a half years to complete, which means the Krabi power plant will be delayed to 2024 from its original schedule of 2019. EGAT officials insisted that a new power plant is still needed in south Thailand to meet the region's power demands, which increase by four to five percent annually.\n\nIn February 2018 the Ministry of Energy put the Krabi coal-fired power plant \"on hold\" for three years pending additional EHIA (environmental and health impact assessment) and EIA (environmental impact assessment) studies.\n\nIn Songkhla Province's Thepha District, a public hearing on EGAT's plans to build a coal-fired plant was ringed with razor wire to prevent opponents of the plan from gaining access to the hearing. The hearing, the third and final hearing on the Environment and Health Impact Assessment (EHIA) for the 2,400 megawatt plant, was policed by 400 soldiers, police, and volunteers. Some attendees admitted being transported to the hearing by local village leaders, who also provided them with gifts and food coupons. Songkhla Governor Thamrong Charoenkul chaired the hearing despite questions raised regarding his neutrality. He told the hearing that the project will benefit Thepa residents. \"Since Egat has proposed the project, Thepha is now known nationwide. Shouldn't we be proud about that?\" he said. Anuchart Palakawongse Na Ayudhya, director of EGAT's Project Environment Division, insisted EGAT's hearings were lawful. \"We have organised the public review step by step according to the law,\" he said. Anuchart said EGAT did not bar anyone from expressing their opinions. \"It's impossible to cancel the project. Most Thepha people support it,\" he said.\n\nOn 17 August 2017, an expert committee of the Natural Resources and Environmental Policy and Planning Office (ONEP) approved the 2,200-megawatt coal-fired power plant's EHIA, removing one of the last hurdles to the plant's construction. The EHIA's approval was met with renewed criticism. The \"Bangkok Post\" commented that, \"These...assessments turn out to be just another rubber stamp for operators — in this case...EGAT...\" ONEP responded to criticism by defending its approval. A construction schedule has not yet been published.\n\nIn February 2018 the Ministry of Energy put the Thepha coal-fired power plant \"on hold\" for three years pending additional EHIA (environmental and health impact assessment) and EIA (environmental impact assessment) studies.\n\nThailand's Power Development Plan, approved in May 2015 (PDP 2015), outlines the government's plans to import up to 10,000 MW of electricity from Myanmar over the next two decades. Much of this electricity is expected to come from planned hydro-power projects on the Salween River (\"Thanlwin\" in Myanmar). Thailand and Myanmar have signed an agreement for the Salween dams project, five dams on the Salween and another dam on the Tenasserim River. EGAT has been pushing forward two projects: the 1,360 MW Hatgyi dam in Kayin State and the 7,100 MW Mong Ton dam in Shan State (formerly known as the Tasang dam).The Mong Ton dam, in central Shan State, would span the Salween and Pang rivers, covering an area the size of Singapore.\n\nEGAT has been the target of several lawsuits brought by neighbours of several of its operations. The best known legal challenge took place in Mae Mo. Mae Mo is the site of a 2,400 MW lignite-fueled power plant run by EGAT. Coal-fired power plants such as Mae Mo can release up to 150 million tonnes of CO over their design life of 20–25 years, according to Greenpeace-Thailand.\nThe plant has been the target of a series of lawsuits brought by locals who claim that the lignite mining operation and the burning of lignite fuel by EGAT has negatively impacted the environment and the health of those living in the vicinity. A 12-year fight by villagers for compensation for damages ended in victory for the plaintiffs in February 2015. The Supreme Administrative Court in Chiang Mai Province upheld a ruling by the Chiang Mai Administrative Court in 2005. The court handed down a verdict ordering EGAT to pay compensation to 131 plaintiffs, some of them deceased. Plant victims were awarded between 20,000-240,000 baht each, commensurate with their suffering. The total amounts to 25 million baht plus 7.5 percent interest.\n\nSeveral days earlier, the court had ordered EGAT to return its Mae Mo golf course, adjacent to the open pit lignite mine, to woodland in order to help clean up the air pollution caused by EGAT's Mae Mo operations.\n\n\n"}
{"id": "2798024", "url": "https://en.wikipedia.org/wiki?curid=2798024", "title": "Ferricyanide", "text": "Ferricyanide\n\nFerricyanide is the anion [Fe(CN)].  It is also called hexacyanoferrate(III) and in rare, but systematic nomenclature, hexacyanidoferrate(III). The most common salt of this anion is potassium ferricyanide, a red crystalline material that is used as an oxidant in organic chemistry.\n\n[Fe(CN)] consists of a Fe center bound in octahedral geometry to six cyanide ligands. The complex has O symmetry. The iron is low spin and easily reduced to the related ferrocyanide ion [Fe(CN)], which is a ferrous (Fe) derivative. This redox couple is reversible and entails no making or breaking of Fe-C bonds:\nThis redox couple is a standard in electrochemistry.\n\nCompared to normal cyanides like potassium cyanide, ferricyanides are much less toxic because of the tight hold of the CN to the Fe. They do react with mineral acids, however, to release highly toxic hydrogen cyanide gas.\n\nTreatment of ferricyanide with ferrous (Iron (II)) salts affords the brilliant, long-lasting pigment Prussian blue, the traditional color of blueprints.\n\n"}
{"id": "35554873", "url": "https://en.wikipedia.org/wiki?curid=35554873", "title": "Fürstenwalde Solar Park", "text": "Fürstenwalde Solar Park\n\nThe Fürstenwalde Solar Park is a photovoltaic power station in Fürstenwalde, Germany. It has a capacity of 39.64 megawatt (MW) and an annual output of 36.5 GWh. The solar park was developed by the company \"Solarhybrid\" and built by conecon using 62,832 225-watt and 110,880 230-watt solar panels, both manufactured by Suntech.\n\nThe PV project was built on a former military airfield on , and was completed in 10 weeks only.\n\n"}
{"id": "1950852", "url": "https://en.wikipedia.org/wiki?curid=1950852", "title": "Glossary of chemical formulas", "text": "Glossary of chemical formulas\n\nThis is a list of common chemical compounds with chemical formulas and CAS numbers, indexed by formula. This complements alternative listing at inorganic compounds by element. There is no complete list of chemical compounds since by nature the list would be infinite.\n\nNote: There are elements for which spellings may differ, such as aluminum/ aluminium, sulfur/ sulphur, and caesium/ cesium.\n\n"}
{"id": "10856401", "url": "https://en.wikipedia.org/wiki?curid=10856401", "title": "Habitat-selection hypothesis", "text": "Habitat-selection hypothesis\n\nHabitat selection hypothesis is an attempt to explain the mechanisms of brood parasite nest selection in cuckoos. Cuckoos are not the only brood parasites, however the behavior is more rare in other groups of birds, including ducks, weavers, and cowbirds. In habitat selection hypothesis, a female cuckoo retains recognition (is imprinted) of the habitat type in which she was reared, and will subsequently return to this habitat type in order to lay eggs. Habitat might be defined as dry or wet, shrubby or forested, lakeside, etc. This habitat preference increases the likelihood of encountering the suitable host species, as most host species are known to be habitat specific. Thus, habitat selection is thought to allow for random host selection by the female cuckoo (Teuschl et al. 1998; Vogl et al. 2002). In some cases an individual may choose a different habitat from their original recognition based on the reproductive success of other individuals.\n\n"}
{"id": "153221", "url": "https://en.wikipedia.org/wiki?curid=153221", "title": "Heat exchanger", "text": "Heat exchanger\n\nA heat exchanger is a device used to transfer heat between two or more fluids. In other words, heat exchangers are used in both cooling and heating processes. The fluids may be separated by a solid wall to prevent mixing or they may be in direct contact. They are widely used in space heating, refrigeration, air conditioning, power stations, chemical plants, petrochemical plants, petroleum refineries, natural-gas processing, and sewage treatment. The classic example of a heat exchanger is found in an internal combustion engine in which a circulating fluid known as engine coolant flows through radiator coils and air flows past the coils, which cools the coolant and heats the incoming air. Another example is the heat sink, which is a passive heat exchanger that transfers the heat generated by an electronic or a mechanical device to a fluid medium, often air or a liquid coolant.\n\nThere are three primary classifications of heat exchangers according to their flow arrangement. In \"parallel-flow\" heat exchangers, the two fluids enter the exchanger at the same end, and travel in parallel to one another to the other side. In \"counter-flow\" heat exchangers the fluids enter the exchanger from opposite ends. The counter current design is the most efficient, in that it can transfer the most heat from the heat (transfer) medium per unit mass due to the fact that the average temperature difference along any unit length is \"higher\". See countercurrent exchange. In a \"cross-flow\" heat exchanger, the fluids travel roughly perpendicular to one another through the exchanger.\n\nFor efficiency, heat exchangers are designed to maximize the surface area of the wall between the two fluids, while minimizing resistance to fluid flow through the exchanger. The exchanger's performance can also be affected by the addition of fins or corrugations in one or both directions, which increase surface area and may channel fluid flow or induce turbulence.\n\nThe driving temperature across the heat transfer surface varies with position, but an appropriate mean temperature can be defined. In most simple systems this is the \"log mean temperature difference\" (LMTD). Sometimes direct knowledge of the LMTD is not available and the NTU method is used.\n\nDouble pipe heat exchangers are the simplest exchangers used in industries. On one hand, these heat exchangers are cheap for both design and maintenance, making them a good choice for small industries. On the other hand, their low efficiency coupled with the high space occupied in large scales, has led modern industries to use more efficient heat exchangers like shell and tube or plate. However, since double pipe heat exchangers are simple, they are used to teach heat exchanger design basics to students as the fundamental rules for all heat exchangers are the same. \n\nShell and tube heat exchangers consist of series of tubes. One set of these tubes contains the fluid that must be either heated or cooled. The second fluid runs over the tubes that are being heated or cooled so that it can either provide the heat or absorb the heat required. A set of tubes is called the tube bundle and can be made up of several types of tubes: plain, longitudinally finned, etc. Shell and tube heat exchangers are typically used for high-pressure applications (with pressures greater than 30 bar and temperatures greater than 260 °C). This is because the shell and tube heat exchangers are robust due to their shape.<br>Several thermal design features must be considered when designing the tubes in the shell and tube heat exchangers:\nThere can be many variations on the shell and tube design. Typically, the ends of each tube are connected to plenums (sometimes called water boxes) through holes in tubesheets. The tubes may be straight or bent in the shape of a U, called U-tubes.\n\nFixed tube liquid-cooled heat exchangers especially suitable for marine and harsh applications can be assembled with brass shells, copper tubes, brass baffles, and forged brass integral end hubs. \"(See: Copper in heat exchangers).\"\n\nAnother type of heat exchanger is the plate heat exchanger. These exchangers are composed of many thin, slightly separated plates that have very large surface areas and small fluid flow passages for heat transfer. Advances in gasket and brazing technology have made the plate-type heat exchanger increasingly practical. In HVAC applications, large heat exchangers of this type are called \"plate-and-frame\"; when used in open loops, these heat exchangers are normally of the gasket type to allow periodic disassembly, cleaning, and inspection. There are many types of permanently bonded plate heat exchangers, such as dip-brazed, vacuum-brazed, and welded plate varieties, and they are often specified for closed-loop applications such as refrigeration. Plate heat exchangers also differ in the types of plates that are used, and in the configurations of those plates. Some plates may be stamped with \"chevron\", dimpled, or other patterns, where others may have machined fins and/or grooves.\n\nWhen compared to shell and tube exchangers, the stacked-plate arrangement typically has lower volume and cost. Another difference between the two is that plate exchangers typically serve low to medium pressure fluids, compared to medium and high pressures of shell and tube. A third and important difference is that plate exchangers employ more countercurrent flow rather than cross current flow, which allows lower approach temperature differences, high temperature changes, and increased efficiencies.\n\nA third type of heat exchanger is a plate and shell heat exchanger, which combines plate heat exchanger with shell and tube heat exchanger technologies. The heart of the heat exchanger contains a fully welded circular plate pack made by pressing and cutting round plates and welding them together. Nozzles carry flow in and out of the platepack (the 'Plate side' flowpath). The fully welded platepack is assembled into an outer shell that creates a second flowpath ( the 'Shell side'). Plate and shell technology offers high heat transfer, high pressure, high operating temperature, uling and close approach temperature. In particular, it does completely without gaskets, which provides security against leakage at high pressures and temperatures.\n\nA fourth type of heat exchanger uses an intermediate fluid or solid store to hold heat, which is then moved to the other side of the heat exchanger to be released. Two examples of this are adiabatic wheels, which consist of a large wheel with fine threads rotating through the hot and cold fluids, and fluid heat exchangers.\n\nThis type of heat exchanger uses \"sandwiched\" passages containing fins to increase the effectiveness of the unit. The designs include crossflow and counterflow coupled with various fin configurations such as straight fins, offset fins and wavy fins.\n\nPlate and fin heat exchangers are usually made of aluminum alloys, which provide high heat transfer efficiency. The material enables the system to operate at a lower temperature difference and reduce the weight of the equipment. Plate and fin heat exchangers are mostly used for low temperature services such as natural gas, helium and oxygen liquefaction plants, air separation plants and transport industries such as motor and aircraft engines.\n\nAdvantages of plate and fin heat exchangers:\n\nDisadvantages of plate and fin heat exchangers:\n\nA pillow plate exchanger is commonly used in the dairy industry for cooling milk in large direct-expansion stainless steel bulk tanks. The pillow plate allows for cooling across nearly the entire surface area of the tank, without gaps that would occur between pipes welded to the exterior of the tank.\n\nThe pillow plate is constructed using a thin sheet of metal spot-welded to the surface of another thicker sheet of metal. The thin plate is welded in a regular pattern of dots or with a serpentine pattern of weld lines. After welding the enclosed space is pressurised with sufficient force to cause the thin metal to bulge out around the welds, providing a space for heat exchanger liquids to flow, and creating a characteristic appearance of a swelled pillow formed out of metal.\n\nThis is a heat exchanger with a gas passing upwards through a shower of fluid (often water), and the fluid is then taken elsewhere before being cooled. This is commonly used for cooling gases whilst also removing certain impurities, thus solving two problems at once. It is widely used in espresso machines as an energy-saving method of cooling super-heated water to use in the extraction of espresso.\n\nA waste heat recovery unit (WHRU) is a heat exchanger that recovers heat from a hot gas stream while transferring it to a working medium, typically water or oils. The hot gas stream can be the exhaust gas from a gas turbine or a diesel engine or a waste gas from industry or refinery.\n\nLarge systems with high volume and temperature gas streams, typical in industry, can benefit from steam Rankine cycle (SRC) in a waste heat recovery unit, but these cycles are too expensive for small systems.\nThe recovery of heat from low temperature systems requires different working fluids than steam.\n\nAn organic Rankine cycle (ORC) waste heat recovery unit can be more efficient at low temperature range using refrigerants that boil at lower temperatures than water. Typical organic refrigerants are ammonia, pentafluoropropane (R-245fa and R-245ca), and toluene.\n\nThe refrigerant is boiled by the heat source in the evaporator to produce super-heated vapor. This fluid is expanded in the turbine to convert thermal energy to kinetic energy, that is converted to electricity in the electrical generator. This energy transfer process decreases the temperature of the refrigerant that, in turn, condenses. The cycle is closed and completed using a pump to send the fluid back to the evaporator.\n\nAnother type of heat exchanger is called \"(dynamic) scraped surface heat exchanger\". This is mainly used for heating or cooling with high-viscosity products, crystallization processes, evaporation and high-fouling applications. Long running times are achieved due to the continuous scraping of the surface, thus avoiding fouling and achieving a sustainable heat transfer rate during the process.\n\nIn addition to heating up or cooling down fluids in just a single phase, heat exchangers can be used either to heat a liquid to evaporate (or boil) it or used as condensers to cool a vapor and condense it to a liquid. In chemical plants and refineries, reboilers used to heat incoming feed for distillation towers are often heat exchangers.\n\nDistillation set-ups typically use condensers to condense distillate vapors back into liquid.\n\nPower plants that use steam-driven turbines commonly use heat exchangers to boil water into steam. Heat exchangers or similar units for producing steam from water are often called boilers or steam generators.\n\nIn the nuclear power plants called pressurized water reactors, special large heat exchangers pass heat from the primary (reactor plant) system to the secondary (steam plant) system, producing steam from water in the process. These are called steam generators. All fossil-fueled and nuclear power plants using steam-driven turbines have surface condensers to convert the exhaust steam from the turbines into condensate (water) for re-use.\n\nTo conserve energy and cooling capacity in chemical and other plants, regenerative heat exchangers can transfer heat from a stream that must be cooled to another stream that must be heated, such as distillate cooling and reboiler feed pre-heating.\n\nThis term can also refer to heat exchangers that contain a material within their structure that has a change of phase. This is usually a solid to liquid phase due to the small volume difference between these states. This change of phase effectively acts as a buffer because it occurs at a constant temperature but still allows for the heat exchanger to accept additional heat. One example where this has been investigated is for use in high power aircraft electronics.\n\nHeat exchangers functioning in multiphase flow regimes may be subject to the Ledinegg instability.\n\nDirect contact heat exchangers involve heat transfer between hot and cold streams of two phases in the absence of a separating wall. Thus such heat exchangers can be classified as:\n\nMost direct contact heat exchangers fall under the Gas – Liquid category, where heat is transferred between a gas and liquid in the form of drops, films or sprays.\n\nSuch types of heat exchangers are used predominantly in air conditioning, humidification, industrial hot water heating, water cooling and condensing plants.\n\nMicrochannel heat exchangers are multi-pass parallel flow heat exchangers consisting of three main elements: manifolds (inlet and outlet), multi-port tubes with the hydraulic diameters smaller than 1mm, and fins. All the elements usually brazed together using controllable atmosphere brazing process. Microchannel heat exchangers are characterized by high heat transfer ratio, low refrigerant charges, compact size, and lower airside pressure drops compared to finned tube heat exchangers. Microchannel heat exchangers are widely used in automotive industry as the car radiators, and as condenser, evaporator, and cooling/heating coils in HVAC industry.\nMicro heat exchangers, Micro-scale heat exchangers, or microstructured heat exchangers are heat exchangers in which (at least one) fluid flows in lateral confinements with typical dimensions below 1 mm. The most typical such confinement are microchannels, which are channels with a hydraulic diameter below 1 mm. Microchannel heat exchangers can be made from metal, ceramic, Microchannel heat exchangers can be used for many applications including:\n\nOne of the widest uses of heat exchangers is for air conditioning of buildings and vehicles. This class of heat exchangers is commonly called \"air coils\", or just \"coils\" due to their often-serpentine internal tubing. Liquid-to-air, or air-to-liquid HVAC coils are typically of modified crossflow arrangement. In vehicles, heat coils are often called heater cores.\n\nOn the liquid side of these heat exchangers, the common fluids are water, a water-glycol solution, steam, or a refrigerant. For \"heating coils\", hot water and steam are the most common, and this heated fluid is supplied by boilers, for example. For \"cooling coils\", chilled water and refrigerant are most common. Chilled water is supplied from a chiller that is potentially located very far away, but refrigerant must come from a nearby condensing unit. When a refrigerant is used, the cooling coil is the evaporator in the vapor-compression refrigeration cycle. HVAC coils that use this direct-expansion of refrigerants are commonly called \"DX coils\". Some \"DX coils\" are \"microchannel\" type.\n\nOn the air side of HVAC coils a significant difference exists between those used for heating, and those for cooling. Due to psychrometrics, air that is cooled often has moisture condensing out of it, except with extremely dry air flows. Heating some air increases that airflow's capacity to hold water. So heating coils need not consider moisture condensation on their air-side, but cooling coils \"must\" be adequately designed and selected to handle their particular \"latent\" (moisture) as well as the \"sensible\" (cooling) loads. The water that is removed is called \"condensate\".\n\nFor many climates, water or steam HVAC coils can be exposed to freezing conditions. Because water expands upon freezing, these somewhat expensive and difficult to replace thin-walled heat exchangers can easily be damaged or destroyed by just one freeze. As such, freeze protection of coils is a major concern of HVAC designers, installers, and operators.\n\nThe introduction of indentations placed within the heat exchange fins controlled condensation, allowing water molecules to remain in the cooled air. This invention allowed for refrigeration without icing of the cooling mechanism.\n\nThe heat exchangers in direct-combustion furnaces, typical in many residences, are not 'coils'. They are, instead, gas-to-air heat exchangers that are typically made of stamped steel sheet metal. The combustion products pass on one side of these heat exchangers, and air to heat on the other. A \"cracked heat exchanger\" is therefore a dangerous situation that requires immediate attention because combustion products may enter living space.\n\nAlthough double-pipe heat exchangers are the simplest to design, the better choice in the following cases would be the helical-coil heat exchanger (HCHE):\n\n\nThese have been used in the nuclear industry as a method for exchanging heat in a sodium system for large liquid metal fast breeder reactors since the early 1970s, using an HCHE device invented by Charles E. Boardman and John H. Germer. There are several simple methods for designing HCHE for all types of manufacturing industries, such as using the Ramachandra K. Patil (et al.) method from India and the Scott S. Haraburda method from the United States.\n\nHowever, these are based upon assumptions of estimating inside heat transfer coefficient, predicting flow around the outside of the coil, and upon constant heat flux. Yet, recent experimental data revealed that the empirical correlations are quite in agreement for designing circular and square pattern HCHEs. During studies published in 2015, several researchers found that the boundary conditions of the outer wall of exchangers were essentially constant heat flux conditions in power plant boilers, condensers and evaporators; while convective heat transfer conditions were more appropriate in food, automobile and process industries.\n\nA modification to the perpendicular flow of the typical HCHE involves the replacement of shell with another coiled tube, allowing the two fluids to flow parallel to one another, and which requires the use of different design calculations. These are the Spiral Heat Exchangers (SHE), which may refer to a helical (coiled) tube configuration, more generally, the term refers to a pair of flat surfaces that are coiled to form the two channels in a counter-flow arrangement. Each of the two channels has one long curved path. A pair of fluid ports are connected tangentially to the outer arms of the spiral, and axial ports are common, but optional.\n\nThe main advantage of the SHE is its highly efficient use of space. This attribute is often leveraged and partially reallocated to gain other improvements in performance, according to well known tradeoffs in heat exchanger design. (A notable tradeoff is capital cost vs operating cost.) A compact SHE may be used to have a smaller footprint and thus lower all-around capital costs, or an oversized SHE may be used to have less pressure drop, less pumping energy, higher thermal efficiency, and lower energy costs.\n\nThe distance between the sheets in the spiral channels is maintained by using spacer studs that were welded prior to rolling. Once the main spiral pack has been rolled, alternate top and bottom edges are welded and each end closed by a gasketed flat or conical cover bolted to the body. This ensures no mixing of the two fluids occurs. Any leakage is from the periphery cover to the atmosphere, or to a passage that contains the same fluid.\n\nSpiral heat exchangers are often used in the heating of fluids that contain solids and thus tend to foul the inside of the heat exchanger. The low pressure drop lets the SHE handle fouling more easily. The SHE uses a “self cleaning” mechanism, whereby fouled surfaces cause a localized increase in fluid velocity, thus increasing the drag (or fluid friction) on the fouled surface, thus helping to dislodge the blockage and keep the heat exchanger clean. \"The internal walls that make up the heat transfer surface are often rather thick, which makes the SHE very robust, and able to last a long time in demanding environments.\"\nThey are also easily cleaned, opening out like an oven where any buildup of foulant can be removed by pressure washing.\n\nSelf-cleaning water filters are used to keep the system clean and running without the need to shut down or replace cartridges and bags.\n\nThere are three main types of flows in a spiral heat exchanger:\n\nThe Spiral heat exchanger is good for applications such as pasteurization, digester heating, heat recovery, pre-heating (see: recuperator), and effluent cooling. For sludge treatment, SHEs are generally smaller than other types of heat exchangers. These are used to transfer the heat.\n\nDue to the many variables involved, selecting optimal heat exchangers is challenging. Hand calculations are possible, but many iterations are typically needed. As such, heat exchangers are most often selected via computer programs, either by system designers, who are typically engineers, or by equipment vendors.\n\nTo select an appropriate heat exchanger, the system designers (or equipment vendors) would firstly consider the design limitations for each heat exchanger type.\nThough cost is often the primary criterion, several other selection criteria are important:\n\nSmall-diameter coil technologies are becoming more popular in modern air conditioning and refrigeration systems because they have better rates of heat transfer than conventional sized condenser and evaporator coils with round copper tubes and aluminum or copper fin that have been the standard in the HVAC industry. Small diameter coils can withstand the higher pressures required by the new generation of environmentally friendlier refrigerants. Two small diameter coil technologies are currently available for air conditioning and refrigeration products: copper microgroove and brazed aluminum microchannel.\n\nChoosing the right heat exchanger (HX) requires some knowledge of the different heat exchanger types, as well as the environment where the unit must operate. Typically in the manufacturing industry, several differing types of heat exchangers are used for just one process or system to derive the final product. For example, a kettle HX for pre-heating, a double pipe HX for the ‘carrier’ fluid and a plate and frame HX for final cooling. With sufficient knowledge of heat exchanger types and operating requirements, an appropriate selection can be made to optimise the process.\n\nOnline monitoring of commercial heat exchangers is done by tracking the overall heat transfer coefficient. The overall heat transfer coefficient tends to decline over time due to fouling.\n\nU=Q/AΔT\n\nBy periodically calculating the overall heat transfer coefficient from exchanger flow rates and temperatures, the owner of the heat exchanger can estimate when cleaning the heat exchanger is economically attractive.\n\nIntegrity inspection of plate and tubular heat exchanger can be tested in situ by the conductivity or helium gas methods. These methods confirm the integrity of the plates or tubes to prevent any cross contamination and the condition of the gaskets.\n\nMechanical integrity monitoring of heat exchanger tubes may be conducted through Nondestructive methods such as eddy current testing.\n\nFouling occurs when impurities deposit on the heat exchange surface.\nDeposition of these impurities can decrease heat transfer effectiveness significantly over time and are caused by:\n\nThe rate of heat exchanger fouling is determined by the rate of particle deposition less re-entrainment/suppression. This model was originally proposed in 1959 by Kern and Seaton.\n\nCrude Oil Exchanger Fouling. In commercial crude oil refining, crude oil is heated from to prior to entering the distillation column. A series of shell and tube heat exchangers typically exchange heat between crude oil and other oil streams to heat the crude to prior to heating in a furnace. Fouling occurs on the crude side of these exchangers due to asphaltene insolubility. The nature of asphaltene solubility in crude oil was successfully modeled by Wiehe and Kennedy. The precipitation of insoluble asphaltenes in crude preheat trains has been successfully modeled as a first order reaction by Ebert and Panchal who expanded on the work of Kern and Seaton.\n\nCooling Water Fouling.\nCooling water systems are susceptible to fouling. Cooling water typically has a high total dissolved solids content and suspended colloidal solids. Localized precipitation of dissolved solids occurs at the heat exchange surface due to wall temperatures higher than bulk fluid temperature. Low fluid velocities (less than 3 ft/s) allow suspended solids to settle on the heat exchange surface. Cooling water is typically on the tube side of a shell and tube exchanger because it's easy to clean. To prevent fouling, designers typically ensure that cooling water velocity is greater than and bulk fluid temperature is maintained less than . Other approaches to control fouling control combine the “blind” application of biocides and anti-scale chemicals with periodic lab testing.\n\nPlate and frame heat exchangers can be disassembled and cleaned periodically. Tubular heat exchangers can be cleaned by such methods as acid cleaning, sandblasting, high-pressure water jet, bullet cleaning, or drill rods.\n\nIn large-scale cooling water systems for heat exchangers, water treatment such as purification, addition of chemicals, and testing, is used to minimize fouling of the heat exchange equipment. Other water treatment is also used in steam systems for power plants, etc. to minimize fouling and corrosion of the heat exchange and other equipment.\n\nA variety of companies have started using water borne oscillations technology to prevent biofouling. Without the use of chemicals, this type of technology has helped in providing a low-pressure drop in heat exchangers.\n\nThe human nasal passages serve as a heat exchanger, which warms air being inhaled and cools air being exhaled. Its effectiveness can be demonstrated by putting the hand in front of the face and exhaling, first through the nose and then through the mouth. Air exhaled through the nose is substantially cooler. This effect can be enhanced with clothing, by, for example, wearing a scarf over the face while breathing in cold weather.\n\nIn species that have external testes (such as humans), the artery to the testis is surrounded by a mesh of veins called the pampiniform plexus. This cools the blood heading to the testes, while reheating the returning blood.\n\n\"Countercurrent\" heat exchangers occur naturally in the circulation system of fish, whales and other marine mammals. Arteries to the skin carrying warm blood are intertwined with veins from the skin carrying cold blood, causing the warm arterial blood to exchange heat with the cold venous blood. This reduces the overall heat loss in cold waters. Heat exchangers are also present in the tongue of baleen whales as large volumes of water flow through their mouths. Wading birds use a similar system to limit heat losses from their body through their legs into the water.\n\nThe carotid rete is a counter-current heat exchanging organ in some ungulates. The blood ascending the carotid arteries on its way to the brain, flows via a network of vessels where heat is discharged to the veins of cooler blood descending from the nasal passages. The carotid rete allows Thomson's gazelle to maintain its brain almost 3 °C (5.4 °F) cooler than the rest of the body, and therefore aids in tolerating bursts in metabolic heat production such as associated with outrunning cheetahs (during which the body temperature exceeds the maximum temperature at which the brain could function).\n\nHeat exchangers are widely used in industry both for cooling and heating large scale industrial processes. The type and size of heat exchanger used can be tailored to suit a process depending on the type of fluid, its phase, temperature, density, viscosity, pressures, chemical composition and various other thermodynamic properties.\n\nIn many industrial processes there is waste of energy or a heat stream that is being exhausted, heat exchangers can be used to recover this heat and put it to use by heating a different stream in the process. This practice saves a lot of money in industry, as the heat supplied to other streams from the heat exchangers would otherwise come from an external source that is more expensive and more harmful to the environment.\n\nHeat exchangers are used in many industries, including:\n\nIn waste water treatment, heat exchangers play a vital role in maintaining optimal temperatures within anaerobic digesters to promote the growth of microbes that remove pollutants. Common types of heat exchangers used in this application are the double pipe heat exchanger as well as the plate and frame heat exchanger.\n\nIn commercial aircraft heat exchangers are used to take heat from the engine's oil system to heat cold fuel. This improves fuel efficiency, as well as reduces the possibility of water entrapped in the fuel freezing in components.\n\nEstimated at US$42.7 billion in 2012, the global demand of heat exchangers will experience robust growth of about 7.8% annually over the next years. The market value is expected to reach US$57.9 billion by 2016 and to approach US$78.16 billion by 2020. Tubular heat exchangers and plate heat exchangers are still the most widely applied product types.\n\nA simple heat exchange might be thought of as two straight pipes with fluid flow, which are thermally connected. Let the pipes be of equal length \"L\", carrying fluids with heat capacity formula_1 (energy per unit mass per unit change in temperature) and let the mass flow rate of the fluids through the pipes, both in the same direction, be formula_2 (mass per unit time), where the subscript \"i\" applies to pipe 1 or pipe 2.\n\nTemperature profiles for the pipes are formula_3 and formula_4 where \"x\" is the distance along the pipe. Assume a steady state, so that the temperature profiles are not functions of time. Assume also that the only transfer of heat from a small volume of fluid in one pipe is to the fluid element in the other pipe at the same position, i.e., there is no transfer of heat along a pipe due to temperature differences in that pipe. By Newton's law of cooling the rate of change in energy of a small volume of fluid is proportional to the difference in temperatures between it and the corresponding element in the other pipe:\n\n, where formula_8 is the thermal energy per unit length and γ is the thermal connection constant per unit length between the two pipes. This change in internal energy results in a change in the temperature of the fluid element. The time rate of change for the fluid element being carried along by the flow is:\n\nwhere formula_11 is the \"thermal mass flow rate\". The differential equations governing the heat exchanger may now be written as:\n\nNote that, since the system is in a steady state, there are no partial derivatives of temperature with respect to time, and since there is no heat transfer along the pipe, there are no second derivatives in \"x\" as is found in the heat equation. These two coupled first-order differential equations may be solved to yield:\n\nwhere formula_16, formula_17,\n\nand \"A\" and \"B\" are two as yet undetermined constants of integration. Let formula_22 and formula_23 be the temperatures at x=0 and let formula_24 and formula_25 be the temperatures at the end of the pipe at x=L. Define the average temperatures in each pipe as:\n\nUsing the solutions above, these temperatures are:\n\nChoosing any two of the temperatures above eliminates the constants of integration, letting us find the other four temperatures. We find the total energy transferred by integrating the expressions for the time rate of change of internal energy per unit length:\n\nBy the conservation of energy, the sum of the two energies is zero. The quantity formula_30 is known as the \"Log mean temperature difference\", and is a measure of the effectiveness of the heat exchanger in transferring heat energy.\n\n\n\n"}
{"id": "3159493", "url": "https://en.wikipedia.org/wiki?curid=3159493", "title": "India paper", "text": "India paper\n\nIndia paper is a type of paper which from 1875 has been based on bleached hemp and rag fibres, that produced a very thin, tough opaque white paper. It has a basis weight of 20 pounds, yet bulks 1,000 pages to the inch.\n\nIt became popular in particular for the printing of Bibles, which could be made relatively small and light while remaining legible. The 1911 Encyclopædia Britannica boasted, \"Printed on thin, but strong opaque India paper, each volume but one inch in thickness.\" The process was used particularly by the Oxford University Press and its paper suppliers. The name arose because the paper imitated fine papers imported from the Indian subcontinent.\n\nIndia paper has also often been used for the printing of die proofs of postage stamps.\n\n"}
{"id": "857028", "url": "https://en.wikipedia.org/wiki?curid=857028", "title": "International Human Powered Vehicle Association", "text": "International Human Powered Vehicle Association\n\nThe International Human Powered Vehicle Association (IHPVA) is dedicated to promoting the design and development of human powered vehicles.\n\nThe IHPVA was founded in 1976 in the USA and was for many years an association of individual members with the publications Human Power and HPV News. In 1997, the IHPVA was reorganised into an international association with national organisations as members and an American association which adopted the name Human Powered Vehicle Association HPVA.\n\nDue to conflicts regarding record keeping and copyrights, the HPVA left the IHPVA in 2004. In 2008, the HPVA decided to rename itself to IHPVA while the IHPVA of this time decided to keep its name, resulting in a brief period with two organisations of the same name.\n\nIn a hostile takeover, the American IHPVA also seized control of the domain name ihpva.org away from the international IHPVA. This tried to regain its domain by appealing to the ICANN's ombudsman but was unsuccessful and in 2009 renamed itself to World Human Powered Vehicle Association (WHPVA).\n\nToday's IHPVA is an association of individuals from all over the world and has affiliate associations - The affiliates are:\n\n\nThe IHPVA maintains speed and distance records for various times and distances for land, water and air vehicles. The best hour record is currently held by Francesco Russo with a total distance of .\n\n"}
{"id": "50032711", "url": "https://en.wikipedia.org/wiki?curid=50032711", "title": "John Henry Holmes", "text": "John Henry Holmes\n\nJohn Henry Holmes (1857-1935) was an English electrical engineer, inventor, Quaker and pioneer of electric lighting who invented the quick break light switch, the technology behind which remains the basis for modern wall mounted light switches.\n\nIn 1880, Holmes attended a public demonstration of Joseph Swan's incandescent light bulb. This seemed to spark his interest in electric lighting, and he approached Swan on multiple occasions in hopes of becoming his apprentice.\n\nJohn Henry Holmes and his brother Theodore, also a Quaker, founded J. H. Holmes & Co. in Shieldfield, Newcastle upon Tyne in 1883, their manufacturing company specializing in early motors, dynamos, switches, and lighting. The company was very active in the early proliferation of electric lighting, having installed Newcastle's first domestic electrical lighting into their father's house, and supplied installations throughout Europe and the British colonies, making deals in the United States as well.\n\nJohn Henry Holmes invented the quick break light switch in 1884. which was patented in Great Britain and the United States that year. The technology augmented the standard switch by ensuring the internal contacts moved apart quickly enough to deter the electric arcing that created a fire hazard and otherwise invariably shortened the switch's lifespan. The quick break technology remains in use in domestic and industrial light switches modern times.\n"}
{"id": "35833065", "url": "https://en.wikipedia.org/wiki?curid=35833065", "title": "Kryptonium ion", "text": "Kryptonium ion\n\nThe kryptonium ion, KrH, is an onium compound, consisting of protonated krypton. Although the existence of the kryptonium cation itself has not been proven, salts of the fluorokryptonium ion, KrF, are known to exist.\n"}
{"id": "11554842", "url": "https://en.wikipedia.org/wiki?curid=11554842", "title": "Land grant to Marduk-apla-iddina I by Meli-Shipak II", "text": "Land grant to Marduk-apla-iddina I by Meli-Shipak II\n\nThe Land grant to Marduk-apla-iddina kudurru is a grey limestone 0.7-meter tall ancient Mesopotamian \"narû\" or entitlement stele recording the gift of four tracts of cultivated land with settlements totaling 84 160 \"qa\" by Kassite king of Babylon, Meli-Šipak (ca. 1186–1172 BC ), to a person described as his servant (arassu irīm: “he granted his servant”) named Marduk-apla-iddina, who may be his son and/or successor or alternatively another homonymous individual. The large size of the grant together with the generous freedom from all territorial obligations (taxation, corvée, draft, foraging) has led historians to assume he was the prince. There are thirty six kudurrus which are placed on the basis of art-history to Meli-Šipak's reign, of which eight are specifically identified by his name. This is the best preserved of all of them.\n\nThe kudurru was recovered in 1899 from Susa, excavation reference Sb 22, by the French archaeological expedition under the auspices of Jacques de Morgan and brought to the Musée du Louvre where it still resides. The object is inscribed on three sides in seven columns and 387 lines. Like most kudurrus, it portrays Mesopotamian gods graphically in segmented \"registers\" on the stone. In this case the divine icons number twenty-four in five registers, rather more than usual.\n\nThe bequest was for communal land of the city of Agade located around the settlement of Tamakku adjacent to the royal canal in Bīt-Piri’-Amurru, a province in northern Babylonia. In a passage granting exemptions from service and taxation to the residents of the transferred territory, a list of officials are forbidden from appropriating the land and levying labor with restrictions placed on their conduct. This includes the king himself, the \"šakin māti\" (the governor of the land), and the \"pīḫātu\" (rank uncertain) of Bīt-Piri’-Amurru, contradicting the image of oriental despotism sometimes portrayed concerning the period.\n\nThe text concludes with an unusual series of blessings and curses including a rather gruesome curse of Gula, “may she place in his body her oozing (sores), a persistent carbuncle, of no release, so that, for as long as he lives, he may bathe in blood and pus like water!”, which seems to have been reproduced on the Stele of Meli-Šipak. There are no witnesses listed to validate the bequest, more evidence to suggest it was a gift between royalty.\n\n\nThe survey team:\n\nThe iconic representations of the gods, where they are known, are given in the sequence left-to-right, top-to-bottom:\n\n\n"}
{"id": "28990256", "url": "https://en.wikipedia.org/wiki?curid=28990256", "title": "Laser fence", "text": "Laser fence\n\nA laser fence or laser wall is a mechanism to detect objects passing the line of sight between the laser source and the detector. Stronger lasers can be used to injure entities passing the laser beam. In fiction, laser fences may have the ability to stop intruders by blocking or injuring them.\n\nA laser fence mechanism detects objects passing the line of sight between a laser source and detector. Stronger lasers can be used to injure entities passing the laser beam, as in a mosquito laser. Fictional uses of laser fences often extend the concept so that fences may have the ability to prevent intruders by blocking or injuring them.\n\nLasers are used to keep birds from eating blueberries by being perceived as a predator. The European Commission is funding research into a laser fence to scare away rats and other pests.\n\nThe Indian Border Security Force (BSF) has implemented a laser wall system—called LASER Wall—along some parts of its border to stop intrusion.\n\nIn the 1992 film \"Fortress\", prisoners are kept secured by laser walls.\n\nLaser walls appear frequently in videogames. The concepts of these fictional fences can be compared to other concepts like tractor or repulsor beams and force fields.\n\n\n"}
{"id": "34023733", "url": "https://en.wikipedia.org/wiki?curid=34023733", "title": "Macquarie Generation", "text": "Macquarie Generation\n\nMacquarie Generation is an electricity generation company in New South Wales, Australia that is owned by AGL Energy, and has a portfolio of generating sites using predominantly thermal coal power. The company now trades as AGL Macquarie and generates electricity for sale under contract. \n\nAGL Macquarie supplies approximately 12% of the National Electricity Market and 30% of the New South Wales electricity market. In early stages, Macquarie has commenced development of solar thermal power as a renewable source of energy.\n\nMacquarie Generation was established by the Government of New South Wales in 1996 under the and the as part of the split up of the Electricity Commission of New South Wales.\n\nIn September 2014, the NSW Government sold Macquarie Generation to AGL Energy for $1.5 billion. Macquarie Generation‘s assets included the 2,640 MW Bayswater Power Station, the 2,000 MW Liddell Power Station, the 50 MW Hunter Valley Gas Turbines and the Liddell Solar Thermal Project. \n\nThe Liddell Power Station is scheduled to close in 2022.\n\nMacquarie Generation owns and operates the following power stations:\n\n"}
{"id": "1472404", "url": "https://en.wikipedia.org/wiki?curid=1472404", "title": "Metabolic ecology", "text": "Metabolic ecology\n\nField of ecology aiming to understand constraints on metabolic organisation, as important for understanding almost all life processes . Main focus is on the metabolism of individuals, emerging intra- and inter-specific patterns, and the evolutionary perspective. \n\nTwo main metabolic theories that have been applied in ecology are Kooijman’s Dynamic energy budget (DEB) theory and the West, Brown, and Enquist (WBE) theory of ecology . Both theories have an individual-based metabolic underpinning, but have fundamentally different assumptions .\n\nModels of individual's metabolism follow the energy uptake and allocation, and can focus on mechanisms and constraints of energy transport (transport models), or on dynamic use of stored metabolites (energy budget models) . \n"}
{"id": "2953922", "url": "https://en.wikipedia.org/wiki?curid=2953922", "title": "Microcrystalline wax", "text": "Microcrystalline wax\n\nMicrocrystalline waxes are a type of wax produced by de-oiling petrolatum, as part of the petroleum refining process. In contrast to the more familiar paraffin wax which contains mostly unbranched alkanes, microcrystalline wax contains a higher percentage of isoparaffinic (branched) hydrocarbons and naphthenic hydrocarbons. It is characterized by the fineness of its crystals in contrast to the larger crystal of paraffin wax. It consists of high molecular weight saturated aliphatic hydrocarbons. It is generally darker, more viscous, denser, tackier and more elastic than paraffin waxes, and has a higher molecular weight and melting point. The elastic and adhesive characteristics of microcrystalline waxes are related to the non-straight chain components which they contain. Typical microcrystalline wax crystal structure is small and thin, making them more flexible than paraffin wax. It is commonly used in cosmetic formulations.\n\nMicrocrystalline waxes when produced by wax refiners are typically produced to meet a number of ASTM specifications. These include congeal point (ASTM D938), needle penetration (D1321), color (ASTM D6045), and viscosity (ASTM D445). Microcrystalline waxes can generally be put into two categories: \"laminating\" grades and \"hardening\" grades. The laminating grades typically have a melt point of 140-175 F (60 - 80 C) and needle penetration of 25 or above. The hardening grades will range from about 175-200 F (80 - 93 C), and have a needle penetration of 25 or below. Color in both grades can range from brown to white, depending on the degree of processing done at the refinery level.\n\nMicrocrystalline waxes are derived from the refining of the heavy distillates from lubricant oil production. This by-product must then be de-oiled at a wax refinery. Depending on the end use and desired specification, the product may then have its odor removed and color removed (which typically starts as a brown or dark yellow). This is usually done by means of a filtration method or by hydro-treating the wax material.\n\nMicrocrystalline wax is often used in industries such as tire and rubber, candles, adhesives, corrugated board, cosmetics, castings, and others. Refineries may use blending facilities to combine paraffin and microcrystalline waxes; this is prevalent in the tire and rubber industries.\n\nMicrocrystalline waxes have considerable application in the custom making of jewelry and small sculptures. Different formulations produce waxes from those soft enough to be molded by hand to those hard enough to be carved with rotary tools. The melted wax can be cast to make multiple copies that are further carved with details. Jewelry suppliers sell wax molded into the basic forms of rings as well as details that can be heat welded together and tubes and sheets for cutting and building the wax models. Rings may be attached to a wax \"tree\" so that many can be cast in one pouring.\n\nA brand of microcrystalline wax, Renaissance Wax, is also used extensively in museum and conservation settings for protection and polishing of antique woods, ivory, gemstones, and metal objects. It was developed by The British Museum in the 1950s to replace the potentially unstable natural waxes that were previously used such as beeswax and carnauba. \n\nMicrocrystalline waxes are excellent materials to use when modifying the crystalline properties of paraffin wax. The microcrystalline wax has significantly more branching of the carbon chains that are the backbone of paraffin wax. This is useful when some desired functional changes in the paraffin are needed, such as flexibility, higher melt point, and increased opacity.\nThey are also used as slip agents in printing ink.\n\nMicrocrystalline wax is used in sports too, specifically in ice hockey and snowboarding. It is applied to the friction tape of an ice hockey stick to prevent degradation of the tape due to water destroying the glue on the tape and also to increase control of the hockey puck due to the wax’s adhesive quality. It is also applied to the underside of snowboards to reduce friction and increase the gliding ability of the board, making it easier to control.\n\nMicrocrystalline wax was used in the final phases of the restoration of the Cosmati pavement, Westminster Abbey, London.\n\nMicrocrystalline wax is also a key component in the manufacture of petrolatum. The branched structure of the carbon chain backbone allows oil molecules to be incorporated into the crystal lattice structure. The desired properties of the petrolatum can be modified by using microcrystalline wax bases of different congeal points (ASTM test D938) and needle penetration (ASTM D1321).\n\nHowever, key industries that utilize petrolatum, such as the personal care, cosmetic, and candle industries, have pushed for more materials that are considered \"green\" and based on renewable resources. As an alternative, hybrid petrolatum can be used. Hybrid petrolatum utilizes a complex mixture of vegetable oils and waxes and combines them with petroleum and micro wax based technologies. This allows a formulator to incorporate higher percentages of renewable resources while maintaining the beneficial properties of the petrolatum.\n\n\n"}
{"id": "13020876", "url": "https://en.wikipedia.org/wiki?curid=13020876", "title": "Micromeritics", "text": "Micromeritics\n\nThe term micromeritics was given to the science and technology of small particles by J. M. DallaValle. It is thus the study of the fundamental and derived properties of individual as well as a collection of particles. The knowledge and control of the size of particles is of importance in pharmacy and materials science. The size, and hence the surface area of a particle, can be related to the physical, chemical and pharmacologic properties of drugs. Clinically, the size of a drug can affect its release from dosage forms that are administered orally, parenterally, rectally and topically. The successful formulation of suspensions, emulsions and tablets; both physical stability and pharmacologic response also depends on the particle size achieved in the product.\n\nThe term was created by J. M. DallaValle in his book \"Micromeritics: the technology of fine particles\". It was derived from the Greek words for \"small\" and \"part\". The size range which he covered in the book was from 10 to 10 micrometers. Anything smaller than this but bigger than a molecule was referred to at the time as a colloid but is now often referred to as a nanoparticle. Applications included soil physics, mineral physics, chemical engineering, geology, and hydrology. Characteristics discussed included particle size and shape, packing, electrical, optical, chemical and surface science.\n\nParticle size and surface area influence the release of a drug from a dosage form that is administered orally, rectally, parenterally, and topically. Higher surface area brings about intimate contact of the drug with the dissolution fluids in vivo and increases the drug solubility and dissolution.\n\nParticle size and surface area influence the drug absorption and subsequently the therapeutic action. The higher the dissolution, the faster the absorption and hence the quicker and greater the drug action.\n\nMicromeritic properties of a particle, i.e. the particle size in a formulation, influence the physical stability of the suspensions and emulsions. The smaller the size of the particle, the better the physical stability of the dosage form owing to the Brownian motion of the particles in the dispersion.\n\nGood flow properties of granules and powders are important in the manufacturing of tablets and capsules. The distribution of particles should be uniform in terms of number and weight. Very small particle size causes attraction, which in turn destabilises the suspension by coagulating.\n"}
{"id": "6243154", "url": "https://en.wikipedia.org/wiki?curid=6243154", "title": "Microporous material", "text": "Microporous material\n\nA microporous material is a material containing pores with diameters less than 2 nm. Examples of microporous materials include zeolites and metal-organic frameworks.\n\nPorous materials are classified into several kinds by their size. The recommendations of a panel convened by the International Union of Pure and Applied Chemistry (IUPAC) are:\n\nMicropores may be defined differently in other contexts. For example, in the context of porous aggregations such as soil, micropores are defined as cavities with sizes less than 30 μm.\n\nMicroporous materials are often used in laboratory environments to facilitate contaminant-free exchange of gases. Mold spores, bacteria, and other airborne contaminants will become trapped, while allowing gases to pass through the material. This allows for a sterile environment within the contained area.\n\nMicroporous adhesive tape is a surgical tape used to hold wound dressings and bandages in place, introduced in 1959 by 3M with the trade name \"Micropore\". It can be used to hold gauze padding over small wounds, usually as a temporary measure until a suitable dressing is applied. The Steri-Strip was derived from Microporous surgical tape.\n\nMicroporous tape is used by some professional extreme yo-yoers to wrap around their fingers and prevent string burn or irritation.\n\nRock climbers use microporous tape to wrap their hands in 'tape gloves', a means of protecting the skin from rock abrasion when jamming hands into cracks as a means of ascent (crack climbing, as opposed to face climbing - gripping holds on the face of the rock).\n\nMicroporous media used in large format printing applications normally with a pigment based ink to maintain colour balance and life expectancy of the resultant printed image.\n\nMicroporous tape is also used by some film and TV sound recordists to affix small radio microphones to actors' skin.\n\nMicroporous material is also used as high performance insulation material used from home applications up to metal furnaces requiring material that can withstand more than 1000 Celsius.\n\n"}
{"id": "3485953", "url": "https://en.wikipedia.org/wiki?curid=3485953", "title": "Moringa oleifera", "text": "Moringa oleifera\n\nMoringa oleifera is the most widely cultivated species in the genus \"Moringa\", the only genus in the plant family Moringaceae. Common names include moringa, drumstick tree (from the long, slender, triangular seed-pods), horseradish tree (from the taste of the roots, which resembles horseradish), and ben oil tree or benzoil tree (from the oil which is derived from the seeds).\n\n\"M. oleifera\" is a fast-growing, drought-resistant tree, native to tropical and subtropical regions of South Asia. It is widely cultivated for its young seed pods and leaves used as vegetables and for traditional herbal medicine. It is also used for water purification. \"M. oleifera\" is considered to be an aggressive invasive species.\n\n\"Moringa\" derives from a Tamil word, \"murungai\", meaning \"twisted pod\", alluding to the young fruit.\n\n\"M. oleifera\" is a fast-growing, deciduous tree that can reach a height of 10–12 m (32–40 ft) and trunk diameter of 45 cm (1.5 ft). The bark has a whitish-grey colour and is surrounded by thick cork. Young shoots have purplish or greenish-white, hairy bark. The tree has an open crown of drooping, fragile branches and the leaves build up a feathery foliage of tripinnate leaves.\n\nThe flowers are fragrant and asexual, surrounded by five unequal, thinly veined, yellowish-white petals. The flowers are about 1.0–1.5 cm (1/2\") long and 2.0 cm (3/4\") broad. They grow on slender, hairy stalks in spreading or drooping flower clusters which have a length of 10–25 cm.\n\nFlowering begins within the first six months after planting. In seasonally cool regions, flowering only occurs once a year between April and June. In more constant seasonal temperatures and with constant rainfall, flowering can happen twice or even all year-round.\n\nThe fruit is a hanging, three-sided brown capsule of 20–45 cm size which holds dark brown, globular seeds with a diameter around 1 cm. The seeds have three whitish papery wings and are dispersed by wind and water.\n\nIn cultivation, it is often cut back annually to 1–2 m (3–6 ft) and allowed to regrow so the pods and leaves remain within arm's reach.\n\nThe moringa tree is grown mainly in semiarid, tropical, and subtropical areas, corresponding in the United States to USDA hardiness zones 9 and 10. It tolerates a wide range of soil conditions, but prefers a neutral to slightly acidic (pH 6.3 to 7.0), well-drained sandy or loamy soil. In waterlogged soil, the roots have a tendency to rot. Moringa is a sun- and heat-loving plant, and does not tolerate freezing or frost. Moringa is particularly suitable for dry regions, as it can be grown using rainwater without expensive irrigation techniques.\n\nIndia is the largest producer of moringa, with an annual production of 1.2 million tonnes of fruits from an area of 380 km².\n\nMoringa is grown in home gardens in West Bengal and Odisha and as living fences in southern India and Thailand, where it is commonly sold in local markets. In the Philippines and Indonesia, it is commonly grown for its leaves which are used as food. Moringa is also actively cultivated by the World Vegetable Center in Taiwan, a center for vegetable research. In Haiti, it is grown as windbreaks and to help reduce soil erosion.\n\nMore generally, moringa grows in the wild or is cultivated in Central America and the Caribbean, northern countries of South America, Africa, Southeast Asia and various countries of Oceania.\n\nAs of 2010, cultivation in Hawaii, for commercial distribution in the United States, is in its early stages.\n\nIn tropical cultivation, soil erosion is a major problem. Therefore, the soil treatment has to be as shallow as possible. Plowing is required only for high planting densities. In low planting densities, \"it is better to dig pits and refill them with the soil. This ensures good root system penetration without causing too much land erosion. The pits must be 30 to 50 cm deep, and 20 to 40 cm wide.\"\n\nMoringa can be propagated from seed or cuttings. \nDirect seeding is possible because the germination rate of \"M. oleifera\" is high. Moringa seeds can be germinated year-round in well-draining soil. Cuttings of 1 m length and at least 4 cm diameter can be used for vegetative propagation.\n\nFor intensive leaf production, \"the spacing of plants should be 15 x 15 cm or 20 x 10 cm, with conveniently spaced alleys (for example: every 4 m) to facilitate plantation management and harvests.\" Weeding and disease prevention are difficult because of the high density. In a semi-intensive production, the plants are spaced 50 cm to 1 m apart. This gives good results with less maintenance.\n\nMoringa trees can also be cultivated in alleys, as natural fences and associated with other crops. The distance between moringa rows in an agroforestry cultivation is usually between 2 and 4 m.\n\nIn India, from where moringa most likely originated, the diversity of wild types is large. This gives a good basis for breeding programs. In countries where moringa has been introduced, the diversity is usually much smaller among the cultivar types. Locally well-adapted wild types, though, can be found in most regions.\n\nBecause moringa is cultivated and used in different ways, there are different breeding aims. The breeding aims for an annual or a perennial plant are obviously different. The yield stability of fruits is an important breeding aim for the commercial cultivation in India, where moringa is cultivated as an annual. On less favorable locations, perennial cultivation has big advantages. Erosion is much smaller with perennial cultivation. In Pakistan, varieties have been tested for their nutritional composition of the leaves on different locations. \nThe different breeding aims result in a different selection. India selects for a higher number of pods and dwarf or semidwarf varieties. Breeders in Tanzania, though, are selecting for higher oil content.\n\n\"M. oleifera\" can be cultivated for its leaves, pods, and/or its kernels for oil extraction and water purification. The yields vary widely, depending on season, variety, fertilization, and irrigation regimen. Moringa yields best under warm, dry conditions with some supplemental fertilizer and irrigation. Harvest is done manually with knives, sickles, and stabs with hooks attached. Pollarding, coppicing, and lopping or pruning are recommended to promote branching, increase production, and facilitate harvesting.\n\nWhen the plant is grown from cuttings, the first harvest can take place 6–8 months after planting. Often, the fruits are not produced in the first year, and the yield is generally low during the first few years. By year two, it produces around 300 pods, by year three around 400–500. A good tree can yield 1000 or more pods. In India, a hectare can produce 31 tons of pods per year. Under North Indian conditions, the fruits ripen during the summer. Sometimes, particularly in South India, flowers and fruit appear twice a year, so two harvests occur, in July to September and March to April.\n\nAverage yields of 6 tons/ha/year in fresh matter can be achieved. The harvest differs strongly between the rainy and dry seasons, with 1120 kg/ha per harvest and 690 kg/ha per harvest, respectively. The leaves and stems can be harvested from the young plants 60 days after seeding and then another seven times in the year. At every harvest, the plants are cut back to within 60 cm of the ground. In some production systems, the leaves are harvested every 2 weeks.\n\nThe cultivation of \"M. oleifera\" can also be done intensively with irrigation and fertilization with suitable varieties. Trials in Nicaragua with 1 million plants per hectare and 9 cuttings/year over 4 years gave an average fresh matter production of 580 metric tons/ha/year, equivalent to about 174 metric tons of fresh leaves.\n\nOne estimate for yield of oil from kernels is 250 l/ha. The oil can be used as a food supplement, as a base for cosmetics, and for hair and the skin.\n\nThe moringa tree is not affected by any serious diseases in its native or introduced ranges. In India, several insect pests are seen, including various caterpillars such as the bark-eating caterpillar, the hairy caterpillar or the green leaf caterpillar. The budworms Noctuidae are known to cause serious defoliation. Damaging agents can also be aphids, stem borers, and fruit flies. In some regions, termites can also cause minor damage. If termites are numerous in soils, insects management costs are not bearable.\n\nThe moringa tree is a host to \"Leveillula taurica\", a powdery mildew which causes damage in papaya crops in south India. Cultivation management should therefore be checked.\n\nMany parts of moringa are edible, with regional uses varying widely:\n\nNutritional content of 100 g of fresh \"M. oleifera\" leaves (about 5 cups) is shown in the table (right; USDA data), while other studies of nutrient values are available.\nThe leaves are the most nutritious part of the plant, being a significant source of B vitamins, vitamin C, provitamin A as beta-carotene, vitamin K, manganese, and protein, among other essential nutrients. When compared with common foods particularly high in certain nutrients per 100 g fresh weight, cooked moringa leaves are considerable sources of these same nutrients. \nSome of the calcium in moringa leaves is bound as crystals of calcium oxalate though at levels 1/25th to 1/45th of that found in spinach, which is a negligible amount.\n\nThe leaves are cooked and used like spinach and are commonly dried and crushed into a powder used in soups and sauces.\n\nThe immature seed pods, called \"drumsticks\", are commonly consumed in South Asia. They are prepared by parboiling, and cooked in a curry until soft. The seed pods/fruits, even when cooked by boiling, remain particularly high in vitamin C (which may be degraded variably by cooking) and are also a good source of dietary fiber, potassium, magnesium, and manganese.\n\nThe seeds, sometimes removed from more mature pods and eaten like peas or roasted like nuts, contain high levels of vitamin C and moderate amounts of B vitamins and dietary minerals.\n\nMature seeds yield 38–40% edible oil called ben oil from its high concentration of behenic acid. The refined oil is clear and odorless, and resists rancidity. The seed cake remaining after oil extraction may be used as a fertilizer or as a flocculent to purify water. Moringa seed oil also has potential for use as a biofuel.\n\nThe roots are shredded and used as a condiment with sharp flavor qualities deriving from significant content of polyphenols.\n\nMoringa trees have been used to combat malnutrition, especially among infants and nursing mothers. Since moringa thrives in arid and semiarid environments, it may provide a versatile, nutritious food source throughout the year. Moringa leaves have been proposed as an iron-rich food source (31% Daily Value per 100 g consumed, table) to combat iron deficiency. However, further study is needed to test practical applications of using this dietary source and its iron bioavailability.\n\nMoringa has numerous applications in cooking throughout its regional distribution. The fruits or seed pods, known as \"drumsticks\", are a culinary vegetable commonly used in soups and curries. The leaves are also commonly eaten with many culinary uses, and the flowers are featured in some recipes as well.\n\nThe long drumsticks are often cut into shorter lengths and stewed in curries and soups. Because the outer skin is tough and fibrous, drumsticks are often chewed to extract the juices and nutrients, with the remaining fibrous material discarded. Others describe a slightly different method of sucking out the flesh and tender seeds and discarding the tube of skin. \nTraditional dishes which commonly include drumsticks prepared this way include South Indian \"sambar\" where it is stewed with lentils, and the Thai dish \"kaeng som\" which is a sour curry with drumsticks and fish.\n\nThe leaves can be used in many ways, perhaps most commonly added to clear broth-based soups, such as the Filipino dishes \"tinola\" and \"utan\". Tender moringa leaves, finely chopped, are used as garnish for vegetable dishes and salads, such as the Kerala dish \"thoran\". It is also used in place of or along with coriander.\n\nIn some regions of India, the flowers are mixed with gram flour and spices, then deep-fried into \"pakoras\" to be served as snacks or added to curries.\n\nThe bark, sap, roots, leaves, seeds and flowers are used in traditional medicine. Research has examined how it might affect blood lipid profiles, although it is not effective at diagnosing, treating, or preventing any human diseases.\n\nExtracts from leaves contain low contents of polyphenols which are under basic research for their potential properties. Despite considerable preliminary research on the biological properties of moringa components, few high-quality studies on humans justify its use to treat human diseases.\n\nVarious adverse effects may occur from consuming moringa bark, roots, or flowers and their extracts, as these components contain chemicals that appear to be toxic when eaten. Moringa has been used safely in doses up to daily for up to 3 weeks.\n\nIn developing countries, moringa has the potential to improve nutrition, boost food security, foster rural development, and support sustainable landcare. It may be used as forage for livestock, a micronutrient liquid, a natural anthelmintic, and possible adjuvant.\n\n\"Moringa oleifera\" leaf powder was as effective as soap for hand washing when wetted in advance to enable anti-septic and detergent properties from phytochemicals in the leaves. \"Moringa oleifera\" seeds and press cake have been implemented as wastewater conditioners for dewatering and drying fecal sludge.\n\nMoringa seed cake, obtained as a byproduct of pressing seeds to obtain oil, is used to filter water using flocculation to produce potable water for animal or human consumption. Moringa seeds contain dimeric cationic proteins which absorb and neutralize colloidal charges in turbid water, causing the colloidal particles to clump together, making the suspended particles easier to remove as sludge by either settling or filtration. Moringa seed cake removes most impurities from water. This use is of particular interest for being nontoxic and sustainable compared to other materials in moringa-growing regions where drinking water is affected by pollutants.\n\n"}
{"id": "39688248", "url": "https://en.wikipedia.org/wiki?curid=39688248", "title": "N-topological space", "text": "N-topological space\n\nIn mathematics, an \"N\"-topological space is a set equipped with \"N\" arbitrary topologies. If \"τ\", \"τ\", ..., \"τ\" are \"N\" topologies defined on a nonempty set X, then the \"N\"-topological space is denoted by (\"X\",\"τ\",\"τ\"...,\"τ\").\nFor \"N\" = 1, the structure is simply a topological space.\nFor \"N\" = 2, the structure becomes a bitopological space introduced by J. C. Kelly.\n\nLet \"X\" = {\"x\", \"x\", ..., \"x\"} be any finite set. Suppose \"A\" = {\"x\", \"x\", ..., \"x\"}. Then the collection \"τ\" = {\"φ\", \"A\", \"A\", ..., \"A\" = \"X\"} will be a topology on \"X\". If \"τ\", \"τ\", ..., \"τ\" be \"m\" such topologies (chain topologies) defined on \"X\", then the structure (\"X\", \"τ\", \"τ\", ..., \"τ\") is an \"m\"-topological space.\n"}
{"id": "40921669", "url": "https://en.wikipedia.org/wiki?curid=40921669", "title": "Nonacosylic acid", "text": "Nonacosylic acid\n\nNonacosylic acid, or \"Nonacosanoic acid\", is a 29-carbon long-chain saturated fatty acid with the chemical formulaCH(CH)COOH.\n\n"}
{"id": "42792237", "url": "https://en.wikipedia.org/wiki?curid=42792237", "title": "Operation Desert (German fuel project)", "text": "Operation Desert (German fuel project)\n\nOperation \"Desert\" () was a German synthetic fuel project during World War II. It attempted to build a shale oil industrial production complex for utilization of Swabian Alb oil shale deposits (Posidonia Shale).\n\nThe project was driven by the fuel needs of the German army at the last phase of World War II due to decreasing conventional petroleum supplies. Three companies conducted pilot tests. \"LIAS-Ölschiefer-Forschungsgesellschaft mbH\", established in September 1942, started tests in Frommern. \"Kohle-Öl-Union von Busse KG\", established on 30 July 1943 in Berlin, tested \"in-situ\" retorting on the outskirts of Schörzingen. \"Deutsche Ölschiefer-Forschungsgesellschaft mbH\", established on 20 September 1943 in Schömberg, became later the core of the Operation \"Desert\". \"Schutzstaffel\" (SS) and Hermann Göring personally became involved in the project in late 1943. On 2 May 1944, SS established oil shale company \"Deutsche Schieferöl GmbH\" for its own shale oil plant near Erzingen. Also IG Farben became involved in shale oil. In July 1944, Operation \"Desert\" became a part of the \"Geilenberg Programme\".\n\nFor the Operation \"Desert\" construction of ten shale oil extraction plants in Württemberg and Hohenzollern were ordered by Edmund Geilenberg. Prisoners from seven nearby subcamps of the Natzweiler-Struthof concentration camp, established by the route of Tübingen - Aulendorf and Nebenstrecke Balingen- Rottweil railway lines, were used as a workforce. The main contractor for building these plants was \"Deutsche Bergwerks- und Hüttenbau GmbH\", a subsidiary of \"Reichswerke Hermann Göring\". About 5,000 prisoners were used for construction works and more than 10,000 prisoners had been exploited for the oil-shale works. When the Soviet troops advance into Estonia in 1944, about 200 oil shale specialists from Estonia, employees of \"Baltische Öl GmbH \", an affiliate of IG Farben, were evacuated to Schömberg.\n\nHowever, out of ten planned plants only four became operational. The technology was primitive carbonization and oil yield was low. Mined oil shale was heaped into mounds which were ignited after covering by peat. Distilled shale oil and oil shale gas were collected through perforated pipes. Oil was separated from gas by electric filters. Until the end of the war, only 1,500 tonnes of shale oil was produced.\n\nAfter the war French occupation forces tried continue the shale oil production operations but due to unprofitability it was halted in 1948.\n\n"}
{"id": "51150814", "url": "https://en.wikipedia.org/wiki?curid=51150814", "title": "Sambhar Ultra Mega Solar Power Project", "text": "Sambhar Ultra Mega Solar Power Project\n\nSambhar Ultra Mega Solar Power Project is a proposed photo-voltaic solar power project with a cumulative capacity of 4,000 MW at Sambhar in Rajasthan state of India.\n\nSambhar is home to many species of birds. The proposed project was opposed by several environmental groups. The Union Government later decided to develop the acquired land in Sambhar as a tourist spot, and shift the proposed solar power station to Kharaghoda in Surendranagar district of Gujarat.\n\n"}
{"id": "3973096", "url": "https://en.wikipedia.org/wiki?curid=3973096", "title": "Saturn Relay", "text": "Saturn Relay\n\nThe Saturn Relay is a minivan that was made by General Motors. It was introduced for the 2005 model year, and was built alongside its sisters, the Buick Terraza, the Chevrolet Uplander, and the Pontiac Montana SV6 in Doraville, Georgia. The Relay was the first Saturn vehicle without polymer side paneling, the first Saturn that is a rebadged Chevrolet or Pontiac, and it was the first (and only) minivan produced by Saturn.\n\nThe Relay was introduced with a 3.5 L LX9 V6 that generates and torque, going from 0-60 mph in the 9-second range. For 2006, a 3.9 L LZ9 V6, with 240 hp (179 kW) and 240 lb·ft (332 Nm) torque, was added as an option, which delivered faster acceleration and better response than the 3.5L engine. For 2007, the 3.5 L V6 was dropped, leaving the 3.9 L as the base engine. Consequently, the optional AWD system was also dropped, since it could not handle the torque of the 3.9 L engine. Also in 2007, the Relay received an optional flex-fuel engine but only for fleet applications. The Relay scored three \"Good\"s (the highest possible score) and two \"acceptable\"s (the second highest possible score) in Insurance Institute for Highway Safety (IIHS) crash tests. In terms of gas mileage, the Relay is rated at city, highway.\n\nThe Relay started at US$22,850. There were three available trim levels, \"1\", \"2\", \"3\". The Relay 3 was available in front-wheel drive and in all-wheel drive. All Relays seat seven via folding/removable 2nd row captains chairs and a 50/50 third-row bench. The third-row bench folds flat, but did not fold entirely into the floor. OnStar assistance and a DVD rear entertainment system came standard on all Relays. A navigation system was optional on Relay 3s. Side airbags were optional on the Relay. The Relay was discontinued after the 2007 model year and was replaced by the 2007 Saturn Outlook.\n\n2006 models added the GM logos on the front doors.\n\nThe final model year of the Relay. The Saturn logos had been dropped from the front doors, and the all-wheel drive option was no longer available. The Relay was discontinued after the 2007 model year. The Doraville Assembly plant closed in September 2008. The last 2007 Relay rolled off the line on November 17, 2006. The Relay was replaced by the 8-passenger, Lambda-based Saturn Outlook crossover SUV.\n\n"}
{"id": "1548811", "url": "https://en.wikipedia.org/wiki?curid=1548811", "title": "Solar-powered watch", "text": "Solar-powered watch\n\nA solar-powered watch or light-powered watch is a watch that is powered entirely or partly by a solar cell.\n\nSome of the early solar watches of the 1970s had innovative and unique designs to accommodate the array of photovoltaic solar cells needed to power them (Synchronar, Nepro, Sicura and some models by Cristalonic, Alba, Rhythm, Seiko and Citizen). In the 1990s, Citizen started to sell light-powered watches under the Eco-Drive series name. Since their introduction, photovoltaic devices have greatly improved their efficiency and thereby their capacity. Watchmakers have developed their technology such that solar-powered watches are now a major part of their range of watches. Several other watch manufacturers offer similar watches or are developing such technology. These other watch manufacturers include (amongst others) Junghans, Casio, Seiko, and Orient.\n\nTypically, sunlight and artificial light are absorbed by a solar panel behind the crystal. The dial is either on a layer above or actually on the solar panel. This solar panel converts the light into electrical energy to power the watch. The watch will usually store energy in a rechargeable cell to power itself during the night or when covered such as a wearer's clothing (e.g., sleeve). Citizen's watches use lithium-ion batteries to store sufficient energy to power the watch for several months/years without light exposure, by allowing the watch to enter a power-saving or hibernation mode during which the seconds hand stops until the watch is re-exposed to light. Not all have a power-save mode, yet will still hold a charge for typically six months, as with uncomplicated (date only) analog versions made by Citizen.\n\nInexpensive solar-powered watches were first sold in the 1980s and were popular amongst children, often featuring famous fictional characters such as Transformers or G.I. Joe.\n"}
{"id": "20640450", "url": "https://en.wikipedia.org/wiki?curid=20640450", "title": "Spinning drop method", "text": "Spinning drop method\n\nThe spinning drop method (rotating drop method) is one of the methods used to measure interfacial tension. Measurements are carried out in a rotating horizontal tube which contains a dense fluid. A drop of a less dense liquid or a gas bubble is placed inside the fluid. Since the rotation of the horizontal tube creates a centrifugal force towards the tube walls, the liquid drop will start to deform into an elongated shape; this elongation stops when the interfacial tension and centrifugal forces are balanced. The surface tension between the two liquids (for bubbles: between the fluid and the gas) can then be derived from the shape of the drop at this equilibrium point. A device used for such measurements is called a “spinning drop tensiometer”.\n\nThe spinning drop method is usually preferred for the accurate measurements of surface tensions below 10 mN/m. It refers to either using the fluids with low interfacial tension or working at very high angular velocities. This method is widely used in many different applications such as measuring the interfacial tension of polymer blends and copolymers.\n\nAn approximate theory was developed by Bernard Vonnegut in 1942 to measure the surface tension of the fluids, which is based on the principle that the interfacial tension and centrifugal forces are balanced at mechanical equilibrium. This theory assumes that the droplet's length L is much greater than its radius R, so that it may be approximated as a straight circular cylinder.\nThe relation between the surface tension and angular velocity of a droplet can be obtained in different ways. One of them involves considering the total mechanical energy of the droplet as the summation of its kinetic energy and its surface energy:\n\nThe kinetic energy of a cylinder of length L and radius R rotating about its central axis is given by\n\nin which\n\nis the moment of inertia of a cylinder rotating about its central axis and \"ω\" is its angular velocity. \nThe surface energy of the droplet is given by\n\nin which V is the constant volume of the droplet and \"σ\" is the interfacial tension. \nThen the total mechanical energy of the droplet is\n\nin which Δ\"ρ\" is the difference between the densities of the droplet and of the surrounding fluid.\nAt mechanical equilibrium, the mechanical energy is minimized, and thus\n\nSubstituting in\n\nfor a cylinder and then solving this relation for interfacial tension yields\n\nThis equation is known as Vonnegut’s expression. Interfacial tension of any liquid that gives a shape very close to a cylinder at steady state, can be estimated using this equation. The straight cylindrical shape will always develop for sufficiently high ω; this typically happens for \"L\"/\"R\" > 4. Once this shape has developed, further increasing ω will decrease \"R\" while increasing \"L\" keeping \"LR\" fixed to meet conservation of volume.\n\nThe full mathematical analysis on the shape of spinning drops was done by Princen and others. Progress in numerical algorithms and available computing resources turned solving the non linear implicit parameter equations to a pretty much 'common' task, which has been tackled by various authors and companies. The results are proving the Vonnegut restriction is no longer valid for the spinning drop method.\n\nThe spinning drop method is convenient compared to other widely used methods for obtaining interfacial tension, because contact angle measurement is not required. Another advantage of the spinning drop method is that it is not necessary to estimate the curvature at the interface, which entails complexities associated with shape of the fluid drop.\n\nOn the other hand, this theory suggested by Vonnegut, is restricted with the rotational velocity. The spinning drop method is not expected to give accurate results for high surface tension measurements, since the centrifugal force that is required to maintain the drop in a cylindrical shape is much higher in the case of liquids that have high interfacial tensions.\n"}
{"id": "53713730", "url": "https://en.wikipedia.org/wiki?curid=53713730", "title": "Sunbury Research Centre", "text": "Sunbury Research Centre\n\nThe Sunbury Research Centre -- also known as ICBT Sunbury -- is a main research institute of BP in north-east Surrey.\n\nIt began in 1917 as the Sunbury Research Station. Research began with the employment of two chemists to look into the viscosity of fuel oil for the Navy in the First World War, and the production of toluene. In the 1920s research took place into cracking, at the plant at Uphall in Scotland (West Lothian). The first new building opened in July 1931. 76 staff were there in 1929, 99 in 1934 and 197 in 1939. \n\nBy the 1950s, BP Research was in a 39-acre site in Sunbury. Geophysical research had also taken place at Kirklington Hall Research Station in Nottinghamshire, until 1957. Around 1958, the site was expanded with a new Physics laboratory and five other buildings. A Linear electron accelerator was installed. By early 1958, Kirklington Hall had been sold.\n\nProducts that the British Petroleum Company made in the 1950s were BP Motor Spirit and BP Energol (\"visco-static\" motor oil). But Britain would not produce much oil of its own until the mid-1970s when North Sea oil arrived at the Forties Oil Field. \n\nThree new buildings were built from 1998 as part of Phase 1. Since 2001, four new buildings were built as part of Phase 2.\n\nIt is situated off the A244 (via the A308) in the north of Sunbury-on-Thames, and Surrey, on the Surrey boundary with London. To the east nearby is Sunbury Common.\n\nThe retail division of BP UK is at Witan Gate House. BP employs around 15,000 people in the UK.\n\nIt has an enhanced oil recovery laboratory.\n\n\n\n"}
{"id": "183932", "url": "https://en.wikipedia.org/wiki?curid=183932", "title": "T Tauri star", "text": "T Tauri star\n\nT Tauri stars (TTS) are a class of variable stars associated with youth. They are less than about ten million years old. This class is named after the prototype, T Tauri, a young star in the Taurus star-forming region. They are found near molecular clouds and identified by their optical variability and strong chromospheric lines. T Tauri stars are pre-main-sequence stars in the process of contracting to the main sequence along the Hayashi track, a luminosity–temperature relationship obeyed by infant stars of less than 3 solar masses () in the pre-main-sequence phase of stellar evolution. It ends when a star of develops a radiative zone, or when a larger star commences nuclear fusion on the main sequence.\n\nWhile T Tauri itself was discovered in 1852, the T Tauri class of stars were initially defined by Alfred Harrison Joy in 1945.\n\nT Tauri stars comprise the youngest visible F, G, K and M spectral type stars (). Their surface temperatures are similar to those of main-sequence stars of the same mass, but they are significantly more luminous because their radii are larger. Their central temperatures are too low for hydrogen fusion. Instead, they are powered by gravitational energy released as the stars contract, while moving towards the main sequence, which they reach after about 100 million years. They typically rotate with a period between one and twelve days, compared to a month for the Sun, and are very active and variable.\n\nThere is evidence of large areas of starspot coverage, and they have intense and variable X-ray and radio emissions (approximately 1000 times that of the Sun). Many have extremely powerful stellar winds; some eject gas in high-velocity bipolar jets. Another source of brightness variability are clumps (protoplanets and planetesimals) in the disk surrounding T Tauri stars.\n\nTheir spectra show a higher lithium abundance than the Sun and other main-sequence stars because lithium is destroyed at temperatures above 2,500,000 K. From a study of lithium abundances in 53 T Tauri stars, it has been found that lithium depletion varies strongly with size, suggesting that \"lithium burning\" by the P-P chain, during the last highly convective and unstable stages during the later pre–main sequence phase of the Hayashi contraction may be one of the main sources of energy for T Tauri stars. Rapid rotation tends to improve mixing and increase the transport of lithium into deeper layers where it is destroyed. T Tauri stars generally increase their rotation rates as they age, through contraction and spin-up, as they conserve angular momentum. This causes an increased rate of lithium loss with age. Lithium burning will also increase with higher temperatures and mass, and will last for at most a little over 100 million years. \n\nThe P-P chain for Lithium burning is as follows\n\nIt will not occur in stars with less than sixty times the mass of Jupiter (). In this way, the rate of lithium depletion can be used to calculate the age of the star.\n\nSeveral types of TTSs exist:\nRoughly half of T Tauri stars have circumstellar disks, which in this case are called protoplanetary discs because they are probably the progenitors of planetary systems like the Solar System. Circumstellar discs are estimated to dissipate on timescales of up to 10 million years. Most T Tauri stars are in binary star systems. In various stages of their life, they are called Young Stellar Objects (YSOs). It is thought that the active magnetic fields and strong solar wind of Alfvén waves of T Tauri stars are one means by which angular momentum gets transferred from the star to the protoplanetary disc. A T Tauri stage for the Solar System would be one means by which the angular momentum of the contracting Sun was transferred to the protoplanetary disc and hence, eventually to the planets.\n\nAnalogs of T Tauri stars in the higher mass range (2–8 solar masses)—A and B spectral type pre–main-sequence stars, are called Herbig Ae/Be-type stars. More massive (>8 solar masses) stars in pre–main sequence stage are not observed, because they evolve very quickly: when they become visible (i.e. disperses surrounding circumstellar gas and dust cloud), the hydrogen in the center is already burning and they are main sequence objects.\n\nPlanets around T Tauri stars include: \n\n\n"}
{"id": "5501120", "url": "https://en.wikipedia.org/wiki?curid=5501120", "title": "Thick-film dielectric electroluminescent technology", "text": "Thick-film dielectric electroluminescent technology\n\nThick-film dielectric electroluminescent (TDEL) technology is a phosphor-based flat panel display technology developed by Canadian company iFire Technology Corp. TDEL is based on inorganic electroluminescent (IEL) technology and has a novel structure that combines both thick- and thin-film processes. An IEL device generates light by applying an alternating electrical field to inorganic light-emitting phosphors. Traditional IEL displays are bright, very fast in video response time and highly tolerant of environmental extremes. However, the lack of full-color capability and large-size scalability has limited their application for the mainstream consumer television market. iFire has addressed these limitations by replacing the thin-film dielectric of traditional IEL technology with its patented thick-film, high-K dielectric material and structure. The result is a unique flat panel display technology that provides iFire displays with high performance and low cost potential. iFire was unable to develop displays competitive with LCD, plasma and OLED devices and wound up research and development in 2007.\n\nThe TDEL structure is made on a glass or other inexpensive substrate consisting of a thick-film dielectric layer and a thin-film phosphor layer squished between two sets of electrodes to make a matrix of pixels. It seems complex, but basically it works when phosphors emit light in the presence of an electric field. And because TDEL uses solid-state phosphors instead of liquids (as with LCDs), gases (as with PDP) or vacuum (as with the CRT), it is probably the most sturdy new technology, less prone to shock and breakage during shipping.\n\nIn 2003, iFire announced the development of a process, known as Color By Blue (CBB), which further simplified the already simple manufacturing process for TDEL. The simpler Color by Blue manufacturing process was made possible by performance improvements to iFire’s blue inorganic phosphor. The Color By Blue process achieves luminance and color superior to the previous triple pattern process, as well as increased contrast, better grayscale rendition and exceptional color uniformity across the panel.\n\nColor By Blue is based on the physics of photoluminescence. With CBB, iFire’s high luminance inorganic blue phosphor is used in combination with special color conversion materials, which absorb the blue light and re-emit red or green light, to generate the other colors. This fluorescence is possible because the photons in blue light operate at higher frequencies than other light, and therefore have higher energy. With optimum color-conversion materials the conversion factors and the color spectrum of the display will exceed the requirements of HDTV systems.\n\nBesides its durability, TDEL's claim to fame is its low cost of production. According to the makers, TDEL display would have half the capital and manufacturing costs of a similar LCD or PDP.\n\niFire claims TDEL also has picture quality similar to CRT TVs. The displays are brighter, more efficient, more resistant to contamination during manufacturing, and more resistant to electrical breakdown than their thin-film counterpart (TFEL), but with less dark contrast and contrast in bright lighting.\niFire has moved on from a 17-inch prototype to a 34-inch full-colour display.\n\nFollowing pilot manufacturing, iFire expected to begin commercial volume production with capacity in the range of 250,000 units per year. Initial planning work for a volume facility is underway and iFire is reviewing its commercialization strategy with potential manufacturing partners.\n\niFire ceased research in 2007 and all physical and intellectual property assets were sold in 2009.\n\n\n\n"}
{"id": "18837003", "url": "https://en.wikipedia.org/wiki?curid=18837003", "title": "Titanium sublimation pump", "text": "Titanium sublimation pump\n\nA titanium sublimation pump (TSP) is a type of vacuum pump used to remove residual gas in ultra high vacuum systems, maintaining the vacuum.\n\nIts construction and principle of operation is simple. It consists of a titanium filament through which a high current (typically around 40 Amps) is passed periodically. This current causes the filament to reach the sublimation temperature of titanium, and hence the surrounding chamber walls become coated with a thin film of clean titanium. Since clean titanium is very reactive, components of the residual gas in the chamber which collide with the chamber wall are likely to react and to form a stable, solid product. Thus the gas pressure in the chamber is reduced.\nBut after some time, the titanium film will no longer be clean and hence the effectiveness of the pump is reduced. Therefore, after a certain time, the titanium filament should be heated again, and a new film of titanium re-deposited on the chamber wall. Since the time taken for the titanium film to react depends on a number of factors (such as the composition of the residual gas, the temperature of the chamber and the total pressure), the period between successive sublimations requires some consideration. Typically, the operator does not know all of these factors, so the sublimation period is estimated according to the total pressure and by observing the effectiveness of the outcome. Some TSP controllers use a signal from the pressure gauge to estimate the appropriate period.\n\nSince the TSP filament has a finite lifetime, TSPs commonly have multiple filaments to allow the operator to switch to a new one without needing to open the chamber. Replacing used filaments can then be combined with other maintenance jobs.\n\nThe effectiveness of the TSP depends on a number of factors. Amongst the most critical are; the area of the titanium film, the temperature of the chamber walls and the composition of the residual gas. The area is typically maximised when considering where to mount the TSP. The reactivity of the new titanium film is increased at lower temperatures, so it is desirable to cool the relevant part of the chamber, typically using liquid nitrogen. However, due to the cost of the nitrogen and the need to ensure a continuous supply, TSPs are commonly operated at room temperature. Finally the residual gas composition is important - typically the pump works well with the more reactive components (such as CO and O), but is very ineffective at pumping inert components such as the noble gasses. Therefore, TSP must be used in conjunction with other pumps.\n\nOther pumps which use exactly the same working principle, but using something other than titanium as a source are also relatively common. This family of pumps are usually called 'getter pumps' or 'getters' and typically consist of metals which are reactive with the components of the residual gas which are not pumped by the TSP. By choosing a number of such sources, most constituents of the residual gas, except for the noble gases, can be targeted.\n\nWhen mounting the TSP in the chamber, a number of important considerations must be made. First, it is desirable that the filament can deposit on a large area. However, one must take care that the titanium is not deposited onto anything it can damage. For example, electrical feedthroughs containing ceramic insulators will fail if the titanium forms a conducting film which bridges the ceramic insulator. Samples may become contaminated by titanium if they have line-of-sight to the pump. Also, titanium is a very hard material, so titanium film which builds up on the inside of the chamber may form flakes which fall into mechanical components (typically turbomolecular pumps and valves) and damage them.\n\nMany chambers containing TSPs also have an ion pump. Often the ion pump provides a good location for the TSP, and some manufacturers promote the use of combined TSP/ion-pumps. Furthermore, TSPs have been shown to be effective against the regurgitation effects of ion pumps.\n"}
{"id": "10343439", "url": "https://en.wikipedia.org/wiki?curid=10343439", "title": "Victorian Model Solar Vehicle Challenge", "text": "Victorian Model Solar Vehicle Challenge\n\nThe Victorian Model Solar Vehicle Challenge is a competition held annually at Scienceworks in Melbourne, Australia. The challenge gives school age children a chance to design and build a car or boat, and more recently a Mars Rover, that operates solely on solar power. It is run by MSV, a voluntary organisation created to administer the challenge.\n\nA 'solar car challenge for schools' was first thought of in 1987 during the first World Solar Challenge. Everyone who attended—car makers, staff and students from universities and secondary schools—believed it to be a great experience. Two men, Paul Wellington (teacher at Chisholm Institute of Technology, part of Monash University), and Ted Mellor (Warragul Technical School), felt that such an opportunity should be available to a wider range of students, because at the time, the high cost of making a life-size solar car meant that only rich schools could afford it. Thus, it was decided that a competition for model solar cars would be created; model cars being much cheaper and less time-consuming to make than life-size cars, but still requiring the same amount of thought and skill. Paul and Ted assembled a team of science and technology teachers, curriculum developers, sponsors, representatives and former World Solar Challenge team members (today known as MSV), and with the financial support of Energy Victoria, they established a Model Solar Car Challenge in Victoria. The first race was run in May 1990, at the Exhibition Buildings in Melbourne.\n\nThe Sponsors of the event are as follows:\n\n\n"}
{"id": "35519476", "url": "https://en.wikipedia.org/wiki?curid=35519476", "title": "Western HVDC Link", "text": "Western HVDC Link\n\nThe Western HVDC Link is a high-voltage direct current (HVDC) electrical link between Hunterston in Western Scotland and Flintshire Bridge (Connah's Quay) in North Wales, routed to the west of the Isle of Man. It has a transmission capacity of 2,200 MW, and was expected to cost £1bn. The link entered operational service on 7 December 2017 at an initial reduced capacity of 900 MW pending further work at Hunterston. It entered full use in October 2018.\n\nIt consists of of cable, of which is underwater. It is the first subsea link at 600 kV. Operating at this higher voltage increases transmission capacity and reduces transmission losses in the cable, reducing power loss from transmission and two AC/DC conversions to under 3%.\n\nThere are two cables, which in shallow water are laid in a paired bundle, and in deep water laid about 30 metres apart, generally buried about 1.5 metres below the seabed. The cables are jointed in 120 km sections.\n\nLine commutated converter technology is used at each HVDC converter to maximise the capacity of electrical power, generated predominantly by renewable sources in Scotland, which can be transferred across the B6 boundary in parallel to two existing double-circuit AC transmission lines. This type of HVDC conversion technology means that large filter halls are required at each converter station. Gas-insulated switchgear is used at the Hunterston converter station to save space.\n\nRouting the connection through the Irish Sea as opposed to a land-based route limits the impact of the link on the visual environment and circumnavigates potential problems associated with the rising cost of land on the overall project costs. For the given distance, it is necessary to use a DC transmission technology for a subsea route, as the capacitive current required for an AC connection would be too high. Due to the proximity of converter stations to the coastline, a decision was taken to host electrical infrastructure within built structures in order to reduce the effects of the coastal air on sensitive electrical assets.\n\nPower can be transferred in either direction (which might become more likely due to the closure of Longannet power station), but it is necessary for the link to be offline for a sufficient time prior to reversing the direction. Mass impregnated non-draining (MIND) cables are used in a bipolar arrangement, but no sea- or earth-return path is permitted for environmental reasons, meaning that both cables must be in service for the link to be operational. The cables are spaced apart to minimise thermal interference, but not so far as to materially impact any marine life which navigates using the magnetic field of the Earth.\n\nCompletion was delayed a year due to cable manufacturing problems.\n\nEngineers working on the project discovered the almost intact wreck of the World War I German submarine UB-85.\n"}
{"id": "49620165", "url": "https://en.wikipedia.org/wiki?curid=49620165", "title": "Wiggleboard", "text": "Wiggleboard\n\nWiggleboard is a special kind of plywood-like wood that is designed to be bent along one of its two axes, also known as \"bendable plywood\" or \"bending plywood\".\n\nUnlike regular plywood that has the grains running at right angles to each other in each layer, wiggleboard has all the grains running the same way, and is designed to be easily rolled up or bent along one axis, and stiff along another axis that is at right angles to that one axis.\n\nIt can be used to quickly make any developable surface.\n\nNoyes, Nick. Easy Composters You Can Build: Storey's Country Wisdom Bulletin A-139. Storey Publishing, 1995.\n\nGarber, Richard, and Wassim Jabi. \"Control and Collaboration: digital fabrication strategies in academia and practice.\" International Journal of Architectural Computing 4, no. 2 (2006): 121-143.\n\nBrown, Glen D. \"Plywood garden container.\" U.S. Patent Application 13/875,134, filed May 1, 2013.\n\nhttp://www.decorgroupinc.com/store/categories/Sheet-Goods/Bendable-Plywood/\n"}
{"id": "15100822", "url": "https://en.wikipedia.org/wiki?curid=15100822", "title": "Wooden box", "text": "Wooden box\n\nA wooden box is a container made of wood for storage or as a shipping container. \n\nConstruction may include several types of wood; lumber (timber), plywood, engineered woods, etc. For some purposes, decorative woods are used. \n\nWooden boxes are often used for heavy duty packaging when \n\nBoxes and crates are not the same. If the sheathing of the container (plywood, lumber, etc.) can be removed, and a framed structure will remain standing, the container would likely be termed a crate. If removal of the sheathing resulted in there being no way of fastening the lumber around the edges of the container, the container would likely be termed a wooden box.\n\nThe strength of a wooden box is rated based on the weight it can carry before the cap (top, ends, and sides) is installed. \"Skids\" or thick bottom runners, are sometimes specified to allow forklift trucks access for lifting.\n\nPerformance is strongly influenced by the specific design, type of wood, type of fasteners (nails, etc.), workmanship, etc.\n\nSome boxes have handles, \"hand holes\", or \"hand holds\".\n\nA nailed wooden box is constructed of pieces of lumber (sometimes with reinforcing battens, cleats, etc.) attached by nails or other suitable fasteners. It usually completely encloses the contents. A box usually has a bottom and four side panels and often has an attachable top.\n\nA cleated box has five or six panel faces with wood strips attached to them. The panels can be made of plywood, solid or corrugated fiberboard, etc. Wooden cleats reinforce the panels.\n\nVery thin lumber is used for a wirebound box. Wires are stapled or stitched to the girth and to wood cleats. These are sometimes used for produce and for heavy loose items for military or export use. These are lighter than wood boxes or crates. They have excellent tensile strength to contain items but not much stacking strength.\n\nA skid box is a wood, corrugated fiberboard, or metal box attached to a heavy duty pallet or platform on a skid (parallel wood runners)\n\nASTM standards:\n\n\n"}
{"id": "3954107", "url": "https://en.wikipedia.org/wiki?curid=3954107", "title": "Zimbabweite", "text": "Zimbabweite\n\nZimbabweite is a mineral; formula (Na,K)PbAs(Nb,Ta,Ti)O. It is generally classed as an arsenite but is notable for also containing niobium and tantalum. A yellow brown mineral with orthorhombic crystal habit and a hardness of 5. It was discovered in 1986 in kaolinized pegmatite, i.e. weathered to clay, in Zimbabwe.\n"}
