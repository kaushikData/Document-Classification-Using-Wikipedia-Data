{"id": "8904606", "url": "https://en.wikipedia.org/wiki?curid=8904606", "title": "Acción Ecológica", "text": "Acción Ecológica\n\nAcción Ecológica is one of the main environmental organizations based in Ecuador. It is based in Quito.\n\nThey campaign on a range of issues including oil extraction, exploration and pipeline transport, Amazon Rainforest protection, food sovereignty, biofuels and plantations for carbon offsets.\n\n"}
{"id": "3859682", "url": "https://en.wikipedia.org/wiki?curid=3859682", "title": "Asperity (materials science)", "text": "Asperity (materials science)\n\nIn materials science, asperity, defined as \"unevenness of surface, roughness, ruggedness\" (OED, from the Latin \"asper\"—\"rough\"), has implications (for example) in physics and seismology. Smooth surfaces, even those polished to a mirror finish, are not truly smooth on a microscopic scale. They are rough, with sharp, rough or rugged projections, termed \"asperities\". Surface asperities exist across multiple scales, often in a self affine or fractal geometry. The fractal dimension of these structures has been correlated with the contact mechanics exhibited at an interface in terms of friction and contact stiffness.\n\nWhen two macroscopically smooth surfaces come into contact, initially they only touch at a few of these asperity points. These cover only a very small portion of the surface area. Friction and wear originate at these points, and thus understanding their behavior becomes important when studying materials in contact. When the surfaces are subjected to a compressive load, the asperities deform through elastic and plastic modes, increasing the contact area between the two surfaces until the contact area is sufficient to support the load.\n\nThe relationship between frictional interactions and asperity geometry is complex and poorly understood. It has been reported that an increased roughness may under certain circumstances result in weaker frictional interactions while smoother surfaces may in fact exhibit high levels of friction owing to high levels of true contact.\n\nThe Archard equation provides a simplified model of asperity deformation when materials in contact are subject to a force. Due to the ubiquitous presence of deformable asperities in self affine hierarchical structures, the true contact area at an interface exhibits a linear relationship with the applied normal load.\n\n"}
{"id": "41449061", "url": "https://en.wikipedia.org/wiki?curid=41449061", "title": "Bose–Einstein condensation of quasiparticles", "text": "Bose–Einstein condensation of quasiparticles\n\nBose–Einstein condensation can occur in quasiparticles, particles that are effective descriptions of collective excitations in materials. Some have integer spins and can be expected to obey Bose–Einstein statistics like traditional particles. Conditions for condensation of various quasiparticles have been predicted and observed. The topic continues to be an active field of study.\n\nBECs form when low temperatures cause nearly all particles to occupy the lowest quantum state. Condensation of quasiparticles occurs in ultracold gases and materials. The lower masses of material quasiparticles relative to atoms lead to higher BEC temperatures. An ideal Bose gas has a phase transitions when inter-particle spacing approaches the thermal De-Broglie wavelength: formula_1. The critical concentration is then formula_2, leading to a critical temperature: formula_3. The particles obey the Bose–Einstein distribution and all occupy the ground state:\n\nThe Bose gas can be considered in a harmonic trap, formula_4, with the ground state occupancy fraction as a function of temperature:\n\nThis can be achieved by cooling and magnetic or optical control of the system. Spectroscopy can detect shifts in peaks indicating thermodynamic phases with condensation. Quasiparticle BEC can be superfluids. Signs of such states include spatial and temporal coherence and polarization changes. Observation for excitons in solids was seen in 2005 and for magnons in materials and polaritons in microcavities in 2006. Graphene is another important solid state system for studies of condensed matter including quasi particles; It's a 2D electron gas, similar to other thin films.\n\nExcitons are electron-hole pairs. Similar to helium-4 superfluidity at the formula_6-point (2.17K); a condensate was proposed by Böer et al. in 1961. Experimental phenomenon were predicted leading to various pulsed laser searches that failed to produce evidence. Signs were first seen by Fuzukawa et al. in 1990, but definite detection was published later in the 2000s. Condensed excitons are a superfluid and will not interact with phonons. While the normal exciton absorption is broadened by phonons, in the superfluid absorption degenerates to a line.\n\nExcitions results from photons exciting electrons creating holes, which are then attracted and can form bound states. The 1s paraexciton and orthoexciton are possible. The 1s triplet spin state, 12.1meV below the degenerate orthoexciton states(lifetime ~ns), is decoupled and has a long lifetime to an optical decay. Dilute gas densities (n~10cm) are possible, but paraexcition generation scales poorly, so significant heating occurs in creating high densities(10cm) preventing BECs. Assuming a thermodynamic phase occurs when separation reaches the de Broglie wavelength(formula_7) gives:\n\nWhere, formula_8 is the exciton density, effective mass(of electron mass order) formula_9, and formula_10,formula_11 are the Planck and Boltzmann constants. Density depends on the optical generation formula_12 and lifetime as: formula_13. Tuned lasers create excitons which efficiently self-annihilate at a rate: formula_14, preventing a high density paraexciton BEC. A potential well limits diffusion, damps exciton decay, and lowers the critical number, yielding an improved critical temperature versus the \"T\" scaling of free particles:\n\nIn an ultrapure CuO crystal: formula_16 = 10s. For an achievable T = 0.01K, a manageable optical pumping rate of 10/s should produce a condensate. More detailed calculations by J. Keldysh and later by D. Snoke et al. started a large number of experimental searches into the 1990s that failed to detect signs. Pulse methods led to overheating, preventing condensate states. Helium cooling allows miili-kelvin setups and continuous wave optics improves on pulsed searches. Relaxation explosion of a condensate at lattice temperature 354 mK was seen by Yoshioka et al. in 2011. Recent experiments by Stolz et al. using a potential trap have given more evidence at ultralow temperature 37 mK. In a parabolic trap with exciton temperature 200 mK and lifetime broadened to 650ns, the dependence of luminescence on laser intensity has a kink which indicates condensation. The theory of a Bose gas is extended to a mean field interacting gas by a Bogoliubov approach to predict the exciton spectrum; The kink is considered a sign of transition to BEC. Signs were seen for a dense gas BEC in a GaAs quantum well.\n\nMagnons, electron spin waves, can be controlled by a magnetic field. Densities from the limit of a dilute gas to a strongly interacting Bose liquid are possible. Magnetic ordering is the analog of superfluidity. The condensate appears as the emission of monochromatic microwaves, which are tunable with the applied magnetic field.\n\nIn 1999 condensation was demonstrated in antiferromagnetic TlCuCl, at temperatures as large as 14 K. The high transition temperature (relative to atomic gases) is due to the small mass (near an electron) and greater density. In 2006, condensation in a ferromagnetic Yttrium-iron-garnet thin film was seen even at room temperature with optical pumping. Condensation was reported in gadolinium in 2011. Magnon BECs have been considered as qubits for quantum computing.\n\nPolaritons, caused by light coupling to excitons, occur in optical cavities and condensation of exciton-polaritons in an optical microcavity was first published in Nature in 2006. Semiconductor cavity polariton gases transition to ground state occupation at 19K. Bogoliubov excitations were seen polariton BECs in 2008.\nThe signatures of BEC were observed at room temperature for the first time in 2013, in a large exciton energy semiconductor device and in a polymer microcavity.\n\nRotons, an elementary excitation in superfluid He introduced by Landau, were discussed by Feynman and others. Rotons condense at low temperature. Experiments have been proposed and the expected spectrum has been studied, but roton condensates have not been detected. Phonons were first observed in a condensate in 2004 by ultrashort pulses in a bismuth crystal at 7K.\n\n\n"}
{"id": "24621440", "url": "https://en.wikipedia.org/wiki?curid=24621440", "title": "Bosphorus Gaz Corporation", "text": "Bosphorus Gaz Corporation\n\nBosphorus Gaz Corporation is a gas importer and distributor in Turkey. It controls about 3% of the Turkey's natural gas market.\n\nThe company was established in 2003 in Istanbul. In 2004, Gazprom Germania a subsidiary of the Russian gas company Gazprom, became shareholder in Bosphorus Gaz. In 2009, Gazprom raised its share in the company from 40% to 51%. Later this year, Gazprom Germania acquired additional 20% of shares. This acquisition is pending of approval from Turkish authorities. Rest of the shares are held by Tur Enerji, a joint venture of the Şen family and Ringas Management B.V. \n\nIn 2005, Bosphorus Gaz won a tender to resell until 2021 750 million cubic meters of the gas purchased by the Turkish energy company BOTAŞ from Gazprom. According to Gazprom, Bosphorus Gaz explores various possibilities of participating in the privatization of the country's gas distribution networks and in building underground gas storage tanks in Turkey.\n\n"}
{"id": "3925035", "url": "https://en.wikipedia.org/wiki?curid=3925035", "title": "Department of Communications, Climate Action and Environment", "text": "Department of Communications, Climate Action and Environment\n\nThe Department of Communications, Climate Action and Environment () is a department of the Government of Ireland that is responsible for the telecommunications and broadcasting sectors and regulates, protects and develops the natural resources of Ireland. The head of the department is the Minister for Communications, Climate Action and Environment who is assisted by one Minister of State.\n\nThe official headquarters and ministerial offices of the department are in 29–31 Adelaide Road, Dublin 2. The departmental team consists of the following:\n\n\nThe Department of Fisheries was created in 1921. Over the years its name and functions have changed several times. \nThe Department of Communications, Climate Action and Environment is subdivided into the following divisions:\n\nAmong the State Agencies and other bodies affiliated to the Department in some way are:\n\nAmong the state-sponsored bodies of the Republic of Ireland under the aegis of the Minister are:\n\n"}
{"id": "8971997", "url": "https://en.wikipedia.org/wiki?curid=8971997", "title": "Ducati Energia", "text": "Ducati Energia\n\nDucati Energia SpA is an Italian company based in Bologna, part of the Ducati group, which produces electrical and electronic components.\n\nIt was founded in 1926 by the Ducati brothers, Adriano, Marcello and Bruno, to produce vacuum tubes, capacitors and other radio components. The original Ducati company was called \"Società Scientifica Radio Brevetti Ducati\".\n\nIn 1935 Ducati became successful enough to enable construction of a new factory in the Borgo Panigale area of the city.\n\nIn the late 1940s Ducati began producing engines for bicycles and later motorcycles, leading to the later split into two companies: Ducati Meccanica and Ducati Elettrotecnica. The former specializing in the manufacture of motorcycles and the latter for radios and electrical devices and components.\n\nDucati Elettrotecnica was renamed Ducati Energia in the 1980s. It designs and produces a wide range of electronic components and subsystems for use in automotive, construction, security, and transmission and distribution (T&D) energy efficiency.\n\nIt has a presence in many countries in Europe, Middle East, Asia, Africa and Latin America.\n\n"}
{"id": "9951602", "url": "https://en.wikipedia.org/wiki?curid=9951602", "title": "Earth mass", "text": "Earth mass\n\nEarth mass (, where ⊕ is the standard astronomical symbol for planet Earth) is the unit of mass equal to that of Earth. \nThe current best estimate for Earth mass is , with a standard uncertainty of \nIt is equivalent to an average density of .\n\nThe Earth mass is a standard unit of mass in astronomy that is used to indicate the masses of other planets, including rocky terrestrial planets and exoplanets. One Solar mass is close to 333,000 Earth masses.\nThe Earth mass excludes the mass of the Moon. The mass of the Moon is about 1.2% of that of the Earth, so that the mass of the Earth+Moon system is close to .\n\nMost of the mass is accounted for by iron and oxygen (c. 32% each), magnesium and silicon (c. 15% each), calcium, aluminium and nickel (c. 1.5% each).\n\nPrecise measurement of the Earth mass is difficult, as it is equivalent to measuring the gravitational constant, which is the fundamental physical constant known with least accuracy, due to the relative weakness of the gravitational force.\nThe mass of the Earth was measured accurately in the Schiehallion experiment in the 1770s, and within 1% of the modern value in the Cavendish experiment of 1798.\n\nThe mass of Earth is estimated to be:\nwhich can be expressed in terms of solar mass as:\n\nThe ratio of Earth mass to lunar mass has been measured to great accuracy. The current best estimate is:\n\nThe \"G\" product for the Earth is called the geocentric gravitational constant and equals . It is determined using laser ranging data from Earth-orbiting satellites, such as LAGEOS-1. The \"G\" product can also be calculated by observing the motion of the Moon or the period of a pendulum at various elevations. These methods are less precise than observations of artificial satellites.\n\nThe relative uncertainty of the geocentric gravitational constant is just , however, (the mass of the Earth in kilograms) can be found out only by dividing the \"G\" product by \"G\", and \"G\" is known only to a relative uncertainty of \nFor this reason and others, astronomers prefer to use the un-reduced \"G\" product, or mass ratios\n(masses expressed in units of Earth mass or Solar mass) rather than mass in kilograms when referencing and comparing planetary objects.\n\nEarth's density varies considerably, between less than in the upper crust to as much as in the inner core.\nThe Earth's core accounts for 15% of Earth's volume but more than 30% of the mass, the mantle for 84% of the volume and close to 70% of the mass, while the crust accounts for less than 1% of the mass.\nAbout 90% of the mass of the Earth is composed of the iron–nickel alloy (95% iron) in the core (30%), and the silicon dioxides (c. 33%) and magnesium oxide (c. 27%) in the mantle and crust. \nMinor contributions are from iron(II) oxide (5%), aluminium oxide (3%) and calcium oxide (2%), besides numerous trace elements (in elementary terms: iron and oxygen c. 32% each, magnesium and silicon c. 15% each, calcium, aluminium and nickel c. 1.5% each). \nCarbon accounts for 0.03%, water for 0.02%, and the atmosphere for about one part per million.\n\nThe mass of Earth is measured indirectly by determining other quantities such as Earth's density, gravity, or gravitational constant.\nThe first measurement in the 1770s Schiehallion experiment resulted in a value about 20% too low.\nThe Cavendish experiment of 1798 found the correct value within 1%.\nUncertainty was reduced to about 0.2% by the 1890s, \nto 0.1% by 1930,\nand to 0.01% (10) by the 2000s.\nThe figure of the Earth has been known to better than four significant digits since the 1960s (WGS66), so that since that time, the uncertainty of the Earth mass is determined essentially by the uncertainty in measuring the gravitational constant.\n\nBefore the direct measurement of the gravitational constant,\nestimates of the Earth mass were limited to estimating Earth's mean density from observation of the crust and estimates on Earth's volume. Estimates on the volume of the earth in the 17th century were based on a circumference estimate of 60 miles to the degree of latitude, corresponding to a radius of about 5,500 km, resulting in an estimated volume of about one third smaller than the correct value.\nThe average density of the Earth was not accurately known. Earth was assumed to consist either mostly of water (Neptunism) or mostly of igneous rock (Plutonism), both suggesting average densities too low by several orders of magnitude,\nconsistent with a total mass of the order of .\nIsaac Newton estimated, without access to reliable measurement, that the density of Earth would be five or six times as great as the density of water, which is surprisingly accurate (the modern value is 5.515).\nNewton under-estimated the Earth's volume by about 30%, so that his estimate would be roughly equivalent to .\nIn the 18th century, knowledge of Newton's law of gravitation permitted indirect estimates on the mean density of the Earth, \nvia estimates of (what in modern terminology is known as) the gravitational constant.\nEarly estimates on the mean density of the Earth were made by observing the slight deflection of a pendulum near a mountain, as in the Schiehallion experiment. Newton considered the experiment in \"Principia\", but pessimistically concluded that the effect would be too small to be measurable.\n\nAn expedition from 1737 to 1740 by Pierre Bouguer and Charles Marie de La Condamine attempted to determine the density of Earth by measuring the period of a pendulum (and therefore the strength of gravity) as a function of elevation. The experiments were carried out in Ecuador and Peru, on Pichincha Volcano and mount Chimborazo.\nBouguer wrote in a 1749 paper that they had been able to detect a deflection of 8 seconds of arc,\nThe accuracy was not enough for a definite estimate on the mean density of the Earth, but Bouguer stated that it was at least sufficient to prove that the Earth was not hollow.\n\nThat a further attempt should be made on the experiment was proposed to the Royal Society in 1772 by Nevil Maskelyne, Astronomer Royal. He suggested that the experiment would \"do honour to the nation where it was made\" and proposed Whernside in Yorkshire, or the Blencathra-Skiddaw massif in Cumberland as suitable targets. The Royal Society formed the Committee of Attraction to consider the matter, appointing Maskelyne, Joseph Banks and Benjamin Franklin amongst its members. The Committee despatched the astronomer and surveyor Charles Mason to find a suitable mountain.\n\nAfter a lengthy search over the summer of 1773, Mason reported that the best candidate was Schiehallion, a peak in the central Scottish Highlands. The mountain stood in isolation from any nearby hills, which would reduce their gravitational influence, and its symmetrical east–west ridge would simplify the calculations. Its steep northern and southern slopes would allow the experiment to be sited close to its centre of mass, maximising the deflection effect.\nNevil Maskelyne, Charles Hutton and Reuben Burrow performed the experiment, completed by 1776.\nHutton (1778) reported that the mean density of the Earth was estimated at \nformula_4 that of Schiehallion mountain.\nThis corresponds to a mean density about 4 higher than that of water (i.e., about ), about 20% below the modern value, but still significantly larger than the mean density of normal rock, suggesting for the first time that the interior of the Earth might be substantially \ncomposed of metal.\nHutton estimated this metallic portion to occupy some (or 65%) of the diameter of the Earth (modern value 55%). \nWith a value for the mean density of the Earth, Hutton was able to set some values to Jérôme Lalande's planetary tables, which had previously only been able to express the densities of the major solar system objects in relative terms.\n\nThe Henry Cavendish (1798) was the first to attempt to measure the gravitational attraction between two bodies directly in the laboratory.\nEarth's mass could be then found by combining two equations; Newton's second law, and Newton's law of universal gravitation.\n\nIn modern notation, the mass of the Earth is derived from the gravitational constant and the mean Earth radius by\nWhere \"little g\":\n\nCavendish found a mean density of , about 1% below the modern value.\n\nWhile the mass of the Earth is implied by stating the Earth's radius and density, it was not usual to state the absolute mass explicitly \nprior to the introduction of scientific notation using powers of 10 in the later 19th century, \nbecause the absolute numbers would have been too awkward. Ritchie (1850) gives the mass of the Earth's atmosphere as \"11,456,688,186,392,473,000 lbs.\" ( = , modern value is ) and states that \"compared with the weight of the globe this mighty sum dwindles to insignificance\".\n\nAbsolute figures for the mass of the Earth are cited only beginning in the second half of the 19th century, mostly in popular rather than expert literature.\nAn early such figure was given as \"14 quadrillion pounds\" (\"14 Quadrillionen Pfund\") [] in Masius (1859).\n\nBeckett (1871) cites the \"weight of the earth\" as \"5842 quintillion tons\" [].\nThe \"mass of the earth in gravitational measure\" is stated as \"9.81996×6370980\" in \"The New Volumes of the Encyclopaedia Britannica\" (Vol. 25, 1902) with a \"logarithm of earth's mass\" given as \"14.600522\" []. This is the gravitational parameter in m·s (modern value ) and not the absolute mass.\n\nExperiments involving pendulums continued to be performed in the first half of the 19th century. By the second half of the century, these were outperformed by repetitions of the Cavendish experiment, and the modern value of \"G\" (and hence, of the Earth mass) is still derived from high-precision repetitions of the Cavendish experiment.\n\nIn 1821, Francesco Carlini determined a density value of ρ = through measurements made with pendulums in the Milan area. This value was refined in 1827 by Edward Sabine to , and then in 1841 by Carlo Ignazio Giulio to . On the other hand, George Biddell Airy sought to determine ρ by measuring the difference in the period of a pendulum between the surface and the bottom of a mine. \nThe first tests took place in Cornwall between 1826 and 1828. The experiment was a failure due to a fire and a flood. Finally, in 1854, Airy got the value by measurements in a coal mine in Harton, Sunderland. Airy's method assumed that the Earth had a spherical stratification. Later, in 1883, the experiments conducted by Robert von Sterneck (1839 to 1910) at different depths in mines of Saxony and Bohemia provided the average density values ρ between 5.0 and . This led to the concept of isostasy, which limits the ability to accurately measure ρ, by either the deviation from vertical of a plumb line or using pendulums. Despite the little chance of an accurate estimate of the average density of the Earth in this way, Thomas Corwin Mendenhall in 1880 realized a gravimetry experiment in Tokyo and at the top of Mount Fuji. The result was ρ = .\n\nThe uncertainty in the modern value for the Earth's mass has been entirely due to the uncertainty in the gravitational constant \"G\" since at least the 1960s.\n\"G\" is notoriously difficult to measure, and some high-precision measurements during the 1980s to 2010s have yielded mutually exclusive results.\nSagitov (1969) based on the measurement of \"G\" by Heyl and Chrzanowski (1942) cited a value of (relative uncertainty ).\n\nAccuracy has improved only slightly since then. Most modern measurements are repetitions of the Cavendish experiment, with results (within standard uncertainty) ranging between 6.672 and 6.676 ×10  m kgs (relative uncertainty 3×10) in results reported since the 1980s, although the 2014 NIST recommended value is close to 6.674×10  m kgs with a relative uncertainty below 10.\nThe \"Astronomical Almanach Online\" as of 2016 recommends a standard uncertainty of for Earth mass, \n\nEarth's mass is variable, subject to both gain and loss due to the accretion of micrometeorites and cosmic dust \nand the loss of hydrogen and helium gas, respectively.\nThe combined effect is a net loss of material, estimated at (54,000 tons) per year. This amount is of the total earth mass. The annual net loss is essentially due to 100,000 tons lost due to atmospheric escape, and an average of 45,000 tons gained from in-falling dust and meteorites. This is well within the mass uncertainty of 0.01% (), so the estimated value of earth's mass is unaffected by this factor.\n\nMass loss is due to atmospheric escape of gases. About 95,000 tons of hydrogen per year () and 1,600 tons of helium per year are lost through atmospheric escape.\nThe main factor in mass gain is in-falling material, cosmic dust, meteors, etc. are the most significant contributors to Earth's increase in mass. The sum of material is estimated to be 37,000 to 78,000 tons annually.\n\nAdditional changes in mass are due to the mass–energy equivalence principal, although these changes are relatively negligible. An increase in mass has been ascribed to rising temperatures (global warming), estimated at 160 tonnes per years as of 2016.\nAnother 16 tons per year are lost in the form of rotational kinetic energy due to the deceleration of the rotation of Earth's inner core. This energy is transferred to the rotational energy of the solar system, and the trend might also be reversible, as rotation speed has been shown to fluctuate over decades. Mass loss due to nuclear fission is estimated to amount to 16 tons per year.\nAn additional loss due to spacecraft on escape trajectories has been estimated at since the mid-20th century. Earth lost about 3473 tons in the initial 53 years of the space age, but the trend is currently decreasing.\n\n"}
{"id": "19767753", "url": "https://en.wikipedia.org/wiki?curid=19767753", "title": "Einasto profile", "text": "Einasto profile\n\nThe Einasto profile (or Einasto model) is a mathematical function that describes how the density formula_1 of a spherical stellar system varies with distance formula_2 from its center. Jaan Einasto introduced his model at a 1963 conference in Alma-Ata, Kazakhstan.\n\nThe Einasto profile possesses a power law logarithmic slope of the form:\nwhich can be rearranged to give\n\nThe parameter formula_5 controls the degree of curvature of the profile. This can be seen by computing the slope on a log-log plot:\n\nThe larger formula_5, the more rapidly the slope varies with radius (see figure). Einasto's law can be described as a generalization of a power law, formula_8, which has a constant slope on a log-log plot.\n\nEinasto's model has the same mathematical form as Sersic's law, which is used to describe the surface brightness (i.e. projected density) profile of galaxies.\n\nEinasto's model has been used to describe many types of system, including galaxies and dark matter halos.\n\n\n"}
{"id": "23433214", "url": "https://en.wikipedia.org/wiki?curid=23433214", "title": "Energy in Ivory Coast", "text": "Energy in Ivory Coast\n\nEnergy in Ivory Coast refers to the energy industry in Ivory Coast. In recent years, the country has been an important supplier of energy to the sub-Saharan region as a result of its reserves of natural gas, excess electrical generating capacity, and recent offshore finds of petroleum and natural gas.\n\nOffshore oil was discovered in 1977, with production starting three years later. The bulk of the country's oil and gas wells, (86%), are situated in shallow marine areas, with another 7% located in deep offshore wells. Only 7% of the country's oil and gas wells are onshore. Estimates by the Oil & Gas Journal have placed the country's proven petroleum reserves at , as of January 1, 2005. Production for 2004 was estimated at , with crude oil accounting for . However, recent finds and new production at several off shore fields and blocks may push the nation's proven reserves and output totals higher. For example, the Espoir field, which began producing in early 2002, is estimated to contain recoverable reserves of of oil and of gas. Also, Block CI-40, which is jointly operated by Canadian Natural Resources, Svenska Petroleum and the state oil corporation, Société Nationale d'Opérations Pétrolières de la Côte d'Ivoire (Petroci), and which lies to the south of the Espoir field, is estimated to have recoverable oil reserves of . In Block CI-112, located off Ivory Coast's western coast, is estimated by Vanco Energy Company to contain of oil in the block's San Pedro ridge and in other deposits.\n\nAlthough natural gas was initially discovered in Ivory Coast in the 1980s, it has only been recently developed as of January 1, 2005, the country is estimated to have of natural gas reserves of 1 trillion cu ft. In 2003, natural gas output and domestic consumption were each estimated at 46 billion cu ft.\nIvory Coast’s oil and gas industry is managed by Petroci. Founded in 1975, Petroci was restructured in 1998 into a holding company, Petroci Holding, with three subsidiaries: Petroci Exploration-Production which handles upstream gas and oil activities; Petroci Gaz, which is responsible for the natural gas sector; and Petroci Industries-Services which manages all other related services. Petroci Holding manages the three subsidiaries as well as the country’s holdings in the gas and oil sectors.\n\nRecent openings of the oil market by Pres Laurent Gbagbo (reported originally in North African press ) to the Russian and Chinese are suggested in these articles as the substantive reason the UN and France and the US are now very concerned about 'democracy' in Ivory Coast as it was traditionally a French fiefdom run by Francophile elites who are long associated with the opposition candidate Alassane Ouatarra who implemented the routine very strict IMF rulings on Ivorians when he was last prime minister.\n\nIvory Coast uses hydroelectric and thermal generating facilities to provide all of its electrical power. As of January 1, 2002, the country's generating capacity stood at an estimated 900 MW. Although hydropower accounts for around two-thirds of its generating capacity, it accounts for less than half of the power generated. In 2002, an estimated 4.8 TWh of electric power was generated, of which 38% was hydroelectric and 62% thermal. Gas powered stations alone generated more than half of the total power produced the use of natural gas fueled power stations has also made Ivory Coast into an exporter of electricity. In 2002, exports of electricity to neighboring countries totaled 1.6 TWh. Benin, Ghana, Burkina Faso, Mali, and Togo are among the countries connected to Ivory Coast’s power grid. Domestic consumption of electric power in 2002 is reported to total 3.109 TWh. Compangnie Ivoirienne d’Electriciti (CIE) is the sole supplier of power, and manages, not only the government-owned generating plants, but also the transmission and distribution of power. Although official estimates place the percentage of people living in urban areas that have access to electricity at 77%, less than 15% of those living in rural areas have such access. Rural electrification has become a major priority with the government.\n\n\n"}
{"id": "302078", "url": "https://en.wikipedia.org/wiki?curid=302078", "title": "Erucic acid", "text": "Erucic acid\n\nErucic acid is a monounsaturated omega-9 fatty acid, denoted 22:1ω9. It has the formula CH(CH)CH=CH(CH)COOH. It is prevalent in wallflower seed with a reported content of 20 to 54% in high erucic acid rapeseed oil, and 42% in mustard oil. Erucic acid is also known as \"cis\"-13-docosenoic acid and the trans isomer is known as brassidic acid.\n\nErucic acid has many of the same uses as mineral oils, but it is more readily biodegradable than some. It has limited ability to polymerize and dry for use in oil paints. Like other fatty acids, it can be converted into surfactants or lubricants, and can be used as a precursor to biodiesel fuel.\n\nDerivatives of erucic acid have many further uses, such as behenyl alcohol (CH(CH)OH), a pour point depressant (enabling liquids to flow at a lower temperature), and silver behenate, for use in photography.\n\nThe name 'erucic' means: of or pertaining to \"Eruca\"; which is a genus of flowering plants in the family Brassicaceae. It is also the Latin for colewort, which today is better known as arugula.\n\nErucic acid is produced naturally (together with other fatty acids) across a great range of green plants, but especially so in members of the genus \"Brassica\". For industrial purposes and production of erucic acid, rapeseed is used; for food purposes a 'low-erucic acid rapeseed' (LEAR) has been developed (canola), which contains fats derived from oleic acid instead of erucic acid.\n\nErucic acid is produced by elongation of oleic acid via oleoyl-coenzyme A and malonyl-CoA. Erucic acid is broken down into shorter-chain fatty acids in the human liver by the long-chain acyl CoA dehydrogenase enzyme.\n\nStudies done on laboratory animals in the early 1970s show that erucic acid appears to have toxic effects on the heart at high enough doses. An association between the consumption of rapeseed oil and increased myocardial lipidosis, or heart disease, has not been established for humans. While there are reports of toxicity from long-term use of Lorenzo's oil (which contains erucic acid and other ingredients), there are no reports of harm to people from dietary consumption of erucic acid.\n\nPublication of animal studies with erucic acid through the 1970s led to governments worldwide moving away from oils with high levels of erucic acid, and tolerance levels for human exposure to erucic acid have been established based on the animal studies.\n\nIn 2003, Food Standards Australia set a provisional tolerable daily intake (PTDI) for an average adult of about 500 mg/day of erucic acid, extrapolated based on \"the level that is associated with increased myocardial lipidosis in nursing pigs.\" \"There is a 120-fold safety margin between this level and the level that is associated with increased myocardial lipidosis in nursing pigs. The dietary exposure assessment has concluded that the majority of exposure to erucic acid by the general population would come from the consumption of canola oil. The dietary intake of erucic acid by an individual consuming at the average level is well below the PTDI, therefore, there is no cause for concern in terms of public health and safety. However, the individual consuming at a high level has the potential to approach the PTDI. This would be particularly so if the level of erucic acid in canola oil were to exceed 2% of the total fatty acids.\"\n\nFood-grade rapeseed oil (also known as canola oil, rapeseed 00 oil, low erucic acid rapeseed oil, LEAR oil, and rapeseed canola-equivalent oil) is regulated to a maximum of 2% erucic acid by weight in the USA and 5% in the EU, with special regulations for infant food.\n\n"}
{"id": "268020", "url": "https://en.wikipedia.org/wiki?curid=268020", "title": "Evolutionary computation", "text": "Evolutionary computation\n\nIn computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\n\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\n\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an \"in silico\" experimental procedure to study common aspects of general evolutionary processes.\n\nThe use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\n\n\"Evolutionary programming\" was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a \"genetic algorithm\". In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced \"evolution strategies\". These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called \"evolutionary computing\". Also in the early nineties, a fourth stream following the general ideas had emerged – \"genetic programming\". Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation.\n\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\n\nSimulations of evolution using evolutionary algorithms and artificial life started with the work of Nils Aall Barricelli in the 1960s, and was extended by Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\n\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\n\n\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\n\nIn this process, there are two main forces that form the basis of evolutionary systems: Recombination and mutation create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\n\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\n\nGenetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.\n\nHowever, the use of algorithms and informatics, in particular of computational theory, beyond the analogy to dynamical systems, is also relevant to understand evolution itself. \n\nThis view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers . Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system .\n\nFurthermore, following concepts from computational theory, micro processes in biological organisms are fundamentally incomplete and undecidable (completeness (logic)), implying that “there is more than a crude metaphor behind the analogy between cells and computers .\n\nThe analogy to computation extends also to the relationship between inheritance systems and biological structure, which is often thought to reveal one of the most pressing problems in explaining the origins of life.\n\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\n\n\nThe main conferences in the evolutionary computation area include \n\n"}
{"id": "182445", "url": "https://en.wikipedia.org/wiki?curid=182445", "title": "Fermi liquid theory", "text": "Fermi liquid theory\n\nFermi liquid theory (also known as Landau–Fermi liquid theory) is a theoretical model of interacting fermions that describes the normal state of most metals at sufficiently low temperatures. The interaction between the particles of the many-body system does not need to be small. The phenomenological theory of Fermi liquids was introduced by the Soviet physicist Lev Davidovich Landau in 1956, and later developed by Alexei Abrikosov and Isaak Khalatnikov using diagrammatic perturbation theory. The theory explains why some of the properties of an interacting fermion system are very similar to those of the ideal Fermi gas (i.e. non-interacting fermions), and why other properties differ.\n\nImportant examples of where Fermi liquid theory has been successfully applied are most notably electrons in most metals and Liquid He-3. Liquid He-3 is a Fermi liquid at low temperatures (but not low enough to be in its superfluid phase). Helium-3 is an isotope of helium, with 2 protons, 1 neutron and 2 electrons per atom. Because there is an odd number of fermions inside the nucleus, the atom itself is also a fermion. The electrons in a normal (non-superconducting) metal also form a Fermi liquid, as do the nucleons (protons and neutrons) in an atomic nucleus. Strontium ruthenate displays some key properties of Fermi liquids, despite being a strongly correlated material, and is compared with high temperature superconductors like cuprates.\n\nThe key ideas behind Landau's theory are the notion of \"adiabaticity\" and the Pauli exclusion principle. Consider a non-interacting fermion system (a Fermi gas), and suppose we \"turn on\" the interaction slowly. Landau argued that in this situation, the ground state of the Fermi gas would adiabatically transform into the ground state of the interacting system.\n\nBy Pauli's exclusion principle, the ground state formula_1 of a Fermi gas consists of fermions occupying all momentum states corresponding to momentum formula_2 with all higher momentum states unoccupied. As interaction is turned on, the spin, charge and momentum of the fermions corresponding to the occupied states remain unchanged, while their dynamical properties, such as their mass, magnetic moment etc. are \"renormalized\" to new values. Thus, there is a one-to-one correspondence between the elementary excitations of a Fermi gas system and a Fermi liquid system. In the context of Fermi liquids, these excitations are called \"quasi-particles\".\n\nLandau quasiparticles are long-lived excitations with a lifetime formula_3 that satisfies formula_4 where formula_5 is the quasiparticle energy (measured from the Fermi energy). At finite temperature, formula_5 is on the order of the thermal energy formula_7, and the condition for Landau quasiparticles can be reformulated as formula_8.\n\nFor this system, the Green's function can be written (near its poles) in the form\n\nformula_9\n\nwhere formula_10 is the chemical potential and formula_11 is the energy corresponding to the given momentum state.\n\nThe value formula_12 is called the \"quasiparticle residue\" and is very characteristic of Fermi liquid theory. The spectral function for the system can be directly observed via angle-resolved photoemission spectroscopy (ARPES), and can be written (in the limit of low-lying excitations) in the form:\n\nformula_13\n\nwhere formula_14 is the Fermi velocity.\n\nPhysically, we can say that a propagating fermion interacts with its surrounding in such a way that the net effect of the interactions is to make the fermion behave as a \"dressed\" fermion, altering its effective mass and other dynamical properties. These \"dressed\" fermions are what we think of as \"quasiparticles\".\n\nAnother important property of Fermi liquids is related to the scattering cross section for electrons. Suppose we have an electron with energy formula_15 above the Fermi surface, and suppose it scatters with a particle in the Fermi sea with energy formula_16. By Pauli's exclusion principle, both the particles after scattering have to lie above the Fermi surface, with energies formula_17 Now, suppose the initial electron has energy very close to the Fermi surface formula_18 Then, we have that formula_19 also have to be very close to the Fermi surface. This reduces the phase space volume of the possible states after scattering, and hence, by Fermi's golden rule, the scattering cross section goes to zero. Thus we can say that the lifetime of particles at the Fermi surface goes to infinity.\n\nThe Fermi liquid is qualitatively analogous to the non-interacting Fermi gas, in the following sense: The system's dynamics and thermodynamics at low excitation energies and temperatures may be described by substituting the non-interacting fermions with interacting quasiparticles, each of which carries the same spin, charge and momentum as the original particles. Physically these may be thought of as being particles whose motion is disturbed by the surrounding particles and which themselves perturb the particles in their vicinity. Each many-particle excited state of the interacting system may be described by listing all occupied momentum states, just as in the non-interacting system. As a consequence, quantities such as the heat capacity of the Fermi liquid behave qualitatively in the same way as in the Fermi gas (e.g. the heat capacity rises linearly with temperature).\n\nThe following differences to the non-interacting Fermi gas arise:\n\nThe energy of a many-particle state is not simply a sum of the single-particle energies of all occupied states. Instead, the change in energy for a given change formula_20 in occupation of states formula_21 contains terms both linear and quadratic in formula_20 (for the Fermi gas, it would only be linear, formula_23, where formula_24 denotes the single-particle energies). The linear contribution corresponds to renormalized single-particle energies, which involve, e.g., a change in the effective mass of particles. The quadratic terms correspond to a sort of \"mean-field\" interaction between quasiparticles, which is parametrized by so-called Landau Fermi liquid parameters and determines the behaviour of density oscillations (and spin-density oscillations) in the Fermi liquid. Still, these mean-field interactions do not lead to a scattering of quasi-particles with a transfer of particles between different momentum states.\n\nThe renormalization of the mass of a fluid of interacting fermions can be calculated from first principles using many-body computational techniques. For the two-dimensional homogeneous electron gas, GW calculations and quantum Monte Carlo methods have been used to calculate renormalized quasiparticle effective masses.\n\nSpecific heat, compressibility, spin-susceptibility and other quantities show the same qualitative behaviour (e.g. dependence on temperature) as in the Fermi gas, but the magnitude is (sometimes strongly) changed.\n\nIn addition to the mean-field interactions, some weak interactions between quasiparticles remain, which lead to scattering of quasiparticles off each other. Therefore, quasiparticles acquire a finite lifetime. However, at low enough energies above the Fermi surface, this lifetime becomes very long, such that the product of excitation energy (expressed in frequency) and lifetime is much larger than one. In this sense, the quasiparticle energy is still well-defined (in the opposite limit, Heisenberg's uncertainty relation would prevent an accurate definition of the energy).\n\nThe structure of the \"bare\" particle's (as opposed to quasiparticle) Green's function is similar to that in the Fermi gas (where, for a given momentum, the Green's function in frequency space is a delta peak at the respective single-particle energy). The delta peak in the density-of-states is broadened (with a width given by the quasiparticle lifetime). In addition (and in contrast to the quasiparticle Green's function), its weight (integral over frequency) is suppressed by a quasiparticle weight factor formula_25. The remainder of the total weight is in a broad \"incoherent background\", corresponding to the strong effects of interactions on the fermions at short time-scales.\n\nThe distribution of particles (as opposed to quasiparticles) over momentum states at zero temperature still shows a discontinuous jump at the Fermi surface (as in the Fermi gas), but it does not drop from 1 to 0: the step is only of size formula_12.\n\nIn a metal the resistivity at low temperatures is dominated by electron-electron scattering in combination with umklapp scattering. For a Fermi liquid, the resistivity from this mechanism varies as formula_27, which is often taken as an experimental check for Fermi liquid behaviour (in addition to the linear temperature-dependence of the specific heat), although it only arises in combination with the lattice. In certain cases, umklapp scattering is not required. For example, the resistivity of compensated semimetals scales as formula_27 because of mutual scattering of electron and hole. This is known as the Baber mechanism.\n\nFermi liquid theory predicts that the scattering rate, which governs the optical response of metals, not only depends quadratically on temperature (thus causing the formula_27 dependence of the DC resistance), but it also depends quadratically on frequency. This is in contrast to the Drude prediction for non-interacting metallic electrons, where the scattering rate is a constant as a function of frequency.\nOne material in which optical Fermi liquid behavior was experimentally observed is the low-temperature metallic phase of SrRuO.\n\nThe experimental observation of exotic phases in strongly correlated systems has triggered an enormous effort from the theoretical community to try to understand their microscopic origin. One possible route to detect instabilities of a Fermi liquid is precisely the analysis done by Isaak Pomeranchuk. Due to that, the Pomeranchuk instability has been studied by several authors with different techniques in the last few years and in particular, the instability of the Fermi liquid towards the nematic phase was investigated for several models.\n\nThe term non-Fermi liquid, also known as \"strange metal\", is used to describe a system which displays breakdown of Fermi-liquid behaviour. The simplest example of such a system is the system of interacting fermions in one-dimension, called the Luttinger liquid. Although Luttinger liquids are physically similar to Fermi liquids, the restriction to one dimension gives rise to several qualitative differences such as the absence of a \"quasiparticle peak\" in the momentum dependent spectral function, spin-charge separation, and the presence of spin density waves. One cannot ignore the existence of interactions in one-dimension and has to describe the problem with a non-Fermi theory, where Luttinger liquid is one of them. At small finite spin-temperatures in one-dimension the ground-state of the system is described by spin-incoherent Luttinger liquid (SILL).\n\nAnother example of such behaviour is observed at quantum critical points of certain second-order phase transitions, such as heavy fermion criticality, Mott criticality and high-formula_30 cuprate phase transitions. The ground state of such transitions is characterized by the presence of a sharp Fermi surface, although there may not be well-defined quasiparticles. That is, on approaching the critical point, it is observed that the quasiparticle residue formula_31\n\nUnderstanding the behaviour of non-Fermi liquids is an important problem in condensed matter physics. Approaches towards explaining these phenomena include the treatment of \"marginal Fermi liquids\"; attempts to understand critical points and derive scaling relations; and descriptions using \"emergent\" gauge theories with techniques of holographic gauge/gravity duality.\n\n"}
{"id": "77473", "url": "https://en.wikipedia.org/wiki?curid=77473", "title": "Flerovium", "text": "Flerovium\n\nFlerovium is a superheavy artificial chemical element with symbol Fl and atomic number 114. It is an extremely radioactive synthetic element. The element is named after the Flerov Laboratory of Nuclear Reactions of the Joint Institute for Nuclear Research in Dubna, Russia, where the element was discovered in 1998. The name of the laboratory, in turn, honours the Russian physicist Georgy Flyorov ( in Cyrillic, hence the transliteration of \"yo\" to \"e\"). The name was adopted by IUPAC on 30 May 2012.\n\nIn the periodic table of the elements, it is a transactinide element in the p-block. It is a member of the 7th period and is the heaviest known member of the carbon group; it is also the heaviest element whose chemistry has been investigated. Initial chemical studies performed in 2007–2008 indicated that flerovium was unexpectedly volatile for a group 14 element; in preliminary results it even seemed to exhibit properties similar to those of the noble gases. More recent results show that flerovium's reaction with gold is similar to that of copernicium, showing that it is a very volatile element that may even be gaseous at standard temperature and pressure, that it would show metallic properties, consistent with it being the heavier homologue of lead, and that it would be the least reactive metal in group 14. The question of whether flerovium behaves more like a metal or a noble gas is still unresolved as of 2018.\n\nAbout 90 atoms of flerovium have been observed: 58 were synthesized directly, and the rest were made from the radioactive decay of heavier elements. All of these flerovium atoms have been shown to have mass numbers from 284 to 290. The most stable known flerovium isotope, flerovium-289, has a half-life of around 2.6 seconds, but it is possible that the unconfirmed flerovium-290 with one extra neutron may have a longer half-life of 19 seconds; this would be one of the longest half-lives of any isotope of any element at these farthest reaches of the periodic table. Flerovium is predicted to be near the centre of the theorized island of stability, and it is expected that heavier flerovium isotopes, especially the possibly doubly magic flerovium-298, may have even longer half-lives.\n\nFrom the late 1940s to the early 1960s, the early days of the synthesis of heavier and heavier transuranium elements, it was predicted that since such heavy elements did not occur naturally, they would have shorter and shorter half-lives to spontaneous fission, until they stopped existing altogether at around element 108 (now known as hassium). Initial work in the synthesis of the actinides appeared to confirm this. The nuclear shell model, introduced in the late 1960s, stated that the protons and neutrons formed shells within a nucleus, somewhat analogous to electrons forming electron shells within an atom. The noble gases are unreactive due to their having full electron shells; thus it was theorized that elements with full nuclear shells – having so-called \"magic\" numbers of protons or neutrons – would be stabilized against radioactive decay. A doubly magic isotope, having magic numbers of both protons and neutrons, would be especially stabilized, and it was calculated that the next doubly magic isotope after lead-208 would be flerovium-298 with 114 protons and 184 neutrons, which would form the centre of a so-called \"island of stability\". This island of stability, supposedly ranging from copernicium (element 112) to oganesson (118), would come after a long \"sea of instability\" from elements 101 (mendelevium) to 111 (roentgenium), and the flerovium isotopes in it were speculated in 1966 to have half-lives in excess of a hundred million years. These early predictions fascinated researchers, and led to the first attempted synthesis of flerovium in 1968 using the reaction Cm(Ar,xn). No isotopes of flerovium were found in this reaction. This was thought to occur because the compound nucleus Fl only has 174 neutrons instead of the hypothesized magic 184, and this would have a significant impact on the half-life and cross section of such a reaction. It then took thirty more years for the first isotopes of flerovium to be synthesized. More recent work suggests that the local islands of stability around hassium and flerovium are due to these nuclei being respectively deformed and oblate, which make them resistant to spontaneous fission, and that the true island of stability for spherical nuclei occurs at around unbibium-306 (with 122 protons and 184 neutrons).\n\nFlerovium was first synthesized in December 1998 by a team of scientists at the Joint Institute for Nuclear Research (JINR) in Dubna, Russia, led by Yuri Oganessian, who bombarded a target of plutonium-244 with accelerated nuclei of calcium-48:\n\nThis reaction had been attempted before, but without success; for this 1998 attempt, the JINR had upgraded all of its equipment to detect and separate the produced atoms better and bombard the target more intensely. A single atom of flerovium, decaying by alpha emission with a lifetime of 30.4 seconds, was detected. The decay energy measured was 9.71 MeV, giving an expected half-life of 2–23 s. This observation was assigned to the isotope flerovium-289 and was published in January 1999. The experiment was later repeated, but an isotope with these decay properties was never found again and hence the exact identity of this activity is unknown. It is possible that it was due to the metastable isomer Fl, but because the presence of a whole series of longer-lived isomers in its decay chain would be rather doubtful, the most likely assignment of this chain is to the 2n channel leading to Fl and electron capture to Nh, which fits well with the systematics and trends across flerovium isotopes, and is consistent with the low beam energy that was chosen for that experiment, although further confirmation would be desirable via the synthesis of Lv in the Cm(Ca,2n) reaction, which would alpha decay to Fl. The team at RIKEN reported a possible synthesis of the isotopes Lv and Fl in 2016 through the Cm(Ca,2n) reaction, but the alpha decay of Lv was missed, alpha decay of Fl to Cn was observed instead of electron capture to Nh, and the assignment to Lv instead of Lv and decay to an isomer of Cn was not certain.\n\nGlenn T. Seaborg, a scientist at the Lawrence Berkeley National Laboratory who had been involved in work to synthesize such superheavy elements, had said in December 1997 that \"one of his longest-lasting and most cherished dreams was to see one of these magic elements\"; he was told of the synthesis of flerovium by his colleague Albert Ghiorso soon after its publication in 1999. Ghiorso later recalled:\n\nSeaborg died a month later, on 25 February 1999.\n\nIn March 1999, the same team replaced the Pu target with a Pu one in order to produce other flerovium isotopes. This time two atoms of flerovium were produced, decaying via alpha emission with a half-life of 5.5 s. They were assigned as Fl. This activity has not been seen again either, and it is unclear what nucleus was produced. It is possible that it was the meta-stable isomer Fl or the result of an electron capture branch of Fl leading to Nh and Rg.\n\nThe now-confirmed discovery of flerovium was made in June 1999 when the Dubna team repeated the first reaction from 1998. This time, two atoms of flerovium were produced; they alpha decayed with a half-life of 2.6 s, different from the 1998 result. This activity was initially assigned to Fl in error, due to the confusion regarding the previous observations that were assumed to come from Fl. Further work in December 2002 finally allowed a positive reassignment of the June 1999 atoms to Fl.\n\nIn May 2009, the Joint Working Party (JWP) of IUPAC published a report on the discovery of copernicium in which they acknowledged the discovery of the isotope Cn. This implied the discovery of flerovium, from the acknowledgement of the data for the synthesis of Fl and Lv, which decay to Cn. The discovery of the isotopes flerovium-286 and -287 was confirmed in January 2009 at Berkeley. This was followed by confirmation of flerovium-288 and -289 in July 2009 at the Gesellschaft für Schwerionenforschung (GSI) in Germany. In 2011, IUPAC evaluated the Dubna team experiments of 1999–2007. They found the early data inconclusive, but accepted the results of 2004–2007 as flerovium, and the element was officially recognized as having been discovered.\n\nWhile the method of chemical characterisation of a daughter was successful in the cases of flerovium and livermorium, and the simpler structure of even–even nuclei made the confirmation of oganesson (element 118) straightforward, there have been difficulties in establishing the congruence of decay chains from isotopes with odd protons, odd neutrons, or both. To get around this problem with hot fusion, the decay chains from which terminate in spontaneous fission instead of connecting to known nuclei as cold fusion allows, experiments were performed at Dubna in 2015 to produce lighter isotopes of flerovium in the reactions of Ca with Pu and Pu, particularly Fl, Fl, and Fl; the last had previously been characterised in the Pu(Ca,5n)Fl reaction at the Lawrence Berkeley National Laboratory in 2010. The isotope Fl was more clearly characterised, while the new isotope Fl was found to undergo immediate spontaneous fission instead of alpha decay to known nuclides around the \"N\" = 162 shell closure, and Fl was not found. This lightest isotope may yet conceivably be produced in the cold fusion reaction Pb(Ge,n)Fl, which the team at RIKEN in Japan has considered investigating: this reaction is expected to have a higher cross-section of 200 fb than the \"world record\" low of 30 fb for Bi(Zn,n)Nh, the reaction which RIKEN used for the official discovery of element 113, now named nihonium. The Dubna team repeated their investigation of the Pu+Ca reaction in 2017, observing three new consistent decay chains of Fl, an additional decay chain from this nuclide that may pass through some isomeric states in its daughters, a chain that could be assigned to Fl (likely stemming from Pu impurities in the target), and some spontaneous fission events of which some could be from Fl, though other interpretations including side reactions involving the evaporation of charged particles are also possible.\n\nUsing Mendeleev's nomenclature for unnamed and undiscovered elements, flerovium is sometimes called \"eka-lead\". In 1979, IUPAC published recommendations according to which the element was to be called \"ununquadium\" (with the corresponding symbol of \"Uuq\"), a systematic element name as a placeholder, until the discovery of the element is confirmed and a permanent name is decided on. Most scientists in the field called it \"element 114\", with the symbol of \"E114\", \"(114)\" or \"114\".\n\nAccording to IUPAC recommendations, the discoverer(s) of a new element has the right to suggest a name.\nAfter the discovery of flerovium and livermorium was recognized by IUPAC on 1 June 2011, IUPAC asked the discovery team at the JINR to suggest permanent names for those two elements. The Dubna team chose to name element 114 \"flerovium\" (symbol Fl), after the Russian Flerov Laboratory of Nuclear Reactions (FLNR), named after the Soviet physicist Georgy Flyorov (also spelled Flerov); earlier reports claim the element name was directly proposed to honour Flyorov. In accordance with the proposal received from the discoverers IUPAC officially named flerovium after the Flerov Laboratory of Nuclear Reactions (an older name for the JINR), not after Flyorov himself. Flyorov is known for writing to Joseph Stalin in April 1942 and pointing out the silence in scientific journals in the field of nuclear fission in the United States, Great Britain, and Germany. Flyorov deduced that this research must have become classified information in those countries. Flyorov's work and urgings led to the development of the USSR's own atomic bomb project. Flyorov is also known for the discovery of spontaneous fission with Konstantin Petrzhak. The naming ceremony for flerovium and livermorium was held on 24 October 2012 in Moscow.\n\nThe physical basis of the chemical periodicity governing the periodic table is the electron shell closures at each noble gas (atomic numbers 2, 10, 18, 36, 54, 86, and 118): as any further electrons must enter a new shell with higher energy, closed-shell electron configurations are markedly more stable, leading to the relative inertness of the noble gases. Since protons and neutrons are also known to arrange themselves in closed nuclear shells, the same effect happens at nucleon shell closures, which happen at specific nucleon numbers often dubbed \"magic numbers\". The known magic numbers are 2, 8, 20, 28, 50, and 82 for protons and neutrons, and additionally 126 for neutrons. Nucleons with magic proton and neutron numbers, such as helium-4, oxygen-16, calcium-48, and lead-208, are termed \"doubly magic\" and are very stable against decay. This property of increased nuclear stability is very important for superheavy elements: without any stabilization, their half-lives would be expected by exponential extrapolation to be in the range of nanoseconds (10 s) when element 110 (darmstadtium) is reached, because of the ever-increasing repulsive electrostatic forces between the positively charged protons that overcome the limited-range strong nuclear force that holds the nucleus together. The next closed nucleon shells and hence magic numbers are thought to be at the centre of the long-sought island of stability, where the half-lives to alpha decay and spontaneous fission lengthen again.\nInitially, by analogy with the neutron magic number 126, the next proton shell was also expected to occur at element 126, too far away from the synthesis capabilities of the mid-20th century to achieve much theoretical attention. In 1966, new values for the potential and spin-orbit interaction in this region of the periodic table contradicted this and predicted that the next proton shell would occur instead at element 114, and that nuclides in this region would be as stable against spontaneous fission as many heavy nuclei such as lead-208. The expected closed neutron shells in this region were at neutron number 184 or 196, thus making Fl and Fl candidates for being doubly magic. 1972 estimates predicted a half-life of about a year for Fl, which was expected to be near a large island of stability with the longest half-life at Ds (10 years, comparable to that of Th). After the synthesis of the first isotopes of elements 112 through 118 at the turn of the 21st century, it was found that the synthesized neutron-deficient isotopes were stabilized against fission. In 2008 it was thus hypothesized that the stabilization against fission of these nuclides was due to their being oblate nuclei, and that a region of oblate nuclei was centred on Fl. Additionally, new theoretical models showed that the expected gap in energy between the proton orbitals 2f (filled at element 114) and 2f (filled at element 120) was smaller than expected, so that element 114 no longer appeared to be a stable spherical closed nuclear shell. The next doubly magic nucleus is now expected to be around Ubb, but the expected low half-life and low production cross section of this nuclide makes its synthesis challenging. Nevertheless, the island of stability is still expected to exist in this region of the periodic table, and nearer its centre (which has not been approached closely enough yet) some nuclides, such as Mc and its alpha- and beta-decay daughters, may be found to decay by positron emission or electron capture and thus move into the centre of the island. Due to the expected high fission barriers, any nucleus within this island of stability decays exclusively by alpha decay and perhaps some electron capture and beta decay, both of which would bring the nuclei closer to the beta stability line where the island is expected to be. Electron capture is needed to reach the island, which is problematic because it is not certain that electron capture becomes a major decay mode in this region of the chart of nuclides.\n\nSeveral experiments have been performed between 2000 and 2004 at the Flerov Laboratory of Nuclear Reactions in Dubna studying the fission characteristics of the compound nucleus Fl by bombarding a plutonium-244 target with accelerated calcium-48 ions. A compound nucleus is a loose combination of nucleons that have not yet arranged themselves into nuclear shells. It has no internal structure and is held together only by the collision forces between the target and projectile nuclei. The results revealed how nuclei such as this fission predominantly by expelling doubly magic or nearly doubly magic fragments such as calcium-40, tin-132, lead-208, or bismuth-209. It was also found that the yield for the fusion-fission pathway was similar between calcium-48 and iron-58 projectiles, indicating a possible future use of iron-58 projectiles in superheavy element formation. It has also been suggested that a neutron-rich flerovium isotope can be formed by the quasifission (partial fusion followed by fission) of a massive nucleus. Recently it has been shown that the multi-nucleon transfer reactions in collisions of actinide nuclei (such as uranium and curium) might be used to synthesize the neutron-rich superheavy nuclei located at the island of stability, although production of neutron-rich nobelium or seaborgium nuclei is more likely.\n\nTheoretical estimation of the alpha decay half-lives of the isotopes of the flerovium supports the experimental data.\nThe fission-survived isotope Fl, long expected to be doubly magic, is predicted to have alpha decay half-life around 17 days. The direct synthesis of the nucleus Fl by a fusion–evaporation pathway is currently impossible since no known combination of target and stable projectile can provide 184 neutrons in the compound nucleus, and radioactive projectiles such as calcium-50 (half-life fourteen seconds) cannot yet be used in the needed quantity and intensity. Currently, one possibility for the synthesis of the expected long-lived nuclei of copernicium (Cn and Cn) and flerovium near the middle of the island include using even heavier targets such as curium-250, berkelium-249, californium-251, and einsteinium-254, that when fused with calcium-48 would produce nuclei such as Mc and Fl (as decay products of Uue, Ts, and Lv), with just enough neutrons to alpha decay to nuclides close enough to the centre of the island to possibly undergo electron capture and move inwards to the centre, though the cross sections would be small and little is yet known about the decay properties of superheavy nuclides near the beta stability line. This may be the best hope currently to synthesize nuclei on the island of stability, but it is speculative and may or may not work in practice. Another possibility is to use controlled nuclear explosions to achieve the high neutron flux necessary to create macroscopic amounts of such isotopes. This would mimic the r-process in which the actinides were first produced in nature and the gap of instability after polonium bypassed, as it would bypass the gaps of instability at Fm and at mass number 275 (atomic numbers 104 to 108). Some such isotopes (especially Cn and Cn) may even have been synthesized in nature, but would have decayed away far too quickly (with half-lives of only thousands of years) and be produced in far too small quantities (about 10 the abundance of lead) to be detectable as primordial nuclides today outside cosmic rays.\n\nFlerovium is a member of group 14 in the periodic table, below carbon, silicon, germanium, tin, and lead. Every previous group 14 element has four electrons in its valence shell, forming a valence electron configuration of nsnp. In flerovium's case, the trend will be continued and the valence electron configuration is predicted to be 7s7p; flerovium will behave similarly to its lighter congeners in many respects. Differences are likely to arise; a largely contributing effect is the spin–orbit (SO) interaction—the mutual interaction between the electrons' motion and spin. It is especially strong for the superheavy elements, because their electrons move faster than in lighter atoms, at velocities comparable to the speed of light. In relation to flerovium atoms, it lowers the 7s and the 7p electron energy levels (stabilizing the corresponding electrons), but two of the 7p electron energy levels are stabilized more than the other four. The stabilization of the 7s electrons is called the inert pair effect, and the effect \"tearing\" the 7p subshell into the more stabilized and the less stabilized parts is called subshell splitting. Computation chemists see the split as a change of the second (azimuthal) quantum number \"l\" from 1 to and for the more stabilized and less stabilized parts of the 7p subshell, respectively. For many theoretical purposes, the valence electron configuration may be represented to reflect the 7p subshell split as 7s7p. These effects cause flerovium's chemistry to be somewhat different from that of its lighter neighbours.\n\nDue to the spin-orbit splitting of the 7p subshell being very large in flerovium, and the fact that both flerovium's filled orbitals in the seventh shell are stabilized relativistically, the valence electron configuration of flerovium may be considered to have a completely filled shell, making flerovium a very noble metal. Its first ionization energy of should be the highest in group 14. The 6d electron levels are also destabilized, leading to some early speculations that they may be chemically active, although newer work suggests that this is unlikely.\n\nThe closed-shell electron configuration of flerovium results in the metallic bonding in metallic flerovium being weaker than in the preceding and following elements; thus, flerovium is expected to have a low boiling point, and has recently been suggested to be possibly a gaseous metal, similar to the predictions for copernicium, which also has a closed-shell electron configuration. The melting and boiling points of flerovium were predicted in the 1970s to be around 70 °C and 150 °C, significantly lower than the values for the lighter group 14 elements (those of lead are 327 °C and 1749 °C respectively), and continuing the trend of decreasing boiling points down the group. Although earlier studies predicted a boiling point of ~1000 °C or 2840 °C, this is now considered unlikely because of the expected weak metallic bonding in flerovium and that group trends would expect flerovium to have a low sublimation enthalpy. Recent experimental indications have suggested that the pseudo-closed shell configuration of flerovium results in very weak metallic bonding and hence that flerovium is probably a gas at room temperature with a boiling point of around −60 °C. Like mercury, radon, and copernicium, but not lead and oganesson (eka-radon), flerovium is calculated to have no electron affinity.\n\nIn the solid state, flerovium is expected to be a dense metal due to its high atomic weight, with a density variously predicted to be either 22 g/cm or 14 g/cm. Flerovium is expected to crystallize in the face-centred cubic crystal structure like that of its lighter congener lead, although earlier calculations predicted a hexagonal close-packed crystal structure due to spin-orbit coupling effects. The electron of the hydrogen-like flerovium ion (oxidized so that it only has one electron, Fl) is expected to move so fast that it has a mass 1.79 times that of a stationary electron, due to relativistic effects. For comparison, the figures for hydrogen-like lead and tin are expected to be 1.25 and 1.073 respectively. Flerovium would form weaker metal–metal bonds than lead and would be adsorbed less on surfaces.\n\nFlerovium is the heaviest known member of group 14 in the periodic table, below lead, and is projected to be the second member of the 7p series of chemical elements. Nihonium and flerovium are expected to form a very short subperiod, coming between the filling of the 6d and 7p subshells. Their chemical behaviour is expected to be very distinctive: nihonium's homology to thallium has been called \"doubtful\" by computational chemists, while flerovium's to lead has been called only \"formal\".\n\nThe first five members of group 14 show the group oxidation state of +4 and the latter members have an increasingly prominent +2 chemistry due to the onset of the inert pair effect. Tin represents the point at which the stability of the +2 and +4 states are similar, and lead(II) is the most stable of all the chemically well-understood group 14 elements in the +2 oxidation state. The 7s orbitals are very highly stabilized in flerovium and thus a very large sp orbital hybridization is required to achieve the +4 oxidation state, so flerovium is expected to be even more stable than lead in its strongly predominant +2 oxidation state and its +4 oxidation state should be highly unstable. For example, flerovium dioxide (FlO) is expected to be highly unstable to decomposition into its constituent elements (and would not be formed from the direct reaction of flerovium with oxygen), and flerovane (FlH), which should have Fl–H bond lengths of 1.787 Å, is predicted to be more thermodynamically unstable than plumbane, spontaneously decomposing into flerovium(II) hydride (FlH) and hydrogen gas. Flerovium tetrafluoride (FlF) would have bonding mostly due to \"sd\" hybridizations rather than \"sp\" hybridizations, and its decomposition to the difluoride and fluorine gas would be exothermic. The gross destabilization of all the tetrahalides (for example, FlCl is destabilized by about 400 kJ/mol) is unfortunate because otherwise these compounds would be very useful in gas-phase chemical studies of flerovium. The corresponding polyfluoride anion should be unstable to hydrolysis in aqueous solution, and flerovium(II) polyhalide anions such as and are predicted to form preferentially in flerovium-containing solutions. The \"sd\" hybridizations were suggested in early calculations as the 7s and 6d electrons in flerovium share approximately the same energy, which would allow a volatile hexafluoride to form, but later calculations do not confirm this possibility. In general, the spin-orbit contraction of the 7p orbital should lead to smaller bond lengths and larger bond angles: this has been theoretically confirmed in FlH. Nevertheless, even FlH should be relativistically destabilized by 2.6 eV to below Fl+H; the large spin–orbit effects also break down the usual singlet–triplet divide in the group 14 dihydrides. FlF and FlCl are predicted to be more stable than FlH.\n\nDue to the relativistic stabilization of flerovium's 7s7p valence electron configuration, the 0 oxidation state should also be more stable for flerovium than for lead, as the 7p electrons begin to also exhibit a mild inert pair effect: this stabilization of the neutral state may bring about some similarities between the behaviour of flerovium and the noble gas radon. Due to the expected relative inertness of flerovium, its diatomic compounds FlH and FlF should have lower energies of dissociation than the corresponding lead compounds PbH and PbF. Flerovium(IV) should be even more electronegative than lead(IV); lead(IV) has electronegativity 2.33 on the Pauling scale; the lead(II) value is only 1.87.\n\nFlerovium(II) should be more stable than lead(II), and polyhalide ions and compounds of types FlX, FlX, , and (X = Cl, Br, I) are expected to form readily. The fluorides would undergo strong hydrolysis in aqueous solution. All the flerovium dihalides are expected to be stable, with the difluoride being water-soluble. Spin-orbit effects would destabilize flerovium dihydride (FlH) by almost . In solution, flerovium would also form the oxoanion flerovite () in aqueous solution, analogous to plumbite. Flerovium(II) sulfate (FlSO) and sulfide (FlS) should be very insoluble in water, and flerovium(II) acetate (FlCHO) and nitrate (Fl(NO)) should be quite water-soluble. The standard electrode potential for the reduction of Fl ions to metallic flerovium is estimated to be around +0.9 V, confirming the increased stability of flerovium in the neutral state. In general, due to the relativistic stabilization of the 7p spinor, Fl is expected to have properties intermediate between those of Hg or Cd and its lighter congener Pb.\n\nFlerovium is currently the heaviest element to have had its chemistry experimentally investigated, although the chemical investigations have so far not led to a conclusive result. Two experiments were performed in April–May 2007 in a joint FLNR-PSI collaboration aiming to study the chemistry of copernicium. The first experiment involved the reaction Pu(Ca,3n)Fl and the second the reaction Pu(Ca,4n)Fl: these reactions produce short-lived flerovium isotopes whose copernicium daughters would then be studied. The adsorption properties of the resultant atoms on a gold surface were compared with those of radon, as it was then expected that copernicium's full-shell electron configuration would lead to noble-gas like behaviour. Noble gases interact with metal surfaces very weakly, which is uncharacteristic of metals.\n\nThe first experiment allowed detection of three atoms of Cn but also seemingly detected 1 atom of Fl. This result was a surprise given the transport time of the product atoms is ~2 s, so the flerovium atoms produced should have decayed to copernicium before adsorption. In the second reaction, 2 atoms of Fl and possibly 1 atom of Fl were detected. Two of the three atoms displayed adsorption characteristics associated with a volatile, noble-gas-like element, which has been suggested but is not predicted by more recent calculations. These experiments provided independent confirmation for the discovery of copernicium, flerovium, and livermorium via comparison with published decay data. Further experiments in 2008 to confirm this important result detected a single atom of Fl, and supported previous data showing flerovium having a noble-gas-like interaction with gold.\n\nThe experimental support for a noble-gas-like flerovium soon weakened. In 2009 and 2010, the FLNR-PSI collaboration synthesized further atoms of flerovium to follow up their 2007 and 2008 studies. In particular, the first three flerovium atoms synthesized in the 2010 study suggested again a noble-gas-like character, but the complete set taken together resulted in a more ambiguous interpretation, unusual for a metal in the carbon group but not fully like a noble gas in character. In their paper, the scientists refrained from calling flerovium's chemical properties \"close to those of noble gases\", as had previously been done in the 2008 study. Flerovium's volatility was again measured through interactions with a gold surface, and provided indications that the volatility of flerovium was comparable to that of mercury, astatine, and the simultaneously investigated copernicium, which had been shown in the study to be a very volatile noble metal, conforming to its being the heaviest group 12 element known. Nevertheless, it was pointed out that this volatile behaviour was not expected for a usual group 14 metal.\n\nIn even later experiments from 2012 at the GSI, the chemical properties of flerovium were found to be more metallic than noble-gas-like. Jens Volker Kratz and Christoph Düllmann specifically named copernicium and flerovium as belonging to a new category of \"volatile metals\"; Kratz even speculated that they might be gaseous at standard temperature and pressure. These \"volatile metals\", as a category, were expected to fall between normal metals and noble gases in terms of adsorption properties. Contrary to the 2009 and 2010 results, it was shown in the 2012 experiments that the interactions of flerovium and copernicium respectively with gold were about equal. Further studies showed that flerovium was more reactive than copernicium, in contradiction to previous experiments and predictions.\n\nIn a 2014 paper detailing the experimental results of the chemical characterisation of flerovium, the GSI group wrote: \"[flerovium] is the least reactive element in the group, but still a metal.\" Nevertheless, in a 2016 conference about the chemistry and physics of heavy and superheavy elements, Alexander Yakushev and Robert Eichler, two scientists who had been active at GSI and FLNR in determining the chemistry of flerovium, still urged caution based on the inconsistencies of the various experiments previously listed, noting that the question of whether flerovium was a metal or a noble gas was still open with the available evidence: one study suggested a weak noble-gas-like interaction between flerovium and gold, while the other suggested a stronger metallic interaction. The same year, new experiments aimed at probing the chemistry of copernicium and flerovium were conducted at GSI's TASCA facility, and the data from these experiments is currently being analysed. As such, unambiguous determination of the chemical characteristics of flerovium has yet to have been established, although the experiments to date have allowed the first experimental estimation of flerovium's boiling point: around −60 °C, so that it is probably a gas at standard conditions. The longer-lived flerovium isotope Fl has been considered of interest for future radiochemical studies.\n\n"}
{"id": "45498354", "url": "https://en.wikipedia.org/wiki?curid=45498354", "title": "Fluid kinematics", "text": "Fluid kinematics\n\nFluid kinematics is a field of physics and mechanics concerned with the movement of fluids. Fluids tend to flow easily, which causes a net motion of molecules from one point in space to another point as a function of time. These fluids may be liquids or may be materials with fluid properties, including crowds of people or volumes of grains.\n\nUsing the continuum hypothesis, fluids are classified into fluid particles, which are composed of numerous fluid molecules. These particles interact with one another and with the surroundings they are in. Using a Eulerian model (the continuum hypothesis), fluid motion can be described in terms of acceleration or velocity.\n\nThe composition of the material contains two types of terms: those involving the time derivative and those involving spatial derivatives. The time derivative portion is denoted as the local derivative, and represents the effects of unsteady flow. The local derivative occurs during unsteady flow, and becomes zero for steady flow.\n\nThe portion of the material derivative represented by the spatial derivatives is called the convective derivative. It accounts for the variation in fluid property, be it velocity or temperature for example, due to the motion of a fluid particle in space where its values are different.\n\nThe acceleration of a particle is the time rate of change of its velocity. Using an Eulerian description for velocity, the velocity field V = V(x,y,z,t), and deriving it with respect to time, we obtain the acceleration field.\n"}
{"id": "38408843", "url": "https://en.wikipedia.org/wiki?curid=38408843", "title": "Gedser wind turbine", "text": "Gedser wind turbine\n\nThe Gedser wind turbine is located near Gedser in the south of the Danish island of Falster. It was constructed by the engineer Johannes Juul in 1957 for the SEAS (\"Sydsjællands Elektricitets Aktieselskab\") electricity company with support from the Marshall Plan. Its innovative design was a major breakthrough in the development of wind turbines.\n\nAs a 17-year-old, Johannes Juul (1887–1969) had studied wind electricity applications under Poul la Cour at Askov Højskole, a folk high school, in 1904. In the early 1950s, he built two smaller alternating current turbines which operated at Vester Egesborg near Næstved and on the island of Bogø. His three-bladed Gedser facility (1957) was Denmark's first large wind turbine. With a blade span of , it produced 200 kW of alternating current fed directly into the grid. Its electromechanical yawing, asynchronous generator, and the three stall-regulated blades with emergency aerodynamic tip brakes (these were invented by Juul) is a design that is still widely used in Denmark. Stall control was provided through an asynchronous generator. The turbine, which for many years was the world's largest, operated from 1957 to 1967 without maintenance, demonstrating incredible durability. In connection with NASA testing for the U.S. wind energy programme, it was refurbished in 1975 and brought back into operation. It continued to run for a few years, providing test data for the further development of wind turbines in Denmark. Over the course of its lifetime, the Gedser wind turbine generated 2.2 million kW-hours (7.9 TJ).\n\nThe turbine's straightforward design, safety features, and low cost were key assets. It was one of Denmark's most important innovations since the Second World War. The design served not only as a basis for developments in Denmark but was adopted by wind turbine manufacturers around the globe, who referred to it as the \"Danish design\". Its design was considered seminal for the modern wind industry. In 2006, it was included in the design section of the Danish Culture Canon.\n\nIn 1992, the turbine was dismantled. In 2006, the nacelle and the rotor blades were taken to the Energy Museum (\"Energimuseet\") near Bjerringbro in central Jutland where they were reassembled as part of the museum's collection.\n\n"}
{"id": "11144232", "url": "https://en.wikipedia.org/wiki?curid=11144232", "title": "Gulf Publishing Company", "text": "Gulf Publishing Company\n\nGulf Publishing Company is an international publishing and events business dedicated to the hydrocarbon energy sector. In mid-2018 it rebranded as Gulf Energy Information. Founded in 1916 by Ray Lofton Dudley, Gulf Energy Information produces and distributes publications in print and web formats, online news, webcasts and databases; hosts conferences and events designed for the energy industry. The company was a subsidiary of Euromoney Institutional Investor from 2001 until a 2016 management buyout by CEO John Royall and Texas investors. The business and strategy publication \"Petroleum Economist\" also transferred to the company in May 2016. In mid-2017 the company acquired 109-year old Oildom Publishing. \n\nThe company's flagship magazines, \"World Oil,\" \"Hydrocarbon Processing\", \"Pipeline & Gas Journal\", and the \"Petroleum Economist\" are published monthly. Gulf is headquartered in Houston, Texas, with sales staff and columnists around the world, due to expansion efforts by William G. Dudley, Sr. The \"Petroleum Economist\" publishing and map cartography staff are based in London, UK. Gulf Energy Info's Data Services staff support on-line \"Energy Web Atlas\" energy data visualization, and \"Construction Boxscore\" downstream project database, from Houston, London and Mumbai. \n\nSince 1916 \"World Oil\" has covered the upstream oil and gas industry for conventional, shale, offshore, exploration and production technology in oil and gas.\n\nSince 1922, \"Hydrocarbon Processing\" has provided job-relevant information to technical staff, operations, maintenance and management in petroleum refining, gas processing facilities, petrochemical and engineer/constructor companies throughout the world. Bi-monthly supplement \"Gas Processing & LNG\" was added in 2012.\n\nSince 1934, the \"Petroleum Economist\" has written about oil, its politics and economics - explained some of the industry’s biggest disruptions: such as the 1973 oil crisis, the Gulf Wars, the rise of China, the Arab uprisings, and the more recent supply-side shocks from North America’s unconventional energy sector.\n\nSince 1859, \"Pipeline & Gas Journal\" has been the essential resource for technology and trends in the midstream industry; written and edited to be of service to those involved in moving, marketing and managing hydrocarbons from wellheads to ultimate consumers. \n\nThe company formerly published trade books, but spun off the division as TaylorWilson (now part of Taylor Trade) in 2000; sold its professional book list to Elsevier in 2013.\n"}
{"id": "10731502", "url": "https://en.wikipedia.org/wiki?curid=10731502", "title": "Honeycomb structure", "text": "Honeycomb structure\n\nHoneycomb structures are natural or man-made structures that have the geometry of a honeycomb to allow the minimization of the amount of used material to reach minimal weight and minimal material cost. The geometry of honeycomb structures can vary widely but the common feature of all such structures is an array of hollow cells formed between thin vertical walls. The cells are often columnar and hexagonal in shape. A honeycomb shaped structure provides a material with minimal density and relative high out-of-plane compression properties and out-of-plane shear properties.\n\nMan-made honeycomb structural materials are commonly made by layering a honeycomb material between two thin layers that provide strength in tension. This forms a plate-like assembly. Honeycomb materials are widely used where flat or slightly curved surfaces are needed and their high Specific strength is valuable. They are widely used in the aerospace industry for this reason, and honeycomb materials in aluminum, fibreglass and advanced composite materials have been featured in aircraft and rockets since the 1950s. They can also be found in many other fields, from packaging materials in the form of paper-based honeycomb cardboard, to sporting goods like skis and snowboards.\n\nNatural honeycomb structures include beehives, honeycomb weathering in rocks, tripe, and bone.\n\nMan-made honeycomb structures include sandwich-structured composites with honeycomb cores. Man-made honeycomb structures are manufactured by using a variety of different materials, depending on the intended application and required characteristics, from paper or thermoplastics, used for low strength and stiffness for low load applications, to high strength and stiffness for high performance applications, from aluminum or fiber reinforced plastics. The strength of laminated or sandwich panels depends on the size of the panel, facing material used and the number or density of the honeycomb cells within it. Honeycomb composites are used widely in many industries, from aerospace industries, automotive and furniture to packaging and logistics. \nThe material takes its name from its visual resemblance to a bee's honeycomb – a hexagonal sheet structure.\n\nThe hexagonal comb of the honey bee has been admired and wondered about from ancient times. The first man-made honeycomb, according to Greek mythology, is said to have been manufactured by Daedalus from gold by lost wax casting more than 3000 years ago. Marcus Varro reports that the Greek geometers Euclid and Zenodorus found that the hexagon shape makes most efficient use of space and building materials. The interior ribbing and hidden chambers in the dome of the Pantheon in Rome is an early example of a honeycomb structure.\n\nGalileo Galilei discusses in 1638 the resistance of hollow solids: \"Art, and nature even more, makes use of these in thousands of operations in which robustness is increased without adding weight, as is seen in the bones of birds and in many stalks that are light and very resistant to bending and breaking”.\nRobert Hooke discovers in 1665 that the natural cellular structure of cork is similar to the hexagonal honeybee comb. and Charles Darwin states in 1859 that \"the comb of the hive-bee, as far as we can see, is absolutely perfect in economizing labour and wax”.\n\nThe first paper honeycomb structures might have been made by the Chinese 2000 years ago for ornaments, but no reference for this has been found. Paper honeycombs and the expansion production process has been invented in Halle/Saale in Germany by Hans Heilbrun in 1901 for decorative applications. First honeycomb structures from corrugated metal sheets had been proposed for bee keeping in 1890. For the same purpose, as foundation sheets to harvest more honey, a honeycomb moulding process using a paper paste glue mixture had been patented in 1878. The three basic techniques for honeycomb production that are still used today—expansion, corrugation and moulding—were already developed by 1901 for non-sandwich applications.\n\nHugo Junkers first explored the idea of a honeycomb core within a laminate structure. He proposed and patented the first honeycomb cores for aircraft application in 1915. He described in detail his concept to replace the fabric covered aircraft structures by metal sheets and reasoned that a metal sheet can also be loaded in compression if it is supported at very small intervals by arranging side by side a series of square or rectangular cells or triangular or hexagonal hollow bodies. The problem of bonding a continuous skin to cellular cores led Junkers later to the open corrugated structure, which could be riveted or welded together.\n\nThe first use of honeycomb structures for structural applications had been independently proposed for building application and published already in 1914. In 1934 Edward G. Budd patented a welded steel honeycomb sandwich panel from corrugated metal sheets and Claude Dornier aimed 1937 to solve the core-skin bonding problem by rolling or pressing a skin which is in a plastic state into the core cell walls. The first successful structural adhesive bonding of honeycomb sandwich structures was achieved by Norman de Bruyne of Aero Research Limited, who patented an adhesive with the right viscosity to form resin fillets on the honeycomb core in 1938. The North American XB-70 Valkyrie made extensive use of stainless steel honeycomb panels using a brazing process they developed.\n\nA summary of the important developments in the history of honeycomb technology is given below:\n\nThe three traditional honeycomb production techniques, expansion, corrugation, and moulding, were all developed by 1901 for non-sandwich applications. For decorative applications the expanded honeycomb production reached a remarkable degree of automation in the first decade of the 20th century.\n\nToday honeycomb cores are manufactured via the expansion process and the corrugation process from composite materials such as glass-reinforced plastic (also known as fiberglass), carbon fiber reinforced plastic, Nomex aramide paper reinforced plastic, or from a metal (usually aluminum).\n\nHoneycombs from metals (like aluminum) are today produced by the expansion process. Continuous processes of folding honeycombs from a single aluminum sheet after cutting slits had been developed already around 1920. \nContinuous in-line production of metal honeycomb can be done from metal rolls by cutting and bending.\n\nThermoplastic honeycomb cores (usually from polypropylene) are usually made by extrusion processed via a block of extruded profiles or extruded tubes from which the honeycomb sheets are sliced.\nRecently a new, unique process to produce thermoplastic honeycombs has been implemented, allowing a continuous production of a honeycomb core as well as in-line production of honeycombs with direct lamination of skins into cost efficient sandwich panel.\n\nToday, a wide variety of materials can be formed into a honeycomb composite. For example paperboard honeycomb is used in paper pallets and package cushioning, blocking and bracing.\n\nComposite honeycomb structures have been used in numerous engineering and scientific applications.\n\nMore recent developments show that honeycomb structures are also advantageous in applications involving nanohole arrays in anodized alumina, microporous arrays in polymer thin films, activated carbon honeycombs, and photonic band gap honeycomb structures.\n\nA honeycomb mesh is often used in aerodynamics to reduce or to create wind turbulence. It is also used to obtain a standard profile in a wind tunnel (temperature, flow speed). A major factor in choosing the right mesh is the length ratio (length vs honeycomb cell diameter) \"L/d\".\n\nLength ratio < 1:\nHoneycomb meshes of low length ratio can be used on vehicles front grille. Beside the aesthetic reasons, these meshes are used as screens to get a uniform profile and to reduce the intensity of turbulence.\n\nLength ratio » 1:\nHoneycomb meshes of large length ratio reduce lateral turbulence and eddies of the flow. Early wind tunnels used them with no screens; unfortunately, this method introduced high turbulence intensity in the test section. Most modern tunnels use both honeycomb and screens.\n\nWhile aluminium honeycombs are common use in the industry, other materials are offered for specific applications. People using metal structures should take care of removing burrs as they can introduce additional turbulences. Polycarbonate structures are a low-cost alternative.\n\nThe honeycombed, screened center of this open-circuit air intake for Langley's first wind tunnel ensured a steady, non-turbulent flow of air. Two mechanics pose near the entrance end of the actual tunnel, where air was pulled into the test section through a honeycomb arrangement to smooth the flow.\n\nHoneycomb is not the only cross-section available in order to reduce eddies in an airflow. Square, rectangular, circular and hexagonal cross-sections are other choices available, although honeycomb is generally the preferred choice.\n\nIn combination with two skins applied on the honeycomb, the structure offers a sandwich panel with excellent rigidity at minimal weight. The behavior of the honeycomb structures is orthotropic, meaning the panels react differently depending on the orientation of the structure. It is therefore necessary to distinguish between the directions of symmetry, the so-called L and W-direction. The L-direction is the strongest and the stiffest direction. The weakest direction is at 60° from the L-direction (in the case of a regular hexagon) and the most compliant direction is the W-direction.\nAnother important property of honeycomb sandwich core is its compression strength. Due to the efficient hexagonal configuration, where walls support each other, compression strength of honeycomb cores is typically higher (at same weight) compared to other sandwich core structures such as, for instance, foam cores or corrugated cores.\n\nThe mechanical properties of honeycombs depend on its cell geometry, the properties of the material from which the honeycomb is constructed (often referred to as the solid), which include the Young's modulus, yield stress, and fracture stress of the material, and the relative density of the honeycomb (the density of the honeycomb normalized by that of the solid, ρ/ρ). The elastic moduli of low-density honeycombs have been found to be independent of the solid. The mechanical properties of honeycombs will also vary based on the direction in which the load is applied.\n\nIn-plane loading: Under in-plane loading, it is often assumed that the wall thickness of the honeycomb is small compared to the length of the wall. For a regular honeycomb, the relative density is proportional to the wall thickness to wall length ratio (t/L) and the Young’s modulus is proportional to (t/L). Under high enough compressive load, the honeycomb reaches a critical stress and fails due to one of the following mechanisms – elastic buckling, plastic yielding, or brittle crushing. The mode of failure is dependent on the material of the solid which the honeycomb is made of. Elastic buckling of the cell walls is the mode of failure for elastomeric materials , ductile materials fail due to plastic yielding, and brittle crushing is the mode of failure when the solid is brittle. The elastic buckling stress is proportional to the relative density cubed, plastic collapse stress is proportional to relative density squared, and brittle crushing stress is proportional to relative density squared. Following the critical stress and failure of the material, a plateau stress is observed in the material, in which increases in strain are observed while the stress of the honeycomb remains roughly constant. Once a certain strain is reached, the material will begin to undergo densification as further compression pushes the cell walls together.\n\nOut of-plane loading: Under out-of-plane loading, the out-of-plane Young’s modulus of a regular hexagonal honeycombs is proportional to the relative density of the honeycomb. The elastic buckling stress is proportional to (t/L) while the plastic buckling stress is proportional to (t/L).\n\nThe shape of the honeycomb cell is often varied to meet different engineering applications. Shapes that are commonly used besides the regular hexagonal cell include triangular cells, square cells, and circular-cored hexagonal cells, and circular-cored square cells. The relative densities of these cells will depend on their new geometry.\n\n"}
{"id": "20120882", "url": "https://en.wikipedia.org/wiki?curid=20120882", "title": "Hongcheng Magic Liquid", "text": "Hongcheng Magic Liquid\n\nThe Hongcheng Magic Liquid incident was a scam in China where Wang Hongcheng (), a bus driver from Harbin with no scientific education, claimed in 1983 that he could turn regular water into a fuel as flammable as petrol by simply dissolving a few drops of his liquid in it. He founded the Hongcheng Magic Liquid company with funds from Chinese governmental agencies and other supporters, raising a total of 300 million yuan (US$37 million), but no product was ever released.\n\nAround that time, in 1994, the Chinese Government, alarmed by an increase in pseudoscience and superstitions since the death of Mao Zedong, made a declaration decrying the deterioration of science education in the country, taking several measures to improve science education and to improve the prevalence of science and technology in courts. One of these efforts was to require the scientific authoritative journal \"Science and Technology Daily\" to carry an article critical of Hongcheng's invention, which had been previously rejected at several major Chinese publications. This created a growing publicity and opposition of Hongcheng's invention, silencing his supporters.\n\nIn 1995 Hongcheng refused an invitation to carry a scientific appraisal of his invention at Beijing, and the notable scientist and debunker He Zuoxiu and 40 other scientists made a statement calling for the Chinese Government to investigate his claims. Hongcheng was eventually arrested, put to trial, and in 1998, was found guilty of fraud and deceit and condemned to 10 years of prison.\n\nWang Hongcheng was a bus driver from Harbin, a former soldier with ninth-grade education and no scientific training. In 1983 he claimed that he invented a liquid that could transform a liter of regular water into a fuel by simply adding two or three drops of his liquid, and that this resulting fuel was as combustible as petrol. Hongcheng called it \"the fifth greatest invention of China\", in a reference to the four Great Inventions of China, and his invention became more popular, finally reaching on 28 January 1993 the front pages of a major national newspaper. In the winter of 1985, Wang showed this technique in Beijing, Hebei, Zhejiang and Shanghai.\n\nThe Chinese security and military departments started to look into his claims and funded his research. In 1992–1993 Hongcheng founded a company called \"Hongcheng Magic Liquid\" to manufacture his product and raised a total of 300 million yuan (US$37 million). No product, working or otherwise, was ever commercially released.\n\nIn 1994 the Chinese Government was alarmed by a recent raise in pseudoscience and a revival of old superstition, and issued a declaration declaring the rise of superstition and ignorance, how the science education had declined among the population, and how it would make efforts to combat this situation. After this declaration, Song Jian, then chief director of the Chinese National Science Committee, held a conference on how to carry this work among the public. Song Jian was then told at the conference was that an article debunking Hongcheng's invention had been refused publication in three major national newspapers and one scientific publication. Song Jian then required \"Science and Technology Daily\", the most authoritative newspaper in China's science and technology, to publish it. This publication explained that when Hongcheng's invention hit newspaper front pages in 1993, different newspapers reported different proportions of oil and water in their announcements, and used this to make a critical analysis. It publicized the invention, started an opposition feeling, and silenced Hongcheng and their supporters.\n\nAfter the public backlash caused by He Zuoxiu's article, the Chinese people started to realize that Hongcheng's \"technique\" was a fraud. Hongcheng faced growing criticism from scientists and media. He Zuoxiu, a member of the Chinese Academy of Sciences as well a notable scientist and debunker, had been invited several times by Hongcheng's supporters to visit the northeast and watch his invention in action. In 1995, Zuoxiu asked them to Beijing to pass a scientific appraisal of his liquid, as the country capital would be a proper place for such a scientific and universal invention. Hongcheng refused, prompting Zuoxiu and a group of other 40 scientists to ask the top Chinese legislative body, the People's Political Consultative Conference, to investigate his claims. After this declaration, his invention collapsed by itself, and it was finally found that it was a fraud.\n\nIn 1998 he was found guilty of fraud and deceit and sent to prison for 10 years Hongcheng acquired the status of a legendary figure, because some people thought that it was a case of cover-up or of free energy suppression, where he would be imprisoned not because his formula would not really work, but because of refusing to release his secret formula to the government.\n\nThe Chinese Government were alarmed by this and other similar cases of promotion of pseudoscience even before Hongcheng's refusal to carry a scientific appraisal at Beijing and the publicity it carried. The Chinese government decided to tighten the appraisal system for scientific claims and other measures like creating science and technology courts. After the death of Mao Zedong, several Western pseudoscience theories appeared in rural areas, together with the return to ancient China practices like ancestor worship, astrology and fortune telling. The government newspaper had lamented \"the superstition of feudal ideology is raising again in the Chinese countryside\". Individual claiming to have \"special powers\" claimed that they project their Qi out of their body to cure people, and called \"masters of Qi Gong\", one of them being arrested after causing the death of several patients. The Asian Rhinoceros was being driven to extinction because pulverized Rhinoceros horn were said to prevent impotence, despite the fact that Rhinoceros horn is composed of keratin, the same substance that is present in human hair and fingernails.\n\nTo address all these issues, the government of China and the Chinese Communist Party made on 5 December 1994 a joint public declaration called \"Some Suggestions on How to Reinforce the Popularization of Science\". In it they lamented the recent withering of public education and the growth of pseudoscience and anti-science ideas, and then indicated that they would reinforce the public education about science.\n\nMost officials support the effort to root out pseudoscience in China, although the government still sends occasionally mixed signals, like one incident when a high-ranking official ordered the Guangming Daily to pull a letter critical of Hongcheng at the last minute. He Zuoxiu explains that corruption will also have to be fought at the same time as pseudoscience, since some officials and journalists may \"have a stake\" in these inventions, and would be harmed by their exposure.\n\n\n"}
{"id": "25266201", "url": "https://en.wikipedia.org/wiki?curid=25266201", "title": "Horsham Stone", "text": "Horsham Stone\n\nHorsham Stone is a type of calcerous, flaggy sandstone containing millions of minute sand grains and occurring naturally in the Wealden clay of the English county of West Sussex. It is also high in mica and quartz. The rock extends in an arc-like formation for several kilometres around the town of Horsham from which it takes its name, and lies just below the Wealden clay surface in bands thick. Horsham Stone is significant for its ripple-marked appearance, formed by the action of the sea similar to the ripples on the sandbanks and beaches of Sussex.\n\nSussex Stone and its limestone equivalent Sussex Marble were formed around 130 million years ago in the Lower Cretaceous period when Britain was quite different from the shape it is today. It is estimated that the latitude for Britain was approximately 30 degrees north of the equator. The fossil evidence in Horsham Stone and Sussex Marble indicate a diversity of life living on a broad, flat subtropical landscape.Towards the end of the Cretaceous period around 90 million years ago, most of Britain including Sussex would have been submerged beneath a tropical sea that was depositing chalk. \n\nThe sediments of the Wealden Basin were buried under hundreds of metres of further sediment over the next 100 million years and were then uplifted gradually by platetonic movement and then eroded to expose Wealden rocks. Horsham Stone would have been visible to early settlers after the Ice Age around 12,000 years ago. When quarried, Horsham Stone could be extracted in \"flat grey slabs of varying thickness\".\n\nHorsham Stone has a long history of use. The earliest record is from the Bronze Age. Archeologists at Amberley found quern fragments made of Horsham Stone at Amberley Mount. It was used extensively by the Romans including in the construction of Stane Street. Villas such as Bignor and Fishbourne have examples of flooring and roof slates of the material. \n\nIn later centuries, and there are numerous examples in Sussex and the surrounding counties of its use as a roofing material, particularly for mills, dovecotes, churches, manor houses and similar buildings. Completely rainproof and long-lasting, it was ideal for these structures. Smaller-scale uses include road surfaces, for which the thickest slabs were typically used; the footpaths leading to St Mary's Church, Shipley, West Sussex and the north wall of St Nicholas' Church, Itchingfield, West Sussex; and for gravestones, fonts and tombs (of which there are several examples at St Mary the Virgin's Church, Horsham). Once exposed it hardens quickly and will last for hundreds of years. The characteristic ripple marks are retained. There are numerous old quarry workings throughout Sussex but just one working quarry in 2018. Large quarries at Nuthurst and Stammerham (near Christ's Hospital) school are no longer extant, but others survive in isolated Wealden settings.\n\nLarge-scale commercial extraction of Horsham stone had stopped by the 1880s, and regular quarrying ceased completely in the 1930s with the closure of a small quarry at Nowhurst, Strood Green near Horsham. Purbeck Stone was regarded as a good substitute for Horsham Stone and repairs to stonework tended to substitute it for the more expensive and scarcer Horsham Stone. The Nowhurst quarry was reopened in 2004 by the Historic Horsham Stone Company. It has started producing roofing slates for the area's historic houses.\n\n"}
{"id": "31037820", "url": "https://en.wikipedia.org/wiki?curid=31037820", "title": "Hutchinson's rule", "text": "Hutchinson's rule\n\nThe observation that the trophic structures (i.e. mouths) of sympatric congeneric species generally vary by a factor of ~1.3. This variation presumably leads to niche differentiation, allowing coexistence of multiple similar species in the same habitat, by partitioning food resources. The rule's legitimacy has been questioned, as other categories of objects also exhibit size ratios of roughly 1.3.\n"}
{"id": "11945196", "url": "https://en.wikipedia.org/wiki?curid=11945196", "title": "Hydrogen isocyanide", "text": "Hydrogen isocyanide\n\nHydrogen isocyanide is a chemical with the molecular formula HNC. It is a minor tautomer of hydrogen cyanide (HCN). Its importance in the field of astrochemistry is linked to its ubiquity in the interstellar medium.\n\nBoth \"hydrogen isocyanide\" and \"azanylidyniummethanide\" are correct IUPAC names for HNC. There is no preferred IUPAC name. The second one is according to the \"substitutive nomenclature rules\", derived from the \"parent hydride\" azane (NH) and the anion methanide (C).\n\nHydrogen isocyanide (HNC) is a linear triatomic molecule with C point group symmetry. It is a zwitterion and an isomer of hydrogen cyanide (HCN). Both HNC and HCN have large, similar dipole moments, with \"μ\" = 3.05 Debye and \"μ\" = 2.98 Debye respectively. These large dipole moments facilitate the easy observation of these species in the interstellar medium.\n\nAs HNC is higher in energy than HCN by 3920 cm (46.9  kJ/mol), one might assume that the two would have an equilibrium ratioformula_1 at temperatures below 100 Kelvin of 10. However, observations show a very different conclusion; formula_2 is much higher than 10, and is in fact on the order of unity in cold environments. This is because of the potential energy path of the tautomerization reaction; there is an activation barrier on the order of roughly 12,000 cm for the tautomerization to occur, which corresponds to a temperature at which HNC would already have been destroyed by neutral-neutral reactions.\n\nIn practice, HNC is almost exclusively observed astronomically using the \"J\" = 1→0 transition. This transition occurs at ~90.66 GHz, which is a point of good visibility in the atmospheric window, thus making astronomical observations of HNC particularly simple. Many other related species (including HCN) are observed in roughly the same window.\n\nHNC is intricately linked to the formation and destruction of numerous other molecules of importance in the interstellar medium—aside from the obvious partners HCN, protonated hydrogen cyanide (HCNH), and cyanide (CN), HNC is linked to the abundances of many other compounds, either directly or through a few degrees of separation. As such, an understanding of the chemistry of HNC leads to an understanding of countless other species—HNC is an integral piece in the complex puzzle representing interstellar chemistry.\n\nFurthermore, HNC (alongside HCN) is a commonly used tracer of dense gas in molecular clouds. Aside from the potential to use HNC to investigate gravitational collapse as the means of star formation, HNC abundance (relative to the abundance of other nitrogenous molecules) can be used to determine the evolutionary stage of protostellar cores.\n\nThe HCO/HNC line ratio is used to good effect as a measure of density of gas. This information provides great insight into the mechanisms of the formation of (Ultra-)Luminous Infrared Galaxies ((U)LIRGs), as it provides data on the nuclear environment, star formation, and even black hole fueling. Furthermore, the HNC/HCN line ratio is used to distinguish between photodissociation regions and X-ray-dissociation regions on the basis that [HNC]/[HCN] is roughly unity in the former, but greater than unity in the latter.\n\nThe study of HNC is a relatively simple pursuit, and this is one of the greatest motivations for its study. Aside from having its \"J\" = 1→0 transition in a clear portion of the atmospheric window, as well as having numerous isotopomers also available for easy study, and in addition to having a large dipole moment that makes observations particularly simple, HNC is, in its molecular nature, a quite simple molecule. This makes the study of the reaction pathways that lead to its formation and destruction a good means of obtaining insight to the workings of these reactions in space. Furthermore, the study of the tautomerization of HNC to HCN (and vice versa), which has been studied extensively, has been suggested as a model by which more complicated isomerization reactions can be studied.\n\nHNC is found primarily in dense molecular clouds, though it is ubiquitous in the interstellar medium. Its abundance is closely linked to the abundances of other nitrogen-containing compounds. HNC is formed primarily through the dissociative recombination of HNCH and HNC, and it is destroyed primarily through ion-neutral reactions with and C. Rate calculations were done at 3.16 × 10 years, which is considered early time, and at 20 K, which is a typical temperature for dense molecular clouds.\n\nThese four reactions are merely the four most dominant, and thus the most significant in the formation of the HNC abundances in dense molecular clouds; there are dozens more reactions for the formation and destruction of HNC. Though these reactions primarily lead to various protonated species, HNC is linked closely to the abundances of many other nitrogen containing molecules, for example, NH and CN. The abundance HNC is also inexorably linked to the abundance of HCN, and the two tend to exist in a specific ratio based on the environment. This is because the reactions that form HNC can often also form HCN, and vice versa, depending on the conditions in which the reaction occurs, and also that there exist isomerization reactions for the two species.\n\nHCN (not HNC) was first detected in June 1970 by L. E. Snyder and D. Buhl using the 36-foot radio telescope of the National Radio Astronomy Observatory. The main molecular isotope, HCN, was observed via its \"J\" = 1→0 transition at 88.6 GHz in six different sources: W3 (OH), Orion A, Sgr A(NH3A), W49, W51, DR 21(OH). A secondary molecular isotope, HCN, was observed via its \"J\" = 1→0 transition at 86.3 GHz in only two of these sources: Orion A and Sgr A(NH3A). HCN was then later detected extragalactically in 1988 using the IRAM 30-m telescope at the Pico de Veleta in Spain. It was observed via its \"J\" = 1→0 transition at 90.7 GHz toward IC 342.\n\nA number of detections have been made towards the end of confirming the temperature dependence of the abundance ratio of [HNC]/[HCN]. A strong fit between temperature and the abundance ratio would allow observers to spectroscopically detect the ratio and then extrapolate the temperature of the environment, thus gaining great insight into the environment of the species. The abundance ratio of rare isotopes of HNC and HCN along the OMC-1 varies by more than an order of magnitude in warm regions versus cold regions. In 1992, the abundances of HNC, HCN, and deuterated analogs along the OMC-1 ridge and core were measured and the temperature dependence of the abundance ratio was confirmed. A survey of the W 3 Giant Molecular Cloud in 1997 showed over 24 different molecular isotopes, comprising over 14 distinct chemical species, including HNC, HNC, and HNC. This survey further confirmed the temperature dependence of the abundance ratio, [HNC]/[HCN], this time ever confirming the dependence of the isotopomers.\n\nThese are not the only detections of importance of HNC in the interstellar medium. In 1997, HNC was observed along the TMC-1 ridge and its abundance relative to HCO was found to be constant along the ridge—this led credence to the reaction pathway that posits that HNC is derived initially from HCO. One significant astronomical detection that demonstrated the practical use of observing HNC occurred in 2006, when abundances of various nitrogenous compounds (including HNC and HNC) were used to determine the stage of evolution of the protostellar core Cha-MMS1 based on the relative magnitudes of the abundances.\n\nOn 11 August 2014, astronomers released studies, using the Atacama Large Millimeter/Submillimeter Array (ALMA) for the first time, that detailed the distribution of HCN, HNC, HCO, and dust inside the comae of comets C/2012 F6 (Lemmon) and C/2012 S1 (ISON).\n\n\n"}
{"id": "16836458", "url": "https://en.wikipedia.org/wiki?curid=16836458", "title": "Hydrogen maser", "text": "Hydrogen maser\n\nA hydrogen maser, also known as hydrogen frequency standard, is a specific type of maser that uses the intrinsic properties of the hydrogen atom to serve as a precision frequency reference.\n\nBoth the proton and electron of a hydrogen atom have spins. The atom has a higher energy if both are spinning in the same direction, and a lower energy if they spin in opposite directions. The amount of energy needed to reverse the spin of the electron is equivalent to a photon at the frequency of , which corresponds to the 21 cm line in hydrogen spectrum.\n\nHydrogen masers are very complex devices and sell for as much as . They are made in two types: active and passive.\n\nIn both types, a small storage bottle of molecular hydrogen, H, leaks a controlled amount of gas into a discharge bulb. The molecules are dissociated in the discharge bulb into individual hydrogen atoms by an arc. This atomic hydrogen passes through a collimator and a magnetic state selector. The atoms are thereby selected for the desired state and passed on to a storage bulb. The storage bulb is roughly 20 cm high and 10 cm in diameter and made of quartz. Its inside is coated with Teflon, allowing many collisions of the atoms with the wall without perturbation of the atomic state, and slowing the recombination of the hydrogen atoms into hydrogen molecules. A durable Teflon & bulb coating technology allows for over 20-year lifetime. The storage bulb is in turn inside a microwave cavity made from a precisely machined copper or silver-plated ceramic cylinder. This cavity is tuned to the  GHz resonance frequency of the atoms. A weak static magnetic field is applied parallel to the cavity axis by a solenoid to lift the degeneracy of the magnetic Zeeman sublevels. To decrease the influence of changing external magnetic fields on the transition line frequency and be compliant to electromagnetic interferences, the cavity is surrounded by several nested layers of shields.\n\nIn the active hydrogen maser, the cavity oscillates by itself. This requires a higher hydrogen atom density and a higher quality factor for the cavity. However, with advanced microwave cavities made out of silver-plated ceramic, the gain factor can be much higher, thereby requiring less hydrogen atom density. The active maser is more complex and more expensive but has better short-term and long-term frequency stabilities.\n\nIn the passive hydrogen maser, the cavity is fed from an external  GHz frequency. The external frequency is tuned to produce a maximum output in the cavity. This allows the use of lower hydrogen atom density and lower cavity quality factor, which reduces the cost.\n\n"}
{"id": "30866010", "url": "https://en.wikipedia.org/wiki?curid=30866010", "title": "Institute of Petroleum Engineering", "text": "Institute of Petroleum Engineering\n\nThe Institute of Petroleum Engineering (IPE) is a specialised centre in teaching, training and research with the largest Petroleum Engineering research programme in the UK. Founded in 1975 and based in the city of Edinburgh, IPE has expanded to the far reaches of Scotland in the Orkney Islands, and internationally, into the Middle East & Asia through its Approved Learning Partnerships.\n\n"}
{"id": "55536670", "url": "https://en.wikipedia.org/wiki?curid=55536670", "title": "KCHT Power Station", "text": "KCHT Power Station\n\nThe KCHT Power Station is a municipal solid waste-fired thermal power station currently under construction at Muthurajawela in Sri Lanka. It is being constructed together with the Aitken Spence Power Station, after it won the bid by the Urban Development Authority from a pool of 121 bidders, 19 of which were foreign. Construction of the facility began on and will cost approximately , with an estimated completion slated for mid-2019.\n\nThe power station will be operated by , a subsidiary of the South Korean company . It will use of waste from the Colombo and Gampaha suburbs. The generated power will be sold to the state-owned Ceylon Electricity Board at a rate of generated. The remaining bottom ash resulting from the process would be used for road construction and other purposes, while the unusable fly ash residue (amounting to 2%) being disposed at locations already identified.\n\n"}
{"id": "22278969", "url": "https://en.wikipedia.org/wiki?curid=22278969", "title": "Korean Air Flight 801", "text": "Korean Air Flight 801\n\nKorean Air Flight 801 (KE801, KAL801) crashed on August 6, 1997, on approach to Antonio B. Won Pat International Airport, in the United States territory of Guam, killing 228 of the 254 people aboard. The aircraft crashed on Nimitz Hill in Asan, Guam, while on approach to the airport.\n\nFlight 801 was normally flown by an Airbus A300; since Korean Air had scheduled the August 5–6 flight to transport Guamanian athletes to the South Pacific Mini Games in American Samoa, the airline designated HL7468, a 12-year-old Boeing 747-300 delivered to Korean Air on December 12, 1984, to fly the route that night.\n\nThe flight was under the command of 42-year-old Captain Park Yong-chul (Korean: 박용철, Hanja: 朴鏞喆, RR: \"Bak Yong-cheol\". M-R: \"Pak Yongch'ŏl\") The captain had close to 9,000 hours of flight time and had recently received a Flight Safety Award for negotiating a 747 engine failure at low altitude. Park had originally been scheduled to fly to Dubai, United Arab Emirates; since he did not have enough rest for the Dubai trip, he was reassigned to Flight 801. The first officer was 40-year-old Song Kyung-ho (Korean: 송경호, Hanja: 宋慶昊, RR: \"Song Gyeong-ho\", M-R: \"Song Kyŏngho\"), who had more than 4,000 hours' flying experience, and the flight engineer was 57-year-old Nam Suk-hoon (Korean: 남석훈, Hanja: 南錫薰, RR: \"Nam Seok-hun\", M-R: \"Nam Sŏkhun\"), a veteran pilot with more than 13,000 flight hours.\n\nFlight 801 departed from Seoul-Kimpo International Airport (now Gimpo Airport) at 8:53 p.m. (9:53 p.m. Guam time) on August 5 on its way to Guam. It carried three flight crew members (the two pilots and the flight engineer), 14 flight attendants, and 237 passengers, a total of 254 people. Of the passengers, three were children between the ages of 2 and 12 and three were 24 months old or younger. Six of the passengers were Korean Air flight attendants, who were deadheading.\n\nThe flight experienced some turbulence but was uneventful until shortly after 1:00 a.m. on August 6, as the jet was preparing to land. There was heavy rain at Guam so visibility was significantly reduced and the crew attempted an instrument landing. The glideslope Instrument Landing System (ILS) for runway 6L was out of service; captain Park believed it was in service, however, and at 1:35 am managed to pick up a signal that was later identified to be from an irrelevant electronic device on the ground. The crew noticed that the aircraft was descending very steeply, and noted several times that the airport \"is not in sight.\" Despite protests from flight engineer Nam that the detected signal was not the glide-slope indicator, Park pressed on. The crew lowered the landing gear and continued to prepare the aircraft for landing. 12 seconds before impact, the Ground Proximity Warning system activated, warning the crew about their actual altitude. First officer Song declared a missed approach, and captain Park declared a go-around, but it was too late. At 1:42 am, the aircraft's main landing gear struck a fuel pipeline, and crashed into Nimitz Hill. The crash site was about short of the runway, at an altitude of .\n\nOf the 254 people on board, 228 died as a result of the crash. One survivor, 36-year-old Hyun Seong Hong (홍현성, also spelled \"Hong Hyun Sung\") of the United States, occupied Seat 3B in first class, and said that the crash occurred so quickly that the passengers \"had no time to scream\" and likened the crash to \"a scene from a film.\"\n\nThe rescue effort was hampered by the weather, terrain, and other problems. Emergency vehicles could not approach due to the fuel pipeline destroyed by the crash and blocking the narrow road. United States Navy Seabees of NMCB133 were some of the first on the scene as they utilized their earth moving equipment to clear roadways and timber from the crash site approach. The Seabees used backhoes to crack open the still-burning plane to rescue survivors and erected mortuary tents for first responders. There was confusion over the administration of the effort; the crash occurred on land owned by the United States Navy but civil authorities initially claimed authority. The hull had disintegrated, and jet fuel in the wing tanks had sparked a fire that was still burning eight hours after impact.\n\nGovernor Carl T.C. Gutierrez found 11-year-old Rika Matsuda, a South Korean citizen from Japan who boarded the flight with her mother, 44-year-old Cho Sung-yeo (also known as Shigeko). They were heading to Guam on vacation. Rika Matsuda described what happened to her and her mother to interpreters. Cho could not free herself from the aircraft and told Rika to run away. Luggage piled on the girl and her mother as the crash occurred; Rika Matsuda said her mother, unable to free herself, asked her to leave. Cho died in the fire. After escaping from the aircraft, Rika discovered a surviving flight attendant, Lee Yong Ho (이용호). They stayed together until Gutierrez discovered them. Rika Matsuda, treated at Guam Memorial Hospital in Tamuning, was released on August 7, 1997, and was reunited with her father, Tatsuo Matsuda. The two were then escorted to the Governor House where they were the guests of Gutierrez and the First Lady of Guam, Geri Gutierrez, for several days; afterwards Rika and Tatsuo Matsuda flew to Japan.\n\nA special weather observation made at 01:47 five minutes after the impact reported:\n\nWind variable at 4 knots; visibility—5 miles; present weather—light rain shower; sky condition—few 1,500 feet, scattered 2,500 feet, overcast 4,000 feet; temperature 26° C; dew point 24° C; altimeter 29.85 inches Hg. \n\nThe U.S. National Transportation Safety Board (NTSB) investigation report stated that a contributing factor was that the ATC Minimum Safe Altitude Warning (MSAW) system at Antonio B. Won Pat International Airport had been deliberately modified so as to limit spurious alarms and could not detect an approaching aircraft that was below minimum safe altitude. The probable cause of the accident was the captain's poor execution of the non-precision approach, the captain's fatigue, poor communication between the flight crew, and Korean Air's lack of flight crew training. The crew had been using an outdated flight map that was missing a 724 foot obstruction symbol depicted at the NIMITZ VOR and that map stated the Minimum Safe Altitude while crossing the NIMITZ VOR for a landing aircraft was as opposed to the updated altitude of . Flight 801 crashed near the NIMITZ VOR, which is situated on Nimitz Hill at a height of at 1:42 am, when it descended below the minimum safe altitude of during its landing approach. The report also identified that the captain may have mistakenly believed that the airplane was closer to the airport\nthan it was and that there may have been confusion about the location of the Distance Measuring Equipment (DME) in relation to the airport, with the crew anticipating the VOR/DME to be located at the airport. The DME was sited at the NIMITZ VOR some 3.3 nm from the airport and such a configuration had not been part of Korean Air's simulator training, the crew's training for such non-precision approaches having been carried out in scenarios where the DME was located at the airport. Nevertheless the correct DME distances were shown on the approach chart.\n\nThe NTSB presented its findings on March 24, 25 and 26, 1998 in the Hawaii Convention Center in Honolulu.\n\nThe section of the report entitled \"Probable Cause\" concluded:\n\nThe National Transportation Safety Board determines that the probable cause of this accident was the captain’s failure to adequately brief and execute the non-precision approach and the first officer’s and flight engineer’s failure to effectively monitor and cross-check the captain’s execution of the approach. Contributing to these failures were the captain’s fatigue and Korean Air’s inadequate flight crew training.\n\nContributing to the accident was the Federal Aviation Administration’s intentional inhibition of the minimum safe altitude warning system at Guam and the agency’s failure to adequately manage the system.\n\nMany of the passengers were vacationers and honeymooners flying to Guam.\n\n11-year old Rika Matsuda, a South Korean passport holder, was described as Japanese in many press reports. One South Korean was an expatriate who lived in Guam, while New Zealander Barry Small worked in Guam.\n\nOf the 254 people on board, 223 – 209 passengers and 14 crew members (3 flight crew and 11 cabin crew) – were killed at the crash site.\n\nOf the 31 occupants found alive by rescue crews, two died en route to the hospital and a further three in hospital. Among the survivors, 16 received burn injuries. The 26 survivors were initially treated at Guam Memorial Hospital (GMH) in Tamuning or at the U.S. Naval Hospital in Agana Heights. Four were subsequently transferred to the U.S. Army Burn Center in San Antonio, Texas. and eight to University Hospital in Seoul.\n\nThere were 23 passengers and three flight attendants who survived the crash with serious injuries.\n\nShin Ki-ha, a four-term South Korean parliamentarian and former leader of the National Congress for New Politics, traveled with his wife and around 20 party members. Shin and his wife were killed.\n\nOn August 13, 1997, 12 sets of remains were brought to Guam's airport to be readied to be flown back to Seoul. Clifford Guzman, a governor's aide, said that two of the 12 were taken back to the morgue. Of the 10, one was misidentified and had to be switched before takeoff. The 10 bodies transported to Seoul were of seven passengers and three female flight attendants. On the same date, an NTSB family affairs official named Matthew Furman said that in total, by that date, 46 bodies had been identified.\n\nAfter the crash occurred, the airline provided several flights for around 300 relatives so that they could go to the crash site.\n\nOn August 13, 1997, 50 protesters staged a sit-in at Guam Airport, saying that the recovery of the dead was taking too long; they sat on blankets and sheets of paper at the Korean Air counter.\n\nOn August 5, 1998, the first anniversary of the crash, a black marble obelisk was unveiled on the crash site as a memorial to the victims.\n\nAfter the accident, Korean Air services to Guam were suspended for more than four years, leading to reduced tourist spending in Guam and reduced revenues for Korean Air. When Seoul-Guam services resumed in December 2001, the flight number was changed to 805. The flight number for its Seoul-Guam route is now 111 and operates out of Incheon instead of Gimpo, using a Boeing 777-300 or an Airbus A330.\n\nIn 2000, a lawsuit was settled in the amount of $70,000,000 United States dollars on behalf of 54 families against the airline.\n\nNew Zealander Barry Small, a helicopter pilot and a survivor of the accident, lobbied for safer storage of duty-free alcohol and redesigns of crossbars on airline seats; he said that the storage of duty-free alcohol on Flight 801 contributed to spreading of the fire and the crossbars injured passengers to the point where they could not escape from the aircraft (Small himself was injured when he broke his leg on one of the crossbars during the crash, but was still able to escape the aircraft).\n\nThe Government of Guam moved its website about the Korean Air crash after the Spamcop program alerted the government that advance fee fraud spam from Nigeria used the website link as a part of the scam. Scam e-mails used names of passengers, such as Sean Burke, as part of the fraud.\n\nFollowing the Korean Air 801 crash, it was brought to the NTSB's attention that foreign carriers flying in and out of the US were not covered by the Aviation Disaster Family Assistance Act of 1996 and Korean Air did not have a plan to deal with the situation they encountered. As a result, US Congress passed the Foreign Air Carrier Family Support Act of 1997 to require those carriers to file family assistance plans and fulfill the same family support requirements as domestic airlines. Not only does the Act ensure that all victims and family members will be treated equitably, regardless of the carrier they use, it also impels many carriers that may not have thought about family assistance issues to give them due consideration in their emergency response plans.\n\n\n\n\nNotes\nBibliography\n\nFurther reading\n\n"}
{"id": "15089590", "url": "https://en.wikipedia.org/wiki?curid=15089590", "title": "L'Express Airlines Flight 508", "text": "L'Express Airlines Flight 508\n\nOn July 10, 1991, a L'Express Airlines Beechcraft C99, flying as Flight 508 originating in New Orleans, and in transit from Mobile to Birmingham, crashed while attempting to make an ILS approach to Runway 5 (since renumbered to Runway 6) at Birmingham Municipal Airport (now Birmingham–Shuttlesworth International Airport) in Birmingham, Alabama. The plane crashed in the Fairview area near Five Points West in the Ensley neighborhood and subsequently injured four persons on the ground, as well as destroying two homes. Of the 15 occupants on board, there were 13 fatalities. The cause of the crash was attributed to the captain's decision to attempt an instrument approach into severe thunderstorms resulting in a loss of control of the airplane. To date it is the deadliest commercial aviation accident in Alabama history.\n\nL'Express Flight 508 was operated with a Beech C99 twin-engine turboprop aircraft. Seating on the plane was five rows of two seats, one on each side of a central aisle. A single seat was located across from the left passenger loading door and a double seat at the rear of the aircraft. Passengers boarded through the rear passenger door. The flight left New Orleans, LA with one passenger, at 4:05 pm CDT, landing in Mobile, AL, at 4:50 pm CDT. After changing crews and boarding 12 passengers, the flight departed for Birmingham Municipal Airport at 5:05 pm CDT. As the flight approached Birmingham, strong thunderstorms developed in the vicinity of the airport. Around the same time four other aircraft either diverted to other airports or delayed their approach and entered a holding pattern until the weather improved. The crew of Flight 508 was aware of the thunderstorm activity but elected to continue the approach. Francis Fernandes, the L'Express captain at the controls of Flight 508, later said the plane experienced a \"significant roll to the left\" on landing approach. Fernandes told investigators that while he and the first officer tried to level the aircraft, it experienced an \"extreme updraft\" that pushed the plane's nose into the air. After entering a severe thunderstorm cell southwest of the airport, the crew lost directional control and was unable to recover the aircraft prior to impacting two houses in the Ensley neighborhood of Birmingham at 6:11:27 pm CDT. The airplane struck one home, crossed a tree lined residential street, and slammed into a second home, immediately erupting into flames. The lone surviving passenger was encountered in the home by the residents before all fled the burning home. The captain was taken to UAB Hospital, passenger Mabry Rogers to Carraway Methodist Medical Center, and three residents to Baptist Princeton. Twelve passengers and the first officer perished in the crash; the captain, one passenger, and four residents were injured.\n\nThe accident occurred during the 6:00 p.m. local evening news broadcasts. Local media reports began around 6:45 pm CDT with local ABC television affiliate WBRC broadcasting live by 6:45 pm CDT. Radio and television coverage continued through the night. Notably, WBRC was recording weather radar images around the time of the crash, these images would later be used in the official NTSB investigation and other crash-related litigation. Coverage of the crash was carried on the front page of newspapers nationwide in the days following the accident.\n\nThe National Transportation Safety Board dispatched a team to investigate the accident. The focus of the investigation was immediately centered on the weather at the time of the accident. Investigators were surprised by the presence of a cockpit voice recorder in the airplane as such recorders were not required for the involved airplane at the time. Following a detailed investigation, the NTSB issued its final report on March 3, 1992; AAR-92/01. The formal probable cause of the accident was \"the decision of the captain to initiate and continue an instrument approach into clearly identified thunderstorm activity, resulting in a loss of control of the airplane from which the flightcrew was unable to recover and subsequent collision with obstacles and the terrain.\"\n\nThe only survivors of the crash were the captain, Francis Fernandez of Niceville, Florida, and Mabry Rogers of Birmingham, Alabama.\n\n"}
{"id": "58673", "url": "https://en.wikipedia.org/wiki?curid=58673", "title": "Liquid hydrogen", "text": "Liquid hydrogen\n\nLiquid hydrogen (LH2 or LH) is the liquid state of the element hydrogen. Hydrogen is found naturally in the molecular H form.\n\nTo exist as a liquid, H must be cooled below hydrogen's critical point of 33 K. However, for hydrogen to be in a fully liquid state without boiling at atmospheric pressure, it needs to be cooled to 20.28 K (−423.17 °F; −252.87 °C). One common method of obtaining liquid hydrogen involves a compressor resembling a jet engine in both appearance and principle. Liquid hydrogen is typically used as a concentrated form of hydrogen storage. As in any gas, storing it as liquid takes less space than storing it as a gas at normal temperature and pressure. However, the liquid density is very low compared to other common fuels. Once liquefied, it can be maintained as a liquid in pressurized and thermally insulated containers.\n\nThere are two spin isomers of hydrogen; liquid hydrogen consists of 99.79% parahydrogen and 0.21% orthohydrogen.\n\nIn 1885, Zygmunt Florenty Wróblewski published hydrogen's critical temperature as 33K; critical pressure, 13.3 atmospheres; and boiling point, 23K.\n\nHydrogen was liquefied by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. The first synthesis of the stable isomer form of liquid hydrogen, parahydrogen, was achieved by Paul Harteck and Karl Friedrich Bonhoeffer in 1929.\n\nThe two nuclei in a dihydrogen molecule can have two different spin states.\nParahydrogen, in which the two nuclear spins are antiparallel, is more stable than orthohydrogen, in which the two are parallel. At room temperature, gaseous hydrogen is mostly in the ortho isomeric form due to thermal energy, but an ortho-enriched mixture is only metastable when liquified at low temperature. It slowly undergoes an exothermic reaction to become the para isomer, with enough energy released as heat to cause some of the liquid to boil. To prevent loss of the liquid during long-term storage, it is therefore intentionally converted to the para isomer as part of the production process, typically using a catalyst such as iron(III) oxide, activated carbon, platinized asbestos, rare earth metals, uranium compounds, chromium(III) oxide, or some nickel compounds.\n\nLiquid hydrogen is a common liquid rocket fuel for rocketry applications — both NASA and the United States Air Force operate a large number of liquid hydrogen tanks with an individual capacity up to 3.8 million liters (1 million U.S. gallons). In most rocket engines fueled by liquid hydrogen, it first cools the nozzle and other parts before being mixed with the oxidizer — usually liquid oxygen (LOX) — and burned to produce water with traces of ozone and hydrogen peroxide. Practical H–O rocket engines run fuel-rich so that the exhaust contains some unburned hydrogen. This reduces combustion chamber and nozzle erosion. It also reduces the molecular weight of the exhaust, which can actually increase specific impulse, despite the incomplete combustion.\nLiquid hydrogen can be used as the fuel for an internal combustion engine or fuel cell. Various submarines (Type 212 submarine, Type 214 submarine) and concept hydrogen vehicles have been built using this form of hydrogen (see DeepC, BMW H2R). Due to its similarity, builders can sometimes modify and share equipment with systems designed for liquefied natural gas (LNG). However, because of the lower volumetric energy, the hydrogen volumes needed for combustion are large. Unless direct injection is used, a severe gas-displacement effect also hampers maximum breathing and increases pumping losses.\n\nLiquid hydrogen is also used to cool neutrons to be used in neutron scattering. Since neutrons and hydrogen nuclei have similar masses, kinetic energy exchange per interaction is maximum (elastic collision). Finally, superheated liquid hydrogen was used in many bubble chamber experiments.\n\nThe first thermonuclear bomb, Ivy Mike, used liquid deuterium (hydrogen-2), for nuclear fusion.\n\nThe product of its combustion with oxygen alone is water vapor (although if its combustion is with oxygen and nitrogen it can form toxic chemicals), which can be cooled with some of the liquid hydrogen. Since water is harmless to the environment, an engine burning it can be considered \"zero emissions.\" Liquid hydrogen also has a much higher specific energy than gasoline, natural gas, or diesel.\n\nThe density of liquid hydrogen is only 70.99 g/L (at 20 K), a relative density of just 0.07. Although the specific energy is more than twice that of other fuels, this gives it a remarkably low volumetric energy density, many fold lower.\n\nLiquid hydrogen requires cryogenic storage technology such as special thermally insulated containers and requires special handling common to all cryogenic fuels. This is similar to, but more severe than liquid oxygen. Even with thermally insulated containers it is difficult to keep such a low temperature, and the hydrogen will gradually leak away (typically at a rate of 1% per day). It also shares many of the same safety issues as other forms of hydrogen, as well as being cold enough to liquefy, or even solidify atmospheric oxygen, which can be an explosion hazard.\n\nThe triple point of hydrogen is at 13.81 K 7.042 kPa.\n\nDue to its cold temperatures, liquid hydrogen is a hazard for cold burns. Apart from that, elemental hydrogen as a liquid is biologically inert and its only human health hazard as a vapor is displacement of oxygen, resulting in asphyxiation. Because of its flammability, liquid hydrogen should be kept away from heat or flame unless ignition is intended.\n"}
{"id": "9543237", "url": "https://en.wikipedia.org/wiki?curid=9543237", "title": "Louisburg Square", "text": "Louisburg Square\n\nLouisburg Square is a private square located in Boston, Massachusetts that is maintained by the Louisburg Square Proprietors. While the Proprietors pay taxes to the City of Boston, the city does not own the square or its garden. It was named for the 1745 Battle of Louisbourg, in which Massachusetts militiamen led by William Pepperrell, who was made the first American baronet for his role, sacked the French Fortress of Louisbourg.\n\nThe square itself is a small grassy oval surrounded by a wrought-iron fence; access is generally not available. There is a statue of Columbus at the north end and of Aristides the Just at the south end.\n\nThe Greek Revival houses around the square reflect the rarefied privilege enjoyed by the 19th century upper class in Beacon Hill. One of the last private residences built on Louisburg Square was 2 Louisburg Square, built in 1847 for wealthy merchant and philanthropist Thomas Handasyd Perkins Jr., known as \"Short-Arm Tom\", who lived at 1 Joy Street. Among the famous people who lived there in the 19th Century were \"Atlantic Monthly\" editor William Dean Howells, architect Charles Bulfinch, painter John Singleton Copley, and teacher A. Bronson Alcott and his daughter, author Louisa May Alcott (who died there). Jenny Lind was married in the parlor of a house on Louisburg Square.\n\n, it is one of the most expensive residential neighborhoods in the USA; townhouses on Louisburg Square sold for $11,500,000 in 2011 and $11,000,000 in 2012, for instance. The square is often included in walking tours and guidebooks. Former U.S. Secretary of State John Kerry owns a townhouse on Louisburg Square.\n"}
{"id": "1945956", "url": "https://en.wikipedia.org/wiki?curid=1945956", "title": "Memetic engineering", "text": "Memetic engineering\n\nMemetic engineering is a term developed by Leveious Rolando, John Sokol, and Gibron Burchett based on Richard Dawkins' theory of memes.\n\n\nIn contrast, gutation is a term developed by Erik Buitenhuis and is:\n\n\nAccording to the theory of Memes coined by Richard Dawkins, evolution depends not on the particular chemical basis of genetics, but only on the existence of a self-replicating unit of transmission—in the case of biological evolution, the gene. For Dawkins, the meme exemplified another self-replicating unit with potential significance in explaining human behavior and cultural evolution: the effect a meme has on society is based on the application of the meme after understanding the qualities essential to the meme. According to the theory, memetic engineering is, simply put, the analysis of an individual or individuals' behavior, the selection of specific memes and the distribution or propagation of those memes with the intent of altering the behavior of others. A memetic engineer doesn't particularly have to consciously make the decision to alter another individuals behavior. It can happen unconsciously when specific behavior is observed, transmitted and then replicated within the observer. Memes themselves are neither good nor bad. For example, \"racism\" is an ideology that is made up of several memes. When a meme is introduced, those concepts begin to take on their own process of evolution based on the person who adopts the ideology, internalizes it, and reintroduces it into society causing it to spread like a virus.\n\nAccording to the above theory, typical memetic engineers include scientists, engineers, industrial designers, ad-men, artists, publicists, political activists, and religious missionaries.\n\nDawkins agrees that much of theology and other theoretical aspects of religion can be viewed as the careful, even worshipful, handling of extremely powerful memeplexes with very odd or difficult traits.\n\nMemetic Engineering developed from diverse influences, including cutting-edge physics of consciousness and memetics research, chaos theory, semiotics, culture jamming, military information warfare, and the viral texts of iconoclasts William S. Burroughs, J.G. Ballard, and Genesis P-Orridge. It draws upon Third Culture sciences and conceptual worldviews for Social Engineering, Values Systems Alignment, and Culture Jamming purposes. An important example of macro-historical memetic engineering analysis explaining how domination, patriarchy, war and violence are culturally programmed is Riane Eisler's \"The Chalice and the Blade\" (San Francisco: Harper SanFrancisco, 1988), which outlines her Dominator and Partnership Culture thesis. The savvy memetic engineer is able to isolate, study, and subtly manipulate the underlying values systems, symbolic balance and primal atavisms that unconsciously influence the individual psyche and collective identity. A highly educated but susceptible intelligentsia, worldwide travel, and information vectors like the Internet, cable television, and tabloid media, means that hysterical epidemics and disinformation campaigns may become more common. This warfare will be conducted using aesthetics, symbols, and doctrines as camouflage that will ultimately influence our cultural meme pool. These contemporary life conditions (Historic Times; Geographic Place; Existential Problems; and Societal Circumstances) are explored in books like Carl Sagan's \"The Demon Haunted World: Science As a Candle in the Dark\" (New York: Ballantine Books, 1996), John Brockman's \"The Third Culture: Beyond the Scientific Revolution\" (New York: Touchstone Books, 1996), and Michael Shermer's \"Why People Believe Weird Things: Pseudo-science, Superstition, and Other Confusions of Our Time\" (New York: W.H. Freeman & Co, 1996). Fictional descriptions of memetic engineering include Isaac Asimov's seminal \"Foundation\" Trilogy (New York: Bantam Books, 1991), George Gurdjieff's artificial mythology \"Beelzebub's Tales to His Grandson\" (New York: Penguin USA, 1999); Neil Stephenson's novels \"Snow Crash\" (New York: Bantam Spectra, 1993) and \"The Diamond Age\" (New York: Bantam Spectra, 1996); and Robert W. Chambers' unearthly \"The King in Yellow\" (Buccaneer Books, 1996) tome, which influenced seminal horror author H. P. Lovecraft.\n\nMemetic engineering as a social science lends examples of itself in multiple areas and disciplines. It is currently being examined and researched by the US military as a means to counter insurgency and combat terrorism as explained below in \"From the Clash to the Confluence of Civilizations\" by Thomas P.M. Barnett, an American military geostrategist, and Richard J. Pech's \"Inhibiting Imitative Terrorism Through Memetic Engineering\".\n\nOther examples of applied memetic engineering are present but not exclusive to the marketing and advertising industries. The question is whether these individuals can be truly considered memetic engineers. Marketing and advertising professionals create memes on an ongoing basis; however, this alone doesn't necessarily qualify them as being memetic engineers. Few if any actually fall into this category. This is possibly due to a lack of understanding of the various memes that have taken root in their target audience minds. According to the definition, industrial designers, musicians, artists, athletes, and other entertainers would more likely better serve this definition. This is because of their ability to create products, phrases and ideas that disseminate the population triggering a response within the brain causing a cultural phenomena.\n\nGame theory provides an empirical means of advancing the science of memetics. Memetic game theory, attempts to mathematically capture behavior in strategic situations; where an individual's success in making choices depends on the choices of others, based on past experiences, emotional behavior and learned behavior. It also offers a scientific approach to analyzing social interactions.\n\nAn example of an engineered meme is Godwin's law, a meme which propagates on mail-lists, and which its author professes to have initiated to reduce spam on those lists; one version is \"When someone posts a metaphor about Nazis the thread is no longer useful.\"\n\nRichard Pech discusses the concept of memetic engineering within the context of countering mind contagions associated with terrorism. School shootings, for example, may be explained as an attempt to demonstrate the ultimate form of rebellion against a system in which the perpetrators feel ostracised or isolated. Acts of violence might appeal to their egos and the means for achieving this is replicated via the shooting meme. To re-engineer such a meme and its ability to infest susceptible minds, all information concerning such violence must be portrayed in an unappealing manner. For example, no one wants to be associated with acts of cowardice. By strongly suggesting that such violence is cowardly and the work of disturbed minds, it has less appeal for replication. In this manner the shooting meme has been re-engineered by removing its attraction, and therefore removing its ability to replicate.\n\n\n"}
{"id": "27351781", "url": "https://en.wikipedia.org/wiki?curid=27351781", "title": "NetRegs", "text": "NetRegs\n\nNetRegs is a website set up to help small businesses in Scotland and Northern Ireland become more environmentally aware. It has been developed by a partnership between the Scottish Environment Protection Agency (SEPA) in Scotland and the Northern Ireland Environment Agency (NIEA) in Northern Ireland. NetRegs is a source of free environmental guidance for small and medium-sized enterprises based in Scotland and Northern Ireland. The website provides information and guidance on key environmental legislation as well as good practice for every type of business. It is free to use and available 24 hours a day. Since 2006 it has also offered to provide updates by email. NetRegs also provides free e-learning tools, an environmental self assessment tool and hosts the GPPs (Guidance for Pollution Prevention documents, a replacement for the old PPGs) \n"}
{"id": "25885832", "url": "https://en.wikipedia.org/wiki?curid=25885832", "title": "Ola Skaalvik Elvevold", "text": "Ola Skaalvik Elvevold\n\nOla Skaalvik Elvevold (born 12 June 1988 in Tromsø) is a Norwegian environmentalist. He is a former chairman of Natur og Ungdom. Prior to his leadership he had been deputy chairman since 2008.\n"}
{"id": "3949418", "url": "https://en.wikipedia.org/wiki?curid=3949418", "title": "Pantechnicon van", "text": "Pantechnicon van\n\nA Pantechnicon van, currently usually shortened to pantec, was originally a furniture removal van drawn by horses and used by the British company \"The Pantechnicon\" for delivering and collecting furniture which its customers wished to store. The name is a word largely of British English usage.\n\nThe word \"Pantechnicon\" is an invented one, formed from the Greek \"pan\" (\"all\") and \"techne\" (\"art\"). It was originally the name of a large establishment in Motcomb Street, Belgravia, London, opened around 1830. It combined a picture gallery, a furniture shop, and the sale of carriages, while its southern half was a sizable warehouse for storing furniture and other items. Seth Smith (property developer), whose family were originally from Wiltshire, was a builder in the early 19th century, and constructed much of the new housing in Belgravia, then a country area. Their clients required storage facilities and this was built on an awkward left-over triangular site with a Greek style Doric column façade, and called Pantechnicon, pseudo-Greek for \"pertaining to all the arts or crafts\".\n\nSubsequently, special wagons were designed with sloping ramps to more easily load furniture, with the building name on the side. The very large, distinctive, and noticeable horse-drawn vans that were used to collect and deliver the customers' furniture came to be known as \"Pantechnicon vans.\" From around 1900, the name was shortened to simply Pantechnicon. The Pantechnicon Ltd, a furniture storage and removal company, continued to trade until the 1970s.\n\nThe building was largely destroyed by fire in 1874, but the facade still exists and the usefulness of the vans was by then well established and they had been adopted by other firms. As of 2015 the façade and the building behind it has been leased by its owner, Grosvenor Estates, to Cubitt House, a company specializing in pubs and restaurants in the Belgravia area, and is to be redeveloped into a \"food and retail emporium\" over six floors, including a basement and a roof-terrace.\n\nThough small by modern standards, the vans were impressively large by those of their own time. They came in lengths of between 12 and 18 feet, and were up to 7 feet broad. The roof was a segment of a cylinder 8 inches higher in the middle than at the edges to ensure ready drainage but it had boards round the edges to allow stowage of extra items. Below the roof-line the body was a cuboid box except that behind the space required by the front wheels when turning tightly, the floor was lowered to permit greater internal headroom. This was achieved by cranking the back axle downwards as in a float. The lowered floor also saved some of the lifting which was a feature of using normal horse-drawn lorries and vans, which needed a deck high enough to fit the steering mechanism below it. Access was obtained through hinged doors at the rear. Outside these, the tailboard was hinged upwards from the level of the well.\n\nSome pantechnicons were drawn by two horses in tandem. This seems to have been so as to allow entry to relatively narrow town lanes and such places as the warehouse doorways. To give the driver a clear view of obstructions and to enable him to control the lead horse, he was usually seated on the front of the roof.\n\nFrom the early 1900s onward lift-off container bodies were introduced which could be lifted off the chassis and transferred to a rail wagon or to the hold of a ship.\n\nThe value of these vans seems to have been quite quickly appreciated so that removal firms other than The Pantechnicon operated them, sometimes over long distances between towns, a business which was eventually superseded by the spread of the railways.\n\nAn adventure with a pantechnicon is one of the episodes in the Arnold Bennett novel, \"The Card\" (1911).\n\nA pantech truck or van is a word derivation of \"pantechnicon\" commonly currently used in Australia. A pantech is a truck or van with a freight hull made of (or converted to) hard panels. Such vehicles can be used for chilled freight, or as removal vans.\n\n"}
{"id": "24006", "url": "https://en.wikipedia.org/wiki?curid=24006", "title": "Paraffin wax", "text": "Paraffin wax\n\nParaffin wax is a soft colourless solid, derived from petroleum, coal or oil shale, that consists of a mixture of hydrocarbon molecules containing between twenty and forty carbon atoms. It is solid at room temperature and begins to melt above approximately ; its boiling point is >. Common applications for paraffin wax include lubrication, electrical insulation, and candles;\n\nUn-dyed, unscented paraffin candles are odorless and bluish-white. Paraffin wax was first created in 1830 in Germany, and marked a major advancement in candlemaking technology, as it burned more cleanly and reliably than tallow candles and was cheaper to produce.\n\nIn chemistry, \"paraffin\" is used synonymously with \"alkane\", indicating hydrocarbons with the general formula CH. The name is derived from Latin \"parum\" (\"barely\") + \"affinis\", meaning \"lacking affinity\" or \"lacking reactivity\", referring to paraffin's unreactive nature.\n\nParaffin wax is mostly found as a white, odorless, tasteless, waxy solid, with a typical melting point between about , and a density of around 900 kg/m. It is insoluble in water, but soluble in ether, benzene, and certain esters. Paraffin is unaffected by most common chemical reagents but burns readily. Its heat of combustion is 42 MJ/kg.\n\nParaffin wax is an excellent electrical insulator, with a resistivity of between 10 and 10 ohm metre. This is better than nearly all other materials except some plastics (notably Teflon). It is an effective neutron moderator and was used in James Chadwick's 1932 .\n\nParaffin wax is an excellent material for storing heat, with a specific heat capacity of 2.14–2.9 J g K (joules per gram kelvin) and a heat of fusion of 200–220 J g. Paraffin wax phase-change cooling coupled with retractable radiators was used to cool the electronics of the Lunar Roving Vehicle during the manned missions to the Moon in the early 1970s. Wax expands considerably when it melts and this allows its use in wax element thermostats for industrial, domestic and, particularly, automobile purposes.\n\nParaffin wax was first created in 1830 by the German chemist Karl von Reichenbach when he tried to develop the means to efficiently separate and refine the waxy substances naturally occurring in petroleum. Paraffin represented a major advance in the candlemaking industry, because it burned more cleanly and reliably, and was cheaper to manufacture than any other candle fuel. Paraffin wax initially suffered from a low melting point; however, this shortcoming was later remedied by the addition of harder stearic acid. The production of paraffin wax enjoyed a boom in the early 20th century as a result of the growth of the meatpacking and oil industries, which created paraffin and stearic acid as byproducts.\n\nThe feedstock for paraffin is slack wax, which is a mixture of oil and wax, a byproduct from the refining of lubricating oil.\n\nThe first step in making paraffin wax is to remove the oil (de-oiling or de-waxing) from the slack wax. The oil is separated by crystallization. Most commonly, the slack wax is heated, mixed with one or more solvents such as a ketone and then cooled. As it cools, wax crystallizes out of the solution, leaving only oil. This mixture is filtered into two streams: solid (wax plus some solvent) and liquid (oil and solvent). After the solvent is recovered by distillation, the resulting products are called \"product wax\" (or \"press wax\") and \"foots oil\". The lower the percentage of oil in the wax, the more refined it is considered (semi-refined versus fully refined). The product wax may be further processed to remove colors and odors. The wax may finally be blended together to give certain desired properties such as melt point and penetration. Paraffin wax is sold in either liquid or solid form.\n\nIn industrial applications, it is often useful to modify the crystal properties of the paraffin wax, typically by adding branching to the existing carbon backbone chain. The modification is usually done with additives, such as EVA copolymers, microcrystalline wax, or forms of polyethylene. The branched properties result in a modified paraffin with a higher viscosity, smaller crystalline structure, and modified functional properties. Pure paraffin wax is rarely used for carving original models for casting metal and other materials in the lost wax process, as it is relatively brittle at room temperature and presents the risks of chipping and breakage when worked. Soft and pliable waxes, like beeswax, may be preferred for such sculpture, but \"investment casting waxes,\" often paraffin-based, are expressly formulated for the purpose.\n\nIn a pathology laboratory, paraffin wax is used to impregnate tissue prior to sectioning thin samples of tissue. Water is removed from the tissue through ascending strengths of alcohol (75% to absolute) and the tissue is cleared in an organic solvent such as xylene. The tissue is then placed in paraffin wax for a number of hours and then set in a mold with wax to cool and solidify; sections are then cut on a microtome.\n\n\nPeople can be exposed to paraffin in the workplace by breathing it in, skin contact, and eye contact. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) for paraffin wax fume exposure of 2 mg/m over an 8-hour workday.\n\n"}
{"id": "4990237", "url": "https://en.wikipedia.org/wiki?curid=4990237", "title": "Peshawar Electric Power Company", "text": "Peshawar Electric Power Company\n\nWater and Power Development Authority (WAPDA) was created in 1958 through WAPDA Act, 1958. Prior to this, the electricity supply service in Pakistan was undertaken by different agencies, both in public and private sectors, in different areas.The local areas electricity distribution service was being performed by various Regions of WAPDA. Then the Area Electricity Board (AEB) Peshawar, on the eight AEBs in Pakistan, was established under the scheme of Area Electricity Boards in 1982, in order to provide more autonomy and representation to provincial government, elected representatives, industrialists, agriculturalists, and other interest groups in functions of the AEBs. Peshawar Area Electricity Board was reorganized into one such corporatized entity under the name of Peshawar Electric Supply Company (PESCO) with effect from 22-03-1998, with the aim of commercialization and eventually privatization.\n\n"}
{"id": "32539212", "url": "https://en.wikipedia.org/wiki?curid=32539212", "title": "Photovoltaic mounting system", "text": "Photovoltaic mounting system\n\nPhotovoltaic mounting systems (also called solar module racking) are used to fix solar panels on surfaces like roofs, building facades, or the ground. These mounting systems generally enable retrofitting of solar panels on roofs or as part of the structure of the building (called BIPV).\n\nThe solar array of a PV system can be mounted on rooftops, generally with a few inches gap and parallel to the surface of the roof. If the rooftop is horizontal, the array is mounted with each panel aligned at an angle. If the panels are planned to be mounted before the construction of the roof, the roof can be designed accordingly by installing support brackets for the panels before the materials for the roof are installed. The installation of the solar panels can be undertaken by the crew responsible for installing the roof. If the roof is already constructed, it is relatively easy to retrofit panels directly on top of existing roofing structures. For a small minority of roofs (often not built to code) that are designed so that it is capable of bearing only the weight of the roof, installing solar panels demands that the roof structure must be strengthened before-hand. In all cases of retrofits particular consideration to weather sealing is necessary There are many low-weight designs for PV systems that can be used on either sloped or flat roofs (e.g. plastic wedges or the PV-pod), most however, rely on a type of extruded aluminum rails (e.g. Unirac). Recently, tension-based PV racking solutions have been tested successfully that reduce weight and cost. In some cases, converting to composition shingles, the weight of the removed roof materials can compensate the additional weight of the panels structure. The general practice for installation of roof-mounted solar panels include having a support bracket per hundred watts of panels.\n\nGround-mounted PV systems are usually large, utility-scale photovoltaic power stations. The PV array consist of solar modules held in place by racks or frames that are attached to ground-based mounting supports.\n\nGround-based mounting supports include:\n\nSolar panels can also be mounted as shade structures where the solar panels can provide shade instead of patio covers. The cost of such shading systems are generally different from standard patio covers, especially in cases where the entire shade required is provided by the panels. The support structure for the shading systems can be normal systems as the weight of a standard PV array is between 3 and 5 pounds/ft. If the panels are mounted at an angle steeper than normal patio covers, the support structures may require additional strengthening. Other issues that are considered include:\n\n\nBuilding-integrated photovoltaics (BIPV) are photovoltaic materials that are used to replace conventional building materials in parts of the building envelope such as the roof (tiles), skylights, or facades. They are increasingly being incorporated into the construction of new buildings as a principal or ancillary source of electrical power, although existing buildings may be retrofitted with BIPV modules as well. The advantage of integrated photovoltaics over more common non-integrated systems is that the initial cost can be offset by reducing the amount spent on building materials and labor that would normally be used to construct the part of the building that the BIPV modules replace.\n\nBuilding-adapted photovoltaics (BAPV) uses solar modules to create solar PV windows and this way, also to retrofit existing building.\n\nA solar cell performs the best when its surface is perpendicular to the sun's rays, which change continuously over the course of the day and season. It is a common practice to tilt a fixed PV module (without solar tracker) at the same angle as the latitude of array's location to maximize the annual energy yield of module. For example, rooftop PV module at the tropics provides highest annual energy yield when inclination of panel surface is close to horizontal direction. A study in the tropics showed that the orientation of low-slope rooftop PV has negligible impact on annual energy yield, but in the case of PV external sunshade applications, east façade and panel slope of 30–40° are the most suitable location and inclination.\n\n"}
{"id": "22285341", "url": "https://en.wikipedia.org/wiki?curid=22285341", "title": "Rickshaws in the United States", "text": "Rickshaws in the United States\n\nRickshaws are used in numerous cities in the United States, primarily for their novelty value as an entertaining form of transportation for tourists and locals. However, they also have environmental benefits and may be quicker than other forms of transport if traffic congestion is high. Various laws regulate their use in different areas.\n\nThe first known commercial use of pedicabs in North America occurred in 1962 at the Seattle World's Fair. San Diego and New York City each host hundreds of pedicabs; dozens of other United States cities also have pedicab services.\n\nIn New York, human powered transport is available as an environmentally friendly means of transit. Local residents in New York City, however, view pedicabs primarily as tourist vehicles due to their high fares and their drivers' aggressive sales pitches to pedestrians. At a rate of $5 plus $1 per block \"per person,\" a 20-block (one mile) pedicab ride for two people will cost $50. In a taxicab, the same ride would cost under $10. According to Peter Meitzler of New York's Manhattan Rickshaw Company, a passenger has an entirely different urban experience when one rides in a rickshaw. He says that he uses the word \"rickshaw\" in his company name because it is internationally known.\n\n\n\n"}
{"id": "19437550", "url": "https://en.wikipedia.org/wiki?curid=19437550", "title": "Roșiești Wind Farm", "text": "Roșiești Wind Farm\n\nThe Roşieşti Wind Farm is a proposed wind power project in Vaslui County, Romania. It will have 46 individual wind turbines with a nominal output of around 1 MW which will deliver up to 46 MW of power, enough to power over 18,216 homes, with a capital investment required of approximately US$104 million.\n"}
{"id": "33328003", "url": "https://en.wikipedia.org/wiki?curid=33328003", "title": "Shock and Awe: The Story of Electricity", "text": "Shock and Awe: The Story of Electricity\n\nShock and Awe: The Story of Electricity is a British television series outlining aspects of the history of electricity. The series was a co-production between the Open University and the BBC and aired from 6 to 20 October 2011 on BBC Four. The programs were presented by Jim Al-Khalili.\n\n\nIn the first episode Al-Khalili introduces the history of our understanding of electricity and the harnessing of its power. He covers the achievements of these \"natural philosophers\" - Francis Hauksbee, Stephen Gray, Musschenbroek, Benjamin Franklin, Henry Cavendish, Galvani, Volta and Humphry Davy.\n\nThe programme starts with Hauksbee's invention of a static-electricity generator and its subsequent demonstration to the high-minded. It covers Franklin and the resulting experiments to capture and tame lightning. The narrative continues with Cavendish's investigations of the electric shock received from the torpedo fish. Al-Khalili expands on the development of the electric battery following Volta's discovery that simultaneously licking a copper coin and a silver spoon would generate a tingle of electricity. The programme finishes with the first breakthrough in finding a commercial use for electricity: Humphry Davy demonstrating the first carbon-arc light before members of the Royal Institution.\n\nIn the second episode Al-Khalili covers the scientists who discovered the links between electricity and magnetism leading to a way to generate electric power- Hans Christian Oersted, Michael Faraday, William Sturgeon and Joseph Henry.\n\nThe development of commercial applications started with Samuel Morse and Al-Khalili then tells the story of the 1866 transatlantic cable. He revisits the War of Currents rivalry between Direct current and Alternating current.\n\nIn the final episode Al-Khalili brings the story up to date covering the achievements of James Clerk Maxwell; Heinrich Hertz; Oliver Lodge; Jagadish Bose; William Crookes; Mataré & Welker; and William Shockley.\n\n"}
{"id": "153862", "url": "https://en.wikipedia.org/wiki?curid=153862", "title": "Sled", "text": "Sled\n\nA sled, sledge, or sleigh is a land vehicle with smooth underside or possessing a separate body supported by two or more smooth, relatively narrow, longitudinal runners that travels by sliding across a surface. Most commonly sleds are used on snow or ice, but in certain cases they may be used on any surfaces, especially on ones with relatively low friction, such as sand or wet grass. Snow sleds are usally used in ice because it reduces thew amount of friction which helps to carry heavy loads. They may be used to transport passengers, cargo, or both. Shades of meaning differentiating the three terms often reflect regional variations depending on historical uses and prevailing climate.\n\nIn Britain \"sledge\" is the general term, and more common than \"sled\". \"Toboggan\" is sometimes used synonymously with \"sledge\" but more often to refer to a particular type of sledge without runners. \"Sleigh\" refers to a moderate to large-sized, usually open-topped vehicle to carry passengers or goods, and typically drawn by horses, dogs, or reindeer.\nIn American usage \"sled\" remains the general term but often implies a smaller device, often for recreational use. \"Sledge\" implies a heavier sled used for moving freight or massive objects (\"syn.\" \"stone boat\"). \"Sleigh\" refers more specifically than in Britain to a vehicle which is essentially a cold-season alternative to a carriage or wagon and has seating for passengers; what can be called a dog-sleigh in Britain is known only as a dog-sled in North America.\n\nIn Australia, where there is limited snow, \"sleigh\" and \"sledge\" are given equal preference in local parlance.\n\nThe people of Ancient Egypt are thought to have used sledges extensively in the construction of their public works, in particular for the transportation of heavy obelisks.\n\nSleds and sledges were found in the Oseberg \"Viking\" ship excavation. Sledges were useful not only in winter but can be drawn over wet fields, muddy roads, and even hard ground, if one helps them along by greasing the blades with oil or alternatively wetting them with water; in cold weather the water will freeze to ice and they glide along more smoothly with less effort to pull them. The sledge was also highly prized, because – unlike wheeled vehicles – it was exempt from tolls.\n\nUntil the late 19th century, a closed winter sled, or vozok, provided a high-speed means of transport through the snow-covered plains of European Russia and Siberia. It was a means of transport preferred by royals, bishops, and boyars of Muscovy. Several royal vozoks of historical importance have been preserved in the Kremlin Armoury.\n\nMan-hauled sledges were the traditional means of transport on British exploring expeditions to the Arctic and Antarctic regions in the 19th and early 20th centuries. Dog sleds were used by most others, such as Roald Amundsen. Today some people use kites to tow exploration sleds in such climes.\n\nThe word \"sled\" comes from Middle English \"sledde\", which itself has the origins in Middle Dutch word \"slēde\", meaning \"sliding\" or \"slider\". The same word shares common ancestry with both \"sleigh\" and \"sledge\". The word sleigh, on the other hand, is an anglicized form of the modern Dutch word \"slee\" and was introduced to the English language by Dutch immigrants to North America.\n\nThere are several types of widely used recreational sleds designed for sliding down snowy hills (sledding):\n\n\nA few types of sleds are used only for a specific sport:\n\n\nVarious types of sleds are pulled by animals such as reindeer, horses, mules, oxen, or dogs.\n\n\n\n"}
{"id": "297466", "url": "https://en.wikipedia.org/wiki?curid=297466", "title": "Spontaneous symmetry breaking", "text": "Spontaneous symmetry breaking\n\nSpontaneous symmetry breaking is a spontaneous process of symmetry breaking, by which a physical system in a symmetric state ends up in an asymmetric state. In particular, it can describe systems where the equations of motion or the Lagrangian obey symmetries, but the lowest-energy vacuum solutions do not exhibit that same symmetry. When the system goes to one of those vacuum solutions, the symmetry is broken for perturbations around that vacuum even though the entire Lagrangian retains that symmetry.\n\nIn explicit symmetry breaking, if we consider two outcomes, the probability of a pair of outcomes can be different. By definition, spontaneous symmetry breaking requires the existence of a symmetric probability distribution—any pair of outcomes has the same probability. In other words, the underlying laws are invariant under a symmetry transformation.\n\nThe system, as a whole, changes under such transformations.\n\nPhases of matter, such as crystals, magnets, and conventional superconductors, as well as simple phase transitions can be described by spontaneous symmetry breaking. Notable exceptions include topological phases of matter like the fractional quantum Hall effect.\n\nConsider a symmetric upward dome with a trough circling the bottom. If a ball is put at the very peak of the dome, the system is symmetric with respect to a rotation around the center axis. But the ball may \"spontaneously break\" this symmetry by rolling down the dome into the trough, a point of lowest energy. Afterward, the ball has come to a rest at some fixed point on the perimeter. The dome and the ball retain their individual symmetry, but the system does not.\n\nIn the simplest idealized relativistic model, the spontaneously broken symmetry is summarized through an illustrative scalar field theory. The relevant Lagrangian of a scalar field formula_1, which essentially dictates how a system behaves, can be split up into kinetic and potential terms,\n\nIt is in this potential term formula_2 that the symmetry breaking is triggered. An example of a potential, due to Jeffrey Goldstone is illustrated in the graph at the right.\n\nThis potential has an infinite number of possible minima (vacuum states) given by\nfor any real \"θ\" between 0 and 2\"π\". The system also has an unstable vacuum state corresponding to . This state has a U(1) symmetry. However, once the system falls into a specific stable vacuum state (amounting to a choice of \"θ\"), this symmetry will appear to be lost, or \"spontaneously broken\".\n\nIn fact, any other choice of \"θ\" would have exactly the same energy, implying the existence of a massless Nambu–Goldstone boson, the mode running around the circle at the minimum of this potential, and indicating there is some memory of the original symmetry in the Lagrangian.\n\n\nIn particle physics the force carrier particles are normally specified by field equations with gauge symmetry; their equations predict that certain measurements will be the same at any point in the field. For instance, field equations might predict that the mass of two quarks is constant. Solving the equations to find the mass of each quark might give two solutions. In one solution, quark A is heavier than quark B. In the second solution, quark B is heavier than quark A \"by the same amount\". The symmetry of the equations is not reflected by the individual solutions, but it is reflected by the range of solutions.\n\nAn actual measurement reflects only one solution, representing a breakdown in the symmetry of the underlying theory. \"Hidden\" is a better term than \"broken\", because the symmetry is always there in these equations. This phenomenon is called \"spontaneous\" symmetry breaking (SSB) because \"nothing\" (that we know of) breaks the symmetry in the equations.\n\nChiral symmetry breaking is an example of spontaneous symmetry breaking affecting the chiral symmetry of the strong interactions in particle physics. It is a property of quantum chromodynamics, the quantum field theory describing these interactions, and is responsible for the bulk of the mass (over 99%) of the nucleons, and thus of all common matter, as it converts very light bound quarks into 100 times heavier constituents of baryons. The approximate Nambu–Goldstone bosons in this spontaneous symmetry breaking process are the pions, whose mass is an order of magnitude lighter than the mass of the nucleons. It served as the prototype and significant ingredient of the Higgs mechanism underlying the electroweak symmetry breaking.\n\nThe strong, weak, and electromagnetic forces can all be understood as arising from gauge symmetries. The Higgs mechanism, the spontaneous symmetry breaking of gauge symmetries, is an important component in understanding the superconductivity of metals and the origin of particle masses in the standard model of particle physics. One important consequence of the distinction between true symmetries and \"gauge symmetries\", is that the spontaneous breaking of a gauge symmetry does not give rise to characteristic massless Nambu–Goldstone physical modes, but only massive modes, like the plasma mode in a superconductor, or the Higgs mode observed in particle physics.\n\nIn the standard model of particle physics, spontaneous symmetry breaking of the gauge symmetry associated with the electro-weak force generates masses for several particles, and separates the electromagnetic and weak forces. The W and Z bosons are the elementary particles that mediate the weak interaction, while the photon mediates the electromagnetic interaction. At energies much greater than 100 GeV all these particles behave in a similar manner. The Weinberg–Salam theory predicts that, at lower energies, this symmetry is broken so that the photon and the massive W and Z bosons emerge. In addition, fermions develop mass consistently.\n\nWithout spontaneous symmetry breaking, the Standard Model of elementary particle interactions requires the existence of a number of particles. However, some particles (the W and Z bosons) would then be predicted to be massless, when, in reality, they are observed to have mass. To overcome this, spontaneous symmetry breaking is augmented by the Higgs mechanism to give these particles mass. It also suggests the presence of a new particle, the Higgs boson, detected in 2012.\n\nSuperconductivity of metals is a condensed-matter analog of the Higgs phenomena, in which a condensate of Cooper pairs of electrons spontaneously breaks the U(1) gauge symmetry associated with light and electromagnetism.\n\nMost phases of matter can be understood through the lens of spontaneous symmetry breaking. For example, crystals are periodic arrays of atoms that are not invariant under all translations (only under a small subset of translations by a lattice vector). Magnets have north and south poles that are oriented in a specific direction, breaking rotational symmetry. In addition to these examples, there are a whole host of other symmetry-breaking phases of matter including nematic phases of liquid crystals, charge- and spin-density waves, superfluids and many others.\n\nThere are several known examples of matter that cannot be described by spontaneous symmetry breaking, including: topologically ordered phases of matter like fractional quantum Hall liquids, and spin-liquids. These states do not break any symmetry, but are distinct phases of matter. Unlike the case of spontaneous symmetry breaking, there is not a general framework for describing such states.\n\nThe ferromagnet is the canonical system which spontaneously breaks the continuous symmetry of the spins below the Curie temperature and at , where \"h\" is the external magnetic field. Below the Curie temperature the energy of the system is invariant under inversion of the magnetization \"m\"(x) such that . The symmetry is spontaneously broken as when the Hamiltonian becomes invariant under the inversion transformation, but the expectation value is not invariant.\n\nSpontaneously-symmetry-broken phases of matter are characterized by an order parameter that describes the quantity which breaks the symmetry under consideration. For example, in a magnet, the order parameter is the local magnetization.\n\nSpontaneously breaking of a continuous symmetry is inevitably accompanied by gapless (meaning that these modes do not cost any energy to excite) Nambu–Goldstone modes associated with slow long-wavelength fluctuations of the order parameter. For example, vibrational modes in a crystal, known as phonons, are associated with slow density fluctuations of the crystal's atoms. The associated Goldstone mode for magnets are oscillating waves of spin known as spin-waves. For symmetry-breaking states, whose order parameter is not a conserved quantity, Nambu–Goldstone modes are typically massless and propagate at a constant velocity.\n\nAn important theorem, due to Mermin and Wagner, states that, at finite temperature, thermally activated fluctuations of Nambu–Goldstone modes destroy the long-range order, and prevent spontaneous symmetry breaking in one- and two-dimensional systems. Similarly, quantum fluctuations of the order parameter prevent most types of continuous symmetry breaking in one-dimensional systems even at zero temperature (an important exception is ferromagnets, whose order parameter, magnetization, is an exactly conserved quantity and does not have any quantum fluctuations).\n\nOther long-range interacting systems such as cylindrical curved surfaces interacting via the Coulomb potential or Yukawa potential has been shown to break translational and rotational symmetries. It was shown, in the presence of a symmetric Hamiltonian, and in the limit of infinite volume, the system spontaneously adopts a chiral configuration, i.e. breaks mirror plane symmetry.\n\nDynamical symmetry breaking (DSB) is a special form of spontaneous symmetry breaking where the ground state of the system has reduced symmetry properties compared to its theoretical description (Lagrangian).\n\nDynamical breaking of a global symmetry is a spontaneous symmetry breaking, that happens not at the (classical) tree level (i.e. at the level of the bare action), but due to quantum corrections (i.e. at the level of the effective action).\n\nDynamical breaking of a gauge symmetry is subtler. In the conventional spontaneous gauge symmetry breaking, there exists an unstable Higgs particle in the theory, which drives the vacuum to a symmetry-broken phase (see e.g. Electroweak interaction). In dynamical gauge symmetry breaking, however, no unstable Higgs particle operates in the theory, but the bound states of the system itself provide the unstable fields that render the phase transition. For example, Bardeen, Hill, and Lindner published a paper which attempts to replace the conventional Higgs mechanism in the standard model, by a DSB that is driven by a bound state of top-antitop quarks (such models, where a composite particle plays the role of the Higgs boson, are often referred to as \"Composite Higgs models\"). Dynamical breaking of gauge symmetries is often due to creation of a fermionic condensate; for example the quark condensate, which is connected to the dynamical breaking of chiral symmetry in quantum chromodynamics. Conventional superconductivity is the paradigmatic example from the condensed matter side, where phonon-mediated attractions lead electrons to become bound in pairs and then condense, thereby breaking the electromagnetic gauge symmetry.\n\nFor spontaneous symmetry breaking to occur, there must be a system in which there are several equally likely outcomes. The system as a whole is therefore symmetric with respect to these outcomes. However, if the system is sampled (i.e. if the system is actually used or interacted with in any way), a specific outcome must occur. Though the system as a whole is symmetric, it is never encountered with this symmetry, but only in one specific asymmetric state. Hence, the symmetry is said to be spontaneously broken in that theory. Nevertheless, the fact that each outcome is equally likely is a reflection of the underlying symmetry, which is thus often dubbed \"hidden symmetry\", and has crucial formal consequences. (See the article on the Goldstone boson.)\n\nWhen a theory is symmetric with respect to a symmetry group, but requires that one element of the group be distinct, then spontaneous symmetry breaking has occurred. The theory must not dictate \"which\" member is distinct, only that \"one is\". From this point on, the theory can be treated as if this element actually is distinct, with the proviso that any results found in this way must be resymmetrized, by taking the average of each of the elements of the group being the distinct one.\n\nThe crucial concept in physics theories is the order parameter. If there is a field (often a background field) which acquires an expectation value (not necessarily a \"vacuum\" expectation value) which is not invariant under the symmetry in question, we say that the system is in the ordered phase, and the symmetry is spontaneously broken. This is because other subsystems interact with the order parameter, which specifies a \"frame of reference\" to be measured against. In that case, the vacuum state does not obey the initial symmetry (which would keep it invariant, in the linearly realized Wigner mode in which it would be a singlet), and, instead changes under the (hidden) symmetry, now implemented in the (nonlinear) Nambu–Goldstone mode. Normally, in the absence of the Higgs mechanism, massless Goldstone bosons arise.\n\nThe symmetry group can be discrete, such as the space group of a crystal, or continuous (e.g., a Lie group), such as the rotational symmetry of space. However, if the system contains only a single spatial dimension, then only discrete symmetries may be broken in a vacuum state of the full quantum theory, although a classical solution may break a continuous symmetry.\n\nOn October 7, 2008, the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking. Yoichiro Nambu, of the University of Chicago, won half of the prize for the discovery of the mechanism of spontaneous broken symmetry in the context of the strong interactions, specifically chiral symmetry breaking. Physicists Makoto Kobayashi and Toshihide Maskawa, of Kyoto University, shared the other half of the prize for discovering the origin of the explicit breaking of CP symmetry in the weak interactions. This origin is ultimately reliant on the Higgs mechanism, but, so far understood as a \"just so\" feature of Higgs couplings, not a spontaneously broken symmetry phenomenon.\n\n\n"}
{"id": "37733720", "url": "https://en.wikipedia.org/wiki?curid=37733720", "title": "Stele of Meli-Šipak", "text": "Stele of Meli-Šipak\n\nThe Stele of Meli-Šipak is an ancient Mesopotamian fragment of the bottom part of a large rectangular stone edifice engraved with reliefs and the remains of Akkadian and Elamite inscriptions. It was taken as spoil of war by Elamite king Šutruk-Naḫḫunte I during his invasion of Babylonia which deposed Kassite king Zababa-šuma-iddina. It was one of the objects found at Susa between 1900 and 1904 by the French excavation team under Jacques de Morgan that seems to have formed part of an ancient Museum of trophies, or \"ex-voto\" offerings to the deity Inšušinak, in a courtyard adjacent to the main temple.\n\nThe limestone stele is engraved with towers, crowning battlements and separating a crenelated wall fortification below, where there is an archway in the lower of perhaps three registers. At least one row of divine symbols appears in an upper register. A human figure dressed in an ornate fringed robe and a high crown of feathers, faces a ship. A standing nude figure has been intentionally chiseled away. \n\nThe object was first published by Jean-Vincent Scheil in 1902 and is currently kept in the Musée du Louvre in Paris, with excavation reference Sb 14 and Museum reference AS 6049. It is the colophon added by Šutruk-Naḫḫunte (one of several, but which is very similar to that inscribed on the Victory stele of Narām-Sîn), written perpendicular to the Middle Babylonian cuneiform and up one of the carved towers, which identifies it as an artifact of Meli-Šipak, as the remnants of the original inscription do not provide any historical information:\n\nThe royal name preserved is given by Brinkman as ˹\"Me-\"x˺ [ ] which he described as providing “little support for such an interpretation” while Slanski observed that the faint traces following ˹\"Me\"˺ conform rather well to the sign \"–li-\". Little more of the text other than the tail end of the curse formula is preserved:\n\nThis is reminiscent of the curses which appear on column seven of the Land grant to Marduk-apal-iddina I kudurru, and provides the main reason for identifying this object within the genre of kudurrus, the Babylonian entitlement \"narûs\".\n"}
{"id": "772301", "url": "https://en.wikipedia.org/wiki?curid=772301", "title": "Super-iron battery", "text": "Super-iron battery\n\nThe Super-iron battery is a moniker for a proposed class of rechargeable electric battery. Such batteries feature cathodes composed of ferrate salts, either potassium ferrate () or barium ferrate (). One attraction to the proposed device is that the spent cathode would consist of a rust-like material, which is preferable to batteries based on cadmium, manganese and nickel.\n\n\n"}
{"id": "2524811", "url": "https://en.wikipedia.org/wiki?curid=2524811", "title": "TANS Perú Flight 204", "text": "TANS Perú Flight 204\n\nTANS Perú Flight 204 refers to a domestic scheduled Lima–Pucallpa–Iquitos passenger service, operated with a Boeing 737-200 Advanced, that crashed on 23 August 2005 on approach to Pucallpa Airport, off the airfield, following an emergency landing attempt because of bad weather, killing 40 of the 98 passengers and crew aboard.\n\nThe aircraft involved was a 1981-built Boeing 737-244 Advanced, registered OB-1809, which had been leased to TANS Perú from the South African lessor company Safair two months prior to the accident. With manufacturer's serial number 22580 and powered with two Pratt & Whitney JT8D-17A engines, the airframe had its maiden flight on 4 August 1981, and was originally delivered to South African Airways. At the time of the accident the aircraft had accumulated 49,865 flight hours and 45,262 cycles, and was years old.\n\nThere was an unusual developing cold front in the vicinity of Pucallpa, minutes before the event took place, with cloud tops estimated to be high. Instead of diverting to another airport, the crew initiated the approach to Pucallpa Airport with torrential rain, hail and strong winds. Some minutes before the scheduled time for landing the aircraft started rocking. Realising that the airport could not be safely reached amid the worsening weather conditions, the pilot attempted an emergency landing. The aircraft was flying through a hailstorm for the last 32 seconds of its ill-fated flight when it was seemingly taken down by a wind shear, hit tree tops, impacted terrain in a swamp located ahead of the runway threshold, broke up as it crash landed and burst into flames, leaving a path of debris and flaming fuel wide and long. The wreckage of the airplane was engulfed by the fire.\n\nThere were 91 passengers and seven crew members on board; passengers and crew lost their lives in the accident. Non-Peruvian occupants of the aircraft included 11 Americans, one Australian, one Colombian, and one Spaniard; Italians were also aboard, but the actual figures for them depend upon the source. Most of the fatalities were recorded for passengers travelling in the front of the aircraft. Fifty-eight people survived the accident, many of them suffering serious injuries, mostly burns and broken limbs.\n\nInvestigation of the crash site was hindered by looters, who descended upon the crash and stole various elements to be sold for scrap. A reward did succeed in securing the return of the flight data recorder. After 312 days of investigations, there were no reports of any technical malfunction. The official cause of the accident was determined to be pilot error for not following standard procedures under adverse weather conditions. The pilot took control of the plane, but the co-pilot did not immediately monitor the instruments; as a result, the crew did not notice the rapid descent in the few crucial seconds they had where they could have avoided danger. According to Aviation Safety Network, the accident ranks among the deadliest ones that took place in 2005. It was also the second major crash involving a TANS Perú airplane in slightly over two years.\n\nFlight 204 has been the subject of a \"Reader's Digest\" story and an MSNBC documentary. The Canadian TV series, \"Mayday\", has also produced an episode about the accident named ″Lack of Vision″.\n\n"}
{"id": "30890448", "url": "https://en.wikipedia.org/wiki?curid=30890448", "title": "Tron (Scotland)", "text": "Tron (Scotland)\n\nA tron was a weighing beam in medieval Scotland, usually located in the marketplaces of burghs. There are various roads and buildings in several Scottish towns that are named after the tron. For example, Trongate in Glasgow and Tron Kirk in Edinburgh. Etymologically the word is derived from the Old French \"tronel\" or \"troneau\", meaning \"balance\".\n\nFrom the 12th century the city fathers of Scottish burghs needed to standardise weights and measurements, partly to collect the correct taxation on goods, and partly to stop unscrupulous merchants shortchanging citizens. Trons were set up in marketplaces throughout Scotland, with each burgh with its own set of, sometimes differing, weights. Some burghs had more than one tron; in Edinburgh a butter tron was located at the head of the West Bow, while a salt tron was located further down the Royal Mile.\n\n"}
{"id": "48995885", "url": "https://en.wikipedia.org/wiki?curid=48995885", "title": "Truss uplift", "text": "Truss uplift\n\nTruss uplift or truss lift is when the wood in wooden trusses shrinks, or cures, and the bottom most piece bows upwards, most notably near the middle. Truss lift is an issue in wood frame construction where non-load bearing drywall walls meet drywall ceilings, as the truss lifts it pulls the ceiling drywall up. This in turn pulls the angle tape out of the corner and can damage the spray texture. One solution is to not fasten the ceiling drywall within two feet of a non load bearing wall, and rather use the drywall on the wall as support. This technique will allow the ceiling board to bow down and stay tight to the angle over time as the truss lifts.\n"}
{"id": "746462", "url": "https://en.wikipedia.org/wiki?curid=746462", "title": "Tungsten carbide", "text": "Tungsten carbide\n\nTungsten carbide (chemical formula: WC) is a chemical compound (specifically, a carbide) containing equal parts of tungsten and carbon atoms. In its most basic form, tungsten carbide is a fine gray powder, but it can be pressed and formed into shapes through a process called sintering for use in industrial machinery, cutting tools, abrasives, armor-piercing rounds, other tools and instruments, and jewelry.\n\nTungsten carbide is approximately twice as stiff as steel, with a Young's modulus of approximately 530–700 GPa (77,000 to 102,000 ksi), and is double the density of steel—nearly midway between that of lead and gold. It is comparable with corundum (α-) in hardness and can only be polished and finished with abrasives of superior hardness such as cubic boron nitride and diamond powder, wheels, and compounds.\n\nHistorically referred to as Wolfram, \"Wolf Rahm\", wolframite ore discovered by Peter Woulfe was then later carburized and cemented with a binder creating a composite now called \"cemented tungsten carbide\". Tungsten is Swedish for \"heavy stone\".\n\nColloquially among workers in various industries (such as machining and carpentry), tungsten carbide is often simply called \"carbide\", despite the imprecision of the usage. Among the lay public, the growing popularity of tungsten carbide rings has also led to consumers calling the material \"tungsten\".\n\nTungsten carbide is prepared by reaction of tungsten metal and carbon at 1400–2000 °C. Other methods include a patented lower temperature fluid bed process that reacts either tungsten metal or blue with CO/ mixture and between 900 and 1200 °C.\n\nWC can also be produced by heating with graphite: directly at 900 °C or in hydrogen at 670 °C following by carburization in argon at 1000 °C. Chemical vapor deposition methods that have been investigated include:\n\nThere are two well-characterized compounds of tungsten and carbon, WC and tungsten semicarbide, . Both compounds may be present in coatings and the proportions can depend on the coating method.\n\nAt high temperatures WC decomposes to tungsten and carbon and this can occur during high-temperature thermal spray, e.g., in high velocity oxygen fuel (HVOF) and high energy plasma (HEP) methods.\n\nOxidation of WC starts at . It is resistant to acids and is only attacked by hydrofluoric acid/nitric acid (HF/) mixtures above room temperature. It reacts with fluorine gas at room temperature and chlorine above and is unreactive to dry up to its melting point. Finely powdered WC oxidizes readily in hydrogen peroxide aqueous solutions. At high temperatures and pressures it reacts with aqueous sodium carbonate forming sodium tungstate, a procedure used for recovery of scrap cemented carbide.\n\nTungsten carbide has a high melting point at , a boiling point of when under a pressure equivalent to , a thermal conductivity of 110 W·m·K, and a coefficient of thermal expansion of 5.5 \"µ\"m·m·K.\n\nTungsten carbide is extremely hard, ranking about 9 on Mohs scale, and with a Vickers number of around 2600. It has a Young's modulus of approximately 530–700 GPa, a bulk modulus of 630–655 GPa, and a shear modulus of 274 GPa. It has an ultimate tensile strength of 344 MPa, an ultimate compression strength of about 2.7 GPa and a Poisson's ratio of 0.31.\n\nThe speed of a longitudinal wave (the speed of sound) through a thin rod of tungsten carbide is 6220 m/s.\n\nTungsten carbide's low electrical resistivity of about 0.2 \"µ\"Ω·m is comparable with that of some metals (e.g. vanadium 0.2 \"µ\"Ω·m).\n\nWC is readily wetted by both molten nickel and cobalt. Investigation of the phase diagram of the W-C-Co system shows that WC and Co form a pseudo binary eutectic. The phase diagram also shows that there are so-called η-carbides with composition that can be formed and the brittleness of these phases makes control of the carbon content in WC-Co cemented carbides important.\n\nThere are two forms of WC, a hexagonal form, α-WC (hP2, space group Pm2, No. 187), and a cubic high-temperature form, β-WC, which has the rock salt structure. The hexagonal form can be visualized as made up of a simple hexagonal lattice of metal atoms of layers lying directly over one another (i.e. not close packed), with carbon atoms filling half the interstices giving both tungsten and carbon a regular trigonal prismatic, 6 coordination. From the unit cell dimensions the following bond lengths can be determined: the distance between the tungsten atoms in a hexagonally packed layer is 291 pm, the shortest distance between tungsten atoms in adjoining layers is 284 pm, and the tungsten carbon bond length is 220 pm. The tungsten-carbon bond length is therefore comparable to the single bond in (218 pm) in which there is strongly distorted trigonal prismatic coordination of tungsten.\n\nMolecular WC has been investigated and this gas phase species has a bond length of 171 pm for .\n\nSintered tungsten carbide - cobalt cutting tools are very abrasion resistant and can also withstand higher temperatures than standard high-speed steel (HSS) tools. Carbide cutting surfaces are often used for machining through materials such as carbon steel or stainless steel, and in applications where steel tools would wear quickly, such as high-quantity and high-precision production. Because carbide tools maintain a sharp cutting edge better than steel tools, they generally produce a better finish on parts, and their temperature resistance allows faster machining. The material is usually called cemented carbide, solid carbide, hardmetal or tungsten-carbide cobalt. It is a metal matrix composite, where tungsten carbide particles are the aggregate, and metallic cobalt serves as the matrix.\n\nTungsten carbide is often used in armor-piercing ammunition, especially where depleted uranium is not available or is politically unacceptable. However, it is also common to use powder metallurgic tungsten alloys (in which metallic tungsten powder has been cemented with a metallic binder). projectiles were first used by German Luftwaffe tank-hunter squadrons in World War II. Owing to the limited German reserves of tungsten, material was reserved for making machine tools and small numbers of projectiles. It is an effective penetrator due to its combination of great hardness and very high density.\n\nTungsten carbide in its monolithic sintered form, or much more often in tungsten carbide cobalt composite (in which fine ceramic tungsten carbide particles are embedded in metallic cobalt binder forming a metal matrix composite or MMC) can be of the sabot type. SLAP, or saboted light armour penetrator where a plastic sabot discards at the barrel muzzle is one of the primary types of saboted small arms ammunition. Non-discarding jackets, regardless of the jacket material, are not perceived as sabots but bullets. Both of the designs are, however, common in designated light armor-piercing small arms ammunition.\n\nDiscarding sabots like used with M1A1 Abrams main gun are more commonplace in precision high-velocity gun amunition.\n\nTungsten carbide is used extensively in mining in top hammer rock drill bits, downhole hammers, roller-cutters, long wall plough chisels, long wall shearer picks, raiseboring reamers, and tunnel boring machines. It is generally utilised as a button insert, mounted in a surrounding matrix of steel that forms the substance of the bit. As the tungsten carbide button is worn away the softer steel matrix containing it is also worn away, exposing yet more button insert.\n\nTungsten carbide is also an effective neutron reflector and as such was used during early investigations into nuclear chain reactions, particularly for weapons. A criticality accident occurred at Los Alamos National Laboratory on 21 August 1945 when Harry Daghlian accidentally dropped a tungsten carbide brick onto a plutonium sphere, known as the demon core, causing the subcritical mass to go supercritical with the reflected neutrons.\n\nTrekking poles, used by many hikers for balance and to reduce pressure on leg joints, generally use carbide tips in order to gain traction when placed on hard surfaces (like rock); carbide tips last much longer than other types of tip.\n\nWhile ski pole tips are generally not made of carbide, since they do not need to be especially hard even to break through layers of ice, rollerski tips usually are. Roller skiing emulates cross country skiing and is used by many skiers to train during warm weather months.\n\nSharpened carbide tipped spikes (known as studs) can be inserted into the drive tracks of snowmobiles. These studs enhance traction on icy surfaces. Longer v-shaped segments fit into grooved rods called wear rods under each snowmobile ski. The relatively sharp carbide edges enhance steering on harder icy surfaces. The carbide tips and segments reduce wear encountered when the snowmobile must cross roads and other abrasive surfaces.\n\nCar, motorcycle and bicycle tires with tungsten carbide studs provide better traction on ice. The tungsten carbide insert protruding from inside of a zinc or aluminium seating is commonly called a \"soul\" in the tire manufacturing business. These are generally preferred to steel studs because of their superior resistance to wear.\n\nTungsten carbide may be used in farriery, the shoeing of horses, to improve traction on slippery surfaces such as roads or ice. Carbide-tipped hoof nails may be used to attach the shoes, or alternatively Borium, a trademark for tungsten carbide in a matrix of softer metal, may be welded to small areas of the underside of the shoe before fitting.\n\nTungsten carbide is also used for making surgical instruments meant for open surgery (scissors, forceps, hemostats, blade-handles, etc.) and laparoscopic surgery (graspers, scissors/cutter, needle holder, cautery, etc.). They are much costlier than their stainless-steel counterparts and require delicate handling, but give better performance.\n\nTungsten carbide, typically in the form of a cemented carbide (carbide particles brazed together by metal), has become a popular material in the bridal jewelry industry due to its extreme hardness and high resistance to scratching. Even with high-impact resistance, this extreme hardness also means that it can occasionally be shattered under certain circumstances. Some consider this useful, since an impact would shatter a tungsten ring, quickly removing it, where precious metals would bend flat and require cutting. Tungsten carbide is roughly 10 times harder than 18k gold. In addition to its design and high polish, part of its attraction to consumers is its technical nature. Special tools, such as locking pliers, may be required if such a ring must be removed quickly (e. g. due to medical emergency following a hand injury accompanied by swelling).\n\nTungsten carbide is widely used to make the rotating ball in the tips of ballpoint pens that disperse ink during writing.\n\nTungsten carbide is a common material used in the manufacture of gauge blocks, used as a system for producing precision lengths in dimensional metrology. \n\nEnglish guitarist Martin Simpson is known to use a custom-made tungsten carbide guitar slide. The hardness, weight, and density of the slide give it superior sustain and volume compared to standard glass, steel, ceramic, or brass slides.\n\nTungsten carbide has been investigated for its potential use as a catalyst and it has been found to resemble platinum in its catalysis of the production of water from hydrogen and oxygen at room temperature, the reduction of tungsten trioxide by hydrogen in the presence of water, and the isomerisation of 2,2-dimethylpropane to 2-methylbutane. It has been proposed as a replacement for the iridium catalyst in hydrazine-powered satellite thrusters.\n\nA tungsten carbide coating has been utilized on brake discs in high performance automotive applications to improve performance, increase service intervals and reduce brake dust.\n\nThe primary health risks associated with tungsten carbide relate to inhalation of dust, leading to fibrosis. Cobalt-cemented tungsten carbide is also anticipated to be a human carcinogen by the American National Toxicology Program.\n\n"}
{"id": "14157979", "url": "https://en.wikipedia.org/wiki?curid=14157979", "title": "Uranyl formate", "text": "Uranyl formate\n\nUranyl formate (UO(CHO)·HO) is a fine yellow free-flowing powder occasionally used in transmission electron microscopy.\n\nIt is occasionally used as a 0.5% or 1% aqueous negative stain in transmission electron microscopy (TEM) because it shows a finer grain structure than uranyl acetate. However, uranyl formate does not easily go into solution, and once dissolved, has a rather limited lifetime as a stain. It is quite sensitive to light, especially ultraviolet light, and will precipitate if exposed.\n\n\n"}
