{"id": "16195616", "url": "https://en.wikipedia.org/wiki?curid=16195616", "title": "1918 Tyler tornado", "text": "1918 Tyler tornado\n\nThe 1918 Tyler tornado was a large and destructive tornado that devastated the town of Tyler, Minnesota on Wednesday, August 21, 1918. The F4-estimated tornado hit the town at approximately 9:20 pm, killing 36 people and injuring over 100 others. Debris from Tyler was found up to away. It is the fourth-deadliest tornado in Minnesota's history.\n\n"}
{"id": "14627980", "url": "https://en.wikipedia.org/wiki?curid=14627980", "title": "2007 South Korea oil spill", "text": "2007 South Korea oil spill\n\nThe MT \"Hebei Spirit\" oil spill was a major oil spill in South Korea that began on the morning of 7 December 2007 local time, with ongoing environmental and economic effects. Government officials called it South Korea's worst oil spill ever, surpassing a spill that took place in 1995. This oil spill was about one-third of the size of the \"Exxon Valdez\" oil spill.\n\nAt about 7:30 local time on 7 December 2007 (2230 UTC on 6 December 2007), a crane barge owned by Samsung Heavy Industries being towed by a tug collided with the anchored Hong Kong registered crude carrier \"Hebei Spirit\" (), carrying of crude oil. The incident occurred near the port of Daesan on the Yellow Sea coast of Taean County. The barge was floating free after the cable linking it to the tug snapped in the rough seas.\n\nAlthough no casualties were reported, the collision punctured three of the five tanks aboard the \"Hebei Spirit\" and resulted in the leaking of some of oil.<ref name=\"Al Jazeera - 07/12\"></ref><ref name=\"Al Jazeera - 09/12\"></ref> The remaining oil from the damaged tanks was pumped into the undamaged tanks and the holes were sealed.\n\nThe spill occurred near Mallipo Beach (in Taean County), considered one of South Korea's most beautiful and popular beaches. The region affected by the spill is home to one of Asia's largest wetland areas, used by migratory birds, and also contains a national maritime park and 445 sea farms.\n\nIt was initially believed the oil spill would not spread due to the cold winter temperatures. However, unseasonably warm weather, combined with strong waves and unexpected wind directions, caused the spill to expand beyond initial expectations.<ref name=\"Korean Times-09/12\"></ref>\n\nOn 9 December it was reported that the oil slick was already long and wide and thick in some areas. It was also reported that at least 30 beaches have been affected and over half of the region's sea farms are believed to have lost their stocks due to the spill. Sinduri Dune, a South Korean natural treasure, is reported to have been saturated by the spill.\n\nAlthough most migratory birds had not yet arrived in the region, seagulls, mallard ducks and other sea life were found tarred by the oil.\n\nOn 14 December, the oil balls had arrived at Anmyeon-do (安眠島; Anmyeon Island), resulting in at least five beaches being contaminated with large tar lumps. It was believed that the oil belt was not going to go down to Anmyeon-do, but bad, windy weather was responsible for it. On 15 December, the tar lumps had also floated to Boryeong (mostly around Wonsan Island and Sapsi Island of Boryeong) and to Gunsan in North Jeolla province as well.\n\nThe South Korean government declared a state of disaster in the region. The cost of cleanup has been estimated at 300 billion South Korean won (US$330 million). The cleanup involved 13 helicopters, 17 airplanes and 327 vessels. It has also been estimated the cleanup will take at least two months. Hundreds of thousands of volunteers and celebrities including South Korean actress Park Jin-hee helped to clean up the beaches in the campaign. As of 4 January 2008, the Navy had deployed 229 vessels and some 22,000 military personnel to help clean up the spill, in addition to civilian aid.\n\nOn 10 January 2008, the number of volunteers topped the 1-million mark to reach 1,037,000 people, 33 days after the accident occurred on 7 December, according to the South Chungcheong provincial government. The Taean office for emergency operations reported that the volunteers included 580,000 ordinary civilians, 186,700 local residents, 127,000 soldiers and policemen, and 57,143 public officials. According to the emergency office, an average of 20,000 people volunteered during weekdays and 3,000 volunteered over the weekends.\n\nBy January 2008, approximately 4,153 tons of crude oil spilled had been collected by utilizing some 268,710 kilograms of oil absorbents and other cleanup devices. Financial contributions combined to 27.76 billion South Korean won (about 20 million euro) in donations, as well as food and clothing. The Taean emergency center said more than seven billion won in donations have come from about 4,200 organizations and individuals.\n\nInternationally, the Regional Oil Spill Contingency Plan under the Northwest Pacific Action Plan (NOWPAP) was activated following a request of the South Korean government. Among the emergency supplies available in the other three NOWPAP members (China, Japan and Russia), South Korean government, taking into account logistical issues, accepted kind offers of 50 and 10 tonnes of sorbents from China and Japan respectively. Japan has also dispatched a team of experts in addition to the teams from the Joint UNEP/OCHA (UN Office for the Coordination of Humanitarian Affairs) Environment Unit and the European Commission Monitoring Information Center, U.S. Coast Guard and the Autonomous University of Barcelona (AUB).\n\nIt has been reported that the regional office of the Ministry of Maritime Affairs and Fisheries had twice tried to warn the barge captain that the barge was too close to the tanker two hours before the incident but was unable to do so. The barge captain is also under investigation for moving through the area in rough weather. The tanker is reported to have been at anchor when it was hit by the barge, which had broken free from its towing lines.\n\nThe South Korean Ministry of Maritime Affairs and Fisheries and police admitted to having not provided sufficient amount of oil absorbent material to fishermen and residents as well as not having paid enough attention to the wind direction.\n\nAccording to a recent report, compensation will be mostly paid by China Shipowners Mutual Assurance Association (China P&I) and Skuld P&I, which are insurers for the \"Hebei Spirit\", and some paid by Samsung Fire and Lloyd P&I. International Oil Pollution Funds (IOPC) will be responsible to pay if China P&I and Skuld P&I become unable to pay for the cost or if the damages exceed the shipowners limitation of liability set under an international convention.\n\nOn 20 December, the South Korean coast guard completed an initial investigation. According to their conclusions, blame is shared between the tug captains, the barge captain, and the captain of the Hebei Spirit. The tug captains and the barge captain are charged with negligence and violating the marine pollution prevention law. The captain of the Hebei Spirit has been charged with violating marine law.\n\nOn 24 June, the trial concluded. The two tug captains were found guilty, while the personnel on the barge and on Hebei Spirit were exonerated. Samsung Heavy Industries was also fined.\n\nHowever, the Hebei Spirit's two most senior officers, Master Jasprit Chawla and chief officer Syam Chetan, continue to be detained in South Korea.\nThey were found guilty of criminal negligence and sentenced to serve time in jail for 18 months (Master Jasprit Chawla) and 8 months (C.O. Syam Chetan). South Korea's detention of the crew has generated much controversy and protests from around the world.\nThere have been strong protests from the shipping world and demands for the crews release, including from organizations like the International Transport Workers’ Federation, International Group of P&I Clubs, BIMCO, International Chamber of Shipping / International Shipping Federation, International Association of Dry Cargo Shipowners (INTERCARGO), International Association of Independent Tanker Owners (INTERTANKO) and the Hong Kong Shipowners' Association.\n\nAccording to Lloyd's List and other media reports, South Korean maritime officials, prosecutors and Samsung lawyers have been accused of colluding in the retrial of the two senior officers. Roberto Giorgi, president of management firm V.Ships, visited South Korea to meet with the detained Hebei Spirit crew. He told the press that he is concerned at recent developments \"which point to collusion\" between the South Korean authorities, prosecutors and Samsung Heavy Industries, operators of a drifting barge that collided with the oil tanker and that efforts of Samsung and prosecutors \"look to be designed to ensure that the master and chief officer are found guilty on appeal,\" Giorgi said. \"I am worried that the captain and chief officer may not get a fair trial this time around.\"\n\n\n"}
{"id": "33791420", "url": "https://en.wikipedia.org/wiki?curid=33791420", "title": "Acrobrycon", "text": "Acrobrycon\n\nAcrobrycon is a genus of characin found in tropical South America.\n\nThere are currently 3 recognized species in this genus:\n"}
{"id": "1197569", "url": "https://en.wikipedia.org/wiki?curid=1197569", "title": "Air purifier", "text": "Air purifier\n\nAn air purifier or air cleaner is a device which removes contaminants from the air in a room. These devices are commonly marketed as being beneficial to allergy sufferers and asthmatics, and at reducing or eliminating second-hand tobacco smoke. The commercially graded air purifiers are manufactured as either small stand-alone units or larger units that can be affixed to an air handler unit (AHU) or to an HVAC unit found in the medical, industrial, and commercial industries. Air purifiers may also be used in industry to remove impurities such as CO from air before processing. Pressure swing adsorbers or other adsorption techniques are typically used for this.\n\nIn 1830, a patent was awarded to Charles Anthony Deane for a device comprising a copper helmet with an attached flexible collar and garment. A long leather hose attached to the rear of the helmet was to be used to supply air, the original concept being that it would be pumped using a double bellows. A short pipe allowed breathed air to escape. The garment was to be constructed from leather or airtight cloth, secured by straps.\n\nIn the 1860s, John Stenhouse filed two patents applying the absorbent properties of wood charcoal to air purification (patents 19 July 1860 and 21 May 1867), thereby creating the first practical respirator.\n\nA few years later, John Tyndall invented an improvement to the fireman's respirator, a hood that filtered smoke and noxious gas from air (1871, 1874).\n\nIn the 1950s, HEPA filters were commercialized as highly efficient air filters, after being put to use in the 1940s in the United States' Manhattan Project to control airborne radioactive contaminants.\n\nDust, pollen, pet dander, mold spores, and dust mite feces can act as allergens, triggering allergies in sensitive people. Smoke particles and volatile organic compounds (VOCs) can pose a risk to health. Exposure to various components such as VOCs increases the likelihood of experiencing symptoms of sick building syndrome.\n\nThere are two types of air purifying technologies, Active and Passive. Active air purifier use ionisation for cleaning the air. Passive air purification units on the other hand use air filters to remove pollutants. They are more efficient since all dust and Particulate Matter is permanently removed from the air and collected in the filters.\n\nSeveral different processes of varying effectiveness can be used to purify air.\n\n\n\n\nOther aspects of air cleaners are hazardous gaseous by-products, noise level, frequency of filter replacement, electrical consumption, and visual appeal. Ozone production is typical for air ionizing purifiers. Although high concentration of ozone is dangerous, most air ionizers produce low amounts (< 0.05 ppm). The noise level of a purifier can be obtained through a customer service department and is usually reported in decibels (dB). The noise levels for most purifiers are low compared to many other home appliances. Frequency of filter replacement and electrical consumption are the major operation costs for any purifier. There are many types of filters; some can be cleaned by water, by hand or by vacuum cleaner, while others need to be replaced every few months or years. In the United States, some purifiers are certified as Energy Star and are energy efficient.\n\nHEPA technology is used in portable air purifiers as it removes common airborne allergens. The US Department of Energy has requirements manufacturers must pass to meet HEPA requirements. The HEPA specification requires removal of at least 99.97% of 0.3 micrometers airborne pollutants. Products that claim to be \"HEPA-type\", \"HEPA-like\", or \"99% HEPA\" do not satisfy these requirements and may not have been tested in independent laboratories.\n\nAir purifiers may be rated on a variety of factors, including Clean Air Delivery Rate (which determines how well air has been purified); efficient area coverage; air changes per hour; energy usage; and the cost of the replacement filters. Two other important factors to consider are the length that the filters are expected to last (measured in months or years) and the noise produced (measured in decibels) by the various settings that the purifier runs on. This information is available from most manufacturers.\n\nAs with other health-related appliances, there is controversy surrounding the claims of certain companies, especially involving ionic air purifiers. Many air purifiers generate some ozone, an energetic allotrope of three oxygen atoms, and in the presence of humidity, small amounts of NO. Because of the nature of the ionization process, ionic air purifiers tend to generate the most ozone. This is a serious concern, because ozone is a criteria air pollutant regulated by health-related US federal and state standards. In a controlled experiment, in many cases, ozone concentrations were well in excess of public and/or industrial safety levels established by US Environmental Protection Agency, particularly in poorly ventilated rooms.\n\nOzone can damage the lungs, causing chest pain, coughing, shortness of breath and throat irritation. It can also worsen chronic respiratory diseases such as asthma and compromise the ability of the body to fight respiratory infections—even in healthy people. People who have asthma and allergy are most prone to the adverse effects of high levels of ozone. For example, increasing ozone concentrations to unsafe levels can increase the risk of asthma attacks.\n\nDue to the below average performance and potential health risks, Consumer Reports has advised against using ozone producing air purifiers. IQAir, the educational partner of the American Lung Association, has been a leading industry voice against ozone-producing air cleaning technology.\n\nOzone generators used for shock treatments (unoccupied rooms) which are needed by smoke, mold, and odor remediation contractors as well as crime scene cleanup companies to oxidize and permanently remove smoke, mold, and odor damage are considered a valuable and effective tool when used correctly for commercial and industrial purposes. However, there is a growing body of evidence that these machines can produce undesirable by-products.\n\nIn September 2007, the California Air Resources Board announced a ban of indoor air cleaning devices which produce ozone above a legal limit. This law, which took effect in 2010, requires testing and certification of all types of indoor air cleaning devices to verify that they do not emit excessive ozone.\n\n\n"}
{"id": "51403611", "url": "https://en.wikipedia.org/wiki?curid=51403611", "title": "American Wood Council", "text": "American Wood Council\n\nThe American Wood Council (AWC) is a trade association that represents over 75 percent of North American wood products manufacturers.\n\nNorth American membership includes companies and industry associations; among them, Boise Cascade LLC, Canfor USA/New South, Georgia-Pacific LLC, Interfor Corporation, Kapstone, Louisiana Pacific, Masonite, Norbord Inc., Plum Creek Timber, Potlatch Corp., Sierra Pacific Industries, West Fraser, West Rock Company, Weyerhaeuser Company, and the Canadian Wood Council.\n\nAWC was re-chartered in June 2010, with a broader mandate than a former predecessor namesake. Up until 2010, the wood products industry was represented by the American Forest & Paper Association (AF&PA), which now represents pulp and paper manufacturers.\n\n"}
{"id": "33414275", "url": "https://en.wikipedia.org/wiki?curid=33414275", "title": "Amoco Cadiz oil spill", "text": "Amoco Cadiz oil spill\n\nThe oil tanker \"Amoco Cadiz\" ran aground on Portsall Rocks, from the coast of Brittany, France, on 16 March 1978, and ultimately split in three and sank, all together resulting in the largest oil spill of its kind in history to that date.\n\nNOAA estimates that the total oil spill amounted to 220,880 metric tonnes of oil.\n\nEn route from the Persian Gulf to Rotterdam, Netherlands, via a scheduled stop at Lyme Bay, Great Britain, the ship encountered stormy weather with gale conditions and high seas while in the English Channel. At around 09:45, a heavy wave slammed into the ship's rudder and it was found that she was no longer responding to the helm. This was due to the shearing of Whitworth thread studs in the Hastie four ram steering gear, built under licence in Spain, causing a loss of hydraulic fluid. Attempts to repair the damage and regain control of the ship were made but proved unsuccessful. While the message \"no longer manoeuvrable\" and asking other vessels to stand by was transmitted at 10:20, no call for tug assistance was issued until 11:20.\n\nThe German tug \"Pacific\" responded to \"Amoco Cadiz\" at 11:28, offering assistance under a Lloyd's Open Form (see below). It arrived on the scene at 12:20, but because of the stormy sea, a tow line was not in place until 14:00, and broke off at 16:15. Several attempts were made to establish another tow line and \"Amoco Cadiz\" dropped its anchor trying to halt its drift. A successful tow line was in place at 20:55, but this measure proved incapable of preventing the supertanker from drifting towards the coast because of its huge mass and Force 10 storm winds.\n\nAt 21:04 \"Amoco Cadiz\" ran aground the first time, flooding its engines, and again at 21:39, this time ripping open the hull and starting the oil spill. Her crew was rescued by French Naval Aviation helicopters at midnight, and her captain and one officer remained aboard until 05:00 the next morning.\n\nAt 10:00 on 17 March the vessel broke in two, releasing its entire cargo of of oil, and broke again eleven days later from the buffeting of high stormy seas. The wreckage was later completely destroyed with depth charges by the French Navy.\n\nAn argument arose between the captain of \"Amoco Cadiz\", Pasquale Bardari, and that of the captain of the German tug \"Pacific\", Hartmut Weinert, on the issue of Lloyd's Open Form (LOF). Captain Weinert thought this a classic LOF case, an oil tanker with damage to its steering gear, rough weather and getting closer to the shore by the minute.\n\nAt the time of the accident, the ship and the cargo were valued at about , so Captain Weinert's company could, in the event of success, have received a large award. Captain Bardari of the Cadiz, on the instructions of his owners, wanted \"...towage rate to Lyme Bay.\"\n\nThe argument dragged on from 11:28 when \"Pacific\" first made contact with \"Amoco Cadiz\" until 16:00 when Captain Bardari finally received approval to accept the LOF from the ship's owners in Chicago. However, this dispute did not delay the salvage operation significantly, because tugging preparations had already started. Captain Weinert was aware that if he were to succeed in bringing the tanker into Lyme Bay on the English coast, his owners could arrest the ship in the English High Court in pursuit of a claim for salvage.\n\nIt was incorrectly reported in the press at the time that, after long negotiations on financial terms between the ship's captain and the master of a West German tugboat and two unsuccessful towing attempts, the towline finally broke during the argument and the ship drifted onto the rocks. This version of events became fixed in the public mind although in fact delay was caused by Captain Bardari of \"Amoco Cadiz\" contacting her owners in Chicago for instructions. The delay in sending a distress message meant that the larger tug \"Seefalke\", which might have been in range an hour earlier, was no longer nearby when the distress call was made.\n\n\"Amoco Cadiz\" contained 1,604,500 barrels (219,797 tons) of light crude oil from Ras Tanura, Saudi Arabia and Kharg Island, Iran. Additionally, she had nearly 4,000 tonnes bunker oil. Severe weather resulted in the complete breakup of the ship before any oil could be pumped out of the wreck, resulting in its entire cargo of crude oil (belonging to Shell) and 4,000 tons of fuel oil being spilled into the sea.\n\nA long slick and heavy pools of oil spread onto of the French shoreline by northwesterly winds. Prevailing westerly winds during the following month spread the oil approximately east along the coast. One week after the accident, oil had reached Côtes d'Armor.\n\nOil penetrated the sand on several beaches to a depth of . Sub-surface oil separated into two or three layers due to the extensive sand transfer that occurred on the beaches during rough weather. Piers and slips in the small harbors from Porspoder to Brehat Island were covered with oil. Other affected areas included the pink granite rock beaches of Trégastel and Perros-Guirec, as well as the tourist beaches at Plougasnou. The total extent of oiling one month after the spill included approximately of coastline. Beaches of 76 different Breton communities were oiled.\n\nOil persisted for only a few weeks along the exposed rocky shores that experienced moderate to high wave action. In the areas sheltered from wave action, however, the oil persisted in the form of an asphalt crust for several years.\nThe isolated location of the grounding and rough seas hampered cleanup efforts for the two weeks following the incident.\n\nAs mandated in the \"Polmar Plan\", the French Navy was responsible for all offshore operations while the Civil Safety Service was responsible for shore cleanup activities. Although the total quantity of collected oil and water reached 100,000 tons, less than 20,000 tons of oil were recovered from this liquid after treatment in refining plants.\n\nThe nature of the oil and rough seas contributed to the rapid formation of a \"chocolate mousse\" emulsification of oil and water. This viscous emulsification greatly complicated the cleanup efforts. French authorities decided not to use dispersants in sensitive areas or the coastal fringe where water depth was less than . Had dispersant been applied from the air in the vicinity of the spill source, the formation of mousse might have been prevented.\n\nAt the time, \"Amoco Cadiz \"incident resulted in the largest loss of marine life ever recorded from an oil spill. Mortalities of most animals occurred over the two months following the spill. Two weeks following the accident, millions of dead molluscs, sea urchins, and other bottom dwelling organisms washed ashore.\n\nDiving birds constituted the majority of the nearly 20,000 dead birds that were recovered. The oyster mortality from the spill was estimated at 9,000 tons. Fishermen in the area caught fish with skin ulcerations and tumors.\n\nSome of the fish caught in the area reportedly had a strong taste of petroleum. Although echinoderm and small crustacean populations almost completely disappeared, the populations of many species recovered within a year. Cleanup activities on rocky shores, such as pressure-washing, also caused habitat impacts.\n\nThe \"Amoco Cadiz\" spill was one of the most studied oil spills in history. Many studies remain in progress. This was the largest recorded spill in history and was the first spill in which estuarine tidal rivers were oiled. No follow-up mitigation existed to deal with asphalt formation and problems that resulted after the initial aggressive cleanup.\n\nAdditional erosion of beaches occurred in several places where no attempt was made to restore the gravel that was removed to lower the beach face. Many of the affected marshes, mudflats, and sandy beaches, were low-energy areas. Evidence of oiled beach sediments can still be seen in some of these sheltered areas. Layers of sub-surface oil still remain buried in many of the impacted beaches.\n\nThe ship and spill features in one of Steve Forbert's songs about oil pollution. Speedy J has a song named \"Amoco Cadiz\" on his album \"A Shocking Hobby\". French popstar Alain Barriere had a disco hit in France with a song called \"Amoco\".\n\nIn 1978, it was estimated to have caused US$250 million in damage to fisheries and tourist amenities. The French government presented claims totalling 1.9 billion french francs to United States courts (using the 1978 exchange rate and with interest added this came to at least US$1.6 billion). In 1984, U.S. District Court Judge Frank J. McGarr held that Amoco was liable for damages when he issued his trial verdict, after 3 1/2 years of legal proceedings. Further, the judge ruled that Amoco had put off needed maintenance on the vessel in order to keep it at sea.\n\nIn 1992, US oil giant Amoco has decided not to appeal against the US court order that it must pay US$200 million to the French government. \n\n\n"}
{"id": "37229785", "url": "https://en.wikipedia.org/wiki?curid=37229785", "title": "Ananth Hegde Ashisara", "text": "Ananth Hegde Ashisara\n\nAnanth Hegde Ashisara is an environmentalist from Uttara Kannada district, Karnataka, India and Chairman, Western Ghats Task Force.\n\nAshisara is involved in protection of environment, especially Western Ghats, a World Heritage site and he is Chairman, Western Ghat Task Force. He is also convenor of \"Vriksh Laksha Andolan\", a group of people from Malenadu dedicated to protect environment.\nHe is involved in people's movement against mining activities in and around Ambaragudda hill range of Western Ghats located in Shimoga District, Karnataka.\n\nHe has put efforts to get \"arboreal heritage\" tag to 10 old trees of Karnataka, which include Big Banyan tree, Bangalore (400 years old), \"Adansonia digitata\" - Malvaceae, Bijapur Taluk (600 years old), \"Pilali\" tree \"Ficus micro corpus\", Banavasi, Uttara Kannada etc. He expressed concern over safety of nuclear energy in the wake of damage experienced by nuclear power projects in Japan tsunami (2010).\n\n"}
{"id": "23321008", "url": "https://en.wikipedia.org/wiki?curid=23321008", "title": "Avalanche boulder tongue", "text": "Avalanche boulder tongue\n\nAn avalanche boulder tongue is an accumulation of debris produced by snow avalanches. Well developed avalanche boulder tongue usually develop below avalanche gullies due to successive avalanches over a long time span. The avalanche boulder tongues were first intensively investigated by Anders Rapp in the areas of Abisko and Kebnekaise in Swedish Lappland.\n"}
{"id": "20785764", "url": "https://en.wikipedia.org/wiki?curid=20785764", "title": "Black Sunday (storm)", "text": "Black Sunday (storm)\n\nBlack Sunday refers to a particularly severe dust storm that occurred on April 14, 1935, as part of the Dust Bowl. It was one of the worst dust storms in American history and it caused immense economic and agricultural damage. It is estimated to have displaced 300 million tons of topsoil from the prairie area in the US.\n\nOn the afternoon of April 14, the residents of the Plains States were forced to take cover as a dust storm, or \"black blizzard\", blew through the region. The storm hit the Oklahoma Panhandle and Northwestern Oklahoma first, and moved south for the remainder of the day. It hit Beaver around 4:00 p.m., Boise City around 5:15 p.m., and Amarillo, Texas, at 7:20 p.m. The conditions were the most severe in the Oklahoma and Texas panhandles, but the storm's effects were felt in other surrounding areas.\n\nThe storm was harsh due to the high winds that hit the area that day. The combination of drought, erosion, bare soil, and winds caused the dust to fly freely and at high speeds.\n\nIn North America, the term \"Dust Bowl\" was first used to describe a series of dust storms that hit the prairies of Canada and the United States during the 1930s, and later to describe the area in the United States that was most affected by the storms, including western Kansas, eastern Colorado, northeastern New Mexico, and the Oklahoma and Texas panhandles.\n\nThe \"black blizzards\" started in the Eastern states in 1930, affecting agriculture from Maine to Arkansas. By 1934 they had reached the Great Plains, stretching from North Dakota to Texas, and from the Mississippi River Valley to the Rocky Mountains. \"The Dust Bowl\" (as an area) received its name following the disastrous \"Black Sunday\" storm in April 1935, when reporter Robert L. Geiger referred to the region as \"The Dust Bowl\" in his account of the storm.\n\nCattle farming and sheep ranching had left much of the West devoid of natural grass and shrubs to anchor the soil, and over-farming and poor soil stewardship left the soil dehydrated and lacking in organic matter. During a massive drought that hit the United States in the 1930s, the lack of rainfall, snowfall, and moisture in the air dried out the top soil in most of the country's farming regions.\n\nThe destruction caused by the dust storms, and especially by the storm on Black Sunday, killed multiple people and caused hundreds of thousands of people to relocate. Poor migrants from the American Southwest (known as \"Okies\" - though only about 20 percent were from Oklahoma) flooded California, overtaxing the state's health and employment infrastructure.\n\nIn 1935, after the massive damage caused by these storms, Congress passed the Soil Conservation Act, which established the Soil Conservation Service (SCS) as a permanent agency of the USDA. The SCS was created in an attempt to provide guidance for land owners and land users to reduce soil erosion, improve forest and field land and conserve and develop natural resources.\n\nDuring the 1930s, many residents of the Dust Bowl kept accounts and journals of their lives and of the storms that hit their areas. Collections of accounts of the dust storms during the 1930s have been compiled over the years and are now available in book collections and online.\n\nLawrence Svobida was a wheat farmer in Kansas during the 1930s. He experienced the period of dust storms, and the effect that they had on the surrounding environment and the society. His observations and feelings are available in his memoirs, \"Farming the Dust Bowl\". Here he describes an approaching dust storm:\n\nThe Black Sunday storm is detailed in the 2012 Ken Burns PBS documentary \"The Dust Bowl\".\n\nMusicians and songwriters began to reflect the Dust Bowl and the events of the 1930s in their music. Woody Guthrie, a singer-songwriter from Oklahoma, wrote a variety of songs documenting his experiences living during the era of dust storms. Several were collected in his first album \"Dust Bowl Ballads\". One of them, \"Great Dust Storm\", describes the events of Black Sunday. An excerpt of the lyrics follows:\n\n\"On the 14th day of April of 1935,\n\"There struck the worst of dust storms that ever filled the sky.\n\"You could see that dust storm comin', the cloud looked deathlike black,\n\"And through our mighty nation, it left a dreadful track.\n\"From Oklahoma City to the Arizona line,\n\"Dakota and Nebraska to the lazy Rio Grande,\n\"It fell across our city like a curtain of black rolled down,\n\"We thought it was our judgement, we thought it was our doom.\n\nMusician Kat Eggleston has written a play, \"The Cyclone Line\", about her father Al Eggleston's experiences growing up in 1930s Oklahoma, Black Sunday, and the Dust Bowl in general. Its first public performances were on Vashon (Island), Washington, where he lived most of his life.\n\nAmericana recording artist Grant Maloy Smith released an album in 2017 called \"Dust Bowl – American Stories\" that featured two songs that directly referenced Black Sunday. The song \"Old Black Roller\" is written from a first person perspective during the Black Sunday storm, and another song \"Never Seen The Rain\" has these chorus lyrics: \"We worked the land to death, me and my brother | 'Til April 14, 1935 | Oklahoma, you were like our mother - oh, my\"\n\nAmerican recording artist Gillian Welch refers to the storm and other historical events in a two-part song on her 2001 album \"Time (The Revelator)\": \"April the 14th Part I\" and \"Ruination Day Part II\".\n"}
{"id": "58205543", "url": "https://en.wikipedia.org/wiki?curid=58205543", "title": "Bluecity", "text": "Bluecity\n\nBluecity is a London based company providing electric car sharing services starting from April 26 2017. The cars provided by the service are Bolloré Bluecars by the Bolloré group.\n\nThe service uses the all electric Bolloré Bluecar, which were adapted to suit London's left-hand traffic. It is a three-door hatchback electric car with four seats and has a 30kWh lithium metal polymer (LMP) battery, coupled to a supercapacitor, that provides an electric range of in urban use, and a maximum speed of .\n\n\n\n"}
{"id": "1777992", "url": "https://en.wikipedia.org/wiki?curid=1777992", "title": "Boom Town (film)", "text": "Boom Town (film)\n\nBoom Town is a 1940 American adventure film directed by Jack Conway and starring Clark Gable, Spencer Tracy, Claudette Colbert, and Hedy Lamarr. The supporting cast features Frank Morgan, Lionel Atwill, and Chill Wills. A story written by James Edward Grant in \"Cosmopolitan\" magazine entitled \"A Lady Comes to Burkburnett\" provided the inspiration for the film. The film was produced and released by Metro-Goldwyn-Mayer.\n\n\"Big John\" McMasters (Clark Gable) and \"Square John\" Sand (Spencer Tracy) are two down-on-their-luck oil wildcatters who join forces. Without enough money, they steal drilling equipment from a skeptical Luther Aldrich (Frank Morgan). Their well proves a bust and they have to hastily depart when Aldrich shows up with the sheriff to take back his property. The two oilmen team up and make enough money to partially pay Aldrich. To get him to back them for a second try, they cut him in for a percentage of the well. This time, they strike it rich.\n\nWhen Elizabeth 'Betsy' Bartlett (Claudette Colbert) shows up, McMasters sweeps her off her feet (without knowing that Sand considers her his girl) and marries her. Sand accepts the situation, wanting Betsy to be happy. However, on their first anniversary, she catches her husband dancing with a barroom floozy. As a result, Sand quarrels with McMasters and they flip a coin for the entire oilfield. Betsy leaves too, but returns when she learns that her husband has lost almost everything to Sand and needs her.\n\nEach man goes through booms and busts. Building on his renewed success as a wildcatter, McMasters moves to New York to expand into refineries and distribution, competing against former customer Harry Compton (Lionel Atwill). Seeking inside information about his rivals, he hires away Compton's adviser Karen Vanmeer (Hedy Lamarr), who uses her social contacts and womanly charms to gather industry information.\n\nMeanwhile, Sand loses everything he has built up in South America to a revolution. When he meets McMasters at an oilmen's convention, the two finally reconcile, and Sand goes to work for his old friend. When he suspects that McMasters is carrying on an affair with Karen, he tries to save Betsy's marriage by offering to marry Karen. However, she deduces his motives and declines. When a miserable Betsy tries to commit suicide by taking sleeping pills, Sand decides that the only way to help her is to bankrupt McMasters. Sand loses his costly battle with his former friend and goes broke. It is only when he asks McMasters to give his wife a divorce that the married man finally comes to his senses. Later, McMasters is prosecuted by the government for violating the Sherman Antitrust Act and loses his business. In the end, poor, but happier, Sand and McMasters team up again, with the blissful Betsy looking on. Aldrich supplies them with equipment and the whole cycle begins again.\n\nCast notes:\n\nMGM had been looking for a project set in the oil fields as a vehicle for Clarke Gable for some time. They optioned the short story \"The Lady Comes to Burkburnett\" in November 1938.\n\nThe actress originally considered for the female lead role was Myrna Loy, for whom the part was written. The role went to Claudette Colbert instead, the second, and last, pairing of Colbert and Gable, who had appeared together in \"It Happened One Night\". Gable and Spencer Tracy had also worked together before, in two other films, \"San Francisco\" and \"Test Pilot\". Eventually Tracy insisted on the same top billing clause in his MGM contract that Gable enjoyed, effectively ending the pairing, though Tracy and Gable liked each other personally and enjoyed working together.\n\nThe movie was the first Gable made under a new seven-year contract with MGM. Tracy, in fact, brooded over his second-billing status during the filming of \"Boom Town\" and was reportedly unpleasant to deal with. He especially did not get along with either of the female leads.\n\nSome of the location shooting for the film took place in Bakersfield, California.\n\nBosley Crowther of \"The New York Times\" praised the scenes involving the oil wells as exciting but found the human part of the story \"peters out into repetitious wrangling along monotonous lines.\" \"Variety\"'s review was positive, writing: \"Unlike many large-budgeted productions carrying multistar setups that tend either to costume background or sophistication for limited appeal, this one breaks out with a dashing, rough-and-tumble yarn of modern adventure that carries all elements for widest audience appeal ... story is repetitious in its cutbacks to new oil fields and gushers, but this fact will be considered unimportant by the customers.\" \"Harrison's Reports\" accurately predicted that the film's star power would make it a big hit, but said the story was \"only fairly good\" and the plot \"somewhat thin.\"\n\n\"Film Daily\" called the screenplay \"excellent\" and wrote that Conway \"has furnished an outstanding job of directing, blending the action, love interest and comedy so that interest is held to the end.\" John Mosher wrote a mixed review for \"The New Yorker\", stating that \"when the plot leaves the West and comes East, it grows rather feeble. Western bars, these boom towns and their peculiar architecture and their customs, and the spectacle of the great oil gushers themselves form a substantial background of interest, I should say, which a commonplace plot merely frames.\"\n\n\"Boom Town\" was second only to \"Gone with the Wind\" in generating ticket sales for a Clark Gable picture. According to MGM records the film earned $3,664,000 in the US and Canada and $1,365,000 elsewhere, resulting in a profit of $1,892,000.\n\n\"Boom Town\" was nominated for two Academy Awards, for Best Black and White Cinematography and Best Special Photographic Effects (A. Arnold Gillespie, Douglas Shearer). It also placed seventh on \"Film Daily\"'s year-end nationwide poll of 546 critics naming the 10 Best Films of 1940.\n\n"}
{"id": "28385839", "url": "https://en.wikipedia.org/wiki?curid=28385839", "title": "Brettstapel", "text": "Brettstapel\n\nBrettstapel is a massive timber construction system, fabricated exclusively from softwood timber posts connected with hardwood timber dowels. It is a relatively simple method of construction that exploits low grade timber, not normally suitable for use in construction, to form load bearing solid timber wall, floor and roof panels.\n\nBrettstapel works by using hardwood dowels with a moisture content lower than that of softwood posts. Over time the dowels expand to achieve moisture equilibrium thus 'locking' the posts together and creating a structural load-bearing system.\n\nIt is one of a few construction methods that can be entirely fabricated from timber. Although some variations do still use glue and nails, these are not necessary and avoiding these means a healthier indoor air quality can be achieved. The timber itself locks in vast amounts of carbon dioxide without emitting harmful toxins found in other materials, benefiting both the user and the environment.\n\nBrettstapel was invented in the 1970s by the German engineer Julius Natterer and is now commonly used across Austria, Switzerland and Germany, whilst slowly emerging in the UK. Originally it consisted of low grade timber posts continuously nailed together to form panels strong enough to support mines and railways. In 1999 a German company developed a doweled system which relied on the varying moisture content to form solid structural panels however many companies introduced glue to the system to strengthen it. In 2001, a new variant of Brettstapel was developed where diagonal dowels replaced the perpendicular ones thus resolving separation issues and allowing for longer spans. Presently only one company uses the diagonal dowel system whilst all except for two (who still use nails) use perpendicular dowels, mostly without glue. There are now a range of companies producing CLT cross laminated timber (rotating layers of boards) using timber dowels based upon Brettstapel.\n\nThroughout the development of Brettstapel, low grade wood (predominantly spruce or fir) has remained the choice raw material for the posts where-as beech is more often used for the dowels. By ensuring natural defects, such as knots in the wooden posts are not adjacent to each other, poor quality timber can be utilised which makes for a highly economical way of using a fast-growing, underused resource, of which the UK and Scotland in particular, has an abundance.\n\nBrettstapel is manufactured to three quality grades depending on how visible the final product will be: Industrial, Standard and Exposed. With this in mind, the most rudimentary solution is to use unfinished timber where the panels are to be covered. Alternatively, the posts can be planed, chamfered and profiled to give a highly aesthetic finish and further still, the posts can be altered with hollow voids along the edge to enhance the acoustic performance. Brettstapel can also be combined with concrete and steel to form composite structures for more demanding projects such as large spans, bridges and trusses.\n\nBrettstapel is commonly produced as part of a prefabricated wall, ceiling or roof panel. The prefabrication takes place in dedicated factories using specialist machinery and an experienced team ensuring tight quality control and fast construction.\n\nThe structural panels are generally manufactured in 600mm wide sections, secured together using timber joints and further combined with sheathing board, insulation, and a vapour barrier to form the completed panels. Structural openings are incorporated into the panels, and in some cases plumbing and electrical fittings can be too, whilst the external finish commonly varies between render and timber cladding which is applied on-site.\n\nThis high degree of prefabrication means buildings can be manufactured and erected extremely quickly and efficiently. It takes approximately five weeks from cutting dried timber to completing the fit-out of a house, whilst a house can be erected on site and made weather tight in a matter of days.\n\nStructurally, manufacturers can produce panels up to 12–15 metres in length, although a 7metre span is more readily achievable with a 210-250mm deep panel. Brettstapel trusses can be made to span more demanding distances, such as those required in large industrial units and bridges.\n\nThe tallest building to use Brettstapel panels is currently the “E3” building in Germany at seven storeys tall. Although this incorporates other timber elements, four storey buildings have been erected solely using Brettstapel for the structure. Fire regulations currently limit the height of a timber building, however these are being challenged by the massive timber systems and taller examples are emerging; a typical 120mm panel has a 60 minute rating whilst complete structural panels can achieve 90 minute ratings which more than matches concrete and steel construction methods.\n\nContemporary Brettstapel construction is also employed in the IBA Soft House in Hamburg, Germany. Whereas the timber skeleton of the E3 building is obscured by plaster, the Brettstapel panels in the Soft House are exposed on the interior floors, walls, and ceilings. The building was then clad in layers of mineral wool insulation, a water barrier, and a larch wood rainscreen to ensure stability against the elements.\n\n\nBrettstapel projects are starting to be realised in the UK, Acharacle Primary School in Scotland being the first and setting a fitting precedent for others to follow. Some private and public housing projects are also being built in the Scottish borders and in London. Plummerswood Active House in the Scottish Borders has received the Scottish Home Award for Architectural Excellence 2012.\n\nCurrently Brettstapel is predominantly manufactured in Austria, Switzerland, Germany and Norway which has a negative effect on British users, due to increased transportation costs and a subsequently a higher embodied energy. British architects and clients are however slowly starting to use massive timber systems more frequently, but for all the benefits this brings, the British timber industry has been missing out.\n\nHowever as of Spring 2013 In-Wood Developments in EastSussex is believed to be the first UK manufacturer of Brettstapel Panels in the UK, bringing this construction opportunity to the UK market on a more competitive basis. The first showing of these UK manufactured panels was made at Ecobuild 2013.\n\nScotland produces the majority of UK timber, 65% of which is used for woodchips, fencing, packaging and sawdust, this is timber that could be used in Brettstapel construction. British grown Sitka spruce is an ideal raw material to use for Brettstapel as it is a low coat timber that grows quickly and accounts for approximately third of all timber in the UK. If enough interest is generated, it is possible to conceive that the industry will soon have an incentive to produce more Brettstapel in the UK using Sitka spruce; this would be a highly profitable use of a resource that is currently predominantly used for cheaper products.\n\nIncreasing the use of British timber will have huge ecological and economical benefits to the global environment and to British industries. Ultimately however, specifying Brettstapel within the UK will contribute towards a higher quality of construction; a higher standard of living; an improved environment and towards the increased health and well-being of future occupants.\n\n"}
{"id": "14830046", "url": "https://en.wikipedia.org/wiki?curid=14830046", "title": "British Coal Utilisation Research Association", "text": "British Coal Utilisation Research Association\n\nBritish Coal Utilisation Research Association (BCURA) was founded in 1938, with the first chairman being John G. Bennett. It is a non-profit association of industrial companies, registered as a charity. According to its website \"The aim of BCURA is to promote research and other activities concerned with the production, distribution, and use of coal and its derivatives\". The member companies provide funds for BCURA to make grants to academic institutions to support research projects. The members are the UK power supply companies, coal producers, some large industrial users of coal and equipment manufacturers. It had a relationship with the now closed Coal Research Establishment at Stoke Orchard, Cheltenham, UK.\n\nThe BCURA maintains a collection of 36 UK Coals, ranging from lignite to anthracite, which are used as reference standards in research.\n\nRosalind Franklin worked on porosity of coal during World War II.\n\nVictor Goldschmidt lectured on rare elements in coal ash during World War II.\n\nMarcello Pirani was scientific consultant during 1941—1947, concerned with carbonaceous materials resistant to high temperatures.\n\nThe family of John G. Bennett have a web site that contains information about him and BCURA \n\nPeter H. Given, Head of Organic Chemistry, went on to Pennsylvania State University, achieving distinction in U.S. \n\nBCURA activities were subject of a review published in Nature Volume 153 Number 3873 p 104 (22 January 1944).\n\n"}
{"id": "35090995", "url": "https://en.wikipedia.org/wiki?curid=35090995", "title": "Burgos Wind Farm", "text": "Burgos Wind Farm\n\nBurgos Wind Farm is a wind farm in Burgos, Ilocos Norte, Philippines. It is the second wind farm built in the province of Ilocos Norte and the largest project of its kind in the Philippines. The estimated cost for the construction of the wind farm was US$450 million. The wind farm was commissioned in November 9, 2014 and upon its completion it became the largest wind farm in the country and in Southeast Asia, covering 600 hectares and three barangays of Burgos, namely Saoit, Poblacion and Nagsurot. The project was the first one to be nominated by the Department of Energy as eligible for the department's feed-in tariff scheme.\n\nUnder the Renewable Energy Act of 2008, the Philippine Energy Regulatory Commission can \"(guarantee) fixed rate per kilowatt-hour – the FIT rates – for power producers harnessing renewable energy under the FIT system.\" In February 2015, the ERC agreed to give a FIT rate of P8.53 per kilowatt hour for 20 years to the Burgos Wind Farm of the Energy Development Corporation.\n"}
{"id": "47963445", "url": "https://en.wikipedia.org/wiki?curid=47963445", "title": "Bus duct", "text": "Bus duct\n\nIn electrical power distribution, a bus duct (also called busway), is a sheet metal duct containing either copper or aluminium busbars for the purpose of conducting a substantial current of electricity. It is an alternative means of conducting electricity to power cables or cable bus.\n\nOriginally a busway consisted of bare copper conductors supported on inorganic insulators, such as porcelain, mounted within a non-ventilated steel housing.\n\nBusways were produced due to request of the automotive industry in Detroit in the late 1920s. Since that time, busways improved and became an integrated part of secondary network for industrial plants.\n\nSimilar to cable tray, bus ducts have thicker, cold-formed steel side rails and thinner sheet metal coverings. Busbars inside may be separated with distinct and even gaps between them, or “sandwiched” together.\n\nTypically, individual busbars are wrapped or coated with a non-conducting, covalent material, such as plastic or (in older systems) electrical tape.\n\nAt the connection point, busbars flare out to enable connection to the next segment.\n\nA plug-in bus duct system or busway can have disconnect switches and other devices mounted on it, for example, to distribute power along a long building. Many forms of busway allow plug-in devices such as switches and motor starters to be easily moved; this provides flexibility for changes on an assembly line,for example.\n\nFeeder busway is used to interconnect equipment, such as between a transformer and a switchgear line up. A variant type is low-impedance busway, which is designed to have lower voltage drop by virtue of close spacing of bus bars, which reduces inductive reactance. \nA trolley busway provides power to equipment that must be frequently moved. The busway is open at the bottom, and a movable collector assembly \"trolley\" is used to connect between the fixed bus bars in the busway and the cable connected to moving equipment.\nBus ducts are building service penetrants that are required to be externally firestopped where they penetrate fire separations required to have a fire-resistance rating. Any internal firestops that are supplied by the manufacturer may be tested as integral components to either UL857 or IEEE C37.23 for North American models, and then via ASTM E814, UL1479 or ULC-S115. Bus duct-internal firestops mitigate the transmission of internal fires, smoke and heat between fire compartments through the combustion of bus duct-internal covalent wrappings, spacers and brackets.\n\n\n"}
{"id": "14139384", "url": "https://en.wikipedia.org/wiki?curid=14139384", "title": "Comparison of anaerobic and aerobic digestion", "text": "Comparison of anaerobic and aerobic digestion\n\nThe following article is a comparison of aerobic and anaerobic digestion. In both aerobic and anaerobic systems the growing and reproducing microorganisms within them require a source of elemental oxygen to survive.\n\nIn an anaerobic system there is an absence of gaseous oxygen. In an anaerobic digester, gaseous oxygen is prevented from entering the system through physical containment in sealed tanks. Anaerobes access oxygen from sources other than the surrounding air. The oxygen source for these microorganisms can be the organic material itself or alternatively may be supplied by inorganic oxides from within the input material. When the oxygen source in an anaerobic system is derived from the organic material itself, then the 'intermediate' end products are primarily alcohols, aldehydes, and organic acids plus carbon dioxide. In the presence of specialised methanogens, the intermediates are converted to the 'final' end products of methane, carbon dioxide with trace levels of hydrogen sulfide. In an anaerobic system the majority of the chemical energy contained within the starting material is released by methanogenic bacteria as methane.\n\nIn an aerobic system, such as composting, the microorganisms access free, gaseous oxygen directly from the surrounding atmosphere. The end products of an aerobic process are primarily carbon dioxide and water which are the stable, oxidised forms of carbon and hydrogen. If the biodegradable starting material contains nitrogen, phosphorus and sulfur, then the end products may also include their oxidised forms- nitrate, phosphate and sulfate. In an aerobic system the majority of the energy in the starting material is released as heat by their oxidisation into carbon dioxide and water.\n\nComposting systems typically include organisms such as fungi that are able to break down lignin and celluloses to a greater extent than anaerobic bacteria. Due to this fact it is possible, following anaerobic digestion, to compost the anaerobic digestate allowing further volume reduction and stabilisation.\n"}
{"id": "525887", "url": "https://en.wikipedia.org/wiki?curid=525887", "title": "Density of states", "text": "Density of states\n\nIn solid state physics and condensed matter physics, the density of states (DOS) of a system describes the number of states per an interval of energy at each energy level available to be occupied. It is mathematically represented by a density distribution and it is generally an average over the space and time domains of the various states occupied by the system. A high 'DOS' at a specific energy level means that there are many states available for occupation. A DOS of zero means that no states can be occupied at that energy level. The DOS is usually represented by one of the symbols \"g, ρ, D, n, or N\". \n\nGenerally, the density of the states of matter is continuous. In isolated systems however, like atoms or molecules in the gas phase, the density distribution is discrete like a spectral density. \n\nLocal\nvariations, most often due to distortions of the original system, are often called local density of states (LDOS). If the DOS of an undisturbed\nsystem is zero, the LDOS can locally be non-zero due to the presence of a local potential.\n\nIn quantum mechanical (QM) systems, waves, or wave-like particles, can occupy modes or states with wavelengths and propagation directions dictated by the system. For example, in some systems, the interatomic spacing and the atomic charge of a material could allow only electrons of certain wavelengths to exist. In other systems, the crystalline structure of a material could allow waves to propagate in one direction, while suppressing wave propagation in another direction. Often, only specific states are permitted. Thus, it can happen that many states are available for occupation at a specific energy level, while no states are available at other energy levels . \n\nFor example, the density of states of electrons at the bandedge between the conduction band and the valence band in a semiconductor is shown in orange in Fig. 4 (in the subsequent section \"Density of states and distribution functions\"). For an electron in the conduction band, an increase of the electron energy causes more states to become available for occupation. Alternatively, the density of state is discontinuous for an interval of energy, which means that there are no states available for electrons to occupy within the bandgap of the material. This condition also means that an electron at the conduction band edge must lose at least the bandgap energy of the material in order to transition to another state in the valence band.\n\nDepending on the quantum mechanical system, the density of states can be calculated for electrons, photons, or phonons, and can be given as a function of either energy or the wave vector \"k\". To convert between the DOS as a function of the energy and the DOS as a function of the wave vector, the system-specific energy dispersion relation between \"E\" and \"k\" must be known. \n\nIn general, the topological properties of the system have a major impact on the properties of the density of states. The most well-known systems, like neutronium in neutron stars and free electron gases in metals (examples of degenerate matter and a Fermi gas), have a 3-dimensional Euclidean topology. Less familiar systems, like two-dimensional electron gases (2DEG) in graphite layers and the quantum Hall effect system in MOSFET type devices, have a 2-dimensional Euclidean topology. Even less familiar are carbon nanotubes, the quantum wire and Luttinger liquid with their 1-dimensional topologies. Systems with 1D and 2D topologies are likely to become more common, assuming developments in nanotechnology and materials science proceed.\n\nIn general the density of states, related to volume \"V\" and \"N\" countable energy levels, is defined by:\n\nUsing formula_2 (the smallest allowed change of formula_3 for a particle in a box of dimension formula_4 and length formula_5) under the limit formula_6, one derives the volume-related density of states for continuous energy levels\n\nWith formula_4 of the spatial dimension of the considered system and formula_9 the wave vector.\n\nEquivalently, the density of states can also be understood as the derivative of the microcanonical partition function formula_10 with respect to the energy:\n\nThe number of states with energy formula_12 (degree of degeneracy) is given by:\nwhere the last equality only applies when the mean value theorem for integrals is valid.\n\nThere is a large variety of systems and types of states for which DOS calculations can be done.\n\nSome condensed matter systems possess a symmetry of its structure on its microscopic scale which simplifies calculations of its density of states. In spherically symmetric systems, the integrals of function are one-dimensional because all variables in the calculation depend only on the radial parameter of the dispersion relation. Fluids, glasses or amorphous solids are example of a symmetric system whose dispersion relations has a rotational symmetry. \nMeasurements on powders or polycrystalline samples require evaluation and calculation functions and integrals over the whole domain, most often a Brillouin zone, of the dispersion relations of the system of interest. Sometimes the symmetry of the system is high, which causes the shape of the functions describing the dispersion relations of the system to appear many times over the whole domain of the dispersion relation. In such cases the effort to calculate the DOS can be reduced by a great amount when the calculation is limited to a reduced zone or fundamental domain. The Brillouin zone of the of a face-centered cubic lattice (FCC) in the figure on the right has the 48-fold symmetry of the point group \"O\" with full octahedral symmetry. This configuration means that the integration over the whole domain of the Brillouin zone can be reduced to a 48-th part of the whole Brillouin zone. As a crystal structure periodic table shows, there are many elements with a FCC crystal structure, like diamond, silicon and platinum and their Brillouin zones and dispersion relations have this 48-fold symmetry. Two other familiar crystal structures are the body-centered cubic lattice (BCC) and hexagonal closed packed structures (HCP) with cubic and hexagonal lattices, respectively. The BCC structure has the 24-fold pyritohedral symmetry of the point group \"T\". The HCP structure has the 12-fold prismatic dihedral symmetry of the point group \"D\". A complete list of symmetry properties of a point group can be found in point group character tables.\n\nIn general it is easier to calculate a DOS when the symmetry of the system is higher and the number of topological dimensions of the dispersion relation is lower. The DOS of dispersion relations with rotational symmetry can often be calculated analytically. This result is fortunate, since many materials of practical interest, such as steel and silicon, have high symmetry.\n\nIn anisotropic condensed matter systems such as a single crystal of a compound, the density of states could be different in one crystallographic direction than in another. These causes the anisotropic density of states to be more difficult to visualize, and might require methods such as calculating the DOS for particular points or directions only, or calculating the projected density of states (PDOS) to a particular crystal orientation.\n\nThe density of states is dependent upon the dimensional limits of the object itself. In a system described by three orthogonal parameters (3 Dimension), the units of DOS is EnergyVolume , in a two dimensional system, the units of DOS is EnergyArea , in a one dimensional system, the units of DOS is EnergyLength. It is important to note that the volume being referenced is the volume of \"k\"-space; the space enclosed by the constant energy surface of the system derived through a dispersion relation that relates \"E\" to \"k\". An example of a 3-dimensional \"k\"-space is given in Fig. 1. It can be seen that the dimensionality of the system itself will confine the momentum of particles inside the system.\n\nThe calculation for DOS starts by counting the \"N\" allowed states at a certain \"k\" that are contained within inside the volume of the system. This procedure is done by differentiating the whole k-space volume formula_14 in n-dimensions at an arbitrary \"k\", with respect to \"k\". The volume, area or length in 3, 2 or 1-dimensional spherical \"k\"-spaces are expressed by\n\nfor a n-dimensional \"k\"-space with the topologically determined constants\n\nfor linear, disk and spherical symmetrical shaped functions in 1, 2 and 3-dimensional Euclidean \"k\"-spaces respectively.\n\nAccording to this scheme, the density of wave vector states \"N\" is, through differentiating formula_17 with respect to \"k\", expressed by\n\nformula_19 is the chemical potential (also denoted as E and called the Fermi level when \"T\"=0), formula_20 is the Boltzmann constant, and formula_21 is temperature. Fig. 4 illustrates how the product of the Fermi-Dirac distribution function and the three-dimensional density of states for a semiconductor can give insight to physical properties such as carrier concentration and Energy band gaps.\n\nBose–Einstein statistics: The Bose–Einstein probability distribution function is used to find the probability that a boson occupies a specific quantum state in a system at thermal equilibrium. Bosons are particles which do not obey the Pauli exclusion principle (e.g. phonons and photons). The distribution function can be written as\n\nFrom these two distributions it is possible to calculate properties such as the internal energy formula_23, the number of particles formula_24, specific heat capacity formula_25, and thermal conductivity formula_3. The relationships between these properties and the product of the density of states and the probability distribution, denoting the density of states by formula_27 instead of formula_28, are given by\n\nformula_4 is dimensionality, formula_31 is sound velocity and formula_32 is mean free path.\n\nThe density of states appears in many areas of physics, and helps to explain a number of quantum mechanical phenomena.\n\nCalculating the density of states for small structures shows that the distribution of electrons changes as dimensionality is reduced. For quantum wires, the DOS for certain energies actually becomes higher than the DOS for bulk semiconductors, and for quantum dots the electrons become quantized to certain energies.\n\nThe photon density of states can be manipulated by using periodic structures with length scales on the order of the wavelength of light. Some structures can completely inhibit the propagation of light of certain colors (energies), creating a photonic bandgap: the DOS is zero for those photon energies. Other structures can inhibit the propagation of light only in certain directions to create mirrors, waveguides, and cavities. Such periodic structures are known as photonic crystals. In nanostructured media the concept of local density of states (LDOS) is often more relevant than that of DOS, as the DOS varies considerably from point to point.\n\nInteresting systems are in general complex, for instance compounds, biomolecules, polymers, etc. Because of the complexity of these systems the analytical calculation of the density of states is in most of the cases impossible. Computer simulations offer a set of algorithms to evaluate the density of states with a high accuracy. One of these algorithms is called the Wang and Landau algorithm.\n\nWithin the Wang and Landau scheme any previous knowledge of the density of states is required. One proceeds as follows: the cost function (for example the energy) of the system is discretized. Each time the bin \"i\" is reached one updates\na histogram for the density of states, formula_33, by\n\nwhere \"f\" is called the modification factor. As soon as each bin in the histogram is visited a certain number of times\n(10-15), the modification factor is reduced by some criterion, for instance,\n\nwhere \"n\" denotes the \"n\"-th update step. The simulation finishes when the modification factor is less than a certain threshold, for instance formula_36.\n\nThe Wang and Landau algorithm has some advantages over other common algorithms such as multicanonical simulations and parallel tempering. For example, the density of states is obtained as the main product of the simulation. Additionally, Wang and Landau simulations are completely independent of the temperature. This feature allows to compute the density of states of systems with very rough energy landscape such as proteins.\n\nMathematically the density of states is formulated in terms of a tower of covering maps.\n\n\n"}
{"id": "24519232", "url": "https://en.wikipedia.org/wiki?curid=24519232", "title": "Dynamic combinatorial chemistry", "text": "Dynamic combinatorial chemistry\n\nDynamic combinatorial chemistry (DCC); also known as constitutional dynamic chemistry (CDC) is a method to the generation of new molecules formed by reversible reaction of simple building blocks under thermodynamic control. The library of these reversibly interconverting building blocks is called a dynamic combinatorial library (DCL). All constituents in a DCL are in equilibrium, and their distribution is determined by their thermodynamic stability within the DCL. The interconversion of these building blocks may involve covalent or non-covalent interactions. When a DCL is exposed to an external influence (such as proteins or nucleic acids), the equilibrium shifts and those components that interact with the external influence are stabilised and amplified, allowing more of the active compound to be formed.\n\nBy modern definition, dynamic combinatorial chemistry is generally considered to be a method of facilitating the generation of new chemical species by the reversible linkage of simple building blocks, under thermodynamic control. This principle is known to select the most thermodynamically stable product from an equilibrating mixture of a number of components, a concept commonly utilised in synthetic chemistry to direct the control of reaction selectivity. Although this approach was arguably utilised in the work of Fischer and Werner as early as the 19th century, their respective studies of carbohydrate and coordination chemistry were restricted to rudimentary speculation, requiring the rationale of modern thermodynamics. It was not until supramolecular chemistry revealed early concepts of molecular recognition, complementarity and self-organisation that chemists could begin to employ strategies for the rational design and synthesis of macromolecular targets. The concept of template synthesis was further developed and rationalised through the pioneering work of Busch in the 1960s, which clearly defined the role of a metal ion template in stabilising the desired ‘thermodynamic’ product, allowing for its isolation from the complex equilibrating mixture. Although the work of Busch helped to establish the template method as a powerful synthetic route to stable macrocyclic structures, this approach remained exclusively within the domain of inorganic chemistry until the early 1990s, when Sanders et al. first proposed the concept of dynamic combinatorial chemistry. Their work combined thermodynamic templation in tandem with combinatorial chemistry, to generate an ensemble complex porphyrin and imine macrocycles using a modest selection of simple building blocks.\n\nSanders then developed this early manifestation of dynamic combinatorial chemistry as a strategy for organic synthesis; the first example being the thermodynamically-controlled macrolactonisation of oligocholates to assemble cyclic steroid-derived macrocycles capable of interconversion via component exchange. Early work by Sanders et al. employed transesterification to generate dynamic combinatorial libraries. In retrospect, it was unfortunate that esters were selected for mediating component exchange, as transesterification processes are inherently slow and require vigorous anhydrous conditions. However, their subsequent investigations identified that both the disulfide and hydrazone covalent bonds exhibit effective component exchange processes and so present a reliable means of generating dynamic combinatorial libraries capable of thermodynamic templation. This chemistry now forms the basis of much research in the developing field of dynamic covalent chemistry, and has in recent years emerged as a powerful tool for the discovery of molecular receptors.\n\nOne of the key developments within the field of DCC is the use of proteins (or other biological macromolecules, such as nucleic acids) to influence the evolution and generation of components within a DCL. Protein-directed DCC provides a way to generate, identify and rank novel protein ligands, and therefore have huge potential in the areas of enzyme inhibition and drug discovery.\n\nThe development of protein-directed DCC has not been straightforward because the reversible reactions employed must occur in aqueous solution at biological pH and temperature, and the components of the DCL must be compatible with proteins.\n\nSeveral reversible reactions have been proposed and/or applied in protein-directed DCC. These included boronate ester formation, diselenides-disulfides exchange, disulphide formation, hemithiolacetal formation, hydrazone formation, imine formation and thiol-enone exchange.\n\nFor reversible reactions that do not occur in aqueous buffers, the pre-equilibrated DCC approach can be used. The DCL was initially generated (or pre-equilibrated) in organic solvent, and then diluted into aqueous buffer containing the protein target for selection. Organic based reversible reactions, including Diels-Alder and alkene cross metathesis reactions, have been proposed or applied to protein-directed DCC using this method.\n\nReversible non-covalent reactions, such as metal-ligand coordination, has also been applied in protein-directed DCC. This strategy is useful for the investigation of the optimal ligand stereochemistry to the binding site of the target protein.\n\nEnzyme-catalysed reversible reactions, such as protease-catalysed amide bond formation/hydrolysis reactions and the aldolase-catalysed aldol reactions, have also been applied to protein-directed DCC.\n\nProtein-directed DCC system must be amenable to efficient screening. Several analytical techniques have been applied to the analysis of protein-directed DCL. These include HPLC, mass spectrometry, NMR spectroscopy, and X-ray crystallography.\n\nAlthough most applications of protein-directed DCC to date involved the use of single protein in the DCL, it is possible to identify protein ligands by using multiple proteins simultaneously, as long as a suitable analytical technique is available to detect the protein species that interact with the DCL components. This approach may be used to identify specific inhibitors or broad-spectrum enzyme inhibitors.\n\nDCC is useful in identifying molecules with unusual binding properties, and provides synthetic routes to complex molecules that aren't easily accessible by other means. These include smart materials, foldamers, self-assembling molecules with interlocking architectures and new soft materials. The application of DCC to detect volatile bioactive compounds, \"i.e.\" the amplification and sensing of scent, was proposed in a concept paper. Recently, DCC was also used to study the abiotic origins of life.\n\n\n"}
{"id": "2199389", "url": "https://en.wikipedia.org/wiki?curid=2199389", "title": "Electrathon", "text": "Electrathon\n\nAn electrathon is a competition to go the farthest in one hour powered only by commercial rechargeable batteries weighing no more than (two car batteries). The record is as of February 2011.\n\nIt involves three- or four-wheeled electric vehicles, somewhat similar in overall appearance to a Go-Kart with an aerodynamic body, but powered by an electric motor and batteries. Electrathon class vehicles are principally defined and constrained by length and width (12 feet long and 4 feet wide or 3.6 m × 1.2 m maximum) and by battery weight and chemistry (, sealed lead acid). Discharged for one hour, this amounts to just under one kilowatt (1.3 hp). Driver's weight is ballasted to for fairness, and the vehicles themselves may weigh from . Safety regulations require features such as braking systems, roll bars, and electrical disconnects.\n\nElectrathon racing started in England, spread to Australia, and arrived in the United States in 1990. The basic format is to determine which car can travel the furthest in one hour within the limitations of battery weight and other factors mentioned above. Design teams must compromise speed in order to gain distance. Success requires efficiency of both the machine and driving technique.\n\nThe relatively low cost of the electrathon racing has made the sport a popular activity for high school age students worldwide who learn skills related to design, problem-solving, teamwork, mathematics, physics, and electricity. Prizes are awarded for high school, college and open divisions, and there are separate classes for solar and advanced battery vehicles, but all generally race together under the same rules. \n\nRaces are held on parking lots, road courses, and oval speedways, but size is a major factor, as cornering friction decreases efficiency. At present, the world record for distance travelled in one hour is , set in July 2009 on the oval at the Ford Michigan Proving Ground, by C. Michael Lewis. Using the USDOE conversion factors, this would be the equivalent of . A significant number of electrathons in the USA are build with body-parts from Blue Sky Design LLC, which sells aerodynamic body-parts specialized for electrathons.\n\nIn 2007, the Utah Salt Flats Racing Association invited several Electrathon Competitors after adding an Electrathon class. It used the same vehicle design rules, but instead of a 60 minute endurance event, it's a straight line top speed event on a one mile course. Speed was/is measured using a 30 foot trap at the end of the 1 mile distance. The first record was set by Kirk Swaney, driving a carbon fiber streamliner designed & built by SHIFT Electric Vehicles LLC (1.6 km). The current record is set by Shannon Cloud in 2008, driving a streamliner designed & built by Dave Cloud.\n\nBefore the 2010 season, the Electrathon America battery weight limit was .\n\n\n"}
{"id": "48845057", "url": "https://en.wikipedia.org/wiki?curid=48845057", "title": "Electricity Forward Agreement", "text": "Electricity Forward Agreement\n\nElectricity Forward Agreement (calendar) (short: EFA system) is a calendar used to specify load profiles when trading on the electricity market. It was officially only valid until October 2014, but is still abundantly used among commodity traders.\n\nOne distinguishes between weekdays \"WD\" and weekend \"WE\" since the electricity consumption is clearly lower on Saturdays and Sundays.\nA \"EFA day\" starts on 11pm local time and runs through to 11pm the next (astronomical) day. A EFA-week \"WK\" consists of five \"WD\" and two \"WE\"-days.\nA \"EFA month\" is defined differently to common months: March, June, September and December have five weeks, all other months are considered to have exactly four weeks. The two \"EFA seasons\" are winter (WK 40 – WK 13) and summer (WK 14 – WK 39), each having exactly 26 weeks. Due to the existence of leap years, some EFA-Decembers have six weeks (e.g. 2004 and 2009), a strong difference from the commonly used Gregorian calendar system where February is the leap month.\n\nThe EFA day is composed of six blocks of 4 hours each. For each block baseload products exist (i.e. WD 1/2/3/4/5/6 and WE 1/2/3/4/5/6). Peak load products only exit for \"WD3\", \"WD4\" and WD5\" (also on bank holidays), and consequently off-peak products are only available for the remaining EFA blocks of each week. Blocks 1 and 2 are usually termed \"overnight blocks\", the other blocks are \"day blocks\".\n"}
{"id": "37761115", "url": "https://en.wikipedia.org/wiki?curid=37761115", "title": "Ferndale Refinery", "text": "Ferndale Refinery\n\nThe Ferndale Refinery, owned by Phillips 66, has 101,000 b/d capacity, making it, as of 2015, the 64th largest in the United States, and produces predominantly transportation fuels consumed in local markets. The plant is located in the Cherry Point Industrial Zone, west of Ferndale, WA. Its secondary processing facilities include a fluid catalytic cracker, an alkylation unit, hydotreating units and a naphtha reformer. The plant follows a 10-5-3-2 crack spread, meaning that for 10 barrels of crude feedstock the refinery produces 5 barrels of gasoline, 3 barrels of distillate and 2 barrels of fuel oil.\n\nThe Ferndale Refinery was the first of five currently operating in Washington state, built by General Petroleum Corp in 1954. The original capacity was rated at 35,000 barrels per stream day. General Petroleum was a subsidiary of Socony (Standard Oil Company of New York) and was integrated into Mobil Chemical Co when the company formed in 1960.\n\nBP took control of the refinery in 1988 when its wholly owned subsidiary, Sohio, received the plant from Mobil Oil in exchange for $152.5 million and crude oil inventories.\n\nIn 1993, Tosco Corp, a California-based downstream and marketing corporation, bought the refinery from BP. The deal included BP’s retail stations and marketing assets across Washington and Oregon. BP left the Northwest refining market only five years after entry.\n\nPhillips Petroleum Company purchased Tosco for $7 billion in February 2001, and assumed control of the refinery thereafter. With the deal’s close, Phillips became the second largest refiner in the U.S. and obtained refineries on both coasts. Even after the Tosco purchase, Phillips sought further expansion. Phillips and Conoco Inc announced a merger in November 2001, forming ConocoPhillips, the new controlling entity of the Ferndale Refinery. This new supermajor boasted the nation's largest downstream system (as of 2001). In 2012 ConocoPhillips spun off its downstream assets as a new independent energy company, Phillips 66, which still operates the Ferndale Refinery. ConocoPhillips became the second company to abandon the vertically integrated model, following Marathon Oil Corporation’s decision to spin off its downstream assets in 2011.\n\nThe Ferndale refinery receives a portion of its crude oil from the Amazon River Basin of South America, a concern of many environmentalists. In 2015 it was refining 989 barels per day of oil from the Amazon.\n"}
{"id": "12242", "url": "https://en.wikipedia.org/wiki?curid=12242", "title": "Germanium", "text": "Germanium\n\nGermanium is a chemical element with the symbol \"Ge\" and atomic number 32. It is a lustrous, hard, grayish-white metalloid in the carbon group, chemically similar to its group neighbors tin and silicon. Pure germanium is a semiconductor with an appearance similar to elemental silicon. Like silicon, germanium naturally reacts and forms complexes with oxygen in nature.\n\nBecause it seldom appears in high concentration, germanium was discovered comparatively late in the history of chemistry. Germanium ranks near fiftieth in relative abundance of the elements in the Earth's crust. In 1869, Dmitri Mendeleev predicted its existence and some of its properties from its position on his periodic table, and called the element ekasilicon. Nearly two decades later, in 1886, Clemens Winkler found the new element along with silver and sulfur, in a rare mineral called argyrodite. Although the new element somewhat resembled arsenic and antimony in appearance, the combining ratios in compounds agreed with Mendeleev's predictions for a relative of silicon. Winkler named the element after his country, Germany. Today, germanium is mined primarily from sphalerite (the primary ore of zinc), though germanium is also recovered commercially from silver, lead, and copper ores.\n\nElemental germanium is used as a semiconductor in transistors and various other electronic devices. Historically, the first decade of semiconductor electronics was based entirely on germanium. Today, the amount of germanium produced for semiconductor electronics is one fiftieth the amount of ultra-high purity silicon produced for the same. Presently, the major end uses are fibre-optic systems, infrared optics, solar cell applications, and light-emitting diodes (LEDs). Germanium compounds are also used for polymerization catalysts and have most recently found use in the production of nanowires. This element forms a large number of organometallic compounds, such as tetraethylgermane, useful in organometallic chemistry.\n\nGermanium is not thought to be an essential element for any living organism. Some complex organic germanium compounds are being investigated as possible pharmaceuticals, though none have yet proven successful. Similar to silicon and aluminum, natural germanium compounds tend to be insoluble in water and thus have little oral toxicity. However, synthetic soluble germanium salts are nephrotoxic, and synthetic chemically reactive germanium compounds with halogens and hydrogen are irritants and toxins.\n\nIn his report on \"The Periodic Law of the Chemical Elements\" in 1869, the Russian chemist Dmitri Ivanovich Mendeleev predicted the existence of several unknown chemical elements, including one that would fill a gap in the carbon family in his Periodic Table of the Elements, located between silicon and tin. Because of its position in his Periodic Table, Mendeleev called it \"ekasilicon (Es)\", and he estimated its atomic weight to be about 72.0.\n\nIn mid-1885, at a mine near Freiberg, Saxony, a new mineral was discovered and named \"argyrodite\" because of the high silver content. The chemist Clemens Winkler analyzed this new mineral, which proved to be a combination of silver, sulfur, and a new element. Winkler was able to isolate the new element in 1886 and found it similar to antimony. He initially considered the new element to be eka-antimony, but was soon convinced that it was instead eka-silicon. Before Winkler published his results on the new element, he decided that he would name his element \"neptunium\", since the recent discovery of planet Neptune in 1846 had similarly been preceded by mathematical predictions of its existence. However, the name \"neptunium\" had already been given to another proposed chemical element (though not the element that today bears the name neptunium, which was discovered in 1940). So instead, Winkler named the new element \"germanium\" (from the Latin word, \"Germania\", for Germany) in honor of his homeland. Argyrodite proved empirically to be AgGeS.\n\nBecause this new element showed some similarities with the elements arsenic and antimony, its proper place in the periodic table was under consideration, but its similarities with Dmitri Mendeleev's predicted element \"ekasilicon\" confirmed that place on the periodic table. With further material from 500 kg of ore from the mines in Saxony, Winkler confirmed the chemical properties of the new element in 1887. He also determined an atomic weight of 72.32 by analyzing pure germanium tetrachloride (), while Lecoq de Boisbaudran deduced 72.3 by a comparison of the lines in the spark spectrum of the element.\n\nWinkler was able to prepare several new compounds of germanium, including fluorides, chlorides, sulfides, dioxide, and tetraethylgermane (Ge(CH)), the first organogermane. The physical data from those compounds — which corresponded well with Mendeleev's predictions — made the discovery an important confirmation of Mendeleev's idea of element periodicity. Here is a comparison between the prediction and Winkler's data:\nUntil the late 1930s, germanium was thought to be a poorly conducting metal. Germanium did not become economically significant until after 1945 when its properties as an electronic semiconductor were recognized. During World War II, small amounts of germanium were used in some special electronic devices, mostly diodes. The first major use was the point-contact Schottky diodes for radar pulse detection during the War. The first silicon-germanium alloys were obtained in 1955. Before 1945, only a few hundred kilograms of germanium were produced in smelters each year, but by the end of the 1950s, the annual worldwide production had reached 40 metric tons.\n\nThe development of the germanium transistor in 1948 opened the door to countless applications of solid state electronics. From 1950 through the early 1970s, this area provided an increasing market for germanium, but then high-purity silicon began replacing germanium in transistors, diodes, and rectifiers. For example, the company that became Fairchild Semiconductor was founded in 1957 with the express purpose of producing silicon transistors. Silicon has superior electrical properties, but it requires much greater purity that could not be commercially achieved in the early years of semiconductor electronics.\n\nMeanwhile, the demand for germanium for fiber optic communication networks, infrared night vision systems, and polymerization catalysts increased dramatically. These end uses represented 85% of worldwide germanium consumption in 2000. The US government even designated germanium as a strategic and critical material, calling for a 146 ton (132 t) supply in the national defense stockpile in 1987.\n\nGermanium differs from silicon in that the supply is limited by the availability of exploitable sources, while the supply of silicon is limited only by production capacity since silicon comes from ordinary sand and quartz. While silicon could be bought in 1998 for less than $10 per kg, the price of germanium was almost $800 per kg.\n\nUnder standard conditions, germanium is a brittle, silvery-white, semi-metallic element. This form constitutes an allotrope known as \"α-germanium\", which has a metallic luster and a diamond cubic crystal structure, the same as diamond. At pressures above 120 kbar, it becomes the allotrope \"β-germanium\" with the same structure as β-tin. Like silicon, gallium, bismuth, antimony, and water, germanium is one of the few substances that expands as it solidifies (i.e. freezes) from the molten state.\n\nGermanium is a semiconductor. Zone refining techniques have led to the production of crystalline germanium for semiconductors that has an impurity of only one part in 10,\nmaking it one of the purest materials ever obtained.\nThe first metallic material discovered (in 2005) to become a superconductor in the presence of an extremely strong electromagnetic field was an alloy of germanium, uranium, and rhodium.\n\nPure germanium suffers from the forming of whiskers by spontaneous screw dislocations. If a whisker grows long enough to touch another part of the assembly or a metallic packaging, it can effectively shunt out a p-n junction. This is one of the primary reasons for the failure of old germanium diodes and transistors.\n\nElemental germanium oxidizes slowly to GeO at 250 °C. Germanium is insoluble in dilute acids and alkalis but dissolves slowly in hot concentrated sulfuric and nitric acids and reacts violently with molten alkalis to produce germanates (). Germanium occurs mostly in the oxidation state +4 although many +2 compounds are known. Other oxidation states are rare: +3 is found in compounds such as GeCl, and +3 and +1 are found on the surface of oxides, or negative oxidation states in germanes, such as −4 in . Germanium cluster anions (Zintl ions) such as Ge, Ge, Ge, [(Ge)] have been prepared by the extraction from alloys containing alkali metals and germanium in liquid ammonia in the presence of ethylenediamine or a cryptand. The oxidation states of the element in these ions are not integers—similar to the ozonides O.\n\nTwo oxides of germanium are known: germanium dioxide (, germania) and germanium monoxide, (). The dioxide, GeO can be obtained by roasting germanium disulfide (), and is a white powder that is only slightly soluble in water but reacts with alkalis to form germanates. The monoxide, germanous oxide, can be obtained by the high temperature reaction of GeO with Ge metal. The dioxide (and the related oxides and germanates) exhibits the unusual property of having a high refractive index for visible light, but transparency to infrared light. Bismuth germanate, BiGeO, (BGO) is used as a scintillator.\n\nBinary compounds with other chalcogens are also known, such as the disulfide (), diselenide (), and the monosulfide (GeS), selenide (GeSe), and telluride (GeTe). GeS forms as a white precipitate when hydrogen sulfide is passed through strongly acid solutions containing Ge(IV). The disulfide is appreciably soluble in water and in solutions of caustic alkalis or alkaline sulfides. Nevertheless, it is not soluble in acidic water, which allowed Winkler to discover the element. By heating the disulfide in a current of hydrogen, the monosulfide (GeS) is formed, which sublimes in thin plates of a dark color and metallic luster, and is soluble in solutions of the caustic alkalis. Upon melting with alkaline carbonates and sulfur, germanium compounds form salts known as thiogermanates.\nFour tetrahalides are known. Under normal conditions GeI is a solid, GeF a gas and the others volatile liquids. For example, germanium tetrachloride, GeCl, is obtained as a colorless fuming liquid boiling at 83.1 °C by heating the metal with chlorine. All the tetrahalides are readily hydrolyzed to hydrated germanium dioxide. GeCl is used in the production of organogermanium compounds. All four dihalides are known and in contrast to the tetrahalides are polymeric solids. Additionally GeCl and some higher compounds of formula GeCl are known. The unusual compound GeCl has been prepared that contains the GeCl unit with a neopentane structure.\n\nGermane (GeH) is a compound similar in structure to methane. Polygermanes—compounds that are similar to alkanes—with formula GeH containing up to five germanium atoms are known. The germanes are less volatile and less reactive than their corresponding silicon analogues. GeH reacts with alkali metals in liquid ammonia to form white crystalline MGeH which contain the GeH anion. The germanium hydrohalides with one, two and three halogen atoms are colorless reactive liquids.\nThe first organogermanium compound was synthesized by Winkler in 1887; the reaction of germanium tetrachloride with diethylzinc yielded tetraethylgermane (). Organogermanes of the type RGe (where R is an alkyl) such as tetramethylgermane () and tetraethylgermane are accessed through the cheapest available germanium precursor germanium tetrachloride and alkyl nucleophiles. Organic germanium hydrides such as isobutylgermane () were found to be less hazardous and may be used as a liquid substitute for toxic germane gas in semiconductor applications. Many germanium reactive intermediates are known: germyl free radicals, germylenes (similar to carbenes), and germynes (similar to carbynes). The organogermanium compound 2-carboxyethylgermasesquioxane was first reported in the 1970s, and for a while was used as a dietary supplement and thought to possibly have anti-tumor qualities.\n\nUsing a ligand called Eind (1,1,3,3,5,5,7,7-octaethyl-s-hydrindacen-4-yl) germanium is able to form a double bond with oxygen (germanone).\n\nGermanium occurs in 5 natural isotopes: , , , , and . Of these, is very slightly radioactive, decaying by double beta decay with a half-life of . is the most common isotope, having a natural abundance of approximately 36%. is the least common with a natural abundance of approximately 7%. When bombarded with alpha particles, the isotope will generate stable , releasing high energy electrons in the process. Because of this, it is used in combination with radon for nuclear batteries.\n\nAt least 27 radioisotopes have also been synthesized, ranging in atomic mass from 58 to 89. The most stable of these is , decaying by electron capture with a half-life of ays. The least stable is , with a half-life of . While most of germanium's radioisotopes decay by beta decay, and decay by delayed proton emission. through isotopes also exhibit minor delayed neutron emission decay paths.\n\nGermanium is created by stellar nucleosynthesis, mostly by the s-process in asymptotic giant branch stars. The s-process is a slow neutron capture of lighter elements inside pulsating red giant stars. Germanium has been detected in some of the most distant stars and in the atmosphere of Jupiter.\n\nGermanium's abundance in the Earth's crust is approximately 1.6 ppm. Only a few minerals like argyrodite, briartite, germanite, and renierite contain appreciable amounts of germanium. Only few of them (especially germanite) are, very rarely, found in mineable amounts. Some zinc-copper-lead ore bodies contain enough germanium to justify extraction from the final ore concentrate. An unusual natural enrichment process causes a high content of germanium in some coal seams, discovered by Victor Moritz Goldschmidt during a broad survey for germanium deposits. The highest concentration ever found was in Hartley coal ash with as much as 1.6% germanium. The coal deposits near Xilinhaote, Inner Mongolia, contain an estimated 1600 tonnes of germanium.\n\nAbout 118 tonnes of germanium was produced in 2011 worldwide, mostly in China (80 t), Russia (5 t) and United States (3 t). Germanium is recovered as a by-product from sphalerite zinc ores where it is concentrated in amounts as great as 0.3%, especially from low-temperature sediment-hosted, massive Zn–Pb–Cu(–Ba) deposits and carbonate-hosted Zn–Pb deposits. A recent study found that at least 10,000 t of extractable germanium is contained in known zinc reserves, particularly those hosted by Mississippi-Valley type deposits, while at least 112,000 t will be found in coal reserves. In 2007 35% of the demand was met by recycled germanium.\n\nWhile it is produced mainly from sphalerite, it is also found in silver, lead, and copper ores. Another source of germanium is fly ash of power plants fueled from coal deposits that contain germanium. Russia and China used this as a source for germanium. Russia's deposits are located in the far east of Sakhalin Island, and northeast of Vladivostok. The deposits in China are located mainly in the lignite mines near Lincang, Yunnan; coal is also mined near Xilinhaote, Inner Mongolia.\nThe ore concentrates are mostly sulfidic; they are converted to the oxides by heating under air in a process known as roasting:\n\nSome of the germanium is left in the dust produced, while the rest is converted to germanates, which are then leached (together with zinc) from the cinder by sulfuric acid. After neutralization, only the zinc stays in solution while germanium and other metals precipitate. After removing some of the zinc in the precipitate by the Waelz process, the residing Waelz oxide is leached a second time. The dioxide is obtained as precipitate and converted with chlorine gas or hydrochloric acid to germanium tetrachloride, which has a low boiling point and can be isolated by distillation:\n\nGermanium tetrachloride is either hydrolyzed to the oxide (GeO) or purified by fractional distillation and then hydrolyzed. The highly pure GeO is now suitable for the production of germanium glass. It is reduced to the element by reacting it with hydrogen, producing germanium suitable for infrared optics and semiconductor production:\n\nThe germanium for steel production and other industrial processes is normally reduced using carbon:\n\nThe major end uses for germanium in 2007, worldwide, were estimated to be: 35% for fiber-optics, 30% infrared optics, 15% polymerization catalysts, and 15% electronics and solar electric applications. The remaining 5% went into such uses as phosphors, metallurgy, and chemotherapy.\n\nThe notable properties of germania (GeO) are its high index of refraction and its low optical dispersion. These make it especially useful for wide-angle camera lenses, microscopy, and the core part of optical fibers. It has replaced titania as the dopant for silica fiber, eliminating the subsequent heat treatment that made the fibers brittle. At the end of 2002, the fiber optics industry consumed 60% of the annual germanium use in the United States, but this is less than 10% of worldwide consumption. GeSbTe is a phase change material used for its optic properties, such as that used in rewritable DVDs.\n\nBecause germanium is transparent in the infrared wavelengths, it is an important infrared optical material that can be readily cut and polished into lenses and windows. It is especially used as the front optic in thermal imaging cameras working in the 8 to 14 micron range for passive thermal imaging and for hot-spot detection in military, mobile night vision, and fire fighting applications. It is used in infrared spectroscopes and other optical equipment that require extremely sensitive infrared detectors. It has a very high refractive index (4.0) and must be coated with anti-reflection agents. Particularly, a very hard special antireflection coating of diamond-like carbon (DLC), refractive index 2.0, is a good match and produces a diamond-hard surface that can withstand much environmental abuse.\n\nSilicon-germanium alloys are rapidly becoming an important semiconductor material for high-speed integrated circuits. Circuits utilizing the properties of Si-SiGe junctions can be much faster than those using silicon alone. Silicon-germanium is beginning to replace gallium arsenide (GaAs) in wireless communications devices. The SiGe chips, with high-speed properties, can be made with low-cost, well-established production techniques of the silicon chip industry.\n\nSolar panels are a major use of germanium. Germanium is the substrate of the wafers for high-efficiency multijunction photovoltaic cells for space applications. High-brightness LEDs, used for automobile headlights and to backlight LCD screens, are an important application.\n\nBecause germanium and gallium arsenide have very similar lattice constants, germanium substrates can be used to make gallium arsenide solar cells. The Mars Exploration Rovers and several satellites use triple junction gallium arsenide on germanium cells.\n\nGermanium-on-insulator substrates are seen as a potential replacement for silicon on miniaturized chips. Other uses in electronics include phosphors in fluorescent lamps and solid-state light-emitting diodes (LEDs). Germanium transistors are still used in some effects pedals by musicians who wish to reproduce the distinctive tonal character of the \"fuzz\"-tone from the early rock and roll era, most notably the Dallas Arbiter Fuzz Face.\n\nGermanium dioxide is also used in catalysts for polymerization in the production of polyethylene terephthalate (PET). The high brilliance of this polyester is especially favored for PET bottles marketed in Japan. In the United States, germanium is not used for polymerization catalysts.\n\nDue to the similarity between silica (SiO) and germanium dioxide (GeO), the silica stationary phase in some gas chromatography columns can be replaced by GeO.\n\nIn recent years germanium has seen increasing use in precious metal alloys. In sterling silver alloys, for instance, it reduces firescale, increases tarnish resistance, and improves precipitation hardening. A tarnish-proof silver alloy trademarked Argentium contains 1.2% germanium.\n\nSemiconductor detectors made of single crystal high-purity germanium can precisely identify radiation sources—for example in airport security. Germanium is useful for monochromators for beamlines used in single crystal neutron scattering and synchrotron X-ray diffraction. The reflectivity has advantages over silicon in neutron and high energy X-ray applications. Crystals of high purity germanium are used in detectors for gamma spectroscopy and the search for dark matter. Germanium crystals are also used in X-ray spectrometers for the determination of phosphorus, chlorine and sulfur.\n\nGermanium is emerging as an important material for spintronics and spin-based quantum computing applications. In 2010, researchers demonstrated room temperature spin transport and more recently donor electron spins in germanium has been shown to have very long coherence times.\n\nGermanium is not considered essential to the health of plants or animals. Germanium in the environment has little or no health impact. This is primarily because it usually occurs only as a trace element in ores and carbonaceous materials, and the various industrial and electronic applications involve very small quantities that are not likely to be ingested. For similar reasons, end-use germanium has little impact on the environment as a biohazard. Some reactive intermediate compounds of germanium are poisonous (see precautions, below).\n\nGermanium supplements, made from both organic and inorganic germanium, have been marketed as an alternative medicine capable of treating leukemia and lung cancer. There is, however, no medical evidence of benefit; some evidence suggests that such supplements are actively harmful.\n\nSome germanium compounds have been administered by alternative medical practitioners as non-FDA-allowed injectable solutions. Soluble inorganic forms of germanium used at first, notably the citrate-lactate salt, resulted in some cases of renal dysfunction, hepatic steatosis, and peripheral neuropathy in individuals using them over a long term. Plasma and urine germanium concentrations in these individuals, several of whom died, were several orders of magnitude greater than endogenous levels. A more recent organic form, beta-carboxyethylgermanium sesquioxide (propagermanium), has not exhibited the same spectrum of toxic effects.\n\nU.S. Food and Drug Administration research has concluded that inorganic germanium, when used as a nutritional supplement, \"presents potential human health hazard\".\n\nCertain compounds of germanium have low toxicity to mammals, but have toxic effects against certain bacteria.\n\nSome of germanium's artificially-produced compounds are quite reactive and present an immediate hazard to human health on exposure. For example, germanium chloride and germane (GeH) are a liquid and gas, respectively, that can be very irritating to the eyes, skin, lungs, and throat.\n\nAs of the year 2000, about 15% of United States consumption of germanium was used for infrared optics technology and 50% for fiber-optics. Over the past 20 years, infrared use has consistently decreased; fiber optic demand, however, is slowly increasing. In America, 30–50% of current fiber optic lines are unused dark fiber, sparking discussion of over-production and a future reduction in demand. Worldwide, demand is increasing dramatically as countries such as China are installing fiber optic telecommunication lines throughout the country.\n\n"}
{"id": "21481649", "url": "https://en.wikipedia.org/wiki?curid=21481649", "title": "Institute of Nuclear Materials Management", "text": "Institute of Nuclear Materials Management\n\nThe Institute of Nuclear Materials Management (INMM) is an international technical and professional organization that works to promote safe handling of nuclear material and the safe practice of nuclear materials management through publications, as well as organized presentations and meetings.\n\nThe INMM's headquarters is located in Deerfield, Illinois in the United States, but its members are located around world including Europe, Asia, South America and North America. There are more than 1,100 members and 32 chapters.\n\nLes Shephard, vice president of Sandia National Laboratories' Energy, Security, and Defense Technology Center, said in February 2009 of the INMM and the American Nuclear Society, \n\nINMM is led by an executive committee of nine members, including a president, vice president, secretary, treasurer, four members-at-large, and the immediate past president. In addition, INMM has several standing and two technical committees. Many organizations, such as Los Alamos National Laboratory and Brookhaven National Laboratory, are Sustaining Members of the INMM.\n\nIn 2010, the INMM Executive Committee approved a restructuring of the Institute. This included changes in the technical divisions. Some were merged and a new division was created. The technical divisions are:\n\n\nThe division is the focal point for information and activities related to the physical protection of nuclear materials, nuclear facilities, and other high-value assets and facilities.\n\nUntil 2010, the INMM had six technical divisions:\n\n\nThe INMM develops and promotes global \"best practices\" for nuclear materials management. Best practices are based on past events, lessons learned, and ways to effectiveness and efficiency. They are focused on the six technical divisions and should be applicable to all countries with nuclear capabilities, both civilian and military.\n\nIn 2008, the INMM joined with Nuclear Threat Initiative, the United States Department of Energy, and the International Atomic Energy Agency to establish a new international organization called the World Institute for Nuclear Security (WINS) aimed at strengthening physical protection and security of the world's nuclear and radioactive materials and facilities. This organization's focus is on collecting information on best management practices from professionals responsible for on-the-ground security and sharing that information with their peer professionals around the world. These security professionals are in the best position to know where the vulnerabilities are, how to improve security, and how to ensure that improvements are implemented quickly and effectively. WINS will place a high priority on protecting sensitive information that may be discussed between members. Initial funding for the WINS included $3 million each from the Peter G. Peterson Foundation and the U.S. Department of Energy plus $100,000 from Norway.\n\nThe INMM has 32 chapters around the world, including six regional chapters in the United States and international chapters in Japan, South Korea, Morocco, Nigeria, Obninsk Regional in Russia, Russian Federation, South Africa, the United Kingdom, Ukraine, Urals, and Vienna.\n\nThe INMM also has 16 student chapters that offer opportunities including participation in a mentor program, meetings and workshops, publication subscriptions, and professional networking. Student chapters currently exist at Federal University of Rio de Janeiro, Georgia Institute of Technology, Idaho State University, Ibn Tofail University, Jordan University of Science and Technology, Mercyhurst College Institute for Intelligence Studies, Middlebury Institute of International Studies at Monterey, North Carolina State University and other Triangle Area universities, Pandit Deendayal Petroleum University, Pennsylvania State University, Texas A&M University, the University of Michigan, University of Missouri, University of New Mexico, University of Tennessee, University of Utah,\nUniversity of Washington, and Universitas Gadjah Mada\n\nINMM holds an annual meeting, an annual Spent Fuel Management Seminar, and a number of other workshops each year. These educational/networking events allow professionals in nuclear materials management to learn new strategies, keep abreast of the science and technology, and to meet with colleagues from around the world. In June 2005, INMM held two workshops in Prague, Czech Republic, sponsored by the Nuclear Threat Initiative (NTI).\n\nINMM publishes the \"Journal of Nuclear Materials Management\", a quarterly, peer-reviewed technical journal. In addition, the \"Communicator\", online newsletter is posted three times annually.\n\nAmerican physicist William Higginbotham served as technical editor of the \"Journal\" from 1974 until his death in 1994.\n\n\n"}
{"id": "41980677", "url": "https://en.wikipedia.org/wiki?curid=41980677", "title": "Land and water hemispheres", "text": "Land and water hemispheres\n\nThe land and water hemispheres of Earth, sometimes capitalised as the Land Hemisphere and Water Hemisphere, are the hemispheres of Earth containing the largest possible total areas of land and ocean, respectively. By definition (assuming that the entire surface can be classed as either \"land\" or \"ocean\") the two hemispheres do not overlap.\n\nDeterminations of the hemispheres vary slightly. One determination places the centre of the Land Hemisphere at (in the city of Nantes, France). The center of the water hemisphere is the antipodal point of the center of the land hemisphere, and is therefore located at , near New Zealand's Bounty Islands in the Pacific Ocean.\n\nAn alternative assignment determines the centre of the Land Hemisphere to be at (in île Dumet near Saint-Nazaire, France).\n\nThe Land Hemisphere has the substantial majority of the planet's land, including Europe, Africa, North America, nearly all of Asia and most of South America. However, even in the Land Hemisphere, the ocean area still slightly exceeds the land area. The Land hemisphere is almost identical to the hemisphere containing the greatest human population.\n\nThe Water Hemisphere has only about one-eighth of the world's land, including Australia, New Zealand, Hawaii, the Maritime Southeast Asia, and the Southern Cone of the Americas. Antarctica is solely within the water hemisphere, though sources differ as to whether it is considered land for the purposes of calculation.\n\nMost of the Pacific Ocean and the Indian Ocean are on the water hemisphere. Proportionately, the Water Hemisphere is approximately 89% water, 6% dry land and 5% polar ice cap.\n\nThe below table follows the assignments of Alphonse Berget of land to the two hemispheres. \n\n"}
{"id": "3263791", "url": "https://en.wikipedia.org/wiki?curid=3263791", "title": "Lattice constant", "text": "Lattice constant\n\nThe lattice constant, or lattice parameter, refers to the physical dimension of unit cells in a crystal lattice. Lattices in three dimensions generally have three lattice constants, referred to as \"a\", \"b\", and \"c\". However, in the special case of cubic crystal structures, all of the constants are equal and we only refer to \"a\". Similarly, in hexagonal crystal structures, the \"a\" and \"b\" constants are equal, and we only refer to the \"a\" and \"c\" constants. A group of lattice constants could be referred to as lattice parameters. However, the full set of lattice parameters consist of the three lattice constants and the three angles between them.\n\nFor example, the lattice constant for diamond is at 300 K. The structure is equilateral although its actual shape cannot be determined from only the lattice constant. Furthermore, in real applications, typically the average lattice constant is given. Near the crystal's surface, lattice constant is affected by the surface reconstruction that results in a deviation from its mean value. This deviation is especially important in nanocrystals since surface-to-nanocrystal core ratio is large. As lattice constants have the dimension of length, their SI unit is the meter. Lattice constants are typically on the order of several ångströms (i.e. tenths of a nanometer). Lattice constants can be determined using techniques such as X-ray diffraction or with an atomic force microscope. Lattice constant of a crystal can be used as a natural length standard of nanometer range.\n\nIn epitaxial growth, the lattice constant is a measure of the structural compatibility between different materials.\nLattice constant matching is important for the growth of thin layers of materials on other materials; when the constants differ, strains are introduced into the layer, which prevents epitaxial growth of thicker layers without defects.\n\nThe volume of the unit cell can be calculated from the lattice constant lengths and angles. If the unit cell sides are represented as vectors, then the volume is the dot product of one vector with the cross product of the other two vectors. The volume is represented by the letter \"V\". For the general unit cell\nFor monoclinic lattices with , , this simplifies to\nFor orthorhombic, tetragonal and cubic lattices with as well, then\n\nMatching of lattice structures between two different semiconductor materials allows a region of band gap change to be formed in a material without introducing a change in crystal structure. This allows construction of advanced light-emitting diodes and diode lasers.\n\nFor example, gallium arsenide, aluminium gallium arsenide, and aluminium arsenide have almost equal lattice constants, making it possible to grow almost arbitrarily thick layers of one on the other one.\n\nTypically, films of different materials grown on the previous film or substrate are chosen to match the lattice constant of the prior layer to minimize film stress.\n\nAn alternative method is to grade the lattice constant from one value to another by a controlled altering of the alloy ratio during film growth. The beginning of the grading layer will have a ratio to match the underlying lattice and the alloy at the end of the layer growth will match the desired final lattice for the following layer to be deposited.\n\nThe rate of change in the alloy must be determined by weighing the penalty of layer strain, and hence defect density, against the cost of the time in the epitaxy tool.\n\nFor example, indium gallium phosphide layers with a band gap above 1.9 eV can be grown on gallium arsenide wafers with index grading.\n"}
{"id": "3069940", "url": "https://en.wikipedia.org/wiki?curid=3069940", "title": "List of Lepidoptera that feed on Galium", "text": "List of Lepidoptera that feed on Galium\n\nGalium species (bedstraws) are used as food plants by the larvae of a number of Lepidoptera species:\n\nSpecies which feed exclusively on \"Galium\"\n\n\nSpecies which feed on \"Galium\" among other plants\n\n\n"}
{"id": "499947", "url": "https://en.wikipedia.org/wiki?curid=499947", "title": "List of sunken nuclear submarines", "text": "List of sunken nuclear submarines\n\nA total of nine nuclear submarines have sunk as a consequence of either accident or extensive damage. The United States Navy (USN) has lost two boats while five were lost in the Soviet Navy (one of which sank twice), and two from the Russian Navy. Only three were lost with all hands: the two from the United States Navy (129 and 99 lives lost) and one from the Russian Navy (118 lives lost), accounting for the three largest losses of life in a submarine. All sank as a result of accident except for , which was scuttled in the Kara Sea when proper decommissioning was considered too expensive. The Soviet submarine carried nuclear ballistic missiles when it was lost with all hands, but as it was a diesel-electric submarine, it is not included in the list.\n\nThe two USN submarines belonged to Submarine Force Atlantic, in the U.S. Atlantic Fleet. All five of the Soviet/Russian nuclear submarines that remain sunken belonged to the Northern Fleet, while the refloated was in the Pacific Fleet.\n\nOf the nine sinkings, two were caused by fires, two by explosions of their weapons systems, two by flooding, one by bad weather, and one by scuttling due to a damaged nuclear reactor. Only 's reason for sinking is unknown. Eight of the submarines are underwater wrecks in the Northern Hemisphere, five in the Atlantic Ocean and three in the Arctic Ocean. The ninth submarine, \"K-429\", was raised and returned to active duty after both of her sinkings.\n\n\n\n\n\n\n"}
{"id": "38616590", "url": "https://en.wikipedia.org/wiki?curid=38616590", "title": "Litgrid", "text": "Litgrid\n\nLitgrid, the Lithuanian electricity transmission system operator, maintains a stable operation of the national power system, controls electricity flows, and enables competition in an open domestic electricity market. Litgrid is responsible for integrating the national power system into the European power infrastructure and electricity market. The company is implementing strategic electricity cross-border links, namely, NordBalt (Lithuania-Sweden) and LitPol Link (Lithuania-Poland).\n\nSince 2010 Litgrid has been listed on the NASDAQ OMX Vilnius stock market.\n\n"}
{"id": "44227075", "url": "https://en.wikipedia.org/wiki?curid=44227075", "title": "Little Dip Conservation Park", "text": "Little Dip Conservation Park\n\n\n"}
{"id": "6483146", "url": "https://en.wikipedia.org/wiki?curid=6483146", "title": "Loloru", "text": "Loloru\n\nLoloru is a pyroclastic shield volcano located in the southern region of Bougainville Island, within the Autonomous Region of Bougainville of northeastern Papua New Guinea. \n\nThe volcano's summit consists of two nested calderas, with an andesitic lava dome.\n\nLake Loloru, a crescent-shaped crater lake, is within the volcano. It is a sacred place of South Bougainvillians, who traditionally believe the souls of their dead go here upon death.\n\n\n"}
{"id": "17033619", "url": "https://en.wikipedia.org/wiki?curid=17033619", "title": "Macrograph", "text": "Macrograph\n\nA macrograph or photomacrograph is an image taken at a scale that is visible to the naked eye, as opposed to a micrographic image. It is sometimes defined more precisely as an image at a scale of less than ten times magnification.\n\nThis term is often applied to a three-dimensional image taken of a material using a low-power stereomicroscope. These images are used in materials science, particularly in the study of stress fractures in metals. This method can also be used to assay the fine structure of steel, in a standardized test called the Baumann method that creates a sulfur print showing the amount and distribution of sulfur inclusions through the metal structure.\n\n"}
{"id": "27327833", "url": "https://en.wikipedia.org/wiki?curid=27327833", "title": "Nanocellulose", "text": "Nanocellulose\n\nNanocellulose is a term referring to nano-structured cellulose. This may be either cellulose nanocrystal (CNC or NCC), cellulose nanofibers (CNF) also called microfibrillated cellulose (MFC), or bacterial nanocellulose, which refers to nano-structured cellulose produced by bacteria.\n\nCNF is a material composed of nanosized cellulose fibrils with a high aspect ratio (length to width ratio). Typical fibril widths are 5–20 nanometers with a wide range of lengths, typically several micrometers. It is pseudo-plastic and exhibits thixotropy, the property of certain gels or fluids that are thick (viscous) under normal conditions, but become less viscous when shaken or agitated. When the shearing forces are removed the gel regains much of its original state. The fibrils are isolated from any cellulose containing source including wood-based fibers (pulp fibers) through high-pressure, high temperature and high velocity impact homogenization, grinding or microfluidization (see manufacture below).\n\nNanocellulose can also be obtained from native fibers by an acid hydrolysis, giving rise to highly crystalline and rigid nanoparticles which are shorter (100s to 1000 nanometers) than the nanofibrils obtained through homogenization, microfluiodization or grinding routes. The resulting material is known as cellulose nanocrystal (CNC).\n\nThe terminology microfibrillated/nanocellulose or (MFC) was first used by Turbak, Snyder and Sandberg in the late 1970s at the ITT Rayonier labs in Whippany, New Jersey, USA to describe a product prepared as a gel type material by passing wood pulp through a Gaulin type milk homogenizer at high temperatures and high pressures followed by ejection impact against a hard surface.\n\nThe terminology first appeared publicly in the early 1980s when a number of patents and publications were issued to ITT Rayonier on a new nanocellulose composition of matter. In later work Herrick at Rayonier also published work on making a dry powder form of the gel. Rayonier has been one of the world's premier producers of purified pulps interested in creating new uses and new markets for pulps and not to compete with new customers. Thus, as the patents issued, Rayonier gave free license to whoever wanted to pursue this new use for cellulose. Rayonier, as a company, never pursued scale-up. Rather, Turbak et al. pursued 1) finding new uses for the MFC/nanocellulose. These included using MFC as a thickener and binder in foods, cosmetics, paper formation, textiles, nonwovens, etc. and 2) evaluate swelling and other techniques for lowering the energy requirements for MFC/Nanocellulose production. After ITT closed the Rayonier Whippany Labs in 1983–84, Herric worked on making a dry powder form of MFC at the Rayonier labs in Shelton, Washington, USA\n\nIn the mid 1990s the group of Taniguchi and co-workers and later Yano and co-workers pursued the effort in Japan. and a host of major companies, see numerous U.S. patents issued to P&G, J&J, 3M, McNeil, etc. using U.S. patent search under inventor name Turbak search base.\n\nNanocellulose, which is also called cellulose nanofibers (CNF), microfibrillated cellulose (MFC) or cellulose nanocrystal (CNC), can be prepared from any cellulose source material, but woodpulp is normally used.\n\nThe nanocellulose fibrils may be isolated from the wood-based fibers using mechanical methods which expose the pulp to high shear forces, ripping the larger wood-fibres apart into nanofibers. For this purpose, high-pressure homogenizers, ultrasonic homogenizers, grinders or microfluidizers can be used. The homogenizers are used to delaminate the cell walls of the fibers and liberate the nanosized fibrils. This process consumes very large amounts of energy and values over 30 MWh/tonne are not uncommon.\n\nTo address this problem, sometimes enzymatic/mechanical pre-treatments and introduction of charged groups for example through carboxymethylation or TEMPO-mediated oxidation are used. these pre-treatments can decrease energy consumption below 1 MWh/tonne.\n\nCellulose nanowhiskers are rodlike highly crystalline particles (relative crystallinity index above 75%) with a rectangular cross section. They are formed by the acid hydrolysis of native cellulose fibers commonly using sulfuric or hydrochloric acid. Amorphous sections of native cellulose are hydrolysed and after careful timing, crystalline sections can be retrieved from the acid solution by centrifugation and washing. Their dimensions depend on the native cellulose source material, and hydrolysis time and temperature.\n\nIn April 2013 breakthroughs in nanocellulose production were announced at an American Chemical Society conference.\n\nAt ICAR-Central Institute for Research on Cotton Technology, Mumbai, India, a novel chemo-mechanical process for production of nanocellulose from cotton linters has been developed in the year 2013. To demonstrate this technology to the industrial users, a nanocellulose pilot plant is now operational at this Institute in Mumbai with a capacity of 10 kg per day. This facility was inaugurated in 2015.\n\nThe ultrastructure of nanocellulose derived from various sources has been extensively studied. Techniques such as transmission electron microscopy (TEM), scanning electron microscopy (SEM), atomic force microscopy (AFM), wide angle X-ray scattering (WAXS), small incidence angle X-ray diffraction and solid state C cross-polarization magic angle spinning (CP/MAS), nuclear magnetic resonance (NMR) and spectroscopy have been used to characterize typically dried nanocellulose morphology.\n\nA combination of microscopic techniques with image analysis can provide information on fibril widths, it is more difficult to determine fibril lengths, because of entanglements and difficulties in identifying both ends of individual nanofibrils. Also, nanocellulose suspensions may not be homogeneous and can consist of various structural components, including cellulose nanofibrils and nanofibril bundles.\n\nIn a study of enzymatically pre-treated nanocellulose fibrils in a suspension the size and size-distribution were established using cryo-TEM. The fibrils were found to be rather mono-dispersed mostly with a diameter of ca. 5 nm although occasionally thicker fibril bundles were present. By combining ultrasonication with an \"oxidation pretreatment\", cellulose microfibrils with a lateral dimension below 1 nm has been observed by AFM. The lower end of the thickness dimension is around 0.4 nm, which is related to the thickness of a cellulose monolayer sheet.\n\nAggregate widths can be determined by CP/MAS NMR developed by Innventia AB, Sweden, which also has been demonstrated to work for nanocellulose (enzymatic pre-treatment). An average width of 17 nm has been measured with the NMR-method, which corresponds well with SEM and TEM. Using TEM, values of 15 nm have been reported for nanocellulose from carboxymethylated pulp. However, thinner fibrils can also be detected. Wågberg et al. reported fibril widths of 5–15 nm for a nanocellulose with a charge density of about 0.5 meq./g. The group of Isogai reported fibril widths of 3–5 nm for TEMPO-oxidized cellulose having a charge density of 1.5 meq./g.\n\nPulp chemistry has a significant influence on nanocellulose microstructure. Carboxymethylation increases the numbers of charged groups on the fibril surfaces, making the fibrils easier to liberate and results in smaller and more uniform fibril widths (5–15 nm) compared to enzymatically pre-treated nanocellulose, where the fibril widths were 10–30 nm. The degree of crystallinity and crystal structure of nanocellulose. Nanocellulose exhibits cellulose crystal I organization and the degree of crystallinity is unchanged by the preparation of the nanocellulose. Typical values for the degree of crystallinity were around 63%.\n\nThe unique rheology of nanocellulose dispersions was recognized by the early investigators. The high viscosity at low nanocellulose concentrations makes nanocellulose very interesting as a non-caloric stabilizer and gellant in food applications, the major field explored by the early investigators.\n\nThe dynamic rheological properties were investigated in great detail and revealed that the storage and loss modulus were independent of the angular frequency at all nanocellulose concentrations between 0.125% to 5.9%. The storage modulus values are particularly high (104 Pa at 3% concentration) compared to results for cellulose nanowhiskers (102 Pa at 3% concentration). There is also a particular strong concentration dependence as the storage modulus increases 5 orders of magnitude if the concentration is increased from 0.125% to 5.9%.\n\nNanocellulose gels are also highly shear thinning (the viscosity is lost upon introduction of the shear forces). The shear-thinning behaviour is particularly useful in a range of different coating applications.\n\nCrystalline cellulose has interesting mechanical properties for use in material applications. Its tensile strength is about 500MPa, similar to that of aluminium. Its stiffness is about 140–220 GPa, comparable with that of Kevlar and better than that of glass fiber, both of which are used commercially to reinforce plastics. Films made from nanocellulose have high strength (over 200 MPa), high stiffness (around 20 GPa) and high strain (12%). Its strength/weight ratio is 8 times that of stainless steel. Fibers made from nanocellulose have high strength (up to 1.57 GPa) and stiffness (up to 86 GPa).\n\nIn semi-crystalline polymers, the crystalline regions are considered to be gas impermeable. Due to relatively high crystallinity, in combination with the ability of the nanofibers to form a dense network held together by strong inter-fibrillar bonds (high cohesive energy density), it has been suggested that nanocellulose might act as a barrier material. Although the number of reported oxygen permeability values are limited, reports attribute high oxygen barrier properties to nanocellulose films. One study reported an oxygen permeability of 0.0006 (cm µm)/(m day kPa) for a ca. 5 µm thin nanocellulose film at 23 °C and 0% RH. In a related study, a more than 700-fold decrease in oxygen permeability of a polylactide (PLA) film when a nanocellulose layer was added to the PLA surface was reported.\n\nThe influence of nanocellulose film density and porosity on film oxygen permeability has recently been explored. Some authors have reported significant porosity in nanocellulose films, which seems to be in contradiction with high oxygen barrier properties, whereas Aulin et al. measured a nanocellulose film density close to density of crystalline cellulose (cellulose Iß crystal structure, 1.63 g/cm) indicating a very dense film with a porosity close to zero.\n\nChanging the surface functionality of the cellulose nanoparticle can also affect the permeability of nanocellulose films. Films constituted of negatively charged cellulose nanowhiskers could effectively reduce permeation of negatively charged ions, while leaving neutral ions virtually unaffected. Positively charged ions were found to accumulate in the membrane.\n\nMulti-Parametric Surface Plasmon Resonance is one of the methods to study barrier properties of natural, modified or coated nanocellulose. The different antifouling, moisture, solvent, antimicrobial barrier formulation quality can be measured on the nanoscale. The adsorption kinetics as well as the degree of swelling can be measured in real-time and label-free.\n\nNanocellulose can also be used to make aerogels/foams, either homogeneously or in composite formulations. Nanocellulose-based foams are being studied for packaging applications in order to replace polystyrene-based foams. Svagan et al. showed that nanocellulose has the ability to reinforce starch foams by using a freeze-drying technique. The advantage of using nanocellulose instead of wood-based pulp fibers is that the nanofibrills can reinforce the thin cells in the starch foam. Moreover, it is possible to prepare pure nanocellulose aerogels applying various freeze-drying and super critical drying techniques. Aerogels and foams can be used as porous templates. Tough ultra-high porosity foams prepared from cellulose I nanofibrill suspensions were studied by Sehaqui et al. a wide range of mechanical properties including compression was obtained by controlling density and nanofibrill interaction in the foams. Cellulose nanowhiskers could also be made to gel in water under low power sonication giving rise to aerogels with the highest reported surface area (>600m2/g) and lowest shrinkage during drying (6.5%) of cellulose aerogels. In another study by Aulin et al., the formation of structured porous aerogels of nanocellulose by freeze-drying was demonstrated. The density and surface texture of the aerogels was tuned by selecting the concentration of the nanocellulose dispersions before freeze-drying. Chemical vapour deposition of a fluorinated silane was used to uniformly coat the aerogel to tune their wetting properties towards non-polar liquids/oils. The authors demonstrated that it is possible to switch the wettability behaviour of the cellulose surfaces between super-wetting and super-repellent, using different scales of roughness and porosity created by the freeze-drying technique and change of concentration of the nanocellulose dispersion. Structured porous cellulose foams can however also be obtained by utilizing the freeze-drying technique on cellulose generated by Gluconobacter strains of bacteria, which bio-synthesize open porous networks of cellulose fibers with relatively large amounts of nanofibrills dispersed inside. Olsson et al. demonstrated that these networks can be further impregnated with metalhydroxide/oxide precursors, which can readily be transformed into grafted magnetic nanoparticles along the cellulose nanofibers. The magnetic cellulose foam may allow for a number of novel applications of nanocellulose and the first remotely actuated magnetic super sponges absorbing 1 gram of water within a 60 mg cellulose aerogel foam were reported. Notably, these highly porous foams (>98% air) can be compressed into strong magnetic nanopapers, which may find use as functional membranes in various applications.\n\nThe surface modification of nanocellulose is currently receiving a large amount of attention. Nanocellulose displays a high concentration of hydroxyl groups at the surface which can be reacted. However, hydrogen bonding strongly affects the reactivity of the surface hydroxyl groups. In addition, impurities at the surface of nanocellulose such as glucosidic and lignin fragments need to be removed before surface modification to obtain acceptable reproducibility between different batches.\n\nHealth, safety and environmental aspects of nanocellulose have been recently evaluated. Processing of nanocellulose does not cause significant exposure to fine particles during friction grinding or spray drying. No evidence of inflammatory effects or cytotoxicity on mouse or human macrophages can be observed after exposure to nanocellulose. The results of toxicity studies suggest that nanocellulose is not cytotoxic and does not cause any effects on inflammatory system in macrophages. In addition, nanocellulose is not acutely toxic to Vibrio fischeri in environmentally relevant concentrations.\n\nThe properties of nanocellulose (e.g. mechanical properties, film-forming properties, viscosity etc.) makes it an interesting material for many applications and the potential for a multibillion-dollar industry.\n\nThere is potential of nanocellulose applications in the area of paper and paperboard manufacture. Nanocelluloses are expected to enhance the fiber-fiber bond strength and, hence, have a strong reinforcement effect on paper materials. Nanocellulose may be useful as a barrier in grease-proof type of papers and as a wet-end additive to enhance retention, dry and wet strength in commodity type of paper and board products. It has been shown that applying CNF as a coating material on the surface of paper and paperboard improves the barrier properties, especially air resistance. It also enhances the structure properties of paperboards (smoother surface).\n\nNanocellulose can be used to prepare flexible and optically transparent paper. Such paper is an attractive substrate for electronic devices because it is recyclable, compatible with biological objects, and easily degrades when disposed of.\n\nLike resin-free lignocellulose fiberboard which are produced using wet process, high tough cellulose nanofiber board with thickness of 3 mm was also introduced by Yousefi et al., 2018.\n\nAs described above the properties of the nanocellulose makes an interesting material for reinforcing plastics. Nanocellulose has been reported to improve the mechanical properties of, for example, thermosetting resins, starch-based matrixes, soy protein, rubber latex, poly(lactide). The composite applications may be for use as coatings and films, paints, foams, packaging.\n\nNanocellulose can be used as a low calorie replacement for today’s carbohydrate additives used as thickeners, flavour carriers and suspension stabilizers in a wide variety of food products and is useful for producing fillings, crushes, chips, wafers, soups, gravies, puddings etc. The food applications were early recognised as a highly interesting application field for nanocellulose due to the rheological behaviour of the nanocellulose gel.\n\nApplications in this field include: Super water absorbent material (e.g. for incontinence pads material), nanocellulose used together with super absorbent polymers, nanocellulose in tissue, non-woven products or absorbent structures and as antimicrobial films. \n\nNanocellulose has numerous applications as a food additive, and in the general area of emulsion and dispersion applications in other fields. Oil in water applications were early recognized. Early investigators had explored the area of non-settling suspensions for pumping sand, coal as well as paints and drilling muds.\n\nHydrocarbon fracturing of oil-bearing formations is a potentially interesting and large-scale application. Nanocellulose has been suggested for use in oil recovery applications as a fracturing fluid. Drilling muds based on nanocellulose have also been suggested.\n\nThe use of nanocellulose in cosmetics and pharmaceuticals was also early recognized. A wide range of high-end applications have been suggested:\n\n\nAlthough wood-driven nanocellulose was first produced in 1983 by Herrick and Turbak, its commercial production postponed till 2010, mainly due to the high production energy consumption and high production cost. Inventia Co. in Sweden was the first nanocellulose company established in 2010. Other first-generation active companies include CelluForce (Canada), Kruger (Canada), Performance BioFilaments (Canada), Nippon (Japan), Nano Novin Polymer Co. (Iran), Maine University (USA), VTT (Finland), Sappi (Netherlands), InoFib (France), and Melodea (Israel).\n\n"}
{"id": "53567230", "url": "https://en.wikipedia.org/wiki?curid=53567230", "title": "Nisinic acid", "text": "Nisinic acid\n\nNisinic acid is a very long chain polyunsaturated omega-3 fatty acid, similar to docosahexaenoic acid (DHA). The lipid name is 24:6 (n-3) and the chemical name is \"all\"-\"cis\"-6,9,12,15,18,21-tetracosahexaenoic acid. It is not well studied, but polyunsaturated fatty acids even longer than DHA, nisinic acid included, may hold scientific promise.\n"}
{"id": "22517546", "url": "https://en.wikipedia.org/wiki?curid=22517546", "title": "Osmium borides", "text": "Osmium borides\n\nOsmium borides are compounds of osmium and boron. Their most remarkable property is potentially high hardness. It is thought that a combination of high electron density of osmium with the strength of boron-osmium covalent bonds will make osmium borides superhard materials. This has not been demonstrated yet. For example, OsB is hard (hardness comparable to that of sapphire), but not superhard.\n\nOsmium borides are produced in vacuum or inert atmosphere to prevent formation\nof osmium tetroxide, which is a hazardous compound. Synthesis occurs at high temperatures (~1000 °C) from a mixture of MgB and OsCl.\n\nThree osmium borides are known: OsB, OsB and OsB. The first two have hexagonal structure, similar to that of rhenium diboride. Osmium diboride was first also sought as hexagonal, but one of its phase's was later reassigned to orthorhombic. In recent methods of synthesis, it has also been found that a hexagonal phase of OsB exists with a similar structure to ReB.\n"}
{"id": "52740225", "url": "https://en.wikipedia.org/wiki?curid=52740225", "title": "Phosphorus trifluorodichloride", "text": "Phosphorus trifluorodichloride\n\nPhosphorus trifluorodichloride is a chemical compound with the chemical formula PFCl.The molecular geometry of PFCl is trigonal bipyramidal with asymmetric charge distribution on the central atom with hybridization: spd. It appears as a colorless gas with a bad-smelling odor which turns to a liquid at -8 °C.\n"}
{"id": "385621", "url": "https://en.wikipedia.org/wiki?curid=385621", "title": "Plasma diagnostics", "text": "Plasma diagnostics\n\nPlasma diagnostics are a pool of methods, instruments, and experimental techniques used to measure properties of a plasma, such as plasma components' density, distribution function over energy (temperature), their spatial profiles and dynamics, which enable to derive plasma parameters.\n\nMeasurements with electric probes, called Langmuir probes, are the oldest and most often used procedures for low-temperature plasmas. The method was developed by Irving Langmuir and his co-workers in the 1920s, and has since been further developed in order to extend its applicability to more general conditions than those presumed by Langmuir. Langmuir probe measurements are based on the estimation of current versus voltage characteristics of a circuit consisting of two metallic electrodes that are both immersed in the plasma under study. Two cases are of interest:\n(a) The surface areas of the two electrodes differ by several orders of magnitude. This is known as the \"single-probe\" method.\n(b) The surface areas are very small in comparison with the dimensions of the vessel containing the plasma and approximately equal to each other. This is the \"double-probe\" method.\n\nConventional Langmuir probe theory assumes collisionless movement of charge carriers in the space charge sheath around the probe. Further it is assumed that the sheath boundary is well-defined and that beyond this boundary the plasma is completely undisturbed by the presence of the probe. This means that the electric field caused by the difference between the potential of the probe and the plasma potential at the place where the probe is located is limited to the volume inside the probe sheath boundary.\n\nThe general theoretical description of a Langmuir probe measurement requires the simultaneous solution of the Poisson equation, the collision-free Boltzmann equation or Vlasov equation, and the continuity equation with regard to the boundary condition at the probe surface and requiring that, at large distances from the probe, the solution approaches that expected in an undisturbed plasma.\n\nA plasma ion analyser measures ion energy distributions at selected ion mass, and mass spectra for selected ion energies, for both positive and negative ions.\nPlasma neutrals, radicals and fast neutrals are analysed by an integrated electron impact ionizer.\nThe plasma ion analyser is typically a quadrupole mass spectrometer configured with an ion energy analyser in a differentially pumped high vacuum chamber.\nThe plasma ion analyser samples plasma ions and neutrals through a pin hole aperture and if properly configured, can provide a non invasive interface for plasma analysis.\n\nA ball-pen probe is novel technique used to measure directly the plasma potential in magnetized plasmas. The probe was invented by Jiří Adámek in the Institute of Plasma Physics AS CR in 2004. The ball-pen probe balances the electron saturation current to the same magnitude as that of the ion saturation current. In this case, its floating potential becomes identical to the plasma potential. This goal is attained by a ceramic shield, which screens off an adjustable part of the electron current from the probe collector due to the much smaller gyro–radius of the electrons. The electron temperature is proportional to the difference of ball-pen probe(plasma potential) and Langmuir probe (floating potential) potential. Thus, the electron temperature can be obtained directly with high temporal resolution without additional power supply.\n\nNonlinear effects like the I-V characteristic of the boundary sheath are utilized for Langmuir probe measurements but they are usually neglected for modelling of RF discharges due to their very inconvenient mathematical treatment. The Self Excited Electron Plasma Resonance Spectroscopy (SEERS) utilizes exactly these nonlinear effects and known resonance effects in RF discharges. The nonlinear elements, in particular the sheaths, provide harmonics in the discharge current and excite the plasma and the sheath at their series resonance characterized by the so-called geometric resonance frequency.\n\nSEERS provides the spatially and reciprocally averaged electron plasma density and the effective electron collision rate. The electron collision rate reflects stochastic (pressure) heating and ohmic heating of the electrons.\n\nThe model for the plasma bulk is based on 2d-fluid model (zero and first order moments of Boltzmann equation) and the full set of the Maxwellian equations leading to the Helmholtz equation for the magnetic field. The sheath model is based additionally on the Poisson equation.\n\nIf the magnetic field in the plasma is not stationary, either because the plasma as a whole is transient or because the fields are periodic (radio-frequency heating), the rate of change of the magnetic field with time (formula_1, read \"B-dot\") can be measured locally with a loop or coil of wire. Such coils exploit Faraday's law, whereby a changing magnetic field induces an electric field. The induced voltage can be measured and recorded with common instruments.\nAlso, by Ampere's law, the magnetic field is proportional to the currents that produce it, so the measured magnetic field gives information about the currents flowing in the plasma. Both currents and magnetic fields are important in understanding fundamental plasma physics.\n\nThe conventional Faraday cup is applied for measurements of ion (or electron) flows from plasma boundaries and for mass spectrometry.\n\nPassive spectroscopic methods simply observe the radiation emitted by the plasma.\n\nIf the plasma (or one ionic component of the plasma) is flowing in the direction of the line of sight to the observer, emission lines will be seen at a different frequency due to the Doppler effect.\n\nThe thermal motion of ions will result in a shift of emission lines up or down, depending on whether the ion is moving toward or away from the observer. The magnitude of the shift is proportional to the velocity along the line of sight. The net effect is a characteristic broadening of spectral lines, known as Doppler broadening, from which the ion temperature can be determined.\n\nThe splitting of some emission lines due to the Stark effect can be used to determine the local electric field.\n\nEven if the macroscopic electric field is zero, any single ion will experience an electric field due to the neighboring charged particles in the plasma. This results in a broadening of some lines that can be used to determine the density of the plasma.\n\nThe brightness of an atomic spectral line emitted by atoms and ions in a gas (or plasma) can depend on the gas's temperature and pressure.\n\nDue to the completeness and accuracy of modern collisional radiative models the temperature and density of plasmas can be measured by taking ratios of the emission intensities of various atomic spectral lines.\n\nThe presence of a magnetic field splits the atomic energy levels due to the Zeeman effect. This leads to broadening or splitting of spectral lines. Analyzing these lines can, therefore, yield the magnetic field strength in the plasma.\n\nActive spectroscopic methods stimulate the plasma atoms in some way and observe the result (emission of radiation, absorption of the stimulating light or others).\n\nBy shining through the plasma a laser with a wavelength, tuned to a certain transition of one of the species present in the plasma, the absorption profile of that transition could be obtained. This profile provides information not only for the plasma parameters, that could be obtained from the emission profile, but also for the line-integrated number density of the absorbing species.\n\nA beam of neutral atoms is fired into a plasma. Some atoms are excited by collisions within the plasma and emit radiation. This can be used to probe density fluctuations in a turbulent plasma.\n\nIn very hot plasmas (as in magnetic fusion experiments), light elements are fully ionized and don't emit line radiation. When a beam of neutral atoms is fired into the plasma, electrons from beam atoms are transferred to hot plasma ions, which form hydrogenic ions which promptly emit line radiation. This radiation is analyzed for ion density, temperature, and velocity.\n\nIf the plasma is not fully ionized but contains ions that fluoresce, laser-induced fluorescence can provide very detailed information on temperature, density, and flows.\n\nIf an atom is moving in a magnetic field, the Lorentz force will act in opposite directions on the nucleus and the electrons, just as an electric field does. In the frame of reference of the atom, there \"is\" an electric field, even if there is none in the laboratory frame. Consequently, certain lines will be split by the Stark effect. With an appropriate choice of beam species and velocity and of geometry, this effect can be used to determine the magnetic field in the plasma.\n\nThe two-photon laser-induced fluorescence (TALIF) is a modification of the laser-induced fluorescence technique. In this approach the upper level is excited by absorbing two photons and registering the resulting emission from the excited state. The advantage of this approach is that the registered light from the fluorescence is with a different wavelength from the exciting laser beam, which leads to improved signal to noise ratio.\n\nThe optical diagnostics above measure line radiation from atoms. Alternatively, the effects of free charges on electromagnetic radiation can be used as a diagnostic.\n\nIn magnetized plasmas, electrons will gyrate around magnetic field lines and emit cyclotron radiation. The frequency of the emission is given by the cyclotron resonance condition. In a sufficiently thick and dense plasma, the intensity of the emission will follow Planck's law, and only depend on the electron temperature.\n\nScattering of laser light from the electrons in a plasma is known as Thomson scattering. The electron temperature can be determined very reliably from the Doppler broadening of the laser line. The electron density can be determined from the intensity of the scattered light, but a careful absolute calibration is required. Although Thomson scattering is dominated by scattering from electrons, since the electrons interact with the ions, in some circumstances information on the ion temperature can also be extracted.\n\nIf a plasma is placed in one arm of an interferometer, the phase shift will be proportional to the plasma density integrated along the path.\n\nThe Faraday effect will rotate the plane of polarization of a beam passing through a plasma with a magnetic field in the direction of the beam. This effect can be used as a diagnostic of the magnetic field, although the information is mixed with the density profile and is usually an integral value only.\n\nFusion plasmas produces 2.5 MeV and 14 MeV neutrons. By measuring the neutron flux, plasma properties such as ion temperature and fusion power can be determined.\n\n\n"}
{"id": "1436150", "url": "https://en.wikipedia.org/wiki?curid=1436150", "title": "Plasma torch", "text": "Plasma torch\n\nA plasma torch (also known as a plasma arc, plasma gun, or plasma cutter, plasmatron) is a device for generating a directed flow of plasma.\n\nThe plasma jet can be used for applications including plasma cutting, plasma arc welding, plasma spraying, and plasma gasification for waste disposal.\n\nThermal plasmas are generated in plasma torches by direct current (DC), alternating current (AC), radio-frequency (RF) and other discharges. DC torches are the most commonly used and researched, because when compared to AC: “there is less flicker generation and noise, a more stable operation, better control, a minimum of two electrodes, lower electrode consumption, slightly lower refractory [heat] wear and lower power consumption”.\n\nIn a DC torch, the electric arc is formed between the electrodes (which can be made of copper, tungsten, graphite, molybdenum, silver etc.), and the thermal plasma is formed from the continual input of carrier/working gas, projecting outward as a plasma jet/flame (as can be seen on the right). In DC torches, the carrier gas can be, for example, either oxygen, nitrogen, argon, helium, air, or hydrogen; and although termed as such, it does not have to be a gas (thus, better termed a carrier fluid).\n\nFor example, a research plasma torch at the Institute of Plasma Physics (IPP) in Prague, Czech Republic, functions with an HO vortex (as well as a small addition of argon to ignite the arc), and produces a high temperature/velocity plasma flame. In fact, early studies of arc stabilization employed a water-vortex. Overall, the electrode materials and carrier fluids have to be specifically matched to avoid excessive electrode corrosion or oxidation (and contamination of materials to be treated), while maintaining ample power and function.\n\nFurthermore, the flow-rate of the carrier gas can be raised to promote a larger, more projecting plasma jet, provided that the arc current is sufficiently increased; and vice versa.\n\nThe plasma flame of a real plasma torch is a few inches long at most; it is to be distinguished from fictional long-range plasma weapons.\n\nIt is important to note that there are two types of DC torches: non-transferred and transferred. In non-transferred DC torches, the electrodes are inside the body/housing of the torch itself (creating the arc there). Whereas in transferred—one electrode is outside (and is usually the conductive material to be treated), allowing the arc to form outside of the torch over a larger distance.\n\nA benefit of transferred DC torches is that the plasma arc is formed outside the water-cooled body, preventing heat loss—as is the case with non-transferred torches, where their electrical-to-thermal efficiency can be as low as 50%, but the hot water can itself be utilized. Furthermore, transferred DC torches can be used in a twin-torch setup, where one torch is cathodic and the other anodic, which has the earlier benefit of a regular transferred single-torch system, but allows their use with non-conductive materials, as there is no need for it to form the other electrode. However, these types of setups are rare as most common non-conductive materials do not require the precise cutting ability of a plasma torch. In addition, the discharge generated by this particular plasma source configuration is characterized by a complex shape and fluid dynamics that requires a 3D description in order to be predicted, making performance unsteady. The electrodes of non-transferred torches are larger, because they suffer more wear by the plasma arc.\n\nThe quality of plasma produced is a function of density (pressure), temperature and torch power (the greater the better). With regards to the efficiency of the torch itself—this can vary among manufacturers and torch technology; though for example, Leal-Quirós reports that for Westinghouse Plasma Corp. torches “a thermal efficiency of 90% is easily possible; the efficiency represents the percentage of arc power that exits the torch and enters the process”.\n\n"}
{"id": "12189908", "url": "https://en.wikipedia.org/wiki?curid=12189908", "title": "Shikoku Electric Power", "text": "Shikoku Electric Power\n\nThe is the electric provider for the 4 prefectures of the Shikoku island in Japan with few exceptions. Their image character is .\n\nOn April 12, 1991 the company instituted Akari-chan as their image character and at the same time introduced the romanized nickname of Yonden (yon is another reading for 4, which occurs in Shikoku).\n\nThe company controls numerous 'ko-gaisha' (subsidiaries), such as an electronic parts maker, a cable media company, electric services pro diver and also an internet service provider called \"Akari-net\". Those who sign a contract with Yonden may be eligible to get free internet access. Yonden institutes automatic filtering of web content.\n\n"}
{"id": "30081200", "url": "https://en.wikipedia.org/wiki?curid=30081200", "title": "Sinovel", "text": "Sinovel\n\nSinovel Wind Group Company () is a Chinese wind turbine manufacturer headquartered in Beijing. It is the largest wind turbine manufacturer in China and by 2011 market share the second largest in the world.\n\nThe company aims to be the largest turbine maker by 2015 with half of sales for foreign markets.\n\nDonghai Bridge Wind Farm, the first offshore wind farm in China, began operation in July 2010 using 34 3 MW Sinovel turbines.\n\nIn December 2010, the Massachusetts Water Resources Authority ordered a 1.5 MW wind turbine to power a wastewater pumping station in Charlestown, Boston.\n\nThe firm was accused and found guilty of stealing software from AMSC and was embroiled in a lawsuit with that firm. On 27 June 2013, the United States Department of Justice declared Sinovel and three Individuals were charged with theft of trade secrets. On 24 January 2018 a U.S. federal jury in Madison, Wisconsin found Sinovel guilty of stealing trade secrets from AMSC causing the American company to lose over US$800 million. Prosecutors stated that Sinovel stole trade secrets and copyrighted information with the intention of building its own versions of AMSC systems and retrofit previously built wind turbines so it could avoid having to pay AMSC for its technology. The federal judge ordered Sinovel to pay $59 Million for the act of stealing. According to a press release by AMSC on 03 July 2018, AMSC has settled the lawsuit with Sinovel.\n\n\n"}
{"id": "6819832", "url": "https://en.wikipedia.org/wiki?curid=6819832", "title": "Sodium lauroamphoacetate", "text": "Sodium lauroamphoacetate\n\nSodium lauroamphoacetate is zwitterionic surfactant of the amphoacetate class. It is used as a very mild cleaning agent originally used in shampoos and body washes for infants but it now sees broader use in other personal care products.\n\nSodium lauroamphoacetate is produced in a 2 step process. Firstly lauric acid reacts with aminoethylethanolamine (AEEA); this initially produces the amide however heating causes this to cyclize to give the imidazoline group. This reacts with 1 equivalent of sodium chloroacetate to give the final product. A reaction with 2 equivalents gives the di-acetate, which is also marketed as di-sodium lauroamphoacetate.\n\nSodium lauroamphoacetate is exceedingly mild, and cases of skin irritation are rare but not unheard of. It has been proposed that these instances are caused not by sodium lauroamphoacetate itself but rather by failures in quality control which result in it being contaminated with AEEA.\n"}
{"id": "17837284", "url": "https://en.wikipedia.org/wiki?curid=17837284", "title": "Solar charger", "text": "Solar charger\n\nA solar charger employs solar energy to supply electricity to devices or charge batteries. They are generally portable.\n\nSolar chargers can charge lead acid or Ni-Cd battery banks up to 48 V and hundreds of ampere-hours (up to 4000 Ah) capacity. Such type of solar charger setups generally use an intelligent charge controller. A series of solar cells are installed in a stationary location (ie: rooftops of homes, base-station locations on the ground etc.) and can be connected to a battery bank to store energy for off-peak usage. They can also be used in addition to mains-supply chargers for energy saving during the daytime.\n\nMost portable chargers can obtain energy from the sun only. Some, including the Kinesis K3, and GeNNex Solar Cell 2 can work either way (recharged by the sun or plugged into a wall plug to charge up). Examples of solar chargers in popular use include:\n\nA solar panel can produce a range of charging voltages depending upon sunlight intensity, so a voltage regulator must be included in the charging circuit so as to not over-charge (overvoltage) a device such as a 12 volt car battery.\n\nPortable solar chargers are used to charge cell phones and other small electronic devices on the go. Chargers on the market today use various types of solar panels, ranging from thin film panels with efficiencies from 7-15% (amorphous silicon around 7%, CIGS closer to 15%), to the slightly more efficient monocrystalline panels which offer efficiencies up to 18%.\n\nThe other type of portable solar chargers are those with wheels which enable them to be transported from one place to another and be used by a lot of people. They are semi-public, considering the fact that are used publicly but not permanently installed. A good example of this kind of portable solar charger is the Strawberry Mini device.\n\nThe solar charger industry has been plagued by companies mass-producing low efficiency solar chargers that don't meet the consumer's expectations. This in turn has made it hard for new solar charger companies to gain the trust of consumers. Solar companies are starting to offer high-efficiency solar chargers. When it comes to permanently installed public solar chargers, Strawberry energy Company from Serbia has invented and developed the first public solar charger for mobile devices, Strawberry Tree. Due to a built in rechargeable battery which stores energy, it can function without sunshine or at night. Other companies such as Voltaic Systems, Poweradd and others have started to push better products onto the market as well.\n\nPortable solar power is being utilized in developing countries to power lighting as opposed to utilizing kerosene lamps which are responsible for respiratory infections, lung and throat cancers, serious eye infections, cataracts as well as low birth weights. Solar power provides an opportunity for rural areas to \"leapfrog\" traditional grid infrastructure and move directly to distributed energy solutions.\n\nSome solar chargers also have an on-board battery which is charged by the solar panel when not charging anything else. This allows the user to be able to use the solar energy stored in the battery to charge their electronic devices at night or when indoors.\n\nSolar chargers can also be rollable or flexible and are manufactured using thin film PV technology. Rollable solar chargers may include Li-ion batteries.\n\nCurrently, foldable solar panels are coming down in price to the point that almost anyone can deploy one while at the beach, biking, hiking, or at any outdoor location and charge their cellphone, tablet, computer etc. As advances in the technology continue to be made, new products will be able to be expand into third world countries faster than ever before. Recently, billionaire Elon Musk unveiled a new battery system that may allow off-grid systems that rely solely on solar power to recharge them to be deployed anywhere on the planet, thus possibly ending the need for strictly grid-based energy systems.\n\n"}
{"id": "5574966", "url": "https://en.wikipedia.org/wiki?curid=5574966", "title": "Split cycle engine", "text": "Split cycle engine\n\nThe split-cycle engine is a type of internal combustion engine.\n\nIn a conventional Otto cycle engine, each cylinder performs four strokes per cycle: intake, compression, power, and exhaust. This means that two revolutions of the crankshaft are required for each power stroke. The split-cycle engine divides these four strokes between two paired cylinders: one for intake and compression, and another for power and exhaust. Compressed air is transferred from the compression cylinder to the power cylinder through a crossover passage. Fuel is then injected and fired to produce the power stroke.\n\nThe Backus Water Motor Company of Newark, New Jersey was producing an early example of a split cycle engine as far back as 1891. The engine, of \"a modified A form, with the crank-shaft at the top\", was water-cooled and consisted of one working cylinder and one compressing cylinder of equal size and utilized a hot-tube ignitor system. It was produced in sizes ranging from 1/2 to and the company had plans to offer a scaled-up version capable of or more.\n\nThe Atkinson differential engine was a two piston, single cylinder four-stroke engine that also used a displacer piston to provide the fuel air mixture for use by the power piston. However, the power piston did the compression.\n\nThe twingle engine (U.S. English) or split-single engine (British English) is a twin cylinder (or more) two-stroke engine; more precisely, it has one or more U-tube cylinders that each use a pair of pistons, one in each arm of the U. However, both pistons in each pair are used for power (and the underside of both supplies fuel air mixture, if crankcase scavenging is used), and they only differ in that one piston works the transfer port to provide the fuel air mixture for use in both cylinders and the other piston works the exhaust port, so that the burnt mixture is exhausted via that cylinder. Unlike the Scuderi both cylinders are connected to the combustion chamber. As neither piston works as a displacer piston at all, this engine has nothing whatsoever to do with the split cycle engine apart from a purely coincidental similarity of the names.\n\nThe Scuderi engine is a design of a split-cycle, internal combustion engine invented by Carmelo J. Scuderi. The Scuderi Group, an engineering and licensing company based in West Springfield, Massachusetts and founded by Carmelo Scuderi’s children, said that the prototype was completed and was unveiled to the public on April 20, 2009.\n\nThe Tour Engine is an opposed-cylinder split-cycle internal combustion engine that uses a novel Spool Shuttle Crossover Valve (SSCV) to transfer fuel/air charge from the cold to hot cylinder. The first prototype was completed on June, 2008. Tour Engine was funded by grants from the Israel Ministry of National Infrastructures, Energy and Water Resources and ARPA-E\n\nAnother split-cycle design, using an external combustion chamber, is the Zajac engine .\n\nA split cycle engine invented by New Zealander Rick Mayne used a multitude of small cylinders arranged in a radial arrangement with pistons operated by a Geneva mechanism.\n"}
{"id": "47205252", "url": "https://en.wikipedia.org/wiki?curid=47205252", "title": "Street Charge", "text": "Street Charge\n\nStreet Charge is a solar-powered phone charging station developed by NRG Energy, Inc., designed to grant free phone charging services to pedestrians and tourists. The charging stations can also be modified to emit Wi-Fi signals and display advertisements. The appliance was released in 2013, and the original prototype was tested in Brooklyn, New York.\n\nThere is currently a collaboration between NRG Energy, Inc. and AT&T to implement Street Charge units throughout New York City, which will each offer six USB ports in total. This project will cost around $300,000-500,000. As of 2013 there was a controversy surrounding Street Charge appliances in that there were no plans implemented to protect appliances and users utilizing the device. The concern was that the wiring might be modified by malicious users to steal or wipe data from devices plugged into the device.\n"}
{"id": "6508460", "url": "https://en.wikipedia.org/wiki?curid=6508460", "title": "Tachyons in fiction", "text": "Tachyons in fiction\n\nThe hypothetical particles tachyons have inspired many occurrences of tachyons in fiction. The use of the word in science fiction dates back at least to 1970 when James Blish's Star Trek novel \"Spock Must Die!\" incorporated tachyons into an ill-fated transporter experiment.\nIn general, tachyons are a standby mechanism upon which many science fiction authors rely to establish faster-than-light communication, with or without reference to causality issues. For example, in the \"Babylon 5\" television series, tachyons are used for real-time communication over long distances. Another instance is Gregory Benford's novel \"Timescape\", winner of the Nebula Award, which involves the use of tachyons to transmit a message of salvation back in time. Likewise, John Carpenter's horror film \"Prince of Darkness\" uses tachyons to explain how future humans send messages backward through time to warn the characters of their impending doom. By contrast, Alan Moore's classic comic book limited series \"Watchmen\" features a character who uses \"a squall of tachyons\" broadcasting from space to muddle the mind of the only person on Earth capable of seeing the future.\n\nThe word \"tachyon\" has become widely recognized to such an extent that it can impart a science-fictional \"sound\" even if the subject in question has no particular relation to superluminal travel (compare positronic brain). Classic Anime fans may associate tachyons with the energy source for the wave-motion gun and wave-motion engine in \"Space Battleship Yamato\" (\"Starblazers\" in the United States). Further examples include the \"Tachion Tanks\" of the PC game \"\" and the \"tachyon beam\" of the game \"Master of Orion.\" The space-combat sim \"\" utilizes \"tachyon gates\" for superluminal travel but gives no exact explanation for the technology, and the MMORPG \"Eve Online\" features six types of \"Large Tachyon Lasers\", technically a contradiction since by definition, lasers emit light—photons, not any kind of hypothetical tachyon.\n\n\n\n\n\n\n\n\n\n"}
{"id": "37542612", "url": "https://en.wikipedia.org/wiki?curid=37542612", "title": "Tallinn Power Plant", "text": "Tallinn Power Plant\n\nThe Tallinn Power Plant () is a former power plant located in Tallinn, Estonia. Construction of the power plant was initiated by Volta company and it was decided by the Tallinn City Council in 1912 after the work of special committee established in 1909. The plant was located next to the Tallinn Gas Factory at the location of the former Stuart fortress. The plant was designed by Volta and the architect was Hans Schmidt. Originally it used three Laval-type steam turbines and three electric generators—all produced by Volta. Two coal-fired boilers were manufactured by AS Franz Krull. The power plant was opened on 24 March 1913 and originally it was fired by coal. In 1919–1920 the plant was expanded and transferred to peat and wood. In 1924 the power plant was switched to oil shale. It was the first power plant in the world to employ oil shale as its primary fuel. In 1939, the plant achieved capacity of 22 MW.\n\nIn 1929, a new turbine hall and in 1932 a new switchboard were commissioned. In 1941, the power plant was destroyed by leaving Soviet troops but was restored by 1948. A new flue-gas stack was built. On 9 October 1959, the plant started to operate as combined heat and power plant providing district heating to Tallinn. In 1965, the plant was switched to fuel oil. The plant ceased electricity production on 2 February 1979.\n\nSince 1984, the former power plant boiler house is used as an energy museum. Since 2011, the power plant complex is used for hosting the Tallinn Creative Hub (\"Kultuurikatel\").\n\n"}
{"id": "2970552", "url": "https://en.wikipedia.org/wiki?curid=2970552", "title": "Terra Nova FPSO", "text": "Terra Nova FPSO\n\nTerra Nova is a Floating Production Storage and Offloading Vessel (FPSO) located in the Terra Nova oil and gas field, approximately east-southeast off the coast of Newfoundland, Canada in the North Atlantic Ocean. The Terra Nova field is operated by Suncor Energy Inc., with a 33.9% interest.\n\nThe Terra Nova field is south of the successful Hibernia field and the more recent White Rose field. All three fields are in the Jeanne d'Arc Basin on the eastern edge of the famous Grand Banks fishing territory.\n\n\"Terra Nova\" lost 165,000 liters of oil into the ocean in 2004 because of two mechanical failures. In June 2006, production on \"Terra Nova\" was halted as the platform was sent to Rotterdam for a refit. She returned to the Terra Nova field on 25 September 2006.\n\n"}
{"id": "172348", "url": "https://en.wikipedia.org/wiki?curid=172348", "title": "Thrust-specific fuel consumption", "text": "Thrust-specific fuel consumption\n\nThrust-specific fuel consumption (TSFC) is the fuel efficiency of an engine design with respect to thrust output. \nTSFC may also be thought of as fuel consumption (grams/second) per unit of thrust (kilonewtons, or kN). It is thus thrust-specific, meaning that the fuel consumption is divided by the thrust.\n\nTSFC or SFC for thrust engines (e.g. turbojets, turbofans, ramjets, rocket engines, etc.) is the mass of fuel needed to provide the net thrust for a given period e.g. lb/(h·lbf) (pounds of fuel per hour-pound of thrust) or g/(s·kN) (grams of fuel per second-kilonewton). Mass of fuel is used, rather than volume (gallons or litres) for the fuel measure, since it is independent of temperature. \n\nSpecific fuel consumption of air-breathing jet engines at their maximum efficiency is more or less proportional to speed. The fuel consumption \"per mile\" or \"per kilometre\" is a more appropriate comparison for aircraft that travel at very different speeds. There also exists power specific fuel consumption, which equals speed times the thrust specific fuel consumption. It can have units of pounds per hour per horsepower.\n\nThis figure is inversely proportional to specific impulse.\n\nSFC is dependent on engine design, but differences in the SFC between different engines using the same underlying technology tend to be quite small. Increasing overall pressure ratio on jet engines tends to decrease SFC.\n\nIn practical applications, other factors are usually highly significant in determining the fuel efficiency of a particular engine design in that particular application. For instance, in aircraft, turbine (jet and turboprop) engines are typically much smaller and lighter than equivalently powerful piston engine designs, both properties reducing the levels of drag on the plane and reducing the amount of power needed to move the aircraft. Therefore, turbines are more efficient for aircraft propulsion than might be indicated by a simplistic look at the table below.\n\nSFC varies with throttle setting,altitude and climate. For jet engines, flight speed also has a significant effect upon SFC; SFC is roughly proportional to air speed (actually exhaust velocity), but speed along the ground is also proportional to air speed. Since work done is force times distance, mechanical power is force times speed. Thus, although the nominal SFC is a useful measure of fuel efficiency, it should be divided by speed to get a way to compare engines that fly at different speeds.\n\nFor example, Concorde cruised at 1354 mph, or 7.15 million feet per hour, with its engines giving an SFC of 1.195 lb/(lbf·h) (see below); this means the engines transferred 5.98 million foot pounds per pound of fuel (17.9 MJ/kg), equivalent to an SFC of 0.50 lb/(lbf·h) for a subsonic aircraft flying at 570 mph, which would be better than even modern engines; the Olympus 593 used in the Concorde was the world's most efficient jet engine. However, Concorde ultimately has a heavier airframe and, due to being supersonic, is less aerodynamically efficient, i.e., the lift to drag ratio is far lower. In general, the total fuel burn of a complete aircraft is of far more importance to the customer.\n\nThe following table gives the efficiency for several engines when running at 80% throttle, which is approximately what is used in cruising, giving a minimum SFC. The efficiency is the amount of power propelling the plane divided by the rate of energy consumption. Since the power equals thrust times speed, the efficiency is given by\nwhere V is speed and h is the energy content per unit mass of fuel (probably the lower heating value is used here).\n\n\n"}
{"id": "54990230", "url": "https://en.wikipedia.org/wiki?curid=54990230", "title": "Uno Bolt", "text": "Uno Bolt\n\nUno Bolt is a one-wheel electric self-balancing vehicle. It was first introduced to the public in August 2017 on Kickstarter. The company that manufactures the devie was founded by entrepreneur Sean Chan and is distributed by LED Impressions. The product has been featured in publications such as Men's Journal, Business Insider, and Digital Trends. Uno Bolt will be featured in James Cameron's 2018 movie \"Alita: Battle Angels\".\n"}
{"id": "39218481", "url": "https://en.wikipedia.org/wiki?curid=39218481", "title": "Water capacitor", "text": "Water capacitor\n\nA water capacitor is a device that uses water as its dielectric insulating medium. \n\nA capacitor is a device in which electrical energy is introduced and can be stored for a later time. A capacitor consists of two conductors separated by a non-conductive region. The non-conductive region is called the dielectric or electrical insulator. Examples of traditional dielectric media are air, paper, and certain semiconductors. A capacitor is a self-contained system, is isolated with no net electric charge. The conductors must hold equal and opposite charges on their facing surfaces.\n\nConventional capacitors use materials such as glass or ceramic as their insulating medium to store an electric charge. Water capacitors were created mainly as a novelty item or for laboratory experimentation, and can be made with simple materials.\nWater exhibits the quality of being self-healing; if there is an electrical breakdown through the water, it quickly returns to its original and undamaged state. Other liquid insulators are prone to carbonization after breakdown, and tend to lose their hold off strength over time.\n\nThe drawback to using water is the short length of time it can hold off the voltage, typically in the microsecond to ten microsecond (μs) range. Deionized water is relatively inexpensive and is environmentally safe. These characteristics, along with the high dielectric constant, make water an excellent choice for building large capacitors. If a way can be found to reliably increase the hold off time for a given field strength, then there will be more applications for water capacitors.\n\nWater has been shown not to be a very reliable substance to store electric charge long term, so more reliable materials are used for capacitors in industrial applications. However water has the advantage of being self healing after a breakdown, and if the water is steadily circulated through a de-ionizing resin and filters, then the loss resistance and dielectric behavior can be stabilized. Thus, in certain unusual situations, such as the generation of extremely high voltage but very short pulses, a water capacitor may be a practical solution – such as in an experimental Xray pulser.\n\nA simple type of water capacitor is created by using water filled glass jars and some form of insulating material to cover the ends of the jar. Water capacitors are not widely used in the industrial community due to their large physical size for a given capacitance. The conductivity of water can change very quickly and is unpredictable if left open to atmosphere. Many variables such as temperature, pH levels, and salinity have been shown to alter conductivity in water. As a result, there are better alternatives to the water capacitor in the majority of applications.\n\nThe pulse withstand voltage of carefully purified water can be very high – over 100kV/cm (comparing to about 100mm for the same voltage in dry air).\n\nA capacitor is designed to store electric energy when disconnected from its charging source. Compared to more conventional devices, water capacitors are currently not practical devices for industrial applications. Capacitance can be increased by the addition of electrolytes and minerals to the water, but this increases the self leakage, and cannot be done beyond its saturation point.\n\nModern high voltage capacitors may retain their charge long after power is removed. This charge can cause dangerous, or even potentially fatal, shocks if the stored energy is more than a few joules. At much lower levels, stored energy can still cause damage to connected equipment. Water capacitors, being self discharging, (for totally pure water, only thermally ionized, at the ratio of conductivity to permittivity means that self discharge time is circa 180µs, faster with higher temperatures or dissolved impurities) usually cannot be made store enough residual electrical energy to cause serious bodily injury. \n\nUnlike many large industrial high voltage capacitors, water capacitors do not require oil. Oil found in many older designs of capacitors can be toxic to both animals and humans. If a capacitor breaks open and its oil is released, the oil often finds its way into the water table, which can cause health problems over time.\n\nCapacitors can originally be traced back to a device called a Leyden jar, created by the Dutch physicist Pieter van Musschenbroek. The Leyden jar consisted of a glass jar with tin foil layers on the inside and outside of the jar. A rod electrode was directly connected to the inlayer of foil by means of a small chain or wire. This device stored static electricity created when amber and wool where rubbed together.\n\nAlthough the design and materials used in capacitors have changed greatly throughout history, the basic fundamentals remain the same. In general, capacitors are very simple electrical devices which can have many uses in today's technologically advanced world. A modern capacitor usually consists of two conducting plates sandwiched around an insulator. Electrical researcher Nicola Tesla described capacitors as the \"electrical equivalent of dynamite\".\n\n"}
{"id": "40499854", "url": "https://en.wikipedia.org/wiki?curid=40499854", "title": "X-1R", "text": "X-1R\n\nX-1R is a patented performance lubricant originally developed for the NASA Space Shuttle Program. It acts as a friction reduction modifier within extreme pressure environments. X-1R is a Certified Space Technology and is an inductee in the NASA Space Technology Hall of Fame.\n\n"}
