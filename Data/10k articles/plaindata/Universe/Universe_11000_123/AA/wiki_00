{"id": "1969364", "url": "https://en.wikipedia.org/wiki?curid=1969364", "title": "Activated alumina", "text": "Activated alumina\n\nActivated alumina is manufactured from aluminium hydroxide by dehydroxylating it in a way that produces a highly porous material; this material can have a surface area significantly over 200 m²/g. The compound is used as a desiccant (to keep things dry by absorbing water from the air) and as a filter of fluoride, arsenic and selenium in drinking water. It is made of aluminium oxide (alumina; AlO). It has a very high surface-area-to-weight ratio, due to the many \"tunnel like\" pores that it has. Activated alumina in its phase composition can be represented only by metastable forms (gamma-AlO etc.). Corundum (alpha-AlO), the only stable form of aluminum oxide, does not have such a chemically active surface and is not used as a sorbent.\n\nActivated alumina is used for a wide range of adsorbent and catalyst applications including the adsorption of catalysts in polyethylene production, in hydrogen peroxide production, as a selective adsorbent for many chemicals including arsenic, fluoride, in sulphur removal from fluid streams (Claus Catalyst process).\n\nUsed as a desiccant, it works by a process called \"adsorption\". The water in the air actually sticks to the alumina itself in between the tiny passages as the air passes through them. The water molecules become trapped so that the air is dried out as it passes through the filter. This process is reversible. If the alumina desiccant is heated to ~200 °C, it will release the trapped water. This process is called \"regenerating\" the desiccant.\n\nActivated alumina is also widely used to remove fluoride from drinking water. In the US, there are widespread programs to fluoridate drinking water. However, in certain regions, such as the Rajasthan region of India, there is enough fluoride in the water to cause fluorosis. A study from the Harvard school of Public Health found exposure to fluoride as a child correlated with lower IQ.\n\nAs per researches conducted by V.K.Chhabra {Chief Chemist (retd.) P.H.E.D.Raj.} Activated alumina, when used as a fluoride filter, under field conditions can best be regenerated by a solution of lye (sodium hydroxide; NaOH), sulphuric acid (HSO).\nThe fluoride uptake capacity (FUC) of activated alumina can be up to 5000 mg/kg. The FUC can be determined as follows:\n\n\"V.K.Chhabra’s method (Chief Chemist retired, PHED Raj., India)\": \nFluoride solution: Dissolve 22.1 g anhydrous NaF in distilled water and dilute to 1,000 mL.\n1 mL = 10 mg fluoride.\n10 mL/L = 100 mg/L fluoride.\n\nProcedure:\nTo one litre of simulated distilled water containing 100 mg/L of fluoride, agitate at 100 rpm using the jar test machine. Add 10 g of the AA under test. After one hour, switch off the machine and take out the solution. After 5 minutes, carefully decant the supernatant solution and determine the fluoride. Calculate the difference between the original and treated water fluoride concentration. Multiply the difference by 100 to give the fluoride uptake capacity of AA in mg/kg.\n\nIn high vacuum applications, activated alumina is used as a charge material in \"fore-line traps\" to prevent oil generated by rotary vane pumps from back streaming into the system.\n\nIts mechanical properties and non-reactivity in the biological environment allow it to be a suitable material used to cover surfaces in friction in body prostheses (e.g. hip or shoulder prostheses).\n\n\n1. Ms. Sunita Yadav, Ms. Dimple Sharma and Chhabra V.K. “Defluoridation with Activated Alumina”, J.IWWA, Vol.42 no.3, 210-213\n2. Chhabra V.K., ”Chhabra’s method for determination of FUC of AA”.J.IWWA, Vol.39 No.3,225-229\n3. Tender documents of PHED Raj.\n4. Domestic defluoridation for drinking water using AA by Leela Iyengar and Raj Kumar Daw p-T-2(1) to T-2(6), National workshop on control & mitigation of excess fluoride in drinking water 5-7 Feb.,2004 at Jaipur organized by PHED Raj. & UNICEF.\n5. Leaflet Bhargava Alumina Industries, Gujarat.\n6. Ms. Savita, A.B.Gupta & Chhabra V.K. “Performance Analysis of Community Level Activated Alumina Defluoridation Plants in Dungarpur, Raj.” Journal Hydro 2008.\n7. Ms. Sunita Yadav, Ms. Dimple Sharma and Chhabra V.K. “Innovative Technology for Defluoridation with Activated Alumina” 42nd annual convention of IWWA page 183-187\n8. Jatin Chhabra & Chhabra V.K. “ Country’s first successful community defluoridation plant based on Activated Alumina” Jr.IWWA vol.38 no.1 page 31-36.\n9. Fluorides in water AWWA 1971.\n10. Operation & Control of water treatment processes by Charles R. Cox 1969.\n11. CWC Publication no.27/88. Status report on evaporation control in reservoirs, Central Water Commission, New Delhi Feb. 1988\n12. Water Treatment for Public & IndustriSupply by G.Nikoladze, Mir Publishers, 1989.\n13. Xu Guo Xun “Fluoride removal from drinking water by AA” Aqua Vol.43 no.2 pp58–64 yr.1994.\n14. Paper written by Dr. C. Venkobachar & Dr. Leela Iyengar IIT Kanpur for the workshop on Defluoridation of water using AA, 7-8th March1996 sponsored by UNICEF, New Delhi.\n"}
{"id": "34036", "url": "https://en.wikipedia.org/wiki?curid=34036", "title": "Area rule", "text": "Area rule\n\nThe Whitcomb area rule, also called the transonic area rule, is a design technique used to reduce an aircraft's drag at transonic and supersonic speeds, particularly between Mach 0.75 and 1.2. \n\nThis is one of the most important operating speed ranges for commercial and military fixed-wing aircraft today, with transonic acceleration being considered an important performance metric for combat aircraft and necessarily dependent upon transonic drag.\n\nAt high-subsonic flight speeds, the local speed of the airflow can reach the speed of sound where the flow accelerates around the aircraft body and wings. The speed at which this development occurs varies from aircraft to aircraft and is known as the critical Mach number. The resulting shock waves formed at these points of sonic flow can result in a sudden increase in drag, called wave drag. To reduce the number and power of these shock waves, an aerodynamic shape should change in cross sectional area as smoothly as possible.\n\nThe area rule says that two airplanes with the same longitudinal cross-sectional area distribution have the same wave drag, independent of how the area is distributed laterally (i.e. in the fuselage or in the wing). Furthermore, to avoid the formation of strong shock waves, this total area distribution must be smooth. As a result, aircraft have to be carefully arranged so that at the location of the wing, the fuselage is narrowed or \"waisted\", so that the total area does not change much. Similar but less pronounced fuselage waisting is used at the location of a bubble canopy and perhaps the tail surfaces.\n\nThe area rule also holds true at speeds exceeding the speed of sound, but in this case, the body arrangement is in respect to the Mach line for the design speed. For example, consider that at Mach 1.3 the angle of the Mach cone formed off the body of the aircraft will be at about μ = arcsin(1/M) = 50.3° (where μ is the angle of the Mach cone, or simply Mach angle, and M is the Mach number). In this case the \"perfect shape\" is biased rearward; therefore, aircraft designed for high speed cruise usually have wings towards the rear. A classic example of such a design is the Concorde. When applying the transonic area rule, the condition that the plane defining the cross-section meets the longitudinal axis at the Mach angle μ no longer prescribes a unique plane for μ other than the 90° given by M = 1. The correct procedure is to average over all possible orientations of the intersecting plane.\n\nA superficially related concept is the Sears–Haack body, which is the shape with the minimum wave drag for a given length and a given volume. However, the Sears–Haack body shape is derived starting with the Prandtl–Glauert equation which governs small-disturbance supersonic flows. But this equation is not valid for transonic flows where the area rule applies. So although the Sears–Haack body shape, being smooth, will have favorable wave drag properties according to the area rule, it is not theoretically optimum.\n\nThe area rule was discovered by Otto Frenzl when comparing a swept wing with a w-wing with extreme high wave drag while working on a transonic wind tunnel at Junkers works in Germany between 1943 and 1945. He wrote a description on 17 December 1943, with the title \"Arrangement of Displacement Bodies in High-Speed Flight\"; this was used in a patent filed in 1944. The results of this research were presented to a wide circle in March 1944 by Theodor Zobel at the \"Deutsche Akademie der Luftfahrtforschung\" (German Academy of Aeronautics Research) in the lecture \"Fundamentally new ways to increase performance of high speed aircraft.\" \n\nSubsequent German wartime aircraft design took account of the discovery, evident in slim mid-fuselage of aircraft including the Messerschmitt P.1112, P.1106 and Focke-Wulf 1000x1000x1000 type A long-range bomber, but also apparent in delta wing designs including the Henschel Hs 135. Several other researchers came close to developing a similar theory, notably Dietrich Küchemann who designed a tapered fighter that was dubbed the \"Küchemann Coke Bottle\" when it was discovered by US forces in 1946. In this case Küchemann arrived at the theory by studying airflow, notably spanwise flow, over a swept wing. The swept wing is already an indirect application of the area rule.\n\nWallace D. Hayes, a pioneer of supersonic flight, developed the transonic area rule in publications beginning in 1947 with his Ph.D. thesis at the California Institute of Technology.\nRichard T. Whitcomb, after whom the rule is named, independently discovered this rule in 1952, while working at the NACA. While using the new Eight-Foot High-Speed Tunnel, a wind tunnel with performance up to Mach 0.95 at NACA's Langley Research Center, he was surprised by the increase in drag due to shock wave formation. Whitcomb realized that, for analytical purposes, an airplane could be reduced to a streamlined body of revolution, elongated as much as possible to mitigate abrupt discontinuities and, hence, equally abrupt drag rise. The shocks could be seen using Schlieren photography, but the reason they were being created at speeds far below the speed of sound, sometimes as low as Mach 0.70, remained a mystery. \n\nIn late 1951, the lab hosted a talk by Adolf Busemann, a famous German aerodynamicist who had moved to Langley after World War II. He talked about the behavior of airflow around an airplane as its speed approached the critical Mach number, when air no longer behaved as an incompressible fluid. Whereas engineers were used to thinking of air flowing smoothly around the body of the aircraft, at high speeds it simply did not have time to \"get out of the way\", and instead started to flow as if it were rigid pipes of flow, a concept Busemann referred to as \"streampipes\", as opposed to streamlines, and jokingly suggested that engineers had to consider themselves \"pipefitters\".\n\nSeveral days later Whitcomb had a \"Eureka\" moment. The reason for the high drag was that the \"pipes\" of air were interfering with each other in three dimensions. One does not simply consider the air flowing over a 2D cross-section of the aircraft as others could in the past; now they also had to consider the air to the \"sides\" of the aircraft which would also interact with these streampipes. Whitcomb realized that the shaping had to apply to the aircraft \"as a whole\", rather than just to the fuselage. That meant that the extra cross-sectional area of the wings and tail had to be accounted for in the overall shaping, and that the fuselage should actually be narrowed where they meet to more closely match the ideal.\n\nThe area rule was immediately applied to a number of development efforts. One of the most famous of these developments was Whitcomb's personal work on the re-design of the Convair F-102 Delta Dagger, a U.S. Air Force jet fighter that was demonstrating performance considerably worse than expected. By indenting the fuselage beside the wings, and (paradoxically) adding more volume to the rear of the plane, transonic drag was considerably reduced and the intended Mach 1.2 design speed was reached. The culminating design of this research was the Convair F-106 Delta Dart, an aircraft which for many years was the USAF's primary all-weather interceptor.\n\nNumerous designs of the era were likewise modified in this fashion, either by adding new fuel tanks or tail extensions to smooth out the profile. The Tupolev Tu-95 'Bear', a Soviet-era bomber, has large bulged landing gear nacelles behind the two inner engines, increasing the aircraft's overall cross section aft of the wing root. Its airliner version has been the fastest propeller-driven aircraft in the world since 1960. The Convair 990 used a similar solution, adding bumps called antishock bodies to the trailing edge of the upper wing. The 990 remains the fastest U.S. airliner in history, cruising at up to Mach 0.89. Designers at Armstrong-Whitworth took the concept a step further in their proposed M-Wing, in which the wing was first swept forward and then to the rear. This allowed the fuselage to be narrowed on either side of the root instead of just behind it, leading to a smoother fuselage that remained wider on average than one using a classic swept wing.\n\nOne interesting outcome of the area rule is the shaping of the Boeing 747's upper deck. The aircraft was designed to carry standard intermodal containers in a two-wide, two-high stack on the main deck, which was considered a serious accident risk for the pilots if they were located in a cockpit at the front of the aircraft. They were instead moved above the deck in a small \"hump\", which was designed to be as small as possible given normal streamlining principles. It was later realized that the drag could be reduced much more by lengthening the hump, using it to reduce wave drag offsetting the tail surface's contribution. The new design was introduced on the 747-300, improving its cruise speed and lowering drag, with the side effect of slightly increasing capacity on passenger flights.\n\nAircraft designed according to Whitcomb's area rule (such as the Blackburn Buccaneer and the Northrop F-5) looked odd at the time they were first tested and were dubbed \"flying Coke bottles,\" but the area rule is effective and came to be an expected part of the appearance of any transonic aircraft. Later designs started with the area rule in mind, and came to look much more pleasing. Although the rule still applies, the visible fuselage \"waisting\" can only be seen on a few aircraft, such as the B-1B Lancer, Learjet 60, and the Tupolev Tu-160 'Blackjack'. The same effect is now achieved by careful positioning of aircraft components, like the boosters and cargo bay on rockets; the jet engines in front of (and not directly below) the wings of the Airbus A380; the jet engines behind (and not purely at the side of) the fuselage of a Cessna Citation X; the shape and location of the canopy on the F-22 Raptor; and the image of the Airbus A380 above showing obvious area rule shaping at the wing root, which is practically invisible from any other angle.\n\n\n"}
{"id": "3633926", "url": "https://en.wikipedia.org/wiki?curid=3633926", "title": "Automotive engine", "text": "Automotive engine\n\nAs of 2013 there were a wide variety of propulsion systems available or potentially available for automobiles and other vehicles. Options included internal combustion engines fueled by petrol, diesel, propane, or natural gas; hybrid vehicles, plug-in hybrids, fuel cell vehicles fueled by hydrogen and all electric cars. Fueled vehicles seemed to have the short term advantage due to the limited range and high cost of batteries. Some options required construction of a network of fueling or charging stations. With no compelling advantage for any particular option car makers pursued parallel development tracks using a variety of options. Reducing the weight of vehicles was one strategy being employed.\n\nThe use of high-technology (such as electronic engine control units) in advanced designs resulting from substantial investments in development research by European countries and Japan seemed to give an advantage to them over Chinese automakers and parts suppliers who, as of 2013, had low development budgets and lacked capacity to produce parts for high-tech engine and power train designs.\n\nThe chief characteristic of an automotive engine (compared to a stationary engine or a marine engine) is a high power-to-weight ratio. This is achieved by using a high rotational speed. However, automotive engines are sometimes modified for marine use, forming a marine automobile engine.\n\nIn the early years, steam engines and electric motors were tried, but with limited success. In the 20th century, the internal combustion (ic) engine became dominant. In 2015, the internal combustion engine remains the most widely used but a resurgence of electricity seems likely because of increasing concern about ic engine exhaust gas emissions.\n\nAs of 2017, the majority of the cars in the United States are gasoline powered. In the early 1900s, the internal combustion engines faced competition from steam and electric engines. The internal combustion engines of the time was powered by gasoline. Internal combustion engines function with the concept of a piston being pushed by the pressure of a certain explosion. This explosion is burning the hydrocarbon within the cylinder head an engine. Out of all the cars manufactured during the time, only around one fourth are actually considered internal combustion. Within the next couple of years, the internal combustion engine came out to become the most popular automotive engine. Sometime within the 19th century, Rudolf Diesel invented a new form of internal combustion power, using a concept of injecting liquid fuel into air heated solely by compression. This is the predecessor to the modern diesel engine used in automobiles, but more specifically, heavy duty vehicles such as semi-trucks.\n\nPetrol engines quickly became the choice of manufacturers and consumers alike. Despite the rough start, noisy and dirty engine, and the difficult gear shifting, new technologies such as the production line and the advancement of the engine allowed the standard production of the gas automobiles. This is the start, from the invention of the gas automobile in 1876, to the beginning of mass production in the 1890s. Henry Ford’s Model T drove down the price of cars to a more affordable price. At the same time, Charles Kettering invented an electric starter, allowing the car to be more efficient than the mechanical starter. The abundance of fuel propelled gas automobiles to be the highly capable and affordable. The demand of gasoline rose from 3 billion barrels in 1919 to around 15 billion in 1929.\n\nAn internal combustion engine is a motor that is powered by the expansion of gas which is created by the combustion of hydrocarbon gases fuels. To elaborate, an internal combustion used the heat of a combustion created by the injected hydrocarbon fuel to create mechanical motion. At the time of the early 1900s, wood alcohol was a popular fuel for French and German automobiles, but as governments imposed large taxes on the production, the price of wood alcohol rose above that of gasoline. Gasoline engines became popular as a result of this, as internal combustion engines were commonly known as gasoline engines. Although gasoline engines became popular, they were not particularly desirable due to the dangers of fuel leaks that may cause explosions. Therefore, many inventors attempted to create a kerosene burning engine as a result. This was not a successful venture applying it for automotive usage. There are many different types of fuels for internal combustion engines. These include diesel, gasoline, and ethanol.\n\nThe steam engine was invented in the late 1700s, and the primary method of powering engines and soon, locomotives. One of the most popular steam automobile was the “Stanley Steamer,” offering low pollution, power, and speed. The downside of these steam automobiles is the unreliability, complexity, and the frequent accidents that occurred with them. The startup time for a steam car may take up to 45 minutes, defeating the purpose of faster transportation. By the time the steam automobile was improved, the complexity of manufacturing relative to the gas automobiles made steam automobiles unprofitable.\n\nA steam engine is a device which transforms heat into mechanical motion. This is provided with the usage of boilers, which create steam by boiling water. In the early 1900s, Abner Doble introduced a steam powered car in the United States which had capabilities that could potentially overpower Ford’s Model T in efficiency. Steam has been known to have very efficient fuel economy with a high power source. That is why half the world was powered by steam for almost the entirety of the 19th century and almost half the 20th century. The main drawback of the steam engine in automobiles was the fact that operators were required to have full knowledge of boilers and steam engines before operating, as it was detrimental to the engine itself if the operator neglected it.\n\nElectric vehicles seemed to be the most viable option, similar to the steam automobiles. It was first invented in the early 1800s, and became a viable option of transportation around 1890, when William Morrison created the first electric car that traveled 14 miles per hour. The electric cars offered low pollution and a soundless ride, unlike their gasoline counterparts. The greatest downside of electric cars was the range. The typical electric car could reach around 20 miles before requiring a recharge. Manufacturers could not increase the number of batteries, due to the bulkiness of the batteries at the time. Without an incentive to purchase the electric automobiles, gas automobiles was the most viable option at the time.\n\nElectric cars use batteries to store electricity which is used to power electric motors. The battery delivers the power to the motor, which is either AC or DC. The difference between AC and DC motors is the sort of system that is required to run it in an electric vehicle. An AC motor is generally cheaper but the components required to run it in an electric vehicle such as the controller and inverter makes it more expensive than the DC motor. A unique feature of electric vehicles compared to its gasoline counterparts, the electric vehicle is more simple than the gasoline vehicle. The electric vehicle bypasses the gasoline car components such as the crankshaft which allows it to generate power much faster than gasoline. Because of the faster transfer of power, the electric vehicle is able to accelerate faster than gasoline cars.\n\nIn the 1970s, the electric vehicle made its reappearance because of the 1973 OPEC Oil Embargo. Previously, the abundant gasoline had become the prime source of fuel for vehicles. But after the shortage, manufacturers began looking towards electric vehicles again. Despite the improved technology from the 1800s, the electric vehicles faced similar technological flaws such as limited mileage and speed. They could only travel up to 45 miles per hour and had a range of approximately 40 miles.\n"}
{"id": "34925883", "url": "https://en.wikipedia.org/wiki?curid=34925883", "title": "Autumn 2000 Western Europe floods", "text": "Autumn 2000 Western Europe floods\n\nThe Autumn of 2000 was the wettest recorded in the United Kingdom since records began in 1766.\n\nSeveral regions of Atlantic Europe from France to Norway received double their average rainfall and there were severe floods and landslides in the southern Alps. In October and November 2000 a successive series of extratropical cyclones caused severe flooding across the UK.\n\nThe United Kingdom saw the most extensive nationwide flooding event since the snow-melt of 1947. Prior to 1947, three similar events occurred in the second half of the 19th century where prolonged rainfall led to widespread flooding throughout England in the month of November, namely 1894, 1875, and 1852.\n\nThe combined effect of the storms across Western Europe caused flooding throughout the United Kingdom. Two storm events (Nicole and Oratia) 28 November to 3 November, and the storm Rebekka from 4 November, resulted in continuous flooding. 10,000 homes were flooded in 700 locations. Peak flows on the Rivers Thames, Trent, Severn, Wharfe and Dee were the highest for 60 years. The River Ouse in Yorkshire reached the highest level since the 17th century.\n\nIn the United Kingdom a series of severe floods affected large parts of the country in the Autumn of 2000. The worst affected areas were Kent and Sussex during October and Shropshire, Worcestershire and Yorkshire in November. The Autumn of 2000 was the wettest on record in the England and Wales precipitation record with several major rainfall events causing flooding in different parts of the country during October and November. England and Wales had an average of 503 mm of rain from September–November exceeding the previous record by nearly 50 mm.\n\nA succession of slow-moving low pressure systems crossed the UK during Autumn 2000 associated with the jet stream being in a more southerly position than average. The flooding in Kent and Sussex resulted from a succession of thunderstorms passing along a near-stationary front. Much of the rock in this area is impermeable and there had already been significant rainfall in the south-east allowing for increased surface flow and river levels. Several fronts passed over central and northern England in the following weeks causing flooding in Shropshire, Worcestershire and Yorkshire.\n\nThe rainfall for this period in the three preceding years had been above the 1961-1990 average. The previous spring was unsettled, with April and May particularly wet, which increased the aquifer recharge season. Heavy rain was also seen in June, leading to high river levels and some flooding in Yorkshire.\n\nSeptember 2000 was generally unsettled, with wet periods between 14-19. This resulted in some flooding on 15 September around Portsmouth and Southsea as a pumping station at Eastney failed after of rain fell in 4.5 hours, being the heaviest rain since 1986 in the area. Total rainfall was also measured at 65mm in Havant making this a 1 in 108 year storm event.\nStorms affected Flanders in Belgium 15 September, with tornados reported in the municipalities of Zwalm, Antwerp and Erpe-Mere. Flooding affected the regions of Ghent and Kortrijk. \n24 out of 27 UK Met Office regions except northern Scotland received higher than normal rainfall during the month, making this the wettest September since 1981.\n\nEx-Hurricane Isaac crossed the Atlantic with eye still visible on 2 October and lashed the west of the British isles with near gale force winds on 3 October, before merging with another extra-tropical low on 4 October north of Scotland. Early October 2000 brought more than the monthly average rainfall in the first 10 days to the southeast of England.\n\nA complex of low pressures, named Heidrun & Imke by FU Berlin formed from the remnants of Tropical Storm Leslie. This formed at 30˚N 76˚W on 5 October, swept westwards and merged with a front on 7 October, and then reintensified to become a storm south of Great Britain with winds of 40-50 knots reported in the Bay of Biscay. An area of convective storms stalled over Sussex and Kent.\n\n\nHomes in Yalding and Maidstone were flooded, however there had been fears that a high tide might lead to the River Medway bursting its banks. This threat passed preventing much more widespread damage. Evacuations took place in some villages in the county.\n\nOn 12 October many roads were flooded across both West and East Sussex including the A21 and A22. A lifeboat crew rescued 20 people trapped in a supermarket in Uckfield and others were rescued by helicopter.\n\nShrewsbury, Ironbridge and Bridgnorth flooded as the Severn breached its banks and reached its highest levels in 53 years.\n\nThe Severn breached its banks in many parts of the county, including at Bewdley, Worcester and Upton-upon-Severn. In Worcester, the Severn peaked on 3 November at its highest level in 53 years. The river remained in flood for several days however and the main road bridge in the city was closed. Homes were flooded in Diglis as well as many businesses on the city's waterfront and the cricket ground. In Bewdley, the floods led to renewed calls for flood defences in the town. These were completed in 2006 and have since reduced the impact of flooding on the town.\n\nFlooding affected York during the summer and autumn of 2000 as the River Ouse reached its highest levels since records began. The floods cost the city council in excess of £1 million and 40 people had to be moved from their homes. The floods were the worst in 375 years; more than 300 homes were flooded and the army were called in to help with flood relief efforts.\n\nIn 2011 a team of climate scientists from the University of Oxford, the Met Office, ETH Zurich, the National Institute of Environmental Studies in Japan and Risk Management Solutions Ltd. published an article on the role of man-made greenhouse gas emissions in the Autumn 2000 floods in the UK. The risk of major flooding occurring during October and November 2000 was estimated to have increased due to anthropogenic greenhouse gas emissions. The team used thousands of model simulations run on the personal computers of members of the public through the Climateprediction.net project.\n\nWarm sea surface temperatures in the English Channel and Norway, along with an abnormally warm North Atlantic, added moisture and energy to weather systems as they crossed the UK. September was the wettest since 1981, October the wettest since 1903 and November the wettest since 1970. Overall the autumn of 2000 was the wettest since 1872, and more rain fell in September, October and November than in any other 3-month period since rainfall records began in 1727. Climatologically it was calculated that Autumn 2000 was a 1-in-500-year event, assuming a static climate.\n\nDefra commissioned an independent review by the Institution of Civil Engineers under George Fleming. The review was to consider methods of estimating and reducing flood risk and look at whether flood risk management could make more use of natural processes. Other terms of reference included the possible impact of climate change and experience of other countries. The resulting report entitled \"Learning to Live with Rivers\" specifically criticised a reluctance to use computer models and inadequate representation of the dynamic effects of land use, catchment processes and climatic variability. More broadly, the report noted that sustainable flood risk management could only be achieved by working with the natural response of the river basin and by providing the necessary storage, flow reduction and discharge capacity. It concluded that floods can only be managed, not prevented, and the community must learn to live with rivers.\n\nThe report found that damage was reduced by flood defences and by timely warnings and evacuations where the defences could not hold back the water. As a result, 280,000 properties were protected from the floods, but over 10,000 properties were still flooded at an estimated cost of £1 billion.\n\n"}
{"id": "58207196", "url": "https://en.wikipedia.org/wiki?curid=58207196", "title": "Ballistic table", "text": "Ballistic table\n\nA ballistic table or ballistic chart is a tool which predicts the trajectory of a projectile, and is used to compensate for physical effects in order to increase the probability of the projectile reaching the intended target. Ballistic tables are used in hunting, sport shooting, military and scientific applications. Corrections in ballistic tables are given relative to a zero range. Ballistic charts are often given in angular measurements, with units in either milliradians (mil) or minutes of arc (moa). The tables are usually generated using computer programs built on mathematical functions. The number of inputs to the ballistic calculator can sometimes vary depended on the specific generator, or the user may choose to only input certain variables.\n\nFor example, a very simple drop table can be made using inputs for the sight adjustment value (in mil or moa), the zero range, intended target ranges, muzzle velocity, caliber, ballistic coefficient and bullet weight. Some of the environmental effects that play a role in calculating the trajectory are gravity, projectile spin, wind, temperature, air pressure and humidity. More advanced tables can take more factors into account to ensure a more accurate prediction of the trajectory, which becomes increasingly important on longer ranges. Some of these variables may have a negligible effect on shorter ranges.\n\n\n"}
{"id": "52103570", "url": "https://en.wikipedia.org/wiki?curid=52103570", "title": "Bangladesh Petroleum Institute", "text": "Bangladesh Petroleum Institute\n\nBangladesh Petroleum Institute or BPI is an autonomous national research institute that carries out research on hydrocarbon and provide technical assistance to organizations in the petroleum industry and is located in Uttara, Dhaka, Bangladesh.\n\nThe institute was created on January 1981, it is under the Ministry of Energy and Mineral Resources of Bangladesh. In 2004 Bangladesh Petroleum Institute act was passed by the parliament which made allocated more resource to skill development in the oil,gas and petroleum industry. \n"}
{"id": "1519526", "url": "https://en.wikipedia.org/wiki?curid=1519526", "title": "Biantitropical distribution", "text": "Biantitropical distribution\n\nBiantitropical (or amphitropical) distribution refers to the pattern of species that exist at comparable latitudes across the equator but not in the tropics. For example, a species may be found north of the Tropic of Cancer and south of the Tropic of Capricorn, but not in between. This usually has to do with the optimal temperature for the species existing at both latitudes. How the life forms distribute themselves to the opposite hemisphere when they can't normally survive in the middle depends on the species; plants may have their seed spread through wind, animal, or other methods (dispersal) and then germinate upon reaching the appropriate climate, while sea life may be able to travel through the tropical regions in a larval state or by going through deep ocean currents with much colder temperatures than on the surface.\n\n\n"}
{"id": "30373180", "url": "https://en.wikipedia.org/wiki?curid=30373180", "title": "Carlos Alberto Ricardo", "text": "Carlos Alberto Ricardo\n\nCarlos Alberto Ricardo is a Brazilian environment pioneer. He was awarded the Goldman Environmental Prize in 1992, for his contribution to environment policy in Brazil.\n"}
{"id": "25399724", "url": "https://en.wikipedia.org/wiki?curid=25399724", "title": "Cetyl palmitate", "text": "Cetyl palmitate\n\nCetyl palmitate is the ester derived from palmitic acid and cetyl alcohol. This white waxy solid is the primary constituent of spermaceti, the once highly prized wax found in the skull of sperm whales. Cetyl palmitate is a component of some solid lipid nanoparticles.\n\nStony corals, which build the coral reefs, contain large amounts of cetyl palmitate wax in their tissues, which may function in part as an antifeedant. \n"}
{"id": "55549720", "url": "https://en.wikipedia.org/wiki?curid=55549720", "title": "Chinet", "text": "Chinet\n\nChinet is an American paper goods company established in the 1930s that produces disposable plates, bowls, napkins, and plastic cups. The company is owned by Huhtamaki and their headquarters are in De Soto, Kansas.\n\nChinet began offering compostable plate products in 2013.\n"}
{"id": "50019576", "url": "https://en.wikipedia.org/wiki?curid=50019576", "title": "Coex (material)", "text": "Coex (material)\n\nCoex is a biopolymer with flame-retardant properties derived from the functionalization of cellulosic fibers such as cotton, linen, jute, cannabis, coconut, ramie, bamboo, raffia palm, stipa, abacà, sisal, nettle and kapok. The treatment effectiveness was also proven on wood and semi-synthetic fibers such as cellulose acetate, cellulose triacetate, viscose, modal, lyocell and cupro.\n\nThe material is obtained by sulfation and phosphorylation reactions on glucan units linked to each other in position 1,4 and in particular on the secondary and tertiary hydroxyl groups of cellulosic biopolymer. The chemical modification of the fibers does not involve physical and visual alterations compared to the starting material.\n\nin 2015 the World Textile Information Network (WTiN) declared Coex the winner of the \"Future material award\" as best innovation for the Home Textile category.\n\nCoex preserves the physical and chemical characteristics of the raw material to which is applied. The main features of Coex materials are comfort, hydrophilicity, antistatic properties, mechanical resistance and a great versatility in the textile sector, like all natural and semi-synthetic cellulosic fibers.\n\nThis materials are resistant to moths, mildew and sunlight. The main feature of biopolymers Coex is the flame resistance, they work with fireproof action, creating a barrier to the flames and not simply delaying the fire. Through molecular modification fibres carbonize and therefore extinguish the flame. The resulting products are hypoallergenic and biodegradable.\n\n"}
{"id": "6415891", "url": "https://en.wikipedia.org/wiki?curid=6415891", "title": "Cyclopropane fatty acid", "text": "Cyclopropane fatty acid\n\nCyclopropane fatty acids (CPA) are a subgroup of fatty acids that contain a cyclopropane group. Although they are usually rare, the seed oil from lychee contains nearly 40% CPAs in the form of triglycerides.\nCPAs are derived from unsaturated fatty acids by cyclopropanation. The methylene donor is a methyl group on S-adenosylmethionine (SAM). The conversion is catalyzed by cyclopropane-fatty-acyl-phospholipid synthase. The mechanism is proposed to involve transfer of a CH group from SAM to the alkene, followed by deprotonation of the newly attached methyl group and ring closure. \n\nCycloprop\"e\"ne fatty acids are even rarer than CPAs. The best-known examples are malvalic acid and sterculic acid. Sterculic acid as its triglyceride is present in sterculia oils and at low levels in kapok seed oil (~12%), cottonseed oil (~1%), and in the seeds of the tree \"Sterculia foetida\" (~65-78%). These acids are highly reactive but the cyclopropene ring is destroyed during refining and hydrogenation of the oils. They have attracted interest because they reduce levels of the enzyme stearoyl-CoA 9-desaturase (SCD), which catalyzes the biodesaturation of stearic acid to oleic acid.\n\n"}
{"id": "8531714", "url": "https://en.wikipedia.org/wiki?curid=8531714", "title": "Electric Power Research Institute", "text": "Electric Power Research Institute\n\nThe Electric Power Research Institute, Inc. or EPRI, is an American independent, nonprofit organization that conducts research and development related to the generation, delivery, and use of electricity to help address challenges in electricity, including reliability, efficiency, affordability, health, safety, and the environment.\n\nEPRI's principal offices and laboratories are located in Palo Alto, California; Charlotte, N.C.; Knoxville, TN; and Lenox, MA.\n\nIn November 1965, the Great Northeastern Blackout left 30 million people in the United States without electricity. Historic in scale and impact, it demonstrated the nation's growing dependence upon electricity and its vulnerability to power loss. The event marked a watershed moment for the U.S. electricity sector and triggered the creation of the Electric Power Research Institute.\n\nFollowing the blackout, leaders in Congress held hearings in the early 1970s about the lack of research supporting the power industry.\n\nDr. Chauncey Starr, then the Dean of the UCLA School of Engineering and Applied Science, led the initiative, proposed by Congress, to create an independent research and development organization to support the electricity sector and address its technical and operational challenges. In 1972, at a formal hearing of the U.S. Senate Commerce Committee, Starr presented a vision for the Electric Power Research Institute to serve Congress's mandate for objective, scientific research. Starr served as the first President of EPRI for five years and formally retired at age 65, but continued to work at EPRI for the next 30 years.\n\nAccording to EPRI's 2018 Research Portfolio, EPRI's work encompasses research in technology, the workforce, operations, systems planning and other areas that guide and support the development of new regulatory frameworks, market opportunities, and value to energy consumers.\n\nEPRI's energy and environment research focuses on providing tools, technology, analysis, and guidance for environmentally sound planning and safe operation of existing generation, transmission, and distribution assets. The R&D is meant to support energy and environmental needs related to strategic sustainability science, electrification, integrated energy planning and environmental aspects of renewables and distributed energy resources.\n\nEPRI's generation research focuses on information, processes and technologies to improve the flexibility, reliability, performance, and efficiency of the existing fossil-fueled and renewable energy generating fleet.\n\nEPRI conducts research on nuclear cost-effective technologies, technical guidance, and knowledge transfer tools to help maximize the value of existing nuclear assets and inform the deployment of new nuclear technology.\n\nEPRI's distributed energy resources and customer research area focuses on distributed energy resource (DER) integration, efficient electrification, connectivity and information technology enabling an integrated grid and cyber security guidance.\n\nThe transmission, distribution, and substation research focuses on improving transmission asset management analytics, technology for mobile field guides, robotics and sensors to automate asset inspections, and improving understanding of electromagnetic pulse (EMP).\n\nEPRI researches and develops early-stage and breakthrough technologies that could lead to promising concepts, new knowledge, and potential breakthroughs.\n\n\n"}
{"id": "23692999", "url": "https://en.wikipedia.org/wiki?curid=23692999", "title": "Fangjiashan Nuclear Power Plant", "text": "Fangjiashan Nuclear Power Plant\n\nFangjiashan Nuclear Power Plant (方家山核电站) is a nuclear power plant in the Zhejiang province, China, adjacent to the existing Qinshan Nuclear Power Plant. \nTwo 1,080 megawatt (MWe) CPR-1000 pressurized water reactors (PWRs) are under construction, at a total cost of 26 billion yuan (US$3.8 billion).\n\n'First concrete' for the first unit at the Fangjiashan plant was poured on 26 December 2008. \nConstruction of the second unit followed in July 2009. \nThe reactors were first connected to the grid December 2014 and January 2015, respectively.\n\nIn December 2014 it was announced that units 3 and 4 would be of the Hualong-1 (updated CPR-1000) design.\n\n"}
{"id": "2684988", "url": "https://en.wikipedia.org/wiki?curid=2684988", "title": "Fluid mechanics", "text": "Fluid mechanics\n\nFluid mechanics is the branch of physics concerned with the mechanics of fluids (liquids, gases, and plasmas) and the forces on them. It has applications in a wide range of disciplines, including mechanical, civil, chemical and biomedical engineering, geophysics, astrophysics, and biology. It can be divided into fluid statics, the study of fluids at rest; and fluid dynamics, the study of the effect of forces on fluid motion. It is a branch of continuum mechanics, a subject which models matter without using the information that it is made out of atoms; that is, it models matter from a \"macroscopic\" viewpoint rather than from \"microscopic\". Fluid mechanics, especially fluid dynamics, is an active field of research, typically mathematically complex. Many problems are partly or wholly unsolved, and are best addressed by numerical methods, typically using computers. A modern discipline, called computational fluid dynamics (CFD), is devoted to this approach. Particle image velocimetry, an experimental method for visualizing and analyzing fluid flow, also takes advantage of the highly visual nature of fluid flow.\n\nThe study of fluid mechanics goes back at least to the days of ancient Greece, when Archimedes investigated fluid statics and buoyancy and formulated his famous law known now as the Archimedes' principle, which was published in his work \"On Floating Bodies\"—generally considered to be the first major work on fluid mechanics. Rapid advancement in fluid mechanics began with Leonardo da Vinci (observations and experiments), Evangelista Torricelli (invented the barometer), Isaac Newton (investigated viscosity) and Blaise Pascal (researched hydrostatics, formulated Pascal's law), and was continued by Daniel Bernoulli with the introduction of mathematical fluid dynamics in \"Hydrodynamica\" (1739).\n\nInviscid flow was further analyzed by various mathematicians Jean le Rond d'Alembert, Joseph Louis Lagrange, Pierre-Simon Laplace, Siméon Denis Poisson) and viscous flow was explored by a multitude of engineers including Jean Léonard Marie Poiseuille and Gotthilf Hagen. Further mathematical justification was provided by Claude-Louis Navier and George Gabriel Stokes in the Navier–Stokes equations, and boundary layers were investigated (Ludwig Prandtl, Theodore von Kármán), while various scientists such as Osborne Reynolds, Andrey Kolmogorov, and Geoffrey Ingram Taylor advanced the understanding of fluid viscosity and turbulence.\n\nFluid statics or hydrostatics is the branch of fluid mechanics that studies fluids at rest. It embraces the study of the conditions under which fluids are at rest in stable equilibrium; and is contrasted with fluid dynamics, the study of fluids in motion. Hydrostatics offers physical explanations for many phenomena of everyday life, such as why atmospheric pressure changes with altitude, why wood and oil float on water, and why the surface of water is always level and horizontal whatever the shape of its container. Hydrostatics is fundamental to hydraulics, the engineering of equipment for storing, transporting and using fluids. It is also relevant to some aspect of geophysics and astrophysics (for example, in understanding plate tectonics and anomalies in the Earth's gravitational field), to meteorology, to medicine (in the context of blood pressure), and many other fields.\n\nFluid dynamics is a subdiscipline of fluid mechanics that deals with fluid flow—the science of liquids and gases in motion. Fluid dynamics offers a systematic structure—which underlies these practical disciplines—that embraces empirical and semi-empirical laws derived from flow measurement and used to solve practical problems. The solution to a fluid dynamics problem typically involves calculating various properties of the fluid, such as velocity, pressure, density, and temperature, as functions of space and time. It has several subdisciplines itself, including aerodynamics (the study of air and other gases in motion) and hydrodynamics (the study of liquids in motion). Fluid dynamics has a wide range of applications, including calculating forces and movements on aircraft, determining the mass flow rate of petroleum through pipelines, predicting evolving weather patterns, understanding nebulae in interstellar space and modeling explosions. Some fluid-dynamical principles are used in traffic engineering and crowd dynamics.\n\nFluid mechanics is a subdiscipline of continuum mechanics, as illustrated in the following table.\n\nIn a mechanical view, a fluid is a substance that does not support shear stress; that is why a fluid at rest has the shape of its containing vessel. A fluid at rest has no shear stress.\n\nThe assumptions inherent to a fluid mechanical treatment of a physical system can be expressed in terms of mathematical equations. Fundamentally, every fluid mechanical system is assumed to obey:\nFor example, the assumption that mass is conserved means that for any fixed control volume (for example, a spherical volume)—enclosed by a control surface—the rate of change of the mass contained in that volume is equal to the rate at which mass is passing through the surface from \"outside\" to \"inside\", minus the rate at which mass is passing from \"inside\" to \"outside\". This can be expressed as an equation in integral form over the control volume.\n\nThe is an idealization of continuum mechanics under which fluids can be treated as continuous, even though, on a microscopic scale, they are composed of molecules. Under the continuum assumption, macroscopic (observed/measurable) properties such as density, pressure, temperature, and bulk velocity are taken to be well-defined at \"infinitesimal\" volume elements—small in comparison to the characteristic length scale of the system, but large in comparison to molecular length scale. Fluid properties can vary continuously from one volume element to another and are average values of the molecular properties. The continuum hypothesis can lead to inaccurate results in applications like supersonic speed flows, or molecular flows on nano scale. Those problems for which the continuum hypothesis fails, can be solved using statistical mechanics. To determine whether or not the continuum hypothesis applies, the Knudsen number, defined as the ratio of the molecular mean free path to the characteristic length scale, is evaluated. Problems with Knudsen numbers below 0.1 can be evaluated using the continuum hypothesis, but molecular approach (statistical mechanics) can be applied for all ranges of Knudsen numbers.\n\nThe Navier–Stokes equations (named after Claude-Louis Navier and George Gabriel Stokes) are differential equations that describe the force balance at a given point within a fluid. For an incompressible fluid with vector velocity field formula_1, the Navier–Stokes equations are\n\nThese differential equations are the analogues for deformable materials to Newton's equations of motion for particles – the Navier–Stokes equations describe changes in momentum (force) in response to pressure formula_3 and viscosity, parameterized by the kinematic viscosity formula_4 here. Occasionally, body forces, such as the gravitational force or Lorentz force are added to the equations.\n\nSolutions of the Navier–Stokes equations for a given physical problem must be sought with the help of calculus. In practical terms only the simplest cases can be solved exactly in this way. These cases generally involve non-turbulent, steady flow in which the Reynolds number is small. For more complex cases, especially those involving turbulence, such as global weather systems, aerodynamics, hydrodynamics and many more, solutions of the Navier–Stokes equations can currently only be found with the help of computers. This branch of science is called computational fluid dynamics.\n\nAn inviscid fluid has no viscosity, formula_5. In practice, an inviscid flow is an idealization, one that facilitates mathematical treatment. In fact, purely inviscid flows are only known to be realized in the case of superfluidity. Otherwise, fluids are generally viscous, a property that is often most important within a boundary layer near a solid surface, where the flow must match onto the no-slip condition at the solid. In some cases, the mathematics of a fluid mechanical system can be treated by assuming that the fluid outside of boundary layers is inviscid, and then matching its solution onto that for a thin laminar boundary layer.\n\nFor fluid flow over a porous boundary, the fluid velocity can be discontinuous between the free fluid and the fluid in the porous media (this is related to the Beavers and Joseph condition). Further, it is useful at low subsonic speeds to assume that a gas is incompressible—that is, the density of the gas does not change even though the speed and static pressure change.\n\nA Newtonian fluid (named after Isaac Newton) is defined to be a fluid whose shear stress is linearly proportional to the velocity gradient in the direction perpendicular to the plane of shear. This definition means regardless of the forces acting on a fluid, it \"continues to flow\". For example, water is a Newtonian fluid, because it continues to display fluid properties no matter how much it is stirred or mixed. A slightly less rigorous definition is that the drag of a small object being moved slowly through the fluid is proportional to the force applied to the object. (Compare friction). Important fluids, like water as well as most gases, behave—to good approximation—as a Newtonian fluid under normal conditions on Earth.\n\nBy contrast, stirring a non-Newtonian fluid can leave a \"hole\" behind. This will gradually fill up over time—this behaviour is seen in materials such as pudding, oobleck, or sand (although sand isn't strictly a fluid). Alternatively, stirring a non-Newtonian fluid can cause the viscosity to decrease, so the fluid appears \"thinner\" (this is seen in non-drip paints). There are many types of non-Newtonian fluids, as they are defined to be something that fails to obey a particular property—for example, most fluids with long molecular chains can react in a non-Newtonian manner.\n\nThe constant of proportionality between the viscous stress tensor and the velocity gradient is known as the viscosity. A simple equation to describe incompressible Newtonian fluid behaviour is\n\nwhere\n\nFor a Newtonian fluid, the viscosity, by definition, depends only on temperature and pressure, not on the forces acting upon it. If the fluid is incompressible the equation governing the viscous stress (in Cartesian coordinates) is\n\nwhere\n\nIf the fluid is not incompressible the general form for the viscous stress in a Newtonian fluid is\n\nwhere formula_19 is the second viscosity coefficient (or bulk viscosity). If a fluid does not obey this relation, it is termed a non-Newtonian fluid, of which there are several types. Non-Newtonian fluids can be either plastic, Bingham plastic, pseudoplastic, dilatant, thixotropic, rheopectic, viscoelastic.\n\nIn some applications another rough broad division among fluids is made: ideal and non-ideal fluids. An Ideal fluid is non-viscous and offers no resistance whatsoever to a shearing force. An ideal fluid really does not exist, but in some calculations, the assumption is justifiable. One example of this is the flow far from solid surfaces. In many cases the viscous effects are concentrated near the solid boundaries (such as in boundary layers) while in regions of the flow field far away from the boundaries the viscous effects can be neglected and the fluid there is treated as it were inviscid (ideal flow). When the viscosity is neglected, the term containing the viscous stress tensor formula_20 in the Navier–Stokes equation vanishes. The equation reduced in this form is called the Euler equation.\n\n\n\n"}
{"id": "12619425", "url": "https://en.wikipedia.org/wiki?curid=12619425", "title": "Gerhard Thielcke", "text": "Gerhard Thielcke\n\nGerhard Thielcke (February 14, 1931 Köthen, Germany – July 22, 2007 Radolfzell, Germany) was a German environmentalist, professor and co-founder of the Bund für Umwelt und Naturschutz Deutschland (BUND: League for the environment and nature conservation, Germany), an important German environmental organization.\n\nThielcke died on July 22, 2007 in Radolfzell, Germany.\n\n"}
{"id": "27621214", "url": "https://en.wikipedia.org/wiki?curid=27621214", "title": "Highly hazardous chemical", "text": "Highly hazardous chemical\n\nA highly hazardous chemical is a substance classified by the American Occupational Safety and Health Administration as material that is both toxic and reactive and whose potential for human injury is high if released. Highly hazardous chemicals may cause cancer, birth defects, induce genetic damage, cause miscarriage, injury and death from relatively small exposures.\n\nHighly hazardous chemicals include:\n\n"}
{"id": "2206712", "url": "https://en.wikipedia.org/wiki?curid=2206712", "title": "History of fluid mechanics", "text": "History of fluid mechanics\n\nThe history of fluid mechanics, the study of how fluids move and the forces on them, dates back to the Ancient Greeks.\n\nA pragmatic, if not scientific, knowledge of fluid flow was exhibited by ancient civilizations, such as in the design of arrows, spears, boats, and particularly hydraulic engineering projects for flood protection, irrigation, drainage, and water supply. The earliest human civilizations began near the shores of rivers, and consequently coincided with the dawn of hydrology, hydraulics, and hydraulic engineering.\n\nThe fundamental principles of hydrostatics and dynamics were given by Archimedes in his work \"On Floating Bodies\" (), around 250 BC. In it, Archimedes develops the law of buoyancy, also known as Archimedes' Principle. This principle states that a body immersed in a fluid experiences a buoyant force equal to the weight of the fluid it displaces. Archimedes maintained that each particle of a fluid mass, when in equilibrium, is equally pressed in every direction; and he inquired into the conditions according to which a solid body floating in a fluid should assume and preserve a position of equilibrium.\n\nIn the Greek school at Alexandria, which flourished under the auspices of the Ptolemies, attempts were made at the construction of hydraulic machinery, and about 120 BC the fountain of compression, the siphon, and the forcing-pump were invented by Ctesibius and Hero. The siphon is a simple instrument; but the forcing-pump is a complicated invention, which could scarcely have been expected in the infancy of hydraulics. It was probably suggested to Ctesibius by the Egyptian wheel or Noria, which was common at that time, and which was a kind of chain pump, consisting of a number of earthen pots carried round by a wheel. In some of these machines the pots have a valve in the bottom which enables them to descend without much resistance, and diminishes greatly the load upon the wheel; and, if we suppose that this valve was introduced so early as the time of Ctesibius, it is not difficult to perceive how such a machine might have led to the invention of the forcing-pump.\n\nNotwithstanding these inventions of the Alexandrian school, its attention does not seem to have been directed to the motion of fluids; and the first attempt to investigate this subject was made by Sextus Julius Frontinus, inspector of the public fountains at Rome in the reigns of Nerva and Trajan. In his work \"De aquaeductibus urbis Romae commentarius\", he considers the methods which were at that time employed for ascertaining the quantity of water discharged from ajutages (tubes), and the mode of distributing the waters of an aqueduct or a fountain. He remarked that the flow of water from an orifice depends not only on the magnitude of the orifice itself, but also on the height of the water in the reservoir; and that a pipe employed to carry off a portion of water from an aqueduct should, as circumstances required, have a position more or less inclined to the original direction of the current. But as he was unacquainted with the law of the velocities of running water as depending upon the depth of the orifice, the want of precision which appears in his results is not surprising.\n\nIslamicate scientists, particularly Abu Rayhan Biruni (973–1048) and later Al-Khazini (fl. 1115–1130), were the first to apply experimental scientific methods to fluid mechanics, especially in the field of fluid statics, such as for determining specific weights. They applied the mathematical theories of ratios and infinitesimal techniques, and introduced algebraic and fine calculation techniques into the field of fluid statics.\n\nIn fluid statics, Biruni discovered that there is a correlation between the specific gravity of an object and the volume of water it displaces. He also introduced the method of checking tests during experiments and measured the weights of various liquids. He also recorded the differences in weight between freshwater and saline water, and between hot water and cold water. During his experiments on fluid mechanics, Biruni invented the conical measure, in order to find the ratio between the weight of a substance in air and the weight of water displaced.\n\nAl-Khazini, in \"The Book of the Balance of Wisdom\" (1121), invented a hydrostatic balance.\n\nIn the 9th century, Banū Mūsā brothers' \"Book of Ingenious Devices\" described a number of early automatic controls in fluid mechanics. Two-step level controls for fluids, an early form of discontinuous variable structure controls, was developed by the Banu Musa brothers. They also described an early feedback controller for fluids. According to Donald Routledge Hill, the Banu Musa brothers were \"masters in the exploitation of small variations\" in hydrostatic pressures and in using conical valves as \"in-line\" components in flow systems, \"the first known use of conical valves as automatic controllers.\" They also described the use of other valves, including a plug valve, float valve and tap. The Banu Musa also developed an early fail-safe system where \"one can withdraw small quantities of liquid repeatedly, but if one withdraws a large quantity, no further extractions are possible.\" The double-concentric siphon and the funnel with bent end for pouring in different liquids, neither of which appear in any earlier Greek works, were also original inventions by the Banu Musa brothers. Some of the other mechanisms they described include a float chamber and an early differential pressure.\n\nIn 1206, Al-Jazari's \"Book of Knowledge of Ingenious Mechanical Devices\" described many hydraulic machines. Of particular importance were his water-raising pumps. The first known use of a crankshaft in a chain pump was in one of al-Jazari's saqiya machines. The concept of minimizing intermittent working is also first implied in one of al-Jazari's saqiya chain pumps, which was for the purpose of maximising the efficiency of the saqiya chain pump. Al-Jazari also invented a twin-cylinder reciprocating piston suction pump, which included the first suction pipes, suction pumping, double-action pumping, and made early uses of valves and a crankshaft-connecting rod mechanism. This pump is remarkable for three reasons: the first known use of a true suction pipe (which sucks fluids into a partial vacuum) in a pump, the first application of the double-acting principle, and the conversion of rotary to reciprocating motion, via the crankshaft-connecting rod mechanism.\n\nBenedetto Castelli, and Evangelista Torricelli, two of the disciples of Galileo, applied the discoveries of their master to the science of hydrodynamics. In 1628 Castelli published a small work, \"Della misura dell' acque correnti\", in which he satisfactorily explained several phenomena in the motion of fluids in rivers and canals; but he committed a great paralogism in supposing the velocity of the water proportional to the depth of the orifice below the surface of the vessel. Torricelli, observing that in a jet where the water rushed through a small ajutage it rose to nearly the same height with the reservoir from which it was supplied, imagined that it ought to move with the same velocity as if it had fallen through that height by the force of gravity, and hence he deduced the proposition that the velocities of liquids are as the square root of the head, apart from the resistance of the air and the friction of the orifice. This theorem was published in 1643, at the end of his treatise \"De motu gravium projectorum\", and it was confirmed by the experiments of Raffaello Magiotti on the quantities of water discharged from different ajutages under different pressures (1648).\n\nIn the hands of Blaise Pascal hydrostatics assumed the dignity of a science, and in a treatise on the equilibrium of liquids (\"Sur l’équilibre des liqueurs\"), found among his manuscripts after his death and published in 1663, the laws of the equilibrium of liquids were demonstrated in the most simple manner, and amply confirmed by experiments.\n\nThe theorem of Torricelli was employed by many succeeding writers, but particularly by Edme Mariotte (1620–1684), whose \"Traité du mouvement des eaux\", published after his death in the year 1686, is founded on a great variety of well-conducted experiments on the motion of fluids, performed at Versailles and Chantilly. In the discussion of some points he committed considerable mistakes. Others he treated very superficially, and in none of his experiments apparently did he attend to the diminution of efflux arising from the contraction of the liquid vein, when the orifice is merely a perforation in a thin plate; but he appears to have been the first who attempted to ascribe the discrepancy between theory and experiment to the retardation of the water's velocity through friction. His contemporary Domenico Guglielmini (1655–1710), who was inspector of the rivers and canals at Bologna, had ascribed this diminution of velocity in rivers to transverse motions arising from inequalities in their bottom. But as Mariotte observed similar obstructions even in glass pipes where no transverse currents could exist, the cause assigned by Guglielmini seemed destitute of foundation. The French philosopher, therefore, regarded these obstructions as the effects of friction. He supposed that the filaments of water which graze along the sides of the pipe lose a portion of their velocity; that the contiguous filaments, having on this account a greater velocity, rub upon the former, and suffer a diminution of their celerity; and that the other filaments are affected with similar retardations proportional to their distance from the axis of the pipe. In this way the medium velocity of the current may be diminished, and consequently the quantity of water discharged in a given time must, from the effects of friction, be considerably less than that which is computed from theory.\n\nThe effects of friction and viscosity in diminishing the velocity of running water were noticed in the \"Principia\" of Sir Isaac Newton, who threw much light upon several branches of hydromechanics. At a time when the Cartesian system of vortices universally prevailed, he found it necessary to investigate that hypothesis, and in the course of his investigations he showed that the velocity of any stratum of the vortex is an arithmetical mean between the velocities of the strata which enclose it; and from this it evidently follows that the velocity of a filament of water moving in a pipe is an arithmetical mean between the velocities of the filaments which surround it. Taking advantage of these results, Italian-born French engineer Henri Pitot afterwards showed that the retardations arising from friction are inversely as the diameters of the pipes in which the fluid moves.\n\nThe attention of Newton was also directed to the discharge of water from orifices in the bottom of vessels. He supposed a cylindrical vessel full of water to be perforated in its bottom with a small hole by which the water escaped, and the vessel to be supplied with water in such a manner that it always remained full at the same height. He then supposed this cylindrical column of water to be divided into two parts – the first, which he called the \"cataract,\" being an hyperboloid generated by the revolution of an hyperbola of the fifth degree around the axis of the cylinder which should pass through the orifice, and the second the remainder of the water in the cylindrical vessel. He considered the horizontal strata of this hyperboloid as always in motion, while the remainder of the water was in a state of rest, and imagined that there was a kind of cataract in the middle of the fluid.\n\nWhen the results of this theory were compared with the quantity of water actually discharged, Newton concluded that the velocity with which the water issued from the orifice was equal to that which a falling body would receive by descending through half the height of water in the reservoir. This conclusion, however, is absolutely irreconcilable with the known fact that jets of water rise nearly to the same height as their reservoirs, and Newton seems to have been aware of this objection. Accordingly, in the second edition of his \"Principia\", which appeared in 1713, he reconsidered his theory. He had discovered a contraction in the vein of fluid (\"vena contracta\") which issued from the orifice, and found that, at the distance of about a diameter of the aperture, the section of the vein was contracted in the subduplicate ratio of two to one. He regarded, therefore, the section of the contracted vein as the true orifice from which the discharge of water ought to be deduced, and the velocity of the effluent water as due to the whole height of water in the reservoir; and by this means his theory became more conformable to the results of experience, though still open to serious objections.\n\nNewton was also the first to investigate the difficult subject of the motion of waves.\n\nIn 1738 Daniel Bernoulli published his \"Hydrodynamica seu de viribus et motibus fluidorum commentarii\". His theory of the motion of fluids, the germ of which was first published in his memoir entitled \"Theoria nova de motu aquarum per canales quocunque fluentes\", communicated to the Academy of St Petersburg as early as 1726, was founded on two suppositions, which appeared to him conformable to experience. He supposed that the surface of the fluid, contained in a vessel which is emptying itself by an orifice, remains always horizontal; and, if the fluid mass is conceived to be divided into an infinite number of horizontal strata of the same bulk, that these strata remain contiguous to each other, and that all their points descend vertically, with velocities inversely proportional to their breadth, or to the horizontal sections of the reservoir. In order to determine the motion of each stratum, he employed the principle of the \"conservatio virium vivarum\", and obtained very elegant solutions. But in the absence of a general demonstration of that principle, his results did not command the .confidence which they would otherwise have deserved, and it became desirable to have a theory more certain, and depending solely on the fundamental laws of mechanics. Colin Maclaurin and John Bernoulli, who were of this opinion, resolved the problem by more direct methods, the one in his \"Fluxions\", published in 1742, and the other in his \"Hydraulica nunc primum detecta\", \"et demonstrata directe ex fundamentis pure mechanicis\", which forms the fourth volume of his works. The method employed by Maclaurin has been thought not sufficiently rigorous; and that of John Bernoulli is, in the opinion of Lagrange, defective in clearness and precision.\n\nThe theory of Daniel Bernoulli was opposed also by Jean le Rond d'Alembert. When generalizing the theory of pendulums of Jacob Bernoulli he discovered a principle of dynamics so simple and general that it reduced the laws of the motions of bodies to that of their equilibrium. He applied this principle to the motion of fluids, and gave a specimen of its application at the end of his \"Dynamics\" in 1743. It was more fully developed in his \"Traité des fluides\", published in 1744, in which he gave simple and elegant solutions of problems relating to the equilibrium and motion of fluids. He made use of the same suppositions as Daniel Bernoulli, though his calculus was established in a very different manner. He considered, at every instant, the actual motion of a stratum as composed of a motion which it had in the preceding instant and of a motion which it had lost; and the laws of equilibrium between the motions lost furnished him with equations representing the motion of the fluid. It remained a desideratum to express by equations the motion of a particle of the fluid in any assigned direction. These equations were found by d'Alembert from two principles – that a rectangular canal, taken in a mass of fluid in equilibrium, is itself in equilibrium, and that a portion of the fluid, in passing from one place to another, preserves the same volume when the fluid is incompressible, or dilates itself according to a given law when the fluid is elastic. His ingenious method, published in 1752, in his \"Essai sur la résistance des fluides\", was brought to perfection in his \"Opuscules mathématiques\", and was adopted by Leonhard Euler.\n\nThe resolution of the questions concerning the motion of fluids was effected by means of Leonhard Euler's partial differential coefficients. This calculus was first applied to the motion of water by d'Alembert, and enabled both him and Euler to represent the theory of fluids in formulae restricted by no particular hypothesis.\n\nOne of the most successful labourers in the science of hydrodynamics at this period was Pierre Louis Georges Dubuat (1734–1809). Following in the steps of the Abbé Charles Bossut (\"Nouvelles Experiences sur la résistance des fluides\", 1777), he published, in 1786, a revised edition of his \"Principes d'hydraulique\", which contains a satisfactory theory of the motion of fluids, founded solely upon experiments. Dubuat considered that if water were a perfect fluid, and the channels in which it flowed infinitely smooth, its motion would be continually accelerated, like that of bodies descending in an inclined plane. But as the motion of rivers is not continually accelerated, and soon arrives at a state of uniformity, it is evident that the viscosity of the water, and the friction of the channel in which it descends, must equal the accelerating force. Dubuat, therefore, assumed it as a proposition of fundamental importance that, when water flows in any channel or bed, the accelerating force which obliges it to move is equal to the sum of all the resistances which it meets with, whether they arise from its own viscosity or from the friction of its bed. This principle was employed by him in the first edition of his work, which appeared in 1779. The theory contained in that edition was founded on the experiments of others, but he soon saw that a theory so new, and leading to results so different from the ordinary theory, should be founded on new experiments more direct than the former, and he was employed in the performance of these from 1780 to 1783. The experiments of Bossut were made only on pipes of a moderate declivity, but Dubuat used declivities of every kind, and made his experiments upon channels of various sizes.\n\nIn 1858 Hermann von Helmholtz published his seminal paper \"Über Integrale der hydrodynamischen Gleichungen, welche den Wirbelbewegungen entsprechen,\" in \"Journal für die reine und angewandte Mathematik\", vol. 55, pp. 25–55. So important was the paper that a few years later P. G. Tait published an English translation, \"On integrals of the hydrodynamical equations which express vortex motion\", in \"Philosophical Magazine\", vol. 33, pp. 485–512 (1867). In his paper Helmholtz established his three \"laws of vortex motion\" in much the same way one finds them in any advanced textbook of fluid mechanics today. This work established the significance of vorticity to fluid mechanics and science in general.\n\nFor the next century or so \"vortex dynamics\" matured as a subfield of fluid mechanics, always commanding at least a major chapter in treatises on the subject. Thus, H. Lamb's well known \"Hydrodynamics\" (6th ed., 1932) devotes a full chapter to vorticity and vortex dynamics as does G. K. Batchelor's \"Introduction to Fluid Dynamics\" (1967). In due course entire treatises were devoted to vortex motion. H. Poincaré's \"Théorie des Tourbillons\" (1893), H. Villat's \"Leçons sur la Théorie des Tourbillons\" (1930), C. Truesdell's \"The Kinematics of Vorticity\" (1954), and P. G. Saffman's \"Vortex Dynamics\" (1992) may be mentioned. Early on individual sessions at scientific conferences were devoted to vortices, vortex motion, vortex dynamics and vortex flows. Later, entire meetings were devoted to the subject.\n\nThe range of applicability of Helmholtz's work grew to encompass atmospheric and oceanographic flows, to all branches of engineering and applied science and, ultimately, to superfluids (today including Bose–Einstein condensates). In modern fluid mechanics the role of vortex dynamics in explaining flow phenomena is firmly established. Well known vortices have acquired names and are regularly depicted in the popular media: hurricanes, tornadoes, waterspouts, aircraft trailing vortices (e.g., wingtip vortices), drainhole vortices (including the bathtub vortex), smoke rings, underwater bubble air rings, cavitation vortices behind ship propellers, and so on. In the technical literature a number of vortices that arise under special conditions also have names: the Kármán vortex street wake behind a bluff body, Taylor vortices between rotating cylinders, Görtler vortices in flow along a curved wall, etc.\n\nThe theory of running water was greatly advanced by the researches of Gaspard Riche de Prony (1755–1839). From a collection of the best experiments by previous workers he selected eighty-two (fifty-one on the velocity of water in conduit pipes, and thirty-one on its velocity in open canals); and, discussing these on physical and mechanical principles, he succeeded in drawing up general formulae, which afforded a simple expression for the velocity of running water.\n\nJ. A. Eytelwein of Berlin, who published in 1801 a valuable compendium of hydraulics entitled \"Handbuch der Mechanik und der Hydraulik\", investigated the subject of the discharge of water by compound pipes, the motions of jets and their impulses against plane and oblique surfaces; and he showed theoretically that a water-wheel will have its maximum effect when its circumference moves with half the velocity of the stream.\n\nJNP Hachette in 1816–1817 published memoirs containing the results of experiments on the spouting of fluids and the discharge of vessels. His object was to measure the contracted part of a fluid vein, to examine the phenomena attendant on additional tubes, and to investigate the form of the fluid vein and the results obtained when different forms of orifices are employed. Extensive experiments on the discharge of water from orifices (\"Expériences hydrauliques\", Paris, 1832) were conducted under the direction of the French government by J. V. Poncelet (1788–1867) and J. A. Lesbros (1790–1860).\n\nP. P. Boileau (1811–1891) discussed their results and added experiments of his own (\"Traité de la mesure des eaux courantes\", Paris, 1854). K. R. Bornemann re-examined all these results with great care, and gave formulae expressing the variation of the coefficients of discharge in different conditions (\"Civil Ingénieur,\" 1880). Julius Weisbach (1806–1871) also made many experimental investigations on the discharge of fluids.\n\nThe experiments of J. B. Francis (\"Lowell Hydraulic Experiments\", Boston, Mass., 1855) led him to propose variations in the accepted formulae for the discharge over weirs, and a generation later a very complete investigation of this subject was carried out by Henri-Émile Bazin. An elaborate inquiry on the flow of water in pipes and channels was conducted by Henry G. P. Darcy (1803–1858) and continued by Bazin, at the expense of the French government (\"Recherches hydrauliques\", Paris, 1866).\n\nGerman engineers have also devoted special attention to the measurement of the flow in rivers; the \"Beiträge zur Hydrographie des Königreiches Böhmen\" (Prague, 1872–1875) of Andreas Rudolf Harlacher contained valuable measurements of this kind, together with a comparison of the experimental results with the formulae of flow that had been proposed up to the date of its publication, and important data were yielded by the gaugings of the Mississippi made for the United States government by Andrew Atkinson Humphreys and Henry Larcom Abbot, by Robert Gordon's gaugings of the Irrawaddy River, and by Allen J. C. Cunningham's experiments on the Ganges canal. The friction of water, investigated for slow speeds by Coulomb, was measured for higher speeds by William Froude (1810–1879), whose work is of great value in the theory of ship resistance (\"Brit. Assoc. Report.\", 1869), and stream line motion was studied by Professor Osborne Reynolds and by Professor Henry S. Hele-Shaw.\n\nVortex dynamics is a vibrant subfield of fluid dynamics, commanding attention at major scientific conferences and precipitating workshops and symposia that focus fully on the subject.\n\nA curious diversion in the history of vortex dynamics was the vortex atom theory of William Thomson, later Lord Kelvin. His basic idea was that atoms were to be represented as vortex motions in the ether. This theory predated the quantum theory by several decades and because of the scientific standing of its originator received considerable attention. Many profound insights into vortex dynamics were generated during the pursuit of this theory. Other interesting corollaries were the first counting of simple knots by P. G. Tait, today considered a pioneering effort in graph theory, topology and knot theory. Ultimately, Kelvin's vortex atom was seen to be wrong-headed but the many results in vortex dynamics that it precipitated have stood the test of time. Kelvin himself originated the notion of circulation and proved that in an inviscid fluid circulation around a material contour would be conserved. This result singled out by Einstein in \"Zum hundertjährigen Gedenktag von Lord Kelvins Geburt, Naturwissenschaften, 12 (1924), 601–602,\" (title translation: \"On the 100th Anniversary of Lord Kelvin's Birth\"), as one of the most significant results of Kelvin's work provided an early link between fluid dynamics and topology.\n\nThe history of vortex dynamics seems particularly rich in discoveries and re-discoveries of important results, because results obtained were entirely forgotten after their discovery and then were re-discovered decades later. Thus, the integrability of the problem of three point vortices on the plane was solved in the 1877 thesis of a young Swiss applied mathematician named Walter Gröbli. In spite of having been written in Göttingen in the general circle of scientists surrounding Helmholtz and Kirchhoff, and in spite of having been mentioned in Kirchhoff's well known lectures on theoretical physics and in other major texts such as Lamb's \"Hydrodynamics\", this solution was largely forgotten. A 1949 paper by the noted applied mathematician J. L. Synge created a brief revival, but Synge's paper was in turn forgotten. A quarter century later a 1975 paper by E. A. Novikov and a 1979 paper by H. Aref on chaotic advection finally brought this important earlier work to light. The subsequent elucidation of chaos in the four-vortex problem, and in the advection of a passive particle by three vortices, made Gröbli's work part of \"modern science\".\n\nAnother example of this kind is the so-called \"localized induction approximation\" (LIA) for three-dimensional vortex filament motion, which gained favor in the mid-1960s through the work of Arms, Hama, Betchov and others, but turns out to date from the early years of the 20th century in the work of Da Rios, a gifted student of the noted Italian mathematician T. Levi-Civita. Da Rios published his results in several forms but they were never assimilated into the fluid mechanics literature of his time. In 1972 H. Hasimoto used Da Rios' \"intrinsic equations\" (later re-discovered independently by R. Betchov) to show how the motion of a vortex filament under LIA could be related to the non-linear Schrödinger equation. This immediately made the problem part of \"modern science\" since it was then realized that vortex filaments can support solitary twist waves of large amplitude.\n\n"}
{"id": "32378301", "url": "https://en.wikipedia.org/wiki?curid=32378301", "title": "Inertial number", "text": "Inertial number\n\nThe Inertial number formula_1 quantifies the significance of dynamic effects in a granular material. It measures the ratio of inertial forces of grains to imposed forces: a small value corresponds to the quasi-static state, while a high value corresponds to the inertial state or even the \"dynamic\" state.\n\nformula_2\n\nwhere formula_3 is the shear rate, formula_4 the average particle diameter, formula_5 is the pressure and formula_6 is the density.\n\nGenerally three regimes are distinguished:\n\nOne model of dense granular flows, the μ(I) rheology, asserts that the coefficient of friction \"μ\" of a granular material is a function of the inertial number only.\n"}
{"id": "18314293", "url": "https://en.wikipedia.org/wiki?curid=18314293", "title": "Jacaric acid", "text": "Jacaric acid\n\nJacaric acid is a conjugated polyunsaturated fatty acid with a melting point of 44 °C. It occurs naturally in the seeds of the \"Jacaranda mimosifolia\", which contain about 36% jacaric acid.\n"}
{"id": "30430262", "url": "https://en.wikipedia.org/wiki?curid=30430262", "title": "January 8–13, 2011 North American blizzard", "text": "January 8–13, 2011 North American blizzard\n\nThe January 8–13, 2011 North American Blizzard was a major Mid-Atlantic nor'easter and winter storm, and a New England blizzard. The storm also affected portions of the Southeastern regions of the United States. This storm came just two weeks after a previous major blizzard severely affected most of these same areas in December 2010. It was the second significant snowstorm to affect the region during the 2010–11 North American winter storm season.\n\nThe storm took on a similar track as the storm that had crippled the region in December 2010. The storm formed as a low pressure system in the Gulf of Mexico, which interacted with an upper level low pressure system that dropped down from central Canada. Like the previous storm, it was fueled by a great amount of southern stream energy. In mid-Atlantic states, the track of the storm was over 50 miles east of the previous one; besides, the storm was a fast-moving system. As a result, snowfall totals in these areas were not expected to reach those of the December 2010 blizzard two weeks earlier. However, the storm dumped over 2 feet of snow in some areas in New England before it moved out to sea on Thursday.\n\nFrom January 8 through January 10, the storm dropped a swath of snow and ice from eastern Texas, and eastward across portions of Arkansas, Louisiana, Mississippi, Alabama, Tennessee, Georgia, and the Carolinas. Significant snows and ice fell in these regions causing significant travel emergencies and accidents. Hartsfield-Jackson Atlanta International Airport, the busiest in the world had only seen around 30 flights take off. Many flights were canceled however the airport did not close. Icy conditions were reported around the Atlanta and Birmingham as numerous traffic accidents were reported.\n\nMeanwhile, a second system swung southeastward from Alberta, Canada, delivering light amounts of snow to the Dakotas, the Upper Midwest and the Great Lakes regions. Energy from the two systems combined off the coast of Cape Hatteras, North Carolina late on January 10, forming the storm that delivered over 2 feet of snow to New England on January 12.\n\nThe storm affected portions of New Jersey, New York, much of Connecticut, Rhode Island, and Massachusetts overnight late Tuesday into Wednesday, January 12, 2011. Although the storm was expected to intensify to become a blizzard, New York City was spared from the worst. The city's public schools remained open. By Wednesday morning, Central Park had received 9.1 inches of snow; however, a lot of the areas in central and eastern Long Island had seen 15 to 20 inches of snow before it all ended. In Edison, New Jersey, there was a reported snowfall total of 10.2 inches of snow.\n\nConnecticut bore the brunt of the storm, with many locations checking in with snowfall totals of 20 to 30 inches. Heavy snow caused the roof of an apartment building in Norwich to collapse, forcing the evacuation of 10 residents. The storm forced state troopers to close a 50-mile stretch of Interstate 95 in southwest Connecticut due to numerous trucks becoming stuck on the highway in the snow. The storm also caused the bubble covering the Cheshire Community Pool in Cheshire, Connecticut to collapse, forcing its closure for the season.\n\nThe highest amount of snowfall was reported at Savoy, Massachusetts, with 40.5 inches. The storm also caused widespread airport delays and school closings in the region. Also, isolated amounts of more than 2 feet occurred in eastern Massachusetts. In Wilmington, MA, Winchester, MA and Lexington, MA, amounts of 24 inches occurred, with Winchester reporting 24.2 inches.\n\nThe Edgewood Yacht Club was completely destroyed by a fire at the height of the storm. The elements made it extremely difficult for the fire to be put out. There were no injuries from the fire, although it is still unknown what caused the fire to start.\n\nThis blizzard together with blizzards later in the month is estimated to have led to a loss of 150,000 jobs in the United States for January.\n\n"}
{"id": "364842", "url": "https://en.wikipedia.org/wiki?curid=364842", "title": "List of national parks of Namibia", "text": "List of national parks of Namibia\n\nThis is a list of national parks in Namibia.\n\n\n\n"}
{"id": "29894101", "url": "https://en.wikipedia.org/wiki?curid=29894101", "title": "Margie Eugene-Richard", "text": "Margie Eugene-Richard\n\nMargie Eugene-Richard is an American environmental activist. Richard had grown up in the middle of Cancer Alleywas. She is the first African-American to win the Goldman Environmental Prize in 2004, for her successful campaign for relocating people who lived in a community close to a chemical plant in Norco, Louisiana. Eugene-Richard says: \"you have to go out and command justice. Somebody has to ask God for the inner strength to be bold.\" \n\n\"Margie believes in the community leading the way,\" says Dr. Beverly Wright, director of the Deep South Center for Environmental Justice. But as Richard recognizes, community is an elusive thing in post-Katrina New Orleans. \"I won't be knocking on doors,\" she says, \"because there are no doors.\"\n"}
{"id": "4682753", "url": "https://en.wikipedia.org/wiki?curid=4682753", "title": "Metal-coated crystal", "text": "Metal-coated crystal\n\nMetal-coated crystals are natural crystals, such as quartz, whose surface has been coated with metal to give them an iridescent metallic sheen. Crystals treated this way are used as gemstones and for other decorative purposes. Possible coatings include gold (resulting in a stone called aqua aura), indium, titanium, niobium and copper. Other names for crystals treated so include angel aura, flame aura, opal aura or rainbow quartz.\n\nAqua aura is created in a vacuum chamber from quartz crystals and gold vapour by vapour deposition. The quartz is heated to 871 °C (1600 °F) in a vacuum, and then gold vapor is added to the chamber. The gold atoms fuse to the crystal's surface, which gives the crystal an iridescent metallic sheen. \nWhen viewed under a gemological microscope in diffused direct transmitted light, aqua aura displays the following properties:\n\nRainbow quartz have been treated with a combination of titanium and gold. Titanium molecules are bonded to the quartz by the natural electrostatic charge of the crystal in a process known as magnetron ionization. The brilliant color of flame aura is the result of optical interference effects produced by layers of titanium. Since only electricity is used to deposit the titanium layers and create these colors, very little heat is involved and the integrity of the crystal is maintained. The crystal does not become brittle or prone to breakage as with other treatments.\n\n"}
{"id": "8170140", "url": "https://en.wikipedia.org/wiki?curid=8170140", "title": "Methernitha", "text": "Methernitha\n\nMethernitha refers to two related entities, both founded by Paul Baumann — Methernitha Christian Alliance and Methernitha Cooperative. One is a religious group, and the other is a community in Linden, Switzerland, based on the group's principles.\n\nThe Methernitha Christian Alliance exists as a loose association of members who subscribe to the teachings of the Holy Scriptures. Members do not have organized meetings, nor do they necessarily live in the same area. Current members are distributed throughout Switzerland and other countries. The Alliance was founded in the 1950s by a small group of Christian oriented people whose goal was to live according to the teachings of the Bible. Eventually the group founded a commune in Linden in the canton of Bern.\n\nOriginally based on the principles of the Methernitha Christian Alliance, the Methernitha Cooperative is now more philosophically inclusive and in 1960 was registered with the Swiss Commercial Register as a legal entity. The organisation was laid down in Articles of Association and adheres to the Rules of the Swiss Federation.\nThe Cooperative is a residential joint venture work group operated on a democratic basis. Administration staff are chosen from the members in a general meeting. It has its own television studios that transmit self-produced programs. The commune also has its own school and nursing home, and a \"club house\", which is the center for worship and their historical archives.\n\nMembers are still Swiss citizens, and the Cooperative pays all relevant taxes and social security to the government.\n\nMethernitha is a Christian group, but it is non-evangelical, refraining from attempts to win new converts. Members do not indulge in alcohol, tobacco or recreational drugs or money.\n\nThe Methernitha Cooperative claimed to have developed years ago a free-energy device called the Testatika:\n\nIn France, the religious group was criticized by an anti-cult association ADFI, which considered it a cult (\"secte\" in French-language). In 1976 Paul Baumann, the founder of the community, was sentenced to six years imprisonment for sexual abuse of children.\n\n"}
{"id": "34994302", "url": "https://en.wikipedia.org/wiki?curid=34994302", "title": "Minister for Local Government, Housing and Planning", "text": "Minister for Local Government, Housing and Planning\n\nThe Minister for Local Government, Housing and Planning is a junior ministerial post in the Scottish Government. As a result, the Minister does not attend the Scottish Cabinet. The minister supports the Cabinet Secretary for Communities and Local Government, who has overall responsible for the portfolio.\n\nThe current Minister for Local Government, Housing and Planning is Kevin Stewart MSP, who was appointed on 18 May 2016. The minister is responsible for housing, local government, planning, building standards, Business Improvement Districts, community planning, homelessness, regeneration, and fuel poverty.\n\nIn 2011, the position of Minister for Local Government and Planning was created; the portfolio changed in November 2014 following a reshuffle by Nicola Sturgeon when she became First Minister of Scotland. The post was rebranded at that point to Minister for Local Government and Community Empowerment.\n\nIn September 2012, the junior post of Minister for Housing and Welfare was announced, with a portfolio intended to reflect the important role of housing in aiding economic recovery and the challenges that face those in poverty.[4] In May 2016, Nicola Sturgeon reshuffled junior ministers and the post joined local government and housing in a new Minister for Local Government and Housing role. Planning was added to the title in June 2018.\n\n"}
{"id": "157511", "url": "https://en.wikipedia.org/wiki?curid=157511", "title": "Munich air disaster", "text": "Munich air disaster\n\nThe Munich air disaster occurred on 6 February 1958 when British European Airways Flight 609 crashed on its third attempt to take off from a slush-covered runway at Munich-Riem Airport, West Germany. On the plane was the Manchester United football team, nicknamed the \"Busby Babes\", along with supporters and journalists. Twenty of the 44 on the aircraft died at the scene. The injured, some unconscious, were taken to the Rechts der Isar Hospital in Munich where three more died, resulting in 23 fatalities with 21 survivors.\n\nThe team was returning from a European Cup match in Belgrade, Yugoslavia, having eliminated Red Star Belgrade to advance to the semi-finals of the competition. The flight stopped to refuel in Munich because a non-stop flight from Belgrade to Manchester was beyond the \"Elizabethan\"-class Airspeed Ambassador's range. After refuelling, pilots James Thain and Kenneth Rayment twice abandoned take-off because of boost surging in the left engine. Fearing they would get too far behind schedule, Captain Thain rejected an overnight stay in Munich in favour of a third take-off attempt. By then, snow was falling, causing a layer of slush to form at the end of the runway. After the aircraft hit the slush, it ploughed through a fence beyond the end of the runway and the left wing was torn off after hitting a house. Fearing the aircraft might explode, Thain began evacuating passengers while Manchester United goalkeeper Harry Gregg helped pull survivors from the wreckage.\n\nAn investigation by West German airport authorities originally blamed Thain, saying he did not de-ice the aircraft's wings, despite eyewitness statements to the contrary. It was later established that the crash was caused by the slush on the runway, which slowed the plane too much to take off. Thain was cleared in 1968, ten years after the incident.\n\nManchester United were trying to become the third club to win three successive English league titles; they were six points behind League leaders Wolverhampton Wanderers with 14 games to go. They also held the Charity Shield and had just advanced into their second successive European Cup semi-finals. The team had not been beaten for 11 matches. The crash not only derailed their title ambitions that year but also virtually destroyed the nucleus of what promised to be one of the greatest generations of players in English football history. It took 10 years for the club to recover, with Busby rebuilding the team and winning the European Cup in 1968 with a new generation of \"Babes\".\n\nIn April 1955, UEFA established the European Cup, a football competition for the champion clubs of UEFA-affiliated nations, to begin in the 1955–56 season; however, the English league winners, Chelsea, were denied entry by the Football League's secretary, Alan Hardaker, who believed not participating was best for English football. The following season, the English league was won by Manchester United, managed by Matt Busby. The Football League again denied their champions entry, but Busby and his chairman, Harold Hardman, with the help of the Football Association's chairman Stanley Rous, defied the league and United became the first English team to play in Europe.\n\nThe team – known as the \"Busby Babes\" for their youth – reached the semi-finals, beaten there by the eventual winners, Real Madrid. Winning the First Division title again that season meant qualification for the 1957–58 tournament, and their cup run in 1956–57 meant they were one of the favourites to win. Domestic league matches were on Saturdays and European matches midweek, so, although air travel was risky, it was the only choice if United were to fulfil their league fixtures, which they would have to do if they were to avoid proving Alan Hardaker right.\n\nAfter overcoming Shamrock Rovers and Dukla Prague in the preliminary and first round respectively, United were drawn with Red Star Belgrade of Yugoslavia for the quarter-finals. After beating them 2–1 at Old Trafford on 14 January 1958, the club was to travel to Yugoslavia for the return leg on 5 February. On the way back from Prague in the previous round, fog over England prevented the team from flying back to Manchester, so they flew to Amsterdam before taking the ferry from the Hook of Holland to Harwich and then the train to Manchester. The trip took its toll on the players and they drew 3–3 with Birmingham City at St Andrew's three days later.\n\nEager not to miss Football League fixtures, and not to have a difficult trip again, the club chartered a British European Airways plane from Manchester to Belgrade for the away leg against Red Star. The match was drawn 3–3 but it was enough to send United to the semi-finals. The takeoff from Belgrade was delayed for an hour after outside right Johnny Berry lost his passport, and the plane landed in Munich for refuelling at 13:15 GMT.\n\nThe aircraft was a six-year-old Airspeed Ambassador 2, built in 1952 and delivered to BEA the same year.\n\nThe pilot, Captain James Thain, was a former RAF flight lieutenant. Originally a sergeant (later a warrant officer), he was given an emergency commission in the RAF as an acting pilot officer on probation in April 1944, and promoted to pilot officer on probation in September that year. He was promoted to flight lieutenant in May 1948, and received a permanent commission in the same rank in 1952. He retired from the RAF to join BEA.\n\nThe co-pilot, Captain Kenneth Rayment, was also a former RAF flight lieutenant and a Second World War flying ace. After joining the RAF in 1940, he was promoted to sergeant in September 1941. He was commissioned as a war substantive pilot officer a year later, and promoted to war substantive flying officer in May 1943. He shot down five German fighters, one Italian plane and a V-1 flying bomb. He was awarded the DFC in July 1943, and promoted to flight lieutenant in September 1943. After leaving the RAF in 1945, he joined BOAC in Cairo, before joining BEA in 1947. He had had experience with Vikings, Dakotas and the Ambassador \"Elizabethan\" class.\n\nThain had flown the \"Elizabethan\"-class Airspeed Ambassador (registration G-ALZU) to Belgrade but handed the controls to Rayment for the return. At 14:19 GMT, the control tower at Munich was told the plane was ready to take off and gave clearance for take-off, expiring at 14:31. Rayment abandoned the take-off after Thain noticed the port boost pressure gauge fluctuating as the plane reached full power and the engine sounded odd while accelerating. A second attempt was made three minutes later, but called off 40 seconds into the attempt because the engines were running on an over-rich mixture, causing them to over-accelerate, a common problem for the \"Elizabethan\". After the second failure, passengers retreated to the airport lounge. By then, it had started to snow heavily, and it looked unlikely that the plane would be making the return journey that day. Manchester United's Duncan Edwards sent a telegram to his landlady in Manchester. It read: \"All flights cancelled, flying tomorrow. Duncan.\"\n\nThain told the station engineer, Bill Black, about the problem with the boost surging in the port engine, and Black suggested that since opening the throttle more slowly had not worked, the only option was to hold the plane overnight for retuning. Thain was anxious to stay on schedule and suggested opening the throttle even more slowly would suffice. This would mean that the plane would not achieve take-off velocity until further down the runway, but with the runway almost long, he believed this would not be a problem. The passengers were called back to the plane 15 minutes after leaving it.\n\nA few of the players were not confident fliers, particularly Liam Whelan, who said, \"This may be death, but I'm ready\". Others, including Duncan Edwards, Tommy Taylor, Mark Jones, Eddie Colman and Frank Swift, moved to the back of the plane, believing it safer. Once everyone was on board, Thain and Rayment got the plane moving again at 14:56. At 14:59, they reached the runway holding point, where they received clearance to line up ready for take-off. On the runway, they made final cockpit checks and at 15:02, they were told their take-off clearance would expire at 15:04. The pilots agreed to attempt take-off, but that they would watch the instruments for surging in the engines. At 15:03, they told the control tower of their decision.\nRayment moved the throttle forward slowly and released the brakes; the plane began to accelerate, and radio officer Bill Rodgers radioed the control tower with the message \"Zulu Uniform rolling\". The plane threw up slush as it gathered speed, and Thain called out the plane's velocity in 10-knot increments. At 85 knots, the port engine began to surge again, and he pulled back marginally on the port throttle before pushing it forward again. Once the plane reached , he announced \"V1\", at which it was no longer safe to abort take-off, and Rayment listened for the call of \"V2\" (), the minimum required to get off the ground. Thain expected the speed to rise, but it fluctuated around 117 knots before suddenly dropping to , and then . Rayment shouted \"Christ, we won't make it!\", as Thain looked up to see what lay ahead.\n\nThe plane skidded off the end of the runway, crashed into the fence surrounding the airport and across a road before its port wing was torn off as it caught a house, home to a family of six. The father and eldest daughter were away and the mother and the other three children escaped as the house caught fire. Part of the plane's tail was torn off before the left side of the cockpit hit a tree. The right side of the fuselage hit a wooden hut, inside which was a truck filled with tyres and fuel, which exploded. Twenty passengers died on board, and three died later in hospital.\n\nOn seeing flames around the cockpit, Thain feared that the aircraft would explode and told his crew to evacuate the area. The stewardesses, Rosemary Cheverton and Margaret Bellis, were the first to leave through a blown-out emergency window in the galley, followed by radio officer Bill Rodgers. Rayment was trapped in his seat by the crumpled fuselage and told Thain to go without him. Thain clambered out of the galley window. On reaching the ground, he saw flames growing under the starboard wing, which held of fuel. He shouted to his crew to get away and climbed back into the aircraft to retrieve two handheld fire extinguishers, stopping to tell Rayment he would be back when the fires had been dealt with.\n\nMeanwhile, in the cabin, goalkeeper Harry Gregg was regaining consciousness, thinking that he was dead. He felt blood on his face and \"didn't dare put [his] hand up. [He] thought the top of [his] head had been taken off, like a hard boiled egg.\" Just above him, light shone into the cabin, so Gregg kicked the hole wide enough for him to escape. He also managed to save some passengers.\n\n\n\n\n\n\n\n\n\n\n\nThe crash was originally blamed on pilot error, but it was later found to have been caused by slush towards the end of the runway, slowing the aircraft and preventing safe flying speed. During take-off, the aircraft had reached , but, on entering the slush, dropped to , too slow to leave the ground, with not enough runway to abort the take-off. Aircraft with tail-wheel undercarriages had not been greatly affected by slush, due to the geometry of these undercarriages in relation to the aircraft's centre of gravity, but newer types, such as the Ambassador, with nose wheel landing-gear and the main wheels behind the centre of gravity, were found to be vulnerable. The accident resulted in the imposition of operating limits for the amount of slush build-up permitted on runways.\n\nDespite this conclusion, the German airport authorities took legal action against Captain Thain, as the one pilot who had survived the crash. They claimed he had taken off without clearing the wings of ice, which caused the crash, despite several witnesses stating that no ice had been seen. De-icing the aircraft was the captain's responsibility, while the state of the airport's runways was the responsibility of the airport authorities, among whom there was widespread ignorance of the danger of slush on runways for aircraft such as the Ambassador.\n\nThe basis of the German authorities' case relied on the icy condition of the wings hours after the crash and a photograph of the aircraft (published in several newspapers) taken shortly before take-off, that appeared to show snow on the upper wing surfaces. When the original negative was examined, no snow or ice could be seen, the \"snow\" in the original having been due to the sun reflecting off the wings, which was clarified when examining the negative rather than the published pictures which had been produced from a copy negative. The witnesses were not called to the German inquiry and proceedings against Thain dragged on until 1968, when he was finally cleared of any responsibility for the crash. As the official cause, British authorities recorded a build-up of melting snow on the runway which prevented the \"Elizabethan\" from reaching the required take-off speed. Thain, having been dismissed by BEA shortly after the accident and never re-engaged, retired and returned to run his poultry farm in Berkshire. He died of a heart attack at the age of 53 in August 1975.\n\n20 people, including seven of Manchester United's players, died at the scene of the crash. The 21st victim, Frank Swift, the journalist and former Manchester City goalkeeper, died on his way to hospital. Duncan Edwards died from his injuries on 21 February at the Rechts der Isar Hospital in Munich, and the final death toll reached 23 several days later when co-pilot Ken Rayment died as a result of serious head injuries. Johnny Berry and Jackie Blanchflower were both injured so severely that they never played again. Matt Busby was seriously injured and had to stay in hospital for more than two months after the crash, and was read his Last Rites twice. After being discharged from hospital, he went to Switzerland to recuperate in Interlaken. At times, he felt like giving up football entirely, until he was told by his wife, Jean, \"You know Matt, the lads would have wanted you to carry on.\" That statement lifted Busby from his depression, and he returned by land to Manchester, before watching his team play in the 1958 FA Cup Final.\n\nMeanwhile, there was speculation that the club would fold, but a threadbare United team completed the 1957–58 season, with Busby's assistant Jimmy Murphy standing in as manager; he had not travelled to Belgrade as he was in Cardiff managing the Welsh national team at the time. A team largely made up of reserve and youth team players beat Sheffield Wednesday 3–0 in the first match after the disaster. The programme for that match showed simply a blank space where each United player's name should have been. With seven players dead (Duncan Edwards died just over 24 hours later), and with only Harry Gregg and Bill Foulkes fit to play out of the surviving players, United were desperate to find replacements with experience, so Murphy signed Ernie Taylor from Blackpool and Stan Crowther from Aston Villa. Three players, Derek Lewin, Bob Hardisty and Warren Bradley, were transferred to United on short-term contracts by non-League club Bishop Auckland. Bradley was the only one of the three players to play for the first team, and the only one to sign a permanent contract. The remaining places in the team were filled by reserve players including Shay Brennan and Mark Pearson. In the aftermath of the crash, Manchester United's fierce rivals Liverpool (who would later be managed by Busby's good friend Bill Shankly) offered United five loan players to help them put a side together.\n\nThere were changes in the backroom staff at the club too, following the deaths of secretary Walter Crickmer and coaches Tom Curry and Bert Whalley. United goalkeeper Les Olive, still registered as a player at the time of the disaster, retired from playing and took over from Crickmer as club secretary, while another former United goalkeeper, Jack Crompton, took over coaching duties after United chairman Harold Hardman had negotiated with Crompton's then-employers Luton Town for his release.\n\nUnited only won one league game after the crash, causing their title challenge to collapse and they fell to ninth place in the league. They did manage to reach the final of the FA Cup, however, losing 2–0 to Bolton Wanderers, and even managed to beat Milan at Old Trafford in the semi-finals of the European Cup, only to lose 4–0 at the San Siro. Real Madrid, who went on to win the trophy for the third year running, suggested that Manchester United be awarded the trophy for that year – a suggestion supported by Red Star Belgrade – but this failed to materialise.\n\nBusby resumed managerial duties the next season (1958–59), and eventually built a second generation of Busby Babes, including George Best and Denis Law, that ten years later won the European Cup, beating Benfica. Bobby Charlton and Bill Foulkes were the only two crash survivors who lined up in that team.\n\nA fund for dependents of victims of the crash was established in March, and chaired by the Chairman of the FA, Arthur Drewry. The fund had raised £52,000 (equivalent to £ as of ) by the time of its disbursement in October 1958.\n\nThe first memorials at Old Trafford to the lost players and staff were unveiled on 25 February 1960. The first, a plaque in the shape of the stadium with the image of a green pitch, inscribed with the names of the victims in black and gold glass, was placed above the entrance to the directors' box. Above the plaque was a teak carving of a player and a supporter, heads bowed either side of a wreath and a football inscribed with the date \"1958\". The plaque was designed by Manchester architect J. Vipond and constructed by Messrs Jaconello (Manchester) Ltd. at a cost of £2,100, and unveiled by Matt Busby.\n\nAlso unveiled that day was a memorial to the members of the press who died at Munich, which consisted of a bronze plaque that named the eight lost journalists. It was unveiled by Munich survivor Frank Taylor on behalf of the Football Writers' Association. The original plaque was stolen in the 1980s and replaced by a replica now located behind the counter in the press entrance. The final memorial was the Munich clock, a simple two-faced clock paid for by the Ground Committee and attached to the south-east corner of the stadium, with the date \"6 Feb 1958\" at the top of both faces and \"Munich\" at the bottom. The clock has remained in the same position since it was first installed. The clock was unveiled on 25 February 1960 by Dan Marsden, the chairman of the Ground Committee.\n\nWhen the stadium was renovated in the mid-1970s, the plaque had to be moved from the directors' entrance to allow the necessary changes. The plaque could not be removed without damaging it, so the old memorial was walled up within the Main Stand and a new memorial was made, simpler than the original, now consisting simply of a slate pitch with the names inscribed upon it, and installed in 1976.\n\nA third version of the memorial, more like the original than the second in that it included the stands around the slate pitch and the figures above it, was installed in 1996, coinciding with the erection of the statue of Matt Busby, who had unveiled the original memorial. This third version was constructed by stonemasons Mather and Ellis from Trafford Park, and the second was put into storage. It is currently awaiting new display panels before being placed into the club museum's Munich display. The third plaque and the statue of Busby were originally located on the north side of the East Stand, but the statue was moved to the front of the East Stand and the plaque to the south side of the stand after the stand's expansion in 2000.\n\nThere are also two memorials in Germany. First, in the Munich suburb of Trudering, on the corner of Karotschstraße and Emplstraße, there is a small wooden memorial depicting Jesus on the Cross, decorated by a stone trough filled with flowers. The trough bears a plaque with the inscription: \"Im Gedenken an die Opfer der Flugzeugkatastrophe am 6.2.1958 unter denen sich auch ein Teil der Fußballmannschaft von Manchester United befand, sowie allen Verkehrstoten der Gemeinde Trudering\" (\"In memory of the victims of the air disaster of 6 February 1958 including members of the football team of Manchester United as well as all the traffic victims from the municipality of Trudering\").\nOn 22 September 2004, a dark blue granite plaque set in a sandstone border was unveiled in the vicinity of the old Munich Airport on the corner of Rappenweg and Emplstraße, just metres from the wooden memorial. With a design in the shape of a football pitch, it reads, in both English and German, \"In memory of all those who lost their lives here in the Munich air disaster on 6 February 1958\". Underneath is a plaque expressing United's gratitude to the municipality of Munich and its people. The new memorial was funded by Manchester United themselves and the unveiling was attended by club officials, including chief executive David Gill, manager Alex Ferguson and director Bobby Charlton, a survivor of the disaster himself. On 24 April 2008, the Munich city council decided to name the site where the memorial stone is placed \"Manchesterplatz\" (\"Manchester Square\").\n\nOn the 57th anniversary of the crash, 6 February 2015, Sir Bobby Charlton and FC Bayern Munich chairman Karl-Heinz Rummenigge opened a new museum exhibit commemorating the disaster at the German club's stadium, the Allianz Arena.\n\nThere is a small display of artefacts in the Majestic Hotel, where the team stayed after the match. These include a menu card signed by 14 of the players, including the eight who were killed, a photograph taken at the meal and a match ticket. The menu card was acquired by the then British ambassador to Yugoslavia and was auctioned by his son in 2006.\n\nIn late 1997, John Doherty (a former United player who had left the club shortly before the disaster) approached club chairman Martin Edwards on behalf of the Manchester United Former Players' Association to request a testimonial for those victims of the Munich disaster – both the survivors and the dependants of the ones who were lost. Edwards was hesitant, but a benefit match was eventually sanctioned for a date as close to the 40th anniversary of the disaster as possible. Red Star Belgrade and Bayern Munich were touted as possible opponents for the match, and fans purchased tickets without the opponents even having been decided.\n\nIn the midst of the preparations, former United player Eric Cantona, who had retired from football to pursue a career in film in 1997, expressed an interest in returning to Manchester United for a farewell match. Edwards took the opportunity to combine the two events into one. Due to Cantona's acting career, his schedule meant that he would not be available in February and the match was moved to 18 August, with the opposition to be a European XI chosen by Cantona. Martin Edwards was criticised for turning the match into a publicity stunt, while Elizabeth Wood, the divorced wife of Munich survivor Ray Wood, compared the treatment of the Munich victims to that of \"dancing bears at the circus\". Nevertheless, the match earned £47,000 for each of the victims' families, while Eric Cantona recouped over £90,000 in expenses directly from the testimonial fund, rather than from the club. The club has also received criticism from some quarters for its poor treatment of the survivors: Johnny Berry was forced to leave the flat he rented from the club to make way for a new player.\n\nOn 7 February 1998, United played Bolton Wanderers at Old Trafford in the Premier League a day after the 40th anniversary of the disaster. The match kicked off at 3:15 pm to allow a minute's silence to be observed at 3:04 pm. Representatives from both teams laid floral tributes to those who lost their lives, with crash survivor and United director Bobby Charlton joined by Bolton president Nat Lofthouse in leading out the two teams.\n\nA memorial service was held at Old Trafford on 6 February 2008. At the conclusion of the service, the surviving members of the 1958 team were the guests of honour at a ceremony to rename the tunnel under the stadium's South Stand as the \"Munich Tunnel\", which features an exhibition about the Busby Babes.\n\nOn 6 February 2008, the England national football team took on Switzerland at Wembley Stadium. Before the game, pictures of the players who lost their lives at Munich were displayed on big screens, and England players wore black armbands. There was also a tribute to the Busby Babes in the match programme. Originally, a minute's silence was not to have been observed on the day, due to the Football Association's fears that the silence would not be respected by fans of Manchester United's rivals; however, they agreed that a minute's silence should be held. In the event, it was generally well-observed, but a small number of supporters made whistles and cat-calls and the referee cut the silence short after less than 30 seconds. Minute silences were also observed at the Northern Ireland, Wales and the Republic of Ireland games.\nOn 10 February 2008, at the derby match between Manchester United and Manchester City at Old Trafford, both teams were led onto the pitch by a lone piper playing \"The Red Flag\", and the managers – Alex Ferguson and Sven-Göran Eriksson – each laid a wreath in the centre circle. This was followed by a minute silence, which, despite previous concerns, was respected by all the fans. Kevin Parker, secretary of Manchester City's supporters club, had originally suggested a minute's applause instead of a minute's silence, so as to drown out anyone who would disrupt the silence, but this was rejected by the Manchester United management as inappropriate. United played in strips reminiscent of those worn by the 1958 team, numbered 1–11 with no advertising on the front or players' names on the back, while City removed sponsors' logos from their kit and the image of a small black ribbon was heat pressed onto the right shoulder; both teams wore black armbands in tribute to the victims of the Munich disaster. Manchester City won 2–1 thanks to first half goals from Darius Vassell and debutant Benjani. Fans in attendance were given commemorative scarves – in red and white for the United fans, and blue and white for the City fans – which were held up during the silence.\n\nSeveral musical tributes to the Munich air disaster have been recorded, the earliest being the song \"The Flowers of Manchester\". Written by an anonymous author, later revealed to be Eric Winter, the editor of the magazine \"Sing\", the song was recorded and released by Liverpool folk band The Spinners on their 1962 debut album \"\"Quayside Songs Old and New\". Manchester-born singer Morrissey also released a song called \"Munich Air Disaster, 1958\" as a B-side to \"Irish Blood, English Heart\"\" in 2004. It later appeared on his live album, \"Live at Earls Court\", in 2005 and his 2009 B-sides compilation, \"Swords\".\n\nMost recently, the English band The Futureheads named their album \"News and Tributes\" in honour of the disaster. The title track pays tribute to those who lost their lives, and includes the verse:\n\nBarry Navidi, producer of the 2004 film \"The Merchant of Venice\", was reported to be working on a script for a Hollywood film about the Munich air crash. The \"Manchester Evening News\" reported on 22 April 2005 that the survivors had not been consulted and were concerned about how accurate the film would be.\n\nBill Foulkes said that, if done right, the film could become a \"tribute to the Busby Babes which could be seen for generations to come\"; however, he expressed concerns about the accuracy of the film, given the filmmakers' lack of first-hand sources about what actually happened in Munich. Fellow survivor Harry Gregg was more concerned about the portrayal of the players, particularly those who died, and whether their families' feelings would be respected.\n\nJohn Doherty, a player who had left United only a few months earlier, was less restrained, saying that \"the only reason anyone would want to make a film like this is to make money\" and that \"while there may be a slight hint of truth in the film, it will be mainly untruths... Unless you were there, how could you know what conversations took place?\".\n\nOn 10 January 2006, the BBC showed a drama/documentary retelling the story in the series \"Surviving Disaster\". The programme was met with criticism from former United winger Albert Scanlon, who claimed that it was full of inaccuracies despite the production having consulted him about the content of the documentary. Errors in the programme included the depiction of Jimmy Murphy giving a pre-match team talk in Belgrade, despite him being in Cardiff at the time, and the plane being shown as only half full when nearly every seat was occupied.\n\nOn 6 February 2008, the 50th anniversary of the crash, several television channels showed programmes about it:\n\nSince the anniversary, two television programmes have been made about the disaster:\n\nThe University of Salford honoured Munich victim Eddie Colman by naming one of its halls of residence after him. Colman was born in Salford in 1936. There is also a network of small roads in Newton Heath named after the players who lost their lives in Munich, including Roger Byrne Close, David Pegg Walk, Geoff Bent Walk, Eddie Colman Close, Billy Whelan Walk, Tommy Taylor Close and Mark Jones Walk. Among those roads is an old people's home named after Duncan Edwards. Edwards was also honoured with street names in his home town of Dudley; there is a small close off Stourbridge Road named Duncan Edwards Close. and in 2008, the Dudley Southern Bypass was renamed Duncan Edwards Way. The road bridge over the Luas tram line at Fassaugh Road, Cabra, Dublin 7 is named after Liam Whelan.\n\n\n\n"}
{"id": "5202510", "url": "https://en.wikipedia.org/wiki?curid=5202510", "title": "Mária Telkes", "text": "Mária Telkes\n\nMária Telkes (December 12, 1900 – December 2, 1995) was a Hungarian-American scientist and inventor who worked on solar energy technologies.\n\nBorn in Budapest, Hungary, Telkes moved from Hungary to the United States after completing her Ph.D. in physical chemistry.\n\nTelkes worked as a biophysicist in the United States; and, from 1939 to 1953, she was involved in solar energy research at Massachusetts Institute of Technology.\n\nTelkes is known for creating the first thermoelectric power generator in 1947, designing the first solar heating system for the Dover Sun House in Dover, Massachusetts with architect Eleanor Raymond, and the first thermoelectric refrigerator in 1953 using the principles of semiconductor thermoelectricity.\n\nShe was a prolific inventor of practical thermal devices, including a miniature desalination unit (solar still) for use on lifeboats, which used solar power and condensation to collect potable water. The still saved the lives of airmen and sailors who would have been without water when abandoned at sea.\n\nOne of her specialties were phase-change materials, including molten salts to store thermal energy. One of her materials of choice was Glauber's salt.\n\nTelkes is considered one of the founders of solar thermal storage systems, earning her the nickname \"the Sun Queen\". She moved to Texas in the 1970s and consulted with a variety of start-up solar companies, including Northrup Solar, which subsequently became ARCO Solar, and eventually BP Solar.\n\nThere are a number of schools named after her, \"Maria Telkes\" in South Carolina, \"Telkes Middle School\" in San Francisco, and \"Telkes Maria High School\" in Ohio.\n\n"}
{"id": "22857322", "url": "https://en.wikipedia.org/wiki?curid=22857322", "title": "Neotropical Ornithological Society", "text": "Neotropical Ornithological Society\n\nThe Neotropical Ornithological Society, or Sociedad de Ornitología Neotropical, is an ornithological non-profit organization, with its principal objective the study and conservation of Neotropical birds and their habitats, including both their breeding and non-breeding areas. It was founded in 1987 by Dr Mario A. Ramos Olmos, and is a member of the Ornithological Council.\n\nThe Society produces the journal Neotropical Ornithology, which publishes papers in Spanish, English and Portuguese. It organises the Neotropical Ornithological Congress and the Francois Vuilleumier Fund for research on Neotropical birds.\n\n\n"}
{"id": "15146939", "url": "https://en.wikipedia.org/wiki?curid=15146939", "title": "Noflan", "text": "Noflan\n\nNoflan is a flame retardant chemical. It was developed in the 1980s by the Moscow State Textile University and the Semenov Institute of Chemical Physics in Moscow, with the aim of fire-proofing the fabric used in Soviet spacecraft. In the 1990s the technology was commercialised and licensed to Firestop Chemicals.\n\nAs halogens and antimony containing flame retardant result in unwanted degradation, alternatives have been under research.\nNoflan is a complex of the amide of alkylphosphonic acid ammonium salt with ammonium chloride but is also alleged to cause corrosion.\n\nNoflan is used to treat fabric and carpets in trains, buses and aircraft, including the Airbus A380.\n\nNoflan was used to protect a giant straw yule goat or julbocken from arson in Gävle, Sweden during December 2006.\n\n\n"}
{"id": "291234", "url": "https://en.wikipedia.org/wiki?curid=291234", "title": "Occultation", "text": "Occultation\n\nAn occultation is an event that occurs when one object is hidden by another object that passes between it and the observer. The term is often used in astronomy, but can also refer to any situation in which an object in the foreground blocks from view (occults) an object in the background. In this general sense, occultation applies to the visual scene observed from low-flying aircraft (or computer-generated imagery) when foreground objects obscure distant objects dynamically, as the scene changes over time.\n\nThe term occultation is most frequently used to describe those relatively frequent occasions when the Moon passes in front of a star during the course of its orbital motion around the Earth. Since the Moon, with an angular speed with respect to the stars of 0.55 arcsec/s or 2.7 µrad/s, has a very thin atmosphere and stars have an angular diameter of at most 0.057 arcseconds or 0.28 µrad, a star that is occulted by the Moon will disappear or reappear in 0.1 seconds or less on the Moon's edge, or limb. Events that take place on the Moon's dark limb are of particular interest to observers, because the lack of glare allows these occultations to more easily be observed and timed.\n\nThe Moon's orbit is inclined to the ecliptic (see orbit of the Moon), and any stars with an ecliptic latitude of less than about 6.5 degrees may be occulted by it. There are three first magnitude stars that are sufficiently close to the ecliptic that they may be occulted by the Moon and by planets – Regulus, Spica and Antares. Occultations of Aldebaran are presently only possible by the Moon, because the planets pass Aldebaran to the north. Neither planetary nor lunar occultations of Pollux are currently possible. However, in the far future, occultations of Pollux will be possible, as they were in the far past. Some deep-sky objects, such as the Pleiades, can also be occulted by the Moon.\n\nWithin a few kilometres of the edge of an occultation's predicted path, referred to as its northern or southern limit, an observer may see the star intermittently disappearing and reappearing as the irregular limb of the Moon moves past the star, creating what is known as a grazing lunar occultation. From an observational and scientific standpoint, these \"grazes\" are the most dynamic and interesting of lunar occultations.\n\nThe accurate timing of lunar occultations is performed regularly by (primarily amateur) astronomers. Lunar occultations timed to an accuracy of a few tenths of a second have various scientific uses, particularly in refining our knowledge of lunar topography. Photoelectric analysis of lunar occultations have also discovered some stars to be very close visual or spectroscopic binaries. Some angular diameters of stars have been measured by timing of lunar occultations, which is useful for determining effective temperatures of those stars. Early radio astronomers found occultations of radio sources by the Moon valuable for determining their exact positions, because the long wavelength of radio waves limited the resolution available through direct observation. This was crucial for the unambiguous identification of the radio source 3C 273 with the optical quasar and its jet, and a fundamental prerequisite for Maarten Schmidt's discovery of the cosmological nature of quasars.\n\nSeveral times during the year, someone on Earth can usually observe the Moon occulting a planet. Since planets, unlike stars, have significant angular sizes, lunar occultations of planets will create a narrow zone on Earth from which a partial occultation of the planet will occur. An observer located within that narrow zone could observe the planet's disk partly blocked by the slowly moving moon. The same mechanic can be seen with the Sun, where observers on Earth will view it as a solar eclipse. Therefore, a total solar eclipse is effectively the same event as the Moon occulting the Sun.\n\nStars may also be occulted by planets. Occultations of bright stars are rare. In 1959, Venus occulted Regulus, and the next occultation of a bright star (also Regulus by Venus) will be in 2044. Uranus's rings were first discovered when that planet occulted a star in 1977. On 3 July 1989, Saturn passed in front of the 5th magnitude star 28 Sagittarii. Pluto occulted stars in 1988, 2002, and 2006, allowing its tenuous atmosphere to be studied via atmospheric limb sounding.\n\nIn rare cases, one planet can pass in front of another. If the nearer planet appears larger than the more distant one, the event is called a mutual planetary occultation.\n\nAn occultation occurs when a minor planet (an asteroid, distant object, or dwarf planet) passes in front of a star (occults a star), temporarily blocking its light (as seen from Earth). These occultations are useful for measuring the size and position of minor planets much more precisely than can be done by any other means. A cross-sectional profile of the shape of an asteroid can even be determined if a number of observers at different, nearby, locations observe the occultation. Occultations have also been used to estimate the diameter of trans-Neptunian objects such as , Ixion, and Varuna.\n\nIn addition, mutual occultation/eclipsing events can occur between a minor planet and its satellite. A large number of these minor-planet moons have been discovered analyzing the photometric light curves of rotating minor planets and detecting a second, superimposed brightness variation, from which an orbital period for the satellite (secondary), and a secondary-to-primary diameter-ratio (for the binary system) can often be derived. \n\n\n\nThe Moon or another celestial body can occult multiple celestial bodies at the same time. Because of its relatively large angular diameter, the Moon at any given time occults an indeterminate number of stars. However, an event when the Moon occults two bright objects (e.g. two planets or a bright star and a planet) simultaneously is extremely rare and can be seen only from a small part of the world. The last event of such type was on 23 April 1998 when the Moon occulted Venus and Jupiter for observers on Ascension Island.\n\nThe \"Big Occulting Steerable Satellite (BOSS)\" was a proposed satellite that would work in conjunction with a telescope to detect planets around distant stars. The satellite consists of a large, very lightweight sheet, and a set of maneuvering thrusters and navigation systems. It would maneuver to a position along the line of sight between the telescope and a nearby star. The satellite would thereby block the radiation from the star, permitting the orbiting planets to be observed.\n\nThe proposed satellite would have a dimension of , a mass of about 600 kg, and maneuver by means of an ion drive engine in combination with using the sheet as a light sail. Positioned at a distance of 100,000 km from the telescope, it would block more than 99.998% of the starlight.\n\nThere are two possible configurations of this satellite. The first would work with a space telescope, most likely positioned near the Earth's Lagrangian point. The second would place the satellite in a highly elliptical orbit about the Earth, and work in conjunction with a ground telescope. At the apogee of the orbit, the satellite would remain relatively stationary with respect to the ground, allowing longer exposure times.\n\nAn updated version of this design is called the Starshade, which uses a sunflower-shaped coronagraph disc. A comparable proposal was also made for a satellite to occult bright X-ray sources, called an \"X-ray Occulting Steerable Satellite\" or XOSS.\n\n\n"}
{"id": "25402762", "url": "https://en.wikipedia.org/wiki?curid=25402762", "title": "Organic molecular tracers", "text": "Organic molecular tracers\n\nOrganic molecular tracers, also referred to as organic molecular markers, are compounds or compound classes of interest in the field of air quality because they can help identify particulate emission sources, as they are relatively unique to those sources. This approach is generally applied to particulate matter under 2.5μm in diameter because of the formation mechanisms and the health risks associated with this size regime. With tracer compounds, the principles of mass balance are used to 'trace' emissions from the source to the receptor site where a sample is taken. Use of organic tracers has become more common as measurement quality has improved, costs have decreased, and compounds that were historically good tracers, such as lead, have decreased in ambient concentrations due to various factors including government regulation.\n\nIn order to be used as a tracer, a compound must be emitted preferentially by some sources and not by others, giving the emission source a relatively unique chemical makeup. The compound must react slowly enough in the atmosphere that it is chemically conserved from the emission source to the receptor site where an ambient sample may be taken. Additionally, a tracer species should not be formed in the atmosphere and it should not volatilize during transport so that mass balance is maintained. Tracer compounds must then be of primary origins (not formed in the atmosphere), which are created through condensation and coagulation of mainly combustion and biological sources.\n\nSamples have been analyzed from many known biogenic and anthropogenic emissions sources such as diesel and gasoline vehicles, cigarette smoke, road dust, vegetative detritus, wood smoke, and meat cooking. Examples of some results of preferential emissions from sources include hopanes, polycyclic aromatic hydrocarbons and steranes\n\nChemical analysis of ambient and source samples is performed using gas chromatography-mass spectrometry, and the chemical profile of the emission sources can be compared to an ambient sample using chemical mass balance techniques to identify the ambient mass contribution from each pollution source. This approach assumes that an ambient air sample has particulate matter contributions from a linear combination of emission sources.\nIf the chemical compositions of local sources are not available, source apportionment models such as positive matrix factorization and principal components analysis can be used by employing statistical methods to identify emissions sources from time series of ambient samples.\n\n"}
{"id": "17921881", "url": "https://en.wikipedia.org/wiki?curid=17921881", "title": "Outline of sustainability", "text": "Outline of sustainability\n\nThe following outline is provided as an overview of and topical guide to sustainability:\n\nSustainability – capacity to endure. For humans, sustainability is the long-term maintenance of well being, which has environmental, economic, and social dimensions, and encompasses the concept of stewardship and responsible resource management.\n\nSustainability\n\n\nSustainabiity is divided into two main branches: sustainability science and sustainability governance. Each of these branches is divided into a number of subfields:\n\nSustainability science\n\nSustainability governance\n\n\n\n\nHistory of sustainability\n\nBiodiversity\n\n\n\nPopulation control\n\n\nEnvironmental technology\n\n\n\nEnergy conservation\n\nOver-consumption\n\nFood\n\nWater\n\nMaterials\n\n\n\n\nPaul Hawken\n\n\n\n"}
{"id": "34207093", "url": "https://en.wikipedia.org/wiki?curid=34207093", "title": "Pechea Wind Farm", "text": "Pechea Wind Farm\n\nThe Pechea Wind Farm is a proposed wind power project in Galați County, Romania. It will have 50 individual wind turbines with a nominal output of around 3 MW which will deliver up to 150 MW of power, enough to power over 130,000 homes, with a capital investment required of approximately €260 million. The project is owned and developed by the French company Électricité de France.\n"}
{"id": "16058443", "url": "https://en.wikipedia.org/wiki?curid=16058443", "title": "Petroleum Exploration and Production Association of New Zealand", "text": "Petroleum Exploration and Production Association of New Zealand\n\nThe Petroleum Exploration and Production Association of New Zealand (PEPANZ) is an incorporated society based in Wellington. Its core focus is to promote the interests of petroleum producers and explorers in New Zealand which includes everything from policy through to education.\n\nFull members are those participating in a mining permit, participating in an exploration permit or a first time holder of any interest in exploration or production acreage. Full members include:\n\nAssociate members include service providers to the oil and gas industry in New Zealand (such as contractors, legal firms, engineers).\n\n"}
{"id": "27786714", "url": "https://en.wikipedia.org/wiki?curid=27786714", "title": "Project Deep Spill", "text": "Project Deep Spill\n\nProject Deep Spill was the first intentional deepwater oil spill, in order to study how crude oil behaved in-depth. A Joint Industry Project comprising 23 oil companies and the Minerals Management Service performed a sea trial in late June 2000 in the Helland Hansen region of the Norwegian Sea. The trial made several releases of varying combinations of crude oil (750 barrels), marine diesel, methane (18 cubic metres) and nitrogen gas from the seabed at 840 metres below sea-level.\n"}
{"id": "21269374", "url": "https://en.wikipedia.org/wiki?curid=21269374", "title": "Pukara of La Compañía", "text": "Pukara of La Compañía\n\nPukara de La Compañia is an archaeological site containing the remains of a promaucae fortress, later used by the Incas, located on the large hill overlooking the village of La Compañia, a village in the commune of Graneros, Chile. It is the southernmost building which remains of the Inca Empire. As such it is an important landmark on what is known as \"The Chilean Inca Trail\", and has been declared a National Monument by the Chilean government.\n\nThere are three main historical periods during which the site was occupied:\n\nThe remains of the pukara consist of the bases of 7 circular structures, a major construction and other nearby buildings that may have been observation posts. In addition, the flat summit of the hill is surrounded by defensive perimeter walls.\n\nPukara La Compañia was declared a Chile National Monument by Decree No. 1191, dated 11 March 1992. Nonetheless, the site lacks suitable access for tourists, and is neither maintained nor adequately protected by the authorities. This lack of respect for the archaeological heritage of the pukara is the evident in the installation of a large antenna of cellular telephony on the summit of the hill. The installation of this antenna, carried out in 1997, consisted of building an access road and movement of earth, which destroyed of one of the defensive walls. \nAs of 2012, O’Higgins Tours had enlisted the skills of graphic designer Eduardo Galdames for an archaeologically-based digital reconstruction of the site. The project is ongoing, will be for educational purposes and was partially funded by the Chilean government. \n\n\n"}
{"id": "10700083", "url": "https://en.wikipedia.org/wiki?curid=10700083", "title": "Quality Protocol", "text": "Quality Protocol\n\nA Quality Protocol is a national end of waste criteria developed by the Waste Protocols Project, a joint initiative between the Environment Agency and Waste & Resources Action Programme (WRAP). It is funded by the Department for Environment, Food and Rural Affairs (Defra), the Welsh Assembly Government (WAG) and the Northern Ireland Environment Agency (NIEA) as a business resource efficiency activity (BREW). \n\nA Quality Protocol sets out criteria for the production of a product from a specific waste type. Compliance with these criteria is considered sufficient to ensure that the fully recovered product may be used without harm to human health or the environment and therefore without the need for waste management controls. In addition, the Quality Protocol indicates how compliance may be demonstrated and points to best practice for the use of the fully recovered product. The Quality Protocol further aims to provide increased market confidence in the quality of products made from waste and so encourage greater recovery and recycling.\n\nThe purpose of the Quality Protocol is to provide a uniform control process for producers, from which they can reasonably state and demonstrate that their product has been fully recovered and is no longer a waste. It also provides purchasers with a quality-managed product to common aggregate standards, which increases confidence in performance. Also, the framework created by the Protocol provides a clear audit trail for those responsible for ensuring compliance with Waste Management Legislation. \n\nWork on producing protocols for eight other waste streams should be issued for public consultation in Summer 2007. This includes waste vegetable oil, flat glass, non-packaging plastics, tyres, contaminated soils, pulverised fuel ash and blast furnace slag. \n\n"}
{"id": "2250414", "url": "https://en.wikipedia.org/wiki?curid=2250414", "title": "Railroad Commission of Texas", "text": "Railroad Commission of Texas\n\nThe Railroad Commission of Texas (RRC; also sometimes called the \"Texas Railroad Commission\", \"TRC\") is the state agency that regulates the oil and gas industry, gas utilities, pipeline safety, safety in the liquefied petroleum gas (LPG) industry, and surface coal and uranium mining. Despite its name, it ceased regulating railroads in 2005.\n\nEstablished by the Texas Legislature in 1891, it is the state's oldest regulatory agency and began as part of the Efficiency Movement of the Progressive Era. From the 1930s to the 1960s it largely set world oil prices, but was displaced by OPEC (Organization of Petroleum Exporting Countries) after 1973. In 1984, the federal government took over transportation regulation for railroads, trucking and buses, but the Railroad Commission kept its name. With an annual budget of $79 million, it now focuses entirely on oil, gas, mining, propane, and pipelines, setting allocations for production each month.\n\nThe three-member commission was initially appointed by the governor, but an amendment to the state's constitution in 1894 established the commissioners as elected officials who serve overlapping six-year terms, like the sequence in the U.S. Senate. No specific seat is designated as chairman; the commissioners choose the chairman from among themselves. Normally the commissioner who faces reelection is the chairman for the preceding two years. The current commissioners are Christi Craddick since December 17, 2012, Ryan Sitton since January 5, 2015, and Wayne Christian since January 9, 2017.\n\nAttempts to establish a railroad commission in Texas began in 1876. After five legislative failures, an amendment to the state constitution providing for a railroad commission was submitted to voters in 1890. The amendment's ratification and the 1890 election of Governor James S. Hogg, a liberal Democrat, permitted the legislature in 1894 to create the Railroad Commission, giving it jurisdiction over operations of railroads, terminals, wharves, and express companies. It could set rates, issue rules on how to classify freight, require adequate railroad reports, and prohibit and punish discrimination and extortion by corporations. George Clark, running as an independent \"Jeffersonian Democratic\" candidate for governor in 1892, denounced the TRE as, \"Wrong in principle, undemocratic, and unrepublican Commissions do no good. They do harm. Their only function is to harass. I regard it as essentially foolish and essentially vicious.\" Clark lost the 1892 election to Hogg but a federal judge ruled the TRC illegal; the judge in turn was overruled by the United States Supreme Court. The governor appointed the first members; the first elections to the commission were held in 1893, with three commissioners serving six-year, overlapping terms. The TRC did not have jurisdiction over interstate rates, but Texas was so large that the in-state traffic it regulated was of dominant importance.\n\nJohn H. Reagan (1818–1903), the first head of the TRC (1891–1903), had been the most outspoken advocate in Congress of bills to regulate railroads in the 1880s. He feared the corruption caused by railroad monopolies and considered their control a moral challenge. As chairman of the TRC, he changed his views when he became acquainted with the realities of the complex forces affecting railroad management. Reagan turned to the Efficiency Movement for ideas, establishing a pattern of regulatory practice that TRC used for decades. He believed that the agency should pursue two main goals: to protect consumers from unfair railway practices and excessive rates, and to support the state's overall economic growth. To find the optimal rates that met these goals he focused the TRC on the collection of data, direct negotiation with railway executives, and compromises with the parties involved. The agency did not have the legal authority to set rates, nor did it have the resources to spend much of its time in court battles. The carrot was far more important than the stick. Freight rates continued to decline dramatically. In 1891, a typical rate was 1.403 cents per ton mile. By 1907 the rate was 1.039 cents – a decline of 25%. However the railroads did not have rates high enough for them to upgrade their equipment and lower costs in the face of competition from pipelines, cars and trucks, and the Texas railway system began a slow decline.\n\nFrom the 1890s through the 1960s, the Texas Railroad Commission found it difficult to enforce fully Jim Crow segregation legislation. Because of the expense involved, Texas railroads often allowed wealthier blacks to mix with whites, rather than provide separate cars, dining facilities, and even depots. In addition, West Texas authorities often refused to enforce Jim Crow laws because few African Americans resided there. In the 1940s, the railroad commission's enforcement of segregation laws began collapsing further, in part because of the great number of African American soldiers that were transported during World War II. The trains were integrated in the early 1960s.\n\nThe agency's reach expanded as it took over responsibility for regulating oil pipelines (in 1917), oil and gas production (1919), natural gas delivery systems (1920), bus lines (1927), and trucking (1929). It grew from 12 employees in 1916 to 69 in 1930 and 566 in 1939. It does not have jurisdiction over investor owned utility companies; that falls under the jurisdiction of the Public Utility Commission of Texas.\n\nA crisis for the petroleum industry was created by the East Texas oil boom of the 1930s, as prices plunged to 25 cents a barrel. The traditional TRC policy of negotiating compromises failed; the governor was forced to call in the state militia to enforce order. Texas oilmen decided they preferred state to federal regulation, and wanted the TRC to give out quotas so that every producer would get higher prices and profits. Pure Oil Company opposed the first statewide oil prorationing order, which was issued by the TRC in August 1930. The order, which was intended to conserve oil resources by limiting the number of barrels drilled per day, was seen by small producers like Pure as a conspiracy between government and major companies to drive them out of business and foster monopoly in the oil industry.\n\nErnest O. Thompson (1892–1966), head of the TRC from 1932 to 1965, took charge of the agency and indeed the oil industry by appealing to an ideal of Texas' role in the global oil order—the civil religion of Texas oil. He cajoled, harangued, and browbeat recalcitrant producers into compliance with the TRC's prorationing orders. The New Deal allowed the TRC to set national oil policy. As late as the 1950s the TRC controlled over 40% of United States crude production and approximately half of estimated national proved reserves. It served as a model in the creation of OPEC. Gordon M. Griffin, chief engineer of the TRC during World War II, developed the formula for prorationing to keep production flowing for the military.\n\nBecause it needed access to the Texas headquarters of the various oil companies, it became a long term tenant at the Milam Building.\n\nRegulation was a practical rather than ideological affair. The TRC typically worked with the regulated industries to improve operations, share best practices, and address consumer complaints. Radical activities like rate setting to favor shippers or producers or consumers, and heated court battles, were the exception rather than the rule.\n\nWithin the oil and gas industry, it took into account production in other states, in effect bringing total available supply, (including imports, which were small) within the principle of prorationing to market demand. Allowable oilfield production was calculated as follows: estimated market demand, minus uncontrolled additions to supply, gave the Texas total; this was then prorated among fields and wells in a manner calculated to preserve equity among producers, and to prevent any well from producing beyond its Maximum Efficient Rate (MER). Scheduled allowables are expressed in numbers of calendar days of permitted production per month at MER. In the Spring of 2013, new hydraulic fracturing water recycling rules were adopted in the state of Texas by the Railroad Commission of Texas. The Water Recycling Rules are intended to encourage Texas hydraulic fracturing operators to conserve water used in the hydraulic fracturing process for oil and gas wells.\n\nAs of January 2017, the commission members are Christi Craddick (Chairman), Ryan Sitton, and Wayne Christian. All three members are Republicans. Craddick was elected for a six-year seat in the 2012 general election. Sitton was elected for a six-year term in the 2014 general election. Christian was elected for a six-year term in the 2016 general election.\n\nEffective October 1, 2005, as a result of House Bill 2702 the rail oversight functions of the Railroad Commission were transferred to the Texas Department of Transportation. The traditional name of the Commission was not changed despite the loss of its titular regulatory duties.\n\nThe Shreveport Rate Case, also known as \"Houston E. & W. Ry. Co. v. United States\", 234 U.S. 342 (1914) arose from the Railroad Commission's setting railroad freight rates unequally. Because of the low intrastate rates, shippers in eastern Texas tended to ship their wares to Dallas (in Texas), rather than to Shreveport, Louisiana, although Shreveport was considerably closer to much of eastern Texas. The Railroad Commission's (and the railroad's) position was that only the state could regulate commerce within a state, and that the federal government had no power so to do. The Supreme Court ruled that the federal government's ability to regulate interstate commerce necessarily included the ability to regulate intrastate \"operations in all matters having a close and substantial relation to interstate traffic\" and to ensure that \"interstate commerce may be conducted upon fair terms\".\n\nThe Railroad Commission has also figured prominently in two major U.S. Supreme Court cases on the doctrine of abstention:\nThe agency is headquartered in the William B. Travis State Office Building at 1701 North Congress Avenue in Austin. In addition, the Texas Railroad Commission has twelve oil and gas district offices located throughout the state. The district offices facilitate communication between industry representatives and the Commission.\n\n\n"}
{"id": "10386571", "url": "https://en.wikipedia.org/wiki?curid=10386571", "title": "Rebound effect (conservation)", "text": "Rebound effect (conservation)\n\nIn conservation and energy economics, the rebound effect (or take-back effect) is the reduction in expected gains from new technologies that increase the efficiency of resource use, because of behavioral or other systemic responses. These responses usually tend to offset the beneficial effects of the new technology or other measures taken.\n\nWhile the literature on the rebound effect generally focuses on the effect of technological improvements on energy consumption, the theory can also be applied to the use of any natural resource or other input, such as labor. The rebound effect is generally expressed as a ratio of the lost benefit compared to the expected environmental benefit when holding consumption constant.\n\nFor instance, if a 5% improvement in vehicle fuel efficiency results in only a 2% drop in fuel use, there is a 60% rebound effect (since = 60%). The 'missing' 3% might have been consumed by driving faster or further than before.\n\nThe existence of the rebound effect is uncontroversial. However, debate continues as to the magnitude and impact of the effect in real world situations.\nDepending on the magnitude of the rebound effect, there are five different rebound effect (RE) types:\n\nIn order to avoid the rebound effect, environmental economists have suggested that any cost savings from efficiency gains be taxed in order to keep the cost of use the same.\n\nThe rebound effect was first described by William Stanley Jevons in his 1865 book \"The Coal Question\", where he observed that the invention in Britain of a more efficient steam engine meant that the use of coal became economically viable for many new uses. This ultimately led to increased coal demand and much increased coal consumption, even as the amount of coal required for any particular use fell. According to Jevons, \"It is a confusion of ideas to suppose that the economical use of fuel is equivalent to diminished consumption. The very contrary is the truth.\"\n\nHowever, most contemporary authors credit Daniel Khazzoom for the re-emergence of the rebound effect in the research literature. Although Khazzoom did not use the term, he raised the idea that there is a less than one-to-one correlation between gains in energy efficiency and reductions in energy use, because of a change in the 'price content' of energy in the provision of the final consumer product. His study was based on energy efficiency gains in home appliances, but the principle applies throughout the economy. A commonly studied example is that of a more fuel-efficient car. As each kilometre of travel becomes cheaper, there will be an increase in driving speed and/or kilometres driven, as long as the price elasticity of demand for car travel is not zero. Other examples might include the growth in garden lighting after the introduction of energy-saving Light Emitting Diodes or the increasing size of houses driven partly by higher fuel efficiency in home heating technologies. If the rebound effect is larger than 100%, all gains from the increased fuel efficiency would be wiped out by increases in demand (the Jevons paradox).\n\nKhazzoom's thesis was criticized heavily by Michael Grubb and Amory Lovins who dismissed any disconnection between energy efficiency improvements in an individual market, and an economy-wide reduction in energy consumption. Developing Khazzoom's idea further, and prompting heated debate in the Energy Policy journal at that time, Len Brookes wrote of the fallacies in the energy-efficiency solution to greenhouse gas emissions. His analysis showed that any economically justified improvements in energy efficiency would in fact stimulate economic growth and increase total energy use. For improvements in energy efficiency to contribute to a reduction in economy-wide energy consumption, the improvement must come at a greater economic cost. Commenting in regard to energy efficiency advocates, he concludes that, \"the present high profile of the topic seems to owe more to the current tide of green fervor than to sober consideration of the facts, and the validity and cost of solutions.\"\n\nIn 1992, economist Harry Saunders coined the term \"Khazzoom-Brookes postulate\" to describe the idea that energy efficiency gains paradoxically result in increases in energy use (the modern day equivalent of the Jevons paradox). He modeled energy efficiency gains using a variety of neo-classical growth models, and showed that the postulate is true over a wide range of assumptions. In the conclusion of his paper, Saunders stated that:\nThis work provided a theoretical grounding for empirical studies and played an important role in framing the problem of the rebound effect. It also reinforced an emerging ideological divide between energy economists on the extent of the yet to be named effect. The two tightly held positions are:\n\nEven though many studies have been undertaken in this area, neither position has yet claimed a consensus view in the academic literature. Recent studies have demonstrated that direct rebound effects are significant (about 30% for energy), but that there is not enough information about indirect effects to know whether or how often back-fire occurs. Economists tend to the first position, but most governments, businesses, and environmental groups adhere to the second. Governments and environmental groups often advocate further research into fuel efficiency and radical increases in the efficient use of energy as the primary means for reducing energy use and reducing greenhouse gas emissions (to alleviate the impacts of climate change). However, if the first position more accurately reflects economic reality, current efforts to invent fuel-efficient technologies may not much reduce energy use, and may in fact paradoxically increase oil and coal consumption, and greenhouse gas emissions, over the long run.\n\nThe full rebound effect can be distinguished into three different economic reactions to technological changes: \nIn the example of improved vehicle fuel efficiency, the direct effect would be the increased fuel use from more driving as driving becomes cheaper. The indirect effect would incorporate the increased consumption of other goods enabled by household cost savings from increased fuel efficiency. Since consumption of other goods increases, the embodied fuel used in the production of those goods would increase as well. Finally, the economy-wide effect would include the long-term effect of the increase in vehicle fuel efficiency on production and consumption possibilities throughout the economy, including any effects on economic growth rates.\n\nFor cost reducing resource efficiency, distinguishing between direct and indirect effects is shown in Figure 1 below. The horizontal axis shows units of consumption of the targets good (which could be for example clothes washing, and measured in terms of kilograms of clean clothes) with consumption of all other goods and services on the vertical axis. An economical technology change that enables each unit of washing to be produced with less electricity results in a reduction of the price per unit of washing. This shifts the household budget line rightwards. The result is a substitution effect because of the decreased relative price, but also an income effect due to the increased real income. The substitution effect increases consumption of washing from Q1 to QS, and the income effect from QS to Q2. The total increase in consumption of washing from Q1 to Q2 and the resulting increase in electricity consumption is the direct effect. The indirect effect comprises the increase in other consumption, from O1 to O2. The scale of each of these effects depends on the elasticity of demand for each of the goods, and the embodied resource or externality associated with each good. Indirect effects are difficult to measure empirically. In the manufacturing sector, it has been estimated that there is about a 24% rebound effect due to increases in fuel efficiency. A parallel effect will happen for cost saving efficient technologies for producers, where output and substitution effects will occur.\n\nThe rebound effect can increase the difficulty of projecting the reduction in greenhouse emissions from an improvement in energy efficiency. Estimation of the scale of direct effects on residential electricity, heating and motor fuel consumption has been common motivation for research of rebound effects. Evaluation and econometric methods are the two approaches generally employed in estimating the size of this effect. Evaluation methods rely on quasi-experimental studies and measure the before and after changes to energy consumption from the implementation of energy efficient technology, while econometric methods utilize elasticity estimates to forecast the likely effects from changes in the effective price of energy services.\n\nResearch has found that in developed countries, the direct rebound effect is usually small to moderate, ranging from roughly 5% to 40% in residential space heating and cooling. Some of the direct rebound effect can be attributed to consumers who were previously unable to use a service. However, the rebound effect may be more significant in the context of the undeveloped markets in developing economies.\n\nFor conservation measures, indirect effects closely approximate the total economy-wide effect. Conservation measures constitute a change in consumption patterns away from particular targeted goods towards other goods. Figure 2 shows that a change in preference of a household results in a new consumption pattern that has less of the target good (QT to QT`), and more of all other goods (QO to QO`). The resource consumption or externalities embodied in this other consumption is the indirect effect.\n\nAlthough a persuasive view has prevailed that indirect effects with respect to energy and greenhouse emissions should be very small due to energy directly comprising only a small component of household expenditure, this view is gradually being eroded. Many recent studies based on life-cycle analysis show the energy consumed indirectly by households is often higher than consumed directly through electricity, gas, and motor fuel, and is a growing proportion. This is evident in the results of recent studies that indicate indirect effects from household conservation can range from 10% to 200% depending on the scenario, with higher indirect rebounds from diet changes aiming to reduce food miles.\n\nEven if the direct and indirect rebound effects add up to less than 100%, technological improvements that increase efficiency may still result in economy-wide effects that results in increased resource use for the economy as a whole. In particular, this would happen if increased resource efficiency enables an expansion of production in the economy, and an increase in the rate of economic growth. For example, for the case of energy use, more efficient technology is equivalent to a lower price for energy resources. It is well known that changes in energy costs have a large impact on economic growth rates. In the 1970s, sharp increases in petroleum prices led to stagflation (recession and inflation) in the developed countries, whereas in the 1990s lower petroleum prices contributed to higher economic growth. An improvement in energy efficiency has the same effect as lower fuel prices, and leads to faster economic growth. Economists generally believe that especially for the case of energy use, more efficient technologies will lead to increased use, because of this growth effect.\n\nTo model the scale of this effect, economists use computational general equilibrium (CGE) models. While CGE methodology is by no means perfect, results indicate that economy-wide rebound effects are likely to be very high, with estimates above 100% being rather common. One simple CGE model has been made available online for use by economists.\n\nResearch has shown that the direct rebound effects for energy services is lower at high income levels, due to less price sensitivity. Studies have found that own-price elasticity of gas consumption by UK households was two times greater for households in the lowest income decile when compared to the highest decile. Studies have also observed higher rebounds in low-income houses for improvements in heating technology. Evaluation methods have also been used to assess the scale of rebound effects from efficient heating installations in lower income homes in the United Kingdom. This research found that direct effects are close to 100% in many cases. High income households in developed countries are likely to set the temperature at the optimum comfort level, regardless of the cost – therefore any cost reduction does not result in increased heating, for it was already optimal. But low-income households are more price sensitive, and have made thermal sacrifices due to the cost of heating. In this case, a high direct rebound is likely. This analogy can be extended to most household energy consumption.\n\nThe size of the rebound effect is likely to be higher in developing countries according to macro-level assessments and case studies. One case study was undertaken in rural India to evaluate the impact of an alternative energy scheme. Households were given solar powered lighting in an attempt to reduce the use of kerosene for lighting to zero except for seasons with insufficient sunshine. The scheme was also designed to encourage a future willingness to pay for efficient lighting. The results were surprising, with high direct rebounds between 50 and 80%, and total direct and indirect rebound above 100%. Because the new lighting source was essentially zero cost, operating hours for lighting went up from an average of 2 to 6 per day, with new lighting consisting of a combination of both the no-cost solar lamps and also kerosene lamps. Also, more cooking was undertaken which enabled an increased trade of food with neighboring villages.\n\nThe individual opportunity of cost is an often overlooked cause of the rebound effect. Just as improved workplace tools result in an increased expectation of productivity, so does the increased availability of time result in an increase in demand for a service. Research articles often examine increasingly convenient and more rapid modes of transportation to determine the rebound effect in energy demand. Because time cost forms a major part of the total cost of commuter transport, rapid modes will reduce real costs, but will also encourage longer commuting distances which will in turn increase energy consumption. While important, it is almost impossible to estimate empirically the scale of such effects due to the subjective nature of the value of time. Time saved can either be used towards additional work or leisure which may have differing degrees of rebound effect. Labor time saved at work due to the increased labour productivity is likely to be spent on further labor time at higher productive rates. For leisure time saving, this may simply encourage people to diversify their leisure interests to fill their generally fixed period of leisure time.\n\nIn order to ensure that efficiency enhancing technological improvements actually reduce fuel use, the ecological economists Mathis Wackernagel and William Rees have suggested that any cost savings from efficiency gains be \"taxed away or otherwise removed from further economic circulation. Preferably they should be captured for reinvestment in natural capital rehabilitation.\" This can be achieved through, for example, the imposition of a green tax, a cap and trade program, higher fuel taxes or the proposed \"restore\" approach where part of the savings is directed back to the resource. Policies can also directly address projected yearly consumption of energy rather than device efficiency, especially for systems where the use can be accurately projected, such as street lighting.\n\n\n"}
{"id": "18484291", "url": "https://en.wikipedia.org/wiki?curid=18484291", "title": "Resource intensity", "text": "Resource intensity\n\nResource intensity is a measure of the resources (e.g. water, energy, materials) needed for the production, processing and disposal of a unit of good or service, or for the completion of a process or activity; it is therefore a measure of the efficiency of resource use. It is often expressed as the quantity of resource embodied in unit cost e.g. litres of water per $1 spent on product. In national economic and sustainability accounting it can be calculated as units of resource expended per unit of GDP. When applied to a single person it is expressed as the resource use of that person per unit of consumption. Relatively high resource intensities indicate a high price or environmental cost of converting resource into GDP; low resource intensity indicates a lower price or environmental cost of converting resource into GDP.\n\nResource productivity and resource intensity are key concepts used in sustainability measurement as they measure attempts to decouple the connection between resource use and environmental degradation. Their strength is that they can be used as a metric for both economic and environmental cost. Although these concepts are two sides of the same coin, in practice they involve very different approaches and can be viewed as reflecting, on the one hand, the efficiency of resource production as outcome per unit of resource use (resource productivity) and, on the other hand, the efficiency of resource consumption as resource use per unit outcome (resource intensity). The sustainability objective is to maximize resource productivity while minimizing resource intensity.\n\n\n"}
{"id": "458202", "url": "https://en.wikipedia.org/wiki?curid=458202", "title": "Semiheavy water", "text": "Semiheavy water\n\nSemiheavy water is the result of replacing one of the protium in light water to deuterium. It exists whenever there is water with light hydrogen (protium, H) and deuterium (D or H) in the mix. This is because hydrogen atoms (hydrogen-1 and deuterium) are rapidly exchanged between water molecules. Water containing 50% H and 50% D in its hydrogen contains about 50% HDO and 25% each of HO and DO, in dynamic equilibrium.\nIn regular water, about 1 molecule in 3,200 is HDO (one hydrogen in 6,400 is D). By comparison, heavy water DO occurs at a proportion of about 1 molecule in 41 million (i.e., one in 6,400). This makes semiheavy water far more common than \"normal\" heavy water.\n\nThe freezing point of semiheavy water is close to the freezing point of heavy water. (3.8°C)\n\n"}
{"id": "2269070", "url": "https://en.wikipedia.org/wiki?curid=2269070", "title": "Simoom", "text": "Simoom\n\nSimoom ( \"samūm\"; from the root \"s-m-m\", \"to poison\") is a strong, dry, dust-laden wind. The word is generally used to describe a local wind that blows in the Sahara, Israel, Jordan, Syria, and the deserts of Arabian Peninsula. Its temperature may exceed and the humidity may fall below 10%. Alternative spellings include samoon, samun, simoun, and simoon. Another name used for this wind is samiel (Persian \"samyeli\"). Simoom winds have an alternative type occurring in the region of Central Asia known as \"Garmsil\" (гармсель).\n\nThe name means \"poison wind\" and is given because the sudden onset of simoom may also cause heat stroke. This is attributed to the fact that the hot wind brings more heat to the body than can be disposed of by the evaporation of perspiration.\n\n\"The Nuttall Encyclopædia\" described the simoom:\n\nThe storm moves in cyclone (circular) form, carrying clouds of dust and sand, and produces on humans and animals a suffocating effect.\n\nA 19th-century account of simoom in Egypt reads:\n\nEgypt is also subject, particularly during the spring and summer, to the hot wind called the \"samoom,\" which is still more oppressive than the khamasin winds, but of much shorter duration, seldom lasting longer than a quarter of an hour or twenty minutes. It generally proceeds from the south-east or south-south-east, and carries with it clouds of dust and sand.\n\nIt has been alleged that a \"simoom\" occurred on June 17, 1859 in Goleta and Santa Barbara, California. Local historian Walker Tompkins wrote that \nduring the morning, the temperature hovered around the normal , but around 1 PM, strong super hot winds filled with dust began to blow from the direction of the Santa Ynez Mountains to the north. By 2 PM, the temperature supposedly reached . This temperature was said to have been recorded by an official U.S. coastal survey vessel that was operating in the waters just offshore, in the Santa Barbara Channel. At 5 PM, the temperature had reportedly dropped to , and by 7 PM, the temperature was back to a normal . Tompkins provided a supposed quote from a U.S. government report saying, \"Calves, rabbits and cattle died on their feet. Fruit fell from trees to the ground scorched on the windward side; all vegetable gardens were ruined. A fisherman in a rowboat made it to the Goleta Sandspit with his face and arms blistered as if he had been exposed to a blast furnace.\" Also according to Tompkins, local inhabitants were saved from the heat by seeking shelter in the thick adobe walled houses that were the standard construction at the time.\n\nHowever, experts contest this account. UCSB Professor Joel Michaelsen, for instance, said: “I have never found any outside source to validate Tompkins' story, and I am highly skeptical of its veracity. I don't doubt that strong hot, dry downslope winds could kick up lots of dust and produce very high temperatures - but in the 110 F - 115 F range at most. The 133 F just isn't physically reasonable, as it would require the creation of an extremely hot air mass somewhere to the northeast. Last Monday's weather was a very good strong example of the sort of conditions that would produce such a heat wave, and our temperatures topped out at least 20 degrees below Tompkins' figure. Stronger winds could have increased the heating a bit, but not nearly that much. Add to all that meteorologically-based skepticism Tompkins' well-known tendency to mix liberal doses of fiction into his 'histories,' and I think you have a strong case for discounting this one.”\n\nMeteorologist Christopher C. Burt wrote about the alleged incident: \"There is no record of who made this measurement or exactly where it was made in Santa Barbara. Some later sources say it was made on a U.S. coastal geo-survey vessel. If that is the case then the temperature is not possible since the waters off Santa Barbara in June are never warmer than about 70°F and any wind blowing over the ocean would have its temperature modified by the cool water no matter how hot the air. This report is singular and there is physical evidence (burnt crops and dead animals) that something amazing happened here this day, but the temperature record is impossible to validate.\"\n\nEdgar Allan Poe's short story \"MS. Found in a Bottle\" (1833) features a storm off the coast of Java, wherein \"every appearance warranted me [the protagonist-narrator] in apprehending a Simoom.\"\n\nIn the political essay \"Chartism\", Thomas Carlyle argues that even the poorest of men who have resigned themselves to misery and toil cannot resign themselves to injustice because they retain an innate sense that a higher (divine) justice must govern the world: \"Force itself, the hopelessness of resistance, has doubtless a composing effect against inanimate Simooms, and much other infliction of the like sort, we have found it suffice to produce complete composure. Yet one would say a permanent Injustice even from an Infinite Power would prove unendurable by men.\"\n\n\"Walden\" (1854), by Henry David Thoreau, references a simoom; he uses it to describe his urge to escape something most unwanted. \"There is no odor so bad as that which arises from goodness tainted. It is human, it is divine, carrion. If I knew for a certainty that a man was coming to my house with the conscious design of doing me good, I should run for my life, as from that dry and parching wind of the African deserts called the simoom, which fills the mouth and nose and ears and eyes with dust till you are suffocated, for fear that I should get some of his good done to me—some of its virus mingled with my blood. No—in this case I would rather suffer evil the natural way.\"\n\nIn his 1854 novel \"Hard Times\", Charles Dickens in describing the oppressive midsummer heat of the sooty, smoky factories of Coketown, writes, \"The atmosphere of those Fairy palaces was like the breath of the simoom; and their inhabitants, wasting with heat, toiled languidly in the desert\" (book 2, chapter 1). In \"American Notes\" Dickens also describes \"that injurious [political] Party Spirit\" as \"the Simoom of America, sickening and blighting everything of wholesome life within its reach.\" (p. 93 in the 1913 Chapman & Hall, Ltd. edition, online from Project Gutenberg).\n\nIn Bram Stoker's novel \"Dracula\" (1897), Lucy, describing the appearance of Dracula in her room, writes in her journal entry on September 17 that \"a whole myriad of little specks seemed to come blowing in through the broken window, and wheeling and circling round like the pillar of dust that travellers describe when there is a simoom in the desert.\"\n\nIn James Joyce's novel \"A Portrait of the Artist as a Young Man\" (1914), there is a reference to \"Stephen's heart [withering] up like a flower of the desert that feels the simoom coming from afar.\"\n\nIn Sinclair Lewis' novel \"Main Street\" (1920), there is a reference to \"Aunt Bessie's simoom of questioning.\"\n\nIn keeping with its tradition of naming its aircraft engines after winds, the Wright Aeronautical R-1200 of 1925 was called the Simoon.\n\nA simoon strikes during chapter 2 of the film serial \"Tarzan the Tiger\" (1929).\n\nIn \"Making a President\" (1932), H. L. Mencken refers to \"a veritable simoon of hiccups.\"\n\nIn Patrick O'Brian's novel \"Post Captain\" (1972), Diana Villiers' mentally troubled cousin, Edward Lowndes, upon learning that Doctor Maturin is a naval surgeon, remarks, \"Very good—you are \"upon\" the sea but not \"in\" it: you are not an advocate for cold baths. The sea, the sea! Where should we be without it? Frizzled to a mere toast, sir; parched, desiccated by the simoom, the dread simoom.\"\nA song titled \"Simoon\" features on the Yellow Magic Orchestra's epoymously titled album that was released in 1978. Also, The Creatures have a song called \"Simoom\" on their 1989 album \"Boomerang\".\n\nIn the film \"The English Patient\" (1996) there is a scene in which Count László Almásy regales Katharine Clifton with histories of named winds, one of them being the \"Simoon.\" Alluding to the records of Herodotus, Almásy tells Katharine that there was once a certain Arabic people who deemed the \"Simoon\" so evil that they marched out to meet it ranked as an army, \"their swords raised.\"\n\nIn the collectible card game , card named \"Simoon\" first appeared in the Visions expansion set on a fictional continent of Jamuraa. This card saw play in the sideboard of contemporary Type II decks and was especially effective against the popular Five Colours Green decks that heavily relied on small creatures with toughness of 1.\n\n\n"}
{"id": "48972532", "url": "https://en.wikipedia.org/wiki?curid=48972532", "title": "Sodium bismuth titanate", "text": "Sodium bismuth titanate\n\nSodium bismuth titanate or bismuth sodium titanium oxide (NBT or BNT) is a solid inorganic compound of sodium, bismuth, titanium and oxygen with the chemical formula of NaBiTiO or BiNaTiO. This compound adopts the perovskite structure.\n\nNaBiTiO is not a naturally occurring mineral and several synthesis routes to obtain the compound have been developed. It can be easily prepared by solid state reaction between NaCO, BiO and TiO at temperatures around 850 °C.\n\nThe exact room-temperature crystal structure of sodium bismuth titanate has been a matter of debate for several years. Early studies in the 1960s using X-ray diffraction suggested NaBiTiO to adopt either a pseudo-cubic or a rhombohedral crystal structure. In 2010, based on the high-resolution single-crystal X-ray diffraction data, a monoclinic structure (space group Cc) was proposed. On heating, NaBiTiO transforms at 533 ± 5 K to a tetragonal structure (space group P4bm) and above 793 ± 5 K to cubic structure (space group Pmm).\n\nNaBiTiO is a relaxor ferroelectric. Its optical band gap was reported to be in the 3.0–3.5 eV.\n\nVarious solid solutions with tetragonal ferroelectric perovskites including BaTiO, BiKTiO have been developed to obtain morphotropic phase boundaries to enhance the piezoelectric properties of NaBiTiO. The extraordinarily large strain generated by a field-induced phase transition in sodium bismuth titanate-based solid solutions prompted researchers to investigate its potential as an alternative to lead zirconate titanate for actuator applications.\n\nLead-Free Piezoelectrics, Ed. Shashank Priya and Sahn Nahm,(2012), Springer-Verlag, New York. DOI:10.1007/978-1-4419-9598-8.\n"}
{"id": "12202917", "url": "https://en.wikipedia.org/wiki?curid=12202917", "title": "Taylor dispersion", "text": "Taylor dispersion\n\nTaylor dispersion is an effect in fluid mechanics in which a shear flow can increase the effective diffusivity of a species. Essentially, the shear acts to smear out the concentration distribution in the direction of the flow, enhancing the rate at which it spreads in that direction. The effect is named after the British fluid dynamicist G. I. Taylor.\n\nThe canonical example is that of a simple diffusing species in uniform\nPoiseuille flow through a uniform circular pipe with no-flux\nboundary conditions.\n\nWe use \"z\" as an axial coordinate and \"r\" as the radial\ncoordinate, and assume axisymmetry. The pipe has radius \"a\", and\nthe fluid velocity is:\n\nThe concentration of the diffusing species is denoted \"c\" and its\ndiffusivity is \"D\". The concentration is assumed to be governed by\nthe linear advection–diffusion equation:\n\nThe concentration and velocity are written as the sum of a cross-sectional average (indicated by an overbar) and a deviation (indicated by a prime), thus:\n\nUnder some assumptions (see below), it is possible to derive an equation just involving the average quantities:\n\nObserve how the effective diffusivity multiplying the derivative on the right hand side is greater than the original value of diffusion coefficient, D. The effective diffusivity is often written as:\n\nwhere formula_7 is the Péclet number, based on the channel diameter formula_8. The effect of Taylor dispersion is therefore more pronounced at higher Péclet numbers.\n\nThe assumption is that formula_9 for given formula_10, which is the case if the length scale in the formula_10 direction is long enough to smoothen out the gradient in the formula_12 direction. This can be translated into the requirement that the length scale formula_13 in the formula_10 direction satisfies:\n\nDispersion is also a function of channel geometry. An interesting phenomena for example is that the dispersion of a flow between two infinite flat plates and a rectangular channel, which is infinitely thin, differs approximately 8.75 times. Here the very small side walls of the rectangular channel have an enormous influence on the dispersion.\n\nWhile the exact formula will not hold in more general circumstances, the mechanism still applies, and the effect is stronger at higher Péclet numbers. Taylor dispersion is of particular relevance for flows in porous media modelled by Darcy's law.\n\n"}
{"id": "1675948", "url": "https://en.wikipedia.org/wiki?curid=1675948", "title": "Traction power network", "text": "Traction power network\n\nA traction network or traction power network is an electricity grid for the supply of electrified rail networks. The installation of a separate traction network generally is only done if the railway in question uses alternating current (AC) with a frequency lower than that of the national grid, such as in Germany, Austria and Switzerland.\n\nAlternatively, the three-phase alternating current of the power grid can be converted in substations by rotary transformers or static inverters into the voltage and type of current required by the trains. For railways which run on direct current (DC), this method is always used, as well as for railways which run on single-phase AC of decreased frequency, as in Mecklenburg-Western Pomerania, Saxony-Anhalt, Norway and Sweden. In these areas there are no traction current networks.\n\nSeparate power for traction apart from industrial power always has historic roots. There is no reason today to apply different frequencies or current types than for transmission and for industrial usage. However, the advantage with DC traction was the easier transmission with single copper wires to the feeder points. The advantage with AC traction is the easier transmission over long distances to the feeder points. Beyond these parameters and securing former investment, no evidence exists to stay with different current schemes in networks.\n\nDedicated traction current lines are used when railways are supplied with low-frequency alternating current (AC). The traction current supply line is connected to substations along the line of the railway and is usually run separately from the overhead catenary wire from which the locomotives are fed.\n\nIn countries in which the electric trains run with direct current or with single-phase alternating current with the frequency of the general power grid, the required conversion of the current is performed in the substations, so again no traction current lines are required.\n\nTraction current supply lines are not usually laid parallel to the railway line, in order to allow a shorter line length and to avoid unnecessary influences to the electrical system near the railway line; this also is applied to the current supply of some rapid-transit railways operating with alternating current in Germany.\n\nIt is also possible to lay out the traction current supply on special cross beams right on the overhead wire pylons above the catenary wire. Because the overhead line pylons have a smaller cross section than traction current supply masts, the cross beams cannot be too wide, so the standard arrangement of four conductor cables in one level cannot be used. In this case, a two-level arrangement is used, or with two electric circuits for double-railed lines the overhead line pylons for both directions are equipped with cross beams for their own traction current system of two conductor cables each.\n\nIn densely populated areas, there are pylons which carry circuits for both traction current and for three-phase alternating current for general power. Such lines are found where right of ways are rare. In particular the parallel route of 110 kV and 220 kV three-phase AC is common. The use of 380 kV power lines on the same pylon requires 220 kV insulators for the traction current line, because in case the 380 kV line fails, voltage spikes can occur along the traction current line, which the 110 kV insulators cannot handle.\n\nAs a rule, traction current lines use single conductors, however for the supply of railways with high traffic and in particular for the supply of high speed railway lines, two bundle conductors are used.\n\nThe Mariazeller railway in Lower Austria operates on single phase AC at a 25 Hz utility frequency. The railway has its own traction current lines with an operating voltage of 27 kV. These lines are mounted on the pylons of the overhead wire over the catenary wire.\n\nIn Germany, single conductors are usually used for traction current lines but, for the ICE train, two bundle conductors are used. The traction current supply lines from the nuclear power station Neckarwestheim to the traction current switching station at Neckarwestheim and from there to the central substation in Stuttgart, Zazenhausen are implemented as a four-bundle conductor circuit.\n\nIn Sweden, Norway and some areas of the former German Democratic Republic, three phase AC-current is converted into single phase AC with a frequency of 16.7 cycles per second at the substations. Unlike in Western Germany, there are no dedicated power plants for railway electricity. All power comes from general electricity suppliers. Although in this region there is, in principle, no requirement for traction power lines, there is a 132 kV-single AC power grid for railway power supply in Central Sweden (see Electric power supply system of railways in Sweden). In Norway, there is a small 55 kV single phase AC network for power supply of trains in the South, fed by Hakavik Power Station. A further power station, at Kjofossen feeds single phase AC directly in the overhead wire. In Denmark and Finland, 50 Hz is used for the main lines (if electrified) and the electricity comes from general suppliers. As such, much simpler equipment than in Sweden and Norway is needed for conversion.\n\nIn the Republic of South Africa there are extensive AC and DC traction schemes, including 50 kV and 25 kV AC single phase systems. Electrification in Natal was stimulated by the takeover of the South African Railways' system by the Electricity Supply Commission (now Eskom) based on the Colenso Power Station.\n\nIn the United Kingdom, the Network Rail 750 V DC electrification system in the southeast of England is supplied with power from an extensive 33 kV power distribution network.\n\n\nTraction current lines are used to power the railway systems of countries which use alternating current of a lower frequency than the public supply. This is typically the case in the German-speaking countries of Europe. For example, 16.7 Hz AC is used in Germany, Austria and Switzerland.\n\nA specific example is the Mariazeller narrow gauge railway in Austria, operating with single phase AC with a frequency of 25 Hz, which has its own traction current lines with an operating voltage of 27 kV. These lines are mounted on the pylons of the overhead wire over the lines.\n\nThe voltages used for traction current lines are 110 kV in Germany and Austria and 66 kV or 132 kV in Switzerland.\n\nTraction current lines are operated symmetrically against earth. In the case of 110 kV lines, for example, each conductor has a voltage of 55 kV against earth. The grounding is made in larger substations and in power stations for traction current, using transformers for the cancellation of the earth leakage current. As is the case for all symmetrical powerlines there are also at traction power lines twisting points. A traction powerline for one circuit has usually two conductors. Since most traction current lines possess two electric circuits, four conductors are on the pylons as a rule (in contrast with three-phase alternating current lines, whose number of conductors are an integral multiple of three).\n\nTraction current lines are not usually laid parallel to the railway line, so as to minimise the line length and to avoid unnecessary influences of electrical system near the railway line. However, there are cases where this practice is not followed (for example, the current supply of some rapid-transit railways operating with alternating current in Germany). In this case, the traction current line is laid on special cross beams of the overhead wire pylons above the overhead line. Because overhead line pylons possess a smaller cross section than traction current masts, these cross beams have to be quite narrow, so the arrangement of four conductor cables in one level, which is standard at traction current lines, cannot be used. Where four conductors are needed, one approach is to employ a two-level arrangement of conductor cables. Alternatively, in cases of double-tracked railway lines, the overhead line pylons for both driving directions are equipped with cross beams for the traction current system (two conductor cables).\n\nIn densely populated areas, where rights of way are rare, it is common to find pylons which carry electric circuits for traction current as well as those for three-phase alternating current. The latter can be 110 kV, 220 kV, or, in some cases, 380 kV three phase AC lines. In such cases, the traction current lines must use insulators which can cope with the maximum peak-to-peak voltage which can occur between the lines.\n\nTraction current lines are implemented as a rule as single leaders. For the supply of railways with much rail traffic and in particular for the power supply of high speed railway lines such as the German ICE (Inter City Express) trains, conductors of two bundles are used. The traction current lines from the nuclear power station at Neckarwestheim to the traction current switching station at Neckarwestheim, and from the traction current switching station at Neckarwestheim to the central substation in Stuttgart Zazenhausen are implemented as four-bundle conductors.\n\nTraction current lines are always equipped with an earth conductor. In some cases, two earth conductors are used: for example in, Germany, in cases where the traction current line is carried on pylons together with three phase AC current, like the line to the nuclear power station at Neckarwestheim. Similarly, in Austria there are some traction current lines equipped with two earth ropes.\n\nIn Sweden, Norway and some areas of the former GDR three phase AC-current from the public grid is converted into single phase AC current with a frequency of 16.7 Hz in substations close to the railways. In these regions there are no traction current lines.\n\nAlso in countries in which the electric trains run with direct current or with single phase AC current with the frequency of the general power grid, the required conversion of the current is performed in the substations, so that in these countries no traction current lines are needed.\n\n"}
{"id": "2361108", "url": "https://en.wikipedia.org/wiki?curid=2361108", "title": "Upholstery coil springs", "text": "Upholstery coil springs\n\nAncient coiled helical or spiral shaped wire objects resembling springs have been discovered in the Balkans and across Europe by archaeological teams. Such objects dating from as far back as 4000 BC, the beginning of the first Bronze Age, abound in museums and collections. Wire coiled objects, possibly designed as rings or jewelry, nevertheless, exhibit the properties of extension or compression springs.\n\nThree technologies drove the development of coiled springs in the mid-1500s, enabled by advances in the quality of steel; these were time-pieces (e.g., clocks and portable watches), firearms (e.g., cannons and pistols), and vehicles (e.g., coaches and carriages). The latter emerged in 16th century Hungary. Sir William Felton's Treatise on Carriages 1793 and George Thrupp's History of Coaches 1877 give examples of evolving applications of springs in transportation that directly transferred to furniture.\n\nGermany and Sweden were known even in fifteen century Europe as being the best steel because indigenous iron had .05-.07% carbon which is excellent spring grade iron for steel.\n\n1690, Sir Ambrose Crowley used German Syrian steel for quality watch springs.\n\nSir Robert Hooke wrote his Lecture on Springs in 1678. In it he explains how anyone can take well formed wire of various compositions and wind it around a form to create a spring. His work demonstrated the phenomenon known today as Hooke's Law -- which characterizes springs as providing a restoring force proportional to the extent of their deformation.\n\nClive Edwards, furniture historian wrote that, \"One of the most tantalizing questions in eighteenth century furniture is that of springing. Commonly thought to have been a nineteenth century innovation, there is evidence of earlier use. The initial demands for springs appear to have come from the coach builders and the second came from furniture-makers. Coach builders were seeking to improve the ride of vehicles as well as to improve upon the interior comfort seats for passengers.\" The English guild category is known as \"The Worshipful Company Of Furniture Makers\" but in sixteenth through nineteenth century times this trade was known as cabinet-makers.\n\nThe common nexus was the iron workers and blacksmith shops that were developing carriage springs and supplying hardware parts for cabinet-makers.\n\nGeorge Thrupp illustrates a coil spring used in a carriage presented to the French Academy in 1703.\n\n1706, Henry Mills received Great Britain patent 376 \"For A New Mathematical Instrument For New Sorts Of Springs...And That The New Invented Springs Are Made And Contrived of several forms-semicircular, circular, angular, oval, or other forms...\" These spring were lighter weight, a set weighing only 20 pounds versus other coach springs in use that weighed 120 pounds per set. These were helical extension springs also known as \"worm\" springs due to the spiral nature.\n\n1762; Richard Tredwell, Great Britain Patent of Rotherham in the County of York received a patent for a leaf spring for carriages, and, again, in 1763, Richard Tredwell received Great Britain Patent 792 for \"Springs For Carriages\". There are four pages in this patent and the fourth page clearly shows nine iterations of helical suspension or worm springs and the patent clearly states it is \"my new method of making and constructing springs for hanging of coaches. Coil compression springs would not be used for hanging a carriage. Mr. Tredwell's patent had no contribution to upholstery spring developments.\n\n1780, William Blakey, an expert clock historian wrote The Art Of Making Watch Springs (translated by M Wayman) who said that, \"The art of making watch and clock springs could possibly be, of all mechanical operations, the one which provides the greatest amount of knowledge of the physical properties of steel. In the process of discovering the qualities of iron which are essential in order to convet it into steel, the artisan cannot avoid recognizing the different qualities of metal, such as its hardness, its malleability, its elasticity, etc.\" He also said it was not easy to ascribe a date in the discovery of steel and its qualities--but this began when clocks and watches were being perfected.\n\n1822; Georg Junigl of Vienna, Austria received a \"privilidg\" which was an Austrian legal term for a patent for a wire spring used in combination with upholstery filling. This patent was announced in the April 24, 1822 edition 94, page 1, column 1, item 3 that Mr. Junigl a bourgeoisie upholsterer of Vienna had been granted the patent. He died in 1840 according to the Vienna, Austria newspaper.\n\n1826, Samuel Pratt of New Bond Street, in the Parish of St. George, Hanover Square in the County of Middlesex received Great Britain Patent 5418 for \"Beds, Bedsteads, Couches, Seats and Other Articles of Furniture. This patent used coiled shaped springs in an arrangement to minimize for furniture for a nautical sailing vessel.\n\n1828, Samuel Pratt received Great Britain Patent 5668 for \"Elastic Beds And Cushions\" which was to be an improvement in compression spring arrangements in furniture. Page 5 of the patent depicts two hour-glass shaped wire coil compression springs in both circular and tri-angular shapes.\n\n1833, August Boschow, a Viennese upholsterer, received an Austrian \"privileg\"(patent) that was published in the Vienna, Austrian newspaper on June 26, 1833. Mr. Boschow's invented a new type of a “clock” spring designed for use in carriages, chairs, and beds; the spring supported the human body in a sitting or lying supine position.\n\n1834, John Saville Crofton published a book entitled The London Upholsterer's Companion The Art Of Spring Stuffing. Mr Crofton a veteran of the upholstery trade describes using nine circular steel coil springs 7\" high and 3-1/2\" wide made of number 8 charcoal wire for upholstering easy chairs. His book also describes spring upholstered sofas, beds, mattresses, pillows, and carriage and coach seats also made of circular coil spring of lighter gauges of wire. Mr Crofton notes that coil spring have been in the trade a number of years indicating the original date of the practice is unknown and this work in the British Library has been unheeded in history accounts on this topic. Crofton wrote on page 34 of his book in the chapter about \"Easy Chairs, Spring-stuffing\":\nCharcoal wire was an early nineteenth century term that designated crucible manufacturing process for making better quality steel for drawing spring wire.\n\nThe hand-tire style of crafting springs by tieing each to adjacent springs is a method that dates back as far as Crofton writes--and probably much earlier as he indicates. Modern upholstery methods may still refer to \"hand tied\" springs or \"eight-way hand-tied springs\" which are closely related to Crofton's method as described.\n\n1834 Crofton also informs that the upholsterer could make their own springs by obtaining wire and bending it around a wooden mold or purchasing springs by the hundred weight.\n\n1843 Holland & Sons offered upholstered chair, sofas, and other articles with descriptions such as \"putting spring in frames\", \"spring stuffed\", \"double spring stuffed\", and \"spring stuffed cushions.\n\nConsidering the evidence from the clock, carriage, and furniture trade it is likely that upholstery springs may have been in use by local craftsmen before the eighteenth century, certainly existed in the eighteenth century, became normal by the beginning of the nineteenth century, and all types of springs were being developed and commonly used by mid-nineteenth century as both the British and American patent records clearly document. Patents do not necessarily mean first use but can only be used to evaluate and estimate trends.\n\n1849, T. E. Warren of the United States Of America received patent 6740 which a complex leaf spring used in seating and was utilized by the American Chair Company for seating on trains.\n\n1850, Alexander Oechslin received an Austrian patent for an “improvement in spiral spring upholstery.”\nBetween 1855 and 1900 hundreds of wire spring patents were issued in Great Britain and the United States for seating and bedding. These British can be viewed in a Master Abridgement of British Patents in Class 52 1855-1900.\nWashburn & Moen Manufacturing, one of America's first wire drawing companies founded in 1834, manufactured wire for a range of products from piano wire to telegraph wire, barbed wire, and wire for coil springs. Between 1837 and 1847, Washburn’s wire quality dictated that all iron billets 12’ by 1-1/8” were imported from Sweden specifically for drawing wire; once in the United States, the billets were rolled into rods at mills in Troy, New York, Fall River, Massachusetts, or Windsor Locks, Connecticut.The Smithsonian Institute and Library of Congress retains portions of their historical records.\n\n1869, Timothy Rose and Platt Buell received an American patent number 97,705 for a coil bed spring.\n\n1871, Edwin Bushnell of Poughkeepsie. New York received an American patent number 4,616 for \"An Improvement In Spring Mattresses.\nPrior to the early 1900s, springs were used as bed bases or box springs; these spring sets were not covered with fabrics. There were no \"innerspring\" mattresses manufactured until after 1900. Bushnell's patent has been cited as an innerspring but it was an under bed spring---which in the mid-1800s was also called a mattress or base layer of a bed.\n\nMr. Crofton's work of 1834 and Mr. Felton's work of 1793 in light of archeological finds and great works such as Sir Robert Hooke's Lecture on Springs clearly indicates that coil springs came into use in the 17th or 18th centuries. The time, location, and inventors are unknown.\n\n"}
{"id": "21482611", "url": "https://en.wikipedia.org/wiki?curid=21482611", "title": "Wasli", "text": "Wasli\n\nWasli, also referred to as wasli paper, is a type of handmade paper used specifically for painting miniatures. It was devised in India, in the 10th century, and figures widely in Mughal-era painting.\n\nRecent uses of Wasli (hand-prepared paper of varying thickness) range from classical/traditional way of painting with opaque water colors to building structures of varying kinds. \n\nMiniature Painting is a term used for making opaque/translucent water color paintings/illustrations on a small scale inspired from, Persian, or Pahari miniature schools and Wasli is used as a canvas for making miniatures.\n\nWasli word came out of a Persian word Vasl which means Union, of coming together, oneness, etc.\n\nWasli is an Acid-free paper and it has archival qualities. Paper eating insects can not eat it because of a poison copper sulphate/Neela Thootha used in the making of this paper. The glue to paste sheets together is also acid free made out of cooking flour. \nTo use it for miniature paintings this layered paper is burnished with either smooth glass or a sea shell. This way the paper is shiny, smooth and have minimal perceptible grain.\n\nA sheet of wasli is created by gluing together several layers of paper, then polishing them by hand until they are shiny and smooth, with minimal perceptible grain.\n\n"}
{"id": "39626985", "url": "https://en.wikipedia.org/wiki?curid=39626985", "title": "Wireless powerline sensor", "text": "Wireless powerline sensor\n\nA Wireless powerline sensor hangs from an overhead power line and sends measurements to a data collection system. Because the sensor does not contact anything but a single live conductor, no high-voltage isolation is needed. The sensor, installed simply by clamping it around a conductor, powers itself from energy scavenged from electrical or magnetic fields surrounding the conductor being measured. Overhead power line monitoring helps distribution system operators provide reliable service at optimized cost.\n\nIn the photos on the right, an antenna on the sensor transmits data to a communication device attached to a nearby utility pole. The communication device gets power from the 240 Volt utility line in a residential neighborhood. The device has two antennas. One antenna collects data from the sensors, and the other antenna forwards the data to the electrical utility control center over cell phone service.\n\nIn some systems, powerline sensors may transmit information on the high voltage conductor itself rather than by transmission of a radio signal.\n\nThe primary purpose of a powerline sensor is to measure current, however, some sensors can either directly measure or derive other data such as:\n\n\n\n6. Patel N., Kumar S. (2017). \"Enhanced Clear Channel Assessment for Slotted CSMA/CA in IEEE 802.15. 4\", Springer \n\nWireless Personal Communications, Vol. 95, No. 4, pp 4063–4081. https://link.springer.com/article/10.1007/s11277- 017-4042-5\n\nExternal links\n\n"}
{"id": "2729585", "url": "https://en.wikipedia.org/wiki?curid=2729585", "title": "Working animal", "text": "Working animal\n\nA working animal is an animal, usually domesticated, that is kept by humans and trained to perform tasks. They may be close members of the family, such as guide dogs or other assistance dogs, or they may be animals trained to provide tractive force, such as draft horses or logging elephants. The latter types of animals are called draft animals (draught animals) or beasts of burden. Most working animals are either service animals or draft animals. They may also be used for milking or herding, jobs that require human training to encourage the animal to cooperate. Some, at the end of their working lives, may also be used for meat or other products such as leather.\n\nThe history of working animals may predate agriculture, with dogs used by our hunter-gatherer ancestors. Around the world, millions of animals work in relationship with their owners. Domesticated species are often bred for different uses and conditions, especially horses and working dogs. Working animals are usually raised on farms, though some are still captured from the wild, such as dolphins and some Asian elephants.\n\nPeople have found uses for a wide variety of abilities in animals, and even industrialized societies use many animals for work. People use the strength of horses, elephants, and oxen to pull carts and move logs. Law enforcement uses the keen sense of smell of dogs to search for drugs and explosives, and others use dogs to find game or search for missing or trapped people. People use various animals—camels, donkeys, horses, dogs, etc.—for transport, either for riding or to pull wagons and sleds. Other animals, including dogs and monkeys, help blind or disabled people.\n\nOn rare occasions, wild animals are not only tamed, but trained to perform work—though often solely for novelty or entertainment, as such animals tend to lack the trustworthiness and mild temper of true domesticated working animals. Conversely, not all domesticated animals are working animals. For example, while cats may catch mice, it is an instinctive behavior, not one that can be trained by human intervention. Other domesticated animals, such as sheep or rabbits, may have agricultural uses for meat, hides and wool, but are not suitable for work. Finally, small domestic pets, such as most small birds (other than certain types of pigeon) are generally incapable of performing work other than providing companionship.\n\nSome animals are used due to sheer physical strength in tasks such as ploughing or logging. Such animals are grouped as a draught or draft animal. Others may be used as pack animals, for animal-powered transport, the movement of people and goods. Some animals are ridden by people on their backs and are known as “mounts”; Alternatively, one or more animals in harness may be used to pull vehicles.\n\nThey include equines such as horses, donkeys, and mules; bovines such as cattle, water buffalo, and yaks, and elephants and camels. Dromedary camels are in arid areas of Australia, North Africa and the Middle East; the less common Bactrian camel inhabits central and East Asia; both are used as working animals. On occasion, reindeer, though usually driven, may be ridden.\n\nCertain wild animals have been tamed and used for riding, usually for novelty purposes, including the zebra and the ostrich. Some mythical creatures are believed to act as divine mounts, such as garuda in Hinduism and the winged horse Pegasus in Greek mythology.\n\nPack animals may be of the same species as mounts or harness animals, though animals such as horses, mules, donkeys, reindeer and both types of camel may have individual bloodlines or breeds that have been selectively bred for packing. Additional species are only used to carry loads, including llamas in the Andes.\n\nDomesticated cattle and yaks are also used as pack animals. Other species used to carry cargo include dogs and pack goats.\n\nAn intermediate use is to harness animals, singly or in teams, to pull (or haul) sleds, wheeled vehicles or plough. \n\nAssorted wild animals have, on occasion, been tamed and trained to harness, including zebras and even moose.\nAs some domesticated animals display extremely protective or territorial behaviour, certain breeds and species can be utilised to guard property, including dogs, geese and llamas.\n\nWorking draught animals may power fixed machinery using a treadmill and have been used throughout history to power a winch to raise water from a well. Turnspit dogs were formerly used to power roasting jacks for roasting meat.\n\nAs predatory species are naturally equipped to catch prey, this is a further use for animals and birds. This can be done either for sustenance, sport, or to reduce the population of undesired animals that are considered harmful to crops, livestock or the environment.\n\n\n\n\n\n\nThe defensive and offensive capabilities of animals (such as fangs and claws) can be used to protect or to attack humans.\n\nIn some jurisdictions, certain working animals are afforded greater legal rights than other animals. One such common example is police dogs, which are often afforded additional protections and the same memorial services as human officers.\n\n"}
