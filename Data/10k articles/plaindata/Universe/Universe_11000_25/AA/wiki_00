{"id": "44038529", "url": "https://en.wikipedia.org/wiki?curid=44038529", "title": "AQSIQ-Registration for the Export of Recycling Material to China", "text": "AQSIQ-Registration for the Export of Recycling Material to China\n\nThe “Registration of Overseas Supplier of Imported Scrap Materials“ is required for Non-Chinese companies for importing industrial waste and recycling parts into China. The application has to be submitted at the AQSIQ (General Administration of Quality Supervision, Inspection and Quarantine of the People's Republic of China; Chinese: 中华人民共和国国家质量监督检验检疫总局). The registration is mandatory for all Non-Chinese exporters. Aim of this registration is to monitor the import of potential environmental harmful material and ensure that no material is imported into China that are not in compliance with the Chinese environment standards. \nIf companies are not registered, parts will be detained in customs and no import process is possible. In addition to the registration of the exporting companies, the parts have to be registered with the CCIC (China Certification & Inspection Group; Chinese 中国检验认证集团), and, furthermore, these parts will be accordingly inspected.\n\nThe following products require this registration\n\n\n"}
{"id": "30454934", "url": "https://en.wikipedia.org/wiki?curid=30454934", "title": "Alan Priddy", "text": "Alan Priddy\n\nAlan Priddy (born 7 April 1953) is a British power boat sailor and adventurer who has set several boating world records. Priddy attempted to circumnavigate the world in a rigid-hulled inflatable boat (RIB) in 2002, and in 2008 successfully completed a circumnavigation by yacht. He has also navigated a RIB around Scotland, Ireland, Britain and across the Bay of Biscay. He set a world RIB record in 2003 for crossing the Atlantic in 103 hours.\n\nAs of 2012, Priddy had recorded 37 world records, and 12 British national records. Boats he has sailed and raced include Fireballs, Finns, Shearwater Cats, Enterprises, Wayfarers, Toppers and yachts.\n\nPriddy was born on a council estate in Portsmouth. He started sailing at eight-years-old and owned a motorised dinghy at the age of eleven.\n\nAlan Priddy took part in several record-breaking journeys around Scotland (1990), Britain (1992), Ireland (1993) and in the Bay of Biscay (1996).\n\nOn Sunday 22 June 1997, Alan Priddy, Vic Palmer, Jan Falkowski and Steve Lloyd set out to cross the Atlantic in \"Spirit of Portsmouth\"—a four-year-old 7.4 m (24 ft) Ribtec rigid inflatable with a single 165 hp Yamaha inboard diesel engine.\nThe journey took them from Portsmouth, New Hampshire, USA to Portsmouth, Hampshire, UK, stopping on the way in Nova Scotia, Newfoundland, Greenland, Iceland and Northern Ireland. The average leg was more than 500 nautical miles with the longest one planned to be nearly 700 nautical miles and scheduled to take 35 hours non-stop. However, the trip from Iceland to Weymouth took 100 hours with stops for fuel in Barra and Bangor.\n\nIn 1998, Priddy and a crew travelled in the RIB \"Spirit of Portsmouth\" from London to Monaco. The 2,100-mile route went down the Thames Estuary, along the length of the Channel to Guernsey, across the Bay of Biscay to Spain, into the Mediterranean and past Gibraltar to Monaco. The team was backed up by two road crews. They completed the voyage in 99 hours 19 minutes 15seconds. The Royal Yachting Association did not ratify the record as they had not been invited to start the race.\n\nIn June 2000, Priddy and his crew circumnavigated the British Isles in five days, six hours and five minutes. This was a trial-run of the RIB \"Spirit of Cardiff\" in preparation for a planned circumnavigation of the world.\n\nPriddy, Clive Tully, Alan Carter and Steve Lloyd attempted to circumnavigate the world in the custom-built RIB \"Spirit of Cardiff\" which was specifically designed for endurance. The route encompassed 21 countries and nearly 25,000 miles and was scheduled for 75 days. At that time, \"Spirit of Cardiff\" was one of the largest RIBs in the world with an overall length of 10 metres (33 feet). This large size allowed a fully enclosed cabin on board to provide the team with shelter.\nThey left Cardiff on 31 March 2002. The voyage ran into difficulties and was prematurely concluded when Lloyd suffered heart problems 350 miles from Newfoundland and required a helicopter airlift to Canada.\n\nIn September 2002, Priddy planned to return \"Spirit of Cardiff\" across the Atlantic. However, the boat's engine compartment had become flooded and the bilge pumps were ineffective. Priddy abandoned the boat in Canada.\n\nIn 2003, Priddy attempted to cross from St. John's, Newfoundland to Cape Wrath in Scotland in less than 100 hours. The \"Spirit of \" was refurbished and became \"The Jolly Sailor\". \nAt 1342 local, 1512 GMT on 27 July 2003 Priddy, Jan Falkowski, Clive Tully and Canadian Egbert Walters embarked. With a brief fuel stop in Nanortalik harbour (Greenland) after being battered by 50 knot winds, they headed across the Denmark Strait towards Iceland onto a further fuel stop at the island of Heimaey, past Cape Wrath (Scotland) on to Bangor (Northern Ireland). Priddy beat his previous time but did not set a world record, but it was the fastest transatlantic crossing by a RIB. After an overnight stop, they continued down the Irish Sea and returned to Portsmouth on 9 September.\n\nIn 2004, Priddy attempted to return the \"Jolly Sailor\" to Newfoundland (Canada) to be used by charitable organisations but the boat was hit by a large wave and sank. The crew was rescued.\n\nFrom 2006 to 2008, Priddy circumnavigated the world aboard Sir Alec Rose's yacht \"Lively Lady\". The 36-foot, 60-year-old, sailing vessel was crewed by a group of 38 disadvantaged young adults. The voyage began in Portsmouth and finished there on 5 July 2008.\n\nPriddy continued the project as the charity \"Around and Around\" with graduates serving as mentors for new crew participants on future world circumnavigations. In 2009 Priddy was leased \"Lively Lady\" by Portsmouth City Council for 25 years for her to be maintained and used by the charity.\n\nIn 2015 Priddy confirmed that an attempt to break the powerboat circumnavigation record would be made after trials of the purpose-built £2.9m, powerboat in the summer. The project was begun in 2008. In April 2015 Priddy announced plans to offer places on the project to \"Blesma\", a group of wounded service personnel who have formed a powerboat racing team. The project was placed in jeopardy in 2015 when one of the sponsors withdrew, but replacement funding was subsequently secured to put the project, named \"Team Britannia\", back on track for the record attempt to be made in 2018.\n\nIn 2002, Priddy was given a Special Achievement Award by the British Inflatable Boat Owners' Association. In 2008, Priddy received a We Can Do It Special Achievement Award. and was made a Paul Harris Fellow for his work with disadvantaged young adults. In July 2017 he was awarded an Honorary Doctorate by the University of Portsmouth.\n\n\n"}
{"id": "4021725", "url": "https://en.wikipedia.org/wiki?curid=4021725", "title": "Appropriate Technology Africa", "text": "Appropriate Technology Africa\n\nAppropriate Technology Africa is a Christian organization which sells and promotes appropriate technology. ATA provides technology to empower people in Africa to make sellable end user products. Concentrating specifically on the African market, ATA has come up with appropriate, relevant, reliable and cost effective solutions to meet small business needs. ATA does this without compromising on quality. Its main branch is located along Seke Road, Graniteside, Harare, Zimbabwe. ATA also has branches at TSF and BOKA tobacco floors and in Karoi during the tobacco selling season .\n\nATA's focus is on mining, Agricultural machinery,agricultural and building related machinery.\n\nIt was started by founder and chairman Andy Noel Whyte in 1998 in Zimbabwe.\n\n"}
{"id": "2293257", "url": "https://en.wikipedia.org/wiki?curid=2293257", "title": "Bank paper", "text": "Bank paper\n\nBank paper is a thin strong writing paper of less than 50g/m. It is commonly used for typewriting and correspondence.\n\nThe term is also used for securities that are issued by banks, instead of governments. See also commercial paper, securities issued by corporations.\n"}
{"id": "2121250", "url": "https://en.wikipedia.org/wiki?curid=2121250", "title": "Baton Rouge Refinery", "text": "Baton Rouge Refinery\n\nExxonMobil's Baton Rouge Refinery in Baton Rouge, Louisiana is the fourth-largest oil refinery in the United States and twelfth-largest in the world, with an input capacity of per day as of January 1, 2016. The refinery is the site of the first commercial fluid catalytic cracking plant that began processing at the refinery on May 25, 1942.\n\nStandard Oil first erected the refinery in 1909. Today's facility is part of a complex made of nine individual plants across the region. The main plant is located on the east bank of the Mississippi River. There are about 6,300 workers spread across these sites, including 4,000 direct employees (the rest are contractors).\n\nGenesis Energy LP recently invested $125 million to improve ExxonMobil's existing assets in the Baton Rouge area. The investment includes plans to build an 18-mile, 20-inch diameter crude oil pipeline that connects Genesis Energy's Port Hudson terminal, to ExxonMobil's Baton Rouge refinery. The Baton Rouge refinery's tank farm has a capacity of 502,500 barrel-per-day (bpd). \n\n"}
{"id": "15103841", "url": "https://en.wikipedia.org/wiki?curid=15103841", "title": "Cabriolet (carriage)", "text": "Cabriolet (carriage)\n\nA cabriolet is a light horse-drawn vehicle, with two wheels and a single horse. The carriage has a folding hood that can cover its two occupants, one of whom is the driver. It has a large rigid apron, upward-curving shafts, and usually a rear platform between the C springs for a groom. The design was developed in France in the eighteenth century and quickly replaced the heavier hackney carriage as the vehicle for hire of choice in Paris and London.\n\nThe \"cab\" of taxi-cab or \"hansom cab\" is a shortening of \"cabriolet\".\nOther horse-drawn cabs include:\n\n\nOne who drives a horse-drawn cab for hire is a \"cabdriver\".\n\n"}
{"id": "580345", "url": "https://en.wikipedia.org/wiki?curid=580345", "title": "Chlamydomonas reinhardtii", "text": "Chlamydomonas reinhardtii\n\nChlamydomonas reinhardtii is a single-cell green alga about 10 micrometres in diameter that swims with two flagella. It has a cell wall made of hydroxyproline-rich glycoproteins, a large cup-shaped chloroplast, a large pyrenoid, and an \"eyespot\" that senses light.\n\n\"Chlamydomonas\" species are widely distributed worldwide in soil and fresh water. \"Chlamydomonas reinhardtii\" is an especially well studied biological model organism, partly due to its ease of culturing and the ability to manipulate its genetics. When illuminated, \"C. reinhardtii\" can grow photoautotrophically, but it can also grow in the dark if supplied with organic carbon. Commercially, \"C. reinhardtii\" is of interest for producing biopharmaceuticals and biofuel, as well being a valuable research tool in making hydrogen.\n\nThe \"C. reinhardtii\" wild-type laboratory strain c137 (mt+) originates from an isolate made near Amherst, Massachusetts, in 1945 by Gilbert M. Smith.\n\nThe species' name has been spelled several different ways because of different transliterations of the name from Russian: \"reinhardi\", \"reinhardii\", and \"reinhardtii\" all refer to the same species, \"C. reinhardtii\" Dangeard.\n\n\"Chlamydomonas\" is used as a model organism for research on fundamental questions in cell and molecular biology such as:\n\n\nThere are many known mutants of \"C. reinhardtii\". These mutants are useful tools for studying a variety of biological processes, including flagellar motility, photosynthesis, and protein synthesis. Since \"Chlamydomonas\" species are normally haploid, the effects of mutations are seen immediately without further crosses.\n\nIn 2007, the complete nuclear genome sequence of \"C. reinhardtii\" was published.\n\nChannelrhodopsin-1 and Channelrhodopsin-2, proteins that function as light-gated cation channels, were originally isolated from \"C. reinhardtii\". These proteins and others like them are increasingly widely used in the field of optogenetics.\n\nThe genome of C. Reinhardtii is significant for mitochondrial study as it is one species where the genes for 6 of the 13 proteins encoded for the mitochondria are found in the nucleus of the cell, leaving 7 in the mitochondria. In all other species these genes are present only in the mitochondria and are unable to be allotopically expressed. This is significant for the testing and development of therapies for genetic mitochondrial diseases.\n\nVegetative cells of the \"reinhardtii\" species are haploid with 17 small chromosomes. Under nitrogen starvation, vegetative cells differentiate into haploid gametes. There are two mating types, identical in appearance, thus isogamous, and known as mt(+) and mt(-), which can fuse to form a diploid zygote. The zygote is not flagellated, and it serves as a dormant form of the species in the soil. In the light, the zygote undergoes meiosis and releases four flagellated haploid cells that resume the vegetative lifecycle.\n\nUnder ideal growth conditions, cells may sometimes undergo two or three rounds of mitosis before the daughter cells are released from the old cell wall into the medium. Thus, a single growth step may result in 4 or 8 daughter cells per mother cell.\n\nThe cell cycle of this unicellular green algae can be synchronized by alternating periods of light and dark. The growth phase is dependent on light, whereas, after a point designated as the transition or commitment point, processes are light-independent.\n\nThe attractiveness of the algae as a model organism has recently increased with the release of several genomic resources to the public domain. The Chlre3 draft of the \"Chlamydomonas\" nuclear genome sequence prepared by Joint Genome Institute of the U.S. Dept of Energy comprises 1557 scaffolds totaling 120 Mb. Roughly half of the genome is contained in 24 scaffolds all at least 1.6 Mb in length. The current assembly of the nuclear genome is available online.\n\nThe ~15.8 Kb mitochondrial genome (database accession: NC_001638) is available online at the NCBI database. The complete ~203.8 Kb chloroplast genome (database accession: NC_005353) is available online.\n\nIn addition to genomic sequence data, there is a large supply of expression sequence data available as cDNA libraries and expressed sequence tags (ESTs). Seven cDNA libraries are available online. A BAC library can be purchased from the Clemson University Genomics Institute. There are also two databases of >50 000 and >160 000 ESTs available online.\n\nThe genome of \"C. reinhardtii\" has been shown to contain N6-Methyldeoxyadenosine (6mA), a mark common in prokaryotes but much rarer in eukaryotes. Some research has indicated that 6mA in \"Chlamydomonas\" may be involved in nucleosome positioning, as it is present in the linker regions between nucleosomes as well as near the transcription start sites of actively transcribed genes.\n\n\"Chlamydomonas\" has been used to study different aspects of evolutionary biology and ecology. It is an organism of choice for many selection experiments because (1) it has a short generation time, (2) it is both a heterotroph and a facultative autotroph, (3) it can reproduce both sexually and asexually, and (4) there is a wealth of genetic information already available.\n\nSome examples (nonexhaustive) of evolutionary work done with \"Chlamydomonas\" include the evolution of sexual reproduction, the fitness effect of mutations, and the effect of adaptation to different levels of CO.\n\nAccording to one frequently cited theoretical hypothesis, sexual reproduction (in contrast to asexual reproduction) is adaptively maintained in benign environments because it reduces mutational load by combining deleterious mutations from different lines of descent and increases mean fitness. However, in a long-term experimental study of \"C. reinhardtii\", evidence was obtained that contradicted this hypothesis. In sexual populations, mutation clearance was not found to occur and fitness was not found to increase.\n\nGene transformation occurs mainly by homologous recombination in the chloroplast and heterologous recombination in the nucleus. The \"C. reinhardtii\" chloroplast genome can be transformed using microprojectile particle bombardment or glass bead agitation, however this last method is far less efficient. The nuclear genome has been transformed with both glass bead agitation and electroporation. The biolistic procedure appears to be the most efficient way of introducing DNA into the chloroplast genome. This is probably because the chloroplast occupies over half of the volume of the cell providing the microprojectile with a large target. Electroporation has been shown to be the most efficient way of introducing DNA into the nuclear genome with maximum transformation frequencies two orders of magnitude higher than obtained using glass bead method.\n\nGenetically engineered \"Chlamydomonas reinhardtii\" has been used to produce a mammalian serum amyloid protein, a human antibody protein, human Vascular endothelial growth factor, a potential therapeutic Human Papillomavirus 16 vaccine, a potential malaria vaccine (an edible algae vaccine), and a complex designer drug that could be used to treat cancer.\n\nIn 1939, the German researcher Hans Gaffron (1902–1979), who was at that time attached to the University of Chicago, discovered the hydrogen metabolism of unicellular green algae. \"Chlamydomonas reinhardtii\" and some other green algae can, under specified circumstances, stop producing oxygen and convert instead to the production of hydrogen. This reaction by hydrogenase, an enzyme active only in the absence of oxygen, is short-lived. Over the next thirty years, Gaffron and his team worked out the basic mechanics of this photosynthetic hydrogen production by algae.\n\nTo increase the production of hydrogen, several tracks are being followed by the researchers.\n\nAoyama, H., Kuroiwa, T. and Nakamura, S. 2009. The dynamic behaviour of mitochondria in living zygotes during maturation and meiosis in \"Chlamydomonas reinhardtii\". \"Eur. J. Phycol.\" 44: 497 - 507.\n\nJamers, A., Lenjou, M., Deraedt, P., van Bockstaele, D., Blust, R. and de Coen, W. 2009. Flow cytometric analysis of the cadmium-exposed green algae \"Chlamydomonas reinhadtii\" (Chlorophyceae). \"Eur. J. Phycol.\" 44: 54 - 550.\n\n"}
{"id": "8236003", "url": "https://en.wikipedia.org/wiki?curid=8236003", "title": "Coastal forests of eastern Africa", "text": "Coastal forests of eastern Africa\n\nThe Coastal forests of eastern Africa is a tropical moist forest region along the east coast of Africa. The region was designated a biodiversity hotspot by Conservation International.\n\nThe forests extend in al narrow band along the coast of the Indian Ocean, from southern Somalia in the north, through coastal Kenya and Tanzania to the mouth of the Limpopo River in southern Mozambique.\n\nThe World Wildlife Fund divides the coastal forests into two ecoregions: the Northern Zanzibar-Inhambane coastal forest mosaic, which extends from southern Somalia through coastal Kenya to southern Tanzania, and includes the islands of Zanzibar and Pemba, and the Southern Zanzibar-Inhambane coastal forest mosaic, which extends from southern Tanzania along the Mozambique coast to the mouth of the Limpopo.\n\n\n"}
{"id": "187473", "url": "https://en.wikipedia.org/wiki?curid=187473", "title": "Compressed air car", "text": "Compressed air car\n\nA compressed air car is a compressed air vehicle that uses a motor powered by compressed air. The car can be powered solely by air, or combined (as in a hybrid electric vehicle) with gasoline, diesel, ethanol, or an electric plant with regenerative braking.\n\nCompressed air cars are powered by motors driven by compressed air, which is stored in a tank at high pressure such as 31 MPa (4500 psi or 310 bar). Rather than driving engine pistons with an ignited fuel-air mixture, \"compressed air cars\" use the expansion of compressed air, in a similar manner to the expansion of steam in a steam engine.\n\nThere have been prototype cars since the 1920s, with compressed air used in torpedo propulsion.\n\nIn contrast to hydrogen's issues of damage and danger involved in high-impact crashes, air, on its own, is non-flammable, it was reported on Seven Network's \"Beyond Tomorrow\" that on its own carbon-fiber is brittle and can split under sufficient stress, but creates no shrapnel when it does so. Carbon-fiber tanks safely hold air at a pressure somewhere around 4500 psi, making them comparable to steel tanks. The cars are designed to be filled up at a high-pressure pump.\n\nIn compressed air vehicles tank designs tend to be isothermal; a heat exchanger of some kind is used to maintain the temperature (and pressure) of the tank as the air is extracted.\n\nCompressed air has relatively low energy density. Air at 30 MPa (4,500 psi) contains about 50 Wh of energy per liter (and normally weighs 372g per liter). For comparison, a lead–acid battery contains 60-75 Wh/l. A lithium-ion battery contains about 250-620 Wh/l. The EPA estimates the energy density of gasoline at 8,890 Wh/l; however, a typical gasoline engine with 18% efficiency can only recover the equivalent of 1694 Wh/l. The energy density of a compressed air system can be more than doubled if the air is heated prior to expansion.\n\nIn order to increase energy density, some systems may use gases that can be liquified or solidified. \"CO2 offers far greater compressibility than air when it transitions from gaseous to supercritical form.\"\n\nCompressed air cars could be emission-free at the exhaust. Since a compressed air car's source of energy is usually electricity, its total environmental impact depends on how clean the source of this electricity is. However, most air cars have petrol engines for different tasks. The emission can be compared to half of the amount of carbon dioxide produced by a Toyota Prius (being around 0.34 pounds per mile). Some engines can be fueled otherwise considering different regions can have very different sources of power, ranging from high-emission power sources such as coal to zero-emission power sources. A given region can also change its electrical power sources over time, thereby improving or worsening total emissions.\n\nHowever, a 2009 study showed that even with very optimistic assumptions, air storage of energy is less efficient than chemical (battery) storage.\n\nThe principal advantages of an air powered engine is\n\nThe principal disadvantages are the steps of energy conversion and transmission, because each inherently has loss. For combustion engine cars, the energy is lost when chemical energy in fossil fuels is converted by the engine to mechanical energy. For electric cars, a power plant's electricity (from whatever source) is transmitted to the car's batteries, which then transmits the electricity to the car's motor, which converts it to mechanical energy. For compressed-air cars, the power plant's electricity is transmitted to a compressor, which mechanically compresses the air into the car's tank. The car's engine then converts the compressed air to mechanical energy.\n\nAdditional concerns:\n\nSafety claims for light weight vehicle air tanks in severe collisions have not been verified.\nNorth American crash testing has not yet been conducted, and skeptics question the ability of an ultralight vehicle assembled with adhesives to produce acceptable crash safety results. Shiva Vencat, vice president of MDI and CEO of Zero Pollution Motors, claims the vehicle would pass crash testing and meet U.S. safety standards. He insists that the millions of dollars invested in the AirCar would not be in vain. To date, there has never been a lightweight, 100-plus mpg car which passed North American crash testing. Technological advances may soon make this possible, but the AirCar has yet to prove itself, and collision safety questions remain.\n\nThe key to achieving an acceptable range with an air car is reducing the power required to drive the car, so far as is practical. This pushes the design towards minimizing weight.\n\nAccording to a report by the U.S. Government's National Highway Traffic Safety Administration, among 10 different classes of passenger vehicles, \"very small cars\" have the highest fatality rate per mile driven. For instance, a person driving 12,000 miles per year for 55 years would have a 1% chance of being involved in a fatal accident. This is twice the fatality rate of the safest vehicle class, a \"large car\". According to the data in this report, the number of fatal crashes per mile is only weakly correlated with the vehicle weight, having a correlation coefficient of just (-0.45). A stronger correlation is seen with the vehicle \"size\" within its class; for example, \"large\" cars, pickups and SUVs, have lower fatality rates than \"small\" cars, pickups and SUVs. This is the case in 7 of the 10 classes, with the exception of mid-size vehicles, where minivans and mid-size cars are among the safest classes, while mid-size SUVs are the second most fatal after very small cars. Even though heavier vehicles sometimes are statistically safer, it is not necessarily the extra \"weight\" that \"causes\" them to be safer. The NHTSA report states: \"Heavier vehicles have historically done a better job cushioning their occupants in crashes. Their longer hoods and extra space in the occupant compartment provide an opportunity for a more gradual deceleration of the vehicle, and of the occupant within the vehicle... While it is conceivable that light vehicles could be built with similarly long hoods and mild deceleration pulses, it would probably require major changes in materials and design and/or taking weight out of their engines, accessories, etc.\"\n\nAir cars may use low rolling resistance tires, which typically offer less grip than normal tires. In addition, the weight (and price) of safety systems such as airbags, ABS and ESC may discourage manufacturers from including them.\n\nVarious companies are investing in the research, development and deployment of \"Compressed air cars\". Overoptimistic reports of impending production date back to at least May 1999. For instance, the MDI Air Car made its public debut in South Africa in 2002, and was predicted to be in production \"within six months\" in January 2004. As of January 2009, the air car never went into production in South Africa.\nMost of the cars under development also rely on using similar technology to low-energy vehicles in order to increase the range and performance of their cars.\n\nMDI has proposed a range of vehicles made up of AIRPod, OneFlowAir, CityFlowAir, MiniFlowAir and MultiFlowAir. One of the main innovations of this company is its implementation of its \"active chamber\", which is a compartment which heats the air (through the use of a fuel) in order to double the energy output. This 'innovation' was first used in torpedoes in 1904.\n\n Tata Motors of India had planned to launch a car with an MDI compressed air engine in 2011. In December 2009 Tata's vice president of engineering systems confirmed that the limited range and low engine temperatures were causing problems.\n\nTata Motors announced in May 2012 that they have assessed the design passing phase 1, the \"proof of the technical concept\" towards full production for the Indian market. Tata has moved onto phase 2, \"completing detailed development of the compressed air engine into specific vehicle and stationary applications\".\n\nIn February 2017 Dr. Tim Leverton, president and head at Advanced and Product Engineering at Tata revealed was at a point of \"starting industrialisation\" with the first vehicles to be available by 2020. Other reports indicate Tata is also looking at reviving plans for a compressed air version of the Tata Nano, which had previously been under consideration as part of their collaboration with MDI.\n\nEngineair is an Australian company which has produced prototypes of a variety of prototype small vehicles using an innovative rotary air engine designed by Angelo Di Pietro. The company is seeking commercial partners to utilise its engine.\n\nPeugeot and Citroën announced that they intended to build a car that uses compressed air as an energy source. However, the car they are designing uses a hybrid system which also uses a gasoline engine (which is used for propelling the car over 70 km/h, or when the compressed air tank has been depleted).\nIn January 2015, there was \"Disappointing news from France: PSA Peugeot Citroen has put an indefinite hold on the development of its promising-sounding Hybrid Air powertrain, apparently because the company has been unable to find a development partner willing to split the huge costs of engineering the system.\" Development costs are estimated to 500 million Euro for the system, which would apparently have need to be fitted to around 500,000 cars a year to make sense.\n\nAPUQ (Association de Promotion des Usages de la Quasiturbine) has made the APUQ Air Car, a car powered by a Quasiturbine.\n\n\n"}
{"id": "415915", "url": "https://en.wikipedia.org/wiki?curid=415915", "title": "Cornucopian", "text": "Cornucopian\n\nA cornucopian is a futurist who believes that continued progress and provision of material items for mankind can be met by similarly continued advances in technology. Fundamentally they believe that there is enough matter and energy on the Earth to provide for the population of the world.\n\nLooking further into the future, they posit that the abundance of matter and energy in space would appear to give humanity almost unlimited room for growth.\n\nThe term comes from the cornucopia, the \"horn of plenty\" of Greek mythology, which magically supplied its owners with endless food and drink. The cornucopians are sometimes known as \"boomsters\", and their philosophic opponents—Malthus and his school—are called \"doomsters\" or \"doomers.\"\n\nAs a society becomes more wealthy, it also creates a well-developed set of legal rules to produce the conditions of freedom and security that progress requires.\n\nIn \"Progress and Poverty\" written in 1879, after describing the powerful reproductive forces of nature, the political economist Henry George wrote, \"That the earth could maintain a thousand billions of people as easily as a thousand millions is a necessary deduction from the manifest truths that, at least so far as our agency is concerned, matter is eternal and force must forever continue to act.\" \n\nStereotypically, a cornucopian is someone who posits that there are few intractable natural limits to growth and believes the world can provide a practically limitless abundance of natural resources. The label 'cornucopian' is rarely self-applied, and is most commonly used derogatorily by those who believe that the target is overly optimistic about the resources that will be available in the future.\n\nOne common example of this labeling is by those who are skeptical of the view that technology can solve, or overcome, the problem of an exponentially-increasing human population living off a finite base of natural resources. Cornucopians might counter that human population growth has slowed dramatically, and not only is currently growing at a linear rate, but is projected to peak and start declining in the second half of the 21st century. Furthermore, it always has in the past, even when population was increasing at a far faster rate of growth.\n\n\n"}
{"id": "384743", "url": "https://en.wikipedia.org/wiki?curid=384743", "title": "Cyclopropane", "text": "Cyclopropane\n\nCyclopropane is a cycloalkane molecule with the molecular formula CH, consisting of three carbon atoms linked to each other to form a ring, with each carbon atom bearing two hydrogen atoms resulting in D molecular symmetry. The small size of the ring creates substantial ring strain in the structure.\n\nCyclopropane is an anaesthetic when inhaled. In modern anaesthetic practice, it has been superseded by other agents. Due to its extreme reactivity, cyclopropane-oxygen mixtures may explode.\n\nCyclopropane was discovered in 1881 by August Freund, who also proposed the correct structure for the new substance in his first paper. Freund treated 1,3-dibromopropane with sodium, causing an intramolecular Wurtz reaction leading directly to cyclopropane. The yield of the reaction was improved by Gustavson in 1887 with the use of zinc instead of sodium. Cyclopropane had no commercial application until Henderson and Lucas discovered its anaesthetic properties in 1929; industrial production had begun by 1936.\n\nCyclopropane was introduced into clinical use by the American anaesthetist Ralph Waters who used a closed system with carbon dioxide absorption to conserve this then-costly agent.\nCyclopropane is a relatively potent, non-irritating and sweet smelling agent with a minimum alveolar concentration of 17.5% and a of 0.55. This meant induction of anaesthesia by inhalation of cyclopropane and oxygen was rapid and not unpleasant. However at the conclusion of prolonged anaesthesia patients could suffer a sudden decrease in blood pressure, potentially leading to cardiac dysrhythmia; a reaction known as \"cyclopropane shock\". For this reason, as well as its high cost and its explosive nature, it was latterly used only for the induction of anaesthesia, and has not been available for clinical use since the mid 1980s.\nCylinders and flow meters were coloured orange.\n\nCyclopropane is inactive at the GABA and glycine receptors, and instead acts as an NMDA receptor antagonist. It also inhibits the AMPA receptor and nicotinic acetylcholine receptors, and activates certain K channels.\n\nThe triangular structure of cyclopropane requires the bond angles between carbon-carbon covalent bonds to be 60°. This is far less than the thermodynamically most stable angle of 109.5° (for bonds between atoms with sp hybridised orbitals) and leads to significant ring strain. The molecule also has torsional strain due to the eclipsed conformation of its hydrogen atoms. As such, the bonds between the carbon atoms are considerably weaker than in a typical alkane, resulting in much higher reactivity.\n\nBonding between the carbon centres is generally described in terms of bent bonds. In this model the carbon-carbon bonds are bent outwards so that the inter-orbital angle is 104°. This reduces the level of bond strain and is achieved by distorting the sp hybridisation of carbon atoms to technically sp hybridisation (i.e. s density and p density) so that the C-C bonds have more π character than normal (at the same time the carbon-to-hydrogen bonds gain more s-character). One unusual consequence of bent bonding is that while the C-C bonds in cyclopropane are weaker than normal, the carbon atoms are also closer together than in a regular alkane bond: 151 pm versus 153 pm (average alkene bond: 146 pm).\n\nStability due to cyclic delocalization of the six electrons of cyclopropane's three CC σ bonds was given by Michael J. S. Dewar as an explanation of the only slightly greater strain of cyclopropane (\"only\" 27.6 kcal/mol) as compared to cyclobutane (26.2 kcal/mol) with cyclohexane as reference with E=0 kcal/mol. This stabilization is referred to as σ aromaticity, in contrast to the usual π aromaticity, that, for example, is a highly stabilizing effect in benzene. Other studies do not support the role of σ-aromaticity in cyclopropane and the existence of an induced ring current; such studies provide an alternative explanation for the energetic stabilization and abnormal magnetic behaviour of cyclopropane.\n\nCyclopropane was first produced via a Wurtz coupling, in which 1,3-dibromopropane was cyclised using sodium. The yield of this reaction can be improved by exchanging the metal for zinc.\n\nCyclopropane rings are found in numerous biomolecules (e.g., pyrethrins, a group of natural insecticides) and pharmaceutical drugs. As such the formation of cyclopropane rings, generally referred to as cyclopropanation, is an active area of chemical research.\n\nOwing to the increased π-character of its C-C bonds, cyclopropane can react like an alkene in certain cases. For instance it undergoes hydrohalogenation with mineral acids to give linear alkyl halides. Substituted cyclopropanes also react, following Markovnikov's rule. Substituted cyclopropanes can oxidatively add to transition metals, in a process referred to as C–C activation.\n\nCyclopropane is highly flammable. However, despite its strain energy it is not substantially more explosive than other alkanes.\n\n\n"}
{"id": "28228644", "url": "https://en.wikipedia.org/wiki?curid=28228644", "title": "Electronic Product Environmental Assessment Tool", "text": "Electronic Product Environmental Assessment Tool\n\nThe Electronic Product Environmental Assessment Tool (EPEAT) is a method for purchasers (governments, institutions, consumers, etc.) to evaluate the effect of a product on the environment. It assesses various lifecycle environmental aspects of a device and ranks products as Gold, Silver or Bronze based on a set of environmental performance criteria.\n\nEPEAT is managed by the Green Electronics Council, a non-profit organization founded in 2005 to inspire and catalyze environmental leadership throughout the lifecycle of electronic technologies.' In the system, device manufacturers self-declare which environmental criteria their products meet, with those declarations overseen by a network of Conformity Assurance Bodies that help manufacturers demonstrate that their products meet the IEEE 1680 family of ‘green electronics’ standards. Products are rated Bronze, Silver or Gold depending upon the number of criteria the devices meet. Bronze-rated products meet all the required criteria in each EPEAT product category. Silver-rated products meet all the required criteria plus at least 50% of the optional criteria. Gold-rated products meet all the required criteria plus at least 75% of the optional criteria.\n\nThe system launched in 2006 with 60 products from three different PC and Display (monitor) manufacturers. The PC category includes 52 different environmental criteria - 23 required and 28 optional — that measure a product's efficiency and environmental attributes. In 2013, two additional categories made their EPEAT debut: Imaging Equipment in February and Televisions in April.\n\nOn January 24, 2007, President George W. Bush issued Executive Order 13423, which required all United States Federal agencies to use EPEAT when purchasing computer systems. This commitment was renewed on October 5, 2009 by President Obama's Executive Order 13514. The U.S. Federal Acquisition Regulations (FAR) were subsequently adjusted to require all federal agencies to purchase 'at least 95 percent' of their electronics based on EPEAT status if an appropriate EPEAT category exists for those devices. The FAR was updated in 2015 to reflect EPEAT's addition of the Imaging Equipment and Television categories.\n\nIn January 2010, Amazon.com began using EPEAT to identify greener electronic products on its website.\n\nMultiple international organizations provide EPEAT-registration services for manufacturers in North America, Europe, Asia, South America and Australia. These organizations, called Conformity Assurance Bodies, include Green Electronics Council's Conformity Assurance Body, Dekra, UL Environment, Intertek, TuV Rheinland, CQC, CESI and VDE. Each company has Auditors qualified to evaluate the conformance claims of electronics manufacturers and suppliers.\n\nIn July 2014, the EPEAT system expanded to support product registrations in India. With the addition of India, EPEAT was available in 43 countries.\n\nIn 2012, Apple's Retina MacBook Pro debuted on EPEAT with a Gold rating after briefly deciding to remove all of its products from EPEAT.\nThe laptop was accepted following a number of \"clarifications\" of the standard, for example specifying that the presence of USB ports was now considered sufficient to meet the upgradability requirement, and that tools to disassemble the laptop need only be available for purchase by the public.\niFixit.org labelled the laptop as \"the least repairable, least recyclable computer encountered in more than a decade of disassembling electronics\" and joined Greenpeace in denouncing a suspected case of greenwashing.\n\n"}
{"id": "31998775", "url": "https://en.wikipedia.org/wiki?curid=31998775", "title": "Energy in Iraq", "text": "Energy in Iraq\n\nPrimary energy use in 2009 in Iraq was 374 TWh and 13 TWh per million persons.\n\nIn 2008, Iraq was the 9th highest crude oil exporter with 88 Mt.\n\nOil provided 85% of government income. Iraq's oil reserves were the third biggest in the world, after Saudi Arabia and Iran. In 2009 the Iraq government set a target to increase oil production from to in six years. In June 2009 oil production rights in the Rumaila oil field were sold to BP and China National Petroleum for 20 years contracts. Investments are estimated as $10–20 billion. Field reserves are . In October 2009 Rumaila’s capacity was . Iraq's total oil production was .\n\nA second auction took place in December 2009, and Iraq sold rights to seven oil fields for 20 years, increasing oil production in future. The production companies will receive between $1 and $5.5 per barrel produced:\n\n"}
{"id": "44309864", "url": "https://en.wikipedia.org/wiki?curid=44309864", "title": "Fluid flow through porous media", "text": "Fluid flow through porous media\n\nIn fluid mechanics, fluid flow through porous media is the manner in which fluids behave when flowing through a porous medium, for example sponge or wood, or when filtering water using sand or another porous material. As commonly observed, some fluid flows through the media while some mass of the fluid is stored in the pores present in the media.\n\nThe basic law governing the flow of fluids through porous media is Darcy's Law, which was formulated by the French civil engineer Henry Darcy in 1856 on the basis of his experiments on vertical water filtration through sand beds.\n\nFor transient processes in which the flux varies from point to-point, the following differential form of Darcy’s law is used.\n\nDarcy's law is valid for situation where the porous material is already saturated with the fluid. For the calculation of capillary imbibition speed of a liquid to an initially dry medium, Washburn's or Bosanquet's equations are used.\n\nMass conservation of fluid across the porous medium involves the basic principle that mass flux in minus mass flux out equals the increase in amount stored by a medium. This means that total mass of the fluid is always conserved. In mathematical form, considering a time period from formula_1 to formula_2, length of porous medium from formula_3 to formula_4 and formula_5 being the mass stored by the medium, we have\n\nFurthermore, we have that formula_7, where formula_8 is the pore volume of the medium between formula_9 and formula_10 and formula_11 is the density. So formula_12 where formula_13 is the porosity. Dividing both sides by formula_14, while formula_15, we have for 1 dimensional linear flow in a porous medium the relation\n\nIn three dimensions, the equation can be written as\n\nThe mathematical operation on the left-hand side of this equation is known as the divergence of formula_18, and represents the rate at which fluid diverges from a given region, per unit volume.\n\nUsing product rule(and chain rule) on right hand side of the above mass conservation equation (i),\n\nHere, formula_20 = compressibility of the fluid and formula_21 = compressibility of porous medium. Now considering the left hand side of the mass conservation equation, which is given by Darcy's Law as\n\nEquating the results obtained in formula_23 & formula_24, we get\n\nThe second term on the left side is usually negligible, and we obtain the diffusion equation in 1 dimension as\n\nwhere formula_27.\n\n"}
{"id": "17267754", "url": "https://en.wikipedia.org/wiki?curid=17267754", "title": "Forward Wind Energy Center", "text": "Forward Wind Energy Center\n\nThe Forward Wind Energy Center is a wind farm in Dodge and Fond du Lac County, Wisconsin near Brownsville, Wisconsin. The farm is planned to consist of 86 General Electric 1.5sle Wind turbines. There was significant controversy in selection of the location and delays in construction due to its proximity to Horicon National Wildlife Refuge just to the west of the site and an Air Route Surveillance Radar to the south as well as other issues. \"Forward\" is also the motto of the State of Wisconsin.\n\n\n"}
{"id": "37507495", "url": "https://en.wikipedia.org/wiki?curid=37507495", "title": "Frasilah", "text": "Frasilah\n\nA frasilah is a historical weight unit that was used in the 19th century on the island of Zanzibar and in eastern Africa. A frasilah is equivalent to about . This weight unit was used mainly by the Arabs involved in the ivory trade to weigh the mass of elephant tusks and other goods used in the trade, such as beads, cloth and brass wire. In 1871, a frasilah of ivory could be purchased inland for 20 US$ (around 500 US$ in 2011) in goods and sold in Zanzibar for about 60 US$ (around 1 500 US$ in 2011).\n\n"}
{"id": "36370082", "url": "https://en.wikipedia.org/wiki?curid=36370082", "title": "Gampi", "text": "Gampi\n\nGampi or Ganpi are a group of Japanese shrubs, members of the genus \"Wikstroemia\", some of which have been used for making paper since the 8th century. It is used to make the high quality washi paper, as are kōzo and mitsumata trees. \n\nVarious sources have identified Gampi or Ganpi as:\n\n"}
{"id": "323137", "url": "https://en.wikipedia.org/wiki?curid=323137", "title": "Geometric phase", "text": "Geometric phase\n\nIn classical and quantum mechanics, the geometric phase, Pancharatnam–Berry phase (named after S. Pancharatnam and Sir Michael Berry), Pancharatnam phase or most commonly Berry phase, is a phase difference acquired over\nthe course of a cycle, when a system is subjected to cyclic adiabatic processes, which results from the geometrical properties of the parameter space of the Hamiltonian. The phenomenon was independently discovered by Pancharantnam in 1956, and by Longuet-Higgins in \n1958. It was generalized by Berry\nin 1984. It can be seen in the Aharonov–Bohm effect and in the conical intersection of potential energy surfaces. In the case of the Aharonov–Bohm effect, the adiabatic parameter is the magnetic field enclosed by two interference paths, and it is cyclic in the sense that these two paths form a loop. In the case of the conical intersection, the adiabatic parameters are the molecular coordinates. Apart from quantum mechanics, it arises in a variety of other wave systems, such as classical optics. As a rule of thumb, it can occur whenever there are at least two parameters characterizing a wave in the vicinity of some sort of singularity or hole in the topology; two parameters are required because either the set of nonsingular states will not be simply connected, or there will be nonzero holonomy.\n\nWaves are characterized by amplitude and phase, and may vary as a function of those parameters. The geometric phase occurs when both parameters are changed simultaneously but very slowly (adiabatically), and eventually brought back to the initial configuration. In quantum mechanics, this could involve rotations but also translations of particles, which are apparently undone at the end. One might expect that the waves in the system return to the initial state, as characterized by the amplitudes and phases (and accounting for the passage of time). However, if the parameter excursions correspond to a loop instead of a self-retracing back-and-forth variation, then it is possible that the initial and final states differ in their phases. This phase difference is the geometric phase, and its occurrence typically indicates that the system's parameter dependence is singular (its state is undefined) for some combination of parameters.\n\nTo measure the geometric phase in a wave system, an interference experiment is required. The Foucault pendulum is an example from classical mechanics that is sometimes used to illustrate the geometric phase. This mechanics analogue of the geometric phase is known as the Hannay angle.\n\nIn a quantum system at the n-th eigenstate, an adiabatic evolution of the Hamiltonian sees the system remain in the n-th eigenstate of the Hamiltonian, while also obtaining a phase factor. The phase obtained has a contribution from the state's time evolution and another from the variation of the eigenstate with the changing Hamiltonian. The second term corresponds to the Berry phase and for non-cyclical variations of the Hamiltonian it can be made to vanish by a different choice of the phase associated with the eigenstates of the Hamiltonian at each point in the evolution.\n\nHowever, if the variation is cyclical, the Berry phase cannot be cancelled; it is invariant and becomes an observable property of the system. By reviewing the proof of the adiabatic theorem given by Max Born and Vladimir Fock, in Zeitschrift für Physik 51, 165 (1928), we could characterize the whole change of the adiabatic process into a phase term. Under the adiabatic approximation, the coefficient of the nth eigenstate under adiabatic process is given by\n\nformula_1\n\nwhere formula_2 is the Berry phase with respect of parameter t. Changing the variable t into generalized parameters, we could rewrite the Berry phase into \n\nwhere formula_4 parametrizes the cyclic adiabatic process. It follows a closed path formula_5 in the appropriate parameter space. Geometric phase along the closed path formula_5 can also be calculated by integrating the Berry curvature over surface enclosed by formula_5.\n\nOne of the easiest examples is the Foucault pendulum. An easy explanation in terms of geometric phases is given by von Bergmann and von Bergmann:\n\nTo put it in different words, there are no inertial forces that could make the pendulum precess, so the precession (relative to the direction of motion of the path along which the pendulum is carried) is entirely due to the turning of this path. Thus the orientation of the pendulum undergoes parallel transport. For the original Foucault pendulum, the path is a circle of latitude, and by the Gauss–Bonnet theorem, the phase shift is given by the enclosed solid angle.\n\nA second example is linearly polarized light entering a single-mode optical fiber. Suppose the fiber traces out some path in space and the light exits the fiber in the same direction as it entered. Then compare the initial and final polarizations. In semiclassical approximation the fiber functions as a waveguide and the momentum of the light is at all times tangent to the fiber. The polarization can be thought of as an orientation perpendicular to the momentum. As the fiber traces out its path, the momentum vector of the light traces out a path on the sphere in momentum space. The path is closed since initial and final directions of the light coincide, and the polarization is a vector tangent to the sphere. Going to momentum space is equivalent to taking the Gauss map. There are no forces that could make the polarization turn, just the constraint to remain tangent to the sphere. Thus the polarization undergoes parallel transport and the phase shift is given by the enclosed solid angle (times the spin, which in case of light is 1).\n\nA stochastic pump is a classical stochastic system that responds with nonzero, on average, currents to periodic changes of parameters.\nThe stochastic pump effect can be interpreted in terms of a geometric phase in evolution of the moment generating function of stochastic currents. \n\nThe geometric phase can be evaluated exactly for a spin-½ particle in a magnetic field.\n\nWhile Berry's formulation was originally defined for linear Hamiltonian systems, it was soon realized by Ning and Haken\n\nThere are several ways to compute the geometric phase in molecules within the Born Oppenheimer framework. One way is through the \"non-adiabatic coupling formula_8 matrix\" defined by\n\nformula_9\n\nwhere formula_10 is the adiabatic electronic wave function, depending on the nuclear parameters formula_11. The nonadiabatic coupling can be used to define a loop integral, analogous to a Wilson loop (1974) in field theory, developed independently for molecular framework by M. Baer (1975, 1980, 2000). Given a closed loop formula_12, parameterized by formula_13 where formula_14 is a parameter and formula_15. The D-matrix is given by:\n\nformula_16\n\n(here, formula_17 is a path ordering symbol). It can be shown that once formula_18 is large enough (i.e. a sufficient number of electronic states is considered) this matrix is diagonal with the diagonal elements equal to formula_19 where formula_20 are the geometric phases associated with the loop for the formula_21 adiabatic electronic state.\n\nFor time-reversal symmetrical electronic Hamiltonians the geometric phase reflects the number of conical intersections encircled by the loop. More accurately:\n\nformula_22\n\nwhere formula_23 is the number of conical intersections involving the adiabatic state formula_24 encircled by the loop formula_12.\n\nAn alternative to the D-matrix approach would be a direct calculation of the Pancharatnam phase. This is especially useful if one is interested only in the geometric phases of a single adiabatic state. In this approach, one takes a number formula_26 of points formula_27 along the loop formula_28 with formula_29 and formula_30 then using only the jth adiabatic states formula_31 computes the Pancharatnam product of overlaps:\n\nformula_32\n\nIn the limit formula_33 one has (See Ryb & Baer 2004 for explanation and some applications):\n\nformula_34\n\nElectron subjected to magnetic field formula_35 moves on a circular (cyclotron) orbit. Classically, any cyclotron radius formula_36 is acceptable. Quantum-mechanically, only discrete energy levels (Landau levels) are allowed and since formula_36 is related to electron's energy, this corresponds to quantized values of formula_36. The energy quantization condition obtained by solving Schrödinger's equation reads, for example, formula_39 for free electrons (in vacuum) or formula_40 for electrons in graphene where formula_41. Although the derivation of these results is not difficult, there is an alternative way of deriving them which offers in some respect better physical insight into the Landau level quantization. This alternative way is based on the semiclassical Bohr-Sommerfeld quantization condition\n\nformula_42\n\nwhich includes the geometric phase formula_43 picked up by the electron while it executes its (real-space) motion along the closed loop of the cyclotron orbit. For free electrons, formula_44 while formula_45 for electrons in graphene. It turns out that the geometric phase is directly linked to formula_46 of free electrons and formula_47 of electrons in graphene.\n\n\n For simplicity, we consider electrons confined to a plane, such as 2DEG and magnetic field perpendicular to the plane.\n\n\n\n"}
{"id": "39567908", "url": "https://en.wikipedia.org/wiki?curid=39567908", "title": "Grupo Gea Perona", "text": "Grupo Gea Perona\n\nPerona Gea Group is a Spanish transportation company primarily active in the oil and gas industry. It was founded in 1927 by Francisco Gea Perona.\n\nThe company transports refined petroleum products by road and distributes products for Repsol, Endesa and Gas Natural. The group also transports food, chemical products, and manages re-gasification plants.\n"}
{"id": "539678", "url": "https://en.wikipedia.org/wiki?curid=539678", "title": "Güssing", "text": "Güssing\n\nGüssing (, ) is a town in Burgenland, Austria. It is located at , with a population of 3,811 (2011), and is the administrative center of the Güssing district.\n\nThe Güssing Castle, built in 1157, is the oldest castle in Burgenland and a regional landmark.\n\nThe lords of Güssing (in Hungarian: \"Kőszeg\", in Slovak: \"Kysak\") were a noble family in the frontier region of Austria and the Kingdom of Hungary. Note that Kőszeg is the name of a nearby Hungarian town (known as \"Güns\" in German) to which that family moved its residence from Güssing in 1274.\nIn 1522, it became the residence of the Batthyány family, one of the most distinguished Magnate families in Hungary.\n\n\nGüssing, a major town in south Burgenland, a district comprising around 27,000 inhabitants, is the first community in the European Union to produce its whole energy demand – electricity, heating/cooling, fuels – out of renewable resources, all resources from within the region.\nHowever, to appreciate the enormity of this achievement, you need to rewind and go back to 1988 when Güssing was one of the poorest regions in Austria. \nThen, the community relied on agriculture, with farmers selling corn, sunflower oil and timber to make a living. While in terms of tourism, the main attraction was a 12th-century castle built by Hungarian nobles.\nOn account of the geographically unfavourable location near the border, major trade or industrial businesses did not exist at that time and the whole district did not have any transportation infrastructure at all (neither railroad nor highway). This resulted in a scarcity of jobs, 70% weekly commuters to Vienna and a high rate of migration to other regions. At the time, the town was said to be hardly able to afford its annual €6 million (£4.7million) fuel bill.\nTo change this situation the “reformers” realised that substantial capital outflow from the region is due to the town's energy being bought from outside sources. This included oil, power and fuels, while existing resources e.g. 45% forest land, remained largely unused.\nThus the “reformers” proposed to abandon fossile energy and start producing and subsequently selling energy to the citizens (customers) themselves. So they wanted to keep that € 6 Million (value for 1992, based on conventional energy prices of that year) in the city.\n\nIn the early 1990s, a policy was proposed which called for a complete abandonment of fossil-fuel-based energy. The objective was to supply, in a first step, the town of Güssing and subsequently the whole district with regionally available renewable energy sources.\nThe election of the town's current mayor, Peter Vadasz, in 1992 accelerated the process, particularly when he appointed Reinhard Koch, an electrical engineer and native of Güssing, to assess how the town could benefit from its natural resources, i.e. forest land. Koch was \"fresh\" out of university and was not willing to share the destiny of many local commuters to Vienna.\nThe first step taken was to order that all public buildings in the town should stop using fossil fuels.\n\nAs result of the energetic optimisation of buildings in the town, expenditure on energy was reduced by almost 50%. Then a wood burning plant that provided heating for 27 houses was built. Then, a facility was constructed which turned rapeseed into car fuel.\nIn 1998, Koch and Vadasz saw a presentation by a Viennese scientist, Hermann Hofbauer, about a technology he had developed to make an alternative fuel from wood.\nThey asked Hofbauer and Vienna's Technical University to build a pilot project in Güssing applying the technology, where wood chips are gasified under high temperature conditions. Gas fuels a Jenbacher engine that produces electricity and the “by-product” heat is used to produce warm water for the district heating system. Plant efficiency about 82-85%. Research and development went further on, so Güssing hosts today a number of innovative technologies, solutions, and patents. Today there is a team of highly trained technicians and scientists working in Güssing.\n\nThe renewable-energy project expanded to the region and there are 27 decentralized power plants within the Güssing county. Güssing today has an “energy” turnover of about € 14 Million p.a. Part of the profit is invested back into renewable energy projects.\n\nWhat effect has this had on the region? A special scheme (very easy – stable energy prices, not linked to oil & gas, guaranteed long-term, 10–15 years) promoting the establishment of enterprises in the area has brought 50 new enterprises with more than 1,000 direct and indirect jobs in the city. (these 1000 jobs are in the City, region not calculated). Güssing has since developed into an important location for industries with high energy consumption, such as parquetry production or hardwood drying. The real highlight is the Blue Chip Energy, first high-efficiency solar cell production in Austria, a joint venture with Solon AG, who came to Güssing only because they can power the plant with clean energy from the renewable resources.\n\nThe town is conscious about the resources and is keen to take care and look after the surrounding forest to ensure they have a good supply of renewable energy for the future. Added to which, the town is currently using less than half of its yearly wood growth supply to feed its power plants. Thus, no cutting the substance.\n\nWithin the process of becoming energy autonomous city, a number of proprietary technologies and patents developed, to be applied in different fields, such as photovoltaic, biomass, etc. Also, extensive experience in analysis, preparation and implementation of such projects was accumulated, that experience to be given to other cities and communities willing to go the same renewable way.\n\nGüssing today enjoys truly international popularity. Thus, ECRE Güssing International AG was established as the central company to expose Güssing internationally (www.ecreag.com).\n\nSignificant power plants include a 2 MW electric power 4.5 MW thermic wood gas generator power plant in Güssing and, in nearby Strem, a 0.5 MW electric power 0.5 MW thermic biomass gasification power plant using green silage re-growing raw materials like grass, clover, mains, sunflower.\n\nAs side effect, extra income is being brought into the town through eco-tourism. Visitors flock from around the world to gain inspiration from the town and keen to stay true to their eco-friendly roots, guests can stay in hotels that are heated and electrically powered all by renewables. Some 30,000 visitors were recorded during 2007.\n\n\n\n"}
{"id": "191884", "url": "https://en.wikipedia.org/wiki?curid=191884", "title": "Headphones", "text": "Headphones\n\nHeadphones (or head-phones in the early days of telephony and radio) are a pair of small loudspeaker drivers worn on or around the head over a user's ears. They are electroacoustic transducers, which convert an electrical signal to a corresponding sound. Headphones let a single user listen to an audio source privately, in contrast to a loudspeaker, which emits sound into the open air for anyone nearby to hear. Headphones are also known as earspeakers, earphones or, colloquially, cans. Circumaural ('around the ear') and supra-aural ('over the ear') headphones use a band over the top of the head to hold the speakers in place. The other type, known as earbuds or earpieces consist of individual units that plug into the user's ear canal. In the context of telecommunication, a headset is a combination of headphone and microphone. Headphones connect to a signal source such as an audio amplifier, radio, CD player, portable media player, mobile phone, video game console, or electronic musical instrument, either directly using a cord, or using wireless technology such as Bluetooth, DECT or FM radio. The first headphones were developed in the late 19th century for use by telephone operators, to keep their hands free. Initially the audio quality was mediocre and a step forward was the invention of high fidelity headphones.\n\nHeadphones are made in a range of different audio reproduction quality capabilities. Headsets designed for telephone use typically cannot reproduce sound with the high fidelity of expensive units designed for music listening by audiophiles. Headphones that use cables typically have either a 1/4 inch (6.35mm) or 1/8 inch (3.5mm) phone jack for plugging the headphones into the audio source. Some stereo earbuds are wireless, using Bluetooth connectivity to transmit the audio signal by radio waves from source devices like cellphones and digital players. Due to the spread of wireless devices in recent years headphones are increasingly used by people in public places such as sidewalks, grocery stores, and public transit. Headphones are also used by people in various professional contexts, such as audio engineers mixing sound for live concerts or sound recordings and DJs, who use headphones to cue up the next song without the audience hearing, aircraft pilots and call center employees. The latter two types of employees use headphones with an integrated microphone.\n\nHeadphones originated from the telephone receiver earpiece, and were the only way to listen to electrical audio signals before amplifiers were developed. The first truly successful set was developed in 1910 by Nathaniel Baldwin, who made them by hand in his kitchen and sold them to the United States Navy.\n\nThese early headphones used moving iron drivers, with either single-ended or balanced armatures. The common single-ended type used voice coils wound around the poles of a permanent magnet, which were positioned close to a flexible steel diaphragm. The audio current through the coils varied the magnetic field of the magnet, exerting a varying force on the diaphragm, causing it to vibrate, creating sound waves. The requirement for high sensitivity meant that no damping was used, so the frequency response of the diaphragm had large peaks due to resonance, resulting in poor sound quality. These early models lacked padding, and were often uncomfortable to wear for long periods. Their impedance varied; headphones used in telegraph and telephone work had an impedance of 75 ohms. Those used with early wireless radio had more turns of finer wire to increase sensitivity. Impedance of 1000 to 2000 ohms was common, which suited both crystal sets and triode receivers. Some very sensitive headphones, such as those manufactured by Brandes around 1919, were commonly used for early radio work.\n\nIn early powered radios, the headphone was part of the vacuum tube's plate circuit and carried dangerous voltages. It was normally connected directly to the positive high voltage battery terminal, and the other battery terminal was securely grounded. The use of bare electrical connections meant that users could be shocked if they touched the bare headphone connections while adjusting an uncomfortable headset.\n\nIn 1958, John C. Koss, an audiophile and jazz musician from Milwaukee, produced the first stereo headphones. Previously, headphones were used only by the US navy, telephone and radio operators, and individuals in similar industries.\n\nSmaller earbud type earpieces, which plugged into the user's ear canal, were first developed for hearing aids. They became widely used with transistor radios, which commercially appeared in 1954 with the introduction of the Regency TR-1. The most popular audio device in history, the transistor radio changed listening habits, allowing people to listen to radio anywhere. The earbud uses either a moving iron driver or a piezoelectric crystal to produce sound. The 3.5 mm radio and phone connector, which is the most commonly used in portable application today, has been used at least since the Sony EFM-117J transistor radio, which was released in 1964. Its popularity was reinforced with its use on the Walkman portable tape player in 1979.\n\nHeadphones may be used with stationary CD and DVD players, home theater, personal computers, or portable devices (e.g., digital audio player/MP3 player, mobile phone). Cordless headphones are not connected to their source by a cable. Instead, they receive a radio or infrared signal encoded using a radio or infrared transmission link, such as FM, Bluetooth or Wi-Fi. These are powered receiver systems, of which the headphone is only a component. Cordless headphones are used with events such as a Silent disco or Silent Gig.\n\nIn the professional audio sector, headphones are used in live situations by disc jockeys with a DJ mixer, and sound engineers for monitoring signal sources. In radio studios, DJs use a pair of headphones when talking to the microphone while the speakers are turned off to eliminate acoustic feedback while monitoring their own voice. In studio recordings, musicians and singers use headphones to play or sing along to a backing track or band. In military applications, audio signals of many varieties are monitored using headphones.\n\nWired headphones are attached to an audio source by a cable. The most common connectors are 6.35 mm (¼″) and 3.5 mm phone connectors. The larger 6.35 mm connector is more common on fixed location home or professional equipment. The 3.5 mm connector remains the most widely used connector for portable application today. Adapters are available for converting between 6.35 mm and 3.5 mm devices.\n\nElectrical characteristics of dynamic loudspeakers may be readily applied to headphones, because most headphones are small dynamic loudspeakers.\n\nHeadphones are available with low or high impedance (typically measured at 1 kHz). Low-impedance headphones are in the range 16 to 32 ohms and high-impedance headphones are about 100-600 ohms. As the impedance of a pair of headphones increases, more voltage (at a given current) is required to drive it, and the loudness of the headphones for a given voltage decreases. In recent years, impedance of newer headphones has generally decreased to accommodate lower voltages available on battery powered CMOS-based portable electronics. This has resulted in headphones that can be more efficiently driven by battery-powered electronics. Consequently, newer amplifiers are based on designs with relatively low output impedance.\n\nThe impedance of headphones is of concern because of the output limitations of amplifiers. A modern pair of headphones is driven by an amplifier, with lower impedance headphones presenting a larger load. Amplifiers are not ideal; they also have some output impedance that limits the amount of power they can provide. To ensure an even frequency response, adequate damping factor, and undistorted sound, an amplifier should have an output impedance less than 1/8 that of the headphones it is driving (and ideally, as low as possible). If output impedance is large compared to the impedance of the headphones, significantly higher distortion is present. Therefore, lower impedance headphones tend to be louder and more efficient, but also demand a more capable amplifier. Higher impedance headphones are more tolerant of amplifier limitations, but produce less volume for a given output level.\n\nHistorically, many headphones had relatively high impedance, often over 500 ohms so they could operate well with high-impedance tube amplifiers. In contrast, modern transistor amplifiers can have very low output impedance, enabling lower-impedance headphones. Unfortunately, this means that older audio amplifiers or stereos often produce poor-quality output on some modern, low-impedance headphones. In this case, an external headphone amplifier may be beneficial.\n\nSensitivity is a measure of how effectively an earpiece converts an incoming electrical signal into an audible sound. It thus indicates how loud the headphones are for a given electrical drive level. It can be measured in decibels of sound pressure level per milliwatt (dB (SPL)/mW) or decibels of sound pressure level per volt (dB (SPL) / V). Unfortunately, both definitions are widely used, often interchangeably. As the output voltage (but not power) of a headphone amplifier is essentially constant for most common headphones, dB/mW is often more useful if converted into dB/V using Ohm's law:\n\nAlternatively, online calculators can be used. Once the sensitivity per volt is known, the maximum volume for a pair of headphones can be easily calculated from the maximum amplifier output voltage. For example, for a headphone with a sensitivity of 100 dB (SPL)/V, an amplifier with an output of 1 root mean square (RMS) voltage produces a maximum volume of 100 dB.\n\nPairing high sensitivity headphones with power amplifiers can produce dangerously high volumes and damage headphones. The maximum sound pressure level is a matter of preference, with some sources recommending no higher than 110 to 120 dB. In contrast, the American Occupational Safety and Health Administration recommends an average SPL of no more than 85 dB(A) to avoid long-term hearing loss, while the European Union standard EN 50332-1:2013 recommends that volumes above 85 dB(A) include a warning, with an absolute maximum volume (defined using 40–4000 Hz noise) of no more than 100 dB to avoid accidental hearing damage. Using this standard, headphones with sensitivities of 90, 100 and 110 dB (SPL)/V should be driven by an amplifier capable of no more than 3.162, 1.0 and 0.3162 RMS volts at maximum volume setting, respectively to reduce the risk of hearing damage.\n\nThe sensitivity of headphones is usually between about 80 and 125 dB/mW and usually measured at 1 kHz.\n\nHeadphone size can affect the balance between fidelity and portability. Generally, headphone form factors can be divided into four separate categories: \"circumaural (over-ear)\", \"supra-aural (on-ear)\", \"earbud\" and \"in-ear\".\n\nCircumaural headphones (sometimes called full size headphones or over-ear headphones) have circular or ellipsoid earpads that encompass the ears. Because these headphones completely surround the ear, circumaural headphones can be designed to fully seal against the head to attenuate external noise. Because of their size, circumaural headphones can be heavy and there are some sets that weigh over . Ergonomic headband and earpad design is required to reduce discomfort resulting from weight. These are commonly used by drummers in recording.\n\nSupra-aural headphones or on-ear headphones have pads that press against the ears, rather than around them. They were commonly bundled with personal stereos during the 1980s. This type of headphone generally tends to be smaller and lighter than circumaural headphones, resulting in less attenuation of outside noise. Supra-aural headphones can also lead to discomfort due to the pressure on the ear as compared to circumaural headphones that sit around the ear. Comfort may vary due to the earcup material.\n\nBoth circumaural and supra-aural headphones can be further differentiated by the type of earcups:\n\nOpen-back headphones have the back of the earcups open. This leaks more sound out of the headphone and also lets more ambient sounds into the headphone, but gives a more natural or speaker-like sound, due to including sounds from the environment.\n\nClosed-back (or sealed) styles have the back of the earcups closed. They usually block some of the ambient noise. Closed-back headphones usually can produce stronger low frequencies than open-back headphones.\n\nSemi-open headphones, have a design that can be considered as a compromise between open-back headphones and closed-back headphones. Some believe the term \"semi-open\" is purely there for marketing purposes. There is no exact definition for the term semi-open headphone. Where the open-back approach has hardly any measure to block sound at the outer side of the diaphragm and the closed-back approach really has a closed chamber at the outer side of the diaphragm, a semi-open headphone can have a chamber to partially block sound while letting some sound through via openings or vents.\n\nEarphones are very small headphones that are fitted directly in the outer ear, facing but not inserted in the ear canal. Earphones are portable and convenient, but many people consider them uncomfortable. They provide hardly any acoustic isolation and leave room for ambient noise to seep in; users may turn up the volume dangerously high to compensate, at the risk of causing hearing loss. On the other hand, they let the user be better aware of their surroundings. Since the early days of the transistor radio, earphones have commonly been bundled with personal music devices. They are sold at times with foam pads for comfort. (The use of the term \"earbuds\", which has been around since at least 1984, did not hit its peak until after 2001, with the success of Apple's MP3 player.)\n\nIn-ear headphones, also known as in-ear monitors (IEMs) or canalphones, are small headphones with similar portability to earbuds that are inserted in the ear canal itself. IEMs are higher-quality in-ear headphones and are used by audio engineers and musicians as well as audiophiles.\n\nThe outer shells of in-ear headphones are made up of a variety of materials, such as plastic, aluminum, ceramic and other metal alloys. Because in-ear headphones engage the ear canal, they can be prone to sliding out, and they block out much environmental noise. Lack of sound from the environment can be a problem when sound is a necessary cue for safety or other reasons, as when walking, driving, or riding near or in vehicular traffic.\n\nGeneric or custom-fitting ear canal plugs are made from silicone rubber, elastomer, or foam. Custom in-ear headphones use castings of the ear canal to create custom-molded plugs that provide added comfort and noise isolation.\n\nThis type combines advantages of earbuds and in-ear headphones – depending on the environment and requirements of the user, they provide passive noise reduction for quality mode (conversation or active music listening) or they give control over the sound environment around user in comfort mode (stand by or background voice/music listening).\n\nA headset is a headphone combined with a microphone. Headsets provide the equivalent functionality of a telephone handset with hands-free operation. Among applications for headsets, besides telephone use, are aviation, theatre or television studio intercom systems, and console or PC gaming. Headsets are made with either a single-earpiece (mono) or a double-earpiece (mono to both ears or stereo). The microphone arm of headsets is either an external microphone type where the microphone is held in front of the user's mouth, or a voicetube type where the microphone is housed in the earpiece and speech reaches it by means of a hollow tube.\n\nTelephone headsets connect to a fixed-line telephone system. A telephone headset functions by replacing the handset of a telephone. Headsets for standard corded telephones are fitted with a standard 4P4C commonly called an RJ-9 connector. Headsets are also available with 2.5 mm jack sockets for many DECT phones and other applications. Cordless bluetooth headsets are available, and often used with mobile telephones. Headsets are widely used for telephone-intensive jobs, in particular by call centre workers. They are also used by anyone wishing to hold telephone conversations with both hands free.\n\nFor older models of telephones, the headset microphone impedance is different from that of the original handset, requiring a telephone amplifier for the telephone headset. A telephone amplifier provides basic pin-alignment similar to a telephone headset adaptor, but it also offers sound amplification for the microphone as well as the loudspeakers. Most models of telephone amplifiers offer volume control for loudspeaker as well as microphone, mute function and switching between headset and handset. Telephone amplifiers are powered by batteries or AC adaptors.\n\nUnwanted sound from the environment can be reduced by excluding sound from the ear by passive noise isolation, or, often in conjunction with isolation, by active noise cancellation.\n\nPassive noise isolation is essentially using the body of the earphone, either over or in the ear, as a passive earplug that simply blocks out sound. The headphone types that provide most attenuation are in-ear canal headphones and closed-back headphones, both circumaural and supra aural. Open-back and earbud headphones provide some passive noise isolation, but much less than the others. Typical closed-back headphones block 8 to 12 dB, and in-ears anywhere from 10 to 15 dB. Some models have been specifically designed for drummers to facilitate the drummer monitoring the recorded sound while reducing sound directly from the drums as much as possible. Such headphones claim to reduce ambient noise by around 25 dB.\n\nActive noise-cancelling headphones use a microphone, amplifier, and speaker to pick up, amplify, and play ambient noise in phase-reversed form; this to some extent cancels out unwanted noise from the environment without affecting the desired sound source, which is not picked up and reversed by the microphone. They require a power source, usually a battery, to drive their circuitry. Active noise cancelling headphones can attenuate ambient noise by 20 dB or more, but the active circuitry is mainly effective on constant sounds and at lower frequencies, rather than sharp sounds and voices. Some noise cancelling headphones are designed mainly to reduce low-frequency engine and travel noise in aircraft, trains, and automobiles, and are less effective in environments with other types of noise.\n\nHeadphones use various types of transducer to convert electrical signals to sound.\n\nThe moving coil driver, more commonly referred to as a \"dynamic\" driver is the most common type used in headphones. It consists of a stationary magnet element affixed to the frame of the headphone, which sets up a static magnetic field. The magnet in headphones is typically composed of ferrite or neodymium. A voice coil, a light coil of wire, is suspended in the magnetic field of the magnet, attached to a diaphragm, typically fabricated from lightweight, high-stiffness-to-mass-ratio cellulose, polymer, carbon material, paper or the like. When the varying current of an audio signal is passed through the coil, it creates a varying magnetic field that reacts against the static magnetic field, exerting a varying force on the coil causing it and the attached diaphragm to vibrate. The vibrating diaphragm pushes on the air to produce sound waves.\n\nElectrostatic drivers consist of a thin, electrically charged diaphragm, typically a coated PET film membrane, suspended between two perforated metal plates (electrodes). The electrical sound signal is applied to the electrodes creating an electrical field; depending on the polarity of this field, the diaphragm is drawn towards one of the plates. Air is forced through the perforations; combined with a continuously changing electrical signal driving the membrane, a sound wave is generated. Electrostatic headphones are usually more expensive than moving-coil ones, and are comparatively uncommon. In addition, a special amplifier is required to amplify the signal to deflect the membrane, which often requires electrical potentials in the range of 100 to 1000 volts.\n\nDue to the extremely thin and light diaphragm membrane, often only a few micrometers thick, and the complete absence of moving metalwork, the frequency response of electrostatic headphones usually extends well above the audible limit of approximately 20 kHz. The high frequency response means that the low midband distortion level is maintained to the top of the audible frequency band, which is generally not the case with moving coil drivers. Also, the frequency response peakiness regularly seen in the high frequency region with moving coil drivers is absent. Well-designed electrostatic headphones can produce significantly better sound quality than other types.\n\nElectrostatic headphones require a voltage source generating 100 V to over 1 kV, and are on the user's head. Since the invention of insulators, there's no actual danger. They do not need to deliver significant electric current, which further limits the electrical hazard to the wearer in case of fault.\n\nAn electret driver functions along the same electromechanical means as an electrostatic driver. However the electret driver has a permanent charge built into it, whereas electrostatics have the charge applied to the driver by an external generator. Electret and electrostatic headphones are relatively uncommon. Original electrets were also typically cheaper and lower in technical capability and fidelity than electrostatics. Patent applications from 2009-2013 have been approved that show by using different materials, i.e. a \"Fluorinated cyclic olefin electret film\", Frequency response chart readings can reach 50 kHz at 100db. When these new improved electrets are combined with a traditional dome headphone driver, headphones can be produced that are recognised by the Japan Audio Society as worthy of joining the Hi Res Audio program. US patents 8,559,660 B2. 7,732,547 B2.7,879,446 B2.7,498,699 B2.\n\nOrthodynamic (also known as Planar Magnetic) headphones use similar technology to electrostatic headphones, with some fundamental differences. They operate similarly to Planar Magnetic Loudspeakers.\n\nAn orthodynamic driver consists of a relatively large membrane that contains an embedded wire pattern. This membrane is suspended between two sets of permanent, oppositely aligned, magnets.\nA current passed through the wires embedded in the membrane produces a magnetic field that reacts with the field of the permanent magnets to induce movement in the membrane, which produces sound.\n\nA balanced armature is a sound transducer design primarily intended to increase the electrical efficiency of the element by eliminating the stress on the diaphragm characteristic of many other magnetic transducer systems. As shown schematically in the first diagram, it consists of a moving magnetic armature that is pivoted so it can move in the field of the permanent magnet. When precisely centered in the magnetic field there is no net force on the armature, hence the term 'balanced.' As illustrated in the second diagram, when there is electric current through the coil, it magnetizes the armature one way or the other, causing it to rotate slightly one way or the other about the pivot thus moving the diaphragm to make sound.\nThe design is not mechanically stable; a slight imbalance makes the armature stick to one pole of the magnet. A fairly stiff restoring force is required to hold the armature in the 'balance' position. Although this reduces its efficiency, this design can still produce more sound from less power than any other. Popularized in the 1920s as Baldwin Mica Diaphragm radio headphones, balanced armature transducers were refined during World War II for use in military sound powered telephones. Some of these achieved astonishing electro-acoustic conversion efficiencies, in the range of 20% to 40%, for narrow bandwidth voice signals.\n\nToday they are typically used only in in-ear headphones and hearing aids, where their high efficiency and diminutive size is a major advantage.\nThey generally are limited at the extremes of the hearing spectrum (e.g. below 20 Hz and above 16 kHz) and require a better seal than other types of drivers to deliver their full potential. Higher-end models may employ multiple armature drivers, dividing the frequency ranges between them using a passive crossover network. A few combine an armature driver with a small moving-coil driver for increased bass output.\n\nThe earliest loudspeakers for radio receivers used balanced armature drivers for their cones.\n\nThe thermoacoustic effect generates sound from the audio frequency Joule heating of the conductor, an effect that is not magnetic and does not vibrate the speaker.\nIn 2013 a carbon nanotube thin-yarn earphone based on the thermoacoustic mechanism was demonstrated by a research group in Tsinghua University. The as-produced CNT thin yarn earphone has a working element called CNT thin yarn thermoacoustic chip. Such a chip is composed of a layer of CNT thin yarn array supported by the silicon wafer, and periodic grooves with certain depth are made on the wafer by micro-fabrication methods to suppress the heat leakage from the CNT yarn to the substrate.\n\nTransducer technologies employed much less commonly for headphones include the Heil Air Motion Transformer (AMT); Piezoelectric film; Ribbon planar magnetic; Magnetostriction and Plasma-ionisation. The first Heil AMT headphone was marketed by ESS Laboratories and was essentially an ESS AMT tweeter from one of the company's speakers being driven at full range. Since the turn of the century, only Precide of Switzerland have manufactured an AMT headphone. Piezoelectric film headphones were first developed by Pioneer, their two models used a flat sheet of film that limited the maximum volume of air movement. Currently, TakeT produces a piezoelectric film headphone shaped similarly to an AMT transducer but, which like the Precide driver, has a variation in the size of transducer folds over the diaphragm. It additionally incorporates a two way design by its inclusion of a dedicated tweeter/supertweeter panel. The folded shape of a diaphragm allows a transducer with a larger surface area to fit within smaller space constraints. This increases the total volume of air that can be moved on each excursion of the transducer given that radiating area.\n\nMagnetostriction headphones, sometimes sold under the label \"Bonephones\", work by vibrating against the side of head, transmitting sound via bone conduction. This is particularly helpful in situations where the ears must be unobstructed, or for people who are deaf for reasons that don't affect the nervous apparatus of hearing. Magnetostriction headphones though, are limited in their fidelity compared to conventional headphones that rely on the normal workings of the ear. Additionally, in the early 1990s, a French company called Plasmasonics tried to market a plasma-ionisation headphone. There are no known functioning examples left.\n\nHeadphones can prevent other people from hearing the sound, either for privacy or to prevent disturbing others, as in listening in a public library. They can also provide a level of sound fidelity greater than loudspeakers of similar cost. Part of their ability to do so comes from the lack of any need to perform room correction treatments with headphones. High-quality headphones can have an extremely flat low-frequency response down to 20 Hz within 3 dB. While a loudspeaker must use a relatively large (often 15\" or 18\") speaker driver to reproduce low frequencies, headphones can accurately reproduce bass and sub-bass frequencies with speaker drivers only 40-50 millimeters wide (or much smaller, as is the case with in-ear monitor headphones). Headphones' impressive low-frequency performance is possible because they are so much closer to the ear that they only need to move relatively small volumes of air.\n\nMarketed claims such as 'frequency response 4 Hz to 20 kHz' are usually overstatements; the product's response at frequencies lower than 20 Hz is typically very small. \nHeadphones are also useful for video games that use 3D positional audio processing algorithms, as they allow players to better judge the position of an off-screen sound source (such as the footsteps of an opponent or their gun fire).\n\nAlthough modern headphones have been particularly widely sold and used for listening to stereo recordings since the release of the Walkman, there is subjective debate regarding the nature of their reproduction of stereo sound. Stereo recordings represent the position of horizontal depth cues (stereo separation) via volume and phase differences of the sound in question between the two channels. When the sounds from two speakers mix, they create the phase difference the brain uses to locate direction. Through most headphones, because the right and left channels do not combine in this manner, the illusion of the phantom center can be perceived as lost. Hard panned sounds are also heard only in one ear rather than from one side.\n\nBinaural recordings use a different microphone technique to encode direction directly as phase, with very little amplitude difference below 2 kHz, often using a dummy head. They can produce a surprisingly lifelike spatial impression through headphones. Commercial recordings almost always use stereo recording, rather than binaural, because loudspeaker listening is more common than headphone listening.\n\nIt is possible to change the spatial effects of stereo sound on headphones, to better approximate the presentation of speaker reproduction, by using frequency-dependent cross-feed between the channels.\n\nHeadsets can have ergonomic benefits over traditional telephone handsets. They allow call center agents to maintain better posture without needing to hand-hold a handset or tilt their head sideways to cradle it.\n\nUsing headphones at a sufficiently high volume level may cause temporary or permanent hearing impairment or deafness. The headphone volume often has to compete with the background noise, especially in loud places such as subway stations, aircraft, and large crowds. Extended periods of exposure to high sound pressure levels created by headphones at high volume settings may be damaging; however, one hearing expert found that \"fewer than 5% of users select volume levels and listen frequently enough to risk hearing loss.\" Some manufacturers of portable music devices have attempted to introduce safety circuitry that limited output volume or warned the user when dangerous volume was being used, but the concept has been rejected by most of the buying public, which favors the personal choice of high volume. Koss introduced the \"Safelite\" line of cassette players in 1983 with such a warning light. The line was discontinued two years later for lack of interest.\n\nThe government of France has imposed a limit on all music players sold in the country: they must not be capable of producing more than 100dBA (the threshold of hearing damage during extended listening is 80 dB, and the threshold of pain, or theoretically of immediate hearing loss, is 130 dB).\nMotorcycle and other power-sport riders benefit by wearing foam earplugs when legal to do so to avoid excessive road, engine, and wind noise, but their ability to hear music and intercom speech is actually enhanced when doing so. The ear can normally detect 1-billionth of an atmosphere of sound pressure level, hence it is incredibly sensitive. At very high sound pressure levels, muscles in the ear tighten the tympanic membrane and this leads to a small change in the geometry of the ossicles and stirrup that results in lower transfer of force to the oval window of the inner ear (the acoustic reflex).\n\nThe risk of hearing damage also depends on the exposure time. The higher the volume, the faster hearing loss occurs. According to OSHA work safety guidelines, workers should be exposed to 90dBA noise for a maximum of 8 hours to avoid hearing loss. An increase to just 95dBA cuts the safe exposure to only 4 hours. But there is little specific consensus on the safe level of exposure. NIOSH has an 8-hour recommended exposure limit set to 85dBA. Additionally, an increase to 93dBA instead of 95dBA halves the safe exposure time to 4 hours.\n\nSome studies have found that people are more likely to raise volumes to unsafe levels while performing strenuous exercise. A Finnish study recommended that exercisers should set their headphone volumes to half of their normal loudness and only use them for half an hour.\n\nNoise cancelling headphones can be considered dangerous because of a lack of awareness the listener may have with their environment. Noise cancelling headphones are so effective that a person may not be able to hear oncoming traffic or pay attention to people around them. There is also a general danger that music in headphones can distract the listener and lead to dangerous situations.\n\nThe usual way of limiting sound volume on devices driving headphones is by limiting output power. This has the additional undesirable effect of being dependent of the efficiency of the headphones; a device producing the maximum allowed power may not produce adequate volume when paired with low-efficiency, high-impedance equipment, while the same amount of power can reach dangerous levels with very efficient earphones.\n\n"}
{"id": "39416750", "url": "https://en.wikipedia.org/wiki?curid=39416750", "title": "Hsintao Power Plant", "text": "Hsintao Power Plant\n\nThe Hsintao Power Plant () is a gas-fired power plant in Guangxi Township, Hsinchu County, Taiwan.\n\nThe natural gas fuel supply for the power plant is supplied by CPC Corporation.\n\n"}
{"id": "49169066", "url": "https://en.wikipedia.org/wiki?curid=49169066", "title": "January 2016 United States blizzard", "text": "January 2016 United States blizzard\n\nThe January 2016 United States blizzard was a crippling and historic blizzard that produced up to 3 ft (91 cm) of snow in parts of the Mid-Atlantic and Northeast United States from January 22 to January 24, 2016. Evolving from a shortwave trough that formed in the Pacific Northwest on January 19, the system consolidated into a defined low-pressure area on January 21 over Texas. Regarding it as a \"potentially historic blizzard\", meteorologists indicated the storm could produce more than of snow across a wide swath of the Mid-Atlantic region and could \"paralyze the eastern third of the nation\". Winter weather expert Paul Kocin described the blizzard as \"kind of a top-10 snowstorm\".\n\nOn January 20–22, the governors of eleven states and the mayor of Washington, D.C. declared a state of emergency in anticipation of significant snowfall and blizzard conditions. Approximately 103 million people were affected by the storm, with 33 million people under blizzard warnings. More than 13,000 flights were cancelled in relation to the storm, with effects rippling internationally. Thousands of National Guardsmen were placed on standby and states deployed millions of gallons of brine and thousands of tons of road salt to lessen the storm's effect on roadways. A travel ban was instituted for New York City and Newark, New Jersey for January 23–24. The storm was given various unofficial names, including \"Winter Storm Jonas\", \"Blizzard of 2016\" and \"Snowzilla\".\n\nSeven states observed snowfall in excess of , with accumulations peaking at in Glengary, West Virginia. Ice- and snow-covered roads led to hundreds of incidents across the affected region, several of which resulted in deaths and injuries. At least 55 people were killed in storm-related incidents: 12 in Virginia, 9 in Pennsylvania, 6 in New Jersey, 6 in New York, 6 in North Carolina, 4 in South Carolina, 3 in Maryland, 3 in Washington, D.C., 1 in Arkansas, 1 in Delaware, 1 in Georgia, 1 in Kentucky, 1 in Massachusetts, and 1 in Ohio. Total economic losses are estimated between $500 million and $3 billion. The storm ranked as a Category 5 \"extreme\" event for the Northeast on the Regional Snowfall Index, and a Category 4 for the Southeast. It is the most recent winter storm to rank as a Category 5, and the first to do so since the 2011 Groundhog Day Blizzard.\n\nA relatively minor storm ahead of the blizzard, similar to an Alberta clipper, caught numerous drivers off-guard, producing a brief period of heavy snow during rush hour in the Mid-Atlantic region on January 20. Although only of snow fell in Washington, D.C., roadways were not treated; any snow that melted on roads quickly froze into black ice, rendering them impassable. Some referred to the event as \"Carmageddon 2.0\". Portions of Interstate 95 and Interstate 495 in Virginia and Maryland (especially on the Woodrow Wilson Bridge), as well as Interstate 270, were brought to a standstill through the early hours of January 21. Virginia State Police responded to 767 accidents and 392 reports of disabled vehicles. The Virginia Department of Transportation mobilized 115 salt trucks to clear roads. A man was killed after being struck by a snow plow in Beltsville, Maryland. Washington, D.C. Mayor Muriel Bowser issued an apology for inadequate preparations in the wake of the storm on January 22. Vehicles in Maryland became stranded, with some residents abandoning their cars altogether.\n\nGeorgia Governor Nathan Deal issued a state of emergency for northern counties on January 19, ensuring areas were better prepared than during a similar storm in 2014. Icy conditions prompted road closures in northern Georgia, including portions of Interstate 75; several crashes resulted from the dangerous conditions. Similar conditions affected Tennessee and Kentucky; schools closed on account of dangerous roads. One person died and another was injured in an accident in Knox County after speeding on slippery roads. Two deaths resulted from snow-related car accidents in North Carolina. Another person died, and two others were injured, when a car collided with a salt truck in Whitley County, Kentucky.\n\nThe development of the winter storm was anticipated by forecasters for at least a week. It originated in a shortwave trough—a weather disturbance in the upper atmosphere—that came ashore at the Pacific Northwest on January 19. The trough strengthened as it moved southeastward through the Great Plains, and on January 21 it spawned a weak low-pressure area over central Texas. The incipient storm system began to intensify as it tracked eastward through the Gulf Coast states, triggering a line of strong to severe thunderstorms and multiple tornado warnings.\n\nDuring the mid-afternoon hours of January 22, a new low-pressure area began to develop over the coast of the Carolinas as the former storm tracked into central Georgia. Owing to uncertainty in short-range guidance but a high confidence of a sharp northern edge of precipitation, many forecasts until just hours before snowfall began were predicting 12\" of snow or less from Allentown, Pennsylvania, toward New York City and the southern coast of New England. As the storm moved further north and rapidly strengthened, it became apparent that snowfall would be much higher farther north and forecasters quickly began upgrading their totals. Early on January 24, as the storm was leaving New England, the system began to become elongated, as a secondary low developed to the southwest of the storm's central low. On January 25, the blizzard left the East Coast of the United States; on the same day, the system was named \"Karin\" by the University of Berlin.\n\nAccompanied by a strong jet stream in the Atlantic, the remnants of the storm crossed the British Isles on January 26. The wind and rain associated with the low was forecast to have the potential to cause disruption in the United Kingdom, and indeed there were areas that saw severe weather. During the next few days, the system accelerated towards the northeast. On January 29, the storm system was absorbed by Windstorm Leone, over Finland.\n\nMultiple offices of the National Weather Service issued various watches and warnings across the storm's projected path. Blizzard warnings covered coastal Connecticut; most of Delaware; most of Maryland; Massachusetts, particularly Martha's Vineyard; most of New Jersey; New York, including New York City; Pennsylvania; Rhode Island, especially Block Island; Virginia; and Washington, D.C. Winter storm warnings were issued from Arkansas to Massachusetts, including parts of Connecticut, northern Georgia, extreme southern Illinois, extreme southern Indiana, the entirety of Kentucky, extreme northeastern Louisiana, southeastern Massachusetts, northeastern Mississippi, extreme southeastern Missouri, extreme southern New York, most of North Carolina, southern Ohio, southern Rhode Island, northern South Carolina, most of Tennessee, most of Virginia, and all of West Virginia. Freezing rain advisories covered parts of North and South Carolina. Further winter weather advisories covered additional portions of the country, including eastern Kansas, southeastern Missouri, and northern Alabama. Offshore, storm warnings covered areas from Georgia to Maine.\n\nOn January 21–22, the governors of Delaware, Georgia, Kentucky, Maryland, New York, New Jersey, North Carolina, Virginia, Pennsylvania, Tennessee, West Virginia, and the mayor of Washington, D.C. declared a state of emergency in anticipation of significant snowfall and blizzard conditions.\n\nAirlines cancelled more than 1,000 flights, with hundreds more preemptively grounded, by the afternoon of January 21 for January 22–24. Ripple effect cancellations spread across the entire East Coast. By the afternoon of January 23, more than 10,100 flights were cancelled across the country, affecting well over 100,000 travelers. Nearly 2,000 more flights were delayed. Most airports in the Mid-Atlantic region suspended service altogether, with Baltimore–Washington International, Philadelphia International Airport, Ronald Reagan Washington National, and Washington Dulles International closed through the evening of January 24. Flights to and from LaGuardia, John F. Kennedy International, and Newark Liberty International were largely cancelled as well. American Airlines suspended all flights departing from Charlotte-Douglas International Airport in North Carolina, causing a ripple of flights being cancelled throughout the country. Effects rippled internationally, with more than 100 flights in Canada, Mexico, and the United Kingdom cancelled. Altogether, 13,046 flights were cancelled between January 22 and 26. Amtrak suspended service for many lines, including the service from New York City to New Orleans; the to Chicago; and the to Miami.\n\nMultiple sporting events, including those held by the Atlantic Coast Conference, National Basketball Association, and the National Hockey League, were postponed by the storm, while the National Football League's Arizona Cardinals vs. Carolina Panthers NFC Championship went on as planned for January 24 in Charlotte, North Carolina.\n\nA snow emergency was declared for Washington, D.C., meaning that residents would not be allowed to park on snow emergency routes after 9:30 p.m. local time on January 22. Mayor Bowser urged people to remain home during the storm, \"[u]nless you absolutely have to be out tomorrow afternoon [January 22], residents should get home as soon as possible\". Across Maryland, 2,700 pieces of snow equipment were mobilized and crews planned to distribute 365,000 tons of road salt. In Virginia, 500 vehicles were deployed to treat roads and 500 members of the Virginia National Guard were placed on standby. Schools across the D.C. area were scheduled to end classes early on January 22, before the storm's arrival. Stores across Maryland, Virginia, and Washington, D.C. reported a substantial uptick in sales, with groceries, heaters, shovels, and similar items sold out in numerous locations. West Virginia Governor Earl Ray Tomblin mobilized the state's National Guard on January 21.\nThe Washington Metropolitan Area Transit Authority (WMATA) announced on January 21 that it would shut down its entire mass transit system over the weekend of January 22 and 23, including the Washington Metro and Metrobus, making it the longest such shutdown in the agency's history. On January 25, the WMATA operated limited Metro bus service, as well as rail service only on the underground portions of the Red, Green, and Orange Lines; however, fares were not charged. In Richmond, Virginia, all flights out of Richmond International Airport on January 23 were cancelled, and the Greater Richmond Transit Company (GTRC) bus system took the rare step of suspending all routes on January 24.\n\nThe Pennsylvania Department of Transportation had 2,200 vehicles and more than 733,000 tons of road salt on standby for use. Trucks deployed brine across major roads in and around Philadelphia, though residents were advised to avoid travel unless necessary. A travel ban was declared for Lancaster, Lebanon, Newberry Township, and York on January 23. The Southeastern Pennsylvania Transportation Authority issued a near complete shutdown of its services, shutting down all bus services and closing all rail except for the Broad Street Line and Market-Frankford Line subways.\n\nNew York City mayor Bill de Blasio declared a hazardous travel advisory for the city, encouraging people not to travel; however, he did not ban traveling. On January 22, he declared a \"winter weather emergency\" and told residents to \"Get done what you have to get done today ... Do not bring your vehicle out tomorrow\". Taking place a year after a storm prompted the closure of the city's subway system in January 2015, only to largely bypass the city, Governor Andrew Cuomo stated that services would remain running. Approximately 1,800 workers equipped with 800 heaters were to keep rails clear for use. Thousands of sanitation workers, 1,700 plows, and 150,000 tons of road salt were on standby to clear city roads. He also put 600 members of the New York National Guard on standby. More than 50 power workers from Vermont were dispatched to Long Island to help restore power outages. Around noon on January 23, owing to a significant increase in expected snowfall, Cuomo issued a travel ban for all roads in New York City and Long Island. The New York City Transit Authority suspended bus service; rail service on the Long Island Rail Road, Metro-North, and Staten Island Railway; and elevated subway service (with underground subway lines remaining open until further notice). In the meantime, the Port Authority of New York and New Jersey closed bridges and tunnels in the region.\n\nA travel ban was instituted for Newark, New Jersey, on January 23 through the afternoon of January 24 in light of hundreds of snow-related accidents.\n\nStrong winds coupled with prolonged onshore flow resulted in a major coastal flood threat for Delaware and New Jersey. Near-shore waves were forecast to reach with a storm surge of . In New Jersey, a mandatory evacuation was ordered for residents in coastal Barnegat Township in anticipation of significant coastal flooding; several other towns were placed under voluntary evacuation orders.\n\nDelaware Governor Jack Markell declared a \"level 1 driving warning\", encouraging people not to travel and indicating drivers should be extra cautious. The Delaware Department of Transportation had 330 snow plows ready to clear roads, though many areas were expected to be impassible on January 23. Shelters were also opened for the homeless. In New Castle County and Kent County in Delaware the Department of Transportation and Governor Jack Markell declared a Level 2 driving restriction (essential personnel on the roads only.)\n\nAnticipating a damaging ice storm, approximately 4,500 linemen were placed on standby to repair downed power lines in North Carolina; 1,000 state transportation workers also prepared for heavy snowfall, with crews placed on 12-hour shifts to be deployed as needed. Crews from across the country arrived to assist North Carolina power companies. Two million gallons of brine were used to pre-treat roads statewide. Across Tennessee, state offices closed for January 22, warming centers opened, and the Red Cross placed shelters on standby. The Tennessee Highway Patrol asked for people to remain off the roads, saying, \"We are desperately asking you please DON'T DRIVE\".\n\nThe storm was forecast to cross the Atlantic Ocean and affect the British Isles from January 26 to 28. The storm was expected to be less severe, with rain rather than snow; however, the possibility of strong winds and localized flooding was noted. Particular concern was raised over areas that suffered from significant flooding during the preceding months. Wind gusts as high as were forecast for the Hebrides and for coastal Scotland. The storm in the British Isles was far less severe than in the United States and Storm Gertrude a few days later may have been worse.\n\nThe storm's widespread effects paralyzed travel across the eastern United States as it produced more than of snow across a large area along the Appalachian Mountains. At least 55 fatalities have been attributed to the storm and its aftermath: 12 in Virginia, 9 in Pennsylvania, 6 in New Jersey, 6 in New York, 6 in North Carolina, 4 in South Carolina, 3 in Maryland, 3 in Washington, D.C., 1 in Arkansas, 1 in Delaware, 1 in Georgia, 1 in Kentucky, 1 in Massachusetts, and 1 in Ohio. Throughout the affected region, more than 631,000 people lost power: 270,000 in New Jersey, 147,000 in North Carolina, 66,000 in Georgia, 55,000 in Virginia, 47,000 collectively in Delaware and Maryland, and nearly 30,000 in South Carolina.\n\nEconomic losses—from lost sales revenue and wages—are estimated between $500 million and $3 billion. Moody's Analytics indicated the highest losses, stating $2.5–3 billion; however, the storm's occurrence on a weekend accounted for less losses than what would otherwise be expected. Planalytics placed losses at $850 million and IHS Global Insight estimated losses between $500 million and $1 billion. Although an estimate was not provided, AON Benfield placed losses in the billions of dollars, noting similarities to the Blizzard of 1996 which inflicted $4.6 billion in economic losses. Despite the expected major losses, the rush to buy supplies ahead of the storm's arrival may mitigate the overall impact. The airline industry suffered approximately $200 million in lost revenue.\n\nUsing the Regional Snowfall Index, the storm ranked as a Category 5, \"extreme\", storm for the Northeast and a Category 4, \"crippling\", for the Southeast. It was also ranked as a Category 3, \"major\", event in the Ohio Valley. Approximately 103 million people were in the storm's path, including 33 million in the expected blizzard area. About 21 million people in the Northeast experienced more than of snow. Half of the affected people were in the Northeast (which includes the Northeast megalopolis); the storm's RSI reached 20.138 in this region, the fourth-highest on record for the region. The primary factor driving its high classification was the affected population. The storm's RSI of 13.776 in the Southeast was the twelfth-highest on record.\n\nDuring January 21 through the early hours of January 22, severe thunderstorms brought damaging winds and hail to portions of Louisiana, Mississippi, Alabama, and Florida. Five tornadoes touched down across Mississippi: an EF0 near Crystal Springs, an EF0 near Homewood, an EF1 near Loyd Star, an EF1 near Pinola, and an EF2 near Sumrall. Numerous trees and power lines were downed, multiple structures were damaged, and a few were destroyed by the tornadoes. Straight line winds near Improve, Lamar County, Mississippi, significantly damaged 10–12 homes. Hail reached in diameter in Wilmer, Louisiana. The most significant damage occurred overnight across the Florida Panhandle and neighboring Alabama. Winds gusting to downed numerous trees and power lines and damaged structures.\n\nSnow fell across a large portion of Arkansas on January 21–22, with a daily record of observed just outside Little Rock. Snowfall was confined to the eastern half of the state, with freezing rain amounting to observed in northeastern counties. Strong winds in excess of —with a gust of measured in Jonesboro—left more than 16,000 people in the state without power. One fatality near Hoxie was due to slippery roads.\n\nEarly on January 22, heavy snow fell across parts of western Tennessee. Roads around Nashville quickly became impassable, including portions of Interstates 40 and 24, and local police reported more than 200 accidents. In Lexington, Kentucky, one accident led to injury; 17 other accidents were reported. Multiple accidents in Rockcastle County prompted the closure of of I-75 in Kentucky. Along a stretch of Interstate 75 in Kentucky, hundreds of drivers became stranded for more than 16 hours on January 22–23. In a separate incident, a stretch of Interstate 77 in West Virginia was blocked by stuck tractor-trailers that were lying across the highway. The National Guard was deployed to provide people with food, water, and fuel. A transportation worker died when his truck skidded off icy roads.\n\nTreacherous road conditions in North Carolina resulted in at least 571 accidents and 800 \"service calls\" from police. Five deaths were reported across the state. One person died in a collision on Interstate 95. Numerous roads were shut down accordingly. One person was killed and three others were injured in Forsyth County. An accident along Interstate 77 near Troutman resulted in the death of a 4-year-old boy. Freezing rain resulted in widespread power outages, bringing down numerous trees and power lines in the state. Many roads closed because of debris, including portions of Interstate 40 in Johnston County. Approximately 147,000 people lost power in North Carolina, with Wake County accounting for 50,000 of the total.\n\nFour people died in South Carolina: two from carbon monoxide poisoning in Greenville, one in a car accident on an icy road in Greenville County, and another from an accident in Jonesville. Nearly 30,000 people lost power in the state. Portions of Interstates 26 and 95 were temporarily shut down for icy conditions.\n\nUpwards of of snow fell across Georgia, with the highest totals confined to northern parts of the state. High winds downed trees and power lines, leaving approximately 66,000 people without power across the state. A postal worker was killed when strong winds blew a large branch off a tree, crushing him in his car. Snow fell as far south as Mobile, Alabama, with additional flurries extending into Jacksonville and Gainesville along the Florida Panhandle late on January 22. Temperatures in Gainesville fell to .\n\nMaximum snow depth was measured at in Glengary, West Virginia, on January 24. Baltimore, Maryland, recorded its largest snowfall on record. Two people died from heart attacks while shoveling snow and a third from undetermined causes across Maryland. A large portion of the Ocean City fishing pier was destroyed by rough seas and high winds. West Virginia's emergency management reported Interstate 77 to be \"completely shut down\" following an accident involving semi-trailers. The National Guard was called in to assist clearing the stranded vehicles. Six people died from snow-related incidents in Virginia. Virginia State Police responded to 989 accidents and 793 disabled vehicles through the evening of January 22. A total of 12 people died in storm-related incidents across Virginia. One person died when their car skid off a road in Chesapeake and collided with a tree. Five people died from hypothermia: one each in Charles City, Gloucester County, Hampton, Henry County, and Wise County. A combination of snow and ice accumulation caused the roof of Donk's Theatre in Hudgins, Virginia, to collapse; the structure was deemed a total loss and will be demolished. The roof of a 4,700 ft (437 m) building collapsed in Charlottesville. Seven people required hospitalization for carbon monoxide poisoning at an apartment complex in Herndon when vents became clogged with snow. Three people died while shoveling snow in Washington, D.C.\nSnow-related incidents resulted in nine deaths across Pennsylvania. In Harrisburg, snowfall from the storm was reported as , breaking the city's previous record snowfall of which was set in February 1983. Near Bedford, approximately 500 vehicles became stuck along a westbound stretch of the Pennsylvania Turnpike for over 24 hours from January 22–23 near the eastern approach to the Allegheny Mountain Tunnel. Among the stranded vehicles were one bus carrying the Duquesne University men's basketball team and another carrying Temple University's women's gymnastics team. Despite the scale of the incident, no major injuries were reported. The band Guster, stranded in Pittsburgh by the blocking of the turnpike, held an impromptu concert in an alley. Four people died while shoveling snow: two in both Lancaster and Montgomery counties. One of the dead in Montgomery County was a woman who was 8-months pregnant; the baby was declared dead on-scene when paramedics arrived. Another man died from carbon monoxide poisoning in his car in Reading after a passing snow plow buried his car in snow.\nOne person died in Magnolia, Delaware, after suffering a heart attack while shoveling. A power outage at the Delaware City Refinery, thought to be related to the storm, forced the facility to shut down after chemicals were released. A mother and her 1-year-old son died from carbon monoxide poisoning in Passaic, New Jersey, after snowfall blocked their vehicle's tailpipe; a 3-year-old girl was hospitalized, but later died on January 27. Three other people died while attempting to walk home during the blizzard: one each in East Greenwich, Hackensack, and Mahwah.\n\nSnowfall across New York City and Long Island was more intense than initially forecast, falling at rates of per hour at times. Before the travel ban was implemented, buses struggled to make their routes and long delays were common. At Central Park, a storm-total accumulation of was observed, the highest total on record for the city since observations began in 1869. This surpassed the previous record of measured during the February 2006 blizzard. Accumulations reached an all-time record high of at John F. Kennedy International Airport. Police across New York City responded to more than 200 accidents and 300 disabled vehicles. Emergency personnel responded to about 3,000 9-1-1 calls across the city. Five people died while shoveling snow: two in Queens, two in unknown parts of the city, and one in Staten Island. At least two deaths in Long Island were from shoveling snow. One person was killed by a snow plow in Oyster Bay Cove on Long Island.\n\nSignificant coastal flooding took place in Delaware and New Jersey. The first, and most severe, round of flooding took place during the morning of January 23. A second round took place after the storm's passage on the morning of January 24, concurrent with high tide. Record tides occurred in Lewes, Delaware. Approximately of dunes along Delaware's oceanfront coast sustained significant damage, with many areas flattened. Forty people required evacuation in Long Neck and Oak Orchard. Delaware Route 1 was shut down between Bethany Beach and Dewey Beach for flooding.\n\nTides at Cape May, New Jersey, reached a record , surpassing the previous highest of set during Hurricane Sandy in 2012. Fifty people required evacuation in Atlantic City, and at least 150 homes were flooded. Strong winds accompanying the flooding caused damage to many homes. Streets in Ocean City and Stone Harbor were inundated with several feet of water; according to a resident in Stone Harbor, water reached in depth on some streets. Effects were most severe along the Delaware Bay. Combined with snowfall and freezing temperatures, the flooded roads became a mix of ice and slush. Long Beach Island, still recovering from the effects of Hurricane Sandy, sustained severe beach erosion. The storm's tide and waves carved cliffs along coastal beaches. A recently constructed dune in Belmar held back most of the water, with one minor breach along its expanse.\n\nBlizzard conditions affected southeastern Massachusetts for several hours, with Nantucket and Martha's Vineyard reporting such conditions for four hours. Wind gusts on Block Island, Rhode Island, reached . One person died after being struck by a snow plow.\n\nAfter leaving the Eastern United States, the storm complex brought light snowfall to coastal areas of Nova Scotia, Canada, on January 24. Environment Canada indicated accumulations up to alongside wind gusts of . Residents, however, reported up to of snow in Shelburne County. Street parking was banned in Halifax on select streets to enable easier clearing of roads. On January 25, a Boeing 767 traveling from Miami, Florida, to Milan, Italy, encountered severe turbulence produced by the storm about off Newfoundland. Of the 203 people on board, including crew, seven were injured. The aircraft made an emergency landing at St. John's International Airport.\n\nMany cities struggled to cope with the snowfall, including Washington, D.C., where schools and government offices remained closed through January 26, three days after the storm. Schools in Baltimore, Maryland, and Philadelphia, Pennsylvania, also remained closed through January 26. Side streets around the D.C. area were unplowed through January 26. Through January 27, Washington, D.C. police issued $1,078,000 worth of parking tickets and $65,600 in fines for vehicles parked or abandoned on snow emergency routes. At least 656 vehicles were towed.\n\nThe \"New York Post\" described Queens, New York, as \"basically forgotten\" on January 25 as roads remained covered with snow. Many residents complained that snowplows did not come through their area until at least a day after the storm passed. Crews expressed that certain areas, such as in the Bronx, were difficult to navigate with snow plows and there was no place to put the snow. A resident of Pelham Bay stated that 3-1-1 operators were not picking up calls. Mayor De Blasio toured areas of Staten Island on January 24 and urged people to be patient. In contrast, both Governor Cuomo and Mayor De Blasio were praised for their handling of the storm prior to its arrival, and for being the first to issue a travel ban, which is credited for potentially saving lives.\n\nNew Jersey Governor Chris Christie, who flew back to the state from New Hampshire during his presidential campaign, left to resume campaigning shortly after the storm passed. He was criticized for his quick departure and heated responses to questions about why he left the state. He stated that while there was some locally significant flood damage in Southern New Jersey, the overall effects across the state did not warrant his presence. He responded to one person by saying, \"I don’t know what you expect me to do. You want me to go down there with a mop?\" A Stockton University graduate started up a fund to provide 1,000 mops to Christie in response to his comment. On January 26, Christie apologized to North Wildwood Mayor Patrick Rosenello for calling him \"crazy\" during his response to criticism the day prior. Christie also requested that Rosenello apologize on his behalf to first responders, residents, and business owners.\n\nIn March, President Barack Obama declared Delaware, Maryland, New Jersey, Pennsylvania, Virginia, and Washington, D.C. major disaster areas. The federal funding would only cover costs incurred by public infrastructure, debris removal, and emergency measures to ensure public safety. Preliminary estimates places the total cost of assistance at $168,334,023, with New Jersey accounting for $82,663,604.\n\nFollowing the storm, the National Weather Service conducted a review of eight snowfall observations following concerns over their quality. Measurements from Ronald Reagan Washington National Airport, Dulles International Airport, Baltimore/Washington International Airport, John F. Kennedy International Airport, LaGuardia International Airport, and Philadelphia International Airport were verified as reliable. The total at Ronald Reagan Washington National Airport was brought into question immediately following the blizzard as the snow observers lost their measuring board during the storm; however, the assessment determined that proper procedure was continued even after the board was lost and the total was considered accurate. A review of measurements at Central Park, New York, revealed the observation was the result of a communication error and revised the storm-total to —an all-time record for New York City. Furthermore, the measurement at Newark International Airport, New Jersey, was invalidated due to measurements being taken every hour instead of every six hours thus inflating the total. The State Climate Extremes Committee also conducted a review of a potential 24-hour state snowfall record at Mount Mitchell in North Carolina. Their results showed the 24-hour observation of to be in error, erroneously inflating the snowfall totals. Properly adjusting for liquid snow ratios, the total was revised to . The storm-total accumulation was also revised from to .\n\nThe storm has received several nicknames from various media outlets. The name \"Winter Storm Jonas\" was created by The Weather Channel at the beginning of the winter storm season, and assigned to the storm system when it was forecast; it has also been used in international media, and was used by New York City Mayor Bill de Blasio and the United States Postal Service. Reception of the naming of storms has been mixed; \"The New York Times\" refers to the name \"Jonas\" as a marketing ploy while The Weather Channel maintains that it helps others prepare in advance. \"The Washington Post\" named the storm Snowzilla, referencing the 2014–16 El Niño event which has been named the \"Godzilla El Niño\". Connecticut-based WFSB named the system \"Anna\", after former First Lady Anna Harrison. Other sources have dubbed the storm simply \"Blizzard of 2016\" or \"Blizzard 2016\".\n\n\n"}
{"id": "27071123", "url": "https://en.wikipedia.org/wiki?curid=27071123", "title": "Lamé's stress ellipsoid", "text": "Lamé's stress ellipsoid\n\nLamé's stress ellipsoid is an alternative to Mohr's circle for the graphical representation of the stress state at a point. The surface of the ellipsoid represents the locus of the endpoints of all stress vectors acting on all planes passing through a given point in the continuum body. In other words, the endpoints of all stress vectors at a given point in the continuum body lie on the stress ellipsoid surface, i.e., the radius-vector from the center of the ellipsoid, located at the material point in consideration, to a point on the surface of the ellipsoid is equal to the stress vector on some plane passing through the point. In two dimensions, the surface is represented by an ellipse.\n\nOnce the equations of the ellipsoid is known, the magnitude of the stress vector can then be obtained for any plane passing through that point.\n\nTo determine the equation of the stress ellipsoid we consider the coordinate axes formula_1 taken in the directions of the principal axes, i.e., in a principal stress space. Thus, the coordinates of the stress vector formula_2 on a plane with normal unit vector formula_3 passing through a given point formula_4 is represented by\n\nAnd knowing that formula_3 is a unit vector we have\n"}
{"id": "38401256", "url": "https://en.wikipedia.org/wiki?curid=38401256", "title": "Land grant to Marduk-zākir-šumi kudurru", "text": "Land grant to Marduk-zākir-šumi kudurru\n\nThe Land grant to Marduk-zākir-šumi kudurru is an ancient Mesopotamian \"narû\", or entitlement stele, recording the gift (\"irīmšu\") of 18 \"bur\" 2 \"eše\" (about 120 hectares or 300 acres) of corn-land by Kassite king of Babylon Marduk-apla-iddina I (ca. 1171–1159 BC) to his \"bēl pīḫati\" (inscribed and meaning \"person responsible\"), or a provincial official. The monument is significant in part because it shows the continuation of royal patronage in Babylonia during a period when most of the near East was beset by collapse and confusion, and in part due to the lengthy genealogy of the beneficiary, which links him to his illustrious ancestors.\n\nThe monument is a large rectangular block of limestone with a base of 51 by 30.5 cm and a height of 91 cm, or around 3 foot, with a broken top making it the tallest of the extant kudurrus and has intentionally flattened sides. It was recovered from the western bank of the Tigris opposite Baghdad and acquired by George Smith for the British Museum while on his 1873–74 expedition to Nineveh sponsored by the Daily Telegraph. It was originally given the collection reference D.T. 273 and later that of BM 90850. The face has three registers featuring eighteen symbolic representations of gods (listed below identifying the corresponding deity) and the back has three columns of text (line-art pictured right).\nFirst register:\n\nSecond register:\n\nThird register:\nThe land grant was situated west of the river Tigris in the province of Ingur-Ištar, one of perhaps twenty-two \"pīḫatus\" or provinces known from the Kassite period, and was bordered by estates belonging to the (house of) \"Bīt-Nazi-Marduk\" and \"Bīt-Tunamissaḫ\", perhaps Kassite nobility.\n\n\nHis ancestors:\n\n\nWitnesses:\n\n\n"}
{"id": "39721914", "url": "https://en.wikipedia.org/wiki?curid=39721914", "title": "Leslie Lawrance Foldy", "text": "Leslie Lawrance Foldy\n\nLeslie Lawrance Foldy (1919–2001) was a theoretical physicist, who made contributions to condensed matter physics and quantum mechanics.\n\nFoldy was born in Sabinov, Czechoslovakia, on October 26, 1919. His parents were of Hungarian origin, and named their son László Földi. In 1920, the family moved to the USA due to the invasions of Czechoslovakia by Hungary around that time, and there he was known as Leslie Foldy.\n\nHis high school education was in Cleveland, Ohio, and at that time he called his middle name \"Lawrance\", upon noticing he hadn't a middle name while the other students had. While at high school, he picked up an interest in physics. In 1941, Foldy graduated with a B.S. degree in physics from the Case School of Applied Science (now renamed to the \"Case School of Engineering\"), his senior thesis was on crystal lattice vibrations.\n\nHe started his PHD in 1945 at the University of California in Berkeley, in J. Robert Oppenheimer's group, and with Luis Alvarez and David Bohm, until 1947. In 1948 he completed his doctoral degree at the Institute for Advanced Study in Princeton, and later returned to the Case Institute of Technology as an assistant professor of physics.\n\nIn 1949 at the University of Rochester, Foldy and Siegfried Adolf Wouthuysen, working with Robert Marshak, wrote a preprint on the nonrelativistic limit of the Dirac equation. In 1950 at Michigan, their work was criticized as erroneous, motivating Foldy and Wouthuysen to make their calculations clear and efficient. A transformation forming part of their calculations is now called the Foldy–Wouthuysen transformation.\n\nPositions he held at the Institute for Advanced Study include:\n\n\n\n"}
{"id": "18487921", "url": "https://en.wikipedia.org/wiki?curid=18487921", "title": "Martin Vosseler", "text": "Martin Vosseler\n\nMartin Vosseler is a co-founder of the organisation Physicians for Social Responsibility, who has been a renewable energy advocate since 1981. After giving up his medical practice in 1995, he began working full-time to raise awareness of the benefits of renewable energy use, by travelling around the world. From 16 October 2006 to 8 May 2007 Dr Vosseler and his crew made history by completing the first trans-atlantic crossing in a motorized boat, using solar power only. Dr Vosseler has received a special prize from Eurosolar.\n\nWebsite of Dr. Martin Vosseler\n"}
{"id": "23093701", "url": "https://en.wikipedia.org/wiki?curid=23093701", "title": "Mascus", "text": "Mascus\n\nMascus is an electronic marketplace for used machinery and related spares and accessories around the world. Product ranges include tractors for sale, trailers for sale, used trucks for sale, diggers, box trailers, mini diggers and excavators, etc. The company was founded in 2000 and is owned by Mascus International BV Netherlands.\n\nIn 2016 Mascus was acquired by Ritchie Bros.\n\nMascus operates in 58 countries globally, covering the whole of Europe, Australasia, USA, Canada, Africa and Asia.\nBesides the online marketplace, Mascus offers:\n\n\n"}
{"id": "51267644", "url": "https://en.wikipedia.org/wiki?curid=51267644", "title": "Meal", "text": "Meal\n\nA meal is an eating occasion that takes place at a certain time and includes prepared food. The names used for specific meals in English vary greatly, depending on the speaker's culture, the time of day, or the size of the meal.\n\nMeals occur primarily at homes, restaurants, and cafeterias, but may occur anywhere. Regular meals occur on a daily basis, typically several times a day. Special meals are usually held in conjunction with such occasions as birthdays, weddings, anniversaries, events and holidays. A meal is different from a snack in that meals are generally larger, more varied, and more filling than snacks.\n\nThe type of meal served or eaten at any given time varies by custom and location. In most modern cultures, three main meals are eaten: in the morning, early afternoon, and evening. Further, the names of meals are often interchangeable by custom as well. Some serve dinner as the main meal at midday, with supper as the late afternoon/early evening meal; while others may call their midday meal lunch and their early evening meal supper. Except for \"breakfast\", these names can vary from region to region or even from family to family.\n\nA study in 2016 by Toluna found that 47% of parents in the United States share fewer meals with their families than when growing up, and 58% wished they could do it more frequently, including 66% of dads.\n\nBreakfast is the first meal of a day, most often eaten in the early morning before undertaking the day's work. Some believe it to be the most important meal of the day. The word \"breakfast\" literally refers to breaking the fasting period of the prior night.\n\nBreakfast foods vary widely from place to place, but often include a carbohydrate such as grains or cereals, fruit, vegetables, a protein food such as eggs, meat or fish, and a beverage such as tea, coffee, milk, or fruit juice. Coffee, milk, tea, juice, breakfast cereals, pancakes, waffles, sausages, French toast, bacon, , fresh fruits, vegetables, eggs, baked beans, muffins, crumpets and toast with butter, margarine, jam or marmalade are common examples of Western breakfast foods, though a large range of preparations and ingredients are associated with breakfast globally.\n\nA full breakfast is a breakfast meal, usually including bacon, sausages, eggs, and a variety of other cooked foods, with a beverage such as coffee or tea. It is especially popular in the UK and Ireland, to the extent that many cafés and pubs offer the meal at any time of day as an \"all-day breakfast\". It is also popular in other English-speaking countries.\n\nIn England it is usually referred to as a 'full English breakfast' (often shortened to 'full English') or 'fry-up'. Other regional names and variants include the 'full Scottish', 'full Welsh', 'full Irish' and the 'Ulster fry'.\n\nThe full breakfast is among the most internationally recognised British dishes, along with such staples as bangers & mash, shepherd's pie, fish and chips and the Christmas dinner. The full breakfast became popular in the British Isles during the Victorian era, and appeared as one among many suggested breakfasts in the home economist Isabella Beeton's \"The Book of Household Management\" (1861). A full breakfast is often contrasted (e.g. on hotel menus) with the lighter alternative of a Continental breakfast, traditionally consisting of tea, milk or coffee and fruit juices with bread, croissants, or pastries.\n\n\"Instant breakfast\" typically refers to breakfast food products that are manufactured in a powdered form, which are generally prepared with the addition of milk and then consumed as a beverage. Some instant breakfasts are produced and marketed in liquid form, being pre-mixed. The target market for instant breakfast products includes consumers who tend to be busy, such as working adults.\n\nA champagne breakfast is a breakfast served with champagne or sparkling wine. It is a new concept in some countries and is not typical of the role of a breakfast.\n\nIt may be part of any day or outing considered particularly luxurious or indulgent. The accompanying breakfast is sometimes of a similarly high standard and include rich foods such as salmon, caviar, chocolate or pastries, which would not ordinarily be eaten at breakfast or more courses. \nInstead of as a formal meal the breakfast can be given to the recipient in a basket or hamper.\n\nLunch, the abbreviation for \"luncheon\", is a light meal typically eaten at midday. The origin of the words \"lunch\" and \"luncheon\" relate to a small snack originally eaten at any time of the day or night. During the 20th century the meaning gradually narrowed to a small or mid-sized meal eaten at midday. Lunch is commonly the second meal of the day after breakfast. The meal varies in size depending on the culture, and significant variations exist in different areas of the world.\n\nA packed lunch (also called pack lunch, sack lunch or bag lunch in North America, or packed lunch in the United Kingdom, as well as the regional variations: bagging in Lancashire, Merseyside and Yorkshire,) is a lunch prepared at home and carried to be eaten somewhere else, such as school, a workplace, or at an outing. The food is usually wrapped in plastic, aluminum foil, or paper and can be carried (\"packed\") in a lunch box, paper bag (a \"sack\"), or plastic bag. While packed lunches are usually taken from home by the people who are going to eat them, in Mumbai, India, tiffin boxes are most often picked up from the home and brought to workplaces later in the day by so-called dabbawallas. It is also possible to buy packed lunches from stores in several countries. Lunch boxes made out of metal, plastic or vinyl are now popular with today's youth. Lunch boxes provide a way to take heavier lunches in a sturdier box or bag. It is also environmentally friendly.\n\nDinner usually refers to the most significant and important meal of the day, which can be the noon or the evening meal. However, the term \"dinner\" can have many different meanings depending on the culture; it may mean a meal of any size eaten at any time of the day. Historically, it referred to the first meal of the day, eaten around noon, and is still sometimes used for a noon-time meal, particularly if it is a large or main meal. The meaning as the evening meal, generally the largest of the day, is becoming a standard in many parts of the English-speaking world.\n\nA full course dinner is a dinner consisting of multiple dishes, or courses. In its simplest form, it can consist of three to five courses, such as appetizers, fish course, entrée, main course and dessert.\n\nMeal preparation, sometimes called \"meal prep,\" is the process of planning and preparing meals. It generally involves food preparation, including cooking.\n\nPreparing food for eating generally requires selection, measurement and combination of ingredients in an ordered procedure so as to achieve desired results. Food preparation includes but is not limited to cooking.\n\nCooking or \"cookery\" is the art, technology and craft of preparing food for consumption with the use of heat. Cooking techniques and ingredients vary widely across the world, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting unique environmental, economic, and cultural traditions and trends. The ways or types of cooking also depend on the skill and type of training an individual cook has. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments. Cooking can also occur through chemical reactions without the presence of heat, most notably with ceviche, a traditional South American dish where fish is cooked with the acids in lemon or lime juice.\n\n\n\n"}
{"id": "21061", "url": "https://en.wikipedia.org/wiki?curid=21061", "title": "Mica", "text": "Mica\n\nThe mica group of sheet silicate (phyllosilicate) minerals includes several closely related materials having nearly perfect basal cleavage. All are monoclinic, with a tendency towards pseudohexagonal crystals, and are similar in chemical composition. The nearly perfect cleavage, which is the most prominent characteristic of mica, is explained by the hexagonal sheet-like arrangement of its atoms.\n\nThe word \"mica\" is derived from the Latin word ', meaning \"a crumb\", and probably influenced by ', to glitter.\n\nChemically, micas can be given the general formula\nin which\nStructurally, micas can be classed as dioctahedral (\"Y\" = 4) and trioctahedral (\"Y\" = 6). If the \"X\" ion is K or Na, the mica is a \"common\" mica, whereas if the \"X\" ion is Ca, the mica is classed as a \"brittle\" mica.\n\n\nCommon micas:\n\nBrittle micas:\n\nVery fine-grained micas, which typically show more variation in ion and water content, are informally termed \"clay micas\". They include:\n\nMica is widely distributed and occurs in igneous, metamorphic and sedimentary regimes. Large crystals of mica used for various applications are typically mined from granitic pegmatites.\n\nUntil the 19th century, large crystals of mica were quite rare and expensive as a result of the limited supply in Europe. However, their price dramatically dropped when large reserves were found and mined in Africa and South America during the early 19th century. The largest documented single crystal of mica (phlogopite) was found in Lacey Mine, Ontario, Canada; it measured and weighed about . Similar-sized crystals were also found in Karelia, Russia.\n\nThe British Geological Survey reported that as of 2005, Koderma district in Jharkhand state in India had the largest deposits of mica in the world. China was the top producer of mica with almost a third of the global share, closely followed by the US, South Korea and Canada. Large deposits of sheet mica were mined in New England from the 19th century to the 1970s. Large mines existed in Connecticut, New Hampshire, and Maine.\n\nScrap and flake mica is produced all over the world. In 2010, the major producers were Russia (100,000 tonnes), Finland (68,000 t), United States (53,000 t), South Korea (50,000 t), France (20,000 t) and Canada (15,000 t). The total global production was 350,000 t, although no reliable data were available for China. Most sheet mica was produced in India (3,500 t) and Russia (1,500 t). Flake mica comes from several sources: the metamorphic rock called schist as a byproduct of processing feldspar and kaolin resources, from placer deposits, and from pegmatites. Sheet mica is considerably less abundant than flake and scrap mica, and is occasionally recovered from mining scrap and flake mica. The most important sources of sheet mica are pegmatite deposits. Sheet mica prices vary with grade and can range from less than $1 per kilogram for low-quality mica to more than $2,000 per kilogram for the highest quality.\n\nThe mica group represents 37 phyllosilicate minerals that have a layered or platy texture. The commercially important micas are muscovite and phlogopite, which are used in a variety of applications. Mica’s value is based on several of its unique physical properties. The crystalline structure of mica forms layers that can be split or delaminated into thin sheets usually causing foliation in rocks. These sheets are chemically inert, dielectric, elastic, flexible, hydrophilic, insulating, lightweight, platy, reflective, refractive, resilient, and range in opacity from transparent to opaque. Mica is stable when exposed to electricity, light, moisture, and extreme temperatures. It has superior electrical properties as an insulator and as a dielectric, and can support an electrostatic field while dissipating minimal energy in the form of heat; it can be split very thin (0.025 to 0.125 millimeters or thinner) while maintaining its electrical properties, has a high dielectric breakdown, is thermally stable to , and is resistant to corona discharge. Muscovite, the principal mica used by the electrical industry, is used in capacitors that are ideal for high frequency and radio frequency. Phlogopite mica remains stable at higher temperatures (to ) and is used in applications in which a combination of high-heat stability and electrical properties is required. Muscovite and phlogopite are used in sheet and ground forms.\n\nThe leading use of dry-ground mica in the US is in the joint compound for filling and finishing seams and blemishes in gypsum wallboard (drywall). The mica acts as a filler and extender, provides a smooth consistency, improves the workability of the compound, and provides resistance to cracking. In 2008, joint compound accounted for 54% of dry-ground mica consumption. In the paint industry, ground mica is used as a pigment extender that also facilitates suspension, reduces chalking, prevents shrinking and shearing of the paint film increases the resistance of the paint film to water penetration and weathering and brightens the tone of colored pigments. Mica also promotes paint adhesion in aqueous and oleoresinous formulations. Consumption of dry-ground mica in paint, the second-ranked use, accounted for 22% of the dry-ground mica used in 2008.\n\nGround mica is used in the well-drilling industry as an additive to drilling fluids. The coarsely ground mica flakes help prevent the loss of circulation by sealing porous sections of the drill hole. Well drilling muds accounted for 15% of dry-ground mica use in 2008. The plastics industry used dry-ground mica as an extender and filler, especially in parts for automobiles as lightweight insulation to suppress sound and vibration. Mica is used in plastic automobile fascia and fenders as a reinforcing material, providing improved mechanical properties and increased dimensional stability, stiffness, and strength. Mica-reinforced plastics also have high-heat dimensional stability, reduced warpage, and the best surface properties of any filled plastic composite. In 2008, consumption of dry-ground mica in plastic applications accounted for 2% of the market. The rubber industry used ground mica as an inert filler and mold release compound in the manufacture of molded rubber products such as tires and roofing. The platy texture acts as an anti-blocking, anti-sticking agent. Rubber mold lubricant accounted for 1.5% of the dry-ground mica used in 2008. As a rubber additive, mica reduces gas permeation and improves resiliency.\n\nDry-ground mica is used in the production of rolled roofing and asphalt shingles, where it serves as a surface coating to prevent sticking of adjacent surfaces. The coating is not absorbed by freshly manufactured roofing because mica’s platy structure is unaffected by the acid in asphalt or by weather conditions. Mica is used in decorative coatings on wallpaper, concrete, stucco, and tile surfaces. It also is used as an ingredient in flux coatings on welding rods, in some special greases, and as coatings for core and mold release compounds, facing agents, and mold washes in foundry applications. Dry-ground phlogopite mica is used in automotive brake linings and clutch plates to reduce noise and vibration (asbestos substitute); as sound-absorbing insulation for coatings and polymer systems; in reinforcing additives for polymers to increase strength and stiffness and to improve stability to heat, chemicals, and ultraviolet (UV) radiation; in heat shields and temperature insulation; in industrial coating additive to decrease the permeability of moisture and hydrocarbons; and in polar polymer formulations to increase the strength of epoxies, nylons, and polyesters.\n\nWet-ground mica, which retains the brilliance of its cleavage faces, is used primarily in pearlescent paints by the automotive industry. Many metallic-looking pigments are composed of a substrate of mica coated with another mineral, usually titanium dioxide (TiO). The resultant pigment produces a reflective color depending on the thickness of the coating. These products are used to produce automobile paint, shimmery plastic containers, high-quality inks used in advertising and security applications. In the cosmetics industry, its reflective and refractive properties make mica an important ingredient in blushes, eye liner, eye shadow, foundation, hair and body glitter, lipstick, lip gloss, mascara, moisturizing lotions, and nail polish. Some brands of toothpaste include powdered white mica. This acts as a mild abrasive to aid polishing of the tooth surface, and also adds a cosmetically pleasing, glittery shimmer to the paste. Mica is added to latex balloons to provide a colored shiny surface.\n\nMica is also used as an insulator in concrete block and home attics and can be poured into walls (usually in retrofitting uninsulated open top walls). Mica may also be used as a soil conditioner, especially in potting soil mixes and in gardening plots. Greases used for axles are composed of a compound of fatty oils to which mica, tar or graphite is added to increase the durability of the grease and give it a better surface.\n\nMuscovite and phlogopite splittings can be fabricated into various built-up mica products. Produced by mechanized or hand setting of overlapping splittings and alternate layers of binders and splittings, built-up mica is used primarily as an electrical insulation material. Mica insulation is used in high-temperature and fire-resistant power cables in aluminium plants, blast furnaces, critical wiring circuits (for example, defense systems, fire and security alarm systems, and surveillance systems), heaters and boilers, lumber kilns, metal smelters, and tanks and furnace wiring. Specific high-temperature mica-insulated wire and cable is rated to work for up to 15 minutes in molten aluminium, glass, and steel. Major products are bonding materials; flexible, heater, molding, and segment plates; mica paper; and tape.\n\nFlexible plate is used in electric motor and generator armatures, field coil insulation, and magnet and commutator core insulation. Mica consumption in flexible plate was about 21 tonnes in 2008 in the US. Heater plate is used where high-temperature insulation is required. Molding plate is sheet mica from which V-rings are cut and stamped for use in insulating the copper segments from the steel shaft ends of a commutator. Molding plate is also fabricated into tubes and rings for insulation in armatures, motor starters, and transformers. Segment plate acts as insulation between the copper commutator segments of direct-current universal motors and generators. Phlogopite built-up mica is preferred because it wears at the same rate as the copper segments. Although muscovite has a greater resistance to wear, it causes uneven ridges that may interfere with the operation of a motor or generator. Consumption of segment plate was about 149 t in 2008 in the US. Some types of built-up mica have the bonded splittings reinforced with cloth, glass, linen, muslin, plastic, silk, or special paper. These products are very flexible and are produced in wide, continuous sheets that are either shipped, rolled, or cut into ribbons or tapes, or trimmed to specified dimensions. Built-up mica products may also be corrugated or reinforced by multiple layering. In 2008, about 351 t of built-up mica was consumed in the US, mostly for molding plates (19%) and segment plates (42%).\n\nTechnical grade sheet mica is used in electrical components, electronics, in atomic force microscopy and as window sheets. Other uses include diaphragms for oxygen-breathing equipment, marker dials for navigation compasses, optical filters, pyrometers, thermal regulators, stove and kerosene heater windows, radiation aperture covers for microwave ovens, and micathermic heater elements. Mica is birefringent and is therefore commonly used to make quarter and half wave plates. Specialized applications for sheet mica are found in aerospace components in air-, ground-, and sea-launched missile systems, laser devices, medical electronics and radar systems. Mica is mechanically stable in micrometer-thin sheets which are relatively transparent to radiation (such as alpha particles) while being impervious to most gases. It is therefore used as a window on radiation detectors such as Geiger-Müller tubes.\n\nIn 2008, mica splittings represented the largest part of the sheet mica industry in the United States. Consumption of muscovite and phlogopite splittings was about 308 t in 2008. Muscovite splittings from India accounted for essentially all US consumption. The remainder was primarily imported from Madagascar.\n\nSmall squared pieces of sheet mica are also used in the traditional Japanese Kodo ceremony to burn incense: A burning piece of coal is placed inside a cone made of white ash. The sheet of mica is placed on top, acting as a separator between the heat source and the incense, in order to spread the fragrance without burning it.\n\nSheet mica is used principally in the electronic and electrical industries. Its usefulness in these applications is derived from its unique electrical and thermal properties and its mechanical properties, which allow it to be cut, punched, stamped, and machined to close tolerances. Specifically, mica is unusual in that it is a good electrical insulator at the same time as being a good thermal conductor. The leading use of block mica is as an electrical insulator in electronic equipment. High-quality block mica is processed to line the gauge glasses of high-pressure steam boilers because of its flexibility, transparency, and resistance to heat and chemical attack. Only high-quality muscovite film mica, which is variously called India ruby mica or ruby muscovite mica, is used as a dielectric in capacitors. The highest quality mica film is used to manufacture capacitors for calibration standards. The next lower grade is used in transmitting capacitors. Receiving capacitors use a slightly lower grade of high-quality muscovite.\n\nMica sheets are used to provide structure for heating wire (such as in Kanthal or Nichrome) in heating elements and can withstand up to .\n\nThin transparent sheets of mica were used for peepholes in boilers, lanterns, stoves, and kerosene heaters because they were less likely to shatter than glass when exposed to extreme temperature gradients. Such peepholes were also used in \"isinglass curtains\" in horse-drawn carriages and early 20th-century cars.\n\nAnother use of mica is as a substrate in the production of ultraflat, thin-film surfaces, e.g. gold surfaces. Although the deposited film surface is still rough due to deposition kinetics, the back side of the film at the mica-film interface is ultraflat once the film is removed from the substrate. Freshly-cleaved mica surfaces have been used as clean imaging substrates in atomic force microscopy, enabling for example the imaging of bismuth films, plasma glycoproteins, membrane bilayers, and DNA molecules.\n\nHuman use of mica dates back to prehistoric times. Mica was known to ancient Indian, Egyptian, Greek and Roman and Chinese civilizations, as well as the Aztec civilization of the New World.\n\nThe earliest use of mica has been found in cave paintings created during the Upper Paleolithic period (40,000 BC to 10,000 BC). The first hues were red (iron oxide, hematite, or red ochre) and black (manganese dioxide, pyrolusite), though black from juniper or pine carbons has also been discovered. White from kaolin or mica was used occasionally.\n\nA few kilometers northeast of Mexico City stands the ancient site of Teotihuacan. The most striking structure of Teotihuacan is the towering Pyramid of the Sun. The pyramid contained considerable amounts of mica in layers up to thick.\n\nNatural mica was and still is used by the Taos and Picuris Pueblos Indians in north-central New Mexico to make pottery. The pottery is made from weathered Precambrian mica schist, and has flecks of mica throughout the vessels. Tewa Pueblo pottery is made by coating the clay with mica to provide a dense, glittery micaceous finish over the entire object.\n\nMica flakes (called \"abrak\" in Urdu and written as ابرک) are also used in Pakistan to embellish women's summer clothes, especially \"dupattas\" (long light-weight scarves, often colorful and matching the dress). Thin mica flakes are added to a hot starch water solution, and the \"dupatta\" is dipped in this water mixture for 3–5 minutes. Then it is hung to air dry.\n\nThroughout the ages, fine powders of mica have been used for various purposes, including decorations. Powdered mica glitter is used to decorate traditional water clay pots in India, Pakistan and Bangladesh; it is also used on traditional Pueblo pottery, though not restricted to use on water pots in this case. The \"gulal\" and \"abir\" (colored powders) used by North Indian Hindus during the festive season of Holi contain fine crystals of mica to create a sparkling effect. The majestic Padmanabhapuram Palace, from Trivandrum in India, has colored mica windows. Mica powder is also used as a decoration in traditional Japanese woodblock printmaking, as when applied to wet ink and allowed to dry it sparkles and reflects light.\n\nAyurveda, the Hindu system of ancient medicine prevalent in India, includes the purification and processing of mica in preparing Abhraka bhasma, which is employed in treating diseases of the respiratory and digestive tracts.\n\nMica dust in the workplace is regarded as a hazardous substance for respiratory exposure above certain concentrations.\n\nThe Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for mica exposure in the workplace as 20 mppcf over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 3 mg/m respiratory exposure over an 8-hour workday. At levels of 1,500 mg/m, mica is immediately dangerous to life and health.\n\nSome lightweight aggregates, such as diatomite, perlite, and vermiculite, may be substituted for ground mica when used as filler. Ground synthetic fluorophlogopite, a fluorine-rich mica, may replace natural ground mica for uses that require thermal and electrical properties of mica. Many materials can be substituted for mica in numerous electrical, electronic, and insulation uses. Substitutes include acrylate polymers, cellulose acetate, fiberglass, fishpaper, nylon, phenolics, polycarbonate, polyester, styrene, vinyl-PVC, and vulcanized fiber. Mica paper made from scrap mica can be substituted for sheet mica in electrical and insulation applications.\n\n"}
{"id": "1155441", "url": "https://en.wikipedia.org/wiki?curid=1155441", "title": "Milbert Amplifiers", "text": "Milbert Amplifiers\n\nMilbert Amplifiers is an American electronics manufacturer based in Gaithersburg, Maryland, which applies advanced power supplies and other patented technologies in its high-end audio equipment. Introduced in 1986, its mobile vacuum-tube audio amplifier has been followed by several models collectively in continuous production for nearly 30 years. Milbert also produces vacuum-tube guitar and musical amplifiers using unique impedance conversion which is claimed to prolong tube operating lifetimes.\n\n"}
{"id": "34741029", "url": "https://en.wikipedia.org/wiki?curid=34741029", "title": "NACA Report No. 1026", "text": "NACA Report No. 1026\n\nNACA Report No. 1026 - NACA Investigation of Fuel Performance in Piston-type Engines was issued by the United States National Advisory Committee for Aeronautics in 1951. It is a review of databases on fuel consumption versus power output in aviation internal combustion engines.\n\nIt is generally recognized that the piston-type engine will continue to play an important role in air transportation for an indefinite period in spite of the intensive effort now being devoted to gas turbine power plants. For this reason, past researches conducted in piston engines are still of interest in the solution of current problems relating to such engines. \n\nIn order to simplify the task of using the data from previous investigations, an effort has been made to compile a portion of these data into a single reference source. This report is a compilation of several researches acquired by NACA on fuel performance in piston engines. The original researches for this compilation are contained in many separate NACA reports.\n\n\n"}
{"id": "2505967", "url": "https://en.wikipedia.org/wiki?curid=2505967", "title": "NACA cowling", "text": "NACA cowling\n\nThe NACA cowling is a type of aerodynamic fairing used to streamline radial engines for use on airplanes and developed by the National Advisory Committee for Aeronautics in 1927. It was a major advance in aerodynamic drag reduction, and paid for its development and installation costs many times over due to the gains in fuel efficiency that it enabled.\n\nThe NACA cowling enhanced speed through drag reduction while delivering improved engine cooling. The idea that the NACA cowling produced thrust through the Meredith effect is fallacious—although in theory the expansion of the air as it was heated by the engine could create some thrust by exiting at high speed, in practice this required a cowling designed and shaped to achieve the high speed exit of air required (which the NACA cowling was not), and in any case, at 1930s airspeeds, the effect is negligible. \n\nThe cowling constitutes a symmetric, circular airfoil, in contrast to the planar airfoil of wings. It directs cool air to flow through the engine where it is routed across the engine's hottest parts, that is, the cylinders and heads. Furthermore, turbulence after the air passes the free-standing cylinders is greatly reduced. The sum of all these effects reduces drag by as much as 60 percent. The test conclusions resulted in almost every radial-engined aircraft being equipped with this cowling, starting in 1932.\n\nThe test aircraft, a Curtiss AT-5A Hawk biplane, featuring a Wright Whirlwind J-5 radial engine, reached an airspeed of equipped with the NACA cowling compared to without it.\n\n\n\n"}
{"id": "16705919", "url": "https://en.wikipedia.org/wiki?curid=16705919", "title": "Naked Imperialism", "text": "Naked Imperialism\n\nNaked Imperialism: The U.S. Pursuit of Global Dominance is a 2006 book by John Bellamy Foster. In the book Foster explains that, since September 11, 2001, the United States has been involved in wars in Afghanistan and Iraq, increased the global reach of its military bases, and spent more money on the military. In his analysis, U.S. militarism and imperialism have deep political and economic roots in U.S. history and the logic of capitalism. The apparent objective of the imperialist system of today (as in the past) is to open up peripheral economies to investment from core capitalist countries, thus ensuring raw material supplies at low prices, and a net outflow of economic surplus from poorer countries to center of the capitalist world.\n"}
{"id": "16038926", "url": "https://en.wikipedia.org/wiki?curid=16038926", "title": "Newtonianism", "text": "Newtonianism\n\nNewtonianism is a philosophical and scientific doctrine inspired by the beliefs and methods of natural philosopher Isaac Newton. While Newton's influential contributions were primarily in physics and mathematics, his broad conception of the universe as being governed by rational and understandable laws laid the foundation for many strands of Enlightenment thought. Newtonianism became an influential intellectual program that applied Newton's principles in many avenues of inquiry, laying the groundwork for modern science (both the natural and social sciences), in addition to influencing philosophy, political thought and theology.\n\nNewton's \"Principia Mathematica\", published by the Royal Society in 1687 but not available widely and in English until after his death, is the text generally cited as revolutionary or otherwise radical in the development of science. The three books of \"Principia\", considered a seminal text in mathematics and physics, are notable for their rejection of hypotheses in favor of inductive and deductive reasoning based on a set of definitions and axioms. This method may be contrasted to the Cartesian method of deduction based on sequential logical reasoning, and showed the efficacy of applying mathematical analysis as a means of making discoveries about the natural world.\n\nNewton's other seminal work was \"Opticks\", printed in 1704 in \"Philosophical Transactions of the Royal Society\", of which he became president in 1703. The treatise, which features his now famous work on the composition and dispersion of sunlight, is often cited as an example of how to analyze difficult questions via quantitative experimentation. Even so, the work was not considered revolutionary in Newton's time. One hundred years later, however, Thomas Young would describe Newton's observations in \"Opticks\" as \"yet unrivalled... they only rise in our estimation as we compare them with later attempts to improve on them.\"\n\nThe first edition of \"Principia\" features proposals about the movements of celestial bodies which Newton initially calls \"hypotheses\"—however, by the second edition, the word \"hypothesis\" was replaced by the word \"rule\", and Newton had added to the footnotes the following statement:... I frame no hypotheses. For whatever is not deduced from the phenomena is to be called a hypothesis; and hypotheses, whether metaphysical or physical, whether of occult qualities or mechanical, have no place in experimental philosophy.Newton's work and the philosophy that enshrines it are based on mathematical empiricism, which is the idea that mathematical and physical laws may be revealed in the real world via experimentation and observation. It is important to note, however, that Newton's empiricism is balanced against an adherence to an exact mathematical system, and that in many cases the \"observed phenomena\" upon which Newton built his theories were actually based on mathematical models, which were representative but not identical to the natural phenomena they described.\n\nNewtonian doctrine can be contrasted with several alternative sets of principles and methods such as Cartesianism, Leibnizianism and Wolffianism.\n\nDespite his reputation for empiricism in historical and scientific circles, Newton was deeply religious and believed in the literal truth of Scripture, taking the story of Genesis to be Moses' eyewitness account of the creation of the solar system. Newton reconciled his beliefs by adopting the idea that the Christian God set in place at the beginning of time the \"mechanical\" laws of nature, but retained the power to enter and alter that mechanism at any time.\n\nNewton further believed that the preservation of nature was in itself an act of God, stating that \"a continual miracle is needed to prevent the Sun and fixed stars from rushing together through Gravity\".\n\nBetween 1726 and 1729, French author, philosopher, and historian Voltaire was exiled in England, where he met several significant English scholars and devotees to the Newtonian system of thought. Voltaire would later bring these ideas back to France with his publication of \"Lettres Philosophiques\" and \"Philosophie de Newton\", which popularized Newton's intellectual practices and general philosophy. Later, prominent natural philosopher and friend of Voltaire, Émilie du Châtelet, would publish a French translation of \"Principia\", which met with great success in France.\n\nWhile Newton was opposed by some members of the religious community for his non-Trinitarian beliefs about God, others believed science itself to be a philosophical exercise, that if done correctly, would lead its practitioners to a greater knowledge and appreciation of God.\n\nIn 1737, Italian scholar Count Frencesco Algarotti published a book entitled \"Newtonianismo per le dame overro dialoghi sopre la luce e i colori\", which aimed to introduce female audiences to the work of Newton. The text explained the principles of Newton's \"Opticks\" while avoiding much of the mathematical rigor of the work in favor of a more \"agreeable\" text. The book was later published with a title that made no reference to women, leading some to believe that the female branding of the book was a ploy to avoid censorship.\n\nScottish philosopher David Hume, likely inspired by the methods of analysis and synthesis which Newton developed in \"Opticks\", was a strong adherent of Newtonian empiricism in his studies of moral phenomena.\n\nNewton and his philosophy of Newtonianism arguably led to the popularization of science in Europe—particularly in England, France, and Germany—catalyzing the Age of Enlightenment.\n"}
{"id": "4957612", "url": "https://en.wikipedia.org/wiki?curid=4957612", "title": "Nike Grind", "text": "Nike Grind\n\nNike Grind is part of Nike's Reuse-A-Shoe program that was started in 1993. The purpose of the program is to eliminate waste and close the loop on Nike's product lifecycle by collecting post-consumer, non-metal-containing athletic shoes of any brand. This includes Nike shoes that are returned due to material or workmanship defects.\n\nOnce collected, the sneakers are ground up and separated into three distinct types of material: rubber from the outsole, foam from the midsole and fabric from the shoe's upper. Nike and its partners (Atlas Track & Tennis, Playtop, Training Ground, Everlast and Rebound Ace) take the granulated rubber and create soccer, football, baseball fields, weight room flooring and running tracks. The first synthetic turf soccer field installed with Nike Grind rubber was Douglas Park in Chicago. That surface was donated by Nike and the U.S. Soccer Foundation.\nNike uses the granulated foam from the shoes for synthetic basketball courts, tennis courts and playground surfacing tiles. The granulated fabric from the shoe uppers becomes the padding under hardwood basketball floors.\n\nThese programs are located in Australia, Canada, Japan, United Kingdom, and United States.\n\nThere are two Nike Grind processing plants, one located in the United States and the other in Belgium. The American facility uses a \"slice and grind\" technique, where each shoe is cut into three slices that contain the main materials. These slices are fed through grinders and purified to become the different types of material. The Belgian facility grinds the shoes whole, and passes the resulting material through a series of separators for the same three groups of material.\n\n"}
{"id": "2235522", "url": "https://en.wikipedia.org/wiki?curid=2235522", "title": "Octahedral molecular geometry", "text": "Octahedral molecular geometry\n\nIn chemistry, octahedral molecular geometry describes the shape of compounds with six atoms or groups of atoms or ligands symmetrically arranged around a central atom, defining the vertices of an octahedron. The octahedron has eight faces, hence the prefix \"octa\". The octahedron is one of the Platonic solids, although octahedral molecules typically have an atom in their centre and no bonds between the ligand atoms. A perfect octahedron belongs to the point group O. Examples of octahedral compounds are sulfur hexafluoride SF and molybdenum hexacarbonyl Mo(CO). The term \"octahedral\" is used somewhat loosely by chemists, focusing on the geometry of the bonds to the central atom and not considering differences among the ligands themselves. For example, [Co(NH)], which is not octahedral in the mathematical sense due to the orientation of the N-H bonds, is referred to as octahedral.\n\nThe concept of octahedral coordination geometry was developed by Alfred Werner to explain the stoichiometries and isomerism in coordination compounds. His insight allowed chemists to rationalize the number of isomers of coordination compounds. Octahedral transition-metal complexes containing amines and simple anions are often referred to as Werner-type complexes.\n\nWhen two or more types of ligands (L, L, ...) are coordinated to an octahedral metal centre (M), the complex can exist as isomers. The naming system for these isomers depends upon the number and arrangement of different ligands.\n\nFor MLL, two isomers exist. These isomers of MLL are \"cis\", if the L ligands are mutually adjacent, and \"trans\", if the L groups are situated 180° to each other. It was the analysis of such complexes that led Alfred Werner to the 1913 Nobel Prize–winning postulation of octahedral complexes.\n\nFor MLL, two isomers are possible - a facial isomer (\"fac\") in which each set of three identical ligands occupies one face of the octahedron surrounding the metal atom, so that any two of these three ligands\nare mutually cis, and a meridional isomer (\"mer\") in which each set of three identical ligands occupies a plane passing through the metal atom.\n\nMore complicated complexes, with several different kinds of ligands or with bidentate ligands can also be chiral, with pairs of isomers which are non-superimposable mirror images or enantiomers of each other.\n\nFor MLLL, a total of six isomers are possible. \n\nThe number of possible isomers can reach 30 for an octahedral complex with six different ligands (in contrast, only two stereoisomers are possible for a tetrahedral complex with four different ligands). The following table lists all possible combinations for monodentate ligands:\n\nThus, all 15 diastereomers of MLLLLLL are chiral, whereas for MLLLLL, six diastereomers are chiral and three are not (the ones where L are \"trans\"). One can see that octahedral coordination allows much greater complexity than the tetrahedron that dominates organic chemistry. The tetrahedron MLLLL exists as a single enatiomeric pair. To generate two diastereomers in an organic compound, at least two carbon centers are required.\n\nThe term can also refer to octahedral influenced by the Jahn–Teller effect, which is a common phenomenon encountered in coordination chemistry. This reduces the symmetry of the molecule from O to D and is known as a tetragonal distortion.\n\nSome molecules, such as XeF or , have a lone pair that distorts the symmetry of the molecule from O to C.. The specific geometry is known as a \"monocapped\" octahedron, since it is derived from the octahedron by placing the lone pair over the centre of one triangular face of the octahedron as a \"cap\".\n\nPairs of octahedra can be fused in a way that preserves the octahedral coordination geometry by replacing terminal ligands with bridging ligands. Two motifs for fusing octahedra are common: edge-sharing and face-sharing. Edge- and face-shared bioctahedra have the formulas [ML(μ-L)] and ML(μ-L), respectively. Polymeric versions of the same linking pattern give the stoichiometries [ML(μ-L)] and [M(μ-L)], respectively.\n\nThe sharing of an edge or a face of an octahedron gives a structure called bioctahedral. Many metal pentahalide and pentaalkoxide compounds exist in solution and the solid with bioctahedral structures. One example is niobium pentachloride. Metal tetrahalides often exist as polymers with edge-sharing octahedra. Zirconium tetrachloride is an example. Compounds with face-sharing octahedral chains include MoBr, RuBr, and TlBr.\n\nFor compounds with the formula MX, the chief alternative to octahedral geometry is a trigonal prismatic geometry, which has symmetry D. In this geometry, the six ligands are also equivalent. There are also distorted trigonal prisms, with C symmetry; a prominent example is W(CH). The interconversion of \"Δ\"- and \"Λ\"-complexes, which is usually slow, is proposed to proceed via a trigonal prismatic intermediate, a process called the \"Bailar twist\". An alternative pathway for the racemization of these same complexes is the Ray–Dutt twist.\n\nFor a free ion, e.g. gaseous Ni or Mo, the energy of the d-orbitals are equal in energy; that is, they are \"degenerate\". In an octahedral complex, this degeneracy is lifted. The energy of the d and d, the so-called e set, which are aimed directly at the ligands are destabilized. On the other hand, the energy of the d, d, and d orbitals, the so-called t set, are stabilized. The labels t and e refer to irreducible representations, which describe the symmetry properties of these orbitals. The energy gap separating these two sets is the basis of crystal field theory and the more comprehensive ligand field theory. The loss of degeneracy upon the formation of an octahedral complex from a free ion is called crystal field splitting or ligand field splitting. The energy gap is labeled \"Δ\", which varies according to the number and nature of the ligands. If the symmetry of the complex is lower than octahedral, the e and t levels can split further. For example, the t and e sets split further in \"trans\"-MLL.\n\nLigand strength has the following order for these electron donors:\n\nSo called \"weak field ligands\" give rise to small \"Δ\" and absorb light at longer wavelengths.\n\nGiven that a virtually uncountable variety of octahedral complexes exist, it is not surprising that a wide variety of reactions have been described. These reactions can be classified as follows:\n\n\nMany reactions of octahedral transition metal complexes occur in water. When an anionic ligand replaces a coordinated water molecule the reaction is called an anation. The reverse reaction, water replacing an anionic ligand, is called aquation. For example, the [CoCl(NH)] slowly aquates to give [Co(NH)(HO)] in water, especially in the presence of acid or base. Addition of concentrated HCl converts the aquo complex back to the chloride, via an anation process.\n\n\n"}
{"id": "4372013", "url": "https://en.wikipedia.org/wiki?curid=4372013", "title": "Plumbosolvency", "text": "Plumbosolvency\n\nPlumbosolvency is the ability of a solvent, notably water, to dissolve lead. In the public supply of water this is an undesirable property. In (usually older) consumers' premises plumbosolvent water can attack lead pipes and any lead in solder used to join copper. \nPlumbosolvency of water can be countered by achieving a pH of 7.5 by increasing the pH with lime or sodium hydroxide (lye), or by providing a protective coating to the inside of lead pipes by the addition of phosphate at the water treatment works.\n\nWhile optimal pH for prevention of plumbosolvency is 7.5, performance remains very good in the range pH 7.2-7.6.\nAchieving this pH has been shown to decrease population blood lead concentrations.(3, 4)\n\nChlorinating water also reduces dissolved lead. It causes the interiors of lead pipes to become coated with lead chloride, which is very insoluble in cold water. However, lead chloride is fairly soluble in hot water. For this reason, water that is to be used for drinking or the preparation of food should never be taken from a hot-water tap, if the water may have been in contact with lead. Water should be taken from a cold-water tap, and heated in a pan or kettle that does not contain lead or lead solder.\n\n"}
{"id": "20914202", "url": "https://en.wikipedia.org/wiki?curid=20914202", "title": "Richard Hayes (biotech policy advocate)", "text": "Richard Hayes (biotech policy advocate)\n\nRichard Hayes is Visiting Scholar at the University of California at Berkeley College of Natural Resources / Energy and Resources Group. He was founding executive director of the Berkeley, California-based Center for Genetics and Society, serving from 1999 through 2012. In the early 1990s he chaired the Sierra Club's Global Warming Campaign Committee. In the 1980s he served on the national staff of the Sierra Club, first as assistant political director and then as national director of volunteer development. He was previously executive director of the San Francisco Democratic Party.\n\nAccording to Bill McKibben in 2004's \"Enough: Staying Human in an Engineered Age\", Hayes is \"one of the leading crusaders against germline manipulation,\" that is, the modification of inheritable human genetic traits. Hayes has briefed United Nations delegates on the need for a global ban on human cloning, and has testified in support of international oversight of human biotechnologies, and against the cloning of pets. He is quoted in a 2002 article in \"Newsweek International\" declaiming the \"vacuum of leadership,\" regarding responsible oversight of human genetic technology, noting that \"[t]hese technologies ... have developed so rapidly that there is not the type of structure to regulate them\". He holds a PhD from Energy and Resources from the University of California at Berkeley.\n\n\n"}
{"id": "43551", "url": "https://en.wikipedia.org/wiki?curid=43551", "title": "Ruby", "text": "Ruby\n\nA ruby is a pink to blood-red colored gemstone, a variety of the mineral corundum (aluminium oxide). Other varieties of gem-quality corundum are called sapphires. Ruby is one of the traditional cardinal gems, together with amethyst, sapphire, emerald, and diamond. The word \"ruby\" comes from \"ruber\", Latin for red. The color of a ruby is due to the element chromium. \n\nThe quality of a ruby is determined by its color, cut, and clarity, which, along with carat weight, affect its value. The brightest and most valuable shade of red called blood-red or pigeon blood, commands a large premium over other rubies of similar quality. After color follows clarity: similar to diamonds, a clear stone will command a premium, but a ruby without any needle-like rutile inclusions may indicate that the stone has been treated. Ruby is the traditional birthstone for July and is usually more pink than garnet, although some rhodolite garnets have a similar pinkish hue to most rubies. The world's most valuable ruby is the Sunrise Ruby.\n\nRubies have a hardness of 9.0 on the Mohs scale of mineral hardness. Among the natural gems only moissanite and diamond are harder, with diamond having a Mohs hardness of 10.0 and moissanite falling somewhere in between corundum (ruby) and diamond in hardness. Sapphire, ruby, and pure corundum are α-alumina, the most stable form of AlO, in which 3 electrons leave each aluminum ion to join the regular octahedral group of six nearby O ions; in pure corundum this leaves all of the aluminum ions with a very stable configuration of no unpaired electrons or unfilled energy levels, and the crystal is perfectly colorless.\n\nWhen a chromium atom replaces an occasional aluminum atom, it too loses 3 electrons to become a chromium ion to maintain the charge balance of the AlO crystal. However, the Cr ions are larger and have electron orbitals in different directions than aluminum. The octahedral arrangement of the O ions is distorted, and the energy levels of the different orbitals of those Cr ions are slightly altered because of the directions to the O ions. Those energy differences correspond to absorption in the ultraviolet, violet, and yellow-green regions of the spectrum.\n\nIf one percent of the aluminum ions are replaced by chromium in ruby, the yellow-green absorption results in a red color for the gem. Additionally, absorption at any of the above wavelengths stimulates fluorescent emission of 694-nanometer-wavelength red light, which adds to its red color and perceived luster.\n\nAfter absorbing short-wavelength light, there is a short interval of time when the crystal lattice of ruby is in an excited state before fluorescence occurs. If 694-nanometer photons pass through the crystal during that time, they can stimulate more fluorescent photons to be emitted in-phase with them, thus strengthening the intensity of that red light. By arranging mirrors or other means to pass emitted light repeatedly through the crystal, a ruby laser in this way produces a very high intensity of coherent red light.\n\nAll natural rubies have imperfections in them, including color impurities and inclusions of rutile needles known as \"silk\". Gemologists use these needle inclusions found in natural rubies to distinguish them from synthetics, simulants, or substitutes. Usually, the rough stone is heated before cutting. These days, almost all rubies are treated in some form, with heat treatment being the most common practice. Untreated rubies of high quality command a large premium.\n\nSome rubies show a three-point or six-point asterism or \"star\". These rubies are cut into cabochons to display the effect properly. Asterisms are best visible with a single-light source and move across the stone as the light moves or the stone is rotated. Such effects occur when light is reflected off the \"silk\" (the structurally oriented rutile needle inclusions) in a certain way. This is one example where inclusions increase the value of a gemstone. Furthermore, rubies can show color changes—though this occurs very rarely—as well as chatoyancy or the \"cat's eye\" effect.\n\nGenerally, gemstone-quality corundum in all shades of red, including pink, are called rubies. However, in the United States, a minimum color saturation must be met to be called a ruby; otherwise, the stone will be called a pink sapphire. Drawing a distinction between rubies and pink sapphires is relatively new, having arisen sometime in the 20th century. Often, the distinction between ruby and pink sapphire is not clear and can be debated. As a result of the difficulty and subjectiveness of such distinctions, trade organizations such as the International Colored Gemstone Association (ICGA) have adopted the broader definition for ruby which encompasses its lighter shades, including pink.\n\nThe Mogok Valley in Upper Myanmar (Burma) was for centuries the world's main source for rubies. That region has produced some exceptional rubies, however in recent years few good rubies have been found. In central Myanmar, the area of Mong Hsu began producing rubies during the 1990s and rapidly became the world's main ruby mining area. The most recently found ruby deposit in Myanmar is in Namya (Namyazeik) located in the northern state of Kachin.\n\nHistorically, rubies have also been mined in Thailand, in the Pailin and Samlout District of Cambodia, as well as in Afghanistan, Australia, Brazil, Colombia, India, Namibia, Japan, and Scotland; after the Second World War ruby deposits were found in Madagascar, Nepal, Pakistan, Tajikistan, Tanzania, and Vietnam. In Sri Lanka, lighter shades of rubies (often \"pink sapphires\") are more commonly found. The Republic of Macedonia is the only country in mainland Europe to have naturally occurring rubies. They can mainly be found around the city of Prilep. Macedonian rubies have a unique raspberry color. The ruby is also included on the Macedonian coat of arms. A few rubies have been found in the U.S. states of Montana, North Carolina, South Carolina and Wyoming.\n\nSpinel, another red gemstone, is sometimes found along with rubies in the same gem gravel or marble. Red spinels may be mistaken for rubies by those lacking experience with gems. However, the finest red spinels can have values approaching that of an average ruby.\n\nRubies, as with other gemstones, are graded using criteria known as the four Cs, namely color, cut, clarity and carat weight. Rubies are also evaluated on the basis of their geographic origin.\n\nColor: In the evaluation of colored gemstones, color is the most important factor. Color divides into three components: \"hue\", \"saturation\" and \"tone\". Hue refers to color as we normally use the term. Transparent gemstones occur in the \"pure spectral hues\" of red, orange, yellow, green, blue, violet. In nature, there are rarely pure hues, so when speaking of the hue of a gemstone, we speak of primary and secondary and sometimes tertiary hues. Ruby is defined to be red. All other hues of the gem species corundum are called sapphire. Ruby may exhibit a range of secondary hues, including orange, purple, violet, and pink.\n\nThe finest ruby is described as being a vivid medium-dark toned red. Secondary hues add an additional complication. Pink, orange, and purple are the normal secondary hues in ruby. Of the three, purple is preferred because it reinforces the red, making it appear richer. Purple also occupies a position on the color wheel halfway between red and blue. When a purplish-red ruby is set in yellow, the yellow neutralizes its complement blue, leaving the stone appearing to be pure red in the setting.\n\nImproving the quality of gemstones by treating them is common practice. Some treatments are used in almost all cases and are therefore considered acceptable. During the late 1990s, a large supply of low-cost materials caused a sudden surge in supply of heat-treated rubies, leading to a downward pressure on ruby prices.\n\nImprovements used include color alteration, improving transparency by dissolving rutile inclusions, healing of fractures (cracks) or even completely filling them.\n\nThe most common treatment is the application of heat. Most rubies at the lower end of the market are heat treated to improve color, remove \"purple tinge\", blue patches, and silk. These heat treatments typically occur around temperatures of 1800 °C (3300 °F). Some rubies undergo a process of low tube heat, when the stone is heated over charcoal of a temperature of about 1300 °C (2400 °F) for 20 to 30 minutes. The silk is partially broken, and the color is improved.\n\nAnother treatment, which has become more frequent in recent years, is lead glass filling. Filling the fractures inside the ruby with lead glass (or a similar material) dramatically improves the transparency of the stone, making previously unsuitable rubies fit for applications in jewelry. The process is done in four steps:\n\n\nIf a color needs to be added, the glass powder can be \"enhanced\" with copper or other metal oxides as well as elements such as sodium, calcium, potassium etc.\n\nThe second heating process can be repeated three to four times, even applying different mixtures. When jewelry containing rubies is heated (for repairs) it should not be coated with boracic acid or any other substance, as this can etch the surface; it does not have to be \"protected\" like a diamond.\n\nThe treatment can identified by noting bubbles in cavities and fractures using a 10x loupe.\n\nIn 1837, Gaudin made the first synthetic rubies by fusing potash alum at a high temperature with a little chromium as a pigment. In 1847, Ebelmen made white sapphire by fusing alumina in boric acid. In 1877, Frenic and Freil made crystal corundum from which small stones could be cut. Frimy and Auguste Verneuil manufactured artificial ruby by fusing BaF and AlO with a little chromium at red heat. In 1903, Verneuil announced he could produce synthetic rubies on a commercial scale using this flame fusion process. By 1910, Verneuil's laboratory had expanded into a 30 furnace production facility, with annual gemstone production having reached in 1907.\n\nOther processes in which synthetic rubies can be produced are through Czochralski's pulling process, flux process, and the hydrothermal process. Most synthetic rubies originate from flame fusion, due to the low costs involved. Synthetic rubies may have no imperfections visible to the naked eye but magnification may reveal curves, striae and gas bubbles. The fewer the number and the less obvious the imperfections, the more valuable the ruby is; unless there are no imperfections (i.e., a perfect ruby), in which case it will be suspected of being artificial. Dopants are added to some manufactured rubies so they can be identified as synthetic, but most need gemological testing to determine their origin.\n\nSynthetic rubies have technological uses as well as gemological ones. Rods of synthetic ruby are used to make ruby lasers and masers. The first working laser was made by Theodore H. Maiman in 1960. Maiman used a solid-state light-pumped synthetic ruby to produce red laser light at a wavelength of 694 nanometers (nm). Ruby lasers are still in use. Rubies are also used in applications where high hardness is required such as at wear exposed locations in modern mechanical clockworks, or as scanning probe tips in a coordinate measuring machine.\n\nImitation rubies are also marketed. Red spinels, red garnets, and colored glass have been falsely claimed to be rubies. Imitations go back to Roman times and already in the 17th century techniques were developed to color foil red—by burning scarlet wool in the bottom part of the furnace—which was then placed under the imitation stone. Trade terms such as balas ruby for red spinel and rubellite for red tourmaline can mislead unsuspecting buyers. Such terms are therefore discouraged from use by many gemological associations such as the Laboratory Manual Harmonisation Committee (LMHC).\n\n\n\n\n"}
{"id": "63415", "url": "https://en.wikipedia.org/wiki?curid=63415", "title": "Sinclair C5", "text": "Sinclair C5\n\nThe Sinclair C5 is a small one-person battery electric velomobile, technically an \"electrically assisted pedal cycle\". It was the culmination of Sir Clive Sinclair's long-running interest in electric vehicles. Although widely described as an \"electric car\", Sinclair characterised it as a \"vehicle, not a car\".\n\nSinclair had become one of the UK's best-known millionaires, and earned a knighthood, on the back of the highly successful Sinclair Research range of home computers in the early 1980s. He hoped to repeat his success in the electric vehicle market, which he saw as ripe for a new approach. The C5 emerged from an earlier project to produce a small electric car called the C1. After a change in the law, prompted by lobbying from bicycle manufacturers, Sinclair developed the C5 as an electrically powered tricycle with a polypropylene body and a chassis designed by Lotus Cars. It was intended to be the first in a series of increasingly ambitious electric vehicles, but in the event the planned development of the follow-up C10 and C15 electric cars never got further than the drawing board.\n\nOn 10 January 1985, the C5 was unveiled at a glitzy launch event but it received a less than enthusiastic reception from the British media. Its sales prospects were blighted by poor reviews and safety concerns expressed by consumer and motoring organisations. The vehicle's limitations – a short range, a maximum speed of only , a battery that ran down quickly and a lack of weatherproofing – made it impractical for most people's needs. It was marketed as an alternative to cars and bicycles, but ended up appealing to neither group of owners, and it was not available in shops until several months after its launch. Within three months of the launch, production had been slashed by 90%. Sales never picked up despite Sinclair's optimistic forecasts and production ceased entirely by August 1985. Out of 14,000 C5s made, only 5,000 were sold before its manufacturer, Sinclair Vehicles, went into receivership.\n\nThe C5 became known as \"one of the great marketing bombs of postwar British industry\" and a \"notorious ... example of failure\". Despite its commercial failure, the C5 went on to become a cult item for collectors. Thousands of unsold C5s were purchased by investors and sold for hugely inflated prices – as much as £5,000, compared to the original retail value of £399. Enthusiasts have established owners' clubs and some have modified their vehicles substantially, adding monster wheels, jet engines, and high-powered electric motors to propel their C5s at speeds of up to .\n\nThe C5 is made predominately of polypropylene, measuring long, wide, and high. It weighs approximately without a battery and with one. The chassis consists of a single Y-shaped steel component with a cross-section of about by The vehicle has three wheels, one of diameter at the front and two of at the rear.\n\nThe driver sits in a recumbent position in an open cockpit, steering via a handlebar that is located under the knees. A power switch and front and rear brake levers are positioned on the handlebar. As a supplement to or replacement for electric power, the C5 can also be propelled via bicycle-style pedals located at the front of the cockpit. The maximum speed of an unmodified C5 is . At the rear of the vehicle is a small luggage compartment with a capacity of 28 litres (1 cu ft). As the C5 does not have a reverse gear, reversing direction is done by getting out, picking up the front end and turning it around by hand.\n\nThe C5 is powered by a 12-volt lead-acid electric battery driving a motor with a continuous rating of 250 watts and a maximum speed of 4,100 revolutions per minute. It is coupled with a two-stage gear-drive that increases torque by a factor of 13, without which the motor would not be able to move the vehicle when a person is on board. However, the motor is vulnerable to overheating. The torque increases as the load on the vehicle increases, for instance by going up too steep a gradient. Sinclair's tests showed that it could cope under power with a maximum slope of 1 in 12 (8%) and could manage a 1 in 7 (14%) slope using the pedals. As the speed of the motor reduces, the current flow through its windings increases, drawing up to 140 amps at stall speed. This would very quickly burn the motor out if sustained, so the motor's load is constantly monitored by the C5's electronics. If it stalls under full load the electronics disable the motor after 4 seconds, while if it is under heavy load (around 80 or 90 amps) it trips after two or three minutes. A heat-sensitive resistor inside the motor warns the driver if the vehicle is beginning to overheat and disconnects the motor after a short time, and a third line of defence is provided by a metallic strip mounted on the motor. If an excessive temperature is reached the strip distorts and the power is disconnected.\n\nAlthough it was usually billed as an electric vehicle, the C5 also depends significantly on pedal power. The vehicle's battery is designed to provide 35 amps for an hour when fully charged or half that for two hours, giving the C5 a claimed range of . A display in the cockpit uses green, amber, and red LEDs to display the state of the battery charge. The segments are extinguished one after the other to indicate how much driving time is left. The last light indicates that only ten minutes of power are left, after which the motor is switched off and the driver is left to rely on the pedals. Another display indicates via green, amber, and red LEDs how much current is being used. The C5 is in its most economical running mode when a low amount of current, indicated by the green LEDs, is being used. When the lights are red, the motor is under a high load and the driver needs to use pedal power to avoid overheating and shutdown.\n\nThe C5 was initially sold at a cost of £399, but to keep the cost under the £400 mark a number of components were sold as optional accessories. These included indicator lights, mirrors, mud flaps, a horn, and a \"High-Vis Mast\" consisting of a reflective strip on a pole, designed to make the C5 more visible in traffic. Sinclair's C5 accessories brochure noted that \"the British climate isn't always ideal for wind-in-the-hair driving\" and offered a range of waterproofs to keep C5 drivers dry in the vehicle's open cockpit. Other accessories included seat cushions and spare batteries.\n\nSir Clive Sinclair's interest in the possibilities of electric vehicles originated in the late 1950s during a holiday job for the electronics company Solatron. Fifteen years later, in the early 1970s, he was the head of his own successful electronics company, Sinclair Radionics, based in St Ives in Cambridgeshire. He tasked one of his employees, Chris Curry – later a co-founder of Acorn Computers – to carry out some preliminary research into electric vehicle design.\n\nSinclair took the view that an electric vehicle needed to be designed from the ground up, completely rethinking the principles of automotive design rather than simply dropping electric components into an established model. He believed that the motor was the key to the design. Sinclair and Curry developed a wafer-thin motor that was mounted on a child's scooter, with a button on the handlebars to activate it. The research got no further, however, as Sinclair's development of the first \"slimline\" pocket calculator – the Sinclair Executive and its successors – took precedence. No further work on electric vehicles took place for most of the rest of the 1970s.\n\nIt was not until late 1979 that Sinclair returned to electric vehicle development. Around Christmas that year, he approached Tony Wood Rogers, an ex-Radionics employee, to carry out consultancy work on \"a preliminary investigation into a personal electric vehicle\". The brief was to assess the options for producing a one-person vehicle which would be a replacement for a moped and would have a maximum speed of . Although Wood Rogers was initially reluctant, he was intrigued by the idea of an electric vehicle and agreed to help Sinclair. The vehicle was dubbed the C1 (the C standing for Clive). He built a number of prototypes to demonstrate various design principles and clarify the final specifications.\n\nA specification of the C1 emerged by the end of the year. It would address short-distance transportation needs, with a minimum range of on a fully charged battery. This reflected official figures showing that the average daily car journey was only , while the average moped or pedal cycle journey was just . The users were envisaged as being housewives, urban commuters, and young people, who might otherwise use cycles or mopeds to travel. The electric vehicle would be safer, more weather-proof, and would offer space to carry items. It would be easy to drive and park and for the driver to enter or exit, and it would require minimum maintenance. The vehicle would be engineered for simplicity using injection-moulded plastic components and a polypropylene body. It would also be much cheaper than a car, costing £500 (now £) at the most.\n\nOne area of development that Sinclair purposely avoided was battery technology. Electric vehicles powered by lead-acid batteries had once actually outnumbered internal combustion engine vehicles; in 1912 nearly 34,000 electric cars were registered in the U.S. However, the efficiency of internal combustion engines greatly improved while battery technology advanced much more slowly, leading to petrol and diesel-driven vehicles dominating the market. By 1978, out of 17.6 million registered vehicles on Britain's roads, only 45,000 were electric vehicles in day-to-day use and of those, 90% were milkfloats. Sinclair chose to rely on existing lead-acid battery technology, avoiding the great expense of developing a more efficient type. His rationale was that if the electric vehicle market took off, battery manufacturers would step up to develop better batteries. Wood Rogers recalls:\n\nThe development programme moved to the University of Exeter in 1982, where the C1 chassis was fitted with fibreglass shells and tested in a wind tunnel. It was recognised at an early stage that the vehicle would have to be aerodynamic; although it was only ever intended to be small and relatively slow, reducing wind resistance was seen as essential for the vehicle's efficiency. By March 1982 the basic design of the C1 had been established. Sinclair then turned to an established motor design company, Ogle Design of Letchworth, to provide professional styling assistance and production engineering. However, Ogle's approach was not to Sinclair's liking; they tackled the project as one of car design and focused more (and expensively) on the aerodynamics rather than the cycle technology around which the C1 was based. The weight of the vehicle increased to over , far more than Sinclair's desired specification. By March 1983, Sinclair and Wood Rogers had decided to stop the C1 programme. Wood Rogers comments that Ogle were convinced that the C1 would be a flop, telling Sinclair that it would not be fast enough, that its drivers would get wet when it rained and that the battery was not good enough.\n\nTo meet the steadily escalating development costs of the vehicle, Sinclair decided to raise capital by selling some of his own shares in Sinclair Research to fund a separate company that would focus on electric vehicles. A £12 million deal was reached in March 1983, of which £8.3 million was used to fund the establishment of the new Sinclair Vehicles company. Sinclair recruited Barrie Wills, a veteran former employee of the DeLorean Motor Company, to lead Sinclair Vehicles as its managing director. Although Wills initially expressed scepticism about the viability of an electric vehicle – his twenty-five years in the motor industry had convinced him that an electric car was never going to happen – Sinclair managed to convince him that the project would work. In 1984, Sinclair Vehicles' new head office was established in Coventry in the West Midlands, an area with a long-established link with the motor industry.\n\nThe project's prospects were boosted by changes in the British government's approach to electric vehicles. In March 1980, it had abolished Vehicle Excise Duty for electric vehicles and by the start of 1983, the Department of Transport was working on legislation that would introduce a new category of vehicle – the \"electrically assisted pedal cycle\". This had a number of significant advantages from Sinclair's point of view. Such a vehicle would be exempt from insurance and vehicle tax, and the user would not need a driving licence or a helmet, all of which were required for mopeds. The legislation, which was passed in August 1983, was prompted by a lobbying campaign by manufacturers such as Raleigh who wanted to sell electric bicycles.\n\nSinclair realised that his electric vehicle design could easily be adapted to meet the new legislation. As the \"electrically assisted pedal cycle\" category was so new, there were no existing vehicles on the market that would meet the standards prescribed by the new legislation. However, it imposed a number of restrictions that limited the performance of any vehicle that would qualify under the new standards. The maximum legal speed of the vehicle would be limited to only ; it could not weigh any more than , including the battery; and its motor could not be rated at any more than 250 watts.\n\nDespite these limitations, the vehicle was seen as only the first step in a series of increasingly ambitious electrical cars. Sinclair intended it to prove the viability of electric personal transport; the hope was that, just as Sinclair had found with home computers like the hugely successful ZX81 and ZX Spectrum, an affordable electric vehicle could unleash pent-up demand for a market that did not previously exist. However, Sinclair performed no market research to ascertain whether there was actually a market for his electric vehicle; as the director of the Primary Contact advertising agency commented in January 1985, the project continued all the way to the prototype stage \"purely on the convictions of Sir Clive\".\n\nWith Sinclair's new specifications in hand, Ogle worked on a three-wheeled design dubbed the C5, which bore similarities with the earlier three-wheeled Bond Bug – another Ogle design. The vehicle's handlebar steering was the brainchild of Wood Rogers, who decided at the outset that a steering wheel would not be practicable as it would make it impossible for a driver to get in and out easily – a serious safety disadvantage. He comments that \"putting the bars at the driver's sides made it easy to steer and felt very natural\". A prototype was presented to 63 families in the A, B, C1 and C2 demographic groups in suburban and town environments to determine that the controls were correctly positioned; this was the only external research carried out on the C5. In the autumn of 1983, Wills brought in Lotus Cars to finish the vehicle's detailing, build prototypes and test rigs, carry out testing and take forward the programme to production. The development of the C5 took place over 19 months in conditions of great secrecy, with testing carried out at the Motor Industry Research Association's proving ground in Leicestershire.\n\nFurther aerodynamic refinements were carried out in Exeter with the development of new body shells which produced further reductions in the vehicle's drag. However, it was felt that something was lacking in the design and a 23-year-old industrial designer, Gus Desbarats, was brought in to refine the shell's appearance. He had won a Sinclair-sponsored electric vehicle design competition at the Royal College of Art and was hired on his graduation to set up an in-house car design studio at Sinclair's Metalab in Cambridge, of which he became the first employee. It was not only Desbarats' first project but, as he later said, \"day one of my working life\", when he turned up at Sinclair's premises. He was taken aback when he saw the C5 for the first time, as he had been expecting a \"proper\" electric car. He said later that he thought \"the concept looked futuristic but was short on practicality. There were no instruments, nowhere to put anything and no security features.\" Desbarats told Sinclair that the design would have to be redone from scratch, \"asking what we were doing about visibility, rear view mirrors, range indications ...\". It was far too late for this, however; all the key design decisions had already been made. Desbarats told Sinclair that he would need four months to revisit the design and was given eight weeks instead. He created the styling that was used for the final production model of the C5, with wheel trims and a small luggage compartment being added subsequently. Desbarats was also responsible for the creation of the High-Vis Mast accessory, as he felt uncomfortable being so close to the ground with other drivers potentially not being able to see him. He later described his contribution as \"convert[ing] an ugly pointless device into a prettier, safer, and more usable pointless device\".\n\nThe chassis of the C5 consists of two identical metal pressings which are joined at top and bottom with a closing plate at the rear. It lacks a separate suspension system, instead relying on the chassis structure having enough torsional flex. Its motor was produced in Italy by Polymotor, a subsidiary of the Dutch company Philips. Although it was later famously said that the C5 was powered by a washing machine motor, the motor was in fact developed from a design produced to drive a truck cooling fan. Lotus provided the gearbox and a rear axle based on a design for car steering columns. The C5's electronics were produced by MetaLab, a Sinclair spin-off. The wheels were assembled from tyres made in Taiwan and wheels from Italy. Oldham Batteries provided a lead-acid battery developed for Sinclair that could manage more than the 300 charge-discharge cycles that had originally been specified.\n\nThe bodywork was made from two injection-moulded polypropylene shells supplied by three manufacturers; J.J. Harvey of Manchester made the moulds, Linpac provided the shells, and ICI supplied the raw material. According to Rodney Dale, the upper shell mould was \"one of the largest – if not the largest – injection mouldings of its type in the UK: possibly even in the world\". The manufacturing process reflected Sinclair's ambition for the C5 production line. A single mould set was capable of producing up to 4,000 parts every week. The two parts of the shell were joined together by wrapping a tape around the joint, aligning them on a jig, pressing them together and passing an electric current through the tape to heat and melt it. The same process was used to make the front and rear bumper assemblies of the Austin Maestro and only took about 70 seconds to complete. Although Sinclair had considered producing the C5 at the DeLorean plant at Dunmurry in Northern Ireland, which had one of Europe's most advanced automated plastic-body manufacturing facilities, this was not to be, as the DeLorean Motor Company failed in a controversial bankruptcy that resulted in the plant's closure.\n\nInstead, the work of assembling the C5 was given to The Hoover Company in the spring of 1983. The Welsh Development Agency (WDA) approached Hoover to ask them if they would be interested becoming the principal subcontractor for Sinclair, \"who are working on an electric car, and as a by-product of the research have designed an electrically assisted bicycle. They are looking for a subcontractor to whom they can entrust the assembly.\" The proposal suited all sides. The WDA was keen to support the Hoover washing machine factory at Merthyr Tydfil, situated in the economically depressed South Wales Valleys. Hoover was enticed by Sinclair's projections of sales of 200,000 units a year, increasing to 500,000. Sinclair saw Hoover's plant and expertise as a good match for their fabrication techniques. A contract was signed within a few months.\n\nThe C5 was produced in great secrecy in a separate part of the Hoover factory with its own duplicate facilities. At first the work was carried out by a small team of people in a sealed room, but as production ramped up Hoover installed two production lines in building MP7, connected to the main factory by an underground tunnel. A rolling testing stand was located at the end of the production line to test each completed C5 for faults. A mechanical arm simulated the weight of a person weighing and the vehicle's brakes were tested under load. At the end of the process the C5s which had passed testing were rolled into cardboard boxes and loaded straight onto distribution lorries in stacks. Around £100,000 was spent to set up the factory.\n\nDistribution centres were set up in Hayes in Middlesex, Preston in Lancashire and Oxford to handle the C5s. Hoover arranged for 19 of its service offices around the UK – responsible for maintaining customers' vacuum cleaners and washing machines – to also maintain C5s and provide spare parts. The C5's major consumable item, the battery, was to be supported by 300 branches of Comet and Woolworths.\n\nHoover trained its engineers to produce C5s and tested its manufacturing processes by assembling, dismantling and re-assembling 100 C5s. Full production began in November 1984 and by early January 1985 over 2,500 C5s had been manufactured. Each production line could produce 50 vehicles an hour, and Hoover were capable of producing up to 8,000 C5s per week.\n\nThe news of Sinclair's C5 project came as a surprise when it became public and attracted considerable interest, as well as scepticism. \"The Economist\" reported in June 1983 that carmakers were \"startled\" but cautious about Sinclair's prospects; as one competitor put it, \"If it were anyone but Sinclair, we'd say he was bonkers\". \"The Economist\" asked, \"Can a man who has made a fortune out of calculators and computers, and could double it on flatscreen televisions, be that crazy?\" and wondered whether he was \"making a ghastly mistake\", a prediction that industry insiders thought was likely.\n\nThe C5 was launched on 10 January 1985 at Alexandra Palace in North London. The event was staged in Sinclair's usual glitzy style, with women handing out press packs and a variety of promotional giveaways: magazines, hats, pullovers, T-shirts, key rings, sun visors, badges, mugs, bags, and even a C5 video game. The vehicle was given a dramatic unveiling; six C5s driven by women dressed in grey and yellow burst out of six cardboard boxes, drove around the arena, and lined up side by side. Sinclair announced the launch of a £3 million, three-month-long advertising print and television advertising campaign. The C5 would be available initially by mail order at a cost of £399 and would subsequently be sold via high-street stores.\n\nSinclair issued a glossy sales brochure characterising the vehicle as a part of an ongoing exercise in \"cutting giants down to size, turning impersonal tyrants into personal servants\". The brochure highlighted Sinclair's achievements in producing affordable pocket calculators, home computers, and pocket televisions and declared, \"with the C5, Sinclair Vehicles puts personal, private transport back where it belongs – in the hands of the individual.\" The photographs accompanying the text showed housewives and teenagers driving the C5 to shops, railway stations, and sports fields – in the words of technology writers Ian Adamson and Richard Kennedy, \"a blue-sky suburbia exclusively populated by electric trikes and their drivers\".\n\nThe press was given an opportunity to try out the C5 but this proved to be, as Adamson and Kennedy put it, \"an unqualified disaster\". A large number of the demonstration machines did not work, as the assembled journalists soon discovered. \"The Sunday Times\" called the C5 a \"Formula One bath-chair\"; its reporter \"had travelled five yards outdoors when everything went phut and this motorised, plastic, lozenge rolled to a halt with all the stationary decisiveness of a mule\". \"The Guardian\"'s reporter had a flat battery after only seven minutes, while \"Your Computer\" found that the C5 could not cope with the slopes at Alexandra Palace: \"The 250 watt electric motor which drives one of the back wheels proved incapable of powering the C5 up even the gentlest slopes without using pedal power. The tricycle was soon making a plaintive \"peep, peep\" noise signalling that the engine had overheated.\" Even the distinguished former racing driver Stirling Moss ran into problems when he tried out the C5 on the roads around Alexandra Palace. The Canadian newspaper \"The Globe and Mail\" reported that while he had started out well, \"a jaunty smile on [his] face as he braved some of the worst exhaust fumes in the world spluttering almost directly into his face from trucks he could almost drive underneath\", he ran into problems when he reached a hill: \"It was at this point that he realised the battery had gone dead. On a cold and foggy London day, the great man was visibly sweating.\"\n\nThe timing and location of the launch event – in the middle of winter, on the top of a snow- and ice-covered hill – later prompted criticism even from Sinclair executives, who admitted off the record that spring conditions might have been better for a vehicle with so little protection from the British climate. The \"Financial Times\" called it \"the worst possible timing to launch what was proclaimed to be a serious, road-going vehicle\". Sinclair's biographer Rodney Dale describes it as \"a calculated (or miscalculated) risk\", pointing out that production was already underway, details were beginning to leak out to the press and \"the launch could hardly have been held up until the possibility of a bright spring day\". He justified the choice of January as being necessitated by a need to release the C5 \"as soon as possible lest the erroneous speculation should have done more harm than good\". Rob Gray offers an alternative explanation, that the launch date had been brought forward because Sinclair's development funds were running low.\n\nIt soon became apparent that the C5 faced more serious problems with public perception than merely a botched launch event. Media reactions to the C5 were generally negative when the first reviews appeared over the following days. As the \"Financial Times\" observed, \"the few hardy journalists who ventured out on the roads returned shivering and dubious about the C5's abilities in such harsh conditions.\"\n\nA common concern was that it was simply too vulnerable in traffic. \"Your Computer\" commented that \"a periscope would be handy if you intended driving the C5 on busy roads since your head is only at bonnet level.\" \"The Guardian's\" motoring correspondent wrote of her \"grave misgivings about its use in congested traffic ... On a sharp turn it too easily lifts a rear wheel, is hazardously silent, and low down. It disappears below a car driver's sight-line when pulling up alongside. The prospect of these vehicles merging into heavy traffic, dwarfed by heavy lorries, buses, and cars, is worrying. Their low speed risks turning them into mobile chicanes for other traffic.\" Another \"Guardian\" writer wrote that he \"would not want to drive [the] C5 in any traffic at all. My head was on a level with the top of a juggernaut's tyres, the exhaust fumes blasted into my face. Even with the minuscule front and rear lights on, I could not feel confident that a lorry driver so high above the ground would see me.\" Sinclair issued a publicity photograph showing the C5's industrial designer, Gus Desbarats, in a C5 alongside a cardboard cutout of an Austin Mini to illustrate that the C5 driver's seated position was actually higher than that of a Mini driver.\n\nAs teenagers were among the target audiences for the C5, some commentators also raised the prospect of (in Adamson and Kennedy's words) \"packs of 14-year-olds terrorising the neighbourhood in their customised C5s\". The secretary of the Cyclists Touring Club raised the prospect of \"kids us[ing] them in a pretty wild way. They may run them over paths and pavements and knock people down.\" Sinclair dismissed such concerns – \"I have qualms about seven-year-olds riding bicycles on the open road, but I have far fewer qualms about a 14-year-old driving one of these\". Teenagers interviewed by \"The Guardian\" were doubtful about whether they would want a C5, commenting that while it was fun to drive they felt insecure in it and preferred their bicycles.\n\nSinclair's claims to have revolutionised the electric vehicle were dismissed by many reviewers; \"Your Computer\" called the C5 \"more of a toy than the 'ideal solution for all types of local journey' which the brochure claims\". \"The Guardian's\" motoring correspondent also characterised it as \"a delightful toy\" \"The Daily Telegraph\" described it as \"a cleverly-designed 'fun' machine that can hardly be regarded as serious, everyday all-weather transport\", while \"The Engineer\" viewed it as \"a smashing big boy's toy, tough enough to take teenage thrashing and possibly a serious vehicle for fit adults to nip out in for the Sunday papers\".\n\nOn the plus side, the C5's handling characteristics were praised by reviewers. \"The Guardian\" called it \"very easy to master once you have become familiar with the under-thigh handlebar steering and the semi-recumbent driving position with feet on bicycle-type pedals.\" The \"Daily Mirror\" described the arrangement as \"surprisingly easy\" to master, although it cautioned that \"on full speed and on full lock it's very easy to tip it onto two wheels.\" The \"Daily Express\" motoring correspondent wrote that he found the C5 \"stable, comfortable and easy to handle\".\n\nThe verdict from motoring organisations, road safety groups, and consumer watchdogs was decidedly negative and probably sealed the C5's fate. The British Safety Council (BSC) tested the C5 at Sinclair Vehicles' headquarters in Warwick and issued a highly critical report to its 32,000 members. Sinclair was furious and announced that he would sue the BSC and its chairman, James Tye, for defamation (although nothing came of it) after Tye told the press: \"I am shattered that within a few days 14-year-old children will be allowed to drive on the road in this Doodle Bug without a licence ... without insurance and without any form of training.\" Several years later, Tye was happy to take responsibility for the C5's failure, describing himself as \"the man entirely to blame for the failure of the Sinclair C5\".\n\nDespite the problems of the press launch day, a more positive response was expected from the 20,000 members of the public who attended the remaining two days of the launch event to try out the C5 on the Alexandra Palace test track. Sinclair reported the day after the event that its switchboard had been overwhelmed by enquirers, and it expected that all 2,700 units from the first production run would be sold by the following Monday. Setting a pattern that would be repeated throughout the C5's short commercial life, this prediction was wildly optimistic; less than 200 were sold during the Alexandra Palace event. However, sales picked up as mail order forms – which had been sent to all of Sinclair's computer customers – were returned with fresh orders. Within four weeks, 5,000 C5s had been sold.\n\nThe C5's users were an eclectic group. They included holiday camps who wanted C5s to rent to campers; the British Royal Family – Princes William and Harry each had one to drive around Kensington Palace before they were old enough to drive; Sir Elton John, who had two; the magician Paul Daniels, who bought a demonstration model he saw being driven around the BBC Television Centre car park; Sir Arthur C. Clarke, who had two shipped out to his home at Colombo in Sri Lanka; and the Mayor of Scarborough, Michael Pitts, who swapped his official Daimler for a C5. However, as \"The Times\" reported, some of the early buyers were disappointed by the vehicle's limitations, citing its slowness, its limited range and its inability to cope with steep hills, which led some people to return their C5s and ask for a refund.\n\nAlthough the C5 reached retail stores at the start of March 1985, sales had tailed off. Sinclair resorted to hiring teams of teenagers to drive around London in C5s to promote the vehicle, at a cost to the company of £20 a day. Similar teams were established in Manchester, Birmingham, and Leeds. The company denied that it was a marketing campaign; a spokesman told \"The Times\" that \"we haven't done ... tests on inner city roads. That is what the team is doing. Marketing is not the prime function but will undoubtedly be a spin-off.\" Sinclair was reported to be surprised at the lack of demand and blamed the press for \"a lack of foresight and pessimistic reporting\". Matters did not improve. The retail chain Comet acquired 1,600 C5s but nine months later most were still unsold.\n\nAdding to Sinclair's problems, production of the C5 had to be halted for three weeks after numerous customers reported that the plastic moulding attached to the gearbox was impairing the performance of their vehicles. 100 Hoover workers were shifted from the C5 production line to work on replacing the faulty mouldings on returned vehicles. Barrie Wills admitted that Sinclair was also taking the opportunity to \"adjust stocks\" in the light of the C5's poor sales. When production resumed a month later it was at only 10% of the previous level, with 90 of the workers being transferred back to the washing machine production lines. Only 100 C5s were now being produced a week, down from the original 1,000. Over 3,000 unsold C5s were piled up in storage at the Hoover factory, with additional unsold stock in 500 retail outlets nationwide.\n\nSinclair tried to put a brave face on it, admitting that \"sales have not been entirely up to expectations\" but claiming to be \"confident of a high level of demand for the vehicle\". A spokesman told the media that \"we expect a rapid rise in sales now that the weather is improving\". Possible sales opportunities were explored in mainland Europe, Asia, and the United States, with Sinclair claiming that he had found \"very big\" levels of interest. Hoover were sufficiently persuaded to allow Sinclair to divert 10 of their employees to modify C5s for overseas export. The bid to sell the C5 abroad failed; the Dutch National Transport Service told Sinclair that the C5 was not suitable for Dutch roads without improvements to its braking system, the addition of more reflectors, and the inclusion of the High-Vis Mast as part of the basic package. Most of the other ten countries that Sinclair inquired of demanded similar changes.\n\nThe C5's reputation received a further battering when major consumer organisations published sceptical evaluations. The Automobile Association questioned many of Sinclair's claims in a report published at the start of May. It found that the range of the vehicle was typically only about 10 miles (16 km) rather than the promised by Sinclair, and reported that the C5's battery ran flat after only on a cold day. The C5's running speed was more usually around than the claimed , while its running costs compared unfavourably to that of a petrol-driven Honda PX50 moped. The stability, general roadworthiness, and especially the safety of the C5 were questioned, and the AA suggested that the High-Vis Mast should be included as part of the standard package. It concluded:\n\nThe Consumers' Association published a critical report on the C5 in the June issue of its magazine \"Which\", concluding that the vehicle was of only limited use and represented poor value for money. All three of the C5s that it tested broke down with a \"major gearbox fault\" and their High-Vis Masts snapped. The longest run between battery charges was only , and a more realistic achievable range was 5–10 miles (8–16 km). It also echoed the AA's concerns about the C5's safety and the omission of the High-Vis Mast from the standard package. The magazine also called the C5 \"too easy to steal\", hardly surprising considering that while a security lock could be used to prevent it being driven away, the C5 was light enough that a would-be thief could simply pick it up and carry it off.\n\nAs the summer of 1985 continued, sales of the C5 remained far below Sinclair's predictions; only 8,000 had been sold by July. In the middle of that month, the Advertising Standards Authority ordered Sinclair to amend or withdraw its advertisements for the C5 after finding that the company's claims about the safety and speed of the C5 either could not be proved or were not justified. Retailers attempted to deal with unsold stocks of C5s by drastically cutting the vehicle's price. Comet first reduced the price to £259.90 but by the end of the year was selling C5s with a complete set of accessories for only £139.99, 65% less than the initial price.\n\nProduction was terminated in August 1985, by which time 14,000 C5s had been assembled. Cashflow problems caused by the paucity of sales caused relations to break down between Sinclair Vehicles and Hoover. In June 1985 Hoover obtained a writ against Sinclair for unpaid debts of over £1.5 million, relating to work carried out over the previous eight months. It did not actually serve the writ but entered negotiations with Sinclair. In mid-August, it publicly announced that it was ceasing production of the C5. A Sinclair spokesman told the media that the halt in production was \"due to a shortage of certain components which are unable to be re-ordered while a financial settlement is pending. Once this has been concluded production is envisaged to recommence.\"\n\nProduction did not recommence and the Hoover production line remained closed permanently. On 19 September 1985, Sinclair Vehicles changed its name to TPD Limited, with a direct subsidiary named Sinclair Vehicles Sales Limited continuing to sell C5s. TPD only lasted until 15 October, when it was placed into receivership. The receivers announced that 4,500 C5s had been sold by Sinclair Vehicles, with another 4,500 remaining in the company's hands. £7.75 million was reportedly owed to creditors, of which £7 million was owed to Sir Clive Sinclair himself in reflection of his personal investment in the project. Hoover was not among the creditors, as Sinclair had managed to settle the dispute on terms that neither company would reveal.\n\nOn 5 November, TPD was formally liquidated at a creditors' meeting. It was revealed, to the anger of the creditors, that Sinclair had taken out a £5 million debenture to cover the money that he had put into the company. Ordinary creditors faced little prospect of recovering the £1 million left outstanding. Primary Contact, the marketing agency used by Sinclair to promote the C5, was left with the biggest unpaid bill, of nearly £500,000. The last of the unsold C5s were bought for £75 each by Ellar (Surplus Goods) Ltd of Liverpool, which planned to sell 1,000 of them to an Egyptian businessman for use on a university campus while another 1,500 were intended to be sold in the UK.\n\nMany reasons have been suggested for the failure of Sinclair Vehicles and what Dale calls \"the jigsaw of the C5's disappointment\". One of the receivers of Sinclair Vehicles, John Sapte, suggested that Sinclair had taken the wrong tack with its marketing of the C5: \"It was presented as a serious transport, when perhaps it should have been presented as a luxury product, an up-market plaything.\" Ellar's director Maurice Levensohn took exactly this tack when he purchased Sinclair Vehicles' remaining stock, saying that his company would market them as \"a sophisticated toy\": \"If you were a little boy, wouldn't you want your parents to get you one this Christmas?\" His strategy was notably successful; Ellar sold nearly 7,000 C5s at up to £700 each, far more (and at a higher price) than Sinclair had ever managed.\n\nSome commentators attributed the C5's failure to problems with Sinclair's marketing strategy; only a year after the demise of Sinclair Vehicles, the \"Globe and Mail\" newspaper called it \"one of the great marketing bombs of postwar British industry\". Andrew P. Marks of Paisley College of Technology criticises Sinclair's marketing strategy as confused; the C5 promotional brochure depicts it as a leisure vehicle, showing boys in C5s at a football pitch, women in C5s on a suburban road, and so on, while the text suggests that the C5 is a serious substitute for a car. He concludes that the C5 was poorly defined, appearing to be \"trying to grasp at two different markets\" but was unable to appeal to either, and so failed to take off. The fact that it was initially only available via mail order was also a mistake, in Marks' view, as it meant that no physical inspection of the product could be made before purchasing it. This was a serious deterrent to consumers as it made the C5 a much more risky purchase.\n\nThe design researcher and academic Nigel Cross calls the C5 a \"notorious ... example of failure\" and describes its basic concept as \"wrong\". He points out that the marketing research for the C5 was carried out \"after\" the vehicle's concept had already been decided; he notes that it appears to have been intended \"mainly to aid promotion\" rather than to guide development. Gus Desbarats, the C5's industrial designer, attributes the vehicle's flawed concept to Sinclair operating in a \"bubble\" and believes that Sinclair \"failed to understand the difference between a new market, computing, and a mature one, transport, where there were more benchmarks to compare against\". He comments that the experience of working on the C5 convinced him of the need for industrial designers such as himself to get \"involved early in the innovation process, shaping basic configurations, never again [being] satisfied to simply decorate a fundamentally bad idea\".\n\nSinclair himself said in 2005 that the C5 \"was early for what it was. People reacted negatively and the press didn't help. It was too low down and people felt insecure, hence it got bad press.\" Sam Dawson of \"Classic and Sport Car Magazine\" described the C5 as \"incredibly fun to drive\", suggesting that the safety concerns \"could have been addressed if it wasn't for the fact that it was already doomed as a national joke.\" He noted the disconnect between the media's expectations of a serious electric car and the reality of the C5, which he called \"just a fun way of getting around\". Professor Stuart Cole of the University of South Wales comments that the C5 suffered from the design of the roads and the attitudes of the time, which were not geared towards pedal or electric vehicles: \"In the days before unleaded petrol, your face would have been at the height of every exhaust pipe, and drivers weren't used to having to consider slower-moving cyclists. But with more cycle lanes, better education, and workplaces providing showers, etc., the world now is much more geared up for people looking for alternatives to the car, and hopefully will become even more so in the future.\"\n\nSinclair envisaged producing follow-up vehicles such as the C10, a two-seater city car, and the C15, a four-seater capable of travelling at . As Wills put it at the launch event, \"We're developing a family of traffic-compatible, quiet, economic and pollution-free vehicles for the end of the '80s.\" The C5 was described as \"the baby of the family\". The C10 was intended to be a city car, capable of carrying two passengers at up to in a roofed but open-sided compartment with two wheels at the front and one at the back. Wood Rogers intended it to effectively be an updated version of the Isetta, a 1960s Italian microcar. Sinclair built a full-scale mock-up of it; according to Wood Rogers, \"it looked great. I specified open sides to keep the cost down and having no doors meant it escaped a lot of regulations too.\" The design is strikingly similar to the modern Renault Twizy electric vehicle; Wood Rogers comments that \"you could put the C10 into production today and it would still look contemporary.\"\n\nAt the time of the C5's launch, Sinclair described the C15 as having \"a futuristic design with an elongated 'tear-drop' shape, a lightweight body made of self-coloured polypropylene and a single, possibly 'roller' type rear wheel\". It would have been launched at the 1988 International Motor Show in Birmingham following a development programme costed at £2 million. Unlike the relatively conventional technology used in the C5, Sinclair intended to use sodium sulphur batteries with four times the power-to-weight ratio of lead-acid batteries to give the C15 much greater speed and range – over on a single charge. It would have had approximately the same dimensions as a conventional small car, measuring long, high, and wide. However, it could only have worked if sodium sulphur batteries had realised their promise. In the event they did not, the project could not continue, due to thermal problems. Neither the C10 nor the C15 ever left the drawing board.\n\nAlthough Sinclair went on to produce more (but much smaller) electric vehicles, the C5 debacle did lasting damage to the reputation of subsequent EVs in the UK, which the media routinely compared to the C5. It was not until a highly regarded manufacturer, Toyota, launched a serious and well-received vehicle in the 1990s, the Prius, that the C5 \"jinx\" was finally laid to rest.\n\nIn 2017 Sir Clive's nephew Grant Sinclair presented what he called an updated version of the Sinclair C5 called the Iris eTrike.\n\nDespite its lack of commercial success when it was first released, the C5 gained an unexpected degree of cult status in the later years. Collectors began purchasing them as investment items, reselling them for considerably more than their original retail price. One such investor, Adam Harper, bought 600 C5s from a film company as a speculative investment in 1987. He sold all but four within two years, selling them to customers who wanted a novel or more environmentally friendly form of transportation. He also found willing customers among drivers who had been banned from the road, as the C5 did not need a driving licence or vehicle tax. According to Harper, C5s could be resold for as much as £2,500 – more than six times the original retail price. By 1996, a Special Edition C5 in its original box was reported to be worth more than £5,000 to collectors.\n\nC5 owners began modifying their vehicles to achieve levels of performance far beyond anything envisaged by Sinclair. Adam Harper used one C5 as a stunt vehicle, driving it through a tunnel of fire, and adapted another to run at , aiming to break a world land speed record for a three-wheeled electric vehicle and the British record for any type of electric vehicle. He said later: \"Up to 100 mph it's like you're running on rails, it's really stable. Then at about 110 to 120 mph it starts getting tricky. At that point if a tyre blew up or something happened you would be surely dead.\"\n\nAs quoted in the 1987 \"Guinness Book of Records\" under battery powered vehicle: \"John W. Owen and Roy Harvey travelled 919 miles 1479 km from John O'Groats to Land's End in a Sinclair C5 in 103 hr 15 min on 30 Apr-4 May 1985.\"(8.9 mph average.)\n\nChris Crosskey, an engineer from Abingdon, set a record for the longest journey completed on a C5 on a trip to Glastonbury – away (\"I nearly died of exhaustion\") – and tried three times to drive one from Land's End to John o' Groats, a distance of . Another engineer, Adrian Bennett, fitted a jet engine to his C5, while plumber Colin Furze turned one into a 5 ft (1.5 m)-high \"monster trike\" with wheels and a petrol engine capable of propelling it at .\n\nJohn Otway regularly uses a C5 in his stage show and publicity.\n\n\n\n"}
{"id": "262712", "url": "https://en.wikipedia.org/wiki?curid=262712", "title": "Slash-and-burn", "text": "Slash-and-burn\n\nSlash-and-burn agriculture, or fire–fallow cultivation, is a farming method that involves the cutting and burning of plants in a forest or woodland to create a field called a swidden. (Preparing fields by deforestation is called \"assarting\".)\n\nIn subsistence agriculture, slash-and-burn typically uses little technology. It is often applied in shifting cultivation agriculture (such as in the Amazon rainforest) and in transhumance livestock herding.\n\nSlash-and-burn is used by 200–500 million people worldwide. In 2004 it was estimated that in Brazil alone, 500,000 small farmers each cleared an average of one hectare (2.47105 acres) of forest per year. The technique is not scalable or sustainable for large human populations. Methods such as Inga alley farming and slash-and-char have been proposed as alternatives which would cause less environmental degradation.\n\nHistorically, slash-and-burn cultivation has been practiced throughout much of the world, in grasslands as well as woodlands.\n\nDuring the Neolithic Revolution, which included agricultural advancements, groups of hunter-gatherers domesticated various plants and animals, permitting them to settle down and practice agriculture, which provides more nutrition per hectare than hunting and gathering. This happened in the river valleys of Egypt and Mesopotamia. Due to this decrease in food from hunting, as human populations increased, agriculture became more important. Some groups could easily plant their crops in open fields along river valleys, but others had forests blocking their farming land.\n\nIn this context, humans used slash-and-burn agriculture to clear more land to make it suitable for plants and animals. Thus, since Neolithic times, slash-and-burn techniques have been widely used for converting forests into crop fields and pasture. Fire was used before the Neolithic as well, and by hunter-gatherers up to present times. Clearings created by the fire were made for many reasons, such as to draw game animals and to promote certain kinds of edible plants such as berries.\n\nSlash-and-burn fields are typically used and owned by a family until the soil is exhausted. At this point the ownership rights are abandoned, the family clears a new field, and trees and shrubs are permitted to grow on the former field. After a few decades, another family or clan may then use the land and claim usufructuary rights. In such a system there is typically no market in farmland, so land is not bought or sold on the open market and land rights are traditional. In slash-and-burn agriculture, forests are typically cut months before a dry season. The \"slash\" is permitted to dry and then burned in the following dry season. The resulting ash fertilizes the soil and the burned field is then planted at the beginning of the next rainy season with crops such as upland rice, maize, cassava, or other staples. Most of this work is typically done by hand, using such basic tools such as machetes, axes, hoes, and makeshift shovels. The old American civilizations, like the Inca, Maya, and Aztecs, also used this old agricultural technique.\n\nLarge families or clans wandering in the lush woodlands long continued to be the most common form of society through human prehistory. Axes to fell trees and sickles for harvesting grain were the only tools people might bring with them. All other tools were made from materials they found at the site, such as fire stakes of birch, long rods (Vanko), and harrows made of spruce tops. The extended family conquered the lush virgin forest, burned and cultivated their carefully selected swidden plots, sowed one or more crops, and then proceeded on to forests that had been noted in their wanderings. In the temperate zone, the forest regenerated in the course of a lifetime. So swidden was repeated several times in the same area over the years. But in the tropics the forest floor gradually depleted. It was not only in the moors, as in Northern Europe, but also in the steppe, Savannah, prairie, pampas and barren desert in tropical areas where shifting cultivation is the oldest type of farming. Cultivation is similar to slash-and-burn. (Clark 1952 91–107).\n\nSouthern European Mediterranean climates have favored evergreen and deciduous forests. With slash-and-burn agriculture, this type of forest was less able to regenerate than those north of the Alps. Although in northern Europe one crop was usually harvested before grass was allowed to grow, in southern Europe it was more common to exhaust the soil by farming it for several years.\n\nClassical authors mentioned large forests, with Homer writing about \"wooded Samothrace,\" Zakynthos, Sicily, and other woodlands. These authors indicated that the Mediterranean area once had more forest; much had already been lost, and the remainder was primarily in the mountains.\n\nAlthough parts of Europe aside from the north remained wooded, by the Roman Iron and early Viking Ages, forests were drastically reduced and settlements regularly moved. The reasons for this pattern of mobility, the transition to stable settlements from the late Viking period on, or the transition from shifting cultivation to stationary farming are unknown. From this period, plows are found in graves. Early agricultural peoples preferred good forests on hillsides with good drainage, and traces of cattle enclosures are evident there.\n\nIn Italy, shifting cultivation was a thing of the past by the birth of Christ. Tacitus describes it as a strange cultivation method, practiced by the Germans. In 98 AD, he wrote about the Germans that their fields were proportional to the participating cultivators but their crops were shared according to status. Distribution was simple, because of wide availability; they changed fields annually, with much to spare because they were producing grain rather than other crops. A W Liljenstrand wrote 1857 in his doctoral dissertation, \"About Changing of Soil\" (p. 5 ff.), that Tacitus discusses shifting cultivation: \"arva per annos mutant\". This is the practice of shifting cultivation.\n\nDuring the Migration Period in Europe, after the Roman Empire and before the Viking Age, the peoples of Central Europe moved to new forests after exhausting old parcels. Forests were quickly exhausted; the practice had ended in the Mediterranean, where forests were less resilient than the sturdier coniferous forests of Central Europe. Deforestation had been partially caused by burning to create pasture. Reduced timber delivery led to higher prices and more stone construction in the Roman Empire (Stewart 1956, p. 123). Although forests gradually decreased in northern Europe, they have survived in the Nordic countries.\n\nTribes in pre-Roman Italy (including the Etruscans, Umbrians, Ligurians, Sabines, Latins, Campanians, Apulians, Saliscans, and Sabellians) apparently lived in temporary locations. They cultivated small patches of land, kept sheep and cattle, traded with foreign merchants, and occasionally fought. These Italic groups developed identities as settlers and warriors around 900 BC. They built forts in the mountains which are studied today, as are the ruins of a large Samnite temple and theater at Pietrabbondante.\n\nMany Italic peoples saw benefits in allying with Rome. When the Romans built the \"Via Amerina\" in 241 BC, the Falisci settled in cities on the plains and aided the Romans in road construction; the Roman Senate gradually acquired representatives from Faliscan and Etruscan families, and the Italic tribes became settled farmers.\n\nClassical writers described peoples who practiced shifting cultivation, which characterized the Migration Period in Europe. The exploitation of forests demanded displacement as areas were deforested. Julius Caesar wrote about the Suebi in \"Commentarii de Bello Gallico\" 4.1, \"They have no private and secluded fields (\"privati ac separati agri apud eos nihil est\") ... They cannot stay more than one year in a place for cultivation’s sake\" (\"neque longius anno remanere uno in loco colendi causa licet\"). The Suebi lived between the Rhine and the Elbe. About the Germani, Caesar wrote: \"No one has a particular field or area for himself, for the magistrates and chiefs give year by year to the people and the clans, who have gathered together, as much land and in such places as seem good to them and then make them move on after a year\" (\"Neque quisquam agri modum certum aut fines habet proprios, sed magistratus ac principes in annos singulos gentibus cognationibusque hominum, qui tum una coierunt, a quantum et quo loco visum est agri attribuunt atque anno post alio transire cogunt\" [Book 6.22]).\n\nStrabo (63 BC—c. 20 AD) also writes about the Suebi in his \"Geography\" (VII, 1, 3): \"Common to all the people in this area is that they can easily change residence because of their sordid way of life; they do not cultivate fields or collect property, but live in temporary huts. They get their nourishment from their livestock for the most part, and like nomads, pack all their goods in wagons and go on to wherever they want\". Horace writes in 17 BC (\"Carmen Saeculare\", 3, 24, 9ff.) about the people of Macedonia: \"The proud Getae also live happily, growing free food and cereal for themselves on land they do not want to cultivate for more than a year\" (\"Vivunt et rigidi Getae, / immetata quibus iugera liberas / fruges et Cererem ferunt, / nec cultura placet longior annua\").\nJordanes, of Gothic descent, became a monk in Italy. In his mid-sixth-century AD \"Getica\" (\"De origine actibusque Getarum\"; \"The Origin and Deeds of the Goths\"), he described the large island of Scandza, on which the Goths originated. According to Jordanes, of the tribes living there, some are Adogit from within 40 days of the midnight sun. After the Adogit were the Screrefennae and Suehans, who also lived in the north. The Screrefennae did not raise crops, instead hunting and collecting bird eggs. The Suehans, a semi-nomadic tribe with good horses (comparable to the Thuringii), hunted furs to sell; grain could not be grown so far north. In about 550 AD, Procopius also described a primitive hunting people he called \"Skrithifinoi\": \"Both men and women engaged incessantly just in hunting the rich forests and mountains, which gave them an endless supply of game and wild animals.\"\nThe use of fire in northeastern Sweden changed as agriculture evolved. Although the Sami people did not burn land (since burning killed the lichen required by their reindeer), later farmers frequently used slash-and-burn techniques. The 19th-century Swedish timber industry moved north, clearing the land of trees but leaving waste behind as a fire risk; during the 1870s, fires were frequent. There was a fire in Norrland in 1851, followed by fires in 1868 and 1878; two towns were lost in 1888.\n\nOne culture which flourished in pre-agricultural Europe survives: the Forest Finns in Scandinavia. Martin Tvengsberg, a descendant the Forest Finns, studied them in his capacity as curator of the Hedmark Museum in Norway.\nThe Savo-Karelians had a sophisticated system for cultivating spruce forests. A runic poem about Finland's spruce forests reads, \"Gåivu on mehdien valgoinen valhe\" (\"The birch is the forest’s white lie\"). The best spruce forests reportedly contain birch trees, which grow only after a forest has burned once or twice.\n\nSlash-and-burn may be defined as the large-scale deforestation of forests for agricultural use. Ashes from the trees help farmers by providing nutrients for the soil.\n\nIn industrialized regions, including Europe and North America, the practice was abandoned with the introduction of market agriculture and land ownership. Slash-and-burn agriculture was initially practiced by European pioneers in North America such as Daniel Boone and his family, who cleared land in the Appalachian Mountains during the late 18th and early 19th centuries. However, land cleared by slash-and-burn farmers was eventually taken over by systems of land tenure focusing on long-term improvement and discouraging practices associated with slash-and-burn agriculture.\n\nTelkkämäki Nature Reserve in Kaavi, Finland, is an open-air museum which still practices slash-and-burn agriculture. Farm visitors can see how people farmed when slash-and-burn agriculture became the norm in the Northern Savonian region of eastern Finland beginning in the 15th century. Areas of the reserve are burnt each year.\n\nTribal groups in the northeastern Indian states of Tripura, Arunachal Pradesh, Meghalaya, Mizoram and Nagaland and the Bangladeshi districts of Rangamati, Khagrachari, Bandarban and Sylhet refer to slash-and-burn agriculture as \"jhum\" or \"jhoom\" cultivation. The system involves clearing land, by fire or clear-felling, for economically-important crops such as upland rice, vegetables or fruits. After a few cycles, the land's fertility declines and a new area is chosen. \"Jhum\" cultivation is most often practiced on the slopes of thickly-forested hills. Cultivators cut the treetops to allow sunlight to reach the land, burning the trees and grasses for fresh soil. Although it is believed that this helps fertilize the land, it can leave it vulnerable to erosion. Holes are made for the seeds of crops such as sticky rice, maize, eggplant and cucumber are planted. After considering \"jhum\"s effects, the government of Mizoram has introduced a policy to end the method in the state. Slash-and-burn is typically a type of subsistence agriculture, not focused on a need to sell crops globally; planting decisions are governed by the needs of the family (or clan) for the coming year.\n\nAlthough a solution for overpopulated tropical countries where subsistence agriculture may be the traditional method of sustaining many families, the consequences of slash-and-burn techniques for ecosystems are almost always destructive. This happens particularly as population densities increase, and as a result farming becomes more intensively practiced. This is because as demand for more land increases, the fallow period by necessity declines. The principal vulnerability is the nutrient-poor soil, pervasive in most tropical forests. When biomass is extracted even for one harvest of wood or charcoal, the residual soil value is heavily diminished for further growth of any type of vegetation.\n\nSometimes there are several cycles of slash-and-burn within a few years' time span. For example, in eastern Madagascar, the following scenario occurs commonly. The first wave might be cutting of all trees for wood use. A few years later, saplings are harvested to make charcoal, and within the next year the plot is burned to create a quick flush of nutrients for grass to feed the family zebu cattle. If adjacent plots are treated in a similar fashion, large-scale erosion will usually ensue, since there are no roots or temporary water storage in nearby canopies to arrest the surface runoff. Thus, any small remaining amounts of nutrients are washed away. The area is an example of desertification, and no further growth of any type may arise for generations.\n\nThe ecological ramifications of the above scenario are further magnified, because tropical forests are habitats for extremely biologically diverse ecosystems, typically containing large numbers of endemic and endangered species. Therefore, the role of slash-and-burn is significant in the current Holocene extinction.\n\nSlash-and-char is an alternative that alleviates some of the negative ecological implications of traditional slash-and-burn techniques.\n\n\n"}
{"id": "29720377", "url": "https://en.wikipedia.org/wiki?curid=29720377", "title": "Solar powered stadiums", "text": "Solar powered stadiums\n\nEnergy efficient stadiums is the latest trend of environmentalism in sports. Many stadiums are beginning to take measures to become more environmentally friendly and energy efficient, such as using solar energy to power the stadiums and using reusable raw materials. The first stadium successfully built to use 100 percent solar power is the Kaohsiung National Stadium in Kaohsiung, Taiwan. It earned the nickname of the 'dragon' stadium. Completed in 2009, the 55,000 capacity stadium was built for the World Games, which took place in July 2009. After the conclusion of the World Games, the stadium will be used mainly for rugby games and other athletic events. Japanese architect Toyo Ito designed the stadium which incorporates 8,844 solar panels on the roof. These solar panels provide enough energy to power the 3,300 lights and two giant television screens. The stadium provides so much power, and not all of it is needed to power the stadium. The Taiwanese government plans to sell the excess energy. The stadium will prevent 660 tons of carbon dioxide from being released. Not only is the Kaohsiung National Stadium 100 percent solar powered, but all of the raw materials used in the stadium are reusable.\n\nThe Dragon stadium is not the only sports arena to become energy efficient. The Philadelphia Eagles of the NFL announced a new plan to make their stadium, Lincoln Financial Field, more energy efficient by the 2011-2012 season. Lincoln Financial Field will have 2,500 solar panels and 80 20-foot-high wind turbines installed, as well as a generator that runs on natural gas. Lincoln Financial Field will be the first stadium in the United States capable of generating all of its own electricity.\nThe Eagles hired the Florida-based company Solar Blue to install their solar panels and wind turbines. The team will spend $30 million to have everything installed by the start of the 2011-2012 season. The solar panels and wind turbines will generate about 25 percent of the stadium's energy needs, and the generator will provide the rest. Not only is the Eagles' stadium energy efficient, but it is also eco-friendly. The Eagles directed Aramark, the food service and cleaning contractor at the Eagles' stadium, to use nontoxic cleaning supplies and toilet paper, tissues, and towels which are made from 100 percent recycled fibers. These efforts have allowed the team to recycle over 30 percent of its waste, which has doubled since 2008. Even more, the Eagles compost over 25 tons of organic waste and over 10,000 gallons of grease and kitchen oil were converted into biodiesel last year. These efforts have made the Eagles the \"greenest\" team in the NFL.\n\nMajor League Baseball teams have also joined in the efforts of making their stadiums more energy efficient and eco-friendly. AT&T Park, the home of the San Francisco Giants, uses 590 solar panels to provide a total of 120 kW. This system provides enough electricity to power the stadium's scoreboard, which is equivalent to powering 40 homes.\n\nFenway Park is also increasing their efforts to become more energy efficient. The Boston Red Sox's stadium has solar thermal panels which generate enough heat to provide the stadium with over 37 percent of its hot water. Fenway also composts grass clippings and encourages recycling.\n"}
{"id": "966104", "url": "https://en.wikipedia.org/wiki?curid=966104", "title": "Stonehenge road tunnel", "text": "Stonehenge road tunnel\n\nThe Stonehenge road tunnel is a planned tunnel in Wiltshire, England drawn up by Highways England to upgrade the A303 road. It would move the A303 into a tunnel under the Stonehenge World Heritage Site, completing the removal of traffic begun with the closure of the A344 road. The wider project was designed to improve the landscape around the monument and to improve safety on the A303, and was part of proposals to change the site in other ways including moving the visitors centre.\n\nThe A303 primary route is one of the main routes from London to the South West of England. Sections have been upgraded to dual carriageway status, though one third of the road remains single carriageway. Traffic flows on the A303 between Amesbury and Winterbourne Stoke (the section including Stonehenge) are above the capacity of the road and the Highways Agency expressed concern about safety on this road and the A344. The two roads passed through Stonehenge and land owned by the National Trust with the A303 passing directly south and the A344 directly to the north, with a pedestrian tunnel passing from the Stonehenge visitor centre to the site underneath this road. As part of the development of the proposals, over 50 routes were considered by the Highways Agency.\n\nSince 1991, 51 proposals have been considered for improving the A303 in the area and to remove it from the Stonehenge site.\n\nIn 1995 it was proposed to build a tunnel for the A303 underneath the World Heritage Site. A conference agreed on a 2.5-mile (4 km) bored tunnel; however, the government instead proposed a cut and cover tunnel, with plans being published in 1999. These plans were criticised by the National Trust, Transport 2000 and others who expressed concern that it would cause damage to archaeological remains along the route, destroy ancient sites and not achieve an improvement in the landscape.\n\nIn 2002, new plans for a bored tunnel of 1.3 miles (2.1 km) were announced by the Secretary of State for Transport as part of a 7.7-mile (12.5 km) plan to upgrade the A303 to dual carriageway status, with the tunnel estimated to cost £183 million. This proposal brought further protests from the National Trust, English Heritage, UNESCO, CPRE, the Council for British Archaeology and local groups as the tunnel approach cutting would cut in two a prehistoric track way between Stonehenge and a nearby river. These groups are calling for a tunnel at least 2.9 km long, which would, while being sited within the world heritage site, clear most of the known major artefacts, claiming that if the government goes ahead with the 2.1 km tunnel there may never be another chance to remove the road from the site completely.\n\nIn 2004 a public enquiry required under the Highways Act 1980 was conducted by a planning inspector, Michael Ellison. His enquiry agreed that the government proposals were adequate. The report stated:\n\nbut concluded:\n\nOn 20 July 2005 the tunnel scheme was withdrawn by the Government, partly due to rising costs of construction, which had doubled to £470 million. The Highways Agency continued to list the project as planned, but gave 2008 as the earliest date for the start of construction.\n\nOn 31 October 2005 a Government steering group was set up to look at possible solutions, with the aim of choosing an \"option in keeping with the special requirements of the location that is affordable, realistic and deliverable.\" The review presented five options – the published tunnel scheme, a cut and cover tunnel, a 'partial solution' (involving a roundabout but maintaining the current road), and two overland bypass routes. Some of these plans have been criticised as being damaging to both archaeology and biodiversity, including the stone curlew, barn owls, bats, and the chalk grassland habitat. Five options were considered including diverting the A303 further away and only closing the A344. The group expected to produce a report in 2006, taking into account the results of public consultation which started on 23 January 2006 and ran until 24 April 2006.\n\nOn 6 December 2007, Roads Minister Tom Harris announced that the whole scheme had been cancelled due to increased costs of £540 million. English Heritage expressed disappointment whilst the group Save Stonehenge (now Stonehenge Alliance) were pleased with the outcome. The Highways Agency stated that they would continue to work on small scale improvements to the A303.\n\nA revised proposal, of closing the A344 road between Stonehenge Bottom and Byway 12, and closing part of the B3086 was put forward in 2010. This also included a proposed new roundabout to replace the current Airman's Corner junction and improvements to the Longbarrow Roundabout on the A303.\n\nA planning inquiry to consider the proposal was started in June 2011. In July 2012 work began on the £27 million project which involved the closure and grassing over of part of the A344 and the closing of the underpass beneath the road at the monument entrance.\n\nIn December 2013 the new visitors' centre at Airman's Corner on the A360 was opened. Shuttle buses take visitors to the monument along the old A344 road, a distance of approximately 2.4 km.\n\nAccording to documentation released in response to a Freedom of Information request, in January 2012 local councils and the South West Local Enterprise Partnership met to discuss their proposals for \"a consortium of Local Authorities to develop and take forward a new scheme for improvements to the A303/ A358/A30\" and to \"develop an effective lobbying framework so that we can take a planned approach to raising our profile both nationally, regionally and locally\". In September 2012 a survey conducted by Somerset County Council found that more than 90% of commuters and businesses in the South West back an upgrade of the A303. In April 2013 it was reported that the chancellor was giving consideration to \"...adding lanes to the A303 – known all too well to holidaymakers – which runs from Basingstoke through Wiltshire (past Stonehenge) and Somerset to the South West of England...\"\n\nThe proposal was finally allowed by the Government on 12 January 2017. The Transport Secretary, Chris Grayling, said that \"it will transform the A303, cutting congestion and improving journey times\". Chairman of Amesbury Museum and Heritage Trust, Andy Rhind-Tutt, described the tunnel plan as a \"self-destructing time bomb\" which would \"do nothing\" for traffic problems in the area. The Stonehenge Alliance campaign group repeated their belief that \"any tunnel shorter than 2.7 miles would cause irreparable damage to the landscape\". The group also responded with a statement:\n\nHighways England held consultations on the scheme in 2018. A cost of £1.6 billion and a planned start date in 2021 were indicated. English Heritage, the National Trust and Historic England are quoted as supporting the concept of the tunnel with some concerns about the linking of byways, whilst the Stonehenge Alliance and Friends of the Earth remain opposed, as are the Campaign for Better Transport.\n\n"}
{"id": "16879260", "url": "https://en.wikipedia.org/wiki?curid=16879260", "title": "Tenke Fungurume Mine", "text": "Tenke Fungurume Mine\n\nThe Tenke Fungurume Mine holds one of the world's largest known copper and cobalt resources. The deposits are located within two concessions totaling over within Katanga Province, in the south eastern region of the Democratic Republic of Congo (DRC).\n\nTenke Fungurume is the largest copper mine in the DRC.\n\nThe mining project is a partnership led by Freeport-McMoRan Copper and Gold, the Lundin Mining Corporation, and the government of the Democratic Republic of Congo, through Gécamines.\n\nAs of September 2009 Freeport-McMoRan operated the mine and held 58.8%, Lundin held 24.8% and Gécamines (La Générale des Carrières et des Mines) held the remaining 17.5%. In May 2016, Freeport-McMoRan sold their stake to China Molybdenum (CMOC) for 2.65 Billion USD, as the area holds cobalt for lithium-ion batteries in addition to copper. The contract had yet to be reviewed by the government under a \"revisitation\" process started earlier that year.\n\nThe first phase at the mine cost US$1.8bn to build.\n\nThe first copper cathode was produced in March 2009, and the plant was working at planned capacity by September 2009. \nIn the third quarter of 2009 the cobalt plant and the sulphuric acid plant were commissioned.\nIt is expected that the operation will produce of copper metal and of cobalt annually.\n\n"}
{"id": "1223782", "url": "https://en.wikipedia.org/wiki?curid=1223782", "title": "Triflate", "text": "Triflate\n\nTriflate, also known by the systematic name trifluoromethanesulfonate, is a functional group with the formula CFSO−. The triflate group is often represented by −OTf, as opposed to −Tf (triflyl). For example, \"n\"-butyl triflate can be written as CHCHCHCHOTf.\n\nThe corresponding triflate anion, , is an extremely stable polyatomic ion, being the conjugate base of triflic acid (CFSOH), one of the strongest acids known. It is defined as a superacid, because it is more acidic than pure sulfuric acid.\n\nA triflate group is an excellent leaving group used in certain organic reactions such as nucleophilic substitution, Suzuki couplings and Heck reactions. Since alkyl triflates are extremely reactive in S2 reactions, they must be stored in conditions free of nucleophiles (such as water). The anion owes its stability to resonance stabilization which causes the negative charge to be spread over the three oxygen atoms and the sulfur atom. An additional stabilization is achieved by the trifluoromethyl group as a strong electron-withdrawing group.\n\nTriflates have also been applied as ligands for group 11 and 13 metals along with lanthanides.\n\nLithium triflates are used in some lithium ion batteries as a component of the electrolyte.\n\nA mild triflating reagent is phenyl triflimide or \"N\",\"N\"-bis(trifluoromethylsulfonyl)aniline, where the by-product is [CFSON−Ph].\n\nTriflate salts are thermally very stable with melting points up to 350 °C for sodium, boron and silver salts especially in water-free form. They can be obtained directly from triflic acid and the metal hydroxide or metal carbonate in water. Alternatively, they can be obtained from reacting metal chlorides with neat triflic acid or silver triflate, or from reacting barium triflate with metal sulfates in water:\n\nMetal triflates are used as Lewis acid catalysts in organic chemistry. Especially useful are the lanthanide triflates of the type Ln(OTf) (where Ln = lanthanide). A related popular catalyst scandium triflate is used in such reactions as aldol reactions and Diels-Alder reactions. An example is the Mukaiyama aldol addition reaction between benzaldehyde and the silyl enol ether of cyclohexanone with an 81% chemical yield. The corresponding reaction with the yttrium salt fails:\n\nTriflate is a commonly used counterion for organometallic complexes.\n\n"}
{"id": "9689298", "url": "https://en.wikipedia.org/wiki?curid=9689298", "title": "West Australian Forest Alliance", "text": "West Australian Forest Alliance\n\nThe West Australian Forest Alliance is an organization made up of a number of West Australian environmental activist groups—concerned with the destruction of Old Growth Forests in the South West region.\nIt is a successor to and includes membership of the earlier groups the Campaign to Save Native Forests, South West Forests Defence Foundation, Great Walk Networking, and other member groups of the Conservation Council of Western Australia\n\nAs found on the WAFA website - (Warning: not all of the groups listed were still currently active in 2016).\n\n\n\n\n\n"}
{"id": "4396843", "url": "https://en.wikipedia.org/wiki?curid=4396843", "title": "Wood drying", "text": "Wood drying\n\nWood drying (also seasoning lumber or wood seasoning) reduces the moisture content of wood before its use. When the drying is done in a kiln, the product is known as kiln-dried timber or lumber, whereas air drying is the more traditional method.\n\nThere are two main reasons for drying wood:\nFor some purposes, wood is not dried at all, and is used green. Often, wood must be in equilibrium with the air outside, as for construction wood, or the air indoors, as for wooden furniture.\n\nWood is air-dried or dried in a purpose built oven (kiln). Usually the wood is sawed before drying, but sometimes the log is dried whole.\n\nCase hardening describes lumber or timber that has been dried too rapidly. Wood initially dries from the shell (surface), shrinking the shell and putting the core under compression. When this shell is at a low moisture content it will 'set' and resist shrinkage. The core of the wood is still at a higher moisture content. This core will then begin to dry and shrink. However, any shrinkage is resisted by the already 'set' shell. This leads to reversed stresses; compression stresses on the shell and tension stresses in the core. This results in unrelieved stress called case hardening. Case-hardened [wood] may warp considerably and dangerously when the stress is released by sawing.\n\nWood is divided, according to its botanical origin, into two kinds: softwoods, from coniferous trees, and hardwoods, from broad-leaved trees. Softwoods are lighter and generally simple in structure, whereas hardwoods are harder and more complex. However, in Australia, \"softwood\" generally describes rainforest trees, and \"hardwood\" describes Sclerophyll species (\"Eucalyptus\" \"spp\").\n\nSoftwoods such as pine are typically much lighter and easier to process than hardwoods such as fruit tree wood. The density of softwoods ranges from to , while hardwoods are to . Once dried, both consist of approximately 12% of moisture (\"Desch and Dinwoodie, 1996\"). Because of hardwood's denser and more complex structure, its permeability is much less than that of softwood, making it more difficult to dry. Although there are about a hundred times more species of hardwood trees than softwood trees, the ability to be dried and processed faster and more easily makes softwood the main supply of commercial wood today.\n\nThe timber of living trees and fresh logs contains a large amount of water which often constitutes over 50% of the wood's weight. Water has a significant influence on wood. Wood continually exchanges moisture or water with its surroundings, although the rate of exchange is strongly affected by the degree to which wood is sealed.\n\nWood contains water in three forms:\n\nThe moisture content of wood is calculated by the formula (Siau, 1984):\nHere, formula_2 is the green mass of the wood, formula_3 is its oven dry mass (the attainment of constant mass generally after drying in an oven set at () for 24 hours as mentioned by Walker \"et al.\", 1993). The equation can also be expressed as a fraction of the mass of the water and the mass of the oven dry wood rather than a percentage. For example, (oven dry basis) expresses the same moisture content as 59% (oven dry basis).\n\nStudents in the United Kingdom would recognise this formula written as\n\nWhere the wet weight is the weight of the original 'wet' sample and the dry weight being the weight of the sample after drying in an oven. Moisture contents being expressed as a percentage.\n\nWhen green wood dries, free water from the cell lumina, held by the capillary forces only, is the first to go. Physical properties, such as strength and shrinkage, are generally not affected by the removal of free water. The fibre saturation point (FSP) is defined as the moisture content at which free water should be completely gone, while the cell walls are saturated with bound water. In most types of woods, the fibre saturation point is at 25 to 30% moisture content. Siau (1984) reported that the fibre saturation point formula_5 (kg/kg) is dependent on the temperature T (°C) according to the following equation:\n\nKeey \"et al.\" (2000) use a different definition of the fibre saturation point (equilibrium moisture content of wood in an environment of 99% relative humidity).\n\nMany properties of wood show considerable change as the wood is dried below the fibre saturation point, including:\n\nWood is a hygroscopic substance. It has the ability to take in or give off moisture in the form of vapour. Water contained in wood exerts vapour pressure of its own, which is determined by the maximum size of the capillaries filled with water at any time. If water vapour pressure in the ambient space is lower than vapour pressure within wood, desorption takes place. The largest-sized capillaries, which are full of water at the time, empty first. Vapour pressure within the wood falls as water is successively contained in smaller capillaries. A stage is eventually reached when vapour pressure within the wood equals vapour pressure in the ambient space above the wood, and further desorption ceases. The amount of moisture that remains in the wood at this stage is in equilibrium with water vapour pressure in the ambient space, and is termed the equilibrium moisture content or EMC (Siau, 1984). Because of its hygroscopicity, wood tends to reach a moisture content that is in equilibrium with the relative humidity and temperature of the surrounding air.\nThe EMC of wood varies with the ambient relative humidity (a function of temperature) significantly, to a lesser degree with the temperature. Siau (1984) reported that the EMC also varies very slightly with species, mechanical stress, drying history of wood, density, extractives content and the direction of sorption in which the moisture change takes place (i.e. adsorption or desorption).\n\nWood retains its hygroscopic characteristics after it is put into use. It is then subjected to fluctuating humidity, the dominant factor in determining its EMC. These fluctuations may be more or less cyclical, such as diurnal changes or annual seasonal changes.\n\nTo minimize the changes in wood moisture content or the movement of wooden objects in service, wood is usually dried to a moisture content that is close to the average EMC conditions to which it will be exposed. These conditions vary for interior uses compared with exterior uses in a given geographic location. For example, according to the Australian Standard for Timber Drying Quality (AS/NZS 4787, 2001), the EMC is recommended to be 10% - 12% for the majority of Australian states, although extreme cases are up to 15 to 18% for some places in Queensland, Northern Territory, Western Australia and Tasmania. However, the EMC is as low as 6 to 7% in dry centrally heated houses and offices or in permanently air-conditioned buildings.\n\nThe primary reason for drying wood to a moisture content equivalent to its mean EMC under use conditions is to minimize the dimensional changes (or movement) in the final product.\n\nShrinkage and swelling may occur in wood when the moisture content is changed (Stamm, 1964). Shrinkage occurs as moisture content decreases, while swelling takes place when it increases. Volume change is not equal in all directions. The greatest dimensional change occurs in a direction tangential to the growth rings. Shrinkage from the pith outwards, or radially, is usually considerably less than tangential shrinkage, while longitudinal (along the grain) shrinkage is so slight as to be usually neglected. The longitudinal shrinkage is 0.1% to 0.3%, in contrast to transverse shrinkages, which is 2% to 10%. Tangential shrinkage is often about twice as great as in the radial direction, although in some species it is as much as five times as great. The shrinkage is about 5% to 10% in the tangential direction and about 2% to 6% in the radial direction (Walker \"et al.\", 1993).\n\nDifferential transverse shrinkage of wood is related to:\nWood drying may be described as the art of ensuring that gross dimensional changes through shrinkage are confined to the drying process. Ideally, wood is dried to that equilibrium moisture content as will later (in service) be attained by the wood. Thus, further dimensional change will be kept to a minimum.\n\nIt is probably impossible to completely eliminate dimensional change in wood, but elimination of change in size may be approximated by chemical modification. For example, wood can be treated with chemicals to replace the hydroxyl groups with other hydrophobic functional groups of modifying agents (Stamm, 1964). Among all the existing processes, wood modification with acetic anhydride has been noted for the high anti-shrink or anti-swell efficiency (ASE) attainable without damage to wood. However, acetylation of wood has been slow to be commercialised due to the cost, corrosion and the entrapment of the acetic acid in wood. There is an extensive volume of literature relating to the chemical modification of wood (Rowell, 1983, 1991; Kumar, 1994; Haque, 1997).\n\nDrying timber is one method of adding value to sawn products from the primary wood processing industries. According to the Australian Forest and Wood Products Research and Development Corporation (FWPRDC), green sawn hardwood, which is sold at about $350 per cubic metre or less, increases in value to $2,000 per cubic metre or more with drying and processing. However, currently used conventional drying processes often result in significant quality problems from cracks, both externally and internally, reducing the value of the product. For example, in Queensland (Anon, 1997), on the assumption that 10% of the dried softwood is devalued by $200 per cubic metre because of drying defects, saw millers are losing about $5 million a year. In Australia, the loss could be $40 million a year for softwood and an equal or higher amount for hardwood. Thus, proper drying under controlled conditions prior to use is of great importance in timber use, in countries where climatic conditions vary considerably at different times of the year. \n\nDrying, if carried out promptly after felling of trees, also protects timber against primary decay, fungal stain and attack by certain kinds of insects. Organisms, which cause decay and stain, generally cannot thrive in timber with a moisture content below 20%. Several, though not all, insect pests can live only in green timber.\n\nIn addition to the above advantages of drying timber, the following points are also significant (Walker \"et al.\", 1993; Desch and Dinwoodie, 1996):\n\nPrompt drying of wood immediately after felling therefore significantly upgrades and adds value to raw timber. Drying enables substantial long-term economy by rationalizing the use of timber resources. The drying of wood is thus an area for research and development, which concern many researchers and timber companies around the world.\n\nWater in wood normally moves from zones of higher to zones of lower moisture content (Walker \"et al.\", 1993). Drying starts from the exterior of the wood and moves towards the centre, and drying at the outside is also necessary to expel moisture from the inner zones of the wood. Wood subsequently attains equilibrium with the surrounding air in moisture content.\n\nThe driving force of moisture movement is chemical potential. However, it is not always easy to relate chemical potential in wood to commonly observable variables, such as temperature and moisture content (Keey \"et al.\", 2000). Moisture in wood moves within the wood as liquid or vapour through several types of passageways, based on the nature of the driving force, (e.g. pressure or moisture gradient), and variations in wood structure (Langrish and Walker, 1993), as explained in the next section on driving forces for moisture movement. These pathways consist of cavities of the vessels, fibres, ray cells, pit chambers and their pit membrane openings, intercellular spaces and transitory cell wall passageways.\n\nMovement of water takes place in these passageways in any direction, longitudinally in the cells, as well as laterally from cell to cell until it reaches the lateral drying surfaces of the wood. The higher longitudinal permeability of sapwood of hardwood is generally caused by the presence of vessels. The lateral permeability and transverse flow is often very low in hardwoods. The vessels in hardwoods are sometimes blocked by the presence of tyloses and/or by secreting gums and resins in some other species, as mentioned earlier. The presence of gum veins, the formation of which is often a result of natural protective response of trees to injury, is commonly observed on the surface of sawn boards of most eucalypts. Despite the generally higher volume fraction of rays in hardwoods (typically 15% of wood volume), the rays are not particularly effective in radial flow, nor are the pits on the radial surfaces of fibres effective in tangential flow (Langrish and Walker, 1993).\n\nThe available space for air and moisture in wood depends on the density and porosity of wood. Porosity is the volume fraction of void space in a solid. The porosity is reported to be 1.2 to 4.6% of dry volume of wood cell wall (Siau, 1984). On the other hand, permeability is a measure of the ease with which fluids are transported through a porous solid under the influence of some driving forces, e.g. capillary pressure gradient or moisture gradient. It is clear that solids must be porous to be permeable, but it does not necessarily follow that all porous bodies are permeable. Permeability can only exist if the void spaces are interconnected by openings. For example, a hardwood may be permeable because there is intervessel pitting with openings in the membranes (Keey \"et al.\", 2000). If these membranes are occluded or encrusted, or if the pits are aspirated, the wood assumes a closed-cell structure and may be virtually impermeable. The density is also important for impermeable hardwoods because more cell-wall material is traversed per unit distance, which offers increased resistance to diffusion (Keey \"et al.\", 2000). Hence lighter woods, in general, dry more rapidly than do the heavier woods. The transport of fluids is often bulk flow (momentum transfer) for permeable softwoods at high temperature while diffusion occurs for impermeable hardwoods (Siau, 1984). These mechanisms are discussed below.\n\nThree main driving forces used in different version of diffusion models are moisture content, the partial pressure of water vapour, and the chemical potential (Skaar, 1988; Keey \"et al.\", 2000). These are discussed here, including capillary action, which is a mechanism for free water transport in permeable softwoods. Total pressure difference is the driving force during wood vacuum drying.\n\nCapillary forces determine the movements (or absence of movement) of free water. It is due to both adhesion and cohesion. Adhesion is the attraction between water to other substances and cohesion is the attraction of the molecules in water to each other.\n\nAs wood dries, evaporation of water from the surface sets up capillary forces that exert a pull on the free water in the zones of wood beneath the surfaces. When there is no longer any free water in the wood capillary forces are no longer of importance.\n\nThe chemical potential is explained here since it is the true driving force for the transport of water in both liquid and vapour phases in wood (Siau, 1984). The Gibbs free energy per mole of substance is usually expressed as the chemical potential (Skaar, 1933). The chemical potential of unsaturated air or wood below the fibre saturation point influences the drying of wood. Equilibrium will occur at the equilibrium moisture content (as defined earlier) of wood when the chemical potential of the wood becomes equal to that of the surrounding air. The chemical potential of sorbed water is a function of wood moisture content. Therefore, a gradient of wood moisture content (between surface and centre), or more specifically of activity, is accompanied by a gradient of chemical potential under isothermal conditions. Moisture will redistribute itself throughout the wood until the chemical potential is uniform throughout, resulting in a zero potential gradient at equilibrium (Skaar, 1988). The flux of moisture attempting to achieve the equilibrium state is assumed to be proportional to the difference in chemical potential, and inversely proportional to the path length over which the potential difference acts (Keey \"et al.\", 2000).\n\nThe gradient in chemical potential is related to the moisture content gradient as explained in above equations (Keey \"et al.\", 2000). The diffusion model using moisture content gradient as a driving force was applied successfully by Wu (1989) and Doe \"et al.\" (1994). Though the agreement between the moisture-content profiles predicted by the diffusion model based on moisture-content gradients is better at lower moisture contents than at higher ones, there is no evidence to suggest that there are significantly different moisture-transport mechanisms operating at higher moisture contents for this timber. Their observations are consistent with a transport process that is driven by the total concentration of water. The diffusion model is used for this thesis based on this empirical evidence that the moisture-content gradient is a driving force for drying this type of impermeable timber.\n\nDifferences in moisture content between the surface and the centre (gradient, the chemical potential difference between interface and bulk) move the bound water through the small passageways in the cell wall by diffusion. In comparison with capillary movement, diffusion is a slow process. Diffusion is the generally suggested mechanism for the drying of impermeable hardwoods (Keey \"et al.\", 2000). Furthermore, moisture migrates slowly due to the fact that extractives plug the small cell wall openings in the heartwood. This is why sapwood generally dries faster than heartwood under the same drying conditions.\n\nIt is reported that the ratio of the longitudinal to the transverse (radial and tangential) diffusion rates for wood ranges from about 100 at a moisture content of 5%, to 2 - 4 at a moisture content of 25% (Langrish and Walker, 1993). Radial diffusion is somewhat faster than tangential diffusion. Although longitudinal diffusion is most rapid, it is of practical importance only when short pieces are dried. Generally the timber boards are much longer than in width or thickness. For example, a typical size of a green board used for this research was 6 m long, 250 mm in width and 43 mm in thickness. If the boards are quartersawn, then the width will be in the radial direction whereas the thickness will be in tangential direction, and vice versa for plain-sawn boards. Most of the moisture is removed from wood by lateral movement during drying.\n\nThe chief difficulty experienced in the drying of timber is the tendency of its outer layers to dry out more rapidly than the interior ones. If these layers are allowed to dry much below the fibre saturation point while the interior is still saturated, stresses (called drying stresses) are set up because the shrinkage of the outer layers is restricted by the wet interior (Keey \"et al.\", 2000). Rupture in the wood tissues occurs, and consequently splits and cracks occur if these stresses across the grain exceed the strength across the grain (fibre to fibre bonding).\n\nThe successful control of drying defects in a drying process consists in maintaining a balance between the rate of evaporation of moisture from the surface and the rate of outward movement of moisture from the interior of the wood. The way in which drying can be controlled will now be explained. One of the most successful ways of wood drying or seasoning would be kiln drying, where the wood is placed into a kiln compartment in stacks and dried by steaming, and releasing the steam slowly.\n\nThe external drying conditions (temperature, relative humidity and air velocity) control the external boundary conditions for drying, and hence the drying rate, as well as affecting the rate of internal moisture movement. The drying rate is affected by external drying conditions (Walker \"et al.\", 1993; Keey \"et al.\", 2000), as will now be described.\n\nTemperature: If the relative humidity is kept constant, the higher the temperature, the higher the drying rate. Temperature influences the drying rate by increasing the moisture holding capacity of the air, as well as by accelerating the diffusion rate of moisture through the wood.\nThe actual temperature in a drying kiln is the dry-bulb temperature (usually denoted by Tg), which is the temperature of a vapour-gas mixture determined by inserting a thermometer with a dry bulb. On the other hand, the wet-bulb temperature (TW) is defined as the temperature reached by a small amount of liquid evaporating in a large amount of an unsaturated air-vapour mixture. The temperature sensing element of this thermometer is kept moist with a porous fabric sleeve (cloth) usually put in a reservoir of clean water. A minimum air flow of 2 m/s is needed to prevent a zone of stagnant damp air formation around the sleeve (Walker \"et al.\", 1993). Since air passes over the wet sleeve, water is evaporated and cools the wet-bulb thermometer. The difference between the dry-bulb and wet-bulb temperatures, the wet-bulb depression, is used to determine the relative humidity from a standard hygrometric chart (Walker \"et al.\", 1993). A higher difference between the dry-bulb and wet-bulb temperatures indicates a lower relative humidity. For example, if the dry-bulb temperature is 100 °C and wet-bulb temperature 60 °C, then the relative humidity is read as 17% from a hygrometric chart.\n\nRelative humidity: The relative humidity of air is defined as the partial pressure of water vapour divided by the saturated vapour pressure at the same temperature and total pressure (Siau, 1984). If the temperature is kept constant, lower relative humidities result in higher drying rates due to the increased moisture gradient in wood, resulting from the reduction of the moisture content in the surface layers when the relative humidity of air is reduced. The relative humidity is usually expressed on a percentage basis. For drying, the other essential parameter related to relative humidity is the absolute humidity, which is the mass of water vapour per unit mass of dry air (kg of water per kg of dry air).However, its influenced by the amount of water in the heated air.\n\nAir circulation rate: Drying time and timber quality depend on the air velocity and its uniform circulation. At a constant temperature and relative humidity, the highest possible drying rate is obtained by rapid circulation of air across the surface of wood, giving rapid removal of moisture evaporating from the wood. However, a higher drying rate is not always desirable, particularly for impermeable hardwoods, because higher drying rates develop greater stresses that may cause the timber to crack or distort. At very low fan speeds, less than 1 m/s, the air flow through the stack is often laminar flow, and the heat transfer between the timber surface and the moving air stream is not particularly effective (Walker \"et al.\", 1993). The low effectiveness (externally) of heat transfer is not necessarily a problem if internal moisture movement is the key limitation to the movement of moisture, as it is for most hardwoods (Pordage and Langrish, 1999).\n\nThe timbers are classified as follows according to their ease of drying and their proneness to drying degrade:\n\n\nThe rate at which wood dries depends upon a number of factors, the most important of which are the temperature, the dimensions of the wood, and the relative humidity. Simpson and Tschernitz have developed a simple model of wood drying as a function of these three variables. Although the analysis was done for red oak, the procedure may be applied to any species of wood by adjusting the constant parameters of the model.\n\nSimply put, the model assumes that the rate of change of the moisture content \"M\" with respect to time \"t\" is proportional to how far the wood sample is from its equilibrium moisture content formula_7, which is a function of the temperature \"T\" and relative humidity \"h\":\n\nwhere formula_9 is a function of the temperature \"T\" and a typical wood dimension \"L\" and has units of time. The typical wood dimension is roughly the smallest value of (formula_10) which are the radial, tangential and longitudinal dimensions respectively, in inches, with the longitudinal dimension divided by ten because water diffuses about 10 times more rapidly in the longitudinal direction (along the grain) than in the lateral dimensions. The solution to the above equation is:\n\nWhere formula_12 is the initial moisture content. It was found that for red oak lumber, the \"time constant\" formula_9 was well expressed as:\n\nwhere \"a\", \"b\" and \"n\" are constants and formula_15 is the saturation vapor pressure of water at temperature \"T\". For time measured in days, length in inches, and formula_16 measured in mmHg, the following values of the constants were found for red oak lumber.\n\nSolving for the drying time yields:\n\nFor example, at 150 deg F, using the Arden Buck equation, the saturation vapor pressure of water is found to be about 192 mmHg. The time constant for drying a red oak board at 150 deg F is then formula_18 days, which is the time required to reduce the moisture content to 1/e = 37% of its initial deviation from equilibrium. If the relative humidity is 0.50, then using the Hailwood-Horrobin equation the moisture content of the wood at equilibrium is about 7.4%. The time to reduce the lumber from 85% moisture content to 25% moisture content is then about 4.5 days. Higher temperatures will yield faster drying times, but they will also create greater stresses in the wood due because the moisture gradient will be larger. For firewood, this is not an issue but for woodworking purposes, high stresses will cause the wood to crack and be unusable. Normal drying times to obtain minimal seasoning checks (cracks) in 25mm (1 inch or 4/4 lumber) Red Oak ranges from 22 to 30 days, and in 8/4, (50mm or 2 inch) it will range from 65 to 90 days.\n\nBroadly, there are two methods by which timber can be dried:\n\nAir drying is the drying of timber by exposing it to the air. The technique of air drying consists mainly of making a stack of sawn timber (with the layers of boards separated by stickers) on raised foundations, in a clean, cool, dry and shady place. Rate of drying largely depends on climatic conditions, and on the air movement (exposure to the wind). For successful air drying, a continuous and uniform flow of air throughout the pile of the timber needs to be arranged (Desch and Dinwoodie, 1996).\n\nThe rate of loss of moisture can be controlled by coating the planks with any substance that is relatively impermeable to moisture; ordinary mineral oil is usually quite effective. Coating the ends of logs with oil or thick paint improves their quality upon drying. Wrapping planks or logs in materials which will allow some movement of moisture, generally works very well provided the wood is first treated against fungal infection by coating in petrol/gasoline or oil. Mineral oil will generally not soak in more than 1–2 mm below the surface and is easily removed by planing when the timber is suitably dry. \n\nThe process of artificial or 'oven' drying consists basically of introducing heat. This may be directly, using natural gas and/or electricity or indirectly, through steam-heated heat exchangers. Solar energy is also an option. In the process, deliberate control of temperature, relative humidity and air circulation creates variable conditions to achieve specific drying profiles. To achieve this, the timber is stacked in chambers, which are fitted with equipment to control atmospheric temperature, relative humidity and circulation rate (Walker \"et al.', 1993; Desch and Dinwoodie, 1996).\n\nChamber drying provides a means of overcoming the limitations imposed by erratic weather conditions. With kiln drying, as is the case with air drying, unsaturated air is used as the drying medium. Almost all commercial timbers of the world are dried in industrial kilns. A comparison of air drying, conventional kiln and solar drying is given below:\n\nSignificant advantages of conventional kiln drying include higher throughput and better control of the final moisture content. Conventional kilns and solar drying both enable wood to be dried to any moisture content regardless of weather conditions. For most large-scale drying operations solar and conventional kiln drying are more efficient than air drying.\n\nCompartment-type kilns are most commonly used in timber companies. A compartment kiln is filled with a static batch of timber through which air is circulated. In these types of kiln, the timber remains stationary. The drying conditions are successively varied according to the type of timber being dried. This drying method is well suited to the needs of timber companies, which have to dry timbers of varied species and thickness, including refractory hardwoods that are more liable than other species to check and split.\n\nThe main elements of chamber drying are:\nThroughout the process, it is necessary to keep close control of the moisture content using a moisture meter system in order to reduce over-drying and allow operators to know when to pull the charge. Preferably, this in-kiln moisture meter will have an auto-shutoff feature.\n\nSatisfactory kiln drying can usually be accomplished by regulating the temperature and humidity of the circulating air to control the moisture content of the lumber at any given time. This condition is achieved by applying kiln-drying schedules. The desired objective of an appropriate schedule is to ensure drying lumber at the fastest possible rate without causing objectionable degrade. The following factors have a considerable bearing on the schedules.\n\nConsidering each of the factors, no one schedule is necessarily appropriate, even for similar loads of the same species. This is why there is so much timber drying research focused on the development of effective drying schedules.\n\nA dehumidification chamber can be an unvented system (closed loop) or a partially vented system which uses a heat pump to condense moisture from the air using cold side of the refrigeration process (evaporator.) The heat thus gathered is sent to the hot side of the refrigeration process (condenser) to re-heat the air and returns this drier and warmer air inside the kiln. Fans blow the air through the piles as in a normal kiln. These kilns traditionally operate from 100 °F to 160 °F and use about half the energy of a conventional kiln.\n\nThese kilns can be the fastest to dry and most efficient with energy usage. At a vacuum water boils at a lower temperature. In addition to speed a vacuum kiln can also produce an improved quality in the wood.\n\nA solar kiln is a cross between kiln drying and air drying. These kilns are generally a greenhouse with a high-temperature fan and either vents or a condensing system. Solar kilns are slower and variable due to the weather, but are low cost.\n\nImmersion in running water quickly removes sap and then the wood is air dried. \"...it reduces the elasticity and durability of the wood and also makes it brittle.\" But there are competing perspectives, e.g., \"Duhamel, who made many experiments on this important subject, states, that timber for the joiner's use is best put in water for some time, and afterwards dried; as it renders the timber less liable to warp and crack in drying; but, he adds, 'where strength is required it ought not to be put in water.'\"\n\nSubmersion in boiling water or the application of steam speed the drying of wood. This method is said to cause less shrinkage \"...but it is expensive to use, and reduces the strength and elasticity of the timber.\"\n\nSalt seasoning is the submersion of wood in a solution of urea, sodium nitrate, all of which act as dehydrating agents. Then the wood is air dried.\n\nElectrical seasoning involves running an electric current through the lumber causing heat to build up drying the wood. This method is expensive but is fast and uniform quality.\n\nDrying defects are the most common form of degrade in timber, next to natural problems such as knots (Desch and Dinwoodie, 1996).\nThere are two types of drying defects, although some defects involve both causes:\n\nThe standard organizations in Australia and New Zealand (AS/NZS 4787, 2001) have developed a standard for timber quality. The five measures of drying quality include:\n\nA variety of wood drying kiln technologies exist today: conventional, dehumidification, solar, vacuum and radio frequency.\n\nConventional wood dry kilns (Rasmussen, 1988) are either package-type (sideloader) or track-type (tram) construction. Most hardwood lumber kilns are sideloader kilns in which fork trucks are used to load lumber packages into the kiln. Most softwood lumber kilns are track types in which lumber packages are loaded on kiln/track cars for loading the kiln.\n\nModern high-temperature, high-air-velocity conventional kilns can typically dry green lumber in 10 hours down to a moisture content of 18%. However, 1-inch-thick green Red Oak requires about 28 days to dry down to a moisture content of 8%.\n\nHeat is typically introduced via steam running through fin/tube heat exchangers controlled by on/off pneumatic valves. Less common are proportional pneumatic valves or even various electrical actuators. Humidity is removed via a system of vents, the specific layout of which are usually particular to a given manufacturer. In general, cool dry air is introduced at one end of the kiln while warm moist air is expelled at the other. Hardwood conventional kilns also require the introduction of humidity via either steam spray or cold water misting systems to keep the relative humidity inside the kiln from dropping too low during the drying cycle. Fan directions are typically reversed periodically to ensure even drying of larger kiln charges.\n\nMost softwood lumber kilns operate below temperature. Hardwood lumber kiln drying schedules typically keep the dry bulb temperature below . Difficult-to-dry species might not exceed .\n\nDehumidification kilns are very similar to conventional kilns in basic construction. Drying times are usually comparable. Heat is primarily supplied by an integral dehumidification unit which also serves to remove humidity. Auxiliary heat is often provided early in the schedule where the heat required may exceed the heat generated by the DH unit.\n\nSolar kilns are conventional kilns, typically built by hobbyists to keep initial investment costs low. Heat is provided via solar radiation, while internal air circulation is typically passive.\n\nIn 1949 a Chicago company introduced a wood drying kiln that used infrared lamps that they claimed reduced the standard drying time from 14 days to 45 minutes.\n\nNewer wood drying technologies have included the use of reduced atmospheric pressure to attempt to speed up the drying process. A variety of vacuum technologies exist, varying primarily in the method heat is introduced into the wood charge. Hot water platten vacuum kilns use aluminum heating plates with the water circulating within as the heat source, and typically operate at significantly reduced absolute pressure. Discontinuous and SSV (super-heated steam) use atmosphere to introduce heat into the kiln charge. Discontinuous technology allows the entire kiln charge to come up to full atmospheric pressure, the air in the chamber is then heated, and finally vacuum is pulled. SSV run at partial atmospheres (typically around 1/3 of full atmospheric pressure) in a hybrid of vacuum and conventional kiln technology (SSV kilns are significantly more popular in Europe where the locally harvested wood is easier to dry versus species found in North America). RF/V (radio frequency + vacuum) kilns use microwave radiation to heat the kiln charge, and typically have the highest operating cost due to the heat of vaporization being provided by electricity rather than local fossil fuel or waste wood sources.\n\nValid economic studies of different wood drying technologies are based on the total energy, capital, insurance/risk, environmental impacts, labor, maintenance, and product degrade costs for the task of removing water from the wood fiber. These costs (which can be a significant part of the entire plant costs)involve the differential impact of the presence of drying equipment in a specific plant. An example of this is that every piece of equipment (in a lumber manufacturing plant) from the green trimmer to the infeed system at the planer mill is the \"drying system\". Since thousands of different types of wood products manufacturing plants exist around the globe, and may be integrated (lumber, plywood, paper, etc.) or stand alone (lumber only), the true costs of the drying system can only be determined when comparing the total plant costs and risks with and without drying.\n\nThe total (harmful) air emissions produced by wood kilns, including their heat source, can be significant. Typically, the higher the temperature the kiln operates at, the larger amount of emissions are produced (per pound of water removed). This is especially true in the drying of thin veneers and high-temperature drying of softwoods.\n\n\n"}
