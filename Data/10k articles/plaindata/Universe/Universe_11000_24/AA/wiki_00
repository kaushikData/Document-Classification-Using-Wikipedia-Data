{"id": "28907973", "url": "https://en.wikipedia.org/wiki?curid=28907973", "title": "Abbeystead disaster", "text": "Abbeystead disaster\n\nThe Abbeystead disaster occurred on the evening of 23 May 1984 when a methane gas explosion destroyed a waterworks' valve house at Abbeystead, Lancashire, England. A group of 44 visitors was inside the underground building at the time attending a public presentation by North West Water Authority (NWWA) to demonstrate the operations of the station. Eight were killed instantly by the explosion, and the others were severely injured. The explosion also caused the concrete roof to fall down on to the group, destroying the steel mesh floor and throwing some of the victims into the water chambers below which rapidly filled with river water. Another eight people subsequently died of their injuries in hospital. An 11-year-old boy and his mother were among the dead. The official inquiry into the disaster concluded that the methane had seeped from coal deposits 3,937 feet (1,200 m) below ground and had built up in an empty pipeline. The gas was then ejected into the valve house by the sudden pressure of water as the pumps were switched on. The cause of ignition has never been determined.\n\nThe Abbeystead valve house was constructed as part of NWWA's 'Lancashire conjunctive use scheme,' a water supply project \"to help in meeting the region's expected increases in water demand during the 1980s.\" The scheme involved the daily extraction of up to of water from the River Lune near Lancaster which was then pumped through Abbeystead into the River Wyre. From here it would be extracted to a treatment works to augment the drinking water supply for south Lancashire.\n\nThe visitors were from the village of St. Michael's on Wyre, situated approximately 10 miles from the valve house and which had previously suffered flooding which residents believed was caused by the station pumping water from the River Lune to the River Wyre. The tour of the valve house had been arranged by NWWA to alleviate public concern about the flooding. George Mann, chairman of NWWA, said that the tour was intended to have a \"family flavour.\" The tour commenced at 7.20pm and the explosion occurred approximately 10 minutes later, with the first telephone call to the emergency services logged at 7.37pm. Oliver Chippendale, the supervisor of a pumping station on the River Lune had received a telephone call from George Lacey, the NWWA district manager conducting the tour, asking him to commence pumping, and a second call 5 minutes later saying that no water was coming through and to activate a second, larger, pump. Twenty minutes later Chippendale called Abbeystead to check water was coming through and the telephone was answered by water engineer John Nelson who shouted \"Get help! There has been a terrible disaster!\" The force of the blast had lifted 30 concrete roof beams, each weighing 2½ tons, upwards through the soil landscaping above them before they fell into the chamber below.\n\nIndependent geological and seismic surveys commissioned by NWWA later identified the source of the methane gas as coal seams 3,937 feet (1,200 m) below the pipeline. The gas had collected over millions of years in a natural limestone reservoir, from which it seeped towards the surface through a complex network of geological faults. The cutting of the tunnel had intersected these faults and allowed the gas to seep in through its concrete lining.\n\nAn inquest into the deaths was held at Lancaster in October 1984. The jury returned a majority verdict of accidental death on all 16 victims.\n\nThe Health and Safety Executive investigated this incident and produced a special report in accordance with the Health and Safety at Work etc. Act 1974.\n\nIn March 1987, at Lancaster High Court, the building's designers Binnie & Partners were found to be 55 per cent liable in negligence for failing to exercise \"reasonable care\" in assessing the risk of methane. NWWA was found to be 30 per cent to blame for failing to ensure the plant was safe for visitors and employees by testing for methane and Nuttall's Ltd., who constructed the works, were found 15 per cent liable for failing to carry out systematic tests for methane. All three appealed, and ultimately Binnie & Partners were found solely liable. Leave to appeal to the House of Lords was refused. In February 1989 most of the injured survivors and relatives of those who died accepted out-of-court settlements from Binnie & Partners.\n\n"}
{"id": "14617000", "url": "https://en.wikipedia.org/wiki?curid=14617000", "title": "Aerial cable", "text": "Aerial cable\n\nAn aerial cable or air cable is an insulated cable usually containing all conductors required for an electrical distribution system (typically using aerial bundled cables) or a telecommunication line, which is suspended between utility poles or electricity pylons. As aerial cables are completely insulated there is no danger of electric shock when touching them and there is no requirement for mounting them with insulators on pylons and poles.\nA further advantage is they require less right of way than overhead lines for the same reason. They can be designed as shielded cables for telecommunication purposes. If the cable falls, it may still operate if its insulation is not damaged. \n\nAs aerial cables are installed on pylons or poles, they may be cheaper to install than underground cables, as no work for digging is required, which can be very expensive in rocky areas.\n\nAerial cables are mostly used for telecommunication systems or for power transmissions with voltages below 1000 volts. Aerial cable for voltages up to 69,000 volts has also been built, for the supply of farms, waterworks, transmitters and other facilities outside of urban areas. Aerial cables are not often used in transmission circuits because of the difficulty in insulating such high voltage wire. Because of the proven reliability benefits of insulated aerial cables over traditional air-insulated wire, the Electric Power Institute has been working with utility companies to develop better insulating materials. In 1996 they were able to successfully convert one lower-voltage transmission circuit to insulated cable. \n\n"}
{"id": "7985689", "url": "https://en.wikipedia.org/wiki?curid=7985689", "title": "Astrocaryum vulgare", "text": "Astrocaryum vulgare\n\nAstrocaryum vulgare (common names Tucum or Tucumã-do-Pará in Brazil, Awara and wara in French Guiana, Wara awara in Guyana, awarra in Suriname, Chontilla in Ecuador) is a palm native to Amazon Rainforest vegetation, typical of the Pará state in Brazil. This plant has edible fruit, which are also used for biodiesel production. This plant is cited in \"Flora Brasiliensis\" by Carl Friedrich Philipp von Martius.\nThis species is native to the Amazonian region, possibly to the state of Pará, where it has its center of distribution, and reaches French Guiana and Suriname. It is a characteristic palm of terra firme, low vegetation cover, or even open fields. The tree is 10 to 15 m in height and regenerates easily by growing multiple stems. The tucumã palm is considered a pioneer plant of aggressive growth, has the ability to grow new shoots after a fire, and mainly inhabits secondary forests and pastures. Seeds take up to 2 years to germinate, the plants grow slowly in cultivation, and start to produce fruits after eight years. Its resistance to diseases and high productivity make this species an alternative for the production of biodiesel, since the operating costs of an orderly plantation is much less than that of the oil palm.\nThe seed is covered externally with an orange oily pulp. On average, the fruit weighs 30 g; 34% of this weight corresponds to the external pulp that has 14% to 16% of the oil when it is raw. A mature tree can produce up to 50 kg of fruits per year (25 kg per tree on average), which corresponds to 2.5 kg of pulp oil and 1.5 kg of seed oil. In one hectare, 400 palm clusters can be planted, each cluster with three trunks. In total, this equals 1200 palm trunks and will result in 4.8 tons of fatty material per hectare. It forms clusters that do not need to be replanted.\n\nThe fruit is composed of a woody core almost black in color, containing a white almond paste, oilseed, very hard and covered with a yellow-orange pulp, little consistency and oily. Two types of oils are produced by this fruit: the oil of external pulp and almond oil. The oil is found in the fruit, as follows.\n\nIn French Guiana, at Easter, Guianans eat a traditional dish made with awara dough called Awara broth.\n\nThe seed in the fruit is notably used by indigenous Amazonians to make black rings. In the 1800s, this ring was used as a symbol of marriage for the slaves and natives, who could not afford to purchase gold. In addition, the ring was also a symbol of friendship and of resistance to the established order – the freedom fighters. Now these rings are worn by Catholic missionaries as a symbol of solidarity with the poor and support in the struggle for equality, social justice and human rights.\n\n\n"}
{"id": "1516103", "url": "https://en.wikipedia.org/wiki?curid=1516103", "title": "Basalt fiber", "text": "Basalt fiber\n\nBasalt fiber is a material made from extremely fine fibers of basalt, which is composed of the minerals plagioclase, pyroxene, and olivine. It is similar to fiberglass, having better physicomechanical properties than fiberglass, but being significantly cheaper than carbon fiber. It is used as a fireproof textile in the aerospace and automotive industries and can also be used as a composite to produce products such as camera tripods.\n\nThe technology of production of BNV is a one-stage process: melting, homogenization of basalt and extraction of fibers. Basalt is heated only once, which allows you to get the desired product: BNV. Further processing of BNV into materials is carried out using \"cold technologies\" with low energy costs.\n\nBasalt fiber is made from a single material, crushed basalt, from a carefully chosen quarry source. Basalt of high acidity (over 46% silica content) and low iron content is considered desirable for fiber production. Unlike with other composites, such as glass fiber, essentially no materials are added during its production. The basalt is simply washed and then melted.\n\nThe manufacture of basalt fiber requires the melting of the crushed and washed basalt rock at about . The molten rock is then extruded through small nozzles to produce continuous filaments of basalt fiber. \n\nThe basalt fibers typically have a filament diameter of between 10 and 20 µm which is far enough above the respiratory limit of 5 µm to make basalt fiber a suitable replacement for asbestos. They also have a high elastic modulus, resulting in excellent specific strength—three times that of steel. Thin fiber is usually used for textile applications mainly for production of woven fabric. Thicker fiber is used in filament winding, for example, for production of CNG cylinders or pipes. The thickest fiber is used for pultrusion, geogrid, UD, multiaxial fabric production and in form of chopped strand for concrete reinforcement. One of the most prospective applications for continuous basalt fiber and the most modern trend at the moment is production of basalt rebar that more and more substitutes traditional steel rebar on construction market.\n\nThe table refers to the continuous basalt fiber specific producer. Data from all the manufacturers are different, the difference is sometimes very large values.\nComparison:\n\nThe first attempts to produce basalt fiber were made in the United States in 1923 by Paul Dhe who was granted . These were further developed after World War II by researchers in the USA, Europe and the Soviet Union especially for military and aerospace applications. Since declassification in 1995 basalt fibers have been used in a wider range of civilian applications.\n\nSince October 18, 2017, JV 297.1325800.2017 \"Fibreconcrete constructions with nonmetallic fiber has been put into operation. Design rules, \"which eliminated the legal vacuum in the design of basalt reinforced fiber reinforced concrete. According to paragraph 1.1. the standard extends to all types of non-metallic fibers (polymers, polypropylene, glass, basalt and carbon). When comparing different fibers, it can be noted that polymer fibers are inferior to mineral strengths, but their use makes it possible to improve the characteristics of building composites.\n\n\n"}
{"id": "33933664", "url": "https://en.wikipedia.org/wiki?curid=33933664", "title": "Batoning", "text": "Batoning\n\nBatoning is the technique of cutting or splitting wood by using a baton-sized stick or mallet to repeatedly strike the spine of a sturdy knife, chisel or blade in order to drive it through wood, similar to how a froe is used. The batoning method can be used to make kindling or desired forms such as boards, slats or notches. The practice is most useful for obtaining dry wood from the inside of logs for the purpose of fire making.\n\nTools used in batoning are: a strong, fixed-blade, preferably full tang knife or machete with a thick spine, and a club-sized length of dense or green wood for striking the knife's spine and tip.\n\nThe basic method involves repeatedly striking the spine of the knife to force the middle of the blade into the wood. The tip is then struck, to continue forcing the blade deeper, until a split is achieved.\n\nThis technique is useful for the simple splitting of wood for kindling, to access dry wood within a wet log, and for the production of shingles, slats, or boards. It is also useful for cutting notches, or making clean crosscuts against the grain of the wood. The technique is also especially useful when a chopping tool is not available.\n\nCare must be taken to avoid damage to the knife. Breakage of the blade is a common result of striking the spine of the knife at an angle. If this happens the broken blade can become irretrievably embedded within the split. In a survival situation, this can be catastrophic.\n\n"}
{"id": "48842973", "url": "https://en.wikipedia.org/wiki?curid=48842973", "title": "Blue goo", "text": "Blue goo\n\nBlue goo is a sticky, plasticky, blueish-grey, clay-textured soil derived from a highly weathered serpentinite mélange. The name derives from the soil's color; a result of undergoing anaerobic conditions and becoming gleyed. A greyer variation is called \"grey goo\". Blue goo is primarily found along the Northern California coast.\n\nThe Franciscan Complex is the bedrock from which blue goo is derived. It stretches along the coastline from Central California up to Southern Oregon and contains sheared materials from both the Pacific and North American Plates that have accumulated in the accretionary wedge.\n\nThe rock types that produce blue goo include: greenstones, cherts, basalts, shales, sandstones, schists, and serpenitites. These materials mixed together forming a \"plum pudding\" or a mélange. This mélange decomposed through weathering to form blue goo.\n\nClay soils like blue goo have the highest water-holding capacity when compared with other soils, giving them a low draining capacity. This kind of habitat is unsuitable for most plants, but the Northern California coastline maintains high levels of vegetation year round.\n\nDue to blue goo's clayey texture, it slips when overly saturated. This slippage is increased in heavy rainfall areas and in shallow soils; deep soils have more total pore space and are not as prone to slippage. These features contribute to the landslide-ridden environments found along the Northern Californian coast.\n\nThe Franciscan Complex, from which blue goo is derived, extends from Central California up the coast through parts of Southern Oregon. But blue goo has only been found in two Northern Californian regions located in Humboldt County: the Trinidad region and the Orick region. Blue goo is thought to also be found in the Eel River region and along the Southern Oregon coastline.\n"}
{"id": "26438853", "url": "https://en.wikipedia.org/wiki?curid=26438853", "title": "Chatter Telephone", "text": "Chatter Telephone\n\nThe Chatter Telephone is a pull toy for toddlers 12 to 36 months of age. Introduced in 1962 by the Fisher-Price company as the \"Talk Back Phone\" for infants and children, the Chatter Telephone is a roll along pull toy. It has a smiling face, and when the toy is pulled, it makes a chattering sound and the eyes move up and down. The toy has a rotary dial that rings a bell, and was conceived as a way to teach children how to dial a phone. \n\nThe original version was made of wood, with a polyethylene receiver and cord. In 2000, Fisher-Price changed the rotary dial for a push-button version with lights in an effort to modernize the toy, but consumers complained and the rotary version returned to the market the following year. The Chatter Telephone was designed by Ernest Thornell, whose daughter Tina would drag around a metal phone while playing. This gave him the idea of adding wheels, which with a bent axle permitted the movement of eyes, adding to the \"whimsical\" nature, that Herman Fisher desired of all Fisher-Price toys (from phone conversation with Ernie Thornell and recollections of Herm Fisher by John Smith). \n\nFrom its introduction through the 1970s, the Chatter Telephone was Fisher-Price's best selling product. It has been cited as one of the company's offerings that helped save Fisher Price in the 1990s following a failed attempt to market toys for older children in the late 1980s, and enjoys continuing popularity. It is available both as an authentic reproduction and in a modern form.\n\nIn 1985, Fisher-Price offered to donate a Chatter Telephone, Rock A Stack, and Activity Center to NASA for Senator Jake Garn to play with while on the STS-51-D space shuttle mission. This offer was rejected as NASA felt there was insufficient time to test the toys for safety.\n\nIn 2003, the Chatter Telephone was listed as one of the 100 most memorable toys of the 20th century by the Toy Industry Association.\n\nIn 2005, the Chatter Telephone was chosen as one of Dr. Toy's Best Classic Toys.\n\nChatter Telephone appears in the 2010 animated film \"Toy Story 3\" as a minor character, helping Woody save his friends from Lotso. He is voiced by Teddy Newton and speaks with a film noir style, and a brooklyn accent.\n\nChatter Telephone appears in the 2017 animated film \"The Boss Baby\".\n\nIn the 1999 movie, The Adventures of Elmo in Grouchland, Elmo owns a chatter telephone. \n\nThe Powerpuff hotline, a telephone from \"The Powerpuff Girls\", somewhat bears a few resemblances to a Chatter Telephone.\n\nIn \"The Simpsons\" episode, \"Moe Baby Blues\", the Chatter Telephone appears in Maggie's room.\n\nThe Chatter Telephone influenced a real-life art car created by Howard Davis for his telecommunications company.\n\nA Chatter Telephone appears on the cover of American rock band Thee Oh Sees' 2011 album Castlemania.\n\n"}
{"id": "28001404", "url": "https://en.wikipedia.org/wiki?curid=28001404", "title": "Chord (astronomy)", "text": "Chord (astronomy)\n\nIn the field of astronomy the term chord typically refers to a line crossing an object which is formed during an occultation event. By taking accurate measurements of the start and end times of the event, in conjunction with the known location of the observer and the object's orbit, the length of the chord can be determined giving an indication of the size of the occulting object. By combining observations made from several different locations, multiple chords crossing the occulting object can be determined giving a more accurate shape and size model. This technique of using multiple observers during the same event has been used to derive more sophisticated shape models for asteroids, whose shape can be highly irregular. A notable example of this occurred in 2002 when the asteroid 345 Tercidina underwent a stellar occultation of a very bright star as seen from Europe. During this event a team of at least 105 observers recorded 75 chords across the asteroid's surface allowing for a very accurate size and shape determination.\n\nIn addition to using a known orbit to determine an objects size, the reverse process can also be used. In this usage the occulting object's size is taken to be known and the occultation time can be used to determine the length of the chord the background object traced across the foreground object. Knowing this chord and the foreground object's size, a more precise orbit for the object can be determined.\n\nThis usage of the term \"chord\" is similar to the geometric concept (see: Chord (geometry)). The difference being that in the geometric sense a chord refers to a line segment whose ends lie on a circle, whereas in the astronomical sense the occulting shape is not necessarily circular.\n\nBecause an occultation event for an individual object is quite rare, the process of observing occultation events begins with the creation of a list of candidate targets. The list is generated from a computer by analyzing the orbital motions of a large collection of objects with known orbital parameters. Once a candidate event has been chosen whose ground track passes over the site of an observer, the preparations for the observation begin. A few minutes before the event is expected to happen the observing telescope is targeted to the target star and the star's lightcurve is recorded. The recording of the lightcurve continues during and for a short time after the predicted event. This extra recording time is due in part to uncertainties in the occulting objects orbit but also due to the possibility of detecting other objects orbiting the primary object (for example in the case of a binary asteroid, also the ring system around the planet Uranus was detected this way).\n\nThe exact method of lightcurve determination is dependent on the specific equipment available to the observer and the goals of the observation, however in all occultation events accurate timing is an essential component of the observation process. The exact time that the foreground object eclipses the other can be used to work out a very precise position along the occulting object's orbit. Also, since the duration of the drop in the measured lightcurve gives the object's size and since occultation events typically only last somewhere on the order of a few seconds, very fast integration times are required to allow for high temporal resolution along the lightcurve. A second method of achieving very high temporal accuracy is to actually use a long exposure and allow the target star to drift across the CCD during the exposure. This method, known as the trailed image method, produces a streak along the photograph whose thickness corresponds to the brightness of the target star with the distance along the streak direction indicates time; this allows for very high temporal accuracy even when the target star may be too dim for the method described above using high frequency short exposures. With high enough temporal resolution even the angular size of the background star can be determined.\n\nOnce the lightcurve has been recorded the chord across the occulting object can be determined via calculation. By using the start and end times of the occultation event the position in space of both the observer and the occulting object can be worked out (a process complicated by the fact that both the object and the observer are moving). Knowing these two locations, combined with the direction to the background object, the two endpoints of the chord can be determined using simple geometry.\n\n"}
{"id": "46176581", "url": "https://en.wikipedia.org/wiki?curid=46176581", "title": "Cobalt oxide nanoparticle", "text": "Cobalt oxide nanoparticle\n\nIn materials and electric battery research, cobalt oxide nanoparticles usually refers to particles of cobalt(II,III) oxide of nanometer size, with various shapes and crystal structures.\n\nCobalt oxide nanoparticles have potential applications in lithium-ion batteries and electronic gas sensors.\n\nThe anodes of lithium-ion batteries are often made of oxides of cobalt, nickel, or iron, that can readily and reversibly incorporate lithium ions in their molecular structure. Cobalt oxide nanoparticles, such as nanotubes, offer high surface-to-volume ratio and short path length for lithium cation transport, leading to high reversible capacity and good cycle life. The particles may incorporate other substances, asfor example diphenylalanine/cobalt oxide hybrid nanowires. \nCobalt oxide particles may be anchored on substrates such as graphene to improve the dimensional stability of the anode and to prevent particle aggregation during lithium charge and discharge processes.\n\nHollow nanospheres of cobalt oxide have been investigated as materials for gas sensor electrodes, for the detection of toluene, acetone, and other organic vapors. \n\nCobalt oxide nanoparticles anchored on single-walled carbon nanotubes have been investigated for sensing nitrogen oxides and hydrogen. This application takes advantage of the reactivity between the gas and the oxide, as well as the electrical connection with the substrate (both being p-type semiconductors). Nitrogen oxides react with the oxide as electron acceptors, reducing the electrode's resitance; whereas hydrogen acts as an electron donor, increasing the resistance.\n\nCobalt oxide nanoparticles have been observed to readily enter cells, a property that conceivably could lead to applications in hyperthermic treatment, gene therapy and drug delivery. However, their toxicity is an obstacle that would have to be overcome.\n\nCobalt oxide is often obtained by hydrothermal synthesis in an autoclave.\n\nOne-pot hydrothermal synthesis of metal oxide hollow spheres starts with carbohydrates and metal salts dissolved in water at 100-200 °C. The reaction produces carbon spheres, with metal ions integrated into the hydrophobic shell. The carbon cores are removed by calcination, leaving hollow metal oxide spheres. Surface area and thickness of the shell can be manipulated by varying the carbohydrate to metal salt concentration, as well as the temperature, pressure, and pH of the reaction medium, and the cations of the starting salts. The completion time for the procedure varies from hours to days.\n\nA drawback of this approach is a smaller yield compared to other methods. \n\nAnother route to the synthesis of cobalt oxide nanoparticles is the thremaldecomposition of organometallic compounds. For example, heating the metal salen complex bis(salicylaldehyde)ethylenediiminecobalt(II) (\"Co-salen\") in air to 500 °C. The precursor Co-salen can be obtained by reacting cobalt(II) acetate tetrahydrate in propanol at 50 °C under nitrogen atmosphere with the salen ligand (bis(salicylaldehyde)ethylenediimine).\n\nCobalt oxide/graphene composite are synthesized by first forming cobalt(II) hydroxide on the graphene sheet from a cobalt(II) salt and ammonium hydroxide , which is then heated to 450 °C for two hours to yield the oxide.\n\nLike most reactive cobalt compounds, cobalt oxide nanoparticles are toxic to humans and also aquatic life.\n"}
{"id": "37605500", "url": "https://en.wikipedia.org/wiki?curid=37605500", "title": "Colloidal probe technique", "text": "Colloidal probe technique\n\nThe colloidal probe technique is commonly used to measure interaction forces acting between colloidal particles and/or planar surfaces in air or in solution. This technique relies on the use of an atomic force microscope (AFM). However, instead of a cantilever with a sharp AFM tip, one uses the \"colloidal probe\". The colloidal probe consists of a colloidal particle of few micrometers in diameter that is attached to an AFM cantilever. The colloidal probe technique can be used in the \"sphere-plane\" or \"sphere-sphere\" geometries (\"see figure\"). One typically achieves a force resolution between 1 and 100 pN and a distance resolution between 0.5 and 2 nm.\n\nThe colloidal probe technique has been developed in 1991 independently by Ducker and Butt. Since its development this tool has gained wide popularity in numerous research laboratories, and numerous reviews are available in the scientific literature.\n\nAlternative techniques to measure force between surfaces involve the surface forces apparatus, total internal reflection microscopy, and optical tweezers techniques to with video microscopy.\n\nThe possibility to measure forces involving particles and surfaces directly is essential since such forces are relevant in a variety of processes involving colloidal and polymeric systems. Examples include particle aggregation, suspension rheology, particle deposition, and adhesion processes. One can equally study similar biological phenomena, such as deposition of bacteria or the infection of cells by viruses. Forces are equally most informative to investigate the mechanical properties of interfaces, bubbles, capsules, membranes, or cell walls. Such measurements permit to make conclusions about the elastic or plastic deformation or eventual rupture in such systems.\n\nThe colloidal probe technique provides a versatile tool to measure such forces between a colloidal particle and a planar substrate or between two colloidal particles (see figure above). The particles used in such experiments have typically a diameter between 1–10 μm. Typical applications involve measurements of electrical double layer forces and the corresponding surface potentials or surface charge, van der Waals forces, or forces induced by adsorbed polymers.\n\nThe colloidal probe technique uses a standard AFM for the force measurements. But instead the AFM cantilever with an attached sharp tip one uses the \"colloidal probe\". This colloidal probe is normally obtained by attaching a colloidal particle to a cantilever. By recording the deflection of the cantilever as a function of the vertical displacement of the AFM scanner one can extract the force acting between the probe and the surface as a function of the surface separation. This type of AFM operation is referred to as the \"force mode\". With this probe, one can study interactions between various surfaces and probe particles in the \"sphere-plane geometry\". It is also possible to study forces between colloidal particles by attaching another particle to the substrate and perform the measurement in the \"sphere-sphere geometry\", see figure above.\nThe force mode used in the colloidal probe technique is illustrated in the figure on the left. The scanner is fabricated from piezoelectric crystals, which enable its positioning with a precision better than 0.1 nm. The scanner is lifted towards the probe and thereby one records the scanner displacement \"D\". At the same time, the deflection of the cantilever \"ξ\" is monitored as well, typically with a comparable precision. One measures the deflection by focusing a light beam originating from a non-coherent laser diode to the back of the cantilever and detecting the reflected beam with a split photodiode. The lever signal \"S\" represents the difference in the photocurrents originating from the two halves of the diode. The lever signal is therefore proportional to the deflection \"ξ\".\n\nDuring an approach-retraction cycle, one records the lever signal \"S\" as a function of the vertical displacement \"D\" of the scanner. Suppose for the moment that the probe and the substrate are hard and non-deformable objects and that no forces are acting between them when they are not in contact. In such a situation, one refers to a \"hard-core repulsion\". The cantilever will thus not deform as long not being in contact with the substrate. When the cantilever touches the substrate, its deflection will be the same as the displacement of the substrate. This response is referred to as the \"constant compliance\" or contact region. The lever signal \"S\" as a function of the scanner displacement \"D\" is shown in the figure below. This graph consists of two straight lines resembling a hockey-stick. When the surfaces are not in contact, the lever signal will be denoted as \"S\". This value corresponds to the non-deformed lever. In the constant compliance region, the lever signal is simply a linear function of the displacement, and can be represented as a straight line\nThe parameters \"a\" and \"b\" can be obtained from a least-squares fit of the constant compliance region. The inverse slope \"a\" is also referred to as the optical lever sensitivity. By inverting this relation for the lever signal \"S\", which corresponds to the non-deformed lever, one can accurately obtain the contact point from \"D\" = (\"S\" − \"b\")/\"a\". Depending on the substrate, the precision in determining this contact point is between 0.5–2 nm. In the constant compliance region, the lever deformation is given by\nIn this fashion, one can detect deflections of the cantilever with typical resolution of better than 0.1 nm.\nLet us now consider the relevant situation where the probe and the substrate interact. Let us denote by \"F\"(\"h\") the force between the probe and the substrate. This force depends on the surface separation \"h\".\nIn equilibrium, this force is compensated by the restoring force of the spring, which is given by the Hooke's law\nwhere \"k\" is the spring constant of the cantilever. Typical spring constants of AFM cantilevers are in the range of 0.1−10 N/m. Since the deflection is monitored with a precision better 0.1 nm, one typically obtains a force resolution of 1−100 pN. The separation distance can be obtained from the displacement of the scanner and the cantilever deflection\nFigure below illustrates how the cantilever responds to different force profiles. In the case of a soft repulsive force, the cantilever is repelled from the surface and only slowly approaches the constant compliance region. In such situations, it might be actually difficult to identify this region correctly. When the force is attractive, the cantilever is attracted to the surface and may become unstable. From stability considerations one finds that the cantilever will be unstable provided\nThis instability is illustrated in the right panel of the figure on the right. As the cantilever approaches, the slope of the force curve increases. When the slope becomes larger than the spring constant of the cantilever, the cantilever jumps into contact when the slope of the force curve exceeds the force constant of the cantilever. Upon retraction, the same phenomenon happens, but the point where the cantilever jumps out is reached at a smaller separation. Upon approach and retraction, the system will show a hysteresis. In such situations, a part of the force profile cannot be probed. However, this problem can be avoided by using a stiffer cantilever, albeit at the expense of an inferior force resolution.\n\nThe colloidal probes are normally fabricated by gluing a colloidal particle to a tip-less cantilever with a micromanipulator in air. The subsequent rewetting of the probe may lead to the formation of nanosized bubbles on the probe surface. This problem can be avoided by attaching the colloidal particles under wet conditions in AFM fluid cell to appropriately functionalized cantilevers. While the colloidal probe technique is mostly utilized in the sphere-plane geometry, it can be also used in the sphere-sphere geometry. The latter geometry further requires a lateral centering of the two particles, which can be either achieved with an optical microscope or an AFM scan. The results obtained in these two different geometries can be related with the Derjaguin approximation.\n\nThe force measurements rely on an accurate value of the spring constant of the cantilever. This spring constant can be measured by different techniques. The \"thermal noise\" method is the simplest to use, as it is implemented on most AFMs. This approach relies on the determination of the mean square amplitude of the cantilever displacement due to spontaneous thermal fluctuations. This quantity is related to the spring constant by means of the equipartition theorem. In the \"added mass method\" one attaches a series of metal beads to the cantilever and each case one determines the resonance frequency. By exploiting the relation for an harmonic oscillator between the resonance frequency and the mass added one can evaluate the spring constant as well. The \"frictional force method\" relies on measurement of the approach and retract curves of the cantilever through a viscous fluid. Since the hydrodynamic drag of a sphere close to a planar substrate is known theoretically, the spring constant of the cantilever can be deduced. The \"geometrical method\" exploits relations between the geometry of the cantilever and its elastic properties.\n\nThe separation is normally measured from the onset of the constant compliance region. While the relative surface separation can be determined with a resolution of 0.1 nm or better, the absolute surface separation is obtained from the onset of the constant compliance region. While this onset can be determined for solid samples with a precision between 0.5–2 nm, the location of this onset can be problematic for soft repulsive interactions and for deformable surfaces. For this reason, techniques have been developed to measure the surface separation independently (e.g., total internal reflection microscopy, reflection interference contrast microscopy).\n\nBy scanning the sample with the colloidal probe laterally permits to exploit friction forces between the probe and the substrate. Since this technique exploits the torsion of the cantilever, to obtain quantitative data the torsional spring constant of the cantilever must be determined.\n\nA related technique involving similar type of force measurements with the AFM is the single molecular force spectroscopy. However, this technique uses a regular AFM tip to which a single polymer molecule is attached. From the retraction part of the force curve, one can obtain information about stretching of the polymer or its peeling from the surface.\n\n"}
{"id": "35794150", "url": "https://en.wikipedia.org/wiki?curid=35794150", "title": "Comber Wind Farm", "text": "Comber Wind Farm\n\nThe Comber Wind Farm is a 165.6 megawatt (MW) wind farm in Lakeshore, Ontario, consisting of 72 2.3 MW Siemens SWT 2.3 wind turbines with 101 meter diameter rotors. Construction was completed in January 2012. It is adjacent to the Gosfield Wind Project.\n\n\n"}
{"id": "157616", "url": "https://en.wikipedia.org/wiki?curid=157616", "title": "Composite material", "text": "Composite material\n\nA composite material (also called a composition material or shortened to composite, which is the common name) is a material made from two or more constituent materials with significantly different physical or chemical properties that, when combined, produce a material with characteristics different from the individual components. The individual components remain separate and distinct within the finished structure, differentiating composites from mixtures and solid solutions.\n\nThe new material may be preferred for many reasons: \ncommon examples include materials which are stronger, lighter, or less expensive when compared to traditional materials.\n\nMore recently, researchers have also begun to actively include sensing, actuation, computation and communication into composites, which are known as Robotic Materials.\n\nTypical engineered composite materials include:\n\nComposite materials are generally used for buildings, bridges, and structures such as boat hulls, swimming pool panels, racing car bodies, shower stalls, bathtubs, storage tanks, imitation granite and cultured marble sinks and countertops.\n\nThe most advanced examples perform routinely on spacecraft and aircraft in demanding environments.\n\nThe earliest man-made composite materials were straw and mud combined to form bricks for building construction. Ancient brick-making was documented by Egyptian tomb paintings.\n\nWattle and daub is one of the oldest man-made composite materials, at over 6000 years old. Concrete is also a composite material, and is used more than any other man-made material in the world. , about 7.5 billion cubic metres of concrete are made each year—more than one cubic metre for every person on Earth.\n\n\nConcrete is the most common artificial composite material of all and typically consists of loose stones (aggregate) held with a matrix of cement. Concrete is an inexpensive material, and will not compress or shatter even under quite a large compressive force. However, concrete cannot survive tensile loading(i.e., if stretched it will quickly break apart). Therefore, to give concrete the ability to resist being stretched, steel bars, which can resist high stretching forces, are often added to concrete to form reinforced concrete.\n\nFibre-reinforced polymers (FRP)s include carbon-fibre-reinforced polymer (CFRP) and glass-reinforced plastic (GRP). If classified by matrix then there are thermoplastic composites, short fibre thermoplastics, long fibre thermoplastics or long fibre-reinforced thermoplastics. There are numerous thermoset composites, including paper composite panels. Many advanced thermoset polymer matrix systems usually incorporate aramid fibre and carbon fibre in an epoxy resin matrix.\n\nShape memory polymer composites are high-performance composites, formulated using fibre or fabric reinforcement and shape memory polymer resin as the matrix. Since a shape memory polymer resin is used as the matrix, these composites have the ability to be easily manipulated into various configurations when they are heated above their activation temperatures and will exhibit high strength and stiffness at lower temperatures. They can also be reheated and reshaped repeatedly without losing their material properties. These composites are ideal for applications such as lightweight, rigid, deployable structures; rapid manufacturing; and dynamic reinforcement.\n\nHigh strain composites are another type of high-performance composites that are designed to perform in a high deformation setting and are often used in deployable systems where structural flexing is advantageous. Although high strain composites exhibit many similarities to shape memory polymers, their performance is generally dependent on the fibre layout as opposed to the resin content of the matrix.\n\nComposites can also use metal fibres reinforcing other metals, as in metal matrix composites (MMC) or ceramic matrix composites (CMC), which includes bone (hydroxyapatite reinforced with collagen fibres), cermet (ceramic and metal) and concrete. Ceramic matrix composites are built primarily for fracture toughness, not for strength. Another class of composite materials involve woven fabric composite consisting of longitudinal and transverse laced yarns. Woven fabric composites are flexible as they are in form of fabric.\n\nOrganic matrix/ceramic aggregate composites include asphalt concrete, polymer concrete, mastic asphalt, mastic roller hybrid, dental composite, syntactic foam and mother of pearl. Chobham armour is a special type of composite armour used in military applications.\n\nAdditionally, thermoplastic composite materials can be formulated with specific metal powders resulting in materials with a density range from 2 g/cm³ to 11 g/cm³ (same density as lead). The most common name for this type of material is \"high gravity compound\" (HGC), although \"lead replacement\" is also used. These materials can be used in place of traditional materials such as aluminium, stainless steel, brass, bronze, copper, lead, and even tungsten in weighting, balancing (for example, modifying the centre of gravity of a tennis racquet), vibration damping, and radiation shielding applications. High density composites are an economically viable option when certain materials are deemed hazardous and are banned (such as lead) or when secondary operations costs (such as machining, finishing, or coating) are a factor.\n\nA sandwich-structured composite is a special class of composite material that is fabricated by attaching two thin but stiff skins to a lightweight but thick core. The core material is normally low strength material, but its higher thickness provides the sandwich composite with high bending stiffness with overall low density.\n\nWood is a naturally occurring composite comprising cellulose fibres in a lignin and hemicellulose matrix. Engineered wood includes a wide variety of different products such as wood fibre board, plywood, oriented strand board, wood plastic composite (recycled wood fibre in polyethylene matrix), Pykrete (sawdust in ice matrix), Plastic-impregnated or laminated paper or textiles, Arborite, Formica (plastic) and Micarta. Other engineered laminate composites, such as Mallite, use a central core of end grain balsa wood, bonded to surface skins of light alloy or GRP. These generate low-weight, high rigidity materials.\n\nParticulate composites have particle as filler material dispersed in matrix, which may be nonmetal, such as glass, epoxy. Automobile tire is an example of particulate composite.\n\nAdvanced diamond-like carbon (DLC) coated polymer composites have been reported where the coating increases the surface hydrophobicity, hardness and wear resistance.\n\nFibre-reinforced composite materials have gained popularity (despite their generally high cost) in high-performance products that need to be lightweight, yet strong enough to take harsh loading conditions such as aerospace components (tails, wings, fuselages, propellers), boat and scull hulls, bicycle frames and racing car bodies. Other uses include fishing rods, storage tanks, swimming pool panels, and baseball bats. The Boeing 787 and Airbus A350 structures including the wings and fuselage are composed largely of composites. Composite materials are also becoming more common in the realm of orthopedic surgery, and it is the most common hockey stick material.\n\nCarbon composite is a key material in today's launch vehicles and heat shields for the re-entry phase of spacecraft. It is widely used in solar panel substrates, antenna reflectors and yokes of spacecraft. It is also used in payload adapters, inter-stage structures and heat shields of launch vehicles. Furthermore, disk brake systems of airplanes and racing cars are using carbon/carbon material, and the composite material with carbon fibres and silicon carbide matrix has been introduced in luxury vehicles and sports cars.\n\nIn 2006, a fibre-reinforced composite pool panel was introduced for in-ground swimming pools, residential as well as commercial, as a non-corrosive alternative to galvanized steel.\n\nIn 2007, an all-composite military Humvee was introduced by TPI Composites Inc and Armor Holdings Inc, the first all-composite military vehicle. By using composites the vehicle is lighter, allowing higher payloads. In 2008, carbon fibre and DuPont Kevlar (five times stronger than steel) were combined with enhanced thermoset resins to make military transit cases by ECS Composites creating 30-percent lighter cases with high strength.\n\nPipes and fittings for various purpose like transportation of potable water, fire-fighting, irrigation, seawater, desalinated water, chemical and industrial waste, and sewage are now manufactured in glass reinforced plastics.\n\nComposite materials used in tensile structures for facade application provides the advantage of being translucent. The woven base cloth combined with the appropriate coating allows better light transmission. This provides a very comfortable level of illumination compared to the full brightness of outside.\nComposites are made up of individual materials referred to as constituent materials. There are two main categories of constituent materials: matrix (binder) and reinforcement. At least one portion of each type is required. The matrix material surrounds and supports the reinforcement materials by maintaining their relative positions. The reinforcements impart their special mechanical and physical properties to enhance the matrix properties. A synergism produces material properties unavailable from the individual constituent materials, while the wide variety of matrix and strengthening materials allows the designer of the product or structure to choose an optimum combination.\n\nEngineered composite materials must be formed to shape. The matrix material can be introduced to the reinforcement before or after the reinforcement material is placed into the mould cavity or onto the mould surface. The matrix material experiences a melding event, after which the part shape is essentially set. Depending upon the nature of the matrix material, this melding event can occur in various ways such as chemical polymerization for a thermoset polymer matrix, or solidification from the melted state for a thermoplastic polymer matrix composite.\n\nA variety of moulding methods can be used according to the end-item design requirements. The principal factors impacting the methodology are the natures of the chosen matrix and reinforcement materials. Another important factor is the gross quantity of material to be produced. Large quantities can be used to justify high capital expenditures for rapid and automated manufacturing technology. Small production quantities are accommodated with lower capital expenditures but higher labour and tooling costs at a correspondingly slower rate.\n\nMany commercially produced composites use a polymer matrix material often called a resin solution. There are many different polymers available depending upon the starting raw ingredients. There are several broad categories, each with numerous variations. The most common are known as polyester, vinyl ester, epoxy, phenolic, polyimide, polyamide, polypropylene, PEEK, and others. The reinforcement materials are often fibres but also commonly ground minerals. The various methods described below have been developed to reduce the resin content of the final product, or the fibre content is increased. As a rule of thumb, lay up results in a product containing 60% resin and 40% fibre, whereas vacuum infusion gives a final product with 40% resin and 60% fibre content. The strength of the product is greatly dependent on this ratio.\n\nMartin Hubbe and Lucian A Lucia consider wood to be a natural composite of cellulose fibres in a matrix of lignin.\n\nPolymers are common matrices (especially used for fibre reinforced plastics). Road surfaces are often made from asphalt concrete which uses bitumen as a matrix. Mud (wattle and daub) has seen extensive use. Typically, most common polymer-based composite materials, including fibreglass, carbon fibre, and Kevlar, include at least two parts, the substrate and the resin.\n\nPolyester resin tends to have yellowish tint, and is suitable for most backyard projects. Its weaknesses are that it is UV sensitive and can tend to degrade over time, and thus generally is also coated to help preserve it. It is often used in the making of surfboards and for marine applications. Its hardener is a peroxide, often MEKP (methyl ethyl ketone peroxide). When the peroxide is mixed with the resin, it decomposes to generate free radicals, which initiate the curing reaction. Hardeners in these systems are commonly called catalysts, but since they do not re-appear unchanged at the end of the reaction, they do not fit the strictest chemical definition of a catalyst.\n\nVinyl ester resin tends to have a purplish to bluish to greenish tint. This resin has lower viscosity than polyester resin and is more transparent. This resin is often billed as being fuel resistant, but will melt in contact with gasoline. It tends to be more resistant over time to degradation than polyester resin and is more flexible. It uses the same hardeners as polyester resin (at a similar mix ratio) and the cost is approximately the same.\n\nEpoxy resin is almost transparent when cured. In the aerospace industry, epoxy is used as a structural matrix material or as a structural glue.\n\nShape memory polymer (SMP) resins have varying visual characteristics depending on their formulation. These resins may be epoxy-based, which can be used for auto body and outdoor equipment repairs; cyanate-ester-based, which are used in space applications; and acrylate-based, which can be used in very cold temperature applications, such as for sensors that indicate whether perishable goods have warmed above a certain maximum temperature. These resins are unique in that their shape can be repeatedly changed by heating above their glass transition temperature (T\"\"). When heated, they become flexible and elastic, allowing for easy configuration. Once they are cooled, they will maintain their new shape. The resins will return to their original shapes when they are reheated above their T\"\". The advantage of shape memory polymer resins is that they can be shaped and reshaped repeatedly without losing their material properties. These resins can be used in fabricating shape memory composites.\n\nTraditional materials such as glues, muds have traditionally been used as matrices for papier-mâché and adobe.\n\nCement (concrete), metals, ceramics, and sometimes glasses are employed. Unusual matrices such as ice are sometime proposed as in pykecrete.\n\nReinforcement usually adds rigidity and greatly impedes crack propagation. Thin fibers can have very high strength, and provided they are mechanically well attached to the matrix they can greatly improve the composite's overall properties.\n\nFibre-reinforced composite materials can be divided into two main categories normally referred to as short fibre-reinforced materials and continuous fiber-reinforced materials. Continuous reinforced materials will often constitute a layered or laminated structure. The woven and continuous fiber styles are typically available in a variety of forms, being pre-impregnated with the given matrix (resin), dry, uni-directional tapes of various widths, plain weave, harness satins, braided, and stitched.\n\nThe short and long fibres are typically employed in compression moulding and sheet moulding operations. These come in the form of flakes, chips, and random mate (which can also be made from a continuous fibre laid in random fashion until the desired thickness of the ply / laminate is achieved).\n\nCommon fibres used for reinforcement include glass fibres, carbon fibres, cellulose (wood/paper fibre and straw) and high strength polymers for example aramid. Silicon carbide fibers are used for some high temperature applications.\n\nParticle\n\nParticle reinforcement adds a similar effect to precipitation hardening in metals and ceramics. Large particles impede dislocation movement and crack propagation as well as contribute to the composite's Young's Modulus. In general, particle reinforcement effect on Young's Modulus lies between values predicted by\n\nformula_1\n\nas a lower bound and\n\nformula_2\n\nas an upper bound.\n\nTherefore it can be expressed as a linear combination of contribution from the matrix and some weighted contribution from the particles.\n\nformula_3\n\nWhere K is an experimentally derived constant between 0 and 1. This range of values for K reflects that particle reinforced composites are not characterized by the isostrain condition.\n\nSimilarly, the tensile strength can be modeled in an equation of similar construction where K is a similarly bounded constant not necessarily of the same value of K \n\nformula_4\n\nThe true value of K and K vary based on factors including particle shape, particle distribution, and particle/matrix interface. Knowing these parameters, the mechanical properties can be modeled based on effects from grain boundary strengthening, dislocation strengthening, and Orowan strengthening.\n\nThe most common particle reinforced composite is concrete, which is a mixture of gravel and sand usually strengthened by addition of small rocks or sand. Metals are often reinforced with ceramics to increase strength at the cost of ductility. Finally polymers and rubber are often reinforced with carbon black, commonly used in auto tires.\n\nMany composite layup designs also include a co-curing or post-curing of the prepreg with various other media, such as honeycomb or foam. This is commonly called a sandwich structure. This is a more common layup for the manufacture of radomes, doors, cowlings, or non-structural parts.\n\nOpen- and closed-cell-structured foams like polyvinylchloride, polyurethane, polyethylene or polystyrene foams, balsa wood, syntactic foams, and honeycombs are commonly used core materials. Open- and closed-cell metal foam can also be used as core materials. Recently, 3D graphene structures ( also called graphene foam) have also been employed as core structures. A recent review by Khurram and Xu et al., have provided the summary of the state-of-the-art techniques for fabrication of the 3D structure of graphene, and the examples of the use of these foam like structures as a core for their respective polymer composites.\n\nFabrication of composite materials is accomplished by a wide variety of techniques, including: \n\nComposite fabrication usually involves wetting, mixing or saturating the reinforcement with the matrix, and then causing the matrix to bind together (with heat or a chemical reaction) into a rigid structure. The operation is usually done in an open or closed forming mold, but the order and ways of introducing the ingredients varies considerably.\n\nWithin a mold, the reinforcing and matrix materials are combined, compacted, and cured (processed) to undergo a melding event. After the melding event, the part shape is essentially set, although it can deform under certain process conditions. For a thermoset polymer matrix material, the melding event is a curing reaction that is initiated by the application of additional heat or chemical reactivity such as an organic peroxide. For a thermoplastic polymeric matrix material, the melding event is a solidification from the melted state. For a metal matrix material such as titanium foil, the melding event is a fusing at high pressure and a temperature near the melting point.\n\nFor many moulding methods, it is convenient to refer to one mould piece as a \"lower\" mould and another mould piece as an \"upper\" mould. Lower and upper refer to the different faces of the moulded panel, not the mould's configuration in space. In this convention, there is always a lower mould, and sometimes an upper mould. Part construction begins by applying materials to the lower mould. Lower mould and upper mould are more generalized descriptors than more common and specific terms such as male side, female side, a-side, b-side, tool side, bowl, hat, mandrel, etc. Continuous manufacturing uses a different nomenclature.\n\nThe moulded product is often referred to as a panel. For certain geometries and material combinations, it can be referred to as a casting. For certain continuous processes, it can be referred to as a profile.\n\nVacuum bag moulding uses a flexible film to enclose the part and seal it from outside air. Vacuum bag material is available in a tube shape or a sheet of material. A vacuum is then drawn on the vacuum bag and atmospheric pressure compresses the part during the cure. When a tube shaped bag is used, the entire part can be enclosed within the bag. When using sheet bagging materials, the edges of the vacuum bag are sealed against the edges of the mould surface to enclose the part against an air-tight mould. When bagged in this way, the lower mold is a rigid structure and the upper surface of the part is formed by the flexible membrane vacuum bag. The flexible membrane can be a reusable silicone material or an extruded polymer film. After sealing the part inside the vacuum bag, a vacuum is drawn on the part (and held) during cure. This process can be performed at either ambient or elevated temperature with ambient atmospheric pressure acting upon the vacuum bag. A vacuum pump is typically used to draw a vacuum. An economical method of drawing a vacuum is with a venturi vacuum and air compressor.\n\nA vacuum bag is a bag made of strong rubber-coated fabric or a polymer film used to compress the part during cure or hardening. In some applications the bag encloses the entire material, or in other applications a mold is used to form one face of the laminate with the bag being a single layer to seal to the outer edge of the mold face.\nWhen using a tube shaped bag, the ends of the bag are sealed and the air is drawn out of the bag through a nipple using a vacuum pump. As a result, uniform pressure approaching one atmosphere is applied to the surfaces of the object inside the bag, holding parts together while the adhesive cures. The entire bag may be placed in a temperature-controlled oven, oil bath or water bath and gently heated to accelerate curing.\n\nVacuum bagging is widely used in the composites industry as well. Carbon fibre fabric and fibreglass, along with resins and epoxies are common materials laminated together with a vacuum bag operation.\n\nIn commercial woodworking facilities, vacuum bags are used to laminate curved and irregular shaped workpieces.\n\nTypically, polyurethane or vinyl materials are used to make the bag. A tube shaped bag is open at both ends. The piece, or pieces to be glued are placed into the bag and the ends sealed. One method of sealing the open ends of the bag is by placing a clamp on each end of the bag. A plastic rod is laid across the end of the bag, the bag is then folded over the rod. A plastic sleeve with an opening in it, is then snapped over the rod. This procedure forms a seal at both ends of the bag, when the vacuum is ready to be drawn.\n\nA \"platen\" is sometimes used inside the bag for the piece being glued to lie on. The platen has a series of small slots cut into it, to allow the air under it to be evacuated. The platen must have rounded edges and corners to prevent the vacuum from tearing the bag.\n\nWhen a curved part is to be glued in a vacuum bag, it is important that the pieces being glued be placed over a solidly built form, or have an air bladder placed under the form. This air bladder has access to \"free air\" outside the bag. It is used to create an equal pressure under the form, preventing it from being crushed.\n\nThis process is related to vacuum bag molding in exactly the same way as it sounds. A solid female mold is used along with a flexible male mold. The reinforcement is placed inside the female mold with just enough resin to allow the fabric to stick in place (wet lay up). A measured amount of resin is then liberally brushed indiscriminately into the mold and the mold is then clamped to a machine that contains the male flexible mold. The flexible male membrane is then inflated with heated compressed air or possibly steam. The female mold can also be heated. Excess resin is forced out along with trapped air. This process is extensively used in the production of composite helmets due to the lower cost of unskilled labor. Cycle times for a helmet bag moulding machine vary from 20 to 45 minutes, but the finished shells require no further curing if the molds are heated.\n\nA process using a two-sided mould set that forms both surfaces of the panel. On the lower side is a rigid mould and on the upper side is a flexible membrane made from silicone or an extruded polymer film such as nylon. Reinforcement materials can be placed manually or robotically. They include continuous fibre forms fashioned into textile constructions. Most often, they are pre-impregnated with the resin in the form of prepreg fabrics or unidirectional tapes. In some instances, a resin film is placed upon the lower mould and dry reinforcement is placed above. The upper mould is installed and vacuum is applied to the mould cavity. The assembly is placed into an autoclave. This process is generally performed at both elevated pressure and elevated temperature. The use of elevated pressure facilitates a high fibre volume fraction and low void content for maximum structural efficiency.\n\nRTM is a process using a rigid two-sided mould set that forms both surfaces of the panel. The mould is typically constructed from aluminum or steel, but composite molds are sometimes used. The two sides fit together to produce a mould cavity. The distinguishing feature of resin transfer moulding is that the reinforcement materials are placed into this cavity and the mould set is closed prior to the introduction of matrix material. Resin transfer moulding includes numerous varieties which differ in the mechanics of how the resin is introduced to the reinforcement in the mould cavity. These variations include everything from the RTM methods used in out of autoclave composite manufacturing for high-tech aerospace components to vacuum infusion (for resin infusion see also boat building) to vacuum assisted resin transfer moulding (VARTM). This process can be performed at either ambient or elevated temperature and is suitable for manufacturing high performance composite components in medium volumes (1,000s to 10,000s of parts).\n\nSimilar to the methods performed in Resin Transfer Molding, Light Resin Transfer Molding (Light RTM) involves a closed mold process. A vacuum holds mold A and mold B together to result in two finished sides with fixed thickness levels. Vacuum rings around the tools hold the molds together for this process after dry fiber reinforcements are loaded into mold A before joining with mold B. The air is vacuumed out of the molds with a lower vacuum level, separate from the tooling. After the air is removed the resin is injected into the part. The vacuum remains in effect into the resin is cured.\n\nOther types of fabrication include press moulding, transfer moulding, pultrusion moulding, filament winding, casting, centrifugal casting, continuous casting and slip forming. There are also forming capabilities including CNC filament winding, vacuum infusion, wet lay-up, compression moulding, and thermoplastic moulding, to name a few. The use of curing ovens and paint booths is also needed for some projects.\n\nThe finishing of the composite parts is also critical in the final design. Many of these finishes will include rain-erosion coatings or polyurethane coatings.\n\nThe mold and mold inserts are referred to as \"tooling.\" The mold/tooling can be constructed from a variety of materials. Tooling materials include invar, steel, aluminium, reinforced silicone rubber, nickel, and carbon fibre. Selection of the tooling material is typically based on, but not limited to, the coefficient of thermal expansion, expected number of cycles, end item tolerance, desired or required surface condition, method of cure, glass transition temperature of the material being moulded, moulding method, matrix, cost and a variety of other considerations.\n\nThe physical properties of composite materials are generally not isotropic (independent of direction of applied force) in nature, but they are typically anisotropic (different depending on the direction of the applied force or load). For instance, the stiffness of a composite panel will often depend upon the orientation of the applied forces and/or moments. The strength of a composite is bounded by two loading conditions as shown in the plot to the right. If both the fibres and matrix are aligned parallel to the loading direction, the deformation of both phases will be the same (assuming there is no delamination at the fibre-matrix interface). This isostrain condition provides the upper bound for composite strength, and is determined by the rule of mixtures:\n\nformula_5\n\nwhere \"E\" is the effective composite Young’s modulus, and \"V\" and \"E\" are the volume fraction and Young’s moduli, respectively, of the composite phases.\n\nFor example, a composite material made up of α and β phases as shown in the figure to the right under isostrain, the Young's modulus would be as follows:formula_6where V and V are the respective volume fractions of each phase.\n\nThe lower bound is dictated by the isostress condition, in which the fibres and matrix are oriented perpendicularly to the loading direction:\n\nformula_7\n\nFollowing the example above, if one had a composite material made up of α and β phases under isostress conditions as shown in the figure to the right, the composition Young's modulus would be:formula_8The isostrain condition implies that under an applied load, both phases experience the same strain but will feel different stress. Comparatively, under isostress conditions both phases will feel the same stress but the strains will differ between each phase.\nThough composite stiffness is maximized when fibres are aligned with the loading direction, so is the possibility of fibre tensile fracture, assuming the tensile strength exceeds that of the matrix. When a fibre has some angle of misorientation θ, several fracture modes are possible. For small values of θ the stress required to initiate fracture is increased by a factor of (cos θ) due to the increased cross-sectional area (\"A\" cos θ) of the fibre and reduced force (\"F/\"cos θ) experienced by the fibre, leading to a composite tensile strength of \"σ/\"cos θ where \"σ\" is the tensile strength of the composite with fibres aligned parallel with the applied force.\n\nIntermediate angles of misorientation θ lead to matrix shear failure. Again the cross sectional area is modified but since shear stress is now the driving force for failure the area of the matrix parallel to the fibres is of interest, increasing by a factor of 1/sin θ. Similarly, the force parallel to this area again decreases (\"F/\"cos θ) leading to a total tensile strength of \"τ /\"sinθ cosθ where \"τ\" is the matrix shear strength.\n\nFinally, for large values of θ (near π/2) transverse matrix failure is the most likely to occur, since the fibres no longer carry the majority of the load. Still, the tensile strength will be greater than for the purely perpendicular orientation, since the force perpendicular to the fibres will decrease by a factor of 1/sin θ and the area decreases by a factor of 1/sin θ producing a composite tensile strength of \"σ /\"sinθ where \"σ\" is the tensile strength of the composite with fibres align perpendicular to the applied force.\n\nThe majority of commercial composites are formed with random dispersion and orientation of the strengthening fibres, in which case the composite Young’s modulus will fall between the isostrain and isostress bounds. However, in applications where the strength-to-weight ratio is engineered to be as high as possible (such as in the aerospace industry), fibre alignment may be tightly controlled.\n\nPanel stiffness is also dependent on the design of the panel. For instance, the fibre reinforcement and matrix used, the method of panel build, thermoset versus thermoplastic, and type of weave.\n\nIn contrast to composites, isotropic materials (for example, aluminium or steel), in standard wrought forms, typically have the same stiffness regardless of the directional orientation of the applied forces and/or moments. The relationship between forces/moments and strains/curvatures for an isotropic material can be described with the following material properties: Young's Modulus, the shear Modulus and the Poisson's ratio, in relatively simple mathematical relationships. For the anisotropic material, it requires the mathematics of a second order tensor and up to 21 material property constants. For the special case of orthogonal isotropy, there are three different material property constants for each of Young's Modulus, Shear Modulus and Poisson's ratio—a total of 9 constants to describe the relationship between forces/moments and strains/curvatures.\n\nTechniques that take advantage of the anisotropic properties of the materials include mortise and tenon joints (in natural composites such as wood) and Pi Joints in synthetic composites.\n\nShock, impact, or repeated cyclic stresses can cause the laminate to separate at the interface between two layers, a condition known as delamination. Individual fibres can separate from the matrix e.g. fibre pull-out.\n\nComposites can fail on the microscopic or macroscopic scale. Compression failures can occur at both the macro scale or at each individual reinforcing fibre in compression buckling. Tension failures can be net section failures of the part or degradation of the composite at a microscopic scale where one or more of the layers in the composite fail in tension of the matrix or failure of the bond between the matrix and fibres.\n\nSome composites are brittle and have little reserve strength beyond the initial onset of failure while others may have large deformations and have reserve energy absorbing capacity past the onset of damage. The variations in fibres and matrices that are available and the mixtures that can be made with blends leave a very broad range of properties that can be designed into a composite structure.\nThe best known failure of a brittle ceramic matrix composite occurred when the carbon-carbon composite tile on the leading edge of the wing of the Space Shuttle Columbia fractured when impacted during take-off. It led to catastrophic break-up of the vehicle when it re-entered the Earth's atmosphere on 1 February 2003.\n\nCompared to metals, composites have relatively poor bearing strength.\n\nTo aid in predicting and preventing failures, composites are tested before and after construction. Pre-construction testing may use finite element analysis (FEA) for ply-by-ply analysis of curved surfaces and predicting wrinkling, crimping and dimpling of composites. Materials may be tested during manufacturing and after construction through several nondestructive methods including ultrasonics, thermography, shearography and X-ray radiography, and laser bond inspection for NDT of relative bond strength integrity in a localized area.\n\n\n\n"}
{"id": "1071950", "url": "https://en.wikipedia.org/wiki?curid=1071950", "title": "Conformal fuel tank", "text": "Conformal fuel tank\n\nConformal fuel tanks (CFTs) are additional fuel tanks fitted closely to the profile of an aircraft that extend either the range or \"time on station\" of the aircraft. CFTs have a reduced aerodynamic penalty compared to external drop tanks, and do not significantly increase an aircraft's radar cross-section. Another advantage of CFTs provide is that they do not occupy ordnance hardpoints like drop tanks, allowing the aircraft to carry its full payload.\n\nConformal fuel tanks have the disadvantage that, unlike drop tanks, they cannot be discarded in flight, because they are plumbed into the aircraft and so can only be removed on the ground. As a result, they will impose a slight drag-penalty and minor weight gain on the aircraft even when the tanks are empty, without any benefit. They can also impose slight g-load limits, although not always an absolute issue, the CFTs on the F-15E actually allow the same maneuverability without g-limitations.\n\n\n\n\nDistended internal tanks are fuel tanks that create a bulge from the fuselage or are mounted flush with the fuselage.\n\n\n"}
{"id": "14642741", "url": "https://en.wikipedia.org/wiki?curid=14642741", "title": "Digital image correlation and tracking", "text": "Digital image correlation and tracking\n\nDigital image correlation and tracking is an optical method that employs tracking and image registration techniques for accurate 2D and 3D measurements of changes in images. This method is often used to measure full-field displacement and strains, and it is widely applied in many areas of science and engineering, with new applications being found all the time. Compared to strain gages and extensometers, the amount of information gathered about the fine details of deformation during mechanical tests is increased manifold. \n\nDigital image correlation (DIC) techniques have been increasing in popularity, especially in micro- and nano-scale mechanical testing applications due to its relative ease of implementation and use. Advances in computer technology and digital cameras have been the enabling technologies for this method and while white-light optics has been the predominant approach, DIC can be and has been extended to almost any imaging technology.\n\nThe concept of using cross-correlation to measure shifts in datasets has been known for a long time, and it has been applied to digital images since at least the early 1970s. The present-day applications are almost innumerable and include image analysis, image compression, velocimetry, and strain estimation. Much early work in DIC in the field of mechanics was led by researchers at the University of South Carolina in the early 1980s and has been optimized and improved in recent years. Commonly, DIC relies on finding the maximum of the correlation array between pixel intensity array subsets on two or more corresponding images, which gives the integer translational shift between them. It is also possible to estimate shifts to a finer resolution than the resolution of the original images, which is often called \"subpixel\" registration because the measured shift is smaller than an integer pixel unit. For subpixel interpolation of the shift, there are other methods that do not simply maximize the correlation coefficient. An iterative approach can also be used to maximize the interpolated correlation coefficient by using nonlinear optimization techniques. The nonlinear optimization approach tends to be conceptually simpler, but as with most nonlinear optimization techniques , it is quite slow, and the problem can sometimes be reduced to a much faster and more stable linear optimization in phase space. \n\nThe two-dimensional discrete cross correlation formula_1 can be defined several ways, one possibility being:\n\nHere \"f\"(\"m\", \"n\") is the pixel intensity or the gray-scale value at a point (\"m\", \"n\") in the original image, \"g\"(\"m\", \"n\") is the gray-scale value at a point (\"m\", \"n\") in the translated image, formula_3 and formula_4 are mean values of the intensity matrices \"f\" and \"g\" respectively. \n\nHowever, in practical applications, the correlation array is usually computed using Fourier-transform methods, since the fast Fourier transform is a much faster method than directly computing the correlation.\n\nThen taking the complex conjugate of the second result and multiplying the Fourier transforms together elementwise, we obtain the Fourier transform of the correlogram, formula_6:\n\nwhere formula_8 is the Hadamard product (entry-wise product). It is also fairly common to normalize the magnitudes to unity at this point, which results in a variation called \"phase correlation\".\n\nThen the cross-correlation is obtained by applying the inverse Fourier transform:\n\nAt this point, the coordinates of the maximum of formula_1 give the integer shift:\n\nFor deformation mapping, the mapping function that relates the images can be derived from comparing a set of subwindow pairs over the whole images. (Figure 1). The coordinates or grid points (\"x\", \"y\") and (\"x\", \"y\") are related by the translations that occur between the two images. If the deformation is small and perpendicular to the optical axis of the camera, then the relation between (\"x\", \"y\") and (\"x\", \"y\") can be approximated by a 2D affine transformation such as:\n\nHere \"u\" and \"v\" are translations of the center of the sub-image in the \"X\" and \"Y\" directions respectively. The distances from the center of the sub-image to the point (\"x\", \"y\") are denoted by formula_14 and formula_15. Thus, the correlation coefficient \"r\" is a function of displacement components (\"u\", \"v\") and displacement gradients \n\nDIC has proven to be very effective at mapping deformation in macroscopic mechanical testing, where the application of specular markers (e.g. paint, toner powder) or surface finishes from machining and polishing provide the needed contrast to correlate images well. However, these methods for applying surface contrast do not extend to the application of freestanding thin films for several reasons. First, vapor deposition at normal temperatures on semiconductor grade substrates results in mirror-finish quality films with RMS roughnesses that are typically on the order of several nanometers. No subsequent polishing or finishing steps are required, and unless electron imaging techniques are employed that can resolve microstructural features, the films do not possess enough useful surface contrast to adequately correlate images. Typically this challenge can be circumvented by applying paint that results in a random speckle pattern on the surface, although the large and turbulent forces resulting from either spraying or applying paint to the surface of a freestanding thin film are too high and would break the specimens. In addition, the sizes of individual paint particles are on the order of μms, while the film thickness is only several hundred nanometers, which would be analogous to supporting a large boulder on a thin sheet of paper.\n\nVery recently, advances in pattern application and deposition at reduced length scales have exploited small-scale synthesis methods including nano-scale chemical surface restructuring and photolithography of computer-generated random specular patterns to produce suitable surface contrast for DIC. The application of very fine powder particles that electrostatically adhere to the surface of the specimen and can be digitally tracked is one approach. For Al thin films, fine alumina abrasive polishing powder was initially used since the particle sizes are relatively well controlled, although the adhesion to Al films was not very good and the particles tended to agglomerate excessively. The candidate that worked most effectively was a silica powder designed for a high temperature adhesive compound (Aremco, inc.), which was applied through a plastic syringe. A light blanket of powder would coat the gage section of the tensile sample and the larger particles could be blown away gently. The remaining particles would be those with the best adhesion to the surface. While the resulting surface contrast is not ideal for DIC, the high intensity ratio between the particles and the background provide a unique opportunity to track the particles between consecutive digital images taken during deformation. This can be achieved quite straightforwardly using digital image processing techniques. Subpixel tracking can be achieved by a number of correlation techniques, or by fitting to the known intensity profiles of particles. Photolithography and Electron Beam Lithography can be used to create micro tooling for micro speckle stamps, and the stamps can print speckle patterns onto the surface of the specimen. Stamp inks can be chosen which are appropriate for optical DIC, SEM-DIC, and simultaneous SEM-DIC/EBSD studies (the ink can be transparent to EBSD).\n\nDigital Volume Correlation (DVC) is a related analysis method with close ties to 2D-DIC. 3D-DIC successfully maps 3D deformations, but it does so by capturing planar images of the surface of a body only. The DVC algorithm is able to track full-field displacement information in the form of voxels instead of pixels. The theory is similar to above except that another dimension is added: the z-dimension. Instead of minimizing a coefficient based on the summed difference of intensity values in a subset of a planar image, minimization is done in a 3D-subset where intensity values corresponding to (x,y,z) values are compared to a standard and the summed difference minimized using predictive, 3D displacement fields.\n\nDVC can be done on any image dataset that represents a volume. The most popular way to do this currently is to use a sectioning microscope to take consecutive images at incremental depths. These images can then be reconstructed into a 3D matrix of intensity values representing the volume. For accurate calculation, it is required that a distinctly original 3D voxel pattern be captured so that minimization will settle on the proper values. This requires low noise levels and unique markers within the imaged volume.\n\nThe experimental DVC method is still being developed and optimized for speed and reliability. The first proposition of DVC was in 1999 by the authors Bay, Smith, Fyhrie, and Saad. This group used X-ray Tomography to image volumes that could then be correlated using the DVC algorithm which they developed in theory. Since then the method has grown in acceptance and has expanded to different imaging techniques. To date it has been used with MRI imaging, Computer Tomography (CT), and microCT. Recently, the technique has been expanded by the development of confocal microscopy, which allows for the imaging and testing of live tissue samples with techniques such as Second-Harmonic Generation (SHG) and Two-photon excitation microscopy. DVC is currently considered to be ideal in the research world for 3D quantization of local displacements, strains, and stress in biological specimens. It is preferred because of the non-invasiveness of the method over traditional experimental methods.\n\nDigital image correlation has demonstrated uses in the following industries:\n\n\n"}
{"id": "13976141", "url": "https://en.wikipedia.org/wiki?curid=13976141", "title": "EB Kraftproduksjon", "text": "EB Kraftproduksjon\n\nEB Kraftproduksjon AS is a Norwegian power company that operates 31 hydroelectric power plants, primarily in Buskerud, with total average annual production of 2.3 TWh. The company is owned by Energiselskapet Buskerud (70%) and E-CO Energi (30%).\n"}
{"id": "49287934", "url": "https://en.wikipedia.org/wiki?curid=49287934", "title": "ENESSERE", "text": "ENESSERE\n\nENESSERE is an Italian company, founded in 2009, with headquarters in Brendola and a manufacturer of small vertical wind turbines. Its foundation and the development of the Hercules wind turbine is a response to the Environmental impact of wind power, namely to the aestethic concerns. Hercules was launched in 2015 as a piece of Design with wooden wings and applying the Golden Ratio. ENESSERE is one of the 100 energy stories that Italy provided to the 2015 United Nations Climate Change Conference in Paris in December 2015.\n\n"}
{"id": "58614271", "url": "https://en.wikipedia.org/wiki?curid=58614271", "title": "Earth's crustal evolution", "text": "Earth's crustal evolution\n\nCrustal evolution involves the formation, destruction and renewal of the rocky outer shell at the Earth's surface.\n\nThe variation in composition within the Earth's crust is much greater than other terrestrial planets. Mars, Venus, Mercury and other planetary bodies have relatively quasi-uniform crusts unlike that of the Earth which contains both oceanic and continental plates. This unique property reflects the complex series of crustal processes that have taken place throughout the planet's history, including the ongoing process of plate tectonics.\n\nThe proposed mechanisms, regarding Earth's crustal evolution, take a theory-orientated approach. Fragmentary geologic evidence and observations provide the basis for hypothetical solutions to problems relating to the early Earth system. Therefore a combination of these theories creates both a framework of current understanding and also a platform for future study.\n\nThe early Earth was entirely molten. This was due to high temperatures created and maintained by the following processes:\n\n\nThe mantle remained hotter than modern day temperatures throughout the Archean. Over time the Earth began to cool as planetary accretion slowed and heat stored within the magma ocean was lost to space through radiation.\n\nA theory for the initiation of magma solidification states that once cool enough, the cooler base of the magma ocean would begin to crystallise first. This is because pressure of 25GPa at the surface cause the solidus to lower. The formation of a thin 'chill-crust' at the extreme surface would provide thermal insulation to the shallow sub surface, keeping it warm enough to maintain the mechanism of crystallisation from the deep magma ocean.\n\nThe composition of the crystals produced during the crystallisation of the magma ocean varied with depth. Experiments involving the melting of peridotite magma show that deep in the ocean (>≈700m), the main mineral present would be Mg-perovskite. Whereas olivine would dominate in the shallower areas along with its high pressure polymorphs e.g. garnet and majorite.\n\nA contributing theory to the formation of the first continental crust is through intrusive plutonic volcanism. The product of these eruptions formed a hot, thick lithosphere which underwent regular cycling with the mantle. The heat released by this form of volcanism, as well as assisting mantle convection, increased the geothermal gradient of the early crust.\n\nCrustal dichotomy represents the distinct contrast in composition and nature of the oceanic and continental plates, which together form the overall crust.\n\nOceanic and continental crusts are, at the present day, produced and maintained through plate tectonic processes. However the same mechanisms are unlikely to have produced the crustal dichotomy of the early lithosphere. This is thought to be true on the basis that sections of the thin, low density continental lithosphere could not have been sub-ducted under each other.\n\nSubsequently, a proposed relative timing for crustal dichotomy is put forward stating that dichotomy took place before the commencement of global plate tectonics. This is so a difference in crustal density could be established to facilitate plate subduction.\n\nLarge and numerous impact craters can be recognised on planetary bodies across the Solar System. These craters are thought to date back to a period where there was an increased frequency and intensity of asteroid impacts with terrestrial planets, known as the Late Heavy Bombardment, which terminated approximately 4Ga. This proposal goes on, claiming the Earth would have also sustained the same relative intensity of cratering as other planetesimals in the Solar System. It is therefore only due to Earth's high erosional rates and constant plate tectonics that the craters are not visible today. By scaling up the number and size of impact craters seen on the Moon to fit the size of Earth, it is predicted that at least 50% of the Earth's initial crust was covered in impact basins. This estimate provides a lower limit of the effect impact cratering had on the Earth's surface. \nThe main effects of impact cratering on the early lithosphere were:\n\n\nThe magnitude of these impacts is interpreted, with a high level of uncertainty, to have converted roughly half of the 'continental' crust into terrestrial maria. Therefore providing a method for the formation of crustal dichotomy, as seen today.\n\nThe initial crystallisation of minerals from the magma ocean formed the primordial crust. \n\nA potential explanation of this process states the resultant solidification of the mantle edge took place approximately 4.43Ga. This would subsequently produce continents composed of komatiite, an ultramafic rock rich in magnesium with a high melting point and low dynamic viscosity. Another line of research follows up on this, proposing that differences in the densities of newly formed crystals caused separation of crustal rocks; upper crust largely composed of fractionated gabbros and lower crust composed of anorthosites. The overall result of initial crystallisation formed a primordial crust roughly 60 km in depth.\n\nThe lack of certainty regarding the formation of primordial crust is due to there being no remaining present day examples. This is due to Earth's high erosional rates and the subduction and subsequent destruction of tectonic plates throughout its 4.5 Ga history. Furthermore, during its existence the primordial crust is thought to have been regularly broken and re-formed by impacts involving other planetesimals. This continued for several hundred million years after accretion, which concluded approximately 4.4Ga. The outcome of this would be the constant alteration in the composition of the primordial crust, increasing the difficulty in determining its nature. \n\nRecycling of existing primordial crust contributes to the production of secondary crust. Partial melting of the existing crust increases the mafic content of the melt producing basaltic secondary crust. A further method of formation due to the decay of radioactive elements within the Earth releasing heat energy and eventually causing the partial melting of upper mantle, also producing basaltic lavas. As a result, most secondary crust on Earth is formed at mid ocean ridges forming the oceanic crust.\n\nThe present day continental crust is an example of a tertiary crust. Tertiary crust is the most differentiated type of crust and so has a composition vastly different to that of the bulk Earth. The tertiary crust contains over 20% of the abundance of incompatible elements, which are elements a size or charge that prevent them from being included in mineral structure. This a result of its generation from the subduction and partial melting of secondary crust where it undergoes further fractional crystallisation. Two stages of evolution produce an increased proportion of incompatible elements.\n\nThe formation and development of plumes in the early mantle contributed to triggering the lateral movement of crust across the Earth's surface. The effect of upwelling mantle plumes on the lithosphere can be seen today through local depressions around hotspots such as Hawaii. The scale of this impact is much less than that exhibited in the Archean eon where mantle temperatures were much greater. Localised areas of hot mantle rose to the surface through a central plume wedge, weakening the damaged and already thin lithosphere. Once the plume head breaks the surface, crust either side of the head is forced downwards through the conservation of mass, initiating subduction. Numerical modelling shows only strongly energetic plumes are capable of weakening the lithosphere enough to rupture it, such plumes would have been present in the hot Archean mantle.\n\nPre-tectonic subduction can also be inferred from the internal volcanism on Venus. Artemis Corona is a large plume formed by the upwelling of mantle derived magma and is on a scale potentially comparable to that in the Archean mantle. Models using its known characteristics showed that continued magmatism from conductive heat through the plume caused gravitational collapse. The weight of collapse caused the spreading of the surrounding crust outwards and subsequent subduction around the margins. The anhydrous nature of the crust on Venus prevents it from sliding past each other, whereas through the study of oxygen isotopes, the presence of water on Earth can be confirmed from 4.3Ga. Thus, this model helps provide a mechanism for how plate tectonics could have been triggered on Earth, although it does not demonstrate that subduction was initiated at the earliest confirmed presence of water on Earth. Based on these models, the onset of subduction and plate tectonics is dated at 3.6Ga.\n\nImpact cratering also had consequences for both the development of plume-induced subduction and the establishment of global plate tectonics. The steepening of geothermal gradients could have directly enhanced convective mantle transport which now beneath an increasingly fractured lithosphere could have created stresses great enough to cause rifting and the separation of crust into plates.\n\nCrustal growth rates can be used to calculate estimates for the age of the continental crust. This can be done through analysis of igneous rocks with the same isotopic composition as initial mantle rock. These igneous rocks are dated and assumed to be direct evidence of new continental crust formation. The resulting ages of isotopically juvenile igneous rocks give distinct peaks, representing an increased proportion of igneous rock and therefore increased crust growth, at 2.7, 1.9 and 1.2 Ga. The validity of these results is questioned as the peaks could represent periods of preservation rather than increased continental crust generation. This is reinforced by the fact that such peaks are not observed in recent geologic time where it is given that magmatism resulting from the plate subduction has strongly contributed to producing new crust.\n\nCrustal growth rates from igneous rocks can be compared to the rates generated from radiogenic isotope ratios in sedimentary rocks. Projections of growth rates using these techniques does not produce staggered peaks, instead smooth shallow curves presenting a more constant rate of crustal growth. Although representative of large periods of time, limitations are found where samples do not solely represent magmatic production events. Instead samples include the mixing of sediments which produces a mix of original and altered isotope ratios.\n\nZircon minerals can be both detrital grains from sedimentary rocks and crystals in igneous rocks. Therefore, a combination of zircon forms can provide a more accurate estimate of crustal growth rates. Further to this, zircon minerals can be subject to Hf and O isotope ratio analysis. This is important as Hf isotopes indicate whether a rock originates from the mantle or an existing rock. High δO values of zircons represent rock recycled at the Earth's surface and thus potentially producing mixed samples. The outcome of this combined analysis is valid zircons showing periods of increased crustal generation at at 1.9 and 3.3Ga, the latter of which representing the time period following the commencement of global plate tectonics.\n"}
{"id": "29173191", "url": "https://en.wikipedia.org/wiki?curid=29173191", "title": "Eerikukivi", "text": "Eerikukivi\n\nEerikukivi is a glacial erratic on the island of Aegna. Its coordinates are: .\n\nOn 1 January 1992, the boulder was declared a Protected Natural Monument by the Estonian government. It is classed as Category III within the IUCN Management Category. The Class III designation is reserved for natural monuments that are \"protected areas managed mainly for conservation of specific natural features\". Or, \"areas containing one, or more, specific natural or natural/cultural features which is of outstanding or unique value because of its inherent rarity, representative or aesthetic qualities or cultural significance.\"\n\n"}
{"id": "2075926", "url": "https://en.wikipedia.org/wiki?curid=2075926", "title": "Empresa Eléctrica del Ecuador", "text": "Empresa Eléctrica del Ecuador\n\nEmpresa Eléctrica del Ecuador (literally \"Electric Company of Ecuador\") or Emelec was the name of an Ecuadorian electric company based in the city of Guayaquil.\n\nUntil 2004, it was owned by Fernando Aspiazu who used to be president of Banco del Progreso, Ecuador's second largest bank that was closed in March 1999. In 2004, Emelec had debts of $400 million with the Ecuadorian state and $800 million with state-owned \"Centro Nacional de Control de Energía\" (Cenace). Therefore, in 2004, the government of Alfredo Palacio confined by decree the administration of Emelec's actives for five years to \"Corporación para la Administración Temporal Eléctrica de Guayaquil\" (CATEG; literally \"Corporation for the Temporal Electric Administration of Guayaquil\"), an entity administrated by Venezuelan power company Electricidad de Valencia (Eleval), whose main duty beside supplying energy is debt collection from EMELEC's private debtors.\n\nA group of employees of EMELEC founded the sports club Club Sport Emelec in 1929, which nowadays is known for one of Ecuador's top soccer teams.\n\n"}
{"id": "30185050", "url": "https://en.wikipedia.org/wiki?curid=30185050", "title": "Empty lattice approximation", "text": "Empty lattice approximation\n\nThe empty lattice approximation is a theoretical electronic band structure model in which the potential is \"periodic\" and \"weak\" (close to constant). One may also consider an empty irregular lattice, in which the potential is not even periodic. The empty lattice approximation describes a number of properties of energy dispersion relations of non-interacting free electrons that move through a crystal lattice. The energy of the electrons in the \"empty lattice\" is the same as the energy of free electrons. The model is useful because it clearly illustrates a number of the sometimes very complex features of energy dispersion relations in solids which are fundamental to all electronic band structures.\n\nThe periodic potential of the lattice in this free electron model must be weak because otherwise the electrons wouldn't be free. The strength of the scattering mainly depends on the geometry and topology of the system. Topologically defined parameters, like scattering cross sections, depend on the magnitude of the potential and the size of the potential well. For 1-, 2- and 3-dimensional spaces potential wells do always scatter waves, no matter how small their potentials are, what their signs are or how limited their sizes are. For a particle in a one-dimensional lattice, like the Kronig-Penney model, it is possible to calculate the band structure analytically by substituting the values for the potential, the lattice spacing and the size of potential well. For two and three-dimensional problems it is more difficult to calculate a band structure based on a similar model with a few parameters accurately. Nevertheless, the properties of the band structure can easily be approximated in most regions by perturbation methods.\n\nIn theory the lattice is infinitely large, so a weak periodic scattering potential will eventually be strong enough to reflect the wave. The scattering process results in the well known Bragg reflections of electrons by the periodic potential of the crystal structure. This is the origin of the periodicity of the dispersion relation and the division of k-space in Brillouin zones. The periodic energy dispersion relation is expressed\nas:\n\nThe formula_2 are the reciprocal lattice vectors to which the bands formula_3 belong.\n\nThe figure on the right shows the dispersion relation for three periods in reciprocal space of a one-dimensional lattice with lattice cells of length \"a\".\nIn a one-dimensional lattice the number of reciprocal lattice vectors formula_2 that determine the bands in an energy interval is limited to two when the energy rises. In two and three dimensional lattices the number of reciprocal lattice vectors that determine the free electron bands formula_3 increases more rapidly when the length of the wave vector increases and the energy rises. This is because the number of reciprocal lattice vectors formula_2 that lie in an interval formula_7 increases. The density of states in an energy interval formula_8 depends on the number of states in an interval formula_7 in reciprocal space and the slope of the dispersion relation formula_3.\nThough the lattice cells are not spherically symmetric, the dispersion relation still has spherical symmetry from the point of view of a fixed central point in a reciprocal lattice cell if the dispersion relation is extended outside the central Brillouin zone. The density of states in a three-dimensional lattice will be the same as in the case of the absence of a lattice. For the three-dimensional case the density of states formula_11 is;\n\nIn three-dimensional space the Brillouin zone boundaries are planes. The dispersion relations show conics of the free-electron energy dispersion parabolas for all possible reciprocal lattice vectors. This results in a very complicated set intersecting of curves when the dispersion relations are calculated because there is a large number of possible angles between evaluation trajectories, first and higher order Brillouin zone boundaries and dispersion parabola intersection cones.\n\n\"Free electrons\" that move through the lattice of a solid with wave vectors formula_13 far outside the first Brillouin zone are still reflected back into the first Brillouin zone. See the external links section for sites with examples and figures.\n\nIn most simple metals, like aluminium, the screening effect strongly reduces the electric field of the ions in the solid. The electrostatic potential is expressed as\n\nwhere \"Z\" is the atomic number, \"e\" is the elementary unit charge, \"r\" is the distance to the nucleus of the embedded ion and \"q\" is a screening parameter that determines the range of the potential. The Fourier transform, formula_15, of the lattice potential, formula_16, is expressed as\n\nWhen the values of the off-diagonal elements formula_15 between the reciprocal lattice vectors in the Hamiltonian almost go to zero. As a result, the magnitude of the band gap formula_19 collapses and the empty lattice approximation is obtained.\n\nApart from a few exotic exceptions, metals crystallize in three kinds of crystal structures: the BCC and FCC cubic crystal structures and the hexagonal close-packed HCP crystal structure. \n\n"}
{"id": "3395132", "url": "https://en.wikipedia.org/wiki?curid=3395132", "title": "Federal Network Agency", "text": "Federal Network Agency\n\nThe Federal Network Agency ( or ) is the German regulatory office for electricity, gas, telecommunications, post and railway markets. It is a federal government agency of the German Federal Ministry of Economics and Technology and headquartered in Bonn, Germany.\n\nIn telecommunications, the agency has the authority over the German telephone numbering plan and other technical number assignments. It also claims to regulate the telecommunication market, including termination fees and open access to subscriber lines and licenses telephone companies. Thus in return for significant license fees, just a few companies are permitted to regulate the nowadays vital area of telecommunication for both private individuals and businesses. But with barriers to entry to this lucrative market created by the high cost of licenses, only a small number of providers can enter. With competition stifled, prices and bandwidths bring huge profits for the service providers, to the disadvantage of users. The result is basic communication breakdown for those who attempt to change to a carrier offering better rates. Because of this widespread failure to maintain services in terms of speed of registration and also actual communication bandwidth delivered, new laws have been passed which claim to bring clients in Germany into the same league as the rest of Europe. Until January 2013 those paying for and expecting to be able to use telephones and internet were powerless when the services failed to materialise, this new law is expected to give them a voice by allowing compensation claims a legitimate backing by the highest court in Germany. \n\nIn radio communications, the Agency manages the radio frequency spectrum, licenses broadcasting transmitters and detects radio interferences. Licensing radio and TV stations (that is, content providers), however, is the task of State authorities.\n\nIt is also a root certificate authority for qualified signatures according to the German Signature Act.\n\nThe Agency's responsibility in the post market include the licensing of companies for postal services and the observation of the market. It also regulates the market, assuring non-discrimintory access to some service facilities, such as PO boxes.\n\nIn the electricity and gas market, the Agency is responsible for ensuring non-discriminatory third-party access to networks and regulating the fees.\n\nThe Agency is not responsible for licensing energy companies. These tasks remain with authorities determined by State law.\n\nIn the area of railway traffic, the Federal Network Agency is responsible for ensuring non-discriminatory access to railway infrastructure. This includes monitoring and regulating the train schedules, allocation of railway track slots, access to service facilities, etc.\n\nThe Agency is not responsible for technical supervision and licensing of railway companies. These tasks remain with the Federal Railway Office (, EBA).\n\nIn the 1990s, the telecommunications and postal services in Germany were privatized. In 1994, the Deutsche Bundespost, was privatised and split into Deutsche Post and Deutsche Telekom, which remained under the supervision of the Federal Office for Post and Telecommunications (, BAPT). When the market was finally opened for competitors on 1 January 1998, the Regulatory Authority for Telecommunications and Posts (, ) was established, superseding the Federal Office as the supervisor for posts and telecommunications.\n\nWhen the government decided to improve competition for the energy and railway markets as of 13 July 2005 and 1 January 2006, it found that the Regulatory Authority's expertise in enabling open access to telecommunication networks would also be useful in these infrastructure markets. To reflect these new competences, the authority was renamed to Federal Network Agency for Electricity, Gas, Telecommunications, Posts and Railway (, ).\n\n\nThe Advisory Council consists of 16 members of the German Bundestag and 16 representatives of the German Bundesrat; the Bundesrat representatives must be members or political representatives of the government of a federal state. The members and deputy members of the Advisory Council are appointed by the federal government upon the proposal of the German Bundestag and the German Bundesrat.\n"}
{"id": "46970604", "url": "https://en.wikipedia.org/wiki?curid=46970604", "title": "Freedom of access to information Directive", "text": "Freedom of access to information Directive\n\nThe Freedom of access to information (2003/4/EC) is a European Union directive with the formal title \"Directive 2003/4/EC of the European Parliament and of the Council of 28 January 2003 on public access to environmental information and repealing Council Directive 90/313/EEC\".\n\nThe purpose of the Directive is to ensure that environmental information is systematically available and distributed to the public. The Directive requires Member States to ensure that public authorities are required to make the environmental information they hold available to any legal or natural person on request.\n\nIn 1998, the European Community signed a Convention on access to information, public participation in decision-making and access to justice in environmental matters (the Aarhus Convention). The Freedom of access to information Directive implements the Convention. \n\nPublic authorities are required to release information on request subject to the following exceptions:\n\n\nIn the United Kingdom, the Directive has been implemented by the Environmental Information Regulations 2004. In the Republic or Ireland, the Directive has been implemented as the European Communities (Access to Information on the Environment) Regulations 2007.\n"}
{"id": "9503180", "url": "https://en.wikipedia.org/wiki?curid=9503180", "title": "Generation time", "text": "Generation time\n\nIn population biology and demography, the generation time is the average time between two consecutive generations in the lineages of a population. In human populations, the generation time typically ranges from 22 to 33 years. Historians sometimes use this to date events, by converting generations into years to obtain rough estimates of time.\n\nThe existing definitions of the generation time fall into two categories: those that treat the generation time as a renewal time of the population, and those that focus on the distance between individuals of one generation and the next. Below are the three most commonly used definitions:\n\nThe net reproductive rate \"R\" is the number of offspring an individual is expected to produce during its lifetime (a net reproductive rate of 1 means that the population is at its demographic equilibrium). This definition envisions the generation time as a renewal time of the population. It justifies the very simple definition used in microbiology (\"the time it takes for the population to double\", or doubling time) since one can consider that during the exponential phase of bacterial growth mortality is very low and as a result a bacterium is expected to be replaced by two bacteria in the next generation (the mother cell and the daughter cell). If the population dynamic is exponential with a growth rate \"r\" (i.e. \"n\"(\"t\") ~ α.e, where \"n\"(\"t\") is the size of the population at time \"t\"), then this measure of the generation time is \ngiven by:\nIndeed, formula_2 is such that \"n\"(\"t\" + \"T\") = \"R\" \"n\"(\"t\"), i.e. e = \"R\".\n\nThis definition is a measure of the distance between generations rather than a renewal time of the population. Since many demographic models are female-based (that is, they only take females into account), this definition is often expressed as a mother-daughter distance (the \"average age of mothers at birth of their daughters\"). However, it is also possible to define a father-son distance (average age of fathers at the birth of their sons) or not to take sex into account at all in the definition. In age-structured population models, an expression is given by:\nwhere \"r\" is the growth rate of the population, \"ℓ\"(\"x\") is the survivorship function (probability that an individual survives to age \"x\") and \"m\"(\"x\") the maternity function (or birth function, or age-specific fertility). For matrix population models, there is a general formula:\nwhere \"λ\" = e is the discrete-time growth rate of the population, F = (\"f\") is its fertility matrix, v its reproductive value (row-vector) and w its stable stage distribution (column-vector); the formula_5 are the elasticities of \"λ\" to the fertilities.\n\nThis definition is very similar to the previous one but the population need not be at its stable age distribution. Moreover, it can be computed for different cohorts and thus provides more information about the generation time in the population. This measure is given by:\nIndeed, the numerator is the sum of the ages at which a member of the cohort reproduces, and the denominator is \"R\", the average number of offspring it produces.\n"}
{"id": "41616282", "url": "https://en.wikipedia.org/wiki?curid=41616282", "title": "George Logothetis", "text": "George Logothetis\n\nGeorge Michael Logothetis (born January 18, 1975) is an international businessman. He is from a Greek shipping family, grew up in London and has British citizenship, and has lived in New York City since 2004. In 2014 \"Fortune\" named him one of its \"40 under 40\".\n\nLogothetis is the chairman and CEO of the Libra Group, a diversified international privately owned conglomerate company which he founded in 2003. As of 2017, the Libra Group owns and operates 30 subsidiary companies around the world, mainly in six separate industries: aviation leasing, renewable energy, hotels and hospitality services, real estate, financial services and diversified investments, and shipping.\n\nGeorge Logothetis was born in 1975 in London to Greek-born parents, and he grew up there. He was educated at Highgate School from 1987 to 1993. \n\nAs a very young child he contracted meningitis, a disease which threatened his life and thereafter created debilitating health problems which left him bed-ridden for many days at a time, several times a year, until he had an operation at the age of 36 which cured the condition. He has stated that living with a compromised immune system for so long gave him an enormous appreciation of life and an ability to find inspiration in the face of difficulties.\n\nHe is the eldest of four sons of Greek shipping businessman Michael G. Logothetis, and his brothers are Constantine, Nicholas, and Leon. His father started a small shipping business, Lomar Shipping, in 1976. George was exposed to the business from a very early age, and as a teenager he also spent summers abroad learning the details of the trade.\n\nLogothetis began his business career immediately after high school and did not complete a university degree. In 2013, on the merits of his numerous accomplishments and philanthropies, he received an honorary doctorate from the American College of Greece.\n\nLogothetis started working at his father's shipping company, Lomar Shipping, in 1993 at the age of 18. By 1995 he was CEO of the company. Over the next ten years, he enlarged the fleet from less than five to more than 70 ships.\n\nLogothetis's philosophy from the start was to work hard, and then take three to five months off every five years to reflect about the next five years. During these hiatuses, he avoids work emails and phone calls, believing that \"ideas need space to be born\". On his first sabbatical, he traveled around Asia, and on his second, he did some university studies in New York City.\n\nDuring his three-month sabbatical in New York, the idea of diversification came to him. In 2003 Logothetis, with his brother Constantine, created and founded the Libra Group, based in New York and London, in order to diversify and expand his company.\n\nBetween 2004 and 2007, during a boom in the shipping business, the Libra Group sold off most of Lomar's ships. With the profit from the ship sales, the new company purchased a fleet of airplanes, which were at a cyclical low, and set up an aircraft leasing division, Lease Corporation International. Logothetis repeated this successful \"buy-low, sell-high\" formula of going counter to prevailing market trends several times in the ensuing decade, building a large diversified multinational conglomerate out of what was once a small shipping company.\n\n\"The idea behind Libra was to diversify away from shipping to create a global group, using the top lieutenants we had worked with in the preceding years\", Logothetis told \"Reuters\" in 2013. He told \"Fortune\" he \"took all the great people from our shipping business, and sent them around the world to start companies. We ended up with a ship's captain running a $200 million real estate company and a Russian fruit seller who runs six biogas plants in Latvia.\" He later reiterated to BBC News how he expanded and diversified the company: \"'We designed a framework of all the places in the world that we wanted to have an office and representation in. Next to each place we put the name of a person that we knew – a family member, a friend or an employee.' These people were sent out to different locations and given the chance to learn new businesses.\"\n\nIn 2006, the Libra Group started expanding beyond aviation. Its new major ventures included real estate, hotels and hospitality, and renewable energy.\n\nIn December 2009, after ship values slumped in the wake of the financial crisis, Logothetis again entered the shipping business and purchased Allocean and its fleet of 26 ships. From December 2009 through December 2014, the company purchased a total of 91 vessels.\n\nThe Libra Group has also expanded beyond its core businesses of shipping, aviation, real estate, hotels and hospitality, and renewable energy, to include a variety of interests including helicopter leasing, financial services and diversified investments, media, and construction. Between 2008 and 2014, the company purchased $7 billion to $7.5 billion of assets globally. As of 2017, the Libra Group owns and operates 30 subsidiary companies around the world.\n\nLogothetis has been an advocate for and an investor in Greece during the Greek financial crisis. And as a result of the crisis, he has become a sought-after business leader for his opinions on investing in Greece and in Greek assets, products, and industries. In June 2012, following prime minister Antonis Samaras's election and instatement, he emphasized that the promise of governmental stability was bringing back business confidence and individual confidence in Greece, and noted Libra Group's numerous recent and projected Greek investments and businesses. In March 2013, he cited Samaras's coalition government's relieving the Greek eurozone crisis as a reason investors were returning to, and should continue to return to, Greece. In late 2013, he sounded a cautionary note concerning the \"buying frenzy\" occurring in the slump-ridden shipping sector. Speaking to a reporter in January 2014 at Davos, he encouraged businesspersons to see the opportunity available in Greece, and stated that he felt that 2014 would be \"the year of Greek growth\". He reiterated his optimism in March 2014, noting that the Libra Group had invested $500 million in Greece since 2012.\n\nLogothetis has been publicly active in outreach and philanthropy, particularly in the wake of the Greek financial crisis. At a dinner meeting of 14 Greek Americans seeking to improve the lives of their countrymen, he conceived a plan for an internship program under the auspices of the Libra Group, in conjunction with the Greek America Foundation and the American College of Greece, placing young Greeks and Greek Americans in Libra Group's 25 locations across the world, giving them hope and opportunity. This project became the Libra Internship Program, which launched in 2011. Initially focused solely on Greek and Greek diaspora students, as of 2015 the program provides internships for up to 100 students per year at Libra Group and its subsidiaries, and includes placements for students from socio-economically disadvantaged backgrounds and Haitian students.\n\nVia the Libra Group, Logothetis also supports MBA students at Athens University of Economics and Business, and engages in a variety of other nonprofit philanthropic and educational programs. In 2012, along with three fellow diaspora Greeks, he founded The Hellenic Initiative, a global nonprofit focused on mobilizing the Greek diaspora and pro-Greek community to invest in Greece through programs of crisis relief, entrepreneurship, and economic development. Also in 2012, the Libra Group pledged €5million to create the initiative's Hellenic Entrepreneurship Award, which provides mentorship and funding to Greek entrepreneurs; as of 2017, the Libra Group has committed €10million to supporting entrepreneurship in Greece through the award.\n\nAt the Greek America Foundation’s 2012 National Innovation Conference, he encouraged fellow Greeks to revitalize Greece by rediscovering and implementing \"philotimo\", a complex and comprehensive core Greek value roughly translating as honor and altruism. He also gave detailed inspirational speeches at the 2011 and 2013 Gabby Awards presented by the Greek America Foundation. He has written several articles for the \"Huffington Post\", beginning in 2012.\n\nLogothetis co-founded the nonprofit Seleni Institute, which supports the mental and emotional well-being of women, with his wife Nitzia, who is a psychotherapist. The institute, which was founded in 2011 and launched in 2013 in New York City, promotes the mental health of mothers and their families through funding research, raising and providing awareness, and providing clinical care. George is the treasurer of the organization. In 2014 the Seleni Institute was honored with the Archbishop Iakovos Leadership 100 Award for Excellence.\n\nAt the 2012 Concordia Summit, Logothetis introduced Bill Clinton, noting his salient traits of wisdom and empathy. At the 2013 Concordia Summit, he opened the summit with a well-received speech exhorting worldwide business leaders and government officials to empower young people with opportunity and hope, in order to reverse the tide of cynicism and hopelessness and to create a better tomorrow. In 2015 he was appointed Chairman of Concordia's Leadership Council, and he has spoken about leadership, overcoming adversity, and empowering the underdog at Concordia and elsewhere, including a 2016 TEDx talk.\n\nLogothetis is on the Board of Directors of the My Brother's Keeper (MBK) Alliance, launched in May 2015 by President Obama. The Libra Group is a founding sponsor of the program. The MBK Alliance helps young men of color throughout the U.S. reach their full potential, by ensuring and improving educational and workplace opportunities. As part of its involvement with the original My Brother's Keeper Initiative launched by Obama in February 2014, the Libra Group has partnered with Leadership Enterprise for a Diverse America (LEDA) by providing internship placements for young men of color at Libra and its global subsidiaries. In 2016 the Libra Group also joined two other Obama administration initiatives, the Partnership for Refugees and the Equal Pay Pledge.\n\n\nLogothetis and his wife Nitzia (née Embiricos) are based in Manhattan, and have\ntwo sons and a daughter. Nitzia, a psychotherapist, is a descendant of a Greek shipping family and grew up in England. She has a psychology degree from Brown University, an MSc in child development from the University of London, and an MA in counseling for mental health and wellness from New York University's Steinhardt School of Culture, Education, and Human Development. She heads the Seleni Institute, a nonprofit devoted to maternal psychological and physical care.\n\n\n \n"}
{"id": "34340717", "url": "https://en.wikipedia.org/wiki?curid=34340717", "title": "GlassPoint Solar", "text": "GlassPoint Solar\n\nGlassPoint Solar is a private company founded in 2009 that designs and manufactures solar steam generators that use solar thermal technology to generate steam for enhanced oil recovery (EOR). \n \n\nGlassPoint’s enclosed trough technology encloses lightweight mirrors in a standard agricultural greenhouse, concentrating the sun’s energy to heat water and create steam. The steam is then injected in an oil reservoir to aid the extraction of heavy crude. The company’s solar thermal technology uses trough-shaped mirrors, standard piping and a greenhouse structure. By using the enclosed trough architecture, GlassPoint claims it can produce emission-free steam for two to three times less than competing concentrated solar power technologies, such as the power tower and linear fresnel. The company is based in Fremont, California, with offices in Bakersfield, California, Muscat, Oman, Kuwait City, Kuwait and Shenzhen, China.\n\nIn December 2012, GlassPoint announced $26 million in Series B financing. Investors in the round included Royal Dutch Shell, RockPort Capital, Nth Power and Chrysalix Energy Venture Capital.\n\nIn September 2014, GlassPoint announced $53 million in Series C financing. Investors in the round included Royal Dutch Shell, State General Reserve Fund of Oman, RockPort Capital, Nth Power and Chrysalix Energy Venture Capital.\n\nTo date, GlassPoint has a total of $86.7 million in funding.\n\nIn 2011, GlassPoint deployed the world’s first commercial solar EOR project at an oil field operated by Berry Petroleum in Kern County, Calif., USA. Built in less than six weeks, the system uses the sun’s radiant heat to produce approximately 1 million British Thermal Units (Btus) per hour of solar heat. GlassPoint partnered with local firms TJ Cross Engineers and PCL Industrial Services to construct the system, which spans 7,000 square feet of land on the 100-year-old oilfield. The enclosed trough system preheats water to 190 °F used as feedwater for Berry Petroleum’s gas-fired steam generators.\n\nIn May 2013, GlassPoint commissioned the Middle East's first solar EOR project, a 7MWth system developed in partnership with Petroleum Development Oman (PDO), the largest oil company in the Sultanate of Oman. The system produces an average of 50 tons of emissions-free steam daily. operated successfully for more than 4 years, generating an average of 50 tons of emissions-free steam per day that fed directly into existing thermal EOR operations at PDO’s Amal West oilfield.\n\nThe system was completed on time, on budget, and with no Lost Time Injuries demonstrating GlassPoint and PDO’s shared commitment to health, safety, security and the environment (HSSE). During the first 12 months of operations, actual performance of the GlassPoint system matched output models within a few percent and steam production continues to exceed contracted performance targets. The system recorded 98.6% uptime and maintained regular operations during severe dust and sand storms. The automated roof-washing unit proved particularly effective in all weather events, restoring system performance overnight. The project served as a performance and operational baseline for larger steam generators in Oman and provided PDO with valuable information for planning potential large-scale projects in the future.\n\nIn July 2015, Petroleum Development Oman and GlassPoint Solar announced that they signed an agreement to build a 1 GWth solar field. The total solar field will be 1.8KM and the total project will be 3KM. The project, named Miraah, will be the world's largest solar field measured by peak thermal capacity.\n\nIn August 2017, GlassPoint and its contractors crossed the threshold of 1.5 million man-hours worked without lost time injury (LTI) at Miraah.\n\nIn November 2017, GlassPoint and Petroleum Development Oman (PDO) completed construction on the first block of the Miraah solar plant safely on schedule and in line with budget, and successfully delivered steam to the Amal West oilfield.\n\nAs of November 2018, the first 100 MWt of Miraah are operating and meeting all performance targets.\n\nIn November 2017, Aera Energy and GlassPoint Solar announced a joint project to create California’s largest solar energy field. Located at the South Belridge Oil Field, the project, called Belridge Solar, will be the first in the world to incorporate solar steam generation with solar electricity generation.\n\nThe Belridge Oil Field resides near Bakersfield, California. Once completed, the facility is projected to produce 12 million barrels of steam annually through a 850MW thermal solar steam generator. It is planned to reduce the natural gas used at the Belridge Oil Field for the production and extraction of heavy oil in its enhanced oil recovery process (EOR).\n\nAdditionally, a separate 26.5 MW electricity generating facility using photovoltaic panels is planned to power oilfield operations.\n\nThis new facility is predicted to reduce carbon emissions by more than 376,000 metric tons annually. This will offset more than one-third of all the cars driven in Bakersfield. It is also expected to create hundreds of jobs throughout California.\n\nThis project highlights a growing trend of oil and gas companies partnering with the solar energy industry.\n\nIn November 2018, GlassPoint and Occidental Petroleum of Oman signed an agreement to cooperate on a solar thermal project to facilitate oil production on an oilfield in the Sultanate of Oman.\n\nThe agreement outlines plans to generate up to 100,000 barrels of solar steam per day for heavy oil production at Mukhaizna oilfield. The proposed solar thermal plant would exceed two gigawatts and could save more than 800,000 tons of carbon dioxide (CO2) emissions each year.\n\nOccidental of Oman would purchase the steam under a long-term off-take agreement, providing a cost-effective, zero-emissions alternative to steam generated using natural gas.\n\nGlassPoint claims its technology can help oil operators free natural gas currently burned for oil production. This gas can be redirected to higher value applications, such as desalination, industrial development or export as LNG, thus increasing economic well-being. New supply chains associated with solar industries can generate significant societal value as well.\n\nIn January 2018, GlassPoint signed a collaboration agreement with a host of partners to establish its corporate social responsibility initiative, the ‘GlassPoint Innovation Spur’. \n\nThe program is set to contribute and sustainably drive innovation within Oman’s renewable energy and water management sectors. The list of partners includes The Research Council (TRC), Innovation Park Muscat (IPM), Public Authority for SME Development (Riyada) and Sharakah.\n\nAn intensive full-cycle incubation program, the GlassPoint Innovation Spur will provide aspiring Omani entrepreneurs with an integrated ecosystem of scientific, technical and business support. The two-year program will equip participants with valuable skills through coaching and mentorship, as they transform their innovations into implementable businesses. Robust screening, selection, and testing criteria is in place to ensure originality and economic feasibility. Selected entrepreneurs will receive practical support to further develop their inventions and establish businesses before being linked to investors.\n\nIn September 2018, GlassPoint Solar in partnership with Petroleum Development Oman (PDO) announced a new technology center.\n\nThe joint initiative is called SolaRISE for Solar Research, Innovation, and Sustainability in Energy. It was formed to develop and test solar technologies in oil fields.\n\nResearchers will focus on cutting costs and automating operations. The center also will pilot and evaluate solar energy uses for other industrial processes in Oman and worldwide.\n\n\n"}
{"id": "53581425", "url": "https://en.wikipedia.org/wiki?curid=53581425", "title": "Grytdalen Hydroelectric Power Station", "text": "Grytdalen Hydroelectric Power Station\n\nThe Grytdalen Hydroelectric Power Station () is a hydroelectric power station in Valsøyfjord in the municipality of Halsa in Møre og Romsdal county, Norway. It stands about south of the village of Engan. It is a run-of-river plant that utilizes a drop from the lake \"Englivatnet\" to the intake dam of the Valsøyfjord Hydroelectric Power Station. It has a Francis turbine and operates at an installed capacity of , with an average annual production of about 5.3 GWh. The plant came into operation in March 2012 and is owned by Svorka Energi.\n"}
{"id": "21503546", "url": "https://en.wikipedia.org/wiki?curid=21503546", "title": "Hans Goksøyr", "text": "Hans Goksøyr\n\nHans Goksøyr (10 February 1923 – 24 August 2016) was a Norwegian businessperson in the petroleum industry.\n\nHe was born in Eidsvoll, and took his secondary education at Oslo Cathedral School in 1943. He then enrolled in the Royal Norwegian Air Force in-exile in Canada, serving in World War II. After the war he returned to Norway, enrolling at the Norwegian Institute of Technology in 1946. He graduated in 1951, and then worked as a research assistant and fellow until 1954. In 1955 he was hired in Shell Norway, advancing to director of planning, public relations and environment in his later career. He left Shell in 1984, and was secretary-general in the Norwegian Petroleum Institute from 1985 to 1989.\n\nGoksøyr was also an amateur painter, and in 1954 he was represented with two works at the national Autumn Exhibit, Høstutstillingen. His other hobbies included golf and giving lectures. He settled in Nesøya in 1963, was married and had two sons. He died in August 2016 at the age of 93.\n"}
{"id": "11734758", "url": "https://en.wikipedia.org/wiki?curid=11734758", "title": "Howland Forest", "text": "Howland Forest\n\nThe Howland Research Forest is a tract of mature evergreen forest in the North Maine Woods, within Penobscot County, central Maine. It is located west of the town of Howland. \n\nThe tract is part of the 1.1 million acres (4,500 km²) of Maine forest sold in 2005 by International Paper (IP) to the Seven Islands Land Company, a private forest investment management holding company. In 2007, the research forest was purchased by Northeast Wilderness Trust ensuring its wild and natural state into the future.\n\nThe Howland Forest is a founding member of the AmeriFlux and FLUXNET research networks. \n\nThe Howland Forest study site is located in a boreal transitional forest of the New England/Acadian forests ecoregion. The forest is dominated by mixed spruce, hemlock, aspen, and birch stands ranging in age from 45 to 130 years. The soils are formed on coarse-loamy granitic basal till. \n\nThe tract had previously been designated as a research forest under IP's ownership, attracting researchers from the US Forest Service, the University of Maine, NASA, NOAA, and the Woods Hole Research Center. Areas of study included acid rain, nutrient cycling, soil ecology, and more recently, forest carbon uptake and loss. The forest has one of the longest records of carbon flux measurement in the world, dating to 1996, providing important information about carbon sequestration in mature forests. \n\n"}
{"id": "12977550", "url": "https://en.wikipedia.org/wiki?curid=12977550", "title": "Instituto Costarricense de Electricidad", "text": "Instituto Costarricense de Electricidad\n\nInstituto Costarricense de Electricidad () (ICE) is the Costa Rican government-run electricity and telecommunications services provider. Jointly with the Radiographic Costarricense SA (RACSA) and Compañía Nacional de Fuerza y Luz (CNFL) form the ICE Group.\n\nICE was founded on 8 April 1949 by Decree-Law No. 449 after the Costa Rican Civil War of 1948, in order to solve the problems of power shortages that occurred in Costa Rica in the 1940s. Since 1963 ICE provides telecommunications services throughout the country.\n\nThe attempts to reform ICE throughout a set of laws in the years 1999 and 2000 generated a great social mobilization. The ruling party at that time, the Social Christian Unity Party and the main opposition National Liberation Party agreed to change the institution. Meanwhile, the citizen opposition reached 274 protests in 14days.\n\nFollowing the Dominican Republic–Central America Free Trade Agreement, the telecommunication market was opened to private companies. Since 2011, América Móvil through Claro Americas and Telefónica through Movistar, are competing against ICE in the Costa Rican mobile market.\n\nICE Group's generates annual revenues of around $ 1,844 million.\n\n"}
{"id": "5194756", "url": "https://en.wikipedia.org/wiki?curid=5194756", "title": "Kasha's rule", "text": "Kasha's rule\n\nKasha's rule is a principle in the photochemistry of electronically excited molecules. The rule states that photon emission (fluorescence or phosphorescence) occurs in appreciable yield only from the lowest excited state of a given multiplicity. It is named for American spectroscopist Michael Kasha, who proposed it in 1950.\n\nThe rule is relevant in understanding the emission spectrum of an excited molecule. Upon absorbing a photon, a molecule in its electronic ground state (denoted \"S\", assuming a singlet state) may – depending on the photon wavelength – be excited to any of a set of higher electronic states (denoted \"S\" where \"n\">0). However, according to Kasha's rule, photon emission (termed fluorescence in the case of an \"S\" state) is expected in appreciable yield only from the lowest excited state, \"S\". Since only one state is expected to yield emission, an equivalent statement of the rule is that the emission wavelength is independent of the excitation wavelength.\n\nThe rule can be explained by the Franck–Condon factors for vibronic transitions. For a given pair of energy levels that differ in both vibrational and electronic quantum number, the Franck–Condon factor expresses the degree of overlap between their vibrational wavefunctions. The greater the overlap, the quicker the molecule can undergo transition from the higher to the lower level. Overlap between pairs is greatest when the two vibrational levels are close in energy; this tends to be the case when the \"vibrationless\" levels of the electronic states coupled by the transition (where the vibrational quantum number \"v\" is zero) are close. In most molecules, the vibrationless levels of the excited states all lie close together, so molecules in upper states quickly reach the lowest excited state, \"S\", before they have time to fluoresce. However, the energy gap between \"S\" and \"S\" is greater, so here fluorescence occurs, since it is now kinetically competitive with internal conversion (IC).\n\nExceptions to Kasha's rule arise when there are large energy gaps between excited states. An example is azulene: the classical explanation is that the \"S\" and \"S\" states lie sufficiently far apart that fluorescence is observed mostly from \"S\". However, recent research has put forward that this may not be the case, and that fluorescence is seen from \"S\" because of crossing in the \"N\"-dimensional potential surface allowing very fast internal conversion from \"S\" to \"S\". \n\nA corollary of Kasha's rule is the Vavilov rule, which states that the quantum yield of luminescence is generally independent of the excitation wavelength. This can be understood as a consequence of the tendency – implied by Kasha's rule – for molecules in upper states to relax to the lowest excited state non-radiatively. Again there are exceptions: for example benzene vapour.\n\n"}
{"id": "3153971", "url": "https://en.wikipedia.org/wiki?curid=3153971", "title": "Mean piston speed", "text": "Mean piston speed\n\nThe mean piston speed is the average speed of the piston in a reciprocating engine. It is a function of stroke and RPM. There is a factor of 2 in the equation to account for one stroke to occur in 1/2 of a crank revolution (or alternatively: two strokes per one crank revolution) and a '60' to convert seconds from minutes in the RPM term.\n\nMPS = 2 * Stroke * RPM / 60\n\nFor example, a piston in an automobile engine which has a stroke of 90 mm will have a mean speed at 3000 rpm of \n2 * (90 / 1000) * 3000 / 60 = 9 m/s.\n\nIt is a good indicator of the class and performance of an engine relative to its competitors. The 5.2-liter V10 that debuted in the 2017 Audi R8 has the highest mean piston speed for any production car (26.9 m/s) thanks to its 92.8 mm stroke and 8700-rpm redline.\n\nCorrected Piston Speed (Frederick Lanchester and Janke and King)\nCorrected Piston speed is a method to more accurately represent stress on an engine, and is calculated as mean piston speed divided by the square root of the stroke/bore ratio.\n\nhttp://autos.groups.yahoo.com/group/mc-engine/message/2928\nClassic Racing Engines Karl Ludvigsen (Glossary)\n\n\nThe mean of any function refers to the average value. In the case of mean piston speed, taken in a narrow mathematical sense, it is zero because half of the time the piston is moving up and half of the time the piston is moving down; this is not useful. The way the term is usually used is to describe the distance traveled by the piston per unit of time, taking distance positive in both up and down senses. It is related to the rate that friction work is done on the cylinder walls, and thus the rate that heat is generated there.\nThis is sort of a non-puzzle. It represents a specification to be designed to rather than as a result of design and the mean piston velocity is a function of the revolutions per minute, that is, the piston at a specific rpm is going to be the same at the peak of the graph as it is at the trough, that is at 286.071 degrees on the crankshaft if the rpm is held consistent. At 0 and 180 degrees, the piston velocity is zero.\nPiston velocity is a test of the strength of the piston and connecting rod subassembly. The alloy used to make the piston itself is what determines the maximum velocity that the piston can reach before friction coefficients, heat levels and reciprocating stress overcome the maximum levels that the piston can sustain before it begins to fail structurally. As the alloy tends to be fairly consistent across most manufacturers, the maximum velocity of the piston at a given rpm is determined by the length of the stroke, that is, the radius of the journal of the crankshaft.\nThe most common engine types in production are built to square, or below square. That is, a square engine has the same diameter of cylinder bore as the total length of the stroke from 0 to 180 degrees, whereas in an undersquare engine, the total length of the stroke is greater than the diameter of the bore. The opposite, oversquare, is mostly used in higher performance engines where the torque curve approaches the peak of the maximum piston velocity. Generally in this type of engine, the volume of the cylinder can be artificially enhanced with turbochargers or superchargers, increasing the amount of fuel/air available for combustion.\nAn example is found in Formula 1 racing engines, where the cylinder diameter is substantially greater than the length of the stroke, resulting in higher available rpm but necessitating greater requirements of the strengths of connecting rods and pistons and higher temperature tolerances for bearings. The cylinder diameter in these engines are fairly small (under 45 mm) and the stroke is less than that, depending on the torque curve and maximum available rpm as designed by the builder. Peak torque is reached at higher rpm and is spread over a wider range of rpm. The specifications of these are known factors and can be designed to.\nTorque is a function of the length of the stroke, the shorter the stroke, the less available torque at lower rpm, but the piston velocity can be taken to much greater speeds, meaning higher engine rpm. These types of engines are much more delicate and require a much higher level of precision in the moving parts than square or oversquare engines. Up until the early 1960s, the focus by designers was on torque rather than piston velocity, probably due to material considerations and machining technologies. As materials have improved, engine rpm has increased.\n"}
{"id": "1816518", "url": "https://en.wikipedia.org/wiki?curid=1816518", "title": "Methanol economy", "text": "Methanol economy\n\nThe methanol economy is a suggested future economy in which methanol and dimethyl ether replace fossil fuels as a means of energy storage, ground transportation fuel, and raw material for synthetic hydrocarbons and their products. It offers an alternative to the proposed hydrogen economy or ethanol economy.\n\nIn the 1990s, Nobel prize winner George A. Olah advocated a methanol economy; in 2006, he and two co-authors, G. K. Surya Prakash and Alain Goeppert, published a summary of the state of fossil fuel and alternative energy sources, including their availability and limitations, before suggesting a methanol economy.\n\nMethanol can be produced from a wide variety of sources including still-abundant fossil fuels (natural gas, coal, oil shale, tar sands, etc.) as well as agricultural products and municipal waste, wood and varied biomass. It can also be made from chemical recycling of carbon dioxide.\n\nMethanol is a fuel for heat engines and fuel cells. Due to its high octane rating it can be used directly as a fuel in flex-fuel cars (including hybrid and plug-in hybrid vehicles) using existing internal combustion engines (ICE). Methanol can also be burned in some other kinds of engine or to provide heat as other liquid fuels are used. Fuel cells, can use methanol either directly in Direct Methanol Fuel Cells (DMFC) or indirectly (after conversion into hydrogen by reforming).\n\nMethanol is already used today on a large scale to produce a variety of chemicals and products. Global methanol demand as a chemical feedstock reached around 42 million metric tonnes per year as of 2015. Through the methanol-to-gasoline (MTG) process, it can be transformed into gasoline. Using the methanol-to-olefin (MTO) process, methanol can also be converted to ethylene and propylene, the two chemicals produced in largest amounts by the petrochemical industry. These are important building blocks for the production of essential polymers (LDPE, HDPE, PP) and like other chemical intermediates are currently produced mainly from petroleum feedstock. Their production from methanol could therefore reduce our dependency on petroleum. It would also make it possible to continue producing these chemicals when fossil fuels reserves are depleted.\n\nToday most methanol is produced from methane through syngas. Trinidad and Tobago is currently the world's largest methanol exporter, with exports mainly to the United States. The natural gas that serves as feedstock for the production of methanol comes from the same sources as other uses. Unconventional gas resources such as coalbed methane, tight sand gas and eventually the very large methane hydrate resources present under the continental shelves of the seas and Siberian and Canadian tundra could also be used to provide the necessary gas. \n\nThe conventional route to methanol from methane passes through syngas generation by steam reforming combined (or not) with partial oxidation. New and more efficient ways to convert methane into methanol are also being developed. These include:\n\nAll these synthetic routes emit the greenhouse gas carbon dioxide CO. To mitigate this, methanol can be made through ways minimizing the emission of CO. One solution is to produce it from syngas obtained by biomass gasification. For this purpose any biomass can be used including wood, wood wastes, grass, agricultural crops and their by-products, animal waste, aquatic plants and municipal waste. There is no need to use food crops as in the case of ethanol from corn, sugar cane and wheat.\nMethanol can be synthesized from carbon and hydrogen from any source, including still available fossil fuels and biomass. CO emitted from fossil fuel burning power plants and other industries and eventually even the CO contained in the air, can be a source of carbon. It can also be made from chemical recycling of carbon dioxide, which Carbon Recycling International has demonstrated with its first commercial scale plant. Initially the major source will be the CO rich flue gases of fossil-fuel-burning power plants or exhaust from cement and other factories. In the longer range however, considering diminishing fossil fuel resources and the effect of their utilization on earth's atmosphere, even the low concentration of atmospheric CO itself could be captured and recycled via methanol, thus supplementing nature’s own photosynthetic cycle. Efficient new absorbents to capture atmospheric CO are being developed, mimicking plants' ability. Chemical recycling of CO to new fuels and materials could thus become feasible, making them renewable on the human timescale.\n\nMethanol can also be produced from CO by catalytic hydrogenation of CO with H where the hydrogen has been obtained from water electrolysis. This is the process used by Carbon Recycling International of Iceland. Methanol may also be produced through CO electrochemical reduction, if electrical power is available. The energy needed for these reactions in order to be carbon neutral would come from renewable energy sources such as wind, hydroelectricity and solar as well as nuclear power. In effect, all of them allow free energy to be stored in easily transportable methanol, which is made immediately from hydrogen and carbon dioxide, rather than attempting to store energy in free hydrogen.\n\nor with electric energy\n\nThe necessary CO would be captured from fossil fuel burning power plants and other industrial flue gases including cement factories. With diminishing fossil fuel resources and therefore CO emissions, the CO content in the air could also be used. Considering the low concentration of CO in air (0.04%) improved and economically viable technologies to absorb CO will have to be developed. For this reason, extraction of CO from water could be more feasible due to its higher concentrations in dissolved form. This would allow the chemical recycling of CO, thus mimicking nature’s photosynthesis.\n\nIn the process of photosynthesis, green plants use the energy of sunlight to split water into free oxygen (which is released) and free hydrogen. Rather than attempt to store the hydrogen, plants immediately capture carbon dioxide from the air to allow the hydrogen to reduce it to storable fuels such as hydrocarbons (plant oils and terpenes) and polyalcohols (glycerol, sugars and starches). In the methanol economy, any process which similarly produces free hydrogen, proposes to immediately use it \"captively\" to reduce carbon dioxide into methanol, which, like plant products from photosynthesis, has great advantages in storage and transport over free hydrogen itself.\n\nMethanol is a liquid under normal conditions, allowing it to be stored, transported and dispensed easily, much like gasoline and diesel fuel. It can also be readily transformed by dehydration into dimethyl ether, a diesel fuel substitute with a cetane number of 55.\n\nMethanol economy advantages compared to a hydrogen economy:\n\n\n\n\n"}
{"id": "14656281", "url": "https://en.wikipedia.org/wiki?curid=14656281", "title": "Oil megaprojects (2006)", "text": "Oil megaprojects (2006)\n\nThis page summarizes projects that brought more than of new liquid fuel capacity to market with the first production of fuel beginning in 2006. This is part of the Wikipedia summary of Oil Megaprojects. 2006 saw 30 projects come on stream with an aggregate capacity of when full production was reached this list does not like include any of the enormous project developed in the United states which dwarf these by +-5000 BOE (which may not have been in 2006).\n\nTerminology \n\n"}
{"id": "1302352", "url": "https://en.wikipedia.org/wiki?curid=1302352", "title": "Panelling", "text": "Panelling\n\nPanelling (or paneling in the U.S.) is a millwork wall covering constructed from rigid or semi-rigid components. These are traditionally interlocking wood, but could be plastic or other materials.\n\nPanelling was developed in antiquity to make rooms in stone buildings more comfortable. The panels served to insulate the room from the cold stone. In more modern buildings, such panelling is often installed for decorative purposes. Panelling, such as wainscoting and boiserie in particular, may be extremely ornate and is particularly associated with seventeenth and eighteenth century interior design, Victorian architecture in Britain, and its international contemporaries.\n\nThe term wainscot ( or ) originally applied to high quality riven oak boards. \n\nWainscot oak came from large, slow-grown forest trees, and produced boards that were knot-free, low in tannin, light in weight, and easy to work with. It was preferred to home-grown oak, especially in Holland and Great Britain, because it was a far superior product and dimensionally stable.\n\nThe \"Oxford English Dictionary\" states that it derives from the medieval German \"wagenschot\" as well as \"wageschot\" or 'wall-board'. \"Johnson's Dictionary\" defined it thus: \n\nA 'wainscot' was therefore a board of riven (and later quarter-sawn) oak, and wainscoting was the panelling made from it. During the 18th century, oak wainscot was almost entirely superseded for panelling in Europe by softwoods (mainly Scots pine and Norway spruce), but the name stuck:\n\n\"The term wainscoting, as applied to the lining of walls, originated in a species of foreign oak of the same name, used for that purpose; and although that has long been superseded by the introduction of fir timber, the term has been continued notwithstanding the change of material\".\n\nAlso in the 18th century, the style of panelling changed from a floor-to-ceiling covering to one in which only the lower part of the wall was covered. Hence wainscot or wainscoting became a panelling style applied to the lower of an interior wall, below the dado rail or chair rail, and above the baseboard or skirting board. It is traditionally constructed from tongue-and-groove boards, though bead-board or decorative panels, such as a wooden door might have, are also common. New manufacturing techniques are capable of milling large panels from one sheet, reducing seams, caulking, and expansion/contraction cracks that have plagued traditional construction. Wainscoting may also refer to other materials used in a similar fashion.\n\nThe original purpose of wainscoting was to cover the lower part of walls, which, in houses constructed with poor or non-existent damp-proof courses, are often affected by rising dampness. Its purpose is now decorative.\n\nFrame and Panel – Wainscoting\n\nA frame and panel can suit a large variety of homes as you have the choice as to what moulding profile is best for the look you are trying to achieve. These mouldings are then constructed into a series of raised square or rectangular sections which sit above the skirting boards and below the chair rail. Some wall panelling extends higher than the chair rail height (typically about mid wall height where you would expect the top of a chair seat height to be) especially in a smaller room where you are trying to achieve a sense of height and space to a smaller room.\n\nBoiserie (; often used in the plural boiseries) is the French term used to define ornate and intricately carved wood panelling. Boiseries became popular in the latter part of the 17th century in French interior design, becoming a \"de rigueur \" feature of fashionable French interiors throughout the 18th century. Such panels were most often painted in two shades of a chosen color or in contrasting colors, with gilding reserved for the main reception rooms. The Palace of Versailles contains many fine examples of white painted \"boiseries\" with gilded mouldings installed in the reigns of Louis XV and Louis XVI. The panels were not confined to just the walls of a room but were used to decorate doors, frames, cupboards, and shelves also. It was standard for mirrors to be installed and framed by the carved \"boiseries\", especially above the mantelpiece of a fireplace. Paintings were also installed within \"boiseries\", above doorways or set into central panels.\n\n"}
{"id": "28228368", "url": "https://en.wikipedia.org/wiki?curid=28228368", "title": "Peat wax", "text": "Peat wax\n\nPeat Wax is a dark waxy substance extracted from peat using organic solvents. It is very similar to the coal derived Montan wax and therefore has similar properties and uses. Raw peat wax is typically a mixture of three primary components, namely asphalt, resins and wax.\n"}
{"id": "32553857", "url": "https://en.wikipedia.org/wiki?curid=32553857", "title": "Peter Taylor (environmentalist)", "text": "Peter Taylor (environmentalist)\n\nPeter Taylor (born 24 January 1948) is a UK environmentalist with a long track record of public activism and scholarship on issues ranging from nuclear safety, ocean pollution, biodiversity strategies, renewable energy and climate change. His recent work on global warming has been questioned by environmentalists. His 2009 book \"Chill: a reassessment of global warming theory\" argued that most of the recent documented warming is caused by peaking natural cycles, that there is also a potential for global cooling and that adaptation not mitigation should be a priority. His views received widespread coverage in the media – with front page on the \"Daily Express\", and articles in the online versions of \"The Mail\", \"The Times\" and an Al Jazeera video.\n\nBorn in January 1948, Taylor was educated at Cowbridge Grammar School in Glamorgan, Wales from where he won an Open Scholarship to St Catherine's College, Oxford University. He graduated with honours in Natural Sciences from the School of Zoology in 1970. As a student he led a successful inter-university biological expedition to East Africa; After six years of what he describes in his autobiography \"Shiva's Rainbow\" as an adventurer and explorer, including a solo vehicle-crossing of the Sahara and climbing the Eiger, he returned to Oxford to study Social Anthropology under the linguistic anthropologist Edwin Ardener. Taylor has been a member of the Institute of Biology and is a Certified Biologist, a former member of the International Union of Radioecologists, the International Society for Radiation Protection and the British Ecological Society. He is currently a member of the Royal Anthropological Institute.\n\nTaylor left his academic studies in anthropology order to develop the Political Ecology Research Group (PERG) which he founded in 1976. Eschewing the academic elements of political ecology and the need for theory in favour of political involvement, the group pioneered scientific and legal support for environmental policy initiatives and worked closely with Greenpeace International, Trade Unions and at times government agencies. The group held copyright on all its work and published over 20 research reports between 1978 and 1992. Taylor published an account of the anti-nuclear movement in The Ecologist - a text used by the Open University for its Control of Technology Course, an on assessments of nuclear risk in the science journal Nature.\n\nTaylor became a public figure following the 1977 Windscale Inquiry into nuclear fuel reprocessing during which he exposed the risks of nuclear waste storage and mounted a successful campaign against radioactive discharges to the marine environment – his work was widely reported in the national press, New Scientist, The Ecologist and the New Internationalist. Between 1980 and 1992 he became an advisor to a wide spectrum of organisations, ranging from government agencies to environmental NGOs, appearing on TV and radio on issues of nuclear risk and pollution. His work uncovered the health impact of the Windscale Fire in 1957 - in the PERG report RR-7, and in association with Yorkshire TV, the excess of childhood cancers around Sellafield. He served on the government commission into nuclear waste dumping at sea (chaired by Sir Fred Holliday) which recommended the practice be banned. He also sat on a research advisory group on nuclear waste management set up by the Department of Environment – resigning when he felt government were not allowing time for detailed comparative assessment of the options.\n\nThe work of PERG played a role in limiting the development of nuclear fuel reprocessing and the 'plutonium economy', particularly in Germany, cleaning up discharges to the Irish Sea, altering perceptions of the risks of ionising radiation and the consequences of reactor meltdowns. The group also produced the first study in renewable energy strategies in a report for the European Parliament in 1980; the first comparative study of organic and conventional agriculture, and the first UK study of forestry as carbon sequestration.\n\nTaylor involved both of his brothers during the 1980s campaigns, with Ron infiltrating the US Nevada weapons test site and leading the Greenpeace climb of Big Ben and Robert heading the Greenpeace international strategy on chemical wastes.\n\nIn 1992, PERG evolved into an international network of independent experts on terrestrial and marine ecosystems – Terramarès – to carry out critical science policy analysis. This group worked collectively and individually behind the scenes in several important developments – with Professor Jackson Davis helping to lay the foundation for the Framework Climate Convention, and in Clean Production Strategies and the Precautionary Principle with Tim Jackson (now Professor of Sustainable Development at the University of Surrey); and further work on energy strategies with Gordon Thompson who now leads the Institute for Resource and Security Studies in Cambridge, Massachusetts (ref IRSS). Taylor's work on ocean pollution culminated in 1993 with a critique of the UN's ocean protection system in the peer-reviewed journal Bulletin of Marine Pollution.\n\nHe then moved from Oxford to North Wales to follow up his long standing interests in wildlife conservation and shamanism. As a member of the British Association of Nature Conservationists he organised the conference 'Wilderness Britain' in 1995 and organised a National Trust seminar on wilderness and wildland values at its Centennial Conference. He was a keynote speaker at the BANC/National Trust 1999 'Nature in Transition' conference in July 1999 and co-authored the National Trust's document 'Call for the Wild'. His many articles in BANC's journal ECOS contributed to the new wave of consciousness in conservation known as 'rewilding' culminating in 2005 with the publication of \"Beyond Conservation\" and the founding of the Wildland Network. In this work Taylor argues that conservation is too preservation oriented and needs to be more creative and focussed upon wilder and larger scale land management. Professor Chris Baines one of Britain's leading conservation experts described Taylor's book as important and brilliantly capturing the changing mood of conservation and Peter Marren gave it a whole page spread in the Independent. Alan Watson Featherstone, of Trees For Life endorsed the cover with 'this book offers a beacon of hope to all those who draw spiritual sustenance from wild Nature' and Professor Bill Adams, at Cambridge University, also endorsing the cover said 'Peter Taylor builds bridges between ecology, countryside policy and spirituality'. In networking ecological practitioners and land managers, Taylor worked to construct a political strategy for rewilding conservation through regional seminars, national conferences and in 2008, his colleague in the network Steve Carver founded the Wildland Research Institute at Leeds University.\n\nIn the lead up to his work on climate change, Taylor had been engaged at government level to develop strategies for the integration of renewable energy into countryside policy on community and biodiversity. Between 2000 and 2003, he was appointed to the UK National Advisory Group of the Community Renewables Initiative – a joint Countryside Agency and Department of Trade and Industry task-force on community scale renewable energy. To aid this work he set up the design consultancy Ethos, which combined science expertise from Terramarès with graphic design and the use of computer virtual reality for visualising change and integrating development in the countryside.\n\nHis controversial reassessment of global warming theory in 2009 outlined his concern that the remedies for climate change might prove more damaging to the environment than the ailment itself. Taylor argued that his work with the CRI had given him a deeper insight into the impacts associated with powering a modern economy from renewable sources. Despite an endorsement by the drafting author of the Kyoto Protocol – Professor Jackson Davis, with whom he worked at the UN and who had played a role in setting up the Framework Climate Convention, the book received little publicity at first – but in the lead up to the Copenhagen summit, his views were widely publicized. However, Taylor is critical of how the environmental movement and left-liberal press have ignored the book - it was not reviewed in The Guardian, The Independent, The Observer, nor in New Scientist, all of which had previously covered his work. He has become a target of \"ad hominem\" attacks focussed either on whether he is adequately qualified to review climate science or on his embrace of mystical philosophy and shamanism.\nHis detractors have focussed upon his statements in \"Shiva's Rainbow\" of how science in public policy is mostly theatre and how he was more of an actor than a scientist. Taylor admits freely that most of his work was as a lawyer - 'the ultimate actor', but argues that his record as a skilled and experienced policy analyst has been glossed over by those who are averse to his message on climate change.\n\nIn his autobiography, Taylor gives an account of studying with the yogic master Babaji, training with the founder of rebirthing Leonard Orr, and practising as a breathing therapist and teacher of meditation – in which he now has an international reputation, whilst at the same time being heavily involved in environmental activism. In recent years he has also trained with western shamanic practitioners and brings this perspective into his ecological conservation work and is often invited to speak at 'alternative' conventions where he has outlined his understanding of the connections between science and consciousness. Taylor featured in Karen Sawyer's \"The Dangerous Man\" as someone who challenges the fixed paradigms of science and social control.\n\nHe warns in his recent presentations and in his autobiography, that humanity faces a crisis of consciousness and that much of the enthusiasm and caring for the Earth especially among young people, is being channelled into collusion with undemocratic corporate power structures in the banking world. In this vein, he argues in \"Chill\" that mitigation of climate change is a delusion and that resources need to be channelled into adaptation and the creation of resilient human communities and a robust biodiversity.\n\n\n\n\nA complete list of eighteen PERG, Terramarès and Ethos reports and nine peer-reviewed scientific papers can be found at www.ethos-uk.com by consulting Peter Taylor's detailed CV.\n\n"}
{"id": "37528192", "url": "https://en.wikipedia.org/wiki?curid=37528192", "title": "Plug-in electric vehicles in Japan", "text": "Plug-in electric vehicles in Japan\n\n, the fleet of light-duty plug-in electric vehicles in Japan ranked as the third largest in the world after China and the United States, with about 151,250 highway legal plug-in electric vehicles sold in the country since 2009. The rate of growth of the Japanese plug-in segment slowed down from 2013, with annual sales falling behind Europe, the U.S. and China during 2014 and 2015. The segment market share fell from 0.68% in 2014 to 0.59% in 2016. The decline in plug-in car sales reflects the Japanese government and the major domestic carmakers decision to adopt and promote hydrogen fuel cell vehicles instead of plug-in electric vehicles, although the first commercially produced hydrogen fuel cell automobiles began in 2015.\n\n, the Nissan Leaf all-electric car ranked as the all-time top selling plug-in electric vehicle in the country, with 68,819 units sold since December 2010. Ranking second is the Mitsubishi Outlander P-HEV with 34,830 units delivered through August 2016, followed by the Toyota Prius PHV with 22,100 units sold through April 2016. \n\n, Japan was the country with the highest ratio of quick charging points to electric vehicles (EVSE/EV), with a ratio of 0.030 . The country's charging infrastructure included 1,381 public quick-charge stations and around 300 non-domestic slow charger points. The Japanese government has set up a target to deploy 2 million slow chargers and 5,000 fast charging points by 2020.\n\nCumulative light-duty plug-in electric vehicle sales in Japan totaled about 151,250 units between July 2009 and December 2016, consisting of 86,390 all-electric cars (57.1%) and 64,860 plug-in hybrids (42.9%). At the end of 2016, Japan ranked as the world's third largest light-duty plug-in vehicle country market after the China and the U.S. , total Japanese sales of light-duty plug-in vehicles represent 8.1% of the global stock of plug-ins. The plug-in segment sales climbed from 1,080 units in 2009 to 12,630 in 2011, and reached 24,440 in 2012. Only all-electric cars were sold in the country between 2009 and 2011. Global sales of pure electric cars in 2012 were led by Japan with a 28% market share of the segment sales. Japan ranked second after the U.S. in terms of its share of plug-in hybrid sales in 2012, with 12% of global sales. \n\nAbout 30,600 highway-capable plug-in electric vehicles were sold in the country in 2013, representing a 0.58% market share of the 5.3 million new automobiles and kei cars sold in 2013. In 2014 the segment sales remained flat with over 30,000 plug-in electric vehicles were sold, with the plug-in market share achieving a record market share of 1.06% of new car sales (kei cars not included). Accounting for kei cars, the plug-in segment market share falls to 0.7%. During 2014, cumulative plug-in sales in the Japanese market passed the 100,000 unit mark. However, as a result of the slow growth from 2013, Japan was surpassed in 2014 by China, with over 50,000 units sold, as the second largest world market that year. Sales totaled 24,660 units in 2015 and 24,851 units in 2016. The segment market share declined from 0.68% in 2014 to 0.59% in 2016. As a result of the slow down in sales that occurred after 2013, annual sales fell behind Europe, the U.S. and China during 2014 and 2015.\n\nThe first electric car available in the Japanese market was the Mitsubishi i MiEV, launched for fleet customers in Japan in late July 2009. Retail sales to the public began in April 2010. Cumulative sales since July 2009 reached 11,144 i-MiEVs through April 2016. Sales of the Mitsubishi Minicab MiEV electric van began in December 2011, and a total of 6,172 units have been sold through April 2016. A truck version of the Minicab MiEV was launched in January 2013, with sales of 927 units through April 2016. Mitsubishi also launched in January 2013 a plug-in hybrid version of the Outlander, called the Mitsubishi Outlander P-HEV, becoming the first SUV plug-in hybrid in the world's market. The SUV has an all-electric range of . The Outlander P-HEV sold 9,608 units during 2013, ranking as the second top selling plug-in electric car in Japan after the Nissan Leaf. , Mitsubishi Motors had sold 52,234 plug-in electric vehicles in Japan since July 2009.\n\nThe first prototype battery switch station from Better Place was demonstrated in Yokohama on May 14, 2009. On April 2010, a 90-day switchable-battery electric taxi demonstration project was launched in Tokyo, using three Nissan Rogue\ncrossover utility vehicles, converted into electric cars with switchable batteries provided by A123 Systems. The battery switch station deployed in Tokyo is more advanced than the Yokohama switch system demonstrated in 2009. During the three-month field test the EV taxis accumulated over and swapped batteries 2,122 times, with an average battery swap time of 59.1 seconds. Nissan decided to continue the trial until late November 2010.\n\nSales of the Nissan Leaf began on December 22, 2010, when the first 10 Leaf were delivered at the Kanagawa Prefecture. The Prefecture Government decided to assign six Leafs for official use and the other four were made available for the car rental service run by the local government. Sales of the Toyota Prius Plug-in Hybrid began in January 2012, and a total of 19,100 units have been sold through September 2014. The Honda Accord Plug-in Hybrid was introduced in Japan in June 2013 and it is available only for leasing, primarily to corporations and government agencies. , the Accord PHEV ranked as the third best selling plug-in hybrid in the Japanese market.\n\nSales of the plug-in electric drive segment during 2013 were led by the Nissan Leaf with 13,021 units sold, up from 11,115 in 2012, allowing the Leaf to continue as the top selling plug-in electric car in the country since 2011. Also during 2013, a total of 45 Nissan NMC all-electric low-speed neighborhood vehicles were sold in the country. Sales during the first nine months of 2014 again were led by the Nissan Leaf with 10,877 units, followed by the Outlander P-HEV with 8,630 units, together representing 78.8% of the plug-in segment sales during this period.\n\nRetail deliveries of the Tesla Model S began in Japan in September 2014. The Leaf continued as the market leader in 2014 for the fourth year running with 14,177 units sold, followed by the Outlander P-HEV with 10,064 units, together representing about 80% of the plug-in segment sales in Japan in 2014.\n\nIn 2015 the Outlander plug-in hybrid surpassed the Leaf as the top selling plug-in electric car in the country that year with 10,996 units sold, while the Leaf sold 9,057 units. Japan is the Outlander P-HEV largest country market with 30,668 units sold through December 2015. Nevertheless, at the end of 2015 the Nissan Leaf continued to rank as the all-time best-selling plug-in car in the country with 57,699 units sold. , cumulative sales of plug-in electric cars totaled 126,420 units since 2009.\n\nDuring the first eight months of 2016 the Nissan Leaf led sales with 11,120 units delivered. Since December 2010, Nissan has sold 68,819 units through August 2016, making the Leaf the all-time best-selling plug-in car in the country. Between January and August 2016, a total of 4,162 Outlander P-HEVs were sold in Japan. Sales of the Outlander plug-in hybrid fell sharply from April 2016 as a result of Mitsubishi's fuel mileage scandal. Since its inception, sales of the plug-in hybrid totaled 34,830 units through August 2016.\n\nThe following table presents sales for the top selling highway-capable plug-in electric vehicles by year since July 2009 up to April 2016. \n\nThe rate of growth of the Japanese plug-in segment slowed down from 2013, with annual sales falling behind Europe, the U.S. and China during 2014 and 2015. This trend reflects the Japanese government and the major domestic carmakers decision to adopt and promote hydrogen fuel cell vehicles instead of plug-in electric vehicles. The Japanese strategy aims to focus in investing heavily in fuel-cell technology and infrastructure as part of a national policy to foster what it calls a hydrogen society, where the zero-emission fuel would power homes and vehicles.\n\nIn August 2012, Toyota announced its plans to start retail sales of a hydrogen fuel-cell sedan in California in 2015. Toyota expects to become a leader in this technology. In addition, in September 2012 Toyota announced that is backing away from fully electric vehicles. The company's vice chairman, Takeshi Uchiyamada, said \"\"The current capabilities of electric vehicles do not meet society’s needs, whether it may be the distance the cars can run, or the costs, or how it takes a long time to charge\".\" Toyota's emphasis would be re-focused on the hybrid concept, and 21 new hybrid gas-electric models scheduled to be on the market by 2015.\n\nAs part of Toyota's effort to maintain its alternative propulsion lead, it launched for retail customers the Toyota Mirai hydrogen fuel cell vehicle in late 2014, and Honda plans to launch the Clarity Fuel Cell by late 2016. Toyota is responding to interest in the hydrogen economy in its home market, where, , there were 100,000 residential hydrogen fuel cells already installed across Japan. The country is aiming for 5.3 million households, or roughly 10%, to have fuel cells by 2030. Nevertheless, in June 2015 Toyota announced its plans to continue a strong promotion of plug-in hybrids starting with the introduction of the Prius Prime.\n\nIn September 2016, Shoichi Kaneko, assistant chief engineer for the Prius Prime, said in an interview with the website AutoblogGreen that creating the next-generation Prius will be a tremendously difficult challenge due to the physical limitations to improve the Prius' fuel economy. And considering that Toyota \"wants to lead the way in reducing (and eventually eliminating) fossil fuels from its vehicles, simply making a better standard hybrid powertrain might not be enough,\" the carmaker is considering making every future Prius a plug-in hybrid beginning with the fifth-generation models.\n\nThe Japanese electric vehicle charging infrastructure climbed from only 60 public charging stations in early 2010 to 1,381 public quick-charge stations , representing the largest deployment of fast chargers in the world. The number of non-domestic slow charger points increased to around 300 units. Japan also is the country with the highest ratio of quick charging points to electric vehicles (EVSE/EV), with a ratio of 0.030 . There are 1,967 CHAdeMO quick charging stations across the country by April 2014. The Japanese government has set up a target to deploy 2 million slow chargers and 5,000 fast charging points by 2020. Currently all Family Mart convenience stores with sufficient parking space have one space specialized for quick-charge use or are in the process of having one installed.\n\nThe Japanese government introduced the first electric vehicle incentive program in 1996, and it was integrated in 1998 with the Clean Energy Vehicles Introduction Project, which provided subsidies and tax discounts for the purchase of electric, natural gas, methanol and hybrid electric vehicles. The project provided a purchase subsidy of up to 50% the incremental costs of a clean energy vehicle as compared with the price of a conventional engine vehicle. This program was extended until 2003.\n\nIn May 2009 the Japanese Diet passed the \"Green Vehicle Purchasing Promotion Measure\" that went into effect on June 19, 2009, but retroactive to April 10, 2009. The program established tax deductions and exemptions for environmentally friendly and fuel efficient vehicles, according to a set of stipulated environmental performance criteria, and the requirements are applied equally to both foreign and domestically produced vehicles. The program provided purchasing subsidies for two type of cases, consumers purchasing a new passenger car without trade-in (non-replacement program), and for those consumers buying a new car trading an used car registered 13 years ago or earlier (scrappage program).\n\nSubsidies for purchases of new environmentally friendly vehicles without scrapping a used car are 100,000 yen (~US$1,100) for the purchase of a standard or small car, and 50,000 yen (~US$550) for the purchase of a mini or kei vehicle. Subsidies for purchasing trucks and buses meeting the stipulated fuel efficiency and emission criteria vary between 200,000 yen (~US$2,100) to 900,000 yen (~US$9,600).\n\nSubsidies for purchases of new environmentally friendly vehicles in the case of owners scrapping a 13-year or older vehicle are 250,000 yen (~US$2,700) for the purchase of a standard or small car, and 125,000 yen (~US$1,300) for the purchase of a mini or kei vehicle. Subsidies for purchasing trucks and buses meeting the stipulated fuel efficiency and emission criteria vary between 400,000 yen (~US$4,300) to 1,800,000 yen (~US$19,000).\n\nAll incentives for new purchases with or without trading were applicable in Japan's fiscal year 2009, from April 1, 2009 through March 31, 2010.\n\n\n"}
{"id": "5735365", "url": "https://en.wikipedia.org/wiki?curid=5735365", "title": "Portsmouth Water", "text": "Portsmouth Water\n\nPortsmouth Water is the utility company responsible for water supply and distribution in the City of Portsmouth, part of East Hampshire and part of West Sussex. Places served include Gosport, Fareham, Portsmouth, Havant, Chichester and Bognor Regis. The company is a private limited company with company number 2536455.\n\nStarted in 1857 as the \"Borough of Portsmouth Waterworks Company\" to supply Portsmouth.\nJoined with Gosport Waterworks Company in 1955. A new HQ in Havant was opened in 1966.\n\nThere are now 18 borehole sites throughout the catchment area from the South Downs groundwater and the natural springs in Havant and Bedhampton. There are up to 25 springs used at any one time producing up to 170 million litres per day.\n\n"}
{"id": "57362029", "url": "https://en.wikipedia.org/wiki?curid=57362029", "title": "Pump as turbine", "text": "Pump as turbine\n\nA Pump As Turbine or (PAT), otherwise known as a ‘Pump in Reverse’, is an unconventional type of reaction water turbine, which behaves in a similar manner to that of a Francis turbine. The function of a PAT is comparable to that of any turbine, to convert kinetic and pressure energy of the fluid into mechanical energy of the runner. They are commonly commercialized as composite pump and motor/generator units, coupled by a fixed shaft to an asynchronous induction type motor unit.\n\nUnlike other conventional machines which require being manufactured according to the client’s specifications, pumps are a very common piece of equipment widely available in different sizes and functionality anywhere around the globe. When used as a turbine, the rotor moves in the opposite direction, or in reverse, as to when it is operating as a pump. In this manner, it allows the motor may generate electrical power.\n\nFirst mentions of the possibility of using pumps as turbines (PAT) dates back to the early 1930s and are associated to lab experiments performed by Thoma and Kittredge, who first identified the potential for a common pump to function quite efficiently as a turbine by reversing the flow.\n\nSubsequently, in the second half of the 20th century, a new impulse for research on this topic came from the pump manufacturing industry. During this time, established collaborations with several research institutes helped develop an in-depth understanding of the phenomena associated with PAT utilization. Efforts were made to develop methods to predict characteristic and efficiency curves. This helped determine the Best Efficiency Point (BEP) of these machines, in turbine mode, and related it to its specifications when used as a pump.\n\nAmong the existing designs of hydraulic pumps/PATs, \"centrifugal\" or \"radial\" units are the most used worldwide in a wide variety of application fields. The name is derived from the radial path followed by the fluid in the rotor: from the centre to the periphery when running as a pump and in the opposite direction when flow is reversed. To achieve a higher head drop across the machine, more impellers can be assembled in series to create a multistage unit. Conversely, a double flow radially split pump/PAT design involves a single radial open rotor fed by two symmetric inlets and enable processing a higher flow rate with respect to a standard radial unit.\nA second type of pump/PAT design is the axial one, in which the fluid interacts with a propeller following a trajectory parallel to the pump axis. Such units are particularly suitable to processing high flow rates with low head difference. Finally, mixed flow pumps/PATs stand in between the applicability range of radial and axial units and have an impeller shaped in a similar way as a Francis turbine.\nAnother special pump/PAT design is that of submersible units, which can possibly be fitted inside a pipe connected to draft tube exploiting small head differences in flowing rivers.\n"}
{"id": "2616884", "url": "https://en.wikipedia.org/wiki?curid=2616884", "title": "Robert Ulanowicz", "text": "Robert Ulanowicz\n\nRobert Edward Ulanowicz is an American theoretical ecologist and philosopher of Polish descent who in his search for a \"unified theory of ecology\" has formulated a paradigm he calls \"Process Ecology\". He was born September 17, 1943 in Baltimore, Maryland.\n\nHe served as Professor of Theoretical Ecology at the University of Maryland Center for Environmental Science's Chesapeake Biological Laboratory in Solomons, Maryland until his retirement in 2008. Ulanowicz received both his BS and PhD in chemical engineering from Johns Hopkins University in 1964 and 1968, respectively.\n\nUlanowicz currently resides in Gainesville, Florida, where he holds a Courtesy Professorship in the Department of Biology at the University of Florida. Since relocating to Florida, Ulanowicz has served as a scientific advisor to the Howard T. Odum Florida Springs Institute, an organization dedicated to the preservation and welfare of Florida's natural springs.\n\nUlanowicz uses techniques from information theory and thermodynamics to study the organization of flows of energy and nutrients within ecosystems. Although his ideas have been primarily applied in ecology, many of his concepts are abstract and have been applied to other areas in which flow networks arise, such as psychology and economics.\n\nThough Ulanowicz began his career modeling ecological systems using differential equations, he soon reached the limits of this approach. Realizing that any ecosystem is a complex system, he decided to move away from what he saw as the inappropriate use of the reductionist approach, and instead began to work towards development of theoretical measures of the ecosystem as a whole, such as ascendency. Gradually, he came to appreciate the ecosystem behavior is not simply a matter of \"mechanics with noise,\" but rather an intricate interplay between opposing tendencies—autocatalytic-like self-organization and entropic decay. This natural conversation could be followed quantitatively using information-theoretic measures applies to networks of trophic processes.\n\nFollowing Gregory Bateson, Ulanowicz points out how ecology differs significantly from physics in that constraints that are absent play important roles in ecosystem dynamics. He also argues how the homogeneous laws of physics only constrain the behavior of very heterogeneous ecosystems but are incapable by themselves of determining outcomes. He goes so far as to suggest that an entirely new metaphysics, which he calls \"Process Ecology\", is required to understand complex living systems.\n\nOne pertinent discovery by Ulanowicz was that ecosystems do not progress to maximum efficiency. Ecosystems that channel too much activity along the most efficient pathways do so at the expense of redundant, less-efficient processes that can function to take over vital activities in the event that mainstream processes are distributed. Ecosystems that persist are those that achieve a balance between the mutually exclusive attributes of efficiency and reliability. This result from nature poses a significant challenge to mainstream economics, wherein market efficiency is held to be the \"sine qua non\".\n\nUlanowicz has authored or co-authored over two hundred articles in theoretical ecology and related areas of philosophy, especially those dealing with autocatalysis and causality. He has authored three books to date.\n\n\nWhile living in Maryland, Ulanowicz took up a hobby of cultivating and casually breeding cold-hardy palm trees; he drew attention for a Windmill palm on Solomons Island that grew taller than the one-story building it was planted outside.\n\nUlanowicz was named the recipient of the 2007 Ilya Prigogine Medal for outstanding research in ecological systems. He participated in the Stock Exchange of Visions project in 2007.\n\nUlanowicz was a featured speaker at the 2009 Ill STOQ International Conference entitled \"Biological Evolution: Facts and Theories,\" which discussed the impacts and effects of the publication of \"On the Origin of Species\" by Charles Darwin.\n\n\n"}
{"id": "48890004", "url": "https://en.wikipedia.org/wiki?curid=48890004", "title": "Sandpiper pipeline", "text": "Sandpiper pipeline\n\nThe Sandpiper pipeline is a underground oil pipeline project in the United States. It would carry light crude oil from the Bakken oil fields in Northwest North Dakota, through Minnesota, to end in Superior, Wisconsin. Enbridge Energy Partners, and Williston Basin Pipe Line LLC, an indirect subsidiary of Marathon Petroleum Corporation have been planning the project since 2013. In 2015 Enbridge estimated the pipeline will cost about $2.6 billion.\n\nThe Sanpiper pipeline project was made public by the media in 2013, and informational hearings for landowners took place in three North Dakota towns during March 2014. The North Dakota Public Service Commission approved the pipeline in June 2014. The Minnesota Public Utilities Commission unanimously approved the Sandpiper pipeline, but its decision was overturned in September 2015. \n\nIn September 2016, Enbridge Energy Partners announced that due to \"extensive and unprecedented [regulatory] delays [which] have plagued the Sandpiper pipeline,\" they were withdrawing their state application and asking for an end to regulatory proceedings, including work on an environmental-impact statement. An Enbridge spokesperson said that the pipeline may be reconsidered once the oil market rebounds but it was then \"outside the company’s current five-year planning horizon\".\n\nIn 2015, Enbridge stated that \"The Sandpiper Pipeline serves the oil conducting needs of North Dakota residents, which constitutes a public benefit\". Per Enbridge, the Sandpiper pipeline represents a \"public use\", a \"statutorily defined public utility.\", and its route was chosen with the \"greatest public benefit and the least private injury.\" and \"As long as the public benefit can be demonstrated, it is immaterial that private interests are also served.\" \n\nPer Enbridge, the pipeline is necessary \"to meet demand for Bakken oil\".\nThe corporation projects economic benefits of $69 million in property tax revenue for the 3 states, and 3000 construction jobs for workers in Minnesota and North Dakota.\n\nThe pipeline would enter Minnesota just south of Grand Forks, North Dakota, east to Clearbrook Enbridge's terminal. and then south toward Park Rapids along an existing crude oil corridor. Afterwards, the pipeline would run in a transmission line corridor to Superior, Wisconsin.\n\nThe route passes through 28 rivers, including the Mississippi River headwaters, and lakes and wetlands that can’t be reached by nearby roads if a spill should occur.\n\nInformational hearings for landowners took place in three North Dakota towns during March 2014. The North Dakota Public Service Commission approved the pipeline on 25 June 2014.\n\nEnbridge sued a couple in Grand Forks in 2014, because they refused to give Enbridge an easement and right-of-way. the couple quoted NDPL's abuse of eminent domain, continued reliance on fossil fuels their effect on the environment and possibility for spills as arguments. In August 2015 the couple agreed on an easement, and forfeited compensation, in order to file an appeal to the North Dakota Supreme Court.\n\nIn November 2013, Enbridge applied at the Minnesota Public Utilities Commission (MPUC). The MPUC unanimously approved the project, allowing to do an environmental review later. In September 2015, the {Minnesota Court of Appeals overruled the PUC decision as a violation of state law.\n\nIn a November 2014 Star Tribune commentary a Polk County commissioner, a Clearwater County commissioner and a Red Lake County commissioner opined, that the Sandpiper pipeline was the \"best choice for the state...better than trucks or rail and also offer[ing] economic benefits.\"\n\nIn February 2015, the White Earth Indian Reservation, represented by Winona LaDuke stated that the pipeline would cross a portion of its land, which Enbridge disputes. La Duke has been against the pipeline because it would violate Indian sovereignty and for environmental reasons.\n\nThe President of North America's Building Trades Unions came out in a December 2015 commentary criticizing the Minnesota Court of Appeals decision, accused the court was \"robbing hard-working Minnesotans of jobs\" which would provide workers with a path to middle class.\n\n"}
{"id": "28088088", "url": "https://en.wikipedia.org/wiki?curid=28088088", "title": "Thai Airways International Flight 601", "text": "Thai Airways International Flight 601\n\nThai Airways International Flight 601 was a Sud Aviation Caravelle that crashed into the sea on landing at the former Kai Tak Airport, Hong Kong, in a typhoon on Friday 30 June 1967.\n\nThai Airways International Flight 601 took off from Taipei Songshan Airport (TSA/RCSS) on a flight to Kai Tak Airport. The Sud Aviation Caravelle (Registration: HS-TGI which had its first flight in 1960) had 73 passengers and 7 crew on board and after a 1-hour flight was on an ILS approach to land on runway 31 at Hong Kong-Kai Tak International Airport (HKG/VHHH). With a typhoon over Hong Kong at the time of the approach, the Captain was busy looking for visual contact with the ground not noticing that the aircraft had descended below the decision height of . The aircraft made an abrupt heading change (while already below the glide slope), then entered a high rate of descent and crashed into the sea short of runway 31 under-shoot. The accident killed 24 passengers out of 80 passengers and crew on board. \n\nThe probable cause of the accident was crew error, not noticing that the aircraft had descended below the glide slope. The presence of strong wind shear and downdrafts is also a probable major factor, however at the time of the accident there was no means of measuring these phenomena available at Kai Tak.\n\n"}
{"id": "7728600", "url": "https://en.wikipedia.org/wiki?curid=7728600", "title": "Tomin", "text": "Tomin\n\nThe tomín (plural tomines, abbreviated t) is an antiquated Spanish unit of weight and currency. It was equivalent to one-eighth of a \"peso\", both in currency and weight, and derived from the Arabic term \"tomn\" (\"one eighth\").\n"}
{"id": "302015", "url": "https://en.wikipedia.org/wiki?curid=302015", "title": "Top", "text": "Top\n\nA spinning top is a toy designed to spin rapidly on the ground, the motion of which causes it to remain precisely balanced on its tip because of its rotational inertia. Such toys have existed since antiquity. Traditionally tops were constructed of wood, sometimes with an iron tip, and would be set in motion by aid of a string or rope coiled around its axis which, when pulled quickly, caused a rapid unwinding that would set the top in motion. Today they are often built of plastic, and modern materials and manufacturing processes allow tops to be constructed with such precise balance that they can be set in motion by a simple twist of the fingers and twirl of the wrist without need for string or rope.\n\nThe motion of a top is produced in the most simple forms by twirling the stem using the fingers. More sophisticated tops are spun by holding the axis firmly while pulling a string or twisting a stick or pushing an auger. In the kinds with an auger, an internal weight rotates, producing an overall circular motion. Some tops can be thrown, while firmly grasping a string that had been tightly wound around the stem, and the centrifugal force generated by the unwinding motion of the string will set them spinning upon touching ground.\n\nThe top is one of the oldest recognizable toys found on archaeological sites. Spinning tops originated independently in cultures all over the world. Besides toys, tops have also historically been used for gambling and prophecy. Some role-playing games use tops to augment dice in generating randomized results; it is in this case referred to as a \"spinner\". A thumbtack may also be made to spin on the same principles. Gould mentions maple seeds, celts (leading to rattlebacks), the fire-drill, the spindle whorl, and the potter's wheel as possible predecessors to the top, which he assumes was invented or discovered multiple times in multiple places.\n\nThe action of a top is described by equations of rigid body dynamics (see the section Rotation in three dimensions). Typically the top will at first wobble until the shape of the tip and its interaction with the surface force it upright; contrary to what is sometimes assumed, longstanding scientific studies (and easy experimentations reproducible by anyone) show that less friction increases the time before the upright position is reached (unless the top is so unbalanced that it falls before reaching it). After spinning upright (in the so-called \"sleep\" position) for an extended period, the angular momentum will gradually lessen (mainly due to friction), leading to ever increasing precession, finally causing the top to topple in a frequently violent last thrash. In the \"sleep\" period, and only in it, provided it is ever reached, less friction means longer \"sleep\" time (whence the common error that less friction implies longer global spinning time)\n\nThere have been many developments within the technology of the top. Bearing tops, with a tip made of a small hard ceramic, tungsten carbide or even ruby ball (sometimes wrongly believed to spin with respect to the body of the top), have been one of the biggest changes. In addition, plastic and metal have largely supplanted the use of wood in tops. Fixed tip tops are featured in National Championships in Chico, California and in the World Championships in Orlando, Florida.\n\nThe following two well-known characteristics of a top that increase its global spinning time () are exploited in various ways in such championships and by the main top makers (obviously, an optimisation of both characteristics has to be somehow restricted by conditions on mass and/or diameter and by practical considerations about the rim not touching the spinning surface too early):\n\n- low center of gravity\n\n- high moment of inertia\nA top may also be used to demonstrate visual properties, such as by James David Forbes and James Clerk Maxwell in Maxwell's disc (see color triangle). By rapidly spinning the top, Forbes created the illusion of a single color that was a mixture of the primaries:\n\nMaxwell took this a step further by using a circular scale around the rim with which to measure the ratios of the primaries, choosing vermilion (V), emerald (EG), and ultramarine (U).\n\nAsymmetric tops of virtually any shape can also be created and designed to balance.\n\nGould lists the six main types of tops as the twirler, supported top, peg-top, whip-top, buzzer, and yo-yo.\n\nfind)\n\nThe Jean Shepherd story \"Scut Farkas and the Murderous Mariah\" revolves around top-spinning in the fictional Depression-era American city of Hohman, Indiana. The bully and the named top in the title are challenged by Shepherd's ongoing protagonist Ralph and a so-called \"gypsy top\" of similar design to Mariah named Wolf.\n\nThe Top is a short story by Czech writer Franz Kafka.\n\n\n"}
{"id": "29000170", "url": "https://en.wikipedia.org/wiki?curid=29000170", "title": "Trash Inc: The Secret Life of Garbage", "text": "Trash Inc: The Secret Life of Garbage\n\nTrash Inc: The Secret Life of Garbage is a one-hour television documentary film that aired on CNBC on September 29, 2010 about trash/garbage, what happens to it when it's \"thrown away\", and its impact on the world. The film is hosted by CNBC \"Squawk Box\" co-anchor Carl Quintanilla as he reports from various landfills (such as the largest in the United States, the Apex Landfill in Clark County, Nevada), business, and other locations in the United States (New York, New Jersey, Hawaii, South Carolina) and China (mostly Beijing).\n\nThe idea for \"Trash, Inc\" was born of the 2008 recession and the relative stability of publicly traded waste management companies.\n"}
{"id": "26098877", "url": "https://en.wikipedia.org/wiki?curid=26098877", "title": "Uranium hydride", "text": "Uranium hydride\n\nUranium hydride, also called uranium trihydride (UH), is an inorganic compound and a hydride of uranium.\n\nUranium hydride is a highly toxic, brownish grey to brownish black pyrophoric powder or brittle solid. Its density at 20 °C is 10.95 g cm, much lower than that of uranium (19.1 g cm). It has a metallic conductivity, is slightly soluble in hydrochloric acid and decomposes in nitric acid.\n\nTwo crystal modifications of uranium hydride exist, both cubic: an α form that is obtained at low temperatures and a β form that is grown when the formation temperature is above 250 °C. After growth, both forms are metastable at room temperature and below, but the α form slowly converts to the β form upon heating to 100 °C. Both α- and β-UH are ferromagnetic at temperatures below ~180 K. Above 180 K, they are paramagnetic.\n\nExposition of uranium metal to hydrogen leads to hydrogen embrittlement. Hydrogen diffuses through metal and forms a network of brittle hydride over the grain boundaries. Hydrogen can be removed and ductility renewed by annealing in vacuum.\n\nUranium metal heated to 250 to 300 °C (482 to 572 °F) reacts with hydrogen to form uranium hydride. Further heating to about 500 °C will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. The reversible reaction proceeds as follows:\n\nUranium hydride is not an interstitial compound, causing the metal to expand upon hydride formation. In its lattice, each uranium atom is surrounded by 6 other uranium atoms and 12 atoms of hydrogen; each hydrogen atom occupies a large tetrahedral hole in the lattice. The density of hydrogen in uranium hydride is approximately the same as in liquid water or in liquid hydrogen. The U-H-U linkage through a hydrogen atom is present in the structure.\n\nUranium hydride forms when uranium metal (e.g. in Magnox fuel with corroded cladding) becomes exposed to water; the reaction proceeds as follows:\nThe resulting uranium hydride is pyrophoric; if the metal (e.g. a damaged fuel rod) is exposed to air afterwards, excessive heat may be generated and the bulk uranium metal itself can ignite. Hydride-contaminated uranium can be passivated by exposition to a gaseous mixture of 98% helium with 2% oxygen. Condensed moisture on uranium metal promotes formation of hydrogen and uranium hydride; a pyrophoric surface may be formed in absence of oxygen. This poses a problem with underwater storage of spent nuclear fuel in spent fuel ponds. Depending on the size and distribution on the hydride particles, self-ignition can occur after an indeterminate length of exposure to air. Such exposure poses risk of self-ignition of fuel debris in radioactive waste storage vaults.\n\nUranium metal exposed to steam produces a mixture of uranium hydride and uranium dioxide.\n\nUranium hydride exposed to water evolves hydrogen. In contact with strong oxidizers this may cause fire and explosions. Contact with halocarbons may cause a violent reaction.\n\nPolystyrene-impregnated uranium hydride powder is non-pyrophoric and can be pressed, however its hydrogen-carbon ratio is unfavorable. Hydrogenated polystyrene was introduced in 1944 instead.\n\nUranium deuteride is said to be usable for design of some types of neutron initiators.\n\nUranium hydride enriched to about 5% uranium-235 is proposed as a combined nuclear fuel/neutron moderator for the Hydrogen Moderated Self-regulating Nuclear Power Module. According to the aforementioned patent application, the reactor design in question begins producing power when hydrogen gas at a sufficient temperature and pressure is admitted to the core (made up of granulated uranium metal) and reacts with the uranium metal to form uranium hydride. Uranium hydride is both a nuclear fuel and a neutron moderator; apparently it, like other neutron moderators, will slow neutrons sufficiently to allow for fission reactions to take place; the uranium-235 atoms within the hydride also serve as the nuclear fuel. Once the nuclear reaction has started, it will continue until it reaches a certain temperature, approximately , where, due to the chemical properties of uranium hydride, it chemically decomposes and turns into hydrogen gas and uranium metal. The loss of neutron moderation due to the chemical decomposition of the uranium hydride will consequently slow — and eventually halt — the reaction. When temperature returns to an acceptable level, the hydrogen will again combine with the uranium metal, forming uranium hydride, restoring moderation and the nuclear reaction will start again.\n\nUranium zirconium hydride (UZrH), a combination of uranium hydride and zirconium(II) hydride, is used as a fuel/moderator in the TRIGA-class reactors.\n\nOn heating with diborane, uranium hydride produces uranium boride. With bromine at 300 °C, uranium(IV) bromide is produced. With chlorine at 250 °C, uranium(IV) chloride is produced. Hydrogen fluoride at 20 °C produces uranium(IV) fluoride. Hydrogen chloride at 300 °C produces uranium(III) chloride. Hydrogen bromide at 300 °C produces uranium(III) bromide. Hydrogen iodide at 300 °C produces uranium(III) iodide. Ammonia at 250 °C produces uranium(III) nitride. Hydrogen sulfide at 400 °C produces uranium(IV) sulfide. Oxygen at 20 °C produces triuranium octoxide. Water at 350 °C produces uranium dioxide.\n\nUranium hydride ion may interfere with some mass spectrometry measurements, appearing as a peak at mass 239, creating false increase of signal for plutonium-239.\n\nUranium hydride slugs were used in the \"tickling the dragon's tail\" series of experiments to determine the critical mass of uranium.\n\nUranium hydride and uranium deuteride were suggested as a fissile material for a uranium hydride bomb. The tests with uranium hydride and uranium deuteride during Operation Upshot–Knothole were disappointing, however. During the early phases of the Manhattan Project, in 1943, uranium hydride was investigated as a promising bomb material; however, it was abandoned by the spring of 1944 as it turned out that such a design would be inefficient.\n\nHydrogen, deuterium, and tritium can be purified by reacting with uranium, then thermally decomposing the resulting hydride/deuteride/tritide. Extremely pure hydrogen has been prepared from beds of uranium hydride for decades. Heating uranium hydride is a convenient way to introduce hydrogen into a vacuum system.\n\nThe swelling and pulverization at uranium hydride synthesis can be used for preparation of very fine uranium metal, if the powdered hydride is thermally decomposed.\n\nUranium hydride can be used for isotope separation of hydrogen, preparing uranium metal powder, and as a reducing agent.\n"}
{"id": "13758130", "url": "https://en.wikipedia.org/wiki?curid=13758130", "title": "Vacuum swing adsorption", "text": "Vacuum swing adsorption\n\nVacuum swing adsorption (VSA) is a non-cryogenic gas separation technology.\n\nUsing special solids, or adsorbents, VSA segregates certain gases from a gaseous mixture under minimal pressure according to the species' molecular characteristics and affinity for the adsorbents. These adsorbents (e.g., zeolites) form a molecular sieve and preferentially adsorb the target gas species at near ambient pressure. The process then swings to a vacuum to regenerate the adsorbent material.\n\nVSA differs from cryogenic distillation techniques of gas separation as well as pressure swing adsorption (PSA) techniques because it operates at near-ambient temperatures and pressures. VSA may actually be best described as a subset of the larger category of PSA. It differs primarily from PSA in that PSA typically vents to atmospheric pressures, and uses a pressurized gas feed into the separation process. VSA typically draws the gas through the separation process with a vacuum. For oxygen and nitrogen VSA systems, the vacuum is typically generated by a blower. Hybrid VPSA systems also exist. VPSA systems apply pressurized gas to the separation process and also apply a vacuum to the purge gas. VPSA systems, like one of the portable oxygen concentrators, are among the most efficient systems, measured on customary industry indices, such as recovery (product gas out/product gas in), productivity (product gas out/mass of sieve material). Generally, higher recovery leads to a smaller compressor, blower, or other compressed gas or vacuum source and lower power consumption. Higher productivity leads to smaller sieve beds. The consumer will most likely consider indices which have a more directly measurable difference in the overall system, like the amount of product gas divided by the system weight and size, the system initial and maintenance costs, the system power consumption or other operational costs, and reliability.\n\n\n"}
