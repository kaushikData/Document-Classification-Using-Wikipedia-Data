{"id": "2570077", "url": "https://en.wikipedia.org/wiki?curid=2570077", "title": "(E)-Stilbene", "text": "(E)-Stilbene\n\n(\"E\")-Stilbene, commonly known as \"trans\"-stilbene, is an organic compound represented by the condensed structural formula CHCH=CHCH. Classified as a diarylethene, it features a central ethylene moiety with one phenyl group substituents on each end of the carbon–carbon double bond. It has an (\"E\") stereochemistry, meaning that the phenyl groups are located on opposite sides of the double bond, the opposite of its geometric isomer, \"cis\"-stilbene. \"Trans\"-stilbene occurs as a white crystalline solid at room temperature and is highly soluble in organic solvents. It can be converted to \"cis\"-stilbene photochemically, and further reacted to produce phenanthrene.\n\nStilbene was discovered in 1843 by the French chemist Auguste Laurent. The name \"stilbene\" is derived from the Greek word \"στίλβω\" (\"stilbo\"), which means \"I shine\", on account of the lustrous appearance of the compound.\n\nStilbene exists as two possible stereoisomers. One is \"trans\"-1,2-diphenylethylene, called (\"E\")-stilbene or \"trans\"-stilbene. The second is \"cis\"-1,2-diphenylethylene, called (\"Z\")-stilbene or \"cis\"-stilbene, and is sterically hindered and less stable because the steric interactions force the aromatic rings out-of-plane and prevent conjugation. \"Cis\"-stilbene is a liquid at room temperature (melting point: ), while \"trans\"-stilbene is a crystalline solid which does not melt until around , illustrating the two isomers have significantly different physical properties.\n\nMany syntheses have been developed. One popular route entails reduction of benzoin using zinc amalgam.\n\nBoth isomers of stilbene can be produced by decarboxylation of α-phenylcinnamic acid, \"trans\"-stilbene being produced from the of the acid.\n\nRicard Heck and Tsutomu Mizoroki independently reported the synthesis of \"trans\"-stilbene by coupling of iodobenzene and styrene using a palladium(II) catalyst, in what is now known as the Mizoroki-Heck reaction. The Mizoroki approach produced the higher yield.\n\nStilbene undergoes reactions typical of alkenes. \"Trans\"-stilbene undergoes epoxidation with peroxymonophosphoric acid, HPO, producing a 74% yield of \"trans\"-stilbene oxide in dioxane. The epoxide product formed is a racemic mixture of the two enantiomers of 1,2-diphenyloxirane. The achiral \"meso\" compound (1\"R\",2\"S\")-1,2-diphenyloxirane arises from \"cis\"-stilbene, though peroxide epoxidations of the \"cis\"-isomer produce both \"cis\"- and \"trans\"-epoxide products. For example, using \"tert\"-butyl hydroperoxide, oxidation of \"cis\"-stilbene produces 0.8% \"cis\"-stilbene oxide, 13.5% \"trans\"-stilbene oxide, and 6.1% benzaldehyde. Enantiopure stilbene oxide has been prepared by Nobel laureate Karl Barry Sharpless.\n\nStilbene can be cleanly oxidised to benzaldehyde by ozonolysis or Lemieux–Johnson oxidation, and stronger oxidants such as acidified potassium permanganate will produce benzoic acid. Vicinal diols can be produced via the Upjohn dihydroxylation or enantioselectively using Sharpless asymmetric dihydroxylation with enantiomeric excesses as high as 100%.\n\nBromination of \"trans\"-stilbene produces predominantly \"meso\"-1,2-dibromo-1,2-diphenylethane (sometimes called \"meso\"-stilbene dibromide), in line with a mechanism involving a cyclic bromonium ion intermediate of a typical electrophilic bromine addition reaction; \"cis\"-stilbene yields a racemic mixture of the two enantiomers of 1,2-dibromo-1,2-diphenylethane in a non-polar solvent such as carbon tetrachloride, but the extent of production of the \"meso\" compound increases with solvent polarity, with a yield of 90% in nitromethane. The formation of small quantities of the two enantiomers of stilbene dibromide from the \"trans\"-isomer suggests that the bromonium ion intermediate exists in chemical equilibrium with a carbocation intermediate PhCHBr–C(H)Ph with a vacant p orbital vulnerable to nucleophilic attack from either face. The addition of bromide or tribromide salts restores much of the stereospecificity even in solvents with a dielectric constant above 35.\n\nUpon UV irradiation it converts to \"cis\"-stilbene, a classic example of a photochemical reaction involving \"trans\"-\"cis\" isomerization, and can undergo further reaction to form phenanthrene.\n\n(\"E\")-Stilbene itself is of little value, but it is a precursor to other derivatives used as dyes, optical brighteners, phosphors, and scintillators. Stilbene is one of the gain mediums used in dye lasers.\n\nDisodium 4,4'-dinitrostilbene-2,2'-disulfonate is prepared by the sulfonation of 4-nitrotoluene to form 4-nitrotoluene-2-sulfonic acid, which can then be oxidatively coupled using sodium hypochlorite to form the (\"E\")-stilbene derivative in a process originally developed by Arthur Green and André Wahl in the late nineteenth century. Improvements to the process with higher yields have been developed, using air oxidation in liquid ammonia. The product is useful as its reaction with aniline derivatives results in the formation of azo dyes. Commercially important dyes derived from this compound include Direct Red 76, Direct Brown 78, and Direct Orange 40.\n\nThe stilbenoids are naturally occurring stilbene derivatives. Examples include resveratrol and its cousin, pterostilbene. The stilbestrols, which are structurally but not synthetically related to (\"E\")-stilbene, exhibit estrogenic activity. Members of this group include diethylstilbestrol, fosfestrol, and dienestrol.\n\nTable 1. Vapor pressures\n"}
{"id": "47547523", "url": "https://en.wikipedia.org/wiki?curid=47547523", "title": "1995 Daegu gas explosions", "text": "1995 Daegu gas explosions\n\nThe 1995 Daegu gas explosions was a gas explosion that occurred at Daegu Metro Line 1, a construction site located in Sangin-dong, Daegu, South Korea, on 28 April 1995. At least 101 people, including 42 Yeoungnam Middle School students were killed with as many as 202 people injured.\n\nThe event occurred on April 28th, 1995 at 7:52 am in the underground construction site of the 2nd Section of the Sagley Subway Line 2 at Nam High School in Sangin-dong, Dalseo-gu, Daegu Metropolitan City. Construction work at the Daegu Store (대구점) in Buk-gu, Daegu, of the Lotte Department Store chain, where 31 holes of 75 millimeter width that were going through a grout curtain were accidentally drilled through a city gas pipeline and resulted in gas leakages into the subway construction site through a nearby sewer lead to the explosion due to an unknown fire at the site. The explosion created a 50-meter pillar of fire. The accident killed 101 people, including 42 students, and injured 202. The 400-meter-long private parking lot on the construction site collapsed, and 60 buildings and 152 cars were damaged and the damage reached 54 billion won.\n\n\n\nThe damage caused by the accident occurred during the school day.\n\nThe Ministry of Health, Labor and Welfare, which was set up to deal with the incident, discovered that it was an accident caused by negligence in the development of the Daegu Department Store, which was responsible for the construction of the Daegu Department Store, and arrested nine employees of the company on charges of business misconduct. received. Of course, excavator operation was also included.\n\nThis accident led to the necessity of computerized capitals for underground cargoes, the criticism of the authorities on safety management administration, the increase of the voice of the government and the public on the improvement of the emergency rescue system and integrated structure system, A countermeasure mechanism was established centering on the revision of related laws such as city gas business law.\n\nOne housewife claimed to have reported the smell of gas four hours before the accident, but that night she said \"I wanted to be on TV and lied\".\n\nThere is a monument in Wolsung - dong Hashan Park in Dalseo - gu, Daegu Metropolitan City .\n\nOn April 28, 2005, the last memorial service was held. The memorial ceremony was the last official memorial ceremony on this day. At the ceremony, about 500 people including family members and citizens attended the ceremony and prayed for the victims' souls.\n"}
{"id": "53189191", "url": "https://en.wikipedia.org/wiki?curid=53189191", "title": "Alaska Interconnection", "text": "Alaska Interconnection\n\nThe Alaska Interconnection (ASCC) is an AC power transmission grid in North America that serves Central and Southeast Alaska. While the Alaska Interconnection is often referred to as one interconnected grid, its two parts are not connected to each other through interties, nor are the two grids connected to any other interconnection, making the grids in Alaska isolated circuits. Both grids, though, are managed by the Alaska Systems Coordinating Council as if they were one entity like the other interconnections in North America. \n\nThe Alaska Interconnection is the smallest individual power transmission grid in North America compared to the three other major interconnections--the Western Interconnection, the Eastern Interconnection, and the Texas Interconnection--both in physical area and electricity generated. In 2015, the Alaska interconnection generated 2,601 gigawatt hours of electricity, with natural gas accounting for 1,219 GWh, while the US State of Washington alone--part of the Western Interconnection--generated 47,385 GWh. While both parts of the Alaska Interconnection have no connection to remaining interconnections of North America, all generating units connected to both of its grids generate at a synchronous speed of 60 Hz, the frequency common to North America. Electricity is also delivered to homes and small businesses in the same manner as in Canada and the contiguous United States with a split-phase 240/120 volt service.\n\nIn 2014, an intertie to the Western Interconnection of British Columbia was proposed to the Alaska Energy Authority in order to bring cleaner cheaper power to Alaska, but as of 2016, no further work on the project had been completed due to economic feasibility.\n\nBecause the transmission networks in Alaska are isolated from other interconnections in North America, average rates for electricity are $0.18 per kWh - the second highest price in the United States after Hawaii, whose average residential rate is $0.37 per kWh. In contrast, the average rate for electricity in the 48 contiguous states is $0.10 per kWh.\n\n"}
{"id": "25313821", "url": "https://en.wikipedia.org/wiki?curid=25313821", "title": "Alltwalis Wind Farm", "text": "Alltwalis Wind Farm\n\nThe Alltwalis Wind Farm is a wind farm near Brechfa Forest located to the north of the town of Carmarthen in Wales.\n\nThe facility is composed of 10 wind turbines with a combined installed capacity of 23 megawatts. The expected annual production of electricity is around 65 gigawatt hours (GWh). The farm was built by Statkraft. Construction began in November 2008, and was completed in December 2009.\n\nResidents of the area have reported severe problems with noise pollution from the wind farm leading to health problems for people living in the area. Criticism has been levelled at Statkraft for not taking the residents' concerns seriously.\n\n"}
{"id": "1118042", "url": "https://en.wikipedia.org/wiki?curid=1118042", "title": "Asymptotic giant branch", "text": "Asymptotic giant branch\n\nThe asymptotic giant branch (AGB) is a region of the Hertzsprung–Russell diagram populated by evolved cool luminous stars. This is a period of stellar evolution undertaken by all low- to intermediate-mass stars (0.6–10 solar masses) late in their lives.\n\nObservationally, an asymptotic-giant-branch star will appear as a bright red giant with a luminosity thousands of times greater than the Sun. Its interior structure is characterized by a central and largely inert core of carbon and oxygen, a shell where helium is undergoing fusion to form carbon (known as helium burning), another shell where hydrogen is undergoing fusion forming helium (known as hydrogen burning), and a very large envelope of material of composition similar to main-sequence stars.\n\nWhen a star exhausts the supply of hydrogen by nuclear fusion processes in its core, the core contracts and its temperature increases, causing the outer layers of the star to expand and cool. The star becomes a red giant, following a track towards the upper-right hand corner of the HR diagram. Eventually, once the temperature in the core has reached approximately , helium burning (fusion of helium nuclei) begins. The onset of helium burning in the core halts the star's cooling and increase in luminosity, and the star instead moves down and leftwards in the HR diagram. This is the horizontal branch (for population II stars) or red clump (for population I stars), or a blue loop for stars more massive than about .\n\nAfter the completion of helium burning in the core, the star again moves to the right and upwards on the diagram, cooling and expanding as its luminosity increases. Its path is almost aligned with its previous red-giant track, hence the name \"asymptotic giant branch\", although the star will become more luminous on the AGB than it did at the tip of the red giant branch. Stars at this stage of stellar evolution are known as AGB stars.\n\nThe AGB phase is divided into two parts, the early AGB (E-AGB) and the thermally pulsing AGB (TP-AGB). During the E-AGB phase, the main source of energy is helium fusion in a shell around a core consisting mostly of carbon and oxygen. During this phase, the star swells up to giant proportions to become a red giant again. The star's radius may become as large as one astronomical unit ().\n\nAfter the helium shell runs out of fuel, the TP-AGB starts. Now the star derives its energy from fusion of hydrogen in a thin shell, which restricts the inner helium shell to a very thin layer and prevents it fusing stably. However, over periods of 10,000 to 100,000 years, helium from the hydrogen shell burning builds up and eventually the helium shell ignites explosively, a process known as a helium shell flash. The luminosity of the shell flash peaks at thousands of times the total luminosity of the star, but decreases exponentially over just a few years. The shell flash causes the star to expand and cool which shuts off the hydrogen shell burning and causes strong convection in the zone between the two shells. When the helium shell burning nears the base of the hydrogen shell, the increased temperature reignites hydrogen fusion and the cycle begins again. The large but brief increase in luminosity from the helium shell flash produces an increase in the visible brightness of the star of a few tenths of a magnitude for several hundred years, a change unrelated to the brightness variations on periods of tens to hundreds of days that are common in this type of star.\n\nDuring the thermal pulses, which last only a few hundred years, material from the core region may be mixed into the outer layers, changing the surface composition, a process referred to as \"dredge-up\". Because of this dredge-up, AGB stars may show S-process elements in their spectra and strong dredge-ups can lead to the formation of carbon stars. All dredge-ups following thermal pulses are referred to as third dredge-ups, after the first dredge-up, which occurs on the red-giant branch, and the second dredge up, which occurs during the E-AGB. In some cases there may not be a second dredge-up but dredge-ups following thermal pulses will still be called a third dredge-up. Thermal pulses increase rapidly in strength after the first few, so third dredge-ups are generally the deepest and most likely to circulate core material to the surface.\n\nAGB stars are typically long-period variables, and suffer mass loss in the form of a stellar wind. Thermal pulses produce periods of even higher mass loss and may result in detached shells of circumstellar material. A star may lose 50 to 70% of its mass during the AGB phase.\n\nThe extensive mass loss of AGB stars means that they are surrounded by an extended circumstellar envelope (CSE). Given a mean AGB lifetime of one Myr and an outer velocity of , its maximum radius can be estimated to be roughly (30 light years). This is a maximum value since the wind material will start to mix with the interstellar medium at very large radii, and it also assumes that there is no velocity difference between the star and the interstellar gas. Dynamically, most of the interesting action is quite close to the star, where the wind is launched and the mass loss rate is determined. However, the outer layers of the CSE show chemically interesting processes, and due to size and lower optical depth, are easier to observe.\n\nThe temperature of the CSE is determined by heating and cooling properties of the gas and dust, but drops with radial distance from the photosphere of the stars which are –. Chemical peculiarities of an AGB CSE outwards include:\n\nThe dichotomy between oxygen-rich and carbon-rich stars has an initial role in determining whether the first condensates are oxides or carbides, since the least abundant of these two elements will likely remain in the gas phase as CO.\n\nIn the dust formation zone, refractory elements and compounds (Fe, Si, MgO, etc.) are removed from the gas phase and end up in dust grains. The newly formed dust will immediately assist in surface catalyzed reactions. The stellar winds from AGB stars are sites of cosmic dust formation, and are believed to be the main production sites of dust in the universe.\n\nThe stellar winds of AGB stars (Mira variables and OH/IR stars) are also often the site of maser emission. The molecules that account for this are SiO, HO, OH, HCN, and SiS. SiO, HO, and OH masers are typically found in oxygen-rich M-type AGB stars such as R Cassiopeiae and U Orionis, while HCN and SiS masers are generally found in carbon stars such as IRC +10216. S-type stars with masers are uncommon.\n\nAfter these stars have lost nearly all of their envelopes, and only the core regions remain, they evolve further into short-lived preplanetary nebulae. The final fate of the AGB envelopes are represented by planetary nebulae (PNe).\n\nAs many as a quarter of all post-AGB stars undergo what is dubbed a \"born-again\" episode. The carbon–oxygen core is now surrounded by helium with an outer shell of hydrogen. If the helium is re-ignited a thermal pulse occurs and the star quickly returns to the AGB, becoming a helium-burning, hydrogen-deficient stellar object. If the star still has a hydrogen-burning shell when this thermal pulse occurs, it is termed a \"late thermal pulse\". Otherwise it is called a \"very late thermal pulse\".\n\nThe outer atmosphere of the born-again star develops a stellar wind and the star once more follows an evolutionary track across the Hertzsprung–Russell diagram. However, this phase is very brief, lasting only about 200 years before the star again heads toward the white dwarf stage. Observationally, this late thermal pulse phase appears almost identical to a Wolf–Rayet star in the midst of its own planetary nebula.\n\nStars such as Sakurai's Object and FG Sagittae are being observed as they rapidly evolve through this phase.\n\nStars close to the upper mass limit to still qualify as AGB stars show some peculiar properties and have been dubbed super-AGB stars. They have masses above and up to 9 or (or more). They represent a transition to the more massive supergiant stars that undergo full fusion of elements heavier than helium. During the triple-alpha process, some elements heavier than carbon are also produced: mostly oxygen, but also some magnesium, neon, and even heavier elements. Super-AGB stars develop partially degenerate carbon–oxygen cores that are large enough to ignite carbon in a flash analogous to the earlier helium flash. The second dredge-up is very strong in this mass range and that keeps the core size below the level required for burning of neon as occurs in higher-mass supergiants. The size of the thermal pulses and third dredge-ups are reduced compared to lower-mass stars, while the frequency of the thermal pulses increases dramatically. Some super-AGB stars may explode as an electron capture supernova, but most will end as an oxygen–neon white dwarf. Since these stars are much more common than higher-mass supergiants, they could form a high proportion of observed supernovae. Detecting examples of these supernovae would provide valuable confirmation of models that are highly dependent on assumptions.\n\n\n"}
{"id": "22130330", "url": "https://en.wikipedia.org/wiki?curid=22130330", "title": "Atmospheric-pressure plasma", "text": "Atmospheric-pressure plasma\n\nAtmospheric-pressure plasma (or AP plasma or normal pressure plasma) is a plasma in which the pressure approximately matches that of the surrounding atmosphere – the so-called normal pressure.\n\nAtmospheric-pressure plasmas have prominent technical significance because in contrast with low-pressure plasma or high-pressure plasma no reaction vessel is needed to ensure the maintenance of a pressure level differing from atmospheric pressure. Accordingly, depending on the principle of generation, these plasmas can be employed directly in the production line. The need for cost-intensive chambers for producing a partial vacuum as used in low-pressure plasma technology is eliminated.\n\nVarious forms of excitation are distinguished:\n\nAtmospheric-pressure plasmas that have attained any noteworthy industrial significance are those generated by DC excitation (electric arc), AC excitation (corona discharge, dielectric barrier discharge, piezoelectric direct discharge and plasma jets as well as 2.45 GHz microwave microplasma).\n\nBy means of a high-voltage discharge (5–15 kV, 10–100 kHz) a pulsed electric arc is generated. A process gas, usually oil-free compressed air flowing past this discharge section, is excited and converted to the plasma state. This plasma then passes through a jet head to arrive on the surface of the material to be treated. The jet head is at earth potential and in this way largely holds back potential-carrying parts of the plasma stream. In addition, it determines the geometry of the emergent beam.\n\nBased on transistor amplifiers up to 200W power RF (radio frequency) and microwave sources are used to generate a microwave plasma. Most of the solutions work at 2.45 GHz. Meanwhile, is a technology developed which provide the ignition on the one hand and the high efficient operation on the other hand with the same electronic and couple network. This kind of atmospheric-pressure plasmas is different. The plasma is only top of the electrode. That is the reason the construction of a cannula jet was possible.\n\nThe plasma jet is used, among other things, in industry for activating and cleaning plastic and metal surfaces prior to adhesive bonding and painting processes. Even sheet materials having treatment widths of several meters can be treated today by aligning a large number of jets in a row. In doing so the modification of the surface achieved by plasma jets is comparable to the effects obtained with low-pressure plasma.\n\nDepending on the power of the jet, the plasma beam can be up to 40 mm long and attain a treatment width of 15 mm. Special rotary systems allow a treatment width per jet tool of up to 13 cm.\nDepending on the required treatment performance, the plasma source is moved at a spacing of 10–40 mm and at a speed of 5–400 m/min relative to the surface of the material to be treated.\n\nA key advantage of this system lies in its capability of being integrated in-line. This means that it can usually be installed without any difficulty in existing production systems. In addition the activation achievable is distinctly higher than in potential-based pretreatment methods (corona discharge).\n\nIt is possible to coat varied surfaces by means of this technique. Thus, anticorrosive layers and adhesion promoter layers can be applied to many metals without the use of solvents and hence in an environmentally friendly manner.\n\n\n\n"}
{"id": "36945432", "url": "https://en.wikipedia.org/wiki?curid=36945432", "title": "Bimal Mukherjee", "text": "Bimal Mukherjee\n\nBimal Mukherjee () (1903–1996) was the first Indian globe trotter who travelled the entire world on a bicycle from the year 1926 to 1937. He wrote the book \"Du Chakay Duniya\" about his experiences.\n\nThe world voyage started on bicycle from Town Hall, Calcutta on December 12, 1926 and halted at Chandannagar for the first night.\n\nHe and his three friends Ashok Mukherjee, Ananda Mukherjee and Manindra Ghosh had crossed the Bohemian Alps in bicycles wearing flannel shirts and no woolen clothing in the month of December. They had kept warm by cycling vigorously and alternated keeping one hand in their pockets to prevent frostbite. When asked how they did the amazing feat. He had replied, \"We are carrying the sun rays from India!\".\n\n"}
{"id": "24970997", "url": "https://en.wikipedia.org/wiki?curid=24970997", "title": "Blyth Harbour Wind Farm", "text": "Blyth Harbour Wind Farm\n\nBlyth Harbour Wind Farm is a coastal wind farm located along the East Pier of the Port of Blyth.\nCommissioned in January 1993 it consists of nine 0.3MW WindMaster turbines giving a total capacity of 2.7MW. It was developed by AMEC Wind and is owned by Hainsford Developments (Blyth Harbour) Limited.\n\nIn January 2008 consent was granted to replace the existing nine turbines with seven new ones. Six of these would generate 2.5MW each, and a seventh larger one would produce 7.58MW (if it is an Enercon E-126)and be the largest land-based turbine in Europe at tall.\n\nAs of September 2012, the first new turbine became operational, producing up to 3.4 MW of power (more than the original nine turbines combined). The turbine is a REpower 3.4M104, with a 76m hub height.\n"}
{"id": "46760353", "url": "https://en.wikipedia.org/wiki?curid=46760353", "title": "Boron monofluoride monoxide", "text": "Boron monofluoride monoxide\n\nBoron monofluoride monoxide or oxoboryl fluoride or fluoroxoborane is an unstable inorganic molecular substance with formula FBO. It is also called boron fluoride oxide, fluoro(oxo)borane or fluoro-oxoborane. The molecule is stable at high temperatures, but below 1000 °C condenses to a trimer (BOF). FBO can be isolated as a triatomic non-metallic molecule in an inert gas matrix, and has been condensed in solid neon and argon. When an attempt is made to condense the gas to a solid in bulk, a polymeric glass is formed, which is deficient in fluoride, and when heated forms a glassy froth like popcorn. Boron fluoride oxide has been studied because of its production in high energy rocket fuels that contain boron and fluorine, and in the form of an oxyfluoride glass. BOF glass is unusual in that it can condense directly from gas.\n\nThe FBO molecule is linear with structure F-B=O. The F-B bond length is 1.283 Å, and B-O bond is 1.207 cmÅ.\n\nThe infrared spectrum of BFO has vibrational bands at 1900, 1050, and 500 cm.\nSpectroscopic constants of the BFO molecule are B=9349.2711 MHz D=3.5335 kHz and for BFO molecule they are B=9347.3843 MHz D=3.5273 kHz\nThe monomer is stable either at low pressures, or temperatures over 1000 °C. Below this temperature, the monomers associate to form a trimer called trifluoroboroxole.\n\nHeat of formation \"H\" is predicted to be -146.1 kcal/mol. Proton affinity 149.6 kcal/mol.\n\nIf a hot BFO gas is cooled slowly it dismutates back into BO and BF. At room temperature this dismuation completes in an hour.\n\nBoron fluoride oxide forms a trimer with a ring composed of alternating oxygen and boron atoms, with fluorine bonded to the boron. (BFO). The ring structure puts it in the class of boroxols. This is also called trifluoroboroxin. The trimer is the predominant form in gas at 1000K. When heated to 1200K it mostly converts to the monomer BFO.\nBoron oxyfluoride can be condensed from vapour to a fluorine deficient glass at temperatures below 190° by very rapid cooling. When heated this deposit has a temperature at which it loses more BF to form a frothy or porous glass that resembles popcorn. The glass deposited at lower temperatures has a higher proportion of fluorine. Deposits at -40 °C are predicted to have a 1:1 ratio of fluorine to oxygen. Below -135° (BFO) is stable.\n\nThe heat of formation of the trimer from the monomer (BFO) → 3BFO is 131 kcal/mol.\n\nBoron oxyfluoride glass is transparent and colourless. It is stable in dry air, but it is hygroscopic and in normal air becomes white and opaque. When heated the glass will encounter a glass transition temperature (T) at which it ceases to be a glass, and produces BF gas and a boron oxyfluoride with less fluorine is left behind. This glass transition temperature is determined from where the pressure of BF produced exceeds the strength of the glass. The hypothetical structure of BOF glass, is of long chains of B-O-B-O with fluorine attached to each boron. These can be considered as BOF triangles linked in a chain by O atoms. These chains are tangled up like spaghetti in the glass. When the substance becomes fluorine deficient, crosslinks with oxygen form between the chains, and it becomes more two dimensional in structure. BF is produced when the terminals of two linear (BF)O chains join with each other. These ends contain -O-BF, and when two meet, BF can be eliminated and the chain extended with oxygen.\n\nBFO is expected to form in supernovae II output in gas between 1,000 and 2,000 °C and pressures around 10 bar.\n\nOtto Ruff noticed that a mixture of BF and SiF passing over molten BO produced some SiO and redistributed BO into cold parts of the reaction tube. He speculated that there must be some heat stable intermediate that converted back into the original components on cooling.\nPaul Baumgarten and Werner Bruns made the boron oxyfluoride trimer by passing BF over solid BO at 450 °C. Their experiment was to try to react BF with various oxides. They announced this find in 1939.\n\nBFO is an intermediate in the hydrolysis of BF along with BF(OH), BFOH and boric acid. \n\nAnother way in which BFO can be made is to vapourise BO with BF.\n\nWhen BF is heated with air, BFO gas predominates from 2800° to 4000 °C, being a maximum at 3200°. Above 4000 °C BO dominates.\n\nHot BF passed over some oxides such as SiO forms BFO. Other oxides that can yield boron oxyfluoride are magnesium oxide, titanium dioxide, carbonates or alumina.\n\nIn the plasma phase HF reacts with BOH, BOH, BO, BO, BO, BOH to make FBO, and other products including FBOH and FBO.\n\nThe B-O-F molecule theoretically exists but it releases energy when it rearranges to F-B-O.\nA related molecule is BOF. Molecules related to the trimer include BOClF, BOClF, and (BOCl).\n\nFBO is predicted to be able to insert noble gas atoms between the fluorine and boron atom yielding FArBO, FKrBO and FXeBO. The molecules are predicted to be linear.\n\nBoron oxyfluoride could be used in boriding steel. By using a gas, sticking solids onto the steel is avoided. Also this method allows control of the boron concentration, and mostly forms FeB instead of the more brittle FeB.\nBurning boron releases much energy, so its use in explosives or fuel is being researched. To maximise energy output, both fluorine and oxygen are used to react, and thus FBO and related molecules are formed and may be in the exhaust.\n"}
{"id": "18729187", "url": "https://en.wikipedia.org/wiki?curid=18729187", "title": "Brăila Power Station", "text": "Brăila Power Station\n\nThe Brăila Power Station is a large thermal power plant located in Brăila, having 7 generation groups, 2 of 3 MW, 1 of 210 MW, 2 of 25 MW, 1 of 50 MW and 1 of 330 MW having a total electricity generation capacity of 856 MW. The power plant also has another 210 MW unit but it is decommissioned.\nIts two chimneys are 250 metres and 200 metres tall.\n\n\n"}
{"id": "63577", "url": "https://en.wikipedia.org/wiki?curid=63577", "title": "Cashew", "text": "Cashew\n\nThe cashew tree (\"Anacardium occidentale\") is a tropical evergreen tree that produces the cashew seed (nut) and the cashew apple. It can grow as high as , but the dwarf cashew, growing up to , has proved more profitable, with earlier maturity and higher yields.\n\nThe species is originally native to northeastern Brazil. Portuguese colonists in Brazil began exporting cashew nuts as early as the 1550s. Major production of cashews occurs in Vietnam, Nigeria, India, and Ivory Coast.\n\nThe cashew nut, often simply called a cashew, is widely consumed. It is eaten on its own, used in recipes, or processed into cashew cheese or cashew butter. The shell of the cashew seed yields derivatives that can be used in many applications including lubricants, waterproofing, paints, and arms production, starting in World War II. The cashew apple is a light reddish to yellow fruit, whose pulp can be processed into a sweet, astringent fruit drink or distilled into liquor.\n\nIts English name derives from the Portuguese name for the fruit of the cashew tree \"caju\" (Portuguese pronunciation: [kaˈʒu]), which itself is derived from the Tupian word \"acajú\", literally meaning \"nut that produces itself\". The generic name \"Anacardium\" (derived from Greek ἀνά (aná), meaning \"outside,\" and καρδία (kardía), meaning \"heart\", refers to the unusual location of the seed (the heart) outside of the fruit.\n\nThe cashew tree is large and evergreen, growing to tall, with a short, often irregularly shaped trunk. The leaves are spirally arranged, leathery textured, elliptic to obovate, long and broad, with smooth margins. The flowers are produced in a panicle or corymb up to long; each flower is small, pale green at first, then turning reddish, with five slender, acute petals long. The largest cashew tree in the world covers an area around ; it is located in Natal, Rio Grande do Norte, Brazil.\n\nThe fruit of the cashew tree is an accessory fruit (sometimes called a pseudocarp or false fruit). What appears to be the fruit is an oval or pear-shaped structure, a hypocarpium, that develops from the pedicel and the receptacle of the cashew flower. Called the cashew apple, better known in Central America as \"marañón\", it ripens into a yellow or red structure about long. It is edible and has a strong \"sweet\" smell and taste.\n\nThe true fruit of the cashew tree is a kidney or boxing-glove shaped drupe that grows at the end of the cashew apple. The drupe develops first on the tree, and then the pedicel expands to become the cashew apple. Within the true fruit is a single seed, which is often considered a nut, in the culinary sense. The seed is surrounded by a double shell containing an allergenic phenolic resin, anacardic acid, a potent skin irritant chemically related to the better-known allergenic oil urushiol which is also a toxin found in the related poison ivy. Some people are allergic to cashews, but cashews are a less frequent allergen than tree nuts or peanuts.\n\nWhile the cashew plant is native to northeast Brazil, the Portuguese took it to Goa, India, between 1560 and 1565. From there, it spread throughout Southeast Asia and eventually Africa.\n\nCulinary uses for cashew seeds in snacking and cooking are similar to those for all tree seeds called nuts.\n\nCashews are commonly used in Indian cuisine and Pakistani cuisine, whole for garnishing sweets or curries, or ground into a paste that forms a base of sauces for curries (e.g., \"korma\"), or some sweets (e.g., \"kaju barfi\"). It is also used in powdered form in the preparation of several Indian sweets and desserts. In Goan cuisine, both roasted and raw kernels are used whole for making curries and sweets. Cashews are also used in Thai and Chinese cuisines, generally in whole form. In the Philippines, cashew is a known product of Antipolo, and is eaten with \"suman\". The province of Pampanga also has a sweet dessert called \"turrones de casuy\", which is cashew marzipan wrapped in white wafers. In Indonesia, roasted and salted cashews are called \"kacang mete\" or \"kacang mede\", while the cashew apple is called \"jambu monyet\" (translates in English to monkey rose apple).\n\nIn the 21st century, cashew cultivation increased in several African countries to meet the demands for manufacturing cashew milk, a plant milk alternative to dairy milk. In Mozambique, \"bolo polana\" is a cake prepared using powdered cashews and mashed potatoes as the main ingredients. This dessert is popular in South Africa.\n\nIn Brazil, cashew fruit juice and the fruit pulp are used in the production of sweets, juice, alcoholic beverages, such as \"cachaça\", and as a flour, milk or cheese. In Panama, the cashew fruit is cooked with water and sugar for a prolonged time to make a sweet, brown, paste-like dessert called \"dulce de marañón\", with \"marañón\" as a Spanish name for cashew.\n\nThe shell of the cashew nut contains oil compounds which may cause contact dermatitis similar in severity to that of poison ivy, primarily resulting from the phenolic lipids, anacardic acid, and cardanol. Due to the possible dermatitis, cashews are typically not sold in the shell to consumers. Readily and inexpensively extracted from the waste shells, cardanol is under research for its potential applications in nanomaterials and biotechnology.\n\nIn 2015, global production of cashew nuts (as the kernel) was 738,861 tonnes, led by India and Côte d'Ivoire each with 23% of the world total (table). Vietnam and Brazil also had significant production of cashew kernels.\n\nIn 2014, rapid growth of cashew cultivation in Côte d'Ivoire made this country the top African exporter. Fluctuations in world market prices, poor working conditions, and low pay for local harvesting have caused discontent in the cashew nut industry.\n\nThe cashew tree is cultivated in the tropics between 25°N and 25°S, and is supremely adapted to hot lowland areas with a pronounced dry season, where the mango and tamarind trees also thrive. The traditional cashew tree is tall (up to 14 m) and takes three years from planting before it starts production, and eight years before economic harvests can begin. More recent breeds, such as the dwarf cashew trees, are up to 6 m tall, and start producing after the first year, with economic yields after three years. The cashew nut yields for the traditional tree are about 0.25 metric tons per hectare, in contrast to over a ton per hectare for the dwarf variety. Grafting and other modern tree management technologies are used to further improve and sustain cashew nut yields in commercial orchards.\n\nIn a 100-gram serving, raw cashews provide 553 Calories, 67% of the Daily Value (DV) in total fats, 36% DV of protein, 13% DV of dietary fiber and 11% DV of carbohydrates (table). Cashews are rich sources (> 19% DV) of dietary minerals, including particularly copper, manganese, phosphorus, and magnesium (79-110% DV), and of thiamin, vitamin B and vitamin K (32-37% DV) (table). Iron, potassium, zinc, and selenium are present in significant content (14-61% DV) (table). Cashews (100 grams, raw) contain of beta-sitosterol.\n\nFor some 6% of people, cashews can lead to complications or allergic reactions which may be life-threatening. These allergies are triggered by the proteins found in tree nuts, and cooking often does not remove or change these proteins. Reactions to cashew and tree nuts can also occur as a consequence of hidden nut ingredients or traces of nuts that may inadvertently be introduced during food processing, handling, or manufacturing, particularly in people of European descent.\nCashew oil is a dark yellow oil for cooking or salad dressing pressed from cashew nuts (typically broken chunks created during processing). This may be produced from a single cold pressing.\n\nCashew nutshell liquid (CNSL) or cashew shell oil (CAS registry number 8007-24-7) is a natural resin with a yellowish sheen found in the honeycomb structure of the cashew nutshell, and is a byproduct of processing cashew nuts. It is a raw material of multiple uses in developing drugs, antioxidants, fungicides, and biomaterials. It is used in tropical folk medicine and for antitermite treatment of timber. Its composition varies depending on how it is processed.\n\nThese substances are skin allergens, like the oils of poison ivy, and present danger during manual cashew processing.\n\nThis natural oil phenol has been found to have interesting chemical structural features which enable a range of chemical modifications to create a wide spectrum of biobased monomers capitalizing on the chemically versatile construct, containing three different functional groups: the aromatic ring, the hydroxyl group, and the double bonds in the flanking alkyl chain. These can be split into key groups, used as polyols, which have recently seen a dramatic increase in demand for their biobased origin and key chemical attributes such as high reactivity, range of functionalities, reduction in blowing agents, and naturally occurring fire retardant properties in the field of ridged polyurethanes aided by their inherent phenolic structure and larger number of reactive units per unit mass.\n\nCNSL may be used as a resin for carbon composite products. CNSL-based Novolac is another versatile industrial monomer deriving from cardanol typically used as a reticulating agent for epoxy matrices in composite applications providing good thermal and mechanical properties to the final composite material.\n\nThe cashew apple, also called cashew fruit, is the fleshy part of the cashew fruit attached to the cashew nut. The top end of the cashew apple is attached to the stem that comes off the tree. The bottom end of the cashew apple attaches to the cashew nut, which is encased in a shell. In botanical terms, the cashew apple is an accessory fruit that grows on the cashew seed (which is the nut).\n\nThe cashew apple can be eaten fresh, cooked in curries, or fermented into vinegar, as well as an alcoholic drink. It is also used to make preserves, chutneys, and jams in some countries such as India and Brazil. In many countries, particularly in South America, the cashew apple is used to flavor drinks, both alcoholic and nonalcoholic.\n\nCashew nuts are more widely traded than cashew apples, because the apple, unlike the nut, is easily bruised and has very limited shelf life. Cashew apple juice, however, may be used for manufacturing blended juices.\nIn cultures that consume cashew apples its astringency is sometimes removed by steaming the fruit for five minutes before washing it in cold water; alternatively, boiling the fruit in salt water for five minutes or soaking it in gelatin solution also reduces the astringency.\n\nIn Goa, the cashew apple is mashed and the juice extracted and kept for fermentation for a few days. Fermented juice then undergoes a double distillation process. The resulting beverage is called \"feni\" or fenny. \"Feni\" is about 40–42% alcohol. The single-distilled version is called \"urrac\", which is about 15% alcohol.\n\nIn the southern region of Mtwara, Tanzania, the cashew apple (\"bibo\" in Swahili) is dried and saved. Later, it is reconstituted with water and fermented, then distilled to make a strong liquor often referred to by the generic name, \"gongo\".\n\nIn Mozambique, cashew farmers commonly make a strong liquor from the cashew apple. It is known under various names in the local languages of Mozambique (muchekele in Emakua spoken in the North, xicadju in Changana spoken in the South). In contrast to the above-mentioned Feni of Goa, the cashew liquor made in Mozambique does not involve the extraction of the juice from the cashew apples. Following harvest and the removal of the nuts, the apples are spread on the ground under trees and courtyards and allowed to lose water and ferment. The shrivelled fruits are then used for distillation.\n\nAccording to one source, an alcohol had been distilled in the early 20th century from the juice of the fruit, and was manufactured in the West Indies.\n\nDiscarded cashew nuts unfit for human consumption, alongside the residues of oil extraction from cashew kernels, can be used to feed livestock. Animals can also eat the leaves of cashew trees.\n\n\n\n"}
{"id": "10832519", "url": "https://en.wikipedia.org/wiki?curid=10832519", "title": "Channapatna toys", "text": "Channapatna toys\n\nChannapatna toys are a particular form of wooden toys (and dolls) that are manufactured in the town of Channapatna in the Ramanagara district of Karnataka state, India. This traditional craft is protected as a geographical indication (GI) under the World Trade Organization, administered by the Government of Karnataka. As a result of the popularity of these toys, Channapatna is known as \"Gombegala Ooru\" (toy-town) of Karnataka. Traditionally, the work involved lacquering the wood of the \"Wrightia tinctoria\" tree, colloquially called \"Aale mara\" (ivory-wood).\n\nThe origin of these toys can be traced to the reign of Tipu Sultan who invited artisans from Persia to train the local artisans in the making of wooden toys. Bavas Miyan is the father of Channapatna Toy. He is the one to sacrifice his life for channapatna toys. He adopted Japanese technology for toys making and help the local artisans improve their art. For nearly two centuries, ivory-wood was the main wood used in the making of these toys, though rosewood and sandalwood were also occasionally used.\n\nThe craft has diversified over time; in addition to the traditional ivory-wood, other woods—including rubber, sycamore, cedar, pine and teak—are now used as well. Manufacturing stages include procuring the wood, seasoning the wood, cutting the wood into the desired shapes, pruning and carving the toys, applying the colours and finally polishing the finished product. Vegetable dyes are used in the colouring process to ensure that the toys and dolls are safe for use by children. As of Oct 2006, more than 6,000 people in Channapatna, working in 254 home manufacturing units and 50 small factories, were engaged in the making of these toys. The Karnataka Handicrafts Development Corporation (KHDC) provides assistance with marketing efforts. Most oldest and popular manufacturing unit Bharath Art and crafts help develop innovative products.\n\nWith no proper backing or marketing, the Channapatna toy industry faced a financial crunch for more than a decade and was almost on the verge of dying out. However, with the help of KHDC, the craft has been revived and the artisans involved are being trained on changing trends in the industry, to help them keep abreast of the current scenario. Prototypes designed by master craftsmen are introduced to the local artisans, who use them to create well-designed toys and dolls. The Government of Karnataka has also provided help by constructing a Lacquerware Craft Complex, which has a manufacturing centre with 32 turning lathe machines, at Channapatna. Financial assistance to the artisans, with help from the Dutch Government and the Karnataka Government's \"Vishwa\" scheme has also been provided.\n\nOver the last 4 years, many new companies and social enterprises have been reviving the Channapatna craft to suit modern tastes. Right now they are working and supporting artisans of Channapatna, but soon they would spread across the country and bring in more varieties under the crafts. iFolk Channapatna toys and handicrafts, a group formed by Bharath Art and crafts, promotes and supports lacquerware artisans to do innovation and modernization of their products. Bangalore-based NGO Maya Organic,\n"}
{"id": "66269", "url": "https://en.wikipedia.org/wiki?curid=66269", "title": "Chloride", "text": "Chloride\n\nThe chloride ion is the anion (negatively charged ion) Cl. It is formed when the element chlorine (a halogen) gains an electron or when a compound such as hydrogen chloride is dissolved in water or other polar solvents. Chloride salts such as sodium chloride are often very soluble in water. It is an essential electrolyte located in all body fluids responsible for maintaining acid/base balance, transmitting nerve impulses and regulating fluid in and out of cells. Less frequently, the word \"chloride\" may also form part of the \"common\" name of chemical compounds in which one or more chlorine atoms are covalently bonded. For example, methyl chloride, with the standard name chloromethane (see IUPAC books) is an organic compound with a covalent C−Cl bond in which the chlorine is not an anion.\n\nA chloride ion is much larger than a chlorine atom, 167 and 99 pm, respectively. The ion is colorless and diamagnetic. In aqueous solution, it is highly soluble in most cases; however, some chloride salts, such as silver chloride, lead(II) chloride, and mercury(I) chloride are slightly soluble in water. In aqueous solution, chloride is bound by the protic end of the water molecules.\n\nSea water contains 1.94% chloride. Some chloride-containing minerals include the chlorides of sodium (halite or NaCl), potassium (sylvite or KCl), and magnesium (bischofite), hydrated MgCl. The concentration of chloride in the blood is called serum chloride, and this concentration is regulated by the kidneys. A chloride ion is a structural component of some proteins, e.g., it is present in the amylase enzyme.\n\nThe chlor-alkali industry is a major consumer of the world's energy budget. This process converts sodium chloride into chlorine and sodium hydroxide, which are used to make many other materials and chemicals. The process involves two parallel reactions:\nAnother major application involving chloride is desalination, which involves the energy intensive removal of chloride salts to give potable water. In the petroleum industry, the chlorides are a closely monitored constituent of the mud system. An increase of the chlorides in the mud system may be an indication of drilling into a high-pressure saltwater formation. Its increase can also indicate the poor quality of a target sand.\n\nChloride is also a useful and reliable chemical indicator of river / groundwater fecal contamination, as chloride is a non-reactive solute and ubiquitous to sewage & potable water. Many water regulating companies around the world utilize chloride to check the contamination levels of the rivers and potable water sources.\n\nChloride salts such as sodium chloride are used to preserve food.\n\nThe presence of chlorides, e.g. in seawater, significantly aggravates the conditions for pitting corrosion of most metals (including stainless steels, aluminum, aluminum alloys, and high-alloyed materials) by enhancing the formation and growth of the pits through an autocatalytic process. \n\nChloride is an essential electrolyte, trafficking in and out of cells through chloride channels and playing a key role in maintaining cell homeostasis and transmitting action potentials in neurons. Characteristic concentrations of chloride in model organisms are: in both \"E. coli\" and budding yeast are 10-200mM (media dependent), in mammalian cell 5-100mM and in blood plasma 100mM.\n\nChloride can be oxidized but not reduced. The first oxidation, as employed in the chlor-alkali process, is conversion to chlorine gas. Chlorine can be further oxidized to other oxides and oxyanions including hypochlorite (ClO, the active ingredient in chlorine bleach), chlorine dioxide (ClO), chlorate (), and perchlorate ().\n\nIn terms of its acid–base properties, chloride is a very weak base as indicated by the negative value of the p\"K\" of hydrochloric acid. Chloride can be protonated by strong acids, such as sulfuric acid:\n\nIonic chloride salts reaction with other salts to exchange anions. The presence of chloride is often detected by its formation of an insoluble silver chloride upon treatment with silver ion:\n\nThe concentration of chloride in an assay can be determined using a chloridometer, which detects silver ions once all chloride in the assay has precipitated via this reaction.\n\nChlorided silver electrodes are commonly used in electrophysiology. \n\nAn example is table salt, which is sodium chloride with the chemical formula NaCl. In water, it dissociates into Na and Cl ions. Salts such as calcium chloride, magnesium chloride, potassium chloride have varied uses ranging from medical treatments to cement formation.\n\nCalcium chloride (CaCl) is a salt that is marketed in pellet form for removing dampness from rooms. Calcium chloride is also used for maintaining unpaved roads and for fortifying roadbases for new construction. In addition, calcium chloride is widely used as a de-icer, since it is effective in lowering the melting point when applied to ice.\n\nExamples of covalently bonded chlorides are phosphorus trichloride, phosphorus pentachloride, and thionyl chloride, all three of which are reactive chlorinating reagents that have been used in a laboratory.\n\nChlorine can assume oxidation states of −1, +1, +3, +5, or +7. Several neutral chlorine oxides are also known.\n\n"}
{"id": "35745858", "url": "https://en.wikipedia.org/wiki?curid=35745858", "title": "Degree of reaction", "text": "Degree of reaction\n\nIn turbomachinery, Degree of reaction or reaction ratio (R) is defined as the ratio of the static pressure drop in the rotor to the static pressure drop in the stage or as the ratio of static enthalpy drop in the rotor to the static enthalpy drop in the stage.\n\nDegree of reaction (R) is an important factor in designing the blades of a turbine, compressors, pumps and other turbo-machinery. It also tells about the efficiency of machine and is used for proper selection of a machine for a required purpose.\n\nVarious definitions exist in terms of enthalpies, pressures or flow geometry of the device. \nIn case of turbines, both impulse and reaction machines, Degree of reaction (R) is defined as the ratio of energy transfer by the change in static head to the total energy transfer in the rotor i.e. \nFor a gas turbine or compressor it is defined as the ratio of isentropic heat drop in the moving blades (i.e. the rotor) to the sum of the isentropic heat drops in the fixed blades (i.e. the stator) and the moving blades i.e.\nIn pumps, degree of reaction deals in static and dynamic head. Degree of reaction is defined as the fraction of energy transfer by change in static head to the total energy transfer in the rotor i.e.\n\nMost turbo machines are efficient to a certain degree and can be approximated to undergo isentropic process in the stage.\nHence from formula_4, \n\nit is easy to see that for isentropic process ∆H ≃ ∆P. Hence it can be implied\nThe same can be expressed mathematically as:\n\nWhere 1 to 3ss in Figure 1 represents the isentropic process beginning from stator inlet at 1 to rotor outlet at 3. And 2 to 3ss is the isentropic process from rotor inlet at 2 to rotor outlet at 3. The velocity triangle (Figure 2.) for the flow process within the stage represents the change in fluid velocity as it flows first in the stator or the fixed blades and then through the rotor or the moving blades. Due to the change in velocities there is a corresponding pressure change.\nAnother useful definition used commonly uses stage velocities as:\nis the enthalpy drop in the rotor and\nis the total enthalpy drop. The degree of reaction is then expressed as\n\nFor axial machines formula_10, then\nThe degree of reaction can also be written in terms of the geometry of the turbomachine as obtained by\nwhere formula_13 is the vane angle of rotor outlet and formula_14 is the vane angle of stator outlet. In practice formula_15 is substituted as ϕ and formula_16 as formula_17 givingformula_18 The degree of reaction now depends only on ϕ and formula_17 which again depend on geometrical parameters β3 and β2 i.e. the vane angles of stator outlet and rotor outlet. Using the velocity triangles degree of reaction can be derived as:\nThis relation is again very useful when the rotor blade angle and rotor vane angle are defined for the given geometry.\n\nThe Figure 3 alongside shows the variation of total-to-static efficiency at different blade loading coefficient with the degree of reaction.\nThe governing equation is written as \nwhere formula_22\nis the stage loading factor. The diagram shows the optimization of total - to - static efficiency at a given stage loading factor, by a suitable choice of reaction. It is evident from the diagram that for a fixed stage loading factor that there is a relatively small change in total-to-static efficiency for a wide range of designs.\n\nThe degree of reaction contributes to the stage efficiency and thus used as a design parameter. Stages having 50% degree of reaction are used where the pressure drop is equally shared by the stator and the rotor for a turbine. \nThis reduces the tendency of boundary layer separation from the blade surface avoiding large stagnation pressure losses.\n\nIf R= then from the relation of degree of reaction,|| α2 = β3 and the velocity triangle (Figure 4.) is symmetric. The stage enthalpy gets equally distributed in the stage (Figure 5.) . In addition the whirl components are also the same at the inlet of rotor and diffuser.\n\nStage having reaction less than half suggest that pressure drop or enthalpy drop in the \nrotor is less than the pressure drop in the stator for the turbine. The same follows for a pump or compressor as shown in Figure 6. Therefore the stator has a larger contribution to the total work extracted or work done. From the relation for degree of reaction, \n\nStage having reaction more than half suggest that pressure drop or enthalpy drop in the rotor is more than the pressure drop in the stator for the turbine. The same follows for a pump or compressor. Thus in this case the rotor has a larger contribution to the total work extracted or work done. From the relation for degree of reaction,|| α2 < β3 which is also shown in corresponding Figure 7.\n\nThis is special case used for impulse turbine which suggest that entire pressure drop in the turbine is obtained in the stator. \nThe stator performs a nozzle action converting pressure head to velocity head and extracting work. It is difficult to achieve adiabatic expansion in the impulse stage, i.e. expansion only in the nozzle, due to irreversibility involved, in actual practice. Figure 8 shows the corresponding enthalpy drop for the reaction = 0 case.\n\n"}
{"id": "41721530", "url": "https://en.wikipedia.org/wiki?curid=41721530", "title": "Double-layer capacitance", "text": "Double-layer capacitance\n\nDouble-layer capacitance is the storing of electrical energy by means of the electrical double layer effect. This electrical phenomenon appears at the interface between a conductive electrode and an adjacent liquid electrolyte, as observed, for example, in a supercapacitor. At this boundary two layers of ions with opposing polarity form if a voltage is applied, one at the surface of the electrode, and one in the electrolyte. The two layers of ions are separated by a single layer of solvent molecules that adheres to the surface of the electrode and acts like a dielectric in a conventional capacitor.\n\nThe amount of electric charge stored in double-layer capacitance is linearly proportional to the applied voltage and depends primarily on the electrode surface. The unit of capacitance is the farad.\n\n\nHelmholtz laid the theoretical foundations for understanding the double layer phenomenon. The formation of double layers is exploited in every electrochemical capacitor to store electrical energy.\n\nEvery capacitor has two electrodes, mechanically separated by a separator. These are electrically connected via the electrolyte, a mixture of positive and negative ions dissolved in a solvent such as water. Where the liquid electrolyte contacts the electrode's conductive metallic surface, an interface is formed which represents a common boundary between the two phases of matter. It is at this interface that the double layer effect occurs.\n\nWhen a voltage is applied to the capacitor, two layers of polarized ions are generated at the electrode interfaces. One layer is within the solid electrode (at the surfaces of crystal grains from which it is made that are in contact with the electrolyte). The other layer, with opposite polarity, forms from dissolved and solvated ions distributed in the electrolyte that have moved towards the polarized electrode. These two layers of polarized ions are separated by a monolayer of solvent molecules. The molecular monolayer forms the inner Helmholtz plane (IHP). It adheres by physical adsorption on the electrode surface and separates the oppositely polarized ions from each other, forming a molecular dielectric.\n\nThe amount of charge in the electrode is matched by the magnitude of counter-charges in the outer Helmholtz plane (OHP). This is the area close to the IHP, in which the polarized electrolyte ions are collected. This separation of two layers of polarized ions through the double-layer stores electrical charges in the same way as in a conventional capacitor. The double-layer charge forms a static electric field in the molecular IHP layer of the solvent molecules that corresponds to the strength of the applied voltage.\n\nThe \"thickness\" of a charged layer in the metallic electrode, i.e., the average extension perpendicular to the surface, is about 0.1 nm, and mainly depends on the electron density because the atoms in solid electrodes are stationary. In the electrolyte, the thickness depends on the size of the solvent molecules and of the movement and concentration of ions in the solvent. It ranges from 0.1 to 10 nm as described by the Debye length. The sum of the thicknesses is the total thickness of a double layer.\n\nThe IHP's small thickness creates a strong electric field E over the separating solvent molecules. At a potential difference of, for example, U = 2 V and a molecular thickness of d = 0.4 nm, the electric field strength is\n\nTo compare this figure with values from other capacitor types requires an estimation for electrolytic capacitors, the capacitors with the thinnest dielectric among conventional capacitors. The voltage proof of aluminum oxide, the dielectric layer of aluminum electrolytic capacitors, is approximately 1.4 nm/V. For a 6.3 V capacitor therefore the layer is 8.8 nm. The electric field is 6.3 V/8.8 nm = 716 kV/mm, around 7 times lower than in the double-layer. The field strength of some 5000 kV/mm is unrealizable in conventional capacitors. No conventional dielectric material could prevent charge carrier breakthrough. In a double-layer capacitor the chemical stability of the solvent's molecular bonds prevents breakthrough.\n\nThe forces that cause the adhesion of solvent molecules in the IHP are physical forces rather than chemical bonds. Chemical bonds exist within the adsorbed molecules, but they are polarized.\n\nThe magnitude of the electric charge that can accumulate in the layers corresponds to the concentration of the adsorbed ions and the electrodes surface. Up to the electrolyte's decomposition voltage, this arrangement behaves like a capacitor in which the stored electrical charge is linearly dependent on the voltage.\n\nThe double-layer is like the dielectric layer in a conventional capacitor, but with the thickness of a single molecule. Using the early Helmholtz model to calculate the capacitance the model predicts a constant differential capacitance C independent from the charge density, even depending on the dielectric constant ε and the charge layer separation δ.\n\nIf the electrolyte solvent is water then the influence of the high field strength creates a permittivity ε of 6 (instead of 80 without an applied electric field) and the layer separation δ ca. 0.3 nm, the Helmholtz model predicts a differential capacitance value of about 18 µF/cm. This value can be used to calculate capacitance values using the standard formula for conventional plate capacitors if only the surface of the electrodes is known. This capacitance can be calculated with:\n\nThe capacitance C is greatest in components made from materials with a high permittivity ε, large electrode plate surface areas A and a small distance d between plates. Because activated carbon electrodes have a very high surface area and an extremely thin double-layer distance which is on the order of a few ångströms (0.3-0.8 nm), it is understandable why supercapacitors have the highest capacitance values among the capacitors (in the range of 10 to 40 µF/cm).\n\nIn real produced supercapacitors with a high amount of double-layer capacitance the capacitance value depends first on electrode surface and DL distance. Parameters such as electrode material and structure, electrolyte mixture, and amount of pseudocapacitance also contribute to capacitance value.\n\nBecause an electrochemical capacitor is composed out of two electrodes, electric charge in the Helmholtz layer at one electrode is mirrored (with opposite polarity) in the second Helmholtz layer at the second electrode. Therefore, the total capacitance value of a double-layer capacitor is the result of two capacitors connected in series. If both electrodes have approximately the same capacitance value, as in symmetrical supercapacitors, the total value is roughly half that of one electrode.\n\n"}
{"id": "23626906", "url": "https://en.wikipedia.org/wiki?curid=23626906", "title": "Evopod", "text": "Evopod\n\nEvopod is a unique tidal energy device being developed by a UK-based company Oceanflow Energy Ltd for generating electricity from tidal streams and ocean currents. It can operate in exposed deep water sites where severe wind and waves also make up the environment.\n\n\n\nThe device differentiates itself from other tidal turbines in that the turbine is mounted on a floating, semi-submerged body that is tethered to the seabed. The power generation equipment is similar to that of a wind turbine and is housed in the cylindrical shaped watertight lower hull, which is deeply submerged below the water line and supported by small waterplane area surface piercing struts.\n\nOne variant of this patented hull concept has three vertical struts that pierce the water surface, much like a multi-hull SWATH design. The two transversely separated aft struts provide the stability that is needed to resist the torque reacted by the single turbine/generator unit. The configuration of the struts also ensure that the device weathervanes about its midwater mooring buoy such that it always points into the direction of the current.\n\nThe device is moored by a mid-water buoy, which is fixed to the seabed by four spread mooring lines which are anchored to the sea-bed by pile or gravity anchors. The buoy design is also unique in that it encompasses a geo-fixed part that is anchored to the seabed and a rotating part that is linked to Evopod by a rigid yoke. The turbine drag forces are therefore transmitted through a bearing system linking the fixed and rotating parts of the buoy. A slip ring power export swivel is located in the buoy so that twist is not imparted into the umbilical cable that takes the power from the midwater buoy to the seabed. A subsea power export cable links the umbilical’s seabed connection point to the shore.\n\nWith the weather-vaning hull design and rotational midwater buoy, Evopod generates electricity with both the ebb and flood tides by always pointing into the tide’s direction of flow. This gives it a generating time of roughly 20 hours per/lunar day (approx 24hrs 50 minutes).\n\nIn comparison to other marine bodies that float on the surface of the ocean, Evopod’s semi-submerged hull form is hardly affected by the passing waves. It is also designed to be readily detachable from the mid-water buoy for recovery operations. Developing safe installation, maintenance and recovery operations in the hazardous environment of fast flowing currents is one of the biggest challenges facing tidal energy device developers.\n\nThe device is designed for deep water sites, such as the Pentland Firth (Up to 60meters water depth, flow speed 6 m/s). Deep water sites in UK waters have the fastest flow speeds and have the greatest potential for electricity generation.\n\nA 1/40th scale model of Evopod was initially tested in the test tank of Newcastle University during a proof of concept phase.\n\nThe 1/10 scale device was initially used to demonstrate the tidal test facilities at the Tees Barrage in Thornaby-on-Tees near Middlesbrough, UK by Narec (National Renewable Energy Centre).\n\nIn 2008 a 1/10 scale Evopod device was installed and tested in the tidal flow through Strangford Narrows near Portaferry, Northern Ireland. Over a period of two years the device collected data but was not connected to the grid under the Supergen Marine Energy Research Programme in collaboration with Queen's University Belfast, amongst others. In 2011 the device was upgraded to include a power export solution which feeds Evopod's generated power onshore to the Queen's University Marine Laboratory. The power is currently fed into the mains circuit of the Marine Laboratory, with plans to be fully grid connected in the near future.\n\nIn 2010 Oceanflow Energy were awarded a Scottish WATERS grant to \"Build and deploy the ‘Evopod’, a 35 kilowatt floating grid connected tidal energy turbine at Sanda Sound in South Kintyre\".\n\nOceanflow Energy and Evopod have won several awards, the most recent being the Shell Springboard Regional award in February 2009. It has also won awards for “innovation of the year” and “green business of the year” in the North East of England.\n\n\n\n"}
{"id": "25828273", "url": "https://en.wikipedia.org/wiki?curid=25828273", "title": "Explosive-driven ferroelectric generator", "text": "Explosive-driven ferroelectric generator\n\nAn explosive-driven ferroelectric generator (EDFEG, explosively pumped ferroelectric generator, EPFEG, or FEG) is a compact pulsed power generator, a device used for generation of short high-voltage high-current pulse. The energies available are fairly low, in the range of single joules, the voltages range in tens of kilovolts to over 100 kV, and the powers range in hundreds of kilowatts to megawatts. They are suitable for delivering high voltage pulses to high-impedance loads and can directly drive radiating circuits.\n\nECFEGs operate by releasing the electrical charge stored in the poled crystal structure of a suitable ferroelectric material, e.g. PZT, by an intense mechanical shock. They are a kind of phase transition generators.\n\nThe structure of an EDFEG is generally a block of a suitable high explosive, accelerating a metal plate into a target made of ferroelectric material.\n\nFEGs find multiple uses due to their compact character; charging banks of capacitors, initiation of slapper detonator arrays in nuclear weapons and other devices, driving nuclear fusion reactions, powering pulsed neutron generators, seed power sources for stronger pulse generators (e.g. EPFCGs), electromagnetic pulse generators, electromagnetic weapons, vector inversion generators, etc.\n\nA 2.4 megawatt HERF generator (an EDFEG with a pulse forming network directly driving a dipole antenna) with peak output frequency at 21.4 MHz was demonstrated.\n\n\n"}
{"id": "34635136", "url": "https://en.wikipedia.org/wiki?curid=34635136", "title": "Firewood processor", "text": "Firewood processor\n\nA firewood processor is a machine designed to cut and split firewood with minimal manual handling of the logs. There are typically four main parts of the machine, each dedicated to a separate function. Processing begins with a log pile – a pile of logs that have been de-limbed and cut to an appropriate length, generally . Popular brands include Hakki Pilke, Wood Beaver, DYNA, Multitek and Blockbuster. Many individuals use processors commercially and also privately as a hobby. Others choose to rent them as an alternative to purchasing.\n\nLogs are stacked onto the log deck using a machine such as a skid steer or small excavator with a grapple. Each log is pulled mechanically into a trough that feeds it into position to be sawn into firewood-length pieces (often called “rounds”).\n\nThe log is sawn by either a hydraulically operated chainsaw harvester bar, or on larger machines, a very large circular saw blade (slasher saw), or a guillotine powered either directly from a pto (tractor or engine powered) or by hydraulics. When the cut is completed, the \"round\" drops into position to be split in the next process. In some guillotine splitters the wood is split as the wood is cut.\n\nHere, the log is simply forced into a wedge that splits the round into anywhere between two and ten pieces, depending on the size of the logs and the intended market.\n\nTypically a conveyor that pulls the split firewood away from the processor and into a waiting delivery truck’s box or a woodpile for later handling. Some setups will use multiple conveyors and introduce a tumbling system to clean the firewood.\n\nThe output capacity of a firewood processor varies with the size and cost of the machine, from one cord per hour on a $10,000 entry-level machine, up to five or six cords per hour on a $100,000 industrial machine. (\"2012 prices\"). \"See notes on output capacity ratings below.\"\n\nThe choice of machine depends on a large number of variables other than straight production output. Different markets require different processing. For example, people who heat with large outside wood boilers prefer large, slow-burning pieces of hardwood, while a good campfire is made of small pieces of fast-burning softwood. Restaurants with wood-fired ovens prefer small pieces as well, but of hardwood or specialty species.\n\nThe physics of the process demand that it requires a larger machine to make smaller pieces of firewood, and the species of logs being processed may also dictate machine size and power requirements.\n\nEvery manufacturer lists an output rating of cords per hour. Even the lightest-duty machines will split two cords of green, frozen Aspen into halves long quickly. Changing any one of these optimal variables (condition, temperature, size, species, number of splits, or length of round) will reduce the rate of output, making meaningful comparison between manufacturers' claims difficult. The most effective way to determine how fast a machine really is would be either in person or by watching detailed videos of it processing wood.\n\nThis term is frequently misunderstood. While the definition is understood to be the time it takes the splitter ram to fully extend and retract, there is no consideration of force in this equation. In proper terms, cycle time is simply a mathematical calculation of the size of a cylinder’s bore, stroke and rod diameter, and the ability of that combination to act at a given flow of hydraulic oil in GPM. A properly designed advanced hydraulic systems can use Regenerative systems to both accelerate cycle times and maintain full pressure.\n\nA properly designed advanced hydraulic systems can use Regenerative systems to both accelerate cycle times and maintain full pressure.\n\nWhile most processors use a hydraulic chainsaw bar to cut the logs to length, some use a very large circular saw blade called a slasher blade. They are fast and efficient, requiring little maintenance once set up properly. While the safety of slasher blades has been proven over decades, there are a number of people who will not walk within of one. The more traditional chainsaw bar setup requires constant oiling and frequent maintenance, but it is simple and familiar work.\n\nThe reduction in manual labor is obvious, but few immediately recognize the improvement in safety conditions. Everything related to logging is dangerous, but eliminating a great deal of the need for workers to use chainsaws or wrestle heavy rounds onto a splitter measurably reduces the incidence of injury on these job sites. The fact that it takes fewer workers to produce the same amount of firewood further reduces these risks.\n\n"}
{"id": "14062993", "url": "https://en.wikipedia.org/wiki?curid=14062993", "title": "Gravity gun", "text": "Gravity gun\n\nA gravity gun is a type of device in video games, particularly first-person shooters using an advanced physics engine, whereby players can directly manipulate objects in the world, often allowing them to be used as projectiles against hostile characters. The concept was first featured in the third-person shooter \"\" which, while not featuring a dedicated 'gravity gun', allows the player to use telekinesis to manipulate both objects and enemies by moving them or forcefully throwing them around.\n\nThe concept was popularized by the gravity gun found in Valve's \"Half-Life 2\", as well as the Temporal Uplink found in Free Radical Design's \"\"; although a similar concept was used by id Software during the production of the earlier game \"Doom 3\", eventually leading to the introduction of a physics-based weapon in the expansion pack \"\". Later games, such as \"Portal\", \"BioShock\", \"Crysis\", and \"Dead Space\", have been influenced by the success of these physics-based weapons, adopting their own styles of comparable abilities or weapons.\n\nValve Software's \"Half-Life 2\" made significant use of physics in the game, powered by the Source engine. The physics engine within Source is derived from Havok, which opens up a wealth of possibilities for object interaction – particularly when later in the game, Freeman receives an energy-beam weapon that lets him move huge objects. Although the player can pick up and throw objects early in the game, this ability is somewhat limited in scope. Around a quarter of the way through the game, the player acquires the gravity gun, properly named as the \"Zero-point Energy Field Manipulator\". Alyx Vance explains that the gravity gun is designed for handling hazardous materials, but is mostly used for heavy lifting. She explains to Gordon that she once found it useful \"for clearing minefields\". The gravity gun significantly increases the player's ability to manipulate objects in the game. Like most other weapons in the game, the gravity gun has two trigger functions. The primary trigger causes the gun to emit a small discharge which emits energy to the targeted object. The distance which the object is forced is dependent on its weight and distance from the gun. The secondary trigger attracts the targeted object to the gun and holds it in midair a few inches away, negating its weight and allowing the player to carry it with them. Using the secondary trigger again will drop the item, while the primary trigger will launch it with considerable force.\n\nBy combining these functions, players can use the gravity gun to scale barriers and obstacles, create cover against enemy characters, or launch the objects at enemy characters, causing them considerable damage. Certain types of objects, such as saw blades, fuel barrels and hydrogen tanks are intentionally designed by Valve to be used as \"gravity gun ammunition\". The gravity gun, however, cannot manipulate heavier objects and enemy characters until the late stages of the game, when the device becomes temporarily infused with dark energy meant to destroy it.\n\nThe gravity gun was very well received by critics, who considered it one of the defining features of \"Half-Life 2\"s entertainment value. Planet Half-Life called the gravity gun \"the next level in interactive gaming.\" \"Electronic Gaming Monthly\" described \"Half-Life 2\"s gravity gun as the \"thinking man's death tool,\" which lets players \"toy with gravity to kill foes with everyday objects.\" \"Call of Duty\" series military adviser Hank Keirsey stated that \"the weapon is not very practical\". He did, however, discuss its historical precedents, further stating that \"The ancients learned very early how to use gravity to their advantage — but this usually involved rolling rocks down hills or pouring boiling oil down the castle walls. Those that failed to respect gravity suffered.\"\n\nIt was not until the third installment of the TimeSplitters franchise, \"\", that the Temporal Uplink was given the ability to manipulate the physics in the game. In \"Future Perfect\", the Temporal Uplink played a much larger role than that of its previous, map and mini-game oriented functionality, which the device was originally used for in \"TimeSplitters 2\". Worn on the wrist of the main player character Sergeant Cortez, the uplink provides a map and the ability to pick up and throw objects, as well as the capability to activate switches remotely. Its grabbing abilities range from picking up weapons and ammo for use by the player in addition to launching objects from a distance at foes. This makes it an especially useful weapon for dispatching larger groups of enemies in the event that the player comes across one of the many explosive barrels placed throughout maps in the campaign mode of the game.\n\nAlthough \"Half-Life 2\" and \"TimeSplitters: Future Perfect\" were the first games released to feature a gravity gun (the former released on November 16, 2004 and the latter released on March 21, 2005), id Software had previously conceived a similar idea during the development of the earlier title \"Doom 3\". id Software designer Matt Hooper noted that \"we actually used it as a tool throughout development where we'd grab physics objects and place them around the world\". The tool was used to create \"damaged\" rooms in \"Doom 3\"; instead of constructing a ruined room, the designers would code a pristine room and use the device to \"damage\" it realistically. Although used to assist the development of \"Doom 3\", the gravity gun was not implemented in the final game. Hooper explained that \"we talked about that quite a few times, but we had such a big arsenal of weapons, and so many other cool things going on, that it was just one of those things that never made it in\". However, Nerve Software revived the code for the weapon five months after the release of \"Half-Life 2\" in \"Doom 3\"s expansion pack, \"Resurrection of Evil\".\n\nThe device is noted in the \"Doom 3\" storyline as an \"ionized plasma levitator\", created by the Union Aerospace Corporation for moving hazardous materials and a forerunner to tractor beams. Usually referred to as the \"grabber\", the player obtains the device early on in the course of \"Resurrection of Evil\". The grabber operates differently from \"Half-Life 2\"s gravity gun, using only a single trigger function. Once the grabber is aimed at an appropriate object, it locks on, allowing the player to lift the object with the trigger. When the player releases the trigger, the object will be propelled forward with force, turning it into an impromptu weapon. One key ability of the grabber is its capacity to lock on to the fireball projectiles cast by some hostile non-player characters, allowing players to turn the attack against their foe. However, unlike the gravity gun in \"Half-Life 2\", the grabber cannot hold objects for as long as the player wishes; if they wait too long to launch the object, the grabber will start to overload and disengage, dropping the object gently on the ground. Critics often compared the grabber directly with \"Half-Life 2\"s gravity gun, some noting that the device was far more combat-focused in operation than the gravity gun; in particular, the ability to turn projectiles cast by enemies against them was praised. However, the grabber was considered somewhat \"awkward\" to use, requiring a \"finesse\" that \"is rarely something the player has time for in a close-quarters situation\".\n\nVarious other video games have included gameplay features that allow players to use the game's physics to their advantage in combat. In some cases, these are manifested as weapons or devices. For instance, \"Portal\" (also created by Valve Software) features the Aperture Science Handheld Portal Device which displays a limited capacity to move objects around the game world, while Crytek's \"Crysis\" allows the player to throw objects and enemy characters considerable distances through the use of an experimental nanosuit.\n\nIn other games, however, it can be represented in a different manner:\n\nOn their list of top 100 weapons which originated in video games, IGN ranked the gravity gun #5. On GamesRadar's list of top 100 weapons in video games, the gravity gun was ranked #2 just behind the Cerebral Bore from \"Turok 2\".\n\n"}
{"id": "5162533", "url": "https://en.wikipedia.org/wiki?curid=5162533", "title": "Gray County Wind Farm", "text": "Gray County Wind Farm\n\nGray County Wind Farm near Montezuma, Kansas is the largest wind farm in Kansas and the largest in the United States not mandated by a state regulatory commission. The site consists of 170 Vestas V-47 wind turbines with a total nameplate capacity of 112 MW. Each turbine tower is 217 feet high, with blades 77 feet long, and a generating capacity of 660 kW. Owned and operated by NextEra Energy Resources, Gray County Wind Farm became fully operational in November 2001. Its construction cost an estimated $100 million. The average wind speed at the site is approximately 20 mph. The area is primarily used for farmland.\n\n"}
{"id": "38578270", "url": "https://en.wikipedia.org/wiki?curid=38578270", "title": "Güstrow Solarpark", "text": "Güstrow Solarpark\n\nGüstrow Solarpark is a 31-megawatt (MW) photovoltaic power station near Güstrow, Germany. It was built on the site of a former sugar factory and covers an area of .\n\n"}
{"id": "31501439", "url": "https://en.wikipedia.org/wiki?curid=31501439", "title": "Hammeren Hydroelectric Power Station", "text": "Hammeren Hydroelectric Power Station\n\nHammeren Hydroelectric Power Station () is a hydroelectric power station located in Oslo, Norway. It is the only power station in Oslo, and among the oldest power stations still running in Norway. It has a total installed capacity of 5 MW, and an annual production of 16 GWh. The power station utilises the waterfalls from Skjærsjøen to Maridalsvannet, with a total height of 105 m. It was established in 1900 by the company Christiania Elektricitetsværk, originally with 4 generator units, and increased to six units in 1901. In 1927 the six generators were replaced by a single 5.6 MW unit.\n"}
{"id": "407299", "url": "https://en.wikipedia.org/wiki?curid=407299", "title": "Hard water", "text": "Hard water\n\nHard water is water that has high mineral content (in contrast with \"soft water\"). Hard water is formed when water percolates through deposits of limestone and chalk which are largely made up of calcium and magnesium carbonates.\n\nHard drinking water may have moderate health benefits, but can pose critical problems in industrial settings, where water hardness is monitored to avoid costly breakdowns in boilers, cooling towers, and other equipment that handles water. In domestic settings, hard water is often indicated by a lack of foam formation when soap is agitated in water, and by the formation of limescale in kettles and water heaters. Wherever water hardness is a concern, water softening is commonly used to reduce hard water's adverse effects.\n\nWater's hardness is determined by the concentration of multivalent cations in the water. Multivalent cations are positively charged metal complexes with a charge greater than 1+. Usually, the cations have the charge of 2+. Common cations found in hard water include Ca and Mg. These ions enter a water supply by leaching from minerals within an aquifer. Common calcium-containing minerals are calcite and gypsum. A common magnesium mineral is dolomite (which also contains calcium). Rainwater and distilled water are soft, because they contain few ions.\n\nThe following equilibrium reaction describes the dissolving and formation of calcium carbonate and calcium bicarbonate (on the right):\n\nThe reaction can go in either direction. Rain containing dissolved carbon dioxide can react with calcium carbonate and carry calcium ions away with it. The calcium carbonate may be re-deposited as calcite as the carbon dioxide is lost to atmosphere, sometimes forming stalactites and stalagmites.\n\nCalcium and magnesium ions can sometimes be removed by water softeners.\n\nTemporary hardness is a type of water hardness caused by the presence of dissolved bicarbonate minerals (calcium bicarbonate and magnesium bicarbonate). When dissolved, these minerals yield calcium and magnesium cations (Ca, Mg) and carbonate and bicarbonate anions (CO, HCO). The presence of the metal cations makes the water hard. However, unlike the permanent hardness caused by sulphate and chloride compounds, this \"temporary\" hardness can be reduced either by boiling the water, or by the addition of lime (calcium hydroxide) through the process of lime softening. Boiling promotes the formation of carbonate from the bicarbonate and precipitates calcium carbonate out of solution, leaving water that is softer upon cooling.\n\nPermanent hardness is hardness (mineral content) that cannot be removed by boiling. When this is the case, it is usually caused by the presence of calcium sulphate/calcium chloride and/or magnesium sulphate/magnesium chloride in the water, which do not precipitate out as the temperature increases. Ions causing permanent hardness of water can be removed using a water softener, or ion exchange column.\n\nTotal Permanent Hardness = Permanent Calcium Hardness + Permanent Magnesium Hardness\n\nWith hard water, soap solutions form a white precipitate (soap scum) instead of producing lather, because the 2+ ions destroy the surfactant properties of the soap by forming a solid precipitate (the soap scum). A major component of such scum is calcium stearate, which arises from sodium stearate, the main component of soap:\nHardness can thus be defined as the soap-consuming capacity of a water sample, or the capacity of precipitation of soap as a characteristic property of water that prevents the lathering of soap. Synthetic detergents do not form such scums.\nHard water also forms deposits that clog plumbing. These deposits, called \"scale\", are composed mainly of calcium carbonate (CaCO), magnesium hydroxide (Mg(OH)), and calcium sulfate (CaSO). Calcium and magnesium carbonates tend to be deposited as off-white solids on the inside surfaces of pipes and heat exchangers. This precipitation (formation of an insoluble solid) is principally caused by thermal decomposition of bicarbonate ions but also happens in cases where the carbonate ion is at saturation concentration. The resulting build-up of scale restricts the flow of water in pipes. In boilers, the deposits impair the flow of heat into water, reducing the heating efficiency and allowing the metal boiler components to overheat. In a pressurized system, this overheating can lead to failure of the boiler. The damage caused by calcium carbonate deposits varies on the crystalline form, for example, calcite or aragonite.\n\nThe presence of ions in an electrolyte, in this case, hard water, can also lead to galvanic corrosion, in which one metal will preferentially corrode when in contact with another type of metal, when both are in contact with an electrolyte. The softening of hard water by ion exchange does not increase its corrosivity \"per se\". Similarly, where lead plumbing is in use, softened water does not substantially increase plumbo-solvency.\n\nIn swimming pools, hard water is manifested by a turbid, or cloudy (milky), appearance to the water. Calcium and magnesium hydroxides are both soluble in water. The solubility of the hydroxides of the alkaline-earth metals to which calcium and magnesium belong (group 2 of the periodic table) increases moving down the column. Aqueous solutions of these metal hydroxides absorb carbon dioxide from the air, forming the insoluble carbonates, giving rise to the turbidity. This often results from the pH being excessively high (pH > 7.6). Hence, a common solution to the problem is, while maintaining the chlorine concentration at the proper level, to lower the pH by the addition of hydrochloric acid, the optimum value being in the range of 7.2 to 7.6.\n\nIt is often desirable to soften hard water. Most detergents contain ingredients that counteract the effects of hard water on the surfactants. For this reason, water softening is often unnecessary. Where softening is practised, it is often recommended to soften only the water sent to domestic hot water systems so as to prevent or delay inefficiencies and damage due to scale formation in water heaters. A common method for water softening involves the use of ion exchange resins, which replace ions like Ca by twice the number of monocations such as sodium or potassium ions.\n\nWashing soda (sodium carbonate - NaCO) is easily obtained and has long been used as a water softener for domestic laundry, in conjunction with the usual soap or detergent.\n\nThe World Health Organization says that \"there does not appear to be any convincing evidence that water hardness causes adverse health effects in humans\". In fact, the United States National Research Council has found that hard water actually serves as a dietary supplement for calcium and magnesium.\n\nSome studies have shown a weak inverse relationship between water hardness and cardiovascular disease in men, up to a level of 170 mg calcium carbonate per litre of water. The World Health Organization has reviewed the evidence and concluded the data was inadequate to allow for a recommendation for a level of hardness.\n\nRecommendations have been made for the maximum and minimum levels of calcium (40–80 ppm) and magnesium (20–30 ppm) in drinking water, and a total hardness expressed as the sum of the calcium and magnesium concentrations of 2–4 mmol/L.\n\nOther studies have shown weak correlations between cardiovascular health and water hardness.\n\nSome studies correlate domestic hard water usage with increased eczema in children.\n\nThe Softened-Water Eczema Trial (SWET), a multicenter randomized controlled trial of ion-exchange softeners for treating childhood eczema, was undertaken in 2008. However, no meaningful difference in symptom relief was found between children with access to a home water softener and those without.\n\nHardness can be quantified by instrumental analysis. The total water hardness is the sum of the molar concentrations of Ca and Mg, in mol/L or mmol/L units. Although water hardness usually measures only the total concentrations of calcium and magnesium (the two most prevalent divalent metal ions), iron, aluminium, and manganese can also be present at elevated levels in some locations. The presence of iron characteristically confers a brownish (rust-like) colour to the calcification, instead of white (the color of most of the other compounds).\n\nWater hardness is often not expressed as a molar concentration, but rather in various units, such as degrees of general hardness (dGH), German degrees (°dH), parts per million (ppm, mg/L, or American degrees), grains per gallon (gpg), English degrees (°e, e, or °Clark), or French degrees (°fH, °F or °HF; lowercase \"f\" is used to prevent confusion with degrees Fahrenheit). The table below shows conversion factors between the various units.\n\nThe various alternative units represent an equivalent mass of calcium oxide (CaO) or calcium carbonate (CaCO) that, when dissolved in a unit volume of pure water, would result in the same total molar concentration of Mg and Ca. The different conversion factors arise from the fact that equivalent masses of calcium oxide and calcium carbonates differ, and that different mass and volume units are used. The units are as follows:\n\nBecause it is the precise mixture of minerals dissolved in the water, together with the water's pH and temperature, that determine the behavior of the hardness, a single-number scale does not adequately describe hardness. However, the United States Geological Survey uses the following classification into hard and soft water,\n\nSeawater is considered to be very hard due to various dissolved salts. Typically seawater's hardness is in the range of 6630 ppm. In contrast, freshwater has hardness in the range of 15 - 375 ppm.\n\nSeveral indices are used to describe the behaviour of calcium carbonate in water, oil, or gas mixtures.\n\nThe Langelier saturation index (sometimes Langelier stability index) is a calculated number used to predict the calcium carbonate stability of water. It indicates whether the water will precipitate, dissolve, or be in equilibrium with calcium carbonate. In 1936, Wilfred Langelier developed a method for predicting the pH at which water is saturated in calcium carbonate (called pHs). The LSI is expressed as the difference between the actual system pH and the saturation pH:\n\n\nIf the actual pH of the water is below the calculated saturation pH, the LSI is negative and the water has a very limited scaling potential. If the actual pH exceeds pHs, the LSI is positive, and being supersaturated with CaCO, the water has a tendency to form scale. At increasing positive index values, the scaling potential increases.\n\nIn practice, water with an LSI between -0.5 and +0.5 will not display enhanced mineral dissolving or scale forming properties. Water with an LSI below -0.5 tends to exhibit noticeably increased dissolving abilities while water with an LSI above +0.5 tends to exhibit noticeably increased scale forming properties.\n\nThe LSI is temperature sensitive. The LSI becomes more positive as the water temperature increases. This has particular implications in situations where well water is used. The temperature of the water when it first exits the well is often significantly lower than the temperature inside the building served by the well or at the laboratory where the LSI measurement is made. This increase in temperature can cause scaling, especially in cases such as hot water heaters. Conversely, systems that reduce water temperature will have less scaling.\n\nThe Ryznar stability index (RSI) uses a database of scale thickness measurements in municipal water systems to predict the effect of water chemistry.\n\nRyznar saturation index (RSI) was developed from empirical observations of corrosion rates and film formation in steel mains. It is defined as:\n\n\nThe Puckorius Scaling Index (PSI) uses slightly different parameters to quantify the relationship between the saturation state of the water and the amount of limescale deposited.\n\nOther indices include the Larson-Skold Index, the Stiff-Davis Index, and the Oddo-Tomson Index.\n\nThe hardness of local water supplies depends on the source of water. Water in streams flowing over volcanic (igneous) rocks will be soft, while water from boreholes drilled into porous rock is normally very hard.\n\nAnalysis of water hardness in major Australian cities by the Australian Water Association shows a range from very soft (Melbourne) to hard (Adelaide).\nTotal Hardness levels of calcium carbonate in ppm are:\nCanberra: 40; Melbourne: 10–26;\nSydney: 39.4–60.1;\nPerth: 29–226;\nBrisbane: 100;\nAdelaide: 134–148;\nHobart: 5.8–34.4;\nDarwin: 31.\n\nPrairie provinces (mainly Saskatchewan and Manitoba) contain high quantities of calcium and magnesium, often as dolomite, which are readily soluble in the groundwater that contains high concentrations of trapped carbon dioxide from the last glaciation. In these parts of Canada, the total hardness in ppm of calcium carbonate equivalent frequently exceed 200 ppm, if groundwater is the only source of potable water. The west coast, by contrast, has unusually soft water, derived mainly from mountain lakes fed by glaciers and snowmelt.\n\nSome typical values are: Montreal 116 ppm, Calgary 165 ppm, Regina 496 ppm, Saskatoon 160-180 ppm, Winnipeg 77 ppm, Toronto 121 ppm, Vancouver < 3 ppm, Charlottetown, PEI 140–150 ppm, Waterloo Region 400 ppm, Guelph 460 ppm, Saint John (West) 160-200 ppm.\n\nInformation from the British Drinking Water Inspectorate shows that drinking water in England is generally considered to be 'very hard', with most areas of England, particularly east of a line between the Severn and Tees estuaries, exhibiting above 200 ppm for the calcium carbonate equivalent. Water in London, for example, is mostly obtained from the River Thames and River Lea both of which derive significant proportion of their dry weather flow from springs in limestone and chalk aquifers. Wales, Devon, Cornwall and parts of North-West England are softer water areas, and range from 0 to 200 ppm. In the brewing industry in England and Wales, water is often deliberately hardened with gypsum in the process of Burtonisation.\n\nGenerally water is mostly hard in urban areas of England where soft water sources are unavailable. A number of cities built water supply sources in the 18th century as the industrial revolution and urban population burgeoned. Manchester was a notable such city in North West England and its wealthy corporation built a number of reservoirs at Thirlmere and Haweswater in the Lake District to the north. There is no exposure to limestone or chalk in their headwaters and consequently the water in Manchester is rated as 'very soft'. Similarly, tap water in Birmingham is also soft as it is sourced from the Elan Valley Reservoirs in Wales.\n\nThe EPA has published a standards handbook for the interpretation of water quality in Ireland in which definitions of water hardness are given.\nIn this section, reference to original EU documentation is given, which sets out no limit for hardness.\nIn turn, the handbook also gives no \"Recommended or Mandatory Limit Values\" for Hardness.\nThe handbooks does indicate that above the midpoint of the ranges defined as \"Moderately Hard\", effects are seen increasingly: \"The chief disadvantages of hard waters are that they neutralise the lathering power of soap... and, more important, that they can cause blockage of pipes and severely reduced boiler efficiency because of scale formation. These effects will increase as the hardness rises to and beyond 200 mg/l CaCO3.\"\n\nA collection of data from the United States found that about half the water stations tested had hardness over 120 mg per litre of calcium carbonate equivalent, placing them in the categories \"hard\" or \"very hard\". The other half were classified as soft or moderately hard. More than 85% of American homes have hard water. The softest waters occur in parts of the New England, South Atlantic-Gulf, Pacific Northwest, and Hawaii regions. Moderately hard waters are common in many of the rivers of the Tennessee, Great Lakes, and Alaska regions. Hard and very hard waters are found in some of the streams in most of the regions throughout the country. The hardest waters (greater than 1,000 ppm) are in streams in Texas, New Mexico, Kansas, Arizona, Utah, parts of Colorado, southern Nevada, and southern California.\n\n\n"}
{"id": "6172162", "url": "https://en.wikipedia.org/wiki?curid=6172162", "title": "High lead logging", "text": "High lead logging\n\nHigh lead logging is a method of cable logging using a spar, yarder and loader. It was developed by Oscar Wirkkala. It is accomplished with two lines (cables) and two winches (or cable drums). The mainline or yarding line extends out from one winch, while a second usually lighter line called the haulback line extends out from the other winch to a 'tail block' or pulley at the tail (back) end of the logging site, and passes through the tail block and connects to the main line. Butt rigging is installed where the two lines join and the logs are hooked to the butt rigging with chokers. The procedure is to wind up the main line and the logs are pulled in, wind up the haulback and the butt rigging is pulled out for more logs or another 'turn'.\n\nThe \"high lead\" feature is added by elevating both lines near the winch or 'head' end. This is accomplished by running the lines through a block (pulley) called the \"head block\" because it is on the head end of the project. Early on, it was customary to trim and top a tree making it into a 'spar pole' or 'spar tree' for the purpose of supporting the head blocks but gradually the use of wooden spars gave way over the 20th century to the use of steel spars stood up for the purpose. In any event the spars are supported by a number of guy wires.\n\nThe reason for elevating the lines (cables) at the head end is to assist in pulling the logs free of obstructions on the ground. Also if the trees are being partially lifted as they are transported it is less disruptive to the ground which can be an environmental issue.\n\nHigh lead is a popular method of logging on the West Coast of America.\n\nFirst used in 1904, with Lidgerwood winches, and a spar tree.\n\nHigh-lead Logging on the Olympic Peninsula in the 1920s-30s\n"}
{"id": "8487828", "url": "https://en.wikipedia.org/wiki?curid=8487828", "title": "Insulation monitoring device", "text": "Insulation monitoring device\n\nAn insulation monitoring device monitors the ungrounded system between an active phase conductor and earth. It is intended to give an alert (light and sound) or disconnect the power supply when the resistance between the two conductors drops below a set value, usually 50 kΩ (sample of IEC standard for medical applications). The main advantage is that the ungrounded or floating system allows a continuous operation of important consumers such as medical, chemical, military, etc. \n\nSome manufacturers of monitors for these systems are capable of handling VFDs (Variable Frequency (Speed) Drives). Most, however are not due to issues with the DC-portions of the VFDs.\n\nMost monitors work by injecting low level dc on the line and detecting. Some manufacturers use a patented AMP-monitoring principle (Adapted Measuring Pulse)\n\n"}
{"id": "31435654", "url": "https://en.wikipedia.org/wiki?curid=31435654", "title": "Integrated Utility Services UK", "text": "Integrated Utility Services UK\n\nIntegrated Utility Services is a high voltage electrical contracting business owned by an electrical distribution company and is based in Yorkshire in England. It is owned by Northern Electric Distribution Limited (NEDL) and Yorkshire Electricity Distribution plc (YEDL) which are the Distribution Network Operators for the North East England and Yorkshire regions.\n\nThe company was created after the takeover of Northern Electric by CalEnergy in 1996. The company disposed of the supply business of Northern Electric to Innogy in 2001 in exchange for the distribution business of Yorkshire Electricity.\n\n"}
{"id": "36745721", "url": "https://en.wikipedia.org/wiki?curid=36745721", "title": "International Urban Design Conference", "text": "International Urban Design Conference\n\nThe International Urban Design Conference is an international event held in Australia dedicated to Urban Design. The International Urban Design Conference was established in 2007 on the Gold Coast Queensland. Subsequent conferences have been held in Canberra, Sydney, Melbourne, Adelaide and Brisbane\n\nUrban Design Australia also uses a Blog, Facebook, Twitter and LinkedIn to engage with its community. There are over 20,000 subscribers to the site which also includes a podcast archive of over 400 presentations by UD practitioners. The 2016 Conference was held in Brisbane Australia.\n\n"}
{"id": "26888436", "url": "https://en.wikipedia.org/wiki?curid=26888436", "title": "International fuel gas code", "text": "International fuel gas code\n\nThe International Fuel Gas Code (IFGC) is published by the International Code Council through the governmental consensus process and is updated on a three year cycle to include the latest advances in technology and safest mechanical practices. The current version of this code is the 2018 edition. The IFGC is published in partnership with the American Gas Association (AGA). The IFGC protects public health and safety for all building systems that use fuel gas for the design, installation and inspection of such systems by providing minimum safeguards for people at homes, schools and workplace. Fuel burning appliances, furnaces, chimneys, vents, gas pipe sizing and many other such issues are addressed in the IFGC.\n\nThe IFGC is the most widely used Fuel Gas code in the United States.\n"}
{"id": "412280", "url": "https://en.wikipedia.org/wiki?curid=412280", "title": "José Cabrera Nuclear Power Station", "text": "José Cabrera Nuclear Power Station\n\nThe José Cabrera Nuclear Power Station (also known as Zorita) was a nuclear power station in Almonacid de Zorita, east of Madrid, Spain.\n\nThe power plant consisted of a single PWR of 160 MWe, built by Westinghouse Electric Company. It was operated by Unión Fenosa.\n\nConstruction of the power station started in 1964. It was commissioned in 1968, and it operated from 1968 until 2006. In 2006, it was closed by ministerial order.\n\nOn 11 February 2010, Unión Fenosa transferred the site's ownership to Enresa, a Spanish company responsible for decommissioning the power station. Enresa planned to clear the site by the end of 2015.\n\nThe Spanish Nuclear Safety Council has investigated the plant's deficient safety system and a missing screw that prevented the reactor from resuming operations after a month-long refueling operation in December 2003.\n\nWestinghouse Electric Company has secured a contract for dismantling and segmentation of the Jose Cabrera nuclear power station.\n\n\n"}
{"id": "25022573", "url": "https://en.wikipedia.org/wiki?curid=25022573", "title": "Kokoro (vegetable)", "text": "Kokoro (vegetable)\n\nKokoro is a variety of \"Dioscorea rotundata\" yam that are abundant in Western Nigeria, Benin and Togo. Their common use by ethnic groups such as the Yoruba that put heavy pressure on the cultivated land suggest that they have been cultivated since ancient times, since they are the only type of yam that gives good yields on degraded soil.\nIn modern times, Kokoro yams are gaining in importance as the yam chips trade is expanding.\nThe Kokoro variety is essential for preparing peeled and dried yam.\n"}
{"id": "32014001", "url": "https://en.wikipedia.org/wiki?curid=32014001", "title": "Kosovo B Power Station", "text": "Kosovo B Power Station\n\nKosovo B Power Station is the largest power station in Obilić, Kosovo. It is a lignite-fired consisting of 2 units with 340 MW generation capacity, which share a tall chimney with 6.8 metres diameter at the top.\n\nKosovo B Power Station was opened in 1983. It was operated by EPS Surface Mining Kosovo and EPS TPP Kosovo until the end of Kosovo War. After UNMIK administration was established in Kosovo on 1 July 1999, Elektroprivreda Srbije (EPS) lost its access to the local coal mines and power plants, including Kosovo A and Kosovo B power plants.\n\nSince then, it is operated by Korporata Energjetike e Kosovës (KEK).\n\n\n"}
{"id": "11465932", "url": "https://en.wikipedia.org/wiki?curid=11465932", "title": "Lifting-line theory", "text": "Lifting-line theory\n\nThe Prandtl lifting-line theory is a mathematical model that predicts lift distribution over a three-dimensional wing based on its geometry. It is also known as the Lanchester–Prandtl wing theory.\n\nThe theory was expressed independently by Frederick W. Lanchester in 1907, and by Ludwig Prandtl in 1918–1919 after working with Albert Betz and Max Munk.\n\nIn this model, the vortex loses strength along the whole wingspan because it is shed as a vortex-sheet from the trailing edge, rather than just at the wing-tips.\n\nOn a three-dimensional, finite wing, lift over each wing segment (local lift per unit span, formula_1 or formula_2) does not correspond simply to what two-dimensional analysis predicts. Instead, this local amount of lift is strongly affected by the lift generated at neighboring wing sections.\n\nIt is difficult to predict analytically the overall amount of lift that a wing of given geometry will generate. The lifting-line theory yields the lift distribution along the span-wise direction, formula_3 based only on the wing geometry (span-wise distribution of chord, airfoil, and twist) and flow conditions (formula_4, formula_5, formula_6).\n\nThe lifting-line theory applies the concept of circulation and the Kutta–Joukowski theorem,\n\nso that instead of the \"lift\" distribution function, the unknown effectively becomes the distribution of circulation over the span, formula_8.\n\nModeling the (unknown and sought-after) local lift with the (also unknown) local circulation allows us to account for the influence of one section over its neighbors. In this view, any span-wise change in lift is equivalent to a span-wise change of circulation. According to Helmholtz's theorems, a vortex filament cannot begin or terminate in the air. Any span-wise \"change in lift\" can be modeled as the shedding of a vortex filament down the flow, behind the wing.\n\nThis shed vortex, whose strength is the derivative of the (unknown) local wing circulation distribution, formula_9, influences the flow left and right of the wing section.\n\nThis sideways influence (upwash on the outboard, downwash on the inboard) is the key to the lifting-line theory. Now, if the \"change\" in lift distribution is known at given lift section, it is possible to predict how that section influences the lift over its neighbors: the vertical induced velocity (upwash or downwash, formula_10) can be quantified using the velocity distribution within a vortex, and related to a change in effective angle of attack over neighboring sections.\n\nIn mathematical terms, the local induced change of angle of attack formula_11 on a given section can be quantified with the integral sum of the downwash induced by every other wing section. In turn, the integral sum of the lift on each downwashed wing section is equal to the (known) total desired amount of lift.\n\nThis leads to an integro-differential equation in the form of formula_12 where formula_8 is expressed solely in terms of the wing geometry and its own span-wise variation, formula_14. The solution to this equation is a function, formula_8, that accurately describes the circulation (and therefore lift) distribution over a finite wing of known geometry.\n\nNomenclature:\n\nThe following are all functions of the wings span-wise station formula_23 (i.e. they can all vary along the wing)\n\nTo derive the model we start with the assumption that the circulation of the wing varies as a function of the spanwise locations. The function assumed is a Fourier function. Firstly, the coordinate for the spanwise location formula_23 is transformed by formula_33, where y is spanwise location, and s is the semi-span of the wing.\n\nand so the circulation is assumed to be:\n\nformula_34\n\nSince the circulation of a section is related the formula_35 by the equation:\n\nformula_36\n\nbut since the coefficient of lift is a function of angle of attack:\n\nformula_37\nhence the vortex strength at any particular spanwise station can be given by the equations:\n\nformula_38\n\nThis one equation has two unknowns: the value for formula_39 and the value for formula_40. However, the downwash is purely a function of the circulation only. So we can determine the value formula_40 in terms of formula_42, bring this term across to the left hand side of the equation and solve. The downwash at any given station is a function of the entire shed vortex system. This is determined by integrating the influence of each differential shed vortex over the span of the wing.\n\nDifferential element of circulation:\n\nformula_43\n\nDifferential downwash due to the differential element of circulation (acts like half an infinite vortex line):\n\nformula_44\n\nThe integral equation over the span of the wing to determine the downwash at a particular location is:\n\nformula_45\n\nAfter appropriate substitutions and integrations we get:\n\nformula_46\n\nAnd so the change in angle attack is determined by (assuming small angles):\n\nformula_47\n\nBy substituting equations 8 and 9 into RHS of equation 4 and equation 1 into the LHS of equation 4, we then get:\n\nformula_48\n\nAfter rearranging, we get the series of simultaneous equations:\n\nformula_49\n\nBy taking a finite number of terms, equation 11 can be expressed in matrix form and solved for coefficients A. Note the left-hand side of the equation represents each element in the matrix, and the terms on the RHS of equation 11 represent the RHS of the matrix form. Each row in the matrix form represents a different span-wise station, and each column represents a different value for n.\n\nAppropriate choices for formula_50 are as a linear variation between formula_51. Note that this range does not include the values for 0 and formula_52, as this leads to a singular matrix, which can't be solved.\n\nThe lift can be determined by integrating the circulation terms:\n\nformula_53\n\nwhich can be reduced to:\n\nformula_54\n\nwhere formula_55 is the first term of the solution of the simultaneous equations shown above.\n\nThe induced drag can be determined from\n\nformula_56\n\nwhich can also be reduced to:\n\nformula_57\n\nwhere formula_58 are all the terms of the solution of the simultaneous equations shown above.\n\nMoreover, this expression may be arranged as a function of formula_59 in the following way :\n\nformula_60\n\nformula_61\n\nformula_62\nformula_63\n\nwhere\n\nformula_64\n\nformula_65 is the span efficiency factor\n\nFor a symmetric wing, the even terms of the series coefficients are identically equal to 0, and so can be dropped.\n\nWhen the aircraft is rolling, an additional term can be added that adds the wing station distance multiplied by the rate of roll to give additional angle of attack change. Equation 3 then becomes:\n\nformula_66\n\nwhere \n\nNote that y can be negative, which introduces non-zero even coefficients in the equation that must be accounted for.\n\nThe effect of ailerons can be accounted by simply changing formula_68 term in Equation 3. For non-symmetric controls such as ailerons the formula_68 term changes on each side of the wing.\n\nFor an elliptical wing with no twist, with:\n\nformula_70\n\nThe chord length is given as a function of span location as:\n\nformula_71\n\nAlso,\n\nformula_72\n\nThis yields the famous equation for the elliptic induced drag coefficient:\n\nformula_73\n\nwhere\n\nA useful approximation is that\nwhere\n\nThe theoretical value for formula_29 is 2formula_52. Note that this equation becomes the thin airfoil equation if \"AR\" goes to infinity.\n\nAs seen above, the lifting-line theory also states an equation for induced drag:.\n"}
{"id": "20922572", "url": "https://en.wikipedia.org/wiki?curid=20922572", "title": "Making Peace with the Planet", "text": "Making Peace with the Planet\n\nMaking Peace With the Planet is a 1990 book by Barry Commoner. Commoner argues that, despite billions of dollars spent to save the environment, America is still in a deep environmental crisis. The book argues that environmental pollution can be prevented only through fundamental redesign of the way we produce goods.\n"}
{"id": "45563730", "url": "https://en.wikipedia.org/wiki?curid=45563730", "title": "Miscibility gap", "text": "Miscibility gap\n\nA miscibility gap is a region in a phase diagram for a mixture of components where the mixture exists as two or more phases - any region of composition of mixtures where the constituents are not completely miscible.\n\nThe IUPAC Gold Book defines \"miscibility gap\" as \"Area within the coexistence curve of an isobaric phase diagram (temperature vs composition) or an isothermal phase diagram (pressure vs composition).\"\n\nA miscibility gap between isostructural phases may be described as the solvus, a term also used to describe the boundary on a phase diagram between a miscibility gap and other phases.\n\nThermodynamically, miscibility gaps indicate a maxima (\"e.g.\" of Gibbs energy) in the composition range.\n\nA number of miscibility gaps in phase systems are named, including:\n\n"}
{"id": "45711268", "url": "https://en.wikipedia.org/wiki?curid=45711268", "title": "NASA Solar Technology Application Readiness", "text": "NASA Solar Technology Application Readiness\n\nThe NASA Solar Technology Application Readiness (NSTAR) is a type of spacecraft ion thruster called electrostatic ion thruster. It is a highly efficient low-thrust spacecraft propulsion running on electrical power generated by solar arrays. It uses high-voltage electrodes to accelerate ions with electrostatic forces.\n\nThe purpose of NSTAR program was to develop a xenon-fueled ion propulsion system for deep space missions.\nThe NSTAR electrostatic ion thruster was developed at NASA's Glenn Research Center and manufactured by Hughes, and Spectrum Astro, Inc. in the early 1990s. The feed system development was a collaborative effort between JPL and Moog Inc.\n\nThe ions are accelerated through two fine grids with roughly a 1300 V difference between them for 2.3 kW operation, with a thrust of 20-92 mN, a specific impulse of 1950-31000 N·s/kg and a total impulse capability of 2.65 x10 Ns. In 1996, the prototype engine endured 8000 hours of continuous operation in a vacuum chamber that simulates conditions of outer space. The results of the prototyping were used to define the design of flight hardware that was built for Deep Space 1 probe. One of the challenges was developing a compact and light weight power processing unit that converts power from the solar arrays into the voltages needed by the engine.\n\nThe engine achieves a specific impulse of one to three thousand seconds. This is an order of magnitude higher than traditional space propulsion methods, resulting in a mass savings of approximately half. This leads to much lighter and less expensive launch vehicles. Although the engine produces just 92 millinewtons (0.331 ounce-force) thrust at maximum power (2,100W on DS1 mission), the craft achieved high speed because ion engines thrust continuously for long periods of time.\n\nThe NSTAR ion thruster was first used on the Deep Space 1 (DS1) spacecraft, launched on 24 October 1998. The Deep Space mission carried out a flyby of asteroid 9969 Braille and Comet Borrelly. It produced 2.3 kW and was the primary propulsion for the probe.\n\nThe second interplanetary mission using NSTAR engine was the \"Dawn\" spacecraft, with three redundant units with a 30 cm diameter each. \"Dawn\" is the first NASA exploratory mission to use ion propulsion to enter and leave more than one orbit.\n\nNASA engineers state that NSTAR engines, in the 5-kilowatt and 0.04 pound-thrust range, are candidates for propelling spacecraft to Europa, Pluto, and other small bodies in deep space.\n\n"}
{"id": "29640043", "url": "https://en.wikipedia.org/wiki?curid=29640043", "title": "Open Automated Demand Response", "text": "Open Automated Demand Response\n\nOpen Automated Demand Response (OpenADR) is a research and standards development effort for energy management led by North American research labs and companies. The typical use is to send information and signals to cause electrical power-using devices to be turned off during periods of high demand.\n\nIn its early phases, the OpenADR research was initiated by Demand Response Research Center (DRRC) which is managed by Lawrence Berkeley National Laboratory (LBNL). The specification was released in April 2009. By contrast, the related OpenHAN standard for home area networks was promoted by utilities themselves and is an attempt to reconcile various home control technologies including X10, Insteon, P1901 and HomePlug.\n\nAn Open Automated Demand Response (OpenADR) outreach collaborative was eventually formed in October 2010 and a related OpenADR Alliance \"to accelerate the development, adoption and compliance of OpenADR standards throughout the energy industry\" and \"provide common language\" for smart meters. The effort sought publicity for its attempt to unify smart grid plans under a common standards umbrella to form a viable cleantech industry with a relatively level playing field. As NIST and NERC were committed to the OpenADR approach all along and the National Broadband Plan (United States) required (in its \"goal 6\") open access to consumer power use data by ADR providers, there was probably little doubt of the standards influence.\n\nDemand Response (DR) is a set of actions taken to reduce load when electric grid contingencies threaten supply-demand balance or market conditions occur that raise electricity costs. Automated demand response consists of fully automated signaling from a utility, ISO/RTO or other appropriate entity to provide automated connectivity to customer end-use control systems and strategies. OpenADR provides a foundation for interoperable information exchange to facilitate automated demand response.\n\nThe OpenADR Alliance is composed of industry stakeholders that are interested in fostering and accelerating the development and adoption of OpenADR standards and compliance with those standards. This extends to de facto standards based on specifications published by LBNL in April 2009, as well as Smart Grid-related standards emerging from OASIS, UCA, NAESB (North American Energy Standards Board), and ASHRAE (American Society of Heating, Refrigeration, and Air‐Conditioning Engineers).\n\nThe California energy crisis of 2002 served as the impetus for the effort that ultimately led to the creation of version 1.0 of the OpenADR standard. The Demand Response Research Center , which is operated by Lawrence Berkeley National Laboratory (LBNL), created the standard with funding from the California Energy Commission’s (www.energy.ca.gov) Public Interest Energy Research (PIER) program. Shortly after 2002, the DRRC worked with the California IOUs (SCE, SDG&E and PG&E) to jointly develop this technology through pilots and actual program implementations. In 2009, OpenADR was included in the Smart Grid Interoperability Standards Framework, and Federal Energy Regulatory Commission (FERC) identified OpenADR as a key standard for Demand Response. Additional standards work was performed by the Smart Grid Interoperability Panel (SGIP), which is being tasked by the U.S. National Institute of Standards and Technology (NIST) to oversee standardization of the Smart Grid. The North American Energy Standards Board contributed to the effort by developing a set of requirements. In that same year, the OpenADR specification was released as an official California Energy Commission (CEC) document, and the DRRC donated version 1.0 of the OpenADR standard to the Organization for the Advancement of Structured Information Standards and the Utilities Communication Architecture (UCA) International Users Group. The work to create version 2.0 of the OpenADR standard is being performed by OASIS through its Energy Interoperation (EI) Technical Committee with assistance from the UCAIug OpenADR Taskforce. Once work is completed on OpenADR version 2.0, with its expanded and complete set of DR and DER signals, the standard will be submitted to the International Electrotechnical Commission (IEC) in Geneva, Switzerland for adoption worldwide. IEC is the world’s leading organization for international standards for all electrical, electronic and related technologies.\n\nIn the Open Automated Demand Response Communications Specification (Version 1.0), LBNL describes OpenADR as:\n\nThe specification also describes the scope of the OpenADR standard:\n\nDuring a Demand Response event, the utility or ISO/RTO provides information to the DRAS about what has changed and on what schedule, such as start and stop times. A typical change would specify one or more of the following: \nNote that in the first three cases, it would be up to the customer to determine how best to participate in the OpenADR event. For example, commercial customers might be notified of a change in Time‐of‐Use pricing during a peak period, and the Energy Management System (EMS) might be programmed to temporarily offset building temperatures by several degrees and dim or turn‐off non‐essential lights. The last two cases normally shed load automatically based on an existing arrangement. If prices continue to climb higher the EMS may escalate the DR program by reducing or turning off rooftop air handlers during the same peak period. The standard also specifies considerable additional information that can be exchanged related to DR and DER events, including event name and identification, event status, operating mode, various enumerations (a fixed set of values characterizing the event), reliability and emergency signals, renewable generation status, market participation data (such as bids), test signals, and more.\n\nIn development of OpenADR, the OASIS Energy Interoperation Technical Committee is working with both the OASIS Energy Market Information Exchange (EMIX) and the OASIS Web Services Calendar (WS‐Calendar) Technical Committees to coordinate development of the full set of standards needed to exchange pricing information using a common schedule across energy markets. EMIX defines a standard way to exchange pricing and other information among ISOs and utilities. Because the price of electricity varies with the time of delivery, EMIX conveys the necessary time and interval data using WS‐Calendar as a common clock. Both EMIX (Common Price Communication Model) and WS‐ Calendar (Common Scheduling Mechanism) take advantage of the SGIP Priority Action Plan (PAP) assessment and recognition process.\n\nOpenADR also interacts, although less directly, with the NAESB Energy Usage Information Model and the ASHRAE Facility Smart Grid Information Model. The Energy Usage Information Model supports load curtailment, load shaping and energy market operations, all of which pertain to Demand Response. The Facility Smart Grid Information Model will create a standardized data exchange that enables control systems in the customer premises to manage electrical loads and generation sources in response to communications from utilities, and other electrical service providers or market operators. OpenADR will also need to interoperate directly or interwork indirectly with other popular protocols now used for energy management, including BACnet, LonMark and the Smart Energy Profile ZigBee. Depending on the protocol, this is expected to be accomplished either with enhancements to these standards, or with separate software or systems, such as a gateway function capable of translating between protocols.\n\nThe OpenADR Alliance was formed to build on the foundation of technical activities to support the development, testing, and deployment of commercial OpenADR and facilitates its acceleration and widespread adoption. This approach needs to engage service providers (such as electric utilities and systems operators) within the domain of the Smart Grid that publish OpenADR signals, as well as the facilities or third-party entities that consume them to manage electric loads. The OpenADR Alliance will enable all stakeholders to participate in automated DR, dynamic pricing, and electricity grid reliability.\n\nThe OpenADR Alliance activities will include, but not be limited to, the following:\n\n\nThe mission of the OpenADR Alliance is to foster the development, adoption and compliance of the OpenADR standards through collaboration, education, training, testing and certification. The Alliance will also promote the worldwide market acceptance of the OpenADR standard. Work being performed by industry coalitions like the OpenADR Alliance is critical to the success of new Smart Grid standards. No new standard is ever 100% complete, and different interpretations can cause interoperability problems in early implementations. For this reason, the Alliance is supported by a broad cross section of utilities, independent system operators, regional transmission operators, regulators and vendors who share a common interest in the success and widespread adoption of the OpenADR standard. In performing its work, the Alliance will adhere to industry best practices as detailed in the Interoperability Process Reference Manual (IPRM) created by the SGIP Testing and Certification Committee (www.sgip.org). The IPRM outlines the conformance, interoperability and cyber‐security testing and certification requirements for Smart Grid standards recommended by the SGIP.\n\n"}
{"id": "37528185", "url": "https://en.wikipedia.org/wiki?curid=37528185", "title": "Plug-in electric vehicles in the Netherlands", "text": "Plug-in electric vehicles in the Netherlands\n\nThe adoption of plug-in electric vehicles in the Netherlands is actively supported by the Dutch government through the exemption of the registration fee and road taxes. These purchase incentives have been adjusted over time. Considering the potential of plug-in electric vehicles in the country due to its relative small size and geography, the Dutch government set a target of 15,000 to 20,000 electric vehicles with three or more wheels on the roads in 2015; 200,000 vehicles in 2020; and 1 million vehicles in 2025. The first government target was achieved in 2013, two years earlier, thanks to the sales peak that occurred at the end of 2013. The stock of light-duty plug-in electric vehicles registered in the Netherlands achieved the 100,000 unit milestone in November 2016.\n\n, there were 121,542 highway legal light-duty plug-in electric vehicles registered in the Netherlands, consisting of 98,217 range-extended and plug-in hybrids, 21,115 pure electric cars, and 2,210 all-electric light utility vans. When buses, trucks, motorcycles, quadricycles and tricycles are accounted for, the Dutch plug-in electric-drive fleet climbs to 123,499 units. The country's electric vehicle stock reaches 165,886 units when electric bicycles (37,652), fuel cell electric vehicles (43), mopeds, and microcars are accounted for. A distinct feature of the Dutch plug-in market is dominance of plug-in hybrids, which represented 81% of the country's stock of passenger plug-in electric cars and vans registered at the end of December 2017.\n\n, the Mitsubishi Outlander P-HEV ranked as the all-time top-selling plug-in car in the country with 25,984 units registered, followed by the Volvo V60 Plug-in Hybrid with 15,804, and the Volkswagen Golf GTE with 10,691. The Tesla Model S is the all-time top selling all-electric car with 6,049 units registered. , the Netherlands had the second largest concentration per capita in the world after Norway. The Dutch market concentration of highway legal plug-in electric cars rose from 1.71 vehicles per 1,000 people in 2013 to 5.6 in July 2016, several times higher than world's largest plug-in car market, the United States. \n\nThe Dutch plug-in passenger car segment's market share surged almost ten times from 2012 to 5.34% new car sales in the country that year, achieving the world's second highest in 2013 after Norway (5.6%). The plug-in market share reached a record 9.7% of new car sales in the Dutch market in 2015, the second highest that year after Norway (22.4%). In 2015 the Netherlands listed as the world's third best-selling country market for light-duty plug-in electric vehicles. Plug-in electric car sales fell sharply during 2016 after changes in the tax rules that went into force at the beginning of 2016. As a result, the plug-in market share declined from 9.9% in 2015, to 6.7% in 2016, and fell to 2.6% in 2017.\n\n, the Netherlands was the country with the highest ratio of slow charging points to electric vehicles (EVSE/EV), with a ratio of more than 0.50, while the U.S had a slow EVSE/EV ratio of 0.20. The Netherlands' mix of slow and fast chargers has allowed it to become the country with the highest number of charging point per capita in the world. , there were 11,768 public slow charging points available 24/7, 14,320 slow charging point with limited public access, 612 public and semi-public fast charging points, and over 72,000 private charging points.\n\nConsidering the potential of plug-in electric vehicles in the country due to its relative small size and geography, the Dutch government set a target of 15,000 to 20,000 electric vehicles with three or more wheels on the roads in 2015; 200,000 vehicles in 2020; and 1 million vehicles in 2025. The first government target was achieved in 2013, two years earlier, thanks to the sales peak that occurred at the end of 2013. According to official figures, 30,086 plug-in electric vehicles with three or more wheels have been registered in the country through 31 December 2013.\n\nInitially, the Dutch government set incentives such as the total exemption of the registration fee and road taxes, which resulted in savings of approximately for private car owners over four years, and for corporate owners over five years. Other vehicles including hybrid electric vehicles were also exempt from these taxes if they emit less than 95 g/km for diesel-powered vehicles, or less than 110 g/km for gasoline-powered vehicles. The exemption from the registration tax ended on January 1, 2014, and thereafter, all-electric vehicles pay a 4% registration fee and plug-in hybrids a 7% fee. \n\nIn addition, the national government offers through the Ministry of Infrastructure and the Environment a subsidy on the purchase of all-electric taxis or delivery vans. This subsidy increases to per vehicle in Amsterdam, Rotterdam, The Hague, Utrecht, and Arnhem-Nijmegen metropolitan area. An additional subsidy is offered by several local government for the purchase of full electric taxis and vans, in Amsterdam and in Limburg and Tilburg.\n\nIn Amsterdam EV owners also have access to parking spaces reserved for battery electric vehicles, so they avoid the current wait for a parking place in Amsterdam, which can reach up to 10 years in some parts of the city. Free charging is also offered in public parking spaces. EV owners in the city of Rotterdam are entitled to one year of free parking in downtown and enjoy subsidies of up to if they install a home charger using green electricity. The city also introduced in 2014 a scrappage program to remove old polluting vehicles to improve air quality in the city. Rotterdam offers a incentive for business buyers to replace the old vehicles with all-electric vehicles. The subsidy is only available to the first 5,000 applicants that buy an eligible vehicle before the end of December 2013.\n\nOther factors contributing to the rapid adoption of plug-in electric vehicles are the relative small size of the country, which reduces range anxiety (the Netherlands stretches about east to west); a long tradition of environmental activism; high gasoline prices ( per gallon as of January 2013), which make the cost of running a car on electricity five times cheaper; and also some EV leasing programs provide free or discounted gasoline-powered vehicles for those who want to take a vacation driving long distances. With all of these incentives and tax breaks, plug-in electric cars have similar driving costs than conventional cars.\n\nInitially, sales of plug-in electric car were lower than expected, and during 2012 the segment captured a market share of less than 1% of new car sales in the country. As a result of the end of the total exemption of the registration fee, the segment sales peaked at the end of 2013, and plug-in electric car sales reached a market share of 5.34% of new car sales in 2013. The total cost of the tax exemptions for the Dutch treasury of the more than 22,000 plug-in electric vehicles sold in 2013 was estimated at ().\n\nOn November 24, 2011, Amsterdam became the fifth city in the world with a Car2Go carsharing service, and the first in Europe with an all-electric fleet. A fleet of 300 Smart electric drives is available on-demand. , these Car2Go vehicles and other electric cars in Amsterdam had access to more than 320 charging stations in the city area. The number is expected to increase significantly up to 1,000 by end of 2012.\n\n, the Netherlands was the country with the highest ratio of slow charging points to electric vehicles (EVSE/EV), with a ratio of more than 0.50, while the U.S had a slow EVSE/EV ratio of 0.20. The Netherlands' mix of slow and fast chargers has allowed it to become the country with the highest number of charging point per capita in the world. There were 80 CHAdeMO quick charging stations across the country by April 2014.\n\nThe number of charging stations increased from 400 units in 2010 to 1,841 in 2011. , there were 3,521 slow charging points available 24/7 to the public, up from 2,782 in December 2012; 2,249 slow charging point with limited public access, up from 829 in December 2012; and 106 public and semi-public fast charging points, up from 63 in December 2012. , the country's charging infrastructure consisted of 11,768 public slow charging points available 24/7 up from 7,395 in December 2015; 14,320 slow charging point with limited public access, up from 10,391 in December 2015; 612 public and semi-public fast charging points; and over 72,000 private charging points, up from 55,000 in 2015.\n\nAs part of its commitment to environmental sustainability, the Dutch government initiated a plan to establish over 200 recharging stations for electric vehicles across the country by 2015. The rollout will be undertaken by Switzerland-based power and automation company ABB and Dutch startup Fastned, and will aim to provide at least one station every for the country's 16 million residents.\n\n, there were 121,542 highway legal light-duty plug-in electric vehicles registered in the Netherlands, consisting of 98,217 range-extended and plug-in hybrids, 21,115 pure electric cars, and 2,210 all-electric light utility vans. When buses, trucks, motorcycles, quadricycles and tricycles are accounted for, the Dutch plug-in electric-drive fleet climbs to 123,499 units. The country's electric vehicle stock reaches 165,886 units when fuel cell electric vehicles (43), mopeds (4,376), electric bicycles (37,652), and microcars (316) are accounted for. A distinct feature of the Dutch plug-in market is dominance of plug-in hybrids, which represented 80.8% of the country's stock of passenger plug-in electric cars and vans registered at the end of December 2017.\n\nThe number of registered electric cars increased from 68 in 2009, through 395 in 2010 to 1,182 in 2011. Registrations reached 6,258 plug-in electric-drive passenger vehicles through December 2012, of which, 4,348 were range extending or plug-in hybrids. Registrations in 2013 rose to 29,342 plug-in passenger cars, of which, 24,512 were plug-in hybrids. The registered light-duty fleet climbed to 45,020 units in December 2014, consisting of 36,937 plug-in hybrids, 6,825 all-electric cars and 1,258 all-electric utility vans. With 43,971 plug-in passenger cars and utility vans registered in 2015, the Netherlands ranked as the world's third best-selling country market for light-duty plug-in vehicles that year.\n\nUntil December 2015, the Netherlands had the world's fourth largest light-duty plug-in vehicle stock after the U.S., China and Japan, and also had the largest fleet light-duty plug-in vehicles in Europe. Sales in the Dutch plug-in market fell sharply during 2016 after changes in the tax rules that went into force at the beginning of 2016. Sales during the first half of 2016 were down 64% from the same period in 2015. By early October 2016, the Netherlands listed as the third largest European plug-in market, after being surpassed by both Norway and France, and in the global ranking fell from fourth to sixth place. The stock of light-duty plug-in electric vehicles registered in the Netherlands achieved the 100,000 unit milestone in November 2016.\n\nIn 2013, the Netherlands reached a market concentration of 1.71 registered plug-in vehicles per 1,000 people, second only to Norway (4 per 1,000 people), and over three times higher than the world's two largest plug-in electric vehicle markets at the time, the United States and Japan. , the market concentration had increased to 5.6 registered plug-in cars per 1,000 people, almost as high as California's (5.8), the leading American market, but still exceeding the U.S. by 3.7 times. According to research report published by Navigant Research in April 2014, the fleet of light duty plug-in electric vehicles in use in the Amsterdam Metropolitan Area in 2013 is expected to represent 7.7% of the city's total registered light-duty vehicle stock.\n\nRegistrations of plug-in electric car represented a 0.57% share of total new car registrations in the country during 2011 and 2012, ahead of other European countries with a larger car market, such as Germany, France, and the U.K. During 2013 plug-in electric passenger car registrations totaled 22,415 units, climbing 338% from 2012, the highest rate of growth of any country in the world in 2013. The segment's market share surged almost ten times from 2012 to 5.34% new car sales in the country during that year, the world's second highest in 2013 after Norway (5.6%).\n\nThe market share of the plug-in electric passenger car segment in 2014 fell to 3.86% of total new passenger car registrations, after the end of some of the tax incentives. With 43,769 plug-in passenger cars registered in 2015, the segment market share rose to a record 9.74% of new car sales in the Dutch market in 2015, the second highest after Norway (22.4%). The Netherlands has the world's largest share of plug-in hybrids among its plug-in electric passenger car stock, with 83,686 plug-in hybrids registered at the end of August 2016, out of 95,088 plug-in electric cars, representing a 88.0% share of the Dutch plug-in car stock.\n\nThe Opel Ampera extended-range electric car became the best selling plug-in electric car in the Netherlands by May 2012, with a market share of more than 50%, and represented 77% of passenger EV sales in the country that month. The Netherlands was the top selling European market for the Ampera in 2012, with cumulative sales of 3,017 cars of the Volt/Ampera family through December 2012, consisting of 2,704 Amperas and 313 Volts. During 2012 the Netherlands also led European sales of the Toyota Prius Plug-in Hybrid (1,184 units) and the Fisker Karma (140 units).\n\nA total of 5,093 plug-in electric cars were registered in the Netherlands during 2012. Sales of range extending and plug-in hybrids in the Netherlands represented 8% of global sales of PHEVs in 2012. Sales of plug-in hybrid cars took the lead over all-electric cars during 2012. The Opel Ampera was the best selling plug-in electric car in 2012 with 2,696 units sold, and the Prius Plug-in Hybrid ranked second, with 1,184 units, followed by the Chevrolet Volt with 306 units sold during the year. Adding 140 Fisker Karmas sold during 2012, the plug-in hybrid segment led the Dutch EV market with 4,326 units sold during 2012, representing 84.9% of all plug-in electric car sales in the country that year. The Nissan Leaf was the top selling all-electric car in the country in 2012 with 265 units sold, and a total of 559 units since their introduction in the country by mid-2011. , the Mitsubishi i-MiEV family had sold 468 units since their release in 2010, including 252 iOns, 137 C-Zeros, and 79 i MiEVs.\n\nPlug-in car sales totaled 1,120 units in August 2013, marking the first time more than 1,000 plug-in electric cars were sold in the country in one month. During the first eight months of 2013, about 80% of plug-in car sales were made by corporate customers. This record was surpassed in September 2013, with 1,315 plug-in electric cars sold, led by the Volvo V60 plug-in with 755 units, followed by the Tesla Model S and the Opel Ampera, both with 170 units.\n\nThe monthly record for plug-in car sales was surpassed one more time in November 2013, with 5,225 plug-in electric cars sold. Plug-in electric car sales also captured a record market share of 12.8% of monthly new car sales. In addition, another record was set in November 2013, when for the first time in the Netherlands a plug-in electric vehicle was listed as the top selling new car that month. That record was set by the Mitsubishi Outlander P-HEV with 2,736 units sold, which represented a market share of 6.8% of all the new cars sold that month.\n\nAgain in December 2013, the Outlander P-HEV ranked as the top selling new car in the country with 4,976 units delivered, representing a 12.6% market share of new car sales that month. December sales reached a record of about 9,300 plug-in electric vehicles delivered, representing a world record market share of 23.8% of new car sales in the country. These record sales allowed the Netherlands to become the second country, after Norway, where plug-in electric cars have topped the monthly ranking of new car sales. The strong increase of plug-in car sales during the last months of 2013 was due to the end of the total exemption of the registration fee for corporate cars, which is valid for 5 years. From January 1, 2014, all-electric vehicles pay a 4% registration fee and plug-in hybrids a 7% fee. The Outlander P-HEV ended 2013 as the best-selling plug-in electric car with 8,039 units sold, followed by the Volvo V60 PHEV with 6,144 units. , out of 28,673 plug-in passenger cars registered in the Netherlands, plug-in hybrids represent 85.5% of total plug-in electric car registrations through 2013.\n\nAfter the end of the registration fee exemption, sales fell to 404 units in January 2014, with the Volvo V60 PHEV leading monthly sales with 272 units, followed by the Outlander P-HEV with 82 units. Sales of all-electric cars were led by the BMW i3, with 15 units delivered. Only 7 units of the Model S were sold in January. In April 2014 the Schiphol Group announced that three companies were selected to provide all-electric taxi service in Amsterdam Airport Schiphol. The concessions started on June 1, 2014 and service is provided with 100 Tesla Model S cars, which jointed service to the electric buses and hybrid cars already operating at the airport. Dutch sales of the Mitsubishi Outlander P-HEV reached the 10,000 unit milestone in April 2014. The Outlander continued as the top selling plug-in electric car in the country during the rest of 2014.\n\nA total of 15,678 light-duty plug-in electric vehicles were registered in the Netherlands in 2014, consisting of 12,425 plug-in hybrids, down 38.4% from 2013, 2,664 all-electric cars, up 18.3% from a year earlier, and 589 vans, up 236.6% from 2013. Sales in 2014 were led by the Outlander P-HEV with 7,712 units, followed by Volvo V60 Plug-in Hybrid with 3,126 units, and the Tesla Model S with 1,533 units. A total of 45,020 light-duty plug-in electric vehicles were registered in the Netherlands at the end of December 2014, consisting of 36,937 plug-in hybrids, 6,825 all-electric cars, and 1,258 all-electric utility vans.\n\nThe top 5 best-selling plug-in electric cars in 2015 were all plug-in hybrids, led by the Mitsubishi Outlander P-HEV (8,757), followed by the Volkswagen Golf GTE (8,183), Audi A3 e-tron (4,354), Volvo V60 Plug-in Hybrid (3,851), and Volkswagen Passat GTE (2,879). The top selling all-electric car was the Tesla Model S (1,842). Plug-in car sales achieved its best monthly volume on record ever in December 2015, with about 15,900 units sold, and allowing the segment to reach a record market share of about 23%. The surge in plug-in car sales was due to reduction of the registration fees for plug-in hybrids. From January 1, 2016, all-electric vehicles continue to pay a 4% registration fee, but for a plug-in hybrid the fee rises from 7% to 15% if its emissions do not exceed 50 g/km. The rate for a conventional internal combustion car is 25% of its book value.\n, the Mitsubishi Outlander P-HEV continues as the all-time top-selling plug-in car in the country with 24,506 registered. Ranking second is the Volvo V60 Plug-in Hybrid (14,470), followed by the Volkswagen Golf GTE (8,806), Opel Ampera (4,947 units), Tesla Model S (4,832), and Audi A3 e-tron (4,657). A total of 78,163 plug-in hybrids out of 87,531 passenger plug-in electric vehicles were registered in the Netherlands , meaning that plug-in hybrids dominate the Dutch market with a share of 89.3% of the country's highway legal plug-in electric car stock.\n\nA total of 5,397 plug-in cars were registered in the first half of 2016 representing a market share of the plug-in car segment of 2.8% of new car sales during the period. Sales during the first half of 2016 were down 64% year-on-year as a result of the changes in the tax rules that went into force at the beginning of 2016. A total of 9,185 plug-in passenger cars were registered in the first three quarters of 2016, consisting of 6,567 plug-in hybrids and 2,618 all-electric cars. The market share of the plug-in car segment captured 3.2% of new car sales during the period. \n\n, the Outlander P-HEV remained as the all-time top-selling plug-in car in the country with 25,984 units registered at the end of that month. Ranking second is the Volvo V60 Plug-in Hybrid (15,804), followed by the Volkswagen Golf GTE (10,691), Volkswagen Passat GTE (7,773), Mercedes-Benz C 350 e (6,226), and the Tesla Model S (6,049). , the Model S listed as the country's all-time top selling all-electric car, and combined sales of Tesla Motors models represented almost half of the 13,105 all-electric cars registered in the country. , plug-in hybrids continued their dominance of the Dutch plug-in market, with 87% (98,903) of the country's stock of 113,636 passenger plug-in electric cars and vans registered at the end of December 2016.\n\nWith a total of 25,134 Mitsubishi Outlander P-HEVs registered in the country by the end of December 2017, the plug-in hybrid continues as the all-time top selling plug-in electric vehicle in the Netherlands. The Tesla Model S also continues the best selling all-electric car with 8,028 units registered. As a result of the changes in tax rules, the plug-in market share declined from 9.9% in 2015, to 6.7% in 2016, and fell to 2.6% in 2017.\n\nThe following table presents annual registrations by model since 2009 through December 2015.\n\n\n"}
{"id": "147203", "url": "https://en.wikipedia.org/wiki?curid=147203", "title": "Polyethylene glycol", "text": "Polyethylene glycol\n\nPolyethylene glycol (PEG) is a polyether compound with many applications, from industrial manufacturing to medicine. PEG is also known as polyethylene oxide (PEO) or polyoxyethylene (POE), depending on its molecular weight. The structure of PEG is commonly expressed as H−(O−CH−CH)−OH.\n\n\"PEG\", \"PEO\", and \"POE\" refer to an oligomer or polymer of ethylene oxide. The three names are chemically synonymous, but historically \"PEG\" is preferred in the biomedical field, whereas \"PEO\" is more prevalent in the field of polymer chemistry. Because different applications require different polymer chain lengths, \"PEG\" has tended to refer to oligomers and polymers with a molecular mass below 20,000g/mol, \"PEO\" to polymers with a molecular mass above 20,000g/mol, and \"POE\" to a polymer of any molecular mass. PEGs are prepared by polymerization of ethylene oxide and are commercially available over a wide range of molecular weights from 300g/mol to 10,000,000g/mol.\n\nPEG and PEO are liquids or low-melting solids, depending on their molecular weights. While PEG and PEO with different molecular weights find use in different applications, and have different physical properties (e.g. viscosity) due to chain length effects, their chemical properties are nearly identical. Different forms of PEG are also available, depending on the initiator used for the polymerization process – the most common initiator is a monofunctional methyl ether PEG, or methoxypoly(ethylene glycol), abbreviated mPEG. Lower-molecular-weight PEGs are also available as purer oligomers, referred to as monodisperse, uniform, or discrete. Very high purity PEG has recently been shown to be crystalline, allowing determination of a crystal structure by x-ray diffraction. Since purification and separation of pure oligomers is difficult, the price for this type of quality is often 10–1000 fold that of polydisperse PEG.\n\nPEGs are also available with different geometries.\n\nThe numbers that are often included in the names of PEGs indicate their average molecular weights (e.g. a PEG with would have an average molecular weight of approximately 400 daltons, and would be labeled PEG 400.) Most PEGs include molecules with a distribution of molecular weights (i.e. they are polydisperse). The size distribution can be characterized statistically by its weight average molecular weight (Mw) and its number average molecular weight (Mn), the ratio of which is called the polydispersity index (Mw/Mn). Mw and Mn can be measured by mass spectrometry.\n\nPEGylation is the act of covalently coupling a PEG structure to another larger molecule, for example, a therapeutic protein, which is then referred to as a \"PEGylated\" protein. PEGylated interferon alfa-2a or −2b are commonly used injectable treatments for hepatitis C infection.\n\nPEG is soluble in water, methanol, ethanol, acetonitrile, benzene, and dichloromethane, and is insoluble in diethyl ether and hexane. It is coupled to hydrophobic molecules to produce non-ionic surfactants.\n\nPEGs potentially contain toxic impurities, such as ethylene oxide and 1,4-dioxane. Ethylene Glycol and its ethers are nephrotoxic if applied to damaged skin.\n\nPolyethylene glycol (PEG) and related polymers (PEG phospholipid constructs) are often sonicated when used in biomedical applications. However, as reported by Murali et al., PEG is very sensitive to sonolytic degradation and PEG degradation products can be toxic to mammalian cells. It is, thus, imperative to assess potential PEG degradation to ensure that the final material does not contain undocumented contaminants that can introduce artifacts into experimental results.\n\nPEGs and methoxypolyethylene glycols are manufactured by Dow Chemical under the tradename \"Carbowax\" for industrial use, and \"Carbowax Sentry\" for food and pharmaceutical use. They vary in consistency from liquid to solid, depending on the molecular weight, as indicated by a number following the name. They are used commercially in numerous applications, including as surfactants, in foods, in cosmetics, in pharmaceutics, in biomedicine, as dispersing agents, as solvents, in ointments, in suppository bases, as tablet excipients, and as laxatives. Some specific groups are lauromacrogols, nonoxynols, octoxynols, and poloxamers.\n\nMacrogol, used as a laxative, is a form of polyethylene glycol. The name may be followed by a number which represents the average molecular weight (e.g. macrogol 3350, macrogol 4000 or macrogol 6000).\n\nThe production of polyethylene glycol was first reported in 1859. Both A. V. Laurence and Charles Adolphe Wurtz independently isolated products that were polyethylene glycols. Polyethylene glycol is produced by the interaction of ethylene oxide with water, ethylene glycol, or ethylene glycol oligomers. The reaction is catalyzed by acidic or basic catalysts. Ethylene glycol and its oligomers are preferable as a starting material instead of water, because they allow the creation of polymers with a low polydispersity (narrow molecular weight distribution). Polymer chain length depends on the ratio of reactants.\n\nDepending on the catalyst type, the mechanism of polymerization can be cationic or anionic. The anionic mechanism is preferable because it allows one to obtain PEG with a low polydispersity. Polymerization of ethylene oxide is an exothermic process. Overheating or contaminating ethylene oxide with catalysts such as alkalis or metal oxides can lead to runaway polymerization, which can end in an explosion after a few hours.\n\nPolyethylene oxide, or high-molecular weight polyethylene glycol, is synthesized by suspension polymerization. It is necessary to hold the growing polymer chain in solution in the course of the polycondensation process. The reaction is catalyzed by magnesium-, aluminium-, or calcium-organoelement compounds. To prevent coagulation of polymer chains from solution, chelating additives such as dimethylglyoxime are used.\n\nAlkaline catalysts such as sodium hydroxide (NaOH), potassium hydroxide (KOH), or sodium carbonate (NaCO) are used to prepare low-molecular-weight polyethylene glycol.\n\n\n\n\n\n\nPEG is generally considered biologically inert and safe. However, studies of clinical safety are generally based on adults, not children. The FDA has been asked to investigate the possible effects of PEG in laxatives for children. Also, a minority of people are allergic to it. Allergy to PEG is usually discovered after a person has been diagnosed with an allergy to an increasing number of seemingly unrelated products, including processed foods, cosmetics, drugs, and other substances that contain PEG or were manufactured with PEG.\n\nWhen PEG is chemically attached to therapeutic molecules (such as protein drugs or nanoparticles), it can sometimes be antigenic, stimulating an anti-PEG antibody response in some patients. This effect has only been shown for a few of the many available PEGylated therapeutics, but it has significant effects on clinical outcomes of affected patients. Other than these few instances where patients have anti-PEG immune responses, it is generally considered to be a safe component of drug formulations.\n\n\n"}
{"id": "25603", "url": "https://en.wikipedia.org/wiki?curid=25603", "title": "Rhenium", "text": "Rhenium\n\nRhenium is a chemical element with symbol Re and atomic number 75. It is a silvery-gray, heavy, third-row transition metal in group 7 of the periodic table. With an estimated average concentration of 1 part per billion (ppb), rhenium is one of the rarest elements in the Earth's crust. Rhenium has the third-highest melting point and second-highest boiling point of any element at 5903 K. Rhenium resembles manganese and technetium chemically and is mainly obtained as a by-product of the extraction and refinement of molybdenum and copper ores. Rhenium shows in its compounds a wide variety of oxidation states ranging from −1 to +7.\n\nDiscovered in 1908, rhenium was the second-last stable element to be discovered. It was named after the river Rhine in Europe.\n\nNickel-based superalloys of rhenium are used in the combustion chambers, turbine blades, and exhaust nozzles of jet engines. These alloys contain up to 6% rhenium, making jet engine construction the largest single use for the element. The second-most important use is as a catalyst: rhenium is an excellent catalyst for hydrogenation and isomerization, and is used for example in catalytic reforming of naphtha for use in gasoline (Rheniforming process). Because of the low availability relative to demand, rhenium is expensive, with price reaching an all-time high in 2008/2009 US$10,600 per kilogram (US$4,800 per pound). Due to increases in rhenium recycling and a drop in demand for Rhenium in catalysts, the price of rhenium has dropped to US$2,844 per kilogram (US$1,290 per pound) as of July 2018.\n\nRhenium ( meaning: \"Rhine\") was the second last-discovered of the elements that have a stable isotope (other new elements discovered in nature since then, such as francium, are radioactive). The existence of a yet-undiscovered element at this position in the periodic table had been first predicted by Dmitri Mendeleev. Other calculated information was obtained by Henry Moseley in 1914. In 1908, Japanese chemist Masataka Ogawa announced that he had discovered the 43rd element and named it \"nipponium\" (Np) after Japan (\"Nippon\" in Japanese). However, recent analysis indicated the presence of rhenium (element 75), not element 43, although this reinterpretation has been questioned by Eric Scerri. The symbol Np was later used for the element neptunium, and the name \"nihonium\", also named after Japan, along with symbol Nh, was later used for element 113. Element 113 was also discovered by a team of Japanese scientists and was named in respectful homage to Ogawa's work.\n\nRhenium is generally considered to have been discovered by Walter Noddack, Ida Noddack, and Otto Berg in Germany. In 1925 they reported that they had detected the element in platinum ore and in the mineral columbite. They also found rhenium in gadolinite and molybdenite. In 1928 they were able to extract 1 g of the element by processing 660 kg of molybdenite. It was estimated in 1968 that 75% of the rhenium metal in the United States was used for research and the development of refractory metal alloys. It took several years from that point before the superalloys became widely used.\n\nRhenium is a silvery-white metal with one of the highest melting points of all elements, exceeded by only tungsten and carbon. It also has one of the highest boiling points of all elements. It is also one of the densest, exceeded only by platinum, iridium and osmium. Rhenium has a hexagonal close-packed crystal structure, with lattice parameters \"a\" = 276.1 pm and \"c\" = 445.6 pm.\n\nIts usual commercial form is a powder, but this element can be consolidated by pressing and sintering in a vacuum or hydrogen atmosphere. This procedure yields a compact solid having a density above 90% of the density of the metal. When annealed this metal is very ductile and can be bent, coiled, or rolled. Rhenium-molybdenum alloys are superconductive at 10 K; tungsten-rhenium alloys are also superconductive around 4–8 K, depending on the alloy. Rhenium metal superconducts at .\nIn bulk form and at room temperature and atmospheric pressure, the element resists alkalis, sulfuric acid, hydrochloric acid, dilute (but not concentrated) nitric acid, and aqua regia.\n\nRhenium has one stable isotope, rhenium-185, which nevertheless occurs in minority abundance, a situation found only in two other elements (indium and tellurium). Naturally occurring rhenium is only 37.4% Re, and 62.6% Re, which is unstable but has a very long half-life (≈10 years). This lifetime can be greatly affected by the charge state of rhenium atom. The beta decay of Re is used for rhenium-osmium dating of ores. The available energy for this beta decay (2.6 keV) is one of the lowest known among all radionuclides. The isotope rhenium-186m is notable as being one of the longest lived metastable isotopes with a half-life of around 200,000 years. There are twenty-five other recognized radioactive isotopes of rhenium.\n\nRhenium compounds are known for all the oxidation states between −3 and +7 except −2. The oxidation states +7, +6, +4, and +2 are the most common. Rhenium is most available commercially as salts of perrhenate, including sodium and ammonium perrhenates. These are white, water-soluble compounds.\n\nThe most common rhenium chlorides are ReCl, ReCl, ReCl, and ReCl. The structures of these compounds often feature extensive Re-Re bonding, which is characteristic of this metal in oxidation states lower than VII. Salts of [ReCl] feature a quadruple metal-metal bond. Although the highest rhenium chloride features Re(VI), fluorine gives the d Re(VII) derivative rhenium heptafluoride. Bromides and iodides of rhenium are also well known.\n\nLike tungsten and molybdenum, with which it shares chemical similarities, rhenium forms a variety of oxyhalides. The oxychlorides are most common, and include ReOCl, ReOCl.\n\nThe most common oxide is the volatile colourless ReO. Rhenium trioxide ReO adopts a perovskite-like structure. Other oxides include ReO, ReO, and ReO. The sulfides are ReS and ReS. Perrhenate salts can be converted to tetrathioperrhenate by the action of ammonium hydrosulfide.\n\nRhenium diboride (ReB) is a hard compound having the hardness similar to that of tungsten carbide, silicon carbide, titanium diboride or zirconium diboride.\n\nDirhenium decacarbonyl is the most common entry to organorhenium chemistry. Its reduction with sodium amalgam gives Na[Re(CO)] with rhenium in the formal oxidation state −1. Dirhenium decacarbonyl can be oxidised with bromine to bromopentacarbonylrhenium(I):\n\nReduction of this pentacarbonyl with zinc and acetic acid gives pentacarbonylhydridorhenium:\n\nMethylrhenium trioxide (\"MTO\"), CHReO is a volatile, colourless solid has been used as a catalyst in some laboratory experiments. It can be prepared by many routes, a typical method is the reaction of ReO and tetramethyltin:\nAnalogous alkyl and aryl derivatives are known. MTO catalyses for the oxidations with hydrogen peroxide. Terminal alkynes yield the corresponding acid or ester, internal alkynes yield diketones, and alkenes give epoxides. MTO also catalyses the conversion of aldehydes and diazoalkanes into an alkene.\n\nA distinctive derivative of rhenium is nonahydridorhenate, originally thought to be the \"rhenide\" anion, Re, but actually containing the anion in which the oxidation state of rhenium is +7.\n\nRhenium is one of the rarest elements in Earth's crust with an average concentration of 1 ppb; other sources quote the number of 0.5 ppb making it the 77th most abundant element in Earth's crust. Rhenium is probably not found free in nature (its possible natural occurrence is uncertain), but occurs in amounts up to 0.2% in the mineral molybdenite (which is primarily molybdenum disulfide), the major commercial source, although single molybdenite samples with up to 1.88% have been found. Chile has the world's largest rhenium reserves, part of the copper ore deposits, and was the leading producer as of 2005. It was only recently that the first rhenium mineral was found and described (in 1994), a rhenium sulfide mineral (ReS) condensing from a fumarole on Kudriavy volcano, Iturup island, in the Kuril Islands. Kudryavy discharges up to 20–60 kg rhenium per year mostly in the form of rhenium disulfide. Named rheniite, this rare mineral commands high prices among collectors. \n\nCommercial rhenium is extracted from molybdenum roaster-flue gas obtained from copper-sulfide ores. Some molybdenum ores contain 0.001% to 0.2% rhenium. Rhenium(VII) oxide and perrhenic acid readily dissolve in water; they are leached from flue dusts and gasses and extracted by precipitating with potassium or ammonium chloride as the perrhenate salts, and purified by recrystallization. Total world production is between 40 and 50 tons/year; the main producers are in Chile, the United States, Peru, and Poland. Recycling of used Pt-Re catalyst and special alloys allow the recovery of another 10 tons per year. Prices for the metal rose rapidly in early 2008, from $1000–$2000 per kg in 2003–2006 to over $10,000 in February 2008. The metal form is prepared by reducing ammonium perrhenate with hydrogen at high temperatures:\n\nRhenium is added to high-temperature superalloys that are used to make jet engine parts, using 70% of the worldwide rhenium production. Another major application is in platinum–rhenium catalysts, which are primarily used in making lead-free, high-octane gasoline.\n\nThe nickel-based superalloys have improved creep strength with the addition of rhenium. The alloys normally contain 3% or 6% of rhenium. Second-generation alloys contain 3%; these alloys were used in the engines for the F-15 and F-16, whereas the newer single-crystal third-generation alloys contain 6% of rhenium; they are used in the F-22 and F-35 engines. Rhenium is also used in the superalloys, such as CMSX-4 (2nd gen) and CMSX-10 (3rd gen) that are used in industrial gas turbine engines like the GE 7FA. Rhenium can cause superalloys to become microstructurally unstable, forming undesirable TCP (topologically close packed) phases. In 4th- and 5th-generation superalloys, ruthenium is used to avoid this effect. Among others the new superalloys are EPM-102 (with 3% Ru) and TMS-162 (with 6% Ru), as well as TMS-138 and TMS-174.\n\nFor 2006, the consumption is given as 28% for General Electric, 28% Rolls-Royce plc and 12% Pratt & Whitney, all for superalloys, whereas the use for catalysts only accounts for 14% and the remaining applications use 18%. In 2006, 77% of the rhenium consumption in the United States was in alloys. The rising demand for military jet engines and the constant supply made it necessary to develop superalloys with a lower rhenium content. For example, the newer CFM International CFM56 high-pressure turbine (HPT) blades will use Rene N515 with a rhenium content of 1.5% instead of Rene N5 with 3%.\n\nRhenium improves the properties of tungsten. Tungsten-rhenium alloys are more ductile at low temperature, allowing them to be more easily machined. The high-temperature stability is also improved. The effect increases with the rhenium concentration, and therefore tungsten alloys are produced with up to 27% of Re, which is the solubility limit. Tungsten-rhenium wire was originally created in efforts to develop a wire that was more ductile after recrystallization. This allows the wire to meet specific performance objectives, including superior vibration resistance, improved ductility, and higher resistivity. One application for the tungsten-rhenium alloys is X-ray sources. The high melting point of both elements, together with their high atomic mass, makes them stable against the prolonged electron impact. Rhenium tungsten alloys are also applied as thermocouples to measure temperatures up to 2200 °C.\n\nThe high temperature stability, low vapor pressure, good wear resistance and ability to withstand arc corrosion of rhenium are useful in self-cleaning electrical contacts. In particular, the discharge occurring during the switching oxidizes the contacts. However, rhenium oxide ReO has poor stability (sublimes at ~360 °C) and therefore is removed during the discharge.\n\nRhenium has a high melting point and a low vapor pressure similar to tantalum and tungsten. Therefore, rhenium filaments exhibit a higher stability if the filament is operated not in vacuum, but in oxygen-containing atmosphere. Those filaments are widely used in mass spectrometers, in ion gauges and in photoflash lamps in photography.\n\nRhenium in the form of rhenium-platinum alloy is used as catalyst for catalytic reforming, which is a chemical process to convert petroleum refinery naphthas with low octane ratings into high-octane liquid products. Worldwide, 30% of catalysts used for this process contain rhenium. The olefin metathesis is the other reaction for which rhenium is used as catalyst. Normally ReO on alumina is used for this process. Rhenium catalysts are very resistant to chemical poisoning from nitrogen, sulfur and phosphorus, and so are used in certain kinds of hydrogenation reactions.\n\nThe isotopes Re and Re are radioactive and are used for treatment of liver cancer. They both have similar penetration depth in tissue (5 mm for Re and 11 mm for Re), but Re has advantage of longer lifetime (90 hours vs. 17 hours).\n\nRe is also being used experimentally in a novel treatment of pancreatic cancer where it is delivered by means of the bacterium \"Listeria monocytogenes\".\n\nRelated by periodic trends, rhenium has a similar chemistry to that of technetium; work done to label rhenium onto target compounds can often be translated to technetium. This is useful for radiopharmacy, where it is difficult to work with technetium – especially the 99m isotope used in medicine – due to its expense and short half-life.\n\nVery little is known about the toxicity of rhenium and its compounds because they are used in very small amounts. Soluble salts, such as the rhenium halides or perrhenates, could be hazardous due to elements other than rhenium or due to rhenium itself. Only a few compounds of rhenium have been tested for their acute toxicity; two examples are potassium perrhenate and rhenium trichloride, which were injected as a solution into rats. The perrhenate had an LD value of 2800 mg/kg after seven days (this is very low toxicity, similar to that of table salt) and the rhenium trichloride showed LD of 280 mg/kg.\n\n"}
{"id": "32009704", "url": "https://en.wikipedia.org/wiki?curid=32009704", "title": "Security assurance", "text": "Security assurance\n\nA security assurance, in the context of nuclear warfare, is an expression of a political position by a nuclear-armed nation intended to placate other non-nuclear-armed nations. There are two types of security assurance: positive and negative. A positive assurance states that the nation giving it will aid any, or a particular, non-nuclear-armed nation in retaliation, should it be a victim of nuclear attack. A negative assurance is not the opposite of this, but instead means that a nuclear-armed nation has promised not to use nuclear weapons, except in retaliation for a nuclear attack against themselves. See also No first use.\n\nSecurity assurances are a key part of nuclear diplomacy, and since they are statements of intent - not guarantees - they are based entirely on trust and the threat of retaliation should they be broken. Thus, Security Assurances have been issued and changed over time, and are vital tools in the Nuclear Non-Proliferation Treaty (NPT). U.S. President Barack Obama modified the specific conditions of the US Negative Security Assurance on 6 April 2010 citing \"the importance of nations meeting their NPT and nuclear non-proliferation obligations.\".\n\nChina is so far the only nation to issue a positive security assurance in April 1995, at the same time it issued a negative security assurance. China stated that despite its willingness to use nuclear weapons, it \"[should] not in any way be construed as endorsing the use of nuclear weapons.\"<ref name=\"S/1995/265\"></ref> The dichotomy of such a position has led for calls to make more use of protocols attached to NWFZ (nuclear-weapon-free zone) treaties, instead of generalized security assurances.\n\n"}
{"id": "7997999", "url": "https://en.wikipedia.org/wiki?curid=7997999", "title": "Sheet moulding compound", "text": "Sheet moulding compound\n\nSheet moulding compound (SMC) or sheet moulding composite is a ready to mould glass-fibre reinforced polyester material primarily used in compression moulding. The sheet is provided in rolls weighing up to 1000 kg. Alternatively the resin and related materials may be mixed on site when a producer wants greater control over the chemistry and filler.\n\nSMC is both a process and reinforced composite material. This is manufactured by dispersing long strands (usually >1”) of chopped fiber (commonly glass fibers or carbon fibers on a bath of thermoset resin (commonly polyester resin, vinyl ester resin or epoxy resin)). The longer fibers in SMC result in better strength properties than standard bulk moulding compound (BMC) products. Typical applications include demanding electrical applications, corrosion resistant needs, structural components at low cost, automotive, and transit. \n\nPaste reservoir dispenses a measured amount of specified resin paste onto a plastic carrier film. This carrier film passes underneath a chopper which cuts the fibers onto the surface. Once these have drifted through the depth of resin paste, another sheet is added on top which sandwiches the glass. The sheets are compacted and then enter onto a take-up roll, which is used to store the product whilst it matures. The carrier film is then later removed and the material is cut into charges. Depending on what shape is required determines the shape of the charge and steel die which it is then added to. Heat and pressure act on the charge and once fully cured, this is then removed from the mould as the finished product. Fillers both reduce weight and change the physical properties, typically adding strength. Production challenges include wetting the filler, which could consist of glass microspheres or aligned fibers rather than random chopped fibers; adjusting die temperature and pressure to provide the proper geometry; and adjusting chemistry to end use.\n\nCompared to similar methods, SMC benefits from a very high volume production ability, excellent part reproducibility, it is cost effective as low labor requirements per production level is very good and industry scrap is reduced substantially. Weight reduction, due to lower dimensional requirements and because of the ability to consolidate many parts into one, is also advantageous. The level of flexibility also exceeds many counterpart processes.\n\nProperties vary depending upon fiber and resin types.\n\n"}
{"id": "84726", "url": "https://en.wikipedia.org/wiki?curid=84726", "title": "Sulfate", "text": "Sulfate\n\nThe sulfate or sulphate (see spelling differences) ion is a polyatomic anion with the empirical formula . Sulfate is the spelling recommended by IUPAC, but sulphate is used in British English. Salts, acid derivatives, and peroxides of sulfate are widely used in industry. Sulfates occur widely in everyday life. Sulfates are salts of sulfuric acid and many are prepared from that acid.\n\nThe sulfate anion consists of a central sulfur atom surrounded by four equivalent oxygen atoms in a tetrahedral arrangement. The symmetry is the same as that of methane. The sulfur atom is in the +6 oxidation state while the four oxygen atoms are each in the −2 state. The sulfate ion carries an overall charge of −2 and it is the conjugate base of the bisulfate (or hydrogen sulfate) ion, , which is in turn the conjugate base of , sulfuric acid. Organic sulfate esters, such as dimethyl sulfate, are covalent compounds and esters of sulfuric acid. The tetrahedral molecular geometry of the sulfate ion is as predicted by VSEPR theory.\n\nThe first description of the bonding in modern terms was by Gilbert Lewis in his groundbreaking paper of 1916 where he described the bonding in terms of electron octets around each atom, that is no double bonds and a formal charge of +2 on the sulfur atom.\n\nLater, Linus Pauling used valence bond theory to propose that the most significant resonance canonicals had two pi bonds involving d orbitals. His reasoning was that the charge on sulfur was thus reduced, in accordance with his principle of electroneutrality. The S−O bond length of 149 pm is shorter than the bond lengths in sulfuric acid of 157 pm for S−OH. The double bonding was taken by Pauling to account for the shortness of the S−O bond. Pauling's use of d orbitals provoked a debate on the relative importance of π bonding and bond polarity (electrostatic attraction) in causing the shortening of the S−O bond. The outcome was a broad consensus that d orbitals play a role, but are not as significant as Pauling had believed.\n\nA widely accepted description involving pπ – dπ bonding was initially proposed by D. W. J. Cruickshank. In this model, fully occupied p orbitals on oxygen overlap with empty sulfur d orbitals (principally the d and d). However, in this description, despite there being some π character to the S−O bonds, the bond has significant ionic character. For sulfuric acid, computational analysis (with natural bond orbitals) confirms a clear positive charge on sulfur (theoretically +2.45) and a low 3d occupancy. Therefore, the representation with four single bonds is the optimal Lewis structure rather than the one with two double bonds (thus the Lewis model, not the Pauling model). In this model, the structure obeys the octet rule and the charge distribution is in agreement with the electronegativity of the atoms. The discrepancy between the S−O bond length in the sulfate ion and the S−OH bond length in sulfuric acid is explained by donation of p-orbital electrons from the terminal S=O bonds in sulfuric acid into the antibonding S−OH orbitals, weakening them resulting in the longer bond length of the latter.\n\nHowever, the bonding representation of Pauling for sulfate and other main group compounds with oxygen is still a common way of representing the bonding in many textbooks. The apparent contradiction can be cleared if one realizes that the covalent double bonds in the Lewis structure in reality represent bonds that are strongly polarized by more than 90% towards the oxygen atom. On the other hand, in the structure with an dipolar bond, the charge is localized as a lone pair on the oxygen.\n\nMethods of preparing metal sulfates include:\n\nMany examples of ionic sulfates are known, and many of these are highly soluble in water. Exceptions include calcium sulfate, strontium sulfate, lead(II) sulfate, and barium sulfate, which are poorly soluble. Radium sulfate is the most insoluble sulfate known. The barium derivative is useful in the gravimetric analysis of sulfate: if one adds a solution of, perhaps, barium chloride to a solution containing sulfate ions, the appearance of a white precipitate, which is barium sulfate, indicates that sulfate anions are present.\n\nThe sulfate ion can act as a ligand attaching either by one oxygen (monodentate) or by two oxygens as either a chelate or a bridge. An example is the complex <nowiki>[</nowiki>Co(en)(SO)]Br or the neutral metal complex PtSO(P(CH)) where the sulfate ion is acting as a bidentate ligand. The metal–oxygen bonds in sulfate complexes can have significant covalent character.\n\nSulfates are widely used industrially. Major compounds include:\n\n\nSulfate-reducing bacteria, some anaerobic microorganisms, such as those living in sediment or near deep sea thermal vents, use the reduction of sulfates coupled with the oxidation of organic compounds or hydrogen as an energy source for chemosynthesis.\n\nSome sulfates were known to alchemists. The vitriol salts, from the Latin \"vitreolum\", glassy, were so-called because they were some of the first transparent crystals known. Green vitriol is iron(II) sulfate heptahydrate, FeSO·7HO; blue vitriol is copper(II) sulfate pentahydrate, CuSO·5HO and white vitriol is zinc sulfate heptahydrate, ZnSO·7HO. Alum, a double sulfate of potassium and aluminium with the formula KAl(SO)·24HO, figured in the development of the chemical industry.\n\nSulfates occur as microscopic particles (aerosols) resulting from fossil fuel and biomass combustion. They increase the acidity of the atmosphere and form acid rain. The anaerobic sulfate-reducing bacteria \"Desulfovibrio desulfuricans\" and \"D. vulgaris\" can remove the black sulfate crust that often tarnishes buildings.\n\nThe main direct effect of sulfates on the climate involves the scattering of light, effectively increasing the Earth's albedo. This effect is moderately well understood and leads to a cooling from the negative radiative forcing of about 0.4 W/m relative to pre-industrial values, partially offsetting the larger (about 2.4 W/m) warming effect of greenhouse gases. The effect is strongly spatially non-uniform, being largest downstream of large industrial areas.\n\nThe first indirect effect is also known as the Twomey effect. Sulfate aerosols can act as cloud condensation nuclei and this leads to greater numbers of smaller droplets of water. Lots of smaller droplets can diffuse light more efficiently than just a few larger droplets.\n\nThe second indirect effect is the further knock-on effects of having more cloud condensation nuclei. It is proposed that these include the suppression of drizzle, increased cloud height, to facilitate cloud formation at low humidities and longer cloud lifetime. Sulfate may also result in changes in the particle size distribution, which can affect the clouds radiative properties in ways that are not fully understood. Chemical effects such as the dissolution of soluble gases and slightly soluble substances, surface tension depression by organic substances and accommodation coefficient changes are also included in the second indirect effect.\n\nThe indirect effects probably have a cooling effect, perhaps up to 2 W/m, although the uncertainty is very large. Sulfates are therefore implicated in global dimming. Sulfate is also the major contributor to stratospheric aerosol formed by oxidation of sulfur dioxide injected into the stratosphere by impulsive volcanoes such as the 1991 eruption of Mount Pinatubo in the Philippines. This aerosol exerts a cooling effect on climate during its 1-2 year lifetime in the stratosphere.\n\nThe conjugate base of sulfuric acid (HSO)—a dense, colourless, oily, corrosive liquid—is the hydrogen sulfate ion (), also called the bisulfate ion. Sulfuric acid is classified as a strong acid; in aqueous solutions it ionizes completely to form hydronium ions (HO) and hydrogen sulfate (). In other words, the sulfuric acid behaves as a Brønsted–Lowry acid and is deprotonated. Bisulfate has a molar mass of 97.078 g/mol. It has a valency of 1. An example of a salt containing the group is sodium bisulfate, NaHSO. In dilute solutions the hydrogen sulfate ions also dissociate, forming more hydronium ions and sulfate ions (). The CAS registry number for hydrogen sulfate is 14996-02-2.\n\n"}
{"id": "32004278", "url": "https://en.wikipedia.org/wiki?curid=32004278", "title": "Surface modification of biomaterials with proteins", "text": "Surface modification of biomaterials with proteins\n\nBiomaterials are materials that are used in contact with biological systems. Biocompatibility and applicability of surface modification with current uses of metallic, polymeric and ceramic biomaterials allow alteration of properties to enhance performance in a biological environment while retaining bulk properties of the desired device.\n\nSurface modification involves the fundamentals of physicochemical interactions between the biomaterial and the physiological environment at the molecular, cellular and tissue levels (reduce bacterial adhesion, promote cell adhesion). Currently, there are various methods of characterization and surface modification of biomaterials and useful applications of fundamental concepts in a several biomedical solutions.\n\nThe function of surface modification is to change the physical and chemical properties of surfaces to improve the functionality of the original material. Protein surface modification of various types biomaterials (ceramics, polymers, metals, composites) is performed to ultimately increase biocompatibility of the material and interact as a bioactive material for specific applications. In various biomedical applications of developing implantable medical devices (such as pacemakers and stents), surface properties/interactions of proteins with a specific material must be evaluated with regards to biocompatibility as it plays a major role in determining a biological response. For instance, surface hydrophobicity or hydrophilicity of a material can be altered. Engineering biocompatibility between the physiological environment and the surface material allows new medical products, materials and surgical procedures with additional biofunctionality.\n\nSurface modification can be done through various methods, which can be classified through three main groups: physical (physical adsorption, Langmuir blodgett film), chemical (oxidation by strong acids, ozone treatment, chemisorption, and flame treatment) and radiation (glow discharge, corona discharge, photo activation (UV), laser, ion beam, plasma immersion ion implantation, electron beam lithography, and γ-irradiation).\n\nIn a biomedical perspective, biocompatibility is the ability of a material to perform with an appropriate host response in a specific application. It is described to be non-toxic, no induced adverse reactions such as chronic inflammatory response with unusual tissue formation, and designed to function properly for a reasonable lifetime. It is a requirement of biomaterials in which the surface modified material will cause no harm to the host, and the material itself will not harmed by the host. Although most synthetic biomaterials have the physical properties that meet or even exceed those of natural tissue, they often result in an unfavorable physiological reaction such as thrombosis formation, inflammation and infection.\n\nBiointegration is the ultimate goal in for example orthopedic implants that bones establish a mechanically solid interface with complete fusion between the artificial implanted material and bone tissues under good biocompatibility conditions. Modifying the surface of a material can improve its biocompatibility, and can be done without changing its bulk properties. The properties of the uppermost molecular layers are critical in biomaterials since the surface layers are in physicochemical contact with the biological environment.\n\nFurthermore, although some of the biomaterials have good biocompatibility, it may possess poor mechanical or physical properties such as wear resistance, anti-corrosion, or wettability or lubricity. In these cases, surface modification is utilized to deposit a layer of coating or mixing with substrate to form a composite layer.\n\nAs proteins are made up of different sequences of amino acids, proteins can have various functions as its structural shape driven by a number of molecular bonds can change. Amino acids exhibit different characteristics such as being polar, non-polar, positively or negatively charged which is determined by having different side chains. Thus, attachment of molecules with different protein for example, those containing Arginine-Glycine-Aspartate (RGD) sequences are expected to modify the surface of tissue scaffolds and result in improvement of cell adhesion when placed into its physiological environment. Additional modifications of the surface could be through attachment of functional groups of 2D or 3D patterns on the surface so that cell alignment is guided and new tissue formation is improved.\n\nSome of the surface modification techniques listed above are particularly used for certain functions or kinds of materials. One of the advantages of plasma immersion ion implantation is its ability to treat most materials. Ion implantation is an effective surface treatment technique that be used to enhance the surface properties of biomaterials. The unique advantage of plasma modification is that the surface properties and biocompatibility can be enhanced selectively while the favorable bulk attributes of the materials such as strength remain unchanged. Overall, it is an effective method to modify medical implants with complex shape. By altering the surface functionalities using plasma modification, the optimal surface, chemical and physical properties can be obtained.\n\nPlasma immersion implantation is a technique suitable for low melting point materials such as polymers, and widely accepted to improve adhesion between pinhole free layers and substrates. The ultimate goal is to enhance the properties of biomaterials such as biocompatibility, corrosion resistance and functionality with the fabrication of different types of biomedical thin films with various biologically important elements such as nitrogen, calcium, and sodium implanted with them. Different thin films such as titanium oxide, titanium nitride, and diamond-like carbon have been treated previously, and results show that the processed material exhibit better biocompatibility compared to the some current ones used in biomedical implants. In order to evaluate the biocompatibility of the fabricated thin films, various in vitro biological environment need to be conducted.\n\nThe immune system will react differently if an implant is coated in extra-cellular matrix proteins. The proteins surrounding the implant serve to \"hide\" the implant from the innate immune system. However, if the implant is coated in allergenic proteins, the patient's adaptive immune response may be initiated. To prevent such a negative immune reaction, immunosuppressive drugs may be prescribed, or autologous tissue may produce the protein coating.\n\nImmediately following insertion, an implant (and the tissue damage from surgery) will result in acute inflammation. The classic signs of acute inflammation are redness, swelling, heat, pain, and loss of function. Hemorrhaging from tissue damage results in clotting which stimulates latent mast cells. The mast cells release chemokines which activate blood vessel endothelium. The blood vessels dilate and become leaky, producing the redness and swelling associated with acute inflammation. The activated endothelium allows extravasation of blood plasma and white blood cells including macrophages which transmigrate to the implant and recognize it as non-biologic. Macrophages release oxidants to combat the foreign body. If antioxidants fail to destroy the foreign body, chronic inflammation begins.\n\nImplantation of non-degradable materials will eventually result in chronic inflammation and fibrous capsule formation. Macrophages that fail to destroy pathogens will merge to form a foreign-body giant cell which quarantines the implant. High levels of oxidants cause fibroblasts to secrete collagen, forming a layer of fibrous tissue around the implant.\n\nBy coating an implant with extracellular matrix proteins, macrophages will be unable to recognize the implant as non-biologic. The implant is then capable of continued interaction with the host, influencing the surrounding tissue toward various outcomes. For instance, the implant may improve healing by secreting angiogenic drugs.\n\nPhysical immobilization is simply coating a material with a biomimetic material without changing the structure of either. Various biomimetic materials with cell adhesive proteins (such as collagen or laminin) have been used in vitro to direct new tissue formation and cell growth. Cell adhesion and proliferation occurs much better on protein-coated surfaces. However, since the proteins are generally isolated, it is more likely to elicit an immune response. Generally, chemistry qualities should be taken into consideration.\n\nAlkali hydrolysis, covalent immobilization, and the wet chemical method are only three of the many ways to chemically modify a surface. The surface is prepped with surface activation, where several functionalities are placed on the polymer to react better with the proteins. In alkali hydrolysis, small protons diffuse between polymer chains and cause surface hydrolysis which cleaves ester bonds. This results in the formation of carboxyl and hydroxyl functionalities which can attach to proteins. In covalent immobilization, small fragments of proteins or short peptides are bonded to the surface. The peptides are highly stable and studies have shown that this method improves biocompatibility. The wet chemical method is one of the preferred methods of protein immobilization. Chemical species are dissolved in an organic solution where reactions take place to reduce the hydrophobic nature of the polymer. Surface stability is higher in chemical modification than in physical adsorption. It also offers higher biocompatibility towards cell growth and bodily fluid flow.\n\nSuccessful attempts at grafting biomolecules onto polymers have been made using photochemical modification of biomaterials. These techniques employ high energy photons (typically UV) to break chemical bonds and release free radicals. Protein adhesion can be encouraged by favorably altering the surface charge of a biomaterial. Improved protein adhesion leads to better integration between the host and the implant. Ma et al. compared cell adhesion for various surface groups and found that OH and CONH improved PLLA wettability more than COOH.\n\nApplying a mask to the surface of the biomaterial allows selective surface modification. Areas that UV light penetrate will be modified such that cells will adhere to the region more favorably.\n\nThe minimum feature size attainable is given by:\n\nwhere\n\nformula_2 is the minimum feature size\n\nformula_3 (commonly called \"k1 factor\") is a coefficient that encapsulates process-related factors, and typically equals 0.4 for production.\n\nformula_4 is the wavelength of light used\n\nformula_5 is the numerical aperture of the lens as seen from the wafer\n\nAccording to this equation, greater resolution can be obtained by decreasing the wavelength, and increasing the numerical aperture.\n\nGraft formation improves the overall hydrophilicity of the material through a ratio of how much glycolic acid and lactic acid is added. Block polymer, or PLGA, decreases hydrophobicity of the surface by controlling the amount of glycolic acid. However, this doesn't increase the hydrophilic tendency of the material.In brush grafting, hydrophilic polymers containing alcohol or hydroxyl groups are placed onto surfaces through photopolymerization.\n\nPlasma techniques are especially useful because they can deposit ultra thin (a few nm), adherent, conformal coatings. Glow discharge plasma is created by filling a vacuum with a low-pressure gas (ex. argon, ammonia, or oxygen). The gas is then excited using microwaves or current which ionizes it. The ionized gas is then thrown onto a surface at a high velocity where the energy produced physically and chemically changes the surface. After the changes occur, the ionized plasma gas is able to react with the surface to make it ready for protein adhesion. However, the surfaces may lose mechanical strength or other inherent properties because of the high amounts of energy.\n\nSeveral plasma-based technologies have been developed to contently immobilize proteins depending on the final application of the resulting biomaterial. This technique is a relatively fast approach to produce smart bioactive surfaces.\n\nExtra-cellular matrix (ECM) proteins greatly dictate the process of bone formation—the attachment and proliferation of osteogenitor cells, differentiation to osteoblasts, matrix formation, and mineralization. It is beneficial to design biomaterials for bone-contacting devices with bone matrix proteins to promote bone growth.It is also possible to covalently and directionally immobilize osteoinductive peptides in the surface of the ceramic materials such as hydroxyapatite/β-tricalcium phosphate to stimulate osteoblast differentiation and better bone regeneration \nRGD peptides have been shown to increase the attachment and migration of osteoblasts on titanium implants, polymeric materials, and glass. Other adhesive peptides that can be recognized by molecules in the cell membrane can also affect binding of bone-derived cells. Particularly, the heparin binding domain in fibronectin is actively involved in specific interaction with osteogenic cells. Modification with heparin binding domains have the potential to enhance the binding of osteoblasts without affecting the attachment of endothelial cells and fibroblasts.Additionally, growth factors such as those in the bone morphogenic protein family are important polypeptides to induce bone formation. These growth factors can be covalently bound to materials to enhance the osteointegration of implants.\n\nPeripheral nervous system damage is typically treated by an autograft of nerve tissue to bridge a severed gap. This treatment requires successful regeneration of neural tissue; axons must grow from the proximal stump without interference in order to make a connection with the distal stump. Neural guidance channels (NGC), have been designed as a conduit for growth of new axons and the differentiation and morphogenesis of these tissues is affect by interaction between neural cells and the surrounding ECM. Studies of laminin have shown the protein to be an important ECM protein in the attachment of neural cells. The penta-peptide YIGSR and IKVAV, which are important sequences in laminin, have been shown to increase attachment of neural cells with the ability to control the spatial organization of the cells.\n\nIt is important that cardiovascular devices such as stents or artificial vascular grafts be designed to mimic properties of the specific tissue region the device is serving to replace. In order to reduce thrombogenicity, surfaces can be coated with fibronectin and RGD containing peptides, which encourages attachment of endothelial cells. The peptides YIGSR and REDV have also been shown to enhance attachment and spreading of endothelial cells and ultimately reduce the thrombogenicity of the implant. \n"}
{"id": "12751881", "url": "https://en.wikipedia.org/wiki?curid=12751881", "title": "TVEL", "text": "TVEL\n\nThe TVEL Fuel Company (TVEL) is a Russian nuclear fuel cycle company headquartered in Moscow. It works mainly in uranium mining and the production of nuclear fuel. TVEL belongs to the Atomenergoprom holding company (part of Rosatom).\n\nTVEL supplies fuel to the Czech Republic, Slovakia, Bulgaria, Hungary, Ukraine, Armenia, Lithuania, Finland, China and India. In the world, 73 power reactors (17% of the world market by number) and 30 research reactors are currently running with TVEL made fuel.\n\nTVEL is developing the TVS-K fuel assembly for Western-designed reactors. As of 2017 TVS-K is in pilot usage, and TVEL hopes it will be ready for commercial supply in 2021.\n\nThe chairman of the board of directors is Alexander Lokshin. The acting president is Yuri Olenin.\n\n\n\n\n"}
{"id": "20687857", "url": "https://en.wikipedia.org/wiki?curid=20687857", "title": "Te Waka Wind Farm", "text": "Te Waka Wind Farm\n\nThe Titiokura/Te Waka Wind Farm was a wind farm project abandoned by Unison Networks and Roaring 40s. It was to be located adjacent to SH5 in the Te Pohue - Titiokura area, 35 km west of Napier, New Zealand.\n\nStage 1 of the project, Titiokura, gained resource consents in 2006 after an Environment Court appeal. This stage is for 15 wind turbines, producing up to 45 MW.\n\nStage 2, Te Waka, was to have a capacity of up to 102 MW from 34 turbines. The Environment Court declined this application, after the hearing was held in December 2008. A revised proposal was prepared, this was declined by the Environment Court in February 2009.\n\nEarly stages of the consent process were administered by the Hastings District Council and the Hawke's Bay Regional Council. Both stages of the wind farm are opposed by local iwi, Ngati Hineuru and Maungaharuru-Tangitu Society.\n\n"}
{"id": "50899153", "url": "https://en.wikipedia.org/wiki?curid=50899153", "title": "Tornadoes of 1968", "text": "Tornadoes of 1968\n\nThis page documents the tornadoes and tornado outbreaks of 1968, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes. 2 F5 tornadoes struck Iowa in the Charles City and Maynard areas, combined they claimed 18 lives and this was one of very few cases in history where two F5s or EF5 tornadoes hit the same state, on the same day.\n"}
{"id": "2022396", "url": "https://en.wikipedia.org/wiki?curid=2022396", "title": "Xanthate", "text": "Xanthate\n\nXanthate usually refers to a salt with the formula (R = alkyl; M = Na, K), thus O-esters of dithiocarbonate. The name \"xanthates\" is derived from Greek \"xanthos\", meaning “yellowish, golden”, and indeed most xanthate salts are yellow. They were discovered and named in 1823 by Danish chemist William Christopher Zeise. These organosulfur compounds are important in two areas: the production of cellophane and related polymers from cellulose and (in mining) for extraction of certain ores. They are also versatile intermediates in organic synthesis. Xanthates also refer to esters of xanthic acid. These esters have the structure ROC(=S)SR′.\n\nXanthate salts are produced by the reaction of an alkoxide salt with carbon disulfide. \nThe reaction involves the attack of the alkoxide nucleophile on the electrophile CS. Often the alkoxide is generated in situ by treating the alcohol with sodium or potassium hydroxide:\nFor example, sodium ethoxide gives sodium ethyl xanthate. Many alcohols can be used in this reaction. Technical grade xanthate salts are usually of 90–95% purity. Impurities include alkali-metal sulfides, sulfates, trithiocarbonates, thiosulfates, sulfites, or carbonates as well as residual raw material such as alcohol and alkali hydroxide. These salts are available commercially as powder, granules, flakes, sticks, and solutions are available. \n\nSome commercially important xanthate salts include:\n\nThe OCS core of xanthate salts, like that of the all-oxygen carbonates and other simple esters is characteristically planar. The central carbon is sp-hybridized.\n\nXanthate salts characteristically decompose in acid:\nThis reaction is the reverse of the method for the preparation of the xanthate salts. The intermediate in the decomposition is the xanthic acid, ROC(S)SH, which can be isolated in certain cases.\n\nXanthate anions also undergo alkylation to give xanthate esters, which are generally stable:\nThe C-O bond in these compounds are susceptible to cleavage by the Barton–McCombie deoxygenation, which provides a means for deoxygenation of alcohols.\n\nThey can be oxidized to the so-called dixanthogens:\n\nXanthates bind to transition metal cations as bidentate ligands. The charge-neutral complexes are soluble in organic solvents.\n\nCellulose reacts with carbon disulfide (CS) in presence of sodium hydroxide (NaOH) to produces sodium cellulose xanthate, which upon neutralization with sulfuric acid (HSO) gives viscose rayon or cellophane paper (Sellotape or Scotch Tape).\n\nCertain xanthate salts and bisxanthates (e.g. Dixanthogen) are used as flotation agents in mineral processing. They are intermediates in the Chugaev elimination process and are used to control radical polymerisation under the RAFT process, also termed MADIX (macromolecular design via interchange of xanthates).\n\nRarely encountered, thioxanthates arise by the reaction of CS with thiolate salts. For example, sodium ethylthioxanthate has the formula CHSCSNa. Dithiocarbamates are also related compounds. They arise from the reaction of a secondary amine with CS. For example, sodium diethyldithiocarbamate has the formula (CH)NCSNa.\n\nXanthates may be toxic to aquatic life at concentrations of less than 1 mg/L. Water downstream of mining operations is often contaminated.\n"}
{"id": "25520575", "url": "https://en.wikipedia.org/wiki?curid=25520575", "title": "Xylomannan", "text": "Xylomannan\n\nXylomannan is an antifreeze molecule, found in the freeze-tolerant Alaskan beetle \"Upis ceramboides\". Unlike antifreeze proteins, xylomannan is not a protein. Instead, it is a combination of a sugar (saccharide) and a fatty acid that is found in cell membranes. As such is expected to work in a different manner than AFPs. It is believed to work by incorporating itself directly into the cell membrane and preventing the freezing of water molecules within the cell.\n\nXylomannan is also found in the red seaweed \"Nothogenia fastigiata\". Fraction F6 of a sulphated xylomannan from Nothogenia fastigiata was found to inhibit replication of a variety of viruses, including Herpes simplex virus types 1 and 2 (HSV-1, HSV-2), Human cytomegalovirus (HCMV, HHV-5), Respiratory syncytial virus (RSV), Influenzavirus A, Influenzavirus B, Junin and Tacaribe virus, Simian immunodeficiency virus, and (weakly) Human immunodeficiency virus types 1 and 2.\n"}
