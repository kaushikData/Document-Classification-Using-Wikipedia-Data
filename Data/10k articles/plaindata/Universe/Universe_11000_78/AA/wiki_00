{"id": "964312", "url": "https://en.wikipedia.org/wiki?curid=964312", "title": "12AX7", "text": "12AX7\n\n12AX7 (also known as ECC83) is a vacuum tube that is a miniature dual triode - 6AV6 with high voltage gain. It was developed around 1946 by RCA engineers in Camden, New Jersey, under developmental number A-4522. It was released for public sale under the 12AX7 identifier on September 15, 1947. The 12AX7 was originally intended as replacement for the 6SL7 family of dual-triode amplifier tubes for audio applications. It is popular with tube amplifier enthusiasts, and its ongoing use in such equipment makes it one of the few small-signal vacuum tubes in continuous production since it was introduced.\n\nThe 12AX7 is basically two of the triodes from a 6AV6 - a double diode triode. The 6AV6 is a miniature repackaging (with just a single cathode) of the triode and twin diodes from the octal 6SQ7 (a double-diode triode used in AM radios), which itself is very similar to the older type 75 triode-diode dating from 1930.\n\nThe 12AX7 is a high-gain (typical amplification factor 100), low-plate-current triode best suited for low-level audio voltage amplification. In this role it is widely used for the preamplifier (input and mid-level) stages of audio amplifiers. It has relatively high Miller capacitance, making it unsuitable for radio-frequency use.\n\nTypically a 12AX7 triode is configured with a high-value plate resistor, 100k ohms in most guitar amps and 220k ohms or more in high-fidelity equipment. Grid bias is most often provided by a cathode resistor. If the cathode resistor is unbypassed, negative feedback is introduced and each half of a 12AX7 provides a typical voltage gain of about 30; the amplification factor is basically twice the maximum stage gain, as the plate impedance must be matched. Thus half the voltage is across the tube at rest, half across the load resistor. The cathode resistor can be bypassed to reduce or eliminate AC negative feedback and thereby increase gain; maximum gain is about 60 times.\n\nformula_1 = µ * Rtot / ( rP + Rtot + ( RK * ( µ + 1 )))\n\nWhere formula_1 = voltage gain, µ is the amplification factor of the valve, rP is the internal plate resistance,\nRK is the cathode resistor and Rtot is the parallel combination of RP (external plate resistor) and Rload.\nIf the cathode resistor is bypassed, use RK = 0.\n\nThe initial \"12\" in the designator implies a 12-volt heater requirement; however, the tube has a center-tapped heater so it can be used in either 6.3V or 12.6V heater circuits.\n\nThe 12AX7 is the most common member of what eventually became a large family of twin-triode vacuum tubes, manufactured all over the world, all sharing the same pinout (EIA 9A). Most use heaters which can be optionally wired in series (12.6V, 150 mA) or parallel (6.3V, 300 mA). Other tubes, which in some cases can be used interchangeably in an emergency, include the 12AT7, 12AU7, 12AV7, 12AY7, and the low-voltage 12U7, plus many 4-digit EIA series dual triodes. They span a wide range of voltage gain and transconductance. Different versions of each were designed for enhanced ruggedness, low microphonics, stability, lifespan, etc.\n\nThose other designs offer lower voltage gain (traded off for higher plate current) than the 12AX7 (which has a voltage gain or formula_1 of 100), and are more suitable for high-frequency applications.\n\nSome American designs similar to the 12AX7:\n\nAlthough commonly known in Europe by its Mullard–Philips tube designation of ECC83, other European variations also exist including the low-noise versions 12AX7A, 12AD7, 6681, 7025, and 7729; European versions B339, B759, CV492, CV4004, CV8156, CV8222, ECC803, ECC803S, E2164, and M8137; and the lower-gain low-noise versions 5751 and 6851, intended for avionics equipment.\n\nIn European usage special-quality valves of some sort were often indicated by exchanging letters and digits in the name: the E83CC was a special-quality ECC83.\nIn the US a \"W\" in the designation, as in 12AX7WA, designates the tube as complying with military grade, higher reliability specifications.\n\nThe 'E' in the European designation classifies this as having a 6.3 volt heater, whereas the American designation of 12AX7 classifies it as having a 12.6 volt heater. It can, of course, be wired for operation off either voltage.\n\n\n"}
{"id": "153316", "url": "https://en.wikipedia.org/wiki?curid=153316", "title": "Alpha-Linolenic acid", "text": "Alpha-Linolenic acid\n\nα-Linolenic acid (ALA) is an \"n\"−3 fatty acid. It is one of two essential fatty acids (the other being linoleic acid), so called because they are necessary for health and cannot be produced within the human body. They must be acquired through diet. ALA is an omega-3 fatty acid found in seeds (chia, flaxseed, hemp, see also table below), nuts (notably walnuts), and many common vegetable oils. In terms of its structure, it is named \"all\"-\"cis\"-9,12,15-octadecatrienoic acid. In physiological literature, it is listed by its lipid number, 18:3, and (\"n\"−3); its isomer GLA is 18:3 (\"n\"−6).\n\nα-Linolenic acid is a carboxylic acid with an 18-carbon chain and three \"cis\" double bonds. The first double bond is located at the third carbon from the methyl end of the fatty acid chain, known as the \"n\" end. Thus, α-linolenic acid is a polyunsaturated \"n\"−3 (omega-3) fatty acid. It is an isomer of gamma-linolenic acid (GLA), a polyunsaturated \"n\"−6 (omega-6) fatty acid.\n\nα-Linolenic acid was first isolated by Rollett as cited in J. W. McCutcheon's synthesis in 1942, and referred to in Green and Hilditch's 1930s survey. It was first artificially synthesized in 1995 from C6 homologating agents. A Wittig reaction of the phosphonium salt of [(\"Z-Z\")-nona-3,6-dien-1-yl]triphenylphosphonium bromide with methyl 9-oxononanoate, followed by saponification, completed the synthesis.\n\nSeed oils are the richest sources of α-linolenic acid, notably those of hempseed, chia, perilla, flaxseed (linseed oil), rapeseed (canola), and soybeans. α-Linolenic acid is also obtained from the thylakoid membranes in the leaves of \"Pisum sativum\" (pea leaves). Plant chloroplasts consisting of more than 95 percent of photosynthetic thylakoid membranes are highly fluid due to the large abundance of linolenic acid, that shows up as sharp resonances in high-resolution carbon-13 NMR spectra, invariably. Some studies state that ALA remains stable during processing and cooking. However, other studies state that ALA might not be suitable for baking, as it will polymerize with itself, a feature exploited in paint with transition metal catalysts. Some ALA may also oxidize at baking temperatures. ALA percentages in the table below refer to the oils extracted from each item.\n\nAlthough the best source of ALA is seeds, most seeds and seed oils are much richer in an \"n\"−6 fatty acid, linoleic acid. Exceptions include flaxseed (must be ground for proper nutrient absorption) and chia seeds. Linoleic acid is the other essential fatty acid, but it, and the other \"n\"−6 fatty acids, compete with \"n\"−3s for positions in cell membranes and have very different effects on human health. There is a complex set of essential fatty acid interactions.\n\nα-Linolenic acid can only be obtained by humans through their diets because the absence of the required 12- and 15-desaturase enzymes makes \"de novo\" synthesis from stearic acid impossible. Eicosapentaenoic acid (EPA; 20:5, \"n\"−3) and docosahexaenoic acid (DHA; 22:6, \"n\"−3) are readily available from fish and algae oil and play a vital role in many metabolic processes. These can also be synthesized by humans from dietary α-linolenic acid, but with an efficiency of only a few percent. \nBecause the efficacy of \"n\"−3 long-chain polyunsaturated fatty acid (LC-PUFA) synthesis decreases down the cascade of α-linolenic acid conversion, DHA synthesis from α-linolenic acid is even more restricted than that of EPA. Conversion of ALA to DHA is higher in women than in men.\n\nMultiple studies have shown a relationship between α-linolenic acid and an increased risk of prostate cancer. This risk was found to be irrespective of source of origin (e.g., meat, vegetable oil). However, a large 2006 study found no association between total α-linolenic acid intake and overall risk of prostate cancer; and a 2009 meta-analysis found evidence of publication bias in earlier studies, and concluded that if ALA contributes to increased prostate cancer risk, the increase in risk is quite small.\n\nα-Linolenic acid is relatively more susceptible to oxidation and will become rancid more quickly than many other oils. Oxidative instability of α-linolenic acid is one reason why producers choose to partially hydrogenate oils containing α-linolenic acid, such as soybean oil. Soybeans are the largest source of edible oils in the U.S., and 40% of soy oil production is partially hydrogenated.\n\nHowever, when partially hydrogenated, part of the unsaturated fatty acids become unhealthy trans fats. Consumers are increasingly avoiding products that contain trans fats, and governments have begun to ban trans fats in food products. These regulations and market pressures have spurred the development of low-α-linolenic acid soybeans. These new soybean varieties yield a more stable oil that doesn't require hydrogenation for many applications, thus providing trans fat-free products, such as frying oil.\n\nSeveral consortia are bringing low-α-linolenic acid soy to market. DuPont's effort involves silencing the FAD2 gene that codes for Δ-desaturase, giving a soy oil with very low levels of both α-linolenic acid and linoleic acid. Monsanto Company has introduced to the market Vistive, their brand of low α-linolenic acid soybeans, which is less controversial than GMO offerings, as it was created via conventional breeding techniques.\n\nThere is some evidence ALA consumption might have a slight preventative effect against cardiovascular diseases.\n\n"}
{"id": "3758840", "url": "https://en.wikipedia.org/wiki?curid=3758840", "title": "Arid Lands Ecology Reserve", "text": "Arid Lands Ecology Reserve\n\nThe Arid Land Ecology Reserve is the largest tract of shrub-steppe ecosystem remaining in the U.S. state of Washington. It is managed for the U.S. Department of Energy by the Pacific Northwest National Laboratory (which is operated for the U.S. Department of Energy by Battelle Memorial Institute). The 320 km² area is a portion of the 1500 km² National Environmental Research Park located on the Hanford Site on the northwest boundary of Richland, Washington.\n\nOn June 27, 2000, a range fire destroyed most of the native sagebrush and bunchgrass. Though the US Fish and Wildlife Service has attempted to re-introduce native flora, the Arid Lands Ecology Reserve is currently dominated by non-native species such as cheatgrass, knapweeds, and Russian thistle (tumbleweed) which flourished after the 2000 fire. Other species such as spiny hop sage and Wyoming big sagebrush were decimated by the fire and in its aftermath.\n\n"}
{"id": "1930923", "url": "https://en.wikipedia.org/wiki?curid=1930923", "title": "Arsenide", "text": "Arsenide\n\nIn chemistry, an arsenide is a compound of arsenic with a less electronegative element or elements. Many metals form binary compounds containing arsenic, and these are called arsenides. They exist with many stoichiometries, and in this respect arsenides are similar to phosphides.\n\nThe group 1 alkali metals and the group 2, alkaline earth metals, form arsenides with isolated arsenic atoms. They form upon heating arsenic powder with excess sodium gives sodium arsenide (NaAs). The structure of NaAs is complex with unusually short Na–Na distances of 328–330 pm which are shorter than in sodium metal. This short distance indicates the complex bonding in these simple phases, i.e. they are not simply salts of As anion, for example. The compound LiAs, has a metallic lustre and electrical conductivity indicating some metallic bonding. These compounds are mainly of academic interest. For example, \"sodium arsenide\" is a structural motif adopted by many compounds with the AB stoichiometry.\n\nIndicative of their salt-like properties, hydrolysis of alkali metal arsenides gives arsine:\n\nMany arsenides of the group 13 elements (group III) are valuable semiconductors. Gallium arsenide (GaAs) features isolated arsenic centers with a zincblende structure (wurtzite structure can eventually also form in nanostructures), and with predominantly covalent bonding – it is a III–V semiconductor.\n\nArsenic anionics are known to catenate, that is, form chains, rings, and cages. The mineral skutterudite (CoAs) features rings that are usually described as . Assigning formal oxidation numbers is difficult because these materials are highly covalent and often are best described with band theory. Sperrylite (PtAs) is usually described as . The arsenides of the transition metals are mainly of interest because they contaminate sulfidic ores of commercial interest. The extraction of the metals – nickel, iron, cobalt, copper – entails chemical processes such as smelting that poses environmental risks. In the mineral, arsenic is immobile and poses no environmental risk. Released from the mineral, arsenic is poisonous and mobile.\n\nPartial reduction of arsenic with alkali metals (and related electropositive elements) affords polyarsenic compounds, which are members of the Zintl phases.\n\n"}
{"id": "12312944", "url": "https://en.wikipedia.org/wiki?curid=12312944", "title": "Azlon", "text": "Azlon\n\nAzlon is a synthetic textile fiber composed of protein material derived from natural sources such as soy, peanut, milk or corn. Currently it is used in clothing.\n\nUnder the \"Textile Labelling and Advertising Regulations\", Section 26(f), Azlon is defined as any fiber made from regenerated protein.\n\nThe name \"Azlon\" is regulated by the Federal Trade Commission, § 303.7(g) \"Rules and Regulations Under the Textile Fiber Products Identification Act\". However, there is currently no domestic production.\n\nAzlon is the common generic name for all man-made protein fibers. Aralac was a registered trademark of Aralac, Inc., a division of National Dairy Products Corporation. Its production from unrationed skimmed-milk supplies may have contributed to its popularization during the Second World War.\n\nAzlon is also a brand of plastic labware. It is a registered trade mark of SciLabware Limited.\n\n\n"}
{"id": "6073742", "url": "https://en.wikipedia.org/wiki?curid=6073742", "title": "Butterfly joint", "text": "Butterfly joint\n\nA Dovetail key, Dutchman Joint, or Butterfly joint is a type of joint used either to hold two or more wooden boards together or to keep two halves of a board that have already started to split from splitting further. They may also be used to stabilize the core of a knothole, preventing it from dropping out over time. They are also commonly used as decorative inlays for non structural aesthetic purposes.\n\nA Dovetail Key resembles two dovetails connected at the narrow part. A negative of the hole is cut out of the board the butterfly will be placed in and the butterfly is then fitted, keeping the joint together. The wood used for the butterfly is usually a contrasting wood, often walnut.\n\nDovetail keys or Butterfly joints have been used both in decorative and structural joints since ancient times. They were prominently used in construction of the Cairo Dahshur Boats, a type of Khufu ship from the Egyptian middle kingdom. They were also historically used in repairing cracks in Dutch tabletops in the 18th century. This is where the term Dutchman joint originates. The dovetail key was installed across the crack to stabilize and inhibit further movement of the crack.\n\nContemporary decorative dovetail keys are commonly seen in and associated with the work of George Nakashima.\n\n"}
{"id": "5299", "url": "https://en.wikipedia.org/wiki?curid=5299", "title": "Carbon", "text": "Carbon\n\nCarbon (from \"coal\") is a chemical element with symbol C and atomic number 6. It is nonmetallic and tetravalent—making four electrons available to form covalent chemical bonds. It belongs to group 14 of the periodic table. Three isotopes occur naturally, C and C being stable, while C is a radionuclide, decaying with a half-life of about 5,730 years. Carbon is one of the few elements known since antiquity.\n\nCarbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.\n\nThe atoms of carbon can bond together in different ways, termed allotropes of carbon. The best known are graphite, diamond, and amorphous carbon. The physical properties of carbon vary widely with the allotropic form. For example, graphite is opaque and black while diamond is highly transparent. Graphite is soft enough to form a streak on paper (hence its name, from the Greek verb \"γράφειν\" which means \"to write\"), while diamond is the hardest naturally occurring material known. Graphite is a good electrical conductor while diamond has a low electrical conductivity. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials. All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form at standard temperature and pressure. They are chemically resistant and require high temperature to react even with oxygen.\n\nThe most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil, and methane clathrates. Carbon forms a vast number of compounds, more than any other element, with almost ten million compounds described to date, and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions. For this reason, carbon has often been referred to as the \"king of the elements\".\n\nThe allotropes of carbon include graphite, one of the softest known substances, and diamond, the hardest naturally occurring substance. It bonds readily with other small atoms, including other carbon atoms, and is capable of forming multiple stable covalent bonds with suitable multivalent atoms. Carbon is known to form almost ten million different compounds, a large majority of all chemical compounds. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point, as its triple point is at and , so it sublimes at about 3,900 K. Graphite is much more reactive than diamond at standard conditions, despite being more thermodynamically stable, as its delocalised pi system is much more vulnerable to attack. For example, graphite can be oxidised by hot concentrated nitric acid at standard conditions to mellitic acid, C(COH), which preserves the hexagonal units of graphite while breaking up the larger structure.\n\nCarbon sublimes in a carbon arc, which has a temperature of about 5800 K (5,530 °C or 9,980 °F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest-melting-point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper, which are weaker reducing agents at room temperature.\n\nCarbon is the sixth element, with a ground-state electron configuration of 1s2s2p, of which the four outer electrons are valence electrons. Its first four ionisation energies, 1086.5, 2352.6, 4620.5 and 6222.7 kJ/mol, are much higher than those of the heavier group-14 elements. The electronegativity of carbon is 2.5, significantly higher than the heavier group-14 elements (1.8–1.9), but close to most of the nearby nonmetals, as well as some of the second- and third-row transition metals. Carbon's covalent radii are normally taken as 77.2 pm (C−C), 66.7 pm (C=C) and 60.3 pm (C≡C), although these may vary depending on coordination number and what the carbon is bonded to. In general, covalent radius decreases with lower coordination number and higher bond order.\n\nCarbon compounds form the basis of all known life on Earth, and the carbon–nitrogen cycle provides some of the energy produced by the Sun and other stars. Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures, carbon reacts with oxygen to form carbon oxides and will rob oxygen from metal oxides to leave the elemental metal. This exothermic reaction is used in the iron and steel industry to smelt iron and to control the carbon content of steel:\n\nCarbon monoxide can be recycled to smelt even more iron:\n\nwith sulfur to form carbon disulfide and with steam in the coal-gas reaction:\nCarbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.\n\nThe system of carbon allotropes spans a range of extremes:\n\nAtomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with different molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite, glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).\n\nGraphene is a two-dimensional sheet of carbon with the atoms arranged in a hexagonal lattice. As of 2009, graphene appears to be the strongest material ever tested. The process of separating it from graphite will require some further technological development before it is economical for industrial processes. If successful, graphene could be used in the construction of a space elevator. It could also be used to safely store hydrogen for use in a hydrogen based engine in cars.\nThe amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot) and activated carbon. At normal pressures, carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a π-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.\nAt very high pressures, carbon forms the more compact allotrope, diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, forming a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium, and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance measured by resistance to scratching. Contrary to the popular belief that \"diamonds are forever\", they are thermodynamically unstable (Δ\"G\"°(diamond, 298 K) = 2.9 kJ/mol) under normal conditions (298 K, 10 Pa) and transform into graphite. Due to a high activation energy barrier, the transition into graphite is so slow at normal temperature that it is unnoticeable. The bottom left corner of the phase diagram for carbon has not been scrutinized experimentally. However, a recent computational study employing density functional theory methods reached the conclusion that as and , diamond becomes \"more stable\" than graphite by approximately 1.1 kJ/mol. Under some conditions, carbon crystallizes as lonsdaleite, a hexagonal crystal lattice with all atoms covalently bonded and properties similar to those of diamond.\n\nFullerenes are a synthetic crystalline formation with a graphite-like structure, but in place of hexagons, fullerenes are formed of pentagons (or even heptagons) of carbon atoms. The missing (or additional) atoms warp the sheets into spheres, ellipses, or cylinders. The properties of fullerenes (split into buckyballs, buckytubes, and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names \"fullerene\" and \"buckyball\" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C buckminsterfullerene). Carbon nanotubes are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid bucky tube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.\n\nOf the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2 kg/m. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure −(C:::C)−. Carbon in this modification is linear with \"sp\" orbital hybridization, and is a polymer with alternating single and triple bonds. This carbyne is of considerable interest to nanotechnology as its Young's modulus is 40 times that of the hardest known material – diamond.\n\nIn 2015, a team at the North Carolina State University announced the development of another allotrope they have dubbed Q-carbon, created by a high energy low duration laser pulse on amorphous carbon dust. Q-carbon is reported to exhibit ferromagetism, fluorescence, and a hardness superior to diamonds.\n\nCarbon is the fourth most abundant chemical element in the observable universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the solar system was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.\n\nIn 2014 NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed \"a couple of billion years\" after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\nIt has been estimated that the solid earth as a whole contains 730 ppm of carbon, with 2000 ppm in the core and 120 ppm in the combined mantle and crust. Since the mass of the earth is , this would imply 4360 million gigatonnes of carbon. This is much more than the amount of carbon in the oceans or atmosphere (below).\n\nIn combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 810 gigatonnes of carbon) and dissolved in all water bodies (approximately 36,000 gigatonnes of carbon). Around 1,900 gigatonnes of carbon are present in the biosphere. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal \"reserves\" (not \"resources\") amount to around 900 gigatonnes with perhaps 18,000 Gt of resources. Oil reserves are around 150 gigatonnes. Proven sources of natural gas are about (containing about 105 gigatonnes of carbon), but studies estimate another of \"unconventional\" deposits such as shale gas, representing about 540 gigatonnes of carbon.\n\nCarbon is also found in methane hydrates in polar regions and under the seas. Various estimates put this carbon between 500, 2500 Gt, or 3,000 Gt.\n\nIn the past, quantities of hydrocarbons were greater. According to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. Another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2,000 Gt.\n\nCarbon is a constituent (about 12% by mass) of the very large masses of carbonate rock (limestone, dolomite, marble and so on). Coal is very rich in carbon (anthracite contains 92–98%) and is the largest commercial source of mineral carbon, accounting for 4,000 gigatonnes or 80% of fossil fuel.\n\nAs for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic \"necks\", or \"pipes\". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. Diamond deposits have also been found in Arkansas, Canada, the Russian Arctic, Brazil, and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. Diamonds are found naturally, but about 30% of all industrial diamonds used in the U.S. are now manufactured.\n\nCarbon-14 is formed in upper layers of the troposphere and the stratosphere at altitudes of 9–15 km by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton. As such, of atmospheric carbon dioxide contains carbon-14.\n\nCarbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in our solar system. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.\n\nIsotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (C) forms 98.93% of the carbon on Earth, while carbon-13 (C) forms the remaining 1.07%. The concentration of C is further increased in biological materials because biochemical reactions discriminate against C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in nuclear magnetic resonance (NMR) experiments is done with the isotope C.\n\nCarbon-14 (C) is a naturally occurring radioisotope, created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. It is found in trace amounts on Earth of 1 part per trillion (0.0000000001%) or more, mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158 MeV β emission. Because of its relatively short half-life of 5730 years, C is virtually absent in ancient rocks. The amount of C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000 years.\n\nThere are 15 known isotopes of carbon and the shortest-lived of these is C which decays through proton emission and alpha decay and has a half-life of 1.98739x10 s. The exotic C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.\n\nFormation of the carbon atomic nucleus occurs within a giant or supergiant star through the triple-alpha process. This requires a nearly simultaneous collision of three alpha particles (helium nuclei), as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. The triple-alpha process happens in conditions of temperatures over 100 megakelvin and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang.\n\nAccording to current physical cosmology theory, carbon is formed in the interiors of stars on the horizontal branch. When massive stars die as supernova, the carbon is scattered into space as dust. This dust becomes component material for the formation of the next-generation star systems with accreted planets. The Solar System is one such star system with an abundance of carbon, enabling the existence of life as we know it.\n\nThe CNO cycle is an additional hydrogen fusion mechanism that powers stars, wherein carbon operates as a catalyst.\n\nRotational transitions of various isotopic forms of carbon monoxide (for example, CO, CO, and CO) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.\n\nUnder terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it from somewhere and dispose of it somewhere else. The paths of carbon in the environment form the carbon cycle. For example, photosynthetic plants draw carbon dioxide from the atmosphere (or seawater) and build it into biomass, as in the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, while some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; if bacteria do not consume it, dead plant or animal matter may become petroleum or coal, which releases carbon when burned.\n\nCarbon can form very long chains of interconnecting carbon–carbon bonds, a property that is called catenation. Carbon-carbon bonds are strong and stable. Through catenation, carbon forms a countless number of compounds. A tally of unique compounds shows that more contain carbon that those that do not. A similar claim can be made for hydrogen because most organic compounds also contain hydrogen.\n\nThe simplest form of an organic molecule is the hydrocarbon—a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other atoms, known as heteroatoms. Common heteroatoms that appear in organic compounds include oxygen, nitrogen, sulfur, phosphorus, and the nonradioactive halogens, as well as the metals lithium and magnesium. Organic compounds containing bonds to metal are known as organometallic compounds (\"see below\"). Certain groupings of atoms, often including heteroatoms, recur in large numbers of organic compounds. These collections, known as \"functional groups\", confer common reactivity patterns and allow for the systematic study and categorization of organic compounds. Chain length, shape and functional groups all affect the properties of organic molecules.\n\nIn most stable compounds of carbon (and nearly all stable \"organic\" compounds), carbon obeys the octet rule and is \"tetravalent\", meaning that a carbon atom forms a total of four covalent bonds (which may include double and triple bonds). Exceptions include a small number of stabilized \"carbocations\" (three bonds, positive charge), \"radicals\" (three bonds, neutral), \"carbanions\" (three bonds, negative charge) and \"carbenes\" (two bonds, neutral), although these species are much more likely to be encountered as unstable, reactive intermediates. \n\nCarbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons that are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals, and as fossil fuels.\n\nWhen combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, and aromatic esters, carotenoids and terpenes. With nitrogen it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells.\n\nCommonly carbon-containing compounds which are associated with minerals or which do not contain bonds to the other carbon atoms, halogens, or hydrogen, are treated separately from classical organic compounds; the definition is not rigid, and the classification of some compounds can vary from author to author (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide () is similar. Nevertheless, due to its physical properties and its association with organic synthesis, carbon disulfide is sometimes classified as an \"organic\" solvent.\n\nThe other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example, it can form the nitride cyanogen molecule ((CN)), similar to diatomic halides. Likewise, the heavier analog of cyanide, cyaphide (CP), is also considered inorganic, though most simple derivatives are highly unstable. Other uncommon oxides are carbon suboxide (), the unstable dicarbon monoxide (CO), carbon trioxide (CO), cyclopentanepentone (CO), cyclohexanehexone (CO), and mellitic anhydride (CO). However, mellitic anhydride is the triple acyl anhydride of mellitic acid; moreover, it contains a benzene ring. Thus, many chemists consider it to be organic.\n\nWith reactive metals, such as tungsten, carbon forms either carbides (C) or acetylides () to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond. Nevertheless, even the most polar and salt-like of carbides are not completely ionic compounds.\n\nOrganometallic compounds by definition contain at least one carbon-metal covalent bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), η-alkene compounds (for example, Zeise's salt), and η-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls and metal cyanides exist (for example, tetracarbonylnickel and potassium ferricyanide); some workers consider metal carbonyl and cyanide complexes without other carbon ligands to be purely inorganic, and not organometallic. However, most organometallic chemists consider metal complexes with any carbon ligand, even 'inorganic carbon' (e.g., carbonyls, cyanides, and certain types of carbides and acetylides) to be organometallic in nature. Metal complexes containing organic ligands without a carbon-metal covalent bond (e.g., metal carboxylates) are termed \"metalorganic\" compounds.\n\nWhile carbon is understood to strongly prefer formation of four covalent bonds, other exotic bonding schemes are also known. An interesting compound containing an octahedral hexacoordinated carbon atom has been reported. The cation of the compound is [(PhPAu)C]. This phenomenon has been attributed to the aurophilicity of the gold ligands, which provide additional stabilization of an otherwise labile species. In nature, the iron-molybdenum cofactor (FeMoco) responsible for microbial nitrogen fixation likewise has an octahedral carbon center (formally a carbide, C(-IV)) bonded to six iron atoms. In 2016, it was confirmed that, in line with earlier theoretical predictions, hexamethylbenzene dication contains a carbon atom with six bonds, with the formulation [MeC(η-CMe)], making it an \"organic metallocene\". Thus, a MeC fragment is bonded to a η-CMe fragment through all five of the carbons of the ring.\n\nIt is important to note that in the cases above, each of the bonds to carbon contain less than two formal electron pairs, making them hypercoordinate, but not hypervalent. Even in cases of alleged 10-C-5 species (that is, a carbon with five ligands and a formal electron count of ten), as reported by Akiba and co-workers, electronic structure calculations conclude that the total number of electrons around carbon is still less than eight, as in the case of other compounds described by three-center bonding.\n\nThe English name \"carbon\" comes from the Latin \"carbo\" for coal and charcoal, whence also comes the French \"charbon\", meaning charcoal. In German, Dutch and Danish, the names for carbon are \"Kohlenstoff\", \"koolstof\" and \"kulstof\" respectively, all literally meaning coal-substance.\n\nCarbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500 BCE in China, while carbon in the form of charcoal was made around Roman times by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.\n\nIn 1722, René Antoine Ferchault de Réaumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram.\nIn 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave \"aerial acid\" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. In their publication they proposed the name \"carbone\" (Latin \"carbonum\") for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.\n\nA new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers – Robert Curl, Harold Kroto and Richard Smalley – received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms lead to the discovery of further exotic allotropes, including glassy carbon, and the realization that \"amorphous carbon\" is not strictly amorphous.\n\nCommercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica and feldspars in schists, gneisses and metamorphosed sandstones and limestone as lenses or veins, sometimes of a metre or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made simply by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.\n\nThere are three types of natural graphite—amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry \"amorphous\" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States. Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.\n\nAccording to the USGS, world production of natural graphite was 1.1 million tonnes in 2010, to which China contributed 800,000 t, India 130,000 t, Brazil 76,000 t, North Korea 30,000 t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998 million was produced in 2009.\n\nThe diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).\n\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\n\nHistorically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.\n\nDiamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the Diamond fields in South Africa. Production has increased over time and now an accumulated total of 4.5 billion carats have been mined since that date. About 20% of that amount has been mined in the last 5 years alone, and during the last ten years 9 new mines have started production while 4 more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.\n\nIn the United States, diamonds have been found in Arkansas, Colorado and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.\n\nToday, most commercially viable diamond deposits are in Russia, Botswana, Australia and the Democratic Republic of Congo. In 2005, Russia produced almost one-fifth of the global diamond output, reports the British Geological Survey. Australia has the richest diamantiferous pipe with production reaching peak levels of per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada, Siberia (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe), Brazil, and in Northern and Western Australia.\n\nCarbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is distilled in refineries by the petrochemical industry to produce gasoline, kerosene, and other products. Cellulose is a natural, carbon-containing polymer produced by plants in the form of wood, cotton, linen, and hemp. Cellulose is used primarily for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.\n\nThe uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a molding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors and as a neutron moderator in nuclear reactors.\n\nCharcoal is used as a drawing material in artwork, barbecue grilling, iron smelting, and in many other applications. Wood, coal and oil are used as fuel for production of energy and heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials.\n\nCarbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.\n\nCarbon black is used as the black pigment in printing ink, artist's oil paint and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification, and kitchen extractor hoods, and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron (smelting). Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron and titanium, are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone and metal.\nThe diamond industry falls into two categories: one dealing with gem-grade diamonds and the other, with industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets act in dramatically different ways.\n\nUnlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity: there is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.\n\nIndustrial diamonds are valued mostly for their hardness and heat conductivity, with the gemological qualities of clarity and color being mostly irrelevant. About 80% of mined diamonds (equal to about 100 million carats or 20 tonnes annually) are unsuitable for use as gemstones are relegated for industrial use (known as \"bort)\". synthetic diamonds, invented in the 1950s, found almost immediate industrial applications; 3 billion carats (600 tonnes) of synthetic diamond is produced annually.\n\nThe dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most of these applications do not require large diamonds; in fact, most diamonds of gem-quality except for their small size can be used industrially. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances in the production of synthetic diamonds, new applications are becoming feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable for microchips, and because of its exceptional heat conductance property, as a heat sink in electronics.\n\nPure carbon has extremely low toxicity to humans and can be handled and even ingested safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract. Consequently, once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and Ötzi the Iceman was found to have carbon tattoos that survived during his life and for 5200 years after his death. Inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease, coalworker's pneumoconiosis. Diamond dust used as an abrasive can be harmful if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harm may result from contaminants (e.g., organic chemicals, heavy metals) rather than from the carbon itself.\n\nCarbon generally has low toxicity to life on Earth; but carbon nanoparticles are deadly to \"Drosophila\".\n\nCarbon may burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air in coal mine waste tips, ship cargo holds and coal bunkers, and storage dumps.\n\nIn nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250 °C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.\n\nThe great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant \"Ricinus communis\", cyanide (CN), and carbon monoxide; and such essentials to life as glucose and protein.\n\n"}
{"id": "2322153", "url": "https://en.wikipedia.org/wiki?curid=2322153", "title": "Carbon offset", "text": "Carbon offset\n\nA carbon offset is a reduction in emissions of carbon dioxide or greenhouse gases made in order to compensate for or to offset an emission made elsewhere.\n\nCarbon offsets are measured in tonnes of carbon dioxide-equivalent (COe). One tonne of carbon offset represents the reduction of one tonne of carbon dioxide or its equivalent in other greenhouse gases.\n\nThere are two markets for carbon offsets. In the larger, compliance market, companies, governments, or other entities buy carbon offsets in order to comply with caps on the total amount of carbon dioxide they are allowed to emit. For instance, an entity could be complying with obligations of Annex 1 Parties under the Kyoto Protocol or of liable entities under the EU Emission Trading Scheme, among others. In 2006, about $5.5 billion of carbon offsets were purchased in the compliance market, representing about 1.6 billion metric tons of COe reductions.\n\nIn the much smaller, voluntary market, individuals, companies, or governments purchase carbon offsets to mitigate their own greenhouse gas emissions from transportation, electricity use, and other sources. For example, an individual might purchase carbon offsets to compensate for the greenhouse gas emissions caused by personal air travel. Carbon offset vendors offer direct purchase of carbon offsets, often also offering other services such as designating a carbon offset project to support or measuring a purchaser's carbon footprint. In 2016, about $705 million of carbon offsets were purchased in the voluntary market, representing about 123.4 million metric tons of COe reductions.\n\nOffsets typically support projects that reduce the emission of greenhouse gases in the short- or long-term. A common project type is renewable energy, such as wind farms, biomass energy, or hydroelectric dams. Others include energy efficiency projects, the destruction of industrial pollutants or agricultural byproducts, destruction of landfill methane, and forestry projects. Some of the most popular carbon offset projects from a corporate perspective are energy efficiency and wind turbine projects.\n\nThe Kyoto Protocol has sanctioned offsets as a way for governments and private companies to earn carbon credits that can be traded on a marketplace. The protocol established the Clean Development Mechanism (CDM), which validates and measures projects to ensure they produce authentic benefits and are genuinely \"additional\" activities that would not otherwise have been undertaken. Organizations that are unable to meet their emissions quota can offset their emissions by buying CDM-approved Certified Emissions Reductions.\n\nOffsets may be cheaper or more convenient alternatives to reducing one's own fossil-fuel consumption. However, some critics object to carbon offsets, and question the benefits of certain types of offsets.\nDue diligence is recommended to help businesses in the assessment and identification of \"good quality\" offsets to ensure offsetting provides the desired additional environmental benefits, and to avoid reputational risk associated with poor quality offsets.\n\nOffsets are viewed as an important policy tool to maintain stable economies and to improve sustainability. One of the hidden dangers of climate change policy is unequal prices of carbon in the economy, which can cause economic collateral damage if production flows to regions or industries that have a lower price of carbon—unless carbon can be purchased from that area, which offsets effectively permit, equalizing the price.\n\nCarbon offsets may represent six primary categories of greenhouse gases: carbon dioxide (), methane (CH), nitrous oxide (NO), perfluorocarbons (PFCs), hydrofluorocarbons (HFCs), and sulfur hexafluoride (SF).\n\nCarbon offsets have several common features:\n\n\nIn 2009, 8.2 billion metric tons of carbon dioxide equivalent changed hands worldwide, up 68 per cent from 2008, according to the study by carbon-market research firm Point Carbon, of Washington and Oslo. But at EUR94 billion, or about $135 billion, the market's value was nearly unchanged compared with 2008, with world carbon prices averaging EUR11.40 a ton, down about 40 per cent from the previous year, according to the study. The World Bank's \"State and Trends of the Carbon Market 2010\" put the overall value of the market at $144 billion, but found that a significant part of this figure resulted from manipulation of a VAT loophole.\n\n\nThe global carbon market is dominated by the European Union, where companies that emit greenhouse gases are required to cut their emissions or buy pollution allowances or carbon credits from the market, under the European Union Emission Trading Scheme (EU ETS). Europe, which has seen volatile carbon prices due to fluctuations in energy prices and supply and demand, will continue to dominate the global carbon market for another few years, as the U.S. and China—the world's top polluters—have yet to establish mandatory emission-reduction policies.\n\nOn the whole, the U.S. market remains primarily a voluntary market, but multiple cap and trade regimes are either fully implemented or near-imminent at the regional level. The first mandatory, market-based cap-and-trade program to cut CO in the U.S., called the Regional Greenhouse Gas Initiative (RGGI), kicked into gear in Northeastern states in 2009, growing nearly tenfold to $2.5 billion, according to Point Carbon. Western Climate Initiative (WCI)—a regional cap-and-trade program including seven western states (California notably among them) and four Canadian provinces—has established a regional target for reducing heat-trapping emissions of 15 percent below 2005 levels by 2020. A component of California's own Global Warming Solutions Act of 2006, kicked off in early 2013, requires high-emissions industries to purchase carbon credits to cover emissions in excess of 25,000 CO2 metric tons.\n\n\n\n\n\n\nIn 2015, the UNFCCC created a dedicated website where organizations, companies, but also private persons are able to offset their footprint with the aim of facilitating everyone's participation in the process of promoting sustainability on a voluntary basis.\n\nThe CDM identifies over 200 types of projects suitable for generating carbon offsets, which are grouped into broad categories. These project types include renewable energy, methane abatement, energy efficiency, reforestation and fuel switching.\n\nRenewable energy offsets commonly include wind power, solar power, hydroelectric power and biofuel. Some of these offsets are used to reduce the cost differential between renewable and conventional energy production, increasing the commercial viability of a choice to use renewable energy sources. Emissions from burning fuel, such as red diesel, has pushed one UK fuel supplier to create a carbon offset fuel named Carbon Offset Red Diesel.\n\nRenewable Energy Credits (RECs) are also sometimes treated as carbon offsets, although the concepts are distinct. Whereas a carbon offset represents a reduction in greenhouse gas emissions, a REC represents a quantity of energy produced from renewable sources. To convert RECs into offsets, the clean energy must be translated into carbon reductions, typically by assuming that the clean energy is displacing an equivalent amount of conventionally produced electricity from the local grid. This is known as an indirect offset (because the reduction doesn't take place at the project site itself, but rather at an external site), and some controversy surrounds the question of whether they truly lead to \"additional\" emission reductions and who should get credit for any reductions that may occur. Intel corporation is the largest purchaser of renewable power in the US.\n\nSome offset projects consist of the combustion or containment of methane generated by farm animals (by use of an anaerobic digester), landfills or other industrial waste. Methane has a global warming potential (GWP) 23 times that of CO; when combusted, each molecule of methane is converted to one molecule of CO, thus reducing the global warming effect by 96%.\n\nAn example of a project using an anaerobic digester can be found in Chile where in December 2000, the largest pork production company in Chile, initiated a voluntary process to implement advanced waste management systems (anaerobic and aerobic digestion of hog manure), in order to reduce greenhouse gas (GHG) emissions.\n\nWhile carbon offsets that fund renewable energy projects help lower the carbon intensity of energy \"supply\", energy conservation projects seek to reduce the overall \"demand\" for energy. Carbon offsets in this category fund projects of several types:\n\n\nIndustrial pollutants such as hydrofluorocarbons (HFCs) and perfluorocarbons (PFCs) have a GWP many thousands of times greater than carbon dioxide by volume. Because these pollutants are easily captured and destroyed at their source, they present a large and low-cost source of carbon offsets. As a category, HFCs, PFCs, and NO reductions represent 71 per cent of offsets issued under the CDM.\n\nLand use, land-use change and forestry (LULUCF) projects focus on natural carbon sinks such as forests and soil. Deforestation, particularly in Brazil, Indonesia and parts of Africa, account for about 20 per cent of greenhouse gas emissions. Deforestation can be avoided either by paying directly for forest preservation, or by using offset funds to provide substitutes for forest-based products. There is a class of mechanisms referred to as REDD schemes (Reducing emissions from deforestation and forest degradation), which may be included in a post-Kyoto agreement. REDD credits provide carbon offsets for the protection of forests, and provide a possible mechanism to allow funding from developed nations to assist in the protection of native forests in developing nations.\n\nAlmost half of the world's people burn wood (or fiber or dung) for their cooking and heating needs. Fuel-efficient cook stoves can reduce fuel wood consumption by 30 to 50%, though the warming of the earth due to decreases in particulate matter (i.e. smoke) from such fuel-efficient stoves has not been addressed. There are a number of different types of LULUCF projects:\n\n\nVoluntary purchasers can offset their carbon emissions by purchasing carbon allowances from legally mandated cap-and-trade programs such as the Regional Greenhouse Gas Initiative or the European Emissions Trading Scheme. By purchasing the allowances that power plants, oil refineries, and industrial facilities need to hold to comply with a cap, voluntary purchases tighten the cap and force additional emissions reductions.\n\nVoluntary purchases can also be made through small-scale and sometimes uncertified schemes such as those offered at South African based Promoting Access to Carbon Equity Centre (PACE), which nevertheless offer clear services such as poverty alleviation in the form of renewable energy development. Also, as \"easy carbon credits are coming to an end\", these projects have the potential to develop projects that are either too small or too complicated to benefit from legally mandated cap-and-trade programs.\n\nOnce it has been accredited by the UNFCCC a carbon offset project can be used as carbon credit and linked with official emission trading schemes, such as the European Union Emission Trading Scheme or Kyoto Protocol, as Certified Emission Reductions. European emission allowances for the 2008–2012 second phase were selling for between 21 and 24 Euros per metric ton of CO as of July 2007.\n\nThe voluntary Chicago Climate Exchange also includes a carbon offset scheme that allows offset project developers to sell emissions reductions to CCX members who have voluntarily agreed to meet emissions reduction targets.\n\nThe Western Climate Initiative, a regional greenhouse gas reduction initiative by states and provinces along the western rim of North America, includes an offset scheme. Likewise, the Regional Greenhouse Gas Initiative, a similar program in the northeastern U.S., includes an offset program. A credit mechanism that uses offsets may be incorporated in proposed schemes such as the Australian Carbon Exchange.\n\nA UK offset provider set up a carbon offsetting scheme that set up a secondary market for treadle pumps in developing countries. These pumps are used by farmers, using human power, in place of diesel pumps. However, given that treadle pumps are best suited to pumping shallow water, while diesel pumps are usually used to pump water from deep boreholes, it is not clear that the treadle pumps are actually achieving real emissions reductions. Other companies have explored and rejected treadle pumps as a viable carbon offsetting approach due to these concerns.\n\nCarbon retirement involves retiring allowances from emission trading schemes as a method for offsetting carbon emissions. Under schemes such as the European Union Emission Trading Scheme, EU Emission Allowances (EUAs), which represent the right to release carbon dioxide into the atmosphere, are issued to all the largest polluters. The theory is that by buying these allowances and permanently removing them, the price of EUAs increases and provides an incentive for industrial companies to reduce their emissions.\n\nDue to their indirect nature, many types of offset are difficult to verify. Some providers obtain independent certification that their offsets are accurately measured, to distance themselves from potentially fraudulent competitors. The credibility of the various certification providers is often questioned. Certified offsets may be purchased from commercial or non-profit organizations for US$2.75–99.00 per tonne of CO, due to fluctuations of market price. Annual carbon dioxide emissions in developed countries range from 6 to 23 tons per capita.\n\nAccounting systems differ on precisely what constitutes a valid offset for voluntary reduction systems and for mandatory reduction systems. However formal standards for quantification exist based on collaboration between emitters, regulators, environmentalists and project developers. These standards include the Voluntary Carbon Standard, Green-e Climate, Chicago Climate Exchange and the Gold Standard, the latter of which expands upon the requirements for the Clean Development Mechanism of the Kyoto Protocol.\n\nAccounting of offsets may address the following basic areas:\n\n\nWhile the primary goal of carbon offsets is to reduce global carbon emissions, many offset projects also claim to lead to improvements in the quality of life for a local population. These additional improvements are termed \"co-benefits\", and may be considered when evaluating and comparing carbon offset projects. For example, possible co-benefits from a project that replaces wood-burning stoves with ovens using a less carbon-intensive fuel could include:\n\n\nOffset projects can also lead to co-benefits such as better air and water quality, and healthier communities.\n\nIn a recent survey conducted by EcoSecurities, Conservation International, CCBA and ClimateBiz, of the 120 corporates surveyed more than 77 per cent rated community and environmental benefits as the prime motivator for purchasing carbon offsets.\n\nCarbon offset projects can also negatively affect quality of life. For example, people who earn their livelihoods from collecting firewood and selling it to households could become unemployed if firewood is no longer used. A July 2007 paper from the Overseas Development Institute offers some indicators to be used in assessing the potential developmental impacts of voluntary carbon offset schemes:\n\nPutting a price on carbon encourages innovation by providing funding for new ways to reduce greenhouse gases in many sectors. Carbon reduction goals drive the demand for offsets and carbon trading, encouraging the development of this new industry and offering opportunities for different sectors to develop and use innovative new technologies.\n\nCarbon offset projects also provide savings – energy efficiency measures may reduce fuel or electricity consumption, leading to a potential reduction in maintenance and operating costs.\n\nThe UNFCCC has created a dedicated website where CDM activities and prior consideration projects are able to report their co-benefits on a voluntary basis.\n\nIn an effort to inform and safeguard business and household consumers purchasing Carbon Offsets, in 2009, the UK Government has launched a scheme for regulating Carbon offset products. DEFRA have created the \"Approved Carbon Offsetting\" brand to use as an endorsement on offsets approved by the UK government.\nThe Scheme sets standards for best practice in offsetting. Approved offsets have to demonstrate the following criteria:\nThe first company to qualify for the scheme was Clear, followed by Carbon Footprint, Carbon Passport, Pure, British Airways and Carbon Retirement Ltd.\n\nOn 20 May 2011 the Department of Energy and Climate Change announced that the Quality Assurance Scheme would close on 30 June 2011. The stated purpose of the Quality Assurance Scheme was 'to provide a straightforward route for those wishing to offset their emissions to identify quality offsets'. Critics of the closure therefore argued that without the scheme, businesses and individuals would struggle to identify quality carbon offsets.\n\nIn 2012 the scheme was relaunched as the Quality Assurance Standard (QAS). The QAS is now run independently by Quality Assurance Standard Ltd which is a company limited by guarantee based in the United Kingdom. The Quality Assurance Standard is a comprehensive independent audit system for carbon offsets. Approved offsets are checked against a 40-point checklist to ensure they meet the very highest standards in the world.\n\nOn 17 July 2012, the first organisations were approved as meeting the new QAS.\n\nThe Australian government is currently in a consultation period on the regulation of Carbon Offsets.\nOn 20 December 2013, the Australian Government released the Emissions Reduction Fund Green Paper outlining its preferred design options for the Emissions Reduction Fund: a carbon buy-back model. The Government invites public comment and written submissions on the Green Paper by 5pm on Friday 21 February 2014.\n\nLess than 30 pence in every pound spent on some carbon offset schemes goes directly to projects designed to reduce emissions. The figures reported by the BBC and based on UN data reported that typically 28p goes to the set up and maintenance costs of an environmental project. 34p goes to the company that takes on the risk that the project may fail. The project's investors take 19p, with smaller amounts of money being distributed between organisations involved in brokering and auditing the carbon credits. In that respect carbon Offsets are similar to most consumer products, with only a fraction of sale prices going to the off-shore producers, the rest being shared between investors and distributors who bring it to the markets, who themselves need to pay their employees and service providers such as advertising agencies most of the time located in expensive areas.\n\nSome activists disagree with the principle of carbon offsets, likening them to Roman Catholic indulgences, a way for the guilty to pay for absolution rather than changing their behavior. George Monbiot, an English environmentalist and writer, says that carbon offsets are an excuse for business as usual with regard to pollution. Proponents hold that the indulgence analogy is flawed because they claim carbon offsets actually reduce carbon emissions, changing the business as usual, and therefore address the root cause of climate change. Proponents of offsets claim that third-party certified carbon offsets are leading to increased investment in renewable energy, energy efficiency, methane biodigesters and reforestation and avoided deforestation projects, and claim that these alleged effects are the intended goal of carbon offsets. On October 16, 2009 responsibletravel.com, once a strong voice in favour of carbon offsetting, announced that it would stop offering carbon offsetting to its clients, stating that \"too often offsets are being used by the tourism industry in developed countries to justify growth plans on the basis that money will be donated to projects in developing countries. Global reduction targets will not be met this way.\"\n\nOn 4 February 2010, travel networking site Vida Loca Travel announced that they would donate 5 percent of profits to International Medical Corps, as they feel that international aid can be more effective at cutting global warming in the long term than carbon offsetting, citing the work of economist Jeffrey Sachs.\n\nSome environmentalists have questioned the effectiveness of tree-planting projects for carbon offset purposes. Critics point to the following issues with tree planting projects:\n\n\nTree-planting projects can cause conflicts with indigenous people who are displaced or otherwise find their use of forest resources curtailed. For example, a World Rainforest Movement report documents land disputes and human rights abuses at Mount Elgon. In March 2002, a few days before receiving Forest Stewardship Council certification for a project near Mount Elgon, the Uganda Wildlife Authority evicted more than 300 families from the area and destroyed their homes and crops. That the project was taking place in an area of on-going land conflict and alleged human rights abuses did not make it into project report.\nA 2011 report by Oxfam International describes a case where over 20,000 farmers in Uganda were displaced for a FSC-certified plantation to offset carbon by London-based New Forests Company\n\nSeveral certification standards exist, offering variations for measuring emissions baseline, reductions, additionality, and other key criteria. However, no single standard governs the industry, and some offset providers have been criticized on the grounds that carbon reduction claims are exaggerated or misleading. Problems include:\n\nBecause offsets provide a revenue stream for the reduction of some types of emissions, they can in some cases provide incentives to emit more, so that emitting entities can later get credit for reducing emissions from an artificially high baseline. This is especially the case for offsets with a high profit margin. For example, one Chinese company generated $500 million in carbon offsets by installing a $5 million incinerator to burn the HFCs produced by the manufacture of refrigerants. The huge profits provided incentive to create new factories or expand existing factories solely for the purpose of increasing production of HFCs and then destroying the resultant pollutants to generate offsets. Not only is this outcome environmentally undesirable, it undermines other offset projects by causing offset prices to collapse. The practice had become so common that offset credits are now no longer awarded for new plants to destroy HFC-23.\n\nIn Nigeria oil companies \"flare off\" 40 per cent of the natural gas found. The Agip Oil Company plans to build plants to generate electricity from this gas and thus claim 1.5 million offset credits a year. United States company Pan Ocean Oil Corporation has also applied for credits in exchange for processing its own waste gas in Nigeria. Oilwatch.org's Michael Karikpo calls this \"outrageous\", as flaring is illegal in Nigeria, adding that \"It's like a criminal demanding money to stop committing crimes\".\n\nAlthough many carbon offset projects tout their environmental co-benefits, some are accused of having negative secondary effects. Point Carbon has reported on an inconsistent approach with regard to some hydro-electric projects as carbon offsets; some countries in the EU are not allowing large projects into the EU ETS, because of their environmental impacts, even though they have been individually approved by the UNFCCC and World Commission on Dams. It is difficult to assess the exact results of carbon offsets given the fact that they are a relatively new form of carbon reduction, and it is possible that some carbon offset purchases are made in an attempt to increase positive business public relations rather than to help solve the issue of greenhouse gas emissions.\n\nOffset projects may also have negative social impacts, for example when local residents are evicted to enable a National Park to be marketed as a carbon offset.\n\n\n"}
{"id": "180735", "url": "https://en.wikipedia.org/wiki?curid=180735", "title": "Cart", "text": "Cart\n\nA cart is a vehicle designed for transport, using two wheels and normally pulled by one or a pair of draught animals. A \"handcart\" is pulled or pushed by one or more people. It is different from a dray or wagon, which is a heavy transport vehicle with \"four\" wheels and typically two or more horses, or a carriage, which is used exclusively for transporting humans.\n\nOver time, the term \"cart\" has come to mean nearly any small conveyance, from shopping carts to golf carts or UTVs, without regard to number of wheels, load carried, or means of propulsion.\n\nThe draught animals used for carts may be horses, donkeys or mules, oxen, and even smaller animals such as goats or large dogs.\n\nCarts have been mentioned in literature as far back as the second millennium B.C. The Indian sacred book Rigveda states that men and women are as equal as two wheels of a cart. Hand-carts pushed by humans have been used around the world. In the 19th century, for instance, some Mormons travelling across the plains of the United States between 1856 and 1860 used handcarts.\n\nThe history of the cart is closely tied to the history of the wheel.\n\nCarts were often used for judicial punishments, both to transport the condemned – a public humiliation in itself (in Ancient Rome defeated leaders were often carried in the victorious general's triumph) – and even, in England until its substitution by the whipping post under Queen Elizabeth I, to tie the condemned to the \"cart-tail\" and administer him or her a public whipping.\n\nLarger carts may be drawn by animals, such as horses, mules, or oxen. They have been in continuous use since the invention of the wheel, in the 4th millennium BC. Carts may be named for the animal that pulls them, such as \"horsecart\" or \"oxcart\". In modern times, horsecarts are used in competition while draft horse showing. A \"dogcart\", however, is usually a cart designed to \"carry\" hunting dogs: an open cart with two cross-seats back to back; the dogs could be penned between the rear-facing seat and the back end.\n\nThe term \"cart\" (synonymous in this sense with \"chair\") is also used for various kinds of lightweight, two-wheeled carriages, some of them \"sprung carts\" (or \"spring carts\"), especially those used as open pleasure or sporting vehicles. They could be drawn by a horse, pony or dog. Examples include:\n\n\nThe builder of a cart may be known as a \"cartwright\"; the surname \"Carter\" also derives from the occupation of transporting goods by cart or wagon.\nCarts have many different shapes, but the basic idea of transporting material (or maintaining a collection of materials in a portable fashion) remains. Carts may have a pair of shafts, one along each side of the draught animal that supports the forward-balanced load in the cart. The shafts are supported by a saddle on the horse. Alternatively (and normally where the animals are oxen or buffalo), the cart may have a single pole between a pair of animals. The draught traces attach to the axle of the vehicle or to the shafts. The traces are attached to a collar (on horses), to a yoke (on other heavy draught animals) or to a harness on dogs or other light animals.\n\nTraces are made from a range of materials depending on the load and frequency of use. Heavy draught traces are made from iron or steel chain. Lighter traces are often leather and sometimes hemp rope, but plaited horse-hair and other similar decorative materials can be used.\n\nThe dray is often associated with the transport of barrels, particularly of beer.\n\nOf the cart types not animal-drawn, perhaps the most common example today is the shopping cart (British English: shopping trolley), which has also come to have a metaphorical meaning in relation to online purchases (here, British English uses the metaphor of the shopping basket). Shopping carts first made their appearance in Oklahoma City in 1937.\n\nIn golf, both manual push or pull and electric golf trolleys are designed to carry a golfer's bag, clubs and other equipment. Also, the golf cart, car, or buggy, is a powered vehicle that carries golfers and their equipment around a golf course faster and with less effort than walking.\n\nA \"Porter's trolley\" is a type of small, hand-propelled wheeled platform. This can also be called a baggage cart.\nsince the 13th century.\n\nAutocarts are a type of small, hand-propelled wheeled utility carts having a pivoting base for collapsible storage in vehicles. They eliminate the need for plastic or paper shopping bags and are also used by tradespersons to carry tools, equipment or supplies.\n\nA soap-box cart (also known as a Billy Cart, Go-Cart, Trolley etc.) is a popular children's construction project on wheels, usually pedaled, but also intended for a test race. Similar, but more sophisticated are modern-day pedal cart toys used in general recreation and racing.\n\nAn electric cart is an electric vehicle.\n\nThe term \"Go-Kart\", which exists since 1959, also shortened as \"Kart\", an alternative spelling of \"cart\", refers to a tiny race car with frame and two-stroke engine; the old term \"go-cart\" originally meant a sedan chair or an infant walker\n\n"}
{"id": "45523355", "url": "https://en.wikipedia.org/wiki?curid=45523355", "title": "Center for Biofilm Engineering", "text": "Center for Biofilm Engineering\n\nThe Center for Biofilm Engineering (CBE) is an interdisciplinary research, education, and technology transfer institution located on the central campus of Montana State University in Bozeman, Montana. The center was founded in April 1990 as the Center for Interfacial Microbial Process Engineering with a grant from the Engineering Research Centers (ERC) program of the National Science Foundation (NSF). The CBE integrates faculty from multiple university departments to lead multidisciplinary research teams—including graduate and undergraduate students—to advance fundamental biofilm knowledge, develop beneficial uses for microbial biofilms, and find solutions to industrially relevant biofilm problems. The center tackles biofilm issues including chronic wounds, bioremediation, and microbial corrosion through cross-disciplinary research and education among engineers, microbiologists and industry.\n\nThe center originated as the Institute for Chemical and Biological Process Analysis (IPA) in 1983. In 1990, the center became a national ERC as the Center for Interfacial Microbial Process Engineering based on a $7.2 million grant from the NSF.<ref name=\"Environment/Ecology Grants\"></ref> In 1993 the center assumed its current name-Center for Biofilm Engineering. The original grants expired in 2001 and the center became self-sufficient. The center celebrated its 25 anniversary as an NSF ERC in 2015.\n\nIn 1979 W.G. (Bill) Characklis came to Montana State University from Rice University as a professor in civil (environmental) and chemical engineering. He assembled a multidisciplinary team of engineers, microbiologists and chemists to study the processes and effects of microbial growth at interfaces He established a cross-disciplinary environmental biotechnology institute to address the needs of industry in the areas of biofouling, microbial corrosion and biofilm technology. The Institute for Chemical and Biological Process Analysis (IPA) was chartered by the Montana Board of Regents in 1983 within the Montana State University College of Engineering. Bill Characklis was its first director. The IPA provided the foundation for eventual Engineering Research Center status in several ways. The IPA conducted fundamental research, development, and testing for industry and government agencies and it pursued biofilm projects that crossed traditional scientific discipline boundaries. The IPA established an Industrial Associates membership program and by 1989 the program had 12 participating members, each contributing $10,000 annually to the center. Membership benefits provided them the opportunity to assist in critiquing and defining the IPA research programs. Members included oil/gas/power companies, a chemical manufacturer, a pulp and paper company and partnership with the Idaho National Engineering Laboratory.\n\nIn 1989, the IPA applied to the NSF for Engineering Research Center status, which was granted in April 1990. The Center for Interfacial Microbial Process Engineering was established as one of three national Engineering Research Centers out of 48 applicants. As an ERC, the new organization was charged with building a cross-disciplinary research and education program at Montana State University, as well as increasing U.S. industrial competitiveness in biofilm related technologies. The center's charter mandated that research, education, and technology transfer programs would be fully integrated within the center's program planning. The Industrial Associates program continued to be used as a mechanism for the center to get information from industry about significant biofilm-related problems and collaborate on center research initiatives designed to address biofilm issues. The center’s education program recruited students to participate on interdisciplinary research teams and to interact with industry representatives.\n\nThe provision of $7.2 million in grants from the NSF during the first five years enabled the expansion of center research into new areas, notably bioremediation and biohydrometallurgy. The center's industrial focus expanded from biofilm control and mitigation to include positive use of biofilm processes to break up soil and water contaminants as well as extracting minerals from low-grade ores. Center projects were designed to span scales of inquiry from fundamental bench-scale to applied field-scale experiments. These projects enabled the continued development of microsensors to measure gradients of gases and pH within biofilm communities, microscopy to elucidate physiological activity of community organisms and modeling to predict biofilm behavior.\n\nIn 1992, two years after its establishment, the center's first director, Bill Characklis, died. Montana State University signaled its commitment to the center by hiring J.W. (Bill) Costerton, a professor of microbiology from the University of Calgary, as Executive Director and James Bryers, a professor of biochemical engineering from Duke University, as Director of Research for the center. In 1993 the center’s name was changed to Center for Biofilm Engineering (CBE).\n\nUnder Costerton's direction the center continued to fulfill its charter and began expanding its scope of inquiry. Costerton encouraged exploration of the bioelectric effect, the phenomenon of cell-cell signaling and its relation to biofilm structure and subsurface biobarrier technologies to protect water and soils from mining contamination. Industrial interest and membership grew in response to more diversified research topics. By 1996 Industrial Associate membership had grown to 19 diversified members including members representing water treatment, mining, government labs, specialty chemicals, consumer products, and oil/energy companies. In June 1996 the National Science Foundation renewed its commitment to the Center for Biofilm Engineering with a new five year grant of $7.6 million.\n\nIn 1996 the national profile of the CBE and biofilm research was on the rise. Numerous scientific and mass media publications began to address biofilm technology in ernest. An article in the September 1996 issue of \"Science\", entitled \"Biofilms Invade Microbiology\" featured the work and history of the Center for Biofilm Engineering. Other magazines and newspapers featuring biofilms and CBE research included \"New Scientist\" (cover article, August 31, 1996), \"Science News\" (April 26, 1997), the \"Chicago Sun-Times\" (May 17, 1998), \"Science\" (March 19, 1999), \"Businessweek\" (September 12, 1999), \"Knight Ridder/Tribune News Service\" (January 7, 2000), \"Nature\" (November 16, 2000) and \"The Boston Globe\" (May 28, 2002). In 2001 Costerton and CBE Deputy Director Philip S. Stewart authored a biofilm article \"Battling Biofilms\" published in \"Scientific American\" (July 1, 2001)\n\nThe NSF ERC program was designed to create institutional centers that would be self-sufficient within ten years. The Center for Biofilm Engineering at Montana State University started planning for self-sufficiency in 1998 through the establishment of a Self-sufficiency Task Force at the university. The center achieved self-sufficiency in 2001 and continues to be funded in part through federal and private grants with continued emphasis on providing value to the Industrial Associates, and support from Montana State University and the State of Montana. In 2005 Philip S. Stewart, a professor of chemical and biological engineering, was selected to be the third CBE Director. Stewart, who had participated as a CBE faculty member since 1991, was a leading expert on antimicrobials and biofilm control. Under Stewart’s tenure, CBE grew in affiliated faculty numbers, industrial membership, the number of testing and industry-sponsored projects, and the participation of undergraduate and graduate students. Matthew Fields became the fourth director of the CBE in 2015. Since FY2016, research grants awarded have nearly tripled, growing from $2.3 million to $6.4 million in 2018. Membership fees collected from industrial associates increased by 35 percent from $481,000 in FY2016 to $648,480 in FY2018. The center is one of 24 self-sustaining Engineering Research Centers in the National Science Foundation program.\n\nThe center's biofilm research program was established in the 1980s with a focus on environmental bioengineering, interdisciplinary investigation, and industrial participation. Faculty expertise from civil/environmental engineering, mathematics, microbiology, chemical and biological engineering, chemistry and biochemistry, mechanical and industrial engineering, computer science, electrical engineering, and statistics contributes to hypothesis development and experimental design. The center conducts research that includes multiple scales of observation, from molecular to industrial field-scale, with projects that cover both fundamental and applied topics. CBE research teams have been part of numerous groundbreaking advances in biofilm science and technology including: quorum sensing, permeability (earth sciences), biomineralization, antimicrobial tolerance, viscoelasticity, detachment, standardized biofilm methods, chronic wounds, algae fuel, microbial corrosion, sulfate-reducing bacteria, and biofilm structure and function. CBE research has been published in high-profile peer-reviewed journals including: \"Nature\", \"The Lancet\", \"Science\", \"JAMA\", \"PNAS\", \"EMBO Journal\", \"ISME Journal\", \"Nature Reviews Microbiology\" and \"Physical Review Letters\". By 2017, CBE authors had published 1,165 peer-reviewed papers. The center has provided more than 18 cover images related to biofilms for peer-reviewed journals.\n\nApplied research topics addressed by the center in 2017 included:\n\nOther research topics in 2017 included:\n\nThe center's standing in the international research community and its encouragement of collaboration regularly attracts visiting students and faculty from numerous institutions in the U.S. and foreign countries. From 1990 to 2017, more than 287 visiting researchers from more than 30 countries and 38 U.S. states spent from several weeks to a year or more studying biofilms in CBE laboratories. In 2017, 8 visiting scientists and engineers conducted research at CBE.\n\nThe center's Industrial Associates program provides access to center information, expertise, training and other benefits for a yearly subscription fee. The program was started in 1983 with the creation of the Institute for Chemical and Biological Process Analysis. CBE’s Standardized Biofilm Methods research group (SBM) focuses on issues of interest to companies developing new products addressing biofilm formation. Researchers develop, refine, and publish quantitative methods for growing, treating, sampling, and analyzing biofilm bacteria. SBM laboratory members work with international standard setting organizations to secure approval of biofilm methods by the standard setting community. Under a contract with the U.S. Environmental Protection Agency, the SBM conducts laboratory research to support the development and standardization of test methods for measuring the performance of antimicrobial products—including those for biofilm bacteria—and provide statistical services related to EPA's Office of Pesticide Programs Antimicrobial Testing Program. \n\nIn 2013 CBE Director Phil Stewart and CBE Industrial Coordinator Paul Sturman worked to partner with the U.S. Food and Drug Administration (FDA) to co-sponsor a one-day workshop on biofilms. The resulting workshop, \"Biofilms, Medical Devices and Anti-Biofilm Technology: Challenges and Opportunities,\" was held on the FDA White Oak campus on February 20, 2014. A follow-up CBE-hosted meeting on February 11, 2015 in College Park, Maryland, titled \"Anti-Biofilm Technologies: Pathways to Productivity\" was held to continue fostering scientific dialog between U.S. government agencies, industry and academia.\n\nMember companies have represented several industrial categories including energy/petroleum, chemicals/specialty chemicals, household/consumer products, medical/healthcare, testing laboratories, government laboratories, water, pulp and paper, and mining. Members range from large Fortune 500 international corporations to small start-up companies.\nGraduate and undergraduate students participate in collaborative, interdisciplinary research at the center. Students work under the guidance of multidisciplinary faculty to solve problems associated with biofilms in medical, industrial, and environmental contexts. Faculty and students from the following MSU departments and programs participated in the center's research from 2011 to 2017.\n\nThe Center for Biofilm Engineering's Undergraduate Research Experience (URE) was founded by Director J.W. (Bill) Costerton and Ryan Jordan in the late 1990s. PhD candidate (and subsequent Senior Research Engineer at the CBE) Jordan directed the CBE's first URE students as part of a backcountry water quality program in Yellowstone National Park and the Bridger-Teton Wilderness where URE students under Jordan's direction became the first researchers to identify the role of biofilm in the failure of portable water treatment filters used in disaster relief, military, and outdoor recreation applications.\n\nUndergraduate students are hired as undergraduate research assistants and work in CBE laboratories as members of research teams on interdisciplinary biofilm projects. CBE undergraduates are encouraged to acquire competence in laboratory skills, experimental design and group communication. By 2017, over 785 undergraduates from 11 disciplines had worked on laboratory biofilm projects under the direction of CBE-affiliated faculty members. Twenty-nine of 62 undergraduates (46%) in the 2016-17 academic year were female.\n\nGraduate students pursue their degree in a discipline offered through one of the science, agriculture, or engineering departments at Montana State University while they conduct their research in CBE laboratories. Student graduate committees are typically interdisciplinary. The student and graduate committee members select coursework appropriate for the student’s interests and degree program. Engineering students are encouraged to take microbiology courses; science students are encouraged to take relevant engineering coursework. Graduate students acquire experience by designing and performing research that crosses traditional academic discipline boundaries and has direct impact on current environmental, industrial, and medical issues. Students work on projects that range from fundamental to applied topics. In addition, the CBE’s Industrial Associates program brings students into working relationships with potential employers. Graduate students are encouraged to develop their communication and leadership skills by presenting at research conferences, mentoring undergraduate students, organizing the CBE’s seminar series, and assisting with outreach efforts. By the end of 2017, 256 graduate students had received advanced degrees, including 30 with doctorate degrees and 22 with masters degrees . During the 2016-17 academic year 53% graduate students were female.\n\n"}
{"id": "23803920", "url": "https://en.wikipedia.org/wiki?curid=23803920", "title": "Corpoelec", "text": "Corpoelec\n\nCorpoelec is a fully integrated state power corporation of Venezuela. It was created in 2007 by merging ten state-owned and six private-owned power companies. The president of the company is Hipolito Izquierdo.\n\n"}
{"id": "33254461", "url": "https://en.wikipedia.org/wiki?curid=33254461", "title": "Culture of microalgae in hatcheries", "text": "Culture of microalgae in hatcheries\n\nMicroalgae or microscopic algae grow in either marine or freshwater systems. They are primary producers in the oceans that convert water and carbon dioxide to biomass and oxygen in the presence of sunlight.\n\nThe oldest documented use of microalgae was 2000 years ago, when the Chinese used the cyanobacteria \"Nostoc\" as a food source during a famine. Another type of microalgae, the cyanobacteria \"Arthrospira\" (Spirulina), was a common food source among populations in Chad and Aztecs in Mexico as far back as the 16th century.\n\nToday cultured microalgae is used as direct feed for humans and land-based farm animals, and as feed for cultured aquatic species such as molluscs and the early larval stages of fish and crustaceans. It is a potential candidate for biofuel production. Microalgae can grow 20 or 30 times faster than traditional food crops, and has no need to compete for arable land. Since microalgal production is central to so many commercial applications, there is a need for production techniques which increase productivity and are economically profitable.\n\nA range of microalgae species are produced in hatcheries and are used in a variety of ways for commercial purposes. Studies have estimated main factors in the success of a microalgae hatchery system as the dimensions of the container/bioreactor where microalgae is cultured, exposure to light/irradiation and concentration of cells within the reactor.\n\nThis method has been employed since the 1950s. There are two main advantages of culturing microalgae using the open pond system. Firstly, an open pond system is easier to build and operate. Secondly, open ponds are cheaper than closed bioreactors because closed bioreactors require a cooling system. However, a downside to using open pond systems is decreased productivity of certain commercially important strains such as \"Arthrospira sp.\", where optimal growth is limited by temperature.\n\nThis method is used in outdoor cultivation and production of microalgae; where air is moved within a system in order to circulate water where microalgae is growing. The culture is grown in transparent tubes that lie horizontally on the ground and are connected by a network of pipes. Air is passed through the tube such that air escapes from the end that rests inside the reactor that contains the culture and creates an effect like stirring.\n\nThe biggest advantage of culturing microalgae within a closed system provides control over the physical, chemical and biological environment of the culture. This means factors that are difficult to control in open pond systems such as evaporation, temperature gradients and protection from ambient contamination make closed reactors favoured over open systems. Photobioreactors are the primary example of a closed system where abiotic factors can be controlled for. Several closed systems have been tested to date for the purposes of culturing microalgae, few important ones are mentioned below:\n\nThis system includes tubes laid on the ground to form a network of loops. Mixing of microalgal suspended culture occurs through a pump that raises the culture vertically at timed intervals into a photobioreactor. Studies have found pulsed mixing at intervals produces better results than the use of continuous mixing. Photobioreactors have also been associated with better production than open pond systems as they can maintain better temperature gradients. An example noted in higher production of \"Arthrospira sp.\" used as a dietary supplement was attributed to higher productivity because of a better suited temperature range and an extended cultivation period over summer months.\n\nThese reactors use vertical polyethylene sleeves hung from an iron frame. Glass tubes can also be used alternatively.\nMicroalgae are also cultured in vertical alveolar panels (VAP) that are a type of photobioreactor. This photobioreactor is characterised by low productivity. However, this problem can be overcome by modifying the surface area to volume ratio; where a higher ratio can increase productivity. Mixing and deoxygenation are drawbacks of this system and can be addressed by bubbling air continuously at a mean flow rate. The two main types of vertical photobioreactors are the Flow-through VAP and the Bubble Column VAP.\n\nFlat plate reactors(FPR) are built using narrow panels and are placed horizontally to maximise sunlight input to the system. The concept behind FPR is to increase the surface area to volume ratio such that sunlight is efficiently used. This system of microalgae culture was originally thought to be expensive and incapable of circulating the culture. Therefore, FPRs were considered to be unfeasible overall for the commercial production of microalgae. However, an experimental FPR system in the 1980s used circulation within the culture from a gas exchange unit across horizontal panels. This overcomes issues of circulation and provides an advantage of an open gas transfer unit that reduces oxygen build up. Examples of successful use of FPRs can be seen in the production of \"Nannochloropsis sp.\" used for its high levels of astaxanthin.\n\nFermentor-type reactors (FTR) are bioreactors where fermentation is carried out. FTRs have not developed hugely in the cultivation of microalgae and pose a disadvantage in the surface area to volume ratio and a decreased efficiency in utilizing sunlight. FTR have been developed using a combination of sun and artificial light have led to lowering production costs. However, information available on large scale counterparts to the laboratory-scale systems being developed is very limited. The main advantage is that extrinsic factors i.e. light can be controlled for and productivity can be enhanced so that FTR can become an alternative for products for the pharmaceutical industry.\n\nMicroalgae is an important source of nutrition and is used widely in the aquaculture of other organisms, either directly or as an added source of basic nutrients. Aquaculture farms rearing larvae of molluscs, echinoderms, crustaceans and fish use microalgae as a source of nutrition. Low bacteria and high microalgal biomass is a crucial food source for shellfish aquaculture.\n\nMicroalgae can form the start of a chain of further aquaculture processes. For example, microalgae is an important food source in the aquaculture of brine shrimp. Brine shrimp produce dormant eggs, called cysts, which can be stored for long periods and then hatched on demand to provide a convenient form of live feed for the aquaculture of larval fish and crustaceans.\n\nOther applications of microalgae within aquaculture include increasing the aesthetic appeal of finfish bred in captivity. One such example can be noted in the aquaculture of salmon, where microalgae is used to make salmon flesh pinker. This is achieved by the addition of natural pigments containing carotenoids such as astaxanthin produced from the microalgae \"Haematococcus\" to the diet of farmed animals.\n\nIn order to meet the demands of fossil fuels, alternate means of fuels are being investigated. Biodiesel and bioethanol are renewable fuels with much potential that are important in current research. However, agriculture based renewable fuels may not be completely sustainable and thus may not be able to replace fossil fuels. Microalgae can be remarkably rich in oils (up to 80% dry weight of biomass) suitable for conversion to fuel. Furthermore, microalgae are more productive than land based agricultural crops and could therefore be more sustainable in the long run. Microalgae for biofuel production is mainly produced using tubular photobioreactors.\n\nThe main species of microalgae grown as health foods are \"Chlorella sp.\" and \"Spirulina sp.\" The main forms of production occur in small scale ponds with artificial mixers. Novel bioactive chemical compounds can be isolated from microalgae like sulphated polysaccharides. These compounds include fucoidans, carrageenans and ulvans that are used for their beneficial properties. These properties are anticoagulants, antioxidants, anticancer agents that are being tested in research. Red microalgae are characterised by pigments called phycobiliproteins that contain natural colourants used in pharmaceuticals and/or cosmetics. Production of long chain omega-3 polyunsaturated fatty acids important for human diet can also be cultured through microalgal hatchery systems.\n\nBlue green alga was first used as a means of fixing nitrogen by allowing cyanobacteria to multiply in the soil. Nitrogen fixation is important as a means of allowing inorganic compounds such as nitrogen to be converted to organic forms which can then be used by plants. The use of cyanobacteria is an economically sound and environmentally friendly method of increasing productivity. Rice production in India and Iran have employed this method of using the nitrogen fixing properties of free living cyanobacteria to supplement nitrogen content in soils.\n\nMicroalgae are a source of valuable molecules such as isotopes i.e. chemical variants of an element that contain different neutrons. Microalgae can effectively incorporate isotopes of carbon (C), nitrogen (N) and hydrogen (H) into their biomass. C and N are used to track the flow of carbon between different trophic levels/food webs. Carbon, nitrogen and sulphur isotopes can also be used to determine disturbances to bottom dwelling communities that are otherwise difficult to study.\n\nCell fragility is the biggest issue that limits the productivity from closed photobioreactors. Damage to cells can be attributed to the turbulent flow within the bioreactor which is required to create mixing so light is available to all cells.\n\n"}
{"id": "21420436", "url": "https://en.wikipedia.org/wiki?curid=21420436", "title": "Cure monitoring", "text": "Cure monitoring\n\nReal-time computing of cure monitoring is an essential component for the control of the manufacturing process of composite materials. The rationale for cure monitoring relies on the various physical or chemical properties that can be used to follow the transformation of an initially liquid thermoset resin into its final rigid solid form (curing). The relationship between the monitoring output and the requirements for feedback-loop control is the subject of extended research activities including considerations of the modelling of the cure reaction.\n\n\n"}
{"id": "38458091", "url": "https://en.wikipedia.org/wiki?curid=38458091", "title": "Early February 2013 North American blizzard", "text": "Early February 2013 North American blizzard\n\nThe Early February 2013 North American blizzard was a powerful blizzard that developed from the combination of two areas of low pressure, primarily affecting\nthe Northeastern United States and parts of Canada, causing heavy snowfall and hurricane-force winds. The storm crossed the Atlantic Ocean, affecting Ireland and the United Kingdom. The nor'easter's effects in the United States received a Category 3 rank on the Northeast Snowfall Impact Scale, classifying it as a \"Major\" Winter Storm.\n\nThe first low-pressure system, originating from the Northern Plains of the United States, produced moderate amounts of snow across the Great Lakes region of the U.S. and Canada. The second low, originating across the state of Texas, produced heavy rains and flooding across much of the Southeast and Mid-Atlantic parts of the U.S. As the two systems merged off the Northeast coast on February 8, 2013, they produced heavy snowfall over a large region from North Jersey and inland from New York City through eastern New England up to coastal Maine and inland to Ontario.\n\nTotal snowfall in Boston, Massachusetts, reached , the fifth-highest total ever recorded in the city. New York City officially recorded of snow at Central Park, and Portland, Maine, set a record of . Hamden, Connecticut recorded the highest snowfall of the storm at , the second highest total in Connecticut was recorded in Milford at 38 inches. Many surrounding cities picked up at least . In addition to the significant snowfall totals, hurricane-force wind gusts were recorded, reaching in Nova Scotia, at Mount Desert Rock, Maine, and off the coast of Cuttyhunk, Massachusetts. Boston experienced a storm surge of , its fourth-highest. The storm affected Atlantic Canada after hitting the Northeastern United States.\n\nWatches and warnings were issued in preparation for the storm, and state governors declared states of emergency in all states in New England and in New York. Flights at many major airports across the region were canceled, and travel bans were put into place on February 8 in several states. Hundreds ended up stranded on Long Island late on February 8 as a result of the rapidly accumulating snowfall. A combination of strong winds and heavy, wet snow left 700,000 customers without electricity at the height of the storm. At least eighteen deaths were attributed to the storm.\n\nAtmospheric conditions leading up to the formation of the February 2013 nor'easter were rather anomalous and were conducive for cyclogenesis. Such conditions included the presence of a ridge over the North Atlantic and the strengthening of a trough in California. The impactful nature of the eventual winter storm event was first predicted in National Centers for Environmental Prediction  and Hydrometeorological Prediction Center  forecasts at 1200 UTC on February 6, when the accumulation of at least of snow in the ensuing hours was considered a moderate probability in parts of Wisconsin and Michigan. Fueled by energy originating from the Gulf of Alaska and carried by the polar jet stream, a low-pressure area formed as anticipated in Montana at 1200 UTC the next day. The cyclone tracked in a general east-northeastward direction throughout the day, reaching central Indiana by early on February 8.\n\nAs the low-pressure system over the United States Midwest developed, energy associated with the subtropical jet over the Mexican plateau tracked into the Gulf of Mexico, resulting in the formation of another low-pressure area just off the Texas coast on February 7. Tracking eastward, the system produced isolated severe weather across the United States Gulf Coast. Nearing noon that day, the cyclone moved into the Florida Panhandle. By February 8, the low-pressure system had moved into Georgia. Both the system in the Gulf of Mexico and the system in the Midwestern United States were supported by the same shortwave trough. Due to blocking steering patterns, both storms began to gravitate towards the Eastern Seaboard.\n\nThe HPC began issuing periodic storm summary bulletins on the two low-pressure areas at 0300 UTC on February 8. Their forecasts suggested that the system associated with the subtropical jet stream would rapidly intensify near the United States East Coast, later absorbing the system associated with the polar jet stream. A few hours later, the southern disturbance began a phase of barometric deepening as it tracked northeastward off the Outer Banks, while the northern system maintained an easterly course into northwestern Ohio. Beginning at 0000 UTC on February 9, the two systems began an extensive merging process, with the strengthening cyclone originating from the subtropics absorbing energy from the cyclone originating from the polar jet stream. This resulted in the cyclone off the United States East Coast to continue intensifying while the initial system to the northwest gradually diffused over the Mid-Atlantic states. Throughout the day the resulting system continued to track to the northeast, bringing along with it a large swath of snowfall that tracked across New England and into Canada. By 1200 UTC, the extratropical cyclone had strengthened enough to classify it as a bomb.\n\nAt 2100 UTC on February 9, the extratropical cyclone reached its peak intensity with a minimum barometric pressure of 968 mbar (hPa; 28.59 inHg) while located roughly off the coast of Massachusetts. After reaching peak intensity the system began to steadily weaken and depart the coast of New England, tracking near Nova Scotia early on February 10, upon which the HPC issued their final storm summary bulletin. Despite the discontinuance of bulletins, the extratropical cyclone continued to persist and assumed an easterly course. By February 13, the storm system had tracked just south of Iceland, bringing a cold and warm front into the British Isles. The system then began to curve northward through the Norwegian Sea over the next few days, before it was absorbed by a larger system on February 18.\n\nEnvironment Canada issued winter storm warnings for much of Ontario, while Toronto issued an extreme weather alert ahead of the storm. Several universities and colleges closed throughout Southern Ontario. Many flights were canceled in the region and Greyhound canceled bus service to New York. In the Maritimes, blizzard warnings were in effect for parts of New Brunswick and all of Nova Scotia and Prince Edward Island, with more than possible.\n\nBy late on February 7, 2013, winter storm warnings and winter weather advisories were issued for the northeastern United States, from the Upper Midwest to New England. There was also a blizzard warning for the New York metropolitan area, all of Connecticut, all of Rhode Island, and eastern Massachusetts, as well as southeast New Hampshire and coastal Maine. On February 8, blizzard warnings were expanded to include inland portions of southeast New Hampshire, and inland portions of Maine's coastal counties. By February 8, storm warnings and hurricane force wind warnings were in effect for the New England and Mid Atlantic waters, in addition to coastal flood warnings.\n\nIn Chicago, officials deployed 199 snow trucks, and the Illinois State Toll Highway Authority sent 182 plows for its road system. In the northeast United States, the storm threatened beaches and dunes in areas affected by Hurricane Sandy from the previous October. Due to the storm's threat, airlines canceled more than 2,700 flights, mostly for February 8. In New York, officials readied snow removal crews, with more than 250,000 tons of salt prepared. Schools were closed in Hartford, Connecticut and Providence, Rhode Island, among other cities.\n\nConnecticut Governor Dannel Malloy declared a state of emergency at 12:00 p.m. EST (1700 UTC) on February 8, and closed limited-access highways statewide at 4:00 p.m. Connecticut Light and Power and United Illuminating planned for 30 percent of customers to lose power in Connecticut, and hired out-of-state line crews to assist with power restoration. Governor Malloy on February 9 ordered all roads in the state closed except to essential vehicles.\n\nRhode Island Governor Lincoln Chafee declared a state of emergency on February 8, and issued a travel ban for interstates and other major highways effective 5 p.m. EST.\n\nMassachusetts Governor Deval Patrick declared a state of emergency at 12:00 p.m. EST on February 8, and banned vehicles from all public roads after 4:00 p.m.; it was the first statewide driving ban in Massachusetts since the Blizzard of 1978, which happened to strike the region exactly 35 years and 1 day earlier. The ban, issued via executive order, included a penalty of one year in jail and/or a five hundred dollar fine. The ban did not apply to emergency vehicles, hospital workers, or the media. The Massachusetts Bay Transportation Authority (MBTA) suspended subway, commuter rail, bus, and boat service at 3:30 p.m on February 8, and flights at Boston Logan International Airport were suspended at 4:00 p.m. Boston mayor Thomas Menino ordered schools to close and recommended that businesses shut down during the storm. The National Hockey League postponed a game between the Boston Bruins and the Tampa Bay Lightning that was scheduled to be played at Boston's TD Garden on February 9 due to the nor'easter; the game was originally set for a delayed start of 7 p.m.; before the game was first delayed to 7 p.m. it was set for 1 p.m.\n\nNew York Governor Andrew Cuomo declared a state of emergency on February 8. Maine Governor Paul LePage declared a state of emergency as well. New Jersey Governor Chris Christie activated his state's Emergency Operations Center on the morning of February 8.\n\nMore than 800 National Guard soldiers and airmen were activated in Connecticut, Massachusetts and New York to support actions needed on state roads.\n\nThe Met Office issued an amber warning for snow and ice for the weakened storm for between 09:05 till 23:55 GMT on Wednesday 13 February. The already existing Cold weather health alert was extended in anticipation of the storm.\nForecasters also warned of severe drifting of snow ahead of the gale force winds.\n\nThe storm brought significant snow to Southern Ontario from February 7 to February 8. Snowdrifts caused major disruptions on roads and freeways. Nearly a thousand flights were canceled at Pearson International Airport, as well as a numerous flights at Ottawa Macdonald-Cartier International Airport, Montreal–Pierre Elliott Trudeau International Airport, and Billy Bishop Toronto City Airport (Toronto island). \n\nThe storm brought snow and high winds to Toronto, Ottawa and Montreal, as well as extensive flooding to parts of Nova Scotia and New Brunswick. In Toronto, it was the largest snowfall in 5 years with accumulations of at the airport and in downtown (And up to in the suburbs). Power was disrupted for thousands of customers in Atlantic Canada, and dozens of flights were canceled at Halifax Stanfield International Airport on February 9. All Marine Atlantic ferries on February 9 were canceled. Nova Scotia was hit with wind gusts up to , and more than 21,000 Nova Scotia Power customers were without electricity as of the afternoon of February 9.\n\nSnow and rainfall began at 9 a.m. on February 8 across New England; by late that day, totals reached near Milton, Vermont. Later that evening, snow was falling at per hour in coastal Massachusetts, and at an extreme rate of over 6 inches per hour in parts of Connecticut. Thunder and lightning along with small hail were reported within the heavy band of snow in Connecticut. In Massachusetts, two neighborhoods in Quincy and about a dozen homes in Salisbury were evacuated due to coastal flooding. Voluntary evacuation orders were issued for oceanfront residents in Revere, Marshfield, and Scituate. Early on February 9, mandatory evacuations were ordered for Massachusetts coastal regions near the town of Hull due to possible flooding and high winds.\n\nParts of coastal New England experienced hurricane-force winds from the storm, with a peak gust of in Mount Desert Rock, Maine. A maximum gust of was recorded on the Isle of Shoals, a small group of islands off the New Hampshire coast. Winds of were recorded in Buzzards Bay near Cuttyhunk, Massachusetts, and Boston's Logan Airport measured a wind gust of .\n\nWhen the snow stopped, the highest amount recorded was in Hamden, Connecticut. The of snow in Portland, Maine, set a new record for the city from a single snowstorm, and Gorham, Maine set a record for the state of . It was the second highest total in Hartford, Connecticut, with ; Concord, New Hampshire, received of snow. Boston received of snow, the fifth highest total in the city from a single storm. It was the third-largest snowfall in Worcester, Massachusetts, at .\n\nAt Central Park in New York, of snow was recorded, while other parts of the city accumulated as much as 16 inches. Much of Long Island was hit with over of snow, with snowfall as high as being recorded in Medford, in Suffolk County.\n\nThe secondary low over Indiana produced light to moderate snowfall that extended to Wisconsin. Totals reached in Muskegon, Michigan. The storm previously dropped of snow in Chicago, and was reported in Beach Park, Illinois on February 8. Several traffic accidents were reported in Illinois.\n\nThe Nuclear Regulatory Commission said the Pilgrim Nuclear Power Plant in Plymouth, Massachusetts experienced an automatic shutdown at around 9:15 p.m. EST on February 8 after losing off-site power; there was no threat to the public.\n\nThe storm caused power outages, shortages at gas stations, and numerous car accidents across the region. As of 12:45 a.m. EST on February 9, 321,000 customers were without power in Massachusetts, 191,000 in Rhode Island, and 32,000 in Connecticut. By 10:30 a.m., more than 655,000 customers were without power due to the storm, including 405,000 in Massachusetts and 185,000 in Rhode Island. As of the evening of February 9, approximately 575,000 people were without power.\n\nAs of the late evening of February 9, 344,000 customers were without power in Massachusetts and 31,000 in Connecticut. About 390,000 remained without power in total in the Northeast. In total, utility companies reported about 700,000 customers were without power across nine states. As of Sunday evening, there were still about 205,000 customers in Massachusetts without power. Some areas were projected not to have power restored until Thursday February 14.\n\nThe Pilgrim Nuclear Power Plant lost power again on February 10, after an offsite outage had caused a shutdown on February 8. A spokeswoman for Entergy Corp., the Louisiana company that owns Pilgrim said, \"There's no worker or public safety concern,\" she said. \"We're troubleshooting the cause.\"\n\nMore than 6,300 commercial flights were canceled on February 8 and 9. In the New York City area, John F. Kennedy Airport, LaGuardia Airport and Newark Airport opened 7 a.m. EST on February 9. In Boston, Logan International Airport reopened at 11 p.m. EST on February 9. Also, in Hartford, Bradley International Airport reopened at 6 a.m. EST on February 10. As of Sunday night, Logan Airport had limited service.\n\nHundreds of cars got stuck on the Long Island Expressway in Suffolk County beginning the afternoon of February 8. On February 10, from 7 a.m. to 5 p.m. EST, about 26 miles of the Long Island Expressway were closed between exits 57 and 73 for snow removal. The closure was extended till 9 p.m. EST however, it was closed indefinitely as of 10:50 p.m. EST. The westbound side of the Long Island Expressway opened early Monday morning, while the eastbound lanes between exits 57 and 73 remained closed for snow removal. At around 7 a.m. on February 11, both lanes of the Long Island Expressway reopened.\n\nIn the afternoon of February 9, a portion of the roof of Smithtown Lanes bowling alley in Smithtown, New York collapsed. There were no injuries as the business was closed at the time. A house in Long Island suffered major structural damage due to heavy snow accumulation on the roof.\n\nThe Boston Bruins postponed their afternoon home game on Saturday, February 9 at the TD Garden against the Tampa Bay Lightning to Thursday, April 25. The Mohegan Sun resort casino in Connecticut postponed a Saturday night Bon Jovi concert, but the band played for the snowbound guests.\n\nThe United States Postal Service (USPS) suspended mail delivery in Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont, and parts of New York during and after the storm. Many school districts on Long Island, especially in Suffolk County closed schools for February 11. Boston Public Schools and several other school districts, including Cambridge and Somerville, remained closed on February 11 and 12. On February 10, President Obama declared a state of emergency for Connecticut.\n\nSouthern England saw up to half a foot (15 cm) of snow, causing travel disruption right across the region. Flooding also was widespread further North and West, where milder air had caused a thaw, and heavy rain fell, which led to the Met Office issuing many Flood warnings.\n\nAn 80-year-old woman in Hamilton, and an 80-year-old man in Niagara Falls collapsed and died while shoveling snow. A 49-year-old Oshawa man was killed in multi-vehicle collision in Pickering. A 57-year-old man from Ottawa died when his car crashed near Prescott.\n\nIn Connecticut an 81-year-old woman using a snowblower was hit by a car in Prospect, a man in Bridgeport was found dead under snow at his home, a man died after he suffered a heart attack while plowing snow in New Milford, a man from Shelton died while digging his truck out of the snow, and police in Danbury said a man was found dead on his back porch on February 9. In Maine, a 75-year-old man crashed his vehicle into a tree in Passadumkeag, then his pickup ended up in the Penobscot River. In Boston, Massachusetts, a 13-year-old boy and a man died separately due to carbon monoxide poisoning while inside cars because the tailpipes were blocked by snow. A man was killed in an automobile accident in Auburn, New Hampshire when his car spun out on a hill and collided with a tree, flattening the front end of his SUV and crushing him.\n\nIn New York, a man was plowing his driveway with a tractor on February 8 in Germantown, when the tractor went off the edge of the road and fell on top of him. In Poughkeepsie, a pedestrian was struck by a car and killed. A 58-year-old man in Selden died while clearing snow at his apartment complex. On February 11, three more deaths, all in Northport, New York, were blamed on the storm, bringing the blizzard death toll in Suffolk County to four.\n\nThe Weather Channel dubbed the storm \"Winter Storm Nemo\", in keeping with a list of names they have given to some winter storms since 2012. Hartford, Connecticut, CBS affiliate WFSB named the storm \"Blizzard Charlotte\", in keeping with a long-standing station tradition of naming major winter storms affecting Connecticut dating back to the early 1970s. The National Weather Service however, has rejected naming winter storms. Other names include the \"Blizzard of 2013\" (or the \"Blizzard of '13\") and \"Blizzard 2013\".\n\n\n"}
{"id": "49401135", "url": "https://en.wikipedia.org/wiki?curid=49401135", "title": "Early January 2014 nor'easter", "text": "Early January 2014 nor'easter\n\nThe Early January 2014 nor'easter was a major winter storm that affected much of the East Coast with snow, and frigid temperatures following the storm. The storm had dumped up to of snow in some areas, especially around Boston, Massachusetts.\n\nLate on December 30, a weak clipper system moved southward from Canada. The system produced light snow in the Midwest around areas like Minneapolis, Minnesota with only light accumulations.\n\nStarting on New Year's Day, however, the clipper began to strengthen and take an eastward turn. It would produce more moderate to heavy snow around areas like Cincinnati, Ohio, with accumulations of roughly 4-6 inches. The storm then continued to move eastwards towards the East Coast, where it would continue to intensify.\n\nEarly on January 2, the system started to become more complex as a new low pressure area had formed in the Gulf of Mexico along a stationary front. Winter storm warnings began to be issued by roughly mid-day from eastern New England to as far west as Pennsylvania as the storm closed in on the Northeast. By 3:00 p.m, the northern low began to weaken as the southern one began to take over as it continued to pull moisture from the Gulf of Mexico. This was the start of the storm's transition to a nor'easter.\n\nLater that night, the low pressure began to strengthen off the Southeast coast and move out to sea as it dumped snow on its northwestern side over the Northeastern United States and New England. Snow continued to fall into the early hours of January 3 even as it pulled away from the coastline.\n\nAfter it had left the coast early that morning, it continued to strengthen and turn to the northeast, and late on January 4, it reached its peak intensity of while situated near Greenland. It was then absorbed into another extratropical cyclone, which later developed into Cyclone Christina which brought high winds to western Europe several days later.\n\nAfter the storm passed, an arctic front associated with it passed through the region, bringing record cold temperatures in the eastern half of the United States.\n\nOn January 3, Boston had a temperature of with a wind chill, and over of snow. Boxford, Massachusetts recorded . Fort Wayne, Indiana had a record low of . In Michigan, over of snow fell outside Detroit and temperatures around the state were near or below . New Jersey had over of snow, and schools and government offices closed. Over a dozen deaths were attributed to the cold wave, with dangerous roadway conditions and extreme cold cited as causes.\n\nEvan Gold of weather intelligence firm Planalytics called the storm and the low temperatures the worst weather event for the economy since Hurricane Sandy just over a year earlier. 200 million people were affected, and Gold calculated the impact at $5 billion. $50 to $100 million was lost by airlines which cancelled a total of 20,000 flights after the storm began on January 2. JetBlue took a major hit because 80 percent of its flights go through New York City or Boston. Tony Madden of the Federal Reserve Bank of Minneapolis said with so many schools closed, parents had to stay home from work or work from home. Even the ones who could work from home, Madden said, might not have done as much. Not included in the total were the insurance industry and government costs for salting roads, overtime and repairs. .\n\n"}
{"id": "10382", "url": "https://en.wikipedia.org/wiki?curid=10382", "title": "Electrothermal-chemical technology", "text": "Electrothermal-chemical technology\n\nElectrothermal-chemical (ETC) technology is an attempt to increase accuracy and muzzle energy of future tank, artillery, and close-in weapon system guns by improving the predictability and rate of expansion of propellants inside the barrel.\n\nAn electrothermal-chemical gun uses a plasma cartridge to ignite and control the ammunition's propellant, using electrical energy to trigger the process. ETC increases the performance of conventional solid propellants, reduces the effect of temperature on propellant expansion and allows for more advanced, higher density propellants to be used.\n\nThe technology has been under development since the mid-1980s and at present is actively being researched in the United States by the Army Research Laboratory, Sandia National Laboratories and defense industry contractors, including FMC Corporation, General Dynamics Land Systems, Olin Ordnance, and Soreq Nuclear Research Center. It is possible that electrothermal-chemical gun propulsion will be an integral part of US Army's future combat system and those of other countries such as Germany and the United Kingdom. \nElectrothermal-chemical technology is part of a broad research and development program that encompasses all electric gun technology, such as railguns and coil guns.\n\nThe constant battle between armour and armor-piercing round has led to continuous development of the main battle tank design. The evolution of American anti-tank weapons can be traced back to requirements to combat Soviet tanks. In the late 1980s, it was thought that the protection level of the Future Soviet Tank (FST) could exceed 700 mm of rolled homogeneous armour equivalence at its maximum thickness, which was effectively immune against the contemporary M-829 armour piercing fin stabilized discarding sabot. In the 1980s the most immediate method available to NATO to counter Soviet advances in armour technology was the adoption of a 140 mm main gun, but this required a redesigned turret that could incorporate the larger breech and ammunition, and it also required some sort of automatic loader. Although the 140 mm gun was considered a real interim solution it was decided after the fall of the Soviet Union that the increase in muzzle energy it provided was not worth the increase in weight. Resources were therefore spent on research into other programs that could provide the needed muzzle energy. One of the most successful alternative technologies remains electrothermal-chemical ignition.\n\nMost proposed advances in gun technology are based on the assumption that the solid propellant as a stand-alone propulsion system is no longer capable of delivering the required muzzle energy. This requirement has been underscored by the appearance of the Russian T-90 main battle tank. Even the elongation of current gun tubes, such as the new German 120 mm L/55, which was introduced by Rheinmetall is considered only an interim solution as it does not offer the required increase in muzzle velocity. Even advanced kinetic energy ammunition such as the United States' M-829A3 is considered only an interim solution against future threats. To that extent the solid propellant is considered to have reached the end of its usefulness, although it will remain the principal propulsion method for at least the next decade until newer technologies mature. To improve on the capabilities of a solid propellant weapon the electrothermal-chemical gun may see production as early as 2016.\n\nETC technology offers a medium-risk upgrade and is developed to the point that further improvements are so minor that it can be considered mature. The lightweight American 120 mm XM-291 came close to achieving 17 MJ of muzzle energy, which is the lower-end muzzle energy spectrum for a 140 mm gun. However, the success of the XM-291 does not imply the success of ETC technology as there are key parts of the propulsion system that are not yet understood or fully developed, such as the plasma ignition process. Nevertheless, there is substantial existing evidence that ETC technology is viable and worth the money to continue development. Furthermore, it can be integrated into current gun systems.\n\nAn electrothermal-chemical gun uses a plasma cartridge to ignite and control the ammunition's propellant, using electrical energy as a catalyst to begin the process. Originally researched by Dr. Jon Parmentola for the U.S. Army, it has grown into a very plausible successor to a standard solid propellant tank gun. Since the beginning of research the United States has funded the XM-291 gun project with USD 4,000,000, basic research with USD 300,000, and applied research with USD 600,000. Since then it has been proven to work, although efficiency to the level required has not yet been accomplished. ETC increases the performance of conventional solid propellants, reduces the effect of temperature on propellant expansion and allows for more advanced, higher density propellants to be used. It will also reduce pressure placed on the barrel in comparison to alternative technologies that offer the same muzzle energy given the fact that it helps spread the propellant's gas much more smoothly during ignition. Currently, there are two principal methods of plasma initiation: the flashboard large area emitter (FLARE) and the triple coaxial plasma igniter (TCPI).\n\nFlashboards run in several parallel strings to provide a large area of plasma or ultraviolet radiation and uses the breakdown and vaporization of gaps of diamonds to produce the required plasma. These parallel strings are mounted in tubes and oriented to have their gaps azimuthal to the tube's axis. It discharges by using high pressure air to move air out of the way. FLARE initiators can ignite propellants through the release of plasma, or even through the use of ultraviolet heat radiation. The absorption length of a solid propellant is sufficient to be ignited by radiation from a plasma source. However, FLARE has most likely not reached optimal design requirements and further understanding of FLARE and how it works is completely necessary to ensure the evolution of the technology. If FLARE provided the XM-291 gun project with the sufficient radiative heat to ignite the propellant to achieve a muzzle energy of 17 MJ one could only imagine the possibilities with a fully developed FLARE plasma igniter. Current areas of study include how plasma will affect the propellant through radiation, the deliverance of mechanical energy and heat directly and by driving gas flow. Despite these daunting tasks FLARE has been seen as the most plausible igniter for future application on ETC guns.\n\nA coaxial igniter consists of a fully insulated conductor, covered by four strips of aluminum foil. All of this is further insulated in a tube about 1.6 cm in diameter that is perforated with small holes. The idea is to use an electrical flow through the conductor and then exploding the flow into vapor and then breaking it down into plasma. Consequently, the plasma escapes through the constant perforations throughout the insulating tube and initiates the surrounding propellant. A TCPI igniter is fitted in individual propellant cases for each piece of ammunition. However, TCPI is no longer considered a viable method of propellant ignition because it may damage the fins and does not deliver energy as efficiently as a FLARE igniter.\n\nThe XM-291 is the best existing example of a working electrothermal-chemical gun. It was an alternate technology to the heavier caliber 140 mm gun by using the dual-caliber approach. It uses a breech that is large enough to accept 140 mm ammunition and be mounted with both a 120 mm barrel and a 135 mm or 140 mm barrel. The XM-291 also mounts a larger gun tube and a larger ignition chamber than the existing M256 L/44 main gun. Through the application of electrothermal-chemical technology the XM-291 has been able to achieve muzzle energy outputs that equate that to a low-level 140 mm gun, while achieving muzzle velocities greater than those of the larger 140 mm gun. Although the XM-291 does not mean that ETC technology is viable it does offer an example that it is possible.\n\nETC is also a more viable option than other alternatives by definition. ETC requires much less energy input from outside sources, like a battery, than a railgun or a coilgun would. Tests have shown that energy output by the propellant is higher than energy input from outside sources on ETC guns. In comparison, a railgun currently cannot achieve a higher muzzle velocity than the amount of energy input. Even at 50% efficiency a rail gun launching a projectile with a kinetic energy of 20 MJ would require an energy input into the rails of 40 MJ, and 50% efficiency has not yet been achieved. To put this into perspective, a rail gun launching at 9 MJ of energy would need roughly 32 MJ worth of energy from capacitors. Current advances in energy storage allow for energy densities as high as 2.5 MJ/dm³, which means that a battery delivering 32 MJ of energy would require a volume of 12.8 dm³ per shot; this is not a viable volume for use in a modern main battle tank, especially one designed to be lighter than existing models. There has even been discussion about eliminating the necessity for an outside electrical source in ETC ignition by initiating the plasma cartridge through a small explosive force.\n\nFurthermore, ETC technology is not only applicable to solid propellants. To increase muzzle velocity even further electrothermal-chemical ignition can work with liquid propellants, although this would require further research into plasma ignition. ETC technology is also compatible with existing projects to reduce the amount of recoil delivered to the vehicle while firing. Understandably, recoil of a gun firing a projectile at 17 MJ or more will increase directly with the increase in muzzle energy in accordance to Newton's third law of motion and successful implementation of recoil reduction mechanisms will be vital to the installation of an ETC powered gun in an existing vehicle design. For example, OTO Melara's new lightweight 120 mm L/45 gun has achieved a recoil force of 25 t by using a longer recoil mechanism (550 mm) and a pepperpot muzzle brake. Reduction in recoil can also be achieved through mass attenuation of the thermal sleeve. The ability of ETC technology to be applied to existing gun designs means that for future gun upgrades there's no longer the necessity to redesign the turret to include a larger breech or caliber gun barrel.\n\nSeveral countries have already determined that ETC technology is viable for the future and have funded indigenous projects considerably. These include the United States, Germany and the United Kingdom, amongst others. The United States' XM360, which is planned to equip the Future Combat Systems Mounted Combat System light tank and may be the M1 Abrams' next gun upgrade, is reportedly based on the XM291 and may include ETC technology, or portions of ETC technology. Tests of this gun have been performed using \"precision ignition\" technology, which may refer to ETC ignition.\n\n"}
{"id": "36379439", "url": "https://en.wikipedia.org/wiki?curid=36379439", "title": "Excon Worldwide, Inc.", "text": "Excon Worldwide, Inc.\n\nExcon Worldwide, Inc. is a Canadian energy, commodities, equities, and services company based in Toronto with other offices worldwide. Its main location is in Toronto, Ontario, Canada. The group comprises many specialized international offices and subsidiaries. Excon Worldwide has more than seven thousand employees around North America and participates in markets around the world, including Canada, the United States, the UK, Australia, New Zealand, Asia, and the Middle East. Excon Worldwide has offices around the world including Toronto, Los Angeles, Las Vegas, New York City, Houston, Texas and Hong Kong.\n\nThe Excon Worldwide, Inc. brand is used by the commodity trading divisions. The mortgage, insurance, and real estate development divisions are managed under a different brand. The only subsidiaries which kept the same name after being acquired by Excon Worldwide, Inc. were Cannix Energy Corporation and Excon Online.\n\nExcon Worldwide is one of the largest commodity trading companies in North America. Excon Worldwide employs approximately 12,500 staff.\n\nExcon Worldwide was launched in 1987 as an energy trading and communications company. Since then, Excon has branched out into mortgages, insurance, and real estate development services. Excon opened its first American location in Los Angeles, the second location which followed was its Las Vegas executive office.\n\nExcon Worldwide owns several subsidiaries which have been renamed to fit under the Excon brand name. These subsidiaries include; Excon Mortgages, Excon Insurance Group, and Excon Development Group. The trading operations fit under the Excon brand name as well.\n\nThe company, known previously as TTL Mortgage International, was founded on October 17, 1968.\n\nIn September 1990, Excon Worldwide announced that it would buy out TTL Mortgage International and relocate its headquarters to Los Angeles. These plans were suspended in January 1991, when the company announced its Los Angeles branch was moving to its new offices at 811 Wilshire Blvd, Los Angeles.\n\nOn December 21, 1999, Excon Worldwide bought all of the insurance operations of Ross & White Insurance Company. The company announced plans to complete the rebranding of Ross & White branches to Excon by late 2000. Since 2001 all Ross & White branches and offices have been rebranded Excon Insurance Group.\n\nChief executive Jacob White was flown in from Seattle to New York on the day the acquisition was completed, and eventually received a $5.5 million sign-on bonus and cash severance of $8.1 million after signing up as Excon Insurance Group's CEO.\n\nIn 2003, Excon Worldwide bought Las Vegas-based real estate development company Nevada Realty Developers Corporation, bringing on current chairman Jonathan Dimon as President and COO and designating him as CEO Gregory Harrilson's successor. Dimon's pay pegged at 90% of Harrilson's. Dimon quickly made his influence felt by embarking on a cost-cutting strategy, and replaced former Excon Worldwide, Inc. executives in key positions with Nevada Realty Developers Corporation executives-many of whom were with Dimon since the beginning of his career at Nevada Realty Developers Corporation. Dimon became CEO in January 2006 and Chairman in December 2006. When Nevada Realty Developers Corporation was bought out by Excon Worldwide then CEO Gregory Harrilson was informed to change the name of the company to Excon Development Group, this allowed the real estate development giant to fit under Excon's branding web, and allowed the company to issue real estate development loans to other development companies for Excon Worldwide.\n\nCannix Energy Corporation is an energy and natural gas corporation based in Houston, Texas. The company specializes in the energy industry by investing or developing energy power plants around the world. The company also specializes in the purchase and resale of energy treating it as a commodity. Founded in September 1971 Cannix Energy grew into one of the most profitable corporations based in Nebraska, the company employs over 500 people who specialize in the energy sector. In March 2005, Excon Worldwide acquired Cannix Energy for $602,070,000.\n\nExcon Online is an internet company which specializes in internet advertising, programming, web development, and is an internet service provider. The company was founded in late 2009 in Toronto, and was sold in 2011 to Excon Worldwide. Excon Online offers internet services, web development, programming, internet advertising, mobile advertising, Search Engine Optimization (SEO).\n\nAlthough the old Excon headquarters were located in Mississauga, Ontario, the current world headquarters for Excon are located at a new downtown office downtown Toronto.\n\nThe bulk of North American operations take place in two office locations, one in Toronto and the other in Calgary. The sales and trading operations take place in its Montreal head office. The operations involved in energy take place in Calgary and Los Angeles. Excon's subsidiary Cannix Energy Corporation is based in Houston, Texas.\n\nThe mortgage operations for Excon Worldwide are headquartered in New York City at the 1221 Avenue of the Americas.\n\nThe Asia Pacific headquarters for Excon Worldwide are ilocated in Hong Kong.\n\nExcon Worldwide, Inc. offers a variety of services; insurance, mortgages, real estate development internet services, and natural gas services. In early 2000s Excon Worldwide, Inc. began offering insurance and mortgage services around the world. In 2005 Excon Worldwide, Inc. purchased energy giant Cannix Energy Corporation, this put Excon Worldwide, Inc. in the energy wholesale business.\n\nExcon Worldwide is a member of the Business Energy Advisor Program.\n\nExcon Worldwide partners with CIBC, BMO Capital Markets, GoldStone Investments, and TD Canada Trust\n\nExcon Worldwide has won several awards for business and customer satisfaction, including the Imagine Canada Award, the HSBC International Business Award, and The Great Canadian Award. In 2009 Excon Worldwide was ranked one of the top 50 managed companies in Canada.\n"}
{"id": "26088168", "url": "https://en.wikipedia.org/wiki?curid=26088168", "title": "February 5–6, 2010 North American blizzard", "text": "February 5–6, 2010 North American blizzard\n\nThe February 5–6, 2010 North American blizzard, commonly referred to as \"Snowmageddon\", was a paralyzing and crippling blizzard that had major and widespread impact in the Northeastern United States. The storm's center tracked from Baja California Sur on February 2nd, 2010 to the East coast on February 6, 2010, before heading east out into the Atlantic. Effects were felt to the north and west of this track in northern Mexico, California, and the Southwestern, Midwestern, Southeastern, and most notably Mid-Atlantic States. Severe weather, including extensive flooding and landslides in Mexico, and historic snowfall totals in every one of the Mid-Atlantic states, brought deaths to Mexico, New Mexico, Virginia, Pennsylvania and Maryland.\n\nMost crippling was the widespread 20 to 35 in (50 to 90 cm) of snow accumulated across southern Pennsylvania, the Eastern Panhandle of West Virginia, northern Virginia, Washington, D.C., Maryland, Delaware, and southern New Jersey, bringing air and Interstate Highway travel to a halt. While rail service south and west of Washington, D.C. was suspended, rail travel between D.C. and Boston was available with limited service. Blizzard conditions were reported in a relatively small area of Maryland, but near-blizzard conditions occurred across much of the Mid-Atlantic region.\n\nThis event was the second of four nor'easters during the 2009-2010 winter that brought heavy snow to enough of the Northeast's population to be numerically recognized by NOAA's NESIS intensity rating. The first and third of these systems, the December 2009 Nor'Easter and the February 9–10, 2010 North American blizzard, respectively, combined with this event to bring the snowiest winter on record to much of the Mid-Atlantic. Additionally, this event was the second of three major Mid-Atlantic snowstorms that occurred over a 12-day period; each subsequent storm focused its heaviest snow slightly farther north: the January 30, 2010 storm (not recognized by NESIS) dropped more than a foot of snow across Virginia and the lower Chesapeake Bay region, while the February 9–10, 2010 North American blizzard bulls-eyed the Maryland-Pennsylvania border with as much as 38.3 inches.\n\nThe main storm system originated in the Pacific Ocean, bringing heavy rain and mountain snow to California and Arizona on February 2. The storm center moved east through northern Mexico on February 3. The system produced over one foot of snowfall in the higher elevations and the eastern plains of New Mexico, shutting down major highways including Interstate 40 east of Albuquerque for several hours on February 3. The storm's center then advanced across Texas to the Louisiana Gulf Coast on February 4, while dropping rain and snow in Oklahoma and northern Texas, and severe thunderstorms further south. Meanwhile, a second, more-northern disturbance tracked from the central Rockies to the lower Missouri River Valley, bringing light snow showers to Montana, the Dakotas, parts of Minnesota, Wisconsin, Iowa, and Illinois.\n\nOn February 5, the two systems phased together, resulting in a band of heavy snow across Illinois, Indiana, Ohio, and Pennsylvania. That evening, the northern system's energy was absorbed into the main southern circulation, promoting fast intensification. Heavy snow subsequently developed over the Mid-Atlantic states as the storm's center tracked across North Carolina towards the Atlantic Coast.\nAn antecedent and nearly-stationary upper-level trough over the Maritime Provinces of Canada served to block the storm system from following the traditional northeast track into New England. Instead, during the AM hours of February 6th, the storm center slowed its northeasterly movement as it continued to deepen east of Virginia Beach, before it eventually was forced eastward. The blocking pattern was reflected on the storm's snowfall map by a sharp northern gradient in northern New Jersey and by the axis of heaviest snow running WNW-ESE through Maryland and Delaware (opposed to the SW-NE pattern found from most Nor'Easters). Only moderate accumulations reached the southern suburbs of New York City, with no more than light snow falling in the city itself. Upstate New York and New England were spared from this system, receiving little more than isolated snow flurries in southern sections. Easterly winds and onshore flow contributed to light snow accumulations of less than one inch in Boston, Cape Cod, and parts of coastal Rhode Island.\n\nAccording to the blog of Weather Channel senior meteorologist Stu Ostro, the storm formed from \"Miller Type B\" cyclogenesis: a storm centered over the Ohio river runs into a blocking ridge and redevelops along the Carolina coast. (This differed from the North American blizzard of 2009, of \"Miller Type A\" cyclogenesis where a storm center develops over the eastern Gulf of Mexico and strengthens while tracking north to a latitude of greater temperature contrast.) This storm was carrying an enormous amount of moisture drawn from both the Gulf of Mexico (as seen on February 3 satellite imagery over Mexico), and from the Atlantic (as seen in radar imagery from early on February 5). Ostro characterized the storm as having \"strong dynamics\" and expected the snowfall to be of long duration, typically leading to large accumulations.\n\nMedia reports emphasized the magnitude of the storm, giving rise to many nicknames for it including Snowmageddon and Snowpocalypse.\n\nThe \"Capital Weather Gang\" blog on \"The Washington Post\" website ran an online poll on February 4, 2010, asking for reader feedback prior to the blizzard, and several blogs, including the paper's own blog, followed that up by using either \"Snowmageddon\" and/or \"Snowpocalypse\" during the following days, before, during, and after the storm hit.\n\nThe \"Washington Post\" also popularized other portmanteaus, including \"snOMG\" (from OMG) and \"kaisersnoze\" (from Keyser Soze), in response to the February snowstorms.\nDuring the evening preceding the first blizzard hitting Washington, D.C., most of the United States federal government closed, and press coverage continued to characterize the storm using either \"Snowmageddon\", \"Snowpocalypse\", or both. The phrase was later popularized by the President of the United States, Barack Obama, on February 8, 2010, who used the term while speaking at the Democratic National Committee's meeting.\n\nAn unofficial storm total snowfall observation of 39.0\" was made from Riverwood, MD (in Frederick County) on CoCoRaHS, and later referenced in the February 2010 Washington Baltimore Climate Review (not published electronically). However NOAA/NWS products list the 38.3\" total at 2WSW Elkridge, MD as the \"storm's greatest (so far)\".\n\nPittsburgh with 21.1\", was the first major city to experience the storm's heavier snowfall, separating sub-20\" amounts over Indiana and Ohio from 20-35\" readings found in the Laurel Highlands and east. This was Pittsburgh's 4th greatest snow event since records began in 1871. Areas south of Pittsburgh received up to 26\" of snowfall. Although initially forecast to bring only 4-8\" of snow to the area, the storm's track farther to the north lead to the explosive accumulations. The National Weather Service in Pittsburgh office recorded 7\" of snow over 700P-1159P February 5 and 5.3\" over 300A-600A on February 6.\n\nThe swath of heaviest snowfall then crossed the Appalachians and squeezed between Washington, D.C. and Baltimore. The District of Columbia's totals were generally near 20\", ranging from 18 to 28\". Baltimore City's totals were around 25\", ranging from 21 to 28\". Dulles\n\nInternational Airport's 32.9\" set all-time records (since 1962) for most 2-day and 3-day snowfall, shattering the old records from the Blizzard of 1996 of 23.2\" and 24.6\", respectively. Dulles received 17.5\" on February 6, however, making the day's total only the 3rd greatest 24-hour snowfall amount. Baltimore Washington International Airport, despite being near numerous 30\"+ reports, saw a relatively tame 25\". This 2-day total was 2nd greatest (since 1892), behind 26.3\" from the 1922 Knickerbocker Storm. Reagan National Airport's 17.8\" was one of the metro region's lowest, but still ranked as Washington's 4th greatest (since 1871) in both 2-day and 3-day totals.\n\nAdditional local reports from Maryland include: Edgemere, 35.4\"; Clarksville, 34.9\"; Crofton, 34.0\"; Columbia, 33.8\"; Laurel, 32.9\"; Pasadena, 31.0\"; Dundalk, 30.5\"; Ellicott City, 30.2\"; Frederick, 29.0\"; Olney, 28.0\"; Germantown, 27.4\"; and Catonsville, 22.9\".\n\nPhiladelphia's 28.5\" ranked the storm as the city's 2nd greatest on record (since 1872), falling not far behind the Blizzard of 1996's 30.7\". This made for Philadelphia's first winter with multiple 20\"+ storms, following the 23.2\" from the North American blizzard of 2009. Wilmington, DE's 25.8\" was their greatest storm total of all-time (since 1894), passing the 1996 storm's 22\".\n\nSouth Jersey saw as much as 29\" of snow; Vineland reported 19.8\". In South Central Pennsylvania, the areas of Harrisburg, Lancaster and York reported receiving over 18\" of snow.\n\nMany counties, including the rural areas across the Appalachian mountains and Delmarva averaged over two feet of snow.\n\nThe storm was well spotted by the winter storm reconnaissance (WSR) program at National Center of Environmental Predictions (NCEP). Two aircraft missions were deployed over the Pacific regions on February 1, 2010. Accurate measurement in the cloudy regions were taken, the data were assimilated by the global forecast models in different numerical forecast prediction centers.\n\nThe WSR program is led by Dr. Yucheng Song from the National Centers for Environmental Prediction (NCEP).\n\n\"Source: CoCoRaHS\"\n\n\"Source: NWS Washington/Baltimore Public Information Statement & Maps\"\n\n\"Source: NWS Philadelphia/Mt. Holly Public Information Statement & Map\"\n\n\"Source: NWS State College, PA Public Information Statement & Maps\"\n\nFreak winter rains across Mexico collapsed hillsides, sent rivers over their banks and left at least 15 people dead, officials said on Friday, February 5. The rain, which began early in the week and peaked on Thursday, February 4, had relented by Friday morning, providing officials with their first good look at the damage. More than half of the country was affected. The hardest area hit by the storm was the western state of Michoacán, a famous reserve for monarch butterflies, where at least 13 people were killed by landslides and flooding. An unknown number of people were missing Friday. Other areas that were hard hit by flooding was the eastern Mexico City borough of Iztapalapa and municipalities in eastern State of Mexico such as Ciudad Nezahualcóyotl and Ecatepec de Morelos. The rain broke records for February in Michoacán, the State of Mexico and Mexico City, with twice the normal amount for the entire month falling in 24 hours. There was a silver lining: Officials said the copious rain had filled reservoirs outside Mexico City that are a key source of water for the metropolis. Water shortages had forced on-and-off rationing since last summer. Water authorities state that most of the country now has a \"positive balance\" in reservoirs with of water added to reservoirs.\n\nThe storm affected Arizona and New Mexico from February 1 to 4. Up to of snow fell in the mountains east of Albuquerque, New Mexico, while snow accumulations in the city varied from less than near downtown to on the West Mesa and in the far northeast foothills. Ice-covered roadways caused numerous accidents – including one fatal crash near Gallup – shutting down Interstate 40 through Tijeras Canyon and between Grants and Gallup for several hours on February 3.\n\nProlonged rains from Thursday morning through Thursday evening (February 4), produced widespread rainfall totals of – statewide with flooding reported in portions of Central and Southern Mississippi. The capital city of Jackson broke a daily rainfall record with of rainfall. Power outages were reported in North Carolina's mountain counties as the winter storm brought a mixture of snow, sleet and freezing rain to much of the state and rain to the rest, with about 40,000 outages late Friday afternoon (5 February). A drenching rain fell early Friday in the Charlotte and Atlanta area and then transitioned to a few inches of snow later in the day, while several inches of snow accumulated farther north. Parts of central and eastern North Carolina were under flood watches in advance of significant rainfall of up to .\n\nHeavy snowfall occurred in Illinois, Indiana, and Ohio February 4–6. Snowfall totals ranged from to over across the region. Drifts of up to were reported in central Indiana.\n\nThe heavy snow, ice storms and low temperatures of January the 26th led to Interstate 90 being closed from Chamberlain, South Dakota to the Minnesota border. On the nightfall on Monday, Interstate 29 was closed from Sioux Falls to the North Dakota border. Power company officials estimated that about 7,600 customers in South Dakota and 100 in North Dakota did not have power on Monday. Some phone systems also experienced brief telecommunications outages. Kristi Truman, director of the North Dakota Office of Emergency Management was concerned about failing water and power supplies.\n\nIn the Dakotas a number of Indian Reservations were left without power or running water.\n\n\"There's been winters this bad before, but not with rain so bad it freezes the power lines and snaps the poles\", said Joseph Brings Plenty, the 38-year-old chairman of the Cheyenne River Sioux tribe.\n\nPower outages began with a storm in December knocking down around 5,000 power poles, and was accelerated by an ice storm Jan. 22 knocking down another 3,000 power lines on the reservation.\n\nAmong the tribes of South Dakota who suffered from the multiple storms were Cheyenne River Sioux, Crow Creek Sioux Tribe, Flandreau-Santee Sioux Tribe, Lower Brule Sioux Tribe, Rosebud Sioux Tribe, Sisseton-Wahpeton Oyate and Standing Rock Sioux Tribe.\n\nThe Episcopal Church stepped in to help the reservations residents survive the winter.\n\nOn February 1, utility crews worked overtime to get power back to the 14,000 residents of Cheyenne River Sioux Reservation. The wind chill factor averaged about 25 °F below zero and there was about 1 foot of snow on average.\n\nPower outages in both the Dakotas power covered only 100 rural electric customers and minimal numbers in Bismarck, North Dakota by February, 5.\n\nThe United States Government implemented an unscheduled leave policy for federal employees on Friday February 5 and shut down four hours early in an effort to clear metropolitan Washington before substantial snow accumulations began. Numerous school districts in the metro DC area announced closures for Friday February 5 well in advance, although District of Columbia Public Schools and some Maryland schools held a half day of class. Many districts had used all their built-in snow days and some began scheduling classes on upcoming holidays (for example, Fairfax County on February 15, the Monday of President's Day weekend). Late on Sunday February 7, the Office of Personnel Management announced that the United States Government would again be closed on Monday February 8, with only emergency/essential personnel required to report, and numerous school districts again canceled classes between February 9–11.\n\nAs of shortly after midnight on February 6, more than 50,000 homes and businesses in the Washington, D.C. metropolitan area were without electricity. In northern Virginia, the total was 33,000 and in northern Maryland and the District of Columbia the total was 19,000. Roadways were blanketed with snow, Metro bus service ended at 9 pm EST, and above ground Metro rail service had also ended. Flights were canceled at the Washington-Baltimore area's three main airports and at Philadelphia International Airport. Delta Air Lines had suspended flights in and out of Washington, Baltimore, and Philadelphia.\nIn Maryland, the Maryland Transit Administration ran special snow trains on its heavy rail and light rail lines to keep tracks clear. Delaware Gov. Jack Markell declared a state of emergency Friday night and ordered all vehicles off the roads by 10 p.m. EST (this was in addition to an earlier state of emergency declared by Virginia Gov. Bob McDonnell and snow emergencies declared in the District of Columbia and some Maryland counties). Maryland was under a state of emergency as of mid-day on February 6, as state and county road crews said they were struggling to keep even one lane open on major roads and 151,000 customers were without power in Maryland, including 34,000 Baltimore Gas and Electric Co. customers in the region. Cars were left abandoned on highways, trees came down and Humvees were used to ferry patients to local hospitals. The United States Postal Service decided to cancel mail delivery and collection in the affected areas for Saturday, February 6.\nThe weight of the snow caused several roof collapses throughout the Washington area. Most notably, the roof of a hangar housing private jets at Dulles International Airport caved in twice due to the snowfall. Also reported were the collapse of a house roof in Northeast, Washington, D.C., a house in the Luxmanor Area in Rockville, Maryland, which collapsed from a fire that resulted from trying to melt the snow from the roof, the entire Prince William Ice Center in Dale City, Virginia,\nand the total collapse of a warehouse in California, Maryland. In none of the four cases were there reports of injuries. Around 2 pm EST on February 6, DC fire and EMS personnel responded to a church collapse in Northeast DC–preliminary reports from the scene were that the weight of the heavy snow caused the 1- or -story wooden building to completely collapse, and subsequent gas leaks caused some neighbors to be evacuated. The roof of St. John's School in Hollywood, Maryland, also collapsed, as did the roof of the truck bay at the volunteer fire station in Bailey's Crossroads, Virginia, on the morning of Monday February 8, but there were no injuries.\n\nAmtrak shut down much of their service in the region, canceling its \"Silver Meteor\", \"Silver Star\", \"Crescent\", \"Carolinian\", \"Palmetto\", and \"Capitol Limited\", as well as canceling \"Cardinal\" service past Huntington, West Virginia However services north continued to operate through the teeth of the storm on a limited schedule.\n\nIn Pittsburgh, both the impact and severity of the storm caught many by surprise. Snow began falling in earnest late Friday morning. The sudden onset of the storm forced many local school districts, especially districts south of the city, to close early due to rapidly deteriorating road conditions; this is an extremely uncommon event for schools in southwestern Pennsylvania. Nearly all schools, including the Pittsburgh Public Schools, cancelled classes the following week. Most local universities were also forced to cancel classes for much of the following week due to the storm's effects. Additionally, over 130,000 people in the Pittsburgh area were without power as a result of the heavy, wet snow. For many residents, power was not restored until Monday, February 15.\n\nIn New Jersey, the southern part of the state got hit with areas of 20+ inches of snow and some parts of the north got 0 to trace. Many reports of sleet and snow were coming in by the thousands in the state of NJ.\n\nMany cross country skiers were spotted throughout Washington, D.C., during the blizzard. Photographs of two skiers were shown in several regional newspapers, making them an iconic image of the storm and local celebrities.\n\nThree children died when their home in Angangueo was overwhelmed by a flooded river, and two other people died under a landslide in Zitácuaro. A sixth victim was crushed beneath a collapsed wall of a home in Ocampo. Two children drowned trying to cross the swollen Chapulin River in the central state of Guanajuato. In total, twenty eight deaths in the states of Michoacán, Mexico State and the Distrito Federal (Mexico City) have been attributed to the storm.\n\nOn February 3, 2010, a family from California was traveling east on snow-covered Interstate 40 near Gallup, New Mexico, when the driver hit a patch of ice, sending their pickup truck across the median into the westbound lanes, striking an oncoming vehicle, killing the adult passenger, and leaving the driver, the child, and the driver of the westbound vehicle critically injured.\n\nOn February 5, 2010, Brendan Burke, son of Toronto Maple Leafs General Manager Brian Burke, was killed while driving in Economy, Indiana, near the Ohio border. While driving in heavy snow, his 2004 Jeep Grand Cherokee slid sideways into the path of an oncoming Ford truck, killing him and his passenger, Mark Reedy (18) of Bloomfield Hills, Michigan.\n\nOn February 6, 2010, a father and son were rendering aid to the occupants of a disabled vehicle on Interstate 81 in Virginia. A tractor trailer that was approaching the scene jackknifed and killed the men.\n\nIn Maryland, a family was traveling north of Aberdeen on Route 462 when they ran into the back of a snow plow. Maryland State Police said that the accident was serious. In Bladensburg, two men were found dead in a running car whose tailpipe was blocked by snow; they died of carbon monoxide poisoning.\n\nIn Delaware, officials investigated 8 deaths in New Castle County related to the storm. Two men were found under snow piles and a third suffered from dementia and wandered outside only to be found an hour later by a family member half buried in snow.\n\nA father and daughter in McKeesport were killed by carbon monoxide poisoning, as a result of improper usage of a generator after a power outage. A Canonsburg man was found dead at the bottom of a snow-covered staircase. Twenty five vehicles were involved in two separate pileups on Interstate 80, killing one and injuring eighteen. Two were killed in Lancaster when their snowmobile was struck at an intersection.\n\n\n"}
{"id": "12460799", "url": "https://en.wikipedia.org/wiki?curid=12460799", "title": "Fort McMoney", "text": "Fort McMoney\n\nFort McMoney is a 2013 web documentary and strategy video game about Fort McMurray, Alberta, Canada and Athabasca oil sands development, directed by . The documentary uses interactive game elements to allow users to decide the city's future and attempt to responsibly develop the world's largest oil sands reserves.\n\nThe game consists of three episodes, each played in real-time over a four-week period. Starting November 25, users will decide on the virtual future of the city, while exploring the social, economic, political and cultural history of Fort McMurray. Players will be able to virtually walk around the city, meet residents and ask them questions. Each week, they'll be able to vote in referendums and surveys that will affect the city's virtual future, engage in debates, and attempt to win other players over to their \"worldview\" in order influence the city's development. The game is trilingual: available in English, French, and German.\n\n\"Fort McMoney\" incorporates 60 days of filming in more than 22 locations in the city, including 55 interviews. Research took place over 2 years, with 2000 hours of footage shot, at a cost of C$870,000. It is a collaboration of the National Film Board of Canada (NFB) and TOXA in association with Arte. Dufresne has stated he is drawn to stories about single-industry towns, having previously directed an award-winning webdoc about Cañon City, Colorado, entitled \"Prison Valley\". The project features photographs by Philippe Brault. It was produced by Philippe Lamarre and Raphaëlle Huysmans (TOXA) and Hugues Sweeney and Dominique Willieme (NFB), in association with Arte.\n\n\"Fort McMoney\" is being launched at the International Documentary Film Festival Amsterdam, with pre-launch events in Paris, Toronto and Montreal, as part of Rencontres internationales du documentaire de Montréal, an international documentary film festival. It will also be accessible online via four media partners: \"The Globe and Mail\" and Radio-Canada in Canada, as well as France's \"Le Monde\" and Germany's \"Süddeutsche Zeitung\".\n\nIn February 2015, \"Fort McMoney\" was named Best Original Interactive Production Produced for Digital Media at the 3rd Canadian Screen Awards.\n\n"}
{"id": "1350362", "url": "https://en.wikipedia.org/wiki?curid=1350362", "title": "Gas focusing", "text": "Gas focusing\n\nGas focusing, also known as ionic focusing. \n\nRather than being dispersed, a beam of charged particles travelling in an inert gas environment sometimes becomes narrower. This is ascribed to the generation of gas ions which diffuse outwards, neutralizing the particle beam globally, and producing an intense radial electric field which applies a radially inward force to the particles in the beam.\n\n\n\n"}
{"id": "58606900", "url": "https://en.wikipedia.org/wiki?curid=58606900", "title": "Global Wind Atlas", "text": "Global Wind Atlas\n\nThe Global Wind Atlas is a web-based application developed to help policymakers and investors identify potential high-wind areas for wind power generation virtually anywhere in the world, and perform preliminary calculations. It provides free access to data on wind power density and wind speed at multiple heights using the latest historical weather data and modeling, at an output resolution of 250 meters. It was developed and is maintained by the Wind Energy Department of the Technical University of Denmark (DTU Wind Energy) in partnership with the World Bank, with funding provided by the Energy Sector Management Assistance Program (ESMAP).\n\nThe original version of the Global Wind Atlas (GWA 1.0) was developed by DTU Wind Energy under the framework of the Clean Energy Ministerial (CEM) and, in particular, the CEM Working Group on Solar and Wind Technologies, led by Germany, Spain and Denmark. The Technology Development and Demonstration Program of the Danish Energy Agency (EUDP 11-II, 64011-0347) funded GWA 1.0 as the Danish contribution to the objectives of the working group. GWA 1.0 was launched in 2015, and benefited from collaboration with the International Renewable Energy Agency (IRENA) and the MASDAR Institute. The IRENA Global Atlas of Renewable Energy created a dedicated platform to serve GWA 1.0 data to a worldwide audience.\n\nIn early 2017, DTU Wind Energy entered into discussions with ESMAP at the World Bank to update and improve the Global Wind Atlas, and bring it into line with the Global Solar Atlas that was launched by the World Bank in January 2017. Utilizing funding provided under ESMAP's existing initiative on renewable energy resource assessment and mapping, the company VORTEX FDC was commissioned to carry out a global mesoscale wind energy modeling exercise at 9 km resolution. This data was provided to DTU Wind Energy for further microscale modeling, initially down to 1 km resolution. In November 2018 a new version of the Global Wind Atlas (GWA 2.0) was launched by DTU Wind Energy and the World Bank at the Wind Europe 2018 conference in Amsterdam, the Netherlands, based on a completely redesigned user interface developed by the company Nazka Mapps. \n\nFollowing further modeling work, a new version of the microscale modeling data was released in July 2018 (GWA 2.1), bringing the resolution down to 250 meters. This was followed by a further release in September 2018 (GWA 2.2) that included a number of improvements to the user interface, a new tool for preparing and downloading poster maps, and various bug fixes. The most recent release, in November 2018 (GWA 2.3) introduced three capacity factor layers, new wind roses, and improved calculation of power density, and the ability to download GIS files, among other features.\n\nThe Global Wind Atlas is based on a coupling of mesoscale to microscale modeling. The mesoscale modeling carried out by Vortex uses ERA Interim meteorological re-analysis data provided by the European Centre for Medium-Range Weather Forecasts. While tools such as the Global Wind Atlas can provide useful data to support planning and initial site scoping, they are no replacement for the more detailed analysis needed when evaluating actual wind farm projects.\n\nIn addition to the data available via the Global Wind Atlas website, users may also download poster maps, GIS data, and Generalized Wind Climate (GWC) files for use in commercial wind resource assessment software such as WAsP.\n\nGIS data from the Global Wind Atlas is available via the by the IRENA Global Atlas for Renewable Energy, and has been included as the core wind data in the RETScreen software. It is used by governments, renewable energy developers, and academics, and has an average of 7,500 unique users per month as of October 2018.\n\n\n"}
{"id": "1191031", "url": "https://en.wikipedia.org/wiki?curid=1191031", "title": "Graviphoton", "text": "Graviphoton\n\nIn theoretical physics, a graviphoton or gravivector is a hypothetical particle which emerges as an excitation of the metric tensor (i.e. gravitational field) in spacetime dimensions higher than four, as described in Kaluza–Klein theory.\nHowever, its crucial physical properties are analogous to a (massive) photon: it induces a \"vector force\", sometimes dubbed a \"fifth force\". The electromagnetic potential formula_1 emerges from an extra component of the metric tensor formula_2, where the figure 5 labels an additional, fifth dimension.\n\nIn gravity theories with extended supersymmetry (extended supergravities), a graviphoton is normally a superpartner of the graviton that behaves like a photon, and is prone to couple with gravitational strength, as was appreciated in the late 1970s. Unlike the graviton, however, it may provide a \"repulsive\" (as well as an attractive) force, and thus, in some technical sense, a type of anti-gravity. Under special circumstances, then, in several natural models, often descending from five-dimensional theories mentioned, it may actually cancel the gravitational attraction in the static limit. Joël Scherk investigated semirealistic aspects of this phenomenon, thereby opening up an ongoing search for physical manifestations of the mechanism.\n\n"}
{"id": "22885172", "url": "https://en.wikipedia.org/wiki?curid=22885172", "title": "Hans Grassmann", "text": "Hans Grassmann\n\nHans Grassmann (Bamberg, 21 May 1960) is a , writer and entrepreneur, who teaches and works in Italy. Grassmann is the author of four books and more than 250 scientific publications, and is the founder and managing director of the research company Isomorph srl.\n\nHis main contributions to physics include the development of a (Tl) calorimeter with a photodiode; developing the analysis of asymmetry in the production of the W particle; a contribution to the discovery of the top quark, the development of a physics theory of information; the design and development of a wind turbine with an external duct; and the realization of the linear mirror for the concentration of solar energy. Grassmann has worked in Italy since 1988.\n\nFrom 1979 to 1984, Grassmann studied physics at the University of Erlangen and the University of Hamburg. For his laurea thesis, he developed a detection method for high energy photons using a scintillating crystal (CsI(Tl)) calorimeter with photodiode readout. Advanced scientific experiments make use of this technology, including the Crystal-Barrel, the BaBar, the CLEO, the Belle experiments and the Glast satellite.\n\nFrom 1984 to 1988, Grassmann was part of the UA1 experiment at CERN in Geneva, where he wrote his PhD thesis.\n\nFrom 1987 to 1999, Grassmann worked with the CDF collaboration at the Tevatron collider in the Fermi National Laboratory (Fermilab), close to Chicago and at the Superconducting Super Collider laboratory (Dallas).\n\nIn 1988 with his student, S. Leone, he developed the study of the asymmetry in production and decay of the W-boson at the Tevatron proton–antiproton collider. W bosons are predominantly produced in collisions of valence quarks; therefore, one can determine the kinematic properties of the up and down quarks in the proton and antiproton from the observation of W production. By analyzing the relative difference in the production of W and W particles, one can substantially reduce the effects of systematic uncertainties in the experimental device.\n\nSince 1988, Grassmann has developed a method for detecting the top quark. The method makes use of the different kinematic properties of production and decay of top quark particles and background events, such as the production of W particles together with hadronic jets. In 1994, this analysis was successfully applied by Grassmann, G. Bellettini and M. Cobal. The top quark was observed in Tevatron collider data. These results were confirmed when the analysis was repeated on a larger data sample.\n\nAfter the top quark discovery, Grassmann worked on a connection between the classic information theory of Claude Shannon, Gregory Chaitin and Andrey Kolmogorov et al. and physics. From work done by Leó Szilárd, Rolf Landauer and Charles H. Bennett, there is a connection between physics and information theory. Storing or deleting one bit of information dissipates energy; however, neither classic information theory nor algorithmic information theory contain any physics variables. The variable entropy used in information theory is not a state function; therefore, it is not the thermodynamic entropy used in physics. Grassmann made use of existing and established concepts, such as message, amount of information or complexity, but set them in a new mathematical framework. His approach is based on vector algebra or on Boolean algebra instead of probability theory.\n\nGrassmann also developed an approach for studying shrouded wind turbines.\n\nIn 2006 Isomorph undertook the development of a system of mirrors - the so-called Linear mirror - for the concentration of solar energy. This system is a very simple and therefore inexpensive structure, which allows to create a full-scale prototype without the need of outside partners. In 'October 2008, the Linear mirror received its first award from the Italian Physical Society, which honors Alessandro Prest, an employee of the Isomorph, for the presentation of the project.\n\nThe mirror came into operation for the first time in autumn 2008, fulfilling all the expectations. In July 2010 the first Linear mirror was installed by the town of Pontebba to provide thermal energy to the local kindergarten. In the same year the town of Pontebba successfully participated to the National contest for the election of the most virtuous municipalities. In April 2011 Hans Grassmann has received the \"Nuclear-Free Future Award, with the motivation that the Linear mirror can be able to contribute to the replacement of nuclear power.\n\nIn May 2012 the Linear mirror received the Solar keymark certificate by CERTCO DIN (DIN EN 12795-1:2006-06 and DIN EN 12795-2:2006-06). Tests for the Solar Keymark were carried out by the Fraunhofer Institute Freiburg.\n\nIn 2004, Grassmann founded Isomorph, which creates scientific concepts, procedures and devices based on physics research. Isomorph's research is independent of the scientific-administrative complex.\n\nIsomorph developed an innovative concentrating mirror system to make economic use of solar energy. It is a simple system and cheap to produce.\n\nGrassmann has explained physics to the general public in books and newspaper articles, noting that \"everybody can understand physics. What cannot be understood is not physics.\" His books about the relationship between science and society are available in several translations.\n\n\n"}
{"id": "35987829", "url": "https://en.wikipedia.org/wiki?curid=35987829", "title": "Hilton Kelley", "text": "Hilton Kelley\n\nHilton Kelley is a former American actor and environmentalist from Port Arthur, Texas. He was awarded the Goldman Environmental Prize in 2011, for his fight against pollution of the Port Arthur district from petrochemical industry and waste facilities.\n"}
{"id": "54726298", "url": "https://en.wikipedia.org/wiki?curid=54726298", "title": "Insheim Geothermal Power Station", "text": "Insheim Geothermal Power Station\n\nThe Insheim Geothermal Power Station is a geothermal power station in Rhineland-Palatinate.\n\nThe Power Station is related to earthquakes, the strongest had a magnitude of 2.4 and was registered on the 09.April.2010 10:52:22. Until 29.December.2017 120 earthquakes were registered in Insheim. The Upper Rhine Plain has a high seismic risk and is home to the BASF, one of the biggest chemical factories in Europe.\n"}
{"id": "28032048", "url": "https://en.wikipedia.org/wiki?curid=28032048", "title": "Killer Company", "text": "Killer Company\n\nKiller Company: James Hardie Exposed is a 2009 Australian book by journalist Matt Peacock. \n\nThe book documents how the use of harmful asbestos fibre in building materials produced by James Hardie Industries \"led to the deaths of thousands of workers and customers, who were never informed of the dangers\". Working with asbestos products, such as \"fibro\", resulted in medical abnormalities, such as asbestosis. The book opens with the story of Bernie Banton, former James Hardie employee, who suffered from asbestos-induced fibrosis and later died.\n\nAccording to Peacock, James Hardie Industries circumvented the rules and regulations designed to protect the community from serious health hazards. Peacock states that \"Hardie embarked on a cold, calculated strategy to maximise profits, minimise compensation and conceal the culprits\". \n\n\"Killer Company\" was a finalist for the Walkley non-fiction book of the year in 2009.\n\"Devil's Dust\", a docudrama based on \"Killer Company\", was released in 2012, with Ewen Leslie portraying Peacock.\n"}
{"id": "1887714", "url": "https://en.wikipedia.org/wiki?curid=1887714", "title": "Large Plasma Device", "text": "Large Plasma Device\n\nThe Large Plasma Device (often stylized as LArge Plasma Device or LAPD) is an experimental physics device housed at the Basic Plasma Science Facility at UCLA. It is designed as a general purpose laboratory for experimental plasma physics research. The device began operation in 1991 and was upgraded twice in 2001 and 2016 to its current version. The modern LAPD is operated as a national user facility, in which half the research time on the device is open to scientists at other institutions and facilities.\n\nThe LAPD is a linear pulsed-discharge device operated at a high (1 Hz) repetition rate, producing a strongly magnetized background plasma which is physically large enough to support Alfvén waves. Plasma is produced from a barium oxide (BaO) cathode-anode discharge at one end of a 20-meter long, 1 meter diameter cylindrical vacuum vessel (diagram). The resulting plasma column is roughly 16.5 meters long and 60 cm in diameter. The background magnetic field, produced by a series of large electromagnets surrounding the chamber, can be varied from 400 gauss to 2.5 kilogauss (40 to 250 mT).\n\nBecause the LAPD is a general-purpose research device, the plasma parameters are carefully selected to make diagnostics simple without the problems associated with hotter (e.g. fusion-level) plasmas, while still providing a useful environment in which to do research. The typical operational parameters are: \n\n\nIn principle, a plasma may be generated from any kind of gas, but inert gases are typically used to prevent the plasma from destroying the coating on the barium oxide cathode. Examples of gases used are helium, argon, nitrogen and neon. Hydrogen is sometimes used for short periods of time. Multiple gases can also be mixed in varying ratios within the chamber to produce multi-species plasmas.\n\nAt these parameters, the ion Larmor radius is a few millimeters, and the Debye length is tens of micrometres. Importantly, it also implies that the Alfvén wavelength is a few meters, and in fact shear Alfvén waves are routinely observed in the LAPD. This is the main reason for the 20-meter length of the device.\n\nThe main source of plasma within the LAPD is produced via discharge from the barium oxide (BaO) coated cathode, which emits electrons via thermionic emission. The cathode is located near the end of the LAPD and is made from a thin nickel sheet, uniformly heated to roughly 900°C. The circuit is closed by a molybdenum mesh anode a short distance away. Typical discharge currents are in the range of 3-8 kiloamperes at 60-90 volts, supplied by a custom-designed transistor switch backed by a 4-farad capacitor bank.\n\nA secondary cathode source made of lanthanum hexaboride (LaB) was developed in 2010 to provide a hotter and denser plasma when required. It consists of four square tiles joined to form a 20 formula_120 cm area and is located at the other end of the LAPD. The circuit is also closed by a molybdenum mesh anode, which may be placed further down the machine, and is slightly smaller in size to the one used to close the BaO cathode source. The LaB cathode is typically heated to temperatures above 1750°C by a graphite heater, and produces discharge currents of 2.2 kiloamperes at 150 volts.\n\nThe plasma in the LAPD is usually pulsed at 1 Hz, with the background BaO source on for 10-20 milliseconds at a time. If the LaB source is being utilized, it typically discharges together BaO cathode, but for a shorter period of time (about 5–8 ms) nearing the end of each discharge cycle. The use of an oxide-cathode plasma source, along with a well-designed transistor switch for the discharge, allows for a plasma environment which is extremely reproducible shot-to-shot.\n\nOne interesting aspect of the BaO plasma source is its ability to act as an \"Alfvén Maser\", a source of large-amplitude, coherent shear Alfvén waves. The resonant cavity is formed by the highly reflective nickel cathode and the semitransparent grid anode. Since the source is located at the end of the solenoid which generates the main LAPD background field, there is a gradient in the magnetic field within the cavity. As shear waves do not propagate above the ion cyclotron frequency, the practical effect of this is to act as a filter on the modes which may be excited. Maser activity occurs spontaneously at certain combinations of magnetic field strength and discharge current, and in practice may be activated (or avoided) by the machine user.\n\nThe main diagnostic is the movable probe. The relatively low electron temperature makes probe construction straightforward and does not require the use of exotic materials. Most probes are constructed in-house within the facility and include magnetic field probes, Langmuir probes, Mach probes (to measure flow), electric dipole probes and many others. Standard probe design also allows external users to bring their own diagnostics with them, if they desire. Each probe is inserted through its own vacuum interlock, which allows probes to be added and removed while the device is in operation. \n\nA 1 Hz rep-rate, coupled with the high reproducibility of the background plasma, allows the rapid collection of enormous datasets. An experiment on LAPD is typically designed to be repeated once per second, for as many hours or days as is necessary to assemble a complete set of observations. This makes it possible to diagnose experiments using a small number of movable probes, in contrast to the large probe arrays used in many other devices.\n\nThe entire length of the device is fitted with \"ball joints,\" vacuum-tight angular couplings (invented by a LAPD staff member) which allow probes to be inserted and rotated, both vertically and horizontally. In practice, these are used in conjunction with computer-controlled motorized probe drives to sample \"planes\" (vertical cross-sections) of the background plasma with whatever probe is desired. Since the only limitation on the amount of data to be taken (number of points in the plane) is the amount of time spent recording shots at 1 Hz, it is possible to assemble large volumetric datasets consisting of many planes at different axial locations.\n\nVisualizations composed from such volumetric measurements can be seen at the LAPD gallery.\n\nIncluding the ball joints, there are a total of 450 access ports on the machine, some of which are fitted with windows for optical or microwave observation.\n\nA variety of other diagnostics are also available at the LAPD to compliment probe measurements. These include photodiodes, microwave interferometers, a high speed camera (3 ns/frame) and laser-induced fluorescence.\n\n\n"}
{"id": "9704718", "url": "https://en.wikipedia.org/wiki?curid=9704718", "title": "Linolelaidic acid", "text": "Linolelaidic acid\n\nLinolelaidic acid is an omega-6 trans fatty acid (TFA) and is a geometric isomer of linoleic acid. It is found in partially hydrogenated vegetable oils. It also comprises 12.39% of the fats from the fruit of the durian species \"Durio graveolens\". It is a white (or colourless) viscous liquid.\n\nTFAs are classified as conjugated and nonconjugated, corresponding usually to the structural elements -CH=CH-CH=CH- and -CH=CH-CH-CH=CH-, respectively. Nonconjugated TFAs are represented by elaidic acid and linolelaidic acid. Their presence is linked heart diseases. The TFA vaccenic acid, which is of animal origin, poses less of a health risk.\n"}
{"id": "24404806", "url": "https://en.wikipedia.org/wiki?curid=24404806", "title": "Luxgen M7", "text": "Luxgen M7\n\nLuxgen M7, previously the Luxgen7 MPV is a 7-passenger Taiwanese automobile produced by Luxgen beginning from 2009. The vehicle was developed under Yulon's R&D center, HAITEC. It was officially shown to the public on 19 August 2009, and hit showrooms on 9/19/2009. The car is powered by a 2.2 L I-4 MEFI turbocharged engine developed by tuned by Delphi. The platform is derived from the Renault Espace. The turbocharger is supplied by Garrett and powers the engine up to at 5,200 rpm and of torque from 2,500 to 4,000 rpm. The 5-speed Manumatic transmission is supplied by Aisin. Pricing starts at NT$798,000 up to NT$1,068,000. ABS, EBD, BAS, and the Think+ Multi-purpose system(slightly different by trim level) are all standard equipment. The targeted competitor is the BYD M6. In 2014, the Luxgen7 MPV received a facelift and name change to Luxgen M7 to fit the new Luxgen naming theme.\n\nThe voice controlled \"Luxgen Think+\" system (co-developed with HTC) combines the on-board Windows CE computer with the Electronic Stability Control system and it provides 3.5G mobile internet connection, GPS navigation, along with travel, shop, and traffic information.\n\nA facelift was done in 2014 with a redesigned front bumper, dark head lamps with DRL, and a new set of LED tail lamps. The name was changed from Luxgen7 MPV to Luxgen M7 turbo Eco hyper, and the engine was also updated with a retuned 2.2- liter I4 turbo one with improved horsepower of 202PS and of torque from 2,400 to 4,000 rpm. The fuel efficiency was also improved to 13.57 km/l(EPA) and 11.8 km/l(EU).\n\nLuxgen also introduced the world's first electric MPV, Luxgen7 MPV electric vehicle co-developed by AC Propulsion. The top speed is and the electric range is at .\nWhile most of the exterior was based on the regular Luxgen7 MPV, the EV+ receives a different front bumper and grille compartment design and clear tail lamp clusters in a lightly tinted blue color.\n\nThe Luxgen 7 CEO is an executive limousine built on the MPV platform. Most parts of the vehicle body remains the same while on the exterior, the CEO receives a different grille design. It has two \"first-class\" rear seats with massage, recline and leg rest. A divider with motorized panels separates the passenger and driver compartments. The glass window can be frosted with the flick of a switch, along with an opaque panel that enhances privacy.\n\nThe Luxgen V7 is Luxgen's wheelchair accessible van based on the M7. It is launched in 2016 and is equipped with an extended and lower tailgate and a raised roof.\n"}
{"id": "56398902", "url": "https://en.wikipedia.org/wiki?curid=56398902", "title": "Micropup", "text": "Micropup\n\nIn electronics, a micropup is a style of triode vacuum tube (valve) developed during World War II for use at very high frequencies such as those used in radar. They are characterized by an external anode block, which allows better heat dissipation. These tubes could deliver radio frequency power on the order of kilowatts at wavelengths as short as 25 cm., and on the order of 100 kW at 200 MHZ in pulses. Micropup tubes used very high voltages to minimize the transit time of electrons between anode and cathode. \n\nThe development of the micropup vacuum tube was made possible by the development of vacuum-tight joining of copper to glass, around 1939. The designs used a cylindrical anode and a concentric cylindrical grid electrode; the cathode was directly heated thoriated tungsten wires, which after the first types were all oxide coated to improve electron emission. One type, the NT99 developed by GEC could produce up to 200 kW peak output (for a pair of tubes) when used in 600 MHZ radar sets. Although widely used in \"metre-band\" radar systems, the cavity magnetron was able to produce significant power at much higher frequencies, as radar systems developed during the war. \n"}
{"id": "2181563", "url": "https://en.wikipedia.org/wiki?curid=2181563", "title": "Nuclear material", "text": "Nuclear material\n\nNuclear material refers to the metals uranium, plutonium, and thorium, in any form, according to the IAEA. This is differentiated further into \"source material\", consisting of natural and depleted uranium, and \"special fissionable material\", consisting of enriched uranium (U-235), uranium-233, and plutonium-239. Uranium ore concentrates are considered to be a \"source material\", although these are not subject to safeguards under the Nuclear Non-Proliferation Treaty.\n\nDifferent countries may use different terminology: in the United States of America, \"nuclear material\" most commonly refers to \"special nuclear materials\" (SNM), with the potential to be made into nuclear weapons as defined in the Atomic Energy Act of 1954. The \"special nuclear materials\" are also plutonium-239, uranium-233, and enriched uranium (U-235).\n\nNote that the 1980 Convention on the Physical Protection of Nuclear Material definition of nuclear material does not include thorium.\n\n"}
{"id": "42320611", "url": "https://en.wikipedia.org/wiki?curid=42320611", "title": "Petro-Islam", "text": "Petro-Islam\n\nPetro-Islam usually refers to the extremist and fundamentalist interpretation of Sunni Islam—sometimes called \"Wahhabism\"—favored by the conservative oil-exporting Kingdom of Saudi Arabia. Its name derives from source of the funding—petroleum exports—that spread it through the Muslim world following the Yom Kippur War. The term is sometimes called \"pejorative\" or a \"nickname\".\nAccording to Sandra Mackey the term was coined by Fouad Ajami. It has been used by French political scientist Gilles Kepel, Bangladeshi religious scholar Imtiyaz Ahmed, and Egyptian philosopher Fouad Zakariyya, among others.\n\nThe use of the term to refer to \"Wahhabism\" (the dominant interpretation of Islam in Saudi Arabia) is widespread but not universal. Variations on, or different uses of, the term include:\n\nOne scholar who spelled out the idea of petro-Islam in some detail is Gilles Kepel. According to Kepel, prior to the 1973 oil embargo, religion throughout the Muslim world was \"dominated by national or local traditions rooted in the piety of the common people.\" Clerics looked to their different schools of fiqh (the four Sunni Madhhabs: Hanafi in the Turkish zones of South Asia, Maliki in Africa, Shafi'i in Southeast Asia, plus Shi'a Ja'fari, and \"held Saudi inspired puritanism\" (using another school of fiqh, Hanbali) in \"great suspicion on account of its sectarian character,\" according to Gilles Kepel.\nWhile the 1973 War (also called the Yom Kippur War) was started by Egypt and Syria to take back land won by Israel in 1967, the \"real victors\" of the war were the Arab \"oil-exporting countries\", (according to Gilles Kepel), whose embargo against Israel's western allies stopped Israel's counter offensive.\n\nThe embargo's political success enhanced the prestige of the embargo-ers and the reduction in the global supply of oil sent oil prices soaring (from US$3 per barrel to nearly $12) and with them, oil exporter revenues. This put Muslim oil exporting states in a \"clear position of dominance within the Muslim world\". The most dominant was Saudi Arabia, the largest exporter by far (see bar chart).\n\nSaudi Arabians viewed their oil wealth not as an accident of geology or history, but connected to religion—a blessing by God of them, to \"be solemnly acknowledged and lived up to\" with pious behavior.\nWith its new wealth the rulers of Saudi Arabia sought to replace nationalist movements in the Muslim world with Islam, to bring Islam \"to the forefront of the international scene\", and to unify Islam worldwide under the \"single creed\" of Wahhabism, paying particular attention to Muslims who had immigrated to the West (a \"special target\").\"\n\nAccording to scholar Gilles Kepel, (who devoted a chapter of his book \"\" to the subject -- \"Building Petro-Islam on the Ruins of Arab Nationalism\"), in the years immediately after the 1973 War, `petro-Islam` was a \"sort of nickname\" for a \"constituency\" of Wahhabi preachers and Muslim intellectuals who promoted \"strict implementation of the sharia [Islamic law] in the political, moral and cultural spheres\".\n\nIn the coming decades, Saudi Arabia's interpretation of Islam became influential (according to Kepel) through \n\nAuthor Sandra Mackey describes the use of petrodollars on facilities for the hajj—for example leveling hill peaks to make room for tents, providing electricity for tents and cooling pilgrims with ice and air conditioning—as part of \"Petro-Islam\", which she describes as a way of building the Muslim faithful's loyalty toward the Saudi government.\n\nThe Saudi ministry for religious affairs printed and distributed millions of Qurans free of charge, along with doctrinal texts that followed the Wahhabi interpretation. In mosques throughout the world \"from the African plains to the rice paddies of Indonesia and the Muslim immigrant high-rise housing projects of European cities, the same books could be found\", paid for by Saudi Arabian government.\n\nImtiyaz Ahmed, a religious scholar and professor of International Relations at University of Dhaka sees changes in religious practices in Bangladesh as linked to Saudi Arabia's efforts to promote Wahhabism through the financial help it provides countries like Bangladesh. The Mawlid, the celebration of the Prophet Muhammad’s birthday and formerly \"an integral part of Bangladeshi culture\" is no longer popular, while black burqas for women are much more so. The discount on the price of oil imports Bangladesh receives doesn't \"come free\", according to Ahmed. \"Saudi Arabia is giving oil, Saudi Arabia would definitely want that some of their ideas to come with oil.\"\n\nMore than 1,500 mosques were built around the world from 1975 to 2000 paid for by Saudi public funds.\nThe Saudi-headquartered and financed Muslim World League played a pioneering role in supporting Islamic associations, mosques, and investment plans for the future. It opened offices in \"every area of the world where Muslims lived.\" The process of financing mosques usually involved presenting a local office of the Muslim World League with evidence of the need for a mosque/Islamic center to obtain the offices 'recommendation' (\"tazkiya\") to \"a generous donor within the kingdom or one of the emirates\".\n\nSaudi-financed mosques were generally built using marble 'international style' design and green neon lighting, in a break with most local Islamic architectural traditions, but following Wahhabi ones.\n\nOne mechanism for the redistribution of (some) oil revenues from Saudi Arabia and other Muslim oil-exporters, to the poorer Muslim nations of African and Asia, was the Islamic Development Bank. Headquartered in Saudi Arabia, it opened for business in 1975. Its lenders and borrowers were member states of Organisation of the Islamic Conference (OIC) and it strengthened \"Islamic cohesion\" between them. \nSaudi Arabians also helped establish Islamic banks with private investors and depositors. DMI (Dar al-Mal al-Islami: the House of Islamic Finance), founded in 1981 by Prince Mohammed bin Faisal Al Saud, and the Al Baraka group, established in 1982 by Sheik Saleh Abdullah Kamel (a Saudi billionaire), were both transnational holding companies.\n\nBy 1975, over one million workers—from unskilled country people to experienced professors from Sudan, Pakistan, India, Southeast Asia, Egypt, Palestine, Lebanon, and Syria—had moved the Saudi Arabia and the Persian Gulf states to work and returned after a few years with savings. A majority of these workers were Arab and most were Muslim. Ten years later the number had increased to 5.15 million and Arabs were no longer in the majority. 43% (mostly Muslims) came from the South Asia. In one country, Pakistan, in a single year, (1983), \n\"the money sent home by Gulf emigrants amounted to $3 billion, compared with a total of $735 million given to the nation in foreign aid. ... The underpaid petty functionary of yore could now drive back to his hometown at the wheel of a foreign car, build himself a house in a residential suburb, and settle down to invest his savings or engage in trade... he owed nothing to his home state, where he could never have earned enough to afford such luxuries.\" \n\nMuslims who had moved to Saudi Arabia, or other \"oil-rich monarchies of the peninsula\" to work, often returned to their poor home country following religious practice more intensely, particularly practices of Wahhabi Muslims. \nHaving \"grown rich in this Wahhabi milieu\" it was not surprising that the returning Muslims believed there was a connection between that milieu and \"their material prosperity\", and that on return they followed religious practices more intensely and that those practices followed Wahhabi tenants. Kepel gives examples of migrant workers returning home with new affluence, asking to be addressed by servants as \"hajja\" rather than \"Madame\" (the old bourgeois custom). Another imitation of Saudi Arabia adopted by affluent migrant workers was increased segregation of the sexes, including shopping areas.\n\nIn the 1950s and 1960s Gamal Abdul-Nasser, the leading exponent of Arab nationalism and the president of the Arab world's largest country had great prestige and popularity.\nHowever, in 1967 Nasser led the Six-Day War against Israel which ended not in the elimination of Israel but in the decisive defeat of the Arab forces and loss of a substantial chunk of Egyptian territory. This defeat, combined with the economic stagnation from which Egypt suffered, were contrasted with the perceived victory of the October 1973 war whose pious battle cry of \"Allahu Akbar\" replaced `Land! Sea! Air!` slogan of the 1967 war, and with the enormous wealth of the resolutely non-nationalist Saudi Arabia.\n\nThis changed \"the balance of power among Muslim states\" toward Saudi Arabia and other oil-exporting countries. gaining as Egypt lost influence. The oil-exporters emphasized \"religious commonality\" among Arabs, Turks, Africans, and Asians, and downplayed \"differences of language, ethnicity, and nationality.\" \n\nThe Organisation of Islamic Cooperation—whose permanent Secretariat is located in Jeddah in Western Saudi Arabia—was founded after the 1967 war.\n\nAt least one observer—\"The New Yorker\" magazine's investigative journalist Seymour Hersh—has suggested that petro-Islam is being spread by those whose motivations are less than earnest/pious. Petro-Islam funding following the Gulf War, according to Hersh, \"amounts to protection money\" from the Saudi regime \"to fundamentalist groups that wish to overthrow it.\"\n\nEgyptian existentialist Fouad Zakariyya has accused purveyors of Petro-Islam as having as their objective the protection of the oil wealth and \"social relations\" of the \"tribal societies that possess the lion's share of this wealth\", at the expense of the long term development of the region and the majority of its people. He further states that it is a \"brand of Islam\" that bills itself as \"pure\" but rather than being the Islam of the early Muslims has \"never\" been \"seen before in history\".\n\nAuthors who criticize the \"thesis\" of Petro-Islam itself—that petrodollars have had a significant effect on Muslim beliefs and practices—include Joel Beinin and Joe Stork. They argue that in Egypt, Sudan and Jordan, \"Islamic movements have demonstrated a high level of autonomy from their original patrons.\" The strength and growth of Muslim Brotherhood, and other forces of conservative political Islam in Egypt can be explained (Beinin and Stork believe), by internal forces—the historical strength of the Muslim Brotherhood, sympathy for the \"martyred\" Sayyid Qutb, anger with the \"autocratic tendencies\" and failed promises of prosperity of the Sadat government.\n\n"}
{"id": "26563427", "url": "https://en.wikipedia.org/wiki?curid=26563427", "title": "Plasma shaping", "text": "Plasma shaping\n\nMagnetically confined fusion plasmas such as those generated in tokamaks and stellarators are characterized by a typical shape. Plasma shaping is the study of the plasma shape in such devices, and is particularly important for next step fusion devices such as ITER. This shape is conditioning partly the performance of the plasma. Tokamaks, in particular, are axisymmetric devices, and therefore one can completely define the shape of the plasma by its cross-section.\n\nEarly fusion reactor designs tended to have circular cross-sections simply because they were easy to design and understand. Generally, fusion machines using a toroidal layout, like the tokamak and most stellarators, arrange their magnetic fields so the ions and electrons in the plasma travel around the torus at high velocities. However, as the circumference of a path on the outside of the plasma area is longer than one on the inside, this caused several effects that disrupted the stability of the plasma.\n\nDuring the 1960s a number of different methods were used to try to address these problems. Generally they used a combination of several magnetic fields to cause the net magnetic field inside the device to be twisted into a helix. Ions and electrons following these lines found themselves moving to the inside and then outside of the plasma, mixing it and suppressing some of the most obvious instabilities.\n\nIn the 1980s, further research along these lines demonstrated that further advances were possible by using external current-carrying coils to make the lines not just helical, but non-symmetric as well. This led to a series of experiments using C and D-shaped plasma volumes..\n\nBy increasing the current in one (or more) shaping coils to a high enough degree, one (or more) 'X-points' can be created. An X-point is defined as a point in space at which the poloidal field has zero magnitude. The magnetic flux surface that intersects with the X-point is called the separatrix, and, as all flux surfaces external to this surface are unconfined, the separatrix defines the last closed flux surface (LCFS). Formerly, the LCFS was established by inserting a material limiter into the plasma, which fixed the plasma temperature and potential (among other quantities) to be equal to that of the limiter. Plasma that escaped the LCFS would do so with no preferential direction, potentially damaging instruments. By establishing an X-point and separatrix, the plasma edge is uncoupled from the vessel walls, and exhausted heat and plasma particles are preferentially diverted towards a known region of the vessel near the X-point.\n\nIn the simple case of a plasma with up-down symmetry, the plasma cross-section is defined using a combination of four parameters:\n\nIn general (no up-down symmetry), there can be an upper-triangularity, and a lower-triangularity.\n\nTokamaks can have negative triangularity.\n\n\n"}
{"id": "1236075", "url": "https://en.wikipedia.org/wiki?curid=1236075", "title": "Quartz crystal microbalance", "text": "Quartz crystal microbalance\n\nA quartz crystal microbalance (QCM) measures a mass variation per unit area by measuring the change in frequency of a quartz crystal resonator. The resonance is disturbed by the addition or removal of a small mass due to oxide growth/decay or film deposition at the surface of the acoustic resonator. The QCM can be used under vacuum, in gas phase (\"gas sensor\", first use described by King) and more recently in liquid environments. It is useful for monitoring the rate of deposition in thin film deposition systems under vacuum. In liquid, it is highly effective at determining the affinity of molecules (proteins, in particular) to surfaces functionalized with recognition sites. Larger entities such as viruses or polymers are investigated, as well. QCM has also been used to investigate interactions between biomolecules. Frequency measurements are easily made to high precision (discussed below); hence, it is easy to measure mass densities down to a level of below 1 μg/cm. In addition to measuring the frequency, the dissipation factor (equivalent to the resonance bandwidth) is often measured to help analysis. The dissipation factor is the inverse quality factor of the resonance, Q = w/f (see below); it quantifies the damping in the system and is related to the sample's viscoelastic properties.\n\nQuartz is one member of a family of crystals that experience the piezoelectric effect. The piezoelectric effect has found applications in high power sources, sensors, actuators, frequency standards, motors, etc., and the relationship between applied voltage and mechanical deformation is well known; this allows probing an acoustic resonance by electrical means. Applying alternating current to the quartz crystal will induce oscillations. With an alternating current between the electrodes of a properly cut crystal, a standing shear wave is generated. The Q factor, which is the ratio of frequency and bandwidth, can be as high as 10. Such a narrow resonance leads to highly stable oscillators and a high accuracy in the determination of the resonance frequency. The QCM exploits this ease and precision for sensing. Common equipment allows resolution down to 1 Hz on crystals with a fundamental resonant frequency in the 4 – 6 MHz range. A typical setup for the QCM contains water cooling tubes, the retaining unit, frequency sensing equipment through a microdot feed-through, an oscillation source, and a measurement and recording device.\n\nThe frequency of oscillation of the quartz crystal is partially dependent on the thickness of the crystal. During normal operation, all the other influencing variables remain constant; thus a change in thickness correlates directly to a change in frequency. As mass is deposited on the surface of the crystal, the thickness increases; consequently the frequency of oscillation decreases from the initial value. With some simplifying assumptions, this frequency change can be quantified and correlated precisely to the mass change using the Sauerbrey equation. \nOther techniques for measuring the properties of thin films include ellipsometry, surface plasmon resonance (SPR) spectroscopy, Multi-Parametric Surface Plasmon Resonance and dual polarisation interferometry.\n\nThe classical sensing application of quartz crystal resonators is microgravimetry. Many commercial instruments, some of which are called thickness monitors, are available. These devices exploit the Sauerbrey relation. For thin films, the resonance frequency is usually inversely proportional to the total thickness of the plate. The latter increases when a film is deposited onto the crystal surface. Monolayer sensitivity is easily reached. However, when the film thickness increases, viscoelastic effects come into play. In the late 80’s, it was recognized that the QCM can also be operated in liquids, if proper measures are taken to overcome the consequences of the large damping. Again, viscoelastic effects contribute strongly to the resonance properties.\n\nToday, microweighing is one of several uses of the QCM.\nMeasurements of viscosity and more general, viscoelastic properties, are of much importance as well. The “non-gravimetric” QCM is by no means an alternative to the conventional QCM. Many researchers, who use quartz resonators for purposes other than gravimetry, have continued to call the quartz crystal resonator “QCM”. Actually, the term \"balance\" makes sense even for non-gravimetric applications if it is understood in the sense of a force balance. At resonance, the force exerted upon the crystal by the sample is balanced by a force originating from the shear gradient inside the crystal. This is the essence of the small-load approximation.\nCrystalline α–quartz is by far the most important material for thickness-shear resonators. Langasite (LaGaSiO, “LGS”) and gallium-orthophosphate (GaPO) are investigated as alternatives to quartz, mainly (but not only) for use at high temperatures. Such devices are also called “QCM”, even though they are not made out of quartz (and may or may not be used for gravimetry).\n\nThe QCM is a member of a wider class of sensing instruments based on acoustic waves at surfaces. Instruments sharing similar principles of operation are shear horizontal surface acoustic wave (SH-SAW) devices, Love-wave devices and torsional resonators. Surface acoustic wave-based devices make use of the fact that the reflectivity of an acoustic wave at the crystal surface depends on the impedance (the stress-to-speed ratio) of the adjacent medium. (Some acoustic sensors for temperature or pressure make use of the fact that the speed of sound inside the crystal depends on temperature, pressure, or bending. These sensors do not exploit surface effects.) In the context of surface-acoustic wave based sensing, the QCM is also termed “bulk acoustic wave resonator (BAW-resonator)” or “thickness-shear resonator”. The displacement pattern of an unloaded BAW resonator is a standing shear wave with anti-nodes at the crystal surface. This makes the analysis particularly easy and transparent.\n\nWhen the QCM was first developed, natural quartz was harvested, selected for its quality and then cut in the lab. However, most of today’s crystals are grown using seed crystals. A seed crystal serves as an anchoring point and template for crystal growth. Grown crystals are subsequently cut and polished into hair-thin discs which support thickness shear resonance in the 1-30 MHz range. The \"AT\" or \"SC\" oriented cuts (discussed below) are widely used in applications.\n\nThe QCM consists of a thin piezoelectric plate with electrodes evaporated onto both sides. Due to the piezo-effect, an AC voltage across the electrodes induces a shear deformation and vice versa. The electromechanical coupling provides a simple way to detect an acoustic resonance by electrical means. Otherwise, it is of minor importance. However, electromechanical coupling can have a slight influence on the resonance frequency via piezoelectric stiffening. This effect can be used for sensing, but is usually avoided. It is essential to have the electric and dielectric boundary conditions well under control. Grounding the front electrode (the electrode in contact with the sample) is one option. A π-network sometimes is employed for the same reason. A π-network is an arrangement of resistors, which almost short-circuit the two electrodes. This makes the device less susceptible to electrical perturbations.\n\nMost acoustic-wave-based sensors employ shear (transverse) waves. Shear waves decay rapidly in liquid and gaseous environments. Compressional (longitudinal) waves would be radiated into the bulk and potentially be reflected back to the crystal from the opposing cell wall. Such reflections are avoided with transverse waves. The range of penetration of a 5 MHz-shear wave in water is 250 nm. This finite penetration depth renders the QCM surface-specific. Also, liquids and gases have a rather small shear-acoustic impedance and therefore only weakly damp the oscillation. The exceptionally high Q-factors of acoustic resonators are linked to their weak coupling to the environment.\n\nEconomic ways of driving a QCM make use of oscillator circuits. Oscillator circuits are also widely employed in time and frequency control applications, where the oscillator serves as a clock. Other modes of operation are impedance analysis, QCM-I, and ring-down, QCM-D. In impedance analysis, the electric conductance as a function of driving frequency is determined by means of a network analyzer. By fitting a resonance curve to the conductance curve, one obtains the frequency and bandwidth of the resonance as fit parameters. In ring-down, one measures the voltage between the electrodes after the exciting voltage has suddenly been turned off. The resonator emits a decaying sine wave, where the resonance parameters are extracted from the period of oscillation and the decay rate.\n\nThe electrodes at the front and the back of the crystal usually are key-hole shaped, thereby making the resonator thicker in the center than at the rim. This confines the displacement field to the center of the crystal by a mechanism called energy trapping. The crystal turns into an acoustic lens and the wave is focused to the center of the crystal. Energy trapping is necessary in order to be able to mount the crystal at the edge without excessive damping. Energy trapping slightly distorts the otherwise planar wave fronts. The deviation from the plane thickness-shear mode entails flexural contribution to the displacement pattern. Flexural waves emit compressional waves into the adjacent medium, which is a problem when operating the crystal in a liquid environment.\n\nPlanar resonators can be operated at a number of overtones, typically indexed by the number of nodal planes parallel to the crystal surfaces. Only odd harmonics can be excited electrically because only these induce charges of opposite sign at the two crystal surfaces. Overtones are to be distinguished from anharmonic side bands (spurious modes), which have nodal planes perpendicular to the plane of the resonator. The best agreement between theory and experiment is reached with planar, optically polished crystals for overtone orders between \"n\" = 5 and \"n\" = 13. On low harmonics, energy trapping is insufficient, while on high harmonics, anharmonic side bands interfere with the main resonance.\n\nThe amplitude of lateral displacement rarely exceeds a nanometer. More specifically one has\n\nformula_1\n\nwith \"u\" the amplitude of lateral displacement, \"n\" the overtone order, \"d\" the piezoelectric strain coefficient, \"Q\" the quality factor, and \"U\" the amplitude of electrical driving. The piezoelectric strain coefficient is given as \"d\" = 3.1·10 m/V for AT-cut quartz crystals. Due to the small amplitude, stress and strain usually are proportional to each other. The QCM operates in the range of linear acoustics.\n\nThe resonance frequency of acoustic resonators depends on temperature, pressure, and bending stress. Temperature-frequency coupling is minimized by employing special crystal cuts. A widely used temperature-compensated cut of quartz is the AT-cut. Careful control of temperature and stress is essential in the operation of the QCM.\n\nAT-cut crystals are singularly rotated Y-axis cuts in which the top and bottom half of the crystal move in opposite directions (thickness shear vibration) during oscillation. \nThe AT-cut crystal is easily manufactured. However, it has limitations at high and low temperature, as it is easily disrupted by internal stresses caused by temperature gradients in these temperature extremes (relative to room temperature, ~25 °C). These internal stress points produce undesirable frequency shifts in the crystal, decreasing its accuracy. The relationship between temperature and frequency is cubic. The cubic relationship has an inflection point near room temperature. As a consequence the AT-cut quartz crystal is most effective when operating at or near room temperature. For applications which are above room temperature, water cooling is often helpful.\n\nStress-compensated (SC) crystals are available with a doubly rotated cut that minimizes the frequency changes due to temperature gradients when the system is operating at high temperatures, and reduces the reliance on water cooling. SC-cut crystals have an inflection point of ~92 °C. In addition to their high temperature inflection point, they also have a smoother cubic relationship and are less affected by temperature deviations from the inflection point. However, due to the more difficult manufacturing process, they are more expensive and are not widely commercially available.\n\nThe QCM can be combined with other surface-analytical instruments. The electrochemical QCM (EQCM) is particularly advanced. Using the EQCM, one determines the ratio of mass deposited at the electrode surface during an electrochemical reaction to the total charge passed through the electrode. This ratio is called the current efficiency.\n\nFor advanced QCMs, both the resonance frequency, \"f\", and the bandwidth, \"w\", are available for analysis. The latter quantifies processes which withdraw energy from the oscillation. These may include damping by the holder and ohmic losses inside the electrode or the crystal. In the literature some parameters other than \"w\" itself are used to quantify bandwidth. The Q-factor (quality factor) is given by \"Q\" = \"f\"/\"w\". The “dissipation factor”, \"D\", is the inverse of the Q-factor: \"D\" = \"Q\" = \"w\"/\"f\". The half-band-half-width, Γ, is Γ = \"w\"/2. The use of Γ is motivated by a complex formulation of the equations governing the motion of the crystal. A complex resonance frequency is defined as \"f\" = \"f\" + iΓ, where the imaginary part, Γ, is half the bandwidth at half maximum. Using a complex notation, one can treat shifts of frequency, Δ\"f\", and bandwidth, ΔΓ, within the same set of (complex) equations.\n\nThe motional resistance of the resonator, \"R\", is also used as a measure of dissipation. \"R\" is an output parameter of some instruments based on advanced oscillator circuits. \"R\" usually is not strictly proportional to the bandwidth (although it should be according to the BvD circuit; see below). Also, in absolute terms, \"R\" – being an electrical quantity and not a frequency – is more severely affected by calibration problems than the bandwidth.\n\nModeling of acoustic resonators often occurs with equivalent electrical circuits. Equivalent circuits are algebraically equivalent to the continuum mechanics description and to a description in terms of acoustic reflectivities. They provide for a graphical representation of the resonator’s properties and their shifts upon loading. These representations are not just cartoons. They are tools to predict the shift of the resonance parameters in response to the addition of the load.\n\nEquivalent circuits build on the electromechanical analogy. In the same way as the current through a network of resistors can be predicted from their arrangement and the applied voltage, the displacement of a network of mechanical elements can be predicted from the topology of the network and the applied force. The electro-mechanical analogy maps forces onto voltages and speeds onto currents. The ratio of force and speed is termed “mechanical impedance”. Note: Here, speed means the time derivative of a displacement, not the speed of sound. There also is an electro-acoustic analogy, within which stresses (rather than forces) are mapped onto voltages. In acoustics, forces are normalized to area. The ratio of stress and speed should not be called \"acoustic impedance\" (in analogy to the mechanical impedance) because this term is already in use for the material property \"Z\" = ρ\"c\" with ρ the density and \"c\" the speed of sound). The ratio of stress and speed at the crystal surface is called load impedance, \"Z\". Synonymous terms are \"surface impedance\" and \"acoustic load.\" The load impedance is in general not equal to the material constant \"Z\" = ρ\"c\" = (\"G\"ρ). Only for propagating plane waves are the values of \"Z\" and \"Z\" the same.\n\nThe electro-mechanical analogy provides for mechanical equivalents of a resistor, an inductance, and a capacitance, which are the dashpot (quantified by the drag coefficient, ξ), the point mass (quantified by the mass, \"m\"), and the spring (quantified by the spring constant, κ). For a dashpot, the impedance by definition is \"Z\"=\"F\" / (d\"u\"/d\"t\")=ξ with \"F\" the force and (d\"u\"/d\"t\") the speed). For a point mass undergoing oscillatory motion \"u\"(\"t\") = \"u\" exp(iω\"t\") we have \"Z\" = iω\"m\". The spring obeys \"Z\" =κ/(iω). Piezoelectric coupling is depicted as a transformer. It is characterized by a parameter φ. While φ is dimensionless for usual transformers (the turns ratio), it has the dimension charge/length in the case of electromechanical coupling. The transformer acts as an impedance converter in the sense that a mechanical impedance, \"Z\", appears as an electrical impedance, \"Z\", across the electrical ports. \" Z\" is given by \"Z\" = φ \"Z\". For planar piezoelectric crystals, φ takes the value φ = \"Ae\"/\"d\", where \"A\" is the effective area, \"e\" is the piezoelectric stress coefficient (\"e\" = 9.65·10 C/m for AT-cut quartz) and \"d\" is the thickness of the plate. The transformer often is not explicitly depicted. Rather, the mechanical elements are directly depicted as electrical elements (capacitor replaces a spring, etc.).\n\nThere is a pitfall with the application of the electro-mechanical analogy, which has to do with how networks are drawn. When a spring pulls onto a dashpot, one would usually draw the two elements in series. However, when applying the electro-mechanical analogy, the two elements have to be placed in parallel. For two parallel electrical elements the currents are additive. Since the speeds (= currents) add when placing a spring behind a dashpot, this assembly has to be represented by a parallel network.\n\nThe figure on the right shows the Butterworth-van Dyke (BvD) equivalent circuit. The acoustic properties of the crystal are represented by the motional inductance, \"L\", the motional capacitance, \"C\", and the motional resistance \"R\". \"Z\" is the load impedance. Note that the load, \"Z\", cannot be determined from a single measurement. It is inferred from the comparison of the loaded and the unloaded state. Some authors use the BvD circuit without the load \"Z\". This circuit is also called “four element network”. The values of \"L\", \"C\", and \"R\" then change their value in the presence of the load (they do not if the element \"Z\" is explicitly included).\n\nThe BvD circuit predicts the resonance parameters. One can show that the following simple relation holds as long as the frequency shift is much smaller than the frequency itself:\n\nformula_2\n\n\"f\" is the frequency of the fundamental. \"Z\" is the acoustic impedance of material. For AT-cut quartz, its value is \"Z\" = 8.8·10 kg m s.\n\nThe small-load approximation is central to the interpretation of QCM-data. It holds for arbitrary samples and can be applied in an average sense. Assume that the sample is a complex material, such as a cell culture, a sand pile, a froth, an assembly of spheres or vesicles, or a droplet. If the average stress-to-speed ratio of the sample at the crystal surface (the load impedance, \"Z\") can be calculated in one way or another, a quantitative analysis of the QCM experiment is in reach. Otherwise, the interpretation will have to remain qualitative.\n\nThe limits of the small-load approximation are noticed either when the frequency shift is large or when the overtone-dependence of Δ\"f\" and Δ(\"w\"/2) is analyzed in detail in order to derive the viscoelastic properties of the sample. A more general relation is\n\nformula_3\n\nThis equation is implicit in Δ\"f\", and must be solved numerically. Approximate solutions also exist, which go beyond the small-load approximation. The small-load approximation is the first order solution of a perturbation analysis.\n\nThe definition of the load impedance implicitly assumes that stress and speed are proportional and that the ratio therefore is independent of speed. This assumption is justified when the crystal is operated in liquids and in air. The laws of linear acoustics then hold. However, when the crystal is in contact with a rough surface, stress can easily become a nonlinear function of strain (and speed) because the stress is transmitted across a finite number of rather small load-bearing asperities. The stress at the points of contact is high, and phenomena like slip, partial slip, yield, etc. set in. These are part of non-linear acoustics. There is a generalization of the small-load equation dealing with this problem. If the stress, σ(\"t\"), is periodic in time and synchronous with the crystal oscillation one has\n\nformula_4\n\nformula_5\n\nAngular brackets denote a time average and σ(\"t\") is the (small) stress exerted by the external surface. The function σ(t) may or may not be harmonic. One can always test for nonlinear behavior by checking for a dependence of the resonance parameters on the driving voltage. If linear acoustics hold, there is no drive level-dependence. Note, however, that quartz crystals have an intrinsic drive level-dependence, which must not be confused with nonlinear interactions between the crystal and the sample.\n\nFor a number of experimental configurations, there are explicit expressions relating the shifts of frequency and bandwidth to the sample properties. The assumptions underlying the equations are the following:\n\n\nFor a semi-infinite medium, one has\n\nformula_6\n\nformula_7\n\nη’ and η’’ are the real and the imaginary part of the viscosity, respectively. \"Z\" = ρ\"c\" =(\"G\" ρ) is the acoustic impedance of the medium. ρ is the density, \"c\", the speed of sound, and \"G\" = i ωη is the shear modulus.\nFor Newtonian liquids (η’ = const, η’’ = 0), Δ\"f\" and Δ(\"w\"/2) are equal and opposite. They scale as the square root of the overtone order, \"n\". For viscoelastic liquids (η’ = η(ω), η’’≠ 0), the complex viscosity can be obtained as\n\nformula_8\n\nformula_9\n\nImportantly, the QCM only probes the region close to the crystal surface. The shear wave evanescently decays into the liquid. In water the penetration depth is about 250 nm at 5 MHz. Surface roughness, nano-bubbles at the surface, slip, and compressional waves can interfere with the measurement of viscosity. Also, the viscosity determined at MHz frequencies sometimes differs from the low-frequency viscosity. In this respect, torsional resonators (with a frequency around 100 kHz) are closer to application than thickness-shear resonators.\n\nThe frequency shift induced by a thin sample which is rigidly coupled to the crystal (such as a thin film), is described by the Sauerbrey equation. The stress is governed by inertia, which implies σ = -ω\"u\"\"m\", where \"u\" is the amplitude of oscillation and \"m\" is the (average) mass per unit area. Inserting this result into the small-load-approximation one finds\n\nformula_10\n\nIf the density of the film is known, one can convert from mass per unit area, \"m\", to thickness, \"d\". The thickness thus derived is also called the Sauerbrey thickness to show that it was derived by applying the Sauerbrey equation to the frequency shift.\nThe shift in bandwidth is zero if the Sauerbrey equation holds. Checking for the bandwidth therefore amounts to checking the applicability of the Sauerbrey equation.\n\nThe Sauerbrey equation was first derived by G. Sauerbrey in 1959 and correlates changes in the oscillation frequency of a piezoelectric crystal with mass deposited on it. He simultaneously developed a method for measuring the resonance frequency and its changes by using the crystal as the frequency-determining component of an oscillator circuit. His method continues to be used as the primary tool in quartz crystal microbalance experiments for conversion of frequency to mass.\n\nBecause the film is treated as an extension of thickness, Sauerbrey’s equation only applies to systems in which (a) the deposited mass has the same acoustic properties as the crystal and (b) the frequency change is small (Δ\"f\" / \"f\" < 0.05).\n\nIf the change in frequency is greater than 5%, that is, Δ\"f\" / \"f\" > 0.05, the Z-match method must be used to determine the change in mass. The formula for the Z-match method is:\n\nformula_11\n\n\"k\" is the wave vector inside the film and \"d\" its thickness. Inserting \"k\" = 2·π·\"f\" /c = 2·π·\"f\"·ρ / \"Z\" as well as \"d\" = \"m\" / ρ yields\n\nformula_12\n\nFor a viscoelastic film, the frequency shift is\n\nformula_13\n\nHere \"Z\" is the acoustic impedance of the film (\"Z\" = ρ\"c\" = (ρ\"G\"))= (ρ/\"J\")), \"k\" is the wave vector and \"d\" is the film thickness. \"J\" is the film's viscoelastic compliance, ρ is the density.\n\nThe poles of the tangent (\"k\" \"d\" = π/2) define the film resonances. At the film resonance, one has \"d\" = λ/4. The agreement between experiment and theory is often poor close to the film resonance. Typically, the QCM only works well for film thicknesses much less than a quarter of the wavelength of sound (corresponding to a few micrometres, depending on the softness of the film and the overtone order).\n\nNote that the properties of a film as determined with the QCM are fully specified by two parameters, which are its acoustic impedance, \"Z\" = ρ\"c\" and its mass per unit area, \"m\" = \"d\"/ρ. The wave number \"k\" = ω/\"c\" is not algebraically independent from \"Z\" and \"m\". Unless the density of the film is known independently, the QCM can only measure mass per unit area, never the geometric thickness itself.\n\nFor a film immersed in a liquid environment, the frequency shift is\n\nformula_14\n\nThe indices \"F\" and \"Liq\" denote the film and the liquid. Here, the reference state is the crystal immersed in liquid (but not covered with a film). For thin films, one can Taylor-expand the above equation to first order in \"d\", yielding\n\nformula_15\n\nApart from the term in brackets, this equation is equivalent to the Sauerbrey equation. The term in brackets is a viscoelastic correction, dealing with the fact that in liquids, soft layers lead to a smaller Sauerbrey thickness than rigid layers.\n\nThe frequency shift depends on the acoustic impedance of the material; the latter in turn depends on the viscoelastic properties of the material. Therefore, in principle, one can derive the complex shear modulus (or equivalently, the complex viscosity). However, there are certain caveats to be kept in mind:\n\n\nFor thin films in liquids, there is an approximate analytical result, relating the elastic compliance of the film, \"J\"’ to the ratio of Δ(w/2); and Δ\"f\". The shear compliance is the inverse of the shear modulus, \"G\". In the thin-film limit, the ratio of Δ(w/2) and –Δ\"f\" is independent of film thickness. It is an intrinsic property of the film. One has\n\nformula_16\n\nFor thin films in air an analogous analytical result is\n\nformula_17\n\nHere \"J\"’’ is the viscous shear compliance.\n\nThe correct interpretation of the frequency shift from QCM experiments in liquids is a challenge. Practitioners often just apply the Sauerbrey equation to their data and term the resulting areal mass (mass per unit area) the \"Sauerbrey mass\" and the corresponding thickness \"Sauerbrey thickness\". Even though the Sauerbrey thickness can certainly serve to compare different experiments, it must not be naively identified with the geometric thickness. Worthwhile considerations are the following:\n\na) The QCM always measures an areal mass density, never a geometric thickness. The conversion from areal mass density to thickness usually requires the physical density as an independent input.\nb) It is difficult to infer the viscoelastic correction factor from QCM data. However, if the correction factor differs significantly from unity, it may be expected that it affects the bandwidth Δ(w/2) and also that it depends on overtone order. If, conversely, such effects are absent (Δ(\"w\"/2) « Δ\"f\", Sauerbrey thickness same on all overtone orders) one may assume that (1-\"Z\"/\"Z\")≈1.\n\nc) Complex samples are often laterally heterogeneous.\n\nd) Complex samples often have fuzzy interfaces. A \"fluffy\" interface will often lead to a viscoelastic correction and, as a consequence, to a non-zero Δ(\"w\"/2) as well as an overtone-dependent Sauerbrey mass. In the absence of such effects, one may conclude that the outer interface of film is sharp.\n\ne) When the viscoelastic correction, as discussed in (b), is insignificant, this does by no means imply that the film is not swollen by the solvent. It only means that the (swollen) film is much more rigid than the ambient liquid. QCM data taken on the wet sample alone do not allow inference of the degree of swelling. The amount of swelling can be inferred from the comparison of the wet and the dry thickness. The degree of swelling is also accessible by comparing the acoustic thickness (in the Sauerbrey sense) to the optical thickness as determined by, for example, surface plasmon resonance (SPR) spectroscopy or ellipsometry. Solvent contained in the film usually does contribute to the acoustic thickness (because it takes part in the movement), whereas it does not contribute to the optic thickness (because the electronic polarizability of a solvent molecule does not change when it is located inside a film). The difference in dry and wet mass is shown with QCM-D and MP-SPR for instance in protein adsorption on nanocellulose and in other soft materials.\n\nThe equations concerning viscoelastic properties assume planar layer systems. A frequency shift is also induced when the crystal makes contact with discrete objects across small, load-bearing asperities. Such contacts are often encountered with rough surfaces. It is assumed that the stress–speed ratio may be replaced by an average stress–speed ratio, where the average stress just is the lateral force divided by the active area of the crystal.\n\nOften, the external object is so heavy that it does not take part in the MHz oscillation of the crystal due to inertia. It then rests in place in the laboratory frame. When the crystal surface is laterally displaced, the contact exerts a restoring force upon the crystal surface. The stress is proportional to the number density of the contacts, \"N\", and their average spring constant, κ. The spring constant may be complex (κ = κ’ + iκ’’), where the imaginary part quantifies a withdrawal of energy from the crystal oscillation (for instance due to viscoelastic effects). For such a situation, the small-load approximation predicts\n\nformula_18\n\nThe QCM allows for non-destructive testing of the shear stiffness of multi-asperity contacts.\n\n\n"}
{"id": "331097", "url": "https://en.wikipedia.org/wiki?curid=331097", "title": "Saab 9-5", "text": "Saab 9-5\n\nThe Saab 9-5 is an executive car that was produced by the Swedish automobile maker Saab.\n\nThe first generation 9-5 was introduced in 1997, for the 1998 model year, as the replacement to the Saab 9000. At the time, the car represented a significant development for the manufacturer. In the United States, the 9-5 was introduced in the spring of 1998, for the 1999 model year.\n\nOn September 15, 2009, the second generation was presented at the Frankfurt Motor Show and production began in June 2010. It was the first Saab to be launched under Spyker Cars' ownership, even though it was developed almost completely under GM's ownership.\n\nSaab badged the model as the Saab 9, but consistently advertised it as the Saab 9-5, pronounced \"nine five\" rather than \"ninety-five\". This model should not be confused with the Saab 95, produced from 1959 to 1978.\n\nThe first generation 9-5 was available with sedan and station wagon body styles. Aerodynamically, the sedan's drag coefficient is 0.29, and the station wagon's is 0.31 (U.S. version 0.33). Introduced in 1999, the wagon features innovations such as floor tracks to secure cargo and a sliding load floor to make loading easier.\n\nThe 9-5 was the first production vehicle to offer ventilated seats, as well as asymmetrical turbocharging in the case of the 3.0L V6 engine.\n\nThe last 9-5 sedan of the first generation rolled off the Trollhättan production line at the beginning of July 2009, and the last wagon was assembled on February 1, 2010. Between the summer of 1997, when 9-5 production began, and 2010, 252,236 sedans, and 231,357 wagons were built. The total production 483,593 units, was narrowly beaten by its predecessor, the 9000, of which 503,000 were built.\n\nProduction equipment for the first-generation 9-5 was sold by General Motors to BAIC of China in 2009.\nThe first-generation 9-5 was powered by Saab's B205 and B235 straight-4 engines, and from 2002 in Europe by an Opel Ecotec X22DTH 2.2 diesel engine (Saab D223L), replaced in 2006 by Fiat's 1.9 JTD 16V diesel straight-4. A turbocharged version of the GM 54° V6 engine, designated by Saab as B308, had a unique asymmetrical low-pressure turbocharger and was available from 1999 to 2003. This engine was available only with an automatic transmission, and cars with this engine installed are distinguishable by their twin tailpipes. The V6 was only available on Arc, SE, and Griffin models. In 2004, the V6 engine was replaced by a high pressure turbo straight-4 engine producing . By 2006 this engine was producing even in the non-Aero or non-sport models (US models).\n\nThe B205 & B235-based 9-5 models have suffered a high rate of engine failures due to engine oil sludge. This primarily affected the 1999–2003 models. Saab refined the engine's positive crankcase ventilation system (PCV) for the 2004 and later model years and required use of fully synthetic oil, virtually eliminating the problem. Additionally, SAAB created update kits to retrofit to the 1999–2003 cars since they cannot be easily modified to accept the 2004 and later system. Mounting complaints by 9-5 owners forced GM to offer an eight-year warranty on the engine in 4-cylinder models for original owners, provided the owner can produce proof that they followed the manufacturer's oil change intervals. Saab recommended the use of fully synthetic or synthetic-blend oil as a preventative measure.\n\nThe 9-5 was available with an Aisin AW 4-speed (50-42LE) automatic transmission saab reference FA47; from 1997 until 2001, when a new Aisin AW unit replaced the dated four-speed automatic with a five-speed automatic. A five-speed manual transmission is fitted as standard to the base models and the Aero. Six-speed manual transmission was not offered in first-generation 9-5s.\n\nIn 2005, an updated version of the 2.0 L turbocharged I4 was introduced in the European market together with the 2006 9-5. The engine was sold as \"2.0T BioPower\", optimized to run on E85 producing 132 kW (180 hp) at 5500 rpm. There was also a 2.3T BioPower version sold from 2007. It was also introduced in Australia.\n\nThere is a Saab 9-5 E100 Concept, based on the turbo 2.0.\n\nThe 9-5 introduced Saab's Active Head Restraints (SAHR), which moved up and forward to prevent whiplash when the car was struck from the rear. This feature won technology and safety awards in Australia, Denmark, and the United Kingdom. The Saab 9-5 also was one of the first cars to have extensive side-crash protection.\n\nThe front seats featured torso airbags and head airbags even on even the earliest models, which few contemporary vehicles did in the late 1990s. The basic structure included a robust passenger safety cage, front and rear deformation zones, reinforced door posts and pillars, as well as the \"Pendulum B-Pillar\", which combined high-strength low-alloy steel at chest and head height with tailored blank steel at the floorpan, designed to direct the crash forces down toward the floor. The design was proven by the Insurance Institute for Highway Safety (IIHS) to protect occupants in side crashes, even without the addition of curtain airbags or rear side airbags. From 2002, ESP (electronic stability control) was included as standard.\n\nAnother Saab feature, the \"Night Panel\", permitted dousing of the instrument panel lighting, except for essential information, for less distraction when driving at night. Once activated, only essential information such as current speed is displayed except, for example, if the car requires fuel or the engine overheats.\n\nIn the United States OnStar was available, and provided as standard equipment in selected 9-5's from 2001 onward.\n\nThe 9-5 had various comfort features both as standard and cost options over the years.\n\nWhile early models frequently had dash mounted cassette decks, CD changers were standard features on many cars and in-dash satellite navigation was also available. Factory-fitted phone kits were similarly optional.\n\nMany models featured leather or part-leather upholstery and both front and rear heated seats were also available. A few models were shipped with the optional ventilated seats.\n\nCruise control was available on various models and xenon headlamps were fitted as standard on high-end variants.\n\nThe performance 9-5 Aero, the earliest versions of which were sometimes referred to as the HOT Aero, was first released in 2000 with a 2.3T B235R engine. The B235R engine of the 9-5 Aero was capable of providing immense torque and, in terms of acceleration, outperformed the contemporary Porsche 911 Turbo from 40-90 mph.\n\nInitially badged as a 230ps engine, Saab later conceded that the 230 hp power figure was quite conservative, with the manual versions rated 250 hp and having more torque than stated. This flagship model had a long list of standard features, a sport tuned suspension, and body side moldings. In 2002 a 2.3 turbo engine was made standard, which allowed for more torque after 4500 rev/min. All Aero models from 2002-2005 have an identical engine layout and management system, with the 2002-05 models just having a slightly remapped version of that ECU from factory. The high-powered version of the 9-5 in the final form produces and of torque ( with its 20-second overboost function accessible on the manual transmission equipped version.\n\nFrom model year 2006 to end of production, the B235R was the standard engine in the 9-5 in both the 2.3T and Aero trims. 2006 had only one badge designation, the 2.3T and appointments normally found on the Aero could be added via a \"Sport Package\".\n\nFrom 2007 onwards, SAAB added an Aero badge to the trunk lid to distinguish from regular 2.3T models. In addition, almost all standard features on the Aero were standard on the 2.3T, the exceptions being sport-tuned chassis, two-tone leather upholstery, \"Anniversary\" wheels and brushed aluminum interior trim, all of which were standard on Aero and not available on the 2.3T.\n\nThe 9-5 was used as a liveried patrol vehicle, as well as undercover, in several parts of its native Sweden, alongside the Volvo V70. Several police forces in the UK also used the 9-5 in their fleets, mostly in Aero specification. The city of Aspen, Colorado, used Saabs as patrol cars from early 1970s until 2005, when the 9-5 was discontinued in favor of Volvo XC90. The town of Vail, Colorado likewise used Saabs from 1980 onwards, but in 2005, the black 9-5 patrol cars were replaced by Ford Explorers, due to budget reasons.\n\nIn 2006, Lothian and Borders Police in Edinburgh, Scotland, began operating three Saab 9-5 Aero 2.3T patrol cars as part of a fleet of 580 vehicles. These 9-5s were customised to police specifications by the Saab, Vauxhall and Chevrolet Special Vehicles Operation (SVO) in Papworth, Cambridgeshire. In undercover guise, these cars were outwardly identical to the Linear Sport models, but featured the 260 bhp Aero drivetrain.\n\nIn Poland, an unmarked 9-5 is used as a video-pursuit vehicle, in the Płock area\n\nA next generation 9-5 built on the Global Epsilon platform was presented at the Frankfurt International Auto Show in September 2009. The vehicle had its North American debut in October 2009 at the South Florida Auto Show in Miami.\nOn November 24, 2009, the first pre-series Saab 9-5 of the new generation rolled off the Trollhättan production line.\n\nWith the announcement of the sale of Saab to Spyker on January 26, 2010, it was confirmed that the new generation Saab 9-5 was already taken into production at the Saab plant in Trollhättan. Full production began in April 2010, with the cars appearing in dealerships on June 19. Saab introduced a wagon variant of the new 9-5, dubbed \"SportCombi,\" at the 2011 Geneva Motor Show. The Saab 9-5 Sedan 2.8V6 Turbo was named Car of the Year in Singapore by \"Wheels Asia\".\n\nAs Saab's business issues came to a head, production of 2011 9-5 models essentially ended in March 2011, when the production line in Trollhättan was stopped due to supply chain issues that were related to the company's lack of liquidity. Total production numbers of the Gen II Saab 9-5 ended at 11,280 units. \n\nAlthough some prototype pilots and a number of production SportCombis were produced, the official variant did not enter serial production prior to Saab's bankruptcy in December 2011, and the vehicles never reached dealerships.\n\nTrim/equipment levels vary from country to country.\n\nIn the US Saab 9-5 trim levels were: Turbo4, Turbo4 Premium, Turbo6 XWD, and Aero. Turbo4 models come with a turbocharged four-cylinder and features that included power adjustable driver and passenger’s seats, leather upholstery, five-spoke alloy wheels, fog lamps, and rain-sensing wipers. \nThe Turbo4 Premium added a panoramic sunroof, headlamp washers, Saab parking assistance, keyless entry and start, memory seats, and 18-inch alloy wheels, while the Turbo6 XWD was powered by a turbocharged six-cylinder engine and features an all-wheel-drive system. \nThe top trim Aero featured 15-spoke “Rotor” 18-inch alloy wheels, leather-trimmed sports seats, a multi-color central information display, Bi-Xenon SmartBeam headlamps, dark titanium-effect interior trim, aluminum sports pedals, a sports-tuned suspension system with real-time damping, and Aero exterior elements.\n\nUK equipment levels for the 2012 model year included the Vector SE and Aero and both sedan and estate. The previous base models, Linear and Vector were replaced by the Vector SE model.\n\nIn Australia the base trim (Linear) was not part of the line up, only the Vector and Aero trims were available.\n\nIn North America, the engine choices were either a turbocharged V6 or an Ecotec I4. Other countries also had an optional turbodiesel I4 engine. Engine performance upgrades that were available from Hirsch Performance (Saab's only factory approved tuner) increased the power of the V6 engine to from and the I4 engine to from .\n\n"}
{"id": "3415348", "url": "https://en.wikipedia.org/wiki?curid=3415348", "title": "Sgoldstino", "text": "Sgoldstino\n\nA sgoldstino is any of the spin-0 superpartners of the goldstino in relativistic quantum field theories with spontaneously broken supersymmetry. The term \"sgoldstino\" was first used in 1998.\n\nIn 2016, Petersson and Torre hypothesized that a sgoldstino particle might be responsible for the observed 750 GeV diphoton excess observed by Large Hadron Collider experiments.\n"}
{"id": "15305798", "url": "https://en.wikipedia.org/wiki?curid=15305798", "title": "Silver mica capacitor", "text": "Silver mica capacitor\n\nSilver mica capacitors are high precision, stable and reliable capacitors. They are available in small values, and are mostly used at high frequencies and in cases where low losses (high Q) and low capacitor change over the time is desired.\n\nCornell Dubilier Electronics, Inc. was the inventor of the mica capacitor. RF mica capacitors feature high Q performance without the cracking associated with ceramic and porcelain capacitors. They are still used in military, aerospace, medical and audio applications.\n\nThere are 2 distinct types of mica capacitor.\n\nNow obsolete, these were in use in the early 20th century. They consisted of sheets of mica and copper foil sandwiched together and clamped. These had even worse tolerance and stability than other clamped capacitors since the mica surface is not perfectly flat & smooth. References to mica capacitors from the 1920s always refers to this type.\n\nCommonly known as silver mica capacitors, these rendered clamped mica capacitors obsolete. Instead of being clamped with foils these are assembled from sheets of mica coated on both sides with deposited metal. The assembly is dipped in epoxy. The advantages are:\n\nThey are sometimes informally referred to as mica capacitors. Any modern reference to mica capacitors can be assumed to mean these, unless pre-war equipment is being discussed. Even though these capacitors are extremely useful, Silver mica capacitors are less commonly used today due to bulkiness and high cost. There is a high level of compositional variation in the raw material leading to higher costs in relation to inspection and sorting. They are getting closer to obsolescence as advances are made in ceramic and porcelain materials.\n\nSilver mica capacitors are still indispensable in some custom applications. Circuit designers still turn to mica capacitors for high-power applications such as RF transmitters and electric instruments and amplifiers because cheaper ceramic and porcelain capacitors can't withstand heat as well. Silver mica remains widely used in high-voltage applications, due to mica’s high breakdown voltage. Silver Mica capacitors are used at 100V to 10KV, ranging from 47pf to 3000dpf and the average temperature coefficient is around 50 ppm/°C. \n\nSilver Mica capacitors are soldered to volume potentiometers in some electric guitars to prevent treble bleed (loss) when the volume is lowered. By using a particular capacitance range (pF) mica capacitor, the higher frequencies pass through the volume potentiometer providing very little tonal coloration and clear harmonics. Values such as 180pf and 280pf at 400V and 500V are often used.\n\n"}
{"id": "3480112", "url": "https://en.wikipedia.org/wiki?curid=3480112", "title": "Skid (aerodynamics)", "text": "Skid (aerodynamics)\n\nIn a straight flight, the tail of the airplane aligns the fuselage into the relative wind. However, in the beginning of a turn, when the ailerons are being applied in order to bank the airplane, the ailerons also cause an adverse yaw of the airplane. For example, if the airplane is rolling clockwise (from the pilot point of view), the airplane yaws to the left. It assumes a crab-like attitude relative to the wind. This is called a slip. The air is flowing crosswise over the fuselage. In order to correct this adverse slip, the pilot must apply rudder (right rudder in this example). If the pilot applies too much rudder, the airplane will then slip to the other side. This is called a skid.\n\nThe skid is more dangerous than the slip if the airplane is close to a stall. In the slip, the raised wing — the left one if the airplane is turning to the right — will stall before the lowered one, and the airplane will reduce the bank angle, which prevents the stall. In the skid, the lowered wing will stall before the raised one, and the airplane will tighten the turn, and the stall can develop to a spin.\n\nAt high altitudes, there is plenty of space for recovery. But during the final approach, when the airplane is close to the ground, a stall-spin accident is often fatal. A common cause of this accident is to enter a skidding turn in the airfield traffic pattern on the turn from base leg to final approach, unconsciously using excessive rudder in an attempt to tighten the turn and avoid overshooting the runway centreline.\n\nDeliberate skids are used in aerobatics and aerial combat. Deliberate slips done with vigorous application of roll and opposite rudder (lower the right wing and step on the left rudder) can be used as a dive brake. By balancing the roll's turn to the right with the rudder's yaw to the left, the plane continues to fly straight ahead but it presents its side rather than its nose to the airstream. The drag from this aerodynamically \"dirty\", clumsy position slows the otherwise sleek airplane. By modulating the amount of skid with rudder and aileron, the pilot can modulate the braking. Thus the plane can be slowed down quickly in level flight or the descent to a landing can be dramatically steepened while holding the approach speed to a desired value.\n\n"}
{"id": "12516292", "url": "https://en.wikipedia.org/wiki?curid=12516292", "title": "Soil solarization", "text": "Soil solarization\n\nSoil solarization is an environmentally friendly method of using solar power for controlling pests such as soilborne plant pathogens including fungi, bacteria, nematodes, and insect and mite pests along with weed seed and seedlings in the soil by mulching the soil and covering it with tarp, usually with a transparent polyethylene cover, to trap solar energy.\nIt may also describe methods of decontaminating soil using sunlight or solar power. This energy causes physical, chemical, and biological changes in the soil.\n\nSoil solarization (referred to as solar heating of the soil in early publications) is a relatively new soil disinfestation method, first described in extensive scientific detail by Katan et al. in 1976, presenting the results of a series of studies performed under field conditions, initiated in 1973, for controlling soilborne pathogens and weeds, mostly as a pre-planting soil treatment. Soil is mulched and then covered with transparent polyethylene during the hot season, thereby heating it and killing the pests and diseases\n\nA 2008 study used a solar cell to generate an electric field for electrokinetic (EK) remediation of cadmium-contaminated soil. The solar cell could drive the electromigration of cadmium in contaminated soil, and the removal efficiency that was achieved by the solar cell was comparable with that achieved by conventional power supply.\n\nIn Korea, various remediation methods of soil slurry and groundwater contaminated with benzene at a polluted gas station site were evaluated, including a solar-driven, photocatalyzed reactor system along with various advanced oxidation processes (AOP). The most synergistic remediation method incorporated a solar light process with TiO2 slurry and H2O2 system, achieving 98% benzene degradation, a substantial increase in the removal of benzene.\n\nAttempts were made to use solar energy for controlling disease agents in soil and in plant material already in the ancient civilization of India. In 1939, Groashevoy, who used the term \"solar energy for sand disinfection,\" controlled Thielaviopsis basicola upon heating the sand by exposure to direct sunlight.\n\nSoil solarization is the third approach for soil disinfestation; the two other main approaches, soil steaming and fumigation; were developed at the end of the 19th century. The idea of solarization was based on observations by extension workers and farmers in the hot Jordan Valley, who noticed the intensive heating of the polyethylene-mulched soil. The involvement of biological control mechanisms in pathogen control and the possible implications were indicated in the first publication, noticing the very long effect of the treatment. In 1977, American scientists from the University of California at Davis reported the control of Verticillium in a cotton field, based on studies started in 1976, thus denoting, for the first time, the possible wide applicability of this method.\n\nThe use of polyethylene for soil solarization differs in principle from its traditional agricultural use. With solarization, soil is mulched during the hottest months (rather than the coldest, as in conventional plasticulture which is aimed at protecting the crop) in order to increase the maximal temperatures in an attempt to achieve lethal heat levels.\n\nIn the first 10 years following the influential 1976 publication, soil solarization was investigated in at least 24 countries and has been now been applied in more than 50, mostly in the hot regions, although there were some important exceptions. Studies have demonstrated effectiveness of solarization with various crops, including vegetables, field crops, ornamentals and fruit trees, against many pathogens, weeds and a soil arthropod. Those pathogens and weeds which are not controlled by solarization were also detected. The biological, chemical and physical changes that take in solarized soil during and after the solarization have been investigated, as well as the interaction of solarization with other methods of control. Long-term effects including biological control and increased growth response were verified in various climatic regions and soils, demonstrating the general applicability of solarization. \nComputerized simulation models have been developed to guide researchers and growers whether the ambient conditions of their locality are suitable for solarization.\n\nStudies of the improvement of solarization by integrating it with other methods or by solarizing in closed glasshouses, or studies concerning commercial application by developing mulching machines were also carried out.\n\nThe use of solarization in existing orchards (e.g. controlling Verticillium in pistachio plantations) is an important deviation from the standard preplanting method and was reported as early as 1979.\n"}
{"id": "12423000", "url": "https://en.wikipedia.org/wiki?curid=12423000", "title": "Solar controller", "text": "Solar controller\n\nA solar controller is an electronic device that controls the circulating pump in a solar hot water system to harvest as much heat as possible from the solar panels and protect the system from overheating. The basic job of the controller is to turn the circulating pump on when there is heat available in the panels, moving the working fluid through the panels to the heat exchanger at the thermal store. Heat is available whenever the temperature of the solar panel is greater than the temperature of the water in the heat exchanger. Overheat protection is achieved by turning the pump off when the store reaches its maximum temperature and sometimes cooling the store by turning the pump on when the store is hotter than the panels.\n\nMost commercial controllers display the temperature of the hot water in the store and provide general status information about the system, including overall energy production.\n\nThe simplest solar controller circuit uses a comparator with two temperature inputs, one at the solar panel and one at the thermal store's heat exchanger, and an output to control the pump. Commercial controllers use a microprocessor usually with a LCD display and simple user interface with a few pushbuttons. Power for the controller and the pump can come from a mains electric supply or from a photovoltaic (PV) module.\n\nThe controller's main function is to switch the circulating pump on or off. The pump is usually switched on when the solar panel is hotter than the water in the store's heat exchanger and off when the panel is colder. Switching the pump on transfers the heat in the panel to the store. Switching it off when the panels cool prevents a reversal of the process and loss of heat from the store. The controller measures and compares the temperatures in the panel and the heat exchanger every few seconds.\n\nCommercial controllers do not turn on the pump until the difference in temperature between the panels and the water in the heat exchanger is sufficient to provide significantly more energy than is consumed by the pump. This temperature difference is called the \"on differential\" (usually 4–15 °C. They turn off the pump when the panels no longer are hot enough to provide significant heat to the store (the \"off differential\"). The wider the difference between these differentials, the fewer pump on-off cycles will take place. These factors are usually set by the solar installer in relation to the particular installation, especially dependent on the efficiency of the heat exhchanger and production capacity of the panels.\n\nControllers provide an overrun time to extract some of the heat energy left in interconnecting pipes after the panels cool off. They may also implement certain safety features such as cooling the store when it exceeds a preset temperature such as 65 °C, by sending excess heat back to the panels to be given off to the environment.\n\nA photovoltaic (PV) powered solar controller uses solar electricity produced on-site to run the pump that delivers the solar-heated transfer fluid to the hot water store.\n\nOne claimed advantage of PV power is that it reduces the overall carbon emissions associated with operating the system since it avoids the need to supply this energy from fossil sources. However, the energy required to operate the system is very small in comparison to the energy produced by the system and the carbon emissions reduction of adding PV power fractional.\n\nThe most practical benefit of a PV powered controller is the resultant simplicity of the overall system. Rather than using complex algorithms based on store and panel temperatures, the pump is driven directly by the PV panel: when the sun shines, the pump runs. In practice this is nearly (90-99%) as efficient a practical control algorithm as most others achieve and has obvious advantages for reduced system complexity.\n\nA disadvantage to the PV powered approach is that the pump stops immediately after the sun is occluded. With vacuum tube and heat pipe solar panels, these can have an appreciable amount of energy stored in each tube at the moment the sun goes in. To avoid overheating the tubes it is necessary to either pump the circuit for a short time after the sun, or else to provide a large reservoir of fluid in the header above the tubes. Neither of these options is really compatible with the simple direct-PV pump approach and so such systems are limited to using the less efficient flat panel collectors.\n\nA PV powered controller may contain a small electricity store to allow the controller to remain powered and display temperatures at night when there is no sunlight. This electricity store is usually in the form of supercapacitors, since these have a much longer life than batteries.\n\nThe benefits of a PV powered solar controller comes at a cost in reduced system performance in the range of 1-10%. This is due to heat losses at times when the panel may be hotter than the water store but there is insufficient sunlight to power the pump. This happens mainly on hot days when hot water is likely to be in excess so the potential reduction is less significant than it would be at times when the store was cooler.\n\n\n"}
{"id": "57210774", "url": "https://en.wikipedia.org/wiki?curid=57210774", "title": "Spinor condensate", "text": "Spinor condensate\n\nSpinor condensates are degenerate Bose gases that have degrees of freedom arising from the internal spin of the constituent particles \nThey are described by a multi-component (spinor) order parameter.\nSince their initial experimental realisation,\na wealth of studies have appeared, both\nexperimental and theoretical, focusing\non the physical properties of spinor condensates, including their \nground states, non-equilibrium dynamics, and\nvortices.\n\nThe study of spinor condensates was initiated in 1998 by experimental groups at JILA \nand MIT. These experiments utilised\nNa and Rb atoms, respectively.\nIn contrast to most prior experiments on ultracold gases, these experiments utilised a purely\noptical trap, which is spin-insensitive. Shortly thereafter, theoretical work appeared\nwhich described the possible mean-field phases of spin-one spinor condensates.\n\nThe Hamiltonian describing a spinor condensate is most frequently written using the language of \nsecond quantization. Here the field operator\nformula_1\ncreates a boson in Zeeman level formula_2 at position formula_3. These\noperators satisfy bosonic commutation relations:\n\nformula_4\n\nThe free (non-interacting) part of the Hamiltonian is\n\nformula_5\n\nwhere formula_2 denotes the mass of the constituent particles and \nformula_7 is an external potential.\nFor a spin-one spinor condensate, the interaction Hamiltonian is\nformula_8\n\nIn this expression, \nformula_9\nis the operator corresponding to the density, \nformula_10\nis the local spin operator (formula_11\nis a vector composed of the spin-one matrices),\nand :: denotes normal ordering. The parameters formula_12\ncan be expressed in terms of the s-wave scattering lengths of the constituent particles.\nHigher spin versions of the\ninteraction Hamiltonian are slightly more involved, but \ncan generally be expressed by using Clebsch–Gordan coefficients.\n\nThe full Hamiltonian then is formula_13.\n\nIn Gross-Pitaevskii mean field theory, one replaces the field operators with c-number functions:\nformula_14. To find the mean-field\nground states, one then minimises the resulting energy with respect to these c-number functions.\nFor a spatially uniform system spin-one system, there are two possible mean-field ground states. \nWhen formula_15, the ground state is\nformula_16\nwhile for formula_17 the ground state is \nformula_18\nThe former expression is referred to as the polar state while the latter is the \nferromagnetic state.\nBoth states are unique up to overall spin rotations. Importantly, formula_19\ncannot be rotated into formula_20.\nThe Majorana stellar representation \ncondensates with larger spin.\n\nDue to being described by a multi-component order parameter, numerous types of\ntopological defects (vortices) can appear in spinor condensates\nHomotopy theory provides a natural description of topological defects\n, and is regularly employed to understand\nvortices in spinor condensates.\n"}
{"id": "18306410", "url": "https://en.wikipedia.org/wiki?curid=18306410", "title": "Sunbreak", "text": "Sunbreak\n\nA sunbreak is a natural phenomenon in which sunlight obscured over a relatively large area penetrates the obscuring material in a localized space. The typical example is of sunlight shining through a hole in cloud cover. A sunbreak piercing clouds normally produces a visible shaft of light reflected by atmospheric dust, called a sunbeam. Another form of sunbreak occurs when sunlight passes into an area otherwise shadowed by surrounding large buildings through a gap temporarily aligned with the position of the sun.\n\nThe word is considered by some to have origins in Pacific Northwest English.\n\nArtists such as cartoonists and filmmakers often use sunbreak to show protection or relief being brought upon an area of land by God or a receding storm.\n"}
{"id": "5486670", "url": "https://en.wikipedia.org/wiki?curid=5486670", "title": "Tasmanian Conservation Trust", "text": "Tasmanian Conservation Trust\n\nTasmanian Conservation Trust is a Tasmania's oldest non-profit conservation organisation; it was formed in 1968. The Trust has a comprehensive and interactive website Tasmanian Conservation Trust and operates a Facebook page.\n\nThe Trust is an independent advocate and guardian for conservation and biodiversity in Tasmania.\n\nThe Trust publishes a quarterly journal \"Tasmanian Conservationist\" on current conservation issues, campaigns and news updates.\n\n\n\n"}
{"id": "241026", "url": "https://en.wikipedia.org/wiki?curid=241026", "title": "Up quark", "text": "Up quark\n\nThe up quark or u quark (symbol: u) is the lightest of all quarks, a type of elementary particle, and a major constituent of matter. It, along with the down quark, forms the neutrons (one up quark, two down quarks) and protons (two up quarks, one down quark) of atomic nuclei. It is part of the first generation of matter, has an electric charge of + \"e\" and a bare mass of .. Like all quarks, the up quark is an elementary fermion with spin , and experiences all four fundamental interactions: gravitation, electromagnetism, weak interactions, and strong interactions. The antiparticle of the up quark is the up antiquark (sometimes called \"antiup quark\" or simply \"antiup\"), which differs from it only in that some of its properties, such as charge have equal magnitude but opposite sign.\n\nIts existence (along with that of the down and strange quarks) was postulated in 1964 by Murray Gell-Mann and George Zweig to explain the Eightfold Way classification scheme of hadrons. The up quark was first observed by experiments at the Stanford Linear Accelerator Center in 1968.\n\nIn the beginnings of particle physics (first half of the 20th century), hadrons such as protons, neutrons and pions were thought to be elementary particles. However, as new hadrons were discovered, the 'particle zoo' grew from a few particles in the early 1930s and 1940s to several dozens of them in the 1950s. The relationships between each of them were unclear until 1961, when Murray Gell-Mann and Yuval Ne'eman (independently of each other) proposed a hadron classification scheme called the Eightfold Way, or in more technical terms, SU(3) flavor symmetry.\n\nThis classification scheme organized the hadrons into isospin multiplets, but the physical basis behind it was still unclear. In 1964, Gell-Mann and George Zweig (independently of each other) proposed the quark model, then consisting only of up, down, and strange quarks. However, while the quark model explained the Eightfold Way, no direct evidence of the existence of quarks was found until 1968 at the Stanford Linear Accelerator Center. Deep inelastic scattering experiments indicated that protons had substructure, and that protons made of three more-fundamental particles explained the data (thus confirming the quark model).\n\nAt first people were reluctant to describe the three bodies as quarks, instead preferring Richard Feynman's parton description, but over time the quark theory became accepted (see \"November Revolution\").\n\nDespite being extremely common, the bare mass of the up quark is not well determined, but probably lies between 1.8 and . Lattice QCD calculations give a more precise value: .\n\nWhen found in mesons (particles made of one quark and one antiquark) or baryons (particles made of three quarks), the 'effective mass' (or 'dressed' mass) of quarks becomes greater because of the binding energy caused by the gluon field between each quark (see mass–energy equivalence). The bare mass of up quarks is so light, it cannot be straightforwardly calculated because relativistic effects have to be taken into account. Due to strong force mediated by gluons in the gluon field, the quarks move at roughly 99.995% of the speed of light, leading to Lorentz factor of roughly 100. As a result, the combined rest mass of quarks is barely 1% of proton or neutron mass.\n\n\n"}
{"id": "11282061", "url": "https://en.wikipedia.org/wiki?curid=11282061", "title": "Uranium-236", "text": "Uranium-236\n\nUranium-236 is an isotope of uranium that is neither fissile with thermal neutrons, nor very good fertile material, but is generally considered a nuisance and long-lived radioactive waste. It is found in spent nuclear fuel and in the reprocessed uranium made from spent nuclear fuel.\n\nThe fissile isotope uranium-235 fuels most nuclear reactors. U-235 that absorbs a thermal neutron may go one of two ways. About 82% of the time, it will fission. About 18% of the time it will not fission, instead emitting gamma radiation and yielding U-236. Thus, the yield of U-236 per U-235+n reaction is about 18%, and the yield of fissile decay products is about 82%. In comparison, the yields of the most abundant individual fission products like Cs-137, Sr-90, Tc-99 are between 6% and 7%, and the combined yield of medium-lived (10 years and up) and long-lived fission products is about 32%, or a few percent less as some are destroyed by neutron capture.\n\nThe second most used fissile isotope plutonium-239 can also fission or not fission on absorbing a thermal neutron. The product plutonium-240 makes up a large proportion of reactor-grade plutonium (plutonium recycled from spent fuel that was originally made with enriched natural uranium and then used once in an LWR). Pu-240 decays with a half-life of 6561 years into U-236. In a closed nuclear fuel cycle, most Pu-240 will be fissioned (possibly after more than one neutron capture) before it decays, but Pu-240 discarded as nuclear waste will decay over thousands of years.\n\nWhile the largest part of uranium-236 has been produced by neutron capture in nuclear power reactors, it is for the most part stored in nuclear reactors and waste repositories. The most significant contribution to uranium-236 abundance in the environment is the U(n,3n)U reaction by fast neutrons in thermonuclear weapons. The A-bomb testing of the 1940s, 1950s, and 1960s has raised the environmental abundance levels significantly above the expected natural levels.\n\nU, on absorption of a thermal neutron, does not undergo fission, but becomes U, which quickly beta decays to Np. However, the neutron capture cross section of U is low, and this process does not happen quickly in a thermal reactor. Spent nuclear fuel typically contains about 0.4% U-236. With a much greater cross-section, Np may eventually absorb another neutron, becoming Np, which quickly beta decays to plutonium-238, or fissioning (Np is fissile).\n\nU and most other actinides are fissionable by fast neutrons in a nuclear bomb or a fast neutron reactor. A small number of fast reactors have been in research use for decades, but widespread use for power production is still in the future.\n\nUranium-236 alpha decays with a half-life of 23.420 million years to thorium-232. It is longer-lived than any other artificial actinides or fission products produced in the nuclear fuel cycle.\n\nUnlike plutonium, minor actinides, fission products, or activation products, chemical processes cannot separate U-236 from U-238, U-235, U-232 or other uranium isotopes. It is even difficult to remove with isotopic separation, as low enrichment will concentrate not only the desirable U-235 and U-233 but the undesirable U-236, U-234 and U-232. On the other hand, U-236 in the environment cannot separate from U-238 and concentrate separately, which limits its radiation hazard in any one place.\n\nU-238's halflife is about 190 times as long as U-236; therefore U-236 should have about 190 times as much specific activity. That is, in reprocessed uranium with 0.5% U-236, the U-236 and U-238 will produce about the same level of radioactivity. (U-235 contributes only a few percent.)\n\nThe ratio is less than 190 when the decay products of each are included. U-238's decay chain to uranium-234 and eventually lead-206 involves emission of eight alpha particles in a time (hundreds of thousands of years) short compared to the halflife of U-238, so that a sample of U-238 in equilibrium with its decay products (as in natural uranium ore) will have eight times the alpha activity of U-238 alone. Even purified natural uranium where the post-uranium decay products have been removed will contain an equilibrium quantity of U-234 and therefore about twice the alpha activity of pure U-238. Enrichment to increase U-235 content will increase U-234 to an even greater degree, and roughly half of this U-234 will survive in the spent fuel. On the other hand, U-236 decays to thorium-232 which has a halflife of 14 billion years, equivalent to a decay rate only 31.4% as great as that of U-238.\n\nDepleted uranium used in kinetic energy penetrators, etc. is supposed to be made from uranium enrichment tailings that have never been irradiated in a nuclear reactor, not reprocessed uranium. However, there have been claims that some depleted uranium has contained small amounts of U-236.\n\n\n"}
{"id": "8193519", "url": "https://en.wikipedia.org/wiki?curid=8193519", "title": "Village Homes", "text": "Village Homes\n\nVillage Homes is a planned community in Davis, Yolo County, California. It is designed to be ecologically sustainable by harnessing the energies and natural resources that exists in the landscape, especially stormwater and solar energy.\n\nThe principal designer of Village Homes was architect Mike Corbett who began planning in the 1960s, with construction continuing from south to north from the 1970s through the 1980s. Village Homes was completed in 1982, and has attracted international attention from its inception as an early model of an environmentally friendly housing development, including a visit from then-French President François Mitterrand.\n\nThe 225 homes and 20 apartment units that now are the Village Homes community use solar panels for heating, and they are oriented around common areas at the rear of the buildings, rather than around the street at the front. All streets are oriented east-west, with all lots positioned north-south. This feature has become standard practice in Davis and elsewhere since it enables homes with passive solar designs to make full use of the sun's energy throughout the year. The development also uses natural drainage, called bioswales, to collect water to irrigate the common areas and support the cultivation of edible foods, such as nut and fruit trees and vegetables for consumption by residents, without incurring the cost of using treated municipal water.\n\n\n"}
{"id": "15416391", "url": "https://en.wikipedia.org/wiki?curid=15416391", "title": "Éolienne Bollée", "text": "Éolienne Bollée\n\nThe Éolienne Bollée is an unusual wind turbine, unique for having a stator and a rotor, as a water turbine has. The eponymous invention was first patented in 1868 by Ernest Sylvain Bollée in France. A further patent dated 1885 differed mainly in two ways: First, in how the turbine was turned to face the wind and second, in an improvement which increased the flow of wind through the turbine was added. The turbines built according to the 1885 patent were commercially successful.\n\nErnest Sylvain Bollée (19 July 1814 – 1891) and Auguste Sylvain Bollée (1847–1906) took out the original patent No. 79985 in 1868 for a \"hydraulic wind engine\". Ernest Bollée described himself as a hydraulic engineer in Le Mans, Sarthe. During the 1860s, due to poor health, Ernest delegated control of the three parts of his business to each of his sons. Auguste was given control of the wind engine manufacturing side of the business. The patent of 1885, with the improvements, is No.167726. In 1898 Auguste sold the business to Édouard-Émile Lebert. Auguste is estimated to have made about 260 Éoliennes. Lebert passed the business to Gaston Duplay in 1918 and on 1 January 1926 the business passed to the Société Anonyme des Éoliennes Bollée (SAEB). SAEB erected at least three éoliennes. Operations seem to have ceased around 1931.\n\nThe Éolienne Bollée was designed to be constructed in a modular form, thus allowing éoliennes of various sizes to be built. The tower could be a standard pylon type, either of triangular or square plan, or a cast-iron column with an external spiral staircase. The éoliennes built with this type of tower have a very distinctive appearance. The actual turbine itself consists of two rings, the first being the stator and the second being the rotor. The stator has more blades than the rotor. A new device added to the 1885 patent was a funnel affixed to the stator, enabling the éolienne to capture wind from a larger area than the rotor, and increasing its speed through the turbine. A small fantail operated upwind of the rotor, and through a system of gears turned the turbine to face the wind. Also, through a counterweight system, it turned the turbine out of wind as the wind speed increased, thus preventing damage in very strong winds, when the éolienne would be edge on into the prevailing wind.\n\nThe cast-iron columns were made in sections of diameter, having twelve cast-iron treads or wrought iron steps forming a complete spiral around the column. A half column was available, allowing éoliennes to be built to any desired height.\n\nThe Éolienne Bollée is unique amongst other forms of windmill because of the stator. All windmills have a rotor, whether it is the sails on a traditional windmill or the blades of a modern wind turbine. The Éolienne Bollée is the only wind-powered turbine where the wind passes through a set of fixed blades (stator) before driving the windmill itself (rotor).\n\nThe rotor is turned by the wind, and through a bevel wheel drives a shaft inside the column (if used) or in the centre of the tower. At the lower end this drives a horizontal shaft through a gearbox, which typically drives three throw pump.\n\nThe éoliennes came in four sizes: , , and diameter.\n\nIt was claimed that a 3.53 m éolienne with a pump would be able to pump:\n\nPumps were available in seven sizes: , , , , , , and diameter.\n\nLebert built some very similar wind engines with a single rotor, and lacking the stator (thus they were not true turbines). They were either or diameter. At least three of these are known to have been built, including at Rugles, Eure and Parigné-l'Évêque, Sarthe.\n\nThe Clarkson wind engine consisted of a rotor or a number of rotors, one behind the other, revolving in a casing with fixed guide vanes between and of opposite pitch to those of the rotors, and having a further casing to admit a fresh supply of wind to the rotors behind. The cylindrical casings are open at each end with a larger opening facing the wind. The wind catches a number of wheels and feathered vanes fixed to a shaft revolving in bearings inside the casings. When the wind has passed between the vanes of the front wheel it is directed by the guide vanes to the second wheel and is again taken up by guides and passed to a third wheel and so on, the action each time increasing the effect of the wind on the shaft and improving efficiency. The Clarkson of which an illustration survives (1919) was erected by the Air Power Co. of Prestwich, Cheshire on the estate of Lord Derby. This small engine was designed to work in a wind, but could start under load in a wind of only . The wind wheel was only diameter and is designed to lift of water per hour to a height of in a wind, or double that quantity in a wind. All the Air Power wind engines were fitted with roller bearings, a starting and stopping arrangement and an automatic gear to cut off all wind above any desired velocity. They were mounted on a strong steel tower, with a ladder and circular platform.\n\nIn the early years, under the Bollée family, the vast majority of purchasers were aristocrats and gentry, only six éoliennes being sold to municipalities by 1888. After Lebert took over, the pattern of sales changed, with more éoliennes being sold for communal water supply, particularly in Indre-et-Loire and Sarthe .\n\nA few éoliennes were sold abroad, including two to a monastery at Cowfold, Sussex; one to a monastery at Tarragona, Spain; one to a hospital in Tunisia; one to a mine in Brazil; and one to Cotonou, Dahomey.\n\nSome éoliennes have survived. In France, the oldest surviving éolienne is at the Bollée bell foundry in Saint-Jean-de-Braye, near Orléans. One is preserved in working order at Épuisay, Loir-et-Cher, and another at the Bollée museum in Orléans. A few have been restored to working order.\n\nThe éolienne at Épuisay is on a square plan lattice tower of eight sections, high. The rotor drives a pump which pumps water from a depth of , the pump itself being at a depth of . A petrol engine was provided to work the pumps in times of calm. By wind, an hour could be pumped.\n\n\n\n"}
