{"id": "36053388", "url": "https://en.wikipedia.org/wiki?curid=36053388", "title": "2000 Guanabara Bay oil spill", "text": "2000 Guanabara Bay oil spill\n\nThe Guanabara Bay oil spill, one of three major spills in the bay, occurred in January 2000 in Brazil when a leaking pipeline released of oil into Guanabara Bay. It leaked from the oil refinery at Duque de Caxias (REDUC) operated by Petrobras. Petrobas the company at the center of the oil spill immediately moved into gear and hired a cleanup crew to assess the damages and start the process to cleaning up the affected areas. This catastrophic accident had a damaging effect on marine life in the ocean, fishes, as well as, other existing areas surrounding the bay area. Many fishes were washing up on the shore dead or covered in oil. The fishing industry took a nose dive and the fishermen's livelihood was gravely affected. As a matter of fact, the fishing industry was brought to a halt giving rise to economic downfall. There was astronomical cost to be incurred with the clean-up process and the stakeholders were in a state of growing panic.Large areas of mangrove forests were killed and had not grown back ten years later. The tucuxi (\"Sotalia fluviatilis\") dolphin species inhabit the bay but were able to avoid the primary effects of the oil spill.\n\nThe oil spill was a catalyst for the passage of new environmental law in Brazil.\n\nThere were affects to society and to the marine life and environment at large. The fishing industry suffered tremendously. The fishes and crabs could not get oxygen and as a result, they succumbed to death. The aquatic plant life e.g. (The mangrove in the bay) began to wither and die because of the oil being trapped on the surface of the water. One major concern that the fishermen had was concerning the fish that survived the oil spill. They had no aquatic plants to feed because the plant life in the ocean became none existent as the oil settle itself on the ocean surface. The fishermen, who relied on fishing for their livelihood, could not have a catch to depend on, and soon the industry resorted to lost of employment for fishermen, trading lost among crab vendors, and loss of revenue for families who depended on fishing as their primary means of survival. Even a decade of efforts by the Petrobras refinery was not enough to restore the Bay to its original condition.\n\nThe cost of the clean-up and the drastic effect the spill had on the fishing industry remained for many years to come. The Petrobras refinery who were responsible for the spill in the first place, accepted liabilities and promised to execute the cleanup process. Among the affected areas were the beaches, mangrove mashes, and the surrounding areas leading to the beach front (pathways and tunnels). There were also, dead birds and fishers along the shoreline of the front. The actual cleanup process cost Petrobras over $100,000,000. The process of recovery, environmental cleaning and compensation for damages took a significant toll on the company’s expenditures, which resulted in great profit loss for Petrobras. Their years of efforts was overwhelming, but the result did little to justify the labor extended to the task.\n\nThe spill impacted the Brazilian society and the overall economy on a hold. The massive death and extinction of many species of fish prompted a significant decrease in the number of fish that supported the bay. A total of about 4000 of the total 6000 fishermen in the Guanabara Bay had resorted to alternative sources of income and jobs, because of the decline in the fishing industry. According to some fishermen in the region one can hardly obtain ten kilos of catch, a scenario which was contrary to the previous period before the spillage when the fishermen could obtain a catch of 100 kilos or more. The quality of the fish from the bay area had also been a source of concern. Research findings have shown that the fish surrounding the bay area was contaminated; therefore the prices in the region were valued at half the price because of quality. In addition, the Petrobas refinery company also lost a substantial amount of money through fines from the Brazilian Government, as well as, compensation and other strict environmental conservation concerns. To combat this growing onslaught Petrobas developed some environmental conservation strategies by putting strict methods and policies in place to offset its critics. However, these measures proved to be extremely costly, putting a dent in the company’s fiscal stability.\n\nGreenpeace (an environmental bureau set up by the government ) challenged Petrobras on its demand to observe dramatic changes on corporate policies. They questioned their ability to execute the environmental conservation mandate and were very vocal in their disapproval of their handling of the situation. To gain some ground Petrobras argued that the spill was caused, not by dumping of oil, but by a broken pipeline, a mere accident that did not result in customer’s being revengeful or unaccommodating. The other affected stakeholders were the government of Brazil, the local authorities, the fishermen in the region, the local business people and the Petrobras refinery company. The fishermen were affected by the decreased in the number of fishes that influenced their daily catch, as well as, the fish quality. There was a high level of contamination in the fishes that occupied the bay area. The local authorities were also affected by the spillage, since it led to the halt of the activities at the coast that would attribute to government revenue collection. The government was first affected by the challenge the event caused on its policies and they had to make immediate re-adjustment. New policies had to be adopted to cater for future tragedies of this nature. Petrobras Refinery Company was the hardest hit, since they had to pay huge fines of over 28 million dollars for the damages caused as a result, of the oil spill. Compensation was also paid out to affected citizens within the bay area and adjacent to it.\n\nChanges and regulatory policies became the order of the day for the government following the oil spill. Transforming the way government performed its regulatory measures were address. One of the main changes that occurred was the enhanced precision by which the governing and the oil industries handle the environmental requirements for incidents such as this oil spill. The government changed their regulations and demanded Risk Management and an Emergency Plan to be put in place. Every oil institution must execute “oil Pollution Risk Assessment”. This law demanded an inclusive policy manual on the avoidance of oil pollution accident. The “Emergency plan” had to take effect during an oil spill and must have the official endorsement of the environmental bureau (Greenpeace). In order to drive home the seriousness of the situation the Government established a task force (in addition to Greenpeace) to regulate and enforce the plans that were laid out by the environmental bureau. This was done in the form of an auditing team. This team must be independent and not be affiliated with any of the immediate parties (government or oil company). Oil industries were given strict guidelines to observe these regulations, by creating higher standards within their institution. This was a very challenging experience for the oil companies, who had suffered enormous financial loss during this period. Complying with the regulation was a must, but the effect it had on the oil companies was tremendous.\n\nImmediate work was done to ensure that Government policies were adhered to:\n- Immediate replacement of storage tanks as well as, underground piping facilities was done to avoid any more disaster. A compulsory replacement plan was put in place to ensure that compliance was followed and adhered too.\n- Transportation method was another important aspect of change that had to be address. The policy demanded a merit based approval of the transportation vessels to minimize or eliminate spillage.\n- There had to be strict approval policies that required stable channels and vessels for the transportation of oil within the environment.\n\n\n"}
{"id": "55499686", "url": "https://en.wikipedia.org/wiki?curid=55499686", "title": "Airborne radioactivity increase in Europe in autumn 2017", "text": "Airborne radioactivity increase in Europe in autumn 2017\n\nAirborne radioactivity was detected in Europe in autumn 2017, starting from the last days of September. The source is widely suspected to be in Russia; however, the Russian government denies that any nuclear mishaps occurred that could have caused the radiation spike. The radioactive isotope detected was ruthenium-106; across Europe, it was found to be in small quantities (from microbecquerels to a few millibecquerels per cubic metre of air), not significant for the health of the population far away from the source. However, it is estimated that the radiation released posed a danger to any employees or residents within several kilometers of the currently-unknown source.\n\nEuropean monitoring networks declared increased radioactivity levels in Europe, coming from Eastern Europe, in the first days of October:\n\nAn assessment of the French nuclear safety institute IRSN indicated that while there was no health risk for the vast majority of people in Europe, the radioactive quantity released was significant, estimated from 100 to 300 terabecquerels, which would require an evacuation of people from a radius of several kilometers from the source, as yet then unidentified. The source of the aforementioned 100–300 TBq activity corresponds approximately to 1–3 grams of the ruthenium-106 isotope.\n\nWhile the release of the gaseous noble gas element Krypton-85 is rountine during nuclear reprocessing, the noble metal Ruthenium that is generated in a fission product yield rate of from \"0.39\" to \"3.103\"% of every fission of a nucleus of uranium or plutonium respectively, is by contrast \"generally\" in metallic form, with a high boiling point of 4150 degrees celsius in spent fuel. However, owing to the ionizing radiation environment of spent fuel and the entry of oxygen, radiolysis-reactions can generate the more volatile compound Ruthenium(VIII) oxide which has a boiling point of approximately 40 degrees celsius and is a strong oxidizer, reacting with virtually any fuel/hydrocarbon. The use of a solution of tributyl phosphate in the hydrocarbons kerosene or dodecane, is frequently used as part of the nuclear reprocessing method known as PUREX. Accidental airborne releases of Ru-106 have occurred, through this pathway in the past, such as the British B205 reprocessing incident in 1973, were a number of employees were heavily exposed. With corrosion of the fuel rod-cladding, an issue in the British case, due to the selection of magnesium for the coating on the fuel rods, in the MAGNOX-reactor spent fuel pools, reprocessing needed to occur within a \"few months\" at the B205 facility. While most other facilities like the French La Hague, place the spent-fuel in spent fuel pools for \"about a decade\", until the Ru-106, which has a half-life of ~ 1 year, has safely decayed into Rh-106 and finally stable Pd-106. All attempts at PUREX applied to relatively freshly discharged spent-fuel, need to take the presence, oxidative and volatile nature of Ruthenium(VIII) oxide into account.\n\nThe French IRSN has put forth the hypothesis in Jan-2018 that a possible reason for the release of Ru-106 radioactivity at Mayak-PA might be an unsuccessful attempt to extract the equally short-lived reactor-generated-isotope cerium-144 for the European/Italian nuetrino-detecton-project \"Borexino\". Mayak-PA had agreed to deliver cerium-144 in early 2018, but canceled the contract in December 2017. Mayak PA was the only facility contractually-willing to attempt the extraction of cerium-144 from \"fresh\" spent nuclear fuel, 2–3 years old. Usually spent fuel is not processed earlier than five years after extraction. The Ru106/Ru103 ratio in analyzed environmental samples during the pollution spread was characteristic of \"fresh\" spent fuel.\n\nInitially, there was no indication given for the source of these radioactive particles, apart from a October 2017 statement coming from German authorities estimating the source in Eastern Europe, at a distance of more than 1,000 km from Germany. A later report from the German Federal Radiation Protection Service indicated that a location to the South of the Urals was possible (as well as other potential locations), however Roshydromet (the meteorology service of Russia) initially announced that the only location in Russia where ruthenium-106 was detected was St. Petersburg, from September 25 until October 7, in very small quantities. As reported by Roshydromet, Ru-106 activity in St. Petersburg was at 115.4 μBq/m³ from October 2 to 6.\n\nThe French Institute of Radioprotection and Nuclear Security (IRSN) ruled out the emissions as coming from a nuclear reactor, estimating that it should have come either from a nuclear treatment site or from a center producing radioactive medicine. The location where emissions have taken place is estimated south of the Ural mountains, between the Urals and the Volga river, in Russia or Kazakhstan.\n\nRoshydromet then issued a report describing a rise in beta activity of aerosols and surfaces at all monitoring posts in South Ural from September 25 to October 1, 2017. In two aerosol samples, an increase in Ru-106 activity was detected. On September 26 and 27, Ru-106 decay products were detected in the Republic of Tatarstan. On September 27 and 28, high pollution levels of aerosols and surfaces was detected in Volgograd and Rostov-on-Don. In two aerosol samples from Chelyabinsk Oblast, 986- and 440-fold activity increases were measured, as compared to the preceding month.\n\nThe Mayak nuclear plant is widely suspected as the source of the radiation. However, authorities at Mayak and Rosatom, the Russian state company running the nuclear industry, have both denied a link.\n\nOn 21 November 2017 Russia reversed itself by confirming that a radiation spike was recorded at two monitoring facilities within a 62-mile radius of the Mayak nuclear reprocessing and isotope production plant. However, Russia continues to deny that the source of the radiation is within Russia, and claims that \"the published data is not sufficient to establish the location of the pollution source.\"\n\nRosatom initially stated that it had not carried out any operations that could have led to the isotope's release into the atmosphere \"for many years\". However, in December 2017, senior Mayak executive Yuri Morkov admitted that ruthenium-106 is routinely released as part of the plant's processing of spent nuclear fuel. Morkov characterized the amount released as \"insignificant\" and denied Mayak was the source of the radiation spike.\n\nDomestic investigation within Russia is hampered by the problem that Mayak is in the walled-off closed city of Ozyorsk, which non-resident Russians are barred from visiting without special permission, and by government harassment of nuclear critics. In Russia prominent nuclear critics experience government raids, are accused on state TV of \"exploiting the nuclear issue to foment revolution\", and risk criminal prosecution on charges of incitement of hatred against nuclear energy employees.\n\nThe Russian Academy of Sciences's Nuclear Safety Institute set up an international committee to investigate the incident. The French Institute of Radioprotection and Nuclear Security (IRSN) provided a report to the committee in January 2018. The report concluded that the most likely source of the pollution is a spent fuel treatment facility located in a region between Volga and Ural. A possible reason for the release of radioactivity at Mayak PA might be an unsuccessful attempt to produce cerium-144 for the European scientific project Borexino. Mayak PA had agreed to deliver cerium-144 in early 2018, but canceled the contract in December 2017. Mayak PA was the only facility capable of producing cerium-144 from \"fresh\" spent nuclear fuel 2–3 years old. Usually spent fuel is not processed earlier than five years after extraction. The Ru106/Ru103 ratio in analyzed environmental samples during the pollution spread was characteristic of \"fresh\" spent fuel. While the non-Russian members of the committee accepted the conclusions of IRSN's report, the Russian members maintain that an inspection of the Mayak PA facility in November 2017 showed no anomalies and a rare meteorological event may have transported the ruthenium-106 from somewhere else to the apparent region of origin.\n\nSimilar announcements came from other authorities:\n\n"}
{"id": "26273498", "url": "https://en.wikipedia.org/wiki?curid=26273498", "title": "Black Gold (1962 film)", "text": "Black Gold (1962 film)\n\nBlack Gold is a 1962 adventure film directed by Leslie H. Martinson and written by Bob Duncan, Wanda Duncan and Harry Whittington . The film stars Philip Carey, Diane McBain, James Best, Fay Spain, Claude Akins and William Edward Phipps. \n\nThe film was released by Warner Bros. on July 21, 1962.\n\nAn Oklahoma farmer discovers oil on his land.\n\n"}
{"id": "13627757", "url": "https://en.wikipedia.org/wiki?curid=13627757", "title": "Central America bioregion", "text": "Central America bioregion\n\nThe Central America bioregion is a biogeographic region comprising southern Mexico and Central America.\n\nThe bioregion covers the southern portion of Mexico, all of Belize, Costa Rica, El Salvador, Guatemala, Honduras, and Nicaragua, and all but easternmost Panama.\n\nWWF defines bioregions as \"geographic clusters of ecoregions that may span several habitat types, but have strong biogeographic affinities, particularly at taxonomic levels higher than the species level (genus, family).\"\n\nThe bioregion lies in the tropics, and is home to tropical moist broadleaf forests, tropical dry broadleaf forests, and tropical coniferous forests. The higher mountains are home to cool-climate montane forests, grasslands and shrublands.\n\nCentral America connects North America to South America. The land bridge was completed 2.8 million years ago, when the Isthmus of Panama was formed, linking the two continents for the first time in tens of millions of years. The resulting Great American Interchange of animals and plants shaped the flora and fauna of the Central America bioregion.\n\nLarge mammals include the white-lipped peccary \"(Tayassu pecari)\", Baird's tapir \"(Tapirus bairdii)\", white-tailed deer \"(Odocoileus virginianus)\", Central American red brocket \"(Mazama temama)\", Yucatan brown brocket \"(Mazama pandora)\", giant anteater \"(Myrmecophaga tridactyla)\", brown-throated sloth \"(Bradypus variegatus)\", jaguar \"(Panthera onca)\", cougar \"(Puma concolor)\", and ocelot \"(Leopardus pardalis)\".\n\nPlants of South American origin came to dominate the tropical lowlands of Central America, as did South American freshwater fish and invertebrates. 95% of Central American freshwater fish are South American in origin, with only the Tropical gar \"(Atractosteus tropicus)\", three clupeids \"(Dorosoma)\", a catostomid \"(Ictiobus)\", and an ictalurid \"(Ictalurus)\" of North American origin.\n\nThe montane vegetation of the region is distinct from the lowland vegetation, and includes species with origins in temperate North America, including oaks \"(Quercus)\", Pines \"(Pinus)\" and alders \"(Alnus)\", as well as a some species with origins in temperate South America, including \"Weinmannia\" and \"Drimys\".\n\n\n\n\n\n\n\n\n\n"}
{"id": "41800327", "url": "https://en.wikipedia.org/wiki?curid=41800327", "title": "Cerro Dominador Solar Thermal Plant", "text": "Cerro Dominador Solar Thermal Plant\n\nCerro Dominador Solar Power Plant (Spanish: Planta Solar Cerro Dominador) is a 210-megawatt (MW) combined concentrated solar power and photovoltaic plant located in the commune of María Elena in the Antofagasta Region of Chile, about west-northwest of Sierra Gorda. The project was approved by the Chilean government in 2013 and construction was started by Abengoa Solar Chile, a branch of the multinational Abengoa Spain.\n\nConstruction started on May 2014. On 29 August 2015, workers mobilization started a strike over poor working conditions. Following Abengoa financial woes, construction halted on January 2016, after about 1,500 workers were fired from the project, leaving only maintenance personnel on site. Construction progress was more than 50% complete. In October 2016, \nEIG Global Energy Partners became the sole owner of the project, after acquiring the participation of Abengoa, which remained as a technological partner and builder.\n\nIn February 2018, after Abengoa completed the construction of the 100 MW photovoltaic section, the plant started operation.\n\nIn May 2018, EIG Global Energy Partners closed a $758 million financing and Abengoa partnered with Acciona to restart construction and execute the second phase of the CSP section, which happened in June 2018. Among the financing funders are Natixis, Deutsche Bank, Société Générale, ABN AMRO, Banco Santander, Commerzbank and BTG Pactual.\n\nThe Cerro Dominador project will see the construction and operation of a 110 MW concentrated solar power plant with storage in the northern Chilean region of Antofagasta, located in the Atacama desert, one of the driest places with the highest solar radiation on earth. When finished, Cerro Dominador will be the largest CSP power plant with storage in Latin America.\n\nThe Cerro Dominador project will have a 110 MW solar-thermal tower. This technology uses a series of mirrors (heliostats) that track the sun on two axes, concentrating the solar radiation on a receiver on the upper part of the tower, where the heat is transferred to molten salts.\nThe molten salts then transfer their heat in a heat exchanger to water, generating superheated steam, which feeds a turbine that transforms the kinetic energy of the steam into electric energy using the Rankine cycle. In this way, the Cerro Dominador plant will be capable of generating around 110 MW of power.\nThe plant will have an advanced storage system enabling it to generate electricity for up to 17.5 hours without direct solar radiation, which allows it to provide a stable electricity supply without interruptions if required.\nAdditionally, the plant will have a sub-station and transmission line connected to the SING (Sistema Interconectado del Norte Grande) or Norte Grande Electric Grid.\nThe plant has an estimated lifespan of 30 to 50 years.\n\nThe cost of the project is estimated at US$1 billion, and construction started in May 2014. The Chilean government, through CORFO, is providing US$20 million of funding and is also loaning the land where the plant is located. The government also negotiated loans from the Inter-American Development Bank, the Clean Technology Fund, the German state-owned development bank (KfW), and the European Union.\n\nThe project is part of Chile’s national renewable energy program, intended to provide Chile with cleaner energy, while also reducing its dependency on fossil fuels like coal and natural gas.\nChile has set a target to produce 20% of its electricity from clean energy sources by 2025.\n\nCerro Dominador will prevent the emission of approximately 643,000 tons of CO into the atmosphere every year. The construction, operation and maintenance of the plant will also act as a catalyst for regional socio-economic development, creating a large number of direct and indirect jobs in construction, development, commissioning and plant operation as well as a network of services that will promote economic growth in the region.\n\n"}
{"id": "6328075", "url": "https://en.wikipedia.org/wiki?curid=6328075", "title": "Commercial waste", "text": "Commercial waste\n\nCommercial waste consists of waste from premises used mainly for the purposes of a trade or business or for the purpose of sport, recreation, education or entertainment, but excluding household, agricultural or industrial waste.\n\n"}
{"id": "6033433", "url": "https://en.wikipedia.org/wiki?curid=6033433", "title": "Cutan (polymer)", "text": "Cutan (polymer)\n\nCutan is one of two biopolymers which occur in the cuticle of some plants. The other and better-known polymer is cutin. Cutan is believed to be a hydrocarbon polymer, whereas cutin is a polyester, but the structure and synthesis of cutan are not yet fully understood. Cutan is not present in as many plants as once thought; for instance it is absent in \"Ginkgo\".\n\nCutan was first detected as a non-saponifiable component, resistant to de-esterification by alkaline hydrolysis, that increases in amount in cuticles of some species such as \"Clivia miniata\" as they reach maturity, apparently replacing the cutin secreted in the early stages of cuticle development (Schmidt and Schönherr, 1982). Evidence that cutan is a hydrocarbon polymer comes from the fact that its flash pyrolysis products are a characteristic homologous series of paired alkanes and alkenes (Nip \"et al.\" 1986)\n\nIts preservation potential is much greater than that of cutin.\n\n"}
{"id": "11543712", "url": "https://en.wikipedia.org/wiki?curid=11543712", "title": "Depolarizer", "text": "Depolarizer\n\nA depolarizer or depolariser, in electrochemistry, according to an IUPAC definition, is a synonym of electroactive substance, i.e., a substance which changes its oxidation state, or partakes in a formation or breaking of chemical bonds, in a charge-transfer step of an electrochemical reaction.\n\nIn the battery industry, the term \"depolarizer\" has been used to denote a substance used in a primary cell to prevent buildup of hydrogen gas bubbles. A battery depolarizer takes up electrons during discharge of the cell; therefore, it is always an oxidizing agent. The term \"depolarizer\" can be considered as outdated or misleading, since it is based on the concept of \"polarization\" which is hardly realistic in many cases.\n\nUnder certain conditions for some electrochemical cells, especially if they use an aqueous electrolyte, hydrogen ions can be converted into hydrogen atoms and H molecules. In the extreme case, bubbles of hydrogen gas might appear at one of the electrodes. If such a layer of hydrogen or even H gas bubbles appear on the positive plate of a battery, they interfere with the chemical action of the cell. An electrode covered with gases is said to be \"polarized\". Polarization in galvanic cells causes the voltage and thus current to be reduced, especially if the bubbles cover a large fraction of a plate. Depolarizers are substances which are intended to remove the hydrogen, and therefore, they help to keep the voltage at a high level. However, this concept is outdated, since if enough depolarizer is present, it will react directly in most cases by getting electrons from the positive plate of the galvanic cell, i.e. there will be no relevant amount of hydrogen gas present. Therefore, the original concept of polarization does not apply to most batteries, and the depolarizer does not react with hydrogen as H. Still, the term is used today, however, in most cases, it might be replaced with oxidizing agent.\n\nMany different substances have been used as depolarizers; the most notable are listed below.\n\nThese oxidize the hydrogen to water. Examples include:\n\n\nNitric and chromic acids are powerful oxidizing agents, and effective depolarizers, but their hazardous nature makes them unsuitable for general use. Manganese dioxide is, therefore, the most widely used depolarizer.\n\nThe hydrogen ions displace metal from the salt so that metal, instead of hydrogen, is deposited on the positive plate. Examples:\n\n\n\n"}
{"id": "54912", "url": "https://en.wikipedia.org/wiki?curid=54912", "title": "Dew point", "text": "Dew point\n\nThe dew point is the temperature to which air must be cooled to become saturated with water vapor. When further cooled, the airborne water vapor will condense to form liquid water (dew). When air cools to its dew point through contact with a surface that is colder than the air, water will condense on the surface. When the temperature is below the freezing point of water, the dew point is called the frost point, as frost is formed rather than dew. The measurement of the dew point is related to humidity. A higher dew point means there will be more moisture in the air.\n\nIf all the other factors influencing humidity remain constant, at ground level the relative humidity rises as the temperature falls. This is because less vapor is needed to saturate the air, so vapor condenses as the temperature falls. In normal conditions, the dew point temperature will not be greater than the air temperature because relative humidity cannot exceed 100%.\n\nIn technical terms, the dew point is the temperature at which the water vapor in a sample of air at constant barometric pressure condenses into liquid water at the same rate at which it evaporates. At temperatures below the dew point, the rate of condensation will be greater than that of evaporation, forming more liquid water. The condensed water is called dew when it forms on a solid surface, or frost if it freezes. The condensed water is called either fog or a cloud, depending on its altitude, when it forms in the air. If the temperature is below the dew point, the vapor is called supersaturated. This can happen if there are not enough particles in the air to act as condensation nuclei.\n\nA high relative humidity implies that the dew point is closer to the current air temperature. A relative humidity of 100% indicates the dew point is equal to the current temperature and that the air is maximally saturated with water. When the moisture content remains constant and temperature increases, relative humidity decreases, but the dew point remains constant.\n\nGeneral aviation pilots use dew point data to calculate the likelihood of carburetor icing and fog, and to estimate the height of a cumuliform cloud base.\n\nIncreasing the barometric pressure increases the dew point. This means that, if the pressure increases, the mass of water vapour in the air must be reduced in order to maintain the same dew point. For example, consider New York ( elevation) and Denver ( elevation). Because Denver is at a higher elevation than New York, it will tend to have a lower barometric pressure. This means that if the dew point and temperature in both cities are the same, the amount of water vapor in the air will be greater in Denver.\n\nWhen the air temperature is high, the human body uses the evaporation of sweat to cool down, with the cooling effect directly related to how fast the perspiration evaporates. The rate at which perspiration can evaporate depends on how much moisture is in the air and how much moisture the air can hold. If the air is already saturated with moisture, perspiration will not evaporate. The body's thermoregulation will produce perspiration in an effort to keep the body at its normal temperature even when the rate it is producing sweat exceeds the evaporation rate, so one can become coated with sweat on humid days even without generating additional body heat (such as by exercising).\n\nAs the air surrounding one's body is warmed by body heat, it will rise and be replaced with other air. If air is moved away from one's body with a natural breeze or a fan, sweat will evaporate faster, making perspiration more effective at cooling the body. The more unevaporated perspiration, the greater the discomfort.\n\nA wet bulb thermometer also uses evaporative cooling, so it provides a good measure for use in evaluating comfort level.\n\nDiscomfort also exists when the dew point is low (below around ). The drier air can cause skin to crack and become irritated more easily. It will also dry out the airways. The US Occupational Safety and Health Administration recommends indoor air be maintained at with a 20–60% relative humidity, equivalent to a dew point of .\n\nLower dew points, less than , correlate with lower ambient temperatures and the body requires less cooling. A lower dew point can go along with a high temperature only at extremely low relative humidity, allowing for relatively effective cooling.\n\nPeople inhabiting tropical and subtropical climates acclimatize somewhat to higher dew points. Thus, a resident of Singapore or Miami, for example, might have a higher threshold for discomfort than a resident of a temperate climate like London or Chicago. Those accustomed to temperate climates often begin to feel uncomfortable when the dew point reaches between , while others might find dew points below comfortable. Most inhabitants of temperate areas will consider dew points above oppressive and tropical-like, while inhabitants of hot and humid areas may not find this uncomfortable. Thermal comfort depends not just on physical environmental factors, but also on psychological factors.\n\nDevices called hygrometers are used to measure dew point over a wide range of temperatures. These devices consist of a polished metal mirror which is cooled as air is passed over it. The temperature at which dew forms is, by definition, the dew point. Manual devices of this sort can be used to calibrate other types of humidity sensors, and automatic sensors may be used in a control loop with a humidifier or dehumidifier to control the dew point of the air in a building or in a smaller space for a manufacturing process.\n\nA dew point of was observed at 14:00 EDT on July 12, 1987, in Melbourne, Florida. A dew point of has been observed in the United States on at least two other occasions: Appleton, Wisconsin, at 17:00 CDT on July 13, 1995, and New Orleans Naval Air Station at 17:00 CDT on July 30, 1987. A dew point of was observed at Dhahran, Saudi Arabia, at 15:00 AST on July 8, 2003, which caused the heat index to reach , the highest value recorded.\n\nA well-known approximation used to calculate the dew point, \"T\", given just the actual (\"dry bulb\") air temperature, \"T\" (in degrees Celsius) and relative humidity (in percent), RH, is the Magnus formula:\nThe more complete formulation and origin of this approximation involves the interrelated saturated water vapor pressure (in units of millibars, also called hectopascals) at \"T\", \"P\"(\"T\"), and the actual vapor pressure (also in units of millibars), \"P\"(\"T\"), which can be either found with \"RH\" or approximated with the barometric pressure (in millibars), \"BP\", and \"wet-bulb\" temperature, \"T\" is (unless declared otherwise, all temperatures are expressed in degrees Celsius):\n\nFor greater accuracy, \"P\"(\"T\") (and therefore \"γ\"(\"T\", RH)) can be enhanced, using part of the \"Bögel modification\", also known as the Arden Buck equation, which adds a fourth constant \"d\":\nwhere\n\nThere are several different constant sets in use. The ones used in NOAA's presentation are taken from a 1980 paper by David Bolton in the \"Monthly Weather Review\":\nThese valuations provide a maximum error of 0.1%, for and .\nAlso noteworthy is the Sonntag1990,\nAnother common set of values originates from the 1974 \"Psychrometry and Psychrometric Charts\", as presented by Paroscientific,\nAlso, in the \"Journal of Applied Meteorology and Climatology\", Arden Buck presents several different valuation sets, with different minimum accuracies for different temperature ranges. Two particular sets provide a range of −40 °C to +50 °C between the two, with even greater minimum accuracy than all of the other, above sets (maximum error at extremes of temperature range):\n\nThere is also a very simple approximation that allows conversion between the dew point, temperature, and relative humidity. This approach is accurate to within about ±1 °C as long as the relative humidity is above 50%:\n\nThis can be expressed as a simple rule of thumb:\n\n\"For every 1 °C difference in the dew point and dry bulb temperatures, the relative humidity decreases by 5%, starting with RH = 100% when the dew point equals the dry bulb temperature.\"\nThe derivation of this approach, a discussion of its accuracy, comparisons to other approximations, and more information on the history and applications of the dew point are given in the Bulletin of the American Meteorological Society.\n\nFor temperatures in degrees Fahrenheit, these approximations work out to\nT_\\mathrm{dp,^\\circ F} &\\approx T_\\mathrm\n"}
{"id": "43584612", "url": "https://en.wikipedia.org/wiki?curid=43584612", "title": "Donald Bannerman Macleod", "text": "Donald Bannerman Macleod\n\nDonald Bannerman Macleod (21 July 1887 – 8 March 1972) was a New Zealand molecular physicist.\n\nBorn at Doyleston, near Christchurch, in 1887, Macleod studied at Canterbury University College, graduating with an MA with first-class honours in chemistry in 1910.\n\nFollowing his graduation, Macleod was appointed as a lecturer in physics at Canterbury and worked there until his retirement in 1953 as an associate professor. He had a research collaboration with Professor Coleridge Farr from 1911 to 1936. In 1922 Macleod was awarded a DSc from Canterbury University College.\n\nHe was elected a Fellow of the Royal Society of New Zealand in 1935 and in 1940 he was awarded the society's Hector Medal for his work in the field of molecular physics.\n\n"}
{"id": "10106", "url": "https://en.wikipedia.org/wiki?curid=10106", "title": "Earthquake", "text": "Earthquake\n\nAn earthquake (also known as a quake, tremor or temblor) is the shaking of the surface of the Earth, resulting from the sudden release of energy in the Earth's lithosphere that creates seismic waves. Earthquakes can range in size from those that are so weak that they cannot be felt to those violent enough to toss people around and destroy whole cities. The seismicity, or seismic activity, of an area is the frequency, type and size of earthquakes experienced over a period of time. The word \"tremor\" is also used for non-earthquake seismic rumbling.\n\nAt the Earth's surface, earthquakes manifest themselves by shaking and displacing or disrupting the ground. When the epicenter of a large earthquake is located offshore, the seabed may be displaced sufficiently to cause a tsunami. Earthquakes can also trigger landslides, and occasionally volcanic activity.\n\nIn its most general sense, the word \"earthquake\" is used to describe any seismic event — whether natural or caused by humans — that generates seismic waves. Earthquakes are caused mostly by rupture of geological faults, but also by other events such as volcanic activity, landslides, mine blasts, and nuclear tests. An earthquake's point of initial rupture is called its focus or hypocenter. The epicenter is the point at ground level directly above the hypocenter.\n\nTectonic earthquakes occur anywhere in the earth where there is sufficient stored elastic strain energy to drive fracture propagation along a fault plane. The sides of a fault move past each other smoothly and aseismically only if there are no irregularities or asperities along the fault surface that increase the frictional resistance. Most fault surfaces do have such asperities and this leads to a form of stick-slip behavior. Once the fault has locked, continued relative motion between the plates leads to increasing stress and therefore, stored strain energy in the volume around the fault surface. This continues until the stress has risen sufficiently to break through the asperity, suddenly allowing sliding over the locked portion of the fault, releasing the stored energy. This energy is released as a combination of radiated elastic strain seismic waves, frictional heating of the fault surface, and cracking of the rock, thus causing an earthquake. This process of gradual build-up of strain and stress punctuated by occasional sudden earthquake failure is referred to as the elastic-rebound theory. It is estimated that only 10 percent or less of an earthquake's total energy is radiated as seismic energy. Most of the earthquake's energy is used to power the earthquake fracture growth or is converted into heat generated by friction. Therefore, earthquakes lower the Earth's available elastic potential energy and raise its temperature, though these changes are negligible compared to the conductive and convective flow of heat out from the Earth's deep interior.\n\nThere are three main types of fault, all of which may cause an interplate earthquake: normal, reverse (thrust) and strike-slip. Normal and reverse faulting are examples of dip-slip, where the displacement along the fault is in the direction of dip and movement on them involves a vertical component. Normal faults occur mainly in areas where the crust is being extended such as a divergent boundary. Reverse faults occur in areas where the crust is being shortened such as at a convergent boundary. Strike-slip faults are steep structures where the two sides of the fault slip horizontally past each other; transform boundaries are a particular type of strike-slip fault. Many earthquakes are caused by movement on faults that have components of both dip-slip and strike-slip; this is known as oblique slip.\n\nReverse faults, particularly those along convergent plate boundaries are associated with the most powerful earthquakes, megathrust earthquakes, including almost all of those of magnitude 8 or more. Strike-slip faults, particularly continental transforms, can produce major earthquakes up to about magnitude 8. Earthquakes associated with normal faults are generally less than magnitude 7. For every unit increase in magnitude, there is a roughly thirtyfold increase in the energy released. For instance, an earthquake of magnitude 6.0 releases approximately 30 times more energy than a 5.0 magnitude earthquake and a 7.0 magnitude earthquake releases 900 times (30 × 30) more energy than a 5.0 magnitude of earthquake. An 8.6 magnitude earthquake releases the same amount of energy as 10,000 atomic bombs like those used in World War II.\n\nThis is so because the energy released in an earthquake, and thus its magnitude, is proportional to the area of the fault that ruptures and the stress drop. Therefore, the longer the length and the wider the width of the faulted area, the larger the resulting magnitude. The topmost, brittle part of the Earth's crust, and the cool slabs of the tectonic plates that are descending down into the hot mantle, are the only parts of our planet which can store elastic energy and release it in fault ruptures. Rocks hotter than about 300 degrees Celsius flow in response to stress; they do not rupture in earthquakes. The maximum observed lengths of ruptures and mapped faults (which may break in a single rupture) are approximately 1000 km. Examples are the earthquakes in Chile, 1960; Alaska, 1957; Sumatra, 2004, all in subduction zones. The longest earthquake ruptures on strike-slip faults, like the San Andreas Fault (1857, 1906), the North Anatolian Fault in Turkey (1939) and the Denali Fault in Alaska (2002), are about half to one third as long as the lengths along subducting plate margins, and those along normal faults are even shorter.\n\nThe most important parameter controlling the maximum earthquake magnitude on a fault is however not the maximum available length, but the available width because the latter varies by a factor of 20. Along converging plate margins, the dip angle of the rupture plane is very shallow, typically about 10 degrees. Thus the width of the plane within the top brittle crust of the Earth can become 50 to 100 km (Japan, 2011; Alaska, 1964), making the most powerful earthquakes possible.\n\nStrike-slip faults tend to be oriented near vertically, resulting in an approximate width of 10 km within the brittle crust, thus earthquakes with magnitudes much larger than 8 are not possible. Maximum magnitudes along many normal faults are even more limited because many of them are located along spreading centers, as in Iceland, where the thickness of the brittle layer is only about 6 km.\n\nIn addition, there exists a hierarchy of stress level in the three fault types. Thrust faults are generated by the highest, strike slip by intermediate, and normal faults by the lowest stress levels. This can easily be understood by considering the direction of the greatest principal stress, the direction of the force that 'pushes' the rock mass during the faulting. In the case of normal faults, the rock mass is pushed down in a vertical direction, thus the pushing force (\"greatest\" principal stress) equals the weight of the rock mass itself. In the case of thrusting, the rock mass 'escapes' in the direction of the least principal stress, namely upward, lifting the rock mass up, thus the overburden equals the \"least\" principal stress. Strike-slip faulting is intermediate between the other two types described above. This difference in stress regime in the three faulting environments can contribute to differences in stress drop during faulting, which contributes to differences in the radiated energy, regardless of fault dimensions.\n\nWhere plate boundaries occur within the continental lithosphere, deformation is spread out over a much larger area than the plate boundary itself. In the case of the San Andreas fault continental transform, many earthquakes occur away from the plate boundary and are related to strains developed within the broader zone of deformation caused by major irregularities in the fault trace (e.g., the \"Big bend\" region). The Northridge earthquake was associated with movement on a blind thrust within such a zone. Another example is the strongly oblique convergent plate boundary between the Arabian and Eurasian plates where it runs through the northwestern part of the Zagros Mountains. The deformation associated with this plate boundary is partitioned into nearly pure thrust sense movements perpendicular to the boundary over a wide zone to the southwest and nearly pure strike-slip motion along the Main Recent Fault close to the actual plate boundary itself. This is demonstrated by earthquake focal mechanisms.\n\nAll tectonic plates have internal stress fields caused by their interactions with neighboring plates and sedimentary loading or unloading (e.g. deglaciation). These stresses may be sufficient to cause failure along existing fault planes, giving rise to intraplate earthquakes.\n\nThe majority of tectonic earthquakes originate at the ring of fire in depths not exceeding tens of kilometers. Earthquakes occurring at a depth of less than 70 km are classified as 'shallow-focus' earthquakes, while those with a focal-depth between 70 and 300 km are commonly termed 'mid-focus' or 'intermediate-depth' earthquakes. In subduction zones, where older and colder oceanic crust descends beneath another tectonic plate, Deep-focus earthquakes may occur at much greater depths (ranging from 300 up to 700 kilometers). These seismically active areas of subduction are known as Wadati–Benioff zones. Deep-focus earthquakes occur at a depth where the subducted lithosphere should no longer be brittle, due to the high temperature and pressure. A possible mechanism for the generation of deep-focus earthquakes is faulting caused by olivine undergoing a phase transition into a spinel structure.\n\nEarthquakes often occur in volcanic regions and are caused there, both by tectonic faults and the movement of magma in volcanoes. Such earthquakes can serve as an early warning of volcanic eruptions, as during the 1980 eruption of Mount St. Helens. Earthquake swarms can serve as markers for the location of the flowing magma throughout the volcanoes. These swarms can be recorded by seismometers and tiltmeters (a device that measures ground slope) and used as sensors to predict imminent or upcoming eruptions.\n\nA tectonic earthquake begins by an initial rupture at a point on the fault surface, a process known as nucleation. The scale of the nucleation zone is uncertain, with some evidence, such as the rupture dimensions of the smallest earthquakes, suggesting that it is smaller than 100 m while other evidence, such as a slow component revealed by low-frequency spectra of some earthquakes, suggest that it is larger. The possibility that the nucleation involves some sort of preparation process is supported by the observation that about 40% of earthquakes are preceded by foreshocks. Once the rupture has initiated, it begins to propagate along the fault surface. The mechanics of this process are poorly understood, partly because it is difficult to recreate the high sliding velocities in a laboratory. Also the effects of strong ground motion make it very difficult to record information close to a nucleation zone.\n\nRupture propagation is generally modeled using a fracture mechanics approach, likening the rupture to a propagating mixed mode shear crack. The rupture velocity is a function of the fracture energy in the volume around the crack tip, increasing with decreasing fracture energy. The velocity of rupture propagation is orders of magnitude faster than the displacement velocity across the fault. Earthquake ruptures typically propagate at velocities that are in the range 70–90% of the S-wave velocity, and this is independent of earthquake size. A small subset of earthquake ruptures appear to have propagated at speeds greater than the S-wave velocity. These supershear earthquakes have all been observed during large strike-slip events. The unusually wide zone of coseismic damage caused by the 2001 Kunlun earthquake has been attributed to the effects of the sonic boom developed in such earthquakes. Some earthquake ruptures travel at unusually low velocities and are referred to as slow earthquakes. A particularly dangerous form of slow earthquake is the tsunami earthquake, observed where the relatively low felt intensities, caused by the slow propagation speed of some great earthquakes, fail to alert the population of the neighboring coast, as in the 1896 Sanriku earthquake.\n\nTides may induce some seismicity, see tidal triggering of earthquakes for details.\n\nMost earthquakes form part of a sequence, related to each other in terms of location and time. Most earthquake clusters consist of small tremors that cause little to no damage, but there is a theory that earthquakes can recur in a regular pattern.\n\nAn aftershock is an earthquake that occurs after a previous earthquake, the mainshock. An aftershock is in the same region of the main shock but always of a smaller magnitude. If an aftershock is larger than the main shock, the aftershock is redesignated as the main shock and the original main shock is redesignated as a foreshock. Aftershocks are formed as the crust around the displaced fault plane adjusts to the effects of the main shock.\n\nEarthquake swarms are sequences of earthquakes striking in a specific area within a short period of time. They are different from earthquakes followed by a series of aftershocks by the fact that no single earthquake in the sequence is obviously the main shock, therefore none have notable higher magnitudes than the other. An example of an earthquake swarm is the 2004 activity at Yellowstone National Park. In August 2012, a swarm of earthquakes shook Southern California's Imperial Valley, showing the most recorded activity in the area since the 1970s.\n\nSometimes a series of earthquakes occur in what has been called an \"earthquake storm\", where the earthquakes strike a fault in clusters, each triggered by the shaking or stress redistribution of the previous earthquakes. Similar to aftershocks but on adjacent segments of fault, these storms occur over the course of years, and with some of the later earthquakes as damaging as the early ones. Such a pattern was observed in the sequence of about a dozen earthquakes that struck the North Anatolian Fault in Turkey in the 20th century and has been inferred for older anomalous clusters of large earthquakes in the Middle East.\n\nQuaking or shaking of the earth is a common phenomenon undoubtedly known to humans from earliest times. Prior to the development of strong-motion accelerometers that can measure peak ground speed and acceleration directly, the intensity of the earth-shaking was estimated on the basis of the observed effects, as categorized on various seismic intensity scales. Only in the last century has the source of such shaking been identified as ruptures in the earth's crust, with the intensity of shaking at any locality dependent not only on the local ground conditions, but also on the strength or \"magnitude\" of the rupture, and on its distance.\n\nThe first scale for measuring earthquake magnitudes was developed by Charles F. Richter in 1935. Subsequent scales (see seismic magnitude scales) have retained a key feature, where each unit represents a ten-fold difference in the amplitude of the ground shaking, and a 32-fold difference in energy. Subsequent scales are also adjusted to have approximately the same numeric value within the limits of the scale.\n\nAlthough the mass media commonly reports earthquake magnitudes as \"Richter magnitude\" or \"Richter scale\", standard practice by most seismological authorities is to express an earthquake's strength on the moment magnitude scale, which is based on the actual energy released by an earthquake.\n\nIt is estimated that around 500,000 earthquakes occur each year, detectable with current instrumentation. About 100,000 of these can be felt. Minor earthquakes occur nearly constantly around the world in places like California and Alaska in the U.S., as well as in El Salvador, Mexico, Guatemala, Chile, Peru, Indonesia, Iran, Pakistan, the Azores in Portugal, Turkey, New Zealand, Greece, Italy, India, Nepal and Japan, but earthquakes can occur almost anywhere, including Downstate New York, England, and Australia. Larger earthquakes occur less frequently, the relationship being exponential; for example, roughly ten times as many earthquakes larger than magnitude 4 occur in a particular time period than earthquakes larger than magnitude 5. In the (low seismicity) United Kingdom, for example, it has been calculated that the average recurrences are:\nan earthquake of 3.7–4.6 every year, an earthquake of 4.7–5.5 every 10 years, and an earthquake of 5.6 or larger every 100 years. This is an example of the Gutenberg–Richter law.\n\nThe number of seismic stations has increased from about 350 in 1931 to many thousands today. As a result, many more earthquakes are reported than in the past, but this is because of the vast improvement in instrumentation, rather than an increase in the number of earthquakes. The United States Geological Survey estimates that, since 1900, there have been an average of 18 major earthquakes (magnitude 7.0–7.9) and one great earthquake (magnitude 8.0 or greater) per year, and that this average has been relatively stable. In recent years, the number of major earthquakes per year has decreased, though this is probably a statistical fluctuation rather than a systematic trend. More detailed statistics on the size and frequency of earthquakes is available from the United States Geological Survey (USGS).\nA recent increase in the number of major earthquakes has been noted, which could be explained by a cyclical pattern of periods of intense tectonic activity, interspersed with longer periods of low-intensity. However, accurate recordings of earthquakes only began in the early 1900s, so it is too early to categorically state that this is the case.\n\nMost of the world's earthquakes (90%, and 81% of the largest) take place in the 40,000 km long, horseshoe-shaped zone called the circum-Pacific seismic belt, known as the Pacific Ring of Fire, which for the most part bounds the Pacific Plate. Massive earthquakes tend to occur along other plate boundaries, too, such as along the Himalayan Mountains.\n\nWith the rapid growth of mega-cities such as Mexico City, Tokyo and Tehran, in areas of high seismic risk, some seismologists are warning that a single quake may claim the lives of up to 3 million people.\n\nWhile most earthquakes are caused by movement of the Earth's tectonic plates, human activity can also produce earthquakes. Four main activities contribute to this phenomenon: storing large amounts of water behind a dam (and possibly building an extremely heavy building), drilling and injecting liquid into wells, and by coal mining and oil drilling. Perhaps the best known example is the 2008 Sichuan earthquake in China's Sichuan Province in May; this tremor resulted in 69,227 fatalities and is the 19th deadliest earthquake of all time. The Zipingpu Dam is believed to have fluctuated the pressure of the fault away; this pressure probably increased the power of the earthquake and accelerated the rate of movement for the fault. The greatest earthquake in Australia's history is also claimed to be induced by humanity, through coal mining. The city of Newcastle was built over a large sector of coal mining areas. The earthquake has been reported to be spawned from a fault that reactivated due to the millions of tonnes of rock removed in the mining process.\n\nThe instrumental scales used to describe the size of an earthquake began with the Richter magnitude scale in the 1930s. It is a relatively simple measurement of an event's amplitude, and its use has become minimal in the 21st century. Seismic waves travel through the Earth's interior and can be recorded by seismometers at great distances. The surface wave magnitude was developed in the 1950s as a means to measure remote earthquakes and to improve the accuracy for larger events. The moment magnitude scale measures the amplitude of the shock, but also takes into account the seismic moment (total rupture area, average slip of the fault, and rigidity of the rock). The Japan Meteorological Agency seismic intensity scale, the Medvedev–Sponheuer–Karnik scale, and the Mercalli intensity scale are based on the observed effects and are related to the intensity of shaking.\n\nEvery tremor produces different types of seismic waves, which travel through rock with different velocities:\nPropagation velocity of the seismic waves ranges from approx. 3 km/s up to 13 km/s, depending on the density and elasticity of the medium. In the Earth's interior the shock- or P waves travel much faster than the S waves (approx. relation 1.7 : 1). The differences in travel time from the epicenter to the observatory are a measure of the distance and can be used to image both sources of quakes and structures within the Earth. Also, the depth of the hypocenter can be computed roughly.\n\nIn solid rock P-waves travel at about 6 to 7 km per second; the velocity increases within the deep mantle to ~13 km/s. The velocity of S-waves ranges from 2–3 km/s in light sediments and 4–5 km/s in the Earth's crust up to 7 km/s in the deep mantle. As a consequence, the first waves of a distant earthquake arrive at an observatory via the Earth's mantle.\n\nOn average, the kilometer distance to the earthquake is the number of seconds between the P and S wave times 8. Slight deviations are caused by inhomogeneities of subsurface structure. By such analyses of seismograms the Earth's core was located in 1913 by Beno Gutenberg.\n\nS waves and later arriving surface waves do main damage compared to P waves. P wave squeezes and expands material in the same direction it is traveling. S wave shakes the ground up and down and back and forth.\n\nEarthquakes are not only categorized by their magnitude but also by the place where they occur. The world is divided into 754 Flinn–Engdahl regions (F-E regions), which are based on political and geographical boundaries as well as seismic activity. More active zones are divided into smaller F-E regions whereas less active zones belong to larger F-E regions.\n\nStandard reporting of earthquakes includes its magnitude, date and time of occurrence, geographic coordinates of its epicenter, depth of the epicenter, geographical region, distances to population centers, location uncertainty, a number of parameters that are included in USGS earthquake reports (number of stations reporting, number of observations, etc.), and a unique event ID.\n\nAlthough relatively slow seismic waves have traditionally been used to detect earthquakes, scientists realized in 2016 that gravitational measurements could provide instantaneous detection of earthquakes, and confirmed this by analyzing gravitational records associated with the 2011 Tohoku-Oki (\"Fukushima\") earthquake.\n\nThe effects of earthquakes include, but are not limited to, the following:\n\nShaking and ground rupture are the main effects created by earthquakes, principally resulting in more or less severe damage to buildings and other rigid structures. The severity of the local effects depends on the complex combination of the earthquake magnitude, the distance from the epicenter, and the local geological and geomorphological conditions, which may amplify or reduce wave propagation. The ground-shaking is measured by ground acceleration.\n\nSpecific local geological, geomorphological, and geostructural features can induce high levels of shaking on the ground surface even from low-intensity earthquakes. This effect is called site or local amplification. It is principally due to the transfer of the seismic motion from hard deep soils to soft superficial soils and to effects of seismic energy focalization owing to typical geometrical setting of the deposits.\n\nGround rupture is a visible breaking and displacement of the Earth's surface along the trace of the fault, which may be of the order of several meters in the case of major earthquakes. Ground rupture is a major risk for large engineering structures such as dams, bridges and nuclear power stations and requires careful mapping of existing faults to identify any which are likely to break the ground surface within the life of the structure.\n\nEarthquakes, along with severe storms, volcanic activity, coastal wave attack, and wildfires, can produce slope instability leading to landslides, a major geological hazard. Landslide danger may persist while emergency personnel are attempting rescue.\n\nEarthquakes can cause fires by damaging electrical power or gas lines. In the event of water mains rupturing and a loss of pressure, it may also become difficult to stop the spread of a fire once it has started. For example, more deaths in the 1906 San Francisco earthquake were caused by fire than by the earthquake itself.\n\nSoil liquefaction occurs when, because of the shaking, water-saturated granular material (such as sand) temporarily loses its strength and transforms from a solid to a liquid. Soil liquefaction may cause rigid structures, like buildings and bridges, to tilt or sink into the liquefied deposits. For example, in the 1964 Alaska earthquake, soil liquefaction caused many buildings to sink into the ground, eventually collapsing upon themselves.\n\nTsunamis are long-wavelength, long-period sea waves produced by the sudden or abrupt movement of large volumes of water – including when an earthquake occurs at sea. In the open ocean the distance between wave crests can surpass , and the wave periods can vary from five minutes to one hour. Such tsunamis travel 600–800 kilometers per hour (373–497 miles per hour), depending on water depth. Large waves produced by an earthquake or a submarine landslide can overrun nearby coastal areas in a matter of minutes. Tsunamis can also travel thousands of kilometers across open ocean and wreak destruction on far shores hours after the earthquake that generated them.\n\nOrdinarily, subduction earthquakes under magnitude 7.5 on the Richter magnitude scale do not cause tsunamis, although some instances of this have been recorded. Most destructive tsunamis are caused by earthquakes of magnitude 7.5 or more.\n\nA flood is an overflow of any amount of water that reaches land. Floods occur usually when the volume of water within a body of water, such as a river or lake, exceeds the total capacity of the formation, and as a result some of the water flows or sits outside of the normal perimeter of the body. However, floods may be secondary effects of earthquakes, if dams are damaged. Earthquakes may cause landslips to dam rivers, which collapse and cause floods.\n\nThe terrain below the Sarez Lake in Tajikistan is in danger of catastrophic flood if the landslide dam formed by the earthquake, known as the Usoi Dam, were to fail during a future earthquake. Impact projections suggest the flood could affect roughly 5 million people.\n\nAn earthquake may cause injury and loss of life, road and bridge damage, general property damage, and collapse or destabilization (potentially leading to future collapse) of buildings. The aftermath may bring disease, lack of basic necessities, mental consequences such as panic attacks, depression to survivors, and higher insurance premiums.\n\nOne of the most devastating earthquakes in recorded history was the 1556 Shaanxi earthquake, which occurred on 23 January 1556 in Shaanxi province, China. More than 830,000 people died. Most houses in the area were yaodongs—dwellings carved out of loess hillsides—and many victims were killed when these structures collapsed. The 1976 Tangshan earthquake, which killed between 240,000 and 655,000 people, was the deadliest of the 20th century.\n\nThe 1960 Chilean earthquake is the largest earthquake that has been measured on a seismograph, reaching 9.5 magnitude on 22 May 1960. Its epicenter was near Cañete, Chile. The energy released was approximately twice that of the next most powerful earthquake, the Good Friday earthquake (March 27, 1964) which was centered in Prince William Sound, Alaska. The ten largest recorded earthquakes have all been megathrust earthquakes; however, of these ten, only the 2004 Indian Ocean earthquake is simultaneously one of the deadliest earthquakes in history.\n\nEarthquakes that caused the greatest loss of life, while powerful, were deadly because of their proximity to either heavily populated areas or the ocean, where earthquakes often create tsunamis that can devastate communities thousands of kilometers away. Regions most at risk for great loss of life include those where earthquakes are relatively rare but powerful, and poor regions with lax, unenforced, or nonexistent seismic building codes.\n\nEarthquake prediction is a branch of the science of seismology concerned with the specification of the time, location, and magnitude of future earthquakes within stated limits. Many methods have been developed for predicting the time and place in which earthquakes will occur. Despite considerable research efforts by seismologists, scientifically reproducible predictions cannot yet be made to a specific day or month.\n\nWhile forecasting is usually considered to be a type of prediction, earthquake forecasting is often differentiated from earthquake prediction. Earthquake forecasting is concerned with the probabilistic assessment of general earthquake hazard, including the frequency and magnitude of damaging earthquakes in a given area over years or decades. For well-understood faults the probability that a segment may rupture during the next few decades can be estimated.\n\nEarthquake warning systems have been developed that can provide regional notification of an earthquake in progress, but before the ground surface has begun to move, potentially allowing people within the system's range to seek shelter before the earthquake's impact is felt.\n\nThe objective of earthquake engineering is to foresee the impact of earthquakes on buildings and other structures and to design such structures to minimize the risk of damage. Existing structures can be modified by seismic retrofitting to improve their resistance to earthquakes. Earthquake insurance can provide building owners with financial protection against losses resulting from earthquakes Emergency management strategies can be employed by a government or organization to mitigate risks and prepare for consequences.\n\nIndividuals can also take preparedness steps like securing water heaters and heavy items that could injure someone, locating shutoffs for utilities, and being educated about what to do when shaking starts. For areas near large bodies of water, earthquake preparedness encompasses the possibility of a tsunami caused by a large quake.\n\nFrom the lifetime of the Greek philosopher Anaxagoras in the 5th century BCE to the 14th century CE, earthquakes were usually attributed to \"air (vapors) in the cavities of the Earth.\" Thales of Miletus, who lived from 625–547 (BCE) was the only documented person who believed that earthquakes were caused by tension between the earth and water. Other theories existed, including the Greek philosopher Anaxamines' (585–526 BCE) beliefs that short incline episodes of dryness and wetness caused seismic activity. The Greek philosopher Democritus (460–371 BCE) blamed water in general for earthquakes. Pliny the Elder called earthquakes \"underground thunderstorms.\"\n\nIn recent studies, geologists claim that global warming is one of the reasons for increased seismic activity. According to these studies melting glaciers and rising sea levels disturb the balance of pressure on Earth's tectonic plates thus causing increase in the frequency and intensity of earthquakes.\n\nIn Norse mythology, earthquakes were explained as the violent struggling of the god Loki. When Loki, god of mischief and strife, murdered Baldr, god of beauty and light, he was punished by being bound in a cave with a poisonous serpent placed above his head dripping venom. Loki's wife Sigyn stood by him with a bowl to catch the poison, but whenever she had to empty the bowl the poison dripped on Loki's face, forcing him to jerk his head away and thrash against his bonds, which caused the earth to tremble.\n\nIn Greek mythology, Poseidon was the cause and god of earthquakes. When he was in a bad mood, he struck the ground with a trident, causing earthquakes and other calamities. He also used earthquakes to punish and inflict fear upon people as revenge.\n\nIn Japanese mythology, Namazu (鯰) is a giant catfish who causes earthquakes. Namazu lives in the mud beneath the earth, and is guarded by the god Kashima who restrains the fish with a stone. When Kashima lets his guard fall, Namazu thrashes about, causing violent earthquakes.\n\nIn modern popular culture, the portrayal of earthquakes is shaped by the memory of great cities laid waste, such as Kobe in 1995 or San Francisco in 1906. Fictional earthquakes tend to strike suddenly and without warning. For this reason, stories about earthquakes generally begin with the disaster and focus on its immediate aftermath, as in \"Short Walk to Daylight\" (1972), \"The Ragged Edge\" (1968) or \"\" (1999). A notable example is Heinrich von Kleist's classic novella, \"The Earthquake in Chile\", which describes the destruction of Santiago in 1647. Haruki Murakami's short fiction collection \"After the Quake\" depicts the consequences of the Kobe earthquake of 1995.\n\nThe most popular single earthquake in fiction is the hypothetical \"Big One\" expected of California's San Andreas Fault someday, as depicted in the novels \"Richter 10\" (1996), \"Goodbye California\" (1977), \"2012\" (2009) and \"San Andreas\" (2015) among other works. Jacob M. Appel's widely anthologized short story, \"A Comparative Seismology\", features a con artist who convinces an elderly woman that an apocalyptic earthquake is imminent.\n\nContemporary depictions of earthquakes in film are variable in the manner in which they reflect human psychological reactions to the actual trauma that can be caused to directly afflicted families and their loved ones. Disaster mental health response research emphasizes the need to be aware of the different roles of loss of family and key community members, loss of home and familiar surroundings, loss of essential supplies and services to maintain survival. Particularly for children, the clear availability of caregiving adults who are able to protect, nourish, and clothe them in the aftermath of the earthquake, and to help them make sense of what has befallen them has been shown even more important to their emotional and physical health than the simple giving of provisions. As was observed after other disasters involving destruction and loss of life and their media depictions, recently observed in the 2010 Haiti earthquake, it is also important not to pathologize the reactions to loss and displacement or disruption of governmental administration and services, but rather to validate these reactions, to support constructive problem-solving and reflection as to how one might improve the conditions of those affected.\n\n\n\n"}
{"id": "2321375", "url": "https://en.wikipedia.org/wiki?curid=2321375", "title": "Elastic recoil detection", "text": "Elastic recoil detection\n\nElastic Recoil Detection Analysis (ERDA), also referred to as forward recoil scattering (or, contextually, spectrometry), is an Ion Beam Analysis technique in materials science to obtain elemental concentration depth profiles in thin films. This technique is known by several different names. These names are listed below. In the technique of ERDA, an energetic ion beam is directed at a sample to be characterized and (as in Rutherford backscattering) there is an elastic nuclear interaction between the ions of beam and the atoms of the target sample. Such interactions are commonly of Coulomb nature. Depending on the kinetics of the ions, cross section area, and the loss of energy of the ions in the matter, Elastic Recoil Detection Analysis helps determine the quantification of the elemental analysis. It also provides information about the depth profile of the sample.\n\nThe incident energetic ions can have a wide range of energy from 2 MeV to 200 MeV. The energy of the beam depends on the sample to be studied. The energy of the beam should be enough to kick out (“recoil”) the atoms of the sample. Thus, ERD usually employs appropriate source and detectors to detect recoiled atoms.\n\nHowever, such experimental setup is expensive and along with a source requirement of high energy ions appears to make this technique relatively less commonly used for materials characterization. Moreover, the angle of incidence that an ion beam makes with the sample must also be taken into account for correct analysis of the sample. This is because, depending on this angle, the recoiled atoms will be collected. Although it is not very clear, the assumption for why this technique is not very well known would be due to the fact that it is hard to have a perfect combination of the source, the angle of incidence, and the detector to have the best characterization of the sample. Such problem would make the technique very time consuming and tedious.\n\nThis article provides information about ERDA that has been around for a long time, since the mid-1970s, still not very well known. The article provides detailed information about the High ion Incident ERDA. However, Low ion Incident ERDA is still not neglected. The comparative analysis of overall ERDA with other techniques such as TEM, AFM, XRR, NR, VASE, XPS, and DSIMS is also mentioned. The article briefly touches upon the history of ERDA but the main focus is on the technique itself. Comprehensive information on the instrumentation as well as its applications in elemental characterization and depth profile are provided.\n\nERDA and RBS have similar theory but minor differences in the set-up of the experiment. In case of RBS, the detector is placed in the back of the sample whereas in ERDA, the detector is placed in the front. This difference in the set-up is shown in figure 1 on the right.\n\nThere are several techniques used today for characterization of materials. Combination of techniques is usually needed in order to obtain correct characterization. When comparing several techniques, important parameters like detection limit, depth resolution and lateral resolution must be taken into account. For this purpose, comparisons of several techniques have been shown in the table below.\n\nTable 1: Comparison of techniques with characteristic properties\n\nThe main characteristics of ERDA are listed below. \nSuch characteristics have led to many groups and scientists to explore applications of ERDA.\n\nERDA was first demonstrated by L’Ecuyer et al. in 1976. They had used 25-40 MeV Cl ions were used to detect the recoils in the sample. After more than three decades, ERDA has been divided into two main groups. First is the Light incident Ion ERDA (LI-ERDA) and the second is the Heavy incident Ion ERDA (HI-ERDA).\n\nLI-ERDA uses low voltage single-ended accelerators, whereas the HI-ERDA uses large tandem accelerators. These techniques were mainly developed after heavy ion accelerators were introduced in the materials research. LI-ERDA is also often performed using a relatively low energy (2 MeV) 4He beam specifically to depth profile hydrogen. In this technique, multiple detectors are used: at backscattering angles to detect heavier elements by RBS and forward (recoil) detector to simultaneously detect the recoiled hydrogen. The recoil detector for LI-ERDA typically has a “range foil” which is typically a Mylar foil placed in front of the detector for blocking scattered incident ions but allow lighter recoiling target atoms to pass through to the detector. Usually a 10 µm thick Mylar foil is able to completely stop 2.6 MeV He+ ions but allows the recoiled protons to go through with a low energy loss.\n\nHI-ERDA is more widely used compared to LI-ERDA because it can be used to study a lot more variety of elements compared to LI-ERDA. It can be used to identify heavy elements. It is used to detect recoiled target atoms and scattered beam ions using several detectors namely silicon diode detector, time-of-flight detector, gas ionization detector, etc. Such detectors are described below. Main advantage of HI-ERDA is its ability to obtain quantitative depth profiling information of all the sample elements in one measurement. Recent studies have shown that depth resolution obtained by using this technique is excellent. Depth resolution less than 1 nm can be obtained with good quantitative accuracy thus giving these techniques significant advantages over other surface analysis methods. Also, the accessible depth of 300 nm can be achieved using this technique. A wide range of ion beams including Cl, Cu, I, and Au, with different energies can be used in this technique in order for recoil to occur.\n\nIt must be noted that LI-ERDA and HI-ERDA both provide similar information. The difference in the name of the technique is only due to the type of ion beam used as a source.\n\nThe setup and the experimental conditions affect the performances of both of these techniques. Factors such as multiple scattering, and ion beam induced damage must be taken into account before obtaining the data because these processes can affect the data interpretation, quantification and the accuracy of the study. Additionally, the incident angle and the scattered angle help determine the sample surface topography. Incorporating surface topography into Ion Beam Analysis technique, a reliable characterization of layers of a surface can be obtained.\n\nERDA is very similar to RBS, but instead of detecting the projectile at the back angle, the recoils are detected in the forward direction. Doyle and Peercey in 1979 were able to establish the use of this technique for hydrogen depth profiling. Some of the prominent features of ERDA with high energy heavy ions are: \n\nERDA can overcome some of the limitations of Rutherford backscattering (RBS). ERDA has enabled depth profiling of elements from lightest elements like hydrogen up to heavy elements with high resolution in the light mass region as discussed above. Also, this technique has been highly sensitive because of the use of large area position sensitive telescope detectors This detector is used especially when the elements in the sample have similar masses. Telescope detectors are one way to distinguish such elements in the sample as it becomes extremely difficult to separate the elements using a normal detector.\n\nThe calculations that model this process are relatively simple, assuming projectile energy is in the range corresponding to Rutherford scattering. Projectile energy range for light incident ions is in 0.5-3.0 MeV/nucleon range. For heavier projectile ions such as I the energy range is usually between 60-120 MeV/nucleon; and for medium heavy ion beams,Cl is a common ion beam used with an energy of approximately 30 MeV/nucleon. For the instrumentation section, the focus will be on heavy ion bombardment. The \"E\" transferred by projectile ions of mass \"m\" and energy \"E\" to sample atoms of mass \"m\" recoiling at an angle \"ϕ\", with respect to the incidence direction is given by the following equation.\n\nEquation 1 models the energy transfer from the incident ions striking the sample atoms and the recoiling effect of the target atoms with an angle of \"ϕ\". For heavier ions in elastic recoil detection analysis, if \"m/m\" «1, all recoiling ions have similar velocities. It can be deduced from the previous equation the maximum scattering angle, \"θ\", as equation 2 describes:\n\nUsing these parameters, absorber foils do not need to be incorporated into the instrument design. When using heavy ion beams and the parameters above, the geometry can be estimated as to allow for incident particle collision and scattering at an angle deflected away from the detector. This will prevent degradation of the detector from the more intense beam energies.\n\nThe differential elastic recoil cross-section \"σ\" is given by:\n\nwhere \"Z\" and \"Z\" are the atomic numbers of projectile and sample atoms, respectively. For \"m/m\" «1 and with approximation \"m=2Z\"; \"Z\" being the atomic number of \"Z\" and \"Z\". In equation (3) two essential consequences can be seen, first the sensitivity is roughly the same for all elements and second it has a \"Z\" dependence on the projector of the ion. This allows for the use of low energy beam currents in HI-ERDA preventing sample degradation and excessive heating of the specimen.\n\nWhen using heavy ion beams, care must be taken for beam-induced damage in sample such as sputtering or amorphization. If only nuclear interaction is taken into account, it has been shown that the ratio of recoiling to displaced atoms is independent of \"Z\" and only weakly dependent on the projectile mass of the incident ion. With heavy ion bombardment, it has been shown that the sputter yield by the ion beam on the sample increases for nonmetallic samples and enhanced radiation damage in superconductors. In any case, the acceptance angle of the detector system should be as large as possible to minimize the radiation damage. However, it may reduce the depth profiling and elemental analysis due to the ion beam not being able to penetrate the sample.\nThe figure on the right summarizes the principles of ERDA and how a spectrum is obtained.\n\nThis demand of a large acceptance angle, however, is in conflict with the requirement of optimum depth resolution dependency on the detection geometry. In the surface approximation and assuming constant energy loss the depth resolution \"δx\" can be written:\n\nwhere \"S\" is the relative energy loss factor defined by:\n\nhere, \"α\" and \"β\" are the incidence angles of the beam and exit angle of the recoiling ion respectively, connected to the scattering angle \"ϕ\" by \"ϕ=α+β\". It should be noticed here that the depth resolution depends on the relative energy resolution only, as well as the relative stopping power of incoming and outgoing ions. The detector resolution and energy broadening associated with the measuring geometry contribute to the energy spread, \"δE\". The detector acceptance angle and the finite beam spot size define a scattering angle range \"δϕ\" causing a kinematic energy spread \"δE\" according to equation 6:\n\nA detailed analysis of the different contributions to depth resolution shows that this kinematic effect is the predominant term near the surface, severely limiting the permitted detector acceptance angle, whereas energy straggling dominates the resolution at larger depth. For example, if one estimates \"δϕ\" for a scattering angle of 37.5° causing a kinematic energy shift comparable to typical detector energy resolutions of 1%, the angular spread \"δψ\" must be less than 0.4°. The angular spread can be maintained within this range by contributions from the beam spot size; however, the solid angle geometry of the detector is only 0.04 msr. Therefore, a detector system with large solid angle as well as high depth resolution may enable corrections for the kinematic energy shift.\n\nIn an elastic scattering event, the kinematics requires that the target atom is recoiled with significant energy. Equation 7 models the recoil kinematical factor that occurs during the ion bombardment.\n\nEquation 7 gives a mathematical model of the collision event when the heavier ions in the beam strike the specimen. \"K\" is termed the \"kinematical factor\" for the scattered particle (Eq. 8) with a scattering angle of \"θ\", and the recoiled particle (Eq. 9) with a recoil angle of \"Φ\". The variable \"r\" is the ratio of mass of the incident nuclei to that of the mass of the target nuclei, (Eq. 10). To achieve this recoil of particles, the specimen needs to be very thin and the geometries need to be precisely optimized to obtain accurate recoil detection. Since ERD beam intensity can damage the specimen and there has been growing interest to invest in the development of low energy beams for reducing the damage of the specimen.\n\nThe cathode is divided into two insulated halves, where particle entrance position is derived from charges induced on the left, \"l\", and right, \"r\", halves of the cathode. Using the following equation, \"x\"-coordinates of particle positions, as they enter the detector, can be calculated from charges \"l\" and \"r\" :\n\nFurthermore, the \"y\"-coordinate is calculated from the following equation due to the position independence of the anode pulses:\n\nFor transformation of the \"(x, y)\" information into scattering angle \"ϕ\" a removable calibration mask in front of the entrance window is used. This mask allows correction for \"x\" and \"y\" distortions too. For notation detail, the cathode has an ion drift time on the order of a few ms. To prevent ion saturation of the detector, a limit of 1 kHz must be applied to the number of particles entering the detector.\n\nElastic Recoil Detection Analysis was originally developed for hydrogen detection or a light element (H, He, Li, C, O, Mg, K) profiling with an absorber foil in front of the energy detector for beam suppression. Using an absorber foil prevents the higher energy ion beam from striking the detector and causing degradation. Absorber foils increase the lifetime of the detector. More advanced techniques have been implemented to negate the use of absorber foils and the associated difficulties that arise through the use of it. In most cases, medium heavy ion beams, typically Cl ions, have been used for ERDA so far with energies around 30 MeV. Depth resolution and element profiling of thin films has been greatly advanced using elastic recoil detection analysis. Figure 2, on the left, depicts the interaction of a heavy ion beam striking the sample atoms and the resulting backscattering and recoil ions.\n\nParticle accelerators, such as a magnetron or cyclotron, implement electromagnetic fields to achieve acceleration of elements. Atoms must be electrically charged (ionized) before they can be accelerated. Ionization involves the removal of electrons from the target atoms. A magnetron can be used to produce H- ions. Van de Graaff generators have also been integrated with particle accelerators, shown in figure 3, for light ion beam generation.\n\nFor heavier ion production, for example, an electron cyclotron resonance (ECR) source can be used. Figure 4 shows a schematic diagram of an ECR. At the National Superconducting Cyclotron Laboratory, neutral atoms have their electrons removed using an ECR ion source. ECR works by ionizing the vapor of a desired element such as chlorine and iodine. Further, utilizing this technique metals (Au, Ag, etc.) can also be ionized using a small oven to achieve a vapor phase. The vapor is maintained within a magnetic field long enough for the atoms to be ionized by collisions with electrons. Microwaves are applied to the chamber as to keep the electrons in motion.\n\nThe vapor is introduced via injection directly into the “magnetic bottle” or the magnetic field. Circular coils provide the shape for the magnetic bottle. The coils are found at the top and bottom of the chamber with a hexapole magnet around the sides. A hexapole magnet consists of permanent magnets or superconducting coils. The plasma is contained within the magnetic trap that is formed from electric current flowing in solenoids located on the sides of the chamber. A radial magnetic field, exerted by the hexapole magnetic, is applied to the system that also confines the plasma. Acceleration of the electrons is achieved using resonance. For this to occur, the electrons must pass through a resonance zone. In this zone, their gyrofrequency or cyclotron frequency is equal to the frequency of the microwave injected into the plasma chamber. Cyclotron frequency is defined as the frequency of a charged particle moving perpendicular to the direction of a uniform magnetic field \"B\". Since the motion is always circular, cyclotron frequency-\"ω\" in radians/second-can be described by the following equation:\n\nwhere \"m\" is the mass of the particle, its charge is \"q\", and the velocity is \"v\". Ionization is a step-by-step process from collisions of the accelerated electrons with the desired vapor atoms. The gyrofrequency of an electron is calculated to be 1.76x107 Brad/second.\n\nNow that the vapor of the desired has been ionized, they must be removed from the magnetic bottle. To do this, a high voltage is between the hexapoles applied to pull out the ions from the magnetic field. The extraction of the ions, from the chamber, is carried out using an electrode system through a hole in a positively biased plasma chamber. Once the ions have been extracted from the chamber, they are then sent to the cyclotron for acceleration.\nIt is very important that the ion source used is optimal for the experiment being carried out. To perform an experiment in a practical amount of time, the ions provided from the accelerator complex should have the correct desired energy. The quality and stability of the ion beam needs to be considered carefully, due to the fact that only the ions with the correct flight trajectory can be injected in the cyclotron and accelerated to the desired energy.\n\nDuring ERDA, the idea is to place an ion beam source at a grazing angle to the sample. In this set up, the angle is calculated as to allow the incident ions to scatter off of the sample so that there is no contact made with the detector. The physical basis that has given the method its name stems from the elastic scattering of incident ions on a sample surface and detecting the recoiling sample atoms while the incident ions backscatter at such an angle, that they do not reach the detector; this is typically in reflection geometry, illustrated in figure shown:\n\nAnother method for preventing incident ions from making contact with the detector is to use an absorber foil. During analysis of the elastically recoiled particles, an absorber foil with selected specific thickness can be used to \"stop\" the heavy recoil and beam ions from reaching the detector; reducing the background noise. Incorporating an absorber into the experimental set up can be the most difficult to achieve. The stopping of the beam using either direct or scattered methods can only be accomplished without also stopping the light impurity atoms, if it is heavier (beam ions) than the impurity atoms being analyzed. There are advantages when using absorber films:\n\nThe major criterion for absorber foils used in ERDA is whether a recoiling impurity atom can be transmitted through the absorber, preferably a commercially available metal foil, while stopping heavy particles. Since the lighter atoms leave the absorber with smaller energies, the kinematic calculations do not provide much help. Favorable results have been obtained by using heavier ion beams of approximately 1 MeV/ nucleon. The best overall candidate is the Cl ion beam; although, Br would give better sensitivity by one order of magnitude compared to the Cl ion beam. The mass resolution, of the detector at \"θ= 0°\", of thin samples is \"ΔM/Δx\" ~ 0.3 amu/1000 Angstroms of the profile width. With thick samples, the mass resolution is feasible at θ≤30°. In thicker samples there is some degradation of mass resolution and slight loss of sensitivity. The detector solid angle has to be closed, but the thick sample can take more current without heating, which decreases sample degradation.\n\nOnce the ion beam has ionized target sample atoms, the sample ions are recoiled toward the detector. The beam ions are scattered at an angle that does not permit them to reach the detector. The sample ions pass through an entrance window of the detector, and depending on the type of detector used, the signal is converted into a spectrum.\n\nIn elastic recoil detection analysis, a silicon diode is the most common detector. This type of detector is commonly used, however, there are some major disadvantages when using this type of detector. For example, the energy resolution decreases significantly with a Si detector when detecting heavy recoiled ions. There is also a possibility of damage to the detector by radiation exposure. These detectors have a short functional lifetime (5–10 years) when doing heavy ion analysis. One of the main advantages of silicon detectors is their simplicity. However, they have to be used with a so-called “range foil” to range out the forward scattered heavy beam ions. Therefore, the simple range foil ERD has two major disadvantages: first, the loss of energy resolution due to the energy straggle and second, thickness inhomogeneity of the range foil, and the intrinsic indistinguishability of the signals for the various different recoiled target elements. Aside from the listed disadvantages, ERDA range foils with silicon detectors is still a powerful method and is relatively simple to work with.\n\nAnother method of detection for ERDA is time of flight (TOF)-ERD. This method does not present the same issues, as those for the silicon detector. However, the throughput of TOF detectors is limited; the detection is performed in a serial fashion (one ion in the detector at a time). The longer the TOF for ions, the better the time resolution (equivalent to energy resolution) will be. TOF spectrometers that have an incorporated solid state detector must be confined to small solid angles. When performing HI-ERDA, TOF detectors are often used and/or \"∆E/E\" detectors-such as ionization chambers. These types of detectors usually implement small solid angles for higher depth resolution. Figure 6, on the left, shows a Time-of-Flight detector commonly used in ERDA. Heavier ions have a longer flight time than the lighter ions. Detectors in modern time-of-flight instruments have improved sensitivity, temporal and spatial resolution, and lifetimes. Hi mass bipolar (high mass ion detection), Gen 2 Ultra Fast (twice as fast as traditional detectors), and High temperature (operated up to 150 °C) TOF are just a few of the commercially available detectors integrated with time-of-flight instruments. Linear and reflectron-TOF are the more common instruments used.\n\nA third type of detector is the gas ionization detector. The gas ionization detectors have some advantages over silicon detectors, for example, they are completely impervious to beam damage, since the gas can be replenished continuously. Nuclear experiments with large area ionization chambers increase the particle and position resolution have been used for many years and can easily be assimilated to any specific geometry. The limiting factor on energy resolution using this type of detector is the entrance window, which needs to be strong enough to withstand the atmospheric pressure of the gas, 20-90 mbar. Ultra-thin silicon nitride windows have been introduced, together with dramatic simplifications in the design, which have been demonstrated to be nearly as good as more complex designs for low energy ERD. These detectors have also been implemented in heavy ion Rutherford Backscattering Spectrometry. Figure 7 shows the Gas Ionization chamber with Isobutane as the detector gas. \n\nThe energy resolution obtained from this detector is better than a silicon detector when using ion beams heavier than helium ions. There are various designs of ionization detectors but a general schematic of the detector consists of a transversal field ionization chamber with a Frisch grid positioned between anode and cathode electrodes. The anode is subdivided into two plates separated by a specific distance. From the anode, signals \"∆E\"(energy lost), \"E\"(residual energy after loss), and \"E\" (the total energy \"E= ΔΕ+E)\" as well as the atomic number \"Z\" can be deduced. For this specific design, the gas used was isobutane at pressures of 20-90 mbar with a flow rate that was electronically controlled, the previous figure. A polypropylene foil was used as the entrance window. It has to be noted that the foil thickness homogeneity is of more importance for the detector energy resolution than the absolute thickness. If heavy ions are used and detected, the effect of energy loss straggling will be easily surpassed by the energy loss variation, which is a direct consequence of different foil thicknesses. The cathode electrode is divided in two insulated halves, thus information of particle entrance position is derived from charges induced at the right and left halves.\n\nERDA in transmission geometry, where only the energy of the recoiling sample atoms is measured, was extensively used for contamination analysis of target foils for nuclear physics experiments. This technique is excellent for discerning different contaminants of foils used in sensitive experiments, such as carbon contamination. Using I ion beam, a profile of various elements can be obtained and the amount of contamination can be determined. High levels of carbon contamination could be associated with beam excursions on the support, such as a graphite support. This could be corrected by using a different support material. Using a Mo support, the carbon content could be reduced from 20-100 at.% to 1-2 at.% level of the oxygen contamination probably originating from residual gas components. For nuclear experiments, high carbon contamination would result in extremely high background and the experimental results would be skewed or less differentiable with the background. With ERDA and heavy ion projectiles, valuable information can be obtained on the light element content of thin foils even if only the energy of the recoils is measured.\n\nGenerally, the energy spectra of different recoil elements overlap due to finite sample thickness, therefore particle identification is necessary to separate the contributions of different elements. Common examples of analysis are thin films of TiNO-Cu and BaBiKO. TiNO-Cu films were developed at the University of Munich and are used as tandem solar absorbers. Figure 8 shows the various components to the film. The copper coating and the glass substrate was also identified. Not only is ERDA is also coupled to Rutherford backscattering spectrometry, which is a similar process to ERDA. Using a solid angle of 7.5 mrs, recoils can be detected for this specific analysis of TiNO-Cu. It is important when designing an experiment to always consider the geometry of the system as to achieve recoil detection. In this geometry and with Cu being the heaviest component of the sample, according to eq. 2, scattered projectiles could not reach the detector. To prevent pileup of signals from these recoiled ions, a limit of 500 Hz needed to be set on the count rate of ΔΕ pulses. This corresponded to beam currents of lass than 20 particle pA.\n\nAnother example of thin film analysis is of BaBiKO. This type of film showed superconductivity at one of the highest-temperatures for oxide superconductors. Elemental analysis, shown in figure 9, of this film was carried out using heavy ion-ERDA. These elemental constitutes of the polymer film (Bi, K, Mg, O, along with carbon contamination) were detected using an ionization chamber. Other than Potassium, the lighter elements are clearly separated in the matrix. From the matrix, there is evidence of a strong carbon contamination within the film. Some films showed a 1:1 ratio of K to carbon contamination. For this specific film analysis, the source for contamination was traced to an oil diffusion pump and replaced with an oil free pumping system.\n\nIn the above examples, the main focus was identification of constituent particles found in thin films and depth resolution was of less significance. Depth resolution is of great importance in applications when a profile of a samples' elemental composition, in different sample layers, has to be measured. This is a powerful tool for materials characterization. Being able to quantify elemental concentration in sub-surface layers can provide a great deal of information pertaining to chemical properties. High sensitivity, i.e. large detector solid angle, can be combined with high depth resolution only if the related kinematic energy shift is compensated.\n\nThe Basic chemistry of Forward recoil scattering process is considered to be charged particle interaction with matters.\nTo Understand Forward recoil spectrometry we should know the physics involved in Elastic and Inelastic collisions. In Elastic collision only Kinetic Energy is conserved in the scattering process and there is no role of particle internal energy. Where as in case of \nInelastic collision both kinetic energy and internal energy are participated in the scattering process.\nPhysical concepts of two-body elastic scattering are the basis of several nuclear methods for elemental material characterization.\n\nThe Fundamental aspects in dealing with recoil spectroscopy involves electron back scattering process of matter such as thin films and solid materials. Energy loss of particles in target materials is evaluated by assuming that the target sample is laterally uniform and constituted by a mono isotopic element. This allows a simple relationship between that of penetration depth profile and elastic scattering yield\n\n\nPhysical concepts that are highly important in interpretation of forward recoil spectrum are depth profile, energy straggling, and multiple scattering. These concepts are described in detail in the following sections :\n\nA key parameter that characterizes recoil spectrometry is the depth resolution. This parameter is defined as the ability of an analytical technique to measure a variation in atomic distribution as a function of depth in a sample layer.\n\nIn terms of low energy forward recoil spectrometry, hydrogen and deuterium depth profiling can be expressed in a mathematical notation.\n\nΔx = ΔE/(dE/dx)\n\nwhere δE defines as the energy width of a channel in a multichannel analyzer, and dE/dx is the effective stopping power of the recoiled particles.\n\nLet us consider an Incoming and outgoing ion beams that are calculated as a function of collisional depth, by considering two trajectories are in a plane perpendicular to target surface, and incoming and outgoing paths are the shortest possible ones for a given collision depth and given scattering and recoil angles .\n\nImpinging ions reach the surface, making an angle θ, with the inward-pointing normal to the surface. After collision their velocity makes an angle θ, with the outward surface normal; and the atom initially at rest recoils, making an angle θ, with this normal. Detection is possible at one of these angles as such that the particle crosses the target surface.\nPaths of particles are related to collisional depth x, measured along a normal to the surface.\n\nThis Figure is plane representation of a scattered projectile on the target surface, when both incoming and outgoing paths are in perpendicular to target surface\n\nFor the impinging ion, length of the incoming path L is given by :formula_14\n\nThe outgoing path length L of the scattered projectile is :formula_15\n\nAnd finally the outgoing path L of the recoil is :formula_16\n\nThis Figure is plane representation of a Recoiled ion path on the target surface, when both incoming and outgoing paths are in perpendicular to target surface\n\nIn this simple case a collisional plane is perpendicular to the target surface, the scattering angle of the impinging ion is θ = π-θ-θ & the recoil angle is φ = π-θ-θ.\n\nTarget angle with the collisional plane is taken as α, and path is augmented by a factor of 1/cos α.\n\nFor the purpose of converting outgoing particle in to collision depth, geometrical factors are chosen.\n\nFor recoil R(φ, α)is defined as sin L = R(φ, α)L\n\nformula_17\n\nFor forward scattering of the projectile R(φ,α)by:L = R(θ,α)L\nR(θ,α) = cos θcosα/Sin θ√(cosα-cosθ)-cosθcosθ\n\nThe Figure below is the Geometrical configuration of recoil spectrometry. Paths of scattered particles are considered to be L for incident beam, L is for scattered particle, and L is for recoiled atoms.\n\nThe energy E(x) of the incident particle at a depth (x) to its initial energy E where scattering occurs is given by the following Equations. (Tirira. J., 1996)\n\nsimilarly Energy expression for scattered particle is:formula_19\n\nand for recoil atom is:formula_20\n\nThe energy loss per unit path is usually defined as stopping power and it is represented by :formula_21\n\nSpecifically, stopping power S(E) is known as a function of the energy E of an ion.\n\nStarting point for energy loss calculations is illustrated by the expression:\n\nBy applying above equation and energy conservation Illustrates expressions in 3 cases \n\nwhere E(x)= KE(x)and E(x)=K'E(x)\n\nS(E)and S_r(E) are stopping powers for projectile and recoil in the Target material\n\nFinally stopping cross section is defined by ɛ(E)= S(E)/N\nɛ is stopping cross section factor.\n\nTo obtain energy path scale We need to evaluate energy variation δE of the outgoing beam of energy E2 from the target surface for an increment δx of collisional depth, here E remains fixed. Evidently this causes changes in path lengths L and L a variation of path around the collision point x is related to the corresponding variation in energy before scattering :\n\nδL1 = δE(x)/S[E(x)----- Equation 5\n\nMoreover, particles with slight energy differences after scattering from a depth x undergo slight energy losses on their outgoing path.\nThen change δ L3 of the path length L3 can be written as \nδL3 = δ(K’E(x)]/ Sr[K’E0(x)) + δ(E)/SE) -----Equation 6\n\nδ L1 is the path variations due to energy variation just after the collision and δ L3 is the path variation because of variation of energy loss along the outward path.\nSolving equations 5 and 6 by considering δ x = 0 for the derivative dL1/dE2 with L3=R(φα)L1,yields\n\ndL1/dE2 = 1/{S(E)/S[K’E(x)]}{[R(φ,α) S[K’E(x)+K’S[E(x)]} -----------Equation 7\n\nIn elastic spectrometry, the term[S] is called as energy loss factor\n[S] = K’S(E(x))/Cos θ + S(K’E(x))2Cos θ -----------------Equation 8\n\nfinally stopping cross section is defined by ε(E) ≡S(E)/N\nwhere N is atomic density of the target material.\n\nStopping cross section factor [ε] = ((K^'ε(E(x) ))/cos θ )+(ε(K^' E(x) )/cosθ)--------Equation 9\n\nAn important parameter that characterizes recoil spectrometer is depth resolution. It is defined as the ability of an analytical technique to detect a variation in atomic distribution as a function of depth. The capability to separate in energy in the recoil system arising from small depth intervals. The expression for depth resolution is given as\n\nδR = δE/[{S(E)/SK'E(x)}][R(φ,α)SK'E(x)+K'SE(x)]-----------Equation 10\n\nWhere δE is the total energy resolution of the system, and the huge expression in the denominator is the sum of the path integrals of initial, scattered and recoil ion beams.\n\nThe concept of depth resolution represents the ability of Recoil spectrometry to separate the energies of scattered particles that occurred at slightly different depths\nδRx is interpreted as an absolute limit for determining concentration profile. From this point of view concentration profile separated by a depth interval of the order of magnitude of δRx would be undistinguishable in the spectrum, and obviously it is impossible to gain accuracy better than δRx to assign depth profile. In particular the fact that the signals corresponding to features of the concentration profile separated by less than δRx strongly overlap in the spectrum.\n\nA finite final depth resolution resulting from both theoretical and experimental limitations has deviation from exact result when consider an ideal situation. Final resolution is not coincide with theoretical evaluation such as the classical depth resolution δRx precisely because it results from three terms that escape from theoretical estimations: \n\nStraggling: Energy loss of particle in a dense medium is statistical in nature due to a large number of individual collisions between the particle and sample. Thus the evolution of an initially mono energetic and mono directional beam leads to dispersion of energy and direction. The resulting statistical energy distribution or deviation from initial energy is called energy straggling. Energy straggling data are plotted as a function of depth in the material.\n\nTheories of energy straggling : Energy straggling distribution is divided into three domains depending on the ratio of ΔE i.e., ΔE /E where ΔE is the mean energy loss and E is the average energy of the particle along the trajectory.\n\n1. Low fraction of energy loss: for very thin films with small path lengths, where ΔE/E ≤ 0.01, Landau and Vavilov derived that infrequent single collisions with large energy transfers contributes certain amount of loss in energy.\n\n2. Medium fraction of energy loss: for regions where 0.01< ΔE/E ≤ 0.2. Bohr’s model based on electronic interactions is useful for estimating energy straggling for this case, and this model includes the amount of energy straggling in terms of the areal density of electrons traversed by the beam. \nThe standard deviation ΩB of the energy distribution is given by : ΩB=4π((Ze)NZ∆x\nWhere NZΔx is the number of electrons per unit area over the path length increment Δx.\n\n3. Large fraction of energy loss: for fractional energy loss in the region of 0.2< ΔE/E ≤ 0.8, the energy dependence of stopping power causes the energy loss distribution to differ from Bohr’s straggling function. Hence the Bohr theory can not be applicable for this case. Various theoretical advances were made in understanding energy straggling in this case.\nAn expression of energy for straggling is proposed by Symon in the region of 0.2< ΔE/E ≤ 0.5 is a function of momentums Mi( Mi = M+M where M is stopping power, M is variation in straggling with depth of a stopping power)\n\nTschalar et al. derived a straggling function solving the differential equation:\nd Ω/dx = S(E) .d Ω/dE\n\nThe Tschalar’s expression which is valid for nearly symmetrical energy loss spectra, is\nΩ T = S[E(x)]σ(E) dE/S(E)\n\nWhere σ(E) represents energy straggling per unit length (or) variance of energy loss distribution per unit length for particles of energy E. E(x)is the mean energy at depth x.\n\nIn a similar way mass resolution is a parameter that characterizes the capability of recoil spectrometry to separate two signals arising from two neighboring elements in the target. The difference in the energy δE of recoil atoms after collision when two types of atoms differ in their masses by a quantity δM is\n\nδE/ δM = E (dK’/dM)\n\nδE/ δM = 4E(M(M-M)cosφ/(M+M)\n\nMass resolution δMR (≡ δE/ δM).\n\nA main limitation of using low beam energies is the reduced mass resolution. The energy separation of different masses is, in fact, directly proportional to the incident energy. The mass resolution is limited by the relative E and velocity v.\n\nExpression for mass resolution is ΔM = √(∂M/∂E.∆E) + √(∂M/∂v.∆v)\n\nΔM = M(√((∆E)/E)+√(2.∆v/v))\n\nE is the Energy, M is the mass and v is the velocity of the particle beam.and ΔM is reduced mass difference.\n\nWhen an Ion beam penetrating in to matter, ions undergo successive scattering events and deviates from original direction. The beam of ions in initial stage are well collimated(single direction), but after passing through a thickness of Δx in a random medium their direction of light propagation certainly differs from normal direction . As a result, both angular and lateral deviations from the initial direction can occur. These two parameters are discussed below. Hence, path length will be increased than expected causing fluctuations in ion beam. This process is called multiple scattering, and it is statistical in nature due to the large number of collisions.\n\nLateral displacement Case 1\n\nIon beam fluctuations because of lateral deviations on Target surface is explained by considering Multiple scattering of an ion beam which is directed in x – direction.\n\nAngular deviation Case 2\n\nIn the below figure there is a considerable difference between in the shape area of Gaussian peak (ideal condition) and angularly deviated peak. and α is an angle due to angular deviation of a penetrated ion beam through matter.\n\nIn the study of Multiple Scattering phenomenon angular distribution of a beam is important quantity for Consideration. The lateral distribution is closely related to the angular one but secondary to it, since lateral displacement is a consequence of angular divergence. Lateral distribution represents the beam profile in the matter. both lateral and angular Multiple scattering distributions are interdependent.\n\nThe analysis of Multiple Scattering was started by Bothe(Bothe, W,1921) and Wentzel (Wentzel, G,1922)in the Nineteen twenties using well-known approximation of small angles. The physics of energy straggling and Multiple Scattering was developed quite far by Williams from 1929 to 1945. Williams devised a theory, which consists of fitting the Multiple Scattering distribution as a Gaussian-like portion due to small scattering angles and the single collision tail due to the large angles. William, E.J., studied beta particle straggling, Multiple scattering of fast electrons and alpha particles, and cloud curvature tracks due to scattering to explain Multiple scattering in different scenario and he proposed a mean projection deflection occurrence due to scattering. His theory later extended to multiple scattering of alpha particles.\nGoudsmit and Saunderson provided a more complete treatment of Multiple Scattering, including large angles. For large angles Goudsmit considered series of Legendre polynomials which are numerically evaluated for distribution of scattering. The angular distribution from Coulomb scattering has been studied in detail by Molière.,(Molière:1948) and further continued by Marion and coworkers. Marion, J.B., and Young, F.C., in their Nuclear Reaction Analysis provided Tabular information regarding energy loss of charged particles in matter, Multiple scattering of charged particles, Range straggling of protons, deuterons and alpha particles, equilibrium charge states of ions in solids and energies of elastically scattered particles. Scott presents a complete review of basic theory, Mathematical methods, as well as results and applications.\nA comparative development of Multiple Scattering at small angles is presented by Meyer, based on a classical calculation of single cross section. Sigmund and Winterbon reevaluated Meyer’s calculation to extend it to a more general case. Marwick and Sigmund carried out development on lateral spreading by Multiple Scattering, which resulted in a simple scaling relation with the angular distribution.\n\nHI-ERDA and LI-ERDA have similar applications. As mentioned previously, the only difference between the two techniques is energy of the source used for the bombardment of the sample.\n\nERDA, in general, has many applications in the areas of polymer science, material science – semiconductor materials, electronics, and thin film characterization. ERDA is widely used in polymer science. This is because polymers are hydrogen-rich materials which can be easily studied by LI-ERDA. One can examine surface properties of polymers, polymer blends and evolution of polymer composition induced by irradiation. HI-ERDA can also be used in the field of new materials processed for microelectronics and opto-electronic applications. Moreover, elemental analysis and depth profiling in thin film can also be performed using ERDA.\n\nAn example of how ERDA can be used by scientists is shown below. In one of the experiments performed by Compsoto, et al., ERDA spectrum was obtained for a thin film of polystyrene (PS) on a deuterated polystyrene (dPS) matrix after annealing for 240 seconds at 171C. This spectrum is shown in figure 16 on the left.\n\nIt must be noted that the plot above is simply the normalized yield at each channel number from a thin dPS layer (about 200 Angstroms) on top of a thick PS. Normalized yield is usually the number of atoms detected. Channeling, however occurs when a beam of ions is carefully aligned with a major symmetry direction of a single crystal, such as a crystal axis or a plane. In this condition, most of the beam is steered through the channels formed by the string of atoms. Channeled particles cannot get close enough to the atomic nuclei to undergo scattering. Several mathematical operations were then performed, electronically, to obtain a concentration versus depth-profile as shown below in figure 17. Please refer to the source for detailed explanation of the mathematical equations.\n\nIn addition to all these applications, ERDA is one of the methods to follow elemental transport mechanism. More importantly, hydrogen transport near interfaces induced by corrosion and wear can be probed using ERDA. ERDA can also be used to perform composition diagnosis in various media.\n\nCharacterizing how polymer molecules behave at free polymer surfaces at interfaces between incompatible polymers and at interfaces with inorganic solid substances is crucial to our fundamental understanding and for improving the performance of polymers in high-tech applications. For example, the adhesion of two polymers strongly depends on the interactions occurring at the interface, between polymer segments. LI-ERDA is one of the most attractive methods for investigating these aspects of polymer science quantitatively.\n\nA typical LI- ERDA spectrum obtained using this technique to study elemental concentration and depth profile of polymers is shown in the figure 18 below. It is ERDA spectra of a thin (20 nm) dPS tracer film on a thick (500 nm) PS matrix.\n\nHydrogen and deuterium profiles can be measured using various polymer blends by using this technique. Green and Russel have studied the segregation of deuterated polystyrene/polymethamethacrylate copolymer at the interface of polystyrene, and polymethylmethacrylate homopolymer at the interface of polystyrene and polymethylmetacrylate homopolymer using ERDA with 2.8 MeV 4He+ ions. They also studied the interface properties of copolymers/Al or Si structures. Figure 19 shows the results obtained which is a typical ERD spectrum of yield vs. energy of P(d-S-b-d-MMA) block copolymer chains that segregated at the interface of the PS and PMMA homopolymers.\n\nThis profile can then be converted to volume fraction versus depth after doing several mathematical operations to obtain figure 20. In the figure 20 shown, the shaded region is the interface excess. The PS phase is located at x<0 whereas the PMMA phase is located at x>0. Please refer to the source to obtain complete analysis of the figure.\n\nThus the authors were able to see that copolymer chains segregate to the interfacial region between the PS and PMMA homolymer phases and elevated temperatures while others remain in bulk. Similar studies can be easily done using the technique of ERDA\n\nThe profile that lies in the energy range between 600 and 1000 keV is the hydrogen from the homopolymers and the other profile, which lies between 1000 and 1400 keV, is that of the deuterium from the copolymer chains.\n\nIon implantation is one of the methods used to transform physical properties of polymers and to improve their electrical, optical, and mechanical performance. Ion implantation is a technique by which the ions of a material are accelerated in an electrical field and impacted into a materials such that ion are inserted into this material. This technique has many important uses. One such example is the introduction of silver plasma into the biomedical titanium. This is important because Titanium-based implantable devices such as joint prostheses, fracture fixation devices and dental implants, are important to human lives and improvement of the life quality of patients. However, biomedical titanium is lack of Osseo integration and antibacterium ability. Plasma immersion ion implantation (PIII) is a physical technique which can enhance the multi-functionality, mechanical and chemical properties as well as biological activities of artificial implants and biomedical devices. ERDA can be used to study this phenomenon very effectively. Moreover, many scientists have measured the evolution of electrical conductivity, optical transparency, corrosion resistance, and wear resistance of different polymers after irradiation by electron or low-energy light ions or high-energy heavy ions.\n\nElectronic devices are usually composed of sequential thin layers made up of oxides, nitrides, silicades, metals, polymers, or doped semiconductor–based media coated on a single crystalline substrate (Si, Ge or AsGa). These structures can be studied by HI-ERDA. This technique has one major advantage over other methods. The profile of impurities can be found in a one-shot measurement at a constant incident energy. Moreover, this technique offers an opportunity to study the density profiles of hydrogen, carbon and oxygen in various materials, as well as the absolute hydrogen, carbon and oxygen content.\n\nCombination of techniques is required in order to study the composition of thin films. Ion beam techniques – RBS and elastic recoil detection analysis combination has proved to be an attractive way to study the elemental composition of the samples as well as the depth profiles of the thin films. ERDA technique is capable of separating masses and energies of scattered incident ions and the recoiled target atoms. It is especially useful to profile light elements such as H, B, C, N, and O in the presence of heavier material background. Thus it has proved to be a useful technique in studying the composition of the thin films. The dependence of the hydrogen density profile on the features of processing and maintenance, and the effect of injected hydrogen on the dielectric properties of ditantalum pentoxide can also be studied.\n\n"}
{"id": "2634195", "url": "https://en.wikipedia.org/wiki?curid=2634195", "title": "Electrical ballast", "text": "Electrical ballast\n\nAn electrical ballast is a device placed in line with the load to limit the amount of current in an electrical circuit. It may be a fixed or variable resistor.\n\nA familiar and widely used example is the inductive ballast used in fluorescent lamps to limit the current through the tube, which would otherwise rise to a destructive level due to the negative differential resistance of the tube's voltage-current characteristic.\n\nBallasts vary greatly in complexity. They may be as simple as a resistor, inductor or capacitor (or a combination of these) wired in series with the lamp; or as complex as the electronic ballasts used in compact fluorescent lamps and high-intensity discharge lamps.\n\nAn electrical ballast is a device which limits the current through an electrical load. These are most often used when a load (such as an arc discharge) has its terminal voltage decline when current through the load increases. If such a device were connected to a constant-voltage power supply, it would draw an increasing amount of current until it will be destroyed or caused the power supply to fail. To prevent this, a ballast provides a positive resistance or reactance that limits the current. The ballast provides for the proper operation of the negative-resistance device by limiting current.\n\nBallasts can also be used simply to limit the current in an ordinary, positive-resistance circuit. Prior to the advent of solid-state ignition, automobile ignition systems commonly included a ballast resistor to regulate the voltage applied to the ignition system.\n\nSeries resistors are used as ballasts to control the current through LEDs.\n\nFor simple, low-powered loads such as a neon lamp or a LED, a fixed resistor is commonly used. Because the resistance of the ballast resistor is large it determines the current in the circuit, even in the face of negative resistance introduced by the neon lamp.\n\nBallast was also a component used in early model automobiles engine that lowered the supply voltage to the ignition system after the engine had been started. Starting the engine requires a significant amount of electrical current from the battery, resulting in an equally significant voltage drop. To allow the engine to start, the ignition system was designed to operate on this lower voltage. But once the vehicle was started and the starter disengaged, the normal operating voltage was too high for the ignition system. To avoid this problem, a ballast resistor was inserted in series with the ignition system, resulting in two different operating voltages for the starting and ignition systems.\n\nOccasionally, this ballast resistor would fail and the classic symptom of this failure was that the engine ran while being cranked (while the resistor was bypassed) but stalled immediately when cranking ceased (and the resistor was reconnected in the circuit via the ignition switch). Modern electronic ignition systems (those used since the 1980s or late '70s) do not require a ballast resistor as they are flexible enough to operate on the lower cranking voltage or the normal operating voltage.\n\nAnother common use of a ballast resistor in the automotive industry is adjusting the ventilation fan speed. The ballast is a fixed resistor with usually two center taps, and the fan speed selector switch is used to bypass portions of the ballast: all of them for full speed, and none for the low speed setting. A very common failure occurs when the fan is being constantly run at the next-to-full speed setting (usually 3 out of 4). This will cause a very short piece of resistor coil to be operated with a relatively high current (up to 10 A), eventually burning it out. This will render the fan unable to run at the reduced speed settings.\n\nIn some consumer electronic equipment, notably in television sets in the era of valves (vacuum tubes), but also in some low-cost record players, the vacuum tube heaters were connected in series. Since the voltage drop across all the heaters in series was usually less than the full mains voltage, it was necessary to provide a ballast to drop the excess voltage. A resistor was often used for this purpose, as it was cheap and worked with both AC and DC.\n\nSome ballast resistors have the property of increasing in resistance as current through them increases, and decreasing in resistance as current decreases. Physically, some such devices are often built quite like incandescent lamps. Like the tungsten filament of an ordinary incandescent lamp, if current increases, the ballast resistor gets hotter, its resistance goes up, and its voltage drop increases. If current decreases, the ballast resistor gets colder, its resistance drops, and the voltage drop decreases. Therefore, the ballast resistor reduces variations in current, despite variations in applied voltage or changes in the rest of an electric circuit. These devices are sometimes called \"barretters\" and were used in the series heating circuits of 1930s to 1960s AC/DC radio and TV home receivers.\n\nThis property can lead to more precise current control than merely choosing an appropriate fixed resistor. The power lost in the resistive ballast is also reduced because a smaller portion of the overall power is dropped in the ballast compared to what might be required with a fixed resistor.\n\nEarlier, household clothes dryers sometimes incorporated a germicidal lamp in series with an ordinary incandescent lamp; the incandescent lamp operated as the ballast for the germicidal lamp. A commonly used light in the home in the 1960s in 220–240 V countries was a circular tube ballasted by an under-run regular mains filament lamp. Self ballasted mercury-vapor lamps incorporate ordinary tungsten filaments within the overall envelope of the lamp to act as the ballast, and it supplements the otherwise lacking red area of the light spectrum produced.\n\nBecause of the power that would be lost, resistors are not used as ballasts for lamps of more than about two watts. Instead, a reactance is used. Losses in the ballast due to its resistance and losses in its magnetic core may be significant, on the order of 5 to 25% of the lamp input electric power. Practical lighting design calculations must allow for ballast loss in estimating the running cost of a lighting installation.\n\nAn inductor is very common in line-frequency ballasts to provide the proper starting and operating electrical condition to power a fluorescent lamp, neon lamp, or high intensity discharge (HID) lamp. (Because of the use of the inductor, such ballasts are usually called \"magnetic ballasts\".) The inductor has two benefits:\n\nA disadvantage of the inductor is that current is shifted out of phase with the voltage, producing a poor power factor. In more expensive ballasts, a capacitor is often paired with the inductor to correct the power factor. In ballasts that control two or more lamps, line-frequency ballasts commonly use different phase relationships between the multiple lamps. This not only mitigates the flicker of the individual lamps, it also helps maintain a high power factor. These ballasts are often called \"lead-lag\" ballasts because the current in one lamp leads the mains phase and the current in the other lamp lags the mains phase.\n\nIn Europe, and most 220-240 V territories, the line voltage is sufficient to start lamps over 20W with a series inductor. In North America and Japan however, the line voltage (120 V or 100 V respectively) may not be sufficient to start lamps over 20 W with a series inductor, so an autotransformer winding is included in the ballast to step up the voltage. The autotransformer is designed with enough leakage inductance (short-circuit inductance) so that the current is appropriately limited.\n\nBecause of the large inductors and capacitors that must be used, reactive ballasts operated at line frequency tend to be large and heavy. They commonly also produce acoustic noise (line-frequency hum).\n\nPrior to 1980 in the United States, PCB-based oils were used as an insulating oil in many ballasts to provide cooling and electrical isolation (see transformer oil).\n\nAn electronic ballast uses solid state electronic circuitry to provide the proper starting and operating electrical conditions to power discharge lamps. An electronic ballast can be smaller and lighter than a comparably-rated magnetic one. An electronic ballast is usually quieter than a magnetic one, which produces a line-frequency hum by vibration of the transformer laminations.\n\nElectronic ballasts are often based on SMPS topology, first rectifying the input power and then chopping it at a high frequency. Advanced electronic ballasts may allow dimming via pulse-width modulation or via changing the frequency to a higher value. Ballasts incorporating a microcontroller (digital ballasts) may offer remote control and monitoring via networks such as LonWorks, DALI, DMX512, DSI or simple analog control using a 0-10 V DC brightness control signal. Systems with remote control of light level via a wireless mesh network have been introduced.\n\nElectronic ballasts usually supply power to the lamp at a frequency of or higher, rather than the mains frequency of ; this substantially eliminates the stroboscopic effect of flicker, a product of the line frequency associated with fluorescent lighting (see photosensitive epilepsy). The high output frequency of an electronic ballast refreshes the phosphors in a fluorescent lamp so rapidly that there is no perceptible flicker. The flicker index, used for measuring perceptible light modulation, has a range from 0.00 to 1.00, with 0 indicating the lowest possibility of flickering and 1 indicating the highest. Lamps operated on magnetic ballasts have a flicker index between 0.04–0.07 while digital ballasts have a flicker index of below 0.01.\n\nBecause more gas remains ionized in the arc stream, the lamp operates at about 9% higher efficacy above approximately 10 kHz. Lamp efficiency increases sharply at about 10 kHz and continues to improve until approximately 20 kHz. Electronic ballast retrofits to existing street lights had been tested in some Canadian provinces circa 2012; since then LED retrofits have become more common.\n\nWith the higher efficiency of the ballast itself and the higher lamp efficacy at higher frequency, electronic ballasts offer higher system efficacy for low pressure lamps like the fluorescent lamp. For HID lamps, there is no improvement of the lamp efficacy in using higher frequency, but for these lamps the ballast losses are lower at higher frequencies and also the light depreciation is lower, meaning the lamp produces more light over its entire lifespan. Some HID lamp types like the ceramic discharge metal halide lamp have reduced reliability when operated at high frequencies in the range of ; for these lamps a square wave low frequency current drive is mostly used with frequency in the range of , with the same advantage of lower light depreciation.\n\nApplication of electronic ballasts to HID lighting is growing in popularity. Most newer generation electronic ballasts can operate both high pressure sodium (HPS) lamps as well as metal-halide lamps, reducing costs for building managers who use both types of lamps. The ballast initially works as a starter for the arc, supplying a high-voltage impulse and, later, it works as a limiter/regulator of the electric flow inside the circuit. Electronic ballasts also run much cooler and are lighter than their magnetic counterparts.\n\nThis technique uses a combination filament–cathode at each end of the lamp in conjunction with a mechanical or automatic (bi-metallic or electronic) switch that initially connect the filaments in series with the ballast to preheat them. When filaments are disconnected, an inductive pulse from the ballast starts the lamp. This system is described as \"Preheat\" in North America and \"Switch Start\" in the UK, and have no specific name in the rest part of the world. This systems is common in 200–240 V countries (and for 100–120 V lamps up to about 30 watts).\n\nAlthough an inductive pulse makes it more likely that the lamp will start when the starter switch opens, it is not actually necessary. The ballast in such systems can equally be a resistor. A number of fluorescent lamp fittings used a filament lamp as the ballast in the late 1950s through to the 1960s. Special lamps were manufactured that were rated at 170 volts and 120 watts. The lamp had a thermal starter built into the 4 pin base. The power requirements were much larger than using an inductive ballast (though the consumed current was the same), but the warmer light from the lamp type of ballast was often preferred by users particularly in a domestic environment.\n\nResistive ballasts were the only type that was usable when the only supply available to power the fluorescent lamp was DC. Such fittings used the thermal type of starter (mostly because they had gone out use long before the glow starter was invented), but it was possible to include a choke in the circuit whose sole purpose was to provide a pulse on opening of the starter switch to improve starting. DC fittings were complicated by the need to reverse the polarity of the supply to the tube each time it started. Failure to do so vastly shortened the life of the tube.\n\nAn instant start ballast does not preheat the electrodes, instead using a relatively high voltage (~600 V) to initiate the discharge arc. It is the most energy efficient type, but yields the fewest lamp-start cycles, as material is blasted from the surface of the cold electrodes each time the lamp is turned on. Instant-start ballasts are best suited to applications with long duty cycles, where the lamps are not frequently turned on and off. Although these were mostly used in countries with 100-120 volt mains supplies (for lamps of 40 W or above), they were briefly popular in other countries because the lamp started without the flicker of switch start systems. The popularity was short lived because of the short lamp life.\n\nA rapid start ballast applies voltage and heats the cathodes simultaneously. It provides superior lamp life and more cycle life, but uses slightly more energy as the electrodes in each end of the lamp continue to consume heating power as the lamp operates. Again, although popular in 100-120 volt countries for lamps of 40 W and above, rapid start is sometimes used in other countries particularly where the flicker of switch start systems is undesirable. This ballast is incompatible with the European energy saver T8 fluorescent lamps, that retrofits the older T12 lamps, since these lamps have much higher starting voltage than the open-circuit voltage of rapid start ballasts.\n\nA dimmable ballast is very similar to a rapid start ballast, but usually has a capacitor incorporated to give a power factor nearer to unity than a standard rapid start ballast. A quadrac type light dimmer can be used with a dimming ballast, which maintains the heating current while allowing lamp current to be controlled. A resistor of about 10 kΩ is required to be connected in parallel with the fluorescent tube to allow reliable firing of the quadrac at low light levels.\n\nUsed in high end electronic fluorescent ballasts. This ballast applies power to the filaments first, it allows the cathodes to preheat and then applies voltage to the lamps to strike an arc. Lamp life typically operates up to 100,000 on/off cycles when using programmed start ballasts. Once started, filament voltage is reduced to increase operating efficiency. This ballast gives the best life and most starts from lamps, and so is preferred for applications with very frequent power cycling such as vision examination rooms and restrooms with a motion detector switch.\n\nA hybrid ballast has a magnetic core-and-coil transformer and an electronic switch for the electrode-heating circuit. Like a magnetic ballast, a hybrid unit operates at line power frequency—50 Hz in Europe, for example. These types of ballasts, which are also referred to as \"cathode-disconnect ballasts\", disconnect the electrode-heating circuit after they start the lamps.\n\nFor a lighting ballast, the ANSI \"ballast factor\" is used in North America to compare the light output (in lumens) of a lamp operated on a ballast compared to the lamp operating on an ANSI reference ballast. Reference ballast operates the lamp at its ANSI specified nominal power rating. The ballast factor of practical ballasts must be considered in lighting design; a low ballast factor may save energy, but will produce less light. With fluorescent lamps, ballast factor can vary from the reference value of 1.0.\n\nEarly tube-based color TV sets used a ballast triode, such as the PD500, as a parallel shunt stabilizer for the CRT acceleration voltage, to keep the CRT's deflection factor constant.\n\n\n"}
{"id": "17248403", "url": "https://en.wikipedia.org/wiki?curid=17248403", "title": "Enova SF", "text": "Enova SF\n\nEnova SF is a Norwegian government enterprise responsible for promotion of environmentally friendly production and consumption of energy. Its stated purpose is to explore new sources of clean energy, reduce overall energy consumption, and to provide educational materials to the public promoting energy-efficient practices. Established in 2001, it is financed through government funding in addition to a tariff of 1 øre per kWh of electricity to consumers. The company is owned by the Norwegian Ministry of Petroleum and Energy and based in Trondheim.\n"}
{"id": "3947744", "url": "https://en.wikipedia.org/wiki?curid=3947744", "title": "Free surface effect", "text": "Free surface effect\n\nThe free surface effect is a mechanism which can cause a watercraft to become unstable and capsize.\n\nIt refers to the tendency of liquids — and of unbound aggregates of small solid objects, like seeds, gravel, or crushed ore, whose behavior approximates that of liquids — to move in response to changes in the attitude of a craft's cargo holds, decks, or liquid tanks in reaction to operator-induced motions (or sea states caused by waves and wind acting upon the craft). When referring to the free surface effect, the condition of a tank that is not full is described as a \"slack tank\", while a full tank is \"pressed up\".\n\nIn a normally loaded vessel any rolling from perpendicular is countered by a righting moment generated from the increased volume of water displaced by the hull on the lowered side. This assumes the center of gravity of the vessel is relatively constant. If a moving mass inside the vessel moves in the direction of the roll, this counters the righting effect by moving the center of gravity towards the lowered side. The free surface effect can become a problem in a craft with large partially full bulk cargo compartments, fuel tanks, or water tanks (especially if they span the full breadth of the ship), or from accidental flooding, such as has occurred in several accidents involving roll-on/roll-off ferries.\n\nIf a compartment or tank is either empty or full, there is no change in the craft's center of mass as it rolls from side to side (in strong winds, heavy seas, or on sharp motions or turns). However, if the compartment is only partially full, the liquid in the compartment will respond to the vessel's heave, pitch, roll, surge, sway or yaw. For example, as a vessel rolls to port, liquid will displace to the port side of a compartment, and this will move the vessel's center of mass to port. This has the effect of slowing the vessel's return to vertical.\n\nThe momentum of large volumes of moving liquids cause significant dynamic forces, which act against the righting effect. When the vessel returns to vertical the roll continues and the effect is repeated on the opposite side. In heavy sea states, this can become a positive feedback loop, causing each roll to become more and more extreme, eventually overcoming the righting effect leading to a capsize. While repeated oscillations of increasing magnitude are commonly associated with the free surface effect, they are not a necessary condition. For example, in the cases of both the and , gradual buildup of water from fire-fighting caused capsizing in a single continuous roll.\n\nTo mitigate this hazard, cargo vessels use multiple smaller bulk compartments or liquid tanks, instead of fewer larger ones, and possibly baffles within bulk compartments or liquid tanks to minimize the free surface effects on the craft as a whole. Keeping individual bulk compartments or liquid tanks either relatively empty or full is another way to minimize the effect and its attendant problems. Hydraulic tankers use water to displace lighter oil to keep the tank full at all times. Tanks or compartments that do not straddle the vessel's centerline are somewhat less prone to destabilising oscillations. Similarly, narrow compartments (aligned bow to stern) and compartments at the extremes away from the centerline, are less prone to cause instability.\n\nFlooding, liquid cargo leakage, or unintended water (from precipitation, waves, or hull damage) in any compartments or on any decks of watercraft, and the resulting free surface effect are often a contributing cause of accidents, capsizes, and casualties e.g. the loss of (Wellington, New Zealand, April 1968), (Zeebrugge, Belgium, March 1987), and (Baltic Sea, September 1994). In the case of the RORO ferry \"al-Salam Boccaccio 98\" (Red Sea, February 2006), improper fire-fighting procedures caused flooding leading directly to instability and capsize. In both the cases of the \"al-Salam Boccaccio 98\" and , severe listing followed immediately after the ship had undergone a hard turn, causing unstable volumes of water (from collision damage in the latter) to surge from one side of the ship to the other.\n\nThe free surface effect can affect any kind of craft, including watercraft (where it is most common), bulk cargo or liquid tanker semi-trailers and trucks (causing either jackknifing or roll-overs), and aircraft (especially fire-fighting water-droppers and refueling tankers where baffles mitigate but do not eliminate the effects). The term \"free surface effect\" implies a liquid under the influence of gravity. Slosh dynamics is the overarching field which covers both free surface effects and situations such as space vehicles, where gravity is inconsequential but inertia and momentum interact with complex fluid mechanics to cause vehicle instability.\n\n\n"}
{"id": "39373538", "url": "https://en.wikipedia.org/wiki?curid=39373538", "title": "Greenheart Group", "text": "Greenheart Group\n\nGreenheart Group () is a listed multi-national forestry company based in Hong Kong.\n\nThe company owns 13,000 hectares of softwood plantation forest in New Zealand and 322,000 hectares of concessions and harvesting rights in Suriname. Greenheart sells softwood logs to China, India, South Korea and also domestically in New Zealand. The company also sells hardwood lumber, and various wood products to China and European countries.\n\nGreenheart Group operates a softwood plantation forest business in New Zealand with radiata pine as main its main commercial species. The company exports its radiata pine logs to countries such as China, India, Korea and Japan.\n\nGreenheart Group operates a hardwood business in Suriname. The company practices sustainable forest harvesting and has two large scale sawmills to produce lumber for sales to Europe and Asia. The company is also building a bioenergy plant to convert timber waste into clean energy.\n\nGreenheart Group currently has Forest Stewardship Council (FSC®) controlled wood status in West Suriname forest concession and full Forest Stewardship Council (FSC®) certification for its Central Suriname forest concession. The company practices sustainable forest management and silviculture systems to ensure a systematic pruning which allows a faster natural regeneration of trees.\n\nGreenheart's annual revenue has increased 29 times since formal branding and establishment in 2010.\n"}
{"id": "54799674", "url": "https://en.wikipedia.org/wiki?curid=54799674", "title": "Harkleroad wind turbine", "text": "Harkleroad wind turbine\n\nThe Harkleroad wind turbine was built to generate electricity. It appears along a heavily traveled freeway in northern California.\n\nAround 1960, Sam Harkleroad designed and constructed three unusual structures on a plot of land in Novato, Marin County, California:\n\nSam Harkleroad was born on October 20, 1909, and grew up on a farm in Fresno. His education ended with high school. He lived much of his life in Marin County. Harkleroad died on June 12, 1993, at the age of 83.\n\nHarkleroad was a contractor, who liked to use recycled materials.\n\nThe Harkleroad structures are on a street named Harkle Road.\n\nThe round house once appeared in a Popular Mechanics article, which explained how the house was able to rotate 320 degrees, while the plumbing and electrical systems continued to function.\nOne of the three buildings has a saddle roof made of 2 by 4 lumber.\nThis house was for sale in 1959, with a minimum bid of $32,000.\n\nThe roof of the workshop has a wind turbine. The turbine once provided electricity for the workshop, however, it is no longer used for electricity. Instead, it provides an novel sight for travelers along U.S. Route 101 in Marin County, California.\n\n"}
{"id": "6734580", "url": "https://en.wikipedia.org/wiki?curid=6734580", "title": "Hatton-Brown Publishers", "text": "Hatton-Brown Publishers\n\nThe Hatton-Brown organization in Montgomery, Alabama dates back to 1948, when local newspaperman Hartwell Hatton founded Hatton Publications at age 49. His first forest-oriented magazine, \"Alabama Lumberman\", was published from 1949-57.\n\nHatton, Brown & Co., Inc. was established in 1953, the same year the company started a new Southern regional logging title, \"Pulpwood Production\". At 33, Charles Cline joined the company as editor in the summer of 1953 and helped get the first issue into print that August. \"Pulpwood Production\"'s title was lengthened to \"Pulpwood Production & Saw Mill Logging\" in 1956 and its circulation was extended into the Lake States and New England in 1962. Dianne Sullivan joined the company as circulation manager in 1964. \n\nIn 1966 and 1967 Hatton and Cline searched for a full time advertising salesman and editor. David Ramsey, joined the staff as sales manager February 1968 and David Knight joined the company 30 days later as an editor.\n\nMr. Hatton retired in 1971 selling his stock to Charles Cline, David Ramsey and David Knight. Dianne Sullivan then became office manager.\n\nAnticipating the need for an economical logging newspaper in the South, Hatton-Brown launched \"Loggin' Times\" (later titled \"Southern Loggin' Times\") in 1972. In late 1974, sensing a change in the nature of the traditional pulpwood market, management decided to phase in a new name for \"Pulpwood Production & Saw Mill Logging\". The latter part of the title was dropped and replaced with the words, \"Timber Harvesting\". The journal carried a double title until it went national in 1977 and became more fittingly known as \"Timber Harvesting\".\nIn 1977 \"Timber Processing Industry\", (later shortened to \"Timber Processing\") began as a regional tabloid newspaper for the sawmill Industry and remained so until 1978. In 1979 \"Timber Processing\" became a national sawmill magazine.\n\nIn 1981 Charles Cline retired and David Ramsey and David Knight became the new owners. Ramsey and Knight formed a new corporation Hatton-Brown Publishers, Inc. Dianne Sullivan became Secretary of the corporation, and was named General Manager and joined the owners on the Board of Directors.\n\nIn June of that year the company moved from 458 S. Lawrence St. to larger facilities at 610 S. McDonough St.\n\nIn 1981 David Ramsey, David Knight and some key employees of Hatton-Brown purchased another publication. A separate corporation, Plywood and Panel World, Inc., was formed to purchase a magazine known as \"Plywood & Panel\". This journal, officially acquired in January 1982, was subsequently renamed \"Plywood & Panel World\" and later \"Panel World\".\n\nHatton-Brown has also published the official program for various logging equipment expositions through the years. Some of which included: Timber Harvesting Expo-SE in south Georgia and Carolina Log'n Demo in eastern North Carolina.\n\nIn late 1988, with employment practically double the number of 1981, the company owners elected to build a new and larger office building at the corner of Clay and Hanrick Streets in Montgomery. The building was completed and occupied in April 1990. The official open house followed on June 7, 1990.\n\nHatton-Brown diversified its publishing interests in January 1991 by acquiring its first non-forestry or paper related title, \"Chain Saw Age & Power Equipment Trade\". To position the publication for expanded outdoor power equipment advertising opportunities, the company in January 1992, officially changed the publication's title to \"Power Equipment Trade\".\n\nIn 1995 Hatton-Brown purchased its first consumer publication, \"IronWorks\", a nine-times-per-year upscale magazine appealing to Harley-Davidson motorcycle enthusiasts. Dennis Stemp, founder of the magazine remained as editor until his death.\nHatton-Brown purchased another wood products magazine, \"Southern Lumberman\", in late summer of 1999. This publication has been published under this title since 1881, making it the oldest forestry related trade magazine in the country.\n\n"}
{"id": "41854940", "url": "https://en.wikipedia.org/wiki?curid=41854940", "title": "Hexanitratoaluminate", "text": "Hexanitratoaluminate\n\nHexanitratoaluminate is an anion of aluminium and six nitrate groups with formula [Al(NO)] that can form salts called hexanitratoaluminates.\n\nHexaperchloratoaluminate [Al(ClO)] has perchlorate groups instead of nitrate, and is similarly sensitive to water. Pentanitratoaluminates have five nitrate groups. Tetranitratoaluminates have only four nitrate groups.\n\nA known salt is potassium hexanitratoaluminate K[Al(NO)].\n\nTetramethylammonium hexanitratoaluminate can be formed from tetramethylammonium chloride, aluminium chloride and dinitrogen tetroxide diluted with nitromethane.\n\nRubidium hexanitratoaluminate also exists.\n"}
{"id": "1849366", "url": "https://en.wikipedia.org/wiki?curid=1849366", "title": "IEC 60870", "text": "IEC 60870\n\nIn electrical engineering and power system automation, the International Electrotechnical Commission 60870 standards define systems used for telecontrol (supervisory control and data acquisition). Such systems are used for controlling electric power transmission grids and other geographically widespread control systems. By use of standardized protocols, equipment from many different suppliers can be made to interoperate. IEC standard 60870 has six parts, defining general information related to the standard, operating conditions, electrical interfaces, performance requirements, and data transmission protocols. The 60870 standards are developed by IEC Technical Committee 57 (Working Group 03).\n\n\nIEC 60870 part 5, known as Transmission protocols, provides a communication profile for sending basic telecontrol messages between two systems, which uses permanent directly connected data circuits between the systems. The IEC TC 57 WG3 have developed a protocol standard for telecontrol, teleprotection, and associated telecommunications for electric power systems. The result of this work is IEC 60870-5. Five documents specify the base IEC 60870-5:\n\nThe IEC TC 57 has also generated companion standards:\n\nIEC 60870-5-101/102/103/104 are companion standards generated for basic telecontrol tasks, transmission of integrated totals, data exchange from protection equipment & network access of IEC101 respectively.\n\nIEC TC 57 WG3 also generated standards for telecontrol protocols compatible with ISO standards and ITU-T recommendations. These standards include:\n"}
{"id": "7463902", "url": "https://en.wikipedia.org/wiki?curid=7463902", "title": "Isobutylgermane", "text": "Isobutylgermane\n\nIsobutylgermane (IBGe, Chemical formula: (CH)CHCHGeH), is an organogermanium compound. It is a colourless, volatile liquid that is used in MOVPE (Metalorganic Vapor Phase Epitaxy) as an alternative to germane. IBGe is used in the deposition of Ge films and Ge-containing thin semiconductor films such as SiGe in strained silicon application, and GeSbTe in NAND Flash applications.\n\nIBGe is a non-pyrophoric liquid source for chemical vapor deposition (CVD) and atomic layer deposition (ALD) of semiconductors. It possesses very high vapor pressure and is considerably less hazardous than germane gas. IBGe also offers lower decomposition temperature (the onset of decomposition at ca. 325-350 °C)., coupled with advantages of low carbon incorporation and reduced main group elemental impurities in epitaxially grown germanium comprising layers such as Ge, SiGe, SiGeC, strained silicon, GeSb, and GeSbTe.\n\nRohm and Haas (now part of The Dow Chemical Company), IMEM, and CNRS have developed a process to grow germanium films on germanium at low temperatures in a Metalorganic Vapor Phase Epitaxy (MOVPE) reactor using isobutylgermane. The research targets Ge/III-V hetero devices. It has been demonstrated that the growth of high quality germanium films at temperatures as low as 350 °C can be achieved. The low growth temperature of 350 °C achievable with this new precursor has eliminated the memory effect of germanium in III-V materials. Recently IBGe is used to deposit Ge epitaxial films on a Si or Ge substrate, followed by the MOVPE deposition of InGaP and InGaAs layers with no memory effect, to enable triple-junction solar cells and integration of III-V compounds with Silicon and Germanium.\n\n\n"}
{"id": "29882763", "url": "https://en.wikipedia.org/wiki?curid=29882763", "title": "K factor (crude oil refining)", "text": "K factor (crude oil refining)\n\nThe K factor or characterization factor is a systematic way of classifying a crude oil according to its paraffinic, naphthenic, intermediate or aromatic nature. 12.5 or higher indicate a crude oil of predominantly paraffinic constituents, while 10 or lower indicate a crude of more aromatic nature. The K factor is also referred to as the UOP K factor or just UOPK.\n\n\n"}
{"id": "35562981", "url": "https://en.wikipedia.org/wiki?curid=35562981", "title": "Liander", "text": "Liander\n\nLiander is a Dutch utility company which operates in the distribution of electricity and natural gas in part of the Netherlands. Liander NV is the largest utility company in the Netherlands, managing the energy network in the provinces of Gelderland and Noord-Holland entirely, and in large parts of Flevoland, Friesland and Zuid-Holland.\n\nLiander NV was formerly known as Continuon, and is now a division of the umbrella-company Alliander. Alliander includes also Liandon (formerly Nuon Tecno), focused on building and maintenance of large energy infrastructures, and Lyandin (formerly Dynamicom) that operates in lighting of public spaces.\n\nLiander was split from the Nuon group in July 2008 and since 12 November 2008 it has operated under the new name Liander. Nuon continues to operate as a production and supply company, under the name Nuon Energy.\n"}
{"id": "21421435", "url": "https://en.wikipedia.org/wiki?curid=21421435", "title": "Limiting oxygen concentration", "text": "Limiting oxygen concentration\n\nThe limiting oxygen concentration, (LOC), also known as the minimum oxygen concentration, (MOC), is defined as the limiting concentration of oxygen below which combustion is not possible, independent of the concentration of fuel. It is expressed in units of volume percent of oxygen. The LOC varies with pressure and temperature. It is also dependent on the type of inert (non-flammable) gas.\n\nThe effect of increasing the concentration of inert gas can be understood by viewing the inert as thermal ballast that quenches the flame temperature to a level below which the flame cannot exist. Carbon dioxide is therefore more effective than nitrogen due to its higher molar heat capacity.\n\nThe concept has important practical use in fire safety engineering. For instance, to safely fill a new container or a pressure vessel with flammable gasses, the atmosphere of normal air (containing 20.9 volume percent of oxygen) in the vessel would first be flushed (purged) with nitrogen or another non-flammable inert gas, thereby reducing the oxygen concentration inside the container. When the oxygen concentration is below the LOC, flammable gas can then be safely admitted to the vessel, because the possibility of internal explosion has been eliminated.\n\nThe limiting oxygen concentration is a necessary parameter when designing hypoxic air fire prevention systems.\n\n\nMonographs\n\n"}
{"id": "58552730", "url": "https://en.wikipedia.org/wiki?curid=58552730", "title": "List of most powerful wind turbines", "text": "List of most powerful wind turbines\n\nThis is a list of the most powerful commercially deployed wind turbines on Earth, as well as prototype, demonstrator, or proposed wind turbines.\n"}
{"id": "9390167", "url": "https://en.wikipedia.org/wiki?curid=9390167", "title": "Lume", "text": "Lume\n\nLume is a short term for the luminous phosphorescent glowing solution applied on watch dials. There are some people who \"relume\" watches, or replace faded lume. Formerly, lume consisted mostly of radium; however, radium is radioactive and has been mostly replaced on new watches by less bright, but less toxic compounds.\n\nCommon pigments used in lume include the phosphorescent pigments zinc sulfide and strontium aluminate. Use of zinc sulfide for safety related products dates back to the 1930s. However, the development of strontium oxide aluminate, with a luminance approximately 10 times greater than zinc sulfide, has relegated most zinc sulfide based products to the novelty category. Strontium oxide aluminate based pigments are now used in exit signs, pathway marking, and other safety related signage.\n\nStrontium aluminate based afterglow pigments are marketed under brandnames like Super-LumiNova, Watchlume Co, NoctiLumina, and Glow in the Dark (Phosphorescent) Technologies.\n\n"}
{"id": "5849354", "url": "https://en.wikipedia.org/wiki?curid=5849354", "title": "MT Haven", "text": "MT Haven\n\nMT \"Haven, formerly Amoco Milford Haven\", was a VLCC (very large crude carrier), leased to Troodos Shipping (a company run by Lucas Haji-Ioannou and his son Stelios Haji-Ioannou). In 1991, while loaded with 144,000 tonnes (1 million barrels) of crude oil, the ship exploded, caught fire and sank off the coast of Genoa, Italy, killing six Cypriot crew and flooding the Mediterranean with up to 50,000 tonnes of crude oil. It broke in two and sank after burning for three days, and for the next 12 years the Mediterranean coast of Italy and France was polluted, especially around Genoa and southern France.\n\n\"Amoco Milford Haven\" was built by Astilleros Españoles S.A. in Cadiz, Spain, the sister ship of \"Amoco Cadiz\", which sank in 1978. Launched in 1973, she worked various routes shipping crude oil from the middle east gulf. In 1988 she was hit by a missile in the Persian Gulf during the Iran-Iraq War. Extensively refitted in Singapore, she was sold to ship brokers who leased her to Troodos Shipping, for whom she ran from Iran's Kharg Island to the Mediterranean.\n\nOn 11 April 1991, \"Haven\" was unloading a cargo of 230,000 tonnes of crude oil to the Multedo floating platform, off the coast of Genoa, Italy. Having transferred 80,000 tonnes, the ship disconnected from the platform for a routine internal transfer operation, to allow oil to be pumped from two side-holds into a central one.\n\nIn later testimony, First Officer Donatos Lilis said: \"I heard a very loud noise, like iron bars beating against each other. Perhaps the cover of a pump had broken. Then there was an awful explosion.\" Five crewmen died immediately, as fire broke out and oil started leaking from the hull as the plates overheated. As the fire engulfed the ship, flames rose 100m high and, after a series of further explosions occurred, between 30-40,000 tons of oil poured into the sea.\n\nThe Italian authorities acted quickly, with hundreds of men fighting a fire which was difficult to access, and distributing more than of inflatable barriers, submerged a metre below the surface, around the vessel to control the spillage. On day two, MT \"Haven\" was to be towed close to the coast, in a bid to reduce the coastal area affected and make intervention easier. As the bow slipped beneath the surface, a steel cable was passed around the rudder and tugs applied towing pressure. But it was quickly clear that the ship had broken its back, and the bow section came to rest in 450m of water. On 14 April, the 250m long main body sank from the coast, between Arenzano and Cogoleto\n\nAfter the wreck was declared safe, a mini sub diver found that the stern section had grazed a rocky spur, though not hard enough to open any new holes in the hull, and come to rest at an angle on the flat, sandy seabed. He reported that most of the remaining 80,000 tons of crude had burnt or was at the surface. Most of the oil on the surface could be sucked up, and what remained below was in a solid state. For the next 12 years the Mediterranean coast of Italy and France was polluted, especially around Genoa and southern France.\n\n\"Haven\" is the largest shipwreck in the sea and lies at a depth of 33 to 83m off the coast of Genoa. It is a popular tourist attraction with deep sea divers.\n\nAt the centre of the case was the allegation that Lucas and Stelios had kept their vessel, the Troodos-owned Cyprus-flagged \"Haven\", in such disrepair that she exploded. According to news items it is also alleged that the tanker was scrapped after being hit by an Exocet missile during the Iran–Iraq War and should not have been put back into operation. Prosecutors had asked for seven-year sentences for manslaughter against both father and son. Christos Dovles, former director of the shipping firm for whom prosecutors had sought a sentence of two years and four months.\n\nLucas and Stelios were later acquitted after three retrials (of which 2002 was the last) and much controversy, with subsequent appeals and demands for compensation also thrown out. Stelios was quoted after the trial: \"My main comment is to ask why it took so long to clear innocent people of these terrible charges.\"\n\nItaly's Environment Ministry under-secretary said he was \"greatly embittered\" by the verdict, saying, \"The victims, the relatives and the marine environment that were all seriously damaged are left without convincing answers.\"\n\nThe Italian president of the World Wildlife Fund, Grazia Francescato, said in a statement that she was disgusted with Stelios' conduct. She drew similarities with the \"Moby Prince\" disaster, an unrelated collision in which 140 people died on a ferry just off the nearby city of Livorno, and the acquittal of four men on charges of manslaughter.\n\nNUMAST, the union that represents merchant officers, described the acquittal as \"depressing\", a sentiment also expressed by the International Transport Workers' Federation. Only by making ship owners accountable for the state of vessels under their control would substandard ships be eliminated, Andrew Linington, head of communications at NUMAST said. \"Even when ship owners were clearly linked with a ship that did not meet acceptable standards it seems no action will be taken,\" Linington said.\n\n\n"}
{"id": "56776231", "url": "https://en.wikipedia.org/wiki?curid=56776231", "title": "March 6–8, 2018 nor'easter", "text": "March 6–8, 2018 nor'easter\n\nThe March 6–8, 2018 nor'easter was a powerful nor'easter that wreaked havoc on the Northeastern United States just days after another intense nor'easter struck the Mid-Atlantic, hampering recovery efforts from that storm. Forming near the Outer Banks late on March 6 as the end phase of a long-tracked winter storm across the country, it rapidly deepened off the Mid-Atlantic coast on March 7 and brought up to of heavy snow, whiteout conditions and even coastal flooding (though nowhere near the levels seen in the prior nor'easter) to those in the impact zone from the storm, many of which were still without power from the previous storm less than a week prior.\n\nThe storm caused up to 1 million people to lose power, and at least two people has been confirmed dead due to the storm as of March 7. Hundreds of flights were cancelled across the region, and many schools closed due to the nor'easter, although some opted to remain open such as those in New York City, causing controversy. Many freeways were also closed in the regions, and several states were put under state of emergencies.\n\nWhile the previous nor'easter hammered the Northeastern U.S. on March 2–3, another winter storm had begun setting up in the western part of the country, dumping very heavy snow in the Sierra Mountains. This storm moved into the Upper Midwest over the next few days and became an intense snowstorm for the affected areas. By early on March 6, the system had reached the Great Lakes, with the surface low beginning to weaken. As it did so, energy began to be transferred over to a new area of low pressure that was forming near the Outer Banks late that day. As the new low began to rapidly intensify, the Weather Prediction Center (WPC) began issuing storm summaries on the intensifying low off the coast of New Jersey early on March 7. A sharp transition of heavy snow to rain was observed near the Jersey Shore, due to warm air aloft pushing slightly inland, with the cold air residing just inland. Thundersnow and snowfall rates of up to an hour were reported in areas around the Tri-State Area, signaling the rapid intensification of the storm. Late in the afternoon, an eye-like feature was spotted near the center of the storm. The pressure bottomed out at around the same time.\n\nAfter peaking, the system began to gradually weaken as it continued moving northeastwards towards the Canadian Maritimes late on March 7–8, while heavy snowfall fell around the areas near Boston, Massachusetts. Turning northwestward slightly, moderate snowfall continued to occur over the coastal and interior sections of New England. The low continued to spin down throughout the day, with snow tapering off in much of the region except for the northern parts of New York and far northern New England. By 15:00 UTC the next day, the system had weakened to the point where the WPC terminated storm summaries on the winter storm. It dissipated shortly afterwards.\n\nAmtrak operated a modified schedule along the Northeast Corridor on March 7 due to the storm. About 2,500 flights were cancelled and several schools were closed in the Northeast United States on March 7 because of the upcoming storm.\n\nThe storm caused coastal flooding along the Outer Banks, with North Carolina Highway 12 closed between Rodanthe and the Herbert C. Bonner Bridge.\n\nSeveral schools in Maryland were closed due to snow. The storm caused flooding along some streets in Annapolis. Flooding from high tides was also seen in some communities along the Chesapeake Bay including Cambridge, Crisfield, and Deal Island. In Delaware, the Delaware Memorial Bridge was temporarily closed due to several tractor-trailers being disabled. It was re-opened a few hours later after the vehicles were removed. The Delaware General Assembly cancelled its session for March 7 while the University of Delaware closed at noon on March 7.\n\nIn Pennsylvania, Governor Tom Wolf declared a state of emergency for several counties in the eastern part of the state. A snow emergency went into effect for the city of Philadelphia on the morning of March 7. Several municipalities in the Philadelphia area declared snow emergencies and many schools and government offices were closed on March 7. Many attractions in the Philadelphia area either closed early or were closed for the entire day on March 7. SEPTA modified their service plan for March 7 due to the snow, with Regional Rail trains running on a modified Saturday schedule and Broad Street Line and Market-Frankford Line trains running during the overnight hours. In addition, several SEPTA bus routes were placed on detours in advance of the storm. On the afternoon of March 7, SEPTA suspended most of their bus services due to the snow. At the Philadelphia International Airport, a ground stop was issued in place. The Pennsylvania Department of Transportation implemented speed restrictions of on several freeways in the southeastern part of the state while the Pennsylvania Turnpike Commission reduced the speed limit to on portions of the Pennsylvania Turnpike and the Northeast Extension (Interstate 476) in the Philadelphia area. High-profile vehicles such as empty trucks, motorcycles, and recreational vehicles were banned along several Interstate Highways in eastern Pennsylvania due to the storm.\n\nIn New Jersey, Governor Phil Murphy declared a state of emergency and state offices were closed on March 7. New Jersey Transit suspended bus service statewide on the afternoon of March 7, with trains running an abbreviated schedule. More than 322,000 customers in the state were without power, including people who had lost power from a prior storm on March 2. Newark Liberty International Airport was closed briefly during the afternoon of March 7, with limited service soon resuming. The storm forced the closure of the Burlington-Bristol Bridge over the Delaware River. Thundersnow was seen in the state, with lightning striking a teacher in Manchester Township, who suffered injuries and was in stable condition. Snowmobiles had to be sent out to rescue travelers on Interstate 280 and Interstate 78 who had become stuck in the snow during the storm, some who had been trapped for over five hours. About 500 vehicles were reported to have gotten stuck during the incident.\n\nIn New York City, the exceptionally heavy snow made travel difficult, with ferry service suspended. Central Park reported 3.2 inches, while JFK International Airport reported only 2.8 inches, which was much less than many of the forecasts, which suggested that 1-2 feet of snow would fall. However, there was a very sharp gradient in snow totals, with Franklin Lakes, NJ, just under 20 miles away from Central Park, reporting a total of 24.8 inches. In North White Plains, 10 people were taken to the hospital with carbon monoxide poisoning from a generator running inside their home. Governor Andrew Cuomo announced a travel ban for tractor-trailers along the New York State Thruway between New York City and Syracuse. Classes were cancelled until March 9 at Binghamton University due to the storm. Many schools in the Lower Hudson Valley closed for the fifth time in a row, eliminating most of the districts’ spring breaks (from Friday, March 2 to Thursday, March 8). Putnam County issued a State Of Emergency, closing all school districts in the county on Thursday, March 8.\nIn Connecticut, Governor Dannel Malloy sent nonessential state employees home early on March 7 ahead of the anticipated heavy snow. At the height of the storm, over 160,000 residents were without electricity. New Fairfield had the greatest accumulation in the state with of snow.\n\nIn Boston, several shuttle bus routes were cancelled for the rest of the week due to the storm. Crews worked to secure a seawall in Duxbury that was damaged by a prior storm on March 2. After the storm had passed, on March 8, a commuter train slid off the tracks in Wilmington after it struck a tree that was lodged into the tracks. No one was injured in the incident.\n\nThe Harvard Sailing Center in Cambridge, Massachusetts partially sank into the Charles River during the nor'easter. The building was compromised due to a failure of the flotation device used to keep the structure afloat.\n\n"}
{"id": "3102609", "url": "https://en.wikipedia.org/wiki?curid=3102609", "title": "Mean effective pressure", "text": "Mean effective pressure\n\nThe mean effective pressure is a quantity relating to the operation of a reciprocating engine and is a valuable measure of an engine's capacity to do work that is independent of engine displacement. When quoted as an \"indicated\" mean effective pressure or IMEP (defined below), it may be thought of as the average pressure acting on a piston during the different portions of its cycle.\n\nLet:\n\nThe power produced by the engine is equal to the work done per operating cycle times the number of operating cycles per second. If \"N\" is the number of revolutions per second, and formula_5 is the number of revolutions per power stroke, the number of power strokes per second is just their ratio. We can write\nReordering to put work on the left:\n\nBy definition:\nso that\n\nSince the torque \"T\" is related to the angular speed (which is just \"N\" 2 \"π\") and power produced by\n\nThen the equation for mep in terms of torque becomes,\n\nNotice that speed has dropped out of the equation and the only variables are the torque and displacement volume. Since the range of maximum brake mean effective pressures for good engine designs is well established, we now have a displacement-independent measure of the torque-producing capacity of an engine design (a specific torque of sorts). This is useful for comparing engines of different displacements. Mean effective pressure is also useful for initial design calculations; that is, given a torque, standard MEP values can be used to estimate the required engine displacement. However, it is important to remember that mean effective pressure does not reflect the actual pressures inside an individual combustion chamber—although the two are certainly related—and serves only as a convenient measure of performance.\n\nBrake mean effective pressure (BMEP) is calculated from measured dynamometer torque. Net indicated mean effective pressure (IMEP) is calculated using the indicated power; i.e., the pressure volume integral in the work per cycle equation. Sometimes the term FMEP (friction mean effective pressure) is used as an indicator of the mean effective pressure lost to friction (or friction torque) and is just the difference between IMEP and BMEP.\n\nMean effective pressure (MEP) is defined by the location measurement and method of calculation, some commonly used MEPs are given here.\n\n\nPetrol Engines :\n\nDiesel Engines :\n\nFor example, a four-stroke motor producing 160 N·m from 2 litres of displacement has a bmep of (4π)(160 N·m)/(0.002 m³) = 1,005,000 N/m =1,005 kPa (10.05 bar). If the same engine produces 76 kW at 5400 rpm (90 Hz), its torque is 134 N·m and its bmep is 8.42 bar (842 kPa). As piston engines usually have their maximum torque at a lower rotating speed than the maximum power output, the BMEP is lower at full power (at higher rotating speed). The exception is with engines with sharp artificial torque limiter past certain RPM, as in 320 N·m, 90 kW RZ4E for trucks, with calculated BMEP of 21.16 bar from 1600 to 2800 rpm, peak power at 2800 rpm.\n\n\n\n"}
{"id": "32393699", "url": "https://en.wikipedia.org/wiki?curid=32393699", "title": "Mechanically stimulated gas emission", "text": "Mechanically stimulated gas emission\n\nMechanically Stimulated Gas Emission\n\nMechanically stimulated gas emission (MSGE) is a complex phenomenon embracing various physical and chemical processes occurring on the surface and in the bulk of a solid under applied mechanical stress and resulting in emission of gases. MSGE is a part of a more general phenomenon of Mechanically Stimulated Neutral Emission (MSNE). The specific characteristics of MSGE as compared with MSNE is that the emitted neutral particles are limited to gas molecules. MSGE is opposite to Mechanically Stimulated Gas Absorption that usually occurs under fretting corrosion of metals, exposure to gases at high pressures, etc. \nThere are three main sources of MSGE: \nGenerally, for producing MSGE, the mechanical action on the solid can be of any type including tension, compression, torsion, shearing, rubbing, fretting, rolling, indentation, etc.\nIn previous studies carried out by various groups it was found that MSGE is associated mainly with plastic deformation, fracture, wear and other irreversible modifications of a solid. Under elastic deformation MSGE is almost negligible and only was observed just below elastic limit due to possible microplastic deformation. \nIn accordance to the main sources, the emitted gases usually contain hydrogen (source type IIa), argon (for coatings obtained using PVD in Ar plasma - source type IIb), methane (source type III), water (source type I and/or III), carbon mono- and dioxide (source type I/III). \nThe knowledge on the mechanisms of MSGE is still vague. On the basis of the experimental findings it was speculated that the following processes can be related with MSGE:\nThermal effect seems to be irrelevant to the gas emission under light load conditions.\n\nEmerging character of this interdisciplinar branch of science is reflected by a lack of established terminology. There are different terms and definitions used by different authors depending on the main approach used (chemical, physical, mechanical, vacuum science, etc.), specific gas emission mechanism (desorption, emanation, emission, etc.) and type of mechanical activation (friction, traction, etc.):\nDesorption (tribodesorption, fractodesorption, etc.) refers to release of gases dissolved in the bulk and adsorbed on the surface. Therefore, desortpion is only one of the contributing processes to MSGE. Outgassing is a technical term usually utilized in vacuum science. Thus, term \"gas emission\" embraces various processes, reflects the physical nature of this complex phenomenon and is preferable for use in scientific publications.\n\nDue to low emission rate experiments should be performed in ultrahigh vacuum (UHV). In some studies the materials were previously doped with tritium. MSGE rate then was measured by radioactivity outcome from the material under applied mechanical stress.\n\nMechanochemistry\n"}
{"id": "4561538", "url": "https://en.wikipedia.org/wiki?curid=4561538", "title": "Mycolic acid", "text": "Mycolic acid\n\nMycolic acids are long fatty acids found in the cell walls of the Mycolata taxon, a group of bacteria that includes \"Mycobacterium tuberculosis\", the causative agent of the disease tuberculosis. They form the major component of the cell wall of mycolata species. Despite their name, mycolic acids have no biological link to fungi; the name arises from the filamentous appearance their presence gives mycolata under high magnification. The presence of mycolic acids in the cell wall also gives mycolata a distinct gross morphological trait known as \"cording\". Mycolic acids were first isolated by Stodola \"et al.\" in 1938 from an extract of \"M. tuberculosis\".\n\nMycolic acids are composed of a longer beta-hydroxy chain with a shorter alpha-alkyl side chain. Each molecule contains between 60 and 90 carbon atoms. The exact number of carbons varies by species and can be used as an identification aid. Most mycolic acids also contain various functional groups.\n\n\"M. tuberculosis\" produces three main types of mycolic acids: alpha-, methoxy-, and keto-. Alpha-mycolic acids make up at least 70% of the mycolic acids of the organism and contain several cyclopropane rings. Methoxy-mycolic acids, which contain several methoxy groups, constitute between 10% and 15% of the mycolic acids in the organism. The remaining 10% to 15% of the mycolic acids are keto-mycolic acids, which contain several ketone groups.\n\nMycolic acids impart \"M. tuberculosis\" with unique properties that defy medical treatment. They make the organism more resistant to chemical damage and dehydration, and limit the effectiveness of hydrophilic antibiotics and biocides. Mycolic acids also allow the bacterium to grow inside macrophages, effectively hiding it from the host immune system. Mycolate biosynthesis is crucial for survival and pathogenesis of \"M. tuberculosis\". The pathway and enzymes have been elucidated and reported in detail. Five distinct stages are involved. These were summarised as follows:\n\nThe fatty acid synthase-I and fatty acid synthase-II pathways producing mycolic acids are linked by the beta-ketoacyl-(acyl-carrier-protein) synthase III enzyme, often designated as mtFabH. Novel inhibitors of this enzyme could potentially be used as therapeutic agents.\n\nThe mycolic acids show interesting inflammation controlling properties. A clear tolerogenic response was promoted by natural mycolic acids in experimental asthma. The natural extracts are however chemically heterogeneous and inflammatory. By organic synthesis, the different homologues from the natural mixture could be obtained in pure form and tested for biological activity. One subclass proved to be a very good suppressor of asthma, through a totally new mode of action. These compounds are now under further investigation. A second subclass triggered a cellular immune response (Th1 and Th17), so studies are ongoing to use this subclass as an adjuvant for vaccination.\n\nThe exact structure of mycolic acids appears to be closely linked to the virulence of the organism, as modification of the functional groups of the molecule can lead to an attenuation of growth \"in vivo\". Further, individuals with mutations in genes responsible for mycolic acid synthesis exhibit altered cording.\n\nAn international multi-centre study has proved that delamanid (OPC-67683), a new agent derived from the nitro-dihydro-imidazooxazole class of compounds that inhibits mycolic acid synthesis, can increase the rate of sputum culture conversion in multi-drug-resistant tuberculosis (MDRTB) at 2 months.\n\nThe mycolic acids of members of the genus \"Rhodococcus\", another member of the mycolata taxon, differ in several ways from those of \"M. tuberculosis\". They contain no functional groups, but instead may have several unsaturated bonds. Two different profiles of \"Rhodococcus\" mycolic acids exist. The first has between 28 and 46 carbon atoms with either 0 or 1 unsaturated bonds. The second has between 34 and 54 carbon atoms with between 0 and 4 unsaturated bonds. Sutcliffe (1998) has proposed that they are linked to the rest of the cell wall by arabinogalactan molecules.\n\n"}
{"id": "26039959", "url": "https://en.wikipedia.org/wiki?curid=26039959", "title": "Myitsone Dam", "text": "Myitsone Dam\n\nThe Myitsone Dam ( ; the Confluence Dam) is a large dam and hydroelectric power development project which was planned to be built in Myanmar (aka Burma). The proposed construction site is at the confluence of the Mali and N’mai rivers and the source of the Irawaddy River (Ayeyawady River) in northern Burma. The project is (as of 2017) suspended.\n\nIf the project had been completed according to plans in 2017 it would have been the fifteenth largest hydroelectric power station in the world. The dam, planned to be long and high, was to be built by the Upstream Ayeyawady Confluence Basin Hydropower Company. The company is a joint venture between the China Power Investment Corporation (CPI), the Burmese Government’s Ministry of Electric Power and the Asia World Company It had been expected to provide 6,000 megawatts of electricity primarily for export to Yunnan, China. CPI contended that China would not be the electricity's primary market and stated that Myanmar would have first claim on the electricity generated, with the remainder sold for export. Opponents remained skeptical because most Burmese are not connected to the electrical grid, and doubted whether the dam would improve their livelihood.\n\nThe dam project has been controversial in Burma due to its enormous flooding area, environmental impacts, location 60 miles from the Sagaing fault line, and uneven share of electricity output between the two countries. The Burmese public regards the Irrawaddy River as the birthplace of Burmese civilization and although the Chinese market guarantees the dam’s electricity sales, to many Burmese Myitsone represents growing Chinese influence in Burma, which they perceive as \"exploitative\" to the country hitherto isolated by Western economic sanctions. Even government officials have differing opinions on the project.\n\nOn 30 September 2011, amid democratic reforms in the country, President Thein Sein announced that the Myitsone dam project was to be suspended during his tenure. Because the government appears to have taken public opinion into account, the unexpected decision is seen as a reversal to the authoritarian rule since the coup in 1962.\n\nThe dam site is located below the confluence of the Mali River and the N'Mai River about north of Myitkyina, the capital of Kachin State, in northern Burma. The source of both the N'mai and Mali Rivers is the Himalaya glaciers of northern Burma in the vicinity of 28° N. The easternmost of the two, N'mai river, is the larger stream and rises in the Languela Glacier north of Putao. It is unnavigable because of the strong current whereas the smaller western river, the Mali, is navigable, despite a few rapids.\n\nThe project location is in the politically unstable Kachin State. Since 1962, the Kachin Independence Army (KIA) has been waging war against Burmese military. Despite ceasefire in 1994, clashes and bomb explosions occasionally occur near the dam site. In 2011, clashes between Burmese military and KIA intensified and the Burmese military ordered airstrikes in Northern Kachin State.\n\nThe Myitsone Dam is part of the Confluence Region Hydropower Project (CRHP), which includes seven dams with a total installed capacity of 20,000 MW. CRHP alone accounts for 41 percent of the total power capacity called for by a 30-year strategic plan. Outlined in 2001, the plan includes 64 hydropower plants and three coal power plants with combined installed capacity of more than .\n\nThe Myanmar Electrical Power Enterprise and the Agriculture and Irrigation Ministry scheduled the \"Irawaddy Myitsone Dam Multipurpose Water Utilization Project\" in 2001. The survey phase was initiated in 2003. First the government contracted the Japanese Kansai Electric Power Company to build a small weather station at Tang Hpre village, near the confluence. Chinese and Burmese contractors, including Yunnan Machinery Equipment Import & Export Company (YMEC), Kunming Hydropower Institute of Design, surveyed the dam site. In 2006, Suntac Technologies Co. Ltd., a Burmese Geographic Information System (GIS) mapping contractor set up an office at the monastery in Tang Hpre village. They also set up a temporary camp at Washawng village to facilitate transport of survey equipment from the YMEC company in China. In October, the Asia World Company built a project implementation camp on a hill top at the dam site downstream from the confluence. when the camp was complete, Chinese inspectors stayed and surveyed the area for five months. In December 2006, the Ministry of Electric Power No. 1 and the China Power Investment Corporation signed a Memorandum of Understanding for a project at Myitsone and a project at Chibwe.\n\nThe design phase began in 2007. The Changjiang Design Institute of China sent several groups of design personnel and conducted geological drilling, reservoir inspection and hydrological measuring near the dam site. To supply electricity for dam construction projects, a small , Chibwe Nge hydropower project was built in April 2007. In May, the \"New Light of Myanmar\" reported that the Ministry of Power No. 1 and CPI would build seven hydropower dams on the N'Mai and Irawaddy rivers.\n\nOn 16 June 2009, Myanmar Ambassador Thein Lwin and President of China Power Investment Corporation Lu Qizhou signed a Memorandum of Agreement between the Department of Hydropower Implementation and the China Power Investment Corporation for the Development, Operation and Transfer of the hydropower Projects in Maykha, Malikha and Upstream of Irrawaddy-Myitsone River Basins. The official opening ceremony of the dam construction phase was held on December 21.\n\nThe majority of total US$3.6 billion cost was to be covered by the China Power Investment Corporation in a joint venture with the Ministry of Electric Power No.1 of Myanmar and the Asia World Company. The Burmese government would get ten percent of the electricity generated and fifteen percent of the project shares for land use. In addition, the government would charge a withholding tax and an export tax on exported electricity to China. After a fifty-year period, the government would totally own the project. The Burmese government would earn about $54 billion by means of tax payment, power and shares, accounting for 60 percent of the total revenue of the Irrawaddy projects during the contracted 50 years, more than CPI's return on investment during the fifty years Chinese operation period according to the President of CPI. However, the government economic calculations have been criticized for not considering potential environmental and societal impacts.\n\nThe dam was planned to be a concrete faced rock-fill dam high and long, and projected to produce 6,000 MW of electricity by 2017. This is equivalent to 27% of the output of the Three Gorges Dam In China, the world's largest electricity-generating plant of any kind.\n\nMinister of Electric Power Zaw Min claimed that the dam was designed to withstand an earthquake of 8.0 Richter Scale, a scale that has never been recorded to have occurred in that region, and the most devastating flood of a millennium.\n\nThe Myitsone Dam is the largest of the seven large dams currently planned on the Mali, N'Mai and Irawaddy rivers. The China Power Investment Corporation is project manager of the Confluence Region Hydropower Projects. The seven dams combined total design installed capacity is 20,000 MW of electricity.\n\nThe dam is to provide electricity primarily to the China Southern Power Grid via its subsidiary, the Yunnan Power Grid Company, in Yunnan Province and then on to the power hungry eastern coastal areas of China, in conformity with the Chinese central Government's 'West to East Transmission Policy'. The hydropower project was being implemented under an agreement signed in late 2006 with the state-owned China Power Investment Corporation and Burma’s Ministry of Electric Power No 1. The dam and reservoir planning and construction is managed by the Burmese government in cooperation with the China Southern Power Grid and several subcontractors.\n\nThe dam would also supply ten percent of its generated electricity to the Myanmar Power Grid if needed. The Chipwi Nge Hydropower Project, which was installed to provide electricity for construction projects, began supplying electricity to Myitkyina, Chipwi, and the Myitsone Resettlement Village. However, few villagers have electrical devices. \"We don’t need to buy candles, this is the only useful thing\" a villager told. They prefer to have their productive land back.\n\nThe dam was expected to flood including 47 villages near the construction site and about 11800 local people would be relocated in the newly built resettlement villages. The activists in exile stated that the dam would submerge historical temples, churches and cultural heritage sites important to Kachin identity and history and the natural heritage of the Kachin people in Myitsone area would be lost.\n\nIn response, the government reported in the state-run \"New Light of Myanmar\" that relocated villages from the project area had been provided with all forms of aids including water, electric power and buildings and that the government also helped in relocation of religious buildings. CPI reported a total expense of 4.1 billion kyats in compensation and 25 million U.S. dollars in resettlement. In addition, the government stated that the remote region would benefit greatly from the new roads and access to electricity.\n\nLocal communities pointed out other issues such as the dam's location on earthquake-prone zone. They opposed the dam site because it is less than from the major Sagaing fault line, posing a risk to basin inhabitants if an earthquake weakened the dam or caused landslides in the reservoir. If the Irrawaddy Myitsone Dam broke during an earthquake, it would endanger the lives of hundred of thousands of people downstream in Kachin State’s largest city, Myikyina.\n\nIn response, the government said that the dam would withstand an earthquake of magnitude 8.0 Richter scale. In an interview with Xinhua News, Lu Qizhou, President of China Power Investment Corporation said that Myitsone Hydropower Station follows the standard of fortification intensity 9, two points above the intensity of Zipingbu Hydropower Station that withstood 2008 Sichuan earthquake of 8.0 Richter scale. On the other hand, Burmese scientists who carried out the environmental assessment, recommended to build two smaller dams farther upstream instead of building on earthquake-prone zone.\n\nAs with other large dam projects, the Myitsone Dam construction would have altered the hydrological characteristics of the river and prevent sediment from enriching the historically highly productive agricultural floodplains downstream. This can affect fertility as far downstream as the Irrawaddy Delta, the major rice-producing area of Myanmar. The government responded that officials had taken sediment accumulation into consideration, and that the Myitsone dam would have capabilities to discharge the accumulated sediment.\n\nOther consequences of flooding by the reservoir include loss of farmland and loss of spawning habitat as some migratory fish will be able to swim upstream after dam completion. This would lead to lost income for fishermen according to Kachin environmental activist groups in exile. On the other hand, research by Biodiversity And Nature Conservation Association (BANCA) contradicts the activists' statement. BANCA's research asserts that commercial fishing is not reported from the project area. But it confirms that some resident aquatic species will be affected by the change in hydrological conditions.\n\nEcological concerns focus on the inundation of an area that is the border of the Indo-Burma and South Central China biodiversity hotspot. The Mali and N'mai River confluence region falls within the Mizoram-Manipur-Kachin rainforests. Nevertheless, the dam would increase the shipping capacity of Irrawaddy River. The floods, currently an annual occurrence, would only occur once every twenty years. Since the hydropower is the renewable and green energy, the energy produced by the dam would decrease the need for air-polluting sources of energy such as fossil fuels. The construction and the maintenance of the dam would have employed a large number of people.\n\nIn Burma, the Irrawaddy river, on the bank of which major historic cities such as Bagan were built, is considered as the birthplace of Burmese civilization. For that reason, The Burmese public protests against the dam project, which would inevitably alter hydrological characteristics of the historic river. Moreover, the growing Chinese influence in Burma is seen as exploitative by Burmese people, due to its association with previous military junta.\n\nOn the other hand, local villages have opposed the project since its proposal in 2001. According to the environmental organization \"International Rivers\" based in the US, in 2007, 12 local leaders from Kachin State sent a letter to Senior General Than Shwe and the junta’s Ministry of Electric Power, asking for the project to be cancelled. The Burma Rivers Network also sent a letter to the Chinese government asking Chinese companies operating in Burma to conduct environmental and social impact assessments, to release information publicly, and to consider opinions of affected communities in the decision-making process. In a parliamentary session, government minister Zaw Min responded that environmental impact had been carried out by the Biodiversity And Nature Conservation Association (BANCA) at a cost of $1.25 million and stated that Myitsone Dam Project was started only when the government had studied the possible environmental impacts. However, the results of BANCA's assessment were not disclosed publicly.\n\nOutside Burma, activists in exile have actively carried out protests in various countries. In February 2010, the UK-based Kachin National Organization (KNO) protested against construction of the dam in front of the Burmese Embassies in the UK, Japan, Australia, and the United States. Leaked United States diplomatic cables by Wikileak revealed that U.S. embassy in Yangon funded some of the activist groups.\n\nWithin the country an activist has been detained for what local authorities said were unrelated charges. Land rights activist and politician Daw Bauk Ja was arrested for medical negligence in 2013, though the case against her had been withdrawn years earlier.\n\nSkirmishes have been broken out between Kachin Independence Army and Burmese military over the dam issue since June 2011. On April 17, 2010, three bombs exploded close to the site of dam, reportedly killing four Chinese workers. The Burmese government blamed Kachin Independence Army for planting the bombs. KIA has denied having anything to do with the Myitsone bombing.\n\nDomestic campaigns against the project are brought together by political activists including Nobel peace prize laureate Aung San Suu Kyi who called for reevaluation, conservationists, scholars, poets and journalists. Local media openly criticized the lack of transparency in dam project. Journalists argued that the deal was agreed by previous government without considering public opinions.\n\nDespite the opposition, Minister Zaw Min for the Ministry of Electric Power responded to the press that the government would continue this project up to the completion. But, in September 2011 he conceded that despite promised benefits from the project, it may be appropriate to minimize the environmental impacts by redesigning the tunnel, shortening the dam's height and reducing the water storage capacity, relocating to the upstream, and reassessing environmental impacts. He also agreed that final decision should depend on the environment impact assessment report by the Ministry of Environmental Conservation and Forestry and the study by the Engineer Group.\n\nOn 30 September 2011, in an address to the parliament, Burma's president Thein Sein announced that the Myitsone Dam project would be halted during the term of his government. The letter from the president consisted of ten points including,\n\nThe decision was universally acclaimed by environmentalists, political activists and the locals alike. It is considered as \"a rare reversal\" in that for the first time, the government had listened to the people in face of public opposition. Western nations including EU and the United States welcome the President's decision.\n\nIn response, Chinese Foreign Ministry spokesman Hong Lei stated that the Chinese government has urged the Burmese government to protect the legal and legitimate rights and interests of Chinese companies. He reminded that the Myitsone Dam is a jointly invested project between China and Myanmar, and one that has been thoroughly examined by both sides. He also confirmed that the matters would be resolved through friendly consultation. The president of CPI, Lu Qizhou, has warned that a halt in construction could lead to legal action.\n\nIn October 2011, Myanmar Foreign Minister traveled to Beijing to settle the dispute.\n\nOn 2 April 2012, Weekly Eleven news, a private Burmese news journal, broke news of ongoing activities by CPI and Asia World on the Myitsone project site. In April 2012, a leaked government document stated that work on the dam has continued, with work to officially resume within 6 months. The letter is a request for the Kachin State government to provide temporary ID cards for 500 engineers and the tax-free import of 10,000 tons of construction equipment (cement, trucks, bulldozers, excavators). The Kachin State government responded that the letter is a forgery. However, uncertainties exist as Chinese workers remain present in the area.\nIn March 2012, villagers who had been evicted from the dam work site in 2009 to 2010 and had returned to reclaim their original homes, were forcibly evicted by the Burmese army. The TangHpre villagers demonstrated again when the Chinese Diplomat and Myanmar governance Organization met to restart the Myitsone Dam Project in Palm Spring Hotel on 4th July, 2016 \n\nAlthough the President Thein Sein's decision has been widely applauded, experts caution that Sino-Burma relation could be ultimately harmed by the suspension. Nicholas Farrelly, a Southeast Asia specialist at the Australian National University in Canberra said that while there would inevitably be some short-term damage to bilateral relations, pragmatism would override any potential for long-term animosity. The issue is further exacerbated by the massacre of 13 Chinese sailors near Burma-Thailand border. However, should the project be terminated entirely, the Burmese government would face legal and financial liability associated with the investments and prior agreements.\n\nInternationally, the suspension is considered as one of the democratic reforms along with other engagements such as dialogues with pro-democracy leader Aung San Suu Kyi and release of some political prisoners. Marie Lall, a BBC South Asia analyst attributes Burma's bid for the ASEAN chair in 2014, the needs to reform the economy for ASEAN Free Trade Area and the government's desire to win the election in 2015 as the main motives of the reforms.\n\n"}
{"id": "10922431", "url": "https://en.wikipedia.org/wiki?curid=10922431", "title": "Renaissance Wax", "text": "Renaissance Wax\n\nRenaissance Wax is a brand of microcrystalline wax polish that is encountered in antique restoration and museum conservation. It is not appropriate for all materials, and is most safely used on metal objects. However, it is also used for the polish and conservation of gemstones and of organic materials such as wood, ivory, and tortoiseshell. Renaissance Wax is sometimes used by reenactors of historic swordsmanship to protect armor and weapons. Waxes are more protective and longer-lasting than oil, especially for swords and helmets that are frequently touched by human hands. It has recently been introduced in the world of guitar building, as a finish that protects and gives colour to the wood. \n\nWax coatings for conservation are most widely, and least controversially, applied to metals. This has several objectives: to produce a barrier that excludes moisture and oxygen from the metal surface, to preclude the introduction of contaminating elements by handling, and to provide a protective layer over anti-corrosion undercoatings. \n\nMicrocrystalline waxes used on ethnographic metal objects are discouraged, as they may require extensive treatment for removal.\n\nThe wax is evenly and lightly applied over the surface, then lightly buffed with a smooth lint-free cloth to give a sheen. Where the shape of the item requires, a brush may be used instead. It is also used in jewelry making to preserve the patina of the metal. \n\nFor retarding further red rot in leather bookbindings, it is common to first consolidate the leather by application of Klucel G or a similar material and then apply a protective coating of a microcrystalline wax. When Renaissance Wax is used for this protective coating, it can create a white residue if applied too heavily. This white residue cannot be removed. Use of a different wax, such as SC6000, is recommended for leather. \n\nRenaissance Wax is also commonly used in the preservation of bronze and copper coins. The wax seals the coins and helps prevent deterioration from moisture and air exposure. It may also help prevent the onset of the chloride-related corrosion commonly called bronze disease, although it won't arrest this once started.\n\nConservation of metals may also involve the application of an undercoat such as Incralac followed by the application of Renaissance Wax.\n\nRenaissance Wax was developed in the British Museum Research Laboratory by Dr A E A Werner in the late 1950s. It is now manufactured by Picreator Enterprises Ltd.\n\nEarlier wax polishes based on beeswax and carnauba wax either contained acids or became acidic over time. Renaissance Wax is based on more stable microcrystalline waxes refined from crude oil.\n\nRenaissance Wax contains polyethylene waxes. Some other microcrystalline waxes intended for conservation use do not contain these.\n\nWax coatings, in general, may accumulate dust and lint. \n\nIn one example where a Benin bust made from a copper-iron alloy had been coated with multiple materials including this wax, the polyethylene component required a higher-temperature solvent for removal than the rest of the wax.\n\n"}
{"id": "27772", "url": "https://en.wikipedia.org/wiki?curid=27772", "title": "Sandstone", "text": "Sandstone\n\nSandstone is a clastic sedimentary rock composed mainly of sand-sized (0.0625 to 2 mm) mineral particles or rock fragments.\n\nMost sandstone is composed of quartz or feldspar (both silicates) because they are the most resistant minerals to weathering processes at the Earth's surface, as seen in Bowen's reaction series. Like uncemented sand, sandstone may be any color due to impurities within the minerals, but the most common colors are tan, brown, yellow, red, grey, pink, white, and black. Since sandstone beds often form highly visible cliffs and other topographic features, certain colors of sandstone have been strongly identified with certain regions.\n\nRock formations that are primarily composed of sandstone usually allow the percolation of water and other fluids and are porous enough to store large quantities, making them valuable aquifers and petroleum reservoirs. Fine-grained aquifers, such as sandstones, are better able to filter out pollutants from the surface than are rocks with cracks and crevices, such as limestone or other rocks fractured by seismic activity.\n\nQuartz-bearing sandstone can be changed into quartzite through metamorphism, usually related to tectonic compression within orogenic belts.\n\nSandstones are \"clastic\" in origin (as opposed to either \"organic\", like chalk and coal, or \"chemical\", like gypsum and jasper).\nThey are formed from cemented grains that may either be fragments of a pre-existing rock or be mono-minerallic crystals. The cements binding these grains together are typically calcite, clays, and silica. Grain sizes in sands are defined (in geology) within the range of 0.0625 mm to 2 mm (0.0025–0.08 inches). Clays and sediments with smaller grain sizes not visible with the naked eye, including siltstones and shales, are typically called \"argillaceous\" sediments; rocks with larger grain sizes, including breccias and conglomerates, are termed \"rudaceous\" sediments.\nThe formation of sandstone involves two principal stages. First, a layer or layers of sand accumulates as the result of sedimentation, either from water (as in a stream, lake, or sea) or from air (as in a desert). Typically, sedimentation occurs by the sand settling out from suspension; i.e., ceasing to be rolled or bounced along the bottom of a body of water or ground surface (e.g., in a desert or erg). Finally, once it has accumulated, the sand becomes sandstone when it is compacted by the pressure of overlying deposits and cemented by the precipitation of minerals within the pore spaces between sand grains.\n\nThe most common cementing materials are silica and calcium carbonate, which are often derived either from dissolution or from alteration of the sand after it was buried. Colors will usually be tan or yellow (from a blend of the clear quartz with the dark amber feldspar content of the sand). A predominant additional colourant in the southwestern United States is iron oxide, which imparts reddish tints ranging from pink to dark red (terracotta), with additional manganese imparting a purplish hue. Red sandstones are also seen in the Southwest and West of Britain, as well as central Europe and Mongolia. The regularity of the latter favours use as a source for masonry, either as a primary building material or as a facing stone, over other forms of construction.\n\nThe environment where it is deposited is crucial in determining the characteristics of the resulting sandstone, which, in finer detail, include its grain size, sorting, and composition and, in more general detail, include the rock geometry and sedimentary structures. Principal environments of deposition may be split between terrestrial and marine, as illustrated by the following broad groupings:\n\nFramework grains are sand-sized ( diameter) detrital fragments that make up the bulk of a sandstone. These grains can be classified into several different categories based on their mineral composition:\n\n\nMatrix is very fine material, which is present within interstitial pore space between the framework grains. The interstitial pore space can be classified into two varieties. One is to call the sandstone an arenite, and the other is to call it a wacke. Below is a definition of the differences between the two matrices:\n\nCement is what binds the siliciclastic framework grains together. Cement is a secondary mineral that forms after deposition and during burial of the sandstone. These cementing materials may be either silicate minerals or non-silicate minerals, such as calcite.\n\nPore space includes the open spaces within a rock or a soil. The pore space in a rock has a direct relationship to the porosity and permeability of the rock. The porosity and permeability are directly influenced by the way the sand grains are packed together.\n\nAll sandstones are composed of the same general minerals. These minerals make up the framework components of the sandstones. Such components are quartz, feldspars, and lithic fragments. Matrix may also be present in the interstitial spaces between the framework grains. Below is a list of several major groups of sandstones. These groups are divided based on mineralogy and texture. Even though sandstones have very simple compositions which are based on framework grains, geologists have not been able to agree on a specific, right way, to classify sandstones. Sandstone classifications are typically done by point-counting a thin section using a method like the Gazzi-Dickinson Method. The composition of a sandstone can have important information regarding the genesis of the sediment when used with a triangular \"Q\"uartz, \"F\"eldspar, \"L\"ithic fragment (QFL diagrams). Many geologists, however, do not agree on how to separate the triangle parts into the single components so that the framework grains can be plotted. Therefore, there have been many published ways to classify sandstones, all of which are similar in their general format.\n\nVisual aids are diagrams that allow geologists to interpret different characteristics about a sandstone. The following QFL chart and the sandstone provenance model correspond with each other therefore, when the QFL chart is plotted those points can then be plotted on the sandstone provenance model. The stage of textural maturity chart illustrates the different stages that a sandstone goes through.\n\nDott's (1964) sandstone classification scheme is one of many such schemes used by geologists for classifying sandstones. Dott's scheme is a modification of Gilbert's classification of silicate sandstones, and it incorporates R.L. Folk's dual textural and compositional maturity concepts into one classification system. The philosophy behind combining Gilbert's and R. L. Folk's schemes is that it is better able to \"portray the continuous nature of textural variation from mudstone to arenite and from stable to unstable grain composition\". Dott's classification scheme is based on the mineralogy of framework grains, and on the type of matrix present in between the framework grains.\n\nIn this specific classification scheme, Dott has set the boundary between arenite and wackes at 15% matrix. In addition, Dott also breaks up the different types of framework grains that can be present in a sandstone into three major categories: quartz, feldspar, and lithic grains.\n\nSandstone has been used for domestic construction and housewares since prehistoric times, and continues to be used.\n\nSandstone was a popular building material from ancient times. It is relatively soft, making it easy to carve. It has been widely used around the world in constructing temples, homes, and other buildings. It has also been used for artistic purposes to create ornamental fountains and statues.\n\nSome sandstones are resistant to weathering, yet are easy to work. This makes sandstone a common building and paving material including in asphalt concrete. However, some that have been used in the past, such as the Collyhurst sandstone used in North West England, have been found less resistant, necessitating repair and replacement in older buildings. Because of the hardness of individual grains, uniformity of grain size and friability of their structure, some types of sandstone are excellent materials from which to make grindstones, for sharpening blades and other implements. Non-friable sandstone can be used to make grindstones for grinding grain, e.g., gritstone.\n\nA type of pure quartz sandstone, orthoquartzite, with more of 90–95 percent of quartz, has been proposed for nomination to the Global Heritage Stone Resource. In some regions of Argentina, the orthoquartzite-stoned facade is one of the main features of the Mar del Plata style bungalows.\n\n\n"}
{"id": "356612", "url": "https://en.wikipedia.org/wiki?curid=356612", "title": "Secretary of State for the Environment, Transport and the Regions", "text": "Secretary of State for the Environment, Transport and the Regions\n\nThe Secretary of State for the Environment, Transport and the Regions was a United Kingdom Cabinet position created in 1997, with responsibility for the Department of the Environment, Transport and the Regions (DETR). The position and department were created for John Prescott by merging the positions and responsibilities of the Secretary of State for Environment, the Secretary of State for Transport and some other functions.\n\nFrank Dobson, who had been Shadow Secretary of State for the Environment prior to the 1997 General Election, was made Secretary of State for Health, while Andrew Smith, who had been Shadow Secretary of State for Transport, was made a junior minister at the Department for Education and Employment. \n\nMichael Meacher, who had been Shadow Minister for Environmental Protection within the Shadow Cabinet, was given the non cabinet position of Minister of State for the Environment, and attended cabinet meetings where the Environment was discussed, and a position of Minister of State for Transport was similarly created, initially held by Gavin Strang.\n\nIn June 2001, the department was renamed the Department for Transport, Local Government and the Regions (DTLR), with the Environment portfolio being merged with the Ministry of Agriculture, Fisheries and Food into the Department of Environment, Food and Rural Affairs. All departments were given their own Secretaries of State. In May 2002, the Local Government and the Regions portfolios were separated from Transport and given to the Office of the Deputy Prime Minister, which in turn became the Department for Communities and Local Government in May 2006.\n\nFrom 1997 to 2001, the Ministers of State with responsibility for Transport were:\n\n\nJohn Reid attended cabinet meetings, but was not formally a member of the cabinet, whereas Gavin Strang was given a seat in the cabinet when he held the position.\n\nMinisters of State with responsibility for the Environment included\n\nHilary Armstrong was Minister of State with responsibility for Local Government (1997–2001).\n"}
{"id": "11443297", "url": "https://en.wikipedia.org/wiki?curid=11443297", "title": "Shear force", "text": "Shear force\n\nShearing forces are unaligned forces pushing one part of a body in one specific direction, and another part of the body in the opposite direction. When the forces are aligned into each other, they are called compression forces. An example is a deck of cards being pushed one way on the top, and the other at the bottom, causing the cards to slide. Another example is when wind blows at the side of a peaked roof of a home - the side walls experience a force at their top pushing in the direction of the wind, and their bottom in the opposite direction, from the ground or foundation. William A. Nash defines shear force in terms of planes: \"If a plane is passed through a body, a force acting along this plane is called a \"shear force\" or \"shearing force\".\"\n\nHere follows a short example of how to work out the shear force of a piece of steel. The factor of 0.6 used to change from tensile to shear force could vary from 0.58–0.62 and will depend on application.\n\nSteel called EN8 bright has a tensile strength of 800MPa and mild steel has a tensile strength of 400MPa.\n\nTo work out the force to shear a 25 mm diameter round steel EN8 bright;\n\nWhen working with a riveted or tensioned bolted joint, the strength comes from friction between the materials bolted together. Bolts are correctly torqued to maintain the friction. The shear force only becomes relevant when the bolts are not torqued.\n\nA bolt with property class 12.9 has a tensile strength of 1200MPa (1MPa = 1N/mm) or 1.2kN/mm and the yield strength is 0.90 times tensile strength, 1080MPa in this case.\n\nA bolt with property class 4.6 has a tensile strength of 400MPa (1MPa = 1N/mm) or 0.4 kN/mm and yield strength is 0.60 times tensile strength, 240MPa in this case.\n\nIn case of fasteners, proof load is specified as it gives a real life picture about the characteristics of the bolt.\n\n"}
{"id": "48661429", "url": "https://en.wikipedia.org/wiki?curid=48661429", "title": "Sillénite", "text": "Sillénite\n\nSillénite or sillenite is a mineral with the chemical formula BiSiO. It is named after the Swedish chemist Lars Gunnar Sillén, who mostly studied bismuth-oxygen compounds. It is found in Australia, Europe, China, Japan, Mexico and Mozambique, typically in association with bismutite.\n\nSillenites refer to a class of bismuth compounds with a structure similar to BiSiO, whose parent structure is γ-BiO, a meta-stable form of bismuth oxide. The cubic crystal sillenite structure is shared by several synthetic materials including bismuth titanate and bismuth germanate. These compounds have been extensively investigated for their non-linear optical properties.\n\nAdditional stoichiometries, and modified structures, are also found in BiGaO, BiFeO, and BiInO. These compounds have gathered recent interest due to their photocatalytic properties.\n"}
{"id": "4626960", "url": "https://en.wikipedia.org/wiki?curid=4626960", "title": "Social Fund for Development (Egypt)", "text": "Social Fund for Development (Egypt)\n\nSocial Fund for Development (Egypt) was established in 1991 by a Presidential decree # 189 and with support of the United Nations Development Programme. SFD was created with a capital of 1.1 Billion Egyptian Pounds to alleviate the hardship created by the returning Egyptian workers from the Gulf area as a result of the First Gulf War. According to Magdy Moussa, Middle East Regional Director at Plant Finance, \"SFD was regarded as a social safety net associated with the Government of Egypt's agreement to undertake its extensive Economic Reform and Structural Adjustment Program.\" SFD focuses on sustainable development projects. Also according to Moussa, a prominent focal point for SFD is promoting self-employment through microcredit projects.\n\nMajor donors are: the World Bank, the European Union, Germany, JICA, Kuwaiti Development Fund.\n"}
{"id": "29324571", "url": "https://en.wikipedia.org/wiki?curid=29324571", "title": "Solar cell efficiency", "text": "Solar cell efficiency\n\nSolar cell efficiency refers to the portion of energy in the form of sunlight that can be converted via photovoltaics into electricity.\n\nThe efficiency of the solar cells used in a photovoltaic system, in combination with latitude and climate, determines the annual energy output of the system. For example, a solar panel with 20% efficiency and an area of 1 m will produce 200 W at Standard Test Conditions, but it can produce more when the sun is high in the sky and will produce less in cloudy conditions or when the sun is low in the sky. In central Colorado, which receives annual insolation of 5.5 kWh/m/day (or 230W/m), such a panel can be expected to produce 400 kWh of energy per year. However, in Michigan, which receives only 3.8 kWh/m/day, annual energy yield will drop to 280 kWh for the same panel. At more northerly European latitudes, yields are significantly lower: 175 kWh annual energy yield in southern England.\n\nSeveral factors affect a cell's conversion efficiency value, including its reflectance efficiency, thermodynamic efficiency, charge carrier separation efficiency, charge carrier collection efficiency and conduction efficiency values. Because these parameters can be difficult to measure directly, other parameters are measured instead, including quantum efficiency, V ratio, and fill factor. Reflectance losses are accounted for by the quantum efficiency value, as they affect \"external quantum efficiency.\" Recombination losses are accounted for by the quantum efficiency, V ratio, and fill factor values. Resistive losses are predominantly accounted for by the fill factor value, but also contribute to the quantum efficiency and V ratio values.\n\nAs of December 2014, the world record for solar cell efficiency at 46.0% was achieved by using multi-junction concentrator solar cells, developed from collaboration efforts of Soitec, , France together with Fraunhofer ISE, Germany.\n\nThe factors affecting energy conversion efficiency were expounded in a landmark paper by William Shockley and Hans Queisser in 1961. See Shockley-Queisser limit for more detail.\n\nIf one has a source of heat at temperature and cooler heat sink at temperature , the maximum theoretically possible value for the ratio of work (or electric power) obtained to heat supplied is , given by a Carnot heat engine. If we take 6000 K for the temperature of the sun and 300 K for ambient conditions on earth, this comes to 95%. In 1981, Alexis de Vos and Herman Pauwels showed that this is achievable with a stack of an infinite number of cells with band gaps ranging from infinity (the first cells encountered by the incoming photons) to zero, with a voltage in each cell very close to the open-circuit voltage, equal to 95% of the band gap of that cell, and with 6000 K blackbody radiation coming from all directions. However, the 95% efficiency thereby achieved means that the electric power is 95% of the net amount of light absorbed – the stack emits radiation as it has non-zero temperature, and this radiation has to be subtracted from the incoming radiation when calculating the amount of heat being transferred and the efficiency. They also considered the more relevant problem of maximizing the power output for a stack being illuminated from all directions by 6000 K blackbody radiation. In this case, the voltages must be lowered to less than 95% of the band gap (the percentage is not constant over all the cells). The maximum theoretical efficiency calculated is 86.8% for a stack of an infinite number of cells, using the incoming concentrated sunlight radiation. When the incoming radiation comes only from an area of the sky the size of the sun, the efficiency limit drops to 68.7%.\n\nNormal photovoltaic systems however have only one p-n junction and are therefore subject to a lower efficiency limit, called the \"ultimate efficiency\" by Shockley and Queisser. Photons with an energy below the band gap of the absorber material cannot generate an electron-hole pair, so their energy is not converted to useful output, and only generates heat if absorbed. For photons with an energy above the band gap energy, only a fraction of the energy above the band gap can be converted to useful output. When a photon of greater energy is absorbed, the excess energy above the band gap is converted to kinetic energy of the carrier combination. The excess kinetic energy is converted to heat through phonon interactions as the kinetic energy of the carriers slows to equilibrium velocity. Traditional single-junction cells have a maximum theoretical efficiency of 33.16%.\n\nSolar cells with multiple band gap absorber materials improve efficiency by dividing the solar spectrum into smaller bins where the thermodynamic efficiency limit is higher for each bin.\n\nAs described above, when a photon is absorbed by a solar cell it can produce an electron-hole pair. One of the carriers may reach the p-n junction and contribute to the current produced by the solar cell; such a carrier is said to be \"collected\". Or, the carriers recombine with no net contribution to cell current.\n\nQuantum efficiency refers to the percentage of photons that are converted to electric current (i.e., collected carriers) when the cell is operated under short circuit conditions. The \"external\" quantum efficiency of a silicon solar cell includes the effect of optical losses such as transmission and reflection.\n\nIn particular, some measures can be taken to reduce these losses. The reflection losses, which can account for up to 10% of the total incident energy, can be dramatically decreased using a technique called texturization, a light trapping method that modifies the average light path.\n\nQuantum efficiency is most usefully expressed as a \"spectral\" measurement (that is, as a function of photon wavelength or energy). Since some wavelengths are absorbed more effectively than others, spectral measurements of quantum efficiency can yield valuable information about the quality of the semiconductor bulk and surfaces. Quantum efficiency alone is not the same as overall energy conversion efficiency, as it does not convey information about the fraction of power that is converted by the solar cell.\n\nA solar cell may operate over a wide range of voltages (V) and currents (I). By increasing the resistive load on an irradiated cell continuously from zero (a \"short circuit\") to a very high value (an \"open circuit\") one can determine the maximum power point, the point that maximizes V×I; that is, the load for which the cell can deliver maximum electrical power at that level of irradiation. (The output power is zero in both the short circuit and open circuit extremes).\n\nA high quality, monocrystalline silicon solar cell, at 25 °C cell temperature, may produce 0.60 V open-circuit (\"V\"). The cell temperature in full sunlight, even with 25 °C air temperature, will probably be close to 45 °C, reducing the open-circuit voltage to 0.55 V per cell. The voltage drops modestly, with this type of cell, until the short-circuit current is approached (\"I\"). Maximum power (with 45 °C cell temperature) is typically produced with 75% to 80% of the open-circuit voltage (0.43 V in this case) and 90% of the short-circuit current. This output can be up to 70% of the \"V x I\" product. The short-circuit current (\"I\") from a cell is nearly proportional to the illumination, while the open-circuit voltage (\"V\") may drop only 10% with an 80% drop in illumination. Lower-quality cells have a more rapid drop in voltage with increasing current and could produce only 1/2 \"V\" at 1/2 \"I\". The usable power output could thus drop from 70% of the \"V x I\" product to 50% or even as little as 25%. Vendors who rate their solar cell \"power\" only as \"V x I\", without giving load curves, can be seriously distorting their actual performance.\n\nThe maximum power point of a photovoltaic varies with incident illumination. For example, accumulation of dust on photovoltaic panels reduces the maximum power point. For systems large enough to justify the extra expense, a maximum power point tracker tracks the instantaneous power by continually measuring the voltage and current (and hence, power transfer), and uses this information to dynamically adjust the load so the maximum power is \"always\" transferred, regardless of the variation in lighting.\n\nAnother defining term in the overall behavior of a solar cell is the fill factor (\"FF\"). This factor is a measure of quality of a solar cell. This is the available \"power\" at the \"maximum power point\" (\"P\") divided by the \"open circuit voltage\" (\"V\") and the \"short circuit current\" (\"I\"):\n\nThe fill factor can be represented graphically by the IV sweep, where it is the ratio of the different rectangular areas.\n\nThe fill factor is directly affected by the values of the cell's series, shunt resistances and diodes losses. Increasing the shunt resistance (R) and decreasing the series resistance (R) lead to a higher fill factor, thus resulting in greater efficiency, and bringing the cell's output power closer to its theoretical maximum.\n\nTypical fill factors range from 50% to 82%. The fill factor for a normal silicon PV cell is 80%.\n\nEnergy conversion efficiency is measured by dividing the electrical output by the incident light power. Factors influencing output include spectral distribution, spatial distribution of power, temperature, and resistive load. IEC standard 61215 is used to compare the performance of cells and is designed around standard (terrestrial, temperate) temperature and conditions (STC): irradiance of 1 kW/m, a spectral distribution close to solar radiation through AM (airmass) of 1.5 and a cell temperature 25 °C. The resistive load is varied until the peak or maximum power point (MPP) is achieved. The power at this point is recorded as Watt-peak (Wp). The same standard is used for measuring the power and efficiency of PV modules.\n\nAir mass affects output. In space, where there is no atmosphere, the spectrum of the sun is relatively unfiltered. However, on earth, air filters the incoming light, changing the solar spectrum. The filtering effect ranges from Air Mass 0 (AM0) in space, to approximately Air Mass 1.5 on Earth. Multiplying the spectral differences by the quantum efficiency of the solar cell in question yields the efficiency. Terrestrial efficiencies typically are greater than space efficiencies. For example, a silicon solar cell in space might have an efficiency of 14% at AM0, but 16% on earth at AM 1.5. Note, however, that the number of incident photons in space is considerably larger, so the solar cell might produce considerably more power in space, despite the lower efficiency as indicated by reduced percentage of the total incident energy captured.\n\nSolar cell efficiencies vary from 6% for amorphous silicon-based solar cells to 44.0% with multiple-junction production cells and 44.4% with multiple dies assembled into a hybrid package. Solar cell energy conversion efficiencies for commercially available \"multicrystalline Si\" solar cells are around 14-19%. The highest efficiency cells have not always been the most economical — for example a 30% efficient multijunction cell based on exotic materials such as gallium arsenide or indium selenide produced at low volume might well cost one hundred times as much as an 8% efficient amorphous silicon cell in mass production, while delivering only about four times the output.\n\nHowever, there is a way to \"boost\" solar power. By increasing the light intensity, typically photogenerated carriers are increased, increasing efficiency by up to 15%. These so-called \"concentrator systems\" have only begun to become cost-competitive as a result of the development of high efficiency GaAs cells. The increase in intensity is typically accomplished by using concentrating optics. A typical concentrator system may use a light intensity 6-400 times the sun, and increase the efficiency of a one sun GaAs cell from 31% at AM 1.5 to 35%.\n\nA common method used to express economic costs is to calculate a price per delivered kilowatt-hour (kWh). The solar cell efficiency in combination with the available irradiation has a major influence on the costs, but generally speaking the overall system efficiency is important. Commercially available solar cells (as of 2006) reached system efficiencies between 5 and 19%.\n\nUndoped crystalline silicon devices are approaching the theoretical limiting efficiency of 29.43%. In 2017, efficiency of 26.63% was achieved in an amorphous silicon/crystalline silicon heterojunction cell that place both positive and negative contacts on the back of the cell.\n\nThe energy payback time is defined as the recovery time required for generating the energy spent for manufacturing a modern photovoltaic module. In 2008 it was estimated to be from 1 to 4 years depending on the module type and location. With a typical lifetime of 20 to 30 years, this means that modern solar cells would be net energy producers, i.e. they would generate more energy over their lifetime than the energy expended in producing them. Generally, thin-film technologies—despite having comparatively low conversion efficiencies—achieve significantly shorter energy payback times than conventional systems (often < 1 year).\n\nA study published in 2013 which the existing literature found that energy payback time was between 0.75 and 3.5 years with thin film cells being at the lower end and multi-si-cells having a payback time of 1.5-2.6 years. A 2015 review assessed the energy payback time and EROI of solar photovoltaics. In this meta study, which uses an insolation of 1700 kWh/m/year and a system lifetime of 30 years, mean harmonized EROIs between 8.7 and 34.2 were found. Mean harmonized energy payback time varied from 1.0 to 4.1 years. Crystalline silicon devices achieve on average an energy payback period of 2 years.\n\nLike any other technology, solar cell manufacture is dependent on and presupposes the existence of a complex global industrial manufacturing system. This comprises not only the fabrication systems typically accounted for in estimates of manufacturing energy, but the contingent mining, refining and global transportation systems, as well as other energy intensive critical support systems including finance, information, and security systems. The uncertainty of that energy component confers uncertainty on any estimate of payback times derived from that estimate, considered by some to be significant.\n\nThe illuminated side of some types of solar cells, thin films, have a transparent conducting film to allow light to enter into the active material and to collect the generated charge carriers. Typically, films with high transmittance and high electrical conductance such as indium tin oxide, conducting polymers or conducting nanowire networks are used for the purpose. There is a trade-off between high transmittance and electrical conductance, thus optimum density of conducting nanowires or conducting network structure should be chosen for high efficiency.\n\nBy lining the light receiving surface of the cell with nano-sized metallic studs, the efficiency of the cell can be substantially increased, as the light reflects off these studs at an oblique angle to the cell, increasing the length of the path the light takes through the cell, thereby increasing the number of photons absorbed by the cell, and so also the amount of current generated.\n\nThe main materials used for the nano-studs are silver, gold, and aluminium, to name a few. However, gold and silver are not very efficient, as they absorb much of the light in the visible spectrum, which contains most of the energy present in sunlight, reducing the amount of light reaching the cell. Aluminium, on the other hand, absorbs only ultraviolet radiation, and reflects both visible and infra-red light, so energy loss is minimized on that front. Aluminium is therefore capable of increasing the efficiency of the cell by up to 22% (in lab conditions).\n\nAn increase in solar cell temperature of around 1 °C leads to a decrease in efficiency of about 0.45%. To prevent decreased efficiency due to heating, a visibly transparent silica crystal layer can be applied to a solar panel. The silica layer acts as a thermal black body which emits heat as infrared radiation into space cooling the cell by up to 13 °C.\n\nAntireflective coatings could result in more destructive interference of incident light waves from the sun. Therefore, all sunlight would be transmitted into the photovoltaic. Additionally, texturizing, in which the surface of a solar cell is altered so that the reflected light strikes the surface again, is another technique used in order to reduce reflection. These surfaces can be created by etching or using lithography. Adding a flat back surface in addition to texturizing the front surface helps to trap the light within the cell for a longer optical path length.\n\nWhile many improvements have been made to the front side of solar cells for mass production, the aluminium back-surface is holding back improvements in efficiency. The efficiency of many solar cells has benefitted by creating so called, passivated emitter and rear cells (PERCs). The chemical deposition of a rear-surface dielectric passivation layer stack that is also made of either a thin silica or aluminium oxide film topped with a silicon nitride film helps to improve efficiency in silicon solar cells by over 1%. This helps increase cell efficiency for commercial Cz-Si wafer material to 20.2% and the cell efficiency for quasi-mono-Si to a record 19.9%.\n\nThin Film Materials show a lot of promise for solar cells in terms of low costs and adaptability to existing structures and frameworks in technology. However, since the materials are so thin, they lack the optical absorption that bulk material solar cells have. While attempts to make correct this problem have been tried, more important is the focus on the thin film surface recombination. Since this is the dominant recombination process of nanoscale thin-film solar cells, it is crucial to their efficiency. Adding a passivating thin layer of silicon dioxide could reduce recombination.\n\n\n"}
{"id": "53811098", "url": "https://en.wikipedia.org/wiki?curid=53811098", "title": "Solar coverage rate", "text": "Solar coverage rate\n\nThe solar coverage rate is the percentage of an amount of energy that is provided by the sun. This may be in reference to a solar thermal installation or a photovoltaic installation, i.e. a calculation of solar heat, electricity or total energy produced. The observation period is typically one year. As a general rule, higher values represent improved energy efficiency and improved environmental outcomes.\n\nThe solar coverage rate is used for need-based planning of solar installations and is a measure of the energetic (non-)dependence on energy sources other than the sun.\n\nDifferentiation between solar coverage rate for buildings for:\n\nThis value depends on the size of the storage unit (hot water tank or storage battery), the size of the harvesting surface (sun collection surface or surface area of photovoltaic modules), and on the amount of energy required. In addition to the total yield, there is another dimension that is important for assessing the effectiveness of a solar facility. This is the total energy loss and storage loss suffered by the facility.\n\nThe solar coverage rate cannot be used as the only measure of the cost effectiveness or quality of a facility. Other conditions must be taken into account. Among other factors, the value depends on the size of the facility, the location and orientation of the collectors, the size of storage available and the amount of energy required. A solar coverage rate of 100% would mean that the system's entire energy requirement can be covered by solar energy. For a solar thermal facility in Europe, this would mean that the entire heat requirement could be covered, even on a cold winter's day. On summer days, however, this same facility would produce a very large surplus that could not be used. The facility would have to be heavily over-dimensioned for the summer and could not necessarily be operated economically. A high level of coverage is therefore not always an advantage. Nevertheless, if very high coverage levels are to be achieved, then solutions can be offered in a local heat network.\n\nWhen planning a solar thermal facility, the optimal goal is to find a balanced compromise between yield, i.e. heat energy provided, and the solar coverage rate. A good compromise between yield and solar coverage usually also represents a good compromise between investment costs for the solar facility and costs saved on conventional energy.\n"}
{"id": "18478221", "url": "https://en.wikipedia.org/wiki?curid=18478221", "title": "Take Care of Texas", "text": "Take Care of Texas\n\nTake Care of Texas is a personal responsibility campaign which provides information on ways to keep air and water clean, conserve water and energy, reduce waste, and save money. The campaign is spearheaded by the Texas Commission on Environmental Quality and is known for its trademarked catchphrase, \"Take Care of Texas. It’s the only one we’ve got!\" \n\nIn June 2013, country music recording artist Kevin Fowler teamed up with Take Care of Texas to produce a public service announcement that promotes outdoor recreation in the state and encourages protection of natural resources.\nTake Care of Texas tapped rising country music star Cody Johnson to perform on public service announcements that begin airing on Texas TV stations on May 14, 2018.\n\n"}
{"id": "45228541", "url": "https://en.wikipedia.org/wiki?curid=45228541", "title": "TaxiBot", "text": "TaxiBot\n\nThe Taxibot is a semi-robotic towbarless aircraft tractor developed by the Lahav Division of Israel Aerospace Industries. The tractor can tow an aircraft from the terminal gate to the take-off point (taxi-out phase) and return it to the gate after landing (taxi-in phase). The TaxiBot eliminates the use of airplane engines during taxi-in and until immediately prior to take-off during taxi-out, significantly reducing aircraft fuel usage and the risk of foreign object damage. The TaxiBot is controlled by the pilot from the cockpit using the regular pilot controls and has an 800-hp hybrid-electric engine.\n\nThe TaxiBot has two models. The Narrow-Body (NB) TaxiBot will be used by existing and future single-aisle aircraft such as the Airbus A320 and Boeing 737 while the Wide-Body (WB) TaxiBot aim for all existing and future twin-aisle aircraft such as Airbus A380 and Boeing 747.\n\nThe TaxiBot completed certification tests on July 2014, was approved for airport towing in November 2014. and had the first commercial flight dispatch-towed, Lufthansa LH140 from Frankfurt to Nuremberg, on November 25, 2014. In February 2015, the TaxiBot entered regular flight operations by Lufthansa at Frankfurt Airport. Certification tests of the wide-body model are expected to start in autumn of 2015 with aim for certification in early 2016.\n\nThe TaxiBot is the only certified and operational alternative taxiing system currently in the market. Competing products in development by WheelTug and EGTS International are different as they are installed directly on the aircraft landing gear. This allows for shorter turnaround time but adds weight to the aircraft. The EGTS partnership has been dissolved due to the new economics imposed by the sharp drop in the price of jet fuel, though Safran will continue to develop the concept.\n\n"}
{"id": "564559", "url": "https://en.wikipedia.org/wiki?curid=564559", "title": "Tinkertoy", "text": "Tinkertoy\n\nThe Tinkertoy Construction Set is a toy construction set for children. It was created in 1914—six years after the Frank Hornby's Meccano sets—by Charles H. Pajeau, who formed the Toy Tinker Company in Evanston, Illinois to manufacture them. Pajeau, a stonemason, designed the toy after seeing children play with sticks and empty spools of thread. Pajeu partnered with Robert Pettit and Gordon Tinker to market a toy that would allow and inspire children to use their imaginations. After an initially slow start, over a million were sold.\n\nThe cornerstone of the set is a wooden spool roughly two inches (5 cm) in diameter with holes drilled every 45 degrees around the perimeter and one through the center. Unlike the center, the perimeter holes do not go all the way through. With the differing-length sticks, the set was intended to be based on the Pythagorean progressive right triangle.\n\nThe sets were introduced to the public through displays in and around Chicago which included model Ferris wheels. Tinkertoys have been used to create complex machines, including Danny Hillis's tic-tac-toe-playing computer (now in the collection of the Computer History Museum in Mountain View, California) and a robot at Cornell University in 1998.\n\nOne of Tinker Toy’s most distinctive features is the toy’s packaging. Initially, the mailing tube design was chosen to reduce shipping costs. Early versions of the packaging included an address label on the tube with space for postage. To assist consumers in differentiating between the various offerings, sets were placed in mail tube packages of different sizes and also delineated with a number (ex: 116, 136) and a name (ex: major, prep, big boy, junior, grad). A colorful “how-to” instruction guide accompanied each set. In the 1950s, color was added and the wooden sticks appeared in red, green, blue, and yellow.\n\nThe main manufacturing location was a 65,000 square foot four story plant at 2012 Ridge Avenue, Evanston, Illinois.\n\nTinkertoys were inducted into the National Toy Hall of Fame at The Strong in Rochester, New York, in 1998.\n\nHasbro owns the Tinkertoy brand and currently produces both Tinkertoy Plastic and Tinkertoy Classic (wood) sets and parts.\n\nIn addition to the spools, a standard Tinkertoy set includes:\nSpools and pulleys all have a single groove around the outside; \"Part W\" has two parallel grooves.\n\nSticks (or \"rods\") are slotted on each end, both to provide some \"give\" when inserted into snug-fitting holes, and to allow thin cards, flags, and strings to be inserted into the slots. They are color-coded by size; in the 1960s-era sets, they were, in order from shortest to longest, orange, yellow, blue, red, green, and violet. Each successively longer rod is (with allowances for the size of the spools) next smaller size times the square root of two; thus any two of the same size will combine with one of the next size up, and three spools, to form an isosceles right triangle (45°–45°–90°).\n\nTinkertoy sticks before 1992 were made with a diameter of 0.25 inch. The earlier sets had natural wood sticks, but changed to colored sticks in the late 1950s. From measurement, the orange sticks are 1.25 inches long; yellow, 2.15; blue, 3.35; red, 5.05; green, 7.40; and, purple, 10.85. Spools are 1.35 inches in diameter with holes of 0.30 inch depth.\n\nMost of the larger sets also include a driveshaft (an unfinished wooden rod without slotted ends, of an intermediate length between \"green\" and \"violet,\" normally turned with a small plastic crank.\n\nThe Ultra Construction Set also includes connectors, small cylindrical plastic pieces approximately 2 inches long with a slot in either end and a slotted hole crosswise through the center of the part.\n\nSets with battery-powered electric motors were available; these sets also typically included at least one wooden \"double pulley,\" with a single snug-fitting through-drilled center hole, and grooved rims at two diameters, allowing different moving parts to operate at different speeds.\n\n\n\n"}
{"id": "23003504", "url": "https://en.wikipedia.org/wiki?curid=23003504", "title": "Transport length", "text": "Transport length\n\nThe transport length in a strongly diffusing medium (noted l*) is the length over which the direction of propagation of the photon is randomized. It is related to the mean free path l by the relation:\n\nformula_1\n\nwith:\ng: the asymmetry coefficient. formula_2 or averaging of the scattering angle θ over a high number of scattering events.\n\ng can be evaluated with the Mie theory.\nIf g=0, l=l*. A single scattering is already isotropic.\nIf g→1, l*→infinite. A single scattering doesn't deviate the photons. Then the scattering never gets isotropic.\n\nThis length is useful for renormalizing a non-isotropic scattering problem into an isotropic one in order to use classical diffusion laws (Fick law and Brownian motion). The transport length might be measured by transmission experiments and backscattering experiments.\n\n"}
{"id": "1072877", "url": "https://en.wikipedia.org/wiki?curid=1072877", "title": "Tropical medicine", "text": "Tropical medicine\n\nTropical medicine is an interdisciplinary branch of medicine that deals with health issues that occur uniquely, are more widespread, or are more difficult to control in tropical and subtropical regions.\n\nPhysicians in this field diagnose and treat a variety of diseases and ailments. Most infections they deal with are endemic to the tropics. A few of the most well-known include malaria, HIV/AIDS, and tuberculosis. They must be knowledgeable in the 18 lesser known neglected tropical diseases, which include Chagas disease, rabies, and dengue. Poor living conditions in underdeveloped tropical countries have led to a rising number of non-communicable diseases. These diseases include cancer and cardiovascular disease, which, in the past, have been more of a worry in developed countries. Physicians trained in tropical medicine must also be prepared to diagnose and treat these diseases.\n\nTraining for physicians wishing to specialize in tropical medicine varies widely over the different countries. They must study epidemiology, virology, parasitology, and statistics, as well as the training required of an ordinary MD. Research on tropical diseases and how to treat them comes from both field research and research centers, including those of the military.\n\nSir Patrick Manson is recognized as the father of tropical medicine. He founded the London School of Hygiene & Tropical Medicine in 1899. He is credited with discovering the vector by which elephantiasis was being passed to humans. He learned it was a microscopic nematode worm called \"filaria\" \"sanguinis hominis.\" He continued to study this worm and its life cycle and determined the worms underwent metamorphosis within female \"culex fatigans\" mosquitoes. Thus he discovered mosquitoes as a vector for elephantiasis. After this discovery he collaborated with Ronald Ross to examine the transmission of malaria via mosquito vector. His work with discovering vectors as modes of transmission was critical in the founding of tropical medicine and our current understanding of many tropical diseases.\n\nTraining in tropical medicine is quite different between countries. Most physicians are trained at institutes of tropical medicine or incorporated into the training of infectious diseases.\n\nIn the UK, if a physician wants to specialize in tropical medicine, they must first train in general internal medicine and get accepted into the Royal College of Physicians. They must simultaneously study the specialty of infectious diseases while completing a full-time course load to receive their Diploma of Tropical Medicine and Hygiene. Their studies are carried out at either the London or Liverpool schools of tropical medicine. Additionally, they must spend two years at one of the UK centers approved for tropical medicine (located in London, Liverpool, or Birmingham). Physicians in the UK who wish to be certified in tropical medicine must spend at least a year abroad in an area lacking resources. Only then can they become certified in tropical medicine.\n\nThe training of United States tropical doctors is similar, though it is not a board recognized specialty in America. Physicians must first complete medical school and a program focusing on infectious diseases. Once completed, physicians can take the certification exam from the American Society of Tropical Medicine and Hygiene in order to receive the Certificate of Knowledge in Clinical Tropical Medicine and Travelers' Health.\n\nIn developing countries alone, 22 million people are living with HIV. Most infections are still in Africa, but Asia, Latin America, and the Caribbean are now seeing large numbers of infections as well. 95% of expected new infections will occur in the low income countries in the tropics. The expected number of new infections is 3-4 million per year. Risk factors such as needle use and unprotected sex are much more prevalent in tropical and underdeveloped areas. Once HIV is transmitted to a tropical area it is spread throughout the sexually active population. Though how fast and how far it spreads varies, some African countries have an HIV prevalence of 10%. More alarming still, in urban areas, prevalence among pregnant women can get as high as 30%. Healthcare professionals themselves are at great risk of exposure to HIV. An HIV prevalence of 10% means any given workforce will also have a 10% prevalence, and this does not exclude the healthcare team. Tuberculosis is thought to cause a more rapid disease progression. Tuberculosis is prevalent in tropical and under-developed countries, only making HIV more devastating. Without the expensive and high-tech medical equipment of developed, western countries, physicians in the tropics are left with few options. If they are able to catch an HIV-related bacterial or mycobacterial disease they can diagnose and manage the disease with basic drugs and standard treatment protocol. Many under-developed countries do not have a care strategy, and of those that do, they aren't as effective as they need to be to stop the spread of HIV.\n\nMalaria is a parasitic disease transmitted by an \"Anopheles\" mosquito to a human host. The parasite that causes malaria belongs to the genus \"Plasmodium\". Once infected, malaria can take a wide variety of forms and symptoms. The disease is placed into the uncomplicated category or the severe category. If quickly diagnosed and treated, malaria can be cured. However, some of the more serious symptoms, such as acute kidney failure, severe anemia, and acute respiratory distress syndrome can be fatal if not dealt with swiftly and properly. Certain types of \"Plasmodium\" can leave dormant parasites in the liver that can reawaken months or years later, causing additional relapses of the disease. In the \"World Malaria Report\" of 2016, the World Health Organization reported a malaria infection rate of 212 million, 90% of which occurred in the African region. However, malaria infection rates had fallen 21% since 2010 at the time of the report. The WHO also reported an estimated mortality rate of 429,000 deaths in the year 2015. The malaria mortality rate had fallen 29% globally since 2010. Children under 5 contract the malaria disease more easily than others, and in the year 2015, an estimated 303,000 children under the age of 5 were killed by malaria. Since the year 2010 however, the mortality rate of children under 5 fell by an estimated 35%.\n\nTuberculosis (TB) is an infectious bacterial disease that can affect any part of the body, though it primarily affects the lungs. It is a disease that affects the poor and weak, and is far more common in developing countries. TB can either be in its latent or active form. TB can be latent for years, sometimes over a decade. Though TB research receives a mere 1/6th the funding of HIV research, the disease has killed more people in the last 200 years than any other infectious disease. According to the Liverpool School of Tropical Medicine, an estimated 9 million people were infected with TB in the year 2013 alone. That same year 1.5 million people died from TB. Of those 1.5 million, 360,000 were HIV positive. Tuberculosis is extremely expensive to treat, and treatments are now becoming ineffective due to drug-resistant TB strains. In the year 2016, 1.3 million people died from TB. An additional 374,000 people died who were co-infected with both TB and HIV. Research has shown that if the subject is infected with HIV, the risk of latent TB becoming active TB is between 12 and 20 times higher.\n\nNon-communicable diseases are a series of chronic illnesses such as cardiovascular disease, cancer, injuries, and respiratory diseases, among others. Historically these diseases have plagued developed countries far more than developing countries. In the Global Burden of Disease Study of 2001, it was discovered that 20% of deaths in sub-Saharan Africa were caused by non-communicable diseases. In 2005, the World Health Organization performed a study that showed 80% of chronic disease deaths occurred in low to middle income countries. Non-communicable disease prevalence has been rising in under-developed countries for a variety of reasons. Lack of education and preventative medicine in under-developed countries, along with malnutrition or poor diet lead to many risk factors for non-communicable diseases.\n\nNeglected tropical diseases (NTDs) have been identified by the World Health Organization (WHO) as 18 tropical diseases, affecting over a billion people worldwide, especially in developing countries. These diseases are heterogeneous, meaning originating outside the organism affected by the disease. NTDs are caused by parasites, viruses, and bacteria. NTDs are neglected because they are not normally fatal on their own but are disabling. Persons with these diseases become more susceptible to other NTDs and fatal diseases such as HIV or malaria.\n\nNeglected tropical diseases effect can be measured in disability-adjusted life year (DALY). Each DALY corresponds to one lost year of healthy life, whether by death or disability. In the year 2010, it was estimated 26.6 million DALYs were lost. In addition to this, it is estimated NTDs cause a loss of 15-30% of productivity in countries that NTDs are endemic too. According to the CDC, 100% of countries categorized as 'low income' were affected by 5 different NTDs at once.\n\nTropical medicine requires an interdisciplinary approach, as the infections and diseases tropical medicine faces are both broad and unique. Tropical medicine requires research and assistance from the fields of epidemiology, microbiology, virology, parasitology, and logistics. Physicians of tropical medicine must have effective communication skills, as many of the patients they interact with do not speak English comfortably. They must be proficient in their knowledge of clinical and diagnostic skills, as they are often without high-tech diagnostic tools when in the field. For example, in an attempt to manage the Chagas disease being brought into the almost Chagas-free Brazilian city São Paulo by Bolivian immigrants, an interdisciplinary team was set up. The Bolivian immigrant population in São Paulo had a prevalence of Chagas disease of 4.4%, while Chagas disease transmission in São Paulo has been under control since the 1970s. This influx of Chagas disease led to an interdisciplinary team being brought together, This team tested the feasibility of managing Chagas disease and transmission at the primary healthcare level. The interdisciplinary team consisted of community health agents and clerical healthcare workers to recruit Chagas infected persons for the study, physicians, nurses, lab workers, and community agents. A pediatrician and cardiologist were also on call. Each were trained in pathology, parasitology, ecoepidemiology, and how to prevent, diagnose, and control Chagas disease. Training from experts in these respective fields was required. They examined reasons for lack of adherence to treatment, and used this knowledge to improve the effectiveness of their interventions. This interdisciplinary approach has been used to train many teams across Brazil in the management of Chagas disease.\n\nTropical medicine also consists of a preventative approach, especially in an educational aspect. For example, from 2009 to 2011, the London School of Hygiene & Tropical Medicine did an interventional study on a cohort of female sex workers (FSW) in Ouagadougou, Burkina Faso, a country in Eastern Africa. 321 HIV-unaffected FSWs were provided with peer-led HIV/STI education, HIV/STI testing and care, psychological support, general healthcare, and services for reproductive health. The same cohort would continue to follow up, quarterly, for 21 months. At each follow up they were tested for HIV and were able to utilize the preventative interventions if need be. Using models based on the same study population had their been no interventions, the expected prevalence of HIV infections was 1.23 infection per 100 person years. In the actual cohort with access to interventions not a single HIV infection was observed in the collective 409 person-years of follow-up.\n\nThroughout history, American military forces have been affected by many tropical diseases. In World War II alone, it was estimated almost one million soldiers had been infected by a tropical disease while serving. Most affected soldiers served in the Pacific, especially in the Philippines and New Guinea. Malaria was especially widespread in the Pacific, though soldiers in Southern Europe and Northern Africa also contracted it. Many diseases now known as neglected tropical diseases affected America soldiers as well. These included helminthiasis, schistosomiasis, dengue, and lymphatic filariasis. Lymphatic filariasis was such a problem it caused a $100 million evacuation of U.S troops out of New Guinea and the Tonga Islands.\n\nIn both the Korean and Vietnam Wars the United States army continued to be affected by tropical diseases. The most prevalent diseases to affect their military were malaria and dengue. Hepatitis A, scrub typhus, and hookworm infections were among the other tropical infections picked up by troops in these conflicts.\n\nTo combat the significant effects tropical diseases were having on their forces, the United States Military worked hard to develop drugs, vaccines, and other methods of disease control. Research done at the Walter Reed Army Institute of Research (WRAIR), the Naval Medical Research Center (NMRC), and various affiliated research centers have greatly improved the military's preparedness against tropical disease. In 1967, Captain Robert Phillips earned the Lasker Award for developing a type of IV therapy that reduced cholera's fatality rate from 60% to less than 1%. Other interventions licensed by the US Army include vaccines for hepatitis A and Japanese encephalitis. They have also developed the drugs mefloquine and malarone, both used in the treatment of malaria.\n\nLooking forward, the United States military currently has clinical trials testing for vaccines of malaria, adenovirus infection, dengue, and HIV/AIDS underway. However, with massive budget cuts to their military, these research centers are getting less and less funding and have lost many contractors already.\n\n\n\n"}
{"id": "49012349", "url": "https://en.wikipedia.org/wiki?curid=49012349", "title": "Tropical timber", "text": "Tropical timber\n\nTropical timber may refer to any type of timber or wood that grows in tropical rainforests and tropical and subtropical moist broadleaf forests and is harvested there. Typical examples of worldwide industrial significance include the hardwoods\namong many others.\n\nOverexploitation of those woods has led to widespread deforestation in the tropics. The intergovernmental organization International Tropical Timber Organization is concerned with conservation of the habitats of tropical timber trees.\n"}
{"id": "4578487", "url": "https://en.wikipedia.org/wiki?curid=4578487", "title": "UOP LLC", "text": "UOP LLC\n\nHoneywell UOP, formerly known as UOP LLC or Universal Oil Products, is a multi-national company developing and delivering technology to the petroleum refining, gas processing, petrochemical production, and major manufacturing industries. \n\nThe company's roots date back to 1914, when the revolutionary Dubbs thermal cracking process created the technological foundation for today's modern refining industry. In the ensuing decades, UOP engineers generated thousands of patents, leading to important advances in process technology, profitability consultation, and equipment design.\n\nUOP was founded in 1914 to exploit the market potential of patents held by inventors Jesse A. Dubbs and his son, Carbon Petroleum (C. P.) Dubbs. Perhaps because he was born in Pennsylvania oil country, Jesse Dubbs was enamored with the oil business. He even named his son Carbon after one of the elemental constituents of oil. Later, Carbon added the P. to make his name \"euphonious,\" he said. People started calling him \"Petroleum\" for fun, and the name stuck. C. P.'s son and grandson were also named Carbon, but each had a different middle initial.\n\nWhen founded in 1914 it was a privately held firm known as the National Hydrocarbon Company. J. Ogden Armour provided initial seed money and kept the firm going the first years it lost money. Most of the losses were incurred during lengthy legal battles with petroleum firms that were using technology patented by Dubbs.\n\nIn 1919 the firm's name became Universal Oil Products. \n\nBy 1931, petroleum firms saw a possible competitive advantage to owning UOP. A consortium of firms banded together to purchase the firm. These firms were Shell Oil Company, Standard Oil Company of California, Standard Oil Company of Indiana, Standard Oil Company of New Jersey, The Texas Company, and N. V. de Bataafsche Petroleum Maatschappij. This worried oil firms that were not part of the group and it helped prompt the Justice Department to begin an investigation of this arrangement as a possible violation of antitrust laws.\n\nThe oil firms placed the assets of UOP into a trust to support the American Chemical Society. In 1959 UOP went public and the income from that sale still provides monies to the American Chemical Society to administer grants to universities worldwide.\n\nIn August 1988 Union Carbide Corporation and AlliedSignal formed a joint venture combining the latter's wholly owned subsidiary, UOP Inc., and the Catalyst, Adsorbents and Process Systems (CAPS) business of Union Carbide.\n\nAlliedSignal acquired Honeywell in 1999 and assumed the latter's name. In 2005, what was now known as Honeywell acquired Union Carbide's stake in UOP, making it again a wholly owned subsidiary. The reported payment to Union Carbide was $835 million, valuing UOP at $1.6 billion.\n\nThe UOP Riverside research and development laboratory was conceived in 1921 by Hiram J. Halle, the chief executive officer of Universal Oil Products (now simply UOP), as a focal point where the best and brightest scientists could create new products and provide scientific support for the oil refining industry. Between 1921 and 1955, Riverside research resulted in 8,790 U.S. and foreign patents and provided the foundation on which UOP built its success.\n\nThe company benefited immensely by the addition to its research staff of Professor Vladimir Ipatieff, famous Russian scientist known internationally for his work in high-pressure catalysis. His contribution in catalytic chemistry gave UOP a position of leadership in the development of catalysis as applied to petroleum processing, the first being catalytic polymerization. Vladimir Haensel, a student of Ipatieff’s, joined UOP and developed Platforming in the 1950s. This process used very small amounts of platinum as a catalyst for the high yield of high-octane gasoline from petroleum-based feeds.\n\nThe Riverside facility was recognized as a National Historic Chemical Landmark by the American Chemical Society in 1995.\n\nUOP products fall into two groupings, physical products that can be seen, and technology products that provide knowledge and design. Physical products tend to be items used within a refinery or petrochemical plant to help convert chemicals into a desired product. Technology products tend to be based upon the ability to convert one chemical into another, refine crude oil, and separate chemicals from each other. For example: One area of UOP's expertise is fluid catalytic cracking that breaks long-chain hydrocarbons from crude oil into shorter compounds.\n\nDistillation is the most common way to separate chemicals with different boiling points. The greater the difference in boiling points, the easier it is to do. However, when boiling points are too similar, this isn't feasible. Adsorption separation might be possible. In adsorption separation, a mixture of chemicals flows past a porous solid called the adsorbent and some chemicals tend to \"hang out\" longer. A valid analogy is to imagine a busy street with people walking in the same direction past great places to eat. The hungriest people will tend to stop right away. The people that were pretty full will make it far down the street. Now imagine flooding the whole town with water and everyone runs out where you can collect them according to how hungry they were. In technical terms the liquid flush is called the desorbent.\n\nThis type of separation was first commonly used in the laboratory to separate small test samples. UOP pioneered a method of separating large volumes of chemicals. They call the counter-current embodiment of it the Sorbex family of processes. These are the major ones designed by UOP:\n<br><br>\nParex: separation of para-xylene from a mixture of xylene isomers<br>\nMX Sorbex: separation of meta-xylene from a mixed of xylene isomers<br>\nMolex: linear paraffins from branched and cyclic hydrocarbons<br>\nOlex: olefins from paraffins<br>\nCresex: para-cresol or meta-cresol from other cresol isomers<br>\nCymex: para-cymene or meta-cymene from other cymene isomers<br>\nSarex: fructose from mixed sugars\n\nIn 2008, UOP revealed its Ecofining process which takes vegetable oils, or lipids, and converts them into replacements for diesel and jet fuels. The resultant fuels from this refining process are indistinguishable from existing fossil-based petro-diesels and jet fuels.\n\nMost of UOP's work is not known to the general public since most applications are within refineries and petrochemical plants. However, one technology UOP helped developed is familiar to automobile owners. During the 1970s, UOP worked on pioneering a combined muffler catalytic converter. To help publicize their work they sponsored CanAm and Formula One teams. The race cars used were developed by Shadow Racing Cars. Many race fans were drawn to the team's innovative designs and underdog status. UOP finally achieved a goal when California adopted the catalytic converter after the UOP governmental relations rep, Donald Gazzaniga, helped push legislation through the state Senate and Assembly. \n\n\n"}
{"id": "22644886", "url": "https://en.wikipedia.org/wiki?curid=22644886", "title": "Yuyun Ismawati", "text": "Yuyun Ismawati\n\nYuyun Ismawati (born 1965) is an Indonesian environmental engineer. She has worked on design of city and rural water supply systems, and later on designing systems for safe waste management. She was awarded the Goldman Environmental Prize in 2009.\n\nShe is Senior Advisor and co-founder of BALIFOKUS Foundation, a Bali-based environmental NGO. Yuyun has broad and rich experiences in urban environmental management issues, environmental health and sanitation, as well as climate and toxics issues.\n"}
{"id": "37563232", "url": "https://en.wikipedia.org/wiki?curid=37563232", "title": "Zambezian and mopane woodlands", "text": "Zambezian and mopane woodlands\n\nThe Zambezian and mopane woodlands is a tropical and subtropical grasslands, savannas, and shrublands ecoregion of southeastern Africa.\n\nThe ecoregion is characterized by the mopane tree \"(Colophospermum mopane)\", and extends across portions of Botswana, Malawi, Mozambique, Namibia, South Africa, Swaziland, Zambia, and Zimbabwe, including the lower basins of the Zambezi and Limpopo rivers. \n\nThe more humid Southern Zanzibar-Inhambane coastal forest mosaic and Maputaland coastal forest mosaic ecoregions lie between the Zambezian and mopane woodlands and the Indian Ocean. The Zambezian and mopane woodlands lie generally at a lower elevation, and has lower rainfall, than the neighboring miombo woodlands ecoregions, which occupy the plateaus and escarpments above the river lowlands. It is bounded to the southwest by the Drakensberg Range and Southern African Bushveld and Drakensberg montane grassland, woodland, and forest ecoregions. To the west, it transitions to the drier Zambezian baikiaea woodlands and Kalahari Acacia-Baikiaea woodlands on the Kalahari sands of the Southern African Plateau.\n\nProtected areas in the ecoregion include:\n\n\n\n"}
