{"id": "27430624", "url": "https://en.wikipedia.org/wiki?curid=27430624", "title": "2010 Central European floods", "text": "2010 Central European floods\n\nThe 2010 Central European floods were a devastating series of weather events which occurred across several Central European countries during May and June 2010. Poland was the worst affected. Austria, Czech Republic, Germany, Hungary, Slovakia, Serbia and Ukraine were also affected.\n\nAt least thirty-seven people died in the floods and approximately 23,000 people were evacuated. The city of Kraków declared a state of emergency.\n\nThe floods forced the closure and relocation of items from the Auschwitz concentration camp museum. On 20 May, aid began arriving to Poland from several European Union countries.\n\nIn Poland, the floods caused the deaths of at least 25 people, the evacuation of approximately 23,000 people, and an estimated economic cost of euros. Poland's Prime Minister Donald Tusk informed the Sejm that ongoing flooding was \"the worst natural disaster in the nation's history ... without precedent in the past 160 years\".\n\nTwo months' worth of rain poured down over a 24‑hour period. The Auschwitz-Birkenau State Museum was closed and important artifacts were moved to higher ground as floodwaters approached. The city of Kraków announced a state of emergency. Due to the high level of the Vistula river, Kraków's Dębnicki bridge, located in the center of the city, and the Nowohucki bridge were closed on 18 May.\n\nThe flooding lasted for a number of days, and escalated on 20 May when the Vistula River broke its banks. In the town of Sandomierz, residents were stranded in their homes while power outages affected telecommunication. The 2010 flooding was considered more severe than the last major flood, in 1997.\n\nWrocław, where the level of the Oder river on 22 May reached 665 cm in Trestno, declared a flood alert. The Kozanów district of Wrocław was flooded after a temporary sandbag wall was breached.\n\nOn Sunday 23 May the Wisła river broke a retaining wall and flooded Świniary near Płock, and nearby villages, including Szady, Wiączemin Polski, Nowy Wiączemin and Nowosiodło. Reports stated that 22 villages in the Płock area had sustained flooding or were under imminent threat. Around 4,000 people and 5,000 animals were evacuated. In Płock, Gmury street was submerged.\n\nIn the Lublin Voivodeship, 800 people had to be evacuated after the river Chodelka flooded in the Gmina Wilków. On 23 May, it was reported that 23 villages were already flooded with 4–5 meters of water and the situation continued to worsen.\n\nDuring the May floods, at least 6,200 households in the Małopolska region alone were fully or partially flooded and 12,000 people were affected by it. Numerous other places in Poland were flooded too. In the Lesser Poland Voivodeship, another flood alert was announced on 2 June in relation to Kraków, Tarnów, the counties of Bochnia, Brzesko, Dąbrowa, and Sucha, and eight gminas. Twelve rivers exceeded the alarm level in 14 places and eleven rivers exceeded warning levels in 21 places. On 4 June the railway bridge between Nowy Sącz and Stary Sącz was broken by the river Poprad. At least three people fell from the bridge into the rushing waters. According to some reports their fate is still unknown while other say they managed to save themselves. The Poprad river also flooded the town of Muszyna. On 5 June the Vistula flooded the Gmina Szczucin and around 3,000 people had to be evacuated.\n\nIn the Silesian Voivodeship, flood alerts were again issued in the Bielsko, Bieruń-Lędziny, Cieszyn, Gliwice, Pszczyna, Racibórz, Wodzisław and Żywiec counties, and in the cities of Bielsko-Biała, Gliwice and Zabrze. In the Lublin Voivodeship, river-side gminas announced flood alerts.\n\nIn the Subcarpathian Voivodeship, the river Ropa flooded the town of Jasło on 5 June.\n\nFrom 3 June, the Trześniówka river flooded the part of the city of Sandomierz (located in the Świętokrzyskie Voivodeship) which lies on the right side of the Vistula, and which was already flooded in May. The city was also threatened by the Vistula river which reached 770 cm, over 100 cm past the alarm level.\n\nIn the Czech Republic, the heaviest rain in the region for eight years was reported. A state of emergency was declared in a total of 302 municipalities across the Zlín Region and Moravian-Silesian Region. One death was reported, due to drowning.\n\nIn Borsod-Abaúj-Zemplén County, Northern Hungary eighteen towns and villages were cut from the outside world by the flood of the rivers Sajó, Hernád and Bódva. More than 480 people had to leave their homes. In Miskolc the Szinva flooded the Diósgyőr district of the city during what was described by locals as \"the biggest flood since 1975\".\n\nSeveral roads became unusable, the border checkpoint of Sátoraljaújhely/Slovenské Nové Mesto was closed on June 1.\nIn Pásztó (Nógrád county), a local reservoir threatened with overflow; the earthen dam was strengthened by sandbags. 2000 people had to leave their homes. Houses would be under 4 m water within seven minutes of the collapse of the dam.\nA short part of Motorway M1 collapsed near Győr.\n\nOn 17 May, the death toll reached five people. Four of these were in Poland and included a fireman. The other, an elderly woman, was in the Czech Republic when she drowned.\n\nOn 21 May, the death toll in Poland had reached at least nine people with the whereabouts of three others being unknown. On 24 May the death toll in Poland was 15 confirmed dead.\n\nThe flood claimed several casualties in Hungary too: a man, whose house collapsed on him, died in Miskolc a woman died and two other persons suffered injuries in a car crash in Fejér county, where a car slipped on the flooded road; also in Fejér county a tree fell during the heavy rain, hitting a man who suffered life-threatening injuries.\n\nPoland asked for assistance from other European Union nations. They came to the rescue from 20 May onwards, with France, Germany, Lithuania, Latvia and Estonia, as well as the Czech Republic, despite that country being affected by the floods too. On 25 May 2010, Poland received help also from Russia (including 18 high-power pumps, 34 boats and 5 mobile power stations).\n\n"}
{"id": "50471084", "url": "https://en.wikipedia.org/wiki?curid=50471084", "title": "7AK7", "text": "7AK7\n\nThe 7AK7 is a pentode vacuum tube (thermionic valve). According to its manufacturer, Sylvania, it was \"designed for service in electronic computers\".\n\nThe tube was developed in 1948, designed at the request of L. D. Wilson for use in the Whirlwind computer.\nSignificant attention was directed towards its manufacturing process in order to ensure the part's\nreliability. Dubbed the \"computer tube\", it became the standard tube for all computers into the late 1950s.\n"}
{"id": "31599549", "url": "https://en.wikipedia.org/wiki?curid=31599549", "title": "Achema", "text": "Achema\n\nAchema is the largest fertilizer producer in the Baltic states. It is located in the city of Jonava in central Lithuania. In 2011 Achema employed about 1700 workers and reached 2.2 billion Litas revenues (about 640 million Euros), net profit was 96.3 million Litas (27.9 million Euros). The current managing director is Arūnas Laurinaitis.\n\nThe factory construction began in 1962 as one of the state-owned enterprises of the Lithuanian SSR.\n\nAn explosion occurred in the chemical fertilizers factory on 20 March 1989, causing a leak of nearly 7,500 tonnes of liquid ammonia. The catastrophe developed further into a fire of nitrophoska and fertilizers storehouses polluting the atmosphere with products of their decomposition: nitrous oxide, chlorine gas, etc. The toxic cloud moved towards Ukmergė, Širvintos, Kėdainiai. The concentration of ammonia surpassed the permissible level 150 times in Upninkai, located 10 km from the enterprise. One day after the accident, a toxic cloud 7 km wide and 50 km long was recorded between Jonava and Kėdainiai. Seven people died during the fire and leakage of ammonia, 29 became handicapped, and more people suffered from acute respiratory and cardiac arrest.\n\nThe beginning of SC “Achema“ subdivision „Sistematika“ work is closely related to the history of SC “Achema“ when automation department was established back in 1964 at that time company “Azotas“. The department was growing together with the company, and in 1999 was reorganized into the subsidiary “Sistematika“ which activity was expanding together with the concern “Achema Group“.\n"}
{"id": "835157", "url": "https://en.wikipedia.org/wiki?curid=835157", "title": "Adhesion", "text": "Adhesion\n\nAdhesion is the tendency of dissimilar particles or surfaces to cling to one another (cohesion refers to the tendency of similar or identical particles/surfaces to cling to one another). The forces that cause adhesion and cohesion can be divided into several types. The intermolecular forces responsible for the function of various kinds of stickers and sticky tape fall into the categories of chemical adhesion, dispersive adhesion, and diffusive adhesion. In addition to the cumulative magnitudes of these intermolecular forces, there are also certain emergent mechanical effects.\n\nSurface energy is conventionally defined as the work that is required to build an area of a particular surface. Another way to view the surface energy is to relate it to the work required to cleave a bulk sample, creating two surfaces. If the new surfaces are identical, the surface energy γ of each surface is equal to half the work of cleavage, W: γ = (1/2)W.\n\nIf the surfaces are unequal, the Young-Dupré equation applies:\nW = γ + γ – γ, where γ and γ are the surface energies of the two new surfaces, and γ is the interfacial energy.\n\nThis methodology can also be used to discuss cleavage that happens in another medium: γ = (1/2)W = (1/2)W. These two energy quantities refer to the energy that is needed to cleave one species into two pieces while it is contained in a medium of the other species. Likewise for a three species system: γ + γ – γ = W + W – W – W = W, where W is the energy of cleaving species 1 from species 2 in a medium of species 3.\n\nA basic understanding of the terminology of cleavage energy, surface energy, and surface tension is very helpful for understanding the physical state and the events that happen at a given surface, but as discussed below, the theory of these variables also yields some interesting effects that concern the practicality of adhesive surfaces in relation to their surroundings.\n\nThere is no single theory covering adhesion, and particular mechanisms are specific to particular material scenarios.\nFive mechanisms of adhesion have been proposed to explain why one material sticks to another:\n\nAdhesive materials fill the voids or pores of the surfaces and hold surfaces together by interlocking. Other interlocking phenomena are observed on different length scales. Sewing is an example of two materials forming a large scale mechanical bond, velcro forms one on a medium scale, and some textile adhesives (glue) form one at a small scale.\n\nTwo materials may form a compound at the joint. The strongest joints are where atoms of the two materials share or swap electrons (known respectively as ionic bonding or covalent bonding, respectively). A weaker bond is formed if a hydrogen atom in one molecule is attracted to an atom of nitrogen, oxygen, or fluorine in another molecule, a phenomenon called hydrogen bonding.\n\nChemical adhesion occurs when the surface atoms of two separate surfaces form ionic, covalent, or hydrogen bonds. The engineering principle behind chemical adhesion in this sense is fairly straightforward: if surface molecules can bond, then the surfaces will be bonded together by a network of these bonds. It bears mentioning that these attractive ionic and covalent forces are effective over only very small distances – less than a nanometer. This means in general not only that surfaces with the potential for chemical bonding need to be brought very close together, but also that these bonds are fairly brittle, since the surfaces then need to be kept close together.\n\nIn dispersive adhesion, also known as physisorption, two materials are held together by van der Waals forces: the attraction between two molecules, each of which has a region of slight positive and negative charge. In the simple case, such molecules are therefore polar with respect to average charge density, although in larger or more complex molecules, there may be multiple \"poles\" or regions of greater positive or negative charge. These positive and negative poles may be a permanent property of a molecule (Keesom forces) or a transient effect which can occur in any molecule, as the random movement of electrons within the molecules may result in a temporary concentration of electrons in one region (London forces).\n\nIn surface science, the term \"adhesion\" almost always refers to dispersive adhesion. In a typical solid-liquid-gas system (such as a drop of liquid on a solid surrounded by air) the contact angle is used to evaluate adhesiveness indirectly, while a Centrifugal Adhesion Balance allows for direct quantitative adhesion measurements. Generally, cases where the contact angle is low are considered of higher adhesion per unit area. This approach assumes that the lower contact angle corresponds to a higher surface energy. Theoretically, the more exact relation between contact angle and work of adhesion is more involved and is given by the Young-Dupre equation. The contact angle of the three-phase system is a function not only of dispersive adhesion (interaction between the molecules in the liquid and the molecules in the solid) but also cohesion (interaction between the liquid molecules themselves). Strong adhesion and weak cohesion results in a high degree of wetting, a lyophilic condition with low measured contact angles. Conversely, weak adhesion and strong cohesion results in lyophobic conditions with high measured contact angles and poor wetting.\n\nLondon dispersion forces are particularly useful for the function of adhesive devices, because they don't require either surface to have any permanent polarity. They were described in the 1930s by Fritz London, and have been observed by many researchers. Dispersive forces are a consequence of statistical quantum mechanics. London theorized that attractive forces between molecules that cannot be explained by ionic or covalent interaction can be caused by polar moments within molecules. Multipoles could account for attraction between molecules having permanent multipole moments that participate in electrostatic interaction. However, experimental data showed that many of the compounds observed to experience van der Waals forces had no multipoles at all. London suggested that momentary dipoles are induced purely by virtue of molecules being in proximity to one another. By solving the quantum mechanical system of two electrons as harmonic oscillators at some finite distance from one another, being displaced about their respective rest positions and interacting with each other's fields, London showed that the energy of this system is given by:\n\nWhile the first term is simply the zero-point energy, the negative second term describes an attractive force between neighboring oscillators. The same argument can also be extended to a large number of coupled oscillators, and thus skirts issues that would negate the large scale attractive effects of permanent dipoles cancelling through symmetry, in particular.\n\nThe additive nature of the dispersion effect has another useful consequence. Consider a single such dispersive dipole, referred to as the origin dipole. Since any origin dipole is inherently oriented so as to be attracted to the adjacent dipoles it induces, while the other, more distant dipoles are not correlated with the original dipole by any phase relation (thus on average contributing nothing), there is a net attractive force in a bulk of such particles. When considering identical particles, this is called cohesive force.\n\nWhen discussing adhesion, this theory needs to be converted into terms relating to surfaces. If there is a net attractive energy of cohesion in a bulk of similar molecules, then cleaving this bulk to produce two surfaces will yield surfaces with a dispersive surface energy, since the form of the energy remain the same. This theory provides a basis for the existence of van der Waals forces at the surface, which exist between any molecules having electrons. These forces are easily observed through the spontaneous jumping of smooth surfaces into contact. Smooth surfaces of mica, gold, various polymers and solid gelatin solutions do not stay apart when their separating becomes small enough – on the order of 1–10 nm. The equation describing these attractions was predicted in the 1930s by De Boer and Hamaker:\n\nwhere P is the force (negative for attraction), z is the separation distance, and A is a material-specific constant called the Hamaker constant.\n\nThe effect is also apparent in experiments where a polydimethylsiloxane (PDMS) stamp is made with small periodic post structures. The surface with the posts is placed face down on a smooth surface, such that the surface area in between each post is elevated above the smooth surface, like a roof supported by columns. Because of these attractive dispersive forces between the PDMS and the smooth substrate, the elevated surface – or “roof” – collapses down onto the substrate without any external force aside from the van der Waals attraction. Simple smooth polymer surfaces – without any microstructures – are commonly used for these dispersive adhesive properties. Decals and stickers that adhere to glass without using any chemical adhesives are fairly common as toys and decorations and useful as removable labels because they do not rapidly lose their adhesive properties, as do sticky tapes that use adhesive chemical compounds.\n\nIt is important to note that these forces also act over very small distances – 99% of the work necessary to break van der Waals bonds is done once surfaces are pulled more than a nanometer apart. As a result of this limited motion in both the van der Waals and ionic/covalent bonding situations, practical effectiveness of adhesion due to either or both of these interactions leaves much to be desired. Once a crack is initiated, it propagates easily along the interface because of the brittle nature of the interfacial bonds.\n\nAs an additional consequence, increasing surface area often does little to enhance the strength of the adhesion in this situation. This follows from the aforementioned crack failure – the stress at the interface is not uniformly distributed, but rather concentrated at the area of failure.\n\nSome conducting materials may pass electrons to form a difference in electrical charge at the joint. This results in a structure similar to a capacitor and creates an attractive electrostatic force between the materials.\n\nSome materials may merge at the joint by diffusion. This may occur when the molecules of both materials are mobile and soluble in each other. This would be particularly effective with polymer chains where one end of the molecule diffuses into the other material. It is also the mechanism involved in sintering. When metal or ceramic powders are pressed together and heated, atoms diffuse from one particle to the next. This joins the particles into one.\nDiffusive forces are somewhat like mechanical tethering at the molecular level. Diffusive bonding occurs when species from one surface penetrate into an adjacent surface while still being bound to the phase of their surface of origin. One instructive example is that of polymer-on-polymer surfaces. Diffusive bonding in polymer-on-polymer surfaces is the result of sections of polymer chains from one surface interdigitating with those of an adjacent surface. The freedom of movement of the polymers has a strong effect on their ability to interdigitate, and hence, on diffusive bonding. For example, cross-linked polymers are less capable of diffusion and interdigitation because they are bonded together at many points of contact, and are not free to twist into the adjacent surface. Uncrosslinked polymers (thermoplastics), on the other hand are freer to wander into the adjacent phase by extending tails and loops across the interface.\n\nAnother circumstance under which diffusive bonding occurs is “scission”. Chain scission is the cutting up of polymer chains, resulting in a higher concentration of distal tails. The heightened concentration of these chain ends gives rise to a heightened concentration of polymer tails extending across the interface. Scission is easily achieved by ultraviolet irradiation in the presence of oxygen gas, which suggests that adhesive devices employing diffusive bonding actually benefit from prolonged exposure to heat/light and air. The longer such a device is exposed to these conditions, the more tails are scissed and branch out across the interface.\n\nOnce across the interface, the tails and loops form whatever bonds are favorable. In the case of polymer-on-polymer surfaces, this means more van der Waals forces. While these may be brittle, they are quite strong when a large network of these bonds is formed. The outermost layer of each surface plays a crucial role in the adhesive properties of such interfaces, as even a tiny amount of interdigitation – as little as one or two tails of 1.25 angstrom length – can increase the van der Waals bonds by an order of magnitude.\n\nThe strength of the adhesion between two materials depends on which of the above mechanisms occur between the two materials, and the surface area over which the two materials contact. Materials that wet against each other tend to have a larger contact area than those that do not. Wetting depends on the surface energy of the materials.\n\nLow surface energy materials such as polyethylene, polypropylene, polytetrafluoroethylene and polyoxymethylene are difficult to bond without special surface preparation.\n\nAnother factor determining the strength of an adhesive contact is its shape. Adhesive contacts of complex shape begin to detach at the \"edges\" of the contact area. The process of destruction of adhesive contacts can be seen in the film.\n\nIn concert with the primary surface forces described above, there are several circumstantial effects in play. While the forces themselves each contribute to the magnitude of the adhesion between the surfaces, the following play a crucial role in the overall strength and reliability of an adhesive device.\n\nStringing is perhaps the most crucial of these effects, and is often seen on adhesive tapes. Stringing occurs when a separation of two surfaces is beginning and molecules at the interface bridge out across the gap, rather than cracking like the interface itself. The most significant consequence of this effect is the restraint of the crack. By providing the otherwise brittle interfacial bonds with some flexibility, the molecules that are stringing across the gap can stop the crack from propagating. Another way to understand this phenomenon is by comparing it to the stress concentration at the point of failure mentioned earlier. Since the stress is now spread out over some area, the stress at any given point has less of a chance of overwhelming the total adhesive force between the surfaces. If failure does occur at an interface containing a viscoelastic adhesive agent, and a crack does propagate, it happens by a gradual process called “fingering”, rather than a rapid, brittle fracture.\nStringing can apply to both the diffusive bonding regime and the chemical bonding regime. The strings of molecules bridging across the gap would either be the molecules that had earlier diffused across the interface or the viscoelastic adhesive, provided that there was a significant volume of it at the interface.\n\nTechnologically advanced adhesive devices sometimes make use of microstructures on surfaces, such as the periodic posts described above. These are biomimetic technologies inspired by the adhesive abilities of the feet of various arthropods and vertebrates (most notably, geckos). By intermixing periodic breaks into smooth, adhesive surfaces, the interface acquires valuable crack-arresting properties. Because crack initiation requires much greater stress than does crack propagation, surfaces like these are much harder to separate, as a new crack has to be restarted every time the next individual microstructure is reached.\n\nHysteresis, in this case, refers to the restructuring of the adhesive interface over some period of time, with the result being that the work needed to separate two surfaces is greater than the work that was gained by bringing them together (W > γ + γ). For the most part, this is a phenomenon associated with diffusive bonding. The more time is given for a pair of surfaces exhibiting diffusive bonding to restructure, the more diffusion will occur, the stronger the adhesion will become. The aforementioned reaction of certain polymer-on-polymer surfaces to ultraviolet radiation and oxygen gas is an instance of hysteresis, but it will also happen over time without those factors.\n\nIn addition to being able to observe hysteresis by determining if W > γ + γ is true, one can also find evidence of it by performing “stop-start” measurements. In these experiments, two surfaces slide against one another continuously and occasionally stopped for some measured amount of time. Results from experiments on polymer-on-polymer surfaces show that if the stopping time is short enough, resumption of smooth sliding is easy. If, however, the stopping time exceeds some limit, there is an initial increase of resistance to motion, indicating that the stopping time was sufficient for the surfaces to restructure.\n\nSome atmospheric effects on the functionality of adhesive devices can be characterized by following the theory of surface energy and interfacial tension. It is known that γ = (1/2)W = (1/2)W. If γ is high, then each species finds it favorable to cohere while in contact with a foreign species, rather than dissociate and mix with the other. If this is true, then it follows that when the interfacial tension is high, the force of adhesion is weak, since each species does not find it favorable to bond to the other. The interfacial tension of a liquid and a solid is directly related to the liquids wettability (relative to the solid), and thus one can extrapolate that cohesion increases in non-wetting liquids and decreases in wetting liquids. One example that verifies this is polydimethyl siloxane rubber, which has a work of self-adhesion of 43.6 mJ/m in air, 74 mJ/m in water (a nonwetting liquid) and 6 mJ/m in methanol (a wetting liquid).\n\nThis argument can be extended to the idea that when a surface is in a medium with which binding is favorable, it will be less likely to adhere to another surface, since the medium is taking up the potential sites on the surface that would otherwise be available to adhere to another surface. Naturally this applies very strongly to wetting liquids, but also to gas molecules that could adsorb onto the surface in question, thereby occupying potential adhesion sites. This last point is actually fairly intuitive: Leaving an adhesive exposed to air too long gets it dirty, and its adhesive strength will decrease. This is observed in the experiment: when mica is cleaved in air, its cleavage energy, W or W, is smaller than the cleavage energy in vacuum, W, by a factor of 13.\n\nLateral adhesion is the adhesion associated with sliding one object on a substrate such as sliding a drop on a surface. When the two objects are solids, either with or without a liquid between them, the lateral adhesion is described as friction. However, the behavior of lateral adhesion between a drop and a surface is tribologically very different from friction between solids, and the naturally adhesive contact between a flat surface and a liquid drop makes the lateral adhesion in this case, an individual field. Lateral adhesion can be measured using the centrifugal adhesion balance (CAB), which uses a combination of centrifugal and gravitational forces to decouple the normal and lateral forces in the problem.\n\n\n"}
{"id": "49201597", "url": "https://en.wikipedia.org/wiki?curid=49201597", "title": "Aliso Canyon Oil Field", "text": "Aliso Canyon Oil Field\n\nThe Aliso Canyon Oil Field (also Aliso Canyon Natural Gas Storage Field, Aliso Canyon Underground Storage Facility) is an oil field and natural gas storage facility in the Santa Susana Mountains in Los Angeles County, California, north of the Porter Ranch neighborhood of the City of Los Angeles. Discovered in 1938 and quickly developed afterward, the field peaked as an oil producer in the 1950s, but has remained active since its discovery. One of its depleted oil and gas producing formations, the Sesnon-Frew zone, was converted into a gas storage reservoir in 1973 by the Southern California Gas Company, the gas utility servicing the southern half of California. This reservoir is the second-largest natural gas storage site in the western United States, with a capacity of over 86 billion cubic feet of natural gas. Currently it is one of four gas storage facilities owned by Southern California Gas, the others being the La Goleta Gas Field west of Santa Barbara, Honor Rancho near Newhall, and Playa del Rey.\n\nOil production on the field continues from 32 active wells as of 2016. The gas storage reservoir is accessed through 115 gas injection wells, along with approximately 38 miles of pipeline internal to the field. Three operators were active on the field: Southern California Gas Company, The Termo Company, and Crimson Resource Management Corp.\n\nThe field is on the southern slope of the Santa Susana Mountains, an east-west trending range dividing the San Fernando Valley on the south from the Santa Clarita Valley on the north-northeast. With some of its productive wells set at an elevation over 3,000 feet, it is one of the highest and most rugged oil fields in California. The main entrance to the oil field is on Limekiln Canyon Trail where it intersects Sesnon Boulevard. Vehicles must pass a guard station and locked gate to enter.\n\nLand uses in the vicinity of the field include industrial (for the oil and gas field itself), open space, parkland, and residential to the south. Areas to the west, north, and east in the Santa Susana Mountains have been identified as Significant Ecological Areas. The Michael D. Antonovich Open Space Preserve abuts the field on the northeast, and numerous parks in Porter Ranch are adjacent on the south.\n\nSince the field is on the south slope of the Santa Susana Mountains, drainage is to the south into the San Fernando Valley, with runoff into Mormon Canyon, Limekiln Canyon, and Aliso Canyon, which all flow into the Los Angeles River, which then flows south through the Los Angeles Basin and out to the ocean at Long Beach. Vegetation on the field includes a mix of native habitat types, including oak woodlands and Venturan sage scrub, as well as non-native grassland, with many disturbed areas around roads and drilling and production pads. Climate in the area is Mediterranean, with warm, almost rainless summers, and mild and rainy winters. Snow is rare although it can fall at the higher elevations. Wildfires are common, particularly in the summer and fall, and some of the storage field was burned over in the October, 2008 14,000-acre Sesnon Fire.\n\nThe Aliso Canyon field consists of multiple layers of oil and gas bearing sediments in a southeast-plunging anticline bounded on the north by the Santa Susana Fault Zone and on the west-northwest by the Frew Fault. These tectonic features have formed a structural trap keeping oil in place. The layered Tertiary sedimentary zones within the anticline resemble a layer-cake elevated on the northwest, with some of the layers containing oil and gas, and other impermeable layers between them, keeping them separate. Older Cretaceous sedimentary rocks have been forced over the top by motion along the Frew Fault.\n\nThe uppermost stratigraphic unit containing oil is the Pliocene-age Pico Formation, which contains the Aliso, Porter, and Upper, Middle, and Lower Del Aliso zones, from top to bottom, ranging in depth from about 4,500 to 8,000 feet. Underneath the Pico is the Middle Miocene Modelo Formation, and beneath that, bounded by an unconformity, the Eocene-age Llajas Formation. Since these units are both permeable and in direct contact, they form a single productive zone, the Sesnon-Frew, the largest of the field's zones and the one used by SoCalGas for gas storage. This unit has an average depth of about 9,000 feet, and averages about 160 feet thick. Beneath the Sesnon-Frew are marine sediments of Cretaceous age, not known to contain oil, and below that crystalline basement rocks of Cretaceous age or older.\n\nThe Santa Susana Mountains are one of several anticlinal formations within the Ventura Basin, and as such have long been of interest to those looking for oil. The oldest oil well in California, and the oldest commercially viable oil well in the western United States, was at the Pico Canyon Area of the Newhall Oil Field less than five miles northwest of the Aliso field boundary, also in the Santa Susana Mountains. J. Paul Getty's Tidewater Associated Oil Company drilled the discovery well for the Aliso field in 1938, finding oil in the Porter zone, 5,393 feet below ground surface. Other producing zones were discovered not long after, including Del Aliso zone in 1938, and the Sesnon-Frew zone in 1940. Several companies operated the field in the early years, including Tidewater, Standard Oil of California, Porter Sesnon et al., Porter Oil Co., Carlton Beal and Associates, and M.L. Orcutt. By the middle of 1959, there were 118 producing wells on the entire field, and over 32 million barrels of oil had been withdrawn.\n\nEarly in production, the Sesnon-Frew zone had been identified as having a strong gas cap, with some wells being completed in gas-only portions of the reservoir, needing to be deepenend. The overproduction of gas led to accusations of wasting, and litigation commenced with Standard Oil and Tidewater accusing Carlton Beal of wasting gas (lacking a modern pipeline transport system, natural gas at this time was not always retained for use – it was commonly flared or just vented to the atmosphere). The State Oil and Gas Supervisor ruled in favor of Standard and Tidewater and limited production on the Sesnon pool to reduce the waste.\n\nOne enhanced recovery technique, waterflooding, was used on the field, beginning in 1976. The Del Aliso zone was produced this way as conventional oil production began to decline. In this method, water pumped up with oil is disposed by being pumped back into the same formation from which it came, restoring reservoir pressure and pushing the remaining reservoir fluid to other recovery wells, even though it becomes more and more diluted with time.\n\nSuburban developments of the San Fernando Valley began approaching the field after it had already been fully developed, with some of the first residential housing in Porter Ranch appearing in the 1960s, but the main buildout started in the 1970s. Development continued into the first decade of the 21st century, expanding into the foothills right up to the SoCalGas property line. Many of these projects were master-planned developments, including gated communities, in one of San Fernando Valley's most affluent areas.\n\nBy the early 1970s the Sesnon zone was depleted of oil. As it was an enormous and structurally sound reservoir, with an average depth of about 9,000 feet, and centrally located in the distribution area of Pacific Lighting (an ancestor of Southern California Gas Company), it was ideal to use as a storage reservoir for gas for the local utility. Pacific Lighting bought rights to that portion of the field from Getty's Tidewater, and worked over the old oil production wells, many dating from 1940s and 1950s, to turn them into gas injection wells. The Aliso Canyon Natural Gas Storage Facility, as this repurposed part of the oil field became known, became the largest gas storage reservoir owned by SoCalGas and the second largest in the western United States. Storage fields such as the four maintained by SoCalGas are necessary to balance the load between summer and winter months; gas can be withdrawn during the winter, when it is in high demand, and injected back into the reservoir during the warmer months. Aliso Canyon was ideally placed near the center of SoCalGas's service region, and connected to the system by an extensive pipeline network.\n\nIn 2009 SoCalGas proposed an expansion and upgrade of the storage facility involving replacement of the obsolete gas turbine compressors with more up-to-date electric versions. This project would increase the gas injection capacity of the site from 300 million to 450 million cubic feet per day, and remove the compressors which were installed in 1971 when the storage facility was first being developed. It would also move guard houses and some other structures, build a substation on the field, and upgrade various transmission and telecommunications lines. After environmental review through draft and final Environmental Impact Reports as required by the California Environmental Quality Act (CEQA), the project was approved and construction began in 2014.\n\nGas wells on the site are old, and have required considerable maintenance in recent years. Of 229 storage wells on the site, half were more than 57 years old as of July 2014. Casing, tubing, and wellhead leaks have occurred in recent years. For example, in 2013, two wells were found with casing leaks, four with tubing leaks, and two with leaks at the wellhead. In 2008, one well – \"Porter 50A\" – was found to have a gas pressure of 400 pounds per square inch on the surface annulus, an indication of a serious underground leak and potential safety hazard; this well was immediately removed from service, and on investigation corrosion was discovered along a 600-foot stretch of the production casing, ending more than 1000 feet below ground surface. SoCalGas designed a Storage Integrity Management Program to address these deficiences, along with a budget, and presented it to the State Public Utility Commission in 2014.\n\nTwo other oil companies continue to operate on the field, outside of the SoCalGas facility boundary: The Termo Company and Crimson Resource Management Corp. These companies produce oil from other, shallower zones than the Sesnon-Frew zone that SoCalGas uses for gas storage. The Termo Company proposed an expansion of their operation, adding another 12 wells to the 15 they already had at the end of 2015, but put their plans on hold after the methane gas eruption from SoCalGas well Standard Sesnon 25 that began on October 23, 2015.\n\nA dramatic break somewhere along the length of an 8,750-foot injection well casing resulted in a gigantic methane eruption from the field on October 23, 2015, spewing on the order of 60 million cubic feet of methane per day at first, before the pressure was reduced. The well, Standard Sesnon 25 (\"SS 25\") had originally been installed in 1953, and reworked as a gas injection well in 1973, but lacked a blowout prevention valve, as it had not been considered a priority given the well's position, at the time, far from a populated area. Fallout from the methane cloud, in the form of oily droplets and persistent noxious odors, caused the evacuation of over 6,000 families, who have relocated to hotels and other rentals at SoCalGas's expense throughout the region. Another 10,000 homes received air-purification systems at the company's expense. On Dec. 4, 2015, SoCalGas commenced drilling a relief well to stop the natural gas blowout by plugging the damaged well at its base. The relief well intercepted the base of the well on Feb. 11, 2016, and the company began pumping heavy fluids to temporarily control the flow of gas out of the well. SoCalGas was able to plug the leak permanently on February 18, 2016. Overall the well is estimated to have released over 100,000 metric tons of natural gas, the largest such release in U.S. history.\n\nIn March 2016, Termo Company was fined $75,000 for piping in methane emissions from another natural gas leak in what the Division of Oil, Gas, and Geothermal Resources called a \"brazen and intentional violations of state law\".\n\nOn April 28, 2016, the California State Assembly passed a bill that would temporarily ban injection of additional natural gas into this storage facility, effectively shutting it down. Governor of California Jerry Brown has already issued an executive order banning natural gas injection until all of the wells have been thoroughly tested for corrosion and leaks. The bill, known as Senate Bill 380, was introduced in the California State Senate by Fran Pavley. If the governor signs this bill, it will extend the moratorium on gas injection, and require the state to consider permanently shutting down this gas storage facility.\n\nGovernor Jerry Brown's sister, Kathleen Brown, is a lobbyist for the oil and gas industry who currently serves on the Board of Directors of Sempra Energy, which owns the SoCal Gas Aliso Canyon Oil Field.\n"}
{"id": "42986", "url": "https://en.wikipedia.org/wiki?curid=42986", "title": "Alternating current", "text": "Alternating current\n\nAlternating current (AC) is an electric current which periodically reverses direction, in contrast to direct current (DC) which flows only in one direction. Alternating current is the form in which electric power is delivered to businesses and residences, and it is the form of electrical energy that consumers typically use when they plug kitchen appliances, televisions, fans and electric lamps into a wall socket. A common source of DC power is a battery cell in a flashlight. The abbreviations \"AC\" and \"DC\" are often used to mean simply \"alternating\" and \"direct\", as when they modify \"current\" or \"voltage\".\n\nThe usual waveform of alternating current in most electric power circuits is a sine wave, whose positive half-period corresponds with positive direction of the current and vice versa. In certain applications, different waveforms are used, such as triangular or square waves. Audio and radio signals carried on electrical wires are also examples of alternating current. These types of alternating current carry information such as sound (audio) or images (video) sometimes carried by modulation of an AC carrier signal. These currents typically alternate at higher frequencies than those used in power transmission.\n\nElectrical energy is distributed as alternating current because AC voltage may be increased or decreased with a transformer. This allows the power to be transmitted through power lines efficiently at high voltage, which reduces the energy lost as heat due to resistance of the wire, and transformed to a lower, safer, voltage for use. Use of a higher voltage leads to significantly more efficient transmission of power. The power losses (formula_1) in the wire are a product of the square of the current (I) and the resistance (R) of the wire, described by the formula\nThis means that when transmitting a fixed power on a given wire, if the current is halved (i.e. the voltage is doubled), the power loss will be four times less.\n\nThe power transmitted is equal to the product of the current and the voltage (assuming no phase difference); that is,\nConsequently, power transmitted at a higher voltage requires less loss-producing current than for the same power at a lower voltage. Power is often transmitted at hundreds of kilovolts, and transformed to 100 V – 240 V for domestic use.\n\nHigh voltages have disadvantages, such as the increased insulation required, and generally increased difficulty in their safe handling. In a power plant, energy is generated at a convenient voltage for the design of a generator, and then stepped up to a high voltage for transmission. Near the loads, the transmission voltage is stepped down to the voltages used by equipment. Consumer voltages vary somewhat depending on the country and size of load, but generally motors and lighting are built to use up to a few hundred volts between phases. The voltage delivered to equipment such as lighting and motor loads is standardized, with an allowable range of voltage over which equipment is expected to operate. Standard power utilization voltages and percentage tolerance vary in the different mains power systems found in the world. High-voltage direct-current (HVDC) electric power transmission systems have become more viable as technology has provided efficient means of changing the voltage of DC power. Transmission with high voltage direct current was not feasible in the early days of electric power transmission, as there was then no economically viable way to step down the voltage of DC for end user applications such as lighting incandescent bulbs.\n\nThree-phase electrical generation is very common. The simplest way is to use three separate coils in the generator stator, physically offset by an angle of 120° (one-third of a complete 360° phase) to each other. Three current waveforms are produced that are equal in magnitude and 120° out of phase to each other. If coils are added opposite to these (60° spacing), they generate the same phases with reverse polarity and so can be simply wired together. In practice, higher \"pole orders\" are commonly used. For example, a 12-pole machine would have 36 coils (10° spacing). The advantage is that lower rotational speeds can be used to generate the same frequency. For example, a 2-pole machine running at 3600 rpm and a 12-pole machine running at 600 rpm produce the same frequency; the lower speed is preferable for larger machines. If the load on a three-phase system is balanced equally among the phases, no current flows through the neutral point. Even in the worst-case unbalanced (linear) load, the neutral current will not exceed the highest of the phase currents. Non-linear loads (e.g. the switch-mode power supplies widely used) may require an oversized neutral bus and neutral conductor in the upstream distribution panel to handle harmonics. Harmonics can cause neutral conductor current levels to exceed that of one or all phase conductors.\n\nFor three-phase at utilization voltages a four-wire system is often used. When stepping down three-phase, a transformer with a Delta (3-wire) primary and a Star (4-wire, center-earthed) secondary is often used so there is no need for a neutral on the supply side. For smaller customers (just how small varies by country and age of the installation) only a single phase and neutral, or two phases and neutral, are taken to the property. For larger installations all three phases and neutral are taken to the main distribution panel. From the three-phase main panel, both single and three-phase circuits may lead off. Three-wire single-phase systems, with a single center-tapped transformer giving two live conductors, is a common distribution scheme for residential and small commercial buildings in North America. This arrangement is sometimes incorrectly referred to as \"two phase\". A similar method is used for a different reason on construction sites in the UK. Small power tools and lighting are supposed to be supplied by a local center-tapped transformer with a voltage of 55 V between each power conductor and earth. This significantly reduces the risk of electric shock in the event that one of the live conductors becomes exposed through an equipment fault whilst still allowing a reasonable voltage of 110 V between the two conductors for running the tools.\n\nA third wire, called the bond (or earth) wire, is often connected between non-current-carrying metal enclosures and earth ground. This conductor provides protection from electric shock due to accidental contact of circuit conductors with the metal chassis of portable appliances and tools. Bonding all non-current-carrying metal parts into one complete system ensures there is always a low electrical impedance path to ground sufficient to carry any fault current for as long as it takes for the system to clear the fault. This low impedance path allows the maximum amount of fault current, causing the overcurrent protection device (breakers, fuses) to trip or burn out as quickly as possible, bringing the electrical system to a safe state. All bond wires are bonded to ground at the main service panel, as is the neutral/identified conductor if present.\n\nThe frequency of the electrical system varies by country and sometimes within a country; most electric power is generated at either 50 or 60 Hertz. Some countries have a mixture of 50 Hz and 60 Hz supplies, notably electricity power transmission in Japan. A low frequency eases the design of electric motors, particularly for hoisting, crushing and rolling applications, and commutator-type traction motors for applications such as railways. However, low frequency also causes noticeable flicker in arc lamps and incandescent light bulbs. The use of lower frequencies also provided the advantage of lower impedance losses, which are proportional to frequency. The original Niagara Falls generators were built to produce 25 Hz power, as a compromise between low frequency for traction and heavy induction motors, while still allowing incandescent lighting to operate (although with noticeable flicker). Most of the 25 Hz residential and commercial customers for Niagara Falls power were converted to 60 Hz by the late 1950s, although some 25 Hz industrial customers still existed as of the start of the 21st century. 16.7 Hz power (formerly 16 2/3 Hz) is still used in some European rail systems, such as in Austria, Germany, Norway, Sweden and Switzerland. Off-shore, military, textile industry, marine, aircraft, and spacecraft applications sometimes use 400 Hz, for benefits of reduced weight of apparatus or higher motor speeds. Computer mainframe systems were often powered by 400 Hz or 415 Hz for benefits of ripple reduction while using smaller internal AC to DC conversion units. In any case, the input to the M-G set is the local customary voltage and frequency, variously 200 V (Japan), 208 V, 240 V (North America), 380 V, 400 V or 415 V (Europe), and variously 50 Hz or 60 Hz.\n\nA direct current flows uniformly throughout the cross-section of a uniform wire. An alternating current of any frequency is forced away from the wire's center, toward its outer surface. This is because the acceleration of an electric charge in an alternating current produces waves of electromagnetic radiation that cancel the propagation of electricity toward the center of materials with high conductivity. This phenomenon is called skin effect. At very high frequencies the current no longer flows \"in\" the wire, but effectively flows \"on\" the surface of the wire, within a thickness of a few skin depths. The skin depth is the thickness at which the current density is reduced by 63%. Even at relatively low frequencies used for power transmission (50 Hz – 60 Hz), non-uniform distribution of current still occurs in sufficiently thick conductors. For example, the skin depth of a copper conductor is approximately 8.57 mm at 60 Hz, so high current conductors are usually hollow to reduce their mass and cost. Since the current tends to flow in the periphery of conductors, the effective cross-section of the conductor is reduced. This increases the effective AC resistance of the conductor, since resistance is inversely proportional to the cross-sectional area. The AC resistance often is many times higher than the DC resistance, causing a much higher energy loss due to ohmic heating (also called IR loss).\n\nFor low to medium frequencies, conductors can be divided into stranded wires, each insulated from one another, and the relative positions of individual strands specially arranged within the conductor bundle. Wire constructed using this technique is called Litz wire. This measure helps to partially mitigate skin effect by forcing more equal current throughout the total cross section of the stranded conductors. Litz wire is used for making high-Q inductors, reducing losses in flexible conductors carrying very high currents at lower frequencies, and in the windings of devices carrying higher radio frequency current (up to hundreds of kilohertz), such as switch-mode power supplies and radio frequency transformers.\n\nAs written above, an alternating current is made of electric charge under periodic acceleration, which causes radiation of electromagnetic waves. Energy that is radiated is lost. Depending on the frequency, different techniques are used to minimize the loss due to radiation.\n\nAt frequencies up to about 1 GHz, pairs of wires are twisted together in a cable, forming a twisted pair. This reduces losses from electromagnetic radiation and inductive coupling. A twisted pair must be used with a balanced signalling system, so that the two wires carry equal but opposite currents. Each wire in a twisted pair radiates a signal, but it is effectively cancelled by radiation from the other wire, resulting in almost no radiation loss.\n\nCoaxial cables are commonly used at audio frequencies and above for convenience. A coaxial cable has a conductive wire inside a conductive tube, separated by a dielectric layer. The current flowing on the surface of the inner conductor is equal and opposite to the current flowing on the inner surface of the outer tube. The electromagnetic field is thus completely contained within the tube, and (ideally) no energy is lost to radiation or coupling outside the tube. Coaxial cables have acceptably small losses for frequencies up to about 5 GHz. For microwave frequencies greater than 5 GHz, the losses (due mainly to the electrical resistance of the central conductor) become too large, making waveguides a more efficient medium for transmitting energy. Coaxial cables with an air rather than solid dielectric are preferred as they transmit power with lower loss.\n\nWaveguides are similar to coaxial cables, as both consist of tubes, with the biggest difference being that the waveguide has no inner conductor. Waveguides can have any arbitrary cross section, but rectangular cross sections are the most common. Because waveguides do not have an inner conductor to carry a return current, waveguides cannot deliver energy by means of an electric current, but rather by means of a \"guided\" electromagnetic field. Although surface currents do flow on the inner walls of the waveguides, those surface currents do not carry power. Power is carried by the guided electromagnetic fields. The surface currents are set up by the guided electromagnetic fields and have the effect of keeping the fields inside the waveguide and preventing leakage of the fields to the space outside the waveguide. Waveguides have dimensions comparable to the wavelength of the alternating current to be transmitted, so they are only feasible at microwave frequencies. In addition to this mechanical feasibility, electrical resistance of the non-ideal metals forming the walls of the waveguide cause dissipation of power (surface currents flowing on lossy conductors dissipate power). At higher frequencies, the power lost to this dissipation becomes unacceptably large.\n\nAt frequencies greater than 200 GHz, waveguide dimensions become impractically small, and the ohmic losses in the waveguide walls become large. Instead, fiber optics, which are a form of dielectric waveguides, can be used. For such frequencies, the concepts of voltages and currents are no longer used.\n\nAlternating currents are accompanied (or caused) by alternating voltages. An AC voltage \"v\" can be described mathematically as a function of time by the following equation:\n\nwhere\n\nThe peak-to-peak value of an AC voltage is defined as the difference between its positive peak and its negative peak. Since the maximum value of formula_10 is +1 and the minimum value is −1, an AC voltage swings between formula_11 and formula_12. The peak-to-peak voltage, usually written as formula_13 or formula_14, is therefore formula_15.\n\nThe relationship between voltage and the power delivered is\n\nRather than using instantaneous power, formula_18, it is more practical to use a time averaged power (where the averaging is performed over any integer number of cycles). Therefore, AC voltage is often expressed as a root mean square (RMS) value, written as formula_19, because\n\n\nBelow it is assumed an AC waveform (with no DC component).\n\nThe RMS voltage is the square Root of the Mean over one cycle of the Square of the instantaneous voltage.\n\n\n\n\n\nTo illustrate these concepts, consider a 230 V AC mains supply used in many countries around the world. It is so called because its root mean square value is 230 V. This means that the time-averaged power delivered is equivalent to the power delivered by a DC voltage of 230 V. To determine the peak voltage (amplitude), we can rearrange the above equation to:\n\nFor 230 V AC, the peak voltage formula_33 is therefore formula_34, which is about 325 V. During the course of one cycle the voltage rises from zero to 325 V, falls through zero to -325 V, and returns to zero.\n\nAlternating current is used to transmit information, as in the cases of telephone and cable television. Information signals are carried over a wide range of AC frequencies. POTS telephone signals have a frequency of about 3 kHz, close to the baseband audio frequency. Cable television and other cable-transmitted information currents may alternate at frequencies of tens to thousands of megahertz. These frequencies are similar to the electromagnetic wave frequencies often used to transmit the same types of information over the air.\n\nThe first alternator to produce alternating current was a dynamo electric generator based on Michael Faraday's principles constructed by the French instrument maker Hippolyte Pixii in 1832. Pixii later added a commutator to his device to produce the (then) more commonly used direct current. The earliest recorded practical application of alternating current is by Guillaume Duchenne, inventor and developer of electrotherapy. In 1855, he announced that AC was superior to direct current for electrotherapeutic triggering of muscle contractions. Alternating current technology had first developed in Europe due to the work of Guillaume Duchenne (1850s), the Hungarian Ganz Works company (1870s), and in the 1880s: Sebastian Ziani de Ferranti, Lucien Gaulard, and Galileo Ferraris.\n\nIn 1876, Russian engineer Pavel Yablochkov invented a lighting system where sets of induction coils were installed along a high voltage AC line. Instead of changing voltage, the primary windings transferred power to the secondary windings which were connected to one or several 'electric candles' (arc lamps) of his own design, used to keep the failure of one lamp from disabling the entire circuit. In 1878, the Ganz factory, Budapest, Hungary, began manufacturing equipment for electric lighting and, by 1883, had installed over fifty systems in Austria-Hungary. Their AC systems used arc and incandescent lamps, generators, and other equipment.\n\nAlternating current systems can use transformers to change voltage from low to high level and back, allowing generation and consumption at low voltages but transmission, possibly over great distances, at high voltage, with savings in the cost of conductors and energy losses. A bipolar open-core power transformer developed by Lucien Gaulard and John Dixon Gibbs was demonstrated in London in 1881, and attracted the interest of Westinghouse. They also exhibited the invention in Turin in 1884. However these early induction coils with open magnetic circuits are inefficient at transferring power to loads. Until about 1880, the paradigm for AC power transmission from a high voltage supply to a low voltage load was a series circuit. Open-core transformers with a ratio near 1:1 were connected with their primaries in series to allow use of a high voltage for transmission while presenting a low voltage to the lamps. The inherent flaw in this method was that turning off a single lamp (or other electric device) affected the voltage supplied to all others on the same circuit. Many adjustable transformer designs were introduced to compensate for this problematic characteristic of the series circuit, including those employing methods of adjusting the core or bypassing the magnetic flux around part of a coil. The direct current systems did not have these drawbacks, giving it significant advantages over early AC systems.\n\nIn the autumn of 1884, Károly Zipernowsky, Ottó Bláthy and Miksa Déri (ZBD), three engineers associated with the Ganz factory, determined that open-core devices were impractical, as they were incapable of reliably regulating voltage. In their joint 1885 patent applications for novel transformers (later called ZBD transformers), they described two designs with closed magnetic circuits where copper windings were either a wound around iron wire ring core or b) surrounded by iron wire core. In both designs, the magnetic flux linking the primary and secondary windings traveled almost entirely within the confines of the iron core, with no intentional path through air (see toroidal cores). The new transformers were 3.4 times more efficient than the open-core bipolar devices of Gaulard and Gibbs. The Ganz factory in 1884 shipped the world's first five high-efficiency AC transformers. This first unit had been manufactured to the following specifications: 1,400 W, 40 Hz, 120:72 V, 11.6:19.4 A, ratio 1.67:1, one-phase, shell form.\n\nThe ZBD patents included two other major interrelated innovations: one concerning the use of parallel connected, instead of series connected, utilization loads, the other concerning the ability to have high turns ratio transformers such that the supply network voltage could be much higher (initially 1400 V to 2000 V) than the voltage of utilization loads (100 V initially preferred). When employed in parallel connected electric distribution systems, closed-core transformers finally made it technically and economically feasible to provide electric power for lighting in homes, businesses and public spaces. The other essential milestone was the introduction of 'voltage source, voltage intensive' (VSVI) systems' by the invention of constant voltage generators in 1885. Ottó Bláthy also invented the first AC electricity meter.\n\nThe AC power systems was developed and adopted rapidly after 1886 due to its ability to distribute electricity efficiently over long distances, overcoming the limitations of the direct current system. In 1886, the ZBD engineers designed the world's first power station that used AC generators to power a parallel-connected common electrical network, the steam-powered Rome-Cerchi power plant. The reliability of the AC technology received impetus after the Ganz Works electrified a large European metropolis: Rome in 1886.\n\nIn the US William Stanley, Jr. designed one of the first practical devices to transfer AC power efficiently between isolated circuits. Using pairs of coils wound on a common iron core, his design, called an induction coil, was an early (1885) transformer. Stanley also worked on engineering and adapting European designs such as the Gaulard and Gibbs transformer for US entrepreneur George Westinghouse who started building AC systems in 1886. The spread of Westinghouse and other AC systems triggered a push back in late 1887 by Edison (a proponent of direct current) who attempted to discredit alternating current as too dangerous in a public campaign called the \"War of Currents\". In 1888 alternating current systems gained further viability with introduction of a functional AC motor, something these systems had lacked up till then. The design, an induction motor, was independently invented by Galileo Ferraris and Nikola Tesla (with Tesla's design being licensed by Westinghouse in the US). This design was further developed into the modern practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown.\n\nThe Ames Hydroelectric Generating Plant (spring of 1891) and the original Niagara Falls Adams Power Plant (August 25, 1895) were among the first hydroelectric alternating current power plants. The first long distance transmission of single-phase electricity was from a hydroelectric generating plant in Oregon at Willamette Falls which in 1890 sent power fourteen miles downriver to downtown Portland for street lighting. In 1891, a second transmission system was installed in Telluride Colorado. The San Antonio Canyon Generator was the third commercial single-phase hydroelectric AC power plant in the United States to provide long-distance electricity. It was completed on December 31, 1892 by Almarian William Decker to provide power to the city of Pomona, California which was 14 miles away. In 1893 he next designed the first commercial three-phase power plant in the United States using alternating current was the hydroelectric Mill Creek No. 1 Hydroelectric Plant near Redlands, California. Decker's design incorporated 10 kV three-phase transmission and established the standards for the complete system of generation, transmission and motors used today. The Jaruga Hydroelectric Power Plant in Croatia was set in operation on 28 August 1895. The two generators (42 Hz, 550 kW each) and the transformers were produced and installed by the Hungarian company Ganz. The transmission line from the power plant to the City of Šibenik was long on wooden towers, and the municipal distribution grid 3000 V/110 V included six transforming stations. Alternating current circuit theory developed rapidly in the latter part of the 19th and early 20th century. Notable contributors to the theoretical basis of alternating current calculations include Charles Steinmetz, Oliver Heaviside, and many others. Calculations in unbalanced three-phase systems were simplified by the symmetrical components methods discussed by Charles Legeyt Fortescue in 1918. \n\n"}
{"id": "26174290", "url": "https://en.wikipedia.org/wiki?curid=26174290", "title": "Anegasaki Power Station", "text": "Anegasaki Power Station\n\n\n"}
{"id": "52537408", "url": "https://en.wikipedia.org/wiki?curid=52537408", "title": "Atlantis Resources", "text": "Atlantis Resources\n\nAtlantis Resources is a vertically integrated turbine supplier and project owner in the tidal power industry based in Singapore, Edinburgh and Bristol.\n\nIn February 2014, Atlantis became the world’s first tidal energy company to float on the LSE's AIM and commenced construction on MeyGen, Europe’s largest planned tidal power project, later that year.\n\nIn September 2015, Atlantis appointed Simon Counsell as Chief Financial Officer.\n\nA number of strategic investments and acquisitions occurred in late 2015 and throughout 2016 culminating in first power being produced at MeyGen.\n\nOn 20th Feb 2017 the company announced that it had completed the first phase (Phase 1a) of the Meygen project in the Pentland Firth. This phase included the design, manufacture and deployment of 4 1.5 MW turbines \n\nAtlantis has commercial and project development teams based in Edinburgh, an operations base located at Nigg Energy Park in Invergordon and the turbine and engineering services division is located in Bristol.\n\n\n"}
{"id": "36685619", "url": "https://en.wikipedia.org/wiki?curid=36685619", "title": "Avioimpex Flight 110", "text": "Avioimpex Flight 110\n\nAvioimpex Flight 110 was a scheduled international passenger flight operated by Avioimpex that crashed on 20 November 1993 while flying from Geneva to Skopje. Before the disaster, Flight 110 had deviated from Skopje International Airport to Ohrid Airport due to a blizzard in the Macedonian capital. The plane, a Yakovlev Yak-42, was carrying 108 passengers and eight crew, and crashed about east of Ohrid Airport. All 116 people onboard were killed as a result of the crash. One passenger lived for eleven days after the disaster but succumbed to his injuries. Most of the victims were Yugoslav citizens of Albanian ethnicity.\n\nThe crash was Macedonia's third aviation disaster in 16 months, and remains the country's deadliest. A subsequent investigation established the cause of the accident as pilot error.\n\nFlight 110 was an international scheduled passenger flight originating in Geneva, Switzerland with a final destination of Skopje, Macedonia. Due to a blizzard at Skopje, Flight 110 was diverted to Ohrid Airport. \n\nCleared for an approach to Runway 02, the Yak-42 was approximately 2,300 feet too high to carry out a successful landing, so a missed approach procedure was executed. Shortly afterwards the crew of Flight 110 radioed that they were not receiving the VOR signal. Air Traffic control was unable to satisfy the request for a bearing and the pilot of Flight 110 advised that he could not see the runway lights. Shortly thereafter Flight 110 crashed killing 115 of the 116 people on board. One passenger, Rade Jevremović, survived but was badly injured.\n\nEighty percent of the passengers were citizens of Yugoslavia, mostly ethnic Albanians, while the remainder were citizens of Macedonia. The four members of the flight crew were Russian and the four cabin crew members were Macedonian. Among the passengers was Pierre Ollier, a French United Nations High Commissioner for Refugees (UNHCR) official in his mid-20s who had just returned from an assignment in war-torn Bosnia and Herzegovina.\n\nDue to Flight 110 being the third aviation disaster in a sixteen-month period to take place in his country, Minister of Urban Planning, Civil Engineering, Communications and Ecology Antoni Pesev resigned. The pilots association complained of broken equipment and poor safety standards at both Skopje and Ohrid Airports.\n\nOn 1 December 1993, Rade Jevremovic died without ever regaining consciousness in the days following the crash of Avioimpex Flight 110.\n\nFlight 110 remains the deadliest aviation crash to ever take place in Macedonia.\n\nThe cause of the crash was attributed to a violation of the airport traffic pattern by the crew of Flight 110, who initiated a turn into rising terrain. A contributing factor was their decision to proceed with the approach even though they were not receiving a navigational signal due to being out of range of the VOR station. \n\n\n"}
{"id": "4485", "url": "https://en.wikipedia.org/wiki?curid=4485", "title": "Bakelite", "text": "Bakelite\n\nBakelite ( ; sometimes spelled Baekelite) or polyoxybenzylmethyleneglycolanhydride was the first plastic made from synthetic components. It is a thermosetting phenol formaldehyde resin, formed from a condensation reaction of phenol with formaldehyde. It was developed by the Belgian-American chemist Leo Baekeland in Yonkers, New York, in 1907.\n\nBakelite was patented on December 7, 1909. The creation of a synthetic plastic was revolutionary for its electrical nonconductivity and heat-resistant properties in electrical insulators, radio and telephone casings and such diverse products as kitchenware, jewelry, pipe stems, children's toys, and firearms. The \"retro\" appeal of old Bakelite products has made them collectible.\n\nBakelite was designated a National Historic Chemical Landmark on November 9, 1993, by the American Chemical Society in recognition of its significance as the world's first synthetic plastic.\n\nBaekeland was already wealthy due to his invention of Velox photographic paper when he began to investigate the reactions of phenol and formaldehyde in his home laboratory. Chemists had begun to recognize that many natural resins and fibers were polymers. Baekeland's initial intent was to find a replacement for shellac, a material in limited supply because it was made naturally from the excretion of lac insects (specifically \"Kerria lacca\"). Baekeland produced a soluble phenol-formaldehyde shellac called \"Novolak\", but it was not a market success.\n\nBaekeland then began experimenting on strengthening wood by impregnating it with a synthetic resin, rather than coating it. By controlling the pressure and temperature applied to phenol and formaldehyde, Baekeland produced a hard moldable material that he named \"Bakelite\", after himself. It was the first synthetic thermosetting plastic produced, and Baekeland speculated on \"the thousand and one ... articles\" it could be used to make. Baekeland considered the possibilities of using a wide variety of filling materials, including cotton, powdered bronze, and slate dust, but was most successful with wood and asbestos fibers.\n\nBaekeland filed a substantial number of patents in the area. Bakelite, his \"method of making insoluble products of phenol and formaldehyde,\" was filed on July 13, 1907, and granted on December 7, 1909. Baekeland also filed for patent protection in other countries, including Belgium, Canada, Denmark, Hungary, Japan, Mexico, Russia, and Spain. He announced his invention at a meeting of the American Chemical Society on February 5, 1909.\n\nBaekeland started semi-commercial production of his new material in his home laboratory, marketing it as a material for electrical insulators. By 1910, he was producing enough material to justify expansion. He formed the General Bakelite Company as a U.S. company to manufacture and market his new industrial material. He also made overseas connections to produce materials in other countries.\n\nBijker gives a detailed discussion of the development of Bakelite and the Bakelite company's production of various applications of materials. As of 1911, the company's main focus was laminating varnish, whose sales volume vastly outperformed both molding material and cast resin. By 1912, molding material was gaining ground, but its sales volume for the company did not exceed that of laminating varnish until the 1930s.\n\nAs the sales figures also show, the Bakelite Company produced \"transparent\" cast resin (which did not include filler) for a small ongoing market during the 1910s and 1920s. Blocks or rods of cast resin, also known as \"artificial amber\", were machined and carved to create items such as pipe stems, cigarette holders and jewelry. However, the demand for molded plastics led the Bakelite company to concentrate on molding, rather than concentrating on cast solid resins.\n\nThe Bakelite Corporation was formed in 1922 after patent litigation favorable to Baekeland, from a merger of three companies: Baekeland's General Bakelite Company; the Condensite Company, founded by J.W. Aylesworth; and the Redmanol Chemical Products Company, founded by L.V. Redman. Under director of advertising and public relations Allan Brown, who came to Bakelite from Condensite, Bakelite was aggressively marketed as \"the material of a thousand uses\". A filing for a trademark featuring the letter B above the mathematical symbol for infinity was made August 25, 1925, and claimed the mark was in use as of December 1, 1924. A wide variety of uses were listed in their trademark applications.\n\nThe first issue of \"Plastics\" magazine, October 1925, featured Bakelite on its cover, and included the article \"Bakelite – What It Is\" by Allan Brown. The range of colors available included \"black, brown, red, yellow, green, gray, blue, and blends of two or more of these\". The article emphasized that Bakelite came in various forms. \"Bakelite is manufactured in several forms to suit varying requirements. In all these forms the fundamental basis is the initial Bakelite resin. This variety includes clear material, for jewelry, smokers' articles, etc.; cement, using in sealing electric light bulbs in metal bases; varnishes, for impregnating electric coils, etc.; lacquers, for protecting the surface of hardware; enamels, for giving resistive coating to industrial equipment; Laminated Bakelite, used for silent gears and insulation; and molding material, from which are formed innumerable articles of utility and beauty. The molding material is prepared ordinarily by the impregnation of cellulose substances with the initial 'uncured' resin.\" In a 1925 report, the United States Tariff Commission hailed the commercial manufacture of synthetic phenolic resin as \"distinctly an American achievement\", and noted that \"the publication of figures, however, would be a virtual disclosure of the production of an individual company\".\n\nIn England, Bakelite Limited, a merger of three British phenol formaldehyde resin suppliers (Damard Lacquer Company Limited of Birmingham, Mouldensite Limited of Darley Dale and Redmanol Chemical Products Company of London), was formed in 1926. A new Bakelite factory opened in Tyseley, Birmingham, around 1928. It was demolished in 1998.\n\nA new factory opened in Bound Brook, New Jersey, in 1931. In 1939, the companies were acquired by Union Carbide and Carbon Corporation. Union Carbide's phenolic resin business including the Bakelite and Bakelit registered trademarks are assigned to Hexion Inc.\n\nIn addition to the original Bakelite material, these companies eventually made a wide range of other products, many of which were marketed under the brand name \"Bakelite plastics\". These included other types of cast phenolic resins similar to Catalin, and urea-formaldehyde resins, which could be made in brighter colors than polyoxy­benzyl­methylene­glycol­anhydride.\n\nOnce Baekeland's heat and pressure patents expired in 1927, Bakelite Corporation faced serious competition from other companies. Because molded Bakelite incorporated fillers to give it strength, it tended to be made in concealing dark colors. In 1927, beads, bangles and earrings were produced by the Catalin company, through a different process which enabled them to introduce 15 new colors. Translucent jewelry, poker chips and other items made of phenolic resins were introduced in the 1930s or 1940s by the Catalin company under the Prystal name. The creation of marbled phenolic resins may also be attributable to the Catalin company.\n\nMaking Bakelite was a multi-stage process. It began with the heating of phenol and formaldehyde in the presence of a catalyst such as hydrochloric acid, zinc chloride, or the base ammonia. This created a liquid condensation product, referred to as \"Bakelite A\", which was soluble in alcohol, acetone, or additional phenol. Heated further, the product became partially soluble and could still be softened by heat. Sustained heating resulted in an \"insoluble hard gum\". However, the high temperatures required to create this tended to cause violent foaming of the mixture, which resulted in the cooled material being porous and breakable. Baekeland's innovative step was to put his \"last condensation product\" into an egg-shaped \"Bakelizer\". By heating it under pressure, at about , Baekeland was able to suppress the foaming that would otherwise occur. The resulting substance was extremely hard and both infusible and insoluble. \n\nMolded Bakelite forms in a condensation reaction of phenol and formaldehyde, with wood flour or asbestos fiber as a filler, under high pressure and heat in a time frame of a few minutes of curing. The result is a hard plastic material.\n\nBakelite's molding process had a number of advantages. Bakelite resin could be provided either as powder, or as preformed partially cured slugs, increasing the speed of the casting. Thermosetting resins such as Bakelite required heat and pressure during the molding cycle, but could be removed from the molding process without being cooled, again making the molding process faster. Also, because of the smooth polished surface that resulted, Bakelite objects required less finishing. Millions of parts could be duplicated quickly and relatively cheaply.\n\nAnother market for Bakelite resin was the creation of phenolic sheet materials. Phenolic sheet is a hard, dense material made by applying heat and pressure to layers of paper or glass cloth impregnated with synthetic resin. Paper, cotton fabrics, synthetic fabrics, glass fabrics and unwoven fabrics are all possible materials used in lamination. When heat and pressure are applied, polymerization transforms the layers into thermosetting industrial laminated plastic.\n\nBakelite phenolic sheet is produced in many commercial grades and with various additives to meet diverse mechanical, electrical and thermal requirements. Some common types include:\n\nBakelite has a number of important properties. It can be molded very quickly, decreasing production time. Moldings are smooth, retain their shape and are resistant to heat, scratches, and destructive solvents. It is also resistant to electricity, and prized for its low conductivity. It is not flexible.\n\nPhenolic resin products may swell slightly under conditions of extreme humidity or perpetual dampness. When rubbed or burnt, Bakelite has a distinctive, acrid, sickly-sweet or fishy odor.\n\nThese characteristics made Bakelite particularly suitable as a molding compound, an adhesive or binding agent, a varnish, and as a protective coating. Bakelite was particularly suitable for the emerging electrical and automobile industries because of its extraordinarily high resistance to electricity, heat and chemical action.\n\nThe earliest commercial use of Bakelite in the electrical industry was the molding of tiny insulating bushings, made in 1908 for the Weston Electrical Instrument Corporation by Richard W. Seabury of the Boonton Rubber Company. Bakelite was soon used for non-conducting parts of telephones, radios and other electrical devices, including bases and sockets for light bulbs and electron tubes, supports for any type of electrical components, automobile distributor caps and other insulators.\nBy 1912, it was being used to make billiard balls, since its elasticity and the sound it made were similar to ivory.\n\nDuring World War I, Bakelite was used widely, particularly in electrical systems. Important projects included the Liberty Motor, the wireless telephone and radio phone and the use of micarta-bakelite propellors in the NBS-1 bomber and the DH-4B aeroplane.\n\nBakelite's availability and ease and speed of molding helped to lower the costs and increase product availability so that both telephones and radios became common household consumer goods. It was also very important to the developing automobile industry. It was soon found in myriad other consumer products ranging from pipe stems and buttons to saxophone mouthpieces, cameras, early machine guns, and appliance casings.\n\nBeginning in the 1920s, it became a popular material for jewelry. Designer Coco Chanel included Bakelite bracelets in her costume jewelry collections. Designers such as Elsa Schiaparelli used it for jewelry and also for specially designed dress buttons. Later, Diana Vreeland, editor of Vogue, was enthusiastic about Bakelite. Bakelite was also used to make presentation boxes for Breitling watches. Jewelry designers such as Jorge Caicedo Montes De Oca still use vintage Bakelite materials to make designer jewelry.\n\nBy 1930, designer Paul T. Frankl considered Bakelite a \"Materia Nova\", \"expressive of our own age\". By the 1930s, Bakelite was used for game pieces like chessmen, poker chips, dominoes and mahjong sets. Kitchenware made with Bakelite, including canisters and tableware, was promoted for its resistance to heat and to chipping. In the mid-1930s, Northland marketed a line of skis with a black \"Ebonite\" base, a coating of Bakelite. By 1935, it was used in solid-body electric guitars Performers such as Jerry Byrd loved the tone of Bakelite guitars but found them difficult to keep in tune.\n\nThe British children's construction toy Bayko, launched in 1933, originally used Bakelite for many of its parts, and took its name from the material.\n\nDuring World War II, Bakelite was used in a variety of wartime equipment including pilot's goggles and field telephones. It was also used for patriotic wartime jewelry. In 1943, the thermosetting phenolic resin was even considered for the manufacture of coins, due to a shortage of traditional material. Bakelite and other non-metal materials were tested for usage for the one cent coin in the US before the Mint settled on zinc-coated steel.\n\nIn 1947, Dutch art forger Han van Meegeren was convicted of forgery, after chemist and curator Paul B. Coremans proved that a purported Vermeer contained Bakelite, which van Meegeren had used as a paint hardener.\n\nBakelite was sometimes used as a substitute for metal in the magazine, pistol grip, fore grip, hand guard, and butt stock of firearms. The AKM and some early AK-74 rifles are frequently mistakenly identified as using Bakelite, but most were made with AG-S4.\n\nBy the late 1940s, newer materials were superseding Bakelite in many areas. Phenolics are less frequently used in general consumer products today due to their cost and complexity of production and their brittle nature. They still appear in some applications where their specific properties are required, such as small precision-shaped components, molded disc brake cylinders, saucepan handles, electrical plugs, switches and parts for electrical irons, as well as in the area of inexpensive board and tabletop games produced in China, Hong Kong and India. Items such as billiard balls, dominoes and pieces for board games such as chess, checkers, and backgammon are constructed of Bakelite for its look, durability, fine polish, weight, and sound. Common dice are sometimes made of Bakelite for weight and sound, but the majority are made of a thermoplastic polymer such as acrylonitrile butadiene styrene (ABS).\nBakelite continues to be used for wire insulation, brake pads and related automotive components, and industrial electrical-related applications. Bakelite stock is still manufactured and produced in sheet, rod and tube form for industrial applications in the electronics, power generation and aerospace industries, and under a variety of commercial brand names.\n\nPhenolic resins have been commonly used in ablative heat shields. Soviet heatshields for ICBM warheads and spacecraft reentry consisted of asbestos textolite, impregnated with Bakelite. Bakelite is also used in the mounting of metal samples in metallography.\n\nBakelite items, particularly jewelry and radios, have become a popular collectible. The term \"Bakelite\" is sometimes used in the resale market to indicate various types of early plastics, including Catalin and Faturan, which may be brightly colored, as well as items made of Bakelite material.\n\nThe United States Patent and Trademark Office granted Baekeland a patent for a \"Method of making insoluble products of phenol and formaldehyde\" on December 7, 1909. Producing hard, compact, insoluble and infusible condensation products of phenols and formaldehyde marked the beginning of the modern plastics industry.\n\n\n\n"}
{"id": "17633222", "url": "https://en.wikipedia.org/wiki?curid=17633222", "title": "Chemically inert", "text": "Chemically inert\n\nIn chemistry, the term chemically inert is used to describe a substance that is not chemically reactive. Most Group 8 or 18 elements that appear in the last column of the periodic table (Helium, Neon, Argon, Krypton, Xenon and Radon) are classified as inert (or unreactive). These elements are stable in their naturally occurring form (gaseous form) and they are called inert gases. \n\nThe noble gases were previously known as \"inert gases\" because of their perceived lack of participation in any chemical reactions. The reason for this is that their outermost electron shells (valence shells) are completely filled, so that they have little tendency to gain or lose electrons. They are said to acquire a noble gas configuration, or a full electron configuration. \n\nIt is now known that most of these gases in fact \"do\" react to form chemical compounds, such as xenon tetrafluoride. Hence, they have been renamed to \"noble gases\". However, a large amount of energy is required to drive such reactions, usually in the form of heat, pressure, or radiation, often assisted by catalysts. The resulting compounds often need to be kept in moisture-free conditions at low temperatures to prevent rapid decomposition back into their elements.\n\nThe term \"inert\" may also be applied in a relative sense. For example, molecular nitrogen is an inert gas under ordinary conditions, existing as diatomic molecules, . The presence of a strong triple covalent bond in the molecule renders it unreactive under normal circumstances. Nevertheless, nitrogen gas does react with the alkali metal lithium to form compound lithium nitride (LiN), even under ordinary conditions. Under high pressures and temperatures and with the right catalysts, nitrogen becomes more reactive; the Haber process uses such conditions to produce ammonia from atmospheric nitrogen .\n\nInert atmospheres consisting of gases such as argon, nitrogen, or helium are commonly used in chemical reaction chambers and in storage containers for oxygen-sensitive or water-sensitive substances, to prevent unwanted reactions of these substances with oxygen or water. \n\nArgon is widely used in fluorescence tubes and low energy light bulbs. Argon gas helps to protect the metal filament inside the bulb from reacting with oxygen and corroding the filament under high temperature. \n\nNeon is used in making advertising signs. Neon gas in a vacuum tube glows bright red in colour when electricity is passed through. Different coloured neon lights can also be made by using other gases. \n\nHelium gas is mainly used to fill hot air and party balloons. Balloons filled with it float upwards and this phenomenon is achieved as helium gas is less dense than air \n"}
{"id": "4045168", "url": "https://en.wikipedia.org/wiki?curid=4045168", "title": "Chinese yam", "text": "Chinese yam\n\nChinese yam (\"Dioscorea polystachya\"), also called cinnamon-vine, is a species of flowering plant in the yam family. This perennial climbing vine native to China now grows throughout East Asia (Japan, Korea, Kuril Islands, Vietnam). It is believed to have been introduced to Japan in the 17th century or earlier. Introduced to the United States as early as the 19th century for culinary and cultural uses, it is now considered an invasive plant species. The plant was introduced to Europe in the 19th century during the European Potato Failure, where cultivation continues to this day for the Asian food market. The edible tubers, often called nagaimo or Chinese-potato, are cultivated largely in Asia and sometimes used in alternative medicine. This species of yam is unique as the tubers can be eaten raw.\n\nThe botanical names \"Dioscorea opposita\" and \"Dioscorea oppositifolia\" have been consistently misapplied to Chinese yam. The name \"D. opposita\" is now an accepted synonym of \"D. oppositifolia\". Botanical works that point out the error may list, e.g., \"Dioscorea opposita\" auct. as a synonym of \"D. polystachya\". Furthermore, neither \"D. oppositifolia\" nor the prior \"D. opposita\" have been found growing in North America and have no historical range in China or East Asia, this grouping is native only to the subcontinent of India and should not be confused with \"Dioscorea polystachya\".\n\n\"Dioscorea polystachya\" vines typically grow 3–5 meters long, but can be longer. They twine clockwise. The leaves are up to 11 centimeters long and wide. They are lobed at the base and larger ones may have lobed edges. The arrangement is variable; they may be alternately or oppositely arranged or borne in whorls. \nIn the leaf axils appear warty rounded bulbils under 2 centimeters long. The bulbils are sometimes informally referred to as \"yam berries\" or \"yamberries\".\nNew plants sprout from the bulbils or parts of them. \n\nThe flowers of Chinese yam are cinnamon-scented. \n\nThe plant produces one or more spindle-shaped or cylindrical tubers. The largest may weigh 10 pounds and grow one meter underground. \"Dioscorea polystachya\" is more tolerant to frost and cooler climates than other yams, which is attributed to its successful introductions and establishment on many continents.\n\nIn Chinese it is known as \"huáishān\" (怀(淮)山), \"shānyào\" (山药, 山藥) (lit. \"mountain medicine.\"), or \"huáishānyào\" (怀(淮)山药, 怀(淮)山藥)(lit. \"mountain medicine from Huai\", i.e. Huai Qing Fu (怀庆俯) region). Rarely, also referred to as \"shǔyù\" (薯蕷). The yam bulbils are referred to as \"shanyao dou\" 山药豆, i.e. \"yam beans\", or \"shanyao dan\" 山药蛋 (\"yam eggs\").\n\nIn Japanese, it is known as \"nagaimo\" (lit. 'long yam'; kanji: 長芋). Furthermore, \"nagaimo\" is classified into \"ichōimo\" (lit. 'ginkgo-leaf yam'; kanji: 銀杏芋), or \"yamatoimo\" (lit. Yamato yam; kanji: 大和芋), depending on root shapes. In Japan, this species is sometimes mistakenly called Japanese mountain yam, which refers to the native Dioscorea japonica.\n\nIn Korea it is called \"ma\" (hangul: 마), \"sanu (山芋, 산우)\", seoyeo (薯蕷, 서여), or sanyak (山藥, 산약) and in Sri Lanka in Sinhala it is called \"wal ala\" (වැල් අල). Sometimes called Korean yam.\n\nIn Vietnam, the yam is called \"củ mài\" or \"khoai mài\". When this yam is processed to become a medicine, the yam is called \"hoài sơn\" or \"tỳ giải\".\n\nIn the Ilocano of the northern Philippines it is called \"tuge\".\n\nIn Latin American countries it's known as white name or white ñame.\n\nIn Manipuri it is called as \"Ha\".\n\nCreams and dietary supplements made from the related \"Dioscorea villosa\" are claimed to contain human hormones and promoted as a medicine for a variety of purposes, including cancer prevention and the treatment of Crohn's disease and whooping cough. However, according to the American Cancer Society, the claims are false and there is no evidence to support these substances being either safe or effective. Huáishān has also been used in traditional Chinese medicine.\n\n\"Dioscorea polystachya\" was introduced to the United States in the 1800s when it was planted as an ornamental or food crop. It and other introduced yam species now grow wild there. It is troublesome in Great Smoky Mountains National Park, where its range is \"rapidly expanding\". It is most prevalent in moist habitat types. It is more tolerant of frost than other yams and can occur in temperate climates as far north as New York.\n\nThe tubers of \"D. polystachya\" can be eaten raw, while other yams must be cooked before consumption (due to harmful substances in the raw state). In Japanese cuisine, it is eaten raw and grated, after only a relatively minimal preparation: the whole tubers are briefly soaked in a vinegar-water solution, to neutralize irritant oxalate crystals found in their skin. The raw vegetable is starchy and bland, mucilaginous when grated, and may be eaten plain as a side dish, or added to noodles.\n\nIt is used in the Japanese noodle dish \"tororo udon/soba\" and as a binding agent in the batter of okonomiyaki. The grated yam is known as \"tororo\" (in Japanese). In \"tororo udon/soba\", the \"tororo\" is mixed with other ingredients that typically include \"tsuyu\" broth (dashi), wasabi, and green onions.\n\nThe Huáishān growing cycle spans approximately one year, and should be planted between winter and spring. The traditional methods growing it are: using smaller tubers, top cut of bigger tubers or through cuttings of branches. The first two methods can produce 20 cm (7.8 in) long tubers and above. The latter produces smaller tubers (10 cm or 4 in) that are usually replanted for the next year.\n\nBetween 7 and 9 months of replanting huáishān seedlings, their leaves start to get dry (a common fact in plants that grow tubers): that indicates that it's time to harvest. In home gardens generally only what will be consumed is harvested, with the rest left in the pot in moist soil.\n\n\n"}
{"id": "36184015", "url": "https://en.wikipedia.org/wiki?curid=36184015", "title": "Clean Energy Regulator", "text": "Clean Energy Regulator\n\nThe Clean Energy Regulator (CER) is an Australian independent statutory authority responsible for administering legislation to reduce carbon emissions and increase the use of clean energy. The CER, based in Canberra, was established on 2 April 2012 under the \"Clean Energy Regulator Act 2011\". The agency is part of the Environment and Energy portfolio.\n\nThe CER has administrative responsibilities in relation to the renewable energy target, the Emissions Reduction Fund, the National Greenhouse and Energy Reporting Scheme and the Australian National Registry of Emissions Units (ANREU). The National Greenhouse and Energy Reporting Scheme provides a national framework for carbon accounting, energy production and energy consumption.\n\nThe CER administers the Renewable Energy Target's two schemes: \n\nThe Emissions Reduction Fund is a voluntary scheme that aims to provide incentives for a range of organisations and individuals to adopt new practices and technologies to reduce their emissions.\n\nOn 23 June 2015 the CER Act was amended to implement the Government's changes to the renewable energy target. The new target for large-scale generation was 33,000 gigawatt hours in 2020. This will result in more than 23.5% of Australia's electricity being derived from renewable sources by 2020. The required gigawatt hours of renewable source electricity from 2017 to 2019 were also adjusted to reflect the new target.\n\nTo help track investment in new renewable energy capacity, the CER prepares an annual statement on progress of the scheme towards meeting the new target and the impact it is having on household electricity bills.\n\nThe CER tabled the 2016 Renewable Energy Target Administrative Report and Annual Statement. The report covers the operations of the \"Renewable Energy (Electricity) Act 2000\" for the 2016 calendar year and the Renewable Energy Target 2016 Annual statement and supporting information about progress towards meeting the revised 2020 large-scale renewable energy target.\n"}
{"id": "27920400", "url": "https://en.wikipedia.org/wiki?curid=27920400", "title": "Dan W. Reicher", "text": "Dan W. Reicher\n\nDan William Reicher is an American lawyer who was U.S. Assistant Secretary of Energy for Energy Efficiency and Renewable Energy at the U.S. Department of Energy (DOE) in the Clinton Administration. Reicher is currently Executive Director of the Steyer-Taylor Center for Energy Policy and Finance at Stanford University, a joint center of the Stanford Graduate School of Business and Stanford Law School, where he also holds faculty positions. Reicher joined Stanford in 2011 from Google, where he served since 2007 as Director of Climate Change and Energy Initiatives for the company’s venture Google.org.\n\nReicher also served as an advisor to the 2008 Obama campaign and a member of the Obama Transition Team where he focused on the energy portions of the Obama stimulus package.\n\nFollowing the 2008 U.S. presidential election, Reicher was considered for the post of Energy Secretary in the Obama Administration but physicist Steven Chu was ultimately chosen.\n\nFollowing his tenure in the Clinton Administration, Reicher was as an energy investor with a private equity firm he co-founded, and where he served as President. He was also a cleantech executive with a venture capital-backed renewable energy company. Early in his career he served as a staff member of President Carter’s Commission on the Accident at Three Mile Island.\n\nIn 2012 Reicher received an honorary doctorate from the State University of New York College of Environmental Science and Forestry and was also named one of the five most influential figures in U.S. clean energy by Oilprice.com.\n\nReicher directs Stanford’s Steyer-Taylor Center for Energy Policy and Finance, which focuses on policy and finance mechanisms that advance clean energy in an economically-sensible manner. He also co-teaches a graduate-level class on the business fundamentals and policy dimensions of cleantech and a seminar on the Chinese solar industry and its global implications.\n\nReicher is also a member of the Secretary of Energy Advisory Board, appointed by current Energy Secretary Ernest Moniz, the National Academy of Sciences Board on Energy and Environmental Systems, co-chairman of the board of the American Council on Renewable Energy, and a member of the board of American Rivers and the American Council for an Energy Efficient Economy.\n\nAt Google, Reicher served as Director of Climate Change and Energy Initiatives for the company's venture Google.org which was capitalized with more than $1 billion of Google stock to make investments, advance policy, and develop products in the areas of climate change and energy, health, and global development. In the energy and climate area, Google.org developed home energy monitoring software called Google PowerMeter, worked on plug-in vehicles under the RechargeIT initiative, and made investments and advocated policies to lower the cost of renewable energy.\n\nPrior to his position at Google, Reicher served as President and Co-founder of New Energy Capital, a private equity firm funded by the California State Teachers Retirement System and Vantage Point Venture Partners to invest in clean energy projects. During Reicher’s tenure, the firm invested in projects such as natural gas-fired cogeneration plants in California and Massachusetts, ethanol plants in Indiana and Michigan, a biodiesel plant in Delaware, and a biomass-fired power plant in Maine. Reicher also served as Executive Vice President of Northern Power Systems, one of the nation’s oldest renewable energy companies. The company received significant venture capital investment, including from Nth Power and Arete Corporation to develop new technologies, including a permanent-magnet direct-drive 2.3 megawatt wind turbine.\n\nReicher was also an adjunct professor at the Yale University School of Forestry and Environmental Studies and Vermont Law School.\n\nFrom 1997-2001, Reicher was Assistant Secretary of Energy for Energy Efficiency and Renewable Energy at DOE. As Assistant Secretary, he directed annually more than $1 billion in investments in energy research, development and deployment related to renewable energy, distributed generation, and energy efficiency. During Reicher’s tenure, DOE launched the Wind Powering America Initiative and the Geopowering the West Initiative, adopted new appliance and equipment efficiency standards, and oversaw the Million Solar Roofs Initiative and the Industries of the Future Initiative. Reicher was a member of the U.S. Delegation to the Climate Change Negotiations, Co-Chair of the U.S Biomass Research and Development Board, and a member of the board of the government-industry Partnership for a New Generation of Vehicles.\n\nPrior to his position as DOE Assistant Secretary, Reicher was Chief of Staff to Energy Secretary Hazel O'Leary (1996-1997), Assistant Secretary for Policy and International Affairs (Acting, 1995-1996), and Deputy Chief of Staff and Counselor to the Secretary (1993-1995).\n\nAfter leaving the Clinton Administration in 2001, he was a consultant to the Senate Committee on Environment and Public Works and a Visiting Fellow at the World Resources Institute.\n\nPrior to his roles at the Department of Energy and in the business community, Reicher was a senior attorney with the Natural Resources Defense Council where he focused on the federal government's energy and nuclear programs as well as environmental law and policy issues in the former Soviet Union. He was also previously Assistant Attorney General for Environmental Protection in Massachusetts, a law clerk to federal district court judge David Nelson in Boston, a legal assistant in the Hazardous Waste Section of the United States Department of Justice and a staff member of President Carter's Commission on the Accident at Three Mile Island.\n\nReicher was born in Syracuse, New York, United States. His late father, Norbert B. Reicher, was a physician and his mother is a retired teacher. Reicher graduated with a B.A. in Biology from Dartmouth College and a J.D. from Stanford Law School. He also studied at the Harvard’s Kennedy School of Government and MIT. He was a member of the first group on record to kayak the Yangtze River in China and a National Geographic-sponsored expedition that was the first to navigate the entire 1888 mile Rio Grande. Reicher’s college friend Senator Rob Portman (R-OH) was a member of both expeditions.\n\nReicher is married to Carole Parker, who headed the Office of Pollution Prevention at the U.S. Department of Defense from 1994 to 1999. Carole and Dan have three children and live in Piedmont, California.\n"}
{"id": "30791232", "url": "https://en.wikipedia.org/wiki?curid=30791232", "title": "Decorative laminate", "text": "Decorative laminate\n\nDecorative laminates are laminated products primarily used as furniture surface materials or wall paneling. It can be manufactured as either high- or low-pressure laminate, with the two processes not much different from each other except for the pressure applied in the pressing process.\n\nAccording to McGraw-Hill Dictionary of Architecture & Construction, high-pressure laminates consists of laminates \"molded and cured at pressures not lower than and more commonly in the range of .\"\n\nHPL is made of resin impregnated cellulose layers, which are consolidated under heat and high pressure. The various layers are described below:\n\nTrade names include Formica, Arborite, Micarta, Consoweld, Alpikord and Duropal.\n\nAfter the papers are impregnated with the resins, the three layers of paper/resin are placed into a press which simultaneously applies heat (120 °C) and pressure (5 MPa). The pressing operation allows the thermoset resins to flow into the paper, then subsequently cure into a consolidated sheet with a density greater than . During the press cycle, the decorative surface can also be cured while in contact with a textured surface to create one of many different surface finishes.\n\nHPL consists of more than 60 to 70% paper, with the remaining 30 to 40% a combination of phenol-formaldehyde resin for the core layers and melamine-formaldehyde resin for the surface layer. Both resins belong to a class of thermosetting resins which crosslink during the press cycle creating irreversible chemical bonds that produce a nonreactive, stable material with characteristics different and superior to those of the component parts.\n\nHPL can be produced using both continuous and discontinuous (batch) manufacturing processes. HPL are supplied in sheet form in a variety of sizes, thicknesses and surface finishes.\n\nLow Pressure laminate is defined as \"a plastic laminate molded and cured at pressures in general of .\"\n\nThere are various industrial standards specifically applied for high-pressure decorative laminates:\n\nThe European Standard EN438 is one of the standards that most decorative laminates manufacturers selling to worldwide market (such as Greenlam, New Mika, Samrat, Resopal, Polyrey, Abet Laminati, Dekodur Laminating Technologie, Violam, Formica, Maica, Wilsonart,) adhere to. The specific code is EN438, entitled: Decorative high-pressure laminates (HPL) sheets based on thermosetting resins, specifications. It replaced all other national European standards.\n\nThe specific part of EN438 which applies to high-pressure laminates is Part 3. The full title to this standard is: \"High-pressure decorative laminates (HPL) Sheets based on thermosetting resins (Usually called laminates) Part 3: Classification and specifications for laminates less than 2 mm thick intended for bonding to supporting substrates\". In total there are 9 parts to the EN438.\n\nDecorative laminates are grouped into the following types according to EN 438:\n\nProduct specifications applicable to HPL include the nine parts of EN 438 and the two parts of ISO 4586 as shown below:\n\nAntibacterial properties are important for decorative laminates because these laminates are used as kitchen tops and counter tops, cabinets and table tops that may be in constant contact with food materials and younger children. Antibacterial properties are there to ensure that bacterial growth is minimal.\n\nOne of the standards for Anti-Bacterial is the ISO 22196:2007, which is based on the Japanese Industrial Standards (JIS), code Z2801. This is one of the standards most often referred to in the industry with regards to tests on microbial activities (specifically bacteria) and in the JIS Z2801, two bacteria species are used as a standard, namely \"E. Coli\" and \"Staphylococcus aureus\". However, some companies may have the initiative to test more than just these two bacteria and may also replace \"Staphylococcus aureus\" with MRSA, the methicillin-resistant version of the same bacteria.\n\nAgain, different countries may choose to specify different types of microbes for testing especially if they identified some bacteria groups which are more intimidating in their countries due to specific reasons.\n\nA common anti-fungi standard is the ASTM G21-09. Not all manufacturers will take the initiatives for product R&D for anti-Fungi attributes. Manufacturers like Maica Laminates send their products for laboratory tests for certification following the ASTM G21-09 standard, while Formica (South America) partners with Microban Protection, which is a company manufacturing additives, including the anti-bacterial additives.\n\nThere are many different standards with regards to fire-resistant and flame-retardant properties of high-pressure decorative laminates. While different countries may have different standards for the building industry to adhere to, most countries may agree on some of the more common standards being used in the industry. Very often, just like other standards applicable to the industry, the tests may be European Standards with their equivalent in the US Standards.\n\nFor example, many Commonwealth countries may be comfortable with the British Standards 476 especially Parts 6 and 7, while there will still be US Standard equivalence in the ASTM.\n\nThe list of tests applicable to decorative laminates will never be exhaustive. As the technology improves, there will be many more tests to ensure the safety of the products upon use by the end consumer, for example perhaps the tests on transfer of surface substance to food materials if prepared on the decorative laminates as a kitchen surface. The core tests will then also branch out based on the specific requirements and standards adopted by different countries.\n\nTwo of the internationally acknowledged \"Green\" certificates for decorative laminates are MAS Certified Green and GREENGUARD. The MAS Certified Green and GREENGUARD marks are to certify that the products have low chemical emissions. Chemicals tested include VOCs, formaldehyde and other harmful particles. The tests are based on single occupancy room with outdoor ventilation following the ANSI/ASHRAE Standard 62.1-2007, \"Ventilation for Acceptable Indoor Air Quality\". GREENGUARD especially, has two main consideration, GREENGUARD and GREENGUARD GOLD. The GREENGUARD n GOLD was previously known as the GREENGUARD Children and Schools Certified, signifying its relevance of very low allowable chemical emissions levels to ensure the safety of young children and school environment.\n\nThere are also many other \"Green\" certifications, some which are requirements by the authorities before the product can be used as building materials. These include the \"Singapore Green Label\" which is recognised by the Global Ecolabelling Network (GEN) and all its member countries.\n\nDecorative high-pressure laminates are usually used for furniture tops especially on flat surfaces, including cabinets and tables. Decorative compact laminates are sometimes constructed as toilet cubicle systems, laboratory tables and kitchen tops. Some new usage models include wall panels with conceptual designs and custom prints.\n\nThe popularity of large format printing using inkjet printers have given a cheaper alternative to decorative laminates, minus the quality. For most uninformed consumers, the large format printing are similar to laminates, and seem to offer more variety of designs and applications. For example, large format prints can be printed on wall stickers, and then installed on walls. Unlike decorative laminates, there are no special adhesive to be used, and the price may sometimes seem so much cheaper comparatively.\n\nHowever, there are health considerations for large format prints because of the solvent inks used, especially with their relatively high concentrations of VOCs.\n\nhttp://www.samratply.in <br>\nhttp://www.royaletouche.com <br>\nhttp://www.greenlamindustries.com/ <br>\nhttp://www.northernlam.com/ <br>\nhttp://www.formica.com/en/ca/products/formica-laminate-home#swatchesTab\n\nhttp://www.icdli.com\n"}
{"id": "486372", "url": "https://en.wikipedia.org/wiki?curid=486372", "title": "Electrodeless plasma excitation", "text": "Electrodeless plasma excitation\n\nElectrodeless plasma excitation methods include helicon plasma sources, inductively coupled plasmas, and surface-wave-sustained discharges.\n\nElectrodeless high-frequency discharges (HF) have two important advantages over plasmas using electrodes, like capacitively coupled plasmas that are of great interest for contemporary plasma research and applications:\n\n"}
{"id": "8898050", "url": "https://en.wikipedia.org/wiki?curid=8898050", "title": "Electron tomography", "text": "Electron tomography\n\nElectron tomography (ET) is a tomography technique for obtaining detailed 3D structures of sub-cellular macro-molecular objects. Electron tomography is an extension of traditional transmission electron microscopy and uses a transmission electron microscope to collect the data. In the process, a beam of electrons is passed through the sample at incremental degrees of rotation around the center of the target sample. This information is collected and used to assemble a three-dimensional image of the target. For biological applications, the typical resolution of ET systems are in the 5–20 nm range, suitable for examining supra-molecular multi-protein structures, although not the secondary and tertiary structure of an individual protein or polypeptide.\n\nIn the field of biology, bright-field transmission electron microscopy (BF-TEM) and high-resolution TEM (HRTEM) are the primary imaging methods for tomography tilt series acquisition. However, there are two issues associated with BF-TEM and HRTEM. First, acquiring an interpretable 3-D tomogram requires that the projected image intensities vary monotonically with material thickness. This condition is difficult to guarantee in BF/HRTEM, where image intensities are dominated by phase-contrast with the potential for multiple contrast reversals with thickness, making it difficult to distinguish voids from high-density inclusions. Second, the contrast transfer function of BF-TEM is essentially a high-pass filter – information at low spatial frequencies is significantly suppressed – resulting in an exaggeration of sharp features. However, the technique of annular dark-field scanning transmission electron microscopy (ADF-STEM), which is typically used on material specimens, more effectively suppresses phase and diffraction contrast, providing image intensities that vary with the projected mass-thickness of samples up to micrometres thick for materials with low atomic number. ADF-STEM also acts as a low-pass filter, eliminating the edge-enhancing artifacts common in BF/HRTEM. Thus, provided that the features can be resolved, ADF-STEM tomography can yield a reliable reconstruction of the underlying specimen which is extremely important for its application in material science. For 3D imaging, the resolution is traditionally described by the Crowther criterion. In 2010, a 3D resolution of 0.5±0.1×0.5±0.1×0.7±0.2 nm was achieved with a single-axis ADF-STEM tomography. Recently, atomic resolution in 3D electron tomography reconstructions has been demonstrated. ADF-STEM tomography has recently been used to directly visualize the atomic structure of screw dislocations in nanoparticles.\n\nThe most popular tilting methods are the single-axis and the dual-axis tilting methods. The geometry of most specimen holders and electron microscopes normally precludes tilting the specimen through a full 180° range, which can lead to artifacts in the 3D reconstruction of the target. By using dual-axis tilting, the reconstruction artifacts are reduced by a factor of formula_1 compared to single-axis tilting. However, twice as many images need to be taken. Another method of obtaining a tilt-series is the so-called conical tomography method, in which the sample is tilted, and then rotated a complete turn.\n\n"}
{"id": "9649", "url": "https://en.wikipedia.org/wiki?curid=9649", "title": "Energy", "text": "Energy\n\nIn physics, energy is the quantitative property that must be transferred to an object in order to perform work on, or to heat, the object. Energy is a conserved quantity; the law of conservation of energy states that energy can be converted in form, but not created or destroyed. The SI unit of energy is the joule, which is the energy transferred to an object by the work of moving it a distance of 1 metre against a force of 1 newton.\n\nCommon forms of energy include the kinetic energy of a moving object, the potential energy stored by an object's position in a force field (gravitational, electric or magnetic), the elastic energy stored by stretching solid objects, the chemical energy released when a fuel burns, the radiant energy carried by light, and the thermal energy due to an object's temperature.\n\nMass and energy are closely related. Due to mass–energy equivalence, any object that has mass when stationary (called rest mass) also has an equivalent amount of energy whose form is called rest energy, and any additional energy (of any form) acquired by the object above that rest energy will increase the object's total mass just as it increases its total energy. For example, after heating an object, its increase in energy could be measured as a small increase in mass, with a sensitive enough scale.\n\nLiving organisms require available energy to stay alive, such as the energy humans get from food. Human civilization requires energy to function, which it gets from energy resources such as fossil fuels, nuclear fuel, or renewable energy. The processes of Earth's climate and ecosystem are driven by the radiant energy Earth receives from the sun and the geothermal energy contained within the earth.\n\nThe total energy of a system can be subdivided and classified into potential energy, kinetic energy, or combinations of the two in various ways. Kinetic energy is determined by the movement of an object – or the composite motion of the components of an object – and potential energy reflects the potential of an object to have motion, and generally is a function of the position of an object within a field or may stored in the field itself.\n\nWhile these two categories are sufficient to describe all forms of energy, it is often convenient to refer to particular combinations of potential and kinetic energy as its own form. For example, macroscopic mechanical energy is the sum of translational and rotational kinetic and potential energy in a system neglects the kinetic energy due to temperature, and nuclear energy which combines utilize potentials from the nuclear force and the weak force), among others.\n\nThe word \"energy\" derives from the , which possibly appears for the first time in the work of Aristotle in the 4th century BC. In contrast to the modern definition, energeia was a qualitative philosophical concept, broad enough to include ideas such as happiness and pleasure.\n\nIn the late 17th century, Gottfried Leibniz proposed the idea of the , or living force, which defined as the product of the mass of an object and its velocity squared; he believed that total \"vis viva\" was conserved. To account for slowing due to friction, Leibniz theorized that thermal energy consisted of the random motion of the constituent parts of matter, although it would be more than a century until this was generally accepted. The modern analog of this property, kinetic energy, differs from \"vis viva\" only by a factor of two.\n\nIn 1807, Thomas Young was possibly the first to use the term \"energy\" instead of \"vis viva\", in its modern sense. Gustave-Gaspard Coriolis described \"kinetic energy\" in 1829 in its modern sense, and in 1853, William Rankine coined the term \"potential energy\". The law of conservation of energy was also first postulated in the early 19th century, and applies to any isolated system. It was argued for some years whether heat was a physical substance, dubbed the caloric, or merely a physical quantity, such as momentum. In 1845 James Prescott Joule discovered the link between mechanical work and the generation of heat.\n\nThese developments led to the theory of conservation of energy, formalized largely by William Thomson (Lord Kelvin) as the field of thermodynamics. Thermodynamics aided the rapid development of explanations of chemical processes by Rudolf Clausius, Josiah Willard Gibbs, and Walther Nernst. It also led to a mathematical formulation of the concept of entropy by Clausius and to the introduction of laws of radiant energy by Jožef Stefan. According to Noether's theorem, the conservation of energy is a consequence of the fact that the laws of physics do not change over time. Thus, since 1918, theorists have understood that the law of conservation of energy is the direct mathematical consequence of the translational symmetry of the quantity conjugate to energy, namely time.\n\nIn 1843, James Prescott Joule independently discovered the mechanical equivalent in a series of experiments. The most famous of them used the \"Joule apparatus\": a descending weight, attached to a string, caused rotation of a paddle immersed in water, practically insulated from heat transfer. It showed that the gravitational potential energy lost by the weight in descending was equal to the internal energy gained by the water through friction with the paddle.\n\nIn the International System of Units (SI), the unit of energy is the joule, named after James Prescott Joule. It is a derived unit. It is equal to the energy expended (or work done) in applying a force of one newton through a distance of one metre. However energy is also expressed in many other units not part of the SI, such as ergs, calories, British Thermal Units, kilowatt-hours and kilocalories, which require a conversion factor when expressed in SI units.\n\nThe SI unit of energy rate (energy per unit time) is the watt, which is a joule per second. Thus, one joule is one watt-second, and 3600 joules equal one watt-hour. The CGS energy unit is the erg and the imperial and US customary unit is the foot pound. Other energy units such as the electronvolt, food calorie or thermodynamic kcal (based on the temperature change of water in a heating process), and BTU are used in specific areas of science and commerce.\n\nIn classical mechanics, energy is a conceptually and mathematically useful property, as it is a conserved quantity. Several formulations of mechanics have been developed using energy as a core concept.\n\nWork, a function of energy, is force times distance.\n\nThis says that the work (formula_2) is equal to the line integral of the force F along a path \"C\"; for details see the mechanical work article. Work and thus energy is frame dependent. For example, consider a ball being hit by a bat. In the center-of-mass reference frame, the bat does no work on the ball. But, in the reference frame of the person swinging the bat, considerable work is done on the ball.\n\nThe total energy of a system is sometimes called the Hamiltonian, after William Rowan Hamilton. The classical equations of motion can be written in terms of the Hamiltonian, even for highly complex or abstract systems. These classical equations have remarkably direct analogs in nonrelativistic quantum mechanics.\n\nAnother energy-related concept is called the Lagrangian, after Joseph-Louis Lagrange. This formalism is as fundamental as the Hamiltonian, and both can be used to derive the equations of motion or be derived from them. It was invented in the context of classical mechanics, but is generally useful in modern physics. The Lagrangian is defined as the kinetic energy \"minus\" the potential energy. Usually, the Lagrange formalism is mathematically more convenient than the Hamiltonian for non-conservative systems (such as systems with friction).\n\nNoether's theorem (1918) states that any differentiable symmetry of the action of a physical system has a corresponding conservation law. Noether's theorem has become a fundamental tool of modern theoretical physics and the calculus of variations. A generalisation of the seminal formulations on constants of motion in Lagrangian and Hamiltonian mechanics (1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian; for example, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n\nIn the context of chemistry, energy is an attribute of a substance as a consequence of its atomic, molecular or aggregate structure. Since a chemical transformation is accompanied by a change in one or more of these kinds of structure, it is invariably accompanied by an increase or decrease of energy of the substances involved. Some energy is transferred between the surroundings and the reactants of the reaction in the form of heat or light; thus the products of a reaction may have more or less energy than the reactants. A reaction is said to be exergonic if the final state is lower on the energy scale than the initial state; in the case of endergonic reactions the situation is the reverse. Chemical reactions are invariably not possible unless the reactants surmount an energy barrier known as the activation energy. The \"speed\" of a chemical reaction (at given temperature \"T\") is related to the activation energy \"E\", by the Boltzmann's population factor ethat is the probability of molecule to have energy greater than or equal to \"E\" at the given temperature \"T\". This exponential dependence of a reaction rate on temperature is known as the Arrhenius equation.The activation energy necessary for a chemical reaction can be in the form of thermal energy.\n\nIn biology, energy is an attribute of all biological systems from the biosphere to the smallest living organism. Within an organism it is responsible for growth and development of a biological cell or an organelle of a biological organism. Energy is thus often said to be stored by cells in the structures of molecules of substances such as carbohydrates (including sugars), lipids, and proteins, which release energy when reacted with oxygen in respiration. In human terms, the human equivalent (H-e) (Human energy conversion) indicates, for a given amount of energy expenditure, the relative quantity of energy needed for human metabolism, assuming an average human energy expenditure of 12,500 kJ per day and a basal metabolic rate of 80 watts. For example, if our bodies run (on average) at 80 watts, then a light bulb running at 100 watts is running at 1.25 human equivalents (100 ÷ 80) i.e. 1.25 H-e. For a difficult task of only a few seconds' duration, a person can put out thousands of watts, many times the 746 watts in one official horsepower. For tasks lasting a few minutes, a fit human can generate perhaps 1,000 watts. For an activity that must be sustained for an hour, output drops to around 300; for an activity kept up all day, 150 watts is about the maximum. The human equivalent assists understanding of energy flows in physical and biological systems by expressing energy units in human terms: it provides a \"feel\" for the use of a given amount of energy.\n\nSunlight's radiant energy is also captured by plants as \"chemical potential energy\" in photosynthesis, when carbon dioxide and water (two low-energy compounds) are converted into the high-energy compounds carbohydrates, lipids, and proteins. Plants also release oxygen during photosynthesis, which is utilized by living organisms as an electron acceptor, to release the energy of carbohydrates, lipids, and proteins. Release of the energy stored during photosynthesis as heat or light may be triggered suddenly by a spark, in a forest fire, or it may be made available more slowly for animal or human metabolism, when these molecules are ingested, and catabolism is triggered by enzyme action.\n\nAny living organism relies on an external source of energy – radiant energy from the Sun in the case of green plants, chemical energy in some form in the case of animals – to be able to grow and reproduce. The daily 1500–2000 Calories (6–8 MJ) recommended for a human adult are taken as a combination of oxygen and food molecules, the latter mostly carbohydrates and fats, of which glucose (CHO) and stearin (CHO) are convenient examples. The food molecules are oxidised to carbon dioxide and water in the mitochondria\nand some of the energy is used to convert ADP into ATP.\nThe rest of the chemical energy in O and the carbohydrate or fat is converted into heat: the ATP is used as a sort of \"energy currency\", and some of the chemical energy it contains is used for other metabolism when ATP reacts with OH groups and eventually splits into ADP and phosphate (at each stage of a metabolic pathway, some chemical energy is converted into heat). Only a tiny fraction of the original chemical energy is used for work:\n\nIt would appear that living organisms are remarkably inefficient (in the physical sense) in their use of the energy they receive (chemical or radiant energy), and it is true that most real machines manage higher efficiencies. In growing organisms the energy that is converted to heat serves a vital purpose, as it allows the organism tissue to be highly ordered with regard to the molecules it is built from. The second law of thermodynamics states that energy (and matter) tends to become more evenly spread out across the universe: to concentrate energy (or matter) in one specific place, it is necessary to spread out a greater amount of energy (as heat) across the remainder of the universe (\"the surroundings\"). Simpler organisms can achieve higher energy efficiencies than more complex ones, but the complex organisms can occupy ecological niches that are not available to their simpler brethren. The conversion of a portion of the chemical energy to heat at each step in a metabolic pathway is the physical reason behind the pyramid of biomass observed in ecology: to take just the first step in the food chain, of the estimated 124.7 Pg/a of carbon that is fixed by photosynthesis, 64.3 Pg/a (52%) are used for the metabolism of green plants, i.e. reconverted into carbon dioxide and heat.\n\nIn geology, continental drift, mountain ranges, volcanoes, and earthquakes are phenomena that can be explained in terms of energy transformations in the Earth's interior, while meteorological phenomena like wind, rain, hail, snow, lightning, tornadoes and hurricanes are all a result of energy transformations brought about by solar energy on the atmosphere of the planet Earth.\n\nSunlight may be stored as gravitational potential energy after it strikes the Earth, as (for example) water evaporates from oceans and is deposited upon mountains (where, after being released at a hydroelectric dam, it can be used to drive turbines or generators to produce electricity). Sunlight also drives many weather phenomena, save those generated by volcanic events. An example of a solar-mediated weather event is a hurricane, which occurs when large unstable areas of warm ocean, heated over months, give up some of their thermal energy suddenly to power a few days of violent air movement.\n\nIn a slower process, radioactive decay of atoms in the core of the Earth releases heat. This thermal energy drives plate tectonics and may lift mountains, via orogenesis. This slow lifting represents a kind of gravitational potential energy storage of the thermal energy, which may be later released to active kinetic energy in landslides, after a triggering event. Earthquakes also release stored elastic potential energy in rocks, a store that has been produced ultimately from the same radioactive heat sources. Thus, according to present understanding, familiar events such as landslides and earthquakes release energy that has been stored as potential energy in the Earth's gravitational field or elastic strain (mechanical potential energy) in rocks. Prior to this, they represent release of energy that has been stored in heavy atoms since the collapse of long-destroyed supernova stars created these atoms.\n\nIn cosmology and astronomy the phenomena of stars, nova, supernova, quasars and gamma-ray bursts are the universe's highest-output energy transformations of matter. All stellar phenomena (including solar activity) are driven by various kinds of energy transformations. Energy in such transformations is either from gravitational collapse of matter (usually molecular hydrogen) into various classes of astronomical objects (stars, black holes, etc.), or from nuclear fusion (of lighter elements, primarily hydrogen). The nuclear fusion of hydrogen in the Sun also releases another store of potential energy which was created at the time of the Big Bang. At that time, according to theory, space expanded and the universe cooled too rapidly for hydrogen to completely fuse into heavier elements. This meant that hydrogen represents a store of potential energy that can be released by fusion. Such a fusion process is triggered by heat and pressure generated from gravitational collapse of hydrogen clouds when they produce stars, and some of the fusion energy is then transformed into sunlight.\n\nIn quantum mechanics, energy is defined in terms of the energy operator\nas a time derivative of the wave function. The Schrödinger equation equates the energy operator to the full energy of a particle or a system. Its results can be considered as a definition of measurement of energy in quantum mechanics. The Schrödinger equation describes the space- and time-dependence of a slowly changing (non-relativistic) wave function of quantum systems. The solution of this equation for a bound system is discrete (a set of permitted states, each characterized by an energy level) which results in the concept of quanta. In the solution of the Schrödinger equation for any oscillator (vibrator) and for electromagnetic waves in a vacuum, the resulting energy states are related to the frequency by Planck's relation: formula_3 (where formula_4 is Planck's constant and formula_5 the frequency). In the case of an electromagnetic wave these energy states are called quanta of light or photons.\n\nWhen calculating kinetic energy (work to accelerate a massive body from zero speed to some finite speed) relativistically – using Lorentz transformations instead of Newtonian mechanics – Einstein discovered an unexpected by-product of these calculations to be an energy term which does not vanish at zero speed. He called it rest energy: energy which every massive body must possess even when being at rest. The amount of energy is directly proportional to the mass of the body:\n\nwhere\n\nFor example, consider electron–positron annihilation, in which the rest energy of these two individual particles (equivalent to their rest mass) is converted to the radiant energy of the photons produced in the process. In this system the matter and antimatter (electrons and positrons) are destroyed and changed to non-matter (the photons). However, the total mass and total energy do not change during this interaction. The photons each have no rest mass but nonetheless have radiant energy which exhibits the same inertia as did the two original particles. This is a reversible process – the inverse process is called pair creation – in which the rest mass of particles is created from the radiant energy of two (or more) annihilating photons.\n\nIn general relativity, the stress–energy tensor serves as the source term for the gravitational field, in rough analogy to the way mass serves as the source term in the non-relativistic Newtonian approximation.\n\nEnergy and mass are manifestations of one and the same underlying physical property of a system. This property is responsible for the inertia and strength of gravitational interaction of the system (\"mass manifestations\"), and is also responsible for the potential ability of the system to perform work or heating (\"energy manifestations\"), subject to the limitations of other physical laws.\n\nIn classical physics, energy is a scalar quantity, the canonical conjugate to time. In special relativity energy is also a scalar (although not a Lorentz scalar but a time component of the energy–momentum 4-vector). In other words, energy is invariant with respect to rotations of space, but not invariant with respect to rotations of space-time (= boosts).\n\nEnergy may be transformed between different forms at various efficiencies. Items that transform between these forms are called transducers. Examples of transducers include a battery, from chemical energy to electric energy; a dam: gravitational potential energy to kinetic energy of moving water (and the blades of a turbine) and ultimately to electric energy through an electric generator; or a heat engine, from heat to work.\n\nExamples of energy transformation include generating electric energy from heat energy via a steam turbine, or lifting an object against gravity using electrical energy driving a crane motor. Lifting against gravity performs mechanical work on the object and stores gravitational potential energy in the object. If the object falls to the ground, gravity does mechanical work on the object which transforms the potential energy in the gravitational field to the kinetic energy released as heat on impact with the ground. Our Sun transforms nuclear potential energy to other forms of energy; its total mass does not decrease due to that in itself (since it still contains the same total energy even if in different forms), but its mass does decrease when the energy escapes out to its surroundings, largely as radiant energy.\n\nThere are strict limits to how efficiently heat can be converted into work in a cyclic process, e.g. in a heat engine, as described by Carnot's theorem and the second law of thermodynamics. However, some energy transformations can be quite efficient. The direction of transformations in energy (what kind of energy is transformed to what other kind) is often determined by entropy (equal energy spread among all available degrees of freedom) considerations. In practice all energy transformations are permitted on a small scale, but certain larger transformations are not permitted because it is statistically unlikely that energy or matter will randomly move into more concentrated forms or smaller spaces.\n\nEnergy transformations in the universe over time are characterized by various kinds of potential energy that has been available since the Big Bang later being \"released\" (transformed to more active types of energy such as kinetic or radiant energy) when a triggering mechanism is available. Familiar examples of such processes include nuclear decay, in which energy is released that was originally \"stored\" in heavy isotopes (such as uranium and thorium), by nucleosynthesis, a process ultimately using the gravitational potential energy released from the gravitational collapse of supernovae, to store energy in the creation of these heavy elements before they were incorporated into the solar system and the Earth. This energy is triggered and released in nuclear fission bombs or in civil nuclear power generation. Similarly, in the case of a chemical explosion, chemical potential energy is transformed to kinetic energy and thermal energy in a very short time. Yet another example is that of a pendulum. At its highest points the kinetic energy is zero and the gravitational potential energy is at maximum. At its lowest point the kinetic energy is at maximum and is equal to the decrease of potential energy. If one (unrealistically) assumes that there is no friction or other losses, the conversion of energy between these processes would be perfect, and the pendulum would continue swinging forever.\n\nEnergy is also transferred from potential energy (formula_8) to kinetic energy (formula_9) and then back to potential energy constantly. This is referred to as conservation of energy. In this closed system, energy cannot be created or destroyed; therefore, the initial energy and the final energy will be equal to each other. This can be demonstrated by the following:\n\nThe equation can then be simplified further since formula_10 (mass times acceleration due to gravity times the height) and formula_11 (half mass times velocity squared). Then the total amount of energy can be found by adding formula_12.\n\nEnergy gives rise to weight when it is trapped in a system with zero momentum, where it can be weighed. It is also equivalent to mass, and this mass is always associated with it. Mass is also equivalent to a certain amount of energy, and likewise always appears associated with it, as described in mass-energy equivalence. The formula \"E\" = \"mc\"², derived by Albert Einstein (1905) quantifies the relationship between rest-mass and rest-energy within the concept of special relativity. In different theoretical frameworks, similar formulas were derived by J.J. Thomson (1881), Henri Poincaré (1900), Friedrich Hasenöhrl (1904) and others (see Mass-energy equivalence#History for further information).\n\nPart of the rest energy (equivalent to rest mass) of matter may be converted to other forms of energy (still exhibiting mass), but neither energy nor mass can be destroyed; rather, both remain constant during any process. However, since formula_13 is extremely large relative to ordinary human scales, the conversion of an everyday amount of rest mass (for example, 1 kg) from rest energy to other forms of energy (such as kinetic energy, thermal energy, or the radiant energy carried by light and other radiation) can liberate tremendous amounts of energy (~formula_14 joules = 21 megatons of TNT), as can be seen in nuclear reactors and nuclear weapons. Conversely, the mass equivalent of an everyday amount energy is minuscule, which is why a loss of energy (loss of mass) from most systems is difficult to measure on a weighing scale, unless the energy loss is very large. Examples of large transformations between rest energy (of matter) and other forms of energy (e.g., kinetic energy into particles with rest mass) are found in nuclear physics and particle physics.\n\nThermodynamics divides energy transformation into two kinds: reversible processes and irreversible processes. An irreversible process is one in which energy is dissipated (spread) into empty energy states available in a volume, from which it cannot be recovered into more concentrated forms (fewer quantum states), without degradation of even more energy. A reversible process is one in which this sort of dissipation does not happen. For example, conversion of energy from one type of potential field to another, is reversible, as in the pendulum system described above. In processes where heat is generated, quantum states of lower energy, present as possible excitations in fields between atoms, act as a reservoir for part of the energy, from which it cannot be recovered, in order to be converted with 100% efficiency into other forms of energy. In this case, the energy must partly stay as heat, and cannot be completely recovered as usable energy, except at the price of an increase in some other kind of heat-like increase in disorder in quantum states, in the universe (such as an expansion of matter, or a randomisation in a crystal).\n\nAs the universe evolves in time, more and more of its energy becomes trapped in irreversible states (i.e., as heat or other kinds of increases in disorder). This has been referred to as the inevitable thermodynamic heat death of the universe. In this heat death the energy of the universe does not change, but the fraction of energy which is available to do work through a heat engine, or be transformed to other usable forms of energy (through the use of generators attached to heat engines), grows less and less.\n\nThe fact that energy can be neither created nor be destroyed is called the law of conservation of energy. In the form of the first law of thermodynamics, this states that a closed system's energy is constant unless energy is transferred in or out by work or heat, and that no energy is lost in transfer. The total inflow of energy into a system must equal the total outflow of energy from the system, plus the change in the energy contained within the system. Whenever one measures (or calculates) the total energy of a system of particles whose interactions do not depend explicitly on time, it is found that the total energy of the system always remains constant.\n\nWhile heat can always be fully converted into work in a reversible isothermal expansion of an ideal gas, for cyclic processes of practical interest in heat engines the second law of thermodynamics states that the system doing work always loses some energy as waste heat. This creates a limit to the amount of heat energy that can do work in a cyclic process, a limit called the available energy. Mechanical and other forms of energy can be transformed in the other direction into thermal energy without such limitations. The total energy of a system can be calculated by adding up all forms of energy in the system.\n\nRichard Feynman said during a 1961 lecture:\nMost kinds of energy (with gravitational energy being a notable exception) are subject to strict local conservation laws as well. In this case, energy can only be exchanged between adjacent regions of space, and all observers agree as to the volumetric density of energy in any given space. There is also a global law of conservation of energy, stating that the total energy of the universe cannot change; this is a corollary of the local law, but not vice versa.\n\nThis law is a fundamental principle of physics. As shown rigorously by Noether's theorem, the conservation of energy is a mathematical consequence of translational symmetry of time, a property of most phenomena below the cosmic scale that makes them independent of their locations on the time coordinate. Put differently, yesterday, today, and tomorrow are physically indistinguishable. This is because energy is the quantity which is canonical conjugate to time. This mathematical entanglement of energy and time also results in the uncertainty principle - it is impossible to define the exact amount of energy during any definite time interval. The uncertainty principle should not be confused with energy conservation - rather it provides mathematical limits to which energy can in principle be defined and measured.\n\nEach of the basic forces of nature is associated with a different type of potential energy, and all types of potential energy (like all other types of energy) appears as system mass, whenever present. For example, a compressed spring will be slightly more massive than before it was compressed. Likewise, whenever energy is transferred between systems by any mechanism, an associated mass is transferred with it.\n\nIn quantum mechanics energy is expressed using the Hamiltonian operator. On any time scales, the uncertainty in the energy is by\n\nwhich is similar in form to the Heisenberg Uncertainty Principle (but not really mathematically equivalent thereto, since \"H\" and \"t\" are not dynamically conjugate variables, neither in classical nor in quantum mechanics).\n\nIn particle physics, this inequality permits a qualitative understanding of virtual particles which carry momentum, exchange by which and with real particles, is responsible for the creation of all known fundamental forces (more accurately known as fundamental interactions). Virtual photons (which are simply lowest quantum mechanical energy state of photons) are also responsible for electrostatic interaction between electric charges (which results in Coulomb law), for spontaneous radiative decay of exited atomic and nuclear states, for the Casimir force, for van der Waals bond forces and some other observable phenomena.\n\nEnergy transfer can be considered for the special case of systems which are closed to transfers of matter. The portion of the energy which is transferred by conservative forces over a distance is measured as the work the source system does on the receiving system. The portion of the energy which does not do work during the transfer is called heat. Energy can be transferred between systems in a variety of ways. Examples include the transmission of electromagnetic energy via photons, physical collisions which transfer kinetic energy, and the conductive transfer of thermal energy.\n\nEnergy is strictly conserved and is also locally conserved wherever it can be defined. In thermodynamics, for closed systems, the process of energy transfer is described by the first law:\n\nwhere formula_16 is the amount of energy transferred, formula_2  represents the work done on the system, and formula_18 represents the heat flow into the system. As a simplification, the heat term, formula_18, is sometimes ignored, especially when the thermal efficiency of the transfer is high.\n\nThis simplified equation is the one used to define the joule, for example.\n\nBeyond the constraints of closed systems, open systems can gain or lose energy in association with matter transfer (both of these process are illustrated by fueling an auto, a system which gains in energy thereby, without addition of either work or heat). Denoting this energy by formula_16, one may write\n\nInternal energy is the sum of all microscopic forms of energy of a system. It is the energy needed to create the system. It is related to the potential energy, e.g., molecular structure, crystal structure, and other geometric aspects, as well as the motion of the particles, in form of kinetic energy. Thermodynamics is chiefly concerned with changes in internal energy and not its absolute value, which is impossible to determine with thermodynamics alone.\n\nThe first law of thermodynamics asserts that energy (but not necessarily thermodynamic free energy) is always conserved and that heat flow is a form of energy transfer. For homogeneous systems, with a well-defined temperature and pressure, a commonly used corollary of the first law is that, for a system subject only to pressure forces and heat transfer (e.g., a cylinder-full of gas) without chemical changes, the differential change in the internal energy of the system (with a \"gain\" in energy signified by a positive quantity) is given as\n\nwhere the first term on the right is the heat transferred into the system, expressed in terms of temperature \"T\" and entropy \"S\" (in which entropy increases and the change d\"S\" is positive when the system is heated), and the last term on the right hand side is identified as work done on the system, where pressure is \"P\" and volume \"V\" (the negative sign results since compression of the system requires work to be done on it and so the volume change, d\"V\", is negative when work is done on the system).\n\nThis equation is highly specific, ignoring all chemical, electrical, nuclear, and gravitational forces, effects such as advection of any form of energy other than heat and pV-work. The general formulation of the first law (i.e., conservation of energy) is valid even in situations in which the system is not homogeneous. For these cases the change in internal energy of a \"closed\" system is expressed in a general form by\n\nwhere formula_23 is the heat supplied to the system and formula_24 is the work applied to the system.\n\nThe energy of a mechanical harmonic oscillator (a mass on a spring) is alternatively kinetic and potential. At two points in the oscillation cycle it is entirely kinetic, and at two points it is entirely potential. Over the whole cycle, or over many cycles, net energy is thus equally split between kinetic and potential. This is called equipartition principle; total energy of a system with many degrees of freedom is equally split among all available degrees of freedom.\n\nThis principle is vitally important to understanding the behaviour of a quantity closely related to energy, called entropy. Entropy is a measure of evenness of a distribution of energy between parts of a system. When an isolated system is given more degrees of freedom (i.e., given new available energy states that are the same as existing states), then total energy spreads over all available degrees equally without distinction between \"new\" and \"old\" degrees. This mathematical result is called the second law of thermodynamics. The second law of thermodynamics is valid only for systems which are near or in equilibrium state. For non-equilibrium systems, the laws governing system’s behavior are still debatable. One of the guiding principles for these systems is the principle of maximum entropy production. It states that nonequilibrium systems behave in such a way to maximize its entropy production.\n\n\n"}
{"id": "21875097", "url": "https://en.wikipedia.org/wiki?curid=21875097", "title": "Gasometer Oberhausen", "text": "Gasometer Oberhausen\n\nThe Gasometer Oberhausen is a former gas holder in Oberhausen, Germany, which has been converted into an exhibition space. It has hosted several large scale exhibitions, including two by Christo and Jeanne-Claude. The Gasometer is an industrial landmark, and an anchor point of the European Route of Industrial Heritage and the Industrial Heritage Trail. It was built in the 1920s, and reconstructed after World War II.\n\nIn the 1920s the coal and steel industry in the Ruhrgebiet produced blast furnace gas and coal gas as a by-product of iron production and coking, while the steel industry as well as coking used large amounts these gasses or alternative fuels. As supply and demand of gas varied independently, sometimes excess gas had to be flared off, while at other times additional fuel had to be purchased. The Gasometer was built as a buffer: storing excess gas and releasing it again when demand exceeded production.\n\nThe Gasometer was built by Gutehoffnungshütte, by the side of the Rhine-Herne Canal. Construction started 27 February 1927 and cost 1.74 million Reichsmark. A framework of 24 steel girders was built on a concrete base, and a skin of 5mm thick sheet metal was riveted to the framework. Inside, a 1,207,000 kg pressure disc was mounted which could freely move up and down, floating on top of the gas underneath and keeping it at a constant pressure. 15 May 1929 the Gasometer was first put into operation, with a maximum capacity of 347,000m³, a height of 117.5m and diameter of 67.6m.\nDuring World War II, the Gasometer was hit by bombs several times, but kept operating. When it was shelled by allied forces it did not explode, but the gas burned up and the pressure disc slowly descended. The Gasometer officially stopped operating 31 December 1944. It was completely disassembled after it had caught fire during repair work on 10 June 1946. Reconstruction began 1949 using the original pressure disc and roof. By 1 June 1950 the Gasometer was operational again.\n\nIn 1977 the Gasometer was repainted, at a cost of 3.5 million DM. In later years many coking plants and iron works closed, reducing supply as well as demand for the gas stored in the Gasometer. In addition, natural gas became cheaper. The Gasometer became superfluous and in 1988 it was decommissioned by its owner, Ruhrkohle AG.\n\nA discussion ensued about the dismantling or possible reuse of the Gasometer. In 1992 the city council of Oberhausen, with a margin of 1 vote decided to acquire the building and convert it to an exhibition space. At the time, plans were being developed for building shopping mall (CentrO) on an adjacent plot, and Internationale Bauausstellung Emscher Park planned to use the Gasometer for its exhibition. Ownership transferred to the city of Oberhausen, with Ruhrkohle AG paying 1.8 million DM in saved demolition costs to the city.\n\nConversion and restoration were done by Deutsche Babcock AG in 1993–1994. The former pressure disc was fixed at 4.5m height, with a 3000m² exhibition space on the ground floor below. The main exhibition space, on top of the pressure disc, was fitted with a stage and seating for 500 people. Lifts and stairs were fitted to provide visitors access to the roof. Conversion cost approximately DM 16 million.\n\nThe exhibition \"Fire and Flame\" documented the history of the coal and iron industry in the Ruhr area and its influence on society. It attracted about 460,000 visitors.\n\n\"I Phoenix\" was an exhibition of contemporary art. It attracted 96,000 visitors.\n\nThe exhibition \"The Dream of Vision\" dealt with the history of television.\n\n\"The Wall\" was organised as part of the IBA Emscher Park, at a cost of nearly 4 million DM. It was an installation by Christo and Jeanne-Claude, consisting of a stack of 13,000 oil drums in 7 colours, 68m wide, 26m high, and 7m deep, and weighed 234,000 kg. It attracted 390,000 visitors.\n\nThis exhibition was organised for the centenary of the German Football Association in 2000, and documented the history of football. It had 216,000 visitors.\n\n\"Blue Gold\" dealt with the subject of water. It included 833,000 kg of sand. An installation by Paul Schütze included a 50m high cone of water in an artificial lake, and video projections.\n\nThis was a video installation by Bill Viola. It had 140,000 visitors.\n\nThis exhibition dealt with Brian Jones and Bertrand Piccard's 1999 round–the–world balloon flight, and it displayed their high Breitling Orbiter 3 balloon inside the Gasometer.\n\n\"Fire Light Sky\" was an installation of sound and light by Christina Kubisch, combined with an exhibition about the history of the Gasometer.\n\nThis exhibition was a cooperation with DLR. It showed large satellite images of the earth and various objects to do with space exploration.\n\nThis exhibition about space and the solar system was organised to mark the International Year of Astronomy. It was a project of Ruhr.2010 and ran from 2 April 2009 till 30 December 2010. Part of the show was a model of the Moon, 25m in diameter.\n\nThis was an exhibition about natural and cultural monuments of the world.\n\n\"Big Air Package\" (16 March 2013 – 30 December 2013) was one of the last three projects started by Christo with Jeanne-Claude, before Jeanne-Claude's death in 2009. It consisted of an envelope made of 20,350m² of semitransparent fabric and 4,500m of rope, weighing 5,300 kg. Inflated to a volume of 177,000m³, it was 90m high, 50m wide and was pressurised at 27 pascal over atmospheric pressure by two fans. Visitors could enter the installation through airlocks, and walk around inside the Gasometer.\n\n"}
{"id": "11190302", "url": "https://en.wikipedia.org/wiki?curid=11190302", "title": "Gran Gasoducto del Sur", "text": "Gran Gasoducto del Sur\n\nGran Gasoducto del Sur (also known as Venezuela-Argentina Gas Line) was a proposed 8,000-15,000-kilometer (5,000-9,000 mi) long natural gas pipeline to connect Venezuela, Brazil and Argentina. The overall project cost was expected to be around US$17-23 billion.\n\nOn 9 December 2005, during an annual meeting of Mercosur in Montevideo, the presidents of Venezuela, Argentina, and Brazil signed an accord for the construction of the pipeline. \n\nIn June 2007 Petrobras president Jose Sergio Gabrielli said that the pipeline would not become operational for 20 to 30 years. In July 2007 Venezuelan President Hugo Chávez said that interest in building the pipeline had cooled.\nThe exact route of the pipeline was never defined. It was only disclosed that the first section was to run from Puerto Ordaz, Venezuela, through Rondônia, Amazonas and Amapá states in Brazil to Marabá, Pará state Brazil. In Manaus, Amazonas State, the pipeline was to be connected to the Urucu–Manaus pipeline and the Urucu-Porto Velho pipeline. From Marabá, a branch line to Fortaleza, Ceará State was foreseen. In Fortaleza the pipeline can be connected to the existing system that extends along the coast to Salvador, Bahia State to be connected with the GASENE pipeline.\n\nThe second long section from Marabá was to continue southward to São Paulo State. From there, the pipeline was to be routed by the section to the border between Rio Grande do Sul State and Uruguay and then cross Uruguay to Argentina.\n\nThe Puerto Ordaz-Marabá section was planned to have a diameter and to have 13 compressor stations of 25,000 hp each. The diameter of the Marabá-Fortaleza branch as planned would gradually decrease from to and the branch project involves the installation of five 15,000 hp compression stations. The gas amount received in Fortaleza was planned to be 12.75 billion cubic meter (bcm) of natural gas per annum. The Marabá-São Paulo section was to be in diameter and to include eight 20,000 hp compression stations. The gas delivered to São Paulo was planned to be 15 bcm per annum. The Brazil-Argentina section with capacity of 18 bcm per annum was to include eight 15,000 hp compressor stations.\n\nThe project was to be financed by Petróleos de Venezuela S.A., Development Bank of Latin America, and Caixa Econômica Federal with possible participation of other public institutions and private investors.\n"}
{"id": "28644889", "url": "https://en.wikipedia.org/wiki?curid=28644889", "title": "Hexachloroplatinate", "text": "Hexachloroplatinate\n\nHexachloroplatinate is an anion with the chemical formula [PtCl].\n\nChemical compounds containing the hexachloroplatinate anion include:\n\n"}
{"id": "24844789", "url": "https://en.wikipedia.org/wiki?curid=24844789", "title": "Holboca Power Station", "text": "Holboca Power Station\n\nThe Holboca Power Station is a large thermal power plant located in Holboca, having 2 generation groups of 50 MW each having a total electricity generation capacity of 100 MW.\n"}
{"id": "52820398", "url": "https://en.wikipedia.org/wiki?curid=52820398", "title": "Hurewicz space", "text": "Hurewicz space\n\nIn mathematics, a Hurewicz space is a topological space that satisfies a certain basic selection principle that generalizes σ-compactness. A Hurewicz space is a space in which for every sequence of open covers formula_1 of the space there are finite sets formula_2 such that every point of the space belongs to all but finitely many sets formula_3 .\n\nIn 1926, Witold Hurewicz introduced the above property of topological spaces that is formally stronger than the Menger property. He didn't know whether Menger's conjecture is true, and whether his property is strictly stronger than the Menger property, but he conjectured that in the class of metric spaces his property is equivalent to formula_4-compactness.\n\nHurewicz conjectured that in ZFC every Hurewicz metric space is σ-compact. Just, Miller, Scheepers, and Szeptycki proved that Hurewicz's conjecture is false, by showing that there is, in ZFC, a set of real numbers that is Menger but not σ-compact. Their proof was dichotomic, and the set witnessing the failure of the conjecture heavily depends on whether a certain (undecidable) axiom holds or not.\n\nBartoszyński and Shelah (see also Tsaban's solution based on their work ) gave a uniform ZFC example of a Hurewicz subset of the real line that is not σ-compact.\n\nHurewicz asked whether in ZFC his property is strictly stronger than the Menger property. In 2002, Chaber and Pol in unpublished note, using dichotomy proof, showed that there is a Hurewicz subset of the real line that is not Menger. In 2008, Tsaban and Zdomskyy gave a uniform example of a Hurewicz subset of the real line that is Menger but not Hurewicz.\n\nFor subsets of the real line, the Hurewicz property can be characterized using continuous functions into the Baire space formula_5. For functions formula_6, write formula_7 if formula_8 for all but finitely many natural numbers formula_9. A subset formula_10 of formula_5 is bounded if there is a function formula_12such that formula_13 for all functions formula_14. A subset of formula_5 is unbounded if it is not bounded. Hurewicz proved that a subset of the real line is Hurewicz iff every continuous image of that space into the Baire space is unbounded. In particular, every subset of the real line of cardinality less than the bounding number formula_16 is Hurewicz.\n\nLet formula_17 be a topological space. The Hurewicz game played on formula_17 is a game with two players Alice and Bob.\n\n1st round: Alice chooses an open cover formula_19 of formula_17. Bob chooses a finite set formula_21.\n\n2nd round: Alice chooses an open cover formula_22 of formula_17. Bob chooses a finite set formula_24.\n\netc.\n\nIf every point of the space formula_17 belongs to all but finitely many sets formula_3 , then Bob wins the Hurewicz game. Otherwise, Alice wins.\n\nA player has a winning strategy if he knows how to play in order to win the game (formally, a winning strategy is a function).\n\nA topological space is Hurewicz iff Alice has no winning strategy in the Hurewicz game played on this space.\n\nA Tychonoff space formula_17 is Hurewicz iff for every compact space formula_29 containing the space formula_17, and a formula_27 subset G of formula_29 containing the space formula_17, there is a formula_4-compact set formula_35 with formula_36.\n\n"}
{"id": "40600688", "url": "https://en.wikipedia.org/wiki?curid=40600688", "title": "Ivlia (ship)", "text": "Ivlia (ship)\n\nIvlia (bireme) is a modern reconstruction of an ancient Greek rowing warship (galley) with oars at two levels and an important example of experimental archaeology. Between 1989 and 1994, this vessel undertook six comprehensive international historical and geographical expeditions in the footsteps of the ancient seafarers.\n\nThe main sponsor of construction of the ship was the Black Sea Shipping Company. \n\nThe ship was constructed in 1989 at the Sochi Naval Shipyard, by a team under shipwright Damir S. Shkhalakhov, as well as the active participation of the future crew members. Ivlia was built from Durmast oak and Siberian larch, the oars are of beech. Technical design of the project was carried out by specialists of the Nikolayev University of Shipbuilding. After processing the available scientific data (ancient illustrations on vases and reliefs, written and archaeological sources) members of the Odessa Archaeological Museum, under the leadership of prof. PhD Vladimir N. Stanko, proposed the building of a bireme since in antiquity it had been the most widely used vessel in the northern Black Sea region.\n\nStarting from Odessa (Ukraine) in 1989, \"Ivlia\" followed the routes of the ancient mariners on the Black Sea and the Mediterranean as well as the Atlantic, covering more than 3.000 nautical miles in six expedition seasons and visiting over 50 European ports, finally sailing up the river Seine to reach Paris. \nThe expedition's progress was widely covered by the international media. During the time of the voyage, hundreds of articles were published, along with dozens of TV and radio reports. The ship was regularly visited by official delegations and thousands of tourists. Ivlia also took part in international maritime festivals: «Colombo'92» (Genoa, Italy), «Brest’92», «Cancal’93», «Whitbread Round the World Race '93» (Portsmouth) «Vieux Greements’94» (France). The radio constantly broadcast the expedition’s call sign. Over six seasons the crew members included more than 200 people – citizens of Russia, Ukraine, Moldova, France, Greece and Georgia.\n\nThe research programme of the expedition was developed by the authors of the project, together with the staff of the Odessa Archaeological Museum and the Nikolayev University of Shipbuilding, primarily to address the following objectives:\nThe practical experience gained on Ivlia's expeditions enabled the project authors to affirm:\nIn addition, the research programme conducted on board Ivlia included the participation of the Institute of Biology of the Southern Seas, the Institute of Water Transport Hygiene, and other scientific centres of Ukraine. During the expedition, regular measurements were made of environmental parameters and the level of pollution of the sea water, assessments of the state of marine flora and fauna, and a variety of medical experiments were conducted.\nThe data obtained during the six years of voyages are summarized in the articles and books subsequently published by the authors of the project.\nOn the whole, Ivlia’s journey around Europe was a bright page in the study of Hellenic shipbuilding and seafaring, and also contributed to the strengthening of international cultural relations.\n\n\n"}
{"id": "10686210", "url": "https://en.wikipedia.org/wiki?curid=10686210", "title": "Jupiter mass", "text": "Jupiter mass\n\nJupiter mass, also called Jovian mass, is the unit of mass equal to the total mass of the planet Jupiter. This value may refer to the mass of the planet alone, or the mass of the entire Jovian system to include the moons of Jupiter. Jupiter is by far the most massive planet in the Solar System. It is approximately 2.5 times more massive than all of the other planets in the Solar System combined.\n\nJupiter mass is a common unit of mass in astronomy that is used to indicate the masses of other similarly-sized objects, including the outer planets and extrasolar planets. It may also be used to describe the masses of brown dwarfs, as this unit provides a convenient scale for comparison.\n\nThe current best known value for the mass of Jupiter can be expressed as :\nwhich is about 1000 times less massive than the sun (about ):\nin scientific notation,\nCompared to the mass of Earth, Jupiter is 318 times more massive.\n\nJupiter's mass is 2.5 times that of all the other planets in the Solar System combined—this is so massive that its barycenter with the Sun lies beyond the Sun's surface at 1.068 solar radii from the Sun's center.\n\nBecause the mass of Jupiter is so large compared to the other objects in the solar system, the effects of its gravity must be included when calculating satellite trajectories and the precise orbits of other bodies in the solar system, including Earth's moon and even Pluto.\n\nTheoretical models indicate that if Jupiter had much more mass than it does at present, its atmosphere would collapse, and the planet would shrink. For small changes in mass, the radius would not change appreciably, but above about (1.6 Jupiter masses) the interior would become so much more compressed under the increased pressure that its volume would \"decrease\" despite the increasing amount of matter. As a result, Jupiter is thought to have about as large a diameter as a planet of its composition and evolutionary history can achieve. The process of further shrinkage with increasing mass would continue until appreciable stellar ignition was achieved, as in high-mass brown dwarfs having around 50 Jupiter masses. Jupiter would need to be about 75 times more massive to fuse hydrogen and become a star.\n\nThe mass of Jupiter is derived from the measured value called the Jovian mass parameter, which is denoted with \"GM\". The mass of Jupiter is calculated by dividing \"GM\" by the constant \"G\". For celestial bodies such as Jupiter, Earth and the Sun, the value of the \"GM\" product is known to many orders of magnitude more precisely than either factor independently. The limited precision available for \"G\" limits the uncertainty of the derived mass. For this reason, astronomers often prefer to refer to the gravitational parameter, rather than the explicit mass. The \"GM\" products are used when computing the ratio of Jupiter mass relative to other objects.\n\nIn 2015, the International Astronomical Union defined the \"nominal Jovian mass parameter\" to remain constant regardless of subsequent improvements in measurement precision of . This constant is defined as exactly:\nIf the explicit mass of Jupiter is needed in SI units, it can be calculated in terms of the Gravitational constant, \"G\" by dividing \"GM\" by \"G\".\n\nThe majority of Jupiter's mass is hydrogen and helium. These two elements make up more than 87% of the total mass of Jupiter. The total mass of heavy elements other than hydrogen and helium in the planet is between 11 and . The bulk of the hydrogen on Jupiter is solid hydrogen. Evidence suggests that Jupiter contains a central dense core. If so, the mass of the core is predicted to be no larger than about . The exact mass of the core is uncertain due to the relatively poor knowledge of the behavior of solid hydrogen at very high pressures.\n\n"}
{"id": "32843728", "url": "https://en.wikipedia.org/wiki?curid=32843728", "title": "Kano air disaster", "text": "Kano air disaster\n\nThe Kano air disaster was a chartered Boeing 707 passenger flight on 22 January 1973 which crashed while attempting to land at Kano International Airport. It is the worst aviation disaster ever to take place in Nigeria, as 176 passengers and crew perished in the crash. There were 26 survivors.\n\nThe aircraft involved in the accident was a 2 year old Boeing 707-3D3C, JY-ADO, owned by Alia Royal Jordanian Airlines, operating on behalf of Nigeria Airways. It first flew in 1971 and was powered by 4 Pratt and Whitney JT3D engines. It had a manufacturer serial number (MSN) of 850.\n\nThe Boeing 707, operated by Alia, had been chartered by Nigeria Airways to fly pilgrims back from Jeddah, Saudi Arabia to Lagos, Nigeria. Bad weather at Lagos caused the crew to divert to Kano. Kano International Airport was experiencing high winds at the time. The aircraft landed nose wheel first, and the nose wheel collapsed after hitting a depression in the runway. The right main landing gear leg subsequently collapsed. The 707 turned 180 degrees, excursed from the runway and caught fire.\n\nOf the 202 passengers and crew on board, 176 died and only 26 survived. At the time it occurred, the Kano air disaster was the deadliest-ever aviation accident, a dubious distinction it only held for about 14 months when Turkish Airlines Flight 981 crashed in France killing 346 people.\n"}
{"id": "24188106", "url": "https://en.wikipedia.org/wiki?curid=24188106", "title": "Kashira Power Plant", "text": "Kashira Power Plant\n\nKashira Power Plant is a coal-fired power plant at Kashira in Moscow Oblast, Russia. Its first unit was commissioned in 1922 with a power capacity of 110 MW. As of today, it has an installed power capacity of 1,910 MW and consists of 6 units. Double units 1 and 2 have capacity of 300 MW, and single units 4, 5 have capacity of 300 MW each, unit 6 has capacity of 330 MW. In addition, unit 7 has thermal capacity of 80 MW.\n\nIn 1951 a HVDC link with 30 MW built from the components of Elbe-Project to Moscow was built. However it is not in service any more. The power plant has an interesting feature as one of its two main chimneys serves as electricity pylon.\n\n\n"}
{"id": "23456604", "url": "https://en.wikipedia.org/wiki?curid=23456604", "title": "Klydonograph", "text": "Klydonograph\n\nThe Klydonograph is a device that records a surge in electrical voltage on a sulphur-dusted photographic film. The device is credited to John F. Peters, who pursued the idea as a means of investigating the effects of lightning on electric power lines. The resulting graphic varies in size and shape as a function of the potential, polarity, and wave shape of the captured lightning discharge.\n\nImaging an electrical impulse with sulphur dust was documented in 1777 by Dr. G. C Lichtenberg, and this idea was further developed by others (to include a photographic plate, for example) before being adopted by the Klydonograph.\n\nKlydonograph is generally used to record impulse voltages between 2 kV and 50 kV.\n"}
{"id": "37506729", "url": "https://en.wikipedia.org/wiki?curid=37506729", "title": "List of electricity organisations in India", "text": "List of electricity organisations in India\n\npspcl\n\n\n\n\n"}
{"id": "3379450", "url": "https://en.wikipedia.org/wiki?curid=3379450", "title": "Lovettsville air disaster", "text": "Lovettsville air disaster\n\nThe Lovettsville air disaster occurred on August 31, 1940 near Lovettsville, Virginia. Pennsylvania Central Airlines Trip 19 was a new Douglas DC-3A that was flying through an intense thunderstorm at . Numerous witnesses reported seeing a large flash of lightning shortly before it nosed over and plunged to earth in an alfalfa field. With limited accident investigation tools at the time, it was at first believed that the most likely cause was the plane flying into windshear, but the Civil Aeronautics Board report concluded that the probable cause was a lightning strike. U.S. Senator Ernest Lundeen from Minnesota was one of the 25 people (21 passengers and 4 crew members) killed.\n\n\"Trip 19\", as it was designated, was under the command of Captain Lowell V. Scroggins with First Officer J. Paul Moore. The pilot and copilot had over eleven thousand and six thousand hours experience respectively, although only a few hundred of those hours were on DC-3s. In the jump seat rode a new administrative employee of the airline, hired on August 26.\n\nThe DC-3A was newly delivered from the Douglas Aircraft on May 25, 1940 equipped with twin Curtiss-Wright R-1820 Cyclone 9 engines (also designated as G-102-A).\n\nThe CAB investigation of the accident was the first major investigation to be conducted under the Bureau of Air Commerce act of 1938.\n"}
{"id": "12420494", "url": "https://en.wikipedia.org/wiki?curid=12420494", "title": "Mythe Water Treatment Works", "text": "Mythe Water Treatment Works\n\nThe Mythe Water Treatment Works in Tewkesbury, Gloucestershire, England is a facility which treats water drawn from the River Severn.\n\nOn 1 March 2002, Severn Trent Water worked with local councillors to create an emergency plan, which was supposed to ensure that in a state of emergency their services would not be affected. \n\nIt came to national attention in July 2007 when it became inundated with water from the River Severn during the Summer 2007 United Kingdom floods. The water coming into the plant was contaminated, and this led to the loss of tap water for approximately 150,000 people in Cheltenham, Gloucester and Tewkesbury.\n\n"}
{"id": "18259051", "url": "https://en.wikipedia.org/wiki?curid=18259051", "title": "PacWind", "text": "PacWind\n\nPacWind, Inc. is a wind turbine manufacturing company based in Torrance, California, United States.\n\nThe company was founded in 1998, and released their first patented vertical axis wind turbines in May 2006. In 2007, they added another two models of turbines to their product lineup.\n\nIn November 2008, certain intellectual property assets of PacWind were acquired by WePOWER, LLC. Since then, the original PacWind website has gone offline, and none of the original turbine designs are available for sale. The WePOWER product lineup as of the end of 2009 were the Falcon series turbines ranging from 600W to 12 kW.\n\nOn May 17, 2007, PacWind gained a large amount of publicity when TV personality Jay Leno announced that, as part of an ongoing project with Popular Mechanics, he would be having one of the 'Delta II' (10Kw) model turbines installed on the roof of his garage.\n\nDuring the \"I Spy Bill Nye\" episode of \"Living With Ed\", Ed Begley, Jr. has a 'SeaHawk' (500w) model turbine installed on the roof of his house.\n\nSixteen of PacWind's turbines were used in the design of the first eco-friendly billboard in Times Square in 2008.\n"}
{"id": "1513065", "url": "https://en.wikipedia.org/wiki?curid=1513065", "title": "Palladium-hydrogen electrode", "text": "Palladium-hydrogen electrode\n\nThe palladium-hydrogen electrode (abbreviation: Pd/H) is one of the common reference electrodes used in electrochemical study. Most of its characteristics are similar to the standard hydrogen electrode (with platinum). But palladium has one significant feature—the capability to absorb (dissolve into itself) molecular hydrogen.\n\nTwo phases can coexist in palladium when hydrogen is absorbed:\n\nThe electrochemical behaviour of a palladium electrode in equilibrium with HO ions in solution parallels the behaviour of palladium with molecular hydrogen\n\nThus the equilibrium is controlled in one case by the partial pressure or fugacity of molecular hydrogen and in other case—by activity of H-ions in solution.\n\nWhen palladium is electrochemically charged by hydrogen, the existence of two phases is manifested by a constant potential of approximately +50 mV compared to the reversible hydrogen electrode. This potential is independent of the amount of hydrogen absorbed over a wide range. This property has been utilized in the construction of a palladium/hydrogen reference electrode. The main feature of such electrode is an absence of non-stop bubbling of molecular hydrogen through the solution as it is absolutely necessary for the standard hydrogen electrode.\n\n\nElectrochimica Acta\n"}
{"id": "32374906", "url": "https://en.wikipedia.org/wiki?curid=32374906", "title": "Panel generation factor", "text": "Panel generation factor\n\nPanel generation factor (PGF) [1] is used while calculating the size of solar photovoltaic cells. It is a varying factor depending upon the climate of the site location (depending upon global geographic location).\n\nFor example: in Thailand it is 3.43, in EU countries it is 2.93, etc. This factor is used in calculation of \"Total Watt-Peak Rating\" while designing the size of solar photovoltaic cells. Therefore, \"Total Watt-Peak Rating\" = \"Total Watt-hours per day needed/generated from the PV modules\" divided by \"PGF\".\n\"Total Watt-Hours per Day\" = \"Total Watt-hours per day needed by appliances\" Multiplied by \"1.3 times\" (the energy lost in the system).\nNow, to calculate \"size of PV cells\" OR \"number of PV cells\" just divide the above obtained \"Total Watt-Peak Rating\" by \"Watt-Peak of each cell OR Watt-Peak of each square meter size\", whichever is convenient.\n\n[1] https://www.researchgate.net/publication/321485724_The_Panel_Generation_Factor_PGF_of_photovoltaic_plants_-_proof"}
{"id": "1418413", "url": "https://en.wikipedia.org/wiki?curid=1418413", "title": "Phugoid", "text": "Phugoid\n\nA phugoid or fugoid is an aircraft motion in which the vehicle pitches up and climbs, and then pitches down and descends, accompanied by speeding up and slowing down as it goes \"downhill\" and \"uphill\". This is one of the basic flight dynamics modes of an aircraft (others include short period, dutch roll, and spiral divergence), and a classic example of a negative feedback system.\nThe phugoid has a nearly constant angle of attack but varying pitch, caused by a repeated exchange of airspeed and altitude. It can be excited by an elevator singlet (a short, sharp deflection followed by a return to the centered position) resulting in a pitch increase with no change in trim from the cruise condition. As speed decays, the nose drops below the horizon. Speed increases, and the nose climbs above the horizon. Periods can vary from under 30 seconds for light aircraft to minutes for larger aircraft. Microlight aircraft typically show a phugoid period of 15–25 seconds, and it has been suggested that birds and model airplanes show convergence between the phugoid and short period modes. A classical model for the phugoid period can be simplified to about (0.85 × speed in knots) seconds, but this only really works for larger aircraft.\n\nPhugoids are often demonstrated to student pilots as an example of the speed stability of the aircraft and the importance of proper trimming. When it occurs, it is considered a nuisance, and in lighter airplanes (typically showing a shorter period) it can be a cause of pilot-induced oscillation.\n\nThe phugoid, for moderate amplitude, occurs at an effectively constant angle of attack, although in practice the angle of attack actually varies by a few tenths of a degree. This means that the stalling angle of attack is never exceeded, and it is possible (in the <1g section of the cycle) to fly at speeds below the known stalling speed. Free flight models with badly unstable phugoid typically stall or loop, depending on thrust.\n\nAn unstable or divergent phugoid is caused, mainly, by a large difference between the incidence angles of the wing and tail. A stable, decreasing phugoid can be attained by building a smaller stabilizer on a longer tail, or, at the expense of pitch and yaw \"static\" stability, by shifting the center of gravity to the rear.\n\nThe term \"phugoid\" was coined by Frederick W. Lanchester, the British aerodynamicist who first characterized the phenomenon. He derived the word from the Greek words and to mean \"flight-like\" but recognized the diminished appropriateness of the derivation given that meant flight in the sense of \"escape\" rather than vehicle flight.\n\nIn the 1975 Tan Son Nhut C-5 accident, USAF C-5 68-0218 with flight controls damaged by failure of the rear cargo/pressure door, encountered phugoid oscillations while the crew was attempting a return to base, and crash-landed in a rice paddy adjacent to the airport. Of the 328 people on board, 153 died, making it the deadliest accident involving a US military aircraft.\n\nIn 1985, Japan Airlines Flight 123 lost all hydraulic controls and its vertical stabiliser, and went into phugoid motion. While the crew were able to maintain near-level flight through the use of engine power, the plane lost height over a mountain range northwest of Tokyo before crashing into Mount Takamagahara. With 520 deaths, it remains the deadliest single-aircraft disaster in history.\n\nIn 1989, United Airlines Flight 232 suffered an uncontained engine failure that caused total hydraulic system failure. The crew flew the aircraft with throttle only. Suppressing the phugoid tendency was particularly difficult. The pilots reached Sioux Gateway Airport but crashed during the landing attempt. The pilots and a majority of the passengers survived.\n\nAnother aircraft that lost all hydraulics was a DHL operated Airbus A300B4 that was hit by a surface-to-air missile fired by the Iraqi resistance in the 2003 Baghdad DHL attempted shootdown incident. This was the first time that a crew landed an air transport aircraft safely by only adjusting engine thrust.\n\nThe 2003 crash of the Helios solar-powered aircraft was precipitated by reacting to an inappropriately diagnosed phugoid oscillation that ultimately made the aircraft structure exceed design loads.\n\nChesley (\"Sully\") Sullenberger, Captain of US Airways Flight 1549 that ditched in the Hudson River on January 15, 2009, said in a Google talk that the landing could have been less violent had the anti-phugoid software installed on the Airbus A320-214 not prevented him from manually getting maximum lift during the four seconds before water impact.\n\n"}
{"id": "27481335", "url": "https://en.wikipedia.org/wiki?curid=27481335", "title": "Plasmaron", "text": "Plasmaron\n\nIn physics, a plasmaron is a quasiparticle arising in a system that has strong plasmon-electron interactions. It is a quasiparticle formed by quasiparticle-quasiparticle interactions, since both plasmons and electron holes are collective modes of different kinds. It has recently been observed in graphene and earlier in elemental bismuth.\n"}
{"id": "57383765", "url": "https://en.wikipedia.org/wiki?curid=57383765", "title": "Roewe i5", "text": "Roewe i5\n\nThe Roewe i5 is a compact sedan and station wagon produced by Roewe. First introduced via the electric version station wagon, the Roewe Ei5, the compact station wagon debuted on the 2017 Guangzhou Auto Show. The engines for the i5 is the same as the ones available in the Excelle GX. \n\nThe Roewe Ei5 is the electric version of the Roewe i5. Debuting during the 2017 Guangzhou Auto Show, the pricing of the Roewe Ei5 ranges from 213,800 yuan to 223,800 yuan. The power of the Ei5 comes from a single electric motor with 116 hp with a range of 300 kilometers and a 145 km/h top speed.\n"}
{"id": "15853493", "url": "https://en.wikipedia.org/wiki?curid=15853493", "title": "Sectional density", "text": "Sectional density\n\nSectional density is the ratio of an object's mass to its cross-sectional area with respect to a given axis. It conveys how well an object's mass is distributed (by its shape) to overcome resistance along that axis. For illustration, a nail can penetrate a target medium with its pointed end first with less force than a coin of the same mass lying flat on the target medium.\n\nSectional density is used in gun ballistics. It is defined either as a projectile's mass divided by its cross sectional area, or as a projectile's mass \"in pounds\" divided by the square of (pi*rad) x by 2 = sq area \"in inches\".\n\nDuring World War II bunker-busting Röchling shells were developed by German engineer August Cönders, based on the theory of increasing sectional density to improve penetration. Röchling shells were tested in 1942 and 1943 against the Belgian Fort d'Aubin-Neufchâteau and saw very limited use during World War II.\n\nIn a physics context sectional density is defined as:\n\nIn a ballistics context sectional density of circular cross-sections is most commonly defined as:\n\nThe sectional density defined this way is usually presented without units. \n\nFor historical reasons, within the field of ballistics it is often assumed that the unit of mass is the pound, and the unit of length is the inch. For example: \".357 magnum\" (not \".357 inch magnum\"). By fixing the units, quantities can be treated as dimensionless.\n\nThe sectional density of a projectile can be employed in two areas of ballistics. Within external ballistics, when the sectional density of a projectile is divided by its coefficient of form (form factor in commercial small arms jargon); it yields the projectile's ballistic coefficient.\nsectional density has the same (implied) units as the ballistic coefficient.\n\nWithin terminal ballistics, the sectional density of a projectile is one of the determining factors for projectile penetration. The interaction between projectile (fragments) and target media is however a complex subject. A study regarding hunting bullets shows that besides sectional density several other parameters determine bullet penetration.\n\nOnly if all other factors are equal, the projectile with the greatest amount of sectional density will penetrate the deepest.\n\n\n"}
{"id": "5098476", "url": "https://en.wikipedia.org/wiki?curid=5098476", "title": "Segmented turning", "text": "Segmented turning\n\nSegmented turning is turning on a lathe where the initial workpiece is composed of multiple glued-together parts. The process involves gluing up several pieces of wood to create patterns and visual effects in turned projects.\n\nSegmented turning is also known as polychromatic turning.\nIn traditional wood turning, the template is a single piece of wood. The size, grain orientation and colors of the wood, will frame how it can be turned into an object like a bowl, platter, or vase. With segmented turning, the size and patterns are limited only by imagination, skill and patience.\n\nWhile the vast majority of segmented turnings are vessels of one sort or another, strictly speaking, any turned object comprising multiple pieces of glued wood could be classified as a segmented turning. Examples include pens, bowls, vases, salt mills, pepper mills, and rolling pins. By cutting and re-assembling pieces after they are turned, unique forms can be created, crossing over to pure art. See for example the work of Malcolm Tibbetts.\n\nIn addition to design skills, segmented turning demands precision woodworking skills as well as turning skills. Design and construction of a bowl blank—the wood piece mounted on the lathe for turning a vessel—requires angled miter joints cut to tolerances of as little as a tenth of one degree or better.\n\nThere are essentially two different techniques for constructing a bowl blank, ring construction and stave construction.\n\nRing construction is the most common. A ring-constructed blank comprises rings glued in a cylindrical stack. Though a platter or shallow bowl could be made from a single ring, stacking many rings is more typical. Apart from a lid or a base made and added after the fact, the height of the finished piece is a function of number of rings, and the height of each ring, which can vary for effect. Each ring comprises three or more pieces cut and glued to form a triangle, square, pentagon, hexagon, etc. The more pieces in a ring, the more challenging for the turner, because there are more opportunities for precision errors—gaps and misalignments in the joints between pieces. The individual pieces making up a ring themselves are often assembled from smaller pieces of contrasting or complementary colors to achieve striking patterns in the finished piece. This too adds to the complexity and challenge for the turner.\n\nNative American pottery, basket, and textile designs, particularly from the southwest U.S., frequently inspire form and design features. These patterns are often geometric in design and thus are easier to recreate than more fluid forms found in nature.\n\nStave constructions are assembled like barrels—cylinders constructed from multiple long, vertically oriented pieces. As a rule, the grain in a ring-constructed turning runs horizontally in the finished piece, while the grain in a stave-constructed turning runs vertically, from top to bottom.\n\nAnother category of segmented turning called Open Segmented Turning is similar to ring construction but small gaps are left between the segments. Successive rings are offset so the segments interlock with the ring above and below. This type of segmentation seems very delicate and is somewhat transparent but it is generally quite strong. This concept was introduced by Yosh Sugiyama in the early 1980s. For example see the work of William Smith.\n\nA segmented turning can combine ring construction, stave construction, and solid, non-segmented wood as well in a single piece. However, wood expands and contracts in the direction perpendicular to its grain as a function of its moisture content, itself a function of ambient humidity. In this case, during design and assembly, the turner has to be mindful of the impact on long term structural integrity of assembling the constituent pieces into incompatible, non-parallel grain directions.\n\nAn important variation of the ring construction technique is known as \"bowl from a board.\" Imagine a flat, square board bisected to create two identical rectangles, like two twin beds placed side-by-side to form a king-sized, square bed. Rings are created by cutting concentric semicircles from each of the two boards, one concentric half-ring a mirrored-shape of a half-ring from the other. The semicircles are typically cut with a bandsaw, but a scrollsaw, jigsaw, or coping saw would work. The blade of the saw used to cut the semicircles is set at an angle from the vertical, typically 45-60 degrees, such that when each mirrored pair is glued together to form a complete ring, the rings are shaped like cross sections of a cone. The rings are then stacked to create a cone-shaped blank. An incredible variety of visual effects can be achieved as a function of how the boards are constructed before the concentric rings are cut.\n\nThere are ongoing discussions as to whether turning is a trade, craft, or an art form. Turning in its original form was certainly utilitarian. If your last name is \"Turner\" chances are there was a production wood turner some generations ago in your family tree. Many turners produce spindle work for furniture, architectural work, toys, and bowls. But in recent years, many artists use turned pieces as a canvas for carving, pyrography, gold leaf work, inlay, stain and painting.\n\n\n\n"}
{"id": "29860338", "url": "https://en.wikipedia.org/wiki?curid=29860338", "title": "Silas Kpanan'Ayoung Siakor", "text": "Silas Kpanan'Ayoung Siakor\n\nSilas Kpanan'Ayoung Siakor is a Liberian environmentalist. He was awarded the Goldman Environmental Prize in 2006, for his revealing of illegal logging in Liberia and its connection to the civil war, leading to export sanctions from the United Nations Security Council.\n\nSilas Siakor was featured in the 2017 film \"Silas\", directed by Anjali Nayar and Hawa Essuman.\n"}
{"id": "11201108", "url": "https://en.wikipedia.org/wiki?curid=11201108", "title": "Snowpack", "text": "Snowpack\n\nSnowpack forms from layers of snow that accumulate in geographic regions and high altitudes where the climate includes cold weather for extended periods during the year. Snowpacks are an important water resource that feed streams and rivers as they melt. Therefore, snowpacks are both the drinking water source for many communities and a potential source of flooding (in case of sudden melting). Snowpacks also contribute mass to glaciers in their accumulation zone.\n\nAssessing the formation and stability of snowpacks is important in the study and prediction of avalanches. Scientists study the physical properties of snow under different conditions and their evolution, and more specifically snow metamorphism, snow hydrology (that is, the contribution of snow melt to catchment hydrology), the evolution of snow cover with climate change and its effect on the ice-albedo feedback and hydrology, both on the ground and by using remote sensing. Snow is also studied in a more global context of impact on animal habitats and plant succession. An important effort is put into snow classification, both as a hydrometeor and on the ground.\n\nSnowpack modeling is done for snow stability, flood forecasting, water resource management, and climate studies. Snowpack modeling is either done by simple, statistical methods such as degree day or complex, physically based energy balance models such as the SNOWPACK model, the CROCUS model or SNOWMODEL.\n\n\n"}
{"id": "4761306", "url": "https://en.wikipedia.org/wiki?curid=4761306", "title": "Sulfur tetrafluoride", "text": "Sulfur tetrafluoride\n\nSulfur tetrafluoride is the chemical compound with the formula SF. It is a colorless gas. It is a corrosive species that releases dangerous HF upon exposure to water or moisture. Despite these unwelcome characteristics, this compound is a useful reagent for the preparation of organofluorine compounds, some of which are important in the pharmaceutical and specialty chemical industries.\n\nSulfur in SF is in the formal +4 oxidation state. Of sulfur's total of six valence electrons, two form a lone pair. The structure of SF can therefore be anticipated using the principles of VSEPR theory: it is a see-saw shape, with S at the center. One of the three equatorial positions is occupied by a nonbonding lone pair of electrons. Consequently, the molecule has two distinct types of F ligands, two axial and two equatorial. The relevant bond distances are = 164.3 pm and = 154.2 pm. It is typical for the axial ligands in hypervalent molecules to be bonded less strongly. In contrast to SF, the related molecule SF has sulfur in the 6+ state, no valence electrons remain nonbonding on sulfur, hence the molecule adopts a highly symmetrical octahedral structure. Further contrasting with SF, SF is extraordinarily inert chemically.\n\nThe F NMR spectrum of SF reveals only one signal, which indicates that the axial and equatorial F atom positions rapidly interconvert via pseudorotation.\n\nSF is produced by the reaction of SCl and NaF in acetonitrile:\n\nSF is also produced in the absence of solvent at elevated temperatures.\n\nAlternatively, SF at high yield is produced using sulfur (S), NaF and chlorine (Cl) in the absence of reaction medium, also at less-desirable elevated reaction temperatures (e.g. 225-450 °C).\n\nA low temperature (e.g. ambient-86 °C) method of producing SF at high yield, without the requirement for reaction medium, has been demonstrated utilizing bromine (Br) instead of chlorine (Cl), S and KF:\n\nIn organic synthesis, SF is used to convert COH and C=O groups into CF and CF groups, respectively. Certain alcohols readily give the corresponding fluorocarbon. Ketones and aldehydes give geminal difluorides. The presence of protons alpha to the carbonyl leads to side reactions and diminished (30–40%) yield. Also diols can give cyclic sulfite esters, (RO)SO. Carboxylic acids convert to trifluoromethyl derivatives. For example, treatment of heptanoic acid with SF at 100-130 °C produces 1,1,1-trifluoroheptane. Hexafluoro-2-butyne can be similarly produced from acetylenedicarboxylic acid. The coproducts from these fluorinations, including unreacted SF together with SOF and SO, are toxic but can be neutralized by their treatment with aqueous KOH.\n\nThe use of SF is being superseded in recent years by the more conveniently handled diethylaminosulfur trifluoride, EtNSF, \"DAST\", where Et = CHCH. This reagent is prepared from SF:\n\nSulfur chloride pentafluoride (), a useful source of the SF group, is prepared from SF.\n\nHydrolysis of SF gives sulfur dioxide:\n\nSF + 2 HO → SO + 4 HF\n\nThis reaction proceeds via the intermediacy of thionyl fluoride, which usually does not interfere with the use of SF as a reagent.\n\n reacts inside the lungs with moisture:\n"}
{"id": "8185932", "url": "https://en.wikipedia.org/wiki?curid=8185932", "title": "Trench art", "text": "Trench art\n\nTrench art is any decorative item made by soldiers, prisoners of war, or civilians where the manufacture is directly linked to armed conflict or its consequences. It offers an insight not only to their feelings and emotions about the war, but also their surroundings and the materials they had available to them.\n\nNot limited to the World Wars, the history of trench art spans conflicts from the Napoleonic Wars to the present day. Although the practice flourished during World War I, the term 'trench art' is also used to describe souvenirs manufactured by service personnel during World War II. Some items manufactured by soldiers, prisoners of war or civilians during earlier conflicts have been retrospectively described as trench art.\n\nThere are four broad categories of trench art:\n\nThere is much evidence to prove that some trench art was made in the trenches, by soldiers, during war.\n\nIn \"With a Machine Gun to Cambrai\", George Coppard tells of pressing his uniform buttons into the clay floor of his trench, then pouring molten lead from shrapnel into the impressions to cast replicas of the regimental crest.\n\nChalk carvings were also popular, with contemporary postcards showing carvings in the rocky outcrops of dug-outs.\n\nMany smaller items such as rings and knives were made by soldiers either in front line or support trenches, especially in quieter parts of the line.\n\nWounded soldiers were encouraged to work at crafts as part of their recuperation, with embroidery and simple forms of woodwork being common. Again from \"With a Machine Gun to Cambrai\", George Coppard recalls that, while recuperating from wounds at a private house in Birkenhead, \"one kind old lady brought a supply of coloured silks and canvas and instructed us in the art of embroidery. A sampler which I produced under her guidance so pleased her that she had it framed for me.\"\n\nAn example of therapeutic embroidery during World War I is the work of British military in Egypt, who were photographed sewing and embroidering for Syrian refugees. There was also the Bradford Khaki Handicrafts Club, which was funded in Bradford, UK, in 1918, to provide occupational therapy and employment for men returning from the trenches in France|url=https://trc-leiden.nl/trc-needles/organisations-and-movements/charities/bradford-khaki-handicrafts-club.\n\nThe second category consists of items made by prisoners of war and interned civilians.\n\nPOWs had good reasons to make decorative objects: free time and limited resources. Much POW work was therefore done with the express intention of trading the finished article for food, money or other privileges.\n\nReference to POW work is made in the recollections of A B Baker, W.A.A.C., contained in the book \"Everyman at War\", published by Purdom in 1930:\n\"Part of my work had to do with prisoners quartered in a camp near to our own. Those Germans were friendly men. They were clever with their hands, and would give me little carvings which they had made.\"\n\nThe third category is items made by civilians, which mainly means civilians in and around the conflict zone, but would also include items made by sweethearts at home.\n\nIn 1914, the US set up the Commission for Relief in Belgium, headed by Herbert Hoover. It shipped staple foodstuffs, mainly flour, in the printed cotton flour sacks typical of the period. As thanks, the Belgians would embroider and paint in the designs, elaborating them with dates and flags and send them back to the US. Examples of these are now in the Herbert Hoover Museum, but some were sold to soldiers in Paris or given as gifts.\n\nCivilians in France, in the zones occupied by troops, were quick to exploit a new market. Embroidered postcards were produced in what quickly became a cottage industry, with civilians buying the surrounds and embroidering a panel of gauze. These postcards depicted regimental crests or patriotic flags and national symbols in abundance, and millions were produced over the course of the war.\n\nAt war's end, when civilians began to reclaim their shattered communities, a new market appeared in the form of pilgrims and tourists. Over the ensuing twenty years mountains of discarded debris, shell casings, and castoff equipment were slowly recycled, with mass-produced town crest motifs being stuck onto bullets, shell casings, fuse caps, and other paraphernalia to be sold to tourists.\n\nThe fourth category is purely commercial production. After the war, tonnes of surplus materials were sold by the government and converted to souvenirs of the conflict.\n\nShip breaking, particularly if the ship had been involved in significant events such as the Battle of Jutland, resulted in much of the wood from the ship being turned into miniature barrels, letter racks, and boxes, with small brass plaques attached announcing, for example, \"Made of teak from HMS \"Shipsname\", which fought at the Battle of Jutland\".\n\n\n\n"}
{"id": "3306100", "url": "https://en.wikipedia.org/wiki?curid=3306100", "title": "Tropical agriculture", "text": "Tropical agriculture\n\nWorldwide more human beings gain their livelihood from agriculture than any other endeavor; the majority are self-employed subsistence farmers living in the tropics. While growing food for local consumption is the core of tropical agriculture, cash crops (normally crops grown for export) are also included in the definition.\n\nWhen people discuss the tropics, it is normal to use generalized labels to group together similar tropical areas. Common terms would include the humid-tropics (rainforests); the arid-tropics (deserts and dry areas); or monsoon zones (those areas that have well defined wet/dry seasons and experience monsoons). Such labeling is very useful when discussing agriculture, because what works in one area of the world will normally work in a similar area somewhere else, even if that area is on the opposite side of the globe.\n\nMost temperate zone agricultural techniques are inappropriate for tropical areas. The second half of the 20th century saw many attempts to duplicate in the tropics farming practices that had been successful in temperate climates. Due to differences in climate, soils, and patterns of land ownership, these largely failed. When they did succeed they tended to heavily favor farmers with substantial land holdings, as a high percentage of temperate agricultural practices are economically \"scale-based\" and favor large scale production. This in turn pushed many small-scale farmers on to ever more marginal land, as the better quality land was consolidated into larger farms.\n\nThe \"Green Revolution\" was an agricultural improvement program undertaken in the tropics. Funded initially by the Rockefeller Foundation, it aimed to improve corn, rice, and other cereal cultivators – breeding plants that would produce more grain for the same amount of effort.\n\nFrom that point, it expanded out to improved basic farming practices, particularly for rice farmers. The growth of crop yields was such that agriculture was able to outstrip population growth — per capita production increased every year following 1950 - with Asia leading the way. The total cost of the Green Revolution by 1990 was about US$100 million.\n\nThe Green Revolution had a flaw; although the crops gave more yield, they were more subject to disease, since this was not a primary concern of the program. To address this problem together with an approach to more small-scale farming crops, substantial interest exists today in creating a second Green Revolution, based on sustainable agricultural practices and geared towards (small-scale) farmers with limited financial resources.\n\nMany tropical food plants are propagated by cuttings. Seeds are necessary for plant embryos to survive the winter and other harsh conditions such as drought. However, where the weather is normally conducive to growth year-round, plants reproducing plants through means other than seeds is often advantageous. By bypassing the seed stage, plants can greatly accelerate their reproductive cycles. Despite this, anyone who wishes so, may still grow tropical crops, e.g., fruits, from seeds. To do so, some special seed germination techniques to germinate them more quickly may be best used.\n\n\"Plants are faced with a dilemma; while they need to attract beneficial pollinators and seed dispensers, they must also minimize the damage caused by the marauding army of herbivores. Without some form of protection the trees would be stripped bare and smaller plants would be completely devastated, and because plants stand still, they cannot run away. This is as true in Amazonian rainforest as it is in Northern coniferous forest.\" - Marcus Wischik.\n\nMany (tropical) plants use toxins to protect themselves. Cassava, one of the most important tropical food crops, produces cyanide upon ingestion unless processed to remove/reduce the cyanide content. Other plants are high in oxalates (the agent that binds calcium to form kidney stones); castor beans are the source of ricin, one of the most powerful poisons in existence; and velvet beans contain 3.1-6.1% L-DOPA, which can be toxic in large quantities. The list of toxic plants is long, but toxicity does not always mean a particular plant should be avoided, the knowledge needed to render toxic plants safe to use already exists in most communities.\n\nThe contents of a bag of commercial fertilizer is described in terms of NPK -nitrogen (N), phosphorus (P) and potassium (K); with nitrogen being the main component of most commercial fertilizers.\n\nOxygen is only a small part of the air; the largest component of air is nitrogen. Nitrogen compounds are the main building block of protein; muscle in mammals and plant tissue in plants. If the level of nitrogen compounds in the soil is increased, plant growth can be significantly increased. Legumes are a group of plants that interact with bacteria (rhizobia) in the soil to fix nitrogen from the air into usable compounds, and deposit them into the soil where it is available for other plants to use. The nitrogen compounds deposited by legumes can be readily converted into larger harvests.\n\nGreen manures are plants grown to improve the soil, suppress weeds, limit erosion, and — when legumes are used — to increase the nitrogen content of the soil. The most common type of green manure used in the tropics is velvet bean. It produces a thick blanket of vines and leaves that in addition to infusing the soil with nitrogen, also smothers most weeds. It has reasonable tolerance to drought, low soil fertility, and highly acidic soil. Alternatives to the velvet bean include the lablab bean, the jack bean, and for use above 500 m altitude, the scarlet runner bean.\n\nOnce the blanket is several centimeters thick, it is chopped down with a machete, and the vines are chopped up. This produces thick mulch on top of the ground that both inhibits weed growth and adds vital nutrients to the soil. Corn or other crops are then planted directly into this mulch.\n\nSlash/mulch is popular in southern Mexico, Guatemala, and Honduras; and in recent years has gained a following in many areas of the tropics, from Brazil to central Africa. Where it has been embraced it has pushed aside slash and burn agriculture, and allowed farmers to use the same land continuously for many years.\n\nCornell University has taken a leading role in researching the effects of mulches and slash/mulch practices in the tropics.\n\nIn most places in the tropics sufficient precipitation occurs to grow enough food to feed the local population; however, it may not fall in a timely or convenient manner. Making maximum use of the water that does fall is an ongoing challenge.\n\nWater is a particularly important issue in dryland farming. The ability to collect and store water at a low cost and without damaging the environment, is what opens up deserts and other arid regions to farmers. When it rains in dryland areas, the rain storms are normally heavy, and the soil unable to absorb the large amounts of rain that comes down. This leads to excessive surface run-off that needs to be captured and retained.\n\nCommercial farms growing cash crops often use irrigation techniques similar to or identical to what would be found on large scale commercial farms located in temperate regions; as an example, the Israeli drip-irrigation lines.\n\nOne of the simplest forms of irrigation - the farmer digs bathtub-sized pits into his fields and lines them with plastic sheets to collect rainwater. Then, once the dry season sets in, the farmer uses the collected water to irrigate his crops. The technique is especially useful in mountainous areas, where rapid run-off otherwise occurs.\n\nDuring years with normal precipitation, the growing season can be increased by an extra month or more by using harvesting pits. An extra month in many places means an extra crop can be grown. For instance, if the local growing season is 5 months long, and the farmers' main crops take 3 or 4 months to grow, an extra month may be enough time to grow a secondary crop. During times of drought, what rain does fall can be collected in the pits and used to secure the farmers' main crops.\n\nAn irrigation system consisting of a bucket hung from a pole, with a hose coming out of the bottom, and holes punched into the hose. The bucket is filled, and gravity feeds the water to the plants. \n\nThe treadle pump is a human-powered pump designed to lift water from a depth of seven metres or less. A treadle is lever device pressed by the foot to drive a machine, in this case a pump. The treadle pump can do most of the work of a motorized pump, but costs considerably less to purchase, and needs no fossil fuel, as it is driven by the operator's body weight and leg muscles. It can lift 5-7 metres of water per hour from wells and boreholes up to seven metres deep and can also be used to draw water from lakes and rivers. Most treadle pumps used are of local manufacture, as they are simple and inexpensive to build.\n\nStandard treadle pumps are suction pumps, and were first developed in the early 1980s in Bangladesh. Most treadle pumps manufactured in Africa are pressure treadle pumps, a modification to the original design that means water is forced out of the pump under pressure. Pressure treadle pumps are more versatile as they allow farmers to pump water uphill, or over long distances, or fill elevated tanks.\n\nCrop rotation is the cornerstone pest control in the tropics. When a single crop is planted repeatedly in the same soil, insects and diseases that attack that crop are allowed to build up to unmanageable levels, greatly reducing the farmer's harvest.\n\nThe most basic form of crop rotation is also the simplest: never plant the same thing in the same place twice. This results in naturally breaking the cycles of weeds, insects and diseases that attack food crops. Rotations are used to prevent or at least partially control several pests and at the same time to reduce the farmer's reliance on chemical pesticides. Crop rotations often are the only economically feasible method for reducing insect and disease damage.\n\nCrop rotation replaces a crop that is susceptible to a serious pest with another crop that is not susceptible. Each food crop comes with its own set of pests that attack that particular crop. By planting a different crop each time, the farmer is able to starve out those pests. Often a set of three or four crops are planted on a rotating basis, ensuring that by the time the first crop is replanted, the pests that attack it are substantially reduced.\n\nAnother side benefit of crop rotation is it improves the soil. Constantly growing the same crop in the same location will strip the soil of the nutrients that particular crop requires. Rotating to a different crop will reduce the pressure placed on the soil. Or if a green manure is used as part of the rotation sequence, the soil can actually be improved.\n\nIntegrated pest management (IPM) was developed as an alternative to the heavy use of chemical pesticides. Eliminating all insect pests requires the extensive use of chemical pesticides, which over time can become self-defeating. Farmers end up using more and more chemicals with diminishing effect as pests quickly adapt –while at the same time natural predator insects are eliminated from the farm. Under IPM, chemicals should be a secondary line of defense, while building up the number of natural predators on a farm is the main goal. The IPM approach calls for keeping the pest populations below the levels at which they cause economic injury, not total eradication.\n\nIPM in its pure form is complex, and beyond the ability of most farmers to manage; however, the underlying principles have gained widespread acceptance in the tropics, with most governments sponsoring IPM educational programs.\n\nPioneering crops are used in places where the land has been striped bare, and the topsoil has been entirely lost to erosion, or where desertification has started. The intent is not to grow food or cash crops, but to repair and reinvigorate the soil in order to prepare the way for the later planting of food or cash crops. Nitrogen fixing plants and trees normally form the basis of such a reclamation project.\n\nThe hunger season is that period of time when all the food from the previous harvest has been consumed, and the next harvest is still some time away. Even in normal years, many households face an annual reduction in the amount of food they have available. Typically the hunger season will coincide with the start of planting the new crop, or shortly thereafter. So farmers are faced with a shortage of food at the very time they are expected to perform their heaviest labor.\n\nOne way of mitigating the effects of the hunger season is growing some non-seasonal crops close to the family home, such as bananas in humid areas, or cassava where it is arid. As an example, a family that has ten banana plants producing fruit during the hunger season is unlikely to experience excessive hardship. Sweet potato, pigeon pea, and \"Moringa oleifera\" should also be considered.\n\nWinters are mild in the tropics; with no frost, snow, or ice, insect populations flourish year-round. In temperate areas, winter reduces most insect pest populations prior to the emergence of new crops, so plants coming up in the spring have a chance to take hold and grow prior to being attacked. In the tropics, plants enter a world already full of adult insects.\n\nMild winters allow winter crops to be grown in some areas, such as India.\n\nSoils in the humid tropics are normally highly acidic and nutrient poor; decomposition is rapid because of high temperatures, high humidity, and frequent heavy rains. Heavy rains, especially monsoon rains, lead to rapid nutrient leaching, and chemical weathering of the soil. Standard temperate strategies for improving nutrient-poor soil, such as composting, have limited application in such an environment due to rapid leaching.\n\nAluminum is the most common metal found in the Earth's crust. It is found in all soils and in all environments, from temperate to tropical. In a soluble state, it is highly toxic to plant life, as it inhibits root growth; however, in neutral and alkaline soils common to the temperate zones, it is insoluble and therefore inert. Soil fertility is directly influenced by how acidic it is, as the more acidic, the higher the level of aluminum toxicity; in areas where the pH drops below 5, aluminum becomes soluble and can enter into plant roots where it accumulates.\n\nAround a third of all tropical soils are too acidic to support traditional food crops. These highly acidic tropical soils represent the largest untapped arable land left in the world, so more productive use of these lands is key to expanding the world food supply.\n\nWinrock International states, \"In the humid tropics, the relative importance of acid soils is greatest in Latin America (81%), but also significant in Africa (56%) and Asia (38%)\".\n\nTraditionally on commercial farms, aluminum toxicity is countered by adding lime to the soil, which neutralizes the acid and renders the aluminum inert. However, many small land holders and resource-poor farmers cannot afford lime, and instead rely on slash-and-burn agriculture. As the original plant life is burnt, the ash acts to neutralize the acidic soil and makes the area acceptable for food plants. In time, acidity increases and only native plants will grow, forcing the farmer to move on and clear a new area.\n\nSoil color in humid areas is related to the level of oxidation that has occurred in the soil, with red soil being the result of iron oxidation, and yellow soil being the result of aluminum oxidation.\n\nSalinization occurs naturally in arid areas where not enough rain falls to wash soluble salts down and out of the root zone. Salinization is a common side effect of irrigation. As water is used by plants and evaporates from the soil surface, the salt in the water concentrates in the soil. The high temperatures and low humidity in arid regions means that salinization often accompanies irrigation.\n\nSome plants have a photoperiod (phototropism) requirement for a certain number of hours of daylight before they will grow, flower, or produce fruit. Without this, they will not complete their life cycle and will not produce fruit and seeds. So, seeds brought from the temperate zones may not perform as expected, or at all in the tropics. Some plants are genetically keyed to only start producing when a certain number of hours of daylight is reached, the same number of hours as is found in their native habitat. With the shorter daylight hours experienced in the tropics, that switch never gets thrown.\n\nA combination of factors make the tropics one of the world's most vulnerable regions to the negative impacts of climate change on agriculture. These include:\n\n\nThe fact that climate change and temperature increases are expected to negatively affect crop yields in the tropics could have troublesome implications for poverty and food security, mainly because populations in the area are so dependent on agriculture as their only means of survival. A 2008 study by the CGIAR Research Program on Climate Change, Agriculture and Food Security matched future climate change \"hotspots\" with regions that are already suffering from chronic poverty and food insecurity to pinpoint regions in the tropics that could be especially vulnerable to future changes in climate. These include regions such as West Africa which are already dependent on drought- and stress-resistant crop varieties and thus left with little room to manoeuvre when the climate becomes even drier. The study says that East and West Africa, India, parts of Mexico and Northeastern Brazil will experience a shortening of growing seasons by more than 5%, negatively impacting a number of important crop staples.\n\n\n\n"}
{"id": "10965157", "url": "https://en.wikipedia.org/wiki?curid=10965157", "title": "Turag River", "text": "Turag River\n\nThe Turag River ( ) is the upper tributary of the Buriganga, a major river in Bangladesh. The Turag originates from the Bangshi River, the latter an important \"tributary\" of the Dhaleshwari River, flows through Gazipur and joins the Buriganga at Mirpur in Dhaka District. It is navigable by boat all year round.\n\nThe Turag suffers from infilling along its banks, which restricts its flow. It also suffers from acute water pollution. While attempts have been made to marginally widen the river, the majority of industry has made little effort to follow environmental law and the water has become visibly discolored.\n\nEarlier this river was called as (Bengali: \"Kohor Doriya\"), \"Kohor river\".\n\nTabligh Jam'at, a popular Islamic movement originating in South Asia, initially took hold in Dhaka in the 1950s as Maulana Abdul Aziz and other leaders set up the regional headquarters at the Kakrail Mosque near Ramna Park. An initiative of the movement is an emphasis on the six \"uṣūl\" or \"basic principles,\" two of which include \"ilm\", the pursuit of knowledge, and \"dhikr\" or \"zikr\", a method of prayer involving repetitive invocation of hadith and Qur'an passages. To this end, the movement places importance on \"ijtema\" or assembly, where members gather to practice and participate in \"dhikr\", hear religious sermons and discuss their activities.\n\nThe largest of these, the Bishwa Ijtema, is situated by the Turag River in Tongi and attracts estimates of between two and four million Muslims annually as well as representatives from over sixty countries, making it the second biggest Islamic congregation after the Hajj.\n"}
{"id": "150250", "url": "https://en.wikipedia.org/wiki?curid=150250", "title": "Tyson turbine", "text": "Tyson turbine\n\nThe Tyson Turbine is a hydropower system that extracts power from the flow of water. This design doesn't need a casement, as it is inserted directly into flowing water. It consists of a propeller mounted below a raft, driving a power system, typically a generator, on top of the raft by belt or gear. The turbine is towed into the middle of a river or stream, where the flow is the fastest, and tied off to shore. It requires no local engineering, and can easily be moved to other locations. The Tyson Turbine is a very common way to reuse energy.\n"}
{"id": "16422485", "url": "https://en.wikipedia.org/wiki?curid=16422485", "title": "Ultrafine particle", "text": "Ultrafine particle\n\nUltrafine particles (UFPs) are particulate matter of nanoscale size (less than 0.1 μm or 100 nm in diameter). Regulations do not exist for this size class of ambient air pollution particles, which are far smaller than the regulated PM and PM particle classes and are believed to have several more aggressive health implications than those classes of larger particulates.\n\nThere are two main divisions that categorize types of UFPs. UFPs can either be carbon-based or metallic, and then can be further subdivided by their magnetic properties. Electron microscopy and special physical lab conditions allow scientists to observe UFP morphology. Airborne UFPs can be measured using a condensation particle counter, in which particles are mixed with alcohol vapor and then cooled allowing the vapor to condense around them which are then counted using a light scanner. UFPs are both manufactured and naturally occurring. UFPs are the main constituent of airborne particulate matter. Owing to their numerous quantity and ability to penetrate deep within the lung, UFPs are a major concern for respiratory exposure and health.\n\nUFPs are both manufactured and naturally occurring. Hot volcanic lava, ocean spray, and smoke are common natural UFPs sources. UFPs can be intentionally fabricated as are fine particles to serve a vast range of applications in both medicine and technology. Other UFPs are byproducts, like emissions, from specific processes, combustion reactions, or equipment such as printer toner and automobile exhaust. In 2014, an air quality study found harmful ultrafine particles from the takeoffs and landings at Los Angeles International Airport to be of much greater magnitude than previously thought. There are a multitude of indoor sources that include but are not limited to laser printers, fax machines, photocopiers, the peeling of citrus fruits, cooking, tobacco smoke, penetration of contaminated outdoor air, chimney cracks and vacuum cleaners.\n\nUFPs have a variety of applications in the medical and technology fields. They are used in diagnostic imagining, and novel drug delivery systems that include targeting the circulatory system, and or passage of the blood brain barrier to name just a few. Certain UFPs like silver based nanostructures have antimicrobial properties that are exploited in wound healing and internal instrumental coatings among other uses, in order to prevent infections. In the area of technology, carbon based UFPs have a plethora of applications in computers. This includes the use of graphene and carbon nanotubes in electronic as well as other computer and circuitry components. Some UFPs have characteristics similar to gas or liquid and are useful in powders or lubricants.\n\nThe main exposure to UFPs is through inhalation. Owing to their size, UFPs are considered to be respirable particles. Contrary to the behaviour of inhaled PM and PM, ultrafine particles are deposited in the lungs, where they have the ability to penetrate tissue and undergo interstitialization, or to be absorbed directly into the bloodstream — and therefore are not easily removed from the body and may have immediate effect. Exposure to UFPs, even if components are not very toxic, may cause oxidative stress, inflammatory mediator release, and could induce heart disease, lung disease, and other systemic effects.\n\nA robust association has been observed between fine particulate levels and lung cancer, and cardiopulmonary disease. The exact mechanism through which UFP exposure leads to health effects remains to be elucidated, but effects on Blood pressure may play a role. It has recently been reported that UFP is associated with an increase in blood pressure in schoolchildren with the smallest particles inducing the largest effect.\n\nThere is a range of potential human exposures that include occupational, due to the direct manufacturing process or a byproduct from an industrial or office environment, as well as incidental, from contaminated outdoor air and other byproduct emissions. In order to quantify exposure and risk, both \"in vivo\" and \"in vitro\" studies of various UFP species are currently being done using a variety of animal models including mouse, rat, and fish. These studies aim to establish toxicological profiles necessary for risk assessment, risk management, and potential regulation and legislation.\nAs the nanotechnology industry has grown, nanoparticles have brought UFPs more public and regulatory attention. UFP risk assessment research is still in the very early stages. There are continuing debates about whether to regulate UFPs and how to research and manage the health risks they may pose. As of March 19, 2008, the EPA does not yet regulate or research ultrafine particles, but has drafted a Nanomaterial Research Strategy, open for independent, external peer review beginning February 7, 2008 (Panel review on April 11, 2008). There is also debate about how the European Union (EU) should regulate UFPs.\n\n"}
{"id": "24183731", "url": "https://en.wikipedia.org/wiki?curid=24183731", "title": "World Nuclear Industry Status Report", "text": "World Nuclear Industry Status Report\n\nThe World Nuclear Industry Status Report is a yearly report that explores the global challenges facing the nuclear power industry. It is produced by Mycle Schneider, an independent energy expert, and gives a detailed overview of the global nuclear industry and special analysis on key events and trends.\n\nIn January a fully interactive visualization on nuclear power construction was launched. This contains information on the 754 reactors that are or have been under-construction since 1951. The Global Nuclear Power Database is hosted by the Bulletin of the Atomic Scientists.\n\nAs of the middle of 2016, 31 countries were operating nuclear reactors for energy purposes. Nuclear power plants generated 2,441 net terawatt-hours (TWh or billion kilowatt-hours) of electricity in 2015 , a 1.3 percent increase, but still less than in 2000 and 8.2 percent below the historic peak nuclear generation in 2006. 59 reactors are considered here as under construction, three fewer than WNISR reported a year ago, and eight less than in mid-2014. Eighty percent of all new-build units (47) are in Asia and Eastern Europe, of which 22 in China alone.\n\nGlobally, the nuclear industry’s situation continued to deteriorate in 2015, except in China. Eight out of the ten nuclear power reactor startups in 2015 were in China.\n\nThe latest report, written by Mycle Schneider and Antony Froggatt with contributions of four other experts from Japan, the UK and France, says that the nuclear industry was struggling with grave problems prior to the Fukushima accident, but that the impact of the accident has become increasingly visible. Global electricity generation from nuclear plants dropped by a historic 7 percent in 2012, adding to the record drop of 4 percent in 2011.\n\nThe 427 operating reactors worldwide, as of 1 July 2013, are 17 lower than the peak in 2002. The nuclear share in the world’s power generation declined steadily from a historic peak of 17 percent in 1993 to about 10 percent in 2012. The report details a range of restart scenarios for Japan's nuclear reactor fleet which, as of September 2013, were all shutdown. Nuclear power’s share of global commercial primary energy production plunged to 4.5 percent, a level last seen in 1984.\n\nBesides an extensive update on nuclear economics, the report also includes an assessment of the major challenges at the Fukushima nuclear site, in particular the highly contaminated water on site. This water contained in the basement of reactors and in storage tanks contains 2.5 times the total amount of caesium-137 released at the Chernobyl accident.\n\nThe report says that China, Germany and Japan, three of the world’s four largest economies, as well as India, now generate more power from renewables than from nuclear power. For the first time in 2012 China and India generated more power from wind alone than from nuclear plants, while in China solar electricity generation grew by 400 percent in one year.\n\nAccording to the \"World Nuclear Industry Status Report 2012\", written by Mycle Schneider and Antony Froggatt, nuclear power accounted for 11 percent of worldwide electricity generation. World atomic power production dropped by a record 4.3 percent in 2011 as the global financial crisis and the Fukushima disaster in Japan prompted plant shutdowns and slowed construction of new sites. Seven reactors began operating in 2011 and 19 were shuttered.\n\nThe report shows that following the Fukushima crisis in March 2011, Germany, Switzerland and Taiwan announced their withdrawal from nuclear power. Output was further restricted as nations suspended construction plans amid safety concerns and economic stagnation, forcing utilities to study extending lifetimes, which raises considerable safety issues.\n\nAt least five countries, including Egypt, Italy and Kuwait, have suspended plans to build their first nuclear reactors. In the U.K., major companies like RWE, EON, and SSE have all abandoned new-build proposals in 2011/12, while companies in Japan and Bulgaria have suspended construction. The Fukushima disaster also created certification and licensing delays.\n\nThe \"World Nuclear Industry Status Report 2010-2011\" is authored by Mycle Schneider, Antony Froggatt, and Steve Thomas and published by the Washington-based Worldwatch Institute. The foreword is written by Amory Lovins.\n\nAccording to the report, the international nuclear industry has been unable to stop the slow decline of nuclear energy. The world’s reactor fleet is aging quickly and not enough new units are coming online. As of April 1, 2011, there were 437 nuclear reactors operating in the world, which was seven fewer than in 2002. The Olkiluoto plant has had particular problems:\nThe flagship EPR project at Olkiluoto in Finland, managed by the largest nuclear builder in the world, AREVA NP, has turned into a financial fiasco. The project is four years behind schedule and at least 90 percent over budget, reaching a total cost estimate of €5.7 billion ($8.3 billion) or close to €3,500 ($5,000) per kilowatt.\nThe report says that the Fukushima Daiichi nuclear disaster is exacerbating many of the problems that nuclear energy is facing. There is \"no obvious sign that the international nuclear industry could eventually turn empirically evident downward trend into a promising future\", and the Fukushima nuclear disaster is likely to accelerate the decline. With long lead times of 10 years and more, it will be difficult to maintain, let alone increase, the number of operating nuclear power plants over the next 20 years. Moreover, says the report, it is clear that nuclear power development cannot keep up with the pace of renewable energy commercialization. For the first time, in 2010 total installed nuclear power capacity in the world (375 gigawatts) fell behind aggregate installed capacity (381 GW) of three specific renewables — wind turbines (193 GW), biomass and waste-to-energy plants (65 GW), and solar power (43 GW).\n\nThe \"World Nuclear Industry Status Report 2009\" presents quantitative and qualitative information on the nuclear power plants in operation, under construction and in planning phases throughout the world. A detailed analyses of the economic performance of past and current nuclear projects is also given. The report was commissioned by the German Federal Ministry of Environment, Nature Conservation and Reactor Safety.\n\nThe \"World Nuclear Industry Status Report 2008\" focused on the difficulties facing nuclear power throughout the world, with particular reference to Western Europe and Asia.\n\nThe \"World Nuclear Industry Status Report 2007\" was commissioned by the Greens-EFA Group in the European Parliament.\n\nThe first \"World Nuclear Industry Status Report\" was issued in 1992 in a joint publication with WISE-Paris, Greenpeace International and the World Watch Institute, Washington. The second report in 2004 was commissioned by the Greens-EFA Group in the European Parliament.\n\n\n\n"}
