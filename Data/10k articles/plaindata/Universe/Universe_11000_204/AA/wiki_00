{"id": "10159009", "url": "https://en.wikipedia.org/wiki?curid=10159009", "title": "Advanced Plant Management System", "text": "Advanced Plant Management System\n\nThe Advanced Plant Management System (APMS) is a SCADA solution developed in partnership by RWE npower and Thales UK.\n\nBased on a real-time application platform, APMS is a monitoring and control system for any large industrial process. APMS is implemented at more than thirty electric power generation units in the UK, including Tilbury, Didcot A, Aberthaw, Drax and Rugeley power stations.\n\nThe APMS system replaced the Cutlass programming language and application system - a real-time control system widely implemented during the 1970s in British power plants by the Central Electricity Generating Board (CEGB) research & development team - in National Power power stations. (PowerGen in contrast retained and modernised its Cutlass systems.)\n\n\n"}
{"id": "1848412", "url": "https://en.wikipedia.org/wiki?curid=1848412", "title": "Alabama Army Ammunition Plant", "text": "Alabama Army Ammunition Plant\n\nThe Alabama Army Ammunition Plant (ALAAP), was a United States munitions plant built and operated during World War II. The facility is located four miles (6 km) north of Childersburg, Alabama in Talladega County, Alabama.\n\nThe ALAAP was built in 1941 as a production facility for nitrocellulose (NC), trinitrotoluene (TNT), dinitrotoluene (DNT), tetryl, and single-base smokeless powder. The facility, operated by DuPont, had a peak production rate of nearly 40 million pounds (18 million kg) of munitions per month. It also produced heavy water for the Manhattan Project. The facility ceased operation in August 1945 and was placed on standby status after the end of the war. During the war, the plant covered more than 13,000 acres (53 km²). \n\nAfter the end of the war, various portions of the plant were leased out for commercial operations. Most of the original structures have been destroyed, although the government retains responsibility for more than 2,000 acres (8 km²) of the facility.\n\nThe U.S. Environmental Protection Agency designated the plant as a Superfund site in 1987. Chemicals from the manufacturing areas of the site had contaminated groundwater and soil on the property. The Army began cleanup activities on the site in the 1990s.\n\n"}
{"id": "9608089", "url": "https://en.wikipedia.org/wiki?curid=9608089", "title": "Asiana Airlines Flight 733", "text": "Asiana Airlines Flight 733\n\nAsiana Airlines Flight 733 (OZ733, AAR733, registration HL7229) was a domestic Asiana Airlines passenger flight from Seoul-Gimpo International Airport (SEL at the time, now GMP) to Mokpo Airport (MPK), South Korea. The flight crashed on July 26, 1993 in the Hwawon area of Haenam County, South Jeolla Province, killing 68 of the 116 passengers and crew on board, including the captain and one other crew member.\n\nThe aircraft was a Boeing 737-500, which made its maiden flight June 14, 1990. The aircraft was delivered to Maersk Air on June 26 the same year (with registration OY-MAB). The aircraft was then leased to Asiana Airlines on November 26, 1992.\n\nThere were three Japanese nationals and two American nationals among the passengers, many of whom were vacationers heading for the popular summer resort there off the Yellow Sea, according to the airline. The captain was Hwang In-ki, but the first officers name was unknown. There were four flight attendants onboard.\nOn July 26, 1993, flight 733 departed Gimpo International Airport in Seoul, bound for Mokpo Airport, which a scheduled arrival by 15:15. At that time, the weather conditions in Mokpo and Yeongam County area consisted of heavy rain and wind. However, the weather conditions were not enough to delay the arrival time. The aircraft made its first landing attempt at 15:24, which failed, followed by a second landing attempt at 15:28, which also failed. At 15:38, after two failed landing attempts, the aircraft made a third attempt. At 15:41 the aircraft disappeared from the radar screens and communication was lost. At 15:48 the aircraft crashed into Mt. Ungeo. At 15:50, the wreckage was found in the near Masanri, Hwasun County, Haenam County, South Jeolla Province, about 10 kilometers southwest of Mokpo Airport. The news was reported by two surviving passengers who also survived the crash of Japan Airlines Flight 123, reported to the Hwawon-myeon branch of the village below the mountain, that the aircraft began to veer off course. However, there was also difficulty.\n\nAsiana Airlines announced that after the accident, the plane was slowed down by three landing attempts and that it appears to have crashed. Experts said the distance to the runway was 1500 meters shorter in one direction only. The runways did not have an ILS installed. Mokpo Airport was only equipped with VOR/DME, resulting in pilots performing excessive landing attempts in some cases. and was a conntributing cause of the accident. A prosecution in charge of investigating the accident, announced that the aircraft had disappeared from the normal flight route, and pilots were likely to make an unintentional landing with a misunderstanding. Chung Jong-hwan, the director general of the Ministry of Transportation said that the cause of the crash was pilot error; the actions of captain Hwang In-ki, who was killed in the crash, were the main cause of the accident. Hwang had started the aircraft's descent prematurely, while it was flying above a mountain peak. According to the cockpit voice recorder (CVR), Hwang flew the aircraft below the minimum safe altitude (1600 feet), as he said \"OK, 800 feet\" a few seconds before impact.\n\nThis was Asiana Airlines' first fatal (and as of 2018, deadliest) aircraft crash since the airline was established. After the accident Asiana suspended the Gimpo - Mokpo route. The airline paid compensation to anguished families of the victims. In addition, at the time the transportation department was planning to build Muan International Airport in Muan County, Jeolla Province. When Muan International Airport was opened in 2007, Mokpo Airport was closed and converted into a military base. The accident also caused Asiana to cancel their order of Boeing 757-200's and instead ordered the Airbus A321.\n\nAfter the crash of flight 733, Asiana Airlines had two more crashes in July the same year, resulting in what the airline referred to as the curse of seven.\n\nFlight 733 was the deadliest aviation accident in South Korea at that time. It was surpassed by Air China Flight 129, which crashed on April 15, 2002, with 129 fatalities. It was also the deadliest accident involving a Boeing 737-500 at that time. It was surpassed by Aeroflot Flight 821, which crashed on September 14, 2008, with 88 fatalities. As of 2018, flight 733 remains the second deadliest crash in both of these categories.\n\nAsiana Airlines still uses the flight number 733 on the late evening Seoul-Incheon - Hanoi route.\n\n\n"}
{"id": "6928882", "url": "https://en.wikipedia.org/wiki?curid=6928882", "title": "Biological hydrogen production (algae)", "text": "Biological hydrogen production (algae)\n\nThe biological hydrogen production with algae is a method of photobiological water splitting which is done in a closed photobioreactor based on the production of hydrogen as a solar fuel by algae. Algae produce hydrogen under certain conditions. In 2000 it was discovered that if \"C. reinhardtii\" algae are deprived of sulfur they will switch from the production of oxygen, as in normal photosynthesis, to the production of hydrogen.\n\nPhotosynthesis in cyanobacteria and green algae splits water into hydrogen ions and electrons. The electrons are transported over ferredoxins. Fe-Fe-hydrogenases (enzymes) combine them into hydrogen gas. In \"Chlamydomonas reinhardtii\" Photosystem II produces in direct conversion of sunlight 80% of the electrons that end up in the hydrogen gas. Light-harvesting complex photosystem II light-harvesting protein LHCBM9 promotes efficient light energy dissipation. The Fe-Fe-hydrogenases\nneed an anaerobic environment as they are inactivated by oxygen. Fourier transform infrared spectroscopy is used to examine metabolic pathways.\n\nThe chlorophyll (Chl) antenna size in green algae is minimized, or truncated, to maximize photobiological solar conversion efficiency and H production. The truncated Chl antenna size minimizes absorption and wasteful dissipation of sunlight by individual cells, resulting in better light utilization efficiency and greater photosynthetic productivity by the green alga mass culture.\n\nIn 1939 Hans Gaffron observed that the algae he was studying, \"Chlamydomonas reinhardtii\" (a green-algae), would sometimes switch from the production of oxygen to the production of hydrogen. He never discovered the cause of this change and for many years other scientists failed in their attempts to discover it. In the late 1990s, Anastasios Melis discovered that if the algae culture medium is deprived of sulfur it will switch from the production of oxygen (normal photosynthesis), to the production of hydrogen. He found that the enzyme responsible for this reaction is hydrogenase, but that the hydrogenase lost this function in the presence of oxygen. Melis found that depleting the amount of sulfur available to the algae interrupted its internal oxygen flow, allowing the hydrogenase an environment in which it can react, causing the algae to produce hydrogen. \"Chlamydomonas moewusii\" is also a good strain for the production of hydrogen.\n\nIt would take about 25,000 square kilometre algal farming to produce biohydrogen equivalent to the energy provided by gasoline in the US alone. This area represents approximately 10% of the area devoted to growing soya in the US.\n\n\nAttempts are in progress to solve these problems via bioengineering.\n\n\n"}
{"id": "4817", "url": "https://en.wikipedia.org/wiki?curid=4817", "title": "Biological membrane", "text": "Biological membrane\n\nA biological membrane or biomembrane is an enclosing or separating membrane that acts as a selectively permeable barrier within living things. Biological membranes, in the form of eukaryotic cell membranes, consist of a phospholipid bilayer with embedded, integral and peripheral proteins used in communication and transportation of chemicals and ions. The bulk of lipid in a cell membrane provides a fluid matrix for proteins to rotate and laterally diffuse for physiological functioning. Proteins are adapted to high membrane fluidity environment of lipid bilayer with the presence of an annular lipid shell, consisting of lipid molecules bound tightly to surface of integral membrane proteins. The cell membranes are different from the isolating tissues formed by layers of cells, such as mucous membranes, basement membranes, and serous membranes.\n\nThe lipid bilayer consists of two layers- an outer leaflet and an inner leaflet. The components of bilayers are distributed unequally between the two surfaces to create asymmetry between the outer and inner surfaces. This asymmetric organization is important for cell functions such as cell signaling. The asymmetry of the biological membrane reflects the different functions of the two leaflets of the membrane. As seen in the fluid membrane model of the phospholipid bilayer, the outer leaflet and inner leaflet of the membrane are asymmetrical in their composition. Certain proteins and lipids rest only on one surface of the membrane and not the other. \n\n• Both the plasma membrane and internal membranes have cytosolic and exoplasmic faces\n• This orientation is maintained during membrane trafficking – proteins, lipids, glycoconjugates facing the lumen of the ER and Golgi get expressed on the extracellular side of the plasma membrane. In eucaryotic cells, new phospholipids are manufactured by enzymes bound to the part of the endoplasmic reticulum membrane that faces the cytosol. These enzymes, which use free fatty acids as substrates, deposit all newly made phospholipids into the cytosolic half of the bilayer. To enable the membrane as a whole to grow evenly, half of the new phospholipid molecules then have to be transferred to the opposite monolayer. This transfer is catalyzed by enzymes called flippases. In the plasma membrane, flippases transfer specific phospholipids selectively, so that different types become concentrated in each monolayer.\n\nUsing selective flippases is not the only way to produce asymmetry in lipid bilayers, however. In particular, a different mechanism operates for glycolipids—the lipids that show the most striking and consistent asymmetric distribution in animal cells.\n\nThe biological membrane is made up of lipids with hydrophobic tails and hydrophilic heads. The hydrophobic tails are hydrocarbon tails whose length and saturation is important in characterizing the cell. Lipid rafts occur when lipid species and proteins aggregate in domains in the membrane. These help organize membrane components into localized areas that are involved in specific processes, such as signal transduction.\n\nRed blood cells, or erythrocytes, have a unique lipid composition. The bilayer of red blood cells is composed of cholesterol and phospholipids in equal proportions by weight. Erythrocyte membrane plays a crucial role in blood clotting. In the bilayer of red blood cells is phosphatidylserine. This is usually in the cytoplasmic side of the membrane. However, it is flipped to the outer membrane to be used during blood clotting.\n\nPhospholipid bilayers contain different proteins. These membrane proteins have various functions and characteristics and catalyze different chemical reactions. Integral proteins span the membranes with different domains on either side. Integral proteins hold strong association with the lipid bilayer and cannot easily become detached. They will dissociate only with chemical treatment that breaks the membrane. Peripheral proteins are unlike integral proteins in that they hold weak interactions with the surface of the bilayer and can easily become dissociated from the membrane. Peripheral proteins are located on only one face of a membrane and create membrane asymmetry.\nOligosaccharides are sugar containing polymers. In the membrane, they can be covalently bound to lipids to form glycolipids or covalently bound to proteins to form glycoproteins. Membranes contain sugar-containing lipid molecules known as glycolipids. In the bilayer, the sugar groups of glycolipids are exposed at the cell surface, where they can form hydrogen bonds. Glycolipids provide the most extreme example of asymmetry in the lipid bilayer. Glycolipids perform a vast number of functions in the biological membrane that are mainly communicative, including cell recognition and cell-cell adhesion. Glycoproteins are integral proteins. They play an important role in the immune response and protection.\n\nThe phospholipid bilayer is formed due to the aggregation of membrane lipids in aqueous solutions. Aggregation is caused by the hydrophobic effect, where hydrophobic ends come into contact with each other and are sequestered away from water. This arrangement maximises hydrogen bonding between hydrophilic heads and water while minimising unfavorable contact between hydrophobic tails and water. The increase in available hydrogen bonding increases the entropy of the system, creating a spontaneous process.\n\nBiological molecules are amphiphilic or amphipathic, i.e. are simultaneously hydrophobic and hydrophilic. The phospholipid bilayer contains charged hydrophilic headgroups, which interact with polar water. The lipids also contain hydrophobic tails, which meet with the hydrophobic tails of the complementary layer. The hydrophobic tails are usually fatty acids that differ in lengths. The interactions of lipids, especially the hydrophobic tails, determine the lipid bilayer physical properties such as fluidity.\n\nMembranes in cells typically define enclosed spaces or compartments in which cells may maintain a chemical or biochemical environment that differs from the outside. For example, the membrane around peroxisomes shields the rest of the cell from peroxides, chemicals that can be toxic to the cell, and the cell membrane separates a cell from its surrounding medium. Peroxisomes are one form of vacuole found in the cell that contain by-products of chemical reactions within the cell. Most organelles are defined by such membranes, and are called \"membrane-bound\" organelles.\n\nProbably the most important feature of a biomembrane is that it is a selectively permeable structure. This means that the size, charge, and other chemical properties of the atoms and molecules attempting to cross it will determine whether they succeed in doing so. Selective permeability is essential for effective separation of a cell or organelle from its surroundings. Biological membranes also have certain mechanical or elastic properties that allow them to change shape and move as required.\n\nGenerally, small hydrophobic molecules can readily cross phospholipid bilayers by simple diffusion.\n\nParticles that are required for cellular function but are unable to diffuse freely across a membrane enter through a membrane transport protein or are taken in by means of endocytosis, where the membrane allows for a vacuole to join onto it and push its contents into the cell. Many types of specialized plasma membranes can separate cell from external environment: apical, basolateral, presynaptic and postsynaptic ones, membranes of flagella, cilia, microvillus, filopodia and lamellipodia, the sarcolemma of muscle cells, as well as specialized myelin and dendritic spine membranes of neurons. Plasma membranes can also form different types of \"supramembrane\" structures such as caveolae, postsynaptic density, podosome, invadopodium, desmosome, hemidesmosome, focal adhesion, and cell junctions. These types of membranes differ in lipid and protein composition.\n\nDistinct types of membranes also create intracellular organelles: endosome; smooth and rough endoplasmic reticulum; sarcoplasmic reticulum; Golgi apparatus; lysosome; mitochondrion (inner and outer membranes); nucleus (inner and outer membranes); peroxisome; vacuole; cytoplasmic granules; cell vesicles (phagosome, autophagosome, clathrin-coated vesicles, COPI-coated and COPII-coated vesicles) and secretory vesicles (including synaptosome, acrosomes, melanosomes, and chromaffin granules).\nDifferent types of biological membranes have diverse lipid and protein compositions. The content of membranes defines their physical and biological properties. Some components of membranes play a key role in medicine, such as the efflux pumps that pump drugs out of a cell.\n\nThe hydrophobic core of the phospholipid bilayer is constantly in motion because of rotations around the bonds of lipid tails. Hydrophobic tails of a bilayer bend and lock together. However, because of hydrogen bonding with water, the hydrophilic head groups exhibit less movement as their rotation and mobility are constrained. This results in increasing viscosity of the lipid bilayer closer to the hydrophilic heads.\n\nBelow a transition temperature, a lipid bilayer loses fluidity when the highly mobile lipids exhibits less movement becoming a gel-like solid. The transition temperature depends on such components of the lipid bilayer as the hydrocarbon chain length and the saturation of its fatty acids. Temperature-dependence fluidity constitutes an important physiological attribute for bacteria and cold-blooded organisms. These organisms maintain a constant fluidity by modifying membrane lipid fatty acid composition in accordance with differing temperatures.\n\nIn animal cells, membrane fluidity is modulated by the inclusion of the sterol cholesterol. This molecule is present in especially large amounts in the plasma membrane, where it constitutes approximately 20% of the lipids in the membrane by weight. Because cholesterol molecules are short and rigid, they fill the spaces between neighboring phospholipid molecules left by the kinks in their unsaturated hydrocarbon tails. In this way, cholesterol tends to stiffen the bilayer, making it more rigid and less permeable.\n\nFor all cells, membrane fluidity is important for many reasons. It enables membrane proteins to diffuse rapidly in the plane of the bilayer and to interact with one another, as is crucial, for example, in cell signaling. It permits membrane lipids and proteins to diffuse from sites where they are inserted into the bilayer after their synthesis to other regions of the cell. It allows membranes to fuse with one another and mix their molecules, and it ensures that membrane molecules are distributed evenly between daughter cells when a cell divides. If biological membranes were not fluid, it is hard to imagine how cells could live, grow, and reproduce.\n\n\n"}
{"id": "14093841", "url": "https://en.wikipedia.org/wiki?curid=14093841", "title": "Book paper", "text": "Book paper\n\nA book paper (or publishing paper) is a paper that is designed specifically for the publication of printed books. \n\nTraditionally, book papers are off-white or low-white papers (easier to read), are opaque to minimise the show-through of text from one side of the page to the other, and are (usually) made to tighter caliper or thickness specifications, particularly for case-bound books. Typically, books papers are light-weight papers 60 to 90 g/m² and often specified by their caliper/substance ratios (volume basis). For example, a bulky 80 g/m² paper may have a caliper of 120 micrometres (0.12 mm) which would be Volume 15 (120×10/80), whereas a low bulk 80 g/m² may have a caliper of 88 micrometres, giving a volume 11. This volume basis then allows the calculation of a book's PPI (printed pages per inch), which is an important factor for the design of book jackets and the binding of the finished book. \n\nDifferent paper qualities can be used as book paper depending on the type of book. Machine-finished coated papers, woodfree uncoated papers, coated fine papers, and special fine papers are common paper grades.\n"}
{"id": "5668", "url": "https://en.wikipedia.org/wiki?curid=5668", "title": "Calcium", "text": "Calcium\n\nCalcium is a chemical element with symbol Ca and atomic number 20. An alkaline earth metal, calcium is a reactive metal that forms a dark oxide-nitride layer when exposed to air. Its physical and chemical properties are most similar to its heavier homologues strontium and barium. It is the fifth most abundant element in Earth's crust and the third most abundant metal, after iron and aluminium. The most common calcium compound on Earth is calcium carbonate, found in limestone and the fossilised remnants of early sea life; gypsum, anhydrite, fluorite, and apatite are also sources of calcium. The name derives from Latin \"calx\" \"lime\", which was obtained from heating limestone.\n\nSome calcium compounds were known to the ancients, though their chemistry was unknown until the seventeenth century. Pure calcium was isolated in 1808 via electrolysis of its oxide by Humphry Davy, who named the element. Calcium compounds are widely used in many industries: in foods and pharmaceuticals for calcium supplementation, in the paper industry as bleaches, as components in cement and electrical insulators, and in the manufacture of soaps. On the other hand, the metal in pure form has few applications due to its high reactivity; still, in small quantities it is often used as an alloying component in steelmaking, and sometimes, as a calcium–lead alloy, in making automotive batteries.\n\nCalcium is the most abundant metal and the fifth-most abundant element in the human body. As electrolytes, calcium ions play a vital role in the physiological and biochemical processes of organisms and cells: in signal transduction pathways where they act as a second messenger; in neurotransmitter release from neurons; in contraction of all muscle cell types; as cofactors in many enzymes; and in fertilization. Calcium ions outside cells are important for maintaining the potential difference across excitable cell membranes as well as proper bone formation.\n\nCalcium is a very ductile silvery metal (sometimes described as pale yellow) whose properties are very similar to the heavier elements in its group, strontium, barium, and radium. A calcium atom has twenty electrons, arranged in the electron configuration [Ar]4s. Like the other elements placed in group 2 of the periodic table, calcium has two valence electrons in the outermost s-orbital, which are very easily lost in chemical reactions to form a dipositive ion with the stable electron configuration of a noble gas, in this case argon. Hence, calcium is almost always divalent in its compounds, which are usually ionic. Hypothetical univalent salts of calcium would be stable with respect to their elements, but not to disproportionation to the divalent salts and calcium metal, because the enthalpy of formation of MX is much higher than those of the hypothetical MX. This occurs because of the much greater lattice energy afforded by the more highly charged Ca cation compared to the hypothetical Ca cation.\n\nCalcium, strontium, barium, and radium are always considered to be alkaline earth metals; the lighter beryllium and magnesium, also in group 2 of the periodic table, are often included as well. Nevertheless, beryllium and magnesium are significantly different from the other members of the group in their physical and chemical behaviour: they behave more like aluminium and zinc respectively and have some of the weaker metallic character of the post-transition metals, which is why the traditional definition of the term \"alkaline earth metal\" excludes them. This classification is mostly obsolete in English-language sources, but is still used in other countries such as Japan. As a result, comparisons with strontium and barium are more germane to calcium chemistry than comparisons with magnesium.\n\nCalcium metal melts at 842 °C and boils at 1494 °C; these values are higher than those for magnesium and strontium, the neighbouring group 2 metals. It crystallises in the face-centered cubic arrangement like strontium; above 450 °C, it changes to an anisotropic hexagonal close-packed arrangement like magnesium. Its density of 1.55 g/cm is the lowest in its group. Calcium is harder than lead but can be cut with a knife with effort. While calcium is a poorer conductor of electricity than copper or aluminium by volume, it is a better conductor by mass than both due to its very low density. While calcium is infeasible as a conductor for most terrestrial applications as it reacts quickly with atmospheric oxygen, its use as such in space has been considered.\n\nThe chemistry of calcium is that of a typical heavy alkaline earth metal. For example, calcium spontaneously reacts with water more quickly than magnesium and less quickly than strontium to produce calcium hydroxide and hydrogen gas. It also reacts with the oxygen and nitrogen in the air to form a mixture of calcium oxide and calcium nitride. When finely divided, it spontaneously burns in air to produce the nitride. In bulk, calcium is less reactive: it quickly forms a hydration coating in moist air, but below 30% relative humidity it may be stored indefinitely at room temperature.\n\nBesides the simple oxide CaO, the peroxide CaO can be made by direct oxidation of calcium metal under a high pressure of oxygen, and there is some evidence for a yellow superoxide Ca(O). Calcium hydroxide, Ca(OH), is a strong base, though it is not as strong as the hydroxides of strontium, barium or the alkali metals. All four dihalides of calcium are known. Calcium carbonate (CaCO) and calcium sulfate (CaSO) are particularly abundant minerals. Like strontium and barium, as well as the alkali metals and the divalent lanthanides europium and ytterbium, calcium metal dissolves directly in liquid ammonia to give a dark blue solution.\n\nDue to the large size of the Ca ion, high coordination numbers are common, up to 24 in some intermetallic compounds such as CaZn. Calcium is readily complexed by oxygen chelates such as EDTA and polyphosphates, which are useful in analytic chemistry and removing calcium ions from hard water. In the absence of steric hindrance, smaller group 2 cations tend to form stronger complexes, but when large polydentate macrocycles are involved the trend is reversed.\n\nAlthough calcium is in the same group as magnesium and organomagnesium compounds are very commonly used throughout chemistry, organocalcium compounds are not similarly widespread because they are more difficult to make and more reactive, although they have recently been investigated as possible catalysts. Organocalcium compounds tend to be more similar to organoytterbium compounds due to the similar ionic radii of Yb (102 pm) and Ca (100 pm). Most of these compounds can only be prepared at low temperatures; bulky ligands tend to favor stability. For example, calcium dicyclopentadienyl, Ca(CH), must be made by directly reacting calcium metal with mercurocene or cyclopentadiene itself; replacing the CH ligand with the bulkier C(CH) ligand on the other hand increases the compound's solubility, volatility, and kinetic stability.\n\nNatural calcium is a mixture of five stable isotopes (Ca, Ca, Ca, Ca, and Ca) and one isotope with a half-life so long that it can be considered stable for all practical purposes (Ca, with a half-life of about 4.3 × 10 years). Calcium is the first (lightest) element to have six naturally occurring isotopes.\n\nBy far the most common isotope of calcium in nature is Ca, which makes up 96.941% of all natural calcium. It is produced in the silicon-burning process from fusion of alpha particles and is the heaviest stable nuclide with equal proton and neutron numbers; its occurrence is also supplemented slowly by the decay of primordial K. Adding another alpha particle would lead to unstable Ti, which quickly decays via two successive electron captures to stable Ca; this makes up 2.806% of all natural calcium and is the second-most common isotope. The other four natural isotopes, Ca, Ca, Ca, and Ca, are significantly rarer, each comprising less than 1% of all natural calcium. The four lighter isotopes are mainly products of the oxygen-burning and silicon-burning processes, leaving the two heavier ones to be produced via neutron-capturing processes. Ca is mostly produced in a \"hot\" s-process, as its formation requires a rather high neutron flux to allow short-lived Ca to capture a neutron. Ca is produced by electron capture in the r-process in type Ia supernovae, where high neutron excess and low enough entropy ensures its survival.\n\nCa and Ca are the first \"classically stable\" nuclides with a six-neutron or eight-neutron excess respectively. Although extremely neutron-rich for such a light element, Ca is very stable because it is a doubly magic nucleus, having 20 protons and 28 neutrons arranged in closed shells. Its beta decay to Sc is very hindered because of the gross mismatch of nuclear spin: Ca has zero nuclear spin, being even–even, while Sc has spin 6+, so the decay is forbidden by the conservation of angular momentum. While two excited states of Sc are available for decay as well, they are also forbidden due to their high spins. As a result, when Ca does decay, it does so by double beta decay to Ti instead, being the lightest nuclide known to undergo double beta decay. The heavy isotope Ca can also theoretically undergo double beta decay to Ti as well, but this has never been observed; the lightest and most common isotope Ca is also doubly magic and could undergo double electron capture to Ar, but this has likewise never been observed. Calcium is the only element to have two primordial doubly magic isotopes. The experimental lower limits for the half-lives of Ca and Ca are 5.9 × 10 years and 2.8 × 10 years respectively.\n\nApart from the practically stable Ca, the longest lived radioisotope of calcium is Ca. It decays by electron capture to stable K with a half-life of about a hundred thousand years. Its existence in the early Solar System as an extinct radionuclide has been inferred from excesses of K: traces of Ca also still exist today, as it is a cosmogenic nuclide, continuously reformed through neutron activation of natural Ca. Many other calcium radioisotopes are known, ranging from Ca to Ca: they are all much shorter-lived than Ca, the most stable among them being Ca (half-life 163 days) and Ca (half-life 4.54 days). The isotopes lighter than Ca usually undergo beta plus decay to isotopes of potassium, and those heavier than Ca usually undergo beta minus decay to isotopes of scandium, although near the nuclear drip lines proton emission and neutron emission begin to be significant decay modes as well.\n\nLike other elements, a variety of processes alter the relative abundance of calcium isotopes. The best studied of these processes is the mass-dependent fractionation of calcium isotopes that accompanies the precipitation of calcium minerals such as calcite, aragonite and apatite from solution. Lighter isotopes are preferentially incorporated into these minerals, leaving the surrounding solution enriched in heavier isotopes at a magnitude of roughly 0.025% per atomic mass unit (amu) at room temperature. Mass-dependent differences in calcium isotope composition are conventionally expressed by the ratio of two isotopes (usually Ca/Ca) in a sample compared to the same ratio in a standard reference material. Ca/Ca varies by about 1% among common earth materials.\n\nCalcium compounds were known for millennia, although their chemical makeup was not understood until the 17th century. Lime as a building material and as plaster for statues was used as far back as around 7000 BC. The first dated lime kiln dates back to 2500 BC and was found in Khafajah, Mesopotamia. At about the same time, dehydrated gypsum (CaSO·2HO) was being used in the Great Pyramid of Giza; this material would later be used for the plaster in the tomb of Tutankhamun. The climate of present-day Italy being warmer than that of Egypt, the ancient Romans instead used lime mortars made by heating limestone (CaCO); the name \"calcium\" itself derives from the Latin word \"calx\" \"lime\". Vitruvius noted that the lime that resulted was lighter than the original limestone, attributing this to the boiling of the water; in 1755, Joseph Black proved that this was due to the loss of carbon dioxide, which as a gas had not been recognised by the ancient Romans.\n\nIn 1787, Antoine Lavoisier suspected that lime might be an oxide of a fundamental chemical element. In his table of the elements, Lavoisier listed five \"salifiable earths\" (i.e., ores that could be made to react with acids to produce salts (\"salis\" = salt, in Latin): \"chaux\" (calcium oxide), \"magnésie\" (magnesia, magnesium oxide), \"baryte\" (barium sulfate), \"alumine\" (alumina, aluminium oxide), and \"silice\" (silica, silicon dioxide). About these \"elements\", Lavoisier speculated: \n\nCalcium, along with its congeners magnesium, strontium, and barium, was first isolated by Humphry Davy in 1808. Following the work of Jöns Jakob Berzelius and Magnus Martin af Pontin on electrolysis, Davy isolated calcium and magnesium by putting a mixture of the respective metal oxides with mercury(II) oxide on a platinum plate which was used as the anode, the cathode being a platinum wire partially submerged into mercury. Electrolysis then gave calcium–mercury and magnesium–mercury amalgams, and distilling off the mercury gave the metal. However, pure calcium cannot be prepared in bulk by this method and a workable commercial process for its production was not found until over a century later.\n\nAt 3%, calcium is the fifth most abundant element in the Earth's crust, and the third most abundant metal behind aluminium and iron. It is also the fourth most abundant element in the lunar highlands. Sedimentary calcium carbonate deposits pervade the Earth's surface as fossilized remains of past marine life; they occur in two forms, the rhombohedral calcite (more common) and the orthorhombic aragonite (forming in more temperate seas). Minerals of the first type include limestone, dolomite, marble, chalk, and iceland spar; aragonite beds make up the Bahamas, the Florida Keys, and the Red Sea basins. Corals, sea shells, and pearls are mostly made up of calcium carbonate. Among the other important minerals of calcium are gypsum (CaSO·2HO), anhydrite (CaSO), fluorite (CaF), and apatite ([Ca(PO)F]).\n\nThe major producers of calcium are China (about 10000 to 12000 tonnes per year), Russia (about 6000 to 8000 tonnes per year), and the United States (about 2000 to 4000 tonnes per year). Canada and France are also among the minor producers. In 2005, about 24000 tonnes of calcium were produced; about half of the world's extracted calcium is used by the United States, with about 80% of the output used each year. In Russia and China, Davy's method of electrolysis is still used, but is instead applied to molten calcium chloride. Since calcium is less reactive than strontium or barium, the oxide–nitride coating that results in air is stable and lathe machining and other standard metallurgical techniques are suitable for calcium. In the United States and Canada, calcium is instead produced by reducing lime with aluminium at high temperatures.\n\nCalcium provides a link between tectonics, climate, and the carbon cycle. In the simplest terms, uplift of mountains exposes calcium-bearing rocks to chemical weathering and releases Ca into surface water. These ions are transported to the ocean where they react with dissolved CO to form limestone (), which in turn settles to the sea floor where it is incorporated into new rocks. Dissolved CO, along with carbonate and bicarbonate ions, are termed \"dissolved inorganic carbon\" (DIC).\n\nThe actual reaction is more complicated and involves the bicarbonate ion (HCO) that forms when CO reacts with water at seawater pH:\nAt seawater pH, most of the CO is immediately converted back into . The reaction results in a net transport of one molecule of CO from the ocean/atmosphere into the lithosphere. The result is that each Ca ion released by chemical weathering ultimately removes one CO molecule from the surficial system (atmosphere, ocean, soils and living organisms), storing it in carbonate rocks where it is likely to stay for hundreds of millions of years. The weathering of calcium from rocks thus scrubs CO from the ocean and atmosphere, exerting a strong long-term effect on climate.\n\nThe largest use of calcium is in steelmaking, due to its strong chemical affinity for oxygen and sulfur. Its oxides and sulfides, once formed, give liquid lime aluminate and sulfide inclusions in steel which float out; on treatment, these inclusions disperse throughout the steel and became small and spherical, improving castability, cleanliness and general mechanical properties. Calcium is also used in maintenance-free automotive batteries, in which the use of 0.1% calcium–lead alloys instead of the usual antimony–lead alloys leads to lower water loss and lower self-discharging. Due to the risk of expansion and cracking, aluminium is sometimes also incorporated into these alloys. These lead–calcium alloys are also used in casting, replacing lead–antimony alloys. Calcium is also used to strengthen aluminium alloys used for bearings, for the control of graphitic carbon in cast iron, and to remove bismuth impurities from lead. Calcium metal is found in some drain cleaners, where it functions to generate heat and calcium hydroxide that saponifies the fats and liquefies the proteins (for example, those in hair) that block drains. Besides metallurgy, the reactivity of calcium is exploited to remove nitrogen from high-purity argon gas and as a getter for oxygen and nitrogen. It is also used as a reducing agent in the production of chromium, zirconium, thorium, and uranium. It can also be used to store hydrogen gas, as it reacts with hydrogen to form solid calcium hydride, from which the hydrogen can easily be re-extracted.\n\nCalcium isotope fractionation during mineral formation has led to several applications of calcium isotopes. In particular, the 1997 observation by Skulan and DePaolo that calcium minerals are isotopically lighter than the solutions from which the minerals precipitate is the basis of analogous applications in medicine and in paleooceanography. In animals with skeletons mineralized with calcium, the calcium isotopic composition of soft tissues reflects the relative rate of formation and dissolution of skeletal mineral. In humans, changes in the calcium isotopic composition of urine have been shown to be related to changes in bone mineral balance. When the rate of bone formation exceeds the rate of bone resorption, the Ca/Ca ratio in soft tissue rises and vice versa. Because of this relationship, calcium isotopic measurements of urine or blood may be useful in the early detection of metabolic bone diseases like osteoporosis. A similar system exists in seawater, where Ca/Ca tends to rise when the rate of removal of Ca by mineral precipitation exceeds the input of new calcium into the ocean. In 1997 Skulan and DePaolo presented the first evidence of change in seawater Ca/Ca over geologic time, along with a theoretical explanation of these changes. More recent papers have confirmed this observation, demonstrating that seawater Ca concentration is not constant, and that the ocean is never in a \"steady state\" with respect to calcium input and output. This has important climatological implications, as the marine calcium cycle is closely tied to the carbon cycle.\n\nMany calcium compounds are used in food, as pharmaceuticals, and in medicine, among others. For example, calcium and phosphorus are supplemented in foods through the addition of calcium lactate, calcium diphosphate, and tricalcium phosphate. The last is also used as a polishing agent in toothpaste and in antacids. Calcium lactobionate is a white powder that is used as a suspending agent for pharmaceuticals. In baking, calcium monophosphate is used as a leavening agent. Calcium sulfite is used as a bleach in papermaking and as a disinfectant, calcium silicate is used as a reinforcing agent in rubber, and calcium acetate is a component of liming rosin and is used to make metallic soaps and synthetic resins.\n\nCalcium is an essential element needed in large quantities. The Ca ion acts as an electrolyte and is vital to the health of the muscular, circulatory, and digestive systems; is indispensable to the building of bone; and supports synthesis and function of blood cells. For example, it regulates the contraction of muscles, nerve conduction, and the clotting of blood. As a result, intra- and extracellular calcium levels are tightly regulated by the body. Calcium can play this role because the Ca ion forms stable coordination complexes with many organic compounds, especially proteins; it also forms compounds with a wide range of solubilities, enabling the formation of skeletons.\n\nCalcium ions may be complexed by proteins through binding the carboxyl groups of glutamic acid or aspartic acid residues; through interacting with phosphorylated serine, tyrosine, or threonine residues; or by being chelated by γ-carboxylated amino acid residues. Trypsin, a digestive enzyme, uses the first method; osteocalcin, a bone matrix protein, uses the third. Some other bone matrix proteins such as osteopontin and bone sialoprotein use both the first and the second. Direct activation of enzymes by binding calcium is common; some other enzymes are activated by noncovalent association with direct calcium-binding enzymes. Calcium also binds to the phospholipid layer of the cell membrane, anchoring proteins associated with the cell surface. As an example of the wide range of solubility of calcium compounds, monocalcium phosphate is very soluble in water, 85% of extracellular calcium is as dicalcium phosphate with a solubility of 2.0 mM and the hydroxyapatite of bones in an organic matrix is tricalcium phosphate at 100 µM.\n\nAbout three-quarters of dietary calcium is from dairy products and grains, the rest being accounted for by vegetables, protein-rich foods, fruits, sugar, fats, and oil. Calcium supplementation is controversial, as the bioavailability of calcium is strongly dependent on the solubility of the salt involved: calcium citrate, malate, and lactate are highly bioavailable while the oxalate is much less so. The intestine absorbs about one-third of calcium eaten as the free ion, and plasma calcium level is then regulated by the kidneys. Parathyroid hormone and vitamin D promote the formation of bone by allowing and enhancing the deposition of calcium ions there, allowing rapid bone turnover without affecting bone mass or mineral content. When plasma calcium levels fall, cell surface receptors are activated and the secretion of parathyroid hormone occurs; it then proceeds to stimulate the entry of calcium into the plasma pool by taking it from targeted kidney, gut, and bone cells, with the bone-forming action of parathyroid hormone being antagonised by calcitonin, whose secretion increases with increasing plasma calcium levels.\n\nExcess intake of calcium may cause hypercalcaemia. However, because calcium is absorbed rather inefficiently by the intestines, high serum calcium is more likely caused by excessive secretion of parathyroid hormone (PTH) or possibly by excessive intake of vitamin D, both which facilitate calcium absorption. It may also be due to bone destruction that occurs when tumours metastasize within bone. All these conditions result in excess calcium salts being deposited in the heart, blood vessels, or kidneys. Symptoms include anorexia, nausea, vomiting, memory loss, confusion, muscle weakness, increased urination, dehydration, and metabolic bone disease. Chronic hypercalcaemia typically leads to calcification of soft tissue and its serious consequences: for example, calcification can cause loss of elasticity of vascular walls and disruption of laminar blood flow—and thence to plaque rupture and thrombosis. Conversely, inadequate calcium or vitamin D intakes may result in hypocalcaemia, often caused also by inadequate secretion of parathyroid hormone or defective PTH receptors in cells. Symptoms include neuromuscular excitability, which potentially causes tetany and disruption of conductivity in cardiac tissue.\n\nAs calcium is heavily involved in bone manufacture, many bone diseases can be traced to problems with the organic matrix or the hydroxyapatite in molecular structure or organisation. For example, osteoporosis is a reduction in mineral content of bone per unit volume, and can be treated by supplementation of calcium, vitamin D, and biphosphates. Calcium supplements may benefit the serum lipids in women who have passed menopause as well as older men; in post-menopausal women calcium supplementation also appears to be inversely correlated with cardiovascular disease. Inadequate amounts of calcium, vitamin D, or phosphates can lead to the softening of bones, known as osteomalacia.\n\nBecause calcium reacts exothermically with water and acids, calcium metal coming into contact with bodily moisture results in severe corrosive irritation. When swallowed, calcium metal has the same effect on the mouth, oesophagus, and stomach, and can be fatal. However, long-term exposure is not known to have distinct adverse effects.\n\nBecause of concerns for long-term adverse side effects, including calcification of arteries and kidney stones, both the U.S. Institute of Medicine (IOM) and the European Food Safety Authority (EFSA) set Tolerable Upper Intake Levels (ULs) for combined dietary and supplemental calcium. From the IOM, people of ages 9–18 years are not to exceed 3,000 mg/day combined intake; for ages 19–50, not to exceed 2,500 mg/day; for ages 51 and older, not to exceed 2,000 mg/day. The EFSA set the UL for all adults at 2,500 mg/day, but decided the information for children and adolescents was not sufficient to determine ULs.\n\n\n"}
{"id": "28057645", "url": "https://en.wikipedia.org/wiki?curid=28057645", "title": "Carbonite (explosive)", "text": "Carbonite (explosive)\n\nCarbonite was one of the earliest and most successful coal-mining explosives. It is made from such ingredients as nitroglycerin, wood meal, and some nitrate as that of sodium; also nitrobenzene, saltpeter, sulfur, and diatomaceous earth. Carbonite was invented by Bichel of Schmidt and Bichel.\n\nThe term Carbonite can refer to these things: \n"}
{"id": "6740960", "url": "https://en.wikipedia.org/wiki?curid=6740960", "title": "Cheerios effect", "text": "Cheerios effect\n\nIn fluid mechanics, the Cheerios effect is the phenomenon that occurs when floating objects that don't normally float attract one another. Wetting, an example of the \"Cheerios effect,\" is when breakfast cereal clumps together or clings to the sides of a bowl of milk. It is named after the common breakfast cereal Cheerios and is due to surface tension. The same effect governs the behavior of bubbles on the surface of soft drinks. It's a purely gravitational phenomena.\n\nThis clumping behaviour applies to any small macroscopic object that floats or clings to the surface of a liquid. Examples of such objects are hair particles in shaving cream and fizzy beer bubbles. The effect is not noticeable in boats and other large floating objects because the force of surface tension is relatively small at that scale.\n\nAt the interface between a liquid and air, molecules of the liquid are subject to greater attractive forces from those below than from air molecules. Opposing these forces is the attraction of the liquid molecules to the surface of the container. The result is that the liquid's surface forms a meniscus which exhibits surface tension and acts as a flexible membrane. This membrane may be curved with the center either higher or lower than the edges. \n\nThe attraction isn't created by the depression or hills per se, but the objects are just following the path of least resistance.\n\nThe object that creates the hill does so because it's less dense than water but more dense than air. The object is actually creating a depression, not in the water but in the air above it. Like a heavy ball on a hill of air, the object will fall \"down\", because all the heavy water \"above\" it is pushing it. \n\nThe attraction between objects that create depressions can be seen as 2 balls in a trampoline, which have a kind of hill between them but they still fall into each other because the \"hill\" at the opposite side is larger than that in the middle. You only see the depression around the object where the bend is enough to be noticeable, but it reaches the edges of the container. \n\nA floating object will seek the highest point of the membrane and thus will find its way to either the center or the edge. A similar argument explains why bubbles on surfaces attract each other: a single bubble raises the liquid level locally causing other bubbles in the area to be attracted to it. Dense objects, like paper clips, can rest on liquid surfaces due to surface tension. These objects deform the liquid surface downward. Other floating objects that are seeking to sink but are constrained by surface tension will be attracted to the first. Objects with an irregular meniscus also deform the water surface forming \"capillary multipoles\". When such objects come close to each other they rotate in the plane of the water surface until they find an optimum relative orientation. Subsequently, they are attracted to each other by surface tension.\n\nWriting in the American Journal of Physics, Dominic Vella and L. Mahadevan of Harvard University discuss the Cheerios effect and suggest that it may be useful in the study of the self-assembly of small structures. They calculate the force between two spheres of density formula_1 and radius formula_2 floating distance formula_3 apart in liquid of density formula_4 as\n\nwhere formula_6 is the surface tension, formula_7 is a modified Bessel function of the first kind, formula_8 is the Bond number, and\n\nis a nondimensional factor in terms of the contact angle formula_10.\nHere formula_11 is a convenient meniscus length scale.\n\n"}
{"id": "21347411", "url": "https://en.wikipedia.org/wiki?curid=21347411", "title": "Chemical compound", "text": "Chemical compound\n\nA chemical compound is a chemical substance composed of many identical molecules (or molecular entities) composed of atoms from more than one element held together by chemical bonds. A chemical element bonded to an identical chemical element is not a chemical compound since only one element, not two different elements, is involved.\n\nThere are four types of compounds, depending on how the constituent atoms are held together:\n\nA chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using the standard abbreviations for the chemical elements, and subscripts to indicate the number of atoms involved. For example, water is composed of two hydrogen atoms bonded to one oxygen atom: the chemical formula is HO. Many chemical compounds have a unique numerical identifier assigned by the Chemical Abstracts Service (CAS): its CAS number.\n\nA compound can be converted to a different chemical composition by interaction with a second chemical compound via a chemical reaction. In this process, bonds between atoms are broken in both of the interacting compounds, and then bonds are reformed so that new associations are made between atoms. \n\nAny substance consisting of two or more different types of atoms (chemical elements) in a fixed stoichiometric proportion can be termed a \"chemical compound\"; the concept is most readily understood when considering pure chemical substances. It follows from their being composed of fixed proportions of two or more types of atoms that chemical compounds can be converted, via chemical reaction, into compounds or substances each having fewer atoms. The ratio of each element in the compound is expressed in a ratio in its chemical formula. A chemical formula is a way of expressing information about the proportions of atoms that constitute a particular chemical compound, using the standard abbreviations for the chemical elements, and subscripts to indicate the number of atoms involved. For example, water is composed of two hydrogen atoms bonded to one oxygen atom: the chemical formula is HO. In the case of non-stoichiometric compounds, the proportions may be reproducible with regard to their preparation, and give fixed proportions of their component elements, but proportions that are not integral [e.g., for palladium hydride, PdH (0.02 < x < 0.58)].\n\nChemical compounds have a unique and defined chemical structure held together in a defined spatial arrangement by chemical bonds. Chemical compounds can be molecular compounds held together by covalent bonds, salts held together by ionic bonds, intermetallic compounds held together by metallic bonds, or the subset of chemical complexes that are held together by coordinate covalent bonds. Pure chemical elements are generally not considered chemical compounds, failing the two or more atom requirement, though they often consist of molecules composed of multiple atoms (such as in the diatomic molecule H, or the polyatomic molecule S, etc.). Many chemical compounds have a unique numerical identifier assigned by the Chemical Abstracts Service (CAS): its CAS number.\n\nThere is varying and sometimes inconsistent nomenclature differentiating substances, which include truly non-stoichiometric examples, from chemical compounds, which require the fixed ratios. Many solid chemical substances—for example many silicate minerals—are chemical substances, but do not have simple formulae reflecting chemically bonding of elements to one another in fixed ratios; even so, these crystalline substances are often called \"non-stoichiometric compounds\". It may be argued that they are related to, rather than being chemical compounds, insofar as the variability in their compositions is often due to either the presence of foreign elements trapped within the crystal structure of an otherwise known true \"chemical compound\", or due to perturbations in structure relative to the known compound that arise because of an excess of deficit of the constituent elements at places in its structure; such non-stoichiometric substances form most of the crust and mantle of the Earth. Other compounds regarded as chemically identical may have varying amounts of heavy or light isotopes of the constituent elements, which changes the ratio of elements by mass slightly.\n\nCompounds are held together through a variety of different types of bonding and forces. The differences in the types of bonds in compounds differ based on the types of elements present in the compound.\n\nLondon dispersion forces are the weakest force of all intermolecular forces. They are temporary attractive forces that form when the electrons in two adjacent atoms are positioned so that they create a temporary dipole. Additionally, London dispersion forces are responsible for condensing non polar substances to liquids, and to further freeze to a solid state dependent on how low the temperature of the environment is.\n\nA covalent bond, also known as a molecular bond, involves the sharing of electrons between two atoms. Primarily, this type of bond occurs between elements that fall close to each other on the periodic table of elements, yet it is observed between some metals and nonmetals. This is due to the mechanism of this type of bond. Elements that fall close to each other on the periodic table tend to have similar electronegativities, which means they have a similar affinity for electrons. Since neither element has a stronger affinity to donate or gain electrons, it causes the elements to share electrons so both elements have a more stable octet.\n\nIonic bonding occurs when valence electrons are completely transferred between elements. Opposite to covalent bonding, this chemical bond creates two oppositely charged ions. The metals in ionic bonding usually lose their valence electrons, becoming a positively charged cation. The nonmetal will gain the electrons from the metal, making the nonmetal a negatively charged anion. As outlined, ionic bonds occur between an electron donor, usually a metal, and an electron acceptor, which tends to be a nonmetal.\n\nHydrogen bonding occurs when a hydrogen atom bonded to an electronegative atom forms an electrostatic connection with another electronegative atom through interacting dipoles or charges.\n\nA compound can be converted to a different chemical composition by interaction with a second chemical compound via a chemical reaction. In this process, bonds between atoms are broken in both of the interacting compounds, and then bonds are reformed so that new associations are made between atoms. Schematically, this reaction could be described as , where A, B, C, and D are each unique atoms; and AB, AD, CD, and CB are each unique compounds.\n\n"}
{"id": "1158235", "url": "https://en.wikipedia.org/wiki?curid=1158235", "title": "Debye length", "text": "Debye length\n\nIn plasmas and electrolytes, the Debye length (also called Debye radius), named after Peter Debye, is a measure of a charge carrier's net electrostatic effect in solution and how far its electrostatic effect persists. A Debye sphere is a volume whose radius is the Debye length. With each Debye length, charges are increasingly electrically screened. Every Debye‐length formula_1, the electric potential will decrease in magnitude by 1/e. Debye length is an important parameter in plasma physics, electrolytes, and colloids (DLVO theory). The corresponding Debye screening wave vector formula_2 \nfor particles of density formula_3, charge formula_4 at a temperature formula_5 is given by formula_6 in Gaussian units. Expressions in MKS units will be given below. The analogous quantities at very low temperatures (formula_7) are known as the Thomas-Fermi length and the Thomas-Fermi wave vector. They are of interest in describing the behaviour of electrons in metals at room temperature.\n\nThe Debye length arises naturally in the thermodynamic description of large systems of mobile charges. In a system of formula_8 different species of charges, the formula_9-th species carries charge formula_10 and has concentration formula_11 at position formula_12. According to the so-called \"primitive model\", these charges are distributed in a continuous medium that is characterized only by its relative static permittivity, formula_13.\nThis distribution of charges within this medium gives rise to an electric potential formula_14 that satisfies Poisson's equation:\n\nwhere formula_16, formula_17 is the electric constant, and formula_18 is a charge density external (logically, not spatially) to the medium.\n\nThe mobile charges not only establish formula_14 but also move in response to the associated Coulomb force, formula_20.\nIf we further assume the system to be in thermodynamic equilibrium with a heat bath at absolute temperature formula_5, then the concentrations of discrete charges, formula_11, may be considered to be thermodynamic (ensemble) averages and the associated electric potential to be a thermodynamic mean field.\nWith these assumptions, the concentration of the formula_9-th charge species is described\nby the Boltzmann distribution,\n\nwhere formula_25 is Boltzmann's constant and where formula_26 is the mean\nconcentration of charges of species formula_9.\n\nIdentifying the instantaneous concentrations and potential in the Poisson equation with their mean-field counterparts in Boltzmann's distribution yields the Poisson–Boltzmann equation:\n\nSolutions to this nonlinear equation are known for some simple systems. Solutions for more general\nsystems may be obtained in the high-temperature (weak coupling) limit, formula_29, by Taylor expanding the exponential:\n\nThis approximation yields the linearized Poisson-Boltzmann equation\nwhich also is known as the Debye–Hückel equation:\nThe second term on the right-hand side vanishes for systems that are electrically neutral. The term in parentheses divided by formula_32, has the units of an inverse length squared and by\ndimensional analysis leads to the definition of the characteristic length scale\n\nthat commonly is referred to as the Debye–Hückel length. As the only characteristic length scale in the Debye–Hückel equation, formula_1 sets the scale for variations in the potential and in the concentrations of charged species. All charged species contribute to the Debye–Hückel length in the same way, regardless of the sign of their charges. For an electrically neutral system, the Poisson equation becomes\nTo illustrate Debye screening, the potential produced by an external point charge formula_36 is\nThe bare Coulomb potential is exponentially screened by the medium, over a distance of the Debye length.\n\nThe Debye–Hückel length may be expressed in terms of the Bjerrum length formula_38 as\n\nwhere formula_40 is the integer charge number that relates the charge on the formula_9-th ionic\nspecies to the elementary charge formula_42.\n\nIn space plasmas where the electron density is relatively low, the Debye length may reach macroscopic values, such as in the magnetosphere, solar wind, interstellar medium and intergalactic medium (see table):\n\nIn a non-isothermic plasma, the temperatures for electrons and heavy species may differ while the background medium may be treated as the vacuum (formula_43), and the Debye length is\n\nwhere\n\nEven in quasineutral cold plasma, where ion contribution virtully seems to be larger due to lower ion temperature, the ion term is actually often dropped, giving\n\nalthough this is only valid when the mobility of ions is negligible compared to the process's timescale.\n\nIn an electrolyte or a colloidal suspension, the Debye length for a monovalent electrolyte is usually denoted with symbol \"κ\"\n\nwhere\n\nor, for a symmetric monovalent electrolyte,\n\nwhere\n\nAlternatively,\n\nwhere\nFor water at room temperature, \"λ\" ≈ 0.7 nm.\n\nAt room temperature (25 °C), one can consider in water the relation:\n\nwhere\n\nThe Debye length has become increasingly significant in the modeling of solid state devices as improvements in lithographic technologies have enabled smaller geometries.\n\nThe Debye length of semiconductors is given:\n\nwhere\n\nWhen doping profiles exceed the Debye length, majority carriers no longer behave according to the distribution of the dopants. Instead, a measure of the profile of the doping gradients provides an \"effective\" profile that better matches the profile of the majority carrier density.\n\nIn the context of solids, the Debye length is also called the Thomas–Fermi screening length.\n\n\n"}
{"id": "859275", "url": "https://en.wikipedia.org/wiki?curid=859275", "title": "Displacement (vector)", "text": "Displacement (vector)\n\nA displacement is a vector whose length is the shortest distance from the initial to the final position of a point P. It quantifies both the distance and direction of an imaginary motion along a straight line from the initial position to the final position of the point. A displacement may be identified with the translation that maps the initial position to the final position.\n\nA displacement may be also described as a 'relative position': the final position of a point (x) relative to its initial position (x), and a displacement vector can be mathematically defined as the difference between the final and initial positions:\n\nIn considering motions of objects over time the instantaneous velocity of the object is the rate of change of the displacement as a function of time. The instantaneous speed then is distinct from velocity, or the time rate of change of the distance traveled along a specific path. The velocity may be equivalently defined as the time rate of change of the position vector. If one considers a moving initial position, or equivalently a moving origin (e.g. an initial position or origin which is fixed to a train wagon, which in turn moves with respect to its rail track), the velocity of P (e.g. a point representing the position of a passenger walking on the train) may be referred to as a relative velocity, as opposed to an absolute velocity, which is computed with respect to a point which is considered to be 'fixed in space' (such as, for instance, a point fixed on the floor of the train station).\n\nFor motion over a given interval of time, the displacement divided by the length of the time interval defines the average velocity. (Note that the average velocity, as a vector, differs from the average speed that is the ratio of the path length — a scalar — and the time interval.)\n\nIn dealing with the motion of a rigid body, the term \"displacement\" may also include the rotations of the body. In this case, the displacement of a particle of the body is called linear displacement (displacement along a line), while the rotation of the body is called angular displacement.\n\nFor a position vector s that is a function of time \"t\", the derivatives can be computed with respect to \"t\". These derivatives have common utility in the study of kinematics, control theory, vibration sensing and other sciences and engineering disciplines.\nVelocity\n\nAcceleration\n\nJerk\n\nThese common names correspond to terminology used in basic kinematics. By extension, the higher order derivatives can be computed in a similar fashion. Study of these higher order derivatives can improve approximations of the original displacement function. Such higher-order terms are required in order to accurately represent the displacement function as a sum of an infinite series, enabling several analytical techniques in engineering and physics. The fourth order derivative is called jounce.\n\n"}
{"id": "19451484", "url": "https://en.wikipedia.org/wiki?curid=19451484", "title": "Enel Corugea Wind Farm", "text": "Enel Corugea Wind Farm\n\nThe Enel Corugea Wind Farm is a wind power project in Tulcea County, Romania. It will have 35 Vestas V90 wind turbines with a nominal output of 2 MW which will deliver up to 70 MW of power. Construction started in May 2011.\n"}
{"id": "10576388", "url": "https://en.wikipedia.org/wiki?curid=10576388", "title": "Environmental impact-minimizing vehicle tuning", "text": "Environmental impact-minimizing vehicle tuning\n\nEnvironmental impact-minimizing vehicle tuning is the modification (or tuning) of cars to reduce energy consumption.\n\n\n\n"}
{"id": "24330963", "url": "https://en.wikipedia.org/wiki?curid=24330963", "title": "Floodline", "text": "Floodline\n\nFloodline is the flood warning system used in the United Kingdom to issue flood alerts and warnings to the public, emergency organisations and businesses. \nThe system uses observed data from rain, river and coastal gauges, combined with weather forecasts to accurately predict the likelihood and timing of flooding. When flooding is predicted within an area, a message is issued through the Floodline service. The service is accessed online or by the dedicated phone line, 0345 988 1188. \n\nThere are different messages the Floodline gives depending on the situation:\n\nFlooding is possible, be prepared. \nThe Environment Agency endeavour to issue this message up to 24 hours in advance of expected river and coastal flooding, but warning time may be as little as 2 hrs. \nFlooding of low lying land is expected. \nFlood Alerts are early warning messages about possible flooding. They prompt you to remain alert and vigilant and provide people with time to make early preparations for potential flooding. \nFlood Alerts are issued for geographically representative areas, usually matching Local Authority boundaries.\nThey are generally 9am-5pm, 7 days a week.In exceptional circumstances, alerts may be issued outside these hours.\n\nFlooding is expected, immediate action required.\nThe target for these to be issued is 3-6 hours in advance of expected flooding. It may not be possible to give 3 hours’ notice in areas prone to rapid flooding or when water levels have escalated quicker than expected.\nFlooding is imminent.\nImmediate action is required, take measures to protect yourself and your property.\nThey are issued 24 hours a day.\nSevere flooding, danger to life.\nThese are issued whenever severe flooding is likely to cause significant risk to life, destruction of properties or local communities. \nFlooding is imminent and could pose a risk to life and cause significant disruption to essential services, such as water and electricity supplies. \nPrepare to evacuate and co-operate with the emergency services.\nThese are issued 24 hours a day.\nThese are issued to notify you when warnings and alerts are no longer in force.\nNo further flooding is currently expected for your area.\nFlood waters may still be around but you can start the clean up process.\n24 hours a day.\n\nYou can check if there are flood alerts and warnings near you by going to http://apps.environment-agency.gov.uk/flood/31618.aspx \nIt is operated by:\n\n"}
{"id": "30502582", "url": "https://en.wikipedia.org/wiki?curid=30502582", "title": "Flora Neotropica", "text": "Flora Neotropica\n\nFlora Neotropica is a series of monographs published by the New York Botanical Garden Press, and is the official publication of the Organization for Flora Neotropica. It covers the taxonomic treatment of American plants and plant families in the region of the Tropic of Cancer to the Tropic of Capricorn. The journal is edited by Thomas A. Zanoni (New York Botanical Garden). The journal was established in 1967 and is published on an irregular basis.\n\n"}
{"id": "2451331", "url": "https://en.wikipedia.org/wiki?curid=2451331", "title": "History of the Teller–Ulam design", "text": "History of the Teller–Ulam design\n\nThis article chronicles the history and origins of the Teller–Ulam design, the technical concept behind modern thermonuclear weapons, also known as hydrogen bombs. This design, the details of which are military secrets known to only a handful of major nations, is believed to be used in virtually all modern nuclear weapons which make up the arsenals of the major nuclear powers.\n\nThe idea of using the energy from a fission device to begin a fusion reaction was first proposed by the Italian physicist Enrico Fermi to his colleague Edward Teller in the fall of 1941 during what would soon become the Manhattan Project, the World War II effort by the United States and United Kingdom to develop the first nuclear weapons. Teller soon was a participant at Robert Oppenheimer's summer conference on the development of a fission bomb held at the University of California, Berkeley, where he guided discussion towards the idea of creating his \"Super\" bomb, which would hypothetically be many times more powerful than the yet-undeveloped fission weapon. Teller assumed creating the fission bomb would be nothing more than an engineering problem, and that the \"Super\" provided a much more interesting theoretical challenge.\n\nFor the remainder of the war, however, the effort was focused on first developing fission weapons. Nevertheless, Teller continued to pursue the \"Super\", to the point of neglecting work assigned to him for the fission weapon at the secret Los Alamos lab where he worked (much of the work Teller declined to do was given instead to Klaus Fuchs, who was later discovered to be a spy for the Soviet Union). Teller was given some resources with which to study the \"Super\", and contacted his friend Maria Göppert-Mayer to help with laborious calculations relating to opacity. The \"Super\", however, proved elusive, and the calculations were incredibly difficult to perform, especially since there was no existing way to run small-scale tests of the principles involved (in comparison, the properties of fission could be more easily probed with cyclotrons, newly created nuclear reactors, and various other tests).\n\nAfter the atomic bombings of Japan, many scientists at Los Alamos rebelled against the notion of creating a weapon thousands of times more powerful than the first atomic bombs. For the scientists the question was in part technical—the weapon design was still quite uncertain and unworkable—and in part moral: such a weapon, they argued, could only be used against large civilian populations, and could thus only be used as a weapon of genocide. Many scientists, such as Teller's colleague Hans Bethe (who had discovered stellar nucleosynthesis, the nuclear fusion which takes place in the stars), urged that the United States should not develop such weapons and set an example towards the Soviet Union. Promoters of the weapon, including Teller and Berkeley physicists Ernest Lawrence and Luis Alvarez, argued that such a development was inevitable, and to deny such protection to the people of the United States—especially when the Soviet Union was likely to create such a weapon themselves—was itself an immoral and unwise act. Still others, such as Oppenheimer, simply thought that the existing stockpile of fissile material was better spent in attempting to develop a large arsenal of tactical atomic weapons rather than potentially squandered on the development of a few massive \"Supers\". \n\nWhen the Soviet Union exploded their own atomic bomb (dubbed \"Joe 1\" by the U.S.) in 1949, it caught Western analysts off guard, and in early 1950 President Harry S. Truman ordered a program to develop a hydrogen bomb. Many scientists returned to Los Alamos to work on the \"Super\" program, but the initial attempts still seemed highly unworkable. In the \"classical Super\", it was thought that the \"heat\" alone from the fission bomb would be used to ignite the fusion material, but this proved to be impossible. For a while, many scientists thought (and many hoped) that the weapon itself would be impossible to construct.\n\nThe exact history of the Teller–Ulam breakthrough is not completely known, due in part to numerous conflicting personal accounts and continued classification of documents which would reveal which was closer to the truth. Previous models of the \"Super\" had apparently placed the fusion fuel either surrounding the fission \"trigger\" (in a spherical formation) or at the heart of it (similar to a \"boosted\" weapon) in the hopes that the closer the fuel was to the fission explosion, the higher the chance it would ignite the fusion fuel by the sheer force of the heat generated.\n\nIn 1951, after still many years of fruitless labor on the \"Super\", a breakthrough idea from the Polish émigré mathematician Stanislaw Ulam was seized upon by Teller and developed into the first workable design for a megaton-range hydrogen bomb. This concept, now called \"staged implosion\" was first proposed in a classified scientific paper, \"On Heterocatalytic Detonations I. Hydrodynamic Lenses and Radiation Mirrors\" by Teller and Ulam on March 9, 1951. The exact amount of contribution provided respectively from Ulam and Teller to what became known as the \"Teller–Ulam design\" is not definitively known in the public domain—the degree of credit assigned to Teller by his contemporaries is almost exactly commensurate with how well they thought of Teller in general. In an interview with \"Scientific American\" from 1999, Teller told the reporter:\n\nI contributed; Ulam did not. I'm sorry I had to answer it in this abrupt way. Ulam was rightly dissatisfied with an old approach. He came to me with a part of an idea which I already had worked out and difficulty getting people to listen to. He was willing to sign a paper. When it then came to defending that paper and really putting work into it, he refused. He said, \"I don't believe in it.\"\n\nThe issue is controversial. Bethe in his “Memorandum on the History of the Thermonuclear Program” (1952) cited Teller as the discoverer of an “entirely new approach to thermonuclear reactions”, which “was a matter of inspiration” and was “therefore, unpredictable” and “largely accidental.” At the Oppenheimer hearing, in 1954, Bethe spoke of Teller’s “stroke of genius” in the invention of the H-bomb. And finally in 1997 Bethe stated that “the crucial invention was made in 1951, by Teller.” \n\nOther scientists (antagonistic to Teller, such as J. Carson Mark) have claimed that Teller would have never gotten any closer without the idea of Ulam. The nuclear weapons designer Ted Taylor was clear about assigning credit for the basic staging and compression ideas to Ulam, while giving Teller the credit for recognizing the critical role of radiation as opposed to hydrodynamic pressure.\n\nTeller became known in the press as the \"father of the hydrogen bomb\", a title which he did not seek to discourage. Many of Teller's colleagues were irritated that he seemed to enjoy taking full credit for something he had only a part in, and in response, with encouragement from Enrico Fermi, Teller authored an article titled \"The Work of Many People,\" which appeared in \"Science\" magazine in February 1955, emphasizing that he was not alone in the weapon's development (he would later write in his memoirs that he had told a \"white lie\" in the 1955 article, and would imply that he should receive full credit for the weapon's invention). Hans Bethe, who also participated in the hydrogen bomb project, once drolly said, \"For the sake of history, I think it is more precise to say that Ulam is the father, because he provided the seed, and Teller is the mother, because he remained with the child. As for me, I guess I am the midwife.\"\n\nThe Teller–Ulam breakthrough—the details of which are still classified—was apparently the separation of the fission and fusion components of the weapons, and to use the radiation produced by the fission bomb to first compress the fusion fuel before igniting it. Some sources have suggested that Ulam initially proposed compressing the \"secondary\" through the shock waves generated by the \"primary\", and that it was Teller who then realized that the radiation from the \"primary\" would be able to accomplish the job (hence, \"radiation implosion\"). But compression alone would not have been enough and the other crucial idea—staging the bomb by separating the \"primary\" and \"secondary\"—seems to have been exclusively contributed by Ulam. The elegance of the design impressed many scientists, to the point that some who previously wondered if it were feasible suddenly believed it was inevitable, and that it would be created by both the USA and USSR. Even Oppenheimer, who was originally opposed to the project, called the idea \"technically sweet\". The \"George\" shot of Operation Greenhouse in 1951 tested the basic concept for the first time on a very small scale (and the next shot in the series, \"Item\", was the first boosted fission weapon), raising expectations to a near certainty that the concept would work.\n\nOn November 1, 1952, the Teller–Ulam configuration was tested in the \"Ivy Mike\" shot at an island in the Enewetak atoll, with a yield of 10.4 megatons (over 450 times more powerful than the bomb dropped on Nagasaki during World War II). The device, dubbed the \"Sausage\", used an extra-large fission bomb as a \"trigger\" and liquid deuterium—kept in its liquid state by 20 tons of cryogenic equipment—as its fusion fuel, and weighed around 80 tons altogether. Though an initial press blackout was attempted, it was soon announced that the U.S. had detonated a megaton-range hydrogen bomb.\n\nThe elaborate refrigeration plant necessary to keep its fusion fuel in a liquid state meant that the \"Ivy Mike\" device was too heavy and too complex to be of practical use. The first \"deployable\" Teller–Ulam weapon in the U.S. would not be developed until 1954, when the liquid deuterium fuel of the \"Ivy Mike\" device would be replaced with a dry fuel of lithium deuteride and tested in the \"Castle Bravo\" shot (the device was code-named the \"Shrimp\"). The dry lithium mixture performed much better than it was expected to, and the \"Castle Bravo\" device that was detonated in 1954 had a yield two and a half times greater than expected (at 15 Mt, it was also the most powerful bomb ever detonated by the United States). Because much of the yield came from the final fission stage of its uranium 238 tamper, it generated much nuclear fallout, which caused one of the worst nuclear accidents in U.S. history when unforeseen weather patterns blew it over populated areas of the atoll and Japanese fishermen on board the \"Daigo Fukuryu Maru\".\n\nAfter an initial period focused on making multi-megaton hydrogen bombs, efforts in the United States shifted towards developing miniaturized Teller–Ulam weapons which could outfit Intercontinental Ballistic Missiles and Submarine Launched Ballistic Missiles. The last major design breakthrough in this respect was accomplished by the mid-1970s, when versions of the Teller–Ulam design were created which could fit on the end of a small MIRVed missile.\n\nIn the Soviet Union, the scientists working on their own hydrogen bomb project also ran into difficulties in developing a megaton-range fusion weapon. Because Klaus Fuchs had only been at Los Alamos at a very early stage of the hydrogen bomb design (before the Teller–Ulam configuration had been completed), none of his espionage information was of much use, and the Soviet physicists working on the project had to develop their weapon independently.\n\nThe first Soviet fusion design, developed by Andrei Sakharov and Vitaly Ginzburg in 1949 (before the Soviets had a working fission bomb), was dubbed the \"Sloika\", after a Russian layered puff pastry, and was not of the Teller–Ulam configuration, but rather used alternating layers of fissile material and lithium deuteride fusion fuel spiked with tritium (this was later dubbed Sakharov's \"First Idea\"). Though nuclear fusion was technically achieved, it did not have the scaling property of a \"staged\" weapon, and their first \"hydrogen bomb\" test, \"Joe 4\" is no longer considered to be a \"true\" hydrogen bomb, and is rather considered a hybrid fission/fusion device more similar to a large boosted fission weapon than a Teller–Ulam weapon (though using an order of magnitude more fusion fuel than a boosted weapon). Detonated in 1953 with a yield equivalent to 400 kilotons (only 15%–20% from fusion), the \"Sloika\" device did, however, have the advantage of being a weapon which could actually be delivered to a military target, unlike the \"Ivy Mike\" device, though it was never widely deployed. Teller had proposed a similar design as early as 1946, dubbed the \"Alarm Clock\" (meant to \"wake up\" research into the \"Super\"), though it was calculated to be ultimately not worth the effort and no prototype was ever developed or tested.\nAttempts to use a \"Sloika\" design to achieve megaton-range results proved unfeasible in the USSR as it had in the calculations done in the USA (though its value as a practical weapon — being 20 times more powerful than their first fission bomb — should not be underestimated), and the Soviet physicists calculated that at best the design might yield a single megaton of energy if pushed to its limits. After the U.S. tested the \"Ivy Mike\" device in 1952, proving that a multimegaton bomb could be created, the Soviets searched for an additional design, while continuing to work on improving the \"Sloika\" (the \"First Idea\"). The \"Second Idea\", as Sakharov referred to it in his memoirs, was a previous proposal by Ginzburg in November 1948 to use lithium deuteride in the bomb, which would, in the course of being bombarded by neutrons, produce tritium. In late 1953, physicist Viktor Davidenko achieved the first breakthrough, that of keeping the \"primary\" and \"secondary\" parts of the bombs in separate pieces (\"staging\"). The next breakthrough was discovered and developed by Sakharov and Yakov Zeldovich, that of using the X-rays from the fission bomb to compress the \"secondary\" before fusion (\"radiation implosion\"), in the spring of 1954. Sakharov's \"Third Idea\", as the Teller–Ulam design was known in the USSR, was tested in the shot \"RDS-37\" in November 1955 with a yield of 1.6 Mt.\n\nIf the Russians had been able to analyze the fallout data from either the \"Ivy Mike\" or \"Castle Bravo\" tests, they would have potentially been able to discern that the fission \"primary\" was being kept separate from the fusion \"secondary\", a key part of the Teller–Ulam device, and potentially that the fusion fuel had been subjected to high amounts of compression before detonation. (De Geer 1991) One of the key Soviet bomb designers, Yuli Khariton, later said that:\n\nAt that time, Soviet research was not organized on a sufficiently high level, and useful results were not obtained, although radiochemical analyses of samples of fallout could have provided some useful information about the materials used to produce the explosion. The relationship between certain short-lived isotopes formed in the course of thermonuclear reactions could have made it possible to judge the degree of compression of the thermonuclear fuel, but knowing the degree of compression would not have allowed Soviet scientists to conclude exactly how the exploded device had been made, and it would not have revealed its design.\n\nSakharov stated in his memoirs that though he and Davidenko had fallout dust in cardboard boxes several days after the \"Mike\" test with the hope of analyzing it for information, a chemist at Arzamas-16 (the Soviet weapons laboratory) had mistakenly poured the concentrate down the drain before it could be analyzed. Only in the fall of 1952 did the Soviet Union set up an organized system for monitoring fallout data. Nonetheless, the \"memoirs\" also say that the yield from one of the American tests, which became an international incident involving Japan, told Sakharov that the US design was much better than theirs, and he decided that they must have exploded a separate fission bomb and somehow used its energy to compress the lithium deuteride. But how, he asked himself, can an explosion to one side be used to compress the ball of fusion fuel within 5% of symmetry? Then it hit him! Focus the X-rays!\n\nThe Soviets demonstrated the power of the \"staging\" concept in October 1961 when they detonated the massive and unwieldy \"Tsar Bomba\", a 50 Mt hydrogen bomb which derived almost 97% of its energy from fusion rather than fission—its uranium tamper was replaced with one of lead shortly before firing, in an effort to prevent excessive nuclear fallout. Had it been fired in its \"full\" form, it would have yielded at around 100 Mt. The weapon was technically deployable (it was tested by dropping it from a specially modified bomber), but militarily impractical, and was developed and tested primarily as a show of Soviet strength. It was the largest nuclear weapon developed and tested by any country.\n\nThe details of the development of the Teller–Ulam design in other countries are less well known. In any event, United Kingdom had initial difficulty in its development of it, failing in its first attempt in May 1957 (its \"Grapple I\" test failed to ignite as planned, though much of its energy did come from fusion in its secondary), though succeeded in its second attempt in its November 1957 \"Grapple X\" test (which yielded 1.8 Mt). The British development of the Teller–Ulam design was apparently independent, though they were allowed to share in some U.S. fallout data which may have been useful to them. After their successful detonation of a megaton-range device (and thus their practical understanding of the Teller–Ulam design \"secret\"), the United States agreed to exchange some of its nuclear designs with Great Britain, leading to the 1958 US-UK Mutual Defence Agreement.\n\nThe People's Republic of China detonated its first device using a Teller–Ulam design June 1967 (\"Test No. 6\"), a mere 32 months after detonating its first fission weapon (the shortest fission-to-fusion development yet known), with a yield of 3.3 Mt. Little is known about the Chinese thermonuclear program, however.\n\nVery little is known about the French development of the Teller–Ulam design beyond the fact that they detonated a 2.6 Mt device in the \"Canopus\" test in August 1968.\n\nIn 1998, India claimed to detonate a \"hydrogen bomb\" in its Operation Shakti tests (\"Shakti I\", specifically), though seismographic readings have led many non-Indian experts to conclude that this is unlikely, or at least it was unlikely to have been a success as claimed, because of its low yield (claimed to be around 45 kt, though outside experts estimate it at around 30 kt, both extremely low for a successful thermonuclear detonation). However even low-yield tests can have a bearing on thermonuclear capability, as they can provide information on the behavior of \"primaries\" without the full ignition of \"secondaries\".\n\nNorth Korea claimed to have tested its miniaturised thermonuclear bomb on 6 January 2016. North Korea's first three nuclear tests (2006, 2009 and 2013) were relatively low yield and do not appear to have been of a thermonuclear weapon design. In 2013, the South Korean Defense Ministry has speculated that North Korea may be trying to develop a \"hydrogen bomb\" and such a device may be North Korea's next weapons test. In January 2016, North Korea claimed to have successfully tested a hydrogen bomb, though only a magnitude 5.1 seismic event was detected at the time of the test, a similar magnitude to the 2013 test of a 6-9 kt atomic bomb. These seismic recordings have scientists worldwide doubting North Korea's claim that a hydrogen bomb was tested and suggest it was a non-fusion nuclear test. On 9 September 2016, North Korea conducted their fifth nuclear test which yielded between 10 and 30 kilotons.\n\nOn 3 September 2017, North Korea conducted sixth nuclear test just few hours after photographs of North Korean leader Kim Jong-un inspecting a device resembling a thermonuclear weapon warhead were released. Initial estimates in first few days were between 70 and 160 kilotons and were raised over a week later to range of 250 to over 300 kilotons. Jane's Information Group estimated, based mainly on visual analysis of propaganda pictures, that the bomb might weigh between 250 and 360 kilograms (~550 - 790 lbs.).\n\nThe Teller–Ulam design was for many years considered one of the top nuclear secrets, and even today it is not discussed in any detail by official publications with origins \"behind the fence\" of classification. United States Department of Energy (DOE) policy has been, and continues to be, that they do not acknowledge when \"leaks\" occur, because doing such would acknowledge the accuracy of the supposed leaked information. Aside from images of the warhead casing (but never of the \"physics package\" itself), most information in the public domain about this design is regulated to a few terse statements by the DOE and the work of a few individual investigators.\n\nBelow is a short discussion of the events which led to the formation of these \"public\" models of the Teller–Ulam design, with some discussions as to their differences and disagreements with those principles outlined above.\n\nThe general principles of the \"classical Super\" design were public knowledge even before thermonuclear weapons were first tested. After President Truman ordered the crash program to develop the H-bomb in January 1950 the \"Boston Daily Globe\" published a cutaway description of a hypothetical H-bomb with the caption \"Artist's conception of how H-bomb might work using atomic bomb as a mere \"trigger\" to generate enough heat to set up the H-bomb's \"thermonuclear fusion\" process.\"\n\nThe fact that a large proportion of the yield of a thermonuclear device stems from the fission of a uranium 238 tamper (fission-fusion-fission principle) was revealed when the Castle Bravo test \"ran away,\" producing a much higher yield than originally estimated and creating large amounts of nuclear fallout.\n\nIn 1972, the DOE declassified a statement that \"The fact that in thermonuclear (TN) weapons, a fission 'primary' is used to trigger a TN reaction in thermonuclear fuel referred to as a 'secondary'\", and in 1979 added: \"The fact that, in thermonuclear weapons, radiation from a fission explosive can be contained and used to transfer energy to compress and ignite a physically separate component containing thermonuclear fuel.\" To this latter sentence they specified that \"Any elaboration of this statement will be classified.\" (emphasis in original) The only statement which may pertain to the \"sparkplug\" was declassified in 1991, \"Fact that fissile and/or fissionable materials are present in some secondaries, material unidentified, location unspecified, use unspecified, and weapons undesignated.\" In 1998, the DOE declassified the statement that \"The fact that materials may be present in channels and the term 'channel filler,' with no elaboration\", which may refer to the polystyrene foam (or an analogous substance). (DOE 2001, sect. V.C.)\n\nWhether these statements vindicate some or all of the models presented above is up for interpretation, and official U.S. government releases about the technical details of nuclear weapons have been purposely equivocating in the past (e.g. Smyth Report). Other information, such as the types of fuel used in some of the early weapons, has been declassified, though precise technical information has not been.\n\nMost of the current ideas of the Teller–Ulam design came into public awareness after the DOE attempted to censor a magazine article by U.S. anti-weapons activist Howard Morland in 1979 on the \"secret of the hydrogen bomb\". In 1978, Morland had decided that discovering and exposing this \"last remaining secret\" would focus attention onto the arms race and allow citizens to feel empowered to question official statements on the importance of nuclear weapons and nuclear secrecy. Most of Morland's ideas about how the weapon worked were compiled from highly accessible sources—the drawings which most inspired his approach came from the \"Encyclopedia Americana\". Morland also interviewed (often informally) many former Los Alamos scientists (including Teller and Ulam, though neither gave him any useful information), and used a variety of interpersonal strategies to encourage informational responses from them (i.e., asking questions such as \"Do they still use sparkplugs?\" even if he wasn't aware what the latter term specifically referred to). (Morland 1981)\n\nMorland eventually concluded that the \"secret\" was that the \"primary\" and \"secondary\" were kept separate and that radiation pressure from the \"primary\" compressed the \"secondary\" before igniting it. When an early draft of the article, to be published in \"The Progressive\" magazine, was sent to the DOE after falling into the hands of a professor who was opposed to Morland's goal, the DOE requested that the article not be published, and pressed for a temporary injunction. After a short court hearing in which the DOE argued that Morland's information was (1). likely derived from classified sources, 2. if not derived from classified sources, itself counted as \"secret\" information under the \"born secret\" clause of the 1954 Atomic Energy Act, and (3). dangerous and would encourage nuclear proliferation, Morland and his lawyers disagreed on all points, but the injunction was granted, as the judge in the case thought that it was safer to grant the injunction and allow Morland, et al., to appeal, which they did in \"United States v. The Progressive, et al.\" (1979).\n\nThrough a variety of more complicated circumstances, the DOE case began to wane, as it became clear that some of the data they were attempting to claim as \"secret\" had been published in a students' encyclopedia a few years earlier. After another H-bomb speculator, Chuck Hansen, had his own ideas about the \"secret\" (quite different from Morland's) published in a Wisconsin newspaper, the DOE claimed \"The Progressive\" case was moot, dropped their suit and allowed the magazine to publish, which it did in November 1979. Morland had by then, however, changed his opinion of how the bomb worked, suggesting that a foam medium (the polystyrene) rather than radiation pressure was used to compress the \"secondary\", and that in the \"secondary\" was a \"sparkplug\" of fissile material as well. He published these changes, based in part on the proceedings of the appeals trial, as a short errata in \"The Progressive\" a month later. In 1981, Morland published a book, \"The secret that exploded\", about his experience, describing in detail the train of thought which led him to his conclusions about the \"secret\".\n\nBecause the DOE sought to censor Morland's work—one of the few times they violated their usual approach of not acknowledging \"secret\" material which had been released—it is interpreted as being at least partially correct, though to what degree it lacks information or has incorrect information is not known with any great confidence. The difficulty which a number of nations had in developing the Teller–Ulam design (even when they understood the design, such as with the United Kingdom) makes it somewhat unlikely that this simple information alone is what provides the ability to manufacture thermonuclear weapons. Nevertheless, the ideas put forward by Morland in 1979 have been the basis for all current speculation on the Teller–Ulam design.\n\n\n\n\n\n\n\n"}
{"id": "314402", "url": "https://en.wikipedia.org/wiki?curid=314402", "title": "Liquid oxygen", "text": "Liquid oxygen\n\nLiquid oxygen—abbreviated LOx, LOX or Lox in the aerospace, submarine and gas industries—is one of the physical forms of elemental oxygen.\n\nLiquid oxygen has a pale blue color and is strongly paramagnetic: it can be suspended between the poles of a powerful horseshoe magnet. Liquid oxygen has a density of 1.141 g/cm (1.141 kg/L or 1141 kg/m), slightly denser than liquid water, and is cryogenic with a freezing point of and a boiling point of at 101.325 kPa (760 mmHg). Liquid oxygen has an expansion ratio of 1:861 under and , and because of this, it is used in some commercial and military aircraft as transportable source of breathing oxygen.\n\nBecause of its cryogenic nature, liquid oxygen can cause the materials it touches to become extremely brittle. Liquid oxygen is also a very powerful oxidizing agent: organic materials will burn rapidly and energetically in liquid oxygen. Further, if soaked in liquid oxygen, some materials such as coal briquettes, carbon black, etc., can detonate unpredictably from sources of ignition such as flames, sparks or impact from light blows. Petrochemicals, including asphalt, often exhibit this behavior.\n\nThe tetraoxygen molecule (O) was first predicted in 1924 by Gilbert N. Lewis, who proposed it to explain why liquid oxygen defied Curie's law. Modern computer simulations indicate that although there are no stable O molecules in liquid oxygen, O molecules do tend to associate in pairs with antiparallel spins, forming transient O units.\n\nLiquid nitrogen has a lower boiling point at −196 °C (77 K) than oxygen's −183 °C (90 K), and vessels containing liquid nitrogen can condense oxygen from air: when most of the nitrogen has evaporated from such a vessel there is a risk that liquid oxygen remaining can react violently with organic material. Conversely, liquid nitrogen or liquid air can be oxygen-enriched by letting it stand in open air; atmospheric oxygen dissolves in it, while nitrogen evaporates preferentially.\n\nIn commerce, liquid oxygen is classified as an industrial gas and is widely used for industrial and medical purposes. Liquid oxygen is obtained from the oxygen found naturally in air by fractional distillation in a cryogenic air separation plant.\n\nAir forces have long recognized the strategic importance of liquid oxygen, both as an oxidizer and as a supply of gaseous oxygen for breathing in hospitals and high-altitude aircraft flights. In 1985 the USAF started a program of building its own oxygen-generation facilities at all major consumption bases.\n\nLiquid oxygen is a common cryogenic liquid oxidizer propellant for spacecraft rocket applications, usually in combination with liquid hydrogen, kerosene or methane.\n\nLiquid oxygen was used in the very first liquid fueled rocket. The World War II V2 missile also used liquid oxygen under the name \"A-Stoff\" and \"Sauerstoff\". In the 1950's, during the Cold War both the United States' Redstone and Atlas rockets, and the Soviet R-7 Semyorka used liquid oxygen. Later, in the 1960's & 70's, the ascent stages of the Apollo Saturn rockets, and the Space Shuttle main engines used liquid oxygen.\n\nIn 2018, many rockets use liquid oxygen: \n\n"}
{"id": "15068283", "url": "https://en.wikipedia.org/wiki?curid=15068283", "title": "List of Nuclear-Free Future Award recipients", "text": "List of Nuclear-Free Future Award recipients\n\nThe Nuclear-Free Future Award annually honours the architects of a nuclear-free planet. This is a list of notable recipients of the Nuclear-Free Future Award, categorised by country:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7395352", "url": "https://en.wikipedia.org/wiki?curid=7395352", "title": "Lixiviant", "text": "Lixiviant\n\nA lixiviant is a liquid medium used in hydrometallurgy to selectively extract the desired metal from the ore or mineral. It assists in rapid and complete leaching. The metal can be recovered from it in a concentrated form after leaching.\n\nLixiviants may work by altering the redox state of an ore, or by altering the pH. Acidic lixiviants, such as sulfuric acid, are commonly used to leach base metals such as copper, whereas basic lixiviants such as a solution of sodium cyanide are used to leach precious metals.\n\nIn the United States, lixiviants which contact the environment are almost always oxidizers of neutral pH because this minimizes risk to the environment.\n\nThe origin is the word lixiviate, meaning to leach, to dissolve out, deriving from the Latin \"lixivium\".\n"}
{"id": "52960114", "url": "https://en.wikipedia.org/wiki?curid=52960114", "title": "London Electrical Society", "text": "London Electrical Society\n\nThe London Electrical Society was established in 1837 to enable amateur electricians to meet and share their interests in “experimental investigation of Electrical Science in all its various branches”. Although it initially flourished the society soon showed weaknesses in its organisation and ways of working. After a period of considerable financial difficulty it closed in 1845.\n\nThe London Electrical Society was founded at a meeting held on 16 May 1837 at Edward M Clarke’s “Laboratory of Science” in Lowther Arcade, near the Strand. The idea for the Society had arisen from discussions during a course of lectures on electricity delivered by William Sturgeon at the same venue. He was assisted in establishing the Society by operative chemist William Leithead; John Peter Gassiot, an amateur scientist with a particular interest in electricity; and Charles Vincent Walker, an electrical engineer.\n\nThe London Electrical Society’s aim was to provide a forum for amateur scientists and to foster their interest in the practical applications of electricity. It served to strengthen the informal links which many of the experimenters had already made. In “The Uses of Experiment: Studies in the Natural Sciences” Secord notes that:\n\nThe bulk of its membership were 'amateur experimentalists', as founder and first president Sturgeon admitted. Typical of the group were the instrument-maker Edward M. Clarke, the young Manchester electrician James Prescott Joule, and William Leithead, employed by the Gallery as a demonstrator. Others, like Walter Hawkins, William G Lettsom, John Peter Gassiot and [Andrew] Crosse were either successful men business or independently wealthy. Conspicuously absent were the real controlling interests in the London scientific world, men like John Herschel, William Grove and Faraday.\nMembership was made up of resident (living within 20 miles of London) and non-resident participants, with annual fees set at two guineas and one guinea respectively. Non-resident members were elsewhere in Great Britain (e.g. Bath and Shrewsbury), or abroad (e.g. Amsterdam and Marseille). At first, members styled themselves Mem. Elec. Soc., but later they used the post-nominal MES.\n\nIn the early days the Society met in Clarke’s Laboratory of Science, but at the first annual general meeting, on 7 October 1837 (with John P Gassiot, Treasurer, in the chair), it was announced that future meetings would be held at the nearby Adelaide Gallery on the Strand, to accommodate the growing membership. The gallery’s proprietors generously made their electrical apparatus freely available for the use of the Society’s members.\n\nThe meetings, initially held on the first and third Saturdays of the month at 7:00pm, involved the reading and discussion of experimental papers by members and their guests. Roughly half of the first 120 papers had an emphasis on technology, typically describing an improvement in a technique, or of a piece of apparatus. An example is Charles V Walker’s “An Account of Experiments with a ‘Constant Voltaic Battery’” read in October 1838, in which he described the results of assembling and testing a 160-cell battery of zinc and copper electrodes immersed in saturated copper sulphate solution. Other papers were of a more theoretical nature, such as Thomas Pollock’s “On the Connection between the Atomic Arrangement and the Conducting power of Bodies.”\n\nThe London Electrical Society had decided early on that it would not prepare standing Rules and Regulations or formally elect Officers until the membership had reached 50. At the October AGM that threshold was raised to 100 resident members, a number which it never reached. Nonetheless, the Society flourished in the early days: membership grew and papers were published.\n\nThen signs began to appear that all was not well. There was a gap of about a year in which meetings did not take place. Membership fell, and the Society’s debts began to grow. This was made worse by the decision to publish many extra copies of the Proceedings and distribute them widely to other organisations in the hope of attracting new members. These factors, and internal rows over ownership of experimental findings, and the departure of Sturgeon for Manchester, all contributed to the Society’s downfall. The story is more fully explained by Morus in “Currents from the Underworld\".\n\nThis list of 94 members was gathered from the Minute Books of the London Electrical Society, and from papers published in the society's Transactions and Proceedings. Names of members which could not be clearly read in the Minutes have not been included. It is not known how many of the total were all members at the same time, although the number listed here is consistent with Morus's observation that \"In 1839 the membership was no greater than eighty of whom only about half were resident members. Reminiscing almost half a century later, Walker recalled that membership had peaked at seventy-six.\"\n"}
{"id": "17906345", "url": "https://en.wikipedia.org/wiki?curid=17906345", "title": "Mandatory renewable energy target", "text": "Mandatory renewable energy target\n\nMandatory renewable energy targets are part of government legislated schemes which require electricity retailers to source specific proportions of total electricity sales from renewable energy sources according to a fixed time frame. The purpose of these schemes is to promote renewable energy and reduce dependency on fossil fuels. If this results in an additional cost of electricity, the additional cost is distributed across most customers by increases in other tariffs. The cost of this measure is therefore not funded by government budgets, except for costs of establishing and monitoring the scheme and any audit and enforcement actions. As the cost of renewable energy has become cheaper than other sources, meeting and exceeding a renewable energy target will also reduce the cost of electricity to consumers.\n\nAt least 67 countries have renewable energy policy targets of some type. In Europe, 28 European Union members states and 8 Energy Community Contracting Parties have legally binding renewable energy targets. The EU baseline target is 20% by 2020, while the United States also has a national RET of 20%. Similarly, Canada has 9 provincial RETs but no national target. Targets are typically for shares of electricity production, but some are defined as by primary energy supply, installed capacity, or otherwise. While some targets are based on 2010-2012 data, many are now for 2020, which ties in with the IPCC suggested greenhouse gas emission cuts of 25 to 40% by Annex I countries by 2020, although some are for 2025.\n\nRenewable energy technologies are essential contributors to the energy supply portfolio, as they contribute to world energy security, reduce dependency on fossil fuels, and provide opportunities for mitigating greenhouse gases. The International Energy Agency has defined three generations of renewable energy technologies, reaching back over 100 years:\n\n\nFirst-generation technologies are well established. However, second-generation technologies and third-generation technologies depend on further promotion by the public sector. The introduction of mandatory renewable energy targets is one important way in which governments can encourage the wider use of renewables.\n\nRenewable energy targets exist in at least 66 countries around the world, including the 27 European Union countries, 29 U.S. states, and 9 Canadian provinces. Most targets are for shares of electricity production, primary energy, and/or final energy for a future year. Most targets aim for the 2010–2012 timeframe, although an increasing number of targets aim for 2020, and there is now an EU-wide target of 20% of final energy by 2020, and a Chinese target of 15% of primary energy by 2020.\n\nIn 2001, the federal government introduced a Mandatory Renewable Energy Target of 9,500 GWh of new generation, with the scheme running until at least 2020. This represents an increase of new renewable electricity generation of about 4% of Australia's total electricity generation and a doubling of renewable generation from 1997 levels. Australia's renewable energy target does not cover heating or transport energy like Europe's or China's, Australia's target is therefore equivalent of approximately 5% of all energy from renewable sources.\n\nAn Expanded Renewable Energy Target was passed on 20 August 2009, to ensure that renewable energy obtains a 20% share of electricity supply in Australia by 2020. To ensure this, the Labor government committed that the MRET will increase from 9,500 gigawatt-hours to 45,000 gigawatt-hours by 2020. The scheme was to continue until 2030. After 2020, the proposed Emissions Trading Scheme and improved efficiencies from innovation and manufacture was expected to allow the MRET to be phased out by 2030. The target was criticised as unambitious and ineffective in reducing Australia's fossil fuel dependency, as it only applied to generated electricity, but not to the 77% of energy production exported, nor to energy sources which are not used for electricity generation, such as the oil used in transportation. Thus 20% renewable energy in electricity generation would represent less than 2% of total energy production in Australia.\n\nIn 2011 the 'expanded MRET' was split into two schemes: a Large-scale Renewable Energy Target (LRET) of 41,000 GWh for utility-scale renewable generators, and an uncapped Small-scale Renewable Energy Scheme for small household and commercial-scale generators. Following the 2014 Warburton Review initiated by the Abbott Government, and subsequent negotiations with the Labor Opposition, in June 2015 the LRET target was reduced to 33,000 GWh.\n\nAs at July 2010, 30 US states and DC have established mandatory renewable energy targets, and a further 6 have voluntary targets.\nThe Energy Independence and Security Act of 2007 has set a target for of biofuel produced annually by 2022. Of that, shall be advanced biofuels (derived from feedstock other than corn starch). Of the , 16 billion shall come from cellulosic ethanol. The remaining shall come from biomass-based diesel and other advanced biofuels. For sources other than biofuels, The United States carries no mandatory renewable energy targets although they do support the growth of renewable energy industries with subsidies, feed-in tariffs, tax exemptions, and other financial support measures.\n\n"}
{"id": "9567916", "url": "https://en.wikipedia.org/wiki?curid=9567916", "title": "Mechanical screening", "text": "Mechanical screening\n\nMechanical screening, often just called screening, is the practice of taking granulated ore material and separating it into multiple grades by particle size.\n\nThis practice occurs in a variety of industries such as mining and mineral processing, agriculture, pharmaceutical, food, plastics, and recycling.\n\n[A method of separating solid particles according to size alone is called screening]\n\nScreening fall under two general categories: dry screening and wet screening. From these categories, screening separates a flow of material into grades, these grades are then either further processed to an intermediary product or a finished product. Additionally the machines can be categorised into moving screen and static screen machines, as well as by whether the screens are horizontal or inclined.\n\nThe mining and mineral processing industry uses screening for a variety of processing applications. For example, after mining the minerals, the material is transported to a primary crusher. Before crushing large boulder are scalped on a shaker with thick shielding screening. Further down stream after crushing the material can pass through screens with openings or slots that continue to become smaller. Finally, screening is used to make a final separation to produce saleable products based on a grade or a size range.\n\nA screening machine consist of a drive that induces vibration, a screen media that causes particle separation, and a deck which holds the screen media and the drive and is the mode of transport for the vibration.\n\nThere are physical factors that makes screening practical. For example, vibration, g force, bed density, and material shape all facilitate the rate or cut. Electrostatic forces can also hinder screening efficiency in way of water attraction causing sticking or plugging, or very dry material generate a charge that causes it to attract to the screen itself.\n\nAs with any industrial process there is a group of terms that identify and define what screening is. Terms like blinding, contamination, frequency, amplitude, and others describe the basic characteristics of screening, and those characteristics in turn shape the overall method of dry or wet screening.\n\nIn addition, the way a deck is vibrated differentiates screens. Different types of motion have their advantages and disadvantages. In addition media types also have their different properties that lead to advantages and disadvantages.\n\nFinally, there are issues and problems associated with screening. Screen tearing, contamination, blinding, and dampening all affect screening efficiency.\n\n\nLike any mechanical and physical entity there are scientific, industrial, and layman terminology. The following is a partial list of terms that are associated with mechanical screening.\n\nThere are a number of types of mechanical screening equipment that cause segregation. These types are based on the motion of the machine through its motor drive.\n\n\nAn improvement on vibration, vibratory, and linear screeners, a tumbler screener uses elliptical action which aids in screening of even very fine material. As like panning for gold, the fine particles tend to stay towards the center and the larger go to the outside. It allows for segregation and unloads the screen surface so that it can effectively do its job. With the addition of multiple decks and ball cleaning decks, even difficult products can be screened at high capacity to very fine separations.\n\nCircle-Throw Vibrating Equipment is a shaker or a series of shakers as to where the drive causes the whole structure to move. The structure extends to a maximum throw or length and then contracts to a base state. A pattern of springs are situated below the structure to where there is vibration and shock absorption as the structure returns to the base state.\n\nThis type of equipment is used for very large particles, sizes that range from pebble size on up to boulder size material. It is also designed for high volume output. As a scalper, this shaker will allow oversize material to pass over and fall into a crusher such a cone crusher, jaw crusher, or hammer mill. The material that passes the screen by-passes the crusher and is conveyed and combined with the crush material.\n\nAlso this equipment is used in washing processes, as material passes under spray bars, finer material and foreign material is washed through the screen. This is one example of wet screening.\n\nHigh frequency vibrating equipment is a shaker whose frame is fixed and the drive vibrates only the screen cloth. High frequency vibration equipment is for particles that are in this particle size range of an 1/8 in (3 mm) down to a +150 mesh.\n\nThese shakers usually make a secondary cut for further processing or make a finished product cut.\n\nThese shakers are usually set at a steep angle relative to the horizontal level plane. Angles range from 25 to 45 degrees relative to the horizontal level plane.\nThis type of equipment has an eccentric drive or weights that causes the shaker to travel in an orbital path. The material rolls over the screen and falls with the induction of gravity and directional shifts. Rubber balls and trays provide an additional mechanical means to cause the material to fall through. The balls also provide a throwing action for the material to find an open slot to fall through.\n\nThe shaker is set a shallow angle relative to the horizontal level plane. Usually, no more than 2 to 5 degrees relative to the horizontal level plane.\n\nThese types of shakers are used for very clean cuts. Generally, a final material cut will not contain any oversize or any fines contamination.\n\nThese shakers are designed for the highest attainable quality at the cost of a reduced feed rate.\n\nTrommel screens have a rotating drum on a shallow angle with screen panels around the diameter of the drum. The feed material always sits at the bottom of the drum and, as the drum rotates, always comes into contact with clean screen. The oversize travels to the end of the drum as it does not pass through the screen, while the undersize passes through the screen into a launder below.\n\nThere are many ways to install screen media into a screen box deck (shaker deck). Also, the type of attachment system has an influence on the dimensions of the media.\n\nTensioned screen cloth is typically 4 feet by the width or the length of the screening machine depending on whether the deck is side or end tensioned. Screen cloth for tensioned decks can be made with hooks and are attached with clamp rails bolted on both sides of the screen box. When the clamp rail bolts are tightened, the cloth is tensioned or even stretched in the case of some types of self-cleaning screen media. To ensure that the center of the cloth does not tap repeatedly on the deck due to the vibrating shaker and that the cloth stays tensioned, support bars are positioned at different heights on the deck to create a crown curve from hook to hook on the cloth. Tensioned screen cloth is available in various materials: stainless steel, high carbon steel and oil tempered steel wires, as well as moulded rubber or polyurethane and hybrid screens (a self-cleaning screen cloth made of rubber or polyurethane and metal wires).\n\nCommonly, vibratory-type screening equipment employs rigid, circular sieve frames to which woven wire mesh is attached. Conventional methods of producing tensioned meshed screens has given way in recent years to bonding, whereby the mesh is no longer tensioned and trapped between a sieve frame body and clamping ring; instead, developments in modern adhesive technologies has allowed the industry to adopt high strength structural adhesives to bond tensioned mesh directly to frames.\n\nModular screen media is typically 1 foot large by 1 or 2 feet long (4 feet long for ISEPREN WS 85 ) steel reinforced polyurethane or rubber panels. They are installed on a flat deck (no crown) that normally has a larger surface than a tensioned deck. This larger surface design compensates for the fact that rubber and polyurethane modular screen media offers less open area than wire cloth. Over the years, numerous ways have been developed to attach modular panels to the screen deck stringers (girders). Some of these attachment systems have been or are currently patented. Self-cleaning screen media is also available on this modular system.\n\nThere are several types of screen media manufactured with different types of material that use the two common types of screen media attachment systems, tensioned and modular.\n\nWoven wire cloth, typically produced from stainless steel, is commonly employed as a filtration medium for sieving in a wide range of industries. Most often woven with a plain weave, or a twill weave for the lightest of meshes, apertures can be produced from a few microns upwards (e.g. 25 microns), employing wires with diameters from as little as 25 microns. A twill weave allows a mesh to be woven when the wire diameter is too thick in proportion to the aperture. Other, less commonplace, weaves, such as Dutch/Hollander, allow the production of meshes that are stronger and/or having smaller apertures.\n\nToday wire cloth is woven to strict international standards, e.g. ISO1944:1999, which dictates acceptable tolerance regarding nominal mesh count and blemishes. The nominal mesh count, to which mesh is generally defined is a measure of the number of openings per lineal inch, determined by counting the number of openings from the centre of one wire to the centre of another wire one lineal inch away. For example, a 2 mesh woven with a wire of 1.6mm wire diameter has an aperture of 11.1mm (see picture below of a 2 mesh with an intermediate crimp). The formula for calculating the aperture of a mesh, with a known mesh count and wire diameter, is as follows:\n\nformula_1\n\nwhere a = aperture, b = mesh count and c = wire diameter.\nOther calculations regarding woven wire cloth/mesh can be made including weight and open area determination. Of note, wire diameters are often referred to by their standard wire gauge (swg); e.g. a 1.6mm wire is a 16 swg.\n\nTraditionally, screen cloth was made with metal wires woven with a loom. Today, woven cloth is still widely used primarily because they are less expensive than other types of screen media. Over the years, different weaving techniques have been developed; either to increase the open area percentage or add wear-life. Slotted opening woven cloth is used where product shape is not a priority and where users need a higher open area percentage. Flat-top woven cloth is used when the consumer wants to increase wear-life. On regular woven wire, the crimps (knuckles on woven wires) wear out faster than the rest of the cloth resulting in premature breakage. On flat-top woven wire, the cloth wears out equally until half of the wire diameter is worn, resulting in a longer wear life. Unfortunately flat-top woven wire cloth is not widely used because of the lack of crimps that causes a pronounced reduction of passing fines resulting in premature wear of con crushers.\n\nOn a crushing and screening plant, punch plates or perforated plates are mostly used on scalper vibrating screens, after raw products pass on grizzly bars. Most likely installed on a tensioned deck, punch plates offer excellent wear life for high-impact and high material flow applications.\n\nSynthetic screen media is used where wear life is an issue. Large producers such as mines or huge quarries use them to reduce the frequency of having to stop the plant for screen deck maintenance. Rubber is also used as a very resistant high-impact screen media material used on the top deck of a scalper screen. To compete with rubber screen media fabrication, polyurethane manufacturers developed screen media with lower Shore Hardness. To compete with self-cleaning screen media that is still primarily available in tensioned cloth, synthetic screen media manufacturers also developed membrane screen panels, slotted opening panels and diamond opening panels. Due to the 7-degree demoulding angle, polyurethane screen media users can experience granulometry changes of product during the wear life of the panel.\n\nSelf-cleaning screen media was initially engineered to resolve screen cloth blinding, clogging and pegging problems. The idea was to place crimped wires side-by-side on a flat surface, creating openings and then, in some way, holding them together over the support bars (crown bars or bucker bars). This would allow the wires to be free to vibrate between the support bars, preventing blinding, clogging and pegging of the cloth. Initially, crimped longitudinal wires on self-cleaning cloth were held together over support bars with woven wire. In the 50s, some manufacturers started to cover the woven cross wires with caulking or rubber to prevent premature wear of the crimps (knuckles on woven wires). One of the pioneer products in this category was ONDAP GOMME made by the French manufacturer Giron. During the mid 90s, Major Wire Industries Ltd., a Quebec manufacturer, developed a “hybrid” self-cleaning screen cloth called Flex-Mat, without woven cross wires. In this product, the crimped longitudinal wires are held in place by polyurethane strips. Instead of locking (impeding) the vibration over the support bars with woven cross wires, the polyurethane strips lessens the vibration of the longitudinal wires over the support bars but does not stop it, consequently allowing vibration from hook to hook. Major Wire quickly started to promote this product as a high-performance screen that helped producers screen more in-specification material for less cost and not simply a problem solver. They claimed that the independent vibrating wires helped produce more product compared to a woven wire cloth with the same opening (aperture) and wire diameter. This higher throughput would be a direct result of the higher vibration frequency of each independent wire of the screen cloth (calculated in hertz) compared to the shaker vibration (calculated in RPM), accelerating the stratification of the material bed. Another benefit that helped the throughput increase is that hybrid self-cleaning screen media offered a better open area percentage than woven wire screen media. Due to its flat surface (no knuckles), hybrid self-cleaning screen media can use a smaller wire diameter for the same aperture than woven wire and still lasts as long, resulting in a greater opening percentage.\n"}
{"id": "1869803", "url": "https://en.wikipedia.org/wiki?curid=1869803", "title": "Michael Zucchet", "text": "Michael Zucchet\n\nMichael J. Zucchet (born December 24, 1969) is an American Democratic politician, a former member of the San Diego City Council, and a former Deputy Mayor of San Diego. In 2005, he briefly served as the Acting Mayor of San Diego.\n\nZucchet earned a bachelor's degree in Business Economics and Environmental Studies from the University of California, Santa Barbara, and a master's degree in Environmental Economics and Policy from Duke University.\n\nHe worked as a renewable energy economist with the Energy Information Administration of the United States Department of Energy in Washington, DC, then returned to San Diego in 1996 to work as a City Council aide to councilmember Valerie Stallings. From 1998 through 2002, he served as the legislative and community affairs director for the San Diego City Fire Fighters.\n\nZucchet has also done work with the non-profit Environmental Defense Center in Santa Barbara while in college. He has served as president of the San Diego League of Conservation Voters, vice-president of the Pacific Beach Town Council, been elected member of the Pacific Beach Community Planning Committee, and appointed member of the city's Select Committee on Government Efficiency and Fiscal Reform. He is a graduate of LEAD San Diego.\n\nMichael Zucchet served as a San Diego city councilmember representing the city's Second District. He won the seat by defeating challenger Kevin Faulconer in the 2002 election. In December 2014, Zucchet became Deputy Mayor. As Deputy Mayor, Zucchet became acting mayor after Dick Murphy resigned July 15, 2005.\n\nThree days after becoming acting Mayor, Zucchet resigned from the City Council after he was convicted in U.S. District Court of one count of conspiracy, five counts of wire fraud and three counts of extortion. This was in connection with an alleged scheme also involving fellow City Council members Ralph Inzunza and Charles L. Lewis to get the city's \"no touch\" laws at strip clubs repealed.\n\nOn November 10, 2005, citing lack of evidence, a judge overturned the convictions against Zucchet. He acquitted Zucchet on seven of the original nine charges and ordered a new trial on the remaining two indictments. The convictions against two co-defendants were upheld.\n\nOn September 1, 2009, the Ninth U.S. Circuit Court of Appeals upheld Zucchet's acquittals. In the decision, the Court emphasized the lack of evidence against Zucchet. The remaining two charges against him were finally dropped in 2010.\n\nAfter leaving the city council, Zucchet worked at the Utility Consumers' Action Network (UCAN) as a Project Manager starting in 2006. In 2009 he became the general manager of the San Diego Municipal Employees Association, the union that represents city workers. In 2017 he was appointed to serve as one of San Diego's three representatives on the San Diego Port Commission, which governs the Port of San Diego. \n\nZucchet lives in the Ocean Beach area of San Diego with his wife Teresa and his daughter and son.\n\nOn July 2, 2013, Zucchet (an avid bridge and card player at Duke University), finished in 4th place for $143,642 in Event #54, $1,000 No-Limit Hold'em, at the World Series of Poker.\n"}
{"id": "5669825", "url": "https://en.wikipedia.org/wiki?curid=5669825", "title": "Midlands Electricity", "text": "Midlands Electricity\n\nThe Midlands Electricity Board was the public sector utility company responsible for electricity generation and electrical infrastructure maintenance in the Midlands of England prior to 1990. As \"Midlands Electricity plc\" it was listed on the London Stock Exchange and was once a constituent of the FTSE 100 Index.\n\nThe \"Midlands Electricity Board\" was formed in 1947, under the Electricity Act of that year. The counterpart of the East Midlands board, it served southern, and western parts of Warwickshire, as well as the counties of Worcestershire, Herefordshire, Shropshire, and Staffordshire, as well as most of Gloucestershire, the West Midlands conurbation and northern Oxfordshire.\n\nAs with the EMEB, it kept a network of showrooms across its area, to allow customers to pay bills, and order many types of electrical goods. The MEB, Southern Electric and Eastern Electricity merged their showrooms, forming the Powerhouse store chain in the early 90's.\n\nIn 1990, as part of the privatisation of the UK electricity industry, the board became Midlands Electricity plc. The new business was split up, and sold several times: the supply business to npower in 1999, the distribution business to GPU Power, and Aquila, Inc., under whose ownership it was renamed Aquila Networks, before finally being purchased by Powergen in 2004, becoming Central Networks, part of E.ON.\n\n"}
{"id": "334290", "url": "https://en.wikipedia.org/wiki?curid=334290", "title": "Neutralino", "text": "Neutralino\n\nIn supersymmetry, the neutralino is a hypothetical particle. In the Minimal Supersymmetric Standard Model (MSSM), a popular model of realization of supersymmetry at a low energy, there are four neutralinos that are fermions and are electrically neutral, the lightest of which is stable in an R-parity conserved scenario of MSSM. They are typically labeled (the lightest), , and (the heaviest) although sometimes formula_1 is also used when formula_2 is used to refer to charginos. These four states are mixtures of the bino and the neutral wino (which are the neutral electroweak gauginos), and the neutral higgsinos. As the neutralinos are Majorana fermions, each of them is identical to its antiparticle. Because these particles only interact with the weak vector bosons, they are not directly produced at hadron colliders in copious numbers. They would primarily appear as particles in cascade decays of heavier particles (decays that happen in multiple steps) usually originating from colored supersymmetric particles such as squarks or gluinos.\n\nIn R-parity conserving models, the lightest neutralino is stable and all supersymmetric cascade-decays end up decaying into this particle which leaves the detector unseen and its existence can only be inferred by looking for unbalanced momentum in a detector.\n\nThe heavier neutralinos typically decay through a neutral Z boson to a lighter neutralino or through a charged W boson to a light chargino:\nThe mass splittings between the different neutralinos will dictate which patterns of decays are allowed.\n\nUp to present, neutralinos have never been observed or detected in an experiment.\n\nIn supersymmetry models, all Standard Model particles have partner particles with the same quantum numbers except for the quantum number spin, which differs by from its partner particle. Since the superpartners of the Z boson (zino), the photon (photino) and the neutral higgs (higgsino) have the same quantum numbers, they can mix to form four eigenstates of the mass operator called \"neutralinos\". In many models the lightest of the four neutralinos turns out to be the lightest supersymmetric particle (LSP), though other particles may also take on this role.\n\nThe exact properties of each neutralino will depend on the details of the mixing (e.g. whether they are more higgsino-like or gaugino-like), but they tend to have masses at the weak scale (100 GeV ~ 1 TeV) and couple to other particles with strengths characteristic of the weak interaction. In this way they are phenomenologically similar to neutrinos, and so are not directly observable in particle detectors at accelerators.\n\nIn models in which R-parity is conserved and the lightest of the four neutralinos is the LSP, the lightest neutralino is stable and is eventually produced in the decay chain of all other superpartners. In such cases supersymmetric processes at accelerators are characterized by the expectation of a large discrepancy in energy and momentum between the visible initial and final state particles, with this energy being carried off by a neutralino which departs the detector unnoticed.\nThis is an important signature to discriminate supersymmetry from Standard Model backgrounds. \n\nAs a heavy, stable particle, the lightest neutralino is an excellent candidate to form the universe's cold dark matter. In many models the lightest neutralino can be produced thermally in the hot early universe and leave approximately the right relic abundance to account for the observed dark matter. A lightest neutralino of roughly is the leading weakly interacting massive particle (WIMP) dark matter candidate.\n\nNeutralino dark matter could be observed experimentally in nature either indirectly or directly. For indirect observation, gamma ray and neutrino telescopes look for evidence of neutralino annihilation in regions of high dark matter density such as the galactic or solar centre. For direct observation, special purpose experiments such as the Cryogenic Dark Matter Search (CDMS) seek to detect the rare impacts of WIMPs in terrestrial detectors. These experiments have begun to probe interesting supersymmetric parameter space, excluding some models for neutralino dark matter, and upgraded experiments with greater sensitivity are under development.\n\n\n"}
{"id": "3777978", "url": "https://en.wikipedia.org/wiki?curid=3777978", "title": "New Zealand National Airways Corporation Flight 441", "text": "New Zealand National Airways Corporation Flight 441\n\nNew Zealand National Airways Corporation Flight 441 (NZ441) was a scheduled flight of the New Zealand National Airways Corporation from Whenuapai, Auckland to Tauranga. On 3 July 1963 at approximately 9:09 am NZST the flight, a Douglas DC-3 Skyliner, flew into a vertical rock face in the Kaimai Ranges near Mount Ngatamahinerua, at an altitude of 2460 feet (750 m). Twenty three people were on board. Twenty two were killed instantly; there is evidence that one person survived the impact but died shortly afterward. Three extra passengers were supposed to be on the flight, but changed their plans at the last minute.\n\nAccording to Civil Aviation Authority investigators, a downdraft carried the aircraft below the level of the crests of the range, where under the very poor weather conditions prevailing at the time, the aircraft encountered an area of extreme turbulence from which it was impossible for the crew to recover altitude. On the day of the crash, another plane was caught in strong downdrafts in the Kaimai Ranges but managed to recover.\n\nFurthermore, the crew was probably unaware of the true position of the aircraft and initiated a premature descent. However, it must be appreciated that the crew decided to descend only to the level officially designated as the minimum safe altitude in the area of the descent.\n\nFollowing this accident, the Civil Aviation Authority made the decision to classify the Kaimai Ranges as mountainous terrain, which raised the minimum safe altitude for the area by 1000 feet (305 m).\n\nDue to the remoteness of the crash, the wreckage was not recovered but secured on site by the New Zealand Army in 1964. This is similar to New Zealand's other major air disaster, Air New Zealand Flight 901, which remains on the slopes of Mount Erebus in Antarctica where it crashed.\n"}
{"id": "3043836", "url": "https://en.wikipedia.org/wiki?curid=3043836", "title": "Nuclear binding energy", "text": "Nuclear binding energy\n\nNuclear binding energy is the \"minimum\" energy that would be required to disassemble the nucleus of an atom into its component parts. These component parts are neutrons and protons, which are collectively called nucleons. The binding is always a positive number, as we need to spend energy in moving these nucleons, attracted to each other by the strong nuclear force, away from each other. The mass of an atomic nucleus is less than the sum of the individual masses of the free protons and neutrons, according to Einstein's equation E=mc. This 'missing mass' is known as the mass defect, and represents the energy that was released when the nucleus was formed.\n\nThe term \"nuclear binding energy\" may also refer to the energy balance in processes in which the nucleus splits into fragments composed of more than one nucleon. If new binding energy is available when light nuclei fuse (nuclear fusion), or when heavy nuclei split (nuclear fission), either process can result in release of this binding energy. This energy may be made available as \"nuclear energy\" and can be used to produce electricity, as in nuclear power, or in a nuclear weapon. When a large nucleus splits into pieces, excess energy is emitted as photon (gamma rays) and as the kinetic energy of a number of different ejected particles (nuclear fission products).\n\nThese nuclear binding energies and forces are on the order of a million times greater than the electron binding energies of light atoms like hydrogen.\n\nThe mass defect of a nucleus represents the amount of mass equivalent to the \nbinding energy of the nucleus (E=mc), which is the difference between the mass of a nucleus and the sum of the individual masses of the nucleons of which it is composed.\n\nNuclear binding energy is explained by the basic principles involved in nuclear physics.\n\nAn absorption or release of nuclear energy occurs in nuclear reactions or radioactive decay; those that absorb energy are called endothermic reactions and those that release energy are exothermic reactions. Energy is consumed or liberated because of differences in the nuclear binding energy between the incoming and outgoing products of the nuclear transmutation.\n\nThe best-known classes of exothermic nuclear transmutations are fission and fusion. Nuclear energy may be liberated by atomic fission, when heavy atomic nuclei (like uranium and plutonium) are broken apart into lighter nuclei. The energy from fission is used to generate electric power in hundreds of locations worldwide. Nuclear energy is also released during atomic fusion, when light nuclei like hydrogen are combined to form heavier nuclei such as helium. The Sun and other stars use nuclear fusion to generate thermal energy which is later radiated from the surface, a type of stellar nucleosynthesis. In any exothermic nuclear process, nuclear mass might ultimately be converted to thermal energy, given off as heat.\n\nIn order to quantify the energy released or absorbed in any nuclear transmutation, one must know the nuclear binding energies of the nuclear components involved in the transmutation.\n\nElectrons and nuclei are kept together by electrostatic attraction (negative attracts positive). Furthermore, electrons are sometimes shared by neighboring atoms or transferred to them (by processes of quantum physics), and this link between atoms is referred to as a chemical bond, and is responsible for the formation of all chemical compounds.\n\nThe force of electric attraction does not hold nuclei together, because all protons carry a positive charge and repel each other. Thus, electric forces do not hold nuclei together, because they act in the opposite direction. It has been established that binding neutrons to nuclei clearly requires a non-electrical attraction.\n\nTherefore, another force, called the \"nuclear force\" (or \"residual strong force\") holds the nucleons of nuclei together. This force is a residuum of the strong interaction, which binds quarks into nucleons at an even smaller level of distance.\n\nThe nuclear force must be stronger than the electric repulsion at short distances, but weaker far away, or else different nuclei might tend to clump together. Therefore, it has short-range characteristics. An analogy to the nuclear force is the force between two small magnets: magnets are very difficult to separate when stuck together, but once pulled a short distance apart, the force between them drops almost to zero.\n\nUnlike gravity or electrical forces, the nuclear force is effective only at very short distances. At greater distances, the electrostatic force dominates: the protons repel each other because they are positively charged, and like charges repel. For that reason, the protons forming the nuclei of ordinary hydrogen—for instance, in a balloon filled with hydrogen—do not combine to form helium (a process that also would require some protons to combine with electrons and become neutrons). They cannot get close enough for the nuclear force, which attracts them to each other, to become important. Only under conditions of extreme pressure and temperature (for example, within the core of a star), can such a process take place.\n\nThere are around 94 naturally occurring elements on earth. The atoms of each element have a nucleus containing a specific number of protons (always the same number for a given element), and some number of neutrons, which is often roughly a similar number. Two atoms of the same element having different numbers of neutrons are known as isotopes of the element. Different isotopes may have different properties - for example one might be stable and another might be unstable, and gradually undergo radioactive decay to become another element.\n\nThe hydrogen nucleus contain just one proton, Its isotope deuterium, or heavy hydrogen, contains a proton and a neutron. Helium contains two protons and two neutrons, and carbon, nitrogen and oxygen - six, seven and eight of each particle, respectively. However, a helium nucleus weighs less than the sum of the weights of the two hydrogen nuclei which combine to make it. The same is true for carbon, nitrogen and oxygen. For example, the carbon nucleus is slightly lighter than three helium nuclei, which can combine to make a carbon nucleus. This difference is known as the mass defect.\n\nMass defect (not to be confused with mass excess in nuclear physics or mass defect in mass spectrometry) is the difference between the mass of a composite particle and the sum of the masses of its parts. For example, a helium atom containing 4 nucleons has a mass about 0.8% less than the total mass of four hydrogen nuclei (which contain one nucleon each).\n\nThe \"mass defect\" can be explained using Albert Einstein's formula \"E\" = \"mc\", describing the equivalence of energy and mass. By this formula, adding energy also increases mass (both weight and inertia), whereas removing energy decreases mass. In the above example, the helium nucleus has four nucleons bound together, and the binding energy which holds them together is, in effect, the missing 0.8% of mass.\n\nIf a combination of particles contains extra energy—for instance, in a molecule of the explosive TNT—weighing it reveals some extra mass, compared to its end products after an explosion. (The weighing must be done after the products have been stopped and cooled, however, as the extra mass must escape from the system as heat before its loss can be noticed, in theory.) On the other hand, if one must inject energy to separate a system of particles into its components, then the initial mass is less than that of the components after they are separated. In the latter case, the energy injected is \"stored\" as potential energy, which shows as the increased mass of the components that store it. This is an example of the fact that energy of all types is seen in systems as mass, since mass and energy are equivalent, and each is a \"property\" of the other.\n\nThe latter scenario is the case with nuclei such as helium: to break them up into protons and neutrons, one must inject energy. On the other hand, if a process existed going in the opposite direction, by which hydrogen atoms could be combined to form helium, then energy would be released. The energy can be computed using \"E\" = Δ\"m\" \"c\" for each nucleus, where Δ\"m\" is the difference between the mass of the helium nucleus and the mass of four protons (plus two electrons, absorbed to create the neutrons of helium).\n\nFor lighter elements, the energy that can be released by assembling them from lighter elements decreases, and energy can be released when they fuse. This is true for nuclei lighter than iron/nickel. For heavier nuclei, more energy is needed to bind them, to the point that energy is released by breaking them up into 2 fragments (known as atomic fission). Nuclear power is generated at present by breaking up uranium nuclei in nuclear power reactors, and capturing the released energy as heat, which is converted to electricity. \n\nAs a rule, very light elements can fuse comparatively easily, and very heavy elements can break up via fission very easily; elements in the middle are more stable and it is difficult to make them undergo either fusion or fission in an earthly environment such as a laboratory.\n\nThe reason the trend reverses after iron is the growing positive charge of the nuclei, which tends to force nuclei to break up. It is resisted by the strong nuclear interaction, which holds nucleons together. The electric force may be weaker than the strong nuclear force, but the strong force has a much more limited range: in an iron nucleus, each proton repels the other 25 protons, while the nuclear force only binds close neighbors. So for larger nuclei, the electrostatic forces tend to dominate and the nucleus will tend over time to break up.\n\nAs nuclei grow bigger still, this disruptive effect becomes steadily more significant. By the time polonium is reached (84 protons), nuclei can no longer accommodate their large positive charge, but emit their excess protons quite rapidly in the process of alpha radioactivity—the emission of helium nuclei, each containing two protons and two neutrons. (Helium nuclei are an especially stable combination.) Because of this process, nuclei with more than 94 protons are not found naturally on Earth (see periodic table). The isotopes beyond uranium (atomic number 92) with the longest half-lives are plutonium-244 (80 million years) and curium-247 (16 million years).\n\nThe nuclear fusion process works as follows: five billion years ago, the new Sun formed when gravity pulled together a vast cloud of hydrogen and dust, from which the Earth and other planets also arose. The gravitational pull released energy and heated the early Sun, much in the way Helmholtz proposed.\n\nThermal energy appears as the motion of atoms and molecules: the higher the temperature of a collection of particles, the greater is their velocity and the more violent are their collisions. When the temperature at the center of the newly formed Sun became great enough for collisions between hydrogen nuclei to overcome their electric repulsion, and bring them into the short range of the attractive nuclear force, nuclei began to stick together. When this began to happen, protons combined into deuterium and then helium, with some protons changing in the process to neutrons (plus positrons, positive electrons, which combine with electrons and become neutral). This released nuclear energy now keeps up the high temperature of the Sun's core, and the heat also keeps the gas pressure high, keeping the Sun at its present size, and stopping gravity from compressing it any more. There is now a stable balance between gravity and pressure.\n\nDifferent nuclear reactions may predominate at different stages of the Sun's existence, including the proton-proton reaction and the carbon-nitrogen cycle—which involves heavier nuclei, but whose final product is still the combination of protons to form helium.\n\nA branch of physics, \"the study of controlled nuclear fusion\", has tried since the 1950s to derive useful power from nuclear fusion reactions that combine small nuclei into bigger ones, typically to heat boilers, whose steam could turn turbines and produce electricity. Unfortunately, no earthly laboratory can match one feature of the solar powerhouse: the great mass of the Sun, whose weight keeps the hot plasma compressed and confines the nuclear furnace to the Sun's core. Instead, physicists use strong magnetic fields to confine the plasma, and for fuel they use heavy forms of hydrogen, which burn more easily. Magnetic traps can be rather unstable, and any plasma hot enough and dense enough to undergo nuclear fusion tends to slip out of them after a short time. Even with ingenious tricks, the confinement in most cases lasts only a small fraction of a second.\n\nSmall nuclei that are larger than hydrogen can combine into bigger ones and release energy, but in combining such nuclei, the amount of energy released is much smaller compared to hydrogen fusion. The reason is that while the overall process releases energy from letting the nuclear attraction do its work, energy must first be injected to force together positively charged protons, which also repel each other with their electric charge.\n\nFor elements that weigh more than iron (a nucleus with 26 protons), the fusion process no longer releases energy. In even heavier nuclei energy is consumed, not released, by combining similar sized nuclei. With such large nuclei, overcoming the electric repulsion (which affects all protons in the nucleus) requires more energy than what is released by the nuclear attraction (which is effective mainly between close neighbors). Conversely, energy could actually be released by breaking apart nuclei heavier than iron.\n\nWith the nuclei of elements heavier than lead, the electric repulsion is so strong that some of them spontaneously eject positive fragments, usually nuclei of helium that form very stable combinations (alpha particles). This spontaneous break-up is one of the forms of radioactivity exhibited by some nuclei.\n\nNuclei heavier than lead (except for bismuth, thorium, and uranium) spontaneously break up too quickly to appear in nature as primordial elements, though they can be produced artificially or as intermediates in the decay chains of lighter elements. Generally, the heavier the nuclei are, the faster they spontaneously decay.\n\nIron nuclei are the most stable nuclei (in particular iron-56), and the best sources of energy are therefore nuclei whose weights are as far removed from iron as possible. One can combine the lightest ones—nuclei of hydrogen (protons)—to form nuclei of helium, and that is how the Sun generates its energy. Or else one can break up the heaviest ones—nuclei of uranium or plutonium—into smaller fragments, and that is what nuclear power reactors do.\n\nAn example that illustrates nuclear binding energy is the nucleus of C (carbon-12), which contains 6 protons and 6 neutrons. The protons are all positively charged and repel each other, but the nuclear force overcomes the repulsion and causes them to stick together. The nuclear force is a close-range force (it is strongly attractive at a distance of 1.0 fm and becomes extremely small beyond a distance of 2.5fm), and virtually no effect of this force is observed outside the nucleus. The nuclear force also pulls neutrons together, or neutrons and protons.\n\nThe energy of the nucleus is negative with regard to the energy of the particles pulled apart to infinite distance (just like the gravitational energy of planets of the solar system), because energy must be utilized to split a nucleus into its individual protons and neutrons. Mass spectrometers have measured the masses of nuclei, which are always less than the sum of the masses of protons and neutrons that form them, and the difference—by the formula \"E\" = \"m\" \"c\"—gives the binding energy of the nucleus.\n\nThe binding energy of helium is the energy source of the Sun and of most stars. The sun is composed of 74 percent hydrogen (measured by mass), an element whose nucleus is a single proton. Energy is released in the sun when 4 protons combine into a helium nucleus, a process in which two of them are also converted to neutrons.\n\nThe conversion of protons to neutrons is the result of another nuclear force, known as the weak (nuclear) force. The weak force, like the strong force, has a short range, but is much weaker than the strong force. The weak force tries to make the number of neutrons and protons into the most energetically stable configuration. For nuclei containing less than 40 particles, these numbers are usually about equal. Protons and neutrons are closely related and are collectively known as nucleons. As the number of particles increases toward a maximum of about 209, the number of neutrons to maintain stability begins to outstrip the number of protons, until the ratio of neutrons to protons is about three to two.\n\nThe protons of hydrogen combine to helium only if they have enough velocity to overcome each other's mutual repulsion sufficiently to get within range of the strong nuclear attraction. This means that fusion only occurs within a very hot gas. Hydrogen hot enough for combining to helium requires an enormous pressure to keep it confined, but suitable conditions exist in the central regions of the Sun, where such pressure is provided by the enormous weight of the layers above the core, pressed inwards by the Sun's strong gravity. The process of combining protons to form helium is an example of nuclear fusion.\n\nThe earth's oceans contain a large amount of hydrogen that could theoretically be used for fusion, and helium byproduct of fusion does not harm the environment, so some consider nuclear fusion a good alternative to supply humanity's energy needs. Experiments to generate electricity from fusion have so far only partially succeeded. Sufficiently hot hydrogen must be ionized and confined. One technique is to use very strong magnetic fields, because charged particles (like those trapped in the Earth's radiation belt) are guided by magnetic field lines. Fusion experiments also rely on heavy hydrogen, which fuses more easily, and gas densities can be moderate. But even with these techniques far more net energy is consumed by the fusion experiments than is yielded by the process.\n\nIn the main isotopes of light nuclei, such as carbon, nitrogen and oxygen, the most stable combination of neutrons and of protons are when the numbers are equal (this continues to element 20, calcium). However, in heavier nuclei, the disruptive energy of protons increases, since they are confined to a tiny volume and repel each other. The energy of the strong force holding the nucleus together also increases, but at a slower rate, as if inside the nucleus, only nucleons close to each other are tightly bound, not ones more widely separated.\n\nThe net binding energy of a nucleus is that of the nuclear attraction, minus the disruptive energy of the electric force. As nuclei get heavier than helium, their net binding energy per nucleon (deduced from the difference in mass between the nucleus and the sum of masses of component nucleons) grows more and more slowly, reaching its peak at iron. As nucleons are added, the total nuclear binding energy always increases—but the total disruptive energy of electric forces (positive protons repelling other protons) also increases, and past iron, the second increase outweighs the first. Iron-56 (Fe) is the most efficiently bound nucleus meaning that it has the least average mass per nucleon. However, nickel-62 is the most tightly bound nucleus in terms of energy of binding per nucleon . (Nickel-62's higher energy of binding does not translate to a larger mean mass loss than Fe-56, because Ni-62 has a slightly higher ratio of neutrons/protons than does iron-56, and the presence of the heavier neutrons increases nickel-62's average mass per nucleon).\n\nTo reduce the disruptive energy, the weak interaction allows the number of neutrons to exceed that of protons—for instance, the main isotope of iron has 26 protons and 30 neutrons. Isotopes also exist where the number of neutrons differs from the most stable number for that number of nucleons. If the ratio of protons to neutrons is too far from stability, nucleons may spontaneously change from proton to neutron, or neutron to proton.\n\nThe two methods for this conversion are mediated by the weak force, and involve types of beta decay. In the simplest beta decay, neutrons are converted to protons by emitting a negative electron and an antineutrino. This is always possible outside a nucleus because neutrons are more massive than protons by an equivalent of about 2.5 electrons. In the opposite process, which only happens within a nucleus, and not to free particles, a proton may become a neutron by ejecting a positron. This is permitted if enough energy is available between parent and daughter nuclides to do this (the required energy difference is equal to 1.022 MeV, which is the mass of 2 electrons). If the mass difference between parent and daughter is less than this, a proton-rich nucleus may still convert protons to neutrons by the process of electron capture, in which a proton simply electron captures one of the atom's K orbital electrons, emits a neutrino, and becomes a neutron.\n\nAmong the heaviest nuclei, starting with tellurium nuclei (element 52) containing 106 or more nucleons, electric forces may be so destabilizing that entire chunks of the nucleus may be ejected, usually as alpha particles, which consist of two protons and two neutrons (alpha particles are fast helium nuclei). (Beryllium-8 also decays, very quickly, into two alpha particles.) Alpha particles are extremely stable. This type of decay becomes more and more probable as elements rise in atomic weight past 106.\n\nThe curve of binding energy is a graph that plots the binding energy per nucleon against atomic mass. This curve has its main peak at iron and nickel and then slowly decreases again, and also a narrow isolated peak at helium, which as noted is very stable. The heaviest nuclei in nature, uranium U, are unstable, but having a half-life of 4.5 billion years, close to the age of the Earth, they are still relatively abundant; they (and other nuclei heavier than helium) have formed in stellar evolution events like supernova explosions preceding the formation of the solar system. The most common isotope of thorium, Th, also undergoes alpha particle emission, and its half-life (time over which half a number of atomformula_1s decays) is even longer, by several times. In each of these, radioactive decay produces daughter isotopes that are also unstable, starting a chain of decays that ends in some stable isotope of lead.\n\nCalculation can be employed to determine the nuclear binding energy of nuclei. The calculation involves determining the \"mass defect\", converting it into energy, and expressing the result as energy per mole of atoms, or as energy per nucleon.\n\nMass defect is defined as the difference between the mass of a nucleus, and the sum of the masses of the nucleons of which it is composed. The mass defect is determined by calculating three quantities. These are: the actual mass of the nucleus, the composition of the nucleus (number of protons and of neutrons), and the masses of a proton and of a neutron. This is then followed by converting the mass defect into energy. This quantity is the nuclear binding energy, however it must be expressed as energy per mole of atoms or as energy per nucleon.\n\nNuclear energy is released by the splitting (fission) or merging (fusion) of the nuclei of atom(s). The conversion of nuclear mass-energy to a form of energy, which can remove some mass when the energy is removed, is consistent with the mass-energy equivalence formula:\n\nΔ\"E\" = Δ\"m\" \"c\",\n\nin which,\n\nΔ\"E\" = energy release,\n\nΔ\"m\" = mass defect,\n\nand \"c\" = the speed of light in a vacuum (a physical constant 299,792,458 m/s by definition).\n\nNuclear energy was first discovered by French physicist Henri Becquerel in 1896, when he found that photographic plates stored in the dark near uranium were blackened like X-ray plates (X-rays had recently been discovered in 1895).\n\nNickel-62 has the highest binding energy per nucleon of any isotope. If an atom of lower average binding energy is changed into two atoms of higher average binding energy, energy is given off. Also, if two atoms of lower average binding energy fuse into an atom of higher average binding energy, energy is given off. The chart shows that fusion of hydrogen, the combination to form heavier atoms, releases energy, as does fission of uranium, the breaking up of a larger nucleus into smaller parts. Stability varies between isotopes: the isotope U-235 is much less stable than the more common U-238.\n\nNuclear energy is released by three \"exoenergetic\" (or exothermic) processes:\n\nThe binding energy of an atom (including its electrons) is not the same as the binding energy of the atom's nucleus. The measured mass deficits of isotopes are always listed as mass deficits of the neutral atoms of that isotope, and mostly in MeV. As a consequence, the listed mass deficits are not a measure for the stability or binding energy of isolated nuclei, but for the whole atoms. This has very practical reasons, because it is very hard to totally ionize heavy elements, i.e. strip them of all of their electrons.\n\nThis practice is useful for other reasons, too: stripping all the electrons from a heavy unstable nucleus (thus producing a bare nucleus) changes the lifetime of the nucleus, or the nucleus of a stable neutral atom can likewise become unstable after stripping, indicating that the nucleus cannot be treated independently. Examples of this have been shown in bound-state β decay experiments performed at the GSI) heavy ion accelerator. \nThis is also evident from phenomena like electron capture. Theoretically, in orbital models of heavy atoms, the electron orbits partially inside the nucleus (it does not \"orbit\" in a strict sense, but has a non-vanishing probability of being located inside the nucleus).\n\nA nuclear decay happens to the nucleus, meaning that properties ascribed to the nucleus change in the event. In the field of physics the concept of \"mass deficit\" as a measure for \"binding energy\" means \"mass deficit of the neutral atom\" (not just the nucleus) and is a measure for stability of the whole atom.\n\nIn the periodic table of elements, the series of light elements from hydrogen up to sodium is observed to exhibit generally increasing binding energy per nucleon as the atomic mass increases. This increase is generated by increasing forces per nucleon in the nucleus, as each additional nucleon is attracted by other nearby nucleons, and thus more tightly bound to the whole.\n\nThe region of increasing binding energy is followed by a region of relative stability (saturation) in the sequence from magnesium through xenon. In this region, the nucleus has become large enough that nuclear forces no longer completely extend efficiently across its width. Attractive nuclear forces in this region, as atomic mass increases, are nearly balanced by repellent electromagnetic forces between protons, as the atomic number increases.\n\nFinally, in elements heavier than xenon, there is a decrease in binding energy per nucleon as atomic number increases. In this region of nuclear size, electromagnetic repulsive forces are beginning to overcome the strong nuclear force attraction.\n\nAt the peak of binding energy, nickel-62 is the most tightly bound nucleus (per nucleon), followed by iron-58 and iron-56. This is the approximate basic reason why iron and nickel are very common metals in planetary cores, since they are produced profusely as end products in supernovae and in the final stages of silicon burning in stars. However, it is not binding energy per defined nucleon (as defined above), which controls which exact nuclei are made, because within stars, neutrons are free to convert to protons to release even more energy, per generic nucleon, if the result is a stable nucleus with a larger fraction of protons. In fact, it has been argued that photodisintegration of Ni to form Fe may be energetically possible in an extremely hot star core, due to this beta decay conversion of neutrons to protons. The conclusion is that at the pressure and temperature conditions in the cores of large stars, energy is released by converting all matter into Fe nuclei (ionized atoms). (However, at high temperatures not all matter will be in the lowest energy state.) This energetic maximum should also hold for ambient conditions, say \"T\" = 298 K and \"p\" = 1 atm, for neutral condensed matter consisting of Fe atoms—however, in these conditions nuclei of atoms are inhibited from fusing into the most stable and low energy state of matter.\n\nIt is generally believed that iron-56 is more common than nickel isotopes in the universe for mechanistic reasons, because its unstable progenitor nickel-56 is copiously made by staged build-up of 14 helium nuclei inside supernovas, where it has no time to decay to iron before being released into the interstellar medium in a matter of a few minutes, as the supernova explodes. However, nickel-56 then decays to cobalt-56 within a few weeks, then this radioisotope finally decays to iron-56 with a half life of about 77.3 days. The radioactive decay-powered light curve of such a process has been observed to happen in type II supernovae, such as SN 1987A. In a star, there are no good ways to create nickel-62 by alpha-addition processes, or else there would presumably be more of this highly stable nuclide in the universe.\n\nThe fact that the maximum binding energy is found in medium-sized nuclei is a consequence of the trade-off in the effects of two opposing forces that have different range characteristics. The attractive nuclear force (strong nuclear force), which binds protons and neutrons equally to each other, has a limited range due to a rapid exponential decrease in this force with distance. However, the repelling electromagnetic force, which acts between protons to force nuclei apart, falls off with distance much more slowly (as the inverse square of distance). For nuclei larger than about four nucleons in diameter, the additional repelling force of additional protons more than offsets any binding energy that results between further added nucleons as a result of additional strong force interactions. Such nuclei become increasingly less tightly bound as their size increases, though most of them are still stable. Finally, nuclei containing more than 209 nucleons (larger than about 6 nucleons in diameter) are all too large to be stable, and are subject to spontaneous decay to smaller nuclei.\n\nNuclear fusion produces energy by combining the very lightest elements into more tightly bound elements (such as hydrogen into helium), and nuclear fission produces energy by splitting the heaviest elements (such as uranium and plutonium) into more tightly bound elements (such as barium and krypton). Both processes produce energy, because middle-sized nuclei are the most tightly bound of all.\n\nAs seen above in the example of deuterium, nuclear binding energies are large enough that they may be easily measured as fractional mass deficits, according to the equivalence of mass and energy. The atomic binding energy is simply the amount of energy (and mass) released, when a collection of free nucleons are joined together to form a nucleus.\n\nNuclear binding energy can be computed from the difference in mass of a nucleus, and the sum of the masses of the number of free neutrons and protons that make up the nucleus. Once this mass difference, called the mass defect or mass deficiency, is known, Einstein's mass-energy equivalence formula \"E\" = \"mc\"² can be used to compute the binding energy of any nucleus. Early nuclear physicists used to refer to computing this value as a \"packing fraction\" calculation.\n\nFor example, the atomic mass unit (1 u) is defined as 1/12 of the mass of a C atom—but the atomic mass of a H atom (which is a proton plus electron) is 1.007825 \"u\", so each nucleon in C has lost, on average, about 0.8% of its mass in the form of binding energy.\n\nFor a nucleus with \"A\" nucleons, including \"Z\" protons and \"N\" neutrons, a semi-empirical formula for the binding energy (BE) per nucleon is:\n\nwhere the coefficients are given by: formula_3; formula_4; formula_5; formula_6; formula_7.\n\nThe first term formula_8 is called the saturation contribution and ensures that the binding energy per nucleon is the same for all nuclei to a first approximation. The term formula_9 is a surface tension effect and is proportional to the number of nucleons that are situated on the nuclear surface; it is largest for light nuclei. The term formula_10 is the Coulomb electrostatic repulsion; this becomes more important as formula_11 increases. The symmetry correction term formula_12 takes into account the fact that in the absence of other effects the most stable arrangement has equal numbers of protons and neutrons; this is because the n-p interaction in a nucleus is stronger than either the n-n or p-p interaction. The pairing term formula_13 is purely empirical; it is + for even-even nuclei and - for odd-odd nuclei.\n\nThe following table lists some binding energies and mass defect values. Notice also that we use 1 u = (931.494028 ± 0.000023) MeV. To calculate the binding energy we use the formula \"Z\" (\"m\" + \"m\") + \"N\" \"m\" − \"m\" where \"Z\" denotes the number of protons in the nuclides and \"N\" their number of neutrons. We take\n\"m\" = (938.2720813±0.0000058) MeV, \"m\" = (0.5109989461±0.000000003) MeV and \"m\" = (939.5654133 ± 0000058) MeV. The letter \"A\" denotes the sum of \"Z\" and \"N\" (number of nucleons in the nuclide). If we assume the reference nucleon has the mass of a neutron (so that all \"total\" binding energies calculated are maximal) we could define the total binding energy as the difference from the mass of the nucleus, and the mass of a collection of \"A\" free neutrons. In other words, it would be (\"Z\" + \"N\") \"m\" − \"m\". The \"\"total\" binding energy per nucleon\" would be this value divided by \"A\".\n\nFe has the lowest nucleon-specific mass of the four nuclides listed in this table, but this does not imply it is the strongest bound atom per hadron, unless the choice of beginning hadrons is completely free. Iron releases the largest energy if any 56 nucleons are allowed to build a nuclide—changing one to another if necessary, The highest binding energy per hadron, with the hadrons starting as the same number of protons \"Z\" and total nucleons \"A\" as in the bound nucleus, is Ni. Thus, the true absolute value of the total binding energy of a nucleus depends on what we are allowed to construct the nucleus out of. If all nuclei of mass number \"A\" were to be allowed to be constructed of \"A\" neutrons, then Fe would release the most energy per nucleon, since it has a larger fraction of protons than Ni. However, if nuclei are required to be constructed of only the same number of protons and neutrons that they contain, then nickel-62 is the most tightly bound nucleus, per nucleon.\n\nIn the table above it can be seen that the decay of a neutron, as well as the transformation of tritium into helium-3, releases energy; hence, it manifests a stronger bound new state when measured against the mass of an equal number of neutrons (and also a lighter state per number of total hadrons). Such reactions are not driven by changes in binding energies as calculated from previously fixed \"N\" and \"Z\" numbers of neutrons and protons, but rather in decreases in the total mass of the nuclide/per nucleon, with the reaction. (Note that the Binding Energy given above for hydrogen-1 is the atomic binding energy, not the nuclear binding energy which would be zero.)\n"}
{"id": "41268746", "url": "https://en.wikipedia.org/wiki?curid=41268746", "title": "OKC Oilfield Expo", "text": "OKC Oilfield Expo\n\nThe OKC Oilfield Expo was an annual oil and gas industry exhibition focused on providing a venue for industry professionals to network, view new oilfield technologies, compare products and services, and make business deals. The event was held each October at the Cox Convention Center in Oklahoma City, and was the largest energy industry trade fair in Oklahoma. .\nIn 2013, the expo featured 335 participating companies across 445 exhibitor booths, and attracted energy industry participants from 25 U.S. states and six different countries. \n"}
{"id": "21797771", "url": "https://en.wikipedia.org/wiki?curid=21797771", "title": "Plumbide", "text": "Plumbide\n\nA plumbide can refer to one of two things: an intermetallic compound that contains lead, or a Zintl phase compound with lead as the anion.\n\nPlumbides can be formed when lead forms a Zintl phase compound with a more metallic element. One salt that can be formed this way is when cryptand reacts with sodium and lead in ethylenediamine (en) to produce [Pb], which is red in solution.\nLead can also create anions with tin, in a series of anions with the formula [SnPb].\n\nLead can also form the [Pb] anion, which is emerald green in solution.\n\nAn example of a plumbide is CeRhPb. The lead atom has a coordination number of 12 in the crystal structure of this compound. It is bound to four rhodiums, six ceriums, and two other lead atoms in the crystal structure of the chemical.\n\nSeveral other plumbides are the MPdPb plumbides, where M is a rare-earth element, and the intermetallic additionally contains a palladium. These plumbides tend to exhibit antiferromagnetism, and all of them are conductors.\n\nA third plumbide is TiPb. Like the above plumbides, it is an intermetallic, but it only contains titanium as the other metal, and not any rare earths.\n\nPlumbides can also be Zintl phase compounds, such as [K(18-crown-6)]KPb·(en). This is not a simple Zintl compound, but rather contains the organic molecules 18-crown-6 and ethylenediamine (en) in order to stabilize the crystal structure.\n"}
{"id": "43431012", "url": "https://en.wikipedia.org/wiki?curid=43431012", "title": "Pressuron", "text": "Pressuron\n\nThe pressuron is a hypothetical scalar particle which couples to both gravity and matter theorised in 2013. Although originally postulated without self-interaction potential, the pressuron is also a dark energy candidate when it has such a potential. The pressuron takes its name from the fact that it decouples from matter in pressure-less regimes, allowing the scalar-tensor theory of gravity involving it to pass solar system tests, as well as tests on the equivalence principle, even though it is fundamentally coupled to matter. Such a decoupling mechanism could explain why gravitation seems to be well described by general relativity at present epoch, while it could actually be more complex than that. Because of the way it couples to matter, the pressuron is a special case of the hypothetical string dilaton. Therefore, it is one of the possible solutions to the present non-observation of various signals coming from massless or light scalar fields that are generically predicted in string theory.\n\nThe action of the scalar-tensor theory that involves the pressuron formula_1 can be written as\n\nwhere formula_3 is the Ricci scalar constructed from the metric formula_4, formula_5 is the metric determinant, formula_6, with formula_7 the gravitational constant and formula_8 the velocity of light in vacuum, formula_9 is the pressuron potential and formula_10 is the matter Lagrangian and formula_11 represents the non-gravitational fields. The gravitational field equations therefore write\n\nand\n\nwhere formula_14 is the stress–energy tensor of the matter field,\nand formula_15 is its trace.\n\nIf one considers a pressure-free perfect fluid (also known as a \"dust\"), the effective material Lagrangian becomes formula_16,\nwhere formula_17 is the mass of the ith particle, formula_18 its position, and formula_19 the Dirac delta function, \nwhile at the same time the trace of the stress-energy tensor reduces to formula_20.\nThus, there is an exact cancellation of the pressuron material source term formula_21, and hence the pressuron effectively decouples from pressure-free matter fields.\n\nIn other words, the specific coupling between the scalar field and the material fields in the Lagrangian leads to a decoupling between the scalar field and the matter fields in the limit that the matter field is exerting zero pressure. \n\nThe pressuron shares some characteristics with the hypothetical string dilaton, and can actually be viewed as a special case of the wider family of possible dilatons. Since perturbative string theory cannot currently give the expected coupling of the string dilaton with material fields in the effective 4-dimension action, it seems conceivable that the pressuron may be the string dilaton in the 4-dimension effective action.\n\nAccording to Minazzoli and Hees, post-Newtonian tests of gravitation in the Solar System should lead to the same results as what is expected from general relativity, except for gravitational redshift experiments, which should deviate from general relativity with a relative magnitude of the order of formula_22, where formula_23 is the current cosmological value of the scalar-field function formula_24, and formula_25 and formula_26 are respectively the mean pressure and density of the Earth (for instance). Current best constraints on the gravitational redshift come from gravity probe A and are at the formula_27 level only. Therefore, the scalar-tensor theory that involves the pressuron is weakly constrained by Solar System experiments.\n\nBecause of its non-minimal couplings, the pressuron leads to a variation of the fundamental coupling constants in regimes where it effectively couples to matter. However, since the pressuron decouples in both the matter-dominated era (which is essentially driven by pressure-less material fields) and the dark-energy-dominated era (which is essentially driven by dark energy), the pressuron is also weakly constrained by current cosmological tests on the variation of the coupling constants.\n\nAlthough no calculations seem to have been performed regarding this issue, it has been argued that binary pulsars should give greater constraints on the existence of the pressuron because of the high pressure of bodies involved in such systems.\n"}
{"id": "12427641", "url": "https://en.wikipedia.org/wiki?curid=12427641", "title": "Ricinelaidic acid", "text": "Ricinelaidic acid\n\nRicinelaidic acid or (+)-(\"R\")-ricinelaidic acid is an unsaturated omega-9 trans fatty acid. It is the trans-isomer of the fatty acid ricinoleic acid.\n\n"}
{"id": "22150292", "url": "https://en.wikipedia.org/wiki?curid=22150292", "title": "Rock Island Rockets (1937)", "text": "Rock Island Rockets (1937)\n\nThe Chicago, Rock Island and Pacific Railroad Rockets were lightweight, streamlined diesel-electric passenger trains built by the Budd Company. These six trains were the first streamlined equipment purchased by the Rock Island, as well as being its first diesel-powered passenger trains. Four of the trains consisted of three cars each, the other two each had four cars.\n\nThe stainless steel trains were each powered by an Electro-Motive Corporation model TA locomotive. Unlike many other early streamlined trains, the locomotives were not permanently attached to the trains.\n\nThe trains were articulated except for the observation cars.\n\nThe six trains as originally assigned were:\n\n\n\n\n"}
{"id": "26187747", "url": "https://en.wikipedia.org/wiki?curid=26187747", "title": "Shell Spher process", "text": "Shell Spher process\n\nThe Shell Spher process (Shell Pellet Heat Exchange Retorting) is an above ground fluidization bed retorting technology for shale oil extraction. It is classified as a hot recycled solids technology.\n\nRaw oil shale is crushed to a fine particles. Heat is transferred to oil shale by heat-carrying ceramic balls of size . Raw oil shale is preheated in fluidized bed at the temperature of in the case if oxygen is used as fluidizing medium, or at if non-oxidizing gases are used. Heated ceramic balls fall then through the bed in counter-current direction. The preheated oil shale is further heated in the retorting vessel. The retorted spent shale is cooled in a fast-fluidized bed by the recirculated cool pellets from the preheater; while cooling the spent shale ceramic balls are heated by the spent shale.\n\n"}
{"id": "3328072", "url": "https://en.wikipedia.org/wiki?curid=3328072", "title": "Silicon nitride", "text": "Silicon nitride\n\nSilicon nitride is a chemical compound of the elements silicon and nitrogen. is the most thermodynamically stable of the silicon nitrides. Hence, is the most commercially important of the silicon nitrides and is generally understood as what is being referred to where the term \"silicon nitride\" is used. It is a white, high-melting-point solid that is relatively chemically inert, being attacked by dilute HF and hot . It is very hard (8.5 on the mohs scale). It has a high thermal stability.\n\nThe material is prepared by heating powdered silicon between 1300 °C and 1400 °C in a nitrogen environment:\n\nThe silicon sample weight increases progressively due to the chemical combination of silicon and nitrogen. Without an iron catalyst, the reaction is complete after several hours (~7), when no further weight increase due to nitrogen absorption (per gram of silicon) is detected. In addition to , several other silicon nitride phases (with chemical formulas corresponding to varying degrees of nitridation/Si oxidation state) have been reported in the literature, for example, the gaseous disilicon mononitride (); silicon mononitride (SiN), and silicon sesquinitride (), each of which are stoichiometric phases. As with other refractories, the products obtained in these high-temperature syntheses depends on the reaction conditions (e.g. time, temperature, and starting materials including the reactants and container materials), as well as the mode of purification. However, the existence of the sesquinitride has since come into question.\n\nIt can also be prepared by diimide route: \n\nCarbothermal reduction of silicon dioxide in nitrogen atmosphere at 1400–1450 °C has also been examined: \n\nThe nitridation of silicon powder was developed in the 1950s, following the \"rediscovery\" of silicon nitride and was the first\nlarge-scale method for powder production. However, use of low-purity raw silicon caused contamination of silicon nitride by silicates and iron. The diimide decomposition results in amorphous silicon nitride, which needs further annealing under nitrogen at 1400–1500 °C to convert it to crystalline powder; this is now the second-most important route for commercial production. The carbothermal reduction was the earliest used method for silicon nitride production and is now considered as the most-cost-effective industrial route to high-purity silicon nitride powder.\n\nElectronic-grade silicon nitride films are formed using chemical vapor deposition (CVD), or one of its variants, such as plasma-enhanced chemical vapor deposition (PECVD): \n\nFor deposition of silicon nitride layers on semiconductor (usually silicon) substrates, two methods are used:\nThe lattice constants of silicon nitride and silicon are different. Therefore, tension or stress can occur, depending on the deposition process. Especially when using PECVD technology this tension can be reduced by adjusting deposition parameters.\n\nSilicon nitride nanowires can also be produced by sol-gel method using carbothermal reduction followed\nby nitridation of silica gel, which contains ultrafine carbon particles. The particles can be produced by decomposition of dextrose in the temperature range 1200–1350 °C. The possible synthesis reactions are:\n\nSilicon nitride is difficult to produce as a bulk material—it cannot be heated over 1850 °C, which is well below its melting point, due to dissociation to silicon and nitrogen. Therefore, application of conventional hot press sintering techniques is problematic. Bonding of silicon nitride powders can be achieved at lower temperatures through adding additional materials (sintering aids or \"binders\") which commonly induce a degree of liquid phase sintering. A cleaner alternative is to use spark plasma sintering where heating is conducted very rapidly (seconds) by passing pulses of electric current through the compacted powder. Dense silicon nitride compacts have been obtained by this techniques at temperatures 1500–1700 °C.\n\nThere exist three crystallographic structures of silicon nitride (), designated as α, β and γ phases. The α and β phases are the most common forms of , and can be produced under normal pressure condition. The γ phase can only be synthesized under high pressures and temperatures and has a hardness of 35 GPa.\nThe α- and β- have trigonal (Pearson symbol hP28, space group P31c, No. 159) and hexagonal (hP14, P6, No. 173) structures, respectively, which are built up by corner-sharing tetrahedra. They can be regarded as consisting of layers of silicon and nitrogen atoms in the sequence ABAB... or ABCDABCD... in β- and α-, respectively. The AB layer is the same in the α and β phases, and the CD layer in the α phase is related to AB by a c-glide plane. The tetrahedra in β- are interconnected in such a way that tunnels are formed, running parallel with the c axis of the unit cell. Due to the c-glide plane that relates AB to CD, the α structure contains cavities instead of tunnels. The cubic γ- is often designated as c modification in the literature, in analogy with the cubic modification of boron nitride (c-BN). It has a spinel-type structure in which two silicon atoms each coordinate six nitrogen atoms octahedrally, and one silicon atom coordinates four nitrogen atoms tetrahedrally.\n\nThe longer stacking sequence results in the α-phase having higher hardness than the β-phase. However, the α-phase is chemically unstable compared with the β-phase. At high temperatures when a liquid phase is present, the α-phase always transforms into the β-phase. Therefore, β- is the major form used in ceramics.\n\nIn general, the main issue with applications of silicon nitride has not been technical performance, but cost. As the cost has come down, the number of production applications is accelerating.\n\nOne of the major applications of sintered silicon nitride is in automobile industry as a material for engine parts. Those include, in diesel engines, glowplugs for faster start-up; precombustion chambers (swirl chambers) for lower emissions, faster start-up and lower noise; turbocharger for reduced engine lag and emissions. In spark-ignition engines, silicon nitride is used for rocker arm pads for lower wear, turbocharger for lower inertia and less engine lag, and in exhaust gas control valves for increased acceleration. As examples of production levels, there is an estimated more than 300,000 sintered silicon nitride turbochargers made annually.\n\nSilicon nitride bearings are both full ceramic bearings and ceramic hybrid bearings with balls in ceramics and races in steel. Silicon nitride ceramics have good shock resistance compared to other ceramics. Therefore, ball bearings made of silicon nitride ceramic are used in performance bearings. A representative example is use of silicon nitride bearings in the main engines of the NASA's Space Shuttle.\n\nSince silicon nitride ball bearings are harder than metal, this reduces contact with the bearing track. This results in 80% less friction, 3 to 10 times longer lifetime, 80% higher speed, 60% less weight, the ability to operate with lubrication starvation, higher corrosion resistance and higher operation temperature, as compared to traditional metal bearings. Silicon nitride balls weigh 79% less than tungsten carbide balls. Silicon nitride ball bearings can be found in high end automotive bearings, industrial bearings, wind turbines, motorsports, bicycles, rollerblades and skateboards. Silicon nitride bearings are especially useful in applications where corrosion, electric or magnetic fields prohibit the use of metals. For example, in tidal flow meters, where seawater attack is a problem, or in electric field seekers.\n\nSiN was first demonstrated as a superior bearing in 1972 but did not reach production until nearly 1990 because of challenges associated with reducing the cost.\nSince 1990, the cost has been reduced substantially as production volume has increased. Although bearings are still 2–5 times more expensive than the best steel bearings, their superior performance and life are justifying rapid adoption. Around 15–20 million bearing balls were produced in the U.S. in 1996 for machine tools and many other applications. Growth is estimated at 40% per year, but could be even higher if ceramic bearings are selected for consumer applications such as in-line skates and computer disk drives.\n\nSilicon nitride has long been used in high-temperature applications. In particular, it was identified as one of the few monolithic ceramic materials capable of surviving the severe thermal shock and thermal gradients generated in hydrogen/oxygen rocket engines. To demonstrate this capability in a complex configuration, NASA scientists used advanced rapid prototyping technology to fabricate a one-inch-diameter, single-piece combustion chamber/nozzle (thruster) component. The thruster was hot-fire tested with hydrogen/oxygen propellant and survived five cycles including a 5-minute cycle to a 1320 °C material temperature.\n\nIn 2010 silicon nitride was used as the main material in the thrusters of the JAXA space probe Akatsuki.\n\nSilicon nitride has many orthopedic applications. The material is also an alternative to PEEK (polyether ether ketone) and titanium, which are used for spinal fusion devices. It is silicon nitride’s hydrophilic, microtextured surface that contributes to the material's strength, durability and reliability compared to PEEK and titanium.\n\nThe first major application of was abrasive and cutting tools. Bulk, monolithic silicon nitride is used as a material for cutting tools, due to its hardness, thermal stability, and resistance to wear. It is especially recommended for high speed machining of cast iron. Hot hardness, fracture toughness and thermal shock resistance mean that sintered silicon nitride can cut cast iron, hard steel and nickel based alloys with surface speeds up to 25 times quicker than those obtained with conventional materials such as tungsten carbide. The use of cutting tools has had a dramatic effect on manufacturing output. For example, face milling of gray cast iron with silicon nitride inserts doubled the cutting speed, increased tool life from one part to six parts per edge, and reduced the average cost of inserts by 50%, as compared to traditional tungsten carbide tools.\n\nSilicon nitride is often used as an insulator and chemical barrier in manufacturing integrated circuits, to electrically isolate different structures or as an etch mask in bulk micromachining. As a passivation layer for microchips, it is superior to silicon dioxide, as it is a significantly better diffusion barrier against water molecules and sodium ions, two major sources of corrosion and instability in microelectronics. It is also used as a dielectric between polysilicon layers in capacitors in analog chips.\n\nSilicon nitride deposited by LPCVD contains up to 8% hydrogen. It also experiences strong tensile stress, which may crack films thicker than 200 nm. However, it has higher resistivity and dielectric strength than most insulators commonly available in microfabrication (10 Ω·cm and 10 MV/cm, respectively).\n\nNot only silicon nitride, but also various ternary compounds of silicon, nitrogen and hydrogen (SiNH) are used insulating layers. They are plasma deposited using the following reactions:\n\nThese SiNH films have much less tensile stress, but worse electrical properties (resistivity 10 to 10 Ω·cm, and dielectric strength 1 to 5 MV/cm).\nThese silicon films are also thermally stable to high temperatures under specific physical conditions.\nSilicon nitride is also used in xerographic process as one of the layer of the photo drum. Silicon nitride is also used as an ignition source for domestic gas appliances. Because of its good elastic properties, silicon nitride, along with silicon and silicon oxide, is the most popular material for cantilevers — the sensing elements of atomic force microscopes.\n\nThe first preparation was reported in 1857 by Henri Etienne Sainte-Claire Deville and Friedrich Wöhler. In their method, silicon was heated in a crucible placed inside another crucible packed with carbon to reduce permeation of oxygen to the inner crucible. They reported a product they termed silicon nitride but without specifying its chemical composition. Paul Schuetzenberger first reported a product with the composition of the tetranitride, , in 1879 that was obtained by heating silicon with brasque (a paste made by mixing charcoal, coal, or coke with clay which is then used to line crucibles) in a blast furnace. In 1910, Ludwig Weiss and Theodor Engelhardt heated silicon under pure nitrogen to produce . E. Friederich and L. Sittig made SiN in 1925 via carbothermal reduction under nitrogen, that is, by heating silica, carbon, and nitrogen at 1250–1300 °C.\n\nSilicon nitride remained merely a chemical curiosity for decades before it was used in commercial applications. From 1948 to 1952, the Carborundum Company, Niagara Falls, New York, applied for several patents on the manufacture and application of silicon nitride. By 1958 Haynes (Union Carbide) silicon nitride was in commercial production for thermocouple tubes, rocket nozzles, and boats and crucibles for melting metals. British work on silicon nitride, started in 1953, was aimed at high-temperature parts of gas turbines and resulted in the development of reaction-bonded silicon nitride and hot-pressed silicon nitride. In 1971, the Advanced Research Project Agency of the US Department of Defense placed a US$17 million contract with Ford and Westinghouse for two ceramic gas turbines.\n\nEven though the properties of silicon nitride were well known, its natural occurrence was discovered only in the 1990s, as tiny inclusions (about 2 µm × 0.5 µm in size) in meteorites. The mineral was named nierite after a pioneer of mass spectrometry, Alfred O. C. Nier. This mineral might have been detected earlier, again exclusively in meteorites, by Soviet geologists.\n"}
{"id": "35325377", "url": "https://en.wikipedia.org/wiki?curid=35325377", "title": "Spider9", "text": "Spider9\n\nSpider9 Inc. is an American, environmental technologies company headquartered in Northville, MI which develops and manufactures advanced control systems for energy storage and solar fields. It was founded in 2011 by Glynne Townsend (A123 Systems), Dave Park (former Vice President of Production at Wave Crest Energy Systems), Dave Smith (former chairman of USABC), and Bill Beckman (former Vice President of Finance at Johnson Controls).\n\nSpider9 acquired control system technology from the University of Michigan, Office of Technology Transfer and is developing the technology at their facilities at the Water Wheel Centre, in the historic Ford Valve Plant.\n\nSpider9 was founded on control system technology patents licensed from the University of Michigan Real-Time Computing Laboratory. In the summer of 2011, the Spider9 leadership team brought the technology out of the Office of Technology Transfer where it had been incubating. In July 2011, Spider9 pivoted the technology’s business plan to target grid energy storage and solar field optimization rather than electric vehicles. On November 3, 2011, Spider9 received a Michigan Economic Development Corporation (MEDC) grant to install a solar field on the roof of the Water Wheel Centre in Northville, MI.\n\nSpider9 systems are capable of reconfiguring the architecture of systems through in-house developed hardware and software algorithms. Using a Spider9 system, individual cells and panels are monitored and managed. The system architecture is dynamically reconfigured around the components to deliver a consistent voltage output, compensating for performance variances and failures.\n\nSpider9’s facilities are equipped with a waterwheel designed by Albert Kahn (architect) which once provided constant, sustainable hydroelectric power to the facility. The waterwheel no longer fulfills the electricity needs of the building. Spider9 is designing a control system for an 85 kW rooftop solar field with backup battery storage to supply the building with renewable power.\n\n"}
{"id": "16797074", "url": "https://en.wikipedia.org/wiki?curid=16797074", "title": "Stonehenge Archer", "text": "Stonehenge Archer\n\nThe Stonehenge Archer is the name given to a Bronze Age man whose body was discovered in the outer ditch of Stonehenge. Unlike most burials in the Stonehenge Landscape, his body was not in a barrow, although it did appear to have been deliberately and carefully buried in the ditch.\n\nExamination of the skeleton indicated that the man was local to the area and aged about 30 when he died. Radiocarbon dating suggests that he died around 2300 BCE, making his death roughly contemporary with the Amesbury Archer and the Boscombe Bowmen buried 3 miles away in Amesbury.\n\nHe came to be known as an archer because of the stone wrist-guard and a number of flint arrowheads buried with him. In fact, several of the arrowheads' tips were located in the skeleton's bones, suggesting that the man had been killed by them.\n\nHis body was excavated in 1978 by Richard Atkinson and John G. Evans who had been re-examining an older trench in the ditch and bank of Stonehenge. His remains are now housed in the Salisbury Museum in Salisbury.\n\n\n\n"}
{"id": "27399776", "url": "https://en.wikipedia.org/wiki?curid=27399776", "title": "The End of Nature", "text": "The End of Nature\n\nThe End of Nature is a book written by Bill McKibben, published by Anchor in 1989. It has been called the first book on global warming written for a general audience. McKibben had thought that simply stating the problem would provoke people to action.\n\nHe describes nature as a force previously independent of human beings, but which is now directly affected by the actions of people.\n\"If the waves crash up against the beach, eroding dunes and destroying homes, it is not the awesome power of Mother Nature. It is the awesome power of Mother Nature as altered by the awesome power of man, who has overpowered in a century the processes that have been slowly evolving and changing of their own accord since the earth was born.\"\n\nHe offers two paths forward: \"The Defiant Reflex\" or a \"more humble\" way of living.\n"}
{"id": "20616113", "url": "https://en.wikipedia.org/wiki?curid=20616113", "title": "The Sacred Balance", "text": "The Sacred Balance\n\nThe Sacred Balance is a book by environmentalist David Suzuki, which is in its second edition as of 2007. The book explores human society's impact on the natural world, both for the planet and the people living on it. Suzuki reveals how dependent humankind is upon the planet's water, soil, sunlight, and the breath of its vegetation. Threats to the planet's balance, ranging from toxic pollution to global warming are also discussed.\n\nA series of documentary films, also called \"The Sacred Balance\", is based on the book produced by Kensington Communications, Inc. Producer Robert Lang was the producer of the series.\n"}
{"id": "6951435", "url": "https://en.wikipedia.org/wiki?curid=6951435", "title": "Théophile de Donder", "text": "Théophile de Donder\n\nThéophile Ernest de Donder (; 19 August 1872 – 11 May 1957) was a Belgian mathematician and physicist famous for his work (published in 1923) in developing correlations between the Newtonian concept of chemical affinity and the Gibbsian concept of free energy.\n\nHe received his doctorate in physics and mathematics from the Université Libre de Bruxelles in 1899, for a thesis entitled \"Sur la Théorie des Invariants Intégraux\" (\"On the Theory of Integral Invariants\").\n\nHe was professor between 1911 and 1942, at the Université Libre de Bruxelles. Initially he continued the work of Henri Poincaré and Élie Cartan. From 1914 on, he was influenced by the work of Albert Einstein and was an enthusiastic proponent of the theory of relativity. He gained significant reputation in 1923, when he developed his definition of chemical affinity. He pointed out a connection between the chemical affinity and the Gibbs free energy.\n\nHe is considered the father of thermodynamics of irreversible processes. De Donder’s work was later developed further by Ilya Prigogine. De Donder was an associate and friend of Albert Einstein.\n\n\n\n"}
{"id": "421597", "url": "https://en.wikipedia.org/wiki?curid=421597", "title": "Tractive force", "text": "Tractive force\n\nAs used in mechanical engineering, the term tractive force can either refer to the total traction a vehicle exerts on a surface, or the amount of the total traction that is parallel to the direction of motion.\n\nIn railway engineering, the term tractive effort is often used synonymously with tractive force to describe the pulling or pushing capability of a locomotive. In automotive engineering, the terms are distinctive: tractive effort is generally higher than tractive force by the amount of rolling resistance present, and both terms are higher than the amount of drawbar pull by the total resistance present (including air resistance and grade). The published tractive force value for any vehicle may be theoretical—that is, calculated from known or implied mechanical properties—or obtained via testing under controlled conditions. The discussion herein covers the term's usage in mechanical applications in which the final stage of the power transmission system is one or more wheels in frictional contact with a roadway or railroad track.\n\nThe term tractive effort is often qualified as starting tractive effort, continuous tractive effort and maximum tractive effort. These terms apply to different operating conditions, but are related by common mechanical factors: input torque to the driving wheels, the wheel diameter, coefficient of friction (μ) between the driving wheels and supporting surface, and the weight applied to the driving wheels (m). The product of μ and m is the factor of adhesion, which determines the maximum torque that can be applied before the onset of wheelspin or wheelslip.\n\n\nVehicles having a hydrodynamic coupling, hydrodynamic torque multiplier or electric motor as part of the power transmission system may also have a maximum continuous tractive effort rating, which is the highest tractive force that can be produced for a short period of time without causing component harm. The period of time for which the maximum continuous tractive effort may be safely generated is usually limited by thermal considerations. such as temperature rise in a traction motor.\n\nSpecifications of locomotives often include tractive effort curves, showing the relationship between tractive effort and velocity.\n\nThe shape of the graph is shown at right. The line AB shows operation at the maximum tractive effort, the line BC shows continuous tractive effort that is inversely proportional to speed (constant power).\n\nTractive effort curves often have graphs of rolling resistance superimposed on them—the intersection of the rolling resistance graph and tractive effort graph gives the maximum velocity (when net tractive effort is zero).\n\nIn order to start a train and accelerate it to a given speed, the locomotive(s) must develop sufficient tractive force to overcome the train's drag (resistance to motion), which is a combination of inertia, axle bearing friction, the friction of the wheels on the rails (which is substantially greater on curved track than on tangent track), and the force of gravity if on a grade. Once in motion, the train will develop additional drag as it accelerates due to aerodynamic forces, which increase with the square of the speed. Drag may also be produced at speed due to truck (bogie) hunting, which will increase the rolling friction between wheels and rails. If acceleration continues, the train will eventually attain a speed at which the available tractive force of the locomotive(s) will exactly offset the total drag, causing acceleration to cease. This top speed will be increased on a downgrade due to gravity assisting the motive power, and will be decreased on an upgrade due to gravity opposing the motive power.\n\nTractive effort can be theoretically calculated from a locomotive's mechanical characteristics (e.g., steam pressure, weight, etc.), or by actual testing with drawbar strain sensors and a dynamometer car. Power at rail is a railway term for the available power for traction, that is, the power that is available to propel the train.\n\nAn estimate for the tractive effort of a single cylinder steam locomotive can be obtained from the cylinder pressure, cylinder bore, stroke of the piston and the diameter of the wheel. The torque developed by the linear motion of the piston depends on the angle that the driving rod makes with the tangent of the radius on the driving wheel. For a more useful value an average value over the rotation of the wheel is used. The driving force is the torque divided by the wheel radius.\n\nAs an approximation, the following formula can be used (for a two-cylinder locomotive):\n\nwhere\n\nThe constant 0.85 was the Association of American Railroads (AAR) standard for such calculations, and overestimated the efficiency of some locomotives and underestimated that of others. Modern locomotives with roller bearings were probably underestimated.\n\nEuropean designers used a constant of 0.6 instead of 0.85, so the two cannot be compared without a conversion factor. In Britain main-line railways generally used a constant of 0.85 but builders of industrial locomotives often used a lower figure, typically 0.75.\n\nThe constant \"c\" also depends on the cylinder dimensions and the time at which the steam inlet valves are open; if the steam inlet valves are closed immediately after obtaining full cylinder pressure the piston force can be expected to have dropped to less than half the initial force. giving a low \"c\" value. If the cylinder valves are left open for longer the value of \"c\" will rise nearer to one.\n\nThe result should be multiplied by 1.5 for a three-cylinder locomotive and by two for a four-cylinder locomotive.\n\nFor other numbers and combinations of cylinders, including double and triple expansion engines the tractive effort can be estimated by adding the tractive efforts due to the individual cylinders at their respective pressures and cylinder strokes.\n\nTractive effort is the figure often quoted when comparing the powers of steam locomotives, but is misleading because tractive effort shows the ability to start a train, not the ability to haul it. Possibly the highest tractive effort ever claimed was for the Virginian Railway's 2-8-8-8-4 Triplex locomotive, which in simple expansion mode had a calculated starting T.E. of 199,560 lbf (887.7 kN) — but the boiler could not produce enough steam to haul at speeds over 5 mph (8 km/h).\n\nOf more successful steam locomotives, those with the highest rated starting tractive effort were the Virginian Railway AE-class 2-10-10-2s, at 176,000 lbf (783 kN) in simple-expansion mode (or 162,200 lb if calculated by the usual formula). The Union Pacific Big Boys had a starting T.E. of 135,375 lbf (602 kN); the Norfolk & Western's Y5, Y6, Y6a, and Y6b class 2-8-8-2s had a starting T.E. of 152,206 lbf (677 kN) in simple expansion mode (later modified to 170,000 lbf (756 kN), claim some enthusiasts); and the Pennsylvania Railroad's freight Duplex Q2 attained 114,860 lbf (510.9 kN, including booster) — the highest for a rigid framed locomotive. Later two-cylinder passenger locomotives were generally 40,000 to 80,000 lbf (170 to 350 kN) of T.E.\n\nFor an electric locomotive or a diesel-electric locomotive, starting tractive effort can be calculated from the amount of weight on the driving wheels (which may be less than the total locomotive weight in some cases), combined stall torque of the traction motors, the gear ratio between the traction motors and axles, and driving wheel diameter. For a diesel-hydraulic locomotive, the starting tractive effort is affected by the stall torque of the torque converter, as well as gearing, wheel diameter and locomotive weight.\n\nFreight locomotives are designed to produce higher maximum tractive effort than passenger units of equivalent power, necessitated by the much higher weight that is typical of a freight train. In modern locomotives, the gearing between the traction motors and axles is selected to suit the type of service in which the unit will be operated. As traction motors have a maximum speed at which they can rotate without incurring damage, gearing for higher tractive effort is at the expense of top speed. Conversely, the gearing used with passenger locomotives favors speed over maximum tractive effort.\n\nElectric locomotives with monomotor bogies are sometimes fitted with two-speed gearing. This allows higher tractive effort for hauling freight trains but at reduced speed. Examples include the SNCF classes BB 8500 and BB 25500.\n\n\n"}
{"id": "8434698", "url": "https://en.wikipedia.org/wiki?curid=8434698", "title": "Vena contracta", "text": "Vena contracta\n\nVena contracta is the point in a fluid stream where the diameter of the stream is the least, and fluid velocity is at its maximum, such as in the case of a stream issuing out of a nozzle, (orifice). (Evangelista Torricelli, 1643). It is a place where the cross section area is minimum. The maximum contraction takes place at a section slightly downstream of the orifice, where the jet is more or less horizontal.\n\nThe effect is also observed in flow from a tank into a pipe, or a sudden contraction in pipe diameter. Streamlines will converge just downstream of the diameter change, and a region of separated flow occurs from the sharp corner of the diameter change and extends past the vena contracta.\n\nThe reason for this phenomenon is that fluid streamlines cannot abruptly change direction. In the case of both the free jet and the sudden pipe diameter change, the streamlines are unable to closely follow the sharp angle in the pipe/tank wall. The converging streamlines follow a smooth path, which results in the narrowing of the jet (or primary pipe flow) observed.\n\nMeasurement of the vena contracta is useful in echocardiography, where it describes the smallest area of the blood flow jet as it exits a heart valve. This corresponds to the Effective Orifice Area calculated for heart valves using the continuity equation.\n\nVena Contracta was also a term used by several English shotgun builders of the 19th and 20th Century. The gun barrels of sporting shotguns tapered very heavily from the breech to the muzzle. Thus a gun with a 12 bore breech would have a 20 bore muzzle. The idea was to retain the advantages a heavy-hitting large bore shotgun while retaining the lesser recoil and easy maneuverability of a small bore. Several leading firms built this type of gun but it proved unpopular and most were returned to the manufacturers for large bore barrels. To most shooters perhaps the idea of placing a 12 bore cartridge into a 20 bore barrel was too \"explosive\". Complete functioning examples are now rare, though they are still not highly sought after.\n\nIt is the ratio between the area of the jet at the vena contracta to the area of the contracted flow section\n\nC = Area at vena contracta/ Area of orifice\n\nThe typical value may be taken as 0.64 for a sharp orifice (concentric with the flow channel). The smaller the value, the more effect the vena contracta has.\n\n"}
{"id": "15634281", "url": "https://en.wikipedia.org/wiki?curid=15634281", "title": "Vitino", "text": "Vitino\n\nVitino () is an oil port on the White Sea in Russia. It is located near Beloye More railway station, a few kilometers south of Kandalaksha, Murmansk Oblast, on the western shore of Kandalaksha Gulf.\n\nThe port started operations in 1995, and since 2001 has been operating year-round. Its principal activity is transfer of oil and oil products arriving by rail from Russia's oil refineries to seagoing oil tankers for export.\nIn 2004, 3.7 million tons of oil and oil products were handled by the port. Its full capacity at the time was 8 million tons a year.\n\nThe port can handle seagoing tankers with displacement up to 40,000 and with the dimensions up to:\nThere are also two piers for handling smaller sea-and-river going boats\n\n"}
{"id": "9742791", "url": "https://en.wikipedia.org/wiki?curid=9742791", "title": "Western Climate Initiative", "text": "Western Climate Initiative\n\nThe Western Climate Initiative, or WCI, was started in February 2007 by the governors of five western U.S. states (Arizona, California, New Mexico, Oregon, and Washington) with the goal of developing a multi-sector, market-based program to reduce greenhouse gas emissions.\n\nThe Western Climate Initiative, or WCI, was started in February 2007 by the governors of Arizona, California, New Mexico, Oregon and Washington to evaluate and implement ways to reduce their states's emissions of greenhouse gases and achieve related co-benefits. These states and future participants in the initiative (collectively known as WCI \"partners\") also committed to set an overall regional goal to reduce emissions (set in August 2007 as 15 percent below 2005 emission levels by 2020), participate in a cross-border greenhouse gas registry to consistently measure and track emissions, and adopt clean tailpipe standards for passenger vehicles. By July 2008, the initiative had expanded to include two more U.S. states (Montana and Utah) and four Canadian provinces (British Columbia, Manitoba, Ontario and Quebec). Together, these partners comprised 20 percent of U.S. GDP and 76 percent of Canadian GDP.\n\nThe most ambitious and controversial objective of the WCI was to develop a multi-sector, market-based program to reduce greenhouse gas emissions. Detailed design recommendations for a regional cap-and-trade program to reduce greenhouse gas emissions were released by the WCI in September 2008 and July 2010. By December 2011, California and Quebec adopted regulations based on these recommendations. (The WCI has no regulatory authority of its own.) Key administrative aspects of the regional cap-and-trade program are being implemented in 2012. Power plants, refineries, and other large emitters must comply with the cap in 2013. Other greenhouse gas emission sources, such as suppliers of transportation fuels, must comply with the cap beginning in 2015. Among other things, the Western Climate Initiative lays the foundation for a North American cap-and-trade program, not only in its design and implementation, but in its potential acceptance of greenhouse gas emissions offsets from projects across North America.\n\nSome observers described the entire project as greenwash designed to avoid committing to the Kyoto Protocol, and cited evidence that much more drastic cuts, up to 40%, could be achieved without affecting investment yield in equities, a good indicator that such cuts would not affect economic prospects in the economy as a whole.\n\nSeveral U.S. partners, although active participants in the design of the program, announced in 2010 that they would either delay or not implement the program in their jurisdictions. The partnership was therefore streamlined to include only California and the four Canadian provinces actively working to implement the program. As of January 2012, regulations have not been issued by British Columbia, Manitoba, or Ontario, although a carbon tax in British Columbia will be increasing to $30/tonne of CO equivalents in July 2012. Several WCI partners also remain active in the International Carbon Action Partnership, an international coordinating body for several such regional carbon trading bodies.\n\nAlberta and Saskatchewan object to cap-and-trade and in July 2008 called WCI's plan a \"cash grab by some of Canada's resource-poor provinces.\" However, Alberta has legislated a small restricted carbon charge of its own. The objections seem to be more related to the reporting and disclosure requirements that would be much higher for a North American project than for one based strictly in Alberta. Some of the states that withdrew by late 2011 also intended to develop oil shale, hydraulic fracturing of natural gas and coal resources that would have broad impacts beyond climate on water, including more ocean acidification.\n\nUntil late 2011, the initiative included two types of participants: partners and observers.\n\nFor several years, the partners were the U.S. states of California, Montana, New Mexico, Oregon, Utah, and Washington, and the Canadian provinces of British Columbia, Manitoba, Ontario, and Quebec. All states except California withdrew in 2011. \"See below re membership.\"\n\nThe observers included at various times Alaska, Colorado, Idaho, Kansas, Nevada, Wyoming, the province of Saskatchewan (which objects to WCI plans for a cap and trade system), and the Mexican states of Baja California, Chihuahua, Coahuila, Nuevo Leon, Sonora and Tamaulipas.\n\n\nAs of December 2011, the remaining WCI members are California and the Canadian provinces British Columbia, Manitoba, Ontario, and Quebec.\n\n"}
{"id": "300068", "url": "https://en.wikipedia.org/wiki?curid=300068", "title": "William Dampier", "text": "William Dampier\n\nWilliam Dampier (baptised 5 September 1651; died March 1715) was an English explorer and navigator who became the first Englishman to explore parts of what is today Australia, and the first person to circumnavigate the world three times. He has also been described as Australia's first natural historian, as well as one of the most important British explorers of the period between Sir Walter Raleigh and James Cook.\n\nAfter impressing the Admiralty with his book \"A New Voyage Round the World\", Dampier was given command of a Royal Navy ship and made important discoveries in western Australia, before being court-martialled for cruelty. On a later voyage he rescued Alexander Selkirk, a former crewmate who may have inspired Daniel Defoe's \"Robinson Crusoe\". Others influenced by Dampier include James Cook, Horatio Nelson, Charles Darwin, and Alfred Russel Wallace.\n\nWilliam Dampier was born at Hymerford House in East Coker, Somerset, in 1651. He was baptised on 5 September, but his precise date of birth is not recorded. He was educated at King's School, Bruton. Dampier sailed on two merchant voyages to Newfoundland and Java before joining the Royal Navy in 1673. He took part in the two Battles of Schooneveld in June of that year.\n\nDampier's service was cut short by a catastrophic illness, and he returned to England for several months of recuperation. For the next several years he tried his hand at various careers, including plantation management in Jamaica and logging in Mexico, before he eventually joined another sailing expedition. Returning to England, he married Judith around 1679, only to leave for the sea a few months later.\n\nIn 1679, Dampier joined the crew of the buccaneer Captain Bartholomew Sharp on the Spanish Main of Central America, twice visiting the Bay of Campeche, or \"Campeachy\" as it was then known, on the north coast of Mexico. This led to his first circumnavigation, during which he accompanied a raid across the Isthmus of Darién in Panama and took part in the capture of Spanish ships on the Pacific coast of that isthmus. The pirates then raided Spanish settlements in Peru before returning to the Caribbean.\nDampier made his way to Virginia, where in 1683 he was engaged by the privateer John Cooke. Cooke entered the Pacific via Cape Horn and spent a year raiding Spanish possessions in Peru, the Galápagos Islands, and Mexico. This expedition collected buccaneers and ships as it went along, at one time having a fleet of ten vessels. Cooke died in Mexico, and a new leader, Edward Davis, was elected captain by the crew, taking the ship \"Batchelor's Delight\", with future Captain George Raynor in the crew.\n\nDampier transferred to the privateer Charles Swan's ship, \"Cygnet\", and on 31 March 1686 they set out across the Pacific to raid the East Indies, calling at Guam and Mindanao. Spanish witnesses saw the predominantly English crew as not only pirates and heretics but also cannibals. Leaving Swan and 36 others behind on Mindanao, the rest of the privateers sailed on to Manila, Poulo Condor, China, the Spice Islands, and New Holland. Contrary to Dampier's later claim that he had not actively participated in actual piratical attacks during this voyage, he was in fact selected in 1687 to command one of the Spanish ships captured by \"Cygnet\"s crew off Manila.\n\nOn 5 January 1688, \"Cygnet\" \"anchored two miles from shore in 29 fathoms\" on the northwest coast of Australia, near King Sound. Dampier and his ship remained there until March 12, and while the ship was being careened Dampier made notes on the fauna and flora and the indigenous peoples he found there. Among his fellows were a significant number of Spanish sailors, most notably Alonso Ramírez, a native of San Juan, Puerto Rico. Later that year, by agreement, Dampier and two shipmates were marooned on one of the Nicobar Islands. They obtained a small canoe which they modified after first capsizing and then, after surviving a great storm at sea, called at \"Acheen\" (Aceh) in Sumatra.\n\nDampier returned to England in 1691 via the Cape of Good Hope, penniless but in possession of his journals. He also had as a source of income a slave known as Prince Jeoly (or Giolo), from Miangas (now Indonesia), who became famous for his tattoos (or \"paintings\" as they were known at the time). Dampier exhibited Jeoly in London, thereby also generating publicity for a book based on his diaries.\n\nThe publication of the book, \"A New Voyage Round the World\", in 1697 was a popular sensation, creating interest at the Admiralty. In 1699, Dampier was given command of the 26-gun warship , with a commission from King William III (who had ruled jointly with Queen Mary II until her death in 1694). His mission was to explore the east coast of New Holland, the name given by the Dutch to what is now Australia, and Dampier's intention was to travel there via Cape Horn.\n\nThe expedition set out on 14 January 1699, too late in the season to attempt the Horn, so it headed to New Holland via the Cape of Good Hope instead. Following the Dutch route to the Indies, Dampier passed between Dirk Hartog Island and the Western Australian mainland into what he called Shark Bay on 6 August 1699. He landed and began producing the first known detailed record of Australian flora and fauna. The botanical drawings that were made are believed to be by his clerk, James Brand. Dampier then followed the coast north-east, reaching the Dampier Archipelago and Lagrange Bay, just south of what is now called Roebuck Bay, all the while recording and collecting specimens, including many shells. From there he bore northward for Timor. Then he sailed east and on 3 December 1699 rounded New Guinea, which he passed to the north. He traced the south-eastern coasts of New Hanover, New Ireland and New Britain, charting the Dampier Strait between these islands (now the Bismarck Archipelago) and New Guinea. En route, he paused to collect specimens such as giant clams.\nBy this time, \"Roebuck\" was in such bad condition that Dampier was forced to abandon his plan to examine the east coast of New Holland while less than a hundred miles from it. In danger of sinking, he attempted to make the return voyage to England, but the ship foundered at Ascension Island on 21 February 1701. While anchored offshore the ship began to take on more water and the carpenter could do nothing with the worm-eaten planking. As a result, the vessel had to be run aground. Dampier's crew was marooned there for five weeks before being picked up on 3 April by an East Indiaman and returned home in August 1701.\n\nAlthough many papers were lost with \"Roebuck\", Dampier was able to save some new charts of coastlines, and his record of trade winds and currents in the seas around Australia and New Guinea. He also preserved a few of his specimens. Many plant specimens were donated to the Fielding-Druce Herbarium (part of the University of Oxford), and in September 1999, they were then loaned to Western Australia for the 300 year celebration.\nIn 2001, the \"Roebuck\" wreck was located in Clarence Bay, Ascension Island, by a team from the Western Australian Maritime Museum. Because of his widespread influence, and also because so little exists that can now be linked to him, it has been argued that the remains of his ship and the objects still at the site on Ascension Island – while the property of Britain and subject to the island government's management – are actually the shared maritime heritage of those parts of the world first visited or described by him. His account of the expedition was published as \"A Voyage to New Holland\" in 1703.\n\nOn his return from the \"Roebuck\" expedition, Dampier was court-martialled for cruelty. On the outward voyage, Dampier had his lieutenant, George Fisher, removed from the ship and jailed in Brazil. Fisher returned to England and complained about his treatment to the Admiralty. Dampier aggressively defended his conduct, but he was found guilty. His pay for the voyage was docked, and he was dismissed from the Royal Navy.\n\nAccording to records held at the UK's National Archives, the Royal Navy court martial held on 8 June 1702 involved the following three charges:\n\n\nThe War of the Spanish Succession had broken out in 1701, and English privateers were being readied to act against French and Spanish interests. Dampier was appointed commander of the 26-gun ship \"St George\", with a crew of 120 men. They were joined by the 16-gun \"Cinque Ports\" with 63 men, and sailed on 11 September 1703 from Kinsale, Ireland. The two ships made a storm-tossed passage round Cape Horn, arriving at the Juan Fernández Islands off the coast of Chile in February 1704. While watering and provisioning there, they sighted a heavily armed French merchantman, which they engaged in a seven-hour battle but were driven off.\n\nDampier succeeded in capturing a number of small Spanish ships along the coast of Peru, but released them after removing only a fraction of their cargoes because he believed they \"would be a hindrance to his greater designs.\" The greater design he had in mind was a raid on Santa María, a town on the Gulf of Panama rumoured to hold stockpiles of gold from nearby mines. When the force of seamen he led against the town met with unexpectedly strong resistance, however, he withdrew. In May 1704, \"Cinque Ports\" separated from \"St George\" and, after putting Alexander Selkirk ashore alone on an island for complaining about the vessel's seaworthiness, sank off the coast of what is today Colombia. Some of its crew survived being shipwrecked but were made prisoners of the Spanish.\n\nIt was now left to \"St George\" to make an attempt on the Manila galleon, the main object of the expedition. The ship was sighted on 6 December 1704, probably \"Nuestra Señora del Rosario\". It was caught unprepared and had not run out its guns. But while Dampier and his officers argued over the best way to mount an attack, the galleon got its guns loaded and the battle was joined. \"St George\" soon found itself out-sized by the galleon's 18- and 24-pounders, and, suffering serious damage, they were forced to break off the attack.\n\nThe failure to capture the Spanish galleon completed the break-up of the expedition. Dampier, with about thirty men, stayed in \"St George\", while the rest of the crew took a captured barque across the Pacific to Amboyna in the Dutch settlements. The undermanned and worm-damaged \"St George\" had to be abandoned on the coast of Peru. He and his remaining men embarked in a Spanish prize for the East Indies, where they were thrown into prison as pirates by their supposed allies the Dutch but later released. Now without a ship, Dampier made his way back to England at the end of 1707.\n\nIn 1708, Dampier was engaged to serve on the privateer \"Duke\", not as captain but as sailing master. \"Duke\" beat its way into the South Pacific Ocean round Cape Horn in consort with a second ship, \"Duchess\". Commanded by Woodes Rogers, this voyage was more successful: Selkirk was rescued on 2 February 1709, and the expedition amassed £147,975 (equivalent to £ today) worth of plundered goods. Most of that came from the capture of a Spanish galleon, \"Nuestra Señora de la Encarnación y Desengaño\", along the coast of Mexico in December 1709.\n\nIn January 1710, Dampier crossed the Pacific in \"Duke\", accompanied by \"Duchess\" and two prizes. They stopped at Guam before arriving in Batavia. Following a refit at Horn Island (near Batavia) and the sale of one of their prize ships, they sailed for the Cape of Good Hope where they remained for more than three months awaiting a convoy. They left the Cape in company with 25 Dutch and English ships, with Dampier now serving as sailing master of \"Encarnación\". After a further delay at the Texel, they dropped anchor at the Thames in London on 14 October 1711.\n\nDampier may not have lived to receive all of his share of the expedition's gains. He died in the Parish of St Stephen Coleman Street, London. The exact date and circumstances of his death, and his final resting place, are all unknown. His will was proven on 23 March 1715, and it is generally assumed he died earlier that month, but this is not known with any certainty. His estate was almost £2,000 in debt.\n\nDampier influenced several figures better known than he:\n\nIn his journal, 'A New Voyage Around The World', Dampier depicted the first inhabitants of Australia as virtually subhuman, likening them to monkeys and calling them the \"miserabilist\" people he had ever seen. This may have been at the behest of the publisher who wanted to cater to public prejudice and increase sales. Dampier spent two months in the peninsular, now named after him, observing the people of that area, the Bardi people. They had been there for thousands of years and were actually a well organised society. In his private journals Dampier did seem to have more respect. Nevertheless, it has been claimed that Dampier set the tone for future explorers and colonisers of Australia, which led to the law of terra nullius, the view that the land was unoccupied and there for the taking.\n\nThe following geographical places/features are named after William Dampier:\n\n\n\n"}
{"id": "2168873", "url": "https://en.wikipedia.org/wiki?curid=2168873", "title": "Yorkshire Wildlife Trust", "text": "Yorkshire Wildlife Trust\n\nThe Yorkshire Wildlife Trust is a wildlife trust covering the traditional county of Yorkshire, England. The Trust is part of the UK-wide partnership of 47 Wildlife Trusts.It was formed in 1946, as the Yorkshire Naturalists’ Trust, essentially to preserve its first nature reserve Askham Bog on the outskirts of York. It now manages more than ninety reserves across the county, including some of the best wildlife sites in the UK. These nature reserves cover the diversity of Yorkshire’s landscape, from woodland to grassland, wetland and moorland, fen and bog, and river and coast. One of the flagship reserves is Potteric Carr, a mixed wetland habitat to the south of Doncaster.\n\nThis trust offers a membership card that users can access the below mentioned reserves:\n\nYorkshire Wildlife Trust received over 2,800 objections to the erection of a £900,000 visitors' centre on the Spurn Heritage Coast, East Riding of Yorkshire, with residents of neighbouring Kilnsea citing visual impact and flooding among their concerns. The planning application was successful on its second attempt in January 2017, after amendments to the original proposals, but despite ongoing concerns of locals.\n\n"}
