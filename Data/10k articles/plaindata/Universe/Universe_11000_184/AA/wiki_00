{"id": "395375", "url": "https://en.wikipedia.org/wiki?curid=395375", "title": "Activated carbon", "text": "Activated carbon\n\nActivated carbon, also called activated charcoal, is a form of carbon processed to have small, low-volume pores that increase the surface area available for adsorption or chemical reactions. \"Activated\" is sometimes substituted with \"active\".\n\nDue to its high degree of microporosity, one gram of activated carbon has a surface area in excess of as determined by gas adsorption. An activation level sufficient for useful application may be obtained solely from high surface area. Further chemical treatment often enhances adsorption properties.\n\nActivated carbon is usually derived from charcoal and is sometimes used as biochar. When derived from coal or corn it is referred to as activated coal. Activated coke is derived from coke.\n\nActivated carbon is used in methane and hydrogen storage, air purification, decaffeination, gold purification, metal extraction, water purification, medicine, sewage treatment, air filters in gas masks and respirators, filters in compressed air, teeth whitening, and many other applications.\n\nOne major industrial application involves use of activated carbon in metal finishing for purification of electroplating solutions. For example, it is a main purification technique for removing organic impurities from bright nickel plating solutions. A variety of organic chemicals are added to plating solutions for improving their deposit qualities and for enhancing properties like brightness, smoothness, ductility, etc. Due to passage of direct current and electrolytic reactions of anodic oxidation and cathodic reduction, organic additives generate unwanted breakdown products in solution. Their excessive build up can adversely affect plating quality and physical properties of deposited metal. Activated carbon treatment removes such impurities and restores plating performance to the desired level.\n\nActivated carbon is used to treat poisonings and overdoses following oral ingestion. Tablets or capsules of activated carbon are used in many countries as an over-the-counter drug to treat diarrhea, indigestion, and flatulence.\n\nHowever, it is ineffective for a number of poisonings including strong acids or alkali, cyanide, iron, lithium, arsenic, methanol, ethanol or ethylene glycol.\n\nIncorrect application (e.g. into the lungs) results in pulmonary aspiration, which can sometimes be fatal if immediate medical treatment is not initiated.\n\nActivated carbon, in 50% w/w combination with celite, is used as stationary phase in low-pressure chromatographic separation of carbohydrates (mono-, di-trisaccharides) using ethanol solutions (5–50%) as mobile phase in analytical or preparative protocols.\n\nCarbon adsorption has numerous applications in removing pollutants from air or water streams both in the field and in industrial processes such as:\n\nDuring early implementation of the 1974 Safe Drinking Water Act in the US, EPA officials developed a rule that proposed requiring drinking water treatment systems to use granular activated carbon. Because of its high cost, the so-called GAC rule encountered strong opposition across the country from the water supply industry, including the largest water utilities in California. Hence, the agency set aside the rule. Activated carbon filtration is an effective water treatment method due to its multi-functional nature. There are specific types of activated carbon filtration methods and equipment that are indicated – depending upon the contaminants involved.\n\nActivated carbon is also used for the measurement of radon concentration in air.\n\nActivated carbon (charcoal) is an allowed substance used by organic farmers in both livestock production and wine making. In livestock production it is used as a pesticide, animal feed additive, processing aid, nonagricultural ingredient and disinfectant. In organic winemaking, activated carbon is allowed for use as a processing agent to adsorb brown color pigments from white grape concentrates.\n\nActivated carbon filters (AC filters) can be used to filter vodka and whiskey of organic impurities which can affect color, taste, and odor. Passing an organically impure vodka through an activated carbon filter at the proper flow rate will result in vodka with an identical alcohol content and significantly increased organic purity, as judged by odor and taste.\n\nResearch is being done testing various activated carbons' ability to store natural gas and hydrogen gas. The porous material acts like a sponge for different types of gases. The gas is attracted to the carbon material via Van der Waals forces. Some carbons have been able to achieve bonding energies of 5–10 kJ per mol. The gas may then be desorbed when subjected to higher temperatures and either combusted to do work or in the case of hydrogen gas extracted for use in a hydrogen fuel cell. Gas storage in activated carbons is an appealing gas storage method because the gas can be stored in a low pressure, low mass, low volume environment that would be much more feasible than bulky on-board pressure tanks in vehicles. The United States Department of Energy has specified certain goals to be achieved in the area of research and development of nano-porous carbon materials. All of the goals are yet to be satisfied but numerous institutions, including the ALL-CRAFT program, are continuing to conduct work in this promising field.\n\nFilters with activated carbon are usually used in compressed air and gas purification to remove oil vapors, odor, and other hydrocarbons from the air. The most common designs use a 1-stage or 2 stage filtration principle in which activated carbon is embedded inside the filter media.\n\nActivated carbon filters are used to retain radioactive gases within the air vacuumed from a nuclear boiling water reactor turbine condenser. The large charcoal beds adsorb these gases and retain them while they rapidly decay to non-radioactive solid species. The solids are trapped in the charcoal particles, while the filtered air passes through.\n\nActivated carbon is commonly used on the laboratory scale to purify solutions of organic molecules containing unwanted colored organic impurities.\n\nFiltration over activated carbon is used in large scale fine chemical and pharmaceutical processes for the same purpose. The carbon is either mixed with the solution then filtered off or immobilized in a filter.\n\nActivated carbon, often infused with sulfur or iodine, is widely used to trap mercury emissions from coal-fired power stations, medical incinerators, and from natural gas at the wellhead. This carbon is a special product costing more than US$4.00 per kg.\n\nSince it is often not recycled, the mercury-laden activated carbon presents a disposal dilemma. If the activated carbon contains less than 260 ppm mercury, United States federal regulations allow it to be stabilized (for example, trapped in concrete) for landfilling. However, waste containing greater than 260 ppm is considered to be in the high-mercury subcategory and is banned from landfilling (Land-Ban Rule). This material is now accumulating in warehouses and in deep abandoned mines at an estimated rate of 1000 tons per year.\n\nThe problem of disposal of mercury-laden activated carbon is not unique to the United States. In the Netherlands, this mercury is largely recovered and the activated carbon is disposed of by complete burning.\n\nActivated carbon is carbon produced from carbonaceous source materials such as bamboo, coconut husk, willow peat, wood, coir, lignite, coal, and petroleum pitch. It can be produced by one of the following processes:\n\nActivated carbons are complex products which are difficult to classify on the basis of their behaviour, surface characteristics and other fundamental criteria. However, some broad classification is made for general purpose based on their size, preparation methods, and industrial applications.\n\nNormally, activated carbons (R 1) are made in particulate form as powders or fine granules less than 1.0 mm in size with an average diameter between 0.15 and 0.25 mm. Thus they present a large surface to volume ratio with a small diffusion distance. Activated carbon (R 1) is defined as the activated carbon particles retained on a 50-mesh sieve (0.297 mm).\n\nPAC material is finer material. PAC is made up of crushed or ground carbon particles, 95–100% of which will pass through a designated mesh sieve. The ASTM classifies particles passing through an 80-mesh sieve (0.177 mm) and smaller as PAC. It is not common to use PAC in a dedicated vessel, due to the high head loss that would occur. Instead, PAC is generally added directly to other process units, such as raw water intakes, rapid mix basins, clarifiers, and gravity filters.\n\nGranular activated carbon (GAC) has a relatively larger particle size compared to powdered activated carbon and consequently, presents a smaller external surface. Diffusion of the adsorbate is thus an important factor. These carbons are suitable for adsorption of gases and vapors, because they diffuse rapidly. Granulated carbons are used for water treatment, deodorization and separation of components of flow system and is also used in rapid mix basins. GAC can be either in granular or extruded form. GAC is designated by sizes such as 8×20, 20×40, or 8×30 for liquid phase applications and 4×6, 4×8 or 4×10 for vapor phase applications. A 20×40 carbon is made of particles that will pass through a U.S. Standard Mesh Size No. 20 sieve (0.84 mm) (generally specified as 85% passing) but be retained on a U.S. Standard Mesh Size No. 40 sieve (0.42 mm) (generally specified as 95% retained). AWWA (1992) B604 uses the 50-mesh sieve (0.297 mm) as the minimum GAC size. The most popular aqueous phase carbons are the 12×40 and 8×30 sizes because they have a good balance of size, surface area, and head loss characteristics.\n\nExtruded activated carbon (EAC) combines powdered activated carbon with a binder, which are fused together and extruded into a cylindrical shaped activated carbon block with diameters from 0.8 to 130 mm. These are mainly used for gas phase applications because of their low pressure drop, high mechanical strength and low dust content. Also sold as CTO filter (Chlorine, Taste, Odor).\n\nBead activated carbon (BAC) is made from petroleum pitch and supplied in diameters from approximately 0.35 to 0.80 mm. Similar to EAC, it is also noted for its low pressure drop, high mechanical strength and low dust content, but with a smaller grain size. Its spherical shape makes it preferred for fluidized bed applications such as water filtration.\n\nPorous carbons containing several types of inorganic impregnate such as iodine, silver, cations such as Al, Mn, Zn, Fe, Li, Ca have also been prepared for specific application in air pollution control especially in museums and galleries. Due to its antimicrobial and antiseptic properties, silver loaded activated carbon is used as an adsorbent for purification of domestic water. Drinking water can be obtained from natural water by treating the natural water with a mixture of activated carbon and Al(OH), a flocculating agent. Impregnated carbons are also used for the adsorption of Hydrogen Sulfide(HS) and thiols. Adsorption rates for HS as high as 50% by weight have been reported.\n\nThis is a process by which a porous carbon can be coated with a biocompatible polymer to give a smooth and permeable coat without blocking the pores. The resulting carbon is useful for hemoperfusion. Hemoperfusion is a treatment technique in which large volumes of the patient's blood are passed over an adsorbent substance in order to remove toxic substances from the blood.\n\nThere is a technology of processing technical rayon fiber into activated carbon cloth for carbon filtering. Adsorption capacity of activated cloth is greater than that of activated charcoal (BET theory surface area: 500–1500 m/g, pore volume: 0.3–0.8 cm/g). Thanks to the different forms of activated material, it can be used in a wide range of applications (supercapacitors, odour-absorbers, CBRN defense industry etc.).\n\nA gram of activated carbon can have a surface area in excess of , with being readily achievable. Carbon aerogels, while more expensive, have even higher surface areas, and are used in special applications.\n\nUnder an electron microscope, the high surface-area structures of activated carbon are revealed. Individual particles are intensely convoluted and display various kinds of porosity; there may be many areas where flat surfaces of graphite-like material run parallel to each other, separated by only a few nanometers or so. These micropores provide superb conditions for adsorption to occur, since adsorbing material can interact with many surfaces simultaneously. Tests of adsorption behaviour are usually done with nitrogen gas at 77 K under high vacuum, but in everyday terms activated carbon is perfectly capable of producing the equivalent, by adsorption from its environment, liquid water from steam at and a pressure of 1/10,000 of an atmosphere.\n\nJames Dewar, the scientist after whom the Dewar (vacuum flask) is named, spent much time studying activated carbon and published a paper regarding its adsorption capacity with regard to gases. In this paper, he discovered that cooling the carbon to liquid nitrogen temperatures allowed it to adsorb significant quantities of numerous air gases, among others, that could then be recollected by simply allowing the carbon to warm again and that coconut based carbon was superior for the effect. He uses oxygen as an example, wherein the activated carbon would typically adsorb the atmospheric concentration (21%) under standard conditions, but release over 80% oxygen if the carbon was first cooled to low temperatures.\n\nPhysically, activated carbon binds materials by van der Waals force or London dispersion force.\n\nActivated carbon does not bind well to certain chemicals, including alcohols, diols, strong acids and bases, metals and most inorganics, such as lithium, sodium, iron, lead, arsenic, fluorine, and boric acid.\n\nActivated carbon adsorbs iodine very well. The iodine capacity, mg/g, (ASTM D28 Standard Method test) may be used as an indication of total surface area.\n\nCarbon monoxide is not well adsorbed by activated carbon. This should be of particular concern to those using the material in filters for respirators, fume hoods or other gas control systems as the gas is undetectable to the human senses, toxic to metabolism and neurotoxic.\n\nSubstantial lists of the common industrial and agricultural gases adsorbed by activated carbon can be found online.\n\nActivated carbon can be used as a substrate for the application of various chemicals to improve the adsorptive capacity for some inorganic (and problematic organic) compounds such as hydrogen sulfide (HS), ammonia (NH), formaldehyde (HCOH), mercury (Hg) and radioactive iodine-131(I). This property is known as chemisorption.\n\nMany carbons preferentially adsorb small molecules. Iodine number is the most fundamental parameter used to characterize activated carbon performance.\nIt is a measure of activity level (higher number indicates higher degree of activation,) often reported in mg/g (typical range 500–1200 mg/g).\nIt is a measure of the micropore content of the activated carbon (0 to 20 Å, or up to 2 nm) by adsorption of iodine from solution.\nIt is equivalent to surface area of carbon between 900 and 1100 m/g.\nIt is the standard measure for liquid-phase applications.\n\nIodine number is defined as the milligrams of iodine adsorbed by one gram of carbon when the iodine concentration in the residual filtrate is at a concentration of 0.02 normal (i.e. 0.02N). Basically, iodine number is a measure of the iodine adsorbed in the pores and, as such, is an indication of the pore volume available in the activated carbon of interest. Typically, water-treatment carbons have iodine numbers ranging from 600 to 1100. Frequently, this parameter is used to determine the degree of exhaustion of a carbon in use. However, this practice should be viewed with caution, as chemical interactions with the adsorbate may affect the iodine uptake, giving false results. Thus, the use of iodine number as a measure of the degree of exhaustion of a carbon bed can only be recommended if it has been shown to be free of chemical interactions with adsorbates and if an experimental correlation between iodine number and the degree of exhaustion has been determined for the particular application.\n\nSome carbons are more adept at adsorbing large molecules.\nMolasses number or molasses efficiency is a measure of the mesopore content of the activated carbon (greater than 20 Å, or larger than 2 nm) by adsorption of molasses from solution.\nA high molasses number indicates a high adsorption of big molecules (range 95–600). Caramel dp (decolorizing performance) is similar to molasses number. Molasses efficiency is reported as a percentage (range 40%–185%) and parallels molasses number (600 = 185%, 425 = 85%).\nThe European molasses number (range 525–110) is inversely related to the North American molasses number.\n\nMolasses Number is a measure of the degree of decolorization of a standard molasses solution that has been diluted and standardized against standardized activated carbon. Due to the size of color bodies, the molasses number represents the potential pore volume available for larger adsorbing species. As all of the pore volume may not be available for adsorption in a particular waste water application, and as some of the adsorbate may enter smaller pores, it is not a good measure of the worth of a particular activated carbon for a specific application. Frequently, this parameter is useful in evaluating a series of active carbons for their rates of adsorption. Given two active carbons with similar pore volumes for adsorption, the one having the higher molasses number will usually have larger feeder pores resulting in more efficient transfer of adsorbate into the adsorption space.\n\nTannins are a mixture of large and medium size molecules.\nCarbons with a combination of macropores and mesopores adsorb tannins.\nThe ability of a carbon to adsorb tannins is reported in parts per million concentration (range 200 ppm–362 ppm).\n\nSome carbons have a mesopore (20 Å to 50 Å, or 2 to 5 nm) structure which adsorbs medium size molecules, such as the dye methylene blue.\nMethylene blue adsorption is reported in g/100g (range 11–28 g/100g).\n\nSome carbons are evaluated based on the dechlorination half-life length, which measures the chlorine-removal efficiency of activated carbon. The dechlorination half-value length is the depth of carbon required to reduce the chlorine level of a flowing stream from 5 ppm to 3.5 ppm. A lower half-value length indicates superior performance.\n\nThe solid or skeletal density of activated carbons will typically range between 2000 and 2100 kg/m (125–130 lbs./cubic foot). However, a large part of an activated carbon sample will consist of air space between particles, and the actual or apparent density will therefore be lower, typically 400 to 500 kg/m (25–31 lbs./cubic foot).\n\nHigher density provides greater volume activity and normally indicates better-quality activated carbon.\nASTM D 2854 -09 (2014) is used to determine the apparent density of activated carbon.\n\nIt is a measure of the activated carbon’s resistance to attrition.\nIt is an important indicator of activated carbon to maintain its physical integrity and withstand frictional forces. There are large differences in the hardness of activated carbons, depending on the raw material and activity levels.\n\nAsh reduces the overall activity of activated carbon and reduces the efficiency of reactivation.\nThe metal oxides (FeO) can leach out of activated carbon resulting in discoloration. Acid/water-soluble ash content is more significant than total ash content. Soluble ash content can be very important for aquarists, as ferric oxide can promote algal growths. A carbon with a low soluble ash content should be used for marine, freshwater fish and reef tanks to avoid heavy metal poisoning and excess plant/algal growth.\nStandard method D 2866-2011 is used to determine the ash content of activated carbon.\n\nMeasurement of the porosity of an activated carbon by the adsorption of saturated carbon tetrachloride vapour.\n\nThe finer the particle size of an activated carbon, the better the access to the surface area and the faster the rate of adsorption kinetics. In vapour phase systems this needs to be considered against pressure drop, which will affect energy cost. Careful consideration of particle size distribution can provide significant operating benefits.\nHowever, in the case of using activated carbon for adsorption of minerals such as gold, the particle size should be in the range of . Activated carbon with particle size less than 1 mm would not be suitable for elution (the stripping of mineral from an activated carbon).\n\nAcid-base, oxidation-reduction and specific adsorption characteristics are strongly dependent on the composition of the surface functional groups.\n\nThe surface of conventional activated carbon is reactive, capable of oxidation by atmospheric oxygen and oxygen plasma steam, and also carbon dioxide and ozone.\n\nOxidation in the liquid phase is caused by a wide range of reagents (HNO, HO, KMnO).\n\nThrough the formation of a large number of basic and acidic groups on the surface of oxidized carbon to sorption and other properties can differ significantly from the unmodified forms.\n\nActivated carbon can be nitrogenated by natural products or polymers or processing of carbon with nitrogenating reagents.\n\nActivated carbon can interact with chlorine, bromine and fluorine.\n\nSurface of activated carbon, like other carbon materials can be fluoralkylated by treatment with (per)fluoropolyether peroxide in a liquid phase, or with wide range of fluoroorganic substances by CVD-method. Such materials combine high hydrophobicity and chemical stability with electrical and thermal conductivity and can be used as electrode material for supercapacitors.\n\nSulfonic acid functional groups can be attached to activated carbon to give \"starbons\" which can be used to selectively catalyse the esterification of fatty acids. Formation of such activated carbons from halogenated precursors gives a more effective catalyst which is thought to be a result of remaining halogens improving stability. It is reported about synthesis of activated carbon with chemically grafted superacid sites –CFSOH.\n\nSome of the chemical properties of activated carbon have been attributed to presence of the surface active carbon double bond.\n\nThe Polyani adsorption theory is a popular method for analyzing adsorption of various organic substances to their surface.\n\nThe most commonly encountered form of chemisorption in industry, occurs when a solid catalyst interacts with a gaseous feedstock, the reactant/s. The adsorption of reactant/s to the catalyst surface creates a chemical bond, altering the electron density around the reactant molecule and allowing it to undergo reactions that would not normally be available to it.\n\nThe reactivation or the regeneration of activated carbons involves restoring the adsorptive capacity of saturated activated carbon by desorbing adsorbed contaminants on the activated carbon surface.\n\nThe most common regeneration technique employed in industrial processes is thermal reactivation. The thermal regeneration process generally follows three steps:\n\nThe heat treatment stage utilises the exothermic nature of adsorption and results in desorption, partial cracking and polymerization of the adsorbed organics. The final step aims to remove charred organic residue formed in the porous structure in the previous stage and re-expose the porous carbon structure regenerating its original surface characteristics. After treatment the adsorption column can be reused. Per adsorption-thermal regeneration cycle between 5–15 wt% of the carbon bed is burnt off resulting in a loss of adsorptive capacity. Thermal regeneration is a high energy process due to the high required temperatures making it both an energetically and commercially expensive process. Plants that rely on thermal regeneration of activated carbon have to be of a certain size before it is economically viable to have regeneration facilities onsite. As a result, it is common for smaller waste treatment sites to ship their activated carbon cores to specialised facilities for regeneration.\n\nCurrent concerns with the high energy/cost nature of thermal regeneration of activated carbon has encouraged research into alternative regeneration methods to reduce the environmental impact of such processes. Though several of the regeneration techniques cited have remained areas of purely academic research, some alternatives to thermal regeneration systems have been employed in industry. Current alternative regeneration methods are:\n\n\n"}
{"id": "13321717", "url": "https://en.wikipedia.org/wiki?curid=13321717", "title": "Active tip-clearance control", "text": "Active tip-clearance control\n\nActive clearance control (ACC) is a method used in gas turbines to improve fuel efficiency. This is achieved by dynamically controlling the turbine tip clearance. \n\nDuring normal cruise flight, the engine is exposed to many loads such as intense heat and centrifugal force. This causes expansion of certain components and alters the gap between the turbine casing and tips of the spinning turbine blades. The amount of air leaking around, past the edge of the blades without passing through them is critical to engine performance and fuel efficiency. For this reason since the late 1960s, blade tip sealing has taken on a prominent role in aircraft engine design. The ACC system dynamically controls the high pressure turbine (HPT) blade clearance. This can be achieved in numerous ways.\n\nHPT (high pressure turbine) blade tip clearance has a significant impact on fuel burn and emissions. \n\nBlade tip sealing has been a challenging problem since the development of the gas turbine engine. It is such because the clearance between the blade tips and surrounding casing (shroud) tends to vary due primarily to changes in thermal and mechanical loads on the rotating (turbine wheel) and stationary (stator, turbine casing) structures. \n\nUsing ACC gives significant benefits in cruise fuel burn, range, and payload capability for long range aircraft.\n\nOne common active clearance control consists of the ACC valve which mixes hot and cold air from the compressor exit and the bypass duct, respectively, to a desired temperature. The air is routed to flow through tubes surrounding the casing at each turbine stage. This air expands or contracts the turbine case and in doing so, it maintains the accurate clearance between the turbine case and the blade tip. This clearance should be maintained accurately which is essential for the engine efficiency and its performance. The ACC valve opening is adjusted automatically by the FADEC system depending on the thrust lever position.\n\nHPT clearance control systems can be categorized as passive and active, active being controlled via a hydro-mechanical control or via FADEC (full authority digital engine control). The systems can be further classified as either thermal or mechanical.\n\n\n"}
{"id": "2364430", "url": "https://en.wikipedia.org/wiki?curid=2364430", "title": "Air France Flight 358", "text": "Air France Flight 358\n\nAir France Flight 358 was a regularly scheduled international flight from Charles de Gaulle Airport in Paris, France, to Toronto Pearson International Airport in Ontario, Canada. On the afternoon of 2 August 2005, while landing at Pearson Airport, the Airbus A340-313E operating the route crashed into nearby Etobicoke Creek, approximately beyond the end of the runway. All 309 passengers and crew on board the Airbus survived, but twelve people sustained serious injuries. The accident highlighted the vital role played by highly trained flight attendants during an emergency.\n\nDue to inclement weather, 540 flights departing and arriving at Pearson were cancelled. Many small and mid-sized aircraft due to arrive were diverted to other Canadian airports in Ottawa, London, Hamilton, and Winnipeg. Most of the larger aircraft were diverted to Montreal, Syracuse, New York, and Buffalo, New York. Flights from Vancouver were turned back. The crash of Air France Flight 358 was the biggest crisis to hit Toronto Pearson since the airport's involvement in Operation Yellow Ribbon.\n\nJean Lapierre, the Canadian Minister of Transport, referred to Flight 358 as a \"miracle\" because all of the passengers survived. Other press sources described the accident as the \"Miracle in Toronto\", the \"Toronto Miracle\", the \" 'Miracle' Escape\", and the \"Miracle of Runway 24L\".\n\nThe accident was investigated by the Transportation Safety Board of Canada (TSB), with a final report issued on 13 December 2007. The unfavorable weather conditions, and the poor landing decisions made by the flight crew, were found to be major factors leading to the crash. This was the first time that an Airbus A340 series had been involved in an accident, ending its 14-year clean record.\n\nThe aircraft operating Flight 358 was an Airbus A340-313E, with Manufacturer's Serial Number (MSN) 289 and registration F-GLZQ; it was powered by four CFM International CFM56 engines. The aircraft made its first flight on 3 August 1999, and was delivered to Air France on 7 September 1999. It had made 3,711 flights for a total of 28,426 flight hours. Its last maintenance check was carried out in France on 5 July 2005.\n\nThere were twelve crew members on board the Airbus. The pilot on the flight was 57-year-old Captain Alain Rosaye, a seasoned pilot with 15,411 total flight hours. The co-pilot was 43-year-old Frédéric Naud, who had accrued 4,834 hours of flight time.\n\nOf the 297 passengers on board the Airbus, there were 168 adult males, 118 adult females, eight children and three infants. There was a mix of different nationalities, including 104 Canadian citizens, 101 French, 19 Italian, 14 American, 8 Indian, and 7 British. The passengers consisted of businesspersons, vacationers and students. Three of the passengers were seated in crew seats, one in the third occupant seat of the flight deck and two in the flight crew rest area.\n\nAt 16:02 EDT (20:02 UTC) on 2 August 2005, Air France Flight 358 overshot the end of the runway after landing at Toronto Pearson International Airport, and came to rest in a small ravine just outside the airport perimeter. All 297 passengers and twelve crew members successfully evacuated the aircraft. Twelve major injuries resulted from the accident and there were no fatalities; the other occupants suffered minor or no injuries. The aircraft was destroyed in a post-crash fire.\n\nThe flight landed during exceptionally poor weather—severe winds, heavy rain, and localized thunderstorms near the airport (see Weather conditions below)—and touched down farther along the runway than usual. Some passengers reported that the plane was rocking from side to side before landing, possibly due to turbulence and gusting winds associated with the storm systems. One passenger described the crash as like a \"car accident, but it keeps going and going, non-stop.\"\n\nThe plane had been cleared to land at 16:01 EDT on Runway 24L, which, at in length, is the shortest runway at Pearson Airport. After touchdown, the aircraft did not stop before the end of the runway, but continued on for another until it slid into the Etobicoke Creek ravine at a speed of , on the western edge of the airport near the interchange of Dixie Road and Highway 401.\n\nAfter the aircraft had stopped, the crew saw fire outside and began evacuation. When the emergency exits were opened, one of the right middle exit slides (R3) deflated after being punctured by debris from the aircraft, while one of the left slides (L2) failed to deploy at all for unknown reasons. The two rear left exits remained closed due to the fire. A number of passengers were forced to jump from the aircraft to escape. The actions of the flight attendants, who ensured that all of the passengers were able to exit the plane quickly, contributed to the safe evacuation of everyone on board. The first officer was the last person to leave the plane, which was evacuated within the required 90-second time frame.\n\nEmergency response teams arrived on site within 52 seconds of the accident occurring. The TSB official report states that \"the first response vehicle arrived at the scene within one minute of the crash alarm sounding\".\n\nAfter the accident, some of the passengers, including those who were injured, scrambled up the ravine onto Highway 401 which runs almost parallel to the runway. Peel Regional Police located the first officer and several passengers along the highway, receiving assistance from motorists who had been passing the airport at the time of the crash. Some of the injured passengers and the co-pilot were taken directly to hospitals by motorists, and the uninjured passengers were transported by motorists to the airport. The main fire continued to burn for two hours, dying out just before 18:00 EDT. All of the fires were extinguished by the early afternoon of the following day, when investigators were able to begin their work.\n\nThe accident led to the cancellation or diversion of hundreds of flights, with ripple effects throughout the North American air traffic system. Four of the five runway surfaces at Pearson Airport were back in service by the night of 2 August, but the flight and passenger backlog continued through the next day.\n\nThe accident also caused heavy traffic congestion throughout Toronto's highway system. Highway 401 is one of the world's busiest highways, and is the main route through the Greater Toronto Area; the crash occurred near the highway's widest point where eighteen lanes of traffic are directed toward major intersections with Highway 403 and Highway 410 to the southwest and Highway 427 to the northeast. Although the fire was extinguished within hours, there was considerable congestion on the highway for days after the accident due to motorists slowing down or pulling over to view the wreckage. This created numerous traffic collisions, prompting the Ontario Provincial Police to increase patrols along that section of the highway.\n\nA METAR (weather observation) for Pearson was released almost exactly at the time of the accident. It stated that the weather at 20:01 UTC (16:01 EDT) consisted of winds from 340° true (north-northwest) at gusting to , with visibility in thunderstorms and heavy rain. The ceiling was overcast at above ground level with towering cumulus clouds. The temperature was . According to the Canada Air Pilot, runway 24L has a heading of 227° true (237° magnetic), and the minima for the ILS approach are ceiling above ground level and visibility or runway visual range (RVR) of . The METAR for 21:00 UTC (17:00 EDT), nearly an hour after the accident, shows wind backing to the south and improving conditions generally, while noting smoke aloft from the burning plane.\n\nThe Canadian Broadcasting Corporation reported that the accident happened two hours after a ground stop was declared at the airport because of severe thunderstorms in the area (\"red alert\" status, which, for safety reasons, halts all ground activity on the apron and gate area. Aircraft can still land, and take off if still in queue). Visibility at the time of the accident was reported to be very poor. There was lightning, strong gusty winds, and hail at the time and the rain just began as the plane was landing. Within two hours the winds increased from 5 to 30 km/h (3 to 20 mph) and the temperature dropped from . A severe thunderstorm warning was in effect since 11:30 a.m. and all outbound flights and ground servicing operations had been canceled but landings were still permitted.\n\nThe table below summarizes the injuries as reported by the TSB.\n\nMost of the injuries occurred to passengers and crew located in the flight deck and forward cabin. Of the twelve occupants who sustained major injuries, nine suffered the injuries from the impact and three from the evacuation. According to passenger reports, the leap from the aircraft to the ground caused numerous injuries, including broken legs, and ruptured vertebrae. The captain sustained back and head injuries during the impact of the crash when his seat was wrenched out of place by the force of the impact, causing him to hit his head against the overhead controls. Minor injuries included twisted ankles, sore necks, bruises and effects from smoke inhalation.\n\nA total of 33 persons were taken to various hospitals within and outside Toronto for treatment, of which 21 were treated for minor injuries and released. The York-Finch campus of the Humber River Regional Hospital treated seven people for smoke inhalation. William Osler Health Centre, Etobicoke General Hospital, Credit Valley Hospital, and Peel Memorial Hospital were additional nearby hospitals that had admitted victims of the crash.\n\nIn addition to the Greater Toronto Airport Authority, on-site emergency services were also provided by Peel Regional Paramedic Services, Peel Regional Police, Mississauga Fire and Emergency Services, Toronto EMS, and the Royal Canadian Mounted Police. Ontario Provincial Police patrolled Highway 401. The Toronto Transit Commission provided two of its transit buses to act as shelter for victims.\n\nOnce the emergency response teams had finished their work, the Transportation Safety Board of Canada (TSB) took control of the accident site and led the investigation, with the cooperation of several other organizations in accordance with the provisions of ICAO Annex 13:\n\n\nThe flight data recorder and cockpit voice recorder were sent to France for analysis. Preliminary results indicated that the plane landed from the start of the runway (much further along than normal) at a ground speed of – 140 knots being considered normal – with a tailwind, skidded down the runway and was traveling over as it overran the tarmac and fell into the ravine. Tire marks extended indicating emergency braking action.\n\nRéal Levasseur Shedalin, the TSB's lead investigator for the accident, said the plane landed too far down the runway to have been able to stop properly on such wet pavement. Investigators have found no evidence of engine trouble, brake failure, or problems with the spoilers or thrust reversers. Why evacuation chutes failed to deploy from two exits remains under study. Some fleeing passengers were forced to jump some to the ground.\n\nOne passenger took four photographs of the evacuation with his camera, which were released to the media. The final TSB report refers to the photographs and draws conclusions about the nature of the disaster based on them. Mark Rosenker, the acting chairman of the National Transportation Safety Board (NTSB), criticized the concept of passengers taking photographs of disasters, stating, \"Your business is to get off the airplane. Your business is to help anybody who needs help.\" According to Rosenker, taking photographs during an evacuation of an airliner is irresponsible. Helen Muir, an aerospace psychology professor at Cranfield University in the United Kingdom, stated that pausing during evacuations \"is just what we don't want people to do.\" However, Muir acknowledged that photographs are \"very valuable to accident investigators\".\n\nThe final TSB report states: \"During the flare, the aircraft entered a heavy shower area, and the crew's forward visibility was significantly reduced as they entered the downpour.\" This suggests the possibility that the plane was hit in heavy weather by a wet downburst, causing the Airbus to land long. Based on the Air France A340-313 Quick Reference Handbook (QRH), page 34G, \"Landing Distance Without Autobrake\", the minimum distance of would be used in dry conditions to bring the aircraft to a complete stop. In wet conditions the braking distance increases with a 5-knot tailwind, reversers operative, and a of downpour on the runway to . There was not enough remaining runway available at the touch down point of AF 358.\n\nOther possible irregularities mentioned in a government report on the accident:\nThe TSB concluded in its final report that the pilots had missed cues that would have prompted them to review their decision to land, and also that:\n\nThe TSB advised changes to bring Canadian runway standards in line with those used abroad, either by extending them to have a 300 m runway end safety area (RESA) or, where that is not possible, providing an equivalently effective backup method of stopping aircraft. Other recommendations made by the TSB included having the Canadian Department of Transport establish clear standards limiting approaches and landings in convective weather for all operators at Canadian airports, and mandate training for all pilots involved in Canadian air operations to better enable them to make landing decisions in bad weather.\n\nWithin one week of the accident, cash payments ranging from $1,000 to $3,700 (all figures in this article in Canadian dollars unless otherwise stated) were given to passengers for interim emergency use. These funds were given to passengers through an emergency centre set up in the Novotel Hotel in Mississauga, near the airport. These payments were independent of the claims process, which has been started for passengers who have not retained counsel. It is expected that the insurers of Air France will pay for all damages as well as extra compensation for having passengers go through the ordeal; however, only amounts of €6,000 to €9,000 have been offered, prompting passengers to turn to the lawsuit to seek legal action. The insurance is handled by the \"Societé de Gestion & D'Expertises D'Assurances\" in France. All passengers have also been offered a free round-trip ticket to any Air France destination in the world in the same fare class in which they were originally booked on AF358.\n\nAfter a lawsuit lasting four and a half years, Air France settled the compensation lawsuit with 184 of the 297 passengers (no crew members included) aboard Flight 358. The compensation is for a total of $12 million. Air France will pay $10 million, and have been released from passengers' claims stemming from the accident, according to the judgment's summary. Airbus and Goodrich, the company that made the emergency evacuation system on the plane will pay $1.65 million, and claims against them in a lawsuit have been released.\n\nJ.J. Camp, a Vancouver lawyer representing claimants, stated that passengers seriously harmed with either physical or psychological injuries were eligible for the maximum payout of $175,000. Passengers who were not seriously harmed in the accident would receive the minimum payment of between $5,000 and $10,000.\n\nWithin a few days of the accident, a class action suit was filed on behalf of all passengers on board by representative plaintiff Suzanne Deak to the Ontario Superior Court of Justice. The attorneys representing Deak and the passengers were Gary R. Will and Paul Miller from Will Barristers in Toronto. The plaintiffs sought payments for general and aggravated damages in the amount of $75 million, and payments for special damages and pecuniary damages in the amount of $250 million. A second class action lawsuit was also filed by plaintiffs Sahar Alqudsi and Younis Qawasmi (her husband) for $150 million a few days later. However, both suits were merged because only one lawsuit was allowed to proceed to court.\n\nIn December 2009, a $12 million settlement agreement was reached between Air France and the class. The settlement resolved the claims of 184 passengers and their families; 45 other passengers had opted out of the suit, while 68 others had already agreed to a settlement with Air France.\n\nAir France stated that it would not lose any money from the lawsuits as it is covered by its insurers. Air France did not provide further contacts and assistance to those who retained counsel of the lawsuit until an agreement has been made between both sides' lawyers.\n\nIn June 2008, almost 3 years after the accident, Air France filed a lawsuit against the Greater Toronto Airports Authority, NAV Canada, and the Government of Canada for $180 million. In the statement of claim filed with the Ontario Superior Court of Justice, Air France alleged that the \"GTAA failed to provide a safe environment for the conduct of civil air operations.\" The statement also claims that \"The overrun and the consequent injuries to persons and damage to property were caused solely by the negligence of the defendants\". Air France says Transport Canada was \"negligent\" by not implementing the recommendations of a coroner's inquest into the 1978 crash that urged the creation of a 300-metre safety area to give aircraft more room to stop after landing.\n\nAn inquiry by the TSB found runway safety zones at the end of runways at some Canadian airports to be below accepted international standards. However, the report highlighted that Toronto Pearson's runways meet current Canadian standards, and that runway 24L has a de facto 150-metre RESA. The TSB also suggested that precautions should be taken by airlines when landing in bad weather.\n\nIn 1978, Air Canada Flight 189 also crashed into Etobicoke Creek, the site of the AF358 crash, resulting in two deaths. The Air Canada DC-9 had landed on the 24R-06L runway, crashing north of the AF358 accident scene and deeper into the ravine. After the Air France crash in 2005, there were calls for the ravine to be filled or spanned by a bridge, but others argued that such an undertaking would have been prohibitively expensive. (Note: The runway on which the Air France plane landed in August 2005, 24L-06R, is an east–west runway with a length of . This runway did not yet exist at the time of the Air Canada crash in 1978. At that time, the current runway 24R-06L was numbered 24L-06R, and the current runway 23-05 was numbered 24R-06L.)\n\nFlight 358 is no longer used on this route (number is now used for Air France flights from Roland Garros Airport in Sainte-Marie, Réunion to Paris). The flight route designation for Air France's Paris-Toronto route is now Flight 356, using a Boeing 777 aircraft.\n\nThe Discovery Channel Canada / National Geographic TV series \"Mayday\" (also called \"Air Crash Investigation\" or \"Air Emergency\") featured the accident in a 2006 episode titled \"Desperate Escape\" which included interviews with survivors and a dramatization of the accident.\n\nThis accident is also featured on The Weather Channel television program \"Storm Stories\".\n\n"}
{"id": "25532479", "url": "https://en.wikipedia.org/wiki?curid=25532479", "title": "American Airlines Flight 331", "text": "American Airlines Flight 331\n\nOn 22 December 2009, an American Airlines Boeing 737-800, operating American Airlines Flight 331 (Washington, D.C.–Miami–Kingston, Jamaica) and carrying 148 passengers and six crew, overran the runway on landing at Kingston in poor weather. The plane continued on the ground outside the airport perimeter and broke apart on the beach, causing injuries.\n\nFactors contributing to the crash include the speed of the aircraft upon landing and the plane touching down more than 4,000 feet from the start of the runway. Contributing factors included American Airlines' failure to provide training on tailwind landings, and the FAA's failure to implement the NTSB's previous recommendation, following a previous fatal accident involving a tailwind landing attempt, that the FAA require commercial operators to train flight crews on tailwind landings.\n\nThe aircraft involved was a Boeing 737-823, registration N977AN. The aircraft had manufacturer's serial number 29550 and made its first flight on 30 November 2001. The aircraft made its first flights under registration N1786B and was delivered to American Airlines on 20 December 2001.\n\nThe flight originated at Ronald Reagan Washington National Airport, Washington, D.C., with a stopover at Miami International Airport, Miami, Florida. At 22:22 local time (03:22, 23 December UTC), the Boeing 737-823 skidded during landing on runway 12 and overran the pavement, sustaining serious damage. Heavy rain was reported at the time. After the accident, a special weather report was issued.\n\nSome passengers indicated cabin service was suspended several times during the flight, before being cancelled outright due to turbulence; others report the jet may have landed long on the runway.\n\nIt was also announced that some of the airport's approach lights were not working at the time of the accident. Jamaican officials downplayed the role of the malfunctioning lights in the crash, noting that aircrews had been notified and that the actual runway was properly lit. The ground-based navigation aids were evaluated by a check aircraft after the accident and were determined to be functioning normally.\n\nThe aircraft sustained substantial damage during the accident, with the entire fuselage fracturing forward and aft of the wing, one wing losing an engine and the other its winglet tip, and the nose section being crushed. The landing gear failed and put the aircraft on its belly. Its momentum carried it through the perimeter fence at freeway speeds, and across Norman Manley Highway before finally coming to rest upright, within meters of Kingston's outer harbor and the open Caribbean Sea. The 737 was damaged beyond economic repair.\n\nAlthough the airport was closed after the accident, delaying about 400 travellers, it later re-opened with a reduced runway length available due to the tail section wreckage. Larger flights were diverted to Montego Bay's Sangster International Airport for two days.\n\nAn investigation into the accident was launched by the National Transportation Safety Board. They sent a team to assist the Jamaica Civil Aviation Authority officials in the investigation. American Airlines also sent a crash team to assist the other investigators.\n\nLater reports showed the crew had contacted Jamaica Air Traffic Control to request the Instrument Landing System approach for Runway 12, the designated runway broadcast by the Automatic Terminal Information Service for arrivals that night.\nThey were, however, advised of tailwind conditions on Runway 12 and offered a circling approach for landing on Runway 30.\n\"The crew repeated their request for Runway 12 and were subsequently cleared to land on that runway with the controller further advising the crew that the runway was wet.\"\n\nJamaican Director General of Civil Aviation Col. Oscar Derby, stated in the week following the accident, that the jet touched down about halfway down the runway. He also noted that the 737-800 is equipped with a head-up display. Other factors that were under investigation included \"tailwinds, and a rain soaked runway;\" the runway in question was not equipped with rain-dispersing grooves common at larger airports. The aircraft held a relatively heavy fuel load at the time of landing; it was carrying enough fuel for a roundtrip flight back to the US.\n\nThe FDR later revealed that the aircraft touched down some down the long runway. Normally touchdown would be between and . The aircraft was still travelling at when it departed the end of the runway. The aircraft landed with a tailwind, just within its limit of .\n\nAfter the crash it was announced American Airlines was involved in an FAA review of company landing procedures following three landing incidents in two weeks; in the other two instances plane wingtips touched the ground during landing. During the NTSB's investigation, the flight crew informed the NTSB in post-accident interviews that they had not received any training on conducting landings in tailwind conditions. In addition, the NTSB was told by other American Airlines pilots that they were not given simulator training on tailwind landings or given guidance about runway overrun risks associated with tailwind landings.\n\nAccording to the U.S. State Department, 76 of the passengers on board were Americans.\n\nAlthough 92 people were taken to the hospital, there were no life-threatening injuries reported.\n\nReports from Jamaica indicate that as of December 28, 2009, most passenger and crew property was yet to be returned due to the investigation; American Airlines provided each passenger $5,000 to compensate for the lengthy quarantine of baggage.\n\nOn December 7, 2011, the NTSB issued a safety recommendation based on the results of its investigation into the crash of Flight 331. The NTSB recommended that the FAA take actions to ensure adequate pilot training in simulator training programs on tailwind approaches and landings, particularly on wet or contaminated runways, and revise its advisories on runway overrun prevention to include a discussion of risks associated with tailwind landings.\n\nThe NTSB also restated its previous recommendation, made following the crash of Southwest Airlines Flight 1248, that the FAA require commercial airline pilots to perform arrival landing distance assessments which include a conservative safety margin before every landing. The NTSB noted that while the FAA had proposed such a rule, operators were still not required to comply and many operators, including American Airlines, were not at the time of the Flight 331 crash. As a result, the NTSB's safety recommendation was reiterated and reclassified as \"Open—Unacceptable Response.\"\n\nOn May 2, 2014, the JCAA issued its final report. The final investigation report identified multiple causes and contributing factors to the accident, which included:\n\n\nLike the NTSB, the JCAA also recommended that flight crews be required to perform arrival landing distance assessments which include a conservative safety margin before every landing, and that actions should be taken to require appropriate flight crew guidance and training regarding tailwind landings.\n\nThe pilots' failure to abort the landing and climb to go around has been compared to the later fatal crash of Asiana Airlines Flight 214. In the Asiana Airlines incident, the pilot failed to abort the landing and initiate a \"go-around\" until it was too late to prevent the crash.\n\n\n"}
{"id": "44059332", "url": "https://en.wikipedia.org/wiki?curid=44059332", "title": "Antiperovskite (structure)", "text": "Antiperovskite (structure)\n\nAntiperovskites (or inverse perovskites) is a type of crystal structure similar to the perovskite structure that is common in nature. The key difference is that the positions of the cation and anion constituents are reversed in the unit cell structure. In contrast to perovskite, antiperovskite compounds consist of two types of anions coordinated with one type of cation. Antiperovskite compounds are an important class of materials because they exhibit interesting and useful physical properties not found in perovskite materials.\n\nThe crystal lattice of an antiperovskite structure is the same as that of the perovskite structure, but the anion and cation positions are switched. The typical perovskite structure is represented by the general formula ABX, where A and B are cations and X is an anion. When the anion is the (divalent) oxide ion, A and B cations can have charges 1 and 5, resepectively, 2 and 4, respectively, or 3 and 3, respectively.\n\nIn antiperovskite compounds, the general formula is reversed, so that the X sites are occupied by an electropositive ion, i.e., cation (such as an alkali metal), while A and B sites are occupied by different types of anion. In the ideal cubic cell, the A anion is at the corners of the cube, the B anion at the octahedral center, and the X cation is at the faces of the cube. Thus the A anion has a coordination number of 12, while the B anion sits at the center of an octahedron with a coordination number of 6.\n\nSimilar to the perovskite structure, most antiperovskite compounds are known to deviate from the ideal cubic structure, forming orthorhombic or tetragonal phases depending on temperature and pressure.\n\nWhether a compound will form an antiperovskite structure depends not only on its chemical formula, but also the relative sizes of the ionic radii of the constituent atoms. This constraint is expressed in terms of the Goldschmidt tolerance factor, which is determined by the radii, r, r and r, of the A, B, and X ions.\n\nFor the antiperovskite structure to be structurally stable, the tolerance factor must be between 0.71 and 1. If between 0.71 and 0.9, the crystal will be orthorhombic or tetragonal. If between 0.9 and 1, it will be cubic. By mixing the B anions with another element of the same valence but different size, the tolerance factor can be altered. Different combinations of elements result in different compounds with different regions of thermodynamic stability for a given crystal symmetry.\n\nAntiperovskites naturally occur in sulphohalite, galeite, schairerite, kogarkoite, nacaphite, arctite, polyphite, and hatrurite. It is also demonstrated in superconductive compounds such as CuNNi and ZnNNi.\n\nMan-made antiperovskites exhibit interesting properties. The physical properties of antiperovskite compounds can be manipulated by altering the stoichiometry, element substitution, and synthesis conditions.\n\nRecently synthesized antiperovskites with chemical formula LiOBr and LiOCl have demonstrated high lithium-ion conductivity. Known as LiRAPs, these are being investigated for use in solid-state batteries and fuel cells. In addition, other alkali-rich antiperovskites such as NaOCl are also being investigated for their superionic conductivity.\n\nDiscovered in 1930, these crystals have the formula MAB where M represents a magnetic element, Mn, Ni, or Fe; A represents a transition or main group element, Ga, Cu, Sn, and Zn; and B represents N, C, or B. These materials exhibit superconductivity, giant magnetoresistance, and other unusual properties.\n\nAntiperovskite manganese nitrides have been shown to exhibit zero thermal expansion.\n\n"}
{"id": "72842", "url": "https://en.wikipedia.org/wiki?curid=72842", "title": "Antofagasta Region", "text": "Antofagasta Region\n\nThe Antofagasta Region (, ) is one of Chile's sixteen first-order administrative divisions. It comprises three provinces, Antofagasta, El Loa and Tocopilla. It is bordered to the north by Tarapacá and by Atacama to the south and is the second-largest region of Chile. To the east it borders Bolivia and Argentina. The capital of the region is the port city of Antofagasta, another important city being Calama. The main economic activity is copper mining in the giant porphyry copper systems located inland.\n\nClimate is extremely arid albeit somewhat milder near the coast. Nearly all of the region is devoid of vegetation except close to the Loa River or at oasises such as San Pedro de Atacama. Much of the inland is covered by salt flats, tephra and lava flows. The coast exhibits prominent cliffs.\n\nThe region was sparsely populated by indigenous Changos and Atacameños until massive Chilean immigration in conjunction with a saltpeter boom in the later 19th century. The region used to be Bolivian until the War of the Pacific broke out in 1879.\n\nAntofagasta's history is divided, as the territory, in two sections, the coastal region and the highlands plateau or altiplano around the Andes. In pre-Columbian times, the coastline was populated by nomadic fishing clans of Changos Indians, of which very little is known, due to very limited contact with the Spanish conquerors.\n\nThe inland section was populated by the Atacaman culture around the great dry salt lake called Salar de Atacama, the Loa River basin and valleys and oasis across the altiplano, with the most important settlement being the village of San Pedro de Atacama.\n\nThe Atacaman culture was deeply influenced by Tiwanaku culture and later fell under Inca rule. The Atacamans harvested mainly corn and beans and developed trade as far as the Amazon basin and Pacific shores. The arrival of the Spaniards in the 16th century did not destroy the culture but transformed it deeply through the process of mestizaje, in which both cultures mixed. Under Spanish rule, Atacaman territory was placed under the administration of the Audiencia Real de Charcas, though it is disputed whether the Audiencia Real de Charcas was to administer only the inland portion or the coast as well. At the time of independence general Simón Bolívar integrated it into the new Republic of Bolivia, under the name of \"Litoral Department\". This decision was disputed by the Chilean Government and has been a source of conflict until present times. Chile claimed that according to the Uti possidetis of the Spanish crown, the coastal region belong to them and their territory bordered directly with Peru.\n\nChileans explorers such as Juan López and José Santos Ossa discovered rich nitrate and guano deposits which produced a massive Chilean colonization of the coastline. Friction between the new settlers from both countries grew until 1879 when the War of the Pacific erupted. Antofagasta was permanently annexed by the Chilean government at the end of the war.\n\nColonization by Chileans followed mainly from the \"Little North\" (the contemporary regions of Atacama and Coquimbo, also known as the III and IV regions), into the new territories of Antofagasta and Tarapacá, nicknamed the \"Norte Grande\" or \"the Great North\".\n\nIn the early 20th century the region became a significant base of Chile's union-organizing movements. It continued to depend economically on the nitrate-extraction industry until its replacement by copper mining. Two of the largest and richest open pit mines in the world are located in Antofagasta: La Escondida and Chuquicamata.\n\nEach province in the region is further subdivided into communes.\n\nThe main river is the Loa.\n\nThe average rainfall in the Antofagasta is just per year. From the coast, east to the Chilean Coast Range, is the south-central part of the Atacama Desert, the driest desert in the world. Further to the east, it is part of the less arid Central Andean dry puna ecoregion. The surroundings of abandoned Yungay town have been named the driest place in the world.\n\nMost of the population lives on the coast in Antofagasta and Mejillones, or in Calama, Chile in the interior, the hub of the mining industry and the home of a large part of its work force.\n\nIn the late 19th and early 20th centuries, immigrant settlers also arrived from Europe (mainly Croatians, Italians, Spaniards, Greeks, English, French, and Portuguese peoples), from Arab countries such as Lebanon and Syria, and plus smaller numbers from China, Japan, Korea, Peru and Bolivia. Various immigration flows joined with the culture of the altiplano region creating the modern culture of the north of Chile, which arguably presents more Andean- and multi-European-features than the Central Valley and mainstream Chilean culture. \n\nThe Antofagasta Region is the heart of the mining industry, Chile's main source of export revenue. It represents 53% of Chile's mining output, led by copper and followed by potassium nitrate, gold, iodine, and lithium. The mining industry accounts for 93.7% of the region's exports.\n\nFishing and manufacturing also contribute to the income of the area.\n\nThe availability of infrastructure and services, due to the region's mining boom, together with its abundance of beautiful natural scenery, have opened vast prospects for the travel industry, both in the interior and on the coast. Interesting tourist attractions include the small town of San Pedro de Atacama, once the center of the Atacameño culture, Atacama Salt Flat, the Valley of the Moon, the Quitor Pukará, the Puritama hot springs and the numerous astronomical observatories including the Very Large Telescope and ALMA.\n\n\n"}
{"id": "3795104", "url": "https://en.wikipedia.org/wiki?curid=3795104", "title": "Ballistic foam", "text": "Ballistic foam\n\nBallistic foam is a foam that sets hard. It is widely used in the manufacture and repair of aircraft to form a light but strong filler for aircraft wings. The foam is used to surround aircraft fuel tanks to reduce the chance of fires caused by the penetration of incendiary projectiles.\n\nBallistic foam is a type of polyurethane foam placed in the dry bays of aircraft. Ballistic foam prevents fires, adds strength to the structure, slows down the speed of shrapnel during attacks, and offers cost-effective protection.\n\nBallistic foam is placed in the dry bays to provide a barrier between the spark and the fuel. As bullets or shrapnel penetrate the mold line skin surrounding the outermost portions of the dry bay, the ballistic foam deprives sparks of oxygen. Thus when the article punctures the fuel tank, a fire is not started. Not only does the foam displace oxygen, but all gases, including explosive vapors which could magnify the destructive effects of ballistic attack. Dry bays, voids, may also contain “onboard ignition sources” like hot surfaces and electrical sparks which benefit both from a lack of gases and the fire-retardant nature of the foam. \n\nBallistic foam strengthens aircraft by protecting it from fire as well as fluid while adding very little weight. The protection from fluid involves resisting damage by “moisture, hydrocarbon fuels, hydraulic fluids, and most common solvents”. The density of the foam varies with the type being used; Type 2.5 is a white to light amber foam weighing 2.5 pounds per cubic foot, while Type 1.8 is a pale blue to green foam weighing 1.8 pounds per cubic foot.\n\nChopped fiberglass strands embedded in the foam add to the structural integrity through physical support and shrapnel mitigation. The layer that strengthens the foam in turn strengthens the airframe. The layer of fiberglass also prevents shrapnel and bullets from rupturing the foam. The fiber glass then allows the damage caused by projectile penetration to heal more effectively.\n\nThe passive protection afforded by ballistic foam is very simple and inexpensive compared to active protection. One method of active protection is done by filling large dry bays with inert gases which will not sustain a flame. This process is very expensive and complex. Active protection only offers a “one time” chance for ballistic protection while the ballistic foam is always available.\n\n"}
{"id": "28513761", "url": "https://en.wikipedia.org/wiki?curid=28513761", "title": "Banagas", "text": "Banagas\n\nBanagas (formally the Bahrain National Gas Company) is a Bahraini gas company, headquartered in Bahrain's Southern Governorate. Banagas operates LPG plant facilities which recover propane, butane and naphtha from the Bahrain Oil Field's associated gas and refinery off-gas. Some 94% of the total workforce are Bahrain nationals. 75% of the company's shares are owned by the Government of Bahrain, with the remaining shares owned by Boubyan Petrochemical Company of Kuwait and Chevron Corporation's Bahrain subsidiary, Chevron Bahrain.\n\nThe company was founded in 1979 by Shaikh Isa bin Salman Al Khalifa with the objective of utilizing Bahrain's gas resources, by processing associated gas and refinery off-gas into marketable products. The entire US$100 million project included the construction of four gas compressor stations, a processing plant to recover propane, butane and naphtha, and a storage area at Sitra. In 1980 the first ship to transport gas was the Japanese tanker \"Yuyo Maru\" which was loaded with 5,000 tonnes of butane. In 1988 the total capacity of the plant was upgraded from 170 (48.1×105m³) million cubic feet/day to 280 million cubic feet/day (79.3×105m³).\n\nBanagas compresses associated gas and refinery off-gas at its compressor stations before being sent to its central gas plant. After processing the treated products are routed to tanks and analyzed then pumped via a pipeline to refrigerated storage tanks at Sitra. Currently, the company has the capacity to produce 3000 bbls of Propane, 3200 bbls of Butane and 4500 bbls of Naphtha per day. In addition to exporting its products, Banagas uses residue gas as fuel for its own furnaces and gas turbines, while the rest is supplied to Aluminium Bahrain, Riffa Power Station, and the Bapco Refinery.\n\n"}
{"id": "6358541", "url": "https://en.wikipedia.org/wiki?curid=6358541", "title": "Black Point Power Station", "text": "Black Point Power Station\n\nBlack Point Power Station () is a gas-fired power station in Hong Kong. The power station is operated by China Light and Power.\n\nBuilt in 1996 near Lung Kwu Tan in Tuen Mun District, the station provides 2,500 MW of power. The complex consists of two halls with 4 turbines in each hall producing cleaner power when compared to coal. The last turbine was completed in 2006.\n\nThe station is owned by the Castle Peak Power Company Limited (CAPCO), a consortium of companies:\n\nSince its deployment, gas is provided from a gas well 750 km south of Hong Kong, in the South China Sea. CLP plans to increase usage of gas for its local generation when more gas becomes available, this change is in line with the Hong Kong Government’s proposed enhancements to the Air Quality Objectives and Climate Change Strategy.\n\n\n"}
{"id": "9363081", "url": "https://en.wikipedia.org/wiki?curid=9363081", "title": "Centre for Energy, Petroleum and Mineral Law and Policy", "text": "Centre for Energy, Petroleum and Mineral Law and Policy\n\nThe Centre for Energy, Petroleum and Mineral Law and Policy (CEPMLP) is a graduate school at the University of Dundee, Scotland, United Kingdom, focused on the fields of international business transactions, energy law and policy, mining and the use of natural resources.\n\nIt is affiliated with, but not part of, the University of Dundee School of Law.\n\nThe CEPMLP is part of the University of Dundee's School of Social Sciences and is based in the Carnegie Building on the Geddes Quadrangle of the University's main campus.\n\nThe CEPMLP adopts an interdisciplinary approach to teaching, research and consultancy providing perspective on how governments, business and communities operate.\n\nThe CEPMLP offers a wide range of degrees from full-time taught master's degrees both full-time on site and by distance learning, as well as research degrees and executive leadership programmes.\n\n\n\n\n"}
{"id": "24739648", "url": "https://en.wikipedia.org/wiki?curid=24739648", "title": "Collapse (film)", "text": "Collapse (film)\n\nCollapse, directed by Chris Smith, is an American documentary film exploring the theories, writings and life story of controversial author Michael Ruppert. \"Collapse\" premiered at the Toronto International Film Festival in September 2009 to positive reviews.\n\nRuppert, a former Los Angeles police officer who describes himself as an investigative reporter and radical thinker, has authored books on the events of the September 11 attacks and of energy issues. Critics in the mainstream media and in D.C. called him a conspiracy theorist and an alarmist.\n\nDirector Smith interviewed Ruppert over the course of fourteen hours in an interrogation-like setting in an abandoned warehouse basement meat locker near downtown Los Angeles. Ruppert’s interview was shot over five days throughout March and April 2009. The filmmakers distilled these interviews down to this 82 minute monologue with archival footage interspersed as illustration.\n\nThe title refers to Ruppert’s belief that unsustainable energy and financial policies have led to an ongoing collapse of modern industrial civilization.\n\nCritics have variously described the film as supportive and as critical of Ruppert’s views. Smith himself, speaking at the Toronto International Film Festival premiere, said that \"What I hoped to reveal was ... that his obsession with the collapse of industrial civilization has led to the collapse of his life. In the end, it is a character study about his obsession.\"\n\nSitting in a room that looks like a bunker, Ruppert briefly recounts his life including his parents' ties to U.S. intelligence agencies and Ruppert’s own career as an LAPD beat cop and detective. Ruppert then summarizes current energy and economic issues, focusing mainly around the core concepts of peak oil and sustainable development. He also criticizes fiat money, fractional reserve banking, compound interest, and leveraging, and discusses alleged CIA drug trafficking.\nThe bulk of the film presents Ruppert making an array of predictions including social unrest, violence, population dislocation and governmental collapses in the United States and throughout the world. He draws on news reports and data available via the Internet, but he applies a unique interpretation which he calls “connecting the dots”.\nSmith periodically stops Ruppert to question his assumptions and provide a note of skepticism.\n\nAfter its premiere at the Toronto Film Festival, Owen Gleiberman of \"Entertainment Weekly\" called \"Collapse\" “one of the few true buzz films of the festival” and wrote that “you may want to dispute [Ruppert], but more than that you’ll want to hear him, because what he says — right or wrong, prophecy or paranoia — takes up residence in your mind.”\n\n\"Daily Variety\" wrote that \"Collapse\" was “unnervingly persuasive much of the time, and merely riveting when it's not, Ruppert's talking-head analysis gets the Errol Morris treatment from director Chris Smith (\"American Movie\"), whose intellectual horror film ranks as another essential work.”\n\n\"The Onion\"s A.V. Club wrote that \"in several immensely poignant moments, we can also see an angry, lonely, vulnerable man whose life epitomizes the title as much as the globe does. There are many layers to the man and the movie, and I for one left the theater shaken.\"\n\nRoger Ebert wrote, \"I don't know when I've seen a thriller more frightening. I couldn't tear my eyes from the screen. \"Collapse\" is even entertaining, in a macabre sense. I think you owe it to yourself to see it.\"\n\nIn October 2009 the filmmakers announced that \"Collapse\" would premiere simultaneously in theaters in New York City and via video on demand on November 6, 2009. According to press announcements, this unique release arrangement “will mark the first time a film will be released this soon after it premiered at a festival without distribution.”\n\n\n"}
{"id": "38658138", "url": "https://en.wikipedia.org/wiki?curid=38658138", "title": "Continuous fiber reinforced thermoplastic", "text": "Continuous fiber reinforced thermoplastic\n\nContinuous fiber reinforced thermoplastic, is a composite material that contains high-performance continuous fiber, such as carbon fiber, glass fiber, or aramid fiber that is impregnated in a matrix of thermoplastics like polycarbonates. CFRTP is producible into both tape and sheet formats that can later be formed using thermoforming techniques.\n"}
{"id": "663729", "url": "https://en.wikipedia.org/wiki?curid=663729", "title": "Cotswold-Severn Group", "text": "Cotswold-Severn Group\n\nThe Cotswold-Severn Group are a series of long barrows erected in an area of western Britain during the Early Neolithic. Around 200 known examples of long barrows are known from the Cotswold-Severn region, although an unknown number of others were likely destroyed prior to being recorded.\n\nThe concept of the \"Cotswold-Severn group\" was coined by 1937 by the archaeologist Glyn Daniel. They represent a regional grouping of long barrows, a broader architectural tradition found across Atlantic Europe. This tradition stretches from southeast Spain up to southern Sweden, taking in the British Isles to the west. Overall, about 40,000 long barrows are known to survive from the Early Neolithic across Europe. The long barrows are not the world's oldest known structures using stone—they are predated by Göbekli Tepe in modern Turkey—but they do represent the oldest widespread tradition of using stone in construction. The archaeologist Frances Lynch has described them as \"the oldest built structures in Europe\" to survive. Although found across this large area, they can be subdivided into clear regionalised traditions based on architectural differences, of which the Cotswold-Severn Group is one.\n\nThe long barrow tradition originated somewhere in the area of modern Spain, Portugal, and western France; here, the long barrows were first erected in the mid-fifth millennium BCE. The tradition then spread north, along the Atlantic coast. It had reached Britain by the first half of the fourth millennium BCE, either soon after farming or in some cases perhaps just before it, and then moved into other parts of northern Europe, for instance arriving in the area of the modern Netherlands by the second half of the fourth millennium BCE.\nOn the basis of dates ascertained from a number of excavations, Darvill argued that long barrows appeared in the Cotswolds-Severn region fairly abruptly around 3700 BCE. They continued to be built for about 600 years. By 2600 BCE, very few of them had chambers that remained in active use and many had been deliberately blocked up.\n\nWithin the Cotswolds-Severn area, there are around 200 known long barrows. An unknown number have been destroyed before ever having been recorded; at least ten of those that had been recorded have since been destroyed or lost.\nOver 140 long barrows are known within the Cotswolds area itself.\nIn northern Wiltshire and in the Dorset chalk hills, the Cotswold-Severn Group of long barrows overlap with the style of earthen barrow found largely across the east of the island.\n\nThe choice of place in which the Cotswold-Severn long barrows were erected is unlikely to have been random.\n\nDarvill noted that \"when these sites were new, they were brutal and hard; bright white rocky mounds covering dark dank shadowy chambers.\"\n\nThe Cotswold-Severn Group long barrows usually contained human bone in large quantities, with said barrows averaging the remains of between 40 and 50 individuals each. In some cases, the individual corpses may have been placed into the chamber whole and then left to decay inside; in others, the body may have been dismembered or excarnated outside the barrow before the bones were then placed into the chamber. Usually, the bones of different individuals were jumbled up within the chambers of the tomb, perhaps reflecting a deliberate decision to symbolically merge the individual with the collective dead. In some cases, the bones were segregated into different chambers within the tomb according to age or sex. In most cases, such deposits of human bone were made successively, at various intervals. It is also apparent that in some cases, select bones appear to have been removed from the chambers, perhaps for use in ritualised practices.\n\nWhen entering the chambers to either add or remove new material, individuals would likely have been exposed to the smell of decaying corpses. It is unknown if entering this area was therefore seen by Early Neolithic Europeans as an ordeal to be overcome or an honourable job to be selected for.\n\nIn a few instances, other items were deposited in the chambers with the human bone. Such deposits included pottery, worked flint, pebbles, stone discs, beads, bone pins, dog bones, and most prominently, cattle bone. The deposition of animal bone—especially the skulls of cattle and pigs—was also a common recurring factor in the forecourts of the Cotswold-Severn long barrows. The purpose of these is not known; they may have represented totemic animals, have been seen as protective deposits, or been the remains of feasts.\n\nWhile the purpose and meaning of these long barrows are not known, archaeologists have made suggestions on the basis of recurring patterns that can be observed within the tradition.\nMany archaeologists have suggested that this is because Early Neolithic people adhered to an ancestor cult that venerated the spirits of the dead, believing that they could intercede with the forces of nature for the benefit of their living descendants. It has furthermore been suggested that Early Neolithic people entered into the tombs—which doubled as temples or shrines—to perform rituals that would honour the dead and ask for their assistance. For this reason, the historian Ronald Hutton termed these monuments \"tomb-shrines\" to reflect their dual purpose.\n\nIn Britain, these tombs were typically located on prominent hills and slopes overlooking the surrounding landscape, perhaps at the junction between different territories. The archaeologist Caroline Malone noted that the tombs would have served as one of a variety of markers in the landscape that conveyed information on \"territory, political allegiance, ownership, and ancestors.\" Many archaeologists have subscribed to the idea that these tomb-shrines served as territorial markers between different tribal groups, although others have argued that such markers would be of little use to a nomadic herding society. Instead it has been suggested that they represent markers along herding pathways. Many archaeologists have suggested that the construction of such monuments reflects an attempt to stamp control and ownership over the land, thus representing a change in mindset brought about by the transition from the hunter-gatherer Mesolithic to the pastoralist Early Neolithic. Others have suggested that these monuments were built on sites already deemed sacred by Mesolithic hunter-gatherers.\n\nTombs of this type are concentrated in the Cotswolds but extend as far as Gower and Avebury with some isolated examples in North Wales. Tombs of all three types are generally evenly distributed and it has been theorised that the design evolved over time. Severn-Cotswold tombs share certain features with the transepted gallery graves of the Loire and may have been inspired by these, with the lateral chambers and other differences being local variations.\n\nIn the 1960s and 1970s Dr John X. W. P. Corcoran and others argued that the group in fact consisted of three contemporary types, and later excavations have supported this.\n\nOne of the first major studies of the subject was \"The Long Barrows of the Cotswolds\", written by the archaeologist O. G. S. Crawford and published in 1925.\n\nDuring the nineteenth and twentieth centuries, a number of sites in the Cotswold-Severn Group were subject to restoration efforts to turn then into visitor attractions.\n\n"}
{"id": "25665491", "url": "https://en.wikipedia.org/wiki?curid=25665491", "title": "Criticism of the Kyoto Protocol", "text": "Criticism of the Kyoto Protocol\n\nAlthough it is a worldwide treaty, the Kyoto Protocol has received criticism.\n\nSome also argue the protocol does not go far enough to curb greenhouse emissions and avoid dangerous climate change (Niue, The Cook Islands, and Nauru added notes to this effect when signing the protocol).\n\nSome environmental economists have been critical of the Kyoto Protocol. Many see the costs of the Kyoto Protocol as outweighing the benefits, some believing the standards which Kyoto sets to be too optimistic, others seeing a highly inequitable and inefficient agreement which would do little to curb greenhouse gas emissions. There are also economists who believe that an entirely different approach needs to be followed than the approach suggested by the Kyoto Protocol.\n\nFurther, there is controversy surrounding the use of 1990 as a base year, as well as not using per capita emissions as a basis. Countries had different achievements in energy efficiency in 1990. For example, the former Soviet Union and eastern European countries did little to tackle the problem and their energy efficiency was at its worst level in 1990, the year just before their communist regimes fell. On the other hand, Japan, as a big importer of natural resources, had to improve its efficiency after the 1973 oil crisis and its emissions level in 1990 was better than most developed countries. However, such efforts were set aside, and the inactivity of the former Soviet Union was overlooked and could even generate big income due to the emission trade. There is an argument that the use of per capita emissions as a basis in the following Kyoto-type treaties can reduce the sense of inequality among developed and developing countries alike, as it can reveal in activities and responsibilities among countries.\n\nJames E. Hansen, director of NASA’s Goddard Institute for Space Studies and eminent climate scientist, has claimed that the United Nations Climate Change Conference taking place at the Bella Center in Copenhagen, Denmark, between December 7–18, 2009 (which includes the 15th Conference of the Parties (COP 15) to the United Nations Framework Convention on Climate Change and the 5th Meeting of the Parties (COP/MOP 5) to the Kyoto Protocol is a 'farce' and planned to boycott it because it was seeking a counter-productive agreement to limit emissions through an inefficient and indulgent “cap and trade” system. “They are selling indulgences there\" Hansen states. \"\"The developed nations want to continue basically business as usual so they are expected to purchase indulgences to give some small amount of money to developing countries. They do that in the form of offsets and adaptation funds.”\" Hansen prefers a progressive “carbon tax”, not the Kyoto Protocol “cap and trade” system; this tax would begin at the equivalent of about $1 per gallon of petrol and revenues would all be returned directly to members of the public as a dividend inversely proportional to their carbon footprint.\n\n\"So, for example, in the Kyoto Protocol, that was very ineffective. Even the countries that took on supposedly the strongest requirements, like Japan for example—if you look at its actual emissions, its actual fossil fuel use, you see that their CO2 emissions actually increased even though they were supposed to decrease. Because their coal use increased and they used offsets to meet their objective. Offsets don’t help significantly. That’s why the approach that Copenhagen is using to specify goals for emission reductions and then to allow offsets to accomplish much of that reduction is really a fake. And that has to be exposed. Otherwise, just like in the Kyoto Protocol, we’ll realize 10 years later, oops, it really didn’t do much.\" \n\nRising Tide North America claims:\n\"Emission limits do not include emissions by international aviation and shipping, but are in addition to the industrial gases, chlorofluorocarbons, or CFCs, which are dealt with under the 1987 Montreal Protocol on Substances that Deplete the Ozone Layer. The benchmark 1990 emission levels were accepted by the Conference of the Parties of UNFCCC (decision 2/CP.3)\"\n\nThere has been criticism (especially from the United States) over the exemption of developing countries, such as China and India, from having to reduce their greenhouse gas emissions under the Kyoto Protocol. The Bush Administration has criticized the Kyoto Protocol on the basis that 80 percent of the world is exempt from emissions reduction standards as well as the potential of economic harm to the United States. Further argument is that developing countries at the time of the creation of the treaty and now have been large emitters of greenhouse gases. Greenhouse gases do not remain in the area in which they are emitted, but rather move throughout the atmosphere of Earth. Therefore, some say that even if the world’s largest greenhouse gas emitter tackled the issue of climate change, there will be minimal impact in the atmosphere if other countries around the world didn’t work on reducing their emission levels as well. There is also criticism over the true impact of the Kyoto Protocol in the long run on reduction of greenhouse gas emissions because it is questioned how much developed countries can offset their emissions while developing countries continue to emit these greenhouse gases.\n\nThere is criticism that the Kyoto Protocol does not do enough to address the issue of climate change and pollution in the long run. One criticism is that climate change is an unique environmental issue, but the Kyoto Protocol followed the format of the other international treaties (not necessarily useful for environmental issues) instead of promoting innovation in approaching the issue of global warming. Another criticism is that the Kyoto Protocol focuses too much on carbon emissions and doesn’t address other pollutants, such as sulfur dioxide and nitrogen oxides, which either do direct harm to human health and/or can be addressed using technology. Some also claim that the Kyoto Protocol does not promote long-term solutions to reduce greenhouse gas emissions, but rather short-term solutions in having countries try to meet emission reduction standards (either by lowering emissions or find ways to obtain trading credits). In the same way, there has been criticism that the Kyoto Protocol does not address the concentration of atmospheric greenhouse gases, but rather greenhouse gas emissions, focusing on the short-term over the long-term.\n\nThere are a large number of critics of carbon trading as a control mechanism. Critics include environmental justice nongovernmental organizations, economists, labor organizations and those concerned about energy supply and excessive taxation. Some see carbon trading as a government takeover of the free market. They argue that trading pollution allowances should be avoided because they result in failures in accounting, dubious science and the destructive impacts of projects upon local peoples and environments. Instead, they advocate making reductions at the source of pollution and energy policies that are justice-based and community-driven. Many argue that emissions trading schemes based upon \"cap and trade\" will necessarily reduce jobs and incomes. Most of the criticisms have focused on the carbon market created through investment in Kyoto Mechanisms. Criticism of cap-and-trade emissions trading has generally been more limited to lack of credibility in the first phase of the EU ETS.\n\nCritics argue that emissions trading does little to solve pollution problems overall, since groups that do not pollute sell their conservation to the highest bidder. Overall reductions would need to come from a sufficient reduction of allowances available in the system.\n\nRegulatory agencies run the risk of issuing too many emission credits, diluting the effectiveness of regulation, and practically removing the cap. In this case, instead of a net reduction in carbon dioxide emissions, beneficiaries of emissions trading simply pollute more. The National Allocation Plans by member governments of the European Union Emission Trading Scheme were criticised for this when it became apparent that actual emissions would be less than the government-issued carbon allowances at the end of Phase I of the scheme. Certain emissions trading schemes have been criticised for the practice of grandfathering, where polluters are given free allowances by governments, instead of being made to pay for them. Critics instead advocate for auctioning the credits. The proceeds could be used for research and development of sustainable technology.\n\nCritics of carbon trading, such as Carbon Trade Watch, argue that it places disproportionate emphasis on individual lifestyles and carbon footprints, distracting attention from the wider, systemic changes and collective political action that needs to be taken to tackle climate change resulting from global warming. Groups such as the Corner House have argued that the market will choose the easiest means to save a given quantity of carbon in the short term, which may be different from the pathway required to obtain sustained and sizable reductions over a longer period, and so a market-led approach is likely to reinforce technological lock-in. For instance, small cuts may often be achieved cheaply through investment in making a technology more efficient, where larger cuts would require scrapping the technology and using a different one. They also argue that emissions trading is undermining alternative approaches to pollution control with which it does not combine well, and so the overall effect it is having is to actually stall significant change to less polluting technologies.\n\nThe corresponding uncertainty under a tax is the level of emissions reductions achieved.\n\nThe \"Financial Times\" published an article about cap-and-trade systems which argued that \"Carbon markets create a muddle\" and \"...leave much room for unverifiable manipulation\".\n\nMore recent criticism of emissions trading regarding implementation is that old growth forests, which have slow carbon absorption rates, are being cleared and replaced with fast-growing vegetation, to the detriment of the local communities.\n\nRecent proposals for alternative schemes to avoid the problems of cap-and-trade schemes include Cap and Share, which was being actively considered by the Irish Parliament in May 2008, and the \"Sky Trust\" schemes. These schemes state that cap-and-trade or \"cap-and-tax\" schemes inherently impact the poor and those in rural areas, who have less choice in energy consumption options.\n\n\n\n"}
{"id": "26296694", "url": "https://en.wikipedia.org/wiki?curid=26296694", "title": "December 1960 nor'easter", "text": "December 1960 nor'easter\n\nThe December 1960 nor'easter was a significant early-season winter storm that impacted the Mid-Atlantic and New England regions of the United States. Moderate to heavy snowfall fell from West Virginia to eastern Maine, amounting to 10 in (25 cm) or more in parts of 13 states and peaking at 21.4 in (54.4 cm) at Newark, New Jersey. The storm was accompanied by strong winds, gusting to over 90 mph (145 km/h) in coastal New England, and left in its wake a dangerously cold air mass. The storm originated in a weak low pressure area which formed over the western Gulf of Mexico on December 10. A secondary low developed over South Carolina on the next day, supported by the merger of two troughs aloft. Sliding southeast of New England, the new storm explosively deepened to become a full-fledged nor'easter, with a minimum central air pressure of 966 mbar. It began to weaken over the Canadian Maritimes.\n\nWidespread blizzard conditions wrought havoc on transportation; streets and highways throughout the hardest-hit areas were clogged with stalled and abandoned vehicles, and multiple major airports were forced to close. Many schools and businesses were closed, some for days after the storm departed. The New York Stock Exchange opened an hour late on December 12, marking its first delayed opening in over 25 years. Extensive drifting of snow left communities isolated and unable to receive deliveries of food and heating fuel. Overall, the storm and subsequent cold snap were blamed for at least 286 deaths across a wide area, but primarily in Pennsylvania, New Jersey, New York, and the New England states. The fatalities resulted from a multitude of tragedies, including automobile and maritime accidents, storm-related fires, carbon monoxide poisoning, over-exertion, and exposure to cold.\n\nThe nor'easter occurred during a positive phase of the North Atlantic oscillation (NAO), though negative NAO values are typically considered optimal for snowstorms in the northeastern United States. It was the first of three severe storms during the 1960–1961 winter season.\n\nIt originated in an area of low pressure that developed over the western Gulf of Mexico on December 10, downstream from an upper-level cyclone. The surface low tracked northward to Oklahoma before turning eastward as it gradually intensified. Simultaneously, a shortwave trough swung southeastward around the periphery of a large vortex over central and eastern Canada. As the upper low opened up into a negatively tilted trough and began to interact with the northern wave, a secondary surface cyclone developed along a frontal boundary over South Carolina late on December 11. Almost immediately thereafter, the original low dissipated over West Virginia. Heading toward the northeast, the incipient storm emerged into the Atlantic Ocean near Cape Hatteras, North Carolina, at 00:00 UTC on December 12.\n\nWith strong upper-level support—including extensive positive vorticity advection and dual divergent jet streak regions—the storm explosively intensified into a powerful nor'easter. From 00:00 to 12:00 UTC on December 12, the system deepened by 27 mbar while moving to a point south of Nantucket. By contrast, the widely accepted criterion for \"bombogenesis\" is a 24-mbar central pressure drop in 24 hours. Its core pressure ultimately fell to 966 mbar by 00:00 UTC the next day, while located off the coast of southwestern Nova Scotia. A rapidly tightening pressure gradient north of the cyclone generated strong winds throughout coastal New England. The storm started to slowly weaken as it moved over the Canadian Maritimes on December 13 and 14.\n\nThe initial low pressure system yielded minor snow accumulations in a wide swath of the Great Plains and Midwest. The nor'easter itself dropped much more significant snowfall over the Mid-Atlantic and New England, with 10 in (25 cm) or greater amounts reported in 13 states from West Virginia to Maine. As much as 20 in (50 cm) of snow fell in small areas of northern New Jersey and eastern Massachusetts. The greatest snowfall total associated with the storm was 21.4 in (54.4 cm) at Newark, New Jersey, within a southwest–northeast oriented corridor of especially heavy snow. In some locations in New Jersey, the storm set new daily snowfall records. Other notable totals include 8.5 in (22 cm) at Washington, D.C.; 14.1 in (36 cm) at Baltimore, Maryland; 14.6 in (37 cm) at Philadelphia, Pennsylvania; 17 in (44 cm) at New York City; 13.4 in (34 cm) at Hartford, Connecticut; 13 in (33 cm) at Boston, Massachusetts; and 14.9 in (38 cm) at Portland, Maine. The storm was rated a \"major\" Category 3 on the Northeast Snowfall Impact Scale (NESIS), and a \"significant\" Category 2 on the Regional Snowfall Index (RSI), an updated counterpart to the NESIS. In parts of southern and eastern Virginia, the snow mixed with sleet and freezing rain, creating particularly treacherous road conditions. Seven deaths and numerous injuries resulted from traffic accidents in the state, primarily in the Richmond and Lynchburg areas.\n\nThe heavy snow combined with strong winds to create widespread and dangerous blizzard conditions, worsened by very cold temperatures left in the wake of a prior cold front. Wind gusts reached on Block Island and on Nantucket. As it pulled away, the large storm system reinforced the Arctic air mass over the eastern United States, with below-normal temperatures persisting for several days; in northwestern Pennsylvania, nighttime temperatures were as cold as . Both during after the storm, persistent gale-force wind gusts produced extensive blowing and drifting of snow. Rough seas and high tides buffeted large sections of shoreline, inflicting damage to boats and coastal installations. A fishing vessel in length capsized off the coast of Maryland, killing two men, and one fatality was attributed to a maritime incident in New York waters. The U.S. Coast Guard rescued six fishermen whose vessel had run aground at a jetty near the entrance to the Nantucket Harbor.\n\nThe storm prompted the closures of public and private schools, businesses, offices, and factories throughout the Northeast. In some areas—such as the city of Boston and parts of New Jersey—classes were canceled for the entire week following the snowstorm. In New York City, student attendance remained extremely low in the days after schools reopened. The New York Public Library was forced to close its main location and all of its 80 branches. Non-essential government offices in Washington, D.C. were also closed because of the storm. Air and ground transportation came to a halt, with multiple major airports closed and rail service suspended. At La Guardia and Idlewild (now John F. Kennedy International) airports alone, over 100 flights were canceled. Railroads reported tens of millions of dollars in lost revenue and storm-related expenses.\n\nSnow drifts reaching in height and abandoned vehicles rendered many roadways impassable. With some communities effectively isolated, supplies of food and heating fuel began to run short. The National Guard arrived in New York City to clear obstructions from streets and allow snow removal work to get underway. In total, 19,000 workers were tasked with clearing snow in New York City, restoring \"near normal\" conditions after several days. The northbound Jersey Turnpike became choked with miles of stalled vehicles, and a large stretch of the Maine Turnpike was closed after \"an unestimated number of motorists were marooned\". At the latter, dozens of state troopers and turnpike employees helped stranded drivers find shelter amid the dangerously cold conditions; at a restaurant in Kennebunk, police ordered 180 motorists to remain inside until the highway could be cleared. In Boston, vehicles stranded in city streets blocked emergency responders from reaching fire situations; such delays were likely responsible for several additional fatalities. A large fire in Mystic, Connecticut, claimed 10 buildings at the height of the storm, with damages estimated at over $1 million.\n\nBy December 15, newspapers had attributed 286 deaths to the blizzard, including 54 in New York, 51 in New Jersey (later confirmed as 52), 26 in Pennsylvania, and 71 in the New England states. Fatalities related to the storm and associated cold snap were far-reaching, involving 23 states. A wide variety of factors contributed to the high death toll, including over-exertion, exposure to cold, transportation accidents, structural fires, and carbon monoxide poisoning. The storm caused some isolated power failures along its path. In north-central Virginia, telephone and electricity lines were damaged. The blizzard cut electricity to all of Newtown, Connecticut, though it was mostly restored within hours. In Rhode Island, Block Island and some inland towns were without power. Short-lived power outages affected southern New Hampshire.\n\nOccurring at an important time for Christmas shopping, the storm had a significant economic impact. In the week after the blizzard, national sales fell as much as seven percent relative to the corresponding week in 1959. The New York Stock Exchange opened an hour late on December 12 in its first delayed opening since 1934. \n\n\n"}
{"id": "4692922", "url": "https://en.wikipedia.org/wiki?curid=4692922", "title": "Direct-ethanol fuel cell", "text": "Direct-ethanol fuel cell\n\nDirect-ethanol fuel cells or DEFCs are a category of fuel cell in which ethanol is fed directly into the cell. They have been used as a model to investigate a range of fuel cell concepts including the use of PEM.\n\nDEFC uses Ethanol in the fuel cell instead of the more toxic methanol. Ethanol is an attractive alternative to methanol because it comes with a supply chain that's already in place. Ethanol also remains the easier fuel to work with for widespread use by consumers.\n\nEthanol is a hydrogen-rich liquid and it has a higher specific energy (8.0 kWh/kg) compared to methanol (6.1 kWh/kg). Ethanol can be obtained in great quantity from biomass through a fermentation process from renewable resources like from sugar cane, wheat, corn, or even straw. Bio-generated ethanol (or bio-ethanol) is thus attractive since growing crops for biofuels absorbs much of the carbon dioxide emitted into the atmosphere from fuel used to produce the biofuels, and from burning the biofuels themselves. This is in sharp contrast to the use of fossil fuels. The use of ethanol would also overcome both the storage and infrastructure challenge of hydrogen for fuel cell applications. In a fuel cell, the oxidation of any fuel requires the use of a catalyst in order to achieve the current densities required for commercially viable fuel cells, and platinum-based catalysts are some of the most efficient materials for the oxidation of small organic molecules.\n\nThe DEFC, similar to the DMFC, relies upon the oxidation of ethanol on a catalyst layer to form carbon dioxide. Water is consumed at the anode and is produced at the cathode. Protons (H) are transported across the proton exchange membrane to the cathode where they react with oxygen to produce water. Electrons are transported through an external circuit from anode to cathode, providing power to connected devices.\n\nThe half-reactions are:\n\nPlatinum-based catalysts are expensive, so practical exploitation of ethanol as fuel for a PEM fuel cell requires a new catalyst. New nanostructured electrocatalysts (HYPERMEC by ACTA SpA for example) have been developed, which are based on non-noble metals, preferentially mixtures of Fe, Co, Ni at the anode, and Ni, Fe or Co alone at the cathode. With ethanol, power densities as high as 140 mW/cm² at 0.5 V have been obtained at 25 °C with self-breathing cells containing commercial anion exchange membranes. This catalyst does not contain any precious metals. In practice tiny metal particles are fixed onto a substrate in such a way that they produce a very active catalyst.\n\nA polymer acts as electrolyte. The charge is carried by the hydrogen ion (proton). The liquid ethanol (CHOH) is oxidized at the anode in the presence of water, generating CO, hydrogen ions and electrons. Hydrogen ions travel through the electrolyte. They react at the cathode with oxygen from the air and the electrons from the external circuit forming water.\n\nBio-Ethanol based fuel cells may improve the well-to-wheel balance of this biofuel because of the increased conversion rate of the fuel cell compared to the internal combustion engine. But real world figures may be only achieved in some years since the development of direct methanol and ethanol fuel cells is lagging behind hydrogen powered fuel cells.\n\nOn 13 May 2007 a team from the University of Applied Sciences in Offenburg presented the world's first vehicle powered by a DEFC at Shell's Eco-marathon in France. The car \"Schluckspecht\" completed a successful test drive on Nogaro Circuit, powered by a DEFC stack giving an output voltage of 20 to 45 V (depending on load).\n\nVarious prototypes of Direct Ethanol Fuel Cell Stack mobile phone chargers were built featuring voltages from 2V to 7V and powers from 800 mW to 2W were built and tested.\n\n\n\n"}
{"id": "22326365", "url": "https://en.wikipedia.org/wiki?curid=22326365", "title": "Dreamland (2009 film)", "text": "Dreamland (2009 film)\n\nDreamland () is a 2009 Icelandic documentary film about politics, environmental preservation and damming, focusing on the Kárahnjúkar Hydropower Plant and its environmental impact. The movie is based on the book \"\" by Andri Snær Magnason. The film's soundtrack is composed by Valgeir Sigurðsson.\n\nThe documentary Dreamland addresses the question of whether Iceland should preserve its unspoiled, unique nature or whether the nation should build enormous dams to produce hydro-electric energy. The film shows how implementing \"green energy\" to provide aluminum industries with cheap energy threatens the natural wonders of Iceland. \n\nThrough interviews with economists, psychologists, historians, poets, editors, and industry managers, the film delivers insight into different point of views. The documentary illustrates the fact that fear is a powerful emotion and a way of controlling people. For example, the fear of unemployment often leads to a limited view of other possible alternatives. To develop the countryside of Iceland and to employ its citizens, the Icelandic government made Iceland the biggest aluminum manufacturer. \n\nThe second theme of the documentary is how the economy benefits from war. The American military base in Keflavík increased the economy of Iceland, but when the United States no longer needed the base, Icelanders feared for their jobs, and the Icelandic government desperately lobbied the American military to stay.\n\nThe documentary streams pictures of a beautiful, untouched nature, including volcanoes, glaciers, mountains, and waterfalls, followed by gigantic hydro towers and dams that destroy the wilderness and beautiful landscape. It makes use of clips from the 2005 documentary \"Wira Pdika\", about Bauxite mining in the Niyamgiri hills of Odisha, to emphasise the relationship of aluminium smelting in Iceland to exploitation elsewhere.\n\n\n"}
{"id": "42024485", "url": "https://en.wikipedia.org/wiki?curid=42024485", "title": "Drive eO", "text": "Drive eO\n\nDrive eO is an engineering company which operates from Latvia and specializes in development of electric and hybrid electric prototype vehicles for demanding applications. It was founded in 2011 to create the first ever hybrid electric rally car OSCar eO for the 2012 Dakar Rally. Driven by and , it successfully completed the rally.\n\nThe company went on to develop a number of electric racing cars for participation at the Pikes Peak International Hill Climb. In 2015, the one-megawatt eO PP03 race car driven by Rhys Millen set a new EV record and became the first ever all-electric vehicle to win the event overall. \n\nThe car is based on the platform of all wheel drive OSCar O3 rally raid car manufactured by OSC. It features a series hybrid drivetrain. Drive eO gives the following vehicle specification:\n\nOSCar eO has participated at three international rally raids:\n\nThe car features an all wheel drive electric drivetrain with one inboard mounted direct drive motor per wheel. It is based on a tubular spaceframe chassis with bodywork derived from Aquila CR1 racing car. Drive eO gives the following vehicle specification:\n\nThe car was driven at the 2013 Pikes Peak International Hill Climb by Latvian and Baltic touring car champion Jānis Horeliks. The adverse weather conditions had made the track surface slippery and he went off road after losing grip in a left hand corner at the Halfway Picnic Grounds. A documentary called \"Uzvaras cena\" has been produced about the project and was first aired in September 2013 on LTV7 during motoring programme \"Tavs auto\".\n\nDrive eO returned to the Pikes Peak International Hill Climb in 2014 with a race car based on a Tesla Roadster chassis. This was the first time that Tesla brand was represented at a major international motorsport event. The original chassis was retained but the entire drive train was replaced by Drive eO to validate the new components ahead of scaling them up for successive electric supercar projects. The car was again piloted by Janis Horeliks and he completed the course in 12 minutes and 57.536 seconds to rank fifth in the Electric Modified division. \n\nDrive eO gives the following vehicle specification:\n\nAs the next step, Drive eO developed an entirely new vehicle eO PP03 for the 93rd running of Pikes Peak International Hill Climb on June 28, 2015. The vehicle featured a one-megawatt electric drive train and all-wheel drive transmission. The team signed Rhys Millen to drive the car and was aiming for overall victory.\n\nOn the race day Rhys Millen completed the course in 9 minutes and 7.222 seconds, setting a new EV record and becoming the overall winner.\n\nIn 2016, the car had Peak power and peak torque 2520 Nm, with a weight of 1200 kg. Millen set a record of 8:57.118.\n\n"}
{"id": "3924362", "url": "https://en.wikipedia.org/wiki?curid=3924362", "title": "Electric spark", "text": "Electric spark\n\nAn electric spark is an abrupt electrical discharge that occurs when a sufficiently high electric field creates an ionized, electrically conductive channel through a normally-insulating medium, often air or other gases or gas mixtures. Michael Faraday described this phenomenon as \"the beautiful flash of light attending the discharge of common electricity\".\n\nThe rapid transition from a non-conducting to a conductive state produces a brief emission of light and a sharp crack or snapping sound. A spark is created when the applied electric field exceeds the dielectric breakdown strength of the intervening medium. For air, the breakdown strength is about 30 kV/cm at sea level. At the beginning stages, free electrons in the gap (from cosmic rays or background radiation) are accelerated by the electrical field. As they collide with air molecules, they create additional ions and newly freed electrons which are also accelerated. At some point, thermal energy will provide a much greater source of ions. The exponentially-increasing electrons and ions rapidly cause regions of the air in the gap to become electrically conductive in a process called dielectric breakdown. Once the gap breaks down, current flow is limited by the available charge (for an electrostatic discharge) or by the impedance of the external power supply. If the power supply continues to supply current, the spark will evolve into a continuous discharge called an electric arc. An electric spark can also occur within insulating liquids or solids, but the breakdown mechanisms are significantly different than for sparks in gases.\n\nSometimes, sparks can be dangerous. They can cause fires and burn skin.\n\nLightning is an example of an electric spark in nature, while electric sparks, large or small, occur in or near many man-made objects, both by design and sometimes by accident.\n\nAround 600 BC, Greek philosopher Thales of Miletus observed that amber could be electrified when rubbed with a cloth and attract other objects and produce sparks. In 1671, Leibniz discovered that sparks were associated with electrical phenomena. In 1708, Samuel Wall performed experiments with amber rubbed with cloth to produce sparks. In 1752, Thomas-François Dalibard, acting on an experiment proposed by Benjamin Franklin, arranged for a retired French dragoon named Coiffier in the village of Marly to collect lightning in a Leyden jar thus proving that lightning and electricity were equivalent. In Franklin's famous kite experiment, he successfully extracted sparks from a cloud during a thunderstorm.\n\nElectric sparks are used in spark plugs in gasoline internal combustion engines to ignite fuel and air mixtures. The electric discharge in a spark plug occurs between an insulated central electrode and a grounded terminal on the base of the plug. The voltage for the spark is provided by an ignition coil or magneto that is connected to the spark plug with an insulated wire.\n\nFlame igniters use electric sparks to initiate combustion in some furnaces and gas stoves in place of a pilot flame. Auto reignition is a safety feature that is used in some flame igniters that senses the electrical conductivity of the flame and uses this information to determine whether a burner flame is lit. This information is used to stop an ignition device from sparking after the flame is lit or restart the flame if it goes out.\n\nA spark-gap transmitter uses an electric spark gap to generate radio frequency electromagnetic radiation that can be used as transmitters for wireless communication. Spark gap transmitters were widely used in the first three decades of radio from 1887–1916. They were later supplanted by vacuum tube systems and by 1940 were no longer used for communication. The wide use of spark-gap transmitters led to the nickname \"sparks\" for a ship's radio officer.\n\nElectric sparks are used in different kinds of metalworking. Electric discharge machining (EDM) is sometimes called spark machining and uses a spark discharge to remove material from a workpiece. Electrical discharge machining is used for hard metals or those that are difficult to machine with traditional techniques.\n\nSpark plasma sintering (SPS) is a sintering technique that uses a pulsed direct current that passes through an conductive powder in a graphite die. SPS is faster than conventional hot isostatic pressing, where the heat is provided by external heating elements.\n\nThe light that is produced by electric sparks can be collected and used for a type of spectroscopy called spark emission spectroscopy.\nA high energy pulsed laser can be used to produce an electric spark. Laser induced breakdown spectroscopy (LIBS) is a type of atomic emission spectroscopy that uses a high pulse energy laser to excite atoms in a sample. LIBS has also been called laser spark spectroscopy (LSS).\n\nElectric sparks can also be used to create ions for mass spectrometry.\n\nSparks can be hazardous to people, animals or even inanimate objects. Electric sparks can ignite flammable materials, liquids, gases and vapors. Even inadvertent static-discharges, or small sparks that occur when switching on lights or other circuits, can be enough to ignite flammable vapors from sources like gasoline, acetone, propane, or dust concentrations in the air, such as those found in flour mills. Sparks often indicate the presence of a high voltage, or \"potential field\". The higher the voltage; the farther a spark can jump across a gap, and with enough energy supplied can lead to greater discharges such as a glow or an arc. When a person is charged with high-voltage static-charges, or is in the presence of high-voltage electrical supplies, a spark can jump between a conductor and a person who is in close enough proximity, allowing the release of much higher energies that can cause severe burns, shut down the heart and internal organs, or even develop into an arc flash. High-voltage sparks, even those with low energy such as from a stun gun, can overload the conductive pathways of the nervous system, causing involuntary muscle-contractions, or interfere with vital nervous-system functions such as heart rhythm. When the energy is low enough most of it may be used just heating the air, so the spark never fully stabilizes into a glow or arc. However, sparks with very low energy still produce a \"plasma tunnel\" through the air, through which electricity can pass. This plasma is heated to temperatures often greater than the surface of the sun, and can cause small, localized burns. Conductive liquids, gels or ointments are often used when applying electrodes to a person's body, preventing sparks from forming at the point of contact and damaging skin. Similarly, sparks can cause damage to metals and other conductors, ablating or pitting the surface; a phenomenon which is exploited in electric etching. Sparks also produce ozone which, in high enough concentrations, can cause respiratory discomfort or distress, itching, or tissue damage, and can be harmful to other materials such as certain plastics.\n\n\n"}
{"id": "23058950", "url": "https://en.wikipedia.org/wiki?curid=23058950", "title": "Energy input labeling", "text": "Energy input labeling\n\nThe term energy input labeling involved producers of goods and services determining how much energy is used to produce their product, and then including that information on their product packaging. Energy input labeling is sometimes known by the acronym EIL. Energy input labeling provides the advantage of knowing how much energy was used to produce a product, but it does not indicate how much energy a product uses to operate, such as the European Union energy label or the Energy rating label used in Australia and New Zealand, and is not in itself a standard for energy efficiency such as Energy Saving Trust Recommended or Energy Star.\n\nEnergy input labeling originated as a project by several energy and economics activists to explore energy accounting.\n\nEnergy input labeling is intended to be easy for producers to implement, At minimum, they can report and label the energy used by their firm to produce products, which is called \"Energy Inputs Added\", some sometimes merely \"Energy Added.\" If a firm is able to also account for all of the energy imputed by its suppliers, then a firm can report and label \"Total Energy Inputs\" or \"Total Energy\", but this is rare. Energy Input Labeling is being used and further developed by the European Organization for Sustainability.\n\nIn Japan, the Top Runner Program is run in which new appliances are regularly tested on efficiency, and the most efficient ones are made the standard\n\n\n"}
{"id": "20888413", "url": "https://en.wikipedia.org/wiki?curid=20888413", "title": "Force lines", "text": "Force lines\n\nForce lines method is used in Solid Mechanics for visualization of internal forces in a deformed body. A force line represents graphically the internal force acting within a body across imaginary internal surfaces. The force lines show the maximal internal forces and their directions.\n\nThe procedure for determining the force lines consists of two stages:\n1) Defining the internal surface. The surface is perpendicular to maximum principal stress in every point of the solid.\n2) Integration of internal stresses on the surface. Stress is a measure of the average amount of force exerted per unit area. The stress distribution can be obtained from known theoretical or numerical (Finite element method) analysis.\n\nThe researcher who builds up the force lines can choose a magnitude of the internal force and the initial border where the drawing procedure starts.\n\nFigure 1 shows an example of force lines in a body with a hole under tension. The force lines are denser near the hole. The visualization helps to explain the stress concentration.\n\nFigure 2 shows the force lines in a body with a crack. The cracks are the most dangerous stress concentrator: the intensity of the force lines is high in the crack tip (see Fracture mechanics).\n\nFigure 3 shows the case of pure bending of a beam with rectangular cross section. There are no internal forces at the neutral axis of the beam. The tensile and compressive force lines are symmetrical and are denser at the beam’s edge.\n\nThe force lines pictures are used for\n\n1) Analysis of stress concentration (Figure1 and Figure 2): the number of the force lines increases in areas with stress concentration.\n\n2) Optimization of structures: reinforcing the structure in the areas with concentration of force lines and deleting the components where there are no force lines.\n\nFigure 4 shows 2 examples of old and new structures. The force lines distribution is more efficient for the second case. \nThe visualization of force lines is a qualitative method, which is used as a supplementary method to the stress analysis.\n\n"}
{"id": "20570809", "url": "https://en.wikipedia.org/wiki?curid=20570809", "title": "Galoter process", "text": "Galoter process\n\nThe Galoter process (also known as TSK, UTT, or SHC; its newest modifications are called Enefit and Petroter) is a shale oil extraction technology for a production of shale oil, a type of synthetic crude oil. In this process, the oil shale is decomposed into shale oil, oil shale gas, and spent residue. A decomposition is caused by mixing raw oil shale with a hot oil shale ash, generated by combustion of carbonaceous residue (semi-coke) in the spent residue. The process was developed in 1950s and it is used commercially for the shale oil production in Estonia. There are projects for further development of this technology and for expansion of its usage, e.g. in Jordan and USA.\n\nResearch of the solid heat carrier process for pyrolysis of lignite, peat, and oil shale started in 1944 at the G. M. Krzhizhanovsky Power Engineering Institute of Academy of Sciences of the USSR. At the laboratory scale, the Galoter process was invented and developed in 1945–1946. The process was named Galoter after the research team leader Israel Galynker whose name was combined with the word \"thermal\".\n\nThe further research continued in Estonia. A pilot unit with capacity of 2.5 tonnes of oil shale per day was built in Tallinn in 1947. The first Galoter-type commercial scale pilot retorts were built at Kiviõli, Estonia, in 1953 and 1963 (closed in 1963 and 1981 respectively) with capacities of 200 and 500 tonnes of oil shale per day respectively. The Narva Oil Plant, annexed to the Eesti Power Plant and operating two Galoter-type 3000 tonnes per day retorts, was commissioned in Estonia in 1980. These retorts were designed by AtomEnergoProject and developed in cooperation with the Krzhizhanovsky Institute. Started as a pilot plant, the process of converting it to commercial scale plant took about 20 years. During this period, the company has modernized more than 70% of the equipment compared to the initial design.\n\nIn 1978, a 12.5-tonnes pilot plant was built in Verkhne-Sinevidnoy, Ukraine. It was used for testing of Lviv–Volinsk lignite, and Carpathian, Kashpir (Russia) and Rotem (Israel) oil shales. In 1996–1997, a test unit was assembled in Tver.\n\nIn 2008, Estonian energy company Eesti Energia, an operator of Galoter retorts at the Narva Oil Plant, established a joint venture with the Finnish technology company Outotec called Enefit Outotec Technology to develop and commercialize a modified Galoter process–the Enefit process–which combines the current process with circulating fluidized bed technologies. In 2013, Enefit Outotec Technology opened an Enefit testing plant in Frankfurt. In 2012 Eesti Energia opened a new generation Galoter-type plant in Narva using Enefit 280 technology.\n\nIn 2009–2015, VKG Oil, a subsidiary of Viru Keemia Grupp, opened in Kohtla-Järve, Estonia three modified Galoter-type oil plants called Petroter. \n\nThe Galoter process is an above-ground oil-shale retorting technology classified as a hot recycled solids technology. The process uses a horizontal cylindrical rotating kiln-type retort, which is slightly declined. It has similarities with the TOSCO II process.\n\nBefore retorting, the oil shale is crushed into fine particles with a size of less than in diameter. The crushed oil shale is dried in the fluidized bed drier (aerofountain drier) by contact with hot gases. After drying and pre-heating to , oil shale particles are separated from gases by cyclonic separation. Oil shale is transported to the mixer chamber, where it is mixed with hot ash of , produced by combustion of spent oil shale in a separate furnace. The ratio of oil shale ash to raw oil shale is 2.8–3:1. The mixture is moved then to the hermetic rotating kiln. When the heat transfers from the hot ash to raw oil shale particles, the pyrolysis (chemical decomposition) begins in oxygen deficit conditions. The temperature of pyrolysis is kept at . Produced oil vapors and gases are cleaned of solids by cyclones and moved to condensation system (rectification column) where shale oil condenses and oil shale gas is separated in gaseous form. Spent shale (semi-coke) is transported then to the separate furnace for combustion to produce hot ash. A portion of the hot ash is separated from the furnace gas by cyclones and recycled to the rotary kiln for pyrolysis. The remaining ash is removed from the combustion gas by more cyclones and cooled and removed for disposal by using water. The cleaned hot gas returns to the oil shale dryer.\n\nThe Galoter process has high thermal and technological efficiency, and high oil recovery ratio. Oil yield reaches 85–90% of Fischer Assay and retort gas yield accounts for 48 cubic meters per tonne. Oil quality is considered good, but the equipment is sophisticated and capacity is relatively low. This process creates less pollution than internal combustion technologies, as it uses less water, but it still generates carbon dioxide as also carbon disulfide and calcium sulfide.\n\nEnefit process is a modification of the Galoter process being developed by Enefit Outotec Technology. In this process, the Galoter technology is combined with proven circulating fluidized bed (CFB) combustion technology used in coal-fired power plants and mineral processing. Oil shale particles and hot oil shale ash are mixed in a rotary drum as in the classical Galoter process. The primary modification is the replacing of the Galoter semi-coke furnace with a CFB furnace. The Enefit process also incorporates fluid bed ash cooler and waste heat boiler commonly used in coal-fired boilers to convert waste heat to steam for power generation. Compared to the traditional Galoter, the Enefit process allows complete combustion of carbonaceous residue, improved energy efficiency by maximum utilization of waste heat, and less water use for quenching. According to promoters, the Enefit process has a lower retorting time compare to the classical Galoter process and therefore it has a greater throughput. Avoidance of moving parts in the retorting zones increases their durability.\n\nTwo Galoter retorts built in 1980 are used for oil production by the Narva Oil Plant, a subsidiary of the Estonian energy company Eesti Energia. Both retorts process 125 tonnes per hour of oil shale. The annual shale oil production is 135,000 tonnes and oil shale gas production is . Since 2012, it also uses a new plant employing Enefit 280 technology with a processing capacity of 2.26 million tonnes of oil shale per year and producing 290,000 tonnes of shale oil and of oil shale gas. In addition, Eesti Energia planned to begin construction of similar Enefit plants in Jordan and in USA. Enefit Outotec Technology analysis suitability of Enefit technology for the Tarfaya oil shale deposit in Morocco, developed by San Leon Energy.\n\nVKG Oil operates in Kohtal-Järve, Estonia three modified Galoter-type oil plants called Petroter. The basic engineering of these retorts was done by Atomenergoproject of Saint Petersburg. The basic engineering of the condensation and distillation plant was done by Rintekno of Finland. The plant has a processing capacity of 1.1 million tonnes of oil shale per year and it produces 100,000 tonnes of shale oil, of oil shale gas, and 150 GWh of steam per year.\n\nSaudi Arabian International Corporation for Oil Shale Investment planned to utilize Galoter (UTT-3000) process to build a shale oil plant in Jordan. Uzbekneftegaz planned to build eight UTT-3000 plants in Uzbekistan. However, in December 2015 Uzbekneftegaz announced a postponement of the project.\n\n"}
{"id": "41207", "url": "https://en.wikipedia.org/wiki?curid=41207", "title": "Gel", "text": "Gel\n\nA gel is a solid jelly-like material that can have properties ranging from soft and weak to hard and tough. Gels are defined as a substantially dilute cross-linked system, which exhibits no flow when in the steady-state. By weight, gels are mostly liquid, yet they behave like solids due to a three-dimensional cross-linked network within the liquid. It is the crosslinking within the fluid that gives a gel its structure (hardness) and contributes to the adhesive stick (tack). In this way gels are a dispersion of molecules of a liquid within a solid in which liquid particles are dispersed in the solid medium . The word \"gel\" was coined by 19th-century Scottish chemist Thomas Graham by clipping from \"gelatine\".\n\nGels consist of a solid three-dimensional network that spans the volume of a liquid medium and ensnares it through surface tension effects. This internal network structure may result from physical bonds (physical gels) or chemical bonds (chemical gels), as well as crystallites or other junctions that remain intact within the extending fluid. Virtually any fluid can be used as an extender including water (hydrogels), oil, and air (aerogel). Both by weight and volume, gels are mostly fluid in composition and thus exhibit densities similar to those of their constituent liquids. Edible jelly is a common example of a hydrogel and has approximately the density of water.\n\nPolyionic polymers are polymers with an ionic functional group. The ionic charges prevent the formation of tightly coiled polymer chains. This allows them to contribute more to viscosity in their stretched state, because the stretched-out polymer takes up more space. This is also the reason gel hardens. See polyelectrolyte for more information.\n\nA hydrogel is a network of polymer chains that are hydrophilic, sometimes found as a colloidal gel in which water is the dispersion medium. A three-dimensional solid results from the hydrophilic polymer chains being held together by cross-links. Because of the inherent cross-links, the structural integrity of the hydrogel network does not dissolve from the high concentration of water. Hydrogels are highly absorbent (they can contain over 90% water) natural or synthetic polymeric networks.\nHydrogels also possess a degree of flexibility very similar to natural tissue, due to their significant water content.\nThe first appearance of the term 'hydrogel' in the literature was in 1894.\nCommon uses for hydrogels include:\n\nCommon ingredients include polyvinyl alcohol, sodium polyacrylate, acrylate polymers and copolymers with an abundance of hydrophilic groups.\n\nNatural hydrogel materials are being investigated for tissue engineering; these materials include agarose, methylcellulose, hyaluronan, Elastin like polypeptides and other naturally derived polymers.\nHydrogels show promise for use in agriculture, as they can release agrochemicals including pesticides and phosphate fertiliser slowly, increasing efficacy and reducing runoff, and at the same time improve the water retention of drier soils such as sandy loams.\n\nAn organogel is a non-crystalline, non-glassy thermoreversible (thermoplastic) solid material composed of a liquid organic phase entrapped in a three-dimensionally cross-linked network. The liquid can be, for example, an organic solvent, mineral oil, or vegetable oil. The solubility and particle dimensions of the structurant are important characteristics for the elastic properties and firmness of the organogel. Often, these systems are based on self-assembly of the structurant molecules. (An example of formation of an undesired thermoreversible network is the occurrence of wax crystallization in petroleum.)\n\nOrganogels have potential for use in a number of applications, such as in pharmaceuticals, cosmetics, art conservation, and food.\n\nA xerogel is a solid formed from a gel by drying with unhindered shrinkage. Xerogels usually retain high porosity (15–50%) and enormous surface area (150–900 m/g), along with very small pore size (1–10 nm). When solvent removal occurs under supercritical conditions, the network does not shrink and a highly porous, low-density material known as an \"aerogel\" is produced. Heat treatment of a xerogel at elevated temperature produces viscous sintering (shrinkage of the xerogel due to a small amount of viscous flow) and effectively transforms the porous gel into a dense glass.\n\nNanocomposite hydrogels or hybrid hydrogels, are highly hydrated polymeric networks, either physically or covalently crosslinked with each other and/or with nanoparticles or nanostructures. Nanocomposite hydrogels can mimic native tissue properties, structure and microenvironment due to their hydrated and interconnected porous structure. A wide range of nanoparticles, such as carbon-based, polymeric, ceramic, and metallic nanomaterials can be incorporated within the hydrogel structure to obtain nanocomposites with tailored functionality. Nanocomposite hydrogels can be engineered to possess superior physical, chemical, electrical, and biological properties.\n\nMany gels display thixotropy – they become fluid when agitated, but resolidify when resting.\nIn general, gels are apparently solid, jelly-like materials. It is a type of non-Newtonian fluid.\nBy replacing the liquid with gas it is possible to prepare aerogels, materials with exceptional properties including very low density, high specific surface areas, and excellent thermal insulation properties.\n\nSome species secrete gels that are effective in parasite control. For example, the long-finned pilot whale secretes an enzymatic gel that rests on the outer surface of this animal and helps prevent other organisms from establishing colonies on the surface of these whales' bodies.\n\nHydrogels existing naturally in the body include mucus, the vitreous humor of the eye, cartilage, tendons and blood clots. Their viscoelastic nature results in the soft tissue component of the body, disparate from the mineral-based hard tissue of the skeletal system. Researchers are actively developing synthetically derived tissue replacement technologies derived from hydrogels, for both temporary implants (degradable) and permanent implants (non-degradable). A review article on the subject discusses the use of hydrogels for nucleus pulposus replacement, cartilage replacement, and synthetic tissue models.\n\nMany substances can form gels when a suitable thickener or gelling agent is added to their formula. This approach is common in manufacture of wide range of products, from foods to paints and adhesives.\n\nIn fiber optics communications, a soft gel resembling \"hair gel\" in viscosity is used to fill the plastic tubes containing the fibers. The main purpose of the gel is to prevent water intrusion if the buffer tube is breached, but the gel also buffers the fibers against mechanical damage when the tube is bent around corners during installation, or flexed. Additionally, the gel acts as a processing aid when the cable is being constructed, keeping the fibers central whilst the tube material is extruded around it.\n\n\n"}
{"id": "13491155", "url": "https://en.wikipedia.org/wiki?curid=13491155", "title": "Gunpowder magazine", "text": "Gunpowder magazine\n\nA gunpowder magazine is a magazine (building) designed to store the explosive gunpowder in wooden barrels for safety. Gunpowder, until superseded, was a universal explosive used in the military and for civil engineering: both applications required storage magazines. Most magazines were purely functional and tended to be in remote and secure locations. They are the successor to the earlier powder towers and powder houses.\n\nHistoric magazines were at the following locations, among others:\n\nThere are magazines at:\n\nThe Ballincollig gunpowder mills were first opened in the late 18th century and were bought, in 1804, by the United Kingdom of Great Britain and Ireland's Board of Ordnance to help defend the Kingdom against attack. They were one of three royal gunpowder factories; but the Ballincollig mills became disused after the end of the Napoleonic Wars. They were sold off by the government in 1832, in a semi-derelict condition; but were bought by a Liverpool merchant and were reopened to manufacture gunpowder; finally closing, just over a century ago, in 1903. Many buildings survive and, together with the associated canals, were incorporated into a regional park – Ballincollig Regional Park. The site contains a number of powder magazines, as well as Expense magazines.\n\nThe No. 2 magazine was built by the Board of Ordnance and is the oldest magazine. It is 29 foot (8.9 m) long by 28 foot (8.6 m) wide. It has a groin-vaulted roof. The magazine is protected by earthen banks on two sides; with doors at both ends. The No. 1 magazine is newer; and was built sometime after 1828. It is 80 foot (24.5 m) long by 25 foot (7.6 m) wide and has solid walls, but is now roofless.\n\nThere is a surviving Magazine at Camden Fort Meagher, part of the defences of Cork Harbour.\n\nRocky Island, midway between the mainland and Haulbowline Island (which at the time was an ordnance depot), is dominated by a magazine complex dating from 1808; it held up to 25,000 barrels, and was the principal store for the whole of Ireland. In 2007 it was restored and converted into Ireland's first crematorium outside Dublin.\n\nThe Order of Saint John built a number of gunpowder magazines in Malta during their rule of the islands. Until the end of the sixteenth century, echauguettes were used to store gunpowder. The Order also built a magazine in Valletta, but this exploded in 1634, killing 22 people. After the explosion, a new magazine was built in Floriana, which was sparsely populated, to avoid another disaster. Various other magazines were built over the years, and their designs were influenced by French military architecture, particularly the style of Vauban. The British, who took over Malta in 1800, also built a number of magazines on the islands.\n\nGunpowder magazines in Malta include:\n\nIn addition, some of the coastal fortifications also had their own magazines or storage areas.\n\nIn the Netherlands three gunpowder magazines still exist. The Kruithuis in Delft, the Kruithuis in Den Bosch (the oldest, built in 1618–1620) and one in Wierickerschans.\n\nGunpowder magazines were called \"Bārūt-Khāneh\" (, or ) in Persia (Iran).\n\nGunpowder Manufacturing of Yusef Abad (), and later the Gunpowder Magazine of Tehran ( \"Bārūt-Khāneh-ye Tehrān\"), was a gunpowder magazine near Tehran which was built during the Qajar dynasty. Nothing remains of this building today, and its exact location is unknown.\n\n\nA Magazine was erected in Bathurst, East Cape, by the British Military in 1821; it is still standing. It usually carried about 273 kg gunpowder, 7,000 ball cartridges and 60 rifles as stock. In 1870 the British Military build a power magazine in the Northern Cape town of Fraserburg, also still standing, in case of war with the neighbouring Griqua people and subsequently used in the Anglo-Boer War.\n\nProduction of gunpowder in England appears to have started in the mid-13th century with the aim of supplying The Crown. Records show that gunpowder was being made in 1346, at the Tower of London; a powder house existed at the Tower in 1461. Gunpowder was also being made or stored at other royal residences such as Greenwich Palace (the reason being that these were where the royal armouries were based). It was also stored in Scotland, in royal castles, such as Edinburgh Castle. Gunpowder manufacture at Faversham began as a private enterprise in the 16th century.\n\nFrom the 18th century, efforts began to be made to site magazines away from inhabited areas. Nevertheless, storage at the older established sites persisted well into the 19th century.\n\nThe use of gunpowder for both military and civil engineering purposes began to be superseded by newer nitrogen-based explosives from the later 19th century. Gunpowder production in the United Kingdom was gradually phased out during the mid-20th century. The last remaining gunpowder mill at the Royal Gunpowder Factory, Waltham Abbey was damaged by a German parachute mine in 1941 and it never reopened. This was followed by the closure of the gunpowder section at the Royal Ordnance Factory, ROF Chorley, the section was closed and demolished at the end of World War II, and ICI Nobel's Roslin gunpowder factory which closed in 1954. This left the sole United Kingdom gunpowder factory at ICI Nobel's Ardeer site in North Ayrshire, Scotland; it too closed in October 1976. Since then gunpowder has been imported into the United Kingdom.\n\nGunpowder magazines survive at several locations in the UK. It can be seen that, in many cases, the gunpowder was stored in locations which were both remote from habitations and could be made secure. They were also often sited in dense woodland (or had trees planted around them) as a way of lessening the effect of any explosion.\n\nThe Gunpowder Magazine in Berwick-upon-Tweed was built in 1745 to service Berwick Barracks and sited at a safe distance from them to the south. It is a solid stone building, heavily buttressed, windowless, stone roofed and enclosed by a stone wall. Along with Purfleet and Tilbury it is one of the few surviving eighteenth-century gunpowder magazines in the country.\n\nBrean Down Fort was one of a number of \"Palmerston Forts\" built to defend the British, Irish and Channel Island coastlines. It was originally built in stages between 1862 and 1870; to protect the Bristol Channel. It had a large, underground, main gunpowder magazine, 15 foot (4.5 m) by 18 foot (5.5 m) by 20 foot (6.1 m) high, built to the recommendations of the 1863 Royal Commission. The magazine still exists. A further two, smaller, underground magazines, \"No. 2 magazine\" and \"No. 3 magazine\", were also built. No. 3 magazine exploded on 3 July 1900 destroying most of the barracks. Gunner Hains was killed. It was concluded that he had killed himself by firing a ball cartridge down a ventilator shaft into the magazine which held 3 tons (3 tonnes) of gunpowder, causing the magazine to explode. The fort was reused in both the First and Second World Wars; and additional expense magazines constructed. The fort is now owned by the National Trust.\n\nAs early as 1461, the Tower of London included a 'powderhous' within its walls. With the establishment of the Board of Ordnance there, its use as a gunpowder store increased. In the Tudor period the White Tower was refitted for this purpose, and by 1657 the entire building apart from the chapel was being used to store gunpowder. Gunpowder was still being stored there when the Ordnance Board was disbanded in the mid-nineteenth century.\n\nThe Board of Ordnance maintained Magazines at both Tilbury Fort and New Tavern Fort, which face each other across the River Thames. Two sizeable Magazines of 1716 remain in place at Tilbury.\n\nPurfleet Royal Gunpowder Magazine was established by Act of Parliament in 1760, built to the design of James Gabriel Montresor and opened in 1765, with a garrison in place to protect it. Previously, gunpowder had been stored on Greenwich Peninsula, but fears of an explosion there prompted the building of this new establishment, further afield. The purpose of Purfleet was to store newly manufactured gunpowder, prior to its distribution elsewhere.\n\nPurfleet was centred on five large magazines, each one capable of holding up to 10,400 barrels of gunpowder. These substantial brick-built sheds were windowless, with copper-lined doors and sand-filled roof voids – all designed to prevent (or mitigate the effects of) an explosion. By the end of the eighteenth century, Purfleet was receiving regular consignments of powder from Waltham Abbey, to provide both the Navy and the Army with supplies.\n\nThe Ministry of Defence finally closed and sold the site in 1962, and several buildings were demolished to make way for a new housing estate. Some significant original buildings remain, however: the clock tower, the proofing house (in which samples of new consignments were tested) and one magazine. This magazine, No. 5, has been designated a Scheduled Ancient Monument and now houses the Purfleet Heritage and Military Centre, a vast collection of local and military memorabilia open to the public. According to English Heritage, it represents (along with the magazine at Priddy's Hard) \"the most outstanding example of a typically British type of magazine, with twin barrel vaults, that relates to a critical period in Britain's growth as a naval power in the decades after the Seven Years' War.\" Inside, a good number of original features have survived, including some unique wooden overhead cranes, early forerunners of the gantry crane.\n\nA sizeable magazine stands in the unexpected surroundings of London's Hyde Park. Opened by the Board of Ordnance in 1805, its structure is similar to other British magazines of this period except for the fact that the exterior is more ornamented here than elsewhere (probably in deference to its setting) with a Palladian style portico and other features. The magazine provided the army with a stock of gunpowder in the capital, in case of 'foreign invasion or popular uprising'. It remained in MOD hands until 1963, after which it served as a storage facility. Since 2013 it has had a new lease of life, having been refurbished and extended by Dame Zaha Hadid, as the Serpentine Sackler Gallery.\n\nIn Plymouth the Board of Ordnance set up Powder Magazines to serve the fleet and defences of Devonport Dockyard initially at the Royal Citadel (later supplemented by a small magazine at the New Gun Wharf (Morice Yard) in 1720); but space was limited and people were living close by, so the Board sought a new, more isolated spot for its Magazines. They first settled on a site at Keyham Point (just north of Morice Yard) in 1775; but with that land required for an expansion of the Dockyard in the mid-19th century a new site was acquired further to the north, at Bull Point.\n\nA magazine of 1744 survives \"in situ\" at Morice Yard (which today forms part of HMNB Devonport). Built to replace the earlier small magazine, which stood at the centre of the site, this is Britain's oldest surviving naval ordnance magazine.\n\nThe Royal Navy Ordnance Base (later RNAD) Bull Point was the last great work of the Board of Ordnance\nbefore its disbandment in 1856. Bull Point was and is unusual in the unity and precise purpose of its design: rather than developing gradually over time, it was planned as a whole, and with a particular view to meeting the storage needs of emerging new types of artillery. Four Magazines were built (1851–54) each holding 10,000 barrels. These were followed by a series of other buildings specifically designed for particular uses. From the start, the site was fully integrated with the adjacent St Budeaux Royal Powder Works on Kinterbury Creek (established in 1805), where damaged powder was treated before being passed on to Bull Point for storage.\n\nThe buildings are mostly still in place within the MOD Bull Point RNAD site: all of one style, mostly ashlar with rock-faced dressings, they are said by English Heritage to comprise \"both the finest ensemble in any of the Ordnance Yards and a remarkable example of integrated factory planning of the period\".\n\nBuilding work on the Square Tower, Portsmouth, started in 1494; and from the end of the 16th century until 1779 it was used as a powder magazine, with a capacity of 12,000 barrels of gunpowder.\n\nThe inhabitants of Portsmouth petitioned the Master General of the Ordnance in 1716 to remove the gunpowder, as they were worried about the hazards it posed to the town, but nothing was done at that time. A further petition was sent to the Board of Ordnance in 1767 following an explosion which caused extensive damage. This led to the construction of the Priddy's Hard magazine at Gosport (see below), in a remote area, across the water from Portsmouth.\n\nThe Square Tower still exists. After 1779 it was used for other purposes; including employment as a semaphore station in 1817.\n\nPriddy's Hard began life as Priddy's Hard Fort; however in 1768 King George III authorised the construction of a gunpowder magazine inside the ramparts to avoid having to store gunpowder in the Square Tower, Portsmouth. Construction was begun in 1771 and the magazine was in use by 1777. A cooperage and shifting house were built alongside at the same time, together with a 'rolling way' for moving gunpowder barrels between the magazine and a nearby wharf; together with the Magazine they are all Grade I listed structures.\n\nBoth the fort and the magazine came under the control of the Board of Ordnance until 1855; control passing, first to the War Office, and then the Admiralty in 1891. Priddy's Hard became a Naval Armaments Depot, finally closing in 1977. The magazine now forms part of the Explosion! Museum of Naval Firepower.\n\nIn the 1780s the Duke of Richmond, Master General of the Ordnance, began to implement a policy of dividing gunpowder stored at the principal coastal locations, and storing it across several sites in the vicinity (to make it less vulnerable to a targeted attack). At Portsmouth, this led to the building of a new magazine at Tipner Point in 1796-8. A second magazine was added in 1856 (part of a policy of expansion following the Crimean War); both still stand flanked by two buildings, the former cooperage and the shifting house, which, along with the magazines, are listed buildings. The magazines remained in use until the 1950s. The surrounding land is earmarked for future redevelopment as part of the Tipner Regeneration scheme.\n\nAnother magazine depot for Portsmouth was established at Marchwood, where three magazines were built in 1814–16 to an innovative design by Sir William Congreve. Movement of gunpowder barrels within the complex was by canal. Four more magazines were added in 1856, and by 1864 Marchwood was Britain's largest magazine complex with capacity for 76,000 barrels.\n\nTwo magazines have survived (one of 1814–16, one of 1856) along with some ancillary buildings (one of which is now home to Marchwood Yacht Club). None of the other magazines has survived, mainly due to the damage that was sustained across the site during The Blitz. The depot closed in 1961; the site is now primarily a residential area.\n\nMagazine Cottage in Sedgeford was built during the 17th century by the Le Strange Family as a gunpowder store during the English Civil War. It is now a residential house and a landmark for the many walkers of the ancient Roman road Peddars Way; it is said that a secret passageway led from the house to the coast.\n\nIn 1668, following the Dutch Raid on the Medway, Upnor Castle was reassigned from serving as an artillery fort to be 'a Place of Store and Magazine'. Thenceforward, barrels of gunpowder were transferred to Upnor, primarily from the Tower of London. The castle was recognized as unsuitable for this role as early as 1808 when a new magazine (since demolished) was built on an adjacent site; another, of similar design, was added in 1857. The latter, which still stands, is described as 'a particularly fine magazine building of the 1850s, distinguished by its historicist style' and 'the most impressive example of a magazine using the catenary arch system'.\nIn 1877, five more new magazines were built inland at Chattenden (the two sites being linked by a railway). Still more magazines were built close by at Lodge Hill, from 1898, primarily for storing the recently developed explosive cordite. Upnor, Chattenden and Lodge Hill depots remained in military ownership until the mid-2010s, when the MOD marketed the land for housing and commercial use.\n\nGunpowder magazines still survive at the Royal Gunpowder Factory, Waltham Abbey, including its Grand Magazine, first constructed in 1804 and rebuilt in 1867–68.\n\nThe former Ordnance Depot at Weedon Bec includes four magazines dating from 1806–1810, along with another built in 1857. The magazines stand in their own compound apart from the main storehouses within a containing wall. Each magazine is separated from its neighbour by an earth-filled 'traverse' building, designed to absorb the impact of an explosion – the first time large magazines had been provided with traverses. Like all the main buildings at Weedon, the magazines lie along the bank of a branch of the Grand Union Canal for ease of transport. In 1827 the four magazines contained 10,500 barrels of powder, along with 1,463,700 ball and 693,746 blank cartridges.\n\nThe remains of old storage magazines are prominent in the landscape around the old Nobel's Explosives site in Ayrshire, many protected by large earth banks which acted as blast walls; these are not all gunpowder magazines, as the site has long been associated with other explosives, particularly dynamite and ballistite.\n\nA gunpowder magazine was located near the site of the Low Well in the village of Barkip, also known as The Den, near Beith, North Ayrshire.\n\nAn explosives magazine at the old Hessilhead limestone Quarry near Beith in North Ayrshire had a small section for blasting caps and a larger section for the sticks of Dynamite.\n\nA restored powder house at Culzean Castle stands close to the sea cliffs. It was used to store gunpowder for the battery and for the 8am daily cannon shot.\n\nDockra limestone quarry lies between Barrmill, Broadstone and Gateside and had two gunpowder magazines; the older one was built some distance from the works. The quarries closed before WW2.\nDumbarton Castle contained two powder magazines; both located high up on Dumbarton Rock. The oldest went out of use in 1748, being replaced by a new Magazine designed by William Skinner. The new magazine, located on \"The Beak\", has a barrel-vaulted roof, with double doors and indirect ventilation. It was designed to hold 150 barrels.\n\nFort George was built between the end of the Jacobite rebellion of 1745 and 1769. The \"Grand Magazine\" was designed to hold 2,500 barrels of gunpowder. It was constructed between 1757 and 1759; and was built strong enough to withstand a direct hit from a mortar. It has a slate roof laid on brick vaults, which sit on stone pillars. To prevent sparks, no iron fittings are used in the magazine: the wooden floor is held by wooden dowels; and the doors and shutters sheathed with copper sheet.\n\nThe Pouther (Scots for Powder) House in Irvine (Map reference: NS 3238 3847), North Ayrshire, Scotland is a rare survival and was possibly first constructed in 1642, as records show that orders for large quantities of gunpowder were met in 1643, 1644, and 1646. James VI, of Scotland, had instructed that all Royal burghs should have powder magazines. The saltpetre derived from deposits in byres, stables and doocots would be stored in the Powder House.\n\nPlans for rebuilding it were made in 1781, at the time of the Napoleonic Wars, and accomplished by 1801; its use was discontinued in 1880. The last use of the building was by Davidson the Ironmonger who stored carbide here for the miners. When the Golffields wash-house was demolished in 1924, its slates were saved by Provost R M Hogg for restoration of the Powder House, a rescue assisted by Rev. Ranken of the Old Parish Church. It was repaired in 1961 and again in 1992 by Irvine Development Corporation. It is an attractive and well built octagonal building topped by a weather cock.\n\nThe 1870 print shows that it was placed in a remote situation, a golf-course being developed around it in later years and when this closed it remained, still fairly remote, in a small park next to the old manse. Ironically, Irvine is close to the site of the old Nobel ICI explosives plant at Ardeer, which from the mid-1930s become the centre of gunpowder manufacture in Britain; and was the last site in Britain to manufacture gunpowder.\n\nAn unusual example exists in East Ayrshire, Scotland at Knockinglaw (now Knockenlaw mound); it is shown on the 1896 OS and still exists in very poor condition . It is near Little Onthank on the outskirts of Kilmarnock, and was originally a tumulus in which urns had been found. A powder magazine was built into this large pre-existing earth mound at an unknown date and the site is now in a housing scheme.\n\nGunpowder magazines survive at the following locations, among others:\n\n\n\n"}
{"id": "1543303", "url": "https://en.wikipedia.org/wiki?curid=1543303", "title": "Handcar", "text": "Handcar\n\nA handcar (also known as a pump trolley, pump car, jigger, Kalamazoo, velocipede, or draisine) is a railroad car powered by its passengers, or by people pushing the car from behind. It is mostly used as a maintenance of way or mining car, but it was also used for passenger service in some cases. A typical design consists of an arm, called the walking beam, that pivots, seesaw-like, on a base, which the passengers alternately push down and pull up to move the car.\n\nIt is not clear who invented the handcar, also written as hand car or hand-car. It is likely that machinists in individual railroad shops began to build them to their own design. Many of the earliest ones operated by turning large cranks. It is likely that the pump handcar, with a reciprocating walking beam, came later. While there are hundreds of US patents pertaining to details of handcars, probably the primary designs of mechanisms for powering handcars were in such common use that they were not patentable when companies started to manufacture handcars for sale to the railroads.\n\nHandcars were absolutely essential to the operation of railroads during a time when railroads were the primary form of public transportation for people and goods in America, from about 1850 to 1910. There may have been handcars as early as the late 1840s but they were quite common during the American Civil War. They were a very important tool in the construction of the Transcontinental Railroad. There were many thousands of them built. They were commonly assigned to a \"section\" of track, the section being between about 6 to 10 miles long, depending upon the traffic weight and locomotive speed experienced on the section. Each section would have a section crew that would maintain that piece of track. Each section usually had a section house which was used to store tools and the section's handcar. Roughly 130,000 miles of track had been constructed in America by 1900. Thus, considering there was a handcar assigned to at least every ten miles of that track, there would have been a minimum of 13,000 handcars operating in the United States. This number is obviously a gross underestimate because many sections were shorter than 10 miles and railroads also had spare handcars for use in unusual circumstances. Telegraph company Western Union and other rail-users had their own handcars, adding to the overall handcar population.\n\nThe first handcars, built in the railroad shops, were probably made of whatever parts the shops had around or could easily make. These cars were probably quite heavy. Heavy handcars need more people to propel them. More people will add more power but at some point the benefits are offset by the weight of the people: their own weight would not be compensated by any extra power they can produce. Many companies made handcars in the years following the American Civil War as evidenced by the number of advertisements in contemporary publications such as The Car Builder's Dictionary. By the mid 1880s The Sheffield Velocipede Car Company, The Kalamazoo Velocipede Company and the Buda Foundry and Manufacturing Company were the three large companies who were the primary builders of handcars. Sheffield was almost immediately acquired by industrial giant Fairbanks Morse. All three companies changed their names over the years but for most of the years that they produced handcars, they were still identified as Sheffield, Kalamazoo and Buda. Hand cars continued to be available through the first half of the 20th Century. Fairbanks Morse was still offering a handcar from their catalog as late as 1950 and Kalamazoo sold them until at least 1955.\n\nWhile depictions on TV and in movies might suggest that being a member of a handcar crew is a joyride, in fact pumping a traditional handcar with bronze bearings rather than modern roller bearings can be very hard work. The disagreeable nature of this experience must have been heightened by the dead weight of typical section crew supplies such as railroad spikes, track nuts and bolts, shovels, pry bars of various sorts and other iron and steel equipment.\n\nMotor section cars began to appear in the very early 1900s, or a few years earlier. They quickly replaced most of the handcars. Those handcars that were not scrapped during World War I, were probably scrapped during World War II. It is not clear how many handcars survived. They can be found in railroad museums and some are in private hands.\n\nHandcars have been normally used by railway service personnel (the latter also known as Gandy dancers) for railroad inspection and maintenance. Because of their low weight and small size, they can be put on and taken off the rails at any place, allowing trains to pass. Handcars have since been replaced by self-propelled vehicles that do not require the use of manual power, instead relying on internal combustion engines or electricity to move the vehicle.\n\nHandcars are nowadays used by handcar enthusiasts at vintage railroad events and for races between handcars driven by five person teams (one to push the car from a halt, four to pump the lever). One such race, the Handcar Regatta, was held in Santa Rosa, California from 2008 to 2011 and other races are held in Australia. See the section on racing below. Aside from handcars built for racing, new handcars are being built with modern roller bearings and milled axles and crankshafts.\n\nIn Australia, hand cars or pump carts are commonly referred to as Kalamazoos after the Kalamazoo Manufacturing Company, which provided many examples to the Australian railway market. Many Kalamazoos are preserved in Australia, some even being used for races.\n\nThere is a push car service along the railroad tracks between Anguiatú in Guatemala and rural towns across the Salvadoran border. Sometimes it is pulled by a horse.\n\nIn Japan, dozens of commercially operated handcar railway lines, called or existed in early 20th century. Those lines were purely built for its passenger/freight service, and \"drivers\" pushed small train cars all the way. The first line, Fujieda-Yaizu Tramway, opened in 1891, and most of the others opened before 1910. Most lines were very short with less than 10 km lengths, and the rail gauges used were either or .\nAs the human-powered system was fairly inefficient, many handcar tramways soon changed their power resources to either horse or gasoline. The system was not strong against a competition with other modes of transport, such as trucks, horses, buses, or other railways. Taishaku Handcar Tramway ceased its operation as early as 1912, and almost all the lines were already closed before 1945.\n\n\nIn Taiwan, commercially operated handcars were called either light railway line (Traditional Chinese: 輕便線; Hanyu Pinyin: qīngbiàn-xiàn), hand-pushed light railway line (手押輕便線; shǒuyā qīngbiàn-xiàn), hand-pushed tramway (手押軌道; shǒuyā guǐdào), or most commonly, hand-pushed wagon (手押臺車; shǒuyā táichē). The first line was built in the 1870s. The network developed later under Japanese rule. In 1933, its peak, there were more than 50 lines in the island with 1,292 km network, transporting local passengers, coal, factory products, sugar, salt, bananas, tea leaves, and others. Most lines, excluding those in mines and isolated islands, have disappeared after Japanese have left. However, a few lines survived well until the 1970s. Currently, only the sightseeing line in Wūlái still exists, although its line is not human-powered anymore.\n\nHandcars are a recurring plot device of twentieth century film, both comedy and drama.\n\nThe Canadian Championship Handcar Races are held annually at the Palmerston Railway Heritage Museum (formerly the old Palmerston CNR station) in Palmerston, Ontario, Canada each June. These races began in 1992 and have been running since.\n\nAn annual handcar race, Dr. E. P. Kitty's Wunderkammer, featuring the Great Sonoma County Handcar Races (formerly known as The Hand-car Regatta), is held in the rail-yard in old downtown Santa Rosa, California.\n\nA multi-faceted festival, it was centered in races of numerous widely varying human-powered vehicles operating on railroad tracks. These included traditional hand-powered carts and others powered by pedals or pushing.\n\nA similar race is happening in the nearby Northern California town of Willits, California, on Sept. 8 and 9, 2012.\nOther races are held in Australia, some using preserved old handcars. See the reference above discussing Kalamazoos in Australia.\n\n\n"}
{"id": "48658919", "url": "https://en.wikipedia.org/wiki?curid=48658919", "title": "Hydrazinium", "text": "Hydrazinium\n\nHydrazinium is the cation with the formula . It can be derived from hydrazine by protonation (treatment with a strong acid). Hydrazinium is a weak acid with pK = 8.1.\n\nSalts of hydrazinium are common reagents in chemistry and are often used in certain industrial processes. Notable examples are hydrazinium hydrogensulfate, or , and hydrazinium azide, or . In the common names of such salts, the cation is often called \"hydrazine\", as in \"hydrazine sulfate\" for hydrazinium hydrogensulfate.\n\nThe terms \"hydrazinium\" and \"hydrazine\" may also be used for the doubly protonated cation , more properly called hydrazinediium or hydrazinium(2+). This cation has an ethane-like structure. Salts of this cation include hydrazinediium sulfate and hydrazinediium bis(6-carboxypyridazine-3-carboxylate), .\n\n"}
{"id": "32496538", "url": "https://en.wikipedia.org/wiki?curid=32496538", "title": "Hydrocycle", "text": "Hydrocycle\n\nA hydrocycle is a bicycle-like watercraft. The concept was known in the 1870s under the title 'water velocipede' and the name was in use by the late 1890s.\n\nPower is collected from the rider via a crank with pedals, as on a bicycle, and delivered to the water or the air via a propeller. Seating may be upright or recumbent, and multiple riders may be accommodated in tandem or side-by-side.\n\nBuoyancy is provided by two or more pontoons or a single surfboard, and some have hydrofoils that can lift the flotation devices out of the water.\n\nBrands include Seacycle, Hydrobike, Water Bike, Seahorse (Cross Trek) and itBike. Kits exist to temporarily convert an existing bicycle into a hydrocycle.\n\n"}
{"id": "7270813", "url": "https://en.wikipedia.org/wiki?curid=7270813", "title": "Kohn effect", "text": "Kohn effect\n\nThe Kohn effect is a dispersion of phonons from the Fermi surface, named for Walter Kohn.\n"}
{"id": "32485414", "url": "https://en.wikipedia.org/wiki?curid=32485414", "title": "Maine Energy Systems", "text": "Maine Energy Systems\n\nMaine Energy Systems (MESys) was founded in summer 2008 by Les Otten and others to aid in the transition to alternative energy in the northeastern United States. The company delivers wood pellets in bulk and sells fully automated wood pellet boilers for hydronic heating. MESys has been involved in numerous academic studies, work with political groups concerned with the environmental and economic aspects of residential and light commercial heating, and works with American regulatory bodies concerned with the safety of heating appliances.\n\n80% of Maine residences are heated with fuel oil. In 2008 during a period of extraordinarily high oil prices former Maine Governor John Baldacci formed The Governor's Wood to Energy Task Force in order to determine the viability of biomass, and wood pellets in particular, as a fuel source for heating in Maine.\n\nThe Governor's Wood to Energy Task Force in 2008 published clear estimates by the Maine Forest Service that the forests of Maine are under-utilized and 5.8 million green tons per year could be sustainably harvested in Maine in addition to the 18.6 million tons per year currently harvested. The Maine Forest Service findings went on to report that an additional 3.8 million green tons per year could be harvested sustainably from New Hampshire and Massachusetts. If 10% of Maine's residences were converted to wood pellet fuel, it would require ~650,000 tons of green wood per year making large scale wood pellet heating a possibility in Maine. It is MESys's goal to make wood pellet fuel a primary heat source for a large number of residences in Maine.\n\nCalculations on the economic impact of foreign oil on the Maine economy made by Co-Director and economist Dr. William \"Bill\" Strauss have been published. Bill Strauss' work \"How to Cure Maine’s Addiction to Heating Oil\" suggests that too much money is leaving the region as a result of foreign heating oil and that keeping some or all of the heating fuel revenues in the region through the use of wood pellet fuel would remedy this drain on the economy.\n\nMuch of Maine Energy Systems work to make wood pellet heating more prevalent in the Northeast has been with regulatory agencies. At the time MESys was founded very few regulations existed which directly referred to Wood Pellet boilers in Maine and the surrounding regions. As a result, rules and regulations created to regulate the installation and use of cordwood and coal stoker boilers were frequently applied to pellet boilers. Co-Owner Dr. Harry \"Dutch\" Dresser Jr. has been instrumental in helping to achieve parity in heating regulations between wood pellet heating appliances and conventional liquid and gaseous fuel appliances in some jurisdictions. Regulations which directly address wood pellet heat have existed for decades in various countries in Europe; the majority of new construction in Upper Austria is heated with wood pellet boilers. As wood pellet boilers gain use in the United States it is necessary that regulations be devised which directly address appropriate wood pellet boiler use. Currently the lack of wood pellet fuel based appliance regulations requires regulatory agencies to hold wood pellet boilers to standards established for a different solid fuel source, such as cordwood, even though the manually fed operation of a cordwood boiler bears no resemblance to the operation of an automatic wood pellet boiler. Regulatory changes that have been made to directly address wood pellet boilers through Maine Energy Systems involvement include the following:\n\nThe Maine DEP has found that some wood pellet boilers are suitable for EPA Phase II certification based on conversion of European test results achieved under EN 303-5 test standards. Vermont DEC certification is nearing completion as well at the time of this writing (August 2011) on the same basis . Use of these test results is forward looking, as US agencies are frequently reluctant to use non-US test standards. The testing considered for these certifications focuses primarily on particulate emissions for boilers installed in unoccupied buildings. The boilers issued certification have particulate emissions values nearly ten times lower than the US EPA requirements.\n\nThe Maine Fuels Board has spent nearly a year considering many of its solid fuel-related installation codes with the help of pellet boiler practitioners. The Board has recognized the actual attributes of modern pellet-fired boiler systems and is proposing modifications to its installation regulations that will bring virtual parity between the rules for automatic pellet fueled boilers and conventional liquid and gaseous fuel boilers.\n\nThe carbon neutrality of wood pellet fuel has been the focus of academic discussions recently. A study by the Manomet group proposed a model on the effects of biomass energy on atmospheric carbon levels. With this model a carbon debt occurs at the time of harvest and a carbon dividend occurs as new trees replace the harvested ones. This model describes wood pellet heat as a less than carbon neutral energy source. MESys co-director and economist Bill Strauss has written a rebuttal to this study which was published in the July 2011 edition of Biomass Power & Thermal Magazine, claiming that there are other methods of modeling biomass energy and that they were not well enough explored in the Manomet study. Strauss claims that the chosen Manomet model ignores the carbon dividend accumulated prior to harvest by the trees being harvested.\n\nMESys sponsored a Worcester Polytechnic Institute student Major Qualifying Project to find and design the best method for automated ash removal for a wood pellet boiler. The project used Axiomatic design as well as conventional design methods to fully explore the functions required of an ash removal system. Two systems were prototyped and tested; a screw conveyor system and a novel cyclonic separator and vacuum system.\n\nMaine Energy Systems' involvement in the transition to wood pellet heat has required the development of global relationships with European businesses that have successful experience in the wood pellet heat industry. This year Maine Energy Systems directors Dutch Dresser and Bill Strauss were keynote speakers at The 2011 World Sustainable Energy Days Conference in Wels, Upper Austria.\n\nMESys relations with Austrian business Tropper allowed the first fully pneumatic bulk pellet delivery truck in the United States to be built and used in Maine.\n\nSince 2008 Maine Energy Systems has offered free training to properly licensed technicians who wish to install and service wood pellet boilers. Training currently focuses on the Austrian OkoFEN AutoPellet pellet boiler and the Auto-Pellet Air hot air furnace, although in the past contractors were trained on the Swedish Janfire NH wood pellet burner.\n\n"}
{"id": "13973150", "url": "https://en.wikipedia.org/wiki?curid=13973150", "title": "Materials science in science fiction", "text": "Materials science in science fiction\n\nMaterials science in science fiction is the study of how materials science is portrayed in works of science fiction. The accuracy of the materials science portrayed spans a wide range – sometimes it is an extrapolation of existing technology, sometimes it is a physically realistic portrayal of a far-out technology, and sometimes it is simply a plot device that looks scientific, but has no basis in science. Examples are:\n\nCritical analysis of materials science in science fiction falls into the same general categories. The predictive aspects are emphasized, for example, in the motto of the Georgia Tech's department of materials science and engineering – \"Materials scientists lead the way in turning yesterday's science fiction into tomorrow's reality\". This is also the theme of many technical articles, such as \"Material By Design: Future Science or Science Fiction?\", found in IEEE Spectrum, the flagship magazine of Institute of Electrical and Electronics Engineers.\n\nOn the other hand, there is criticism of the unrealistic materials science used in science fiction. In the professional materials science journal JOM, for example, there are articles such as \"The (Mostly Improbable) Materials Science and Engineering of the Star Wars Universe\" and \"Personification: The Materials Science and Engineering of Humanoid Robots\".\n\nIn many cases, the materials science aspect of a fictional work was interesting enough that someone other than the author has remarked on it. Here are some of these examples, and their relationship to the real world materials science usage, if any.\n\n\n"}
{"id": "1206615", "url": "https://en.wikipedia.org/wiki?curid=1206615", "title": "Micro pitting", "text": "Micro pitting\n\nMicro pitting is a fatigue failure of the surface of a material commonly seen in rolling bearings and gears.\nIt is also known as grey staining, micro spalling or frosting.\n\nThe difference between pitting corrosion and micropitting is the size of the pits after surface fatigue. Pits formed by micropitting are approximately 10-20 μm in depth, and micropitted metal often has a frosted or gray appearance. Normal pitting creates larger and more visible pits. Micropits are originated from the local contact of asperities produced by improper lubrication.\n\nIn a normal bearing the surfaces are separated by a layer of oil, this is known as elastohydrodynamic (EHD) lubrication. If the thickness of the EHD film is of the same order of magnitude as the surface roughness, the surface topography is able to interact and cause micro pitting. A thin EHD film may be caused by excess load or temperature, a lower oil viscosity than is required, low speed or water in the oil. Water in the oil can make micro pitting worse by causing hydrogen embrittlement of the surface. Micro pitting occurs only under poor EHD lubrication conditions.\n\nA surface with a deep scratch might break exactly at the scratch if stress is applied. One can imagine that the surface roughness is a composite of many very small scratches. So high surface roughness decreases the stability on heavy stressed parts. To get a good overview of the surface an areal scan (Surface metrology) gives more information that a measurement along a single profile (profileometer). To quantify the surface roughness the ISO 25178 can be used.\n\n"}
{"id": "11042923", "url": "https://en.wikipedia.org/wiki?curid=11042923", "title": "Monolithic column", "text": "Monolithic column\n\nA monolithic column or single-piece column is a large column of which the shaft is made from a single piece of stone instead of in vertical sections. Smaller columns are very often made from single pieces of stone, but are less often described as monolithic, as the term is normally reserved for less common, larger columns made in this way. Choosing to use monolithic columns produces considerable extra difficulties in quarrying and transport, and may be seen as a statement of grandeur and importance in a building.\n\nMonolithic columns are characteristic of Ancient Egyptian temples, and the examples in the portico of the Pantheon in Rome were also transported from Egypt.\n\nIn modern architecture using concrete the situation is different, and the term is less likely to be used in this context.\n"}
{"id": "393035", "url": "https://en.wikipedia.org/wiki?curid=393035", "title": "NRX", "text": "NRX\n\nNRX (National Research Experimental) was a heavy-water-moderated, light-water-cooled, nuclear research reactor at the Canadian Chalk River Laboratories, which came into operation in 1947 at a design power rating of 10 MW (thermal), increasing to 42 MW by 1954. At the time of its construction it was Canada's most expensive science facility and the world's most powerful nuclear research reactor.\nNRX was remarkable both in terms of its heat output and the number of free neutrons it generated. When a nuclear reactor is operating its nuclear chain reaction generates many free neutrons, and in the late 1940s NRX was the most intense neutron source in the world.\n\nNRX experienced one of the world's first major reactor accidents on 12 December 1952. The reactor began operation on 22 July 1947 under the National Research Council of Canada, and was taken over by Atomic Energy of Canada Limited (AECL) shortly before the 1952 accident. The accident was cleaned up and the reactor restarted within two years. NRX operated for 45 years, being shut down permanently on 30 March 1993. It is currently undergoing decommissioning at the Chalk River Laboratories site.\n\nNRX was the successor to Canada's first reactor, ZEEP. Because the operating life of a research reactor was not expected to be very long, in 1948 planning started for construction of a successor facility, the National Research Universal reactor, which went critical in 1957.\n\nA heavy water moderated reactor is governed by two main processes. First, the water slows down (moderates) the neutrons which are produced by nuclear fission, increasing the chances of the high energy neutrons causing further fission reactions. Second, control rods absorb neutrons and adjust the power level or shut down the reactor in the course of normal operation. Either inserting the control rods or removing the heavy water moderator can stop the reaction.\n\nThe NRX reactor incorporated a calandria, a sealed vertical aluminium cylindrical vessel with a diameter of 8 m and height of 3 m. The core vessel held about 175 six-centimetre-diameter vertical tubes in a hexagonal lattice, 14,000 litres of heavy water and helium gas to displace air and prevent corrosion. The level of water in the reactor could be adjusted to help set the power level. Sitting in the vertical tubes and surrounded by air were fuel elements or experimental items. This design was a forerunner of the CANDU reactors.\n\nThe fuel elements contained fuel rods 3.1 m long, 31 mm in diameter and weighing 55 kg, containing uranium fuel and sheathed in aluminium. Surrounding the fuel element was an aluminium coolant tube with up to 250 litres per second of cooling water from the Ottawa River flowing through it. Between the coolant sheath and the calandria an air flow of 8 kg/second was maintained.\n\nTwelve of the vertical tubes contained control rods made of boron carbide powder inside steel tubes. These could be raised and lowered to control the reaction, with any seven inserted being enough to absorb sufficient neutrons that no chain reaction could happen. The rods were held up by electromagnets, so that a power failure would cause them to fall into the tubes and terminate the reaction. A pneumatic system could use air pressure from above to quickly force them into the reactor core or from below to slowly raise them from it. Four of these were called the \"safeguard bank\" while the other eight were controlled in an automatic sequence. Two pushbuttons on the main panel in the control room activated magnets to seal the rods to the pneumatic system, and the pushbutton to cause the pneumatic insertion of the rods into the core was located a few feet away.\n\n NRX was for a time the world's most powerful research reactor, vaulting Canada into the forefront of physics research. Emerging from a World War II cooperative effort between Britain, the United States, and Canada, NRX was a multipurpose research reactor used to develop new isotopes, test materials and fuels, and produce neutron radiation beams, that became an indispensable tool in the blossoming field of condensed matter physics.\n\nThe nuclear physics design of NRX emerged from the \"Montreal Laboratory\" of Canada's National Research Council, which was established at the University of Montreal during WWII to engage a team of Canadian, British, and other European scientists in top-secret heavy-water reactor research. When the decision was made to build the NRX at what is now known as Chalk River Laboratories, the detailed engineering design was contracted to Canada's Defense Industries Ltd. (DIL), who subcontracted construction to Fraser Brace Ltd.\n\nIn 1994 Dr. Bertram Brockhouse shared the Nobel Prize in Physics for his work in the 1950s at NRX, which advanced the detection and analysis techniques used in the field of neutron scattering for condensed matter research.\n\nThe CIRUS reactor, based on this design, was built in India. It was ultimately used to produce plutonium for India's Operation Smiling Buddha nuclear test.\n\nIt is claimed that the term \"crud\" originally stood for \"Chalk River Unidentified Deposit\", used to describe the radioactive scaling that builds up on internal reactor components, first observed in the NRX facility. Crud has since become common parlance for \"Corrosion Related Unidentified Deposit\" and similar expressions and is commonly used with no relation to the Chalk River plant.\n\nOn December 12, 1952, the NRX reactor suffered a partial meltdown due to operator error and mechanical problems in the shut-off systems. For test purposes, some of the tubes were disconnected from high pressure water cooling and connected by hoses to a temporary cooling system and one was cooled only by airflow.\n\nDuring tests on low power, with low coolant flux through the core, the supervisor noticed several control rods being pulled from the core, and found an operator in the basement opening pneumatic valves. Wrongly opened valves were immediately closed, but some of the control rods did not reenter the core and stuck in almost withdrawn positions, but still low enough for their status lights to indicate them as lowered. Due to a miscommunication between the supervisor and the control room operator, wrong buttons were pressed when the supervisor asked for lowering the control rods into the core. Instead of sealing the withdrawn control rods to the pneumatic system, the safeguard bank of four control rods was withdrawn from the core. The operator noticed that the power level was exponentially increasing, doubling each 2 seconds, and tripped the reactor. Three of the safeguard control rods however were not inserted into the core and the fourth took an abnormally long time, about 90 seconds, to slide back, while the power kept rising. After just 10 seconds 17 MW were reached. The cooling water boiled in the tubes connected to the temporary cooling system, and some of them ruptured; the positive void coefficient of the reactor led to yet higher power increase rate. About 14 seconds later valves were opened to drain the heavy water from the calandria. As this took some time, power increased for 5 more seconds, peaked at 80 MW, then went down as the moderator level decreased and was at zero 25 seconds later. Meanwhile, some fuel elements melted and the calandria was pierced at several places; helium leaked and air was aspired inside. Hydrogen and other gases evolved by high-temperature reaction of metals with cooling water, and 3–4 minutes later oxyhydrogen exploded in the calandria. During the incident, some gaseous fission products were vented to the atmosphere and heavy water in calandria was contaminated with the cooling water and the fission products.\n\nTo remove decay heat, the water cooling system was kept operating, leaking contaminated coolant to the floor. About of radioactive materials, contained in about a million gallons (about 4000 m) of water, were dumped to the basement of the reactor building during the next few days.\n\nClean-up of the site required several months of work, partially carried out by 150 US Navy personnel who had been training in the area, including future US president Jimmy Carter. The NRX reactor core and calandria, damaged beyond repair, were removed and buried, and an improved replacement was installed; the refurbished reactor was operating again within two years.\n\nThe lessons learned in the 1952 accident advanced the field of reactor safety significantly, and the concepts it highlighted (diversity and independence of safety systems, guaranteed shutdown capability, efficiency of man-machine interface) became fundamentals of reactor design.\n\n\n"}
{"id": "381465", "url": "https://en.wikipedia.org/wiki?curid=381465", "title": "National Energy Program", "text": "National Energy Program\n\nThe National Energy Program (NEP) was an energy policy of the Government of Canada from 1980 to 1985. It was created under the Liberal government of Prime Minister Pierre Trudeau by Minister of Energy Marc Lalonde in 1980, and administered by the Department of Energy, Mines and Resources.\n\nIn his preamble to the announcement of the National Energy Program, introduced as part of the October 1980 federal budget, Finance Minister Allan MacEachen echoed concerns by leaders of developed countries globally regarding the recession following the two oil crises of the 1970s and the \"deeply troubling air of uncertainty and anxiety\" shared by Canadians. The Bank of Canada reported on economic problems that were accelerated and magnified. Inflation was most commonly between 9 and 10 percent annually and prime interest rates over 10 percent.\n\nHistorically, the United States had been by far the world's largest oil producer, and the world oil market had been dominated by a small number of giant multinational (mostly American) oil companies (the so-called \"Seven Sisters of oil\": Standard Oil of New Jersey, alias Exxon (US); Standard Oil of New York, alias Mobil (US/UK); Standard Oil of California, alias Chevron (US), Gulf Oil, now part of Chevron (US); Texaco, now part of Chevron (US); Anglo-Persian Oil Company, alias BP (UK); and Royal Dutch Shell, alias Shell (UK/Netherlands). During the late 1940s, 1950s, 1960s, and early 1970s, the discovery and development of a large number of giant oil and gas fields outside of the United States by these and other companies kept the world flooded with cheap oil. At the same time, global demand increased to take advantage of the increased global supply at lower prices. In particular, American oil consumption increased faster than American production, and the US, which had previously been a net oil exporter, became a major oil importer.\n\nIn 1970, US oil production unexpectedly peaked and started to decline, causing global oil markets to tighten rapidly as the US started to import more and more Arab oil. As the decade continued, global demand caught up with global supply, and there were two major oil price shocks: the 1973 oil crisis and the 1979 oil crisis. The first occurred when the Organization of Arab Petroleum Exporting Countries (OAPEC) (whose membership consists of the Arab members of the similarly-named Organization of Petroleum Exporting Countries (OPEC), plus Egypt, Syria, and Tunisia) imposed an embargo on oil exports to the US, the UK, the Netherlands, Japan, and Canada in retaliation for those countries' support for Israel during the Yom Kippur War. US producers had been able to defeat the previous 1967 oil embargo by ramping up domestic production, flooding the world market with additional product at cut-rate prices, but declining domestic production combined with the ongoing rise in global demand made a similar response to the 1973 Arab embargo impossible. The result was immediate shortages and lineups for gasoline in the importing countries, particularly the US, signaling the end of decades of cheap oil and a change in the balance of power from the consuming countries (which now included the United states) to the producing countries. On October 16, 1973, the Ministerial Committee of the Persian Gulf OPEC membership announced an immediate rise in their posted price from $2.18 to $5.12 per barrel of oil. \"Thus for the first time in oil history, the producing countries assumed power to consider and set the oil price \"unilaterally\", and independently of the\" Seven Sisters.\n\nThe Yom Kippur war ended in October, but the price of oil continued to increase, and by January 1974 it had quadrupled to US$12 a barrel. \"The more than seven-fold increase in the oil price from $1.80/b in 1970 to $13.54/b in 1978 created profound and far reaching changes in the world oil balance, as well as the prevailing relationships among major oil producers, principal oil importers, and the major oil companies ... [and the] spectacular jump of the crude spot price to more than $US40/b following the 1979 Iranian Revolution, turned the global oil market into total disarray.\" Norwegian economics historian Ola Honningdal Grytten described this period in the 1970s as one of a prolonged global recession and slow growth that affected most developed economies.\n\nThe 1979 oil crisis, precipitated by the Iranian Revolution and compounded by the Iran–Iraq War, was the second major market disturbance of the 1970s. \"The curtailment of oil supplies and the skyrocketing of oil prices had far-reaching effects on producers, consumers, and the oil industry itself.\"\n\nIn his State of the Union Address in January 1980 President Jimmy Carter described how United States' \"excessive dependence on foreign oil is a clear and present danger\", and he called for a \"clear, comprehensive energy policy for the United States\".\n\nThe Canadian petroleum industry arose in parallel with that of the United States. The first oil well in North America was dug in Ontario in 1848 using picks and shovels, the year before the first oil well in the United States was drilled in Pennsylvania. By 1870 Canada had 100 oil refineries in operation and was exporting oil to Europe. However, the oil fields of Ontario were shallow and small, and oil production started to decline around 1900, at the same time as the automobile started to become popular. In contrast, oil production in the United States grew rapidly after huge discoveries were made in Texas, Oklahoma, California and elsewhere. By the end of World War II, Canada was importing 90% of its oil, most of it from the United States.\n\nThe situation changed dramatically in 1947, when Imperial Oil drilled a well near Leduc, Alberta, to see what was causing peculiar anomalies on its newly introduced reflection seismology surveys. The peculiar anomalies turned out to be oil fields, and Leduc No. 1 was the discovery well for the first of many large oil fields. As a consequence of these large finds, cheap and plentiful Alberta oil produced a huge surplus of oil on the Canadian Prairies, which had no immediate market since the major oil markets were in Ontario and Quebec. In 1949, Imperial Oil applied to the federal government to build the Interprovincial Pipeline (IPL) to Lake Superior, which allowed it to supply the US Midwestern United States. By 1956 the pipeline was extended via Sarnia, Ontario, to Toronto and became, at the longest oil pipeline in the world. In the other direction, the federal government gave approval to build a pipeline west, and in 1953 the Transmountain Pipeline was built from Edmonton to Vancouver, British Columbia, with an extension to Seattle, Washington. The pipelines did more to improve the energy security of the United States than that of Canada; the Canadian government was more interested in Canada's trade balance than in military or energy security. The Canadian government assumed that eastern Canada could always import enough oil to meet its needs, and that imported oil would always be cheaper than domestic oil.\n\nThe National Energy Board (NEB) was created in 1959 \"to monitor and report on all federal matters of energy as well as regulate pipelines, energy imports and exports and utility rates and tariffs\". The NEB regulated mostly the construction and operation of oil and natural gas pipelines crossing provincial or international borders. The Board approved pipeline traffic, tolls and tariffs under the authority of the National Energy Board Act.\n\nFrom its introduction in 1961 to its end in September 1973, the National Oil Policy (NOP), was the cornerstone of Canadian energy policy. The NOP \"established a protected market for domestic oil west of the Ottawa Valley, which freed the industry from foreign competition\", while the five eastern provinces, which included major refineries in Ontario and Quebec, continued to rely on foreign imports of crude oil, for example from Venezuela. In 1973 \"the federal government announced the extension of the inter-provincial oil pipeline to Montreal (completed in 1976), froze prices of domestic crude and certain oil products, and sought to control export prices. The federal government announced this change in policy so that supply problems in the United States would not automatically raise prices for Canadian consumers.\" After the first OPEC price shock in 1973 the federal government \"formally broke the link between domestic prices and international prices. The objective of 'made-in-Canada' prices for crude oil was to protect Canadians across the country from the whims of the world oil market and to provide producers with enough incentives to develop new energy resources.\"\n\nIn 1981, Edmonton economist Brian Scarfe claimed that the NEB setting the price of oil and natural gas in Canada meant that producers did not receive full world prices for the resource and consumers were not charged world prices. He claimed that these subsidies had a number of side effects, including larger trade deficits, larger federal budget deficits, higher real interest rates, and higher inflation.\n\nIn 1974, Canada inaugurated its first system for pricing oil, with three objectives: to regulate prices of domestic crude oil through federal-provincial agreements, to subsidize imported oil so that consumers in eastern Canada would enjoy lower prices, and to control prices and quantities of crude oil and products in the export market. Synthetic crude oil (upgraded petroleum from oil sands) was exempted from this policy and sold at the world price. The federal government levied a tax on all oil refined in Canada to pay for the difference between the prices of synthetic and conventional crude oil.\n\nThe Canadian federal budget of October 1980 reflected the concern that Canada could \"become increasingly dependent on insecure foreign supplies and, therefore, unnecessarily subject to the vagaries of the world oil market\".\n\nOn 28 October 1980, the Minister of Finance, Allan J. MacEachen, introduced the National Energy Program but cautioned that things could get worse if there were \"new shocks coming from the price of oil or food or if the upward momentum of costs and prices proves impervious to the economic climate I am seeking to create.\" \n\nIn 1975 the federal government created Petro-Canada, a Crown Corporation of Canada and national oil company in response to the world energy crisis. Petro-Canada was involved in the big Hibernia oil find off Newfoundland and was a partner in the Syncrude oil sands venture in Fort McMurray, Alberta. At that time the Alberta oil industry was overwhelmingly owned by Americans. The United States was also the major importer of Albertan oil. The Petro-Canada Centre (1975–2009) was known in the oil patch as \"Red Square\" until it was purchased by Suncor. The NEP included plans for a \"greatly expanded Petro-Canada\"\n\nThe goals of the Program were \"security of supply and ultimate independence from the world oil market; opportunity for all Canadians to participate in the energy industry; particularly oil and gas, and to share in the benefits of its expansion; and fairness, with a pricing and revenue-sharing regime which recognizes the needs and rights of all Canadians\".\n\nThe NEP was designed to promote oil self-sufficiency for Canada, maintain the oil supply, particularly for the industrial base in eastern Canada, promote Canadian ownership of the energy industry, promote lower prices, promote exploration for oil in Canada, promote alternative energy sources, and increase government revenues from oil sales through a variety of taxes and agreements.\n\nThe NEP's Petroleum Gas Revenue Tax (PGRT) instituted a double-taxation mechanism that did not apply to other commodities, such as gold and copper (see \"Program details\" item (c), below). The program would \"... redistribute revenue from the [oil] industry and lessen the cost of oil for Eastern Canada...\" in an attempt to insulate the Canadian economy from the shock of rising global oil prices (see \"Program details\" item (a), below). In 1981 Scarfe argued that by keeping domestic oil prices below world market prices, the NEP was essentially mandating provincial generosity and subsidizing all Canadian consumers of fuel, thanks to Alberta and the other oil producing provinces (such as Newfoundland, which as a result of the NEP received funding for the Hibernia project).\n\nThe National Energy Program \"had three principles: (1) security of supply and ultimate independence from the world market, (2) opportunity for all Canadians to participate in the energy industry, particularly oil and gas, and to share in the benefits of its expansion, and (3) fairness, with a pricing and revenue-sharing regime which recognizes the needs and rights of all Canadians\".\n\n\"The main elements of the program included:\n\n(a) a blended or 'made-in-Canada' price of oil, an average of the costs of imported and domestic oil, which will rise gradually and predictably but will remain well below world prices and will never be more than 85 per cent of the lower of the price of imported oil or of oil in the US, and which will be financed by a Petroleum Compensation Charge levied on refiners...;\n\n(b) natural gas prices which will increase less quickly than oil prices, but which will include a new and rising federal tax on all natural gas and gas liquids;\n\n(c) a petroleum and gas revenue tax of 8 per cent applied to net operating revenues before royalty and other expense deductions on all production of oil and natural gas in Canada...;\n\n(d) the phasing out of the depletion allowances for oil and gas exploration and development, which will be replaced with a new system of direct incentive payments, structured to encourage investment by Canadian companies, with added incentives for exploration on Canada Lands (lands which the federal government held the mineral rights as opposed to private lands and lands which provinces held the mineral rights);\n\n(e) a federal share of petroleum production income at the wellhead which will rise from about 10 per cent in recent years to 24 per cent over the 1980-83 period, with the share of the producing provinces falling from 45 to 43 per cent and that of the industry falling from 45 to 33 per cent over the same period;\n\n(f) added incentives for energy conservation and energy conversion away from oil, particularly applicable to Eastern Canada, including the extension of the natural gas pipe-line system to Quebec City and the maritimes, with the additional transport charges being passed back to the producer; and\n\n(g) a Canadian ownership levy to assist in financing the acquisition of the Canadian operations of one or more multinational oil companies, with the objective of achieving at least 50 per cent Canadian ownership of oil and gas production by 1990, Canadian control of a significant number of the major oil and gas corporations, and an early increase in the share of the oil and gas sector owned by the Government of Canada.\"\n\nIn the early 1980s, the global economy deepened into the worst economic downturn since the Great Depression. Canada, along with all of the economies of Europe (except for Norway due to their petroleum industry) and the economy of the United States, fell into a worldwide recession.\n\n\"National Post\" journalist Jen Gerson would state that \"the NEP was considered by Albertans to be among the most unfair federal policies ever implemented. Scholars calculated the program cost Alberta between $50 and $100 billion.\"\n\nIn 1981, Trudeau and Lougheed signed \"an oil and gas prices and revenue sharing\" agreement, marking an end to \"long bitter dispute\".\nHelliwell et al. (1983) reported that energy price declines of the early 1980s prompted the federal and provincial governments to update their revenue sharing agreements. The amended agreements allowed for $4.2 billion in higher revenues ($1.7 billion federal government, $1.2 billion each for provincial government and industry), which was 30 per cent of the increase that would have been gained from going to world prices. According to Helliwell et al., under the NEP, industry was in fact not significantly exposed to the declining global oil prices but rather the largest part of direct revenue losses accrued to governments, meaning that the industry operated throughout the period of the NEP under relatively similar oil prices, the 'made-in-Canada' price of oil (see item (a) in National Energy Program Details, above).\n\nIn 1981 Edmonton Economist Scarfe argued that the greatest impact was the NEP's failure to deliver the revenues forecast originally in the 1980 federal budget. The 1980 federal government budget introduced by Minister of Finance Allan MacEachen projected a reduction of federal deficits from $14.2 billion in 1980 to $11.8 billion in fiscal 1984 due primarily to substantial increases in revenues from the oil and gas sector while maintaining expenditures. Scarfe speculated the NEP would discourage large-scale oil investment projects and thus reduce these projected revenues. Federal deficits had been expected to decrease primarily due to substantial increases in revenues from the oil and gas sector. Instead, by 1983 the Department of Finance had concluded that the federal government had established a structural deficit of $29.7 billion, an increase from 3.5 per cent of GNP in 1980 to 6.2 per cent of GNP in 1983.\n\nCanada experienced higher inflation, interest rates and underemployment than the United States during this recession. The bank of Canada rate hit 21% in August 1981, and the inflation rate averaged more than 12%. According to the Bank of Canada, during this inflationary period, Canadians sought to protect themselves through investment in the housing market. Some saw an advantage to high interest rates through speculation in real estate and other assets. This increase in transactions was financed through borrowing and ultimately caused debt levels to rise. In the early 1980s, Canada’s unemployment rate peaked at 12%. It took almost four years for the number of full-time jobs to be restored.\n\nAs cited in a report by Phillips, Hager and North, the U.S. Office of the Federal Housing Oversight (OFHEO) reported overall declines in real estate prices of between 10% and 15% from 1980 through 1985. That same report presents information from the Canadian Real Estate Association (CREA) showing that during those years (1980–1985) most eastern Canadian markets fell 10%-15% and the Toronto market held relatively steady. In contrast, the CREA historical data shows a decline from 1980 through to 1985 of approximately 20% for Vancouver, Saskatoon and Winnipeg while the drop approached 40% in the oil dominated economies of Edmonton and Calgary, yet through those years oil prices were still historically high (see figure Long-Term Oil Prices, 1861–2007).\n\nThroughout the 1950s, 1960s, and 1970s, the retail price of petroleum in Canada consistently remained close to the price of gasoline in the United States (and at oftentimes lower than prices seen in the U.S., especially during the price spikes of the 1970s). Following NEP (which raised the price of fuel in the West and coincided with a hike in provincial gas taxes in Ontario and Quebec), the retail price of gasoline in Canada became noticeably higher than in the U.S. (a trend which continues to this day).\n\nIn 1982 during the severe worldwide economic depression, there were over 30,000 consumer bankruptcies in Canada, a 33% increase over the previous year. The bankruptcy rate began to fall from 1983 to 1985 as the economy strengthened. For the period 1980 through 1985, bankruptcies per 1,000 businesses in Canada peaked at 50% above the 1980 rate.\n\nDuring that same time the bankruptcy rate in Alberta's economy rose by 150% after the NEP took effect despite those years being amongst the most expensive for oil prices on record (see figure Long-Term Oil Prices, 1861–2007).\n\nGiven that bankruptcies and real estate prices did not fare as negatively in Central Canada as in the rest of Canada and the United States during the NEP, it is possible that the NEP had a positive effect in Central Canada.\n\nFurthermore, given that bankruptcies and real estate did much worse in Alberta than in other parts of Canada and the United States, petroleum exporting economies like Norway performed well, coupled with the estimated loss of between $50 and $100 billion in provincial GDP (at the time, this was an entire year's GDP for the province) due to the NEP during this period, it is possible that the NEP had a negative effect in Alberta. However, Norway also had a state owned oil company and its version of NEP (government involvement in regulating the industry) during this period (See Norway's Petroleum History, https://www.norskoljeoggass.no/en/Facts/Petroleum-history/). A closer comparison between both national energy programs is required to determine whether Canada's NEP was responsible for Alberta's economic decline during this period and not some other factors. For example, Alberta also began reducing royalty rates beginning in the early 80s and this too may have had a negative economic impact in Alberta.\n\nThe key areas of GDP, per capita federal contributions (since this was a federal program), housing prices and bankruptcy rates during the years of the NEP (1980–1985) are examined in this section. For housing prices and bankruptcy rates, the experience of Alberta in particular is contrasted to the other regions of the country in an attempt to see whether the problems experienced due to the early 1980s recession were worse in Alberta perhaps due to the NEP (or because Alberta began to reduce royalties collected from oil companies).\n\nAlberta GDP was between $60 billion and $80 billion annually through the years of the NEP, 1980 to 1986. While it is unclear whether the estimates took into account the decline in world crude oil prices that began only a few months after the NEP came into force, the graph of long-term oil prices show that prices adjusted for inflation did not drop below pre-1980s levels until 1985. Given that the program was cancelled in 1986, the NEP was active for five years which are amongst the most expensive for oil prices on record and the NEP prevented Alberta's economy from fully realising those prices.\n\nIn inflation adjusted 2004 dollars, the year the NEP took effect (1980) per capita fiscal contributions by Alberta to the federal government increased 77% over 1979 levels - from $6,578 in 1979 to $11,641 in 1980. In the five years prior to the NEP (1975–1979), the per capita contributions by Alberta had approximated the fluctuations in the price of oil (see graph Fluctuations: Oil Prices & Alberta Per Capita Federal Contributions 1975-1981). In 1980, however, the inflation adjusted average price of oil was only 5% higher than the previous year yet the per capita contributions from Alberta rose 77% (see graph Fluctuations: Oil Prices & Alberta Per Capita Federal Contributions 1975-1981).\nAgain in inflation adjusted 2004 dollars, the year the NEP was terminated (1986) per capita contributions to the federal government by Alberta collapsed to $680, a mere 10% of 1979 levels.\n\nDuring the NEP years, 1980–1985, only one other province was a net contributor per capita to the federal government. It was Saskatchewan, another oil producer. In 1980 and 1981 Saskatchewan was a net per capita contributor to the federal government with their peak in 1981 at a mere $514 in comparison to Alberta's peak of $12,735 that same year, both values being 2004 inflation adjusted dollars. Thus, during the NEP years from 1980 to 1985 the province of Alberta was the sole overall net contributor to the federal government while all other provinces enjoyed being net recipients.\n\nIn around 1970 Norway started to become an oil dominated export economy comparable to Alberta. As with most of the world's manufacturing economies, Norway's manufacturing experienced recession beginning in the 1970s. However, in the late 1970s the rise in oil prices saw Norway's oil exports grow and provide the nation with a trade surplus (see figure North Sea Oil Prices and Norway's Trade Balance, 1975–2000).\n\nAccording to Grytten, \"Norway saw de-industrialization at a more rapid pace than most of her largest trading partners. Due to the petroleum sector, however, Norway experienced high growth rates in all the three last decades of the twentieth century, bringing Norway to the top of the world GDP per capita list at the dawn of the new millennium.\"\n\nThus, not all oil based economies suffered as Alberta did during the global slowdown of the early 1980s. Norway experienced an economic boom during the NEP years thanks to the historically high oil prices (see figure Long-Term Oil Prices, 1861–2007). The economic boom of the early 1980s in Norway lasted until the price of oil collapsed in late 1985 just before the NEP was terminated (see figure North Sea Oil Prices and Norway's Trade Balance, 1975–2000).\n\nThe NEP was extremely unpopular in Western Canada, especially in Alberta where most of Canada's oil is produced. With natural resources falling constitutionally within the domain of provincial jurisdictions, many Albertans viewed the NEP as a detrimental intrusion by the federal government into the province's affairs. Edmonton economist Scarfe argued that in Western Canada—and Alberta especially—the NEP was perceived to be at their expense in benefiting the eastern provinces.\nParticularly vilified was Prime Minister Pierre Trudeau, whose Liberals didn't hold a seat west of Manitoba. Ed Clark, a senior bureaucrat in the Trudeau Liberal government, helped develop the National Energy Program earning himself the moniker \"Red Ed\" in the Alberta oil industry. Shortly after Brian Mulroney took office, Clark was fired.\n\nPetro-Canada, established in 1976, was responsible for implementing much of the Program. Petro-Canada was backronymed to \"Pierre Elliott Trudeau Rips Off Canada\" by opponents of the National Energy Program.\n\nAccording to Mary Elizabeth Vicente, an Edmonton librarian who wrote an article on the National Energy Program in 2005, the popular western slogan during the NEP – appearing on many bumper stickers – was \"Let the Eastern bastards freeze in the dark\".\n\nMcKenzie argued in 1981 that politically the NEP heightened distrust of the federal government in Western Canada, especially in Alberta where many Albertans believed that the NEP was an intrusion of the federal government into an area of provincial jurisdiction.\n\nAccording to a National Post journalist,\n\nThe rationale for the program weakened when world oil prices began to slowly decline in the early 1980s and then collapsed in late 1985 (see figure above, \"Long-Term Oil Prices, 1861–2007\"). A phased shutdown was commenced by Jean Chrétien while he was Minister of Energy, Mines and Resources.\n\nIn the 1984 election the Progressive Conservative Party of Brian Mulroney was elected to a majority in the House of Commons with the support of Western Canada after campaigning against the NEP. However, Mulroney did not eliminate the last vestiges of the program until two and a half years later, at which time world oil prices had dropped below pre-1980s levels (as adjusted for inflation - see figure Long-Term Oil Prices, 1861–2007).\n\nOn June 1, 1985, after extensive discussions between the federal Government and the governments of the oil-producing provinces, the \"Western Accord on Energy\" was agreed to. Under the Western Accord, there was full deregulation of oil prices, allowing the market forces of international and local supply and demand determine prices. Included in the full deregulation of domestic oil prices, the Western Accord also \"abolished import subsidies, the export tax on crude and oil products, and the petroleum compensation charge. It also phased out PIP grants and the PGRT. In addition, controls were lifted on oil exports.\"\n\n\n"}
{"id": "32345845", "url": "https://en.wikipedia.org/wiki?curid=32345845", "title": "Nike Laakdal Wind Park", "text": "Nike Laakdal Wind Park\n\nThe Nike Windpark Laakdal is a wind park containing 6 RePower MD77 wind turbines with a capacity of 1.5 MW each. It is located on the site of the Nike corporation in Laakdal, Belgium. The wind turbines have a rotor diameter of 77 meters and are installed on a 111.5 meter high steel framework. Roads capable of carrying heavy trucks run between the legs of some of these steel towers. \n\n"}
{"id": "56208311", "url": "https://en.wikipedia.org/wiki?curid=56208311", "title": "Nkenda–Mpondwe–Beni High Voltage Power Line", "text": "Nkenda–Mpondwe–Beni High Voltage Power Line\n\nThe Nkenda–Mpondwe–Beni High Voltage Power Line is a proposed high voltage electricity power line, connecting the high voltage substation at Nkenda, in Kasese District, in the Western Region of Uganda, to another high voltage substation at Beni, in North Kivu Province, in the Democratic Republic of the Congo.\n\nThe 220kV power line, begins at the substation at Nkenda, in Kasese District, Western Uganda. The line travels in a south-westerly direction through Mpondwe, to Kasindi, in the Democratic Republic of the Congo. There, it takes a general north-westerly course, to end at Beni, North Kivu Province, in the Democratic Republic of the Congo. The distance traveled by this power line in Uganda is approximately . The line travels approximately , in the Democratic Republic of the Congo.\n\nThis power line is planned to transmit electricity to the eastern parts of the Democratic Republic of the Congo, as part the regional power-sharing protocols of the Nile Equatorial Lakes Subsidiary Action Program. Uganda plans to sell electricity to neighboring countries, including the Democratic Republic of the Congo, after Karuma Hydroelectric Power Station and Isimba Hydroelectric Power Station become operational in 2019. The government of the Democratic Republic of the Congo has plans to extend the high-voltage power line to Bunia and Butebo.\n\nThe two governments are in discussions on how to fund the construction of the power line, using loans from the African Development Bank, with each country being responsible for the portion of the line in her territory.\n\n\n"}
{"id": "7180512", "url": "https://en.wikipedia.org/wiki?curid=7180512", "title": "Nonequilibrium Gas and Plasma Dynamics Laboratory", "text": "Nonequilibrium Gas and Plasma Dynamics Laboratory\n\nThe Nonequilibrium Gas and Plasma Dynamics Laboratory (NGPDL) at the Aerospace Engineering Department of the University of Michigan is headed by Professor Iain D. Boyd and performs research of nonequilibrium gases and plasmas involving the development of physical models for various gas systems of interest, numerical algorithms on the latest supercomputers, and the application of challenging flows for several exciting projects. The lab places a great deal of emphasis on comparison of simulation with external experimental and theoretical results, having ongoing collaborative studies with colleagues at the University of Michigan such as the Plasmadynamics and Electric Propulsion Laboratory, other universities, and government laboratories such as NASA, United States Air Force Research Laboratory, and the United States Department of Defense.\n\nCurrent research areas of the NGPDL include electric propulsion, hypersonic aerothermodynamics, flows involving very small length scales (MEMS devices), and materials processing (jets used in deposition thin films for advanced materials). Due to nonequilibrium effects, these flows cannot always be computed accurately with the macroscopic equations of gas dynamics and plasma physics. Instead, the lab has adopted a microscopic approach in which the atoms/molecules in a gas and the ions/electrons in a plasma are simulated on computationally using a large number of model particles within sophisticated Monte Carlo methods. The lab has developed a general 2D/axi-symmetric/3D code, MONACO, for simulating nonequilibrium neutral flows that can run either on scalar workstations or in a parallel computing environment.\n\nThe lab also has developed a general 2D/axi-symmetric/3D code, LeMANS, to numerically solve the Navier-Stokes equations using computational fluid dynamics when the Knudsen number is sufficiently small. This allows lab members to explore flows that would otherwise be too computationally expensive with a particle method. Work is currently being done to combine the two codes into a hybrid that uses MONACO when the flow is in the collisional nonequilibrium regime and LeMANS when the flow can be considered continuous.\n\nCurrent and past plasma and nonequilibrium flow projects include simulation of ion thrusters, Hall effect thrusters, and pulsed plasma thrusters) as well as numerous NASA contracts to study reentry aerothermodynamics for space vehicles, including the Crew Exploration Vehicle. Other plasma research includes modeling wall ablation from directed energy weapons and the plasma-propellant interaction in electrothermal chemical guns.\n\nhttp://ngpdlab.engin.umich.edu/\n"}
{"id": "4222539", "url": "https://en.wikipedia.org/wiki?curid=4222539", "title": "Nuclear safety and security", "text": "Nuclear safety and security\n\nNuclear safety is defined by the International Atomic Energy Agency (IAEA) as \"The achievement of proper operating conditions, prevention of accidents or mitigation of accident consequences, resulting in protection of workers, the public and the environment from undue radiation hazards\". The IAEA defines nuclear security as \"The prevention and detection of and response to, theft, sabotage, unauthorized access, illegal transfer or other malicious acts involving nuclear material, other radioactive substances or their associated facilities\".\n\nThis covers nuclear power plants and all other nuclear facilities, the transportation of nuclear materials, and the use and storage of nuclear materials for medical, power, industry, and military uses.\n\nThe nuclear power industry has improved the safety and performance of reactors, and has proposed new and safer reactor designs. However, a perfect safety cannot be guaranteed. Potential sources of problems include human errors and external events that have a greater impact than anticipated: The designers of reactors at Fukushima in Japan did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake.\nCatastrophic scenarios involving terrorist attacks, insider sabotage, and cyberattacks are also conceivable.\n\nNuclear weapon safety, as well as the safety of military research involving nuclear materials, is generally handled by agencies different from those that oversee civilian safety, for various reasons, including secrecy. There are ongoing concerns about terrorist groups acquiring nuclear bomb-making material.\n\n, nuclear safety considerations occur in a number of situations, including:\n\nWith the exception of thermonuclear weapons and experimental fusion research, all safety issues specific to nuclear power stems from the need to limit the biological uptake of committed dose (ingestion or inhalation of radioactive materials), and external radiation dose due to radioactive contamination.\n\nNuclear safety therefore covers at minimum: –\n\nInternationally the International Atomic Energy Agency \"works with its Member States and multiple partners worldwide to promote safe, secure and peaceful nuclear technologies.\" Some scientists say that the 2011 Japanese nuclear accidents have revealed that the nuclear industry lacks sufficient oversight, leading to renewed calls to redefine the mandate of the IAEA so that it can better police nuclear power plants worldwide.\n\nThe IAEA Convention on Nuclear Safety was adopted in Vienna on 17 June 1994 and entered into force on 24 October 1996. The objectives of the Convention are to achieve and maintain a high level of nuclear safety worldwide, to establish and maintain effective defences in nuclear installations against potential radiological hazards, and to prevent accidents having radiological consequences.\n\nThe Convention was drawn up in the aftermath of the Three Mile Island and Chernobyl accidents at a series of expert level meetings from 1992 to 1994, and was the result of considerable work by States, including their national regulatory and nuclear safety authorities, and the International Atomic Energy Agency, which serves as the Secretariat for the Convention.\n\nThe obligations of the Contracting Parties are based to a large extent on the application of the safety principles for nuclear installations contained in the IAEA document Safety Fundamentals ‘The Safety of Nuclear Installations’ (IAEA Safety Series No. 110 published 1993). These obligations cover the legislative and regulatory framework, the regulatory body, and technical safety obligations related to, for instance, siting, design, construction, operation, the availability of adequate financial and human resources, the assessment and verification of safety, quality assurance and emergency preparedness.\n\nThe convention was amended in 2015 by the Vienna Declaration on Nuclear Safety This resulted in the following principles:\n\n1. New nuclear power plants are to be designed, sited, and constructed, consistent with the objective of preventing accidents in the commissioning and operation and, should an accident occur, mitigating possible releases of radionuclides causing long-term off site contamination and avoiding early radioactive releases or radioactive releases large enough to require long-term protective measures and actions.\n\n2. Comprehensive and systematic safety assessments are to be carried out periodically and regularly for existing installations throughout their lifetime in order to identify safety improvements that are oriented to meet the above objective. Reasonably practicable or achievable safety improvements are to be implemented in a timely manner.\n\n3. National requirements and regulations for addressing this objective throughout the lifetime of nuclear power plants are to take into account the relevant IAEA Safety Standards and, as appropriate, other good practices as identified inter alia in the Review Meetings of the CNS.\n\nThere are several problems with the IAEA, says Najmedin Meshkati of University of Southern California, writing in 2011:\n\n\"It recommends safety standards, but member states are not required to comply; it promotes nuclear energy, but it also monitors nuclear use; it is the sole global organization overseeing the nuclear energy industry, yet it is also weighed down by checking compliance with the Nuclear Non-Proliferation Treaty (NPT)\".\nMany nations utilizing nuclear power have specialist institutions overseeing and regulating nuclear safety. Civilian nuclear safety in the U.S. is regulated by the Nuclear Regulatory Commission (NRC). However, critics of the nuclear industry complain that the regulatory bodies are too intertwined with the industries themselves to be effective. The book \"The Doomsday Machine\" for example, offers a series of examples of national regulators, as they put it 'not regulating, just waving' (a pun on \"waiving\") to argue that, in Japan, for example, \"regulators and the regulated have long been friends, working together to offset the doubts of a public brought up on the horror of the nuclear bombs\". Other examples offered include:\n\n\nThe book argues that nuclear safety is compromised by the suspicion that, as Eisaku Sato, formerly a governor of Fukushima province (with its infamous nuclear reactor complex), has put it of the regulators: “They’re all birds of a feather”.\n\nThe safety of nuclear plants and materials controlled by the U.S. government for research, weapons production, and those powering naval vessels is not governed by the NRC. In the UK nuclear safety is regulated by the Office for Nuclear Regulation (ONR) and the Defence Nuclear Safety Regulator (DNSR). The Australian Radiation Protection and Nuclear Safety Agency (ARPANSA) is the Federal Government body that monitors and identifies solar radiation and nuclear radiation risks in Australia. It is the main body dealing with ionizing and non-ionizing radiation and publishes material regarding radiation protection.\n\nOther agencies include:\n\nNuclear power plants are some of the most sophisticated and complex energy systems ever designed. Any complex system, no matter how well it is designed and engineered, cannot be deemed failure-proof. Veteran journalist and author Stephanie Cooke has argued:\n\nThe reactors themselves were enormously complex machines with an incalculable number of things that could go wrong. When that happened at Three Mile Island in 1979, another fault line in the nuclear world was exposed. One malfunction led to another, and then to a series of others, until the core of the reactor itself began to melt, and even the world's most highly trained nuclear engineers did not know how to respond. The accident revealed serious deficiencies in a system that was meant to protect public health and safety. \nThe 1979 Three Mile Island accident inspired Perrow's book \"Normal Accidents\", where a nuclear accident occurs, resulting from an unanticipated interaction of multiple failures in a complex system. TMI was an example of a normal accident because it was \"unexpected, incomprehensible, uncontrollable and unavoidable\". \n\nPerrow concluded that the failure at Three Mile Island was a consequence of the system's immense complexity. Such modern high-risk systems, he realized, were prone to failures however well they were managed. It was inevitable that they would eventually suffer what he termed a 'normal accident'. Therefore, he suggested, we might do better to contemplate a radical redesign, or if that was not possible, to abandon such technology entirely. \nA fundamental issue contributing to a nuclear power system's complexity is its extremely long lifetime. The timeframe from the start of construction of a commercial nuclear power station through the safe disposal of its last radioactive waste, may be 100 to 150 years.\n\nThere are concerns that a combination of human and mechanical error at a nuclear facility could result in significant harm to people and the environment:\n\nOperating nuclear reactors contain large amounts of radioactive fission products which, if dispersed, can pose a direct radiation hazard, contaminate soil and vegetation, and be ingested by humans and animals. Human exposure at high enough levels can cause both short-term illness and death and longer-term death by cancer and other diseases.\nIt is impossible for a commercial nuclear reactor to explode like a nuclear bomb since the fuel is never sufficiently enriched for this to occur.\n\nNuclear reactors can fail in a variety of ways. Should the instability of the nuclear material generate unexpected behavior, it may result in an uncontrolled power excursion. Normally, the cooling system in a reactor is designed to be able to handle the excess heat this causes; however, should the reactor also experience a loss-of-coolant accident, then the fuel may melt or cause the vessel in which it is contained to overheat and melt. This event is called a nuclear meltdown.\n\nAfter shutting down, for some time the reactor still needs external energy to power its cooling systems. Normally this energy is provided by the power grid to which that plant is connected, or by emergency diesel generators. Failure to provide power for the cooling systems, as happened in Fukushima I, can cause serious accidents.\n\nNuclear safety rules in the United States \"do not adequately weigh the risk of a single event that would knock out electricity from the grid and from emergency generators, as a quake and tsunami recently did in Japan\", Nuclear Regulatory Commission officials said in June 2011.\n\nAs a safeguard against mechanical failure, many nuclear plants are designed to shut down automatically after two days of continuous and unattended operation.\n\nNuclear reactors become preferred targets during military conflict and, over the past three decades, have been repeatedly attacked during military air strikes, occupations, invasions and campaigns:\n\n\nIn the U.S., plants are surrounded by a double row of tall fences which are electronically monitored. The plant grounds are patrolled by a sizeable force of armed guards. In Canada, all reactors have an \"on-site armed response force\" that includes light-armored vehicles that patrol the plants daily. The NRC's \"Design Basis Threat\" criterion for plants is a secret, and so what size of attacking force the plants are able to protect against is unknown. However, to scram (make an emergency shutdown) a plant takes fewer than 5 seconds while unimpeded restart takes hours, severely hampering a terrorist force in a goal to release radioactivity.\n\nAttack from the air is an issue that has been highlighted since the September 11 attacks in the U.S. However, it was in 1972 when three hijackers took control of a domestic passenger flight along the east coast of the U.S. and threatened to crash the plane into a U.S. nuclear weapons plant in Oak Ridge, Tennessee. The plane got as close as 8,000 feet above the site before the hijackers’ demands were met.\n\nThe most important barrier against the release of radioactivity in the event of an aircraft strike on a nuclear power plant is the containment building and its missile shield. Former NRC Chairman Dale Klein has said \"Nuclear power plants are inherently robust structures that our studies show provide adequate protection in a hypothetical attack by an airplane. The NRC has also taken actions that require nuclear power plant operators to be able to manage large fires or explosions—no matter what has caused them.\"\n\nIn addition, supporters point to large studies carried out by the U.S. Electric Power Research Institute that tested the robustness of both reactor and waste fuel storage and found that they should be able to sustain a terrorist attack comparable to the September 11 terrorist attacks in the U.S. Spent fuel is usually housed inside the plant's \"protected zone\" or a spent nuclear fuel shipping cask; stealing it for use in a \"dirty bomb\" would be extremely difficult. Exposure to the intense radiation would almost certainly quickly incapacitate or kill anyone who attempts to do so.\n\nNuclear power plants are considered to be targets for terrorist attacks. Even during the construction of the first nuclear power plants, this issue has been advised by security bodies. Concrete threats of attack against nuclear power plants by terrorists or criminals are documented from several states. While older nuclear power plants were built without special protection against air accidents in Germany, the later nuclear power plants built with a massive concrete buildings are partially protected against air accidents. They are designed against the impact of combat aircraft at a speed of about 800 km / h. It was assumed as a basis of assessment of the impact of an aircraft of type Phantom II with a mass of 20 tonnes and speed of 215 m / s.\n\nThe dangers arising from a terrorist caused large aircraft crash on a nuclear power plant is currently being discussed. Such a terrorist attack could have catastrophic consequences. For example, the German government has confirmed that the nuclear power plant Biblis A not against the crash had secured a military aircraft. Following the terrorist attacks in Brussels in 2016 several nuclear power plants have been partially evacuated. At the same time it became known that the terrorists had spied on the nuclear power plants. Several employees access privileges has been withdrawn.\n\nMoreover, even \"nuclear terrorism\", for instance with a so-called \"Dirty bomb\" pose a considerable potential hazard. For their production would come any radioactive waste or enriched for nuclear power plants uranium in question.\n\nIn many countries, plants are often located on the coast, in order to provide a ready source of cooling water for the essential service water system. As a consequence the design needs to take the risk of flooding and tsunamis into account. The World Energy Council (WEC) argues disaster risks are changing and increasing the likelihood of disasters such as earthquakes, cyclones, hurricanes, typhoons, flooding. High temperatures, low precipitation levels and severe droughts may lead to fresh water shortages. Failure to calculate the risk of flooding correctly lead to a event on the International Nuclear Event Scale during the 1999 Blayais Nuclear Power Plant flood, while flooding caused by the 2011 Tōhoku earthquake and tsunami lead to the Fukushima I nuclear accidents.\n\nThe design of plants located in seismically active zones also requires the risk of earthquakes and tsunamis to be taken into account. Japan, India, China and the USA are among the countries to have plants in earthquake-prone regions. Damage caused to Japan's Kashiwazaki-Kariwa Nuclear Power Plant during the 2007 Chūetsu offshore earthquake underlined concerns expressed by experts in Japan prior to the Fukushima accidents, who have warned of a \"genpatsu-shinsai\" (domino-effect nuclear power plant earthquake disaster).\n\nThe Fukushima nuclear disaster illustrated the dangers of building multiple nuclear reactor units close to one another. Because of the closeness of the reactors, Plant Director Masao Yoshida \"was put in the position of trying to cope simultaneously with core meltdowns at three reactors and exposed fuel pools at three units\".\n\nThe three primary objectives of nuclear safety systems as defined by the Nuclear Regulatory Commission are to shut down the reactor, maintain it in a shutdown condition, and prevent the release of radioactive material during events and accidents. These objectives are accomplished using a variety of equipment, which is part of different systems, of which each performs specific functions.\n\nDuring everyday routine operations, emissions of radioactive materials from nuclear plants are released to the outside of the plants although they are quite slight amounts.\nThe daily emissions go into the air, water and soil.\n\nNRC says, \"nuclear power plants sometimes release radioactive gases and liquids into the environment under controlled, monitored conditions to ensure that they pose no danger to the public or the environment\", and \"routine emissions during normal operation of a nuclear power plant are never lethal\".\n\nAccording to the United Nations (UNSCEAR), regular nuclear power plant operation including the nuclear fuel cycle amounts to 0.0002 millisieverts (mSv) annually in average public radiation exposure; the legacy of the Chernobyl disaster is 0.002 mSv/a as a global average as of a 2008 report; and natural radiation exposure averages 2.4 mSv annually although frequently varying depending on an individual's location from 1 to 13 mSv.\n\nIn March 2012, Prime Minister Yoshihiko Noda said that the Japanese government shared the blame for the Fukushima disaster, saying that officials had been blinded by an image of the country's technological infallibility and were \"all too steeped in a safety myth.\"\n\nJapan has been accused by authors such as journalist Yoichi Funabashi of having an \"aversion to facing the potential threat of nuclear emergencies.\" According to him, a national program to develop robots for use in nuclear emergencies was terminated in midstream because it \"smacked too much of underlying danger.\" Though Japan is a major power in robotics, it had none to send in to Fukushima during the disaster. He mentions that Japan's Nuclear Safety Commission stipulated in its safety guidelines for light-water nuclear facilities that \"the potential for extended loss of power need not be considered.\" However, this kind of extended loss of power to the cooling pumps caused the Fukushima meltdown.\n\nIn other countries such as the UK, nuclear plants have not been claimed to be absolutely safe. It is instead claimed that a major accident has a likelihood of occurrence lower than (for example) 0.0001/year.\n\nIncidents such as the Fukushima Daiichi nuclear disaster could have been avoided with stricter regulations over nuclear power. In 2002, TEPCO, the company that operated the Fukushima plant, admitted to falsifying reports on over 200 occasions between 1997 and 2002. TEPCO faced no fines for this. Instead, they fired four of their top executives. Three of these four later went on to take jobs at companies that do business with TEPCO.\n\nThere is currently a total of 47,000 tonnes of high-level nuclear waste stored in the USA. Nuclear waste is approximately 94% Uranium, 1.3% Plutonium, 0.14% other Actinides, and 5.2% fission products. About 1.0% of this waste consists of long-lived isotopes Se, Zr, Te, Pd, Sn, I and Cs. Shorter lived isotopes including Sr, Sr, Ru, Sn, Cs, Cs, and Pm constitute 0.9% at one year, decreasing to 0.1% at 100 years. The remaining 3.3–4.1% consists of non-radioactive isotopes. There are technical challenges, as it is preferable to lock away the long-lived fission products, but the challenge should not be exaggerated. One tonne of waste, as described above, has measurable radioactivity of approximately 600 TBq equal to the natural radioactivity in one km of the Earth's crust, which if buried, would add only 25 parts per trillion to the total radioactivity.\n\nThe difference between short-lived high-level nuclear waste and long-lived low-level waste can be illustrated by the following example. As stated above, one mole of both I and I release 3x10 decays in a period equal to one half-life. I decays with the release of 970 keV whilst I decays with the release of 194 keV of energy. 131gm of I would therefore release 45 Gigajoules over eight days beginning at an initial rate of 600 EBq releasing 90 Kilowatts with the last radioactive decay occurring inside two years. In contrast, 129gm of I would therefore release 9 Gigajoules over 15.7 million years beginning at an initial rate of 850 MBq releasing 25 microwatts with the radioactivity decreasing by less than 1% in 100,000 years.\n\nOne tonne of nuclear waste also reduces CO emission by 25 million tonnes.\n\nGovernments around the world are considering a range of waste management and disposal options, usually involving deep-geologic placement, although there has been limited progress toward implementing long-term waste management solutions. This is partly because the timeframes in question when dealing with radioactive waste range from 10,000 to millions of years, according to studies based on the effect of estimated radiation doses.\n\nSince the fraction of a radioisotope's atoms decaying per unit of time is inversely proportional to its half-life, the relative radioactivity of a quantity of buried human radioactive waste would diminish over time compared to natural radioisotopes (such as the decay chain of 120 trillion tons of thorium and 40 trillion tons of uranium which are at relatively trace concentrations of parts per million each over the crust's 3 * 10 ton mass). For instance, over a timeframe of thousands of years, after the most active short half-life radioisotopes decayed, burying U.S. nuclear waste would increase the radioactivity in the top 2000 feet of rock and soil in the United States (10 million km) by ≈ 1 part in 10 million over the cumulative amount of natural radioisotopes in such a volume, although the vicinity of the site would have a far higher concentration of artificial radioisotopes underground than such an average.\n\nOne relatively prevalent notion in discussions of nuclear safety is that of safety culture. The International Nuclear Safety Advisory Group, defines the term as “the personal dedication and accountability of all individuals engaged in any activity which has a bearing on the safety of nuclear power plants”. The goal is “to design systems that use human capabilities in appropriate ways, that protect systems from human frailties, and that protect humans from hazards associated with the system”.\n\nAt the same time, there is some evidence that operational practices are not easy to change. Operators almost never follow instructions and written procedures exactly, and “the violation of rules appears to be quite rational, given the actual workload and timing constraints under which the operators must do their job”. Many attempts to improve nuclear safety culture “were compensated by people adapting to the change in an unpredicted way”.\n\nAccording to Areva's Southeast Asia and Oceania director, Selena Ng, Japan's Fukushima nuclear disaster is \"a huge wake-up call for a nuclear industry that hasn't always been sufficiently transparent about safety issues\". She said \"There was a sort of complacency before Fukushima and I don't think we can afford to have that complacency now\".\n\nAn assessment conducted by the \"Commissariat à l’Énergie Atomique\" (CEA) in France concluded that no amount of technical innovation can eliminate the risk of human-induced errors associated with the operation of nuclear power plants. Two types of mistakes were deemed most serious: errors committed during field operations, such as maintenance and testing, that can cause an accident; and human errors made during small accidents that cascade to complete failure.\n\nAccording to Mycle Schneider, reactor safety depends above all on a 'culture of security', including the quality of maintenance and training, the competence of the operator and the workforce, and the rigour of regulatory oversight. So a better-designed, newer reactor is not always a safer one, and older reactors are not necessarily more dangerous than newer ones. The 1979 Three Mile Island accident in the United States occurred in a reactor that had started operation only three months earlier, and the Chernobyl disaster occurred after only two years of operation. A serious loss of coolant occurred at the French Civaux-1 reactor in 1998, less than five months after start-up.\n\nHowever safe a plant is designed to be, it is operated by humans who are prone to errors. Laurent Stricker, a nuclear engineer and chairman of the World Association of Nuclear Operators says that operators must guard against complacency and avoid overconfidence. Experts say that the \"largest single internal factor determining the safety of a plant is the culture of security among regulators, operators and the workforce — and creating such a culture is not easy\".\n\nThe routine health risks and greenhouse gas emissions from nuclear fission power are small relative to those associated with coal, but there are several \"catastrophic risks\":\n\nThe extreme danger of the radioactive material in power plants and of nuclear technology in and of itself is so well known that the US government was prompted (at the industry's urging) to enact provisions that protect the nuclear industry from bearing the full burden of such inherently risky nuclear operations. The Price-Anderson Act limits industry's liability in the case of accidents, and the 1982 Nuclear Waste Policy Act charges the federal government with responsibility for permanently storing nuclear waste.\n\nPopulation density is one critical lens through which other risks have to be assessed, says Laurent Stricker, a nuclear engineer and chairman of the World Association of Nuclear Operators:\n\nThe KANUPP plant in Karachi, Pakistan, has the most people — 8.2 million — living within 30 kilometres of a nuclear plant, although it has just one relatively small reactor with an output of 125 megawatts. Next in the league, however, are much larger plants — Taiwan's 1,933-megawatt Kuosheng plant with 5.5 million people within a 30-kilometre radius and the 1,208-megawatt Chin Shan plant with 4.7 million; both zones include the capital city of Taipei.\n172,000 people living within a 30 kilometre radius of the Fukushima Daiichi nuclear power plant, have been forced or advised to evacuate the area. More generally, a 2011 analysis by \"Nature\" and Columbia University, New York, shows that some 21 nuclear plants have populations larger than 1 million within a 30-km radius, and six plants have populations larger than 3 million within that radius.\n\nBlack Swan events are highly unlikely occurrences that have big repercussions. Despite planning, nuclear power will always be vulnerable to black swan events:\n\nA rare event – especially one that has never occurred – is difficult to foresee, expensive to plan for and easy to discount with statistics. Just because something is only supposed to happen every 10,000 years does not mean that it will not happen tomorrow. Over the typical 40-year life of a plant, assumptions can also change, as they did on September 11, 2001, in August 2005 when Hurricane Katrina struck, and in March, 2011, after Fukushima.\n\nThe list of potential black swan events is \"damningly diverse\":\n\nNuclear reactors and their spent-fuel pools could be targets for terrorists piloting hijacked planes. Reactors may be situated downstream from dams that, should they ever burst, could unleash massive floods. Some reactors are located close to faults or shorelines, a dangerous scenario like that which emerged at Three Mile Island and Fukushima – a catastrophic coolant failure, the overheating and melting of the radioactive fuel rods, and a release of radioactive material.\nThe AP1000 has an estimated core damage frequency of 5.09 x 10 per plant per year. The Evolutionary Power Reactor (EPR) has an estimated core damage frequency of 4 x 10 per plant per year. In 2006 General Electric published recalculated estimated core damage frequencies per year per plant for its nuclear power plant designs:\n\nThe Fukushima I nuclear accident was caused by a \"beyond design basis event,\" the tsunami and associated earthquakes were more powerful than the plant was designed to accommodate, and the accident is directly due to the tsunami overflowing the too-low seawall. Since then, the possibility of unforeseen beyond design basis events has been a major concern for plant operators.\n\nAccording to journalist Stephanie Cooke, it is difficult to know what really goes on inside nuclear power plants because the industry is shrouded in secrecy. Corporations and governments control what information is made available to the public. Cooke says \"when information is made available, it is often couched in jargon and incomprehensible prose\".\n\nKennette Benedict has said that nuclear technology and plant operations continue to lack transparency and to be relatively closed to public view:\n\nDespite victories like the creation of the Atomic Energy Commission, and later the Nuclear Regular Commission, the secrecy that began with the Manhattan Project has tended to permeate the civilian nuclear program, as well as the military and defense programs.\nIn 1986, Soviet officials held off reporting the Chernobyl disaster for several days. The operators of the Fukushima plant, Tokyo Electric Power Co, were also criticised for not quickly disclosing information on releases of radioactivity from the plant. Russian President Dmitry Medvedev said there must be greater transparency in nuclear emergencies.\n\nHistorically many scientists and engineers have made decisions on behalf of potentially affected populations about whether a particular level of risk and uncertainty is acceptable for them. Many nuclear engineers and scientists that have made such decisions, even for good reasons relating to long term energy availability, now consider that doing so without informed consent is wrong, and that nuclear power safety and nuclear technologies should be based fundamentally on morality, rather than purely on technical, economic and business considerations.\n\n\"Non-Nuclear Futures: The Case for an Ethical Energy Strategy\" is a 1975 book by Amory B. Lovins and John H. Price. The main theme of the book is that the most important parts of the nuclear power debate are not technical disputes but relate to personal values, and are the legitimate province of every citizen, whether technically trained or not.\n\nThe nuclear industry has an excellent safety record and the deaths per megawatt hour are the lowest of all the major energy sources. According to Zia Mian and Alexander Glaser, the \"past six decades have shown that nuclear technology does not tolerate error\". Nuclear power is perhaps the primary example of what are called ‘high-risk technologies’ with ‘catastrophic potential’, because “no matter how effective conventional safety devices are, there is a form of accident that is inevitable, and such accidents are a ‘normal’ consequence of the system.” In short, there is no escape from system failures.\n\nWhatever position one takes in the nuclear power debate, the possibility of catastrophic accidents and consequent economic costs must be considered when nuclear policy and regulations are being framed.\n\nKristin Shrader-Frechette has said \"if reactors were safe, nuclear industries would not demand government-guaranteed, accident-liability protection, as a condition for their generating electricity\". No private insurance company or even consortium of insurance companies \"would shoulder the fearsome liabilities arising from severe nuclear accidents\".\n\nThe Hanford Site is a mostly decommissioned nuclear production complex on the Columbia River in the U.S. state of Washington, operated by the United States federal government. Plutonium manufactured at the site was used in the first nuclear bomb, tested at the Trinity site, and in Fat Man, the bomb detonated over Nagasaki, Japan. During the Cold War, the project was expanded to include nine nuclear reactors and five large plutonium processing complexes, which produced plutonium for most of the 60,000 weapons in the U.S. nuclear arsenal. Many of the early safety procedures and waste disposal practices were inadequate, and government documents have since confirmed that Hanford's operations released significant amounts of radioactive materials into the air and the Columbia River, which still threatens the health of residents and ecosystems. The weapons production reactors were decommissioned at the end of the Cold War, but the decades of manufacturing left behind of high-level radioactive waste, an additional of solid radioactive waste, of contaminated groundwater beneath the site and occasional discoveries of undocumented contaminations that slow the pace and raise the cost of cleanup. The Hanford site represents two-thirds of the nation's high-level radioactive waste by volume. Today, Hanford is the most contaminated nuclear site in the United States and is the focus of the nation's largest environmental cleanup.\n\nThe Chernobyl disaster was a nuclear accident that occurred on 26 April 1986 at the Chernobyl Nuclear Power Plant in Ukraine. An explosion and fire released large quantities of radioactive contamination into the atmosphere, which spread over much of Western USSR and Europe. It is considered the worst nuclear power plant accident in history, and is one of only two classified as a level 7 event on the International Nuclear Event Scale (the other being the Fukushima Daiichi nuclear disaster). The battle to contain the contamination and avert a greater catastrophe ultimately involved over 500,000 workers and cost an estimated 18 billion rubles, crippling the Soviet economy.\nThe accident raised concerns about the safety of the nuclear power industry, slowing its expansion for a number of years.\n\nUNSCEAR has conducted 20 years of detailed scientific and epidemiological research on the effects of the Chernobyl accident. Apart from the 57 direct deaths in the accident itself, UNSCEAR predicted in 2005 that up to 4,000 additional cancer deaths related to the accident would appear \"among the 600 000 persons receiving more significant exposures (liquidators working in 1986–87, evacuees, and residents of the most contaminated areas)\". Russia, Ukraine, and Belarus have been burdened with the continuing and substantial decontamination and health care costs of the Chernobyl disaster.\n\nEleven of Russia's reactors are of the RBMK 1000 type, similar to the one at Chernobyl Nuclear Power Plant. Some of these RBMK reactors were originally to be shut down but have instead been given life extensions and uprated in output by about 5%. Critics say that these reactors are of an \"inherently unsafe design\", which cannot be improved through upgrades and modernization, and some reactor parts are impossible to replace. Russian environmental groups say that the lifetime extensions \"violate Russian law, because the projects have not undergone environmental assessments\".\n\nDespite all assurances, a major nuclear accident on the scale of the 1986 Chernobyl disaster happened again in 2011 in Japan, one of the world's most industrially advanced countries. Nuclear Safety Commission Chairman Haruki Madarame told a parliamentary inquiry in February 2012 that \"Japan's atomic safety rules are inferior to global standards and left the country unprepared for the Fukushima nuclear disaster last March\". There were flaws in, and lax enforcement of, the safety rules governing Japanese nuclear power companies, and this included insufficient protection against tsunamis.\n\nA 2012 report in \"The Economist\" said: \"The reactors at Fukushima were of an old design. The risks they faced had not been well analysed. The operating company was poorly regulated and did not know what was going on. The operators made mistakes. The representatives of the safety inspectorate fled. Some of the equipment failed. The establishment repeatedly played down the risks and suppressed information about the movement of the radioactive plume, so some people were evacuated from more lightly to more heavily contaminated places\".\n\nThe designers of the Fukushima I Nuclear Power Plant reactors did not anticipate that a tsunami generated by an earthquake would disable the backup systems that were supposed to stabilize the reactor after the earthquake. Nuclear reactors are such \"inherently complex, tightly coupled systems that, in rare, emergency situations, cascading interactions will unfold very rapidly in such a way that human operators will be unable to predict and master them\".\n\nLacking electricity to pump water needed to cool the atomic core, engineers vented radioactive steam into the atmosphere to release pressure, leading to a series of explosions that blew out concrete walls around the reactors. Radiation readings spiked around Fukushima as the disaster widened, forcing the evacuation of 200,000 people. There was a rise in radiation levels on the outskirts of Tokyo, with a population of 30 million, 135 miles (210 kilometers) to the south.\n\nBack-up diesel generators that might have averted the disaster were positioned in a basement, where they were quickly overwhelmed by waves. The cascade of events at Fukushima had been predicted in a report published in the U.S. several decades ago:\n\nThe 1990 report by the U.S. Nuclear Regulatory Commission, an independent agency responsible for safety at the country’s power plants, identified earthquake-induced diesel generator failure and power outage leading to failure of cooling systems as one of the “most likely causes” of nuclear accidents from an external event.\n\nThe report was cited in a 2004 statement by Japan’s Nuclear and Industrial Safety Agency, but it seems adequate measures to address the risk were not taken by TEPCO. Katsuhiko Ishibashi, a seismology professor at Kobe University, has said that Japan’s history of nuclear accidents stems from an overconfidence in plant engineering. In 2006, he resigned from a government panel on nuclear reactor safety, because the review process was rigged and “unscientific”.\n\nAccording to the International Atomic Energy Agency, Japan \"underestimated the danger of tsunamis and failed to prepare adequate backup systems at the Fukushima Daiichi nuclear plant\". This repeated a widely held criticism in Japan that \"collusive ties between regulators and industry led to weak oversight and a failure to ensure adequate safety levels at the plant\". The IAEA also said that the Fukushima disaster exposed the lack of adequate backup systems at the plant. Once power was completely lost, critical functions like the cooling system shut down. Three of the reactors \"quickly overheated, causing meltdowns that eventually led to explosions, which hurled large amounts of radioactive material into the air\".\n\nLouise Fréchette and Trevor Findlay have said that more effort is needed to ensure nuclear safety and improve responses to accidents:\n\nThe multiple reactor crises at Japan's Fukushima nuclear power plant reinforce the need for strengthening global instruments to ensure nuclear safety worldwide. The fact that a country that has been operating nuclear power reactors for decades should prove so alarmingly improvisational in its response and so unwilling to reveal the facts even to its own people, much less the International Atomic Energy Agency, is a reminder that nuclear safety is a constant work-in-progress.\n\nDavid Lochbaum, chief nuclear safety officer with the Union of Concerned Scientists, has repeatedly questioned the safety of the Fukushima I Plant's General Electric Mark 1 reactor design, which is used in almost a quarter of the United States' nuclear fleet.\n\nA report from the Japanese Government to the IAEA says the \"nuclear fuel in three reactors probably melted through the inner containment vessels, not just the core\". The report says the \"inadequate\" basic reactor design — the Mark-1 model developed by General Electric — included \"the venting system for the containment vessels and the location of spent fuel cooling pools high in the buildings, which resulted in leaks of radioactive water that hampered repair work\".\n\nFollowing the Fukushima emergency, the European Union decided that reactors across all 27 member nations should undergo safety tests.\n\nAccording to UBS AG, the Fukushima I nuclear accidents are likely to hurt the nuclear power industry’s credibility more than the Chernobyl disaster in 1986:\n\nThe accident in the former Soviet Union 25 years ago 'affected one reactor in a totalitarian state with no safety culture,' UBS analysts including Per Lekander and Stephen Oldfield wrote in a report today. 'At Fukushima, four reactors have been out of control for weeks – casting doubt on whether even an advanced economy can master nuclear safety.'\nThe Fukushima accident exposed some troubling nuclear safety issues:\n\nDespite the resources poured into analyzing crustal movements and having expert committees determine earthquake risk, for instance, researchers never considered the possibility of a magnitude-9 earthquake followed by a massive tsunami. The failure of multiple safety features on nuclear power plants has raised questions about the nation's engineering prowess. Government flip-flopping on acceptable levels of radiation exposure confused the public, and health professionals provided little guidance. Facing a dearth of reliable information on radiation levels, citizens armed themselves with dosimeters, pooled data, and together produced radiological contamination maps far more detailed than anything the government or official scientific sources ever provided.\nAs of January 2012, questions also linger as to the extent of damage to the Fukushima plant caused by the earthquake even before the tsunami hit. Any evidence of serious quake damage at the plant would \"cast new doubt on the safety of other reactors in quake-prone Japan\".\n\nTwo government advisers have said that \"Japan's safety review of nuclear reactors after the Fukushima disaster is based on faulty criteria and many people involved have conflicts of interest\". Hiromitsu Ino, Professor Emeritus at the University of Tokyo, says\n\"The whole process being undertaken is exactly the same as that used previous to the Fukushima Dai-Ichi accident, even though the accident showed all these guidelines and categories to be insufficient\".\n\nIn March 2012, Prime Minister Yoshihiko Noda acknowledged that the Japanese government shared the blame for the Fukushima disaster, saying that officials had been blinded by a false belief in the country's \"technological infallibility\", and were all too steeped in a \"safety myth\".\n\nSerious nuclear and radiation accidents include the Chalk River accidents (1952, 1958 & 2008), Mayak disaster (1957), Windscale fire (1957), SL-1 accident (1961), Soviet submarine K-19 accident (1961), Three Mile Island accident (1979), Church Rock uranium mill spill (1979), Soviet submarine K-431 accident (1985), Goiânia accident (1987), Zaragoza radiotherapy accident (1990), Costa Rica radiotherapy accident (1996), Tokaimura nuclear accident (1999), Sellafield THORP leak (2005), and the cobalt-60 spill (2006).\n\nFour hundred and thirty-seven nuclear power stations are presently in operation but, unfortunately, five major nuclear accidents have occurred in the past. These accidents occurred at Kyshtym (1957), Windscale (1957), Three Mile Island (1979), Chernobyl (1986), and Fukushima (2011). A report in \"Lancet\" says that the effects of these accidents on individuals and societies are diverse and enduring:\n\nIn spite of accidents like these, studies have shown that nuclear deaths are mostly in uranium mining and that nuclear energy has generated far fewer deaths than the high pollution levels that result from the use of conventional fossil fuels. However, the nuclear power industry relies on uranium mining, which itself is a hazardous industry, with many accidents and fatalities.\n\nJournalist Stephanie Cooke says that it is not useful to make comparisons just in terms of number of deaths, as the way people live afterwards is also relevant, as in the case of the 2011 Japanese nuclear accidents:\n\n\"You have people in Japan right now that are facing either not returning to their homes forever, or if they do return to their homes, living in a contaminated area for basically ever... It affects millions of people, it affects our land, it affects our atmosphere ... it's affecting future generations ... I don't think any of these great big massive plants that spew pollution into the air are good. But I don't think it's really helpful to make these comparisons just in terms of number of deaths\".\nThe Fukushima accident forced more than 80,000 residents to evacuate from neighborhoods around the plant.\n\nA survey by the Iitate, Fukushima local government obtained responses from some 1,743 people who have evacuated from the village, which lies within the emergency evacuation zone around the crippled Fukushima Daiichi Plant. It shows that many residents are experiencing growing frustration and instability due to the nuclear crisis and an inability to return to the lives they were living before the disaster. Sixty percent of respondents stated that their health and the health of their families had deteriorated after evacuating, while 39.9 percent reported feeling more irritated compared to before the disaster.\n\n\"Summarizing all responses to questions related to evacuees' current family status, one-third of all surveyed families live apart from their children, while 50.1 percent live away from other family members (including elderly parents) with whom they lived before the disaster. The survey also showed that 34.7 percent of the evacuees have suffered salary cuts of 50 percent or more since the outbreak of the nuclear disaster. A total of 36.8 percent reported a lack of sleep, while 17.9 percent reported smoking or drinking more than before they evacuated.\"\nChemical components of the radioactive waste may lead to cancer.\nFor example, Iodine 131 was released along with the radioactive waste when Chernobyl disaster and Fukushima disasters occurred. It was concentrated in leafy vegetation after absorption in the soil. It also stays in animals’ milk if the animals eat the vegetation. When Iodine 131 enters the human body, it migrates to the thyroid gland in the neck and can cause thyroid cancer.\n\nOther elements from nuclear waste can lead to cancer as well. For example, Strontium 90 causes breast cancer and leukemia, Plutonium 239 causes liver cancer.\n\nNewer reactor designs intended to provide increased safety have been developed over time. These designs include those that incorporate passive safety and Small Modular Reactors. While these reactor designs \"are intended to inspire trust, they may have an unintended effect: creating distrust of older reactors that lack the touted safety features\".\n\nThe next nuclear plants to be built will likely be Generation III or III+ designs, and a few such are already in operation in Japan. Generation IV reactors would have even greater improvements in safety. These new designs are expected to be passively safe or nearly so, and perhaps even inherently safe (as in the PBMR designs).\n\nSome improvements made (not all in all designs) are having three sets of emergency diesel generators and associated emergency core cooling systems rather than just one pair, having quench tanks (large coolant-filled tanks) above the core that open into it automatically, having a double containment (one containment building inside another), etc.\n\nHowever, safety risks may be the greatest when nuclear systems are the newest, and operators have less experience with them. Nuclear engineer David Lochbaum explained that almost all serious nuclear accidents occurred with what was at the time the most recent technology. He argues that \"the problem with new reactors and accidents is twofold: scenarios arise that are impossible to plan for in simulations; and humans make mistakes\". As one director of a U.S. research laboratory put it, \"fabrication, construction, operation, and maintenance of new reactors will face a steep learning curve: advanced technologies will have a heightened risk of accidents and mistakes. The technology may be proven, but people are not\".\n\nThere are concerns about developing countries \"rushing to join the so-called nuclear renaissance without the necessary infrastructure, personnel, regulatory frameworks and safety culture\". Some countries with nuclear aspirations, like Nigeria, Kenya, Bangladesh and Venezuela, have no significant industrial experience and will require at least a decade of preparation even before breaking ground at a reactor site.\n\nThe speed of the nuclear construction program in China has raised safety concerns. The challenge for the government and nuclear companies is to \"keep an eye on a growing army of contractors and subcontractors who may be tempted to cut corners\". China has asked for international assistance in training more nuclear power plant inspectors.\n\nNuclear power plants, civilian research reactors, certain naval fuel facilities, uranium enrichment plants, and fuel fabrication plants, are vulnerable to attacks which could lead to widespread radioactive contamination. The attack threat is of several general types: commando-like ground-based attacks on equipment which if disabled could lead to a reactor core meltdown or widespread dispersal of radioactivity; and external attacks such as an aircraft crash into a reactor complex, or cyber attacks.\n\nThe United States 9/11 Commission has said that nuclear power plants were potential targets originally considered for the September 11, 2001 attacks. If terrorist groups could sufficiently damage safety systems to cause a core meltdown at a nuclear power plant, and/or sufficiently damage spent fuel pools, such an attack could lead to widespread radioactive contamination. The Federation of American Scientists have said that if nuclear power use is to expand significantly, nuclear facilities will have to be made extremely safe from attacks that could release massive quantities of radioactivity into the community. New reactor designs have features of passive safety, which may help. In the United States, the NRC carries out \"Force on Force\" (FOF) exercises at all Nuclear Power Plant (NPP) sites at least once every three years.\n\nNuclear reactors become preferred targets during military conflict and, over the past three decades, have been repeatedly attacked during military air strikes, occupations, invasions and campaigns. Various acts of civil disobedience since 1980 by the peace group Plowshares have shown how nuclear weapons facilities can be penetrated, and the groups actions represent extraordinary breaches of security at nuclear weapons plants in the United States. The National Nuclear Security Administration has acknowledged the seriousness of the 2012 Plowshares action. Non-proliferation policy experts have questioned \"the use of private contractors to provide security at facilities that manufacture and store the government's most dangerous military material\". Nuclear weapons materials on the black market are a global concern, and there is concern about the possible detonation of a small, crude nuclear weapon by a militant group in a major city, with significant loss of life and property. \"Stuxnet\" is a computer worm discovered in June 2010 that is believed to have been created by the United States and Israel to attack Iran's nuclear facilities.\n\nNuclear fusion power is a developing technology still under research. It relies on fusing rather than fissioning (splitting) atomic nuclei, using very different processes compared to current nuclear power plants. Nuclear fusion reactions have the potential to be safer and generate less radioactive waste than fission. These reactions appear potentially viable, though technically quite difficult and have yet to be created on a scale that could be used in a functional power plant. Fusion power has been under theoretical and experimental investigation since the 1950s.\n\nConstruction of the International Thermonuclear Experimental Reactor facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027 – 11 years after initially anticipated. A follow on commercial nuclear fusion power station, DEMO, has been proposed. There is also suggestions for a power plant based upon a different fusion approach, that of a Inertial fusion power plant.\n\nFusion powered electricity generation was initially believed to be readily achievable, as fission power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050.\n\nMatthew Bunn, the former US Office of Science and Technology Policy adviser, and Heinonen, the former Deputy Director General of the IAEA, have said that there is a need for more stringent nuclear safety standards, and propose six major areas for improvement:\n\nCoastal nuclear sites must also be further protected against rising sea levels, storm surges, flooding, and possible eventual \"nuclear site islanding\".\n\n\n"}
{"id": "22404946", "url": "https://en.wikipedia.org/wiki?curid=22404946", "title": "PEC Power Electric Cooperation", "text": "PEC Power Electric Cooperation\n\nThe Power Group (Egyptian: Power Electric Cooperation) is Egypt's largest HVAC manufacturer. It is composed of numerous international businesses, all united under the Power brand, including Power Electric, Africa's largest electronics company.\n\nThe Power brand is the best known Egyptian brand in Egypt. In 1989, Power overtook Egyptian rival ColdAir as the Egypt's leading air conditioning brand and became part of the top 20 African brands overall. It is also the leader in many domestic industries, such as the financial, chemical, retail and entertainment industries.\n\n"}
{"id": "21520816", "url": "https://en.wikipedia.org/wiki?curid=21520816", "title": "Peroxyoxalate", "text": "Peroxyoxalate\n\nPeroxyoxalates are esters initially formed by the reaction of hydrogen peroxide with oxalate diesters or oxalyl chloride, with or without base, although the reaction is much faster with base:\nPeroxyoxalates are intermediates that will rapidly transform into 1,2-dioxetanedione, another high-energy intermediate. The likely mechanism of 1,2-dioxetanedione formation from peroxyoxalate in base is illustrated below:\n\n1,2-Dioxetanedione will rapidly decompose into carbon dioxide (CO). If there is no fluorescer present, only heat will be released. However, in the presence of a fluorescer, light can be generated (chemiluminescence). \n\nPeroxyoxalate chemiluminescence (CL) was first reported by Rauhut in 1967 [1] in the reaction of diphenyl oxalate. The emission is generated by the reaction of an oxalate ester with hydrogen peroxide in the presence of a suitably fluorescent energy acceptor. This reaction is used in glow sticks.\n\nThe three most widely used oxalates are bis(2,4,6-trichlorophenyl)oxalate (TCPO), Bis(2,4,5-trichlorophenyl-6-carbopentoxyphenyl)oxalate (CPPO) and bis(2,4-dinitrophenyl) oxalate (DNPO). Other aryl oxalates have been synthesized and evaluated with respect to their possible analytical applications [2]. Divanillyl oxalate, a more eco-friendly or \"green\" oxalate for chemiluminescence, decomposes into the nontoxic, biodegradable compound vanillin and works in nontoxic, biodegradable triacetin [16] . Peroxyoxalate CL is an example of indirect or sensitized chemiluminescence in which the energy from an excited intermediate is transferred to a suitable fluorescent molecule, which relaxes to the ground state by emitting a photon. Rauhut and co-workers have reported that the intermediate responsible for providing the energy of excitation is 1,2-dioxetanedione [1,3]. The peroxyoxalate reaction is able to excite many different compounds, having emissions spanning the visible and infrared regions of the spectrum [3,4], and the reaction can supply up to 440 kJ mol-1, corresponding to excitation at 272 nm [5]. It has been found, however, that the chemiluminescence intensity corrected for quantum yield decreases as the singlet excitation energy of the fluorescent molecule increases [6]. There is also a linear relationship between the corrected chemiluminescence intensity and the oxidation potential of the molecule [6]. This suggests the possibility of an electron transfer step in the mechanism, as demonstrated in several other chemiluminescence systems [7-10]. It has been postulated that a transient charge transfer complex is formed between the intermediate 1,2-dioxetanedione and the fluorescer [11], and a modified mechanism was proposed involving the transfer of an electron from the fluorescer to the reactive intermediate [12]. The emission of light is thought to result from the annihilation of the fluorescer radical cation with the carbon dioxide radical anion formed when the 1,2-dioxetanedione decomposes [13]. This process is called chemically induced electron exchange luminescence (CIEEL).\n\nChemiluminescent reactions are widely used in analytical chemistry [14, 15]\n\n"}
{"id": "31844819", "url": "https://en.wikipedia.org/wiki?curid=31844819", "title": "RazorUSA", "text": "RazorUSA\n\nRazorUSA LLC, better known as Razor, is an American designer and manufacturer of electric personal transporters. The company was founded in Cerritos, California in 2000 by Carlton Calvin and JD Corporation. Razor also owns the RipStik, Sole Skate, and Pocket Pros brands.\n\nThe first Razor scooter was manufactured by JD, and distributed by The Sharper Image. Since JD founded RazorUSA, JD also began to sell scooters under the JDBUG brand.\n\n\n\n\nATV\nGo-carts\nMotorcycles\nMotorscooters\nSelf-balancing hoverboards\nCrazy Carts\n\nCaster driven scooters are a modern form of three-wheeler popular during the 1970s and 80s.\n\nRazor branded bicycles are provided by Kent under license. All the following are Freestyle bikes except as noted.\n\nRazor branded skateboards, pads, and helmets are provided by Kent under license.\n\n"}
{"id": "21090965", "url": "https://en.wikipedia.org/wiki?curid=21090965", "title": "Robert Huggins", "text": "Robert Huggins\n\nRobert A. Huggins is Professor Emeritus at the Department of Materials Science and Engineering at the School of Engineering at Stanford University and Chief Scientist at the Center for Solar Energy and Hydrogen Research at the University of Ulm.\n\nHuggins earned his BA in Physics from Amherst College, and went on to obtain an MS and Sc.D. in Metallurgy from the Massachusetts Institute of Technology, where he also served as an instructor. He joined the Stanford faculty in 1954.\n\nHuggins is known for his association with the controversial theory of cold fusion. While at Stanford, he attempted to recreate the discredited work of Stanley Pons and Martin Fleischmann, at one point reporting success. Only weeks later, such claims were rejected by a colleague.\n\nEnergy Storage, Springer | 2010 | | 400 pages\nAdvanced Batteries: Materials Science Aspects, Springer | 2008 | | 474 pages\n"}
{"id": "3699152", "url": "https://en.wikipedia.org/wiki?curid=3699152", "title": "Rotary phase converter", "text": "Rotary phase converter\n\nA rotary phase converter, abbreviated RPC, is an electrical machine that converts power from one polyphase system (including frequency) to another, converting through rotary motion. Typically, single-phase electric power is used to produce three-phase electric power locally to run three-phase loads (any industrial machinery with three-phase motors) in premises (often residential or consumer) where only single-phase is available.\n\nThe main principles of RPC operation are as follows:\n\nThree-phase induction motors have three terminals called \"legs\", usually numbered (arbitrarily) L1, L2, and L3.\n\nA three-phase induction motor can be run at two-thirds of its rated horsepower on single-phase power applied to any pair of legs, once spun up by some means.\n\nA three-phase induction motor that is spinning under single-phase power applied to legs L1 and L2, generates an electric potential (and can deliver power through) leg L3, although without some form of current injection, special windings in the idler, or other means the voltage will sag when a load is applied.\n\nPower factor correction is a very important consideration when building or choosing an RPC. This is desirable because an RPC that has power factor correction will consume less current from the single-phase service supplying power to the phase converter and its loads.\n\nBalanced voltage between the three legs of power is important for operational life of the equipment receiving that power. Unbalanced three-phase power can damage the equipment that it is meant to operate.\n\nAt the beginning of the 20th century, there were two main principles of electric railway traction current systems:\nThese systems used series-wound traction motors. All of them needed a separated supply system or converters to take power from the standard 50 Hz electric network.\n\nKálmán Kandó recognized that the electric traction system must be supplied by single-phase 50 Hz power from the standard electric network, and it must be converted in the locomotive to three-phase power for traction motors.\nHe created an electric machine called a synchronous phase converter, which was a single-phase synchronous motor and a three-phase synchronous generator with common stator and rotor.\n\nIt had two independent windings:\n\nThe direct feed from a standard electric network makes the system less complicated than the earlier systems and makes possible simple recuperation.\n\nThe single-phase feed makes it possible to use a single overhead line. More overhead lines increase the costs, and restrict the maximum speed of train.\n\nThe asynchronous traction motor can run on a single RPM determined by the frequency of the feeding current and the loading torque.\n\nThe solution was to use more secondary windings on phase converter, and more windings on motor different number of magnetic poles.\n\nA rotary phase converter (RPC) may be built as a motor-generator set. These completely isolate the load from the single-phase supply and produce balanced three-phase output. However, due to weight, cost, and efficiency concerns, most RPCs are not built this way.\n\nInstead, they are built out of a three-phase induction motor or generator, called an idler, on which two of the terminals (the idler inputs) are powered from the single-phase line. The rotating flux in the motor produces a voltage on the third terminal. A voltage is induced in the third terminal that is shifted by 120° from the voltage between the first two terminals. In a three-winding motor, two of the windings are acting as a motor, and the third winding is acting as a generator.\n\nA common measure of the quality of the power produced by an RPC or any phase converter is the voltage balance, which may be measured while the RPC is driving a balanced load such as a three-phase motor. Other quality measures include the harmonic content of the power produced and the power factor of the RPC motor combination as seen by the utility. Selection of the best phase converter for any application depends on the sensitivity of the load to these factors. Three-phase induction motors are very sensitive to voltage imbalances.\n\nThe quality of three-phase power generated by such a phase converter depends upon a number of factors including:\n\nRPC manufacturers use a variety of techniques to deal with these problems. Some of the techniques include,\n\n\nDemand exists for phase converters due to the use of three-phase motors. With increasing power output, three-phase motors have preferable characteristics to single-phase motors; the latter not being available in sizes over and, though available, rarely seen larger than . (Three-phase motors have higher efficiency, reduced complexity, with regards to starting, and three-phase power is significantly available where they are used.)\n\nRotary phase converters are used to produce a single-phase for the single overhead conductor in electric railways. Five European countries (Germany, Austria, Switzerland, Norway, and Sweden), where electricity is three-phase AC as 50 Hz, have standardised on single-phase AC at 15 kV 16⅔ Hz for railway electrification; phase converters are, therefore, used to change both phases and frequency. In the Soviet Union, they were used on AC locomotives to convert single phase, 50 Hz to 3-phase for driving induction motors for traction motor cooling blowers, etc.\n\nAlternatives exist to rotary phase converters for operation of three-phase equipment on a single-phase power supply.\n\nThese may be an alternative where the issue at hand is starting a motor, rather than polyphase power itself. The static phase converter is used to start a three-phase motor. The motor then runs on a single phase with a synthesised third pole. However, this makes the power balance, and thus motor efficiency, extremely poor, requiring de-rating the motor (typically to 60% or less). Overheating, and quite often destruction of the motor, will result from failing to do so. (Many manufacturers and dealers specifically state that using a static converter will void any warranty.) An oversized static converter can remove the need to de-rate the motor, but at an increased cost.\n\nThree-phase inverters and related variable frequency drives, may, under certain circumstances, produce large amounts of harmonic distortion in their output, and the efficiency of the inverter-motor combination can be poor.\n\nWhen poor efficiency in a VFD powered motor has been investigated, in every case documented to date, the root cause for the loss in efficiency was either improper installation or a failure to adhere to best practices. Almost universally, a VFD increases the efficiency of a system and reduces operating costs.\n\nInverter drives (VFDs) can damage some motors not rated for use with an inverter, but this is not a concern when motor voltages are less than 440 VAC. Motor manufacturers may void warranties on non-inverter rated motors if they are run at a variable speed or with an inverter, but most three-phase motors over sold today are rated for use with an inverter drive.\n\nThis damage can be prevented by the use of load and line reactors or in Europe a DU/DT filter. This reactor or filter reduces the voltage stack up that can occur on older high impedance motors such as wound rotor motors. The filter prevents the windings of the motor form insulation breakdown caused by voltage stack up.\n\nVFDs that have a capacity greater than 3 hp can be costly, thus making the RPC an attractive option.\n\n\n"}
{"id": "14102544", "url": "https://en.wikipedia.org/wiki?curid=14102544", "title": "ScanWind", "text": "ScanWind\n\nScanWind was a Norwegian manufacturing company that produced wind turbines. In 2009 Scanwind was bought by General Electric, and became the base for GE Wind Energy in Norway. The company has its head office in Trondheim and production facilities in Verdal, both Norway, as well as engineering division in Karlstad, Sweden. The company was founded in 1999.\n\nThe two models produced by ScanWind are ScanWind 3000 DL with an output of 3.0 MW and ScanWind 3500 DL at 3.5 MW. These windmills have been installed in the Hundhammerfjellet wind farm in Nærøy, owned by NTE.\n"}
{"id": "22552583", "url": "https://en.wikipedia.org/wiki?curid=22552583", "title": "Solar Roadways", "text": "Solar Roadways\n\nSolar Roadways Incorporated is an American company based in Sandpoint, Idaho aiming to develop solar powered road panels to form a smart highway.\n\nTheir proof-of-concept technology combines a transparent driving surface with underlying solar cells, electronics and sensors to act as a solar array with programmable capability. The road panels are to be made from recycled materials and incorporate photovoltaic cells. The project has received criticism in regards to its feasibility and its efficiency compared to traditional solar panel installations.\n\nThe company was founded in 2006 by Scott and Julie Brusaw, with Scott as President and CEO. They envisioned replacing asphalt surfaces with structurally-engineered solar panels capable of withstanding vehicular traffic. The proposed system would require the development of strong, transparent, and self-cleaning glass with the necessary traction and impact-resistance properties at competitive cost.\n\nIn 2009, Solar Roadways received a $100,000 Small Business Innovation Research (SBIR) grant from the United States Department of Transportation (USDOT) for Phase I to determine the feasibility of the proposed project. In 2011, Solar Roadways received $750,000 SBIR grant from the DOT for Phase II to develop and build a solar parking lot; from this, they built a parking lot covered with hexagonal glass-covered solar panels sitting on top of a concrete base, heated to prevent snow and ice accumulation, with LEDs to illuminate road lines and display messages. According to the Brusaws, the panels can sustain a load.\n\nIn April 2014, the company started a crowdfunding drive at Indiegogo to raise money so they could get the product into production. The campaign raised 2.2 million dollars and became Indiegogo’s most popular campaign ever in terms of the number of backers it attracted. The success was attributed in part to a tweet made by actor George Takei, due to his more than 8 million followers. One of the Brusaws’ videos went viral, with over 20 million views as of November 2015. In December 2015, the USDOT announced that it had awarded Solar Roadways a Phase IIB SBIR contract to further their research. In 2016 they were given an additional $750,000.00 \n\nThe first public installation was in Jeff Jones Town Square in Sandpoint, Idaho. It opened to the public on September 30, 2016. As a pilot install it is for walkways only. This installation consists of 30 Solar Roadways SR3 panels covering an area of roughly . The cost of this installation was roughly $60,000 with the majority of the money coming from a grant from the Idaho Department of Commerce ($47,134), and a $10,000 grant from the Sandpoint Urban Renewal Agency. A webcam was installed to broadcast a view of the installation. The 30 tiles in Sandpoint generate power which is fed into the electricity meter at Jeff Jones Town Square, averaging around 10W as of August 2018.\n\nIn 2014, Jonathan Levine, a professor of urban planning at the University of Michigan, expressed doubt regarding the political feasibility of the project on a national scale. He suggested, however, that a single town might be able to deploy the concept in a limited test case such as a parking lot. \n\nJournalist David Biello, writing in \"Scientific American\", noted the difficulties of the project in dealing with material limitations, particularly in its choice of making the surface of the panels from glass, which \"must be tempered, self-cleaning, and capable of transmitting light to the PV below under trying conditions, among other characteristics—a type of glass that does not yet exist.\"\n\nSebastian Anthony noted in ExtremeTech that the cost to replace all roads in the United States with Solar Roadways panels would come to approximately $56 trillion, based on Scott Brusaw's cost estimate of $10,000 for a section. The USDOT announcement of Phase IIB funding in December 2015 mentioned that because the solar cells were still manufactured by hand, they were \"very costly to produce\". \n\nPhil Mason made a similar argument about cost, adding his doubts about traction on a glass surface.\n\n\n\n"}
{"id": "7832792", "url": "https://en.wikipedia.org/wiki?curid=7832792", "title": "Southwest Petroleum University", "text": "Southwest Petroleum University\n\nSouthwest Petroleum University (SWPU, ) is a Chinese university founded in 1958. It is a Chinese Ministry of Education Double First Class Discipline University, with Double First Class status in certain disciplines.\n\nSouthwest Petroleum University (SWPU), located in Chengdu City, the capital city of the Sichuan province, was founded in 1958. SWPU was originally named Sichuan Petroleum Institute and in 1970, was renamed to Southwest Petroleum Institute. It became Southwest Petroleum University (SWPU) after achieving university status in 2005. Originally, SWPU was supported and administered by the Ministry of Petroleum Industry and the China National Petroleum Corporation (CNPC). Since 2000, the university has been co-administered by both Sichuan Province and CNPC.\n\nSWPU has two campuses: the original campus at Nanchong City and the main campus at Chengdu City with a total area of about 3,000 mu (494 acres). The total floor space is more than 900,000 square meters.\n\nThe university is divided into 18 schools and departments that, together, offer 60 bachelor's degree programs. SWPU is one of the first universities in China that was authorized to confer bachelor, masters and doctoral degrees. It has one provincial-level graduate school offering three post-doctoral research programs. SWPU has 23 doctor’s degree programs, 85 master’s degree programs, and one key discipline program in oil and gas engineering. At the provincial and ministerial level, it has seven laboratories and 25 research and technical centers, including a center for well-completion techniques built with the assistance of the United Nations. In addition, it has the Sino-Canadian Training Center for Natural Gas Exploration and Exploitation, built with the support of the Canadian government. At the national level, it has one key laboratory of Oil and Gas Reservoir Geology and Exploitation in which three key disciplines are pursued. Under the auspices of the Ministry of Education, SWPU owns an oil and gas equipment key laboratory, the Natural Gas Exploitation Engineering Research Center, and the Oil Field Petrochemical Engineering Research Center. At the university, two extensive libraries are available offering a collection of over two million volumes .\n\nStudents from across the nation attend SWPU. Each year, more than 5,000 first-year undergraduates enroll in addition to over 1,000 first-year master’s and doctoral degree candidates. The present enrollment of full-time students is more than 32,498, including 8 international students, 26,490 undergraduates, 3,801 postgraduates and 715 PhD students. More than 100,000 students have graduated from SWPU since its establishment. Within the past ten years, the proportion of students successfully being employed is more than 94 percent every year.\n\nCurrently there are 2,397 academic staff and general staff, of which there are 1.664 full-time teaching staff, including 211 full-professors .\n\nSWPU has been involved in over 6,121 scientific programs since 2001 and has obtained 783 million RMB (renminbi) in scientific funding, winning 95 provincial or ministerial prizes. 495 patents have been approved, 184 of which are invented patents .\n\nSWPU has established worldwide cooperative relationships with universities and research institutions, including those in the United States, United Kingdom, Japan, France, Germany, Canada, Russia and India. One example being a 2+2 degree program with the University of Leeds, UK. \n\nThe schools and departments are organized into the following divisions:\n\n\nSouthwest Petroleum University has a number of National Key Disciplines.\n\n\n1958～1962 Sichuan Petroleum Institute run by the local government of Sichuan Province\n\n1962～1970 Sichuan Petroleum Institute run by the Ministry of Petroleum Industry, P.R.China\n\n1970～1978 Southwest Petroleum Institute run by the local government of Sichuan Province\n\n1978～1988 Southwest Petroleum Institute run by the Ministry of Petroleum Industry, P.R. China\n\n1988～1998 Southwest Petroleum Institute run by China National Petroleum Corporation, P.R.China\n\n1998～2000 Southwest Petroleum Institute run by China National Petroleum Corporation (Group), P.R.China\n\n2000～2005 Southwest Petroleum Institute run by the local government of Sichuan Province\n\n2005～ Southwest Petroleum University run by the local government of Sichuan Province\n\n"}
{"id": "49552891", "url": "https://en.wikipedia.org/wiki?curid=49552891", "title": "Wax emulsion", "text": "Wax emulsion\n\nA wax emulsion is a stable mixture of one or more waxes in water. Waxes and water are normally immiscible but can be brought together stably by the use of surfactants and a clever preparation process. Strictly speaking a wax emulsion should be called a wax dispersion since the wax is solid at room temperature. However, because the preparation takes place above the melting point of the wax, the actual process is called emulsification, hence the name wax emulsion. In praxis, wax dispersion is used for solvent based systems.\n\nA wide range of emulsions based on different waxes and blends thereof are available, depending on the final application. Waxes that are found in wax emulsions can be of natural or synthetic origin. Common non-fossil natural waxes are carnaubawax, beeswax, candelilla wax or ricebran wax. Paraffin, microcrystalline and montanwax are the most used fossil natural waxes that are found in emulsions. Synthetic waxes that are used include (oxidised) LDPE and HDPE, maelic grafted PP and Fischer-Tropsch waxes.\n\nA range of different emulsifiers or surfactants are used to emulsify waxes. These can be anionic, cationic or non-ionic in nature. The most common however are fatty alcohol ethoxylates as non-ionic surfactants due to their superb stability against hard water, pH-shock and electrolytes. Some applications demand different emulsifier systems for example anionic surfactants for better hydrophobicity or cationic surfactants for better adhesion to certain materials like textile fibers.\n\nWax emulsions are widely used in a variety of technical applications like printing inks & lacquers, leather and textiles, paper, wood, metal, polishes, glass fiber sizing, glass bottle protection among other things. The most important properties that can be improved by the addition of wax emulsions are matting & gloss, hydrophobicity, soft touch, abrasion & rub resistance, scratch resistance, release, corrosion protection and anti-blocking.\n\nEmulsions based on natural waxes are used for coating fruits and candies and crop protection. Synthetic wax based emulsions are often used in food packaging.\n\nWax emulsions based on beeswax, carnauba wax and paraffin wax are used in creams and ointments.\n\nThe emergence of soybean waxes with varying properties and melt points has led to the use of vegetable wax emulsions in applications such as paper coatings, paint and ink additives, and even wet sizing for pulp and paper applications. These wax emulsions can be formulated to deliver some of the same properties that petroleum-based wax emulsions deliver, but offer advantages of being a green product and offer more consistent availability.\n"}
{"id": "31335398", "url": "https://en.wikipedia.org/wiki?curid=31335398", "title": "WheelTug", "text": "WheelTug\n\nElectric taxiing was invented at least as far back as 1943. Other patents date to 1945 , the 1960s and the 1970s..\n\nWheelTug is an in-wheel electric taxi system under development by a company of the same name, Wheeltug is a subsidiary of and majority owned by Borealis Exploration. The system will enable airplanes to taxi forward and backward without needing a tow tractor or using main jet engines. WheelTug will accomplish this goal through twin electric motors installed in the nose wheels; these motors will be powered by the aircraft's Auxiliary Power Unit (APU). According to Aviation Week, the WheelTug system was projected to provide savings in ground turnaround time and increased aircraft utilization. \n\nIn June 2005 Chorus Motors ground tested the WheelTug concept on an Air Canada 767 at the Evergreen Air Center at Pinal Air Park in Marana, Arizona with an electric motor attached to the nose wheel for taxi testing, the first demonstration of electric taxiing onboard an actual aircraft. Delta Airlines issued a press release in 2007 that Delta would become a development partner and launch customer for Wheeltug expecting installation of first production units on Delta's 737s by late 2009. According to a Wheeltug press release, roller tests were conducted at Prague Airport in November 2010 in snowy and icy conditions, and the first fully 'in-wheel' demonstration unit was tested there June 2012. In December 2016, the FAA accepted the company's Supplemental Type Certification (STC) plan for the Boeing 737 Next Generation models. As of January 2017 more than 20 commercial airlines accepted optional production slots. The WheelTug system is hoped to enter service for the 737NG once the certification process is complete in late 2018, with Canadian carrier Air Transat as the launch customer. \nTaxiBot, a semi-robotic towbar-less tractor which meets and connects to aircraft, currently it is the only alternative E-Taxiing system certified and currently in use. TaxiBot can tow aircraft from the gate to the takeoff point.\n\nAnother competitor previously under development by EGTS International, a joint venture between Honeywell and Safran, sought to install ground taxi motors in the main landing gear wheels, the partnership was dissolved due to the new economics imposed by the sharp drop in the price of jet fuel but Safran continues to work on the concept. \n\n"}
