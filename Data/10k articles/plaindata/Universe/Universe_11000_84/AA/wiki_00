{"id": "40182090", "url": "https://en.wikipedia.org/wiki?curid=40182090", "title": "2013 Rosario gas explosion", "text": "2013 Rosario gas explosion\n\nA gas explosion in a residential area of Rosario, the third-largest city in Argentina, occurred on August 6, 2013. It was caused by a large gas leak; a nearby building collapsed, and others were at high risk of structural failure. Twenty-two people died, and sixty were injured. Several organizations helped secure the area, search for survivors and aid people who lost their homes. Shortly after the explosion, the time needed for reconstruction was estimated at six months.\n\nThe provincial judiciary launched an investigation into the cause of the explosion. Primary suspects were Litoral Gas (the natural-gas provider for Rosario) and an employee who carried out maintenance work at the building that day. Several public figures sent condolences, and most of the candidates for the 2013 primary elections suspended their political campaigns.\n\nThe explosion occurred at 9:30 a.m. near the intersection of Oroño and Salta Streets in central Rosario. Initial reports confirmed eight people dead, sixty injured and fifteen missing; eight more deaths were later confirmed. Searches the following day revealed twelve fatalities, ten of whom were identified. A number of people were missing; some were found dead among the debris, while others were rescued. The search for survivors ended on August 13, with twenty-two people confirmed dead. A 65-year-old woman who had been injured died on October 8.\n\nThe explosion was caused by a gas leak in a 30-year-old building. It severely damaged a nearby nine-story apartment building, causing it to collapse. Mónica Fein, mayor of Rosario, asked residents to avoid the area because of the risk that more buildings might collapse, and to ease the work of disaster management personnel. The streets were covered with broken glass from damaged buildings. Gas and electricity were immediately disconnected, and the national government sent an Argentine Federal Police task force to the scene.\n\nThe natural gas supplier, Litoral Gas, immediately began sealing the distribution pipe to the area. The Center for Ambulatory Medical Specialties of Rosario () managed the information about the dead and injured, and tents were prepared for those left homeless. Firefighters and other workers found people trapped on the upper floors of buildings and evacuated them over adjacent roofs. Although the building was not destroyed by the explosion, a high risk of structural failure remained.\n\nNeighbors reported to the press that they had smelled a gas leak several hours before the explosion and had called Litoral Gas. Company director José María González said that the company had received no such calls, and thought that callers might have dialed the 911 emergency number instead. Prosecutor Camporini reported at the trial that the building had experienced several gas leaks before the explosion.\n\nThe provincial judiciary launched an investigation into the circumstances surrounding the explosion. The prosecution conducted a search and seizure at the offices of Litoral Gas to confirm the absence of customer complaints about the gas leak. Judge Juan Carlos Curto ordered the arrest of Carlos Osvaldo García, an employee of the department responsible for gas service to the area. He was captured during the night, and his assistant Pablo Miño surrendered to police the following day. According to witnesses, one employee fled in a van before the explosion, when he realized the severity of the gas leak, while another remained to try to evacuate people from the endangered area. The van belonged to García, who experienced an acute stress reaction during the trial. Curto checked the remnants of the gas employee's workshop to verify García's testimony.\n\nProsecutor Graciela Argüelles said that, according to the investigation, Litoral Gas ignored calls for help from García, who was not properly trained to manage such a situation. The judge suggested that documents seized from Litoral Gas might prove the existence of customer reports of a gas leak. Curto thought that the employees might not bear sole responsibility, and that the liability of Litoral Gas had to be investigated as well.\n\nPablo Miño was released from prison, but Curto refused to release García, saying that Miño had extenuating circumstances which García did not. Miño's job was to give García the required tools, not to do the maintenance. He was in the street, watching over the van, which was not properly parked and locked, and did not see García's work before the explosion. Curto stopped short of pronouncing Miño innocent at that early stage.\n\nAs the case expanded beyond his jurisdiction, Curto recused himself from the trial and was replaced by Javier Beltramone, who released García from prison. Litoral Gas demanded Beltramone's recusal for expressing an opinion about the case to the press. The appeal court agreed in a 2–1 vote to remove Beltramone, and the case was transferred to Patricia Bilotta. García had claimed that he was following instructions received in the days before the explosion, so Bilotta summoned the technical officers of Litoral Gas to clarify that point. Litoral Gas said that García had not received any instructions prior to the explosion.\n\nLitoral Gas proposed an out-of-court settlement to the relatives of the victims, offering about 1200 US dollars per square meter of collapsed building, in addition to compensation for loss of life. Vice Governor Jorge Henn rejected it as immoral, and most of the families also initially rejected the proposal. By May 2014, however, almost half of the families had accepted the settlement.\n\nThe explosion occurred shortly before the primary 2013 Argentine legislative elections on August 11. The governor of Santa Fe province, Antonio Bonfatti, asked the political parties to end their campaigns to allow mourning for the victims of the explosion. The Front for Victory and Progressive, Civic and Social Front candidates suspended their campaigns, and the national government declared two days of mourning. The period of mourning was observed by all candidates in Buenos Aires and most other provinces, who ended their political campaigns.\n\nPresident Cristina Fernández de Kirchner, who had recently returned from a diplomatic visit to the United Nations, visited the site of the explosion on August 7. She was berated by local residents; some were angry because her surprise visit halted work at the site, and others thought her presence was politically motivated. The president stayed briefly, visited the CEMAR and met Bonfatti. Kirchner's entourage was surrounded by members of La Cámpora, who tried to prevent demonstrations against her and keep journalists and residents at bay.\n\nWeeks before the explosion, several social networking sites had scheduled a country-wide \"cacerolazo\" (a pot-banging protest demonstration), known as 8A, against Kirchner for August 8. The websites had already conducted successful \"cacerolazos\" (8N and 18A). Despite the national mourning, the 8A protest went ahead as planned, with the added slogan \"No more pointless deaths\". Candidate Ricardo Gil Lavedra thought the \"cacerolazo\" should have been canceled, as the campaigning was, but fellow candidate Rodolfo Terragno supported it. It was attended by fewer people than previous ones in Buenos Aires and the rest of the country. The demonstration in Rosario was not a \"cacerolazo\", but a silent candlelight vigil attended by nearly a hundred people. There was a second demonstration in Rosario on August 22, proceeding from the National Flag Memorial to the headquarters of Litoral Gas.\n\nPope Francis sent a letter of condolence to Archbishop José Luis Mollaghan of Rosario, and it was read during a mass and procession for Saint Cajetan at Plaza 25 de Mayo. Newell's Old Boys and Rosario Central, two local soccer teams and rivals in the Rosario derby, organized a charity match for the victims at the Gabino Sosa Stadium, and Rosario-born Lionel Messi provided support through the \"Leo Messi\" charity. The charity match collected 120,000 pesos. Musicians Fito Páez, Vicentico, Babasónicos, Las Pelotas, Chaqueño Palavecino, Ciro Pertusi, Lisandro Aristimuño, Pablo Dacal and Coki de Bernardis performed concerts in several Argentine cities to raise money for the victims.\n\nBonfatti announced that Santa Fe province would provide financial help to the victims of the explosion. Since most houses in the vicinity were damaged, affected families would receive a subsidy of $20,000 to rent homes during reconstruction. They would receive $50,000 in credit to buy furniture and appliances, payable in 60 months with five percent interest. Rosario's real estate firms prepared a list of houses for rent without charging victims their regular fee. Some of the affected buildings may have had cheap insurances which would not cover the risk of an explosion. Some cars trapped in an underground parking lot could not be retrieved.\n\nWhen the search for survivors ended, authorities closed Salta Street. Engineers began checking the buildings at ground zero, trying to restore the original layout of the street and demolishing unstable structures. Secretary of Public Works Omar Saab said that the two remaining buildings were beyond repair and had to be demolished. As a sign of respect, the demolition would not be carried out with explosives. Secretary of Housing Gustavo Leone estimated that the work would take nearly six months. People were allowed to enter their destroyed houses in small groups at a time, starting on August 9. Nearby streets began to be reopened on August 13.\n\nThe CGT union signed a deal with the association of factories of Rosario and the government of Rosario to make sure that all the victims of the explosions would keep their jobs.\n\n"}
{"id": "31633769", "url": "https://en.wikipedia.org/wiki?curid=31633769", "title": "Acceptance angle (solar concentrator)", "text": "Acceptance angle (solar concentrator)\n\nAcceptance angle is the maximum angle at which incoming sunlight can be captured by a solar concentrator. Its value depends on the concentration of the optic and the refractive index in which the receiver is immersed. Maximizing the acceptance angle of a concentrator is desirable in practical systems and it may be achieved by using nonimaging optics.\n\nFor concentrators that concentrate light in two dimensions, the acceptance angle may be different in the two directions.\n\nThe \"acceptance angle\" figure illustrates this concept.\n\nThe concentrator is a lens with a receiver \"R\". The left section of the figure shows a set of parallel rays incident on the concentrator at an angle \"α\" < \"θ\" to the optical axis. All rays end up on the receiver and, therefore, all light is captured. In the center, this figure shows another set of parallel rays, now incident on the concentrator at an angle \"α\" = \"θ\" to the optical axis. For an ideal concentrator, all rays are still captured. However, on the right, this figure shows yet another set of parallel rays, now incident on the concentrator at an angle \"α\" > \"θ\" to the optical axis. All rays now miss the receiver and all light is lost. Therefore, for incidence angles \"α\" < \"θ\" all light is captured while for incidence angles \"α\" > \"θ\" all light is lost. The concentrator is then said to have an (half) acceptance angle \"θ\", or a total acceptance angle 2\"θ\" (since it accepts light within an angle ±\"θ\" to the optical axis).\nIdeally, a solar concentrator has a transmission curve \"c\" as shown in the \"transmission curves\" figure. Transmission (efficiency) is \"τ\" = 1 for all incidence angles \"α\" < \"θ\" and \"τ\" = 0 for all incidence angles \"α\" > \"θ\".\n\nIn practice, real transmission curves are not perfect and they typically have a shape similar to that of curve \"c\", which is normalized so that \"τ\" = 1 for \"α\" = 0. In that case, the real acceptance angle \"θ\" is typically defined as the angle for which transmission \"τ\" drops to 90% of its maximum.\nFor line-focus systems, such as a trough concentrator or a linear Fresnel lens, the acceptance angle is one dimensional, and the concentration has only weak dependence on off-pointing perpendicular to the focus direction. Point focus systems, on the other hand, are sensitive to off-pointing in both directions. In the general case, the acceptance angle in one direction may be different from the other.\n\nThe acceptance angle \"θ\" of a concentrator may be seen as a measure of how precisely it must track the sun in the sky. The smaller the \"θ\", the more precise the tracking needs to be or the concentrator will not capture the incoming sunlight. It is, therefore, a measure of the tolerance a concentrator has to tracking errors.\nHowever, other errors also affect the acceptance angle. The \"optical imperfections\" figure shows this.\n\nThe left part of the figure shows a perfectly made lens with good optical surfaces \"s\" and \"s\" capturing all light rays incident at an angle \"α\" to the optical axis. However, real optics are never perfect and the right part of the figure shows the effect of a badly made bottom surface \"s\". Instead of being smooth, \"s\" now has undulations and some of the light rays that were captured before are now lost. This decreases the transmission of the concentrator for incidence angle \"α\", decreasing the acceptance angle. Actually, any imperfection in the system such as:\ncontributes to a decrease in the acceptance angle of the concentrator. The acceptance angle may then be seen as a \"tolerance budget\" to be spent on all these imperfections. At the end, the concentrator must still have enough acceptance to capture sunlight which also has some angular dispersion \"θ\" when seen from earth. It is, therefore, very important to design a concentrator with the widest possible acceptance angle. That is possible using nonimaging optics, which maximize the acceptance angle for a given concentration.\nFigure \"angular aperture of sunlight\" on the right shows the effect of the angular dispersion of sunlight on the acceptance angle.\n\nSunlight is not a set of perfectly parallel rays (shown in blue), but it has a given angular aperture \"θ\", as indicated by the green rays. If the acceptance angle of the optic is wide enough, sunlight incident along the optical axis will be captured by the concentrator, as shown in the \"angular aperture of sunlight\" figure. However, for wider incidence angles \"α\" some light may be lost, as shown on the right. Perfectly parallel rays (shown in blue) would be captured, but sunlight, due to its angular aperture, is partially lost.\n\nParallel rays and sunlight are therefore transmitted differently by a solar concentrator and the corresponding transmission curves are also different. Different acceptance angles may then be determined for parallel rays or for sunlight.\n\nFor a given acceptance angle \"θ\", for a point-focus concentrator, the maximum concentration possible, \"C\", is given by\n\nwhere \"n\" is the refractive index of the medium in which the receiver is immersed. In practice, real concentrators either have a lower than ideal concentration for a given acceptance or they have a lower than ideal acceptance angle for a given concentration. This can be summarized in the expression\n\nwhich defines a quantity CAP (concentration acceptance product), which must be smaller than the refractive index of the medium in which the receiver is immersed. \n\nFor a linear-focue concentrator, the equation is not squared\n\nThe Concentration Acceptance Product is a consequence of the conservation of etendue. The higher the CAP, the closer the concentrator is to the maximum possible in concentration and acceptance angle.\n\n"}
{"id": "4345279", "url": "https://en.wikipedia.org/wiki?curid=4345279", "title": "Aircraft dynamic modes", "text": "Aircraft dynamic modes\n\nThe dynamic stability of an aircraft refers to how the aircraft behaves after it has been disturbed following steady non-oscillating flight.\n\nOscillating motions can be described by two parameters, the period of time required for one complete oscillation, and the time required to damp to half-amplitude, or the time to double the amplitude for a dynamically unstable motion. The longitudinal motion consists of two distinct oscillations, a long-period oscillation called a phugoid mode and a short-period oscillation referred to as the short-period mode.\n\nThe longer period mode, called the \"phugoid mode\" is the one in which there is a large-amplitude variation of air-speed, pitch angle, and altitude, but almost no angle-of-attack variation. The phugoid oscillation is really a slow interchange of kinetic energy (velocity) and potential energy (height) about some equilibrium energy level as the aircraft attempts to re-establish the equilibrium level-flight condition from which it had been disturbed. The motion is so slow that the effects of inertia forces and damping forces are very low. Although the damping is very weak, the period is so long that the pilot usually corrects for this motion without being aware that the oscillation even exists. Typically the period is 20–60 seconds. This oscillation can generally be controlled by the pilot.\n\nWith no special name, the shorter period mode is called simply the \"short-period mode\". The short-period mode is a usually heavily damped oscillation with a period of only a few seconds. The motion is a rapid pitching of the aircraft about the center of gravity. The period is so short that the speed does not have time to change, so the oscillation is essentially an angle-of-attack variation. The time to damp the amplitude to one-half of its value is usually on the order of 1 second. Ability to quickly self damp when the stick is briefly displaced is one of the many criteria for general aircraft certification.\n\n\"Lateral-directional\" modes involve rolling motions and yawing motions. Motions in one of these axes almost always couples into the other so the modes are generally discussed as the \"Lateral-Directional modes\".\n\nThere are three types of possible lateral-directional dynamic motion: roll subsidence mode, spiral mode, and Dutch roll mode.\n\nRoll subsidence mode is simply the damping of rolling motion. There is no direct aerodynamic moment created tending to directly restore wings-level, i.e. there is no returning \"spring force/moment\" proportional to roll angle. However, there is a damping moment (proportional to roll \"rate\") created by the slewing-about of long wings. This prevents large roll rates from building up when roll-control inputs are made or it damps the roll \"rate\" (not the angle) to zero when there are no roll-control inputs.\n\nRoll mode can be improved by dihedral effects coming from design characteristics, such as high wings, dihedral angles or sweep angles.\n\nThe second lateral motion is an oscillatory combined roll and yaw motion called Dutch roll, perhaps because of its similarity to an ice-skating motion of the same name made by Dutch skaters; the origin of the name is unclear. The Dutch roll may be described as a yaw and roll to the right, followed by a recovery towards the equilibrium condition, then an overshooting of this condition and a yaw and roll to the left, then back past the equilibrium attitude, and so on. The period is usually on the order of 3–15 seconds, but it can vary from a few seconds for light aircraft to a minute or more for airliners. Damping is increased by large directional stability and small dihedral and decreased by small directional stability and large dihedral. Although usually stable in a normal aircraft, the motion may be so slightly damped that the effect is very unpleasant and undesirable. In swept-back wing aircraft, the Dutch roll is solved by installing a yaw damper, in effect a special-purpose automatic pilot that damps out any yawing oscillation by applying rudder corrections. Some swept-wing aircraft have an unstable Dutch roll. If the Dutch roll is very lightly damped or unstable, the yaw damper becomes a safety requirement, rather than a pilot and passenger convenience. Dual yaw dampers are required and a failed yaw damper is cause for limiting flight to low altitudes, and possibly lower Mach numbers, where the Dutch roll stability is improved.\n\nSpiraling is inherent. Most aircraft trimmed for straight-and-level flight, if flown stick-fixed, will eventually develop a tightening spiral-dive. If a spiral dive is entered unintentionally, the result can be fatal.\n\nA spiral dive is not a spin; it starts, not with a stall or from torque but with a random, increasing roll and airspeed. Without prompt intervention by the pilot, this can lead to structural failure of the airframe, either as a result of excess aerodynamic loading or flight into terrain. The aircraft initially gives little indication that anything has changed. The pilot's \"down\" sensation continues to be with respect to the bottom of the airplane, although the aircraft actually has increasingly rolled off the true vertical. Under VFR conditions, the pilot corrects for this deviation from level automatically using the true horizon, while it is very small; but in IMC or dark conditions it can go unnoticed: the roll will increase and the lift, no longer vertical, is insufficient to support the airplane. The nose drops and speed increases: the spiral dive has begun.\n\nSay the roll is to the right. A sideslip develops, resulting in a slip-flow which is right-to-left. Now examine the resulting forces one at a time, calling any rightward influence yaw-in, leftward yaw-out, or roll-in or -out, whichever applies. The slip-flow will:\n\n\nAlso, an aerodynamic force is imposed by the relative vertical positions of the fuselage and the wings, creating a roll-in leverage if the fuselage is above the wings, as in a low wing configuration; or roll-out if below, as in a high-wing configuration.\n\nA propeller rotating under power will influence the airflow passing it. Its effect depends on throttle setting (high at high rpm, low at low) and the attitude of the aircraft.\n\nThus, a spiral dive results from the netting-out of many forces depending partly on the design of the aircraft, partly on its attitude, and partly on its throttle setting (a susceptible design will spiral dive under power but may not in the glide).\n\nA diving aircraft has more kinetic energy (which varies as the square of speed) than when straight-and-level. To get back to straight-and-level, the recovery must get rid of this excess energy safely. The sequence is: Power all off; level the wings to the horizon or, if horizon has been lost, to the instruments; reduce speed using gentle back-pressure on the controls until a desired speed is reached; level off and restore power. The pilot should be alert to a pitch up tendency as the aircraft is rolled to wings level.\n\n"}
{"id": "2729206", "url": "https://en.wikipedia.org/wiki?curid=2729206", "title": "Carleton Farms Landfill", "text": "Carleton Farms Landfill\n\nThe Carleton Farms Landfill is located in Sumpter Township of Wayne County in the U.S. state of Michigan.\n\nThe landfill sits on of property and has a solid waste boundary of . It is owned by Republic Services Inc., and is about west of Detroit since 2002. From 1970s to 2002 the site was operated by Carleton Farms Incorporated as a Liquid industrial waste generator.\n\nAs of September, 2005, one million tons of waste is shipped from Toronto to the Carleton Farms Landfill annually after 2002.\nThe site no longer takes Toronto's trash and now accounts for about 10% of Michigan's total waste.\n\nThe landfill is located near Crosswind Marsh Preserve, a reconstructed marsh created to replace the one formerly located at Detroit Metropolitan Wayne County Airport.\n\n\n"}
{"id": "36806", "url": "https://en.wikipedia.org/wiki?curid=36806", "title": "Cotton", "text": "Cotton\n\nCotton is a soft, fluffy staple fiber that grows in a boll, or protective case, around the seeds of the cotton plants of the genus \"Gossypium\" in the mallow family \"Malvaceae\". The fiber is almost pure cellulose. Under natural conditions, the cotton bolls will increase the dispersal of the seeds.\n\nThe plant is a shrub native to tropical and subtropical regions around the world, including the Americas, Africa, Egypt and India. The greatest diversity of wild cotton species is found in Mexico, followed by Australia and Africa. Cotton was independently domesticated in the Old and New Worlds.\n\nThe fiber is most often spun into yarn or thread and used to make a soft, breathable textile. The use of cotton for fabric is known to date to prehistoric times; fragments of cotton fabric dated to the fifth millennium BC have been found in the Indus Valley Civilization. \nAlthough cultivated since antiquity, it was the invention of the cotton gin that lowered the cost of production that led to its widespread use, and it is the most widely used natural fiber cloth in clothing today.\n\nCurrent estimates for world production are about 25 million tonnes or 110 million bales annually, accounting for 2.5% of the world's arable land. China is the world's largest producer of cotton, but most of this is used domestically. The United States has been the largest exporter for many years. In the United States, cotton is usually measured in bales, which measure approximately and weigh .\n\nThere are four commercially grown species of cotton, all domesticated in antiquity:\n\nThe two New World cotton species account for the vast majority of modern cotton production, but the two Old World species were widely used before the 1900s. While cotton fibers occur naturally in colors of white, brown, pink and green, fears of contaminating the genetics of white cotton have led many cotton-growing locations to ban the growing of colored cotton varieties.\n\nThe earliest evidence of cotton use in the Indian subcontinent has been found at the site of Mehrgarh and Rakhigarhi where cotton threads have been found preserved in copper beads; these finds have been dated to the Neolithic (5th millennium BC). Cotton cultivation in the region is dated to the Indus Valley Civilization, which covered parts of modern eastern Pakistan and northwestern India between 3300 and 1300 BC. The Indus cotton industry was well-developed and some methods used in cotton spinning and fabrication continued to be used until the industrialization of India. Between 2000 and 1000 BC cotton became widespread across much of India. For example, it has been found at the site of Hallus in Karnataka dating from around 1000 BC.\n\nCotton bolls discovered in a cave near Tehuacán, Mexico, have at first been\ndated to as early as 5500 BC, but this date has been challenged. More securely dated is the domestication of Gossypium hirsutum in Mexico between around 3400 and 2300 BC.\n\nIn Peru, cultivation of the indigenous cotton species \"Gossypium barbadense\" has been dated, from a find in Ancon, to c 4200 BC, and was the backbone of the development of coastal cultures such as the Norte Chico, Moche, and Nazca. Cotton was grown upriver, made into nets, and traded with fishing villages along the coast for large supplies of fish. The Spanish who came to Mexico and Peru in the early 16th century found the people growing cotton and wearing clothing made of it.\n\nThe Greeks and the Arabs were not familiar with cotton until the Wars of Alexander the Great, as his contemporary Megasthenes told Seleucus I Nicator of \"there being trees on which wool grows\" in \"Indica\". This may be a reference to \"tree cotton\", Gossypium arboreum, which is a native of the Indian subcontinent.\n\nAccording to the \"Columbia Encyclopedia\":\n\nIn Iran (Persia), the history of cotton dates back to the Achaemenid era (5th century BC); however, there are few sources about the planting of cotton in pre-Islamic Iran. The planting of cotton was common in Merv, Ray and Pars of Iran. In Persian poets' poems, especially Ferdowsi's Shahname, there are references to cotton (\"panbe\" in Persian). Marco Polo (13th century) refers to the major products of Persia, including cotton. John Chardin, a French traveler of the 17th century who visited Safavid Persia, spoke approvingly of the vast cotton farms of Persia.\n\nDuring the Han dynasty (207 BC - 220 AD), cotton was grown by Chinese peoples in the southern Chinese province of Yunnan.\n\nEgyptians grew and spun cotton in the first seven centuries of the Christian era.\n\nHandheld roller cotton gins had been used in India since the 6th century, and was then introduced to other countries from there. Between the 12th and 14th centuries, dual-roller gins appeared in India and China. The Indian version of the dual-roller gin was prevalent throughout the Mediterranean cotton trade by the 16th century. This mechanical device was, in some areas, driven by water power.\n\nThe spinning wheel was invented in India, between 500 and 1000 AD. The earliest clear illustrations of the spinning wheel come from the Islamic world in the eleventh century.\n\nDuring the late medieval period, cotton became known as an imported fiber in northern Europe, without any knowledge of how it was derived, other than that it was a plant. Because Herodotus had written in his \"Histories\", Book III, 106, that in India trees grew in the wild producing wool, it was assumed that the plant was a tree, rather than a shrub. This aspect is retained in the name for cotton in several Germanic languages, such as German \"Baumwolle\", which translates as \"tree wool\" (\"Baum\" means \"tree\"; \"Wolle\" means \"wool\"). Noting its similarities to wool, people in the region could only imagine that cotton must be produced by plant-borne sheep. John Mandeville, writing in 1350, stated as fact that \"There grew there [India] a wonderful tree which bore tiny lambs on the endes of its branches. These branches were so pliable that they bent down to allow the lambs to feed when they are hungry.\" (See Vegetable Lamb of Tartary.)\nCotton manufacture was introduced to Europe during the Muslim conquest of the Iberian Peninsula and Sicily. The knowledge of cotton weaving was spread to northern Italy in the 12th century, when Sicily was conquered by the Normans, and consequently to the rest of Europe. The spinning wheel, introduced to Europe circa 1350, improved the speed of cotton spinning. By the 15th century, Venice, Antwerp, and Haarlem were important ports for cotton trade, and the sale and transportation of cotton fabrics had become very profitable.\n\nUnder the Mughal Empire, which ruled in the Indian subcontinent from the early 16th century to the early 18th century, Indian cotton production increased, in terms of both raw cotton and cotton textiles. The Mughals introduced agrarian reforms such as a new revenue system that was biased in favour of higher value cash crops such as cotton and indigo, providing state incentives to grow cash crops, in addition to rising market demand.\n\nThe largest manufacturing industry in the Mughal Empire was cotton textile manufacturing, which included the production of piece goods, calicos, and muslins, available unbleached and in a variety of colours. The cotton textile industry was responsible for a large part of the empire's international trade. India had a 25% share of the global textile trade in the early 18th century. Indian cotton textiles were the most important manufactured goods in world trade in the 18th century, consumed across the world from the Americas to Japan. The most important center of cotton production was the Bengal Subah province, particularly around its capital city of Dhaka.\n\nThe worm gear roller cotton gin, which was invented in India during the early Delhi Sultanate era of the 13th–14th centuries, came into use in the Mughal Empire some time around the 16th century, and is still used in India through to the present day. Another innovation, the incorporation of the crank handle in the cotton gin, first appeared in India some time during the late Delhi Sultanate or the early Mughal Empire. The production of cotton, which may have largely been spun in the villages and then taken to towns in the form of yarn to be woven into cloth textiles, was advanced by the diffusion of the spinning wheel across India shortly before the Mughal era, lowering the costs of yarn and helping to increase demand for cotton. The diffusion of the spinning wheel, and the incorporation of the worm gear and crank handle into the roller cotton gin, led to greatly expanded Indian cotton textile production during the Mughal era.\n\nIt was reported that, with an Indian cotton gin, which is half machine and half tool, one man and one woman could clean 28 pounds of cotton per day. With a modified Forbes version, one man and a boy could produce 250 pounds per day. If oxen were used to power 16 of these machines, and a few people's labour was used to feed them, they could produce as much work as 750 people did formerly.\n\nIn the early 19th century, a Frenchman named M. Jumel proposed to the great ruler of Egypt, Mohamed Ali Pasha, that he could earn a substantial income by growing an extra-long staple Maho (\"Gossypium barbadense\") cotton, in Lower Egypt, for the French market. Mohamed Ali Pasha accepted the proposition and granted himself the monopoly on the sale and export of cotton in Egypt; and later dictated cotton should be grown in preference to other crops.\n\nEgypt under Muhammad Ali in the early 19th century had the fifth most productive cotton industry in the world, in terms of the number of spindles per capita. The industry was initially driven by machinery that relied on traditional energy sources, such as animal power, water wheels, and windmills, which were also the principal energy sources in Western Europe up until around 1870. It was under Muhammad Ali in the early 19th century that steam engines were introduced to the Egyptian cotton industry.\n\nBy the time of the American Civil war annual exports had reached $16 million (120,000 bales), which rose to $56 million by 1864, primarily due to the loss of the Confederate supply on the world market. Exports continued to grow even after the reintroduction of US cotton, produced now by a paid workforce, and Egyptian exports reached 1.2 million bales a year by 1903.\n\nThe English East India Company introduced the Britain to cheap calico and chintz cloth on the restoration of the monarchy in the 1660s. Initially imported as a novelty side line, from its spice trading posts in Asia, the cheap colourful cloth proved popular and overtook the EIC's spice trade by value in the late 17th century. The EIC embraced the demand, particularly for calico, by expanding its factories in Asia and producing and importing cloth in bulk, creating competition for domestic woollen and linen textile producers. The impacted weavers, spinners, dyers, shepherds and farmers objected and the calico question became one of the major issues of National politics between the 1680s and the 1730s. Parliament began to see a decline in domestic textile sales, and an increase in imported textiles from places like China and India. Seeing the East India Company and their textile importation as a threat to domestic textile businesses, Parliament passed the 1700 Calico Act, blocking the importation of cotton cloth. As there was no punishment for continuing to sell cotton cloth, smuggling of the popular material became commonplace. In 1721, dissatisfied with the results of the first act, Parliament passed a stricter addition, this time prohibiting the sale of most cottons, imported and domestic (exempting only thread Fustian and raw cotton). The exemption of raw cotton from the prohibition initially saw 2 thousand bales of cotton imported annually, to become the basis of a new indigenous industry, initially producing Fustian for the domestic market, though more importantly triggering the development of a series of mechanised spinning and weaving technologies, to process the material. This mechanised production was concentrated in new cotton mills, which slowly expanded till by the beginning of the 1770s seven thousand bales of cotton were imported annually, and pressure was put on Parliament, by the new mill owners, to remove the prohibition on the production and sale of pure cotton cloth, as they could easily compete with anything the EIC could import.\n\nThe acts were repealed in 1774, triggering a wave of investment in mill based cotton spinning and production, doubling the demand for raw cotton within a couple of years, and doubling it again every decade, into the 1840s\n\nIndian cotton textiles, particularly those from Bengal, continued to maintain a competitive advantage up until the 19th century. In order to compete with India, Britain invested in labour-saving technical progress, while implementing protectionist policies such as bans and tariffs to restrict Indian imports. At the same time, the East India Company's rule in India contributed to its deindustrialization, opening up a new market for British goods, while the capital amassed from Bengal after its 1757 conquest was used to invest in British industries such as textile manufacturing and greatly increase British wealth. British colonization also forced open the large Indian market to British goods, which could be sold in India without tariffs or duties, compared to local Indian producers who were heavily taxed, while raw cotton was imported from India without tariffs to British factories which manufactured textiles from Indian cotton, giving Britain a monopoly over India's large market and cotton resources. India served as both a significant supplier of raw goods to British manufacturers and a large captive market for British manufactured goods. Britain eventually surpassed India as the world's leading cotton textile manufacturer in the 19th century.\n\nIndia's cotton-processing sector changed during EIC expansion in India in the late 18th and early 19th centuries. From focusing on supplying the British market to supplying East Asia with raw cotton. As the Artisan produced textiles were no longer competitive with those produced Industrially, and Europe preferring the cheaper slave produced, long staple American, and Egyptian cottons, for its own materials.\n\nThe advent of the Industrial Revolution in Britain provided a great boost to cotton manufacture, as textiles emerged as Britain's leading export. In 1738, Lewis Paul and John Wyatt, of Birmingham, England, patented the roller spinning machine, as well as the flyer-and-bobbin system for drawing cotton to a more even thickness using two sets of rollers that traveled at different speeds. Later, the invention of the James Hargreaves' spinning jenny in 1764, Richard Arkwright's spinning frame in 1769 and Samuel Crompton's spinning mule in 1775 enabled British spinners to produce cotton yarn at much higher rates. From the late 18th century on, the British city of Manchester acquired the nickname \"Cottonopolis\" due to the cotton industry's omnipresence within the city, and Manchester's role as the heart of the global cotton trade.\n\nProduction capacity in Britain and the United States was improved by the invention of the modern cotton gin by the American Eli Whitney in 1793. Before the development of cotton gins, the cotton fibers had to be pulled from the seeds tediously by hand. By the late 1700s, a number of crude ginning machines had been developed. However, to produce a bale of cotton required over 600 hours of human labor, making large-scale production uneconomical in the United States, even with the use of humans as slave labor. The gin that Whitney manufactured (the Holmes design) reduced the hours down to just a dozen or so per bale. Although Whitney patented his own design for a cotton gin, he manufactured a prior design from Henry Odgen Holmes, for which Holmes filed a patent in 1796. Improving technology and increasing control of world markets allowed British traders to develop a commercial chain in which raw cotton fibers were (at first) purchased from colonial plantations, processed into cotton cloth in the mills of Lancashire, and then exported on British ships to captive colonial markets in West Africa, India, and China (via Shanghai and Hong Kong).\n\nBy the 1840s, India was no longer capable of supplying the vast quantities of cotton fibers needed by mechanized British factories, while shipping bulky, low-price cotton from India to Britain was time-consuming and expensive. This, coupled with the emergence of American cotton as a superior type (due to the longer, stronger fibers of the two domesticated Native American species, \"Gossypium hirsutum\" and \"Gossypium barbadense\"), encouraged British traders to purchase cotton from plantations in the United States and plantations in the Caribbean. By the mid-19th century, \"King Cotton\" had become the backbone of the southern American economy. In the United States, cultivating and harvesting cotton became the leading occupation of slaves.\n\nDuring the American Civil War, American cotton exports slumped due to a Union blockade on Southern ports, and also because of a strategic decision by the Confederate government to cut exports, hoping to force Britain to recognize the Confederacy or enter the war. This prompted the main purchasers of cotton, Britain and France, to turn to Egyptian cotton. British and French traders invested heavily in cotton plantations. The Egyptian government of Viceroy Isma'il took out substantial loans from European bankers and stock exchanges. After the American Civil War ended in 1865, British and French traders abandoned Egyptian cotton and returned to cheap American exports, sending Egypt into a deficit spiral that led to the country declaring bankruptcy in 1876, a key factor behind Egypt's occupation by the British Empire in 1882.\n\nDuring this time, cotton cultivation in the British Empire, especially Australia and India, greatly increased to replace the lost production of the American South. Through tariffs and other restrictions, the British government discouraged the production of cotton cloth in India; rather, the raw fiber was sent to England for processing. The Indian Mahatma Gandhi described the process:\n\n\nIn the United States, Southern cotton provided capital for the continuing development of the North. The cotton was largely produced through the labor of enslaved African Americans. It enriched both the Southern landowners and the Northern merchants. Much of the Southern cotton was trans-shipped through northern ports. In this era the slogan \"Cotton is king\" characterized the attitude of the South toward this monocrop.\n\nCotton remained a key crop in the Southern economy after emancipation and the end of the Civil War in 1865. Across the South, sharecropping evolved, in which landless black and white farmers worked land owned by others in return for a share of the profits. Some farmers rented the land and bore the production costs themselves. Until mechanical cotton pickers were developed, cotton farmers needed additional labor to hand-pick cotton. Picking cotton was a source of income for families across the South. Rural and small town school systems had split vacations so children could work in the fields during \"cotton-picking.\"\n\nIt was not until the 1950s that reliable harvesting machinery was introduced (prior to this, cotton-harvesting machinery had been too clumsy to pick cotton without shredding the fibers). During the first half of the 20th century, employment in the cotton industry fell, as machines began to replace laborers and the South's rural labor force dwindled during the World Wars.\n\nCotton remains a major export of the southern United States, and a majority of the world's annual cotton crop is of the long-staple American variety.\n\nSuccessful cultivation of cotton requires a long frost-free period, plenty of sunshine, and a moderate rainfall, usually from . Soils usually need to be fairly heavy, although the level of nutrients does not need to be exceptional. In general, these conditions are met within the seasonally dry tropics and subtropics in the Northern and Southern hemispheres, but a large proportion of the cotton grown today is cultivated in areas with less rainfall that obtain the water from irrigation. Production of the crop for a given year usually starts soon after harvesting the preceding autumn. Cotton is naturally a perennial but is grown as an annual to help control pests. Planting time in spring in the Northern hemisphere varies from the beginning of February to the beginning of June. The area of the United States known as the South Plains is the largest contiguous cotton-growing region in the world. While dryland (non-irrigated) cotton is successfully grown in this region, consistent yields are only produced with heavy reliance on irrigation water drawn from the Ogallala Aquifer.\nSince cotton is somewhat salt and drought tolerant, this makes it an attractive crop for arid and semiarid regions. As water resources get tighter around the world, economies that rely on it face difficulties and conflict, as well as potential environmental problems. For example, improper cropping and irrigation practices have led to desertification in areas of Uzbekistan, where cotton is a major export. In the days of the Soviet Union, the Aral Sea was tapped for agricultural irrigation, largely of cotton, and now salination is widespread.\n\nCotton can also be cultivated to have colors other than the yellowish off-white typical of modern commercial cotton fibers. Naturally colored cotton can come in red, green, and several shades of brown.\n\nGenetically modified (GM) cotton was developed to reduce the heavy reliance on pesticides. The bacterium \"Bacillus thuringiensis\" (Bt) naturally produces a chemical harmful only to a small fraction of insects, most notably the larvae of moths and butterflies, beetles, and flies, and harmless to other forms of life. The gene coding for Bt toxin has been inserted into cotton, causing cotton, called Bt cotton, to produce this natural insecticide in its tissues. In many regions, the main pests in commercial cotton are lepidopteran larvae, which are killed by the Bt protein in the transgenic cotton they eat. This eliminates the need to use large amounts of broad-spectrum insecticides to kill lepidopteran pests (some of which have developed pyrethroid resistance). This spares natural insect predators in the farm ecology and further contributes to noninsecticide pest management.\n\nBut Bt cotton is ineffective against many cotton pests, however, such as plant bugs, stink bugs, and aphids; depending on circumstances it may still be desirable to use insecticides against these. A 2006 study done by Cornell researchers, the Center for Chinese Agricultural Policy and the Chinese Academy of Science on Bt cotton farming in China found that after seven years these secondary pests that were normally controlled by pesticide had increased, necessitating the use of pesticides at similar levels to non-Bt cotton and causing less profit for farmers because of the extra expense of GM seeds. However, a 2009 study by the Chinese Academy of Sciences, Stanford University and Rutgers University refuted this. They concluded that the GM cotton effectively controlled bollworm. The secondary pests were mostly miridae (plant bugs) whose increase was related to local temperature and rainfall and only continued to increase in half the villages studied. Moreover, the increase in insecticide use for the control of these secondary insects was far smaller than the reduction in total insecticide use due to Bt cotton adoption. A 2012 Chinese study concluded that Bt cotton halved the use of pesticides and doubled the level of ladybirds, lacewings and spiders. The International Service for the Acquisition of Agri-biotech Applications (ISAAA) said that, worldwide, GM cotton was planted on an area of 25 million hectares in 2011. This was 69% of the worldwide total area planted in cotton.\n\nGM cotton acreage in India grew at a rapid rate, increasing from 50,000 hectares in 2002 to 10.6 million hectares in 2011. The total cotton area in India was 12.1 million hectares in 2011, so GM cotton was grown on 88% of the cotton area. This made India the country with the largest area of GM cotton in the world. A long-term study on the economic impacts of Bt cotton in India, published in the Journal PNAS in 2012, showed that Bt cotton has increased yields, profits, and living standards of smallholder farmers. The U.S. GM cotton crop was 4.0 million hectares in 2011 the second largest area in the world, the Chinese GM cotton crop was third largest by area with 3.9 million hectares and Pakistan had the fourth largest GM cotton crop area of 2.6 million hectares in 2011. The initial introduction of GM cotton proved to be a success in Australia – the yields were equivalent to the non-transgenic varieties and the crop used much less pesticide to produce (85% reduction). The subsequent introduction of a second variety of GM cotton led to increases in GM cotton production until 95% of the Australian cotton crop was GM in 2009 making Australia the country with the fifth largest GM cotton crop in the world. Other GM cotton growing countries in 2011 were Argentina, Myanmar, Burkina Faso, Brazil, Mexico, Colombia, South Africa and Costa Rica.\n\nCotton has been genetically modified for resistance to glyphosate a broad-spectrum herbicide discovered by Monsanto which also sells some of the Bt cotton seeds to farmers. There are also a number of other cotton seed companies selling GM cotton around the world. About 62% of the GM cotton grown from 1996 to 2011 was insect resistant, 24% stacked product and 14% herbicide resistant.\n\nCotton has gossypol, a toxin that makes it inedible. However, scientists have silenced the gene that produces the toxin, making it a potential food crop. On October 17, 2018, the USDA deregulated GE low-gossypol cotton.\n\nOrganic cotton is generally understood as cotton from plants not genetically modified and that is certified to be grown without the use of any synthetic agricultural chemicals, such as fertilizers or pesticides. Its production also promotes and enhances biodiversity and biological cycles. In the United States, organic cotton plantations are required to enforce the National Organic Program (NOP). This institution determines the allowed practices for pest control, growing, fertilizing, and handling of organic crops. As of 2007, 265,517 bales of organic cotton were produced in 24 countries, and worldwide production was growing at a rate of more than 50% per year.\n\nThe cotton industry relies heavily on chemicals, such as herbicides, fertilizers and insecticides, although a very small number of farmers are moving toward an organic model of production, and organic cotton products are now available for purchase at limited locations. These are popular for baby clothes and diapers. Under most definitions, organic products do not use genetic engineering. All natural cotton products are known to be both sustainable and hypoallergenic.\n\nHistorically, in North America, one of the most economically destructive pests in cotton production has been the boll weevil. Due to the US Department of Agriculture's highly successful Boll Weevil Eradication Program (BWEP), this pest has been eliminated from cotton in most of the United States. This program, along with the introduction of genetically engineered Bt cotton (which contains a bacterial gene that codes for a plant-produced protein that is toxic to a number of pests such as cotton bollworm and pink bollworm), has allowed a reduction in the use of synthetic insecticides.\n\nOther significant global pests of cotton include the pink bollworm, \"Pectinophora gossypiella\"; the chili thrips, \"Scirtothrips dorsalis\"; the cotton seed bug, \"Oxycarenus hyalinipennis\"; the tarnish plant bug, \"Lygus lineolaris\"; and the fall armyworm, \"Spodoptera frugiperda\", \"Xanthomonas citri subsp. malvacearum\".\n\nMost cotton in the United States, Europe and Australia is harvested mechanically, either by a cotton picker, a machine that removes the cotton from the boll without damaging the cotton plant, or by a cotton stripper, which strips the entire boll off the plant. Cotton strippers are used in regions where it is too windy to grow picker varieties of cotton, and usually after application of a chemical defoliant or the natural defoliation that occurs after a freeze. Cotton is a perennial crop in the tropics, and without defoliation or freezing, the plant will continue to grow.\n\nCotton continues to be picked by hand in developing countries.\n\nThe era of manufactured fibers began with the development of rayon in France in the 1890s. Rayon is derived from a natural cellulose and cannot be considered synthetic, but requires extensive processing in a manufacturing process, and led the less expensive replacement of more naturally derived materials. A succession of new synthetic fibers were introduced by the chemicals industry in the following decades. Acetate in fiber form was developed in 1924. Nylon, the first fiber synthesized entirely from petrochemicals, was introduced as a sewing thread by DuPont in 1936, followed by DuPont's acrylic in 1944. Some garments were created from fabrics based on these fibers, such as women's hosiery from nylon, but it was not until the introduction of polyester into the fiber marketplace in the early 1950s that the market for cotton came under threat. The rapid uptake of polyester garments in the 1960s caused economic hardship in cotton-exporting economies, especially in Central American countries, such as Nicaragua, where cotton production had boomed tenfold between 1950 and 1965 with the advent of cheap chemical pesticides. Cotton production recovered in the 1970s, but crashed to pre-1960 levels in the early 1990s.\n\nCotton is used to make a number of textile products. These include terrycloth for highly absorbent bath towels and robes; denim for blue jeans; cambric, popularly used in the manufacture of blue work shirts (from which we get the term \"blue-collar\"); and corduroy, seersucker, and cotton twill. Socks, underwear, and most T-shirts are made from cotton. Bed sheets often are made from cotton. Cotton also is used to make yarn used in crochet and knitting. Fabric also can be made from recycled or recovered cotton that otherwise would be thrown away during the spinning, weaving, or cutting process. While many fabrics are made completely of cotton, some materials blend cotton with other fibers, including rayon and synthetic fibers such as polyester. It can either be used in knitted or woven fabrics, as it can be blended with elastine to make a stretchier thread for knitted fabrics, and apparel such as stretch jeans. Cotton can be blended also with linen as Linen-cotton blends which give benefit of both plant materials which wrinkle resistant, lightweight, breathable and can keep heat more effectively than only linen. These blends are thinner and lighter, but stronger than only cotton.\n\nIn addition to the textile industry, cotton is used in fishing nets, coffee filters, tents, explosives manufacture (see nitrocellulose), cotton paper, and in bookbinding. The first Chinese paper was made of cotton fiber. Fire hoses were once made of cotton.\n\nThe cottonseed which remains after the cotton is ginned is used to produce cottonseed oil, which, after refining, can be consumed by humans like any other vegetable oil. The cottonseed meal that is left generally is fed to ruminant livestock; the gossypol remaining in the meal is toxic to monogastric animals. Cottonseed hulls can be added to dairy cattle rations for roughage. During the American slavery period, cotton root bark was used in folk remedies as an abortifacient, that is, to induce a miscarriage. Gossypol was one of the many substances found in all parts of the cotton plant and it was described by the scientists as 'poisonous pigment'. It also appears to inhibit the development of sperm or even restrict the mobility of the sperm. Also, it is thought to interfere with the menstrual cycle by restricting the release of certain hormones.\n\nCotton linters are fine, silky fibers which adhere to the seeds of the cotton plant after ginning. These curly fibers typically are less than long. The term also may apply to the longer textile fiber staple lint as well as the shorter fuzzy fibers from some upland species. Linters are traditionally used in the manufacture of paper and as a raw material in the manufacture of cellulose. In the UK, linters are referred to as \"cotton wool\". This can also be a refined product (\"absorbent cotton\" in U.S. usage) which has medical, cosmetic and many other practical uses. The first medical use of cotton wool was by Sampson Gamgee at the Queen's Hospital (later the General Hospital) in Birmingham, England.\n\nShiny cotton is a processed version of the fiber that can be made into cloth resembling satin for shirts and suits. However, it is hydrophobic (does not absorb water easily), which makes it unfit for use in bath and dish towels (although examples of these made from shiny cotton are seen).\n\nLong staple (LS cotton) is cotton of a longer fibre length and therefore of higher quality, while Extra-long staple cotton (ELS cotton) has longer fibre length still and of even higher quality. The name \"Egyptian cotton\" is broadly associated high quality cottons and is often an LS or (less often) an ELS cotton. The American cotton variety \"Pima\" cotton is often compared to Egyptian cotton, as both are used in high quality bed sheets and other cotton products. While Pima cotton is often grown in the American southwest, the Pima name is now used by cotton-producing nations such as Peru, Australia and Israel. Not all products bearing the Pima name are made with the finest cotton: American-grown ELS Pima cotton is trademarked as \"Supima\" cotton.\n\nCotton lisle is a finely-spun, tightly twisted type of cotton that is noted for being strong and durable. Lisle is composed of two strands that have each been twisted an extra twist per inch than ordinary yarns and combined to create a single thread. The yarn is spun so that it is compact and solid. This cotton is used mainly for underwear, stockings, and gloves. Colors applied to this yarn are noted for being more brilliant than colors applied to softer yarn. This type of thread was first made in the city of Lisle, France (now Lille), hence its name.\n\nThe largest producers of cotton, currently (2009), are China and India, with annual production of about 34 million bales and 33.4 million bales, respectively; most of this production is consumed by their respective textile industries. The largest exporters of raw cotton are the United States, with sales of $4.9 billion, and Africa, with sales of $2.1 billion. The total international trade is estimated to be $12 billion. Africa's share of the cotton trade has doubled since 1980. Neither area has a significant domestic textile industry, textile manufacturing having moved to developing nations in Eastern and South Asia such as India and China. In Africa, cotton is grown by numerous small holders. Dunavant Enterprises, based in Memphis, Tennessee, is the leading cotton broker in Africa, with hundreds of purchasing agents. It operates cotton gins in Uganda, Mozambique, and Zambia. In Zambia, it often offers loans for seed and expenses to the 180,000 small farmers who grow cotton for it, as well as advice on farming methods. Cargill also purchases cotton in Africa for export.\n\nThe 25,000 cotton growers in the United States are heavily subsidized at the rate of $2 billion per year although China now provides the highest overall level of cotton sector support. The future of these subsidies is uncertain and has led to anticipatory expansion of cotton brokers' operations in Africa. Dunavant expanded in Africa by buying out local operations. This is only possible in former British colonies and Mozambique; former French colonies continue to maintain tight monopolies, inherited from their former colonialist masters, on cotton purchases at low fixed prices.\n\nThe five leading exporters of cotton in 2011 are (1) the United States, (2) India, (3) Brazil, (4) Australia, and (5) Uzbekistan. The largest nonproducing importers are Korea, Taiwan, Russia, and Japan.\n\nIn India, the states of Maharashtra (26.63%), Gujarat (17.96%) and Andhra Pradesh (13.75%) and also Madhya Pradesh are the leading cotton producing states, these states have a predominantly tropical wet and dry climate.\n\nIn the United States, the state of Texas led in total production as of 2004, while the state of California had the highest yield per acre.\n\nCotton is an enormously important commodity throughout the world. However, many farmers in developing countries receive a low price for their produce, or find it difficult to compete with developed countries.\n\nThis has led to an international dispute (see United States – Brazil cotton dispute):\n\nOn 27 September 2002, Brazil requested consultations with the US regarding prohibited and actionable subsidies provided to US producers, users and/or exporters of upland cotton, as well as legislation, regulations, statutory instruments and amendments thereto providing such subsidies (including export credits), grants, and any other assistance to the US producers, users and exporters of upland cotton.\n\nOn 8 September 2004, the Panel Report recommended that the United States \"withdraw\" export credit guarantees and payments to domestic users and exporters, and \"take appropriate steps to remove the adverse effects or withdraw\" the mandatory price-contingent subsidy measures.\nWhile Brazil was fighting the US through the WTO's Dispute Settlement Mechanism against a heavily subsidized cotton industry, a group of four least-developed African countries – Benin, Burkina Faso, Chad, and Mali – also known as \"Cotton-4\" have been the leading protagonist for the reduction of US cotton subsidies through negotiations. The four introduced a \"Sectoral Initiative in Favour of Cotton\", presented by Burkina Faso's President Blaise Compaoré during the Trade Negotiations Committee on 10 June 2003.\n\nIn addition to concerns over subsidies, the cotton industries of some countries are criticized for employing child labor and damaging workers' health by exposure to pesticides used in production. The Environmental Justice Foundation has campaigned against the prevalent use of forced child and adult labor in cotton production in Uzbekistan, the world's third largest cotton exporter. The international production and trade situation has led to \"fair trade\" cotton clothing and footwear, joining a rapidly growing market for organic clothing, fair fashion or \"ethical fashion\". The fair trade system was initiated in 2005 with producers from Cameroon, Mali and Senegal.\n\nCotton is bought and sold by investors and price speculators as a tradable commodity on 2 different commodity exchanges in the United States of America.\n\n\nA temperature range of is the optimal range for mold development. At temperatures below , rotting of wet cotton stops. Damaged cotton is sometimes stored at these temperatures to prevent further deterioration.\n\nEgypt has a unique climatic temperature that the soil and the temperature provide an exceptional environment for cotton to grow rapidly.\n\n\nThe chemical composition of cotton is as follows:\n\nThere is a public effort to sequence the genome of cotton. It was started in 2007 by a consortium of public researchers. Their aim is to sequence the genome of cultivated, tetraploid cotton. \"Tetraploid\" means that its nucleus has two separate genomes, called A and D. The consortium agreed to first sequence the D-genome wild relative of cultivated cotton (\"G. raimondii\", a Central American species) because it is small and has few repetitive elements. It has nearly one-third of the bases of tetraploid cotton, and each chromosome occurs only once. Then, the A genome of \"G. arboreum\" would be sequenced. Its genome is roughly twice that of \"G. raimondii\". Part of the difference in size is due the amplification of \"retrotransposons\" (GORGE). After both diploid genomes are assembled, they would be used as models for sequencing the genomes of tetraploid cultivated species. Without knowing the diploid genomes, the euchromatic DNA sequences of AD genomes would co-assemble, and their repetitive elements would assemble independently into A and D sequences respectively. There would be no way to untangle the mess of AD sequences without comparing them to their diploid counterparts.\n\nThe public sector effort continues with the goal to create a high-quality, draft genome sequence from reads generated by all sources. The effort has generated Sanger reads of BACs, fosmids, and plasmids, as well as 454 reads. These later types of reads will be instrumental in assembling an initial draft of the D genome. In 2010, the companies Monsanto and Illumina completed enough Illumina sequencing to cover the D genome of \"G. raimondii\" about 50x. They announced that they would donate their raw reads to the public. This public relations effort gave them some recognition for sequencing the cotton genome. Once the D genome is assembled from all of this raw material, it will undoubtedly assist in the assembly of the AD genomes of cultivated varieties of cotton, but much work remains.\n\n\n"}
{"id": "4723731", "url": "https://en.wikipedia.org/wiki?curid=4723731", "title": "Critical points of the elements (data page)", "text": "Critical points of the elements (data page)\n\nDavid R. Lide (ed), \"CRC Handbook of Chemistry and Physics, 85th Edition\", online version. CRC Press. Boca Raton, Florida, 2003; Section 6, Fluid Properties; Critical Constants. Also agrees with Celsius values from Section 4: Properties of the Elements and Inorganic Compounds, Melting, Boiling, Triple, and Critical Point Temperatures of the Elements\n\nJ.A. Dean (ed), \"Lange's Handbook of Chemistry\" (15th Edition), McGraw-Hill, 1999; Section 6; Table 6.5 Critical Properties\n\nNational Physical Laboratory, \"Kaye and Laby Tables of Physical and Chemical Constants\"; D. Ambrose, M.B. Ewing, M.L. McGlashan, Critical constants and second virial coefficients of gases (retrieved Dec 2005)\n\nW.E. Forsythe (ed.), \"Smithsonian Physical Tables 9th ed.\", online version (1954; Knovel 2003). Table 259, Critical Temperatures, Pressures, and Densities of Gases\n"}
{"id": "40528319", "url": "https://en.wikipedia.org/wiki?curid=40528319", "title": "Damping capacity", "text": "Damping capacity\n\nDamping capacity is the ability of a material to absorb energy by converting mechanical energy into heat. \n\nA large damping capacity is desirable for materials used in structures where unwanted vibrations are induced during operation such as machine tool bases or crankshafts. Materials like brass and steel have small damping capacities allowing vibration energy to be transmitted through them without attenuation. An example of a material with a large damping capacity is gray cast iron. \n\nAn understanding of this effect can be gained from observation of a stress-strain diagram with exaggerated features. The units of stress are force per unit area, while strain has units of length per length. Any area covered by integrating each instant of a loading and unloading cycle will then be in terms of force times length per volume, which is equivalent to energy per unit volume. This energy represents the amount of mechanical energy being converted to heat in a volume of material resulting in damping.\n"}
{"id": "26984587", "url": "https://en.wikipedia.org/wiki?curid=26984587", "title": "Darkhovin Nuclear Power Plant", "text": "Darkhovin Nuclear Power Plant\n\nThe Darkhovin Nuclear Power Plant (also known as \"Esteghlal Nuclear Power Plant\") is a planned nuclear power plant located about 70 kilometers south of Ahvaz, Iran at the Karun river, as part of the nuclear program of Iran. One reactor is firmly planned. Some other projects on this site were cancelled.\n\nBefore the Iranian Revolution, Iran had signed a 2 billion dollar contract with French company Framatome to build two 910 MW pressurized water reactors, at Darkhovin. After the Revolution, France withdrew from the project and the engineering components of the plant were withheld in France. The Iranian components were then used to build the units 5 and 6 of Gravelines Nuclear Power Station in France which went online in 1985. Construction of the power station was halted during Iran–Iraq War. In 1992, Iran signed an agreement with China to build two 300 MW reactors at the site, which were to be completed within ten years and would have been similar to Chashma Nuclear Power Plant in Pakistan which is built by China. But later on China withdrew from the project under United States pressure.\n\nThe project was subsequently taken up by Iran itself, as no other country was ready to cooperate in its construction. Iran started to indigenously design the reactor for Darkhovin Nuclear Power Plant basing the design on IR-40 reactor using heavy water. The Iranian nuclear reactor design has a capacity of 360 MW. The plant was announced in 2008, originally scheduled to come online in 2016, but construction has been delayed. There is currently no public information on how many reactors the power station is planned to house. The plant is going to be Iran's first indigenously designed and built nuclear power plant besides the research reactor of IR-40.\n\nIn 1976 novel of Paul Erdman, \"Crash of '79\", Darkhovin Nuclear Power Plant is mentioned to have been completed by France and Mohammed Reza Pahlavi the then Shah of Iran uses the plant with the help from Israel and Switzerland to manufacture a dozen salted bombs.\n\nPlans for the site are not clear\n\n\n"}
{"id": "44494356", "url": "https://en.wikipedia.org/wiki?curid=44494356", "title": "Dekatherm", "text": "Dekatherm\n\nA dekatherm (dth) is a unit of energy used primarily to measure natural gas, developed in about 1972 by the Texas Eastern Transmission Corporation, a natural gas pipeline company. It is equal to 10 therms or 1,000,000 British thermal units (MMBtu) or 1.055 GJ. It is also approximately equal to one thousand cubic feet (Mcf) of natural gas or exactly one Mcf of natural gas with a heating value of 1000 Btu/cf.\n\nNatural gas is a mixture of gases containing approximately 80% methane (CH4) and its heating value varies from about 975 Btu/cf (1.026 Mcf/dth) to 1100 Btu/cf (0.91 Mcf/dth) depending on the mix of different gases in the gas stream. Noncombustible carbon dioxide (CO2) lowers the heating value of natural gas. Heavier hydrocarbons such as ethane (C2H6), propane (C3H8), and butane (C4H10) increase its heating value. Since customers who buy natural gas are actually buying heat, gas distribution companies who bill by volume routinely adjust their rates to compensate for this.\n\nTo simplify billing, Texas Eastern staff members coined the term decatherm (ten therms) and proposed using calorimeters to measure and bill gas delivered to customers in dekatherms. This would eliminate the constant calculation of rate adjustments to $/Mcf rates in order to assure that all customers received the same amount of heat per dollar. A settlement agreement reflecting the new billing procedure and settlement rates was filed in 1973. The FPC issued an order approving the settlement agreement and the new tariff using dekatherms later that year, Other gas distribution companies also began to use this process.\n\nIn spite of the need for adjustments, many companies continue to use cubic feet rather than dekatherms to measure and bill natural gas.\n"}
{"id": "2696466", "url": "https://en.wikipedia.org/wiki?curid=2696466", "title": "Demand response", "text": "Demand response\n\nDemand response is a change in the power consumption of an electric utility customer to better match the demand for power with the supply. Until recently electric energy could not be easily stored, so utilities have traditionally matched demand and supply by throttling the production rate of their power plants, taking generating units on or off line, or importing power from other utilities. There are limits to what can be achieved on the supply side, because some generating units can take a long time to come up to full power, some units may be very expensive to operate, and demand can at times be greater than the capacity of all the available power plants put together. Demand response seeks to adjust the demand for power instead of adjusting the supply.\n\nUtilities may signal demand requests to their customers in a variety of ways, including simple off-peak metering, in which power is cheaper at certain times of the day, and smart metering, in which explicit requests or changes in price can be communicated to customers.\n\nThe customer may adjust power demand by postponing some tasks that require large amounts of electric power, or may decide to pay a higher price for their electricity. Some customers may switch part of their consumption to alternate sources, such as on-site diesel generators.\n\nIn many respects, demand response can be put simply as a technology-enabled economic rationing system for electric power supply. In demand response, voluntary rationing is accomplished by price incentives—offering lower net unit pricing in exchange for reduced power consumption in peak periods. The direct implication is that users of electric power capacity not reducing usage (load) during peak periods will pay \"surge\" unit prices, whether directly, or factored into general rates.\n\nInvoluntary rationing, if employed, would be accomplished via rolling blackouts during peak load periods. Practically speaking, summer heat waves and winter deep freezes might be characterized by planned power outages for consumers and businesses if voluntary rationing via incentives fail to reduce load adequately to match total power supply.\n\nAccording to the Federal Energy Regulatory Commission, demand response (DR) is defined as: \n“Changes in electric usage by end-use customers from their normal consumption patterns in response to changes in the price of electricity over time, or to incentive payments designed to induce lower electricity use at times of high wholesale market prices or when system reliability is jeopardized.” DR includes all intentional modifications to consumption patterns of electricity to induce customers that are intended to alter the timing, level of instantaneous demand, or the total electricity consumption. It is expected that demand response programs will be designed to decrease electricity consumption or shift it from on-peak to off-peak periods depending on consumers’ \npreferences and lifestyles. Demand Response can be defined as \"a wide range of actions which can be taken at the customer side of the electricity meter in response to particular conditions within the electricity system (such as peak period network congestion or high prices)\". Demand response is a reduction in demand designed to reduce peak demand or avoid system emergencies. Hence, demand response can be a more cost-effective alternative than adding generation capabilities to meet the peak and or occasional demand spikes. The underlying objective of DR is to actively engage customers in modifying their consumption in response to pricing signals. The goal is to reflect supply expectations through consumer price signals or controls and enable dynamic changes in consumption relative to price.\n\nIn electricity grids, DR is similar to dynamic demand mechanisms to manage customer consumption of electricity in response to supply conditions, for example, having electricity customers reduce their consumption at critical times or in response to market prices. The difference is that demand response mechanisms respond to explicit requests to shut off, whereas dynamic demand devices passively shut off when stress in the grid is sensed. Demand response can involve actually curtailing power used or by starting on-site generation which may or may not be connected in parallel with the grid. This is a quite different concept from energy efficiency, which means using less power to perform the same tasks, on a continuous basis or whenever that task is performed. At the same time, demand response is a component of smart energy demand, which also includes energy efficiency, home and building energy management, distributed renewable resources, and electric vehicle charging.\n\nCurrent demand response schemes are implemented with large and small commercial as well as residential customers, often through the use of dedicated control systems to shed loads in response to a request by a utility or market price conditions. Services (lights, machines, air conditioning) are reduced according to a preplanned load prioritization scheme during the critical time frames. An alternative to load shedding is on-site generation of electricity to supplement the power grid. Under conditions of tight electricity supply, demand response can significantly decrease the peak price and, in general, electricity price volatility.\n\nDemand response is generally used to refer to mechanisms used to encourage consumers to reduce demand, thereby reducing the peak demand for electricity. Since electrical generation and transmission systems are generally sized to correspond to peak demand (plus margin for forecasting error and unforeseen events), lowering peak demand reduces overall plant and capital cost requirements. Depending on the configuration of generation capacity, however, demand response may also be used to increase demand (load) at times of high production and low demand. Some systems may thereby encourage energy storage to arbitrage between periods of low and high demand (or low and high prices).\n\nThere are three types of demand response - emergency demand response, economic demand response and ancillary services demand response. Emergency demand response is employed to avoid involuntary service interruptions during times of supply scarcity. Economic demand response is employed to allow electricity customers to curtail their consumption when the productivity or convenience of consuming that electricity is worth less to them than paying for the electricity. Ancillary services demand response consists of a number of specialty services that are needed to ensure the secure operation of the transmission grid and which have traditionally been provided by generators.\n\nSmart grid applications improve the ability of electricity producers and consumers to communicate with one another and make decisions about how and when to produce and consume electrical power. This emerging technology will allow customers to shift from an event-based demand response where the utility requests the shedding of load, towards a more 24/7-based demand response where the customer sees incentives for controlling load all the time. Although this back-and-forth dialogue increases the opportunities for demand response, customers are still largely influenced by economic incentives and are reluctant to relinquish total control of their assets to utility companies.\n\nOne advantage of a smart grid application is time-based pricing. Customers who traditionally pay a fixed rate for consumed energy (kWh) and requested peak load can set their threshold and adjust their usage to take advantage of fluctuating prices. This may require the use of an energy management system to control appliances and equipment and can involve economies of scale. Another advantage, mainly for large customers with generation, is being able to closely monitor, shift, and balance load in a way that allows the customer to save peak load and not only save on kWh and kW/month but be able to trade what they have saved in an energy market. Again this involves sophisticated energy management systems, incentives, and a viable trading market.\n\nSmart grid applications increase the opportunities for demand response by providing real time data to producers and consumers, but the economic and environmental incentives remain the driving force behind the practice.\n\nOne of the most important means of demand response in the future smart grids is electric vehicles. Aggregation of this new source of energy, which is also a new source of uncertainty in the electrical systems, is critical to preserving the stability and quality of smart grids, consequently, the electric vehicle parking lots can be considered a demand response aggregation entity.\n\nIn most electric power systems, some or all consumers pay a fixed price per unit of electricity independent of the cost of production at the time of consumption. The consumer price may be established by the government or a regulator, and typically represents an average cost per unit of production over a given timeframe (for example, a year). Consumption therefore is not sensitive to the cost of production in the short term (e.g. on an hourly basis). In economic terms, consumers' usage of electricity is inelastic in short time frames since the consumers do not face the actual price of production; if consumers were to face the short run costs of production they would be more inclined to change their use of electricity in reaction to those price signals. A pure economist might extrapolate the concept to hypothesize that consumers served under these fixed-rate tariffs are endowed with theoretical \"call options\" on electricity, though in reality, like any other business, the customer is simply buying what is on offer at the agreed price. A customer in a department store buying a $10 item at 9.00 am might notice 10 sales staff on the floor but only one occupied serving him or her, while at 3.00 pm the customer could buy the same $10 article and notice all 10 sales staff occupied. In a similar manner, the department store cost of sales at 9.00 am might therefore be 5-10 times that of its cost of sales at 3.00 pm, but it would be far-fetched to claim that the customer, by not paying significantly more for the article at 9.00 am than at 3.00 pm, had a 'call option' on the $10 article.\n\nIn virtually all power systems electricity is produced by generators that are dispatched in merit order, i.e., generators with the lowest marginal cost (lowest variable cost of production) are used first, followed by the next cheapest, etc., until the instantaneous electricity demand is satisfied. In most power systems the wholesale price of electricity will be equal to the marginal cost of the highest cost generator that is injecting energy, which will vary with the level of demand. Thus the variation in pricing can be significant: for example, in Ontario between August and September 2006, wholesale prices (in Canadian Dollars) paid to producers ranged from a peak of $318 per MW·h to a minimum of - (negative) $3.10 per MW·h. It is not unusual for the price to vary by a factor of two to five due to the daily demand cycle. A negative price indicates that producers were being charged to provide electricity to the grid (and consumers paying real-time pricing may have actually received a rebate for consuming electricity during this period). This generally occurs at night when demand falls to a level where all generators are operating at their minimum output levels and some of them must be shut down. The negative price is the inducement to bring about these shutdowns in a least-cost manner.\n\nTwo Carnegie Mellon studies in 2006 looked at the importance of demand response for the electricity industry in general terms and with specific application of real-time pricing for consumers for the PJM Interconnection Regional Transmission authority. The latter study found that even small shifts in peak demand would have a large effect on savings to consumers and avoided costs for additional peak capacity: a 1% shift in peak demand would result in savings of 3.9%, billions of dollars at the system level. An approximately 10% reduction in peak demand (achievable depending on the elasticity of demand) would result in systems savings of between $8 to $28 billion.\n\nIn a discussion paper, Ahmad Faruqui, a principal with the Brattle Group, estimates that a 5 percent reduction in US peak electricity demand could produce approximately $35 billion in cost savings over a 20-year period, exclusive of the cost of the metering and communications needed to implement the dynamic pricing needed to achieve these reductions. While the net benefits would be significantly less than the claimed $35 billion, they would still be quite substantial. In Ontario, Canada, the Independent Electricity System Operator has noted that in 2006, peak demand exceeded 25,000 megawatts during only 32 system hours (less than 0.4% of the time), while maximum demand during the year was just over 27,000 megawatts. The ability to \"shave\" peak demand based on reliable commitments would therefore allow the province to reduce built capacity by approximately 2,000 megawatts.\n\nIn an electricity grid, electricity consumption and production must balance at all times; any significant imbalance could cause grid instability or severe voltage fluctuations, and cause failures within the grid. Total generation capacity is therefore sized to correspond to total peak demand with some margin of error and allowance for contingencies (such as plants being off-line during peak demand periods). Operators will generally plan to use the least expensive generating capacity (in terms of marginal cost) at any given period, and use additional capacity from more expensive plants as demand increases. Demand response in most cases is targeted at reducing peak demand to reduce the risk of potential disturbances, avoid additional capital cost requirements for additional plants, and avoid use of more expensive and/or less efficient operating plants. Consumers of electricity will also pay higher prices if generation capacity is used from a higher-cost source of power generation.\n\nDemand response may also be used to increase demand during periods of high supply and/or low demand. Some types of generating plant must be run at close to full capacity (such as nuclear), while other types may produce at negligible marginal cost (such as wind and solar). Since there is usually limited capacity to store energy, demand response may attempt to increase load during these periods to maintain grid stability. For example, in the province of Ontario in September 2006, there was a short period of time when electricity prices were negative for certain users. Energy storage such as pumped-storage hydroelectricity is a way to increase load during periods of low demand for use during later periods. Use of demand response to increase load is less common, but may be necessary or efficient in systems where there are large amounts of generating capacity that cannot be easily cycled down.\n\nSome grids may use pricing mechanisms that are not real-time, but easier to implement (users pay higher prices during the day and lower prices at night, for example) to provide some of the benefits of the demand response mechanism with less demanding technological requirements. In the UK, Economy 7 and similar schemes that attempt to shift demand associated with electric heating to overnight off-peak periods have been in operation since the 1970s. More recently, in 2006 Ontario began implementing a \"smart meter\" program that implements \"time-of-use\" (TOU) pricing, which tiers pricing according to on-peak, mid-peak and off-peak schedules. During the winter, on-peak is defined as morning and early evening, mid-peak as midday to late afternoon, and off-peak as nighttime; during the summer, the on-peak and mid-peak periods are reversed, reflecting air conditioning as the driver of summer demand. As of May 1, 2015, most Ontario electrical utilities have completed converting all customers to \"smart meter\" time-of-use billing with on-peak rates about 200% and mid-peak rates about 150% of the off-peak rate per kWh.\n\nAustralia has national standards for Demand Response (AS/NZS 4755 series), which has been implemented nation wide by electricity distributors for several decades, e.g. controlling storage water heaters, air conditioners and pool pumps. In 2016, how to manage electrical energy storage (e.g. batteries) has been added into the series of standards.\n\nElectrical generation and transmission systems may not always meet peak demand requirements— the greatest amount of electricity required by all utility customers within a given region. In these situations, overall demand must be lowered, either by turning off service to some devices or cutting back the supply voltage (brownouts), in order to prevent uncontrolled service disruptions such as power outages (widespread blackouts) or equipment damage. Utilities may impose load shedding on service areas via rolling blackouts or by agreements with specific high-use industrial consumers to turn off equipment at times of system-wide peak demand.\n\nEnergy consumers need some incentive to respond to such a request from a demand response provider (see list of providers below). Demand response incentives can be formal or informal. For example, the utility might create a tariff-based incentive by passing along short-term increases in the price of electricity, or they might impose mandatory cutbacks during a heat wave for selected high-volume users, who are compensated for their participation. Other users may receive a rebate or other incentive based on firm commitments to reduce power during periods of high demand, sometimes referred to as \"negawatts.\"\n\nCommercial and industrial power users might impose load shedding on themselves, without a request from the utility. Some businesses generate their own power and wish to stay within their energy production capacity to avoid buying power from the grid. Some utilities have commercial tariff structures that set a customer's power costs for the month based on the customer's moment of highest use, or peak demand. This encourages users to flatten their demand for energy, known as energy demand management, which sometimes requires cutting back services temporarily.\n\nSmart metering has been implemented in some jurisdictions to provide real-time pricing for all types of users, as opposed to fixed-rate pricing throughout the demand period. In this application, users have a direct incentive to reduce their use at high-demand, high-price periods. Many users may not be able to effectively reduce their demand at various times, or the peak prices may be lower than the level required to induce a change in demand during short time periods (users have low price sensitivity, or elasticity of demand is low). Automated control systems exist, which, although effective, may be too expensive to be feasible for some applications.\n\nThe modern power grid is making a transition from the traditional vertically integrated utility structures to distributed systems as we begin to integrate higher penetrations of renewable energy generation. These sources of energy are often diffusely distributed and intermittent by nature. These features introduce problems in grid stability and efficiency which lead to limitations on the amount of these resources which can be effectively added to the grid. In a traditional vertically integrated grid, energy is provided by utility generators which are able to respond to changes in demand. Generation output by renewable resources is governed by environmental conditions and is generally not able to respond to changes in demand. Responsive control over non-critical loads which are connected to the grid has been shown to be an effective strategy which is able to mitigate harmful fluctuations introduced by these renewable resources. In this way instead of letting the generation respond to changes in demand, we have the demand respond to changes in generation. This is the basis of demand response. In order to implement demand response systems, we must be able to coordinate large numbers of distributed resources through sensors, actuators, and communications protocols. To be effective, the devices need to be economical, robust, and yet still effective at managing their tasks of control. In addition, a strong control mechanism must be created which is able to coordinate over large networks of devices to manage and optimize these distributed systems both from an economic standpoint and a security standpoint in grid stabilization.\n\nIn addition, the increased presence of variable renewable generation drives a greater need for authorities to procure more ancillary services (AS) for grid balance. One of these services is contingency reserve (CR), which is used to regulate the grid frequency in contingencies. Many independent system operators (ISO) are structuring the rules of AS markets such that demand response (DR) can participate alongside traditional supply-side resources. The available capacity of the generators can be used more efficiently for power production which they were designed for and not CR, thereby cutting costs and reducing pollution. As the ratio of inverter-based generation compared to conventional generation increases, the mechanical inertia used to stabilize frequency decreases. When coupled with the sensitivity of inverter-based generation to transient frequencies, the provision of ancillary services from other sources than generators becomes increasingly important.\n\nTechnologies are available, and more are under development, to automate the process of demand response. Such technologies detect the need for load shedding, communicate the demand to participating users, automate load shedding, and verify compliance with demand-response programs. GridWise and EnergyWeb are two major federal initiatives in the United States to develop these technologies. Universities and private industry are also doing research and development in this arena. Scalable and comprehensive software solutions for DR enable business and industry growth.\n\nSome utilities are considering and testing automated systems connected to industrial, commercial and residential users that can reduce consumption at times of peak demand, essentially delaying draw marginally. Although the amount of demand delayed may be small, the implications for the grid (including financial) may be substantial, since system stability planning often involves building capacity for extreme peak demand events, plus a margin of safety in reserve. Such events may only occur a few times per year.\n\nThe process may involve turning down or off certain appliances or sinks (and, when demand is unexpectedly low, potentially increasing usage). For example, heating may be turned down or air conditioning or refrigeration may be turned up (turning up to a higher temperature uses less electricity), delaying slightly the draw until a peak in usage has passed. In the city of Toronto, certain residential users can participate in a program (Peaksaver AC) whereby the system operator can automatically control hot water heaters or air conditioning during peak demand; the grid benefits by delaying peak demand (allowing peaking plants time to cycle up or avoiding peak events), and the participant benefits by delaying consumption until after peak demand periods, when pricing should be lower. Although this is an experimental program, at scale these solutions have the potential to reduce peak demand considerably. The success of such programs depends on the development of appropriate technology, a suitable pricing system for electricity, and the cost of the underlying technology. Bonneville Power experimented with direct-control technologies in Washington and Oregon residences, and found that the avoided transmission investment would justify the cost of the technology.\n\nOther methods to implementing demand response approach the issue of subtly reducing duty cycles rather than implementing thermostat setbacks. These can be implemented using customized building automation systems programming, or through swarm-logic methods coordinating multiple loads in a facility (e.g. Encycle's EnviroGrid controllers).\n\nSimilar approach can be implemented for managing air conditioning peak demand in summer peak regions. Pre-cooling or maintaining slightly higher thermostat setting can help with the peak demand reduction. \n\nIn 2008 it was announced that electric refrigerators will be sold in the UK sensing dynamic demand which will delay or advance the cooling cycle based on monitoring grid frequency but they are not readily available as of 2018.\n\nIndustrial customers are also providing demand response. Compared with commercial and residential loads, industrial loads have the following advantages: the magnitude of power consumption by an industrial manufacturing plant and the change in power it can provide are generally very large; besides, the industrial plants usually already have the infrastructures for control, communication and market participation, which enables the provision of demand response; moreover, some industrial plants such as the aluminum smelter are able to offer fast and accurate adjustments in their power consumption. For example, Alcoa's Warrick Operation is participating in MISO as a qualified demand response resource, and the Trimet Aluminium uses its smelter as a short-term mega-battery. The selection of suitable industries for demand response provision is typically based on an assessment of the so-called value of lost load.\n\nShedding loads during peak demand is important because it reduces the need for new power plants. To respond to high peak demand, utilities build very capital-intensive power plants and lines. Peak demand happens just a few times a year, so those assets run at a mere fraction of their capacity. Electric users pay for this idle capacity through the prices they pay for electricity. According to the Demand Response Smart Grid Coalition, 10%–20% of electricity costs in the United States are due to peak demand during only 100 hours of the year. DR is a way for utilities to reduce the need for large capital expenditures, and thus keep rates lower overall; however, there is an economic limit to such reductions because consumers lose the productive or convenience value of the electricity not consumed. Thus, it is misleading to only look at the cost savings that demand response can produce without also considering what the consumer gives up in the process.\n\nIt is estimated that a 5% lowering of demand would have resulted in a 50% price reduction during the peak hours of the California electricity crisis in 2000–2001. With consumers facing peak pricing and reducing their demand, the market should become more resilient to intentional withdrawal of offers from the supply side.\n\nResidential and commercial electricity use often vary drastically during the day, and demand response attempts to reduce the variability based on pricing signals. There are three underlying tenets to these programs:\nIn addition, significant peaks may only occur rarely, such as two or three times per year, requiring significant capital investments to meet infrequent events.\n\nThe United States Energy Policy Act of 2005 has mandated the Secretary of Energy to submit to the US Congress \"a report that identifies and quantifies the national benefits of demand response and makes a recommendation on achieving specific levels of such benefits by January 1, 2007.\" Such a report was published in February 2006.\n\nThe report estimates that in 2004 potential demand response capability equaled about 20,500 megawatts (MW), 3% of total U.S. peak demand, while actual delivered peak demand reduction was about 9,000 MW (1.3% of peak), leaving ample margin for improvement. It is further estimated that load management capability has fallen by 32% since 1996. Factors affecting this trend include fewer utilities offering load management services, declining enrollment in existing programs, the changing role and responsibility of utilities, and changing supply/demand balance.\n\nTo encourage the use and implementation of demand response in the United States, the Federal Energy Regulatory Commission (FERC) issued Order No. 745 in March 2011, which requires a certain level of compensation for providers of economic demand response that participate in wholesale power markets. The order is highly controversial and has been opposed by a number of energy economists, including Professor William W. Hogan at Harvard University's Kennedy School. Professor Hogan asserts that the order overcompensates providers of demand response, thereby encouraging the curtailment of electricity whose economic value exceeds the cost of producing it. Professor Hogan further asserts that Order No. 745 is anticompetitive and amounts to “…an application of regulatory authority to enforce a buyer’s cartel.” Several affected parties, including the State of California, have filed suit in federal court challenging the legality of Order 745. A debate regarding the economic efficiency and fairness of Order 745 appeared in a series of articles published in The Electricity Journal.\n\nOn May 23, 2014, the D.C. Circuit Court of Appeals vacated Order 745 in its entirety. On May 4, 2015, the United States Supreme Court agreed to review the DC Circuit's ruling, addressing two questions: \n\nOn January 25, 2016, the United States Supreme Court in a 6-2 decision in \"FERC v. Electric Power Supply Ass'n\" concluded that the Federal Energy Regulatory Commission acted within its authority to ensure \"just and reasonable\" rates in the wholesale energy market.\n\nAs of December 2009 UK National Grid had 2369 MW contracted to provide demand response, known as STOR, the demand side provides 839 MW (35%) from 89 sites. Of this 839 MW approximately 750 MW is back-up generation with the remaining being load reduction. A paper based on extensive half-hourly demand profiles and observed electricity demand shifting for different commercial and industrial buildings in the UK shows that only a small minority engaged in load shifting and demand turn-down, while the majority of demand response is provided by stand-by generators.\n"}
{"id": "83193", "url": "https://en.wikipedia.org/wiki?curid=83193", "title": "Drawbridge mentality", "text": "Drawbridge mentality\n\nA drawbridge mentality is the attitude of people who have migrated to a more exclusive or more \"unspoilt\" community and then campaign to preserve the tranquility of that community by opposing further inward migration by people or businesses and, possibly, any development or refurbishment, including plans put forward by those already located there.\n\nThe term can imply a selfish attitude and can be taken as an insult by people who have strong affection for their home locality and wish to protect it from any changes. It is closely related to the NIMBY attitude.\n\nA drawbridge was historically the hinged bridge at a castle's gates providing entry across a defensive moat. Raising the drawbridge to a vertical position was therefore one's means by which intruders could be shut out of the castle.\n\n"}
{"id": "4269119", "url": "https://en.wikipedia.org/wiki?curid=4269119", "title": "Earth Liberation Front Press Office", "text": "Earth Liberation Front Press Office\n\nThe North American Earth Liberation Front Press Office (NAELFPO or ELFPO) is a legal, above-ground news service dedicated to publicizing the direct action of the Earth Liberation Front (ELF). \n\nThe Press Office receives anonymous communiques from the ELF and distributes the political and social motives behind the covert and underground cells to the mass media and public.\n\nNAELFPO was founded in 1999 by Craig Rosebraugh and Leslie James Pickering. It was founded in Portland, Oregon, \"to work to explain the importance and necessity of clandestine guerrilla action in a revolutionary movement to liberate the Earth from the stranglehold of the system.\"\n\n\n"}
{"id": "24488126", "url": "https://en.wikipedia.org/wiki?curid=24488126", "title": "Edward Hoare (environmentalist)", "text": "Edward Hoare (environmentalist)\n\nEdward Hoare (born 1949), formerly of Hoare's Bank, is an English philanthropist, environmentalist and mind-mapping pioneer.\n\nBorn at Stourhead in Wiltshire, he left home as a young man, living in Rio de Janeiro, Brazil, from 1968 to 1971, where he worked in a bank. He returned home and took training as a Chartered Accountant from 1972 to 1976. In 1977, he embarked on a trip around the world, during which he visited the Royal Chitwan National Park in Nepal, leading to his interests in environmental issues.\n\nHoare joined the family bank, C. Hoare & Co, and became a bankers' agent, a position he held for over 30 years. He was a member of the Hoare's Bank Family Forum, which distributes funds from the Golden Bottle Trust, formed in 1985 and continuing the Hoare family tradition of giving to good causes since the early 18th century.\n\nHe is a vice-president of two conservation charities: Fauna and Flora International and the International Trust for Nature Conservation.\n\nHe is one of the founders of Gooisoft, the makers of Thortspace 3D collaborative mind-mapping software and creator of the Global Goals (SDGs) ecosystem.\n\nEdward Hoare is husband to Suzie Hoare and father to Rennie Hoare, Victoria Hoare and James Hoare\n"}
{"id": "17904365", "url": "https://en.wikipedia.org/wiki?curid=17904365", "title": "Eolica Sǎcele Wind Farm", "text": "Eolica Sǎcele Wind Farm\n\nThe Eolica Săcele Wind Farm is a proposed wind power project in Săcele, Constanţa County, Romania. It will consist of eight individual wind farms connected together. It will have 126 individual wind turbines with a nominal output of around 2 MW which will deliver up to 252 MW of power, enough to power over 165,000 homes, with a capital investment required of approximately US$310 million.\n"}
{"id": "532034", "url": "https://en.wikipedia.org/wiki?curid=532034", "title": "Fluorophore", "text": "Fluorophore\n\nA fluorophore (or fluorochrome, similarly to a chromophore) is a fluorescent chemical compound that can re-emit light upon light excitation. Fluorophores typically contain several combined aromatic groups, or planar or cyclic molecules with several π bonds.\n\nFluorophores are sometimes used alone, as a tracer in fluids, as a dye for staining of certain structures, as a substrate of enzymes, or as a probe or indicator (when its fluorescence is affected by environmental aspects such as polarity or ions). More generally they are covalently bonded to a macromolecule, serving as a marker (or dye, or tag, or reporter) for affine or bioactive reagents (antibodies, peptides, nucleic acids). Fluorophores are notably used to stain tissues, cells, or materials in a variety of analytical methods, i.e., fluorescent imaging and spectroscopy.\n\nFluorescein, by its amine reactive isothiocyanate derivative FITC, has been one of the most popular fluorophores. From antibody labeling, the applications have spread to nucleic acids thanks to (FAM (Carboxyfluorescein), TET...). Other historically common fluorophores are derivatives of rhodamine (TRITC), coumarin, and cyanine. Newer generations of fluorophores, many of which are proprietary, often perform better, being more photostable, brighter, and/or less pH-sensitive than traditional dyes with comparable excitation and emission.\n\nThe fluorophore absorbs light energy of a specific wavelength and re-emits light at a longer wavelength. The absorbed wavelengths, energy transfer efficiency, and time before emission depend on both the fluorophore structure and its chemical environment, as the molecule in its excited state interacts with surrounding molecules. Wavelengths of maximum absorption (≈ excitation) and emission (for example, Absorption/Emission = 485 nm/517 nm) are the typical terms used to refer to a given fluorophore, but the whole spectrum may be important to consider. The excitation wavelength spectrum may be a very narrow or broader band, or it may be all beyond a cutoff level. The emission spectrum is usually sharper than the excitation spectrum, and it is of a longer wavelength and correspondingly lower energy. Excitation energies range from ultraviolet through the visible spectrum, and emission energies may continue from visible light into the near infrared region.\n\nMain characteristics of fluorophores are :\n\nThese characteristics drive other properties, including the photobleaching or photoresistance (loss of fluorescence upon continuous light excitation). Other parameters should be considered, as the polarity of the fluorophore molecule, the fluorophore size and shape (i.e. for polarization fluorescence pattern), and other factors can change the behavior of fluorophores.\n\nFluorophores can also be used to quench the fluorescence of other fluorescent dyes (see article Quenching (fluorescence)) or to relay their fluorescence at even longer wavelength (see article FRET)\n\nSee more on fluorescence principle.\n\nMost fluorophores are organic small molecules of 20 - 100 atoms (200 - 1000 Dalton - the molecular weight may be higher depending on grafted modifications, and conjugated molecules), but there are also much larger natural fluorophores that are proteins: Green fluorescent protein (GFP) is 27 kDa and several phycobiliproteins (PE, APC...) are ≈240kDa.\n\nFluorescence particles are not considered fluorophores (quantum dot: 2-10 nm diameter, 100-100,000 atoms).\n\nThe size of the fluorophore might sterically hinder the tagged molecule, and affect the fluorescence polarity.\n\nFluorophore molecules could be either utilized alone, or serve as a fluorescent motif of a functional system. Based on molecular complexity and synthetic methods, fluorophore molecules could be generally classified into four categories: proteins and peptides, small organic compounds, synthetic oligomers and polymers, and multi-component systems.\nFluorescent proteins GFP (green), YFP (yellow) and RFP (red) can be attached to other specific proteins to form a fusion protein, synthesized in cells after transfection of a suitable plasmid carrier.\n\nNon-protein organic fluorophores belong to following major chemical families:\n\nThese fluorophores fluoresce thanks to delocalized electrons which can jump a band and stabilize the energy absorbed. Benzene, one of the simplest aromatic hydrocarbons, for example, is excited at 254 nm and emits at 300 nm. This discriminates fluorophores from quantum dots, which are fluorescent semiconductor nanoparticles.\n\nThey can be attached to protein to specific functional groups, such as\n- amino groups (Active ester, Carboxylate, Isothiocyanate, hydrazine)\n- carboxyl groups (carbodiimide)\n- thiol (maleimide, acetyl bromide)\n- azide (via click chemistry or non-specifically (glutaraldehyde)).\n\nAdditionally, various functional groups can be present to alter its properties, such as solubility, or confer special properties, such as boronic acid which binds to sugars or multiple carboxyl groups to bind to certain cations. When the dye contains an electron-donating and an electron-accepting group at opposite ends of the aromatic system, this dye will probably be sensitive to the environment's polarity (solvatochromic), hence called environment-sensitive. Often dyes are used inside cells, which are impermeable to charged molecules, as a result of this the carboxyl groups are converted into an ester, which is removed by esterases inside the cells, e.g., fura-2AM and fluorescein-diacetate.\n\nThe following dye families are trademark groups, and do not necessarily share structural similarities.\n\nAbbreviations: <br>\nEx (nm): Excitation wavelength in nanometers <br>\nEm (nm): Emission wavelength in nanometers<br>\nMW: Molecular weight<br>\nQY: Quantum yield\n\nAbbreviations: <br>\nEx (nm): Excitation wavelength in nanometers <br>\nEm (nm): Emission wavelength in nanometers<br>\nMW: Molecular weight <br>\nQY: Quantum yield <br>\nBR: Brightness: Molar absorption coefficient * quantum yield / 1000 <br>\nPS: Photostability: time [sec] to reduce brightness by 50%\n\nFluorophores have particular importance in the field of biochemistry and protein studies, e.g., in immunofluorescence but also in cell analysis, e.g. immunohistochemistry\n\nAdditionally fluorescent dyes find a wide use in industry, going under the name of \"neon colours\", such as\n\n\n"}
{"id": "29262361", "url": "https://en.wikipedia.org/wiki?curid=29262361", "title": "Folding endurance", "text": "Folding endurance\n\nIn paper testing, folding endurance is defined as the logarithm (to the base of ten) of the number of double folds that are required to make a test piece break under standardized conditions:\n\nwhere \"F\" is the folding endurance and \"d\" the number of double folds.\n\nFolding endurance is especially applicable for papers used for maps, bank notes, archival documents, etc. The direction of the grain in relation to the folding line, the type of fibres used, the fibre contents, the calliper of the test piece, etc., as well as which type of folding tester that is used affect how many double folds a test piece can take.\n\nFolding endurance must not be confused with the related term fold number.\n\n\n"}
{"id": "10320835", "url": "https://en.wikipedia.org/wiki?curid=10320835", "title": "Frank Isakson Prize for Optical Effects in Solids", "text": "Frank Isakson Prize for Optical Effects in Solids\n\nThe Frank Isakson Prize for Optical Effects in Solids is a prize that has been awarded every second year by the American Physical Society since 1980. The recipient is chosen for \"outstanding optical research that leads to breakthroughs in the condensed matter sciences.\". The prize is named after Frank Isakson, and as of 2007 it is valued at $5,000.\n\n\n"}
{"id": "30325652", "url": "https://en.wikipedia.org/wiki?curid=30325652", "title": "Garth Owen-Smith", "text": "Garth Owen-Smith\n\nGarth Owen-Smith is a Namibian environmentalist. He was awarded the Goldman Environmental Prize in 1993, jointly with Margaret Jacobsohn, for their efforts on conservation of wildlife in Namibia, where illegal hunting was threatening species such as elephants, lions and black rhinos.\n\nHe was awarded the Global 500 Roll of Honour in 1994.\n"}
{"id": "9956818", "url": "https://en.wikipedia.org/wiki?curid=9956818", "title": "Geoplin", "text": "Geoplin\n\nGeoplin is a natural gas company in Slovenia. The biggest shareholder of the company is the Government of Slovenia.\n\n"}
{"id": "13117257", "url": "https://en.wikipedia.org/wiki?curid=13117257", "title": "Hemacite", "text": "Hemacite\n\nHemacite is a material made from sawdust and the blood of slaughtered animals cattle and pig. It was invented (and patented) by Dr W H Dibble of New Jersey in the last quarter of the nineteenth century. Hydraulic pressure (40,000 psi) and chemical compounds, blood and sawdust were transformed by Dibble's Hemacite Manufacturing Company into everything from doorknobs and roller skate wheels to cash register buttons and telephone receivers; there is even extensive use in Victorian jewellery. Hemacite was inexpensive but fell out of favor with the popularity of new plastics like Bakelite, it is quite easy to misidentify Hemacite with Bakelite.\n\nThis composition was pre-plastic, and ideal for everything from doorknobs to roller skate wheels to products such as buttons, cash register keys and jewelry. The composition of Hemacite was touted as susceptible to a high polish, impervious to heat, moisture, atmospheric changes, it acted very similarity to the material Pykrete. Hemacite was cheap to produce due to blood and sawdust both being cheap materials in order to make the product.\n\nAlthough Hemacite is a cheap material Bakelite was cheaper to produce, the popularity of plastics like Bakelite almost entirely replaced the production of Hemacite by the to mid to late-1900’s.\n"}
{"id": "2531815", "url": "https://en.wikipedia.org/wiki?curid=2531815", "title": "High Energy Liquid Laser Area Defense System", "text": "High Energy Liquid Laser Area Defense System\n\nThe High Energy Liquid Laser Area Defense System (HELLADS), is a Counter-RAM system under development that will use a powerful (150 kW) laser to shoot down rockets, missiles, artillery shells and mortars. The initial system will be demonstrated from a static ground-based installation, but in order to eventually be integrated on an aircraft, design requirements are maximum weight of 750 kg (1,650 lb) and maximum envelope of 2 cubic meters (70.6 feet).\n\nDevelopment is being funded by The Pentagon's Defense Advanced Research Projects Agency (DARPA).\n\nLiquid lasers that have large cooling systems can fire continuous beams, while solid state laser beams are more intense but generally must be fired in pulses to stop them from overheating. (As long as the heat transfer requirements are met solid state lasers can run continuously.) In the past, both types of lasers were very bulky because of their need for these huge cooling systems. The only aircraft in which they could fit were the size of jumbo jets.\n\nNeed for such a system was reinforced during the 2006 Lebanon War. Israel had participated in similar work in the past by funding the Mobile Tactical High Energy Laser (MTHEL). This system was tested on August 24, 2004, and was found to be effective at neutralizing mortar threats under an actual scenario. However, this test was administered with short, 20 km range missiles.\n\nFor the first few years of the program the Photonics Division of General Atomics was the prime contractor. The design combined the high energy density of a solid-state laser with the thermal management of a liquid laser. Dubbed the \"HEL weapon\", the initial prototype demonstrated firing a mild one kilowatt (kW) beam. Phase 3 of the program in 2007 demonstrated 15 kW power in a laboratory setting, and at the end of 2008 under the General Atomics bid, Lockheed Martin was selected as the weapon system integrator.\n\nIn September 2007, DARPA contracted Textron Systems to supply an alternate laser module using its proprietary \"ThinZag\" ceramic solid-state technology. Unlike the GA/Lockheed partnership, Textron will also perform the system integration function for their device. DARPA planned a \"shoot-off\" between the two contenders in 2009 to determine which would be funded to continue the program to further phases.\n\nThe more powerful version will produce a 150-kW beam capable of knocking down missiles with the weight and size requirements for fitting onto fighter aircraft or a Humvee. In mid 2008, \"Jane's International Defence Review\" quoted the US military that the program is on schedule to meet this ground test. Phase 4 of the program, involving outdoor testing of a weapon-power laser against tactical targets, was planned for 2010.\n\nA prototype was expected to be available by the end of 2012. DARPA planned to use the completed prototypes against targets at White Sands Missile Range in early 2013. This included ground testing against rockets, mortars, and surface-to-air missiles.\n\nDARPA planned for General Atomics to produce a second HELLADS system in January 2013 for use by the Office of Naval Research to test against targets \"relevant to surface ships.\" The first example is committed to Air Force use and cannot be made available for the Navy. Fabrication of the system was planned to be completed in 2012, with power, thermal management, beam control, and command-and-control subsystem integration planned through 2013. The system has a weight goal of per kW of energy. Both services plan for demonstrations in 2014.\n\nGeneral Atomics revealed in April 2015 that its Gen 3 High Energy Laser (HEL) completed beam quality and power measurements tests. The Gen 3 laser has a number of upgrades that provide improved beam quality, increased electrical to optical efficiency, and reduced size and weight; the assembly is small at only , and is powered by a compact Lithium-ion battery to demonstrate deployability on tactical platforms. Beam quality remained constant through the 30-second demonstration, proving that the beam quality of electrically-pumped lasers can be maintained above 50 kilowatts. General Atomics plans to have the laser module deployable on their Avenger unmanned aerial vehicle by 2018. Demonstrating sufficient laser power and beam quality ended the program's laboratory development phase and achieved acceptance for field trials. Ground-based field testing will assess its effects against rockets, mortars, vehicles, and surrogate surface-to-air missiles.\n\nHELLADS was to be tested during summer 2015 at White Sands. General Atomics has also proposed its Gen 3 HEL to the Navy after an ONR solicitation for a 150 kW laser weapon suitable for installation on \"Arleigh Burke\"-class destroyers, to be tested in 2018. The company has displayed the laser as a Tactical Laser Weapon Module which includes high-power-density lithium-ion batteries, liquid cooling, one or more laser unit cells, and optics to clean up and stabilize the beam before it enters the beam-director telescope; a unit cell produces a 75 kW beam, and modules can be combined to create beams of 150-300 kW in power with no beam combining like low-power fiber lasers. General Atomics also plans to offer the Gen 3 to the U.S. Army for their High Energy Laser Mobile Demonstrator (HEL-MD) when its power levels increase to 120 kW in the early 2020s.\n\n\n\n"}
{"id": "42470137", "url": "https://en.wikipedia.org/wiki?curid=42470137", "title": "High strain composite structure", "text": "High strain composite structure\n\nHigh Strain Composite Structures (HSC Structures) are a class of composite material structures designed to perform in a high deformation setting. High strain composite structures transition from one shape to another upon the application of external forces. A single HSC Structure component is designed to transition between at least two, but often more, dramatically different shapes. At least one of the shapes is designed to function as a structure which can support external loads.\n\nHigh strain composite structures usually consist of fiber-reinforced polymers (FRP), which are designed to undergo relatively high material strain levels under the course of normal operating conditions in comparison to most FRP structural applications. FRP materials are anisotropic and highly tailor-able which allows for unique effects upon deformation. As a result, many HSC Structures are configured to possess one or more stable states (shapes at which the structure will remain without external constraints) which are tuned for a particular application. HSC Structures with multiple stable states can also be classified as bi-stable structures.\n\nHSC Structures are most often used in applications where low weight structures are desired that can also be stowed in a small volume. Flexible composite structures are used within the aerospace industry for deployable mechanisms such antennas or solar arrays on spacecraft. Other applications focus on materials or structures in which multiple stable configurations are required.\n\nMetals commonly used in springs (e.g. high strength steel, aluminum and beryllium copper alloys) have been utilized in deformable aerospace structures for several decades with considerable success. They continue to be used in the majority of high strain deployable structure applications and excel where the greatest compaction ratios and electrical conductivity are required. But metals suffer from having high densities, high coefficients of thermal expansion, and lower strain capacities when compared to composite materials. In recent decades, the increasing need for high performance deployable structures, coupled with the emergence of a robust composite materials industry, has increased the demand and utility for High Strain Composites Structures. Today HSCs are used in a variety of niche aerospace applications, mostly in areas where extreme precision and low mass are required.\n\nIn early 2014 the American Institute of Aeronautics and Astronautics Spacecraft Structures Technical Committee recognized that the level of active research and development in High Strain Composites warranted an independent focus group to distinguish high strain composites as a technical area with uniquely identifiable challenges, technologies, mechanics, test methods, and applications. The High Strains Composite Technical Subcommittee was formed to provide a forum and framework to support HSC technical challenges and successes, and will promote continued advances in the field.\n\nThe use of high strain deployable structures dates back to the pioneering days of space exploration and has played a crucial role in enabling a robust spacefaring industry.\n\nMilestones in Space-Based Deformable Structures\n\nRigid Polymer\n\nRigidizable Polymer\n\nElastomeric Polymer\n\nCreep\n\nThin Shell Buckling\n\nSimulation Methods\n\nComposite material\n\nFiber-reinforced plastic\n\nBistability\n\nAmerican Institute of Aeronautics and Astronautics, Structures Technical Committee, High Strain Composite Structures Subcommittee\n"}
{"id": "1460907", "url": "https://en.wikipedia.org/wiki?curid=1460907", "title": "Hillerich &amp; Bradsby", "text": "Hillerich &amp; Bradsby\n\nHillerich & Bradsby Company (H&B) is a company located in Louisville, Kentucky that produces the famous Louisville Slugger baseball bat. The Louisville Slugger Museum & Factory in downtown Louisville features a retrospective of the product and its use throughout baseball history. H&B also makes baseball gloves, golf clubs, golf gloves and other equipment (under the \"PowerBilt\" brand).\n\nIn 2015, the company announced plans on March 23 to sell its Louisville Slugger division to sporting goods manufacturer Wilson Sporting Goods.\n\nJ. F. Hillerich opened his woodworking shop in Louisville in 1855. During the 1880s, Hillerich hired his seventeen-year-old son, John \"Bud\" Hillerich. Legend has it that Bud, who played baseball himself, slipped away from work one afternoon in to watch Louisville's major league team, the Louisville Eclipse. The team's star, Pete \"Louisville Slugger\" Browning, mired in a hitting slump, broke his bat.\n\nBud invited Browning to his father's shop to hand-craft him a new bat to his own specifications. Browning accepted the offer, and got three hits to immediately break out of his slump with his new bat the first day he used it. Browning told his teammates, which began a surge of professional ball players to the Hillerich woodworking shop.\n\nJ. F. Hillerich was uninterested in making bats; he saw the company future in stair railings, porch columns and swinging butter churns. In fact, for a brief time in the 1880s, he even turned away ball players. Bud, however, saw the potential in producing baseball bats, and the elder Hillerich eventually relented to his son.\n\nThe bats were sold under the name \"Falls City Slugger\" until Bud Hillerich took over his father's company in , and the name \"Louisville Slugger\" was registered with the U.S. Patent Office. In , Honus Wagner signed a deal with the company, becoming the first baseball player to officially endorse a bat.\n\nFrank Bradsby, a salesman, became a partner in , and the company's name changed to Hillerich and Bradsby. By , H&B was selling more bats than any other bat maker in the country, and legends like Ty Cobb, Babe Ruth and Lou Gehrig were all using them.\n\nDuring World War II, the company produced wooden rifle stocks and billy clubs for the U.S. Army. In 1954, the company purchased Larimer and Norton, Inc., a Pennsylvania lumber company to ensure a supply of hardwood for their products.\n\nIn 1976, the company moved across the Ohio River, to Jeffersonville, Indiana, to take advantage of the railroad line there. In the 1990s, the company returned south to Louisville.\n\nMost of Hillerich and Bradsby's wood bats are made from Northern white ash grown in proprietary forests on the New York–Pennsylvania border. Ash trees in the United States are now under attack from the emerald ash borer, an invasive insect species native to Asia and first detected in Michigan in 2002. While H&B's forests are not yet infested by the beetle, the area being destroyed is growing and moving closer to them. The company is making plans to utilize other woods in the event North America's ash forests are totally destroyed.\n\nThe P72 model bat was created in 1954 for professional baseball player Leslie Wayne Pinkham. It became one of baseball's most popular bats. Baseball Hall of Famers Cal Ripken Jr. and Robin Yount are among the players who used the P72 over the years. New York Yankee Derek Jeter used the P72 for every at bat in his 20 MLB seasons, with over 12,500 plate appearances. On September 25, 2014, in honor of Jeter's impending retirement, the P72 designation was retired, and the bat was renamed the DJ2 (Jeter wore #2). However descendants of Les Pinkam will still be allowed to get the bat with its P72 designation.\n\nIn addition to retiring the P72 model number, Louisville Slugger also promised to give the final 72 P72 bats produced to Jeter to raise funds for his Turn 2 Foundation.\n\nThere are three branches of Hillerich & Bradsby: Louisville Slugger, Bionic Gloves, and Powerbilt Golf. Bionic Gloves were designed by a leading orthopedic surgeon, and are used in many sports and recreational activities. Currently, there are 11 types of gloves: Golf, Gardening, Tennis, Fitness, Equestrian, Driving, Dress, Motorcycle, Racquetball, Baseball, and Drummer gloves. Powerbilt Golf produces many types of golf clubs. They recently introduced clubs that feature nitrogen charged technology.\n\nBesides its products, H&B attaches the Louisville Slugger name to other entities as a sponsor. The Cincinnati Reds' Triple A affiliate, the Louisville Bats, play at Louisville Slugger Field in downtown Louisville. The Louisville Slugger name is also attached to awards for top power-hitters at both the high school and college levels, and the Silver Slugger Award given annually to the Major League Baseball player with the best offensive output in each position. The Louisville Slugger Batting Champion award is given to the American Legion \"player with the highest batting average during national competition.\"\n\nCurrent players\nFormer MLB players\nWorks cited\n\n"}
{"id": "11084234", "url": "https://en.wikipedia.org/wiki?curid=11084234", "title": "Hurricane Alley", "text": "Hurricane Alley\n\nHurricane Alley is an area of warm water in the Atlantic Ocean stretching from the west coast of northern Africa to the east coast of Central America and Gulf Coast of the Southern United States. Many hurricanes form within this area. The sea surface temperature of the Atlantic in Hurricane Alley has grown slightly warmer over the past decades. A particularly warm summer in 2005 led climate scientists to begin studying whether this trend would lead to an increase in hurricane activity. See Effects of Climate Change below. \n\nHurricanes form over tropical waters in areas of high humidity, light winds, and warm sea surface temperatures. These areas which were described above are usually between the latitudes of 8° and 20° north. The perfect temperature for a hurricane is approximately 26 °C. This temperature has been set as a standard. If the water is colder the hurricane will most likely weaken, but if the waters are warmer rapid growth can occur.\n\nThe area between 10° and 20°N create the most hurricanes in a given season because of the warmer temperatures. Hurricanes do not form outside this range because the Coriolis effect is not strong enough to create the tight circulation needed and above this range the temperatures are too cool. The waters are only at the necessary temperatures from July until Mid-October. In the Atlantic this is the height of the season. \n\nSince hurricanes rely on sea surface temperature, sometimes an active season in the beginning becomes quiet later. This is because the hurricanes are so strong that they churn the waters and bring colder waters up from the deep. This creates an area of the sea the size of the hurricane, which has cooler waters, which can be 5-10 °C lower than before the hurricane. When a new hurricane moves over the cooler waters they have no fuel to continue to thrive, so they weaken or even die out.\n\nThe relationship of \"hurricane alley\" to climate change is unclear. Recent scientific evidence suggests that hurricane intensity may be increasing due to warmer tropical sea surface temperature, but the connection to Atlantic hurricane frequency is less conclusive. There is a debate as to whether climate change is causing more severe storms, but more research is needed into hurricane dynamics.\n\nAccording to an Azores High hypothesis of geographer Kam-biu Liu, an anti-phase pattern is expected to exist between the Gulf of Mexico coast and the North American Atlantic coast. During the quiescent periods (3000–1400 BC, and 1000 AD to present), a more northeasterly position of the Azores High would result in more hurricanes being steered towards the Atlantic coast. During the hyperactive period (1400 BC to 1000 AD), more hurricanes were steered towards the Gulf coast as the Azores High was shifted to a more southwesterly position near the Caribbean. Such a displacement of the Azores High is consistent with paleoclimatic evidence that shows an abrupt onset of a drier climate in Haiti around 3200 years BP, and a change towards more humid conditions in the Great Plains during the late-Holocene as more moisture was pumped up the Mississippi Valley through the Gulf coast. Preliminary data from the northern Atlantic coast seem to support the Azores High hypothesis. A 3,000-year proxy record from a coastal lake in Cape Cod suggests that hurricane activity has increased significantly during the past 500–1,000 years, just as the Gulf coast was amid a quiescent period of the last millennium.\n\n\n"}
{"id": "37396309", "url": "https://en.wikipedia.org/wiki?curid=37396309", "title": "Hyperconcentrated flow", "text": "Hyperconcentrated flow\n\nA hyperconcentrated flow is a two-phase flowing mixture of water and sediment in a channel which has properties intermediate between fluvial flow and debris flow. Large quantities of sand may be transported throughout the flow column, but the transport of suspended and bedload sediment along the channel depends on flow turbulence and high flow velocities, and coarser sediment remains as bedload. Hyperconcentrated flows do not show the characteristics of non-Newtonian flow typical of debris flows, e.g., levees, coarsening up or matrix supported deposits.\n\nHyperconcentrated flows may contain anywhere from 5–60 % sediment by volume. Higher concentrations tend to be characteristic of debris flows, less of normal fluvial flow.\n"}
{"id": "56011890", "url": "https://en.wikipedia.org/wiki?curid=56011890", "title": "Japan Electric Power Exchange", "text": "Japan Electric Power Exchange\n\nJapan Electric Power Exchange, Public Interest incorporated organisation (Japanese:一般社団法人日本卸電力取引所（にっぽんおろしでんりょくとりひきしょ)、abbr: JEPX）is an exchange that facilitates with spot and forward transactions of electrical power by power among power utilities in Japan. Formed as an intermediary corporation in November 2003, it was based on the recommendation of the same year by the Electric-power Industry Sub-committee to form a electric power wholesale market. In 2004 it invited members for its central committee and in 2005 April 1, commenced transacting in wholesale power transactions. Later on, in the November 2008, it also started a Green-power exchange. Pursuant to the change in Japan's corporate law for public interest organisations, it is now registered as a \"Public Interest Incorporated Organization\".\n\nIt is mandatory to be a registered member in order to be able to conduct transactions. As of October 2017 there are 127 registered companies.\n\n"}
{"id": "34546876", "url": "https://en.wikipedia.org/wiki?curid=34546876", "title": "KRAV (agriculture)", "text": "KRAV (agriculture)\n\nKRAV is a Swedish organization that develops and maintains regulations for ecological sustainable agriculture.\nKRAV is a member of International Federation of Organic Agriculture Movements.\n\n"}
{"id": "919232", "url": "https://en.wikipedia.org/wiki?curid=919232", "title": "Liaoning University of Petroleum and Chemical Technology", "text": "Liaoning University of Petroleum and Chemical Technology\n\nLiaoning University of Petroleum and Chemical Technology () is a university in Fushun, Liaoning, People's Republic of China under the provincial government.\n\nIt was founded in 1950 at Dalian as the first petroleum and chemical technology university of People's Republic of China. In 1953, it moved to Fushun.\n\nMore than 60,000 students graduated, most of whom have become managers and core technicians in Chinese petroleum and related companies. There are 13 academicians of Chinese Academy of Science and Chinese Academy of Engineering. Its engineering is relatively strong in China, especially in petroleum related areas.\n\n"}
{"id": "28151062", "url": "https://en.wikipedia.org/wiki?curid=28151062", "title": "Marin Municipal Water District", "text": "Marin Municipal Water District\n\nThe Marin Municipal Water District (or MMWD) is the government agency that provides drinking water to southern and central Marin County, California. Chartered in 1912, it became California's first municipal water district. It serves 195,000 people in a area that includes ten towns and cities.\n\nThe District's seven reservoirs in Marin County provide about 75% of the water it uses:\n\nAs of May 7, 2016, these 7 reservoirs stored 77,605 acre-feet, which is 97.54% of capacity. The average storage for this date is 70,864 acre-feet, or 89.06%.\n\nOver of land in District ownership are open to the public for recreational use from sunrise to sunset. There are of trails and unpaved roads available for hiking. The is one of the most popular trails. Many of the trails are also open for dog walking and horseback riding, while bicycling is only allowed on fire roads. Portions of the seven District reservoirs are open for hiking, biking, horseback riding, fishing, and picnicking. Camping, swimming, and boating are prohibited.\n\nIts administrative offices are located at 220 Nellen Avenue in Corte Madera: .\n\n"}
{"id": "22359454", "url": "https://en.wikipedia.org/wiki?curid=22359454", "title": "Marine steam engine", "text": "Marine steam engine\n\nA marine steam engine is a steam engine that is used to power a ship or boat. This article deals mainly with marine steam engines of the reciprocating type, which were in use from the inception of the steamboat in the early 19th century to their last years of large-scale manufacture during World War II. Reciprocating steam engines were progressively replaced in marine applications during the 20th century by steam turbines and marine diesel engines.\n\nThe first commercially successful steam engine was developed by Thomas Newcomen in 1712. The steam engine improvements brought forth by James Watt in the later half of the 18th century greatly improved steam engine efficiency and allowed more compact engine arrangements. Successful adaptation of the steam engine to marine applications in England would have to wait until almost a century later after Newcomen, when Scottish engineer William Symington built the world's \"first practical steamboat\", the \"Charlotte Dundas\", in 1802. In 1807, the American Robert Fulton built the world's first commercially successful steamboat, simply known as the \"North River Steamboat\", and powered by a Watt engine.\n\nFollowing Fulton's success, steamboat technology developed rapidly on both sides of the Atlantic. Steamboats initially had a short range and were not particularly seaworthy due to their weight, low power, and tendency to break down, but they were employed successfully along rivers and canals, and for short journeys along the coast. The first successful transatlantic crossing by a steamship occurred in 1819 when sailed from Savannah, Georgia to Liverpool, England. The first steamship to make regular transatlantic crossings was the sidewheel steamer in 1838.\n\nAs the 19th century progressed, marine steam engines and steamship technology developed alongside each other. Paddle propulsion gradually gave way to the screw propeller, and the introduction of iron and later steel hulls to replace the traditional wooden hull allowed ships to grow ever larger, necessitating steam power plants that were increasingly complex and powerful.\n\nA wide variety of reciprocating marine steam engines were developed over the course of the 19th century. The two main methods of classifying such engines are by \"connection mechanism\" and \"cylinder technology\".\n\nMost early marine engines had the same cylinder technology (simple expansion, see below) but a number of different methods of supplying power to the crankshaft (i.e. connection mechanism) were in use. Thus, early marine engines are classified mostly according to their connection mechanism. Some common connection mechanisms were side-lever, steeple, walking beam and direct-acting (see following sections).\n\nHowever, steam engines can also be classified according to cylinder technology (simple-expansion, compound, annular etc.). One can therefore find examples of engines classified under both methods. An engine can be a compound walking beam type, \"compound\" being the cylinder technology, and \"walking beam\" being the connection method. Over time, as most engines became direct-acting but cylinder technologies grew more complex, people began to classify engines solely according to cylinder technology.\n\nMore commonly encountered marine steam engine types are listed in the following sections. Note that not all these terms are exclusive to marine applications.\n\nThe side-lever engine was the first type of steam engine widely adopted for marine use in Europe. In the early years of steam navigation (from c1815), the side-lever was the most common type of marine engine for inland waterway and coastal service in Europe, and it remained for many years the preferred engine for oceangoing service on both sides of the Atlantic.\n\nThe side-lever was an adaptation of the earliest form of steam engine, the beam engine. The typical side-lever engine had a pair of heavy horizontal iron beams, known as side levers, that connected in the centre to the bottom of the engine with a pin. This connection allowed a limited arc for the levers to pivot in. These levers extended, on the cylinder side, to each side of the bottom of the vertical engine cylinder. A piston rod, connected vertically to the piston, extended out of the top of the cylinder. This rod attached to a horizontal crosshead, connected at each end to vertical rods (known as side-rods). These rods connected down to the levers on each side of the cylinder. This formed the connection of the levers to the piston on the cylinder side of the engine. The other side of the levers (the opposite end of the lever pivot to the cylinder) were connected to each other with a horizontal crosstail. This crosstail in turn connected to and operated a single connecting rod, which turned the crankshaft. The rotation of the crankshaft was driven by the levers—which, at the cylinder side, were driven by the piston's vertical oscillation.\n\nThe main disadvantage of the side-lever engine was that it was large and heavy. For inland waterway and coastal service, lighter and more efficient designs soon replaced it. It remained the dominant engine type for oceangoing service through much of the first half of the 19th century however, due to its relatively low centre of gravity, which gave ships more stability in heavy seas. It was also a common early engine type for warships, since its relatively low height made it less susceptible to battle damage. From the first Royal Navy steam vessel in 1820 until 1840, 70 steam vessels entered service, the majority with side-lever engines, using boilers set to 4psi maximum pressure. The low steam pressures dictated the large cylinder sizes for the side-lever engines, though the effective pressure on the piston was the difference between the boiler pressure and the vacuum in the condenser.\n\nThe side-lever engine was a paddlewheel engine and was not suitable for driving screw propellers. The last ship built for transatlantic service that had a side-lever engine was the Cunard Line's paddle steamer , considered an anachronism when it entered service in 1862.\nThe grasshopper or 'half-lever' engine was a variant of the side-lever engine. The grasshopper engine differs from the conventional side-lever in that the location of the lever pivot and connecting rod are more or less reversed, with the pivot located at one end of the lever instead of the centre, while the connecting rod is attached to the lever between the cylinder at one end and the pivot at the other.\n\nChief advantages of the grasshopper engine were cheapness of construction and robustness, with the type said to require less maintenance than any other type of marine steam engine. Another advantage is that the engine could be easily started from any crank position. Like the conventional side-lever engine however, grasshopper engines were disadvantaged by their weight and size. They were mainly used in small watercraft such as riverboats and tugs.\n\nThe crosshead engine, also known as a square, sawmill or A-frame engine, was a type of paddlewheel engine used in the United States. It was the most common type of engine in the early years of American steam navigation.\n\nThe crosshead engine is described as having a vertical cylinder above the crankshaft, with the piston rod secured to a horizontal crosshead, from each end of which, on opposite sides of the cylinder, extended a connecting rod that rotated its own separate crankshaft. The crosshead moved within vertical guides so that the assembly maintained the correct path as it moved. The engine's alternative name—\"A-frame\"—presumably derived from the shape of the frames that supported these guides. Some crosshead engines had more than one cylinder, in which case the piston rods were usually all connected to the same crosshead. An unusual feature of early examples of this type of engine was the installation of flywheels—geared to the crankshafts—which were thought necessary to ensure smooth operation. These gears were often noisy in operation.\n\nBecause the cylinder was above the crankshaft in this type of engine, it had a high center of gravity, and was therefore deemed unsuitable for oceangoing service. This largely confined it to vessels built for inland waterways. As marine engines grew steadily larger and heavier through the 19th century, the high center of gravity of square crosshead engines became increasingly impractical, and by the 1840s, ship builders abandoned them in favor of the walking beam engine.\n\nThe name of this engine can cause confusion, as \"crosshead\" is also an alternative name for the steeple engine (below). Many sources thus prefer to refer to it by its informal name of \"square\" engine to avoid confusion. Additionally, the marine crosshead or square engine described in this section should not be confused with the term \"square engine\" as applied to internal combustion engines, which in the latter case refers to an engine whose bore is equal to its stroke.\n\nThe walking beam, also known as a \"vertical beam\", \"overhead beam\", or simply \"beam\", was another early adaptation of the beam engine, but its use was confined almost entirely to the United States. After its introduction, the walking beam quickly became the most popular engine type in America for inland waterway and coastal service, and the type proved to have remarkable longevity, with walking beam engines still being occasionally manufactured as late as the 1940s. In marine applications, the beam itself was generally reinforced with iron struts that gave it a characteristic diamond shape, although the supports on which the beam rested were often built of wood. The adjective \"walking\" was applied because the beam, which rose high above the ship's deck, could be seen operating, and its rocking motion was (somewhat fancifully) likened to a walking motion.\n\nWalking beam engines were a type of paddlewheel engine and were rarely used for powering propellers. They were used primarily for ships and boats working in rivers, lakes and along the coastline, but were a less popular choice for seagoing vessels because the great height of the engine made the vessel less stable in heavy seas. They were also of limited use militarily, because the engine was exposed to enemy fire and could thus be easily disabled. Their popularity in the United States was due primarily to the fact that the walking beam engine was well suited for the shallow-draft boats that operated in America's shallow coastal and inland waterways.\n\nWalking beam engines remained popular with American shipping lines and excursion operations right into the early 20th century. Although the walking beam engine was technically obsolete in the later 19th century, it remained popular with excursion steamer passengers who expected to see the \"walking beam\" in motion. There were also technical reasons for retaining the walking beam engine in America, as it was easier to build, requiring less precision in its construction. Wood could be used for the main frame of the engine, at a much lower cost than typical practice of using iron castings for more modern engine designs. Fuel was also much cheaper in America than in Europe, so the lower efficiency of the walking beam engine was less of a consideration. The Philadelphia shipbuilder Charles H. Cramp blamed America's general lack of competitiveness with the British shipbuilding industry in the mid-to-late 19th century upon the conservatism of American domestic shipbuilders and shipping line owners, who doggedly clung to outdated technologies like the walking beam and its associated paddlewheel long after they had been abandoned in other parts of the world.\n\nThe steeple engine, sometimes referred to as a \"crosshead\" engine, was an early attempt to break away from the beam concept common to both the walking beam and side-lever types, and come up with a smaller, lighter, more efficient design. In a steeple engine, the vertical oscillation of the piston is not converted to a horizontal rocking motion as in a beam engine, but is instead used to move an assembly, composed of a crosshead and two rods, through a vertical guide at the top of the engine, which in turn rotates the crankshaft connecting rod below. In early examples of the type, the crosshead assembly was rectangular in shape, but over time it was refined into an elongated triangle. The triangular assembly above the engine cylinder gives the engine its characteristic \"steeple\" shape, hence the name.\n\nSteeple engines were tall like walking beam engines, but much narrower laterally, saving both space and weight. Because of their height and high centre of gravity, they were, like walking beams, considered less appropriate for oceangoing service, but they remained highly popular for several decades, especially in Europe, for inland waterway and coastal vessels.\n\nSteeple engines began to appear in steamships in the 1830s and the type was perfected in the early 1840s by the Scottish shipbuilder David Napier. The steeple engine was gradually superseded by the various types of direct-acting engine.\n\nThe Siamese engine, also referred to as the \"double cylinder\" or \"twin cylinder\" engine, was another early alternative to the beam or side-lever engine. This type of engine had two identical, vertical engine cylinders arranged side-by-side, whose piston rods were attached to a common, T-shaped crosshead. The vertical arm of the crosshead extended down between the two cylinders and was attached at the bottom to both the crankshaft connecting rod and to a guide block that slid between the vertical sides of the cylinders, enabling the assembly to maintain the correct path as it moved.\n\nThe Siamese engine was invented by British engineer Joseph Maudslay (son of Henry), but although he invented it after his oscillating engine (see below), it failed to achieve the same widespread acceptance, as it was only marginally smaller and lighter than the side-lever engines it was designed to replace. It was however used on a number of mid-century warships, including the first warship fitted with a screw propeller, .\n\nThere are two definitions of a direct-acting engine encountered in 19th-century literature. The earlier definition applies the term \"direct-acting\" to any type of engine other than a beam (i.e. walking beam, side-lever or grasshopper) engine. The later definition only uses the term for engines that apply power directly to the crankshaft via the piston rod and/or connecting rod. Unless otherwise noted, this article uses the later definition.\n\nUnlike the side-lever or beam engine, a direct-acting engine could be readily adapted to power either paddlewheels or a propeller. As well as offering a lower profile, direct-acting engines had the advantage of being smaller and weighing considerably less than beam or side-lever engines. The Royal Navy found that on average a direct-acting engine (early definition) weighed 40% less and required an engine room only two thirds the size of that for a side-lever of equivalent power. One disadvantage of such engines is that they were more prone to wear and tear and thus required more maintenance.\n\nAn oscillating engine was a type of direct-acting engine that was designed to achieve further reductions in engine size and weight. Oscillating engines had the piston rods connected directly to the crankshaft, dispensing with the need for connecting rods. To achieve this, the engine cylinders were not immobile as in most engines, but secured in the middle by trunnions that let the cylinders themselves pivot back and forth as the crankshaft rotated—hence the term, \"oscillating\". Steam was supplied and exhausted through the trunnions. The oscillating motion of the cylinder was usually used to line up ports in the trunnions to direct the steam feed and exhaust to the cylinder at the correct times. However, separate valves were often provided, controlled by the oscillating motion. This let the timing be varied to enable expansive working (as in the engine in the paddle ship PD \"Krippen\"). This provides simplicity but still retains the advantages of compactness.\n\nThe first patented oscillating engine was built by Joseph Maudslay in 1827, but the type is considered to have been perfected by John Penn. Oscillating engines remained a popular type of marine engine for much of the 19th century.\n\nThe trunk engine, another type of direct-acting engine, was originally developed as a means of reducing an engine's height while retaining a long stroke. (A long stroke was considered important at this time because it reduced the strain on components.)\n\nA trunk engine locates the connecting rod \"within\" a large-diameter hollow piston rod. This \"trunk\" carries almost no load. The interior of the trunk is open to outside air, and is wide enough to accommodate the side-to-side motion of the connecting rod, which links a gudgeon pin at the piston head to an outside crankshaft.\n\nThe walls of the trunk were either bolted to the piston or cast as one piece with it, and moved back and forth with it. The working portion of the cylinder is annular or ring-shaped, with the trunk passing through the centre of the cylinder itself.\n\nEarly examples of trunk engines had vertical cylinders. However, ship builders quickly realized that the type was compact enough to lay horizontally across the keel. In this configuration, it was very useful to navies, as it had a profile low enough to fit entirely below a ship's waterline, as safe as possible from enemy fire. The type was generally produced for military service by John Penn.\n\nTrunk engines were common on mid-19th century warships. They also powered commercial vessels, where—though valued for their compact size and low centre of gravity—they were expensive to operate. Trunk engines, however, did not work well with the higher boiler pressures that became prevalent in the latter half of the 19th century, and builders abandoned them for other solutions.\n\nTrunk engines were normally large, but a small, mass-produced, high-revolution, high-pressure version was produced for the Crimean War. In being quite effective, the type persisted in later gunboats. An original trunk engine of the gunboat type exists in the Western Australian Museum in Fremantle. After sinking in 1872, it was raised in 1985 from the and can now be turned over by hand. The engine's mode of operation, illustrating its compact nature, could be viewed on the \"Xantho\" project's website.\n\nThe vibrating lever, or half-trunk engine, was a development of the conventional trunk engine conceived by Swedish-American engineer John Ericsson. Ericsson needed a small, low-profile engine like the trunk engine to power the U.S. Federal government's monitors, a type of warship developed during the American Civil War that had very little space for a conventional powerplant. The trunk engine itself was, however, unsuitable for this purpose, because the preponderance of weight was on the side of the engine that contaied the cylinder and trunk—a problem that designers could compensate for on the small monitor warships.\n\nEricsson resolved this problem by placing two horizontal cylinders back-to-back in the middle of the engine, working two \"vibrating levers\", one on each side, which by means of shafts and additional levers rotated a centrally located crankshaft. Vibrating lever engines were later used in some other warships and merchant vessels, but their use was confined to ships built in the United States and in Ericsson's native country of Sweden, and as they had few advantages over more conventional engines, were soon supplanted by other types.\n\nThe back-acting engine, also known as the return connecting rod engine, was another engine designed to have a very low profile. The back-acting engine was in effect a modified steeple engine, laid horizontally across the keel of a ship rather than standing vertically above it. Instead of the triangular crosshead assembly found in a typical steeple engine however, the back-acting engine generally used a set of two or more elongated, parallel piston rods terminating in a crosshead to perform the same function. The term \"back-acting\" or \"return connecting rod\" derives from the fact that the connecting rod \"returns\" or comes back from the side of the engine opposite the engine cylinder to rotate a centrally located crankshaft.\n\nBack-acting engines were another type of engine popular in both warships and commercial vessels in the mid-19th century, but like many other engine types in this era of rapidly changing technology, they were eventually abandoned for other solutions. There is only one known surviving back-acting engine—that of the TV \"Emery Rice\" (formerly ), now the centerpiece of a display at the American Merchant Marine Museum.\n\nAs steamships grew steadily in size and tonnage through the course of the 19th century, the need for low profile, low centre-of-gravity engines correspondingly declined. Freed increasingly from these design constraints, engineers were able to revert to simpler, more efficient and more easily maintained designs. The result was the growing dominance of the so-called \"vertical\" engine (more correctly known as the vertical inverted direct acting engine).\n\nIn this type of engine, the cylinders are located directly above the crankshaft, with the piston rod/connecting rod assemblies forming a more or less straight line between the two. The configuration is similar to that of a modern internal combustion engine (one notable difference being that the steam engine is double acting, see below, whereas almost all internal combustion engines generate power only in the downward stroke). Vertical engines are sometimes referred to as \"hammer\", \"forge hammer\" or \"steam hammer\" engines, due to their roughly similar appearance to another common 19th-century steam technology, the steam hammer.\n\nVertical engines came to supersede almost every other type of marine steam engine toward the close of the 19th century. Because they became so common, vertical engines are not usually referred to as such, but are instead referred to based upon their cylinder technology, i.e. as compound, triple-expansion, quadruple-expansion etc. It should be noted that the term \"vertical\" for this type of engine is imprecise, since technically any type of steam engine is \"vertical\" if the cylinder is vertically oriented. An engine someone describes as \"vertical\" might not be of the vertical inverted direct-acting type, unless they use the term \"vertical\" without qualification.\n\nA simple-expansion engine is a steam engine that expands the steam through only one stage, which is to say, all its cylinders are operated at the same pressure. Since this was by far the most common type of engine in the early period of marine engine development, the term \"simple expansion\" is rarely encountered. An engine is assumed to be simple-expansion unless otherwise stated.\n\nA compound engine is a steam engine that operates cylinders through more than one stage, at different pressure levels. Compound engines were a method of improving efficiency. Until the development of compound engines, steam engines used the steam only once before they recycled it back to the boiler. A compound engine recycles the steam into one or more larger, lower-pressure second cylinders first, to use more of its heat energy. Compound engines could be configured to either increase a ship's economy or its speed. Broadly speaking, a compound engine can refer to a steam engine with any number of different-pressure cylinders—however, the term usually refers to engines that expand steam through only two stages, i.e., those that operate cylinders at only two different pressures (or \"double-expansion\" engines).\n\nNote that a compound engine (including multiple-expansion engines, see below) can have more than one \"set\" of variable-pressure cylinders. For example, an engine might have two cylinders operating at pressure x and two operating at pressure y, or one cylinder operating at pressure x and three operating at pressure y. What makes it compound (or double-expansion) as opposed to multiple-expansion is that there are only two \"pressures\", x and y.\n\nThe first compound engine believed to have been installed in a ship was that fitted to \"Henry Eckford\" by the American engineer James P. Allaire in 1824. However, many sources attribute the \"invention\" of the marine compound engine to Glasgow's John Elder in the 1850s. Elder made improvements to the compound engine that made it safe and economical for ocean-crossing voyages for the first time.\n\nA triple-expansion engine is a compound engine that expands the steam in three stages—i.e. an engine with three cylinders at three different pressures. A quadruple-expansion engine expands the steam in four stages, and so on. The first successful commercial use was an engine built at Govan in Scotland by Alexander C. Kirk for the SS \"Aberdeen\" in 1881.\n\nMultiple-expansion engine manufacture continued well into the 20th century. All 2,700 Liberty ships built by the United States during World War II were powered by triple-expansion engines, because the capacity of the US to manufacture marine steam turbines was entirely directed to the building of warships. The biggest manufacturer of triple-expansion engines during the war was the Joshua Hendy Iron Works. Toward the end of the war, turbine-powered Victory ships were manufactured in increasing numbers.\n\nAn annular engine is an unusual type of engine that has an annular (ring-shaped) cylinder. Some of American pioneering engineer James P. Allaire's early compound engines were of the annular type, with a smaller, high-pressure cylinder placed in the centre of a larger, ring-shaped low-pressure cylinder. Trunk engines were another type of annular engine. A third type of annular marine engine used the Siamese engine connecting mechanism—but instead of two separate cylinders, it had a single, annular-shaped cylinder wrapped around the vertical arm of the crosshead (see diagram under \"Siamese\" above).\n\nSome other terms are encountered in marine engine literature of the period. These terms, listed below, are usually used in conjunction with one or more of the basic engine classification terms listed above.\n\nA simple engine is an engine that operates with single expansion of steam, regardless of the number of cylinders fitted to the engine. Up until about the mid-19th century, most ships had engines with only one cylinder, although some vessels had multiple cylinder simple engines, and/or more than one engine.\n\nA double acting engine is an engine where steam is applied to both sides of the piston. Earlier steam engines applied steam in only one direction, allowing momentum or gravity to return the piston to its starting place, but a double acting engine uses steam to force the piston in both directions, thus increasing rotational speed and power. Like the term \"simple engine\", the term \"double acting\" is less frequently encountered in the literature since almost all marine engines were of the double acting type.\n\nThese terms refer to the orientation of the engine cylinder. A vertical cylinder stands vertically with its piston rod operating above (or below) it. A vertical inverted engine is defined as a vertical cylinder arrangement, with the crankshaft mounted directly below the cylinder(s). With an inclined or horizontal type, the cylinder and piston are positioned at an incline or horizontally. An inclined inverted cylinder is an inverted cylinder operating at an incline. These terms are all generally used in conjunction with the engine types above. Thus, one may have a horizontal direct-acting engine, or an inclined compound double acting engine, etc.\n\nInclined and horizontal cylinders could be very useful in naval vessels as their orientation kept the engine profile as low as possible and thus less susceptible to damage. They could also be used in a low profile ship or to keep a ship's centre of gravity lower. In addition, inclined or horizontal cylinders had the advantage of reducing the amount of vibration by comparison with a vertical cylinder.\n\nA geared engine or \"geared screw\" turns the propeller at a different rate to that of the engine. Early marine propeller engines were geared upward, which is to say the propeller was geared to run at a higher rotational speed than the engine itself ran at. As engines became faster and more powerful through the latter part of the 19th century, gearing was almost universally dispensed with, and the propeller ran at the same rotational speed as the engine. This direct drive arrangement is mechanically most efficient, and reciprocating steam engines are well suited to the rotational speed most efficient for screw propellers.\n\n\n\n"}
{"id": "1705830", "url": "https://en.wikipedia.org/wiki?curid=1705830", "title": "Minami-Fukumitsu", "text": "Minami-Fukumitsu\n\nMinami-Fukumitsu is the name given to an HVDC back-to-back station for the interconnection of the power grids of West and Eastern Japan. This facility went in service in March 1999. It operates with a voltage of 125 kV and can transfer a power up to 300 megawatts. The station is located in Nanto, Toyama Prefecture\n"}
{"id": "37210492", "url": "https://en.wikipedia.org/wiki?curid=37210492", "title": "National Solar Thermal Test Facility", "text": "National Solar Thermal Test Facility\n\nOperated by Sandia National Laboratories for the U.S. Department of Energy (DOE), the National Solar Thermal Test Facility (NSTTF) is the only test facility of this type in the United States. The NSTTF’s primary goal is to provide experimental engineering data for the design, construction, and operation of unique components and systems in proposed solar thermal electrical plants planned for large-scale power generation.\n\nThe site was built and instrumented to provide test facilities for a variety of solar and non-solar applications. The facility can provide\n\n\nNational Solar Thermal Test Facility\nDuring the late 1970s, rising fuel costs and the demand for a cleaner environment gave impetus to advance technology which used solar energy to create electricity. Studies identified the central receiver concept as having high potential to generate electricity on a large scale. The United States government initiated support through the National Science Foundation Research Applied to National Needs (RANN) program in 1972 which was eventually funded by the Energy Research and Development Administration and the Department of Energy. Six central receiver pilot plants were constructed including the 5 MW (thermal) test facility at Sandia National Laboratories.\n\nThe heliostat field has 218 individual heliostats. This capability directly supports the SunShot goals by providing flux levels of greater than 250 W/cm and total power in excess of 6 MWt. Each heliostat has two motors and two drives (one azimuth and one elevation), one 480 V power box, one electronics box, and one control box and associated cabling. The total reflective area on each heliostat is 37 m. The reflectivity on the recently replaced facets is 96%.\n\nThe tower is a 61 m (200 ft) high concrete structure with three test locations on the north side and the top of the tower. The tower can support testing for CSP experiments and large-scale, high-flux materials samples. The equipment in the tower includes a 100-ton capacity elevating module for lifting experiments to the top of the tower, internal cranes for receiver fabrication, water glycol cooling systems and air coolers to provide heat removal from experiments, air compressors, control valves, generators, uninterruptible power supplies, piping systems, and pressure-relief valves.\n\nThe molten salt test loop directly supports the SunShot goals by providing development for thermal energy storage costs ≤$15/kWhth and by allowing greater collection efficiencies and higher-temperature operation for linear Fresnel and trough systems through utilization of molten salt HTF. The facility also provides a means of performing accelerated lifetime testing on components, thus reducing the risk of the technology. Though operating below 600 °C, many of the lessons learned at this facility will be directly applicable to molten salt systems operating in the SunShot temperature range ≥ 650 °C.\n\nThis area of the site allows industry partners to install full-scale solar dishes for long-term reliability testing and evaluation. There are currently ten dishes from Stirling Energy Systems and six Infinia dishes at this location. The site also includes two SNL-developed solar dishes that are available for research. Solar dishes can be used for the high-temperature portions of the SunShot goals.\n"}
{"id": "1341098", "url": "https://en.wikipedia.org/wiki?curid=1341098", "title": "Natural Energy Laboratory of Hawaii Authority", "text": "Natural Energy Laboratory of Hawaii Authority\n\nThe Natural Energy Laboratory of Hawaii Authority (NELHA) administers the Hawaii Ocean Science and Technology Park (HOST Park). NELHA was founded in 1974. At , HOST Park is perhaps the largest single green economic development project in the world solely devoted to growing a green economy. NELHA also administers a small site, , in Puna on the eastern side of the Island of Hawaii for geothermal research. The original mission was for research into the uses of Deep Ocean Water in Ocean thermal energy conversion (OTEC) renewable energy production and in aquaculture. It later added research into sustainable uses of natural energy sources such as solar energy.\nIts administration offices are located in the HOST Park Keahole Point in the North Kona District.\nThe entrance is on the Hawaii Belt Road at coordinates , just south of the Kona International Airport. The main administration office is in the 4 acre research campus at the end of the road along the coastline on Keahole Point.\n\nThe laboratory was founded in 1974 with , associated with the University of Hawaii. Large pipelines pump cold sea water from a depth of . For three months in 1979, a small amount of electricity was generated. A larger plant was constructed in the Almost $250M was spent on Ocean thermal energy conversion, but by 1991, the research shifted to other areas. The adjacent Science and Technology Park was merged into the facility, expanding it to . A neutrino detector was partially constructed in the 1990s called Project DUMAND.\n\nIn 2002, were leased to a commercial company which filters and bottles the water for sale in Japan.\nThe pipelines were broken in the October 15, 2006 earthquake, but one of the three has been repaired over two years later.\nMakai Ocean Engineering, working with Lockheed Martin, restarted OTEC research. Aquaculture, biofuel from algae, solar thermal energy, solar concentrating and wind power are some of the 40 tenants.\n\n\n"}
{"id": "779929", "url": "https://en.wikipedia.org/wiki?curid=779929", "title": "Neutron generator", "text": "Neutron generator\n\nNeutron generators are neutron source devices which contain compact linear accelerators and that produce neutrons by fusing isotopes of hydrogen together. The fusion reactions take place in these devices by accelerating either deuterium, tritium, or a mixture of these two isotopes into a metal hydride target which also contains deuterium, tritium or a mixture of these isotopes. Fusion of deuterium atoms (D + D) results in the formation of a He-3 ion and a neutron with a kinetic energy of approximately 2.5 MeV. Fusion of a deuterium and a tritium atom (D + T) results in the formation of a He-4 ion and a neutron with a kinetic energy of approximately 14.1 MeV. Neutron generators have applications in medicine, security, and materials analysis.\n\nThousands of such small, relatively inexpensive systems have been built over the past five decades.\nSmall neutron generators using the deuterium (D, hydrogen-2, H) tritium (T, hydrogen-3, H) fusion reactions are the most common accelerator based (as opposed to radioactive isotopes) neutron sources. In these systems, neutrons are produced by creating ions of deuterium, tritium, or deuterium and tritium and accelerating these into a hydride target loaded with deuterium, tritium, or deuterium and tritium. The DT reaction is used more than the DD reaction because the yield of the DT reaction is 50–100 times higher than that of the DD reaction.\n\nD + T → n + He E = 14.1 MeV\n\nD + D → n + He E = 2.5 MeV\n\nNeutrons produced by DD and DT reactions are emitted somewhat anisotropically from the target, slightly biased in the forward (in the axis of the ion beam) direction. The anisotropy of the neutron emission from DD and DT reactions arises from the fact the reactions are isotropic in the center of momentum coordinate system (COM) but this isotropy is lost in the transformation from the COM coordinate system to the laboratory frame of reference. In both frames of reference, the He nuclei recoil in the opposite direction to the emitted neutron consistent with the law of conservation of momentum.\n\nThe gas pressure in the ion source region of the neutron tubes generally ranges between 0.1–0.01 mm Hg. The mean free path of electrons must be shorter than the discharge space to achieve ionization (lower limit for pressure) while the pressure must be kept low enough to avoid formation of discharges at the high extraction voltages applied between the electrodes. The pressure in the accelerating region, however, has to be much lower, as the mean free path of electrons must be longer to prevent formation of a discharge between the high voltage electrodes.\n\nThe ion accelerator usually consists of several electrodes with cylindrical symmetry, acting as an einzel lens. The ion beam can thus be focused to a small point at the target. The accelerators typically require power supplies of 100 - 500 kV. They usually have several stages, with voltage between the stages not exceeding 200 kV to prevent field emission.\n\nIn comparison with radionuclide neutron sources, neutron tubes can produce much higher neutron fluxes and consistent (monochromatic) neutron energy spectrums can be obtained. The neutron production rate can also be controlled.\n\nThe central part of a neutron generator is the particle accelerator itself, sometimes called a neutron tube.\nNeutron tubes have several components including an ion source, ion optic elements, and a beam target; all of these are enclosed within a vacuum-tight enclosure. High voltage insulation between the ion optical elements of the tube is provided by glass and/or ceramic insulators. The neutron tube is, in turn, enclosed in a metal housing, the accelerator head, which is filled with a dielectric medium to insulate the high voltage elements of the tube from the operating area. The accelerator and ion source high voltages are provided by external power supplies. The control console allows the operator to adjust the operating parameters of the neutron tube. The power supplies and control equipment are normally located within 10–30 feet of the accelerator head in laboratory instruments, but may be several kilometers away in well logging instruments.\n\nIn comparison with their predecessors, sealed neutron tubes do not require vacuum pumps and gas sources for operation. They are therefore more mobile and compact, while also durable and reliable. For example, sealed neutron tubes have replaced radioactive neutron initiators, in supplying a pulse of neutrons to the imploding core of modern nuclear weapons.\n\nExamples of neutron tube ideas date as far back as the 1930s, pre-nuclear weapons era, by German scientists filing a 1938 German patent (March 1938, patent # 261,156) and obtaining a United States Patent (July 1941, USP#2,251,190); examples of present state of the art are given by developments such as the Neutristor, a mostly solid state device, resembling a computer chip, invented at Sandia National Laboratories in Albuquerque NM. Typical sealed designs are used in a pulsed mode and can be operated at different output levels, depending on the life from the ion source and loaded targets.\n\nA good ion source should provide a strong ion beam without consuming much of the gas. For hydrogen isotopes, production of atomic ions is favored over molecular ions, as atomic ions have higher neutron yield on collision. The ions generated in the ion source are then extracted by an electric field into the accelerator region, and accelerated towards the target. The gas consumption is chiefly caused by the pressure difference between the ion generating and ion accelerating spaces that has to be maintained. Ion currents of 10 mA at gas consumptions of 40 cm/hour are achievable.\n\nFor a sealed neutron tube, the ideal ion source should use low gas pressure, give high ion current with large proportion of atomic ions, have low gas clean-up, use low power, have high reliability and high lifetime, its construction has to be simple and robust and its maintenance requirements have to be low.\n\nGas can be efficiently stored in a replenisher, an electrically heated coil of zirconium wire. Its temperature determines the rate of absorption/desorption of hydrogen by the metal, which regulates the pressure in the enclosure.\n\nThe Penning source is a low gas pressure, cold cathode ion source which utilizes crossed electric and magnetic fields. The ion source anode is at a positive potential, either dc or pulsed, with respect to the source cathode. The ion source voltage is normally between 2 and 7 kilovolts. A magnetic field, oriented parallel to the source axis, is produced by a permanent magnet. A plasma is formed along the axis of the anode which traps electrons which, in turn, ionize gas in the source. The ions are extracted through the exit cathode. Under normal operation, the ion species produced by the Penning source are over 90% molecular ions. This disadvantage is however compensated for by the other advantages of the system.\n\nOne of the cathodes is a cup made of soft iron, enclosing most of the discharge space. The bottom of the cup has a hole through which most of the generated ions are ejected by the magnetic field into the acceleration space. The soft iron shields the acceleration space from the magnetic field, to prevent a breakdown.\n\nIons emerging from the exit cathode are accelerated through the potential difference between the exit cathode and the accelerator electrode. The schematic indicates that the exit cathode is at ground potential and the target is at high (negative) potential. This is the case in many sealed tube neutron generators. However, in cases when it is desired to deliver the maximum flux to a sample, it is desirable to operate the neutron tube with the target grounded and the source floating at high (positive) potential. The accelerator voltage is normally between 80 and 180 kilovolts.\n\nThe accelerating electrode has the shape of a long hollow cylinder. The ion beam has a slightly diverging angle (about 0.1 radian). The electrode shape and distance from target can be chosen so the entire target surface is bombarded with ions. Acceleration voltages of up to 200 kV are achievable.\n\nThe ions pass through the accelerating electrode and strike the target. When ions strike the target, 2–3 electrons per ion are produced by secondary emission. In order to prevent these secondary electrons from being accelerated back into the ion source, the accelerator electrode is biased negative with respect to the target. This voltage, called the suppressor voltage, must be at least 500 volts and may be as high as a few kilovolts. Loss of suppressor voltage will result in damage, possibly catastrophic, to the neutron tube.\n\nSome neutron tubes incorporate an intermediate electrode, called the focus or extractor electrode, to control the size of the beam spot on the target. The gas pressure in the source is regulated by heating or cooling the gas reservoir element.\n\nIons can be created by electrons formed in high-frequency electromagnetic field. The discharge is formed in a tube located between electrodes, or inside a coil. Over 90% proportion of atomic ions is achievable.\n\nThe targets used in neutron generators are thin films of metal such as titanium, scandium, or zirconium which are deposited onto a silver, copper or molybdenum substrate. Titanium, scandium, and zirconium form stable chemical compounds called metal hydrides when combined with hydrogen or its isotopes. These metal hydrides are made up of two hydrogen (deuterium or tritium) atoms per metal atom and allow the target to have extremely high densities of hydrogen. This is important to maximize the neutron yield of the neutron tube. The gas reservoir element also uses metal hydrides, e.g. uranium hydride, as the active material.\n\nTitanium is preferred to zirconium as it can withstand higher temperatures (200 °C), and gives higher neutron yield as it captures deuterons better than zirconium. The maximum temperature allowed for the target, above which hydrogen isotopes undergo desorption and escape the material, limits the ion current per surface unit of the target; slightly divergent beams are therefore used. A 1 microampere ion beam accelerated at 200 kV to a titanium-tritium target can generate up to 10 neutrons per second. The neutron yield is mostly determined by the accelerating voltage and the ion current level.\n\nAn example of a tritium target in use is a 0.2 mm thick silver disc with a 1 micrometer layer of titanium deposited on its surface; the titanium is then saturated with tritium.\n\nMetals with sufficiently low hydrogen diffusion can be turned into deuterium targets by bombardment of deuterons until the metal is saturated. Gold targets under such condition show four times higher efficiency than titanium. Even better results can be achieved with targets made of a thin film of a high-absorption high-diffusivity metal (e.g. titanium) on a substrate with low hydrogen diffusivity (e.g. silver), as the hydrogen is then concentrated on the top layer and can not diffuse away into the bulk of the material. Using a deuterium-tritium gas mixture, self-replenishing D-T targets can be made. The neutron yield of such targets is lower than of tritium-saturated targets in deuteron beams, but their advantage is much longer lifetime and constant level of neutron production. Self-replenishing targets are also tolerant to high-temperature bake-out of the tubes, as their saturation with hydrogen isotopes is performed after the bakeout and tube sealing.\n\nOne particularly interesting approach for generating the high voltage fields needed to accelerate ions in a neutron tube is to use a pyroelectric crystal. In April 2005 researchers at UCLA demonstrated the use of a thermally cycled pyroelectric crystal to generate high electric fields in a neutron generator application. In February 2006 researchers at Rensselaer Polytechnic Institute demonstrated the use of two oppositely poled crystals for this application. Using these low-tech power supplies it is possible to generate a sufficiently high electric field gradient across an accelerating gap to accelerate deuterium ions into a deuterated target to produce the D + D fusion reaction. These devices are similar in their operating principle to conventional sealed-tube neutron generators which typically use Cockcroft–Walton type high voltage power supplies. The novelty of this approach is in the simplicity of the high voltage source. Unfortunately, the relatively low accelerating current that pyroelectric crystals can generate, together with the modest pulsing frequencies that can be achieved (a few cycles per minute) limits their near-term application in comparison with today's commercial products (see below). Also see pyroelectric fusion.\n\nIn addition to the conventional neutron generator design described above several other approaches exist to use electrical systems for producing neutrons.\n\nAnother type of innovative neutron generator is the inertial electrostatic confinement fusion device. This neutron generator avoids using a solid target which will be sputter eroded causing metalization of insulating surfaces. Depletion of the reactant gas within the solid target is also avoided. Far greater operational lifetime is achieved. Originally called a fusor, it was invented by Philo Farnsworth, the inventor of electronic television.\n\n\n"}
{"id": "22808689", "url": "https://en.wikipedia.org/wiki?curid=22808689", "title": "Ornithological Council", "text": "Ornithological Council\n\nThe Ornithological Council is an association of ornithological organisations based in the Americas involved in bird study and conservation. It was established by Richard C. Banks and incorporated in Washington, D.C. in 1992 as a nonprofit organization. Its original members comprised the American Ornithologists' Union, Association of Field Ornithologists, Colonial Waterbirds Society (now the Waterbird Society), Cooper Ornithological Society, Pacific Seabird Group, Raptor Research Foundation and Wilson Ornithological Society. Since then they have been joined by CIPAMEX, the Neotropical Ornithological Society, the Society of Canadian Ornithologists and the Society for the Conservation and Study of Caribbean Birds.\n\nThe Council focuses not only on issues affect birds and their survival, but also the needs of ornithologists. It tries to resolve conflicts and promotes sound management and the sustainable use of natural resources. Issues include funding for research and educational programs, government regulations and permits for research activities, decision-making in habitat and biodiversity management, the impacts of birds on fisheries, in agriculture and in urban areas, threatened species, the role of birds in ecosystems, and standards concerning the use and maintenance of live birds in research.\n\n"}
{"id": "950191", "url": "https://en.wikipedia.org/wiki?curid=950191", "title": "Petroleum politics", "text": "Petroleum politics\n\nPetroleum politics have been an increasingly important aspect of diplomacy since the rise of the petroleum industry in the Middle East in the early 20th century. As competition continues for a vital resource, the strategic calculations of major and minor countries alike place prominent emphasis on the pumping, refining, transport, sale and use of petroleum products. However, international climate policy and unconventional oil and gas developments may change the balance of power between petroleum exporting and importing countries with major negative implications expected for the exporting states.\n\nThe Achnacarry Agreement or \"As-Is Agreement\" was an early attempt to restrict petroleum production, signed in Scotland on 17 September 1928. The discovery of the East Texas Oil Field in the 1930s led to a boom in production that caused prices to fall, leading the Railroad Commission of Texas to control production. The Commission retained \"de facto\" control of the market until the rise of OPEC in the 1970s.\n\nThe Anglo-American Petroleum Agreement of 1944 tried to extend these restrictions internationally but was opposed by the industry in the United States and so Franklin Roosevelt withdrew from the deal.\n\nVenezuela was the first country to move towards the establishment of OPEC by approaching Iran, Gabon, Libya, Kuwait and Saudi Arabia in 1949, but OPEC was not set up until 1960, when the United States forced import quotas on Venezuelan and Persian Gulf oil in order to support the Canadian and Mexican oil industries. OPEC first wielded its power with the 1973 oil embargo against the United States and Western Europe.\n\nThe term \"petro-aggression\" has been used to describe the tendency of oil-rich states to instigate international conflicts. There are many examples including: Iraq’s invasion of Iran and Kuwait; Libya’s repeated incursions into Chad in the 1970s and 1980s; Iran’s long-standing suspicion of Western powers. Some scholars have also suggested that oil-rich states are frequently the targets of \"resource wars.\"\n\nIn 1956, a Shell geophysicist named M. King Hubbert accurately predicted that U.S. oil production would peak in 1970.\n\nIn June 2006, former U.S. president Bill Clinton said in a speech,\n\n\"We may be at a point of peak oil production. You may see $100 a barrel oil in the next two or three years, but what still is driving this globalization is the idea that is you cannot possibly get rich, stay rich and get richer if you don’t release more greenhouse gases into the atmosphere. That was true in the industrial era; it is simply factually not true. What is true is that the old energy economy is well organized, financed and connected politically.\"\n\nIn a 1999 speech, Dick Cheney, the US Vice President and former CEO of Halliburton (one of the world's largest energy services corporations), said,\n\n\"By some estimates there will be an average of two per cent annual growth in global oil demand over the years ahead along with conservatively a three per cent natural decline in production from existing reserves. That means by 2010 we will need on the order of an additional fifty million barrels a day. So where is the oil going to come from?...While many regions of the world offer great oil opportunities, the Middle East with two thirds of the world's oil and the lowest cost, is still where the prize ultimately lies, even though companies are anxious for greater access there, progress continues to be slow.\"\n\nCheney went on to argue that the oil industry should become more active in politics:\n\n\"Oil is the only large industry whose leverage has not been all that effective in the political arena. Textiles, electronics, agriculture all seem often to be more influential. Our constituency is not only oilmen from Louisiana and Texas, but software writers in Massachusetts and specialty steel producers in Pennsylvania. I am struck that this industry is so strong technically and financially yet not as politically successful or influential as are often smaller industries. We need to earn credibility to have our views heard.\"\n\nThe Baku–Tbilisi–Ceyhan pipeline was built to transport crude oil and the Baku-Tbilisi-Erzurum pipeline was built to transport natural gas from the western side (Azerbaijani sector) of the Caspian Sea to the Mediterranean Sea bypassing Russian pipelines and thus Russian control. Following the construction of the pipelines, the United States and the European Union proposed extending them by means of the proposed Trans-Caspian Oil Pipeline and the Trans-Caspian Gas Pipeline under the Caspian Sea to oil and gas fields on the eastern side (Kazakhstan and Turkmenistan sectors) of the Caspian Sea. In 2007, Russia signed agreements with Turkmenistan and Kazakhstan to connect their oil and gas fields to the Russian pipeline system effectively killing the undersea route.\n\nChina has completed the Kazakhstan–China oil pipeline from the Kazakhstan oil fields to the Chinese Alashankou-Dushanzi Crude Oil Pipeline in China. China is also working on the Kazakhstan-China gas pipeline from the Kazakhstan gas fields to the Chinese West-East Gas Pipeline in China.\n\nSeveral countries have nationalised foreign-run oil businesses, often failing to compensate investors. Enrique Mosconi, the director of the Argentine state owned oil company \"Yacimientos Petrolíferos Fiscales\" (YPF, which was the first state owned oil company in the world, preceding the French \"Compagnie française des pétroles\" (CFP, French Company of Petroleums), created in 1924 by the conservative Raymond Poincaré), advocated oil nationalization in the late 1920s among Latin American countries. The latter was achieved in Mexico during Lázaro Cárdenas's rule, with the \"Expropiación petrolera\".\n\nSimilarly Venezuela nationalized its oil industry in 1976.\n\nVinod Khosla (a well known investor in IT firms and alternative energy) has argued that the political interests of environmental advocates, agricultural businesses, energy security advocates (such as ex-CIA director James Woolsey) and automakers, are all aligned for the increased production of ethanol. He pointed out that from 2003 to 2006, ethanol fuel in Brazil replaced 40% of its gasoline consumption while flex fuel vehicles went from 3% of car sales to 70%. Brazilian ethanol, which is produced using sugarcane, reduces greenhouse gases by 60-80% (20% for corn-produced ethanol). Khosla also said that ethanol was about 10% cheaper per given distance. There are currently ethanol subsidies in the United States but they are all blender's credits, meaning the oil refineries receive the subsidies rather than the farmers. There are indirect subsidies due to subsidising farmers to produce corn. Vinod says after one of his presentations in Davos, a senior Saudi oil official came up to him and threatened: \"If biofuels start to take off, we will drop the price of oil.\" Since then, Vinod has come up with a new recommendation that oil should be taxed if it drops below $40.00/barrel in order to counter price manipulation.\n\nEx-CIA director James Woolsey and U.S. Senator Richard Lugar are also vocal proponents of ethanol.\n\nIn 2005, Sweden announced plans to end its dependence on fossil fuels by the year 2020.\n\nMultibillion-dollar inflows and outflows of petroleum money have worldwide macroeconomic consequences, and major oil exporters can gain substantial influence from their petrodollar recycling activities.\n\nAs development in the Alberta oil sands, deep sea drilling in the North Atlantic and the prospects of arctic oil continue to grow Canada increasingly grows as a global oil exporter. There are currently three major pipelines under proposal that would ship oil to the pacific, atlantic and gulf ports. These projects have stirred internal controversy, receiving fierce opposition from First Nations groups and environmentalists.\n\nDiscovery of oil in 1908 at Masjed Soleiman in Iran initiated the quest for oil in the Middle East. The Anglo-Iranian Oil Company (AIOC) was founded in 1909.\nIn 1951, Iran nationalized its oil fields initiating the Abadan Crisis. The United States of America and Great Britain thus punished Iran by arranging coup against its democratically elected prime minister, Mosaddeq, and brought the former Shah's son, a dictator, to power. In 1953 the US and GB arranged the arrest of the Prime Minister Mosaddeq.\nIran exports oil to China and Russia. \"See also: Iranian Oil Subsidies\"\n\nIraq holds the world's second-largest proven oil reserves, with increasing exploration expected to enlarge them beyond of \"high-grade crude, extraordinarily cheap to produce.\" Organizations such as the Global Policy Forum (GPF) have asserted that Iraq's oil is \"the central feature of the political landscape\" there, and that as a result of the 2003 invasion,\"'friendly' companies expect to gain most of the lucrative oil deals that will be worth hundreds of billions of dollars in profits in the coming decades.\" According to GPF, U.S. influence over the 2005 Constitution of Iraq has made sure it \"contains language that guarantees a major role for foreign companies.\"\n\nMexico has a largely oil-based economy, being the seventh largest producer of petroleum. Though Mexico has gradually explored different types of electricity, oil is still crucial, recently generating 10% of revenue.\n\nBefore 1938, all petroleum companies in Mexico were foreign based, often from the United States or Europe. The petroleum industry was nationalized in the late 1930s to early 1940s by then-president Lázaro Cárdenas, creating PEMEX. Mexico's oil industry still remains heavily nationalized. Though oil production has fallen in recent years, Mexico still remains in seventh place.\n\nPetroleum in Nigeria was discovered in 1955 at Oloibiri in the Niger Delta.\n\nHigh oil prices were the driving force behind Nigeria’s economic growth in 2005. The country’s real gross domestic product (GDP) grew approximately 4.5 percent in 2005 and was expected to grow by 6.2 percent in 2006. The Nigerian economy is heavily dependent on the oil sector, which accounts for 95 percent of government revenues. Even with the substantial oil wealth, Nigeria ranks as one of the poorest countries in the world, with a $1,000 per capita income and more than 70 percent of the population living in poverty. In October 2005, the 15-member Paris Club announced that it would cancel 60 percent of the debt owed by Nigeria. However, Nigeria must still pay $12.4 billion in arrears amongst meeting other conditions. In March 2006, phase two of the Paris Club agreement will include an additional 34 percent debt cancellation, while Nigeria will be responsible for paying back any remaining eligible debts to the lending nations. The International Monetary Fund (IMF), which recently praised the Nigerian government for adopting tighter fiscal policies, will be allowed to monitor Nigeria without having to disburse loans to the country.\n\nHigh-priced oil allowed the USSR to subsidize the struggling economies of the Soviet bloc for a time, and the loss of petrodollar income during the 1980s oil glut contributed to the bloc's collapse in 1989.\n\nSaudi Arabia is an oil-based economy with strong government controls over major economic activities. It possesses both the world's largest known oil reserves, which are 25% of the world's proven reserves, and produces the largest amount of the world's oil. As of 2005, Ghawar field accounts for about half of Saudi Arabia's total oil production capacity.\n\nSaudi Arabia ranks as the largest exporter of petroleum, and plays a leading role in OPEC, its decisions to raise or cut production almost immediately impact world oil prices. It is perhaps the best example of a contemporary energy superpower, in terms of having power and influence on the global stage (due to its energy reserves and production of not just oil, but natural gas as well). Saudi Arabia is often referred to as the world's only \"oil superpower\".\n\nIn 1998, about 40% of the energy consumed by the United States came from oil. The United States is responsible for 25% of the world's oil consumption, while having only 3% of the world's proven oil reserves and less than 5% of the world's population. In January 1980, President Jimmy Carter explicitly declared: \"An attempt by any outside force to gain control of the Persian Gulf region will be regarded as an assault on the vital interests of the United States.\"\n\nAccording to the Oil and Gas Journal (OGJ), Venezuela has of proven conventional oil reserves, the largest of any country in the Western Hemisphere. In addition it has non-conventional oil deposits similar in size to Canada's - at approximately equal to the world's reserves of conventional oil. About of this may be producible at current prices using current technology. Venezuela's Orinoco tar sands are less viscous than Canada's Athabasca oil sands – meaning they can be produced by more conventional means, but are buried deeper – meaning they cannot be extracted by surface mining. In an attempt to have these \"extra heavy\" oil reserves recognized by the international community, Venezuela has moved to add them to its conventional reserves to give nearly of total oil reserves. This would give it the largest oil reserves in the world, even ahead of Saudi Arabia.\n\nVenezuela nationalized its oil industry in 1975–1976, creating Petróleos de Venezuela S.A. (PdVSA), the country's state-run oil and natural gas company. Along with being Venezuela's largest employer, PdVSA accounts for about one-third of the country’s GDP, 50 percent of the government’s revenue and 80 percent of Venezuela’s exports earnings. In recent years, under the influence of President Chavez, the Venezuelan government has reduced PdVSA’s previous autonomy and amended the rules regulating the country’s hydrocarbons sector.\n\nIn the 1990s, Venezuela opened its upstream oil sector to private investment. This collection of policies, called apertura, facilitated the creation of 32 operating service agreements (OSA) with 22 separate foreign oil companies, including international oil majors like Chevron, BP, Total, and Repsol-YPF. Hugo Chávez, the President of Venezuela sharply diverged from previous administrations' economic policies. PDVSA is now used as a cash-cow and as an employer-of-last-resort; foreign oil businesses were nationalised and the government refused to pay compensation.\n\nEstimates of Venezuelan oil production vary. Venezuela claims its oil production is over , but oil industry analysts and the U.S. Energy Information Administration believe it to be much lower. In addition to other reporting irregularities, much of its production is extra-heavy oil, which may or may not be included with conventional oil in the various production estimates. The U.S. Energy Information Agency estimated Venezuela's oil production in December 2006 was only , a 24% decline from its peak of 3.3 million in 1997.\n\nRecently, Venezuela has pushed the creation of regional oil initiatives for the Caribbean (Petrocaribe), the Andean region (Petroandino), and South America (Petrosur), and Latin America (Petroamerica). The initiatives include assistance for oil developments, investments in refining capacity, and preferential oil pricing. The most developed of these three is the Petrocaribe initiative, with 13 nations signing a preliminary agreement in 2005. Under Petrocaribe, Venezuela will offer crude oil and petroleum products to Caribbean nations under preferential terms and prices, with Jamaica as the first nation to sign on in August 2005.\n\n"}
{"id": "47308628", "url": "https://en.wikipedia.org/wiki?curid=47308628", "title": "Petrowatch", "text": "Petrowatch\n\nPetrowatch is an English-language market intelligence newsletter on the Indian oil industry published uninterrupted since February 1997.\n\n\"Petrowatch\" has been cited by the BBC, \"The New York Times\", \"The Wall Street Journal\", \"Forbes India\", and the \"Financial Times\" for its coverage of the Indian oil industry, the Ministry of Petroleum and Natural Gas, the Directorate General of Hydrocarbons, and the Petroleum and Natural Gas Regulatory Board.\n\nIn addition \"Petrowatch\" has been cited several times by Japanese investment bank Nomura, most recently in its India Gas Report published in February 2015 on Liquefied Natural Gas importer Petronet LNG.\n\nIn 2008 the Directorate General of Hydrocarbons, the upstream technical arm of the Ministry of Petroleum and Natural Gas, complained about \"Petrowatch\" coverage of the NELP-VII licensing round for the auction of 57 exploration blocks where the newsletter said attracting new companies to India would be difficult.\n\nIn 2009 \"Petrowatch\" was engaged in a dispute with VK Sibal, director general of the DGH, during which Sibal advised Canadian explorer Canoro Resources and other foreign companies interested in the Indian oil industry to unsubscribe from the website immediately. In his letter Sibal said \"Petrowatch\" falsely alleged that there had been a raid by India's Central Bureau of Investigation law enforcement agency on the DGH office and that \"Petrowatch\" was engaged in \"crass yellow journalism\" and publishing misleading information.\n\nIn July 2011 Sibal was investigated by the CBI for his alleged favouritism towards Reliance Industries ahead of its $5.69bn development of the KG-D6 gasfield in the Krishna Godavari Basin on India's East Coast.\n\nIn February 2015 \"Petrowatch\" was implicated in the \"Leakgate\" controversy over the theft of classified government correspondence from the Indian oil ministry, leading to an apology from the Indian financial daily \"Business Standard\" newspaper, after it emerged that the newspaper had wrongly referred to Petrowatch.com as among paid websites that uploaded government documents.\n\nSince 2011 \"Petrowatch\" articles on the leak and theft of confidential government documents by oil ministry personnel on behalf of corporate bodies and parallel oil and gas websites for as little as Rs500 ($10) for a cabinet note or Rs50,000/mth ($1000) for a regular stream of classified information had been largely ignored.\n\nFollowing the Leakgate scandal, \"Petrowatch\" head of operations Vivek Mahajan published a code of conduct detailing its remit and the ethical news-gathering policy followed by its journalists. In its code of conduct Petrowatch makes it clear that no one expects bribes from its reporters or staff or advises them to undertake\ncorrupt activity knowing that any such suggestion will be published for industry at large.\n\n\n"}
{"id": "26251306", "url": "https://en.wikipedia.org/wiki?curid=26251306", "title": "Plasma transferred wire arc thermal spraying", "text": "Plasma transferred wire arc thermal spraying\n\nPlasma transferred wire arc (PTWA) thermal spraying is a thermal spraying process that deposits a coating on the internal surface of a cylindrical surface, or external surface of any geometry. It is predominantly known for its use in coating the cylinder bores of an engine, enabling the use of aluminum engine blocks without the need for heavy cast iron sleeves. A single conductive wire is used as \"feedstock\" for the system. A supersonic plasma jet melts the wire, atomizes it and propels it onto the substrate. The plasma jet is formed by a transferred arc between a non-consumable cathode and the wire. After atomization, forced gas transports the stream of molten droplets onto the bore wall. The particles flatten when they impinge on the surface of the substrate, due to their high kinetic energy. The particles rapidly solidify upon contact and can form both crystalline and amorphus phases. There is also the possibility to produce multi-layer coatings. The stacked particles make up a highly wear-resistant coating. All conductive wires up to and including 0.0625\" (1.6mm) can be used as feedstock material, including \"cored\" wires. PTWA can be used to apply a coating to the wear surface of engine or transmission components to replace a bushing or bearing. For example, using PTWA to coat the bearing surface of a connecting rod offers a number of benefits including reductions in weight, cost, friction potential, and stress in the connecting rod.\n\nThe inventors of PTWA received the 2009 IPO National Inventor of the Year award. This technology was initially patented and developed by inventors by Flame-Spray Industries, Inc. The technology was subsequently improved upon by Ford and Flame-Spray Industries. PTWA is currently in use by Nissan in the Nissan GTR, Ford is implementing it in the new Mustang GT500 Shelby, Caterpillar and other manufacturers are using it for re-manufacturing.\n\nOther applications for this process include the spraying of internal diameters of pipes. Any conductive wire can be used as the feedstock material, including \"cored\" wire. Refractory metals as well as low melt materials are easily deposited.\n\nThe recent use of PTWA by Nissan and Ford has been to apply a wear resistant coating on the internal surface of engine block cylinder bores. For hypoeutectic aluminum silicon alloy blocks, PTWA provides a great alternative to cast iron liners which are a higher cost and heavier. PTWA also delivers increased displacement in the same size engine package and a potential for better heat transfer.\n\nPTWA coatings are also applied directly to cast iron engine blocks for re-manufacturing. PTWA coated test engines have been run for over 3 million combined miles of trouble free on-the-road performance. The technology is currently in use at a number of major production facilities around the world. It is also being used to coat worn parts, to make them like-new in re-manufacturing facilities.\n\n"}
{"id": "3010704", "url": "https://en.wikipedia.org/wiki?curid=3010704", "title": "Precooled jet engine", "text": "Precooled jet engine\n\nThe precooled jet engine is a concept that enables jet engines with turbomachinery, as opposed to ramjets, to be used at high speeds. Precooling restores some or all of the performance degredation of the engine compressor (by preventing rotating stall/choking/reduced flow), as well as that of the complete gas generator (by maintainig a significant combustor temperature rise within a fixed turbine temperature limit), which would otherwise prevent flight with high ram temperatures.\n\nFor higher flight speeds precooling may feature a cryogenic fuel-cooled heat exchanger before the air enters the compressor. After gaining heat and vapourising in the heat exchanger, the fuel (e.g. H) burns in the combustor. Precooling using a heat exchanger has not been used in flight, but is predicted to have significantly high thrust and efficiency at speeds up to Mach 5.5. Precooled jet engine cycles were analyzed by Robert P. Carmichael in 1955. Pre-cooled engines avoid the need for an air condenser because, unlike liquid air cycle engines (LACE), pre-cooled engines cool the air without liquefying it.\n\nFor lower flight speeds precooling can be done with mass injection, known as WIPCC (water injection precompressor cooling) This method has been used for short duration (due to limited coolant capacity) increases to an aircraft's normal maximum speed. \"Operation Skyburner\", which gained a world speed record with a McDonnell Douglas F-4 Phantom II, and the Mikoyan Ye-266 (Mig 25). both used a water/alcohol spray to cool the air ahead of the compressor.\n\nPrecooling (as well as combustion chamber water injection) is used at the lowest flight speeds, ie during take off, to increase thrust at high ambient temperatures. \n\nOne main advantage of pre-cooling is (as predicted by the ideal gas law) for a given overall pressure ratio, there is a significant reduction in compressor delivery temperature (T3), which delays reaching the T3 limit to a higher Mach number. Consequently, sea-level conditions (corrected flow) can be maintained after the pre-cooler over a very wide range of flight speeds, thus maximizing net thrust even at high speeds. The compressor and ducting after the inlet is subject to much lower and more consistent temperatures, and hence may be made of light alloys. This reduces the weight of the engine, which further improves the thrust/weight ratio.\n\nHydrogen is a suitable fuel because it is liquid at deeply cryogenic temperatures, and over its useful range has a very high total specific heat capacity, including the latent heat of vapourisation, higher than water.\n\nHowever, the low density of liquid hydrogen has negative effects on the rest of the vehicle, and the vehicle physically becomes very large, although the weight on the undercarriage and wing loading may remain low.\n\nHydrogen causes structural weakening in many materials, known as hydrogen embrittlement.\n\nThe weight of the precooler adds to the weight of the engine, thereby reducing its thrust to weight ratio.\n\nPassing the intake air through the precooler adds to the inlet drag, thereby reducing the engine net thrust, and so reducing the thrust to weight ratio.\n\nDepending on the amount of cooling required, despite its high thermal capacity, more hydrogen may be needed to cool the air than can be burnt with the cooled air. In some cases, part of the excess hydrogen can be burnt in a ramjet with uncooled air to reduce this inefficiency.\n\nUnlike a LACE engine, a precooled engine doesn't need to liquefy the oxygen, so the amount of cooling is reduced as there is no need to cover of fusion of the oxygen and a smaller total temperature drop is required. This in turn reduces the amount of hydrogen used as a heat-sink, but unable to be burnt. In addition a condenser isn't required, giving a weight saving.\n\nRobert P. Carmichael in 1955 devised several engine cycles that used liquid hydrogen to precool the inlet air to the engine before using it as fuel.\n\nInterest in precooled engines saw an emergence in the UK in 1982, when Alan Bond created a precooled air breathing rocket engine design he called SATAN. The idea was developed as part of the HOTOL SSTO spaceplane project, and became the Rolls-Royce RB545. In 1989, after the HOTOL project was discontinued, some of the RB545 engineers created a company, Reaction Engines Ltd, to develop the idea into the SABRE engine, and the associated Skylon spaceplane.\n\nIn 1987, N Tanatsugu published \"Analytical Study of Space Plane Powered by Air-Turbo Ramjet with Intake Air Cooler.\" part of Japan's ISAS (now JAXA) study into an Air-Turbo Ramjet (ATR, later ATREX after the addition of an expander cycle) intended to power the first stage of a TSTO spaceplane. ATREX was superseded by the Preecooled Turbojet (PCTJ) and Hypersonic Turbojet studies. A liquid nitrogen precooled hydrogen burning test engine was flown at Mach 2 at Taiki Aerospace Research Field in September 2010.\n\n"}
{"id": "9212270", "url": "https://en.wikipedia.org/wiki?curid=9212270", "title": "Punicic acid", "text": "Punicic acid\n\nPunicic acid (also called trichosanic acid) is a polyunsaturated fatty acid, 18:3 \"cis\"-9, \"trans\"-11, \"cis\"-13. It is named for the pomegranate, (\"Punica granatum\"), and is obtained from pomegranate seed oil. \nIt is also found in the seed oils of snake gourd.\n\nPunicic acid is a conjugated linolenic acid or CLnA; i.e. it has three conjugated double bonds. It is chemically similar to the conjugated linoleic acids, or CLA, which have two. It has been erroneous classified as \"n-5\" or \"omega-5\" polyunsaturated fatty acid because the last double bond is located five carbons from the terminal \"omega\" carbon, but the \"n-x\" notation can only be applied to all \"cis\", methylene-interrupted polyunsaturated fatty acids.\nIn lab rats, punicic acid was converted to the CLA rumenic acid (9Z11E-CLA). \"In vitro\", it shows anticancer activity against prostate cancer cells. OLETF rats—a strain which becomes obese—remained relatively lean when punicic acid was added to their feed. \n"}
{"id": "17895738", "url": "https://en.wikipedia.org/wiki?curid=17895738", "title": "Renewable energy sculpture", "text": "Renewable energy sculpture\n\nA renewable energy sculpture is a sculpture that produces power from renewable sources, such as solar, wind, geothermal, hydroelectric or tidal.\nSuch a sculpture is functionally both a renewable energy generator and an artwork, fulfilling utilitarian, aesthetic, and cultural functions. The idea of renewable energy sculptures has been pioneered by ecofuturist visionaries such as artists Patrice Stellest, Sarah Hall, Julian H. Scaff, Patrick Marold, Elena Paroucheva, architects Laurie Chetwood and Nicholas Grimshaw, University of Illinois professor Bil Becket, and collaborations such as the Land Art Generator Initiative. Echoing the philosophy of the environmental art movement as a whole, artists creating renewable energy sculpture believe that the aesthetics of the artworks are inextricably linked to their ecological function.\n\n"}
{"id": "33167122", "url": "https://en.wikipedia.org/wiki?curid=33167122", "title": "Root carving", "text": "Root carving\n\nRoot carving is a traditional Chinese art form. It consists of carving and polishing tree roots into various artistic creations.\n\nUsing roots to make necessities has been practiced since primitive society. Like other artistic crafts, art of roots produced from primitive labor. The earliest root carvings are “辟邪” and “角形器” showing up in the Warring States period.\n\nIn the Sui and Tang dynasties, root carving works not only prevailed in folk, but they were also cherished by the governing class. In the Tang dynasty, people laid emphasis on the natural forms of roots, cleverly taking advantage of the effect of corrosion and moth-eaten.\n\nIn the Song and Yuan dynasties, art of root carving not only developed in the court and folk, but also appeared in grottoes and temples. Roots were used to carve the statues of the Buddha, always comparing favorably with the clay.\n\nRoot carving preserves natural beauty. Ancient artists created lifelike and vivid works by a special technique using expression based on the roots' natural forms. This kind of creation is not completely artificial, but created by both human beings and nature.\n\nRoot carving is different from engraving. It combines peculiarity with ingeniousness. Although its aesthetic principals share common ground with engraving, at the same time they are applied uniquely. The common ground is that they share expressive techniques of wood carving, sculpture, stone carving and so on, overcoming weaknesses by acquiring others strong points. The difference lies in the natural shape of roots. During the creative process, root carving mostly maintains the natural form of the root, adding some artificial polishing. In other words, root carving is guided by the inherent qualities of the root, rather than by shaping images merely through carving.\n\nCreative effect achieved from the same material can vary from artist to artist. Within the field three factors are considered of major importance.\n\n\n\n"}
{"id": "56158508", "url": "https://en.wikipedia.org/wiki?curid=56158508", "title": "Rural Power Company Limited", "text": "Rural Power Company Limited\n\nRural Power Company Limited is an autonomous government company responsible for rural electrification and power in Bangladesh and is located in Dhaka, Bangladesh.\n\nRural Power Company Limited was established on 31 December 1994. It was the first independent power producer in Bangladesh not under the Bangladesh Power Development Board. The decision to form it was made by Executive Committee of the National Economic Council (ECNEC) on 23 November 1994. It was formed by the Rural Electrification Board and is managed by a board of directors. On 23 December 2004 it started work on a power plant in Savar in partnership with Dhaka North Power Project and Swiss company Alsthom despite specific instructions and opinions from the Power Cell, Power Development Board and the Prime Minister's Office. Sigma Huda was its legal advisor. Lahmeyer International Palli Power Services Limited sued the company in October 2005 in an arbitration court in Singapore over the cancellation of the Mymensingh 140-megawatt power plant. Both companies had rivalries over the management of the power plant. The government of Bangladesh filed a case against the LIPPS chief AZ Rezaul Haq for attempting to sabotage the power plan and attempted to arrest him.\n"}
{"id": "24343159", "url": "https://en.wikipedia.org/wiki?curid=24343159", "title": "Spoolbase", "text": "Spoolbase\n\nA spoolbase is a shore-based facility used to facilitate continuous pipe laying for offshore oil and gas production. The facility allows the welding of single or double joints (40' or 80') of steel pipe of 4\" to 18\" diameter, into predetermined lengths for spooling onto a reel lay vessel.\n\nShore based spoolbases serve the oil and gas sector from locations in the USA, UK, Norway, Brazil, and Angola, and portable spoolbases may be set up in any location to suit local requirements.\n"}
{"id": "8311050", "url": "https://en.wikipedia.org/wiki?curid=8311050", "title": "Unconventional wind turbines", "text": "Unconventional wind turbines\n\nUnconventional wind turbines are those that differ significantly from the most common types in use. , the most common type of wind turbine is the three-bladed upwind horizontal-axis wind turbine (HAWT), where the turbine rotor is at the front of the nacelle and facing the wind upstream of its supporting turbine tower. A second major unit type is also classified by its axis: the vertical-axis wind turbine (VAWT), with blades extending upwards that are supported by a rotating framework.\n\nDue to the large growth of the wind power industry and the length of its historical development dating back to windmills, many different wind turbine designs exist, are in current development, or have been proposed due to their unique features. The wide variety of designs reflects ongoing commercial, technological, and inventive interests in harvesting wind resources both more efficiently and to the greatest extent possible, with costs that may be either lower or greater than conventional three-bladed HAWT designs.\n\nSome turbine designs that differ from the standard type have had limited commercial use, while others have only been demonstrated or are only theoretical concepts with no practical applications. Such unconventional designs cover a wide gamut of innovations, including different rotor types, basic functionalities, supporting structures and form-factors.\n\nNearly all modern wind turbines uses rotors with three blades, but some use only two blades. This was the type used at Kaiser-Wilhelm-Koog, Germany, where a large experimental two-bladed unit—the GROWIAN, or \"Große Windkraftanlage\" (big wind turbine)—operated from 1983 to 1987. Other prototypes and several wind turbine types were also manufactured by NedWind. The Eemmeerdijk Wind Park in Zeewolde, Netherlands uses only two-bladed turbines. Wind turbines with two blades are manufactured by Nordic Windpower, such as model # N 1000, and by GC China Turbine Corp. The NASA wind turbines (1975-1996) each had 2-blade rotors, producing the same energy at lower cost than three-blade rotor designs.\n\nNearly all wind turbines are of an upwind design, meaning the rotor is in front of the nacelle when the wind is blowing. Some turbines are of a downwind design, meaning the rotor is behind the nacelle when the wind is blowing.\nStill something of a research project, the ducted rotor consists of a turbine inside a duct that flares out at the back. They are also referred as Diffuser-Augmented Wind Turbines (i.e. DAWT). The main advantage of the ducted rotor is that it can operate in a wide range of winds and generate a higher power per unit of rotor area. Another advantage is that the generator operates at a high rotation rate, so it doesn't require a bulky gearbox, allowing the mechanical portion to be smaller and lighter. A disadvantage is that (apart from the gearbox) it is more complicated than the unducted rotor and the duct is usually quite heavy, which puts an added load on the tower. The Éolienne Bollée is an example of a DAWT.\n\nTwo or more rotors may be mounted to the same driveshaft, with their combined co-rotation together turning the same generator: fresh wind is brought to each rotor by sufficient spacing between rotors combined with an offset angle (alpha) from the wind direction. Wake vorticity is recovered as the top of a wake hits the bottom of the next rotor. Power has been multiplied several times using co-axial, multiple rotors in testing conducted by inventor and researcher Douglas Selsam, for the California Energy Commission in 2004. The first commercially available co-axial multi-rotor turbine is the patented dual-rotor American Twin Superturbine from Selsam Innovations in California, with 2 propellers separated by 12 feet. It is the most powerful turbine available, due to this extra rotor. In 2015, Iowa State University aerospace engineers Hui Hu and Anupam Sharma were optimizing designs of multi-rotor systems, including a horizontal-axis co-axial dual-rotor model. In addition to a conventional three-blade rotor, it has a smaller secondary three-blade rotor, covering the near-axis region usually inefficiently harvested. They were considering the overall efficiency of the wind farm, and checking many variations beyond the one mentioned. Preliminary results indicated 10-20% gains, less efficient than is claimed by existing counter-rotating designs but those are complex.\nWhen a system expels or accelerates mass in one direction, the accelerated mass causes a proportional but opposite force on that system. The spinning blade of a single rotor wind turbine causes a significant amount of tangential or rotational air flow. The energy of this tangential air flow is wasted in a single-rotor propeller design. To use this wasted effort, the placement of a second rotor behind the first takes advantage of the disturbed airflow, and can gain up to 40% more energy from a given swept area as compared with a single rotor. Other advantages of contra-rotation include no gear boxes and auto-centering on the wind (no yaw motors/mechanism required). A patent application dated 1992 exists based on work done with the Trimblemill.\n\nWhen the counter-rotating turbines are on the same side of the tower, the blades in front are angled forwards slightly so as to avoid hitting the rear ones. If the turbine blades are on opposite sides of the tower, it is best that the blades at the back be smaller than the blades at the front and set to stall at a higher wind speed. This allows the generator to function at a wider wind speed range than a single-turbine generator for a given tower. To reduce sympathetic vibrations, the two turbines should turn at speeds with few common multiples, for example 7:3 speed ratio.\n\nWhen land or sea area for a second wind turbine does not come at a premium the 40% gain with a second rotor has to be compared with a 100% gain via the expense of a separate foundation and tower with cabling for the second turbine. , no large practical counter-rotating HAWTs are commercially sold.\n\nIn addition to variable pitch blades, furling tails and twisting blades are other improvements on wind turbines. Similar to the variable pitch blades, they may also greatly increase the efficiency of the turbine and be used in \"do-it-yourself\" construction\nDe Nolet is a wind turbine in Rotterdam disguised as a windmill.\n\nThe \"Fuller\" wind turbine is a fully enclosed wind turbine that uses boundary layers instead of blades. Much like a Tesla turbine.\n\nThe concept is similar to a stack of disks on a central shaft, separated by a small air gap. The surface tension of air through the small gaps creates friction, making the disks rotate around the shaft. Vanes help direct the air for improved performance, hence it is not totally bladeless.\n\nIt has been demonstrated that wind turbines could be flown in high-speed winds using high altitude wind power tactics, taking advantage of the winds at high altitudes. A system of automatically controlled tethered kites could also be used to capture energy from high-altitude winds.\n\nThis is a vertical axis turbine, but it isn't favored because of its poor efficiency. One blade is pushed by the wind while the other is being pushed in the opposite direction. Consequently, only one blade is working at a time.\n\nSheerWind's INVELOX technology was developed by Dr. Daryoush Allaei. The invention is really not a turbine, rather a wind capturing and delivery system to a turbine. In a sense, INVELOX is a wind injection system, much like a fuel injection system for cars. It works by accelerating the wind. A large intake captures wind, funnels it down using tapered pipes leading to a concentrator that ends in a Venturi section and finally wind exits from a diffuser. Turbine(s) are placed inside the Venturi section of the INVELOX. Inside the Venturi the dynamic pressure is very high while the static pressure is low. The Turbine converts dynamic pressure or kinetic energy to mechanical rotation and thereby to electrical power using a generator. The device has been constructed and tested, but also criticized for lack of efficiency. , prototypes are being installed, while another is inoperable.\n\nThe Saphonian design produced by Tunisian startup Saphon Energy uses a dish to generate wind pressure and back-and-forth motion that drives a piston.\n\nZephyr Energy Corporation’s patented Windbeam micro generator captures energy from airflow to recharge batteries and power electronic devices. The Windbeam’s novel design allows it to operate silently in wind speeds as low as 2 mph. The generator consists of a lightweight beam suspended by durable long-lasting springs within an outer frame. The beam oscillates rapidly when exposed to airflow due to the effects of multiple fluid flow phenomena. A linear alternator assembly converts the oscillating beam motion into usable electrical energy. A lack of bearings and gears eliminates frictional inefficiencies and noise. The generator can operate in low-light environments unsuitable for solar panels (e.g. HVAC ducts) and is inexpensive due to low cost components and simple construction. The scalable technology can be optimized to satisfy the energy requirements and design constraints of a given application.\n\nInvented by Shawn Frayne. A tensioned but flexible belt vibrates by the passing flow of air, due to aeroelastic flutter. A magnet, mounted at one end of the belt translates in and out of coiled windings producing electricity. The company and product are no longer in existence.\n\nThe Vortex Bladeless device deliberately maximizes vortex shedding, converting wind energy to fluttering of a lightweight vertical pole, then captures that energy with a generator at the bottom of the pole.\n\nA vaneless ion wind generator is a theoretical device that produces electrical energy by using the wind to move electric charge from one electrode to another.\n\nAnother special type of wind turbines are the piezoelectric wind turbines. Turbines with diameters on the scale of 10 centimeters work by flexing piezoelectric crystals as they rotate, sufficient to power small electronic devices.\n\nA few proposals call for generating power from the otherwise wasted energy in the draft created by traffic.\n\nDesigned by Imad Mahawili with Honeywell/WindTronics. This design uses many nylon blades and turns a permanent magnet generator inside out. The magnets are on the tips of the blades, and the stator is on the outside of the generator.\n\nWind turbines may also be used in conjunction with a solar collector to extract the energy due to air heated by the Sun and rising through a large vertical Solar updraft tower.\n\nMost wind turbines around the world belong to individuals or corporations who use them to generate electric power or to perform mechanical work. As such, wind turbines are primarily working devices. However, the large size and height above surroundings of modern industrial wind turbines, combined with their moving rotors, often makes them conspicuous. A few localities have exploited the attention-getting nature of wind turbines, either by putting visitor centers on their bases, or by providing viewing areas. The wind turbines themselves are generally of conventional horizontal-axis, three-bladed design, and generate power to feed electrical grids, but they also serve the unconventional roles of technology demonstration, public relations, and education.\n\nWind-turbines can be installed on roofs of buildings, but this is less common than one might expect. Some examples include Marthalen Landi-Silo in Switzerland, Council House 2 in Melbourne, Australia. Ridgeblade in the UK is like a vertical wind turbine on its side mounted on the apex of a pitched roof. While the Ridgeblade is still in the design stage another example like this, already available in France is the Aeolta AeroCube. Discovery Tower is an office building in Houston, Texas, that incorporates 10 wind turbines in its architecture.\n\nThe Museum of Science in Boston, Massachusetts began constructing a rooftop Wind Turbine Lab in 2009. The lab is testing nine wind turbines from five different manufacturers. Rooftop wind turbines may suffer from turbulence, especially in cities, which reduces power output and accelerates turbine wear. The lab seeks to address the general lack of performance data for urban wind turbines.\n\nDue to structural limitations of buildings, limited space in urban areas, and safety considerations, wind turbines mounted on buildings are usually small (with nameplate capacities in the low kilowatts), rather than the megawatt-class wind turbines that are most economical for wind farms. An exception is the Bahrain World Trade Centre with three 225 kW wind turbines mounted between twin skyscrapers.\n\n"}
{"id": "3436335", "url": "https://en.wikipedia.org/wiki?curid=3436335", "title": "Voltage reduction", "text": "Voltage reduction\n\nIn a simple resistive circuit, a reduction in the voltage across the resistance will result in a reduction in the power dissipated by the circuit.\n\nElectric utilities have discovered that this basic principle can save utility companies, and their customers, a significant amount of money. Utilities are able to shave the peak of their power demand curves by reducing the voltage across their distribution system. When a utility reaches a point where power demand is expected to exceed supply, utilities only have two options. Either purchase power from another utility, usually at substantial prices, or reduce demand. Often utilities use load management systems to turn off customers' air conditioners, water heaters, and pool pumps to reduce demand. Voltage reduction has become another option for utilities to reduce demand—typically unbeknownst to the customer. However, only the resistive portion of the load responds to the reduction in voltage to reduce aggregate demand. Loads such as incandescent lights and heater coils will use less power as the voltage is lowered. On the other hand, induction motor loads are unaffected by the reduction in voltage, because the current simply rises to account for no change or even a slight increase in power consumption.\n\nEven some resistive loads provide only short-term benefits. A phenomenon known as load diversity plays a role in voltage reduction and can counteract its effects on occasion. The concept of load diversity can most easily be explained with an example. In your neighborhood, it is unlikely that all of the homes' water heaters are on at the same time. Particularly during non-hot water usage hours (morning and evening showers), when your hot water heater is on, your neighbor's may be off. Due to the distributed and noncoincident nature of these loads, the aggregate peak can remain relatively constant. However, if the voltage is reduced to all of the resistive elements in the water heaters, the elements will not be able to heat the water as quickly. While an immediate reduction in the power demand will be recognized upon initiating voltage reduction, over time water heaters will need to be on longer to achieve the thermostat-set water temperature. Thus, more water heaters will be on at the same time. This will cause the aggregate peak to increase substantially. Therefore, with respect to thermostat controlled resistive loads the benefits of voltage reduction can be short lived, and may occasionally end up increasing the aggregate load demand.\n\nConversely increasing the voltage may increase power demand from the resistive loads. Again, the thermostat-controlled resistive loads will react differently, but it is clear that it will cost a customer more to turn on that incandescent reading lamp with higher voltage than it would at a lower voltage.\n\n"}
{"id": "8940291", "url": "https://en.wikipedia.org/wiki?curid=8940291", "title": "World-Wide Volkswagen Corp. v. Woodson", "text": "World-Wide Volkswagen Corp. v. Woodson\n\nWorld-Wide Volkswagen Corp v. Woodson, 444 U.S. 286 (1980), is a United States Supreme Court case involving strict products liability, personal injury and various procedural issues and considerations. The 1980 opinion, written by Justice Byron White, is included in the first-year civil procedure curriculum at nearly every American law school for its focus on personal jurisdiction.\n\nHarry and Kay Robinson purchased a new Audi 100 LS automobile from Seaway Volkswagen, Inc. in Massena, New York, in 1976. The following year, as Kay Robinson passed through Oklahoma on Interstate 44 en route to the Robinsons' new home in Arizona, the Audi was struck from the rear by a drunk driver in a 1971 Ford Torino. The impact of the collision itself did not directly injure any of the Robinsons, but the crash resulted in the Audi's doors jamming shut and a puncture in the car's gas tank. A fire then severely burned the trapped Kay Robinson and her two children riding in the Audi, Eva and Sam.\n\nThe Robinsons did not bring a suit against Lloyd Hull, the drunk driver. He had no insurance or assets and was therefore judgment proof. The Robinsons claimed that a product defect in the car led to the injuries they sustained—specifically, the Audi's gas tank was located beneath the trunk, in an area that the Robinsons claimed was susceptible to being punctured and igniting in a rear-end collision. They brought suit against the automobile’s manufacturer (Audi), its importer (Volkswagen of America), its regional distributor (World-Wide Volkswagen Corp.), and its retailer dealer (Seaway Volkswagen).\n\nThe Robinsons' Oklahoma attorney brought the lawsuit in state court in Creek County, Oklahoma, the county in which the accident had occurred. Creek County was at that time known as home to some of the most plaintiff sympathetic juries in the country. However, since the lawsuit met requirements for concurrent jurisdiction in both state and federal court, Audi and Volkswagen would have had the ability to ask for the case to be removed from state court in Creek County and taken directly to federal court. One of the factors which governs concurrent jurisdiction is diversity of citizenship, or whether a defendant and plaintiff are from the same state. In the case of multiple defendants, if one defendants' state citizenship matches the plaintiff's, concurrent federal jurisdiction does not apply and the case cannot be removed to federal court unless the case concerns a matter of federal law. It has therefore been stated that the reason the Robinsons' attorney added the New York regional distributor and New York dealership as defendants was to prevent Audi and Volkswagen from being able to remove the case from what was generally seen as a Creek County pro-plaintiffs' jury to what would be a federal court jury in Tulsa that might be more sympathetic to the car manufacturers' case. The Robinsons had not yet completed a move to Arizona, so they were still considered to be legal residents of New York.\n\nThe Robinsons first sued only Volkswagen of America, World-Wide, and Seaway. They later amended the suit to include Volkswagenwerk Aktiengesellschaft (Volkswagen AG), the German parent company. A second amendment was included after they learned during formal discovery that Audi NSU Auto Union Aktiengesellschaft (Audi AG) was the manufacturing parent company rather than Volkswagen AG; they substituted Audi AG for Volkswagen AG.\n\nWhen they were brought in as defendants in the case, World-Wide and Seaway claimed that Oklahoma’s exercise of personal or \"in personam\" jurisdiction over them would offend the limitations on states' jurisdiction imposed by the Due Process Clause of the Fourteenth Amendment to the Constitution of the United States; they asked to be removed from the suit. Audi and Volkswagen, which sold cars in the state of Oklahoma, did not attempt to assert that the Oklahoma state court had no jurisdiction over them.\n\nWhether Seaway Volkswagen and Worldwide Volkswagen had sufficient minimum contacts with Oklahoma, such that these defendants would be subject to the jurisdiction of the Oklahoma state courts.\n\nThe United States Supreme Court excerpts the reasoning of the Oklahoma Supreme Court in affirming that Oklahoma's long-arm statute provides for jurisdiction over World Wide and Seaway. \"...the product being sold and distributed by the petitioners [World-Wide and Seaway] is by its very design and purpose so mobile that petitioners can foresee its possible use in Oklahoma. This is especially true of the distributor [Seaway], who has the exclusive right to distribute such automobile in New York, New Jersey, and Connecticut. The evidence presented below demonstrated that goods sold and distributed by the petitioners were used in the State of Oklahoma, and under the facts we believe it reasonable to infer, given the retail value of the automobile, that the petitioners derive substantial income from automobiles which from time to time are used in the State of Oklahoma. This being the case, we hold that under the facts presented, the trial court was justified in concluding that the petitioners derive substantial revenue from goods used or consumed in this State.\"\n\nThe emphasis on \"substantial revenue\" comes from the relevant long-arm statute, since repealed, stating that a court can exercise jurisdiction over persons (corporate or natural) who cause injury in Oklahoma and derive, \"...substantial revenue from goods used or consumed or services rendered, in this state...\" \n\nThe district court rejected World-Wide and Seaway's constitutional claim and reaffirmed that original ruling in denying petitioners’ motion for reconsideration.\n\nThe United States Supreme Court reversed the decision of the federal appeals court and agreed with World-Wide and Seaway that Oklahoma did not have jurisdiction over them.\n\nThe Due Process Clause of the 14th Amendment limits the power of a state court to exercise personal or \"in personam\" jurisdiction against a nonresident defendant. A state court may exercise personal jurisdiction over a nonresident only so long as there exist \"minimum contacts\" between the defendant and the forum state.\n\nThe court stated that the concept of minimum contacts can be seen to perform two related but distinguishable functions. It protects the defendant against the burdens of litigating in a distant or inconvenient forum and it acts to ensure that the States, through their courts, do not reach out beyond the limits imposed on them by their status as coequal sovereigns in a federal system.\n\nThe 14th Amendment provides protection against inconvenient litigation, typically described in terms of “reasonableness” or “fairness”: “Does not offend ‘traditional notions of fair play and substantial justice.” The relationship between the defendant and forum must be “reasonable.” The burden on the defendant is to be balanced against other factors, including the plaintiff’s interest in obtaining convenient and effective relief.\n\nThe due process clause “does not contemplate that a state may make binding a judgment in personam against an individual or corporate defendant with which the state has no contacts, ties or relations.” Even if the defendant would suffer minimal or no inconvenience from being forced to litigate before the tribunals of another state, even if the forum state has a strong interest in applying its law to the controversy, and even if the forum state is the most convenient location for litigation, the Due Process Clause may sometimes act to divest the state of its power to render a valid judgment.\n\nThe petitioners' contentions were deemed correct by the Supreme Court, which agreed that the two corporations did not have minimum contacts in Oklahoma, did not avail themselves of any of the privileges or benefits of Oklahoma law; Oklahoma therefore had no jurisdiction over the two companies.\n\nThe Robinson's counterclaim and Justice Brennan's dissenting opinion were based on foreseeability – a car sold in New York is mobile, and therefore it was \"foreseeable\" by World-Wide and Seaway that a car sold by them could subsequently lead to an injury in Oklahoma. The majority opinion of the Supreme Court rejected this argument, saying that foreseeability alone could not provide the basis for personal jurisdiction over a defendant and the two petitioning companies had no other contacts with Oklahoma.\n\nWith World-Wide and Seaway unable to be held as defendants in the Robinsons' case against Audi and Volkswagen, the case now had diversity of citizenship and was concurrently eligible for both state and federal court. Audi and Volkswagen removed the case from Creek County into federal district court in Tulsa, Oklahoma, where a jury sided with the two car companies. The Tulsa jury indicated that they believed the speed of Lloyd Hull's car, rather than the Audi's gas tank, was responsible for the fire.\n\n\n"}
{"id": "57183712", "url": "https://en.wikipedia.org/wiki?curid=57183712", "title": "Zel'dovich–Liñán model", "text": "Zel'dovich–Liñán model\n\nIn combustion, Zel'dovich–Liñán model is a two-step reaction model for the combustion processes, named after Yakov Borisovich Zel'dovich and Amable Liñán. The model includes a chain-branching and a chain-breaking (or radical recombination) reaction. The model was first introduced by Zel'dovich in 1948 and later analysed by Liñán using activation energy asymptotics in 1971. The mechanism reads as\n\nwhere formula_2 is the fuel, formula_3 is an intermediate radical, formula_4 is the third body and formula_5 is the product. The first reaction is the chain-branching reaction, which is considered to be auto-catalytic (consumes no heat or releases no heat), with very large activation energy and the second reaction is the chain-breaking (or radical-recombination) reaction, where all of the heat in the combustion is released, with almost negligible activation energy.\n\n"}
{"id": "34420", "url": "https://en.wikipedia.org/wiki?curid=34420", "title": "Zinc", "text": "Zinc\n\nZinc is a chemical element with symbol Zn and atomic number 30. It is the first element in group 12 of the periodic table. In some respects zinc is chemically similar to magnesium: both elements exhibit only one normal oxidation state (+2), and the Zn and Mg ions are of similar size. Zinc is the 24th most abundant element in Earth's crust and has five stable isotopes. The most common zinc ore is sphalerite (zinc blende), a zinc sulfide mineral. The largest workable lodes are in Australia, Asia, and the United States. Zinc is refined by froth flotation of the ore, roasting, and final extraction using electricity (electrowinning).\n\nBrass, an alloy of copper and zinc in various proportions, was used as early as the third millennium BC in the Aegean, Iraq, the United Arab Emirates, Kalmykia, Turkmenistan and Georgia, and the second millennium BC in West India, Uzbekistan, Iran, Syria, Iraq, and Israel (Judea). Zinc metal was not produced on a large scale until the 12th century in India, though it was known to the ancient Romans and Greeks. The mines of Rajasthan have given definite evidence of zinc production going back to the 6th century BC. To date, the oldest evidence of pure zinc comes from Zawar, in Rajasthan, as early as the 9th century AD when a distillation process was employed to make pure zinc. Alchemists burned zinc in air to form what they called \"philosopher's wool\" or \"white snow\".\n\nThe element was probably named by the alchemist Paracelsus after the German word \"Zinke\" (prong, tooth). German chemist Andreas Sigismund Marggraf is credited with discovering pure metallic zinc in 1746. Work by Luigi Galvani and Alessandro Volta uncovered the electrochemical properties of zinc by 1800. Corrosion-resistant zinc plating of iron (hot-dip galvanizing) is the major application for zinc. Other applications are in electrical batteries, small non-structural castings, and alloys such as brass. A variety of zinc compounds are commonly used, such as zinc carbonate and zinc gluconate (as dietary supplements), zinc chloride (in deodorants), zinc pyrithione (anti-dandruff shampoos), zinc sulfide (in luminescent paints), and zinc methyl or zinc diethyl in the organic laboratory.\n\nZinc is an essential mineral, including to prenatal and postnatal development. Zinc deficiency affects about two billion people in the developing world and is associated with many diseases. In children, deficiency causes growth retardation, delayed sexual maturation, infection susceptibility, and diarrhea. Enzymes with a zinc atom in the reactive center are widespread in biochemistry, such as alcohol dehydrogenase in humans. Consumption of excess zinc can cause ataxia, lethargy, and copper deficiency.\n\nZinc is a bluish-white, lustrous, diamagnetic metal, though most common commercial grades of the metal have a dull finish. It is somewhat less dense than iron and has a hexagonal crystal structure, with a distorted form of hexagonal close packing, in which each atom has six nearest neighbors (at 265.9 pm) in its own plane and six others at a greater distance of 290.6 pm. The metal is hard and brittle at most temperatures but becomes malleable between 100 and 150 °C. Above 210 °C, the metal becomes brittle again and can be pulverized by beating. Zinc is a fair conductor of electricity. For a metal, zinc has relatively low melting (419.5 °C) and boiling points (907 °C). The melting point is the lowest of all the d-block metals aside from mercury and cadmium; for this, among other reasons, zinc, cadmium, and mercury are often not considered to be transition metals like the rest of the d-block metals are.\n\nMany alloys contain zinc, including brass. Other metals long known to form binary alloys with zinc are aluminium, antimony, bismuth, gold, iron, lead, mercury, silver, tin, magnesium, cobalt, nickel, tellurium, and sodium. Although neither zinc nor zirconium are ferromagnetic, their alloy exhibits ferromagnetism below 35 K.\n\nA bar of zinc generates a characteristic sound when bent, similar to tin cry.\n\nZinc makes up about 75 ppm (0.0075%) of Earth's crust, making it the 24th most abundant element. Soil contains zinc in 5–770 ppm with an average 64 ppm. Seawater has only 30 ppb and the atmosphere, 0.1–4 µg/m. The element is normally found in association with other base metals such as copper and lead in ores. Zinc is a chalcophile, meaning the element is more likely to be found in minerals together with sulfur and other heavy chalcogens, rather than with the light chalcogen oxygen or with non-chalcogen electronegative elements such as the halogens. Sulfides formed as the crust solidified under the reducing conditions of the early Earth's atmosphere. Sphalerite, which is a form of zinc sulfide, is the most heavily mined zinc-containing ore because its concentrate contains 60–62% zinc.\n\nOther source minerals for zinc include smithsonite (zinc carbonate), hemimorphite (zinc silicate), wurtzite (another zinc sulfide), and sometimes hydrozincite (basic zinc carbonate). With the exception of wurtzite, all these other minerals were formed by weathering of the primordial zinc sulfides.\n\nIdentified world zinc resources total about 1.9–2.8 billion tonnes. Large deposits are in Australia, Canada and the United States, with the largest reserves in Iran. The most recent estimate of reserve base for zinc (meets specified minimum physical criteria related to current mining and production practices) was made in 2009 and calculated to be roughly 480 Mt. Zinc reserves, on the other hand, are geologically identified ore bodies whose suitability for recovery is economically based (location, grade, quality, and quantity) at the time of determination. Since exploration and mine development is an ongoing process, the amount of zinc reserves is not a fixed number and sustainability of zinc ore supplies cannot be judged by simply extrapolating the combined mine life of today's zinc mines. This concept is well supported by data from the United States Geological Survey (USGS), which illustrates that although refined zinc production increased 80% between 1990 and 2010, the reserve lifetime for zinc has remained unchanged. About 346 million tonnes have been extracted throughout history to 2002, and scholars have estimated that about 109–305 million tonnes are in use.\n\nFive isotopes of zinc occur in nature. Zn is the most abundant isotope (48.63% natural abundance). That isotope has such a long half-life, at , that its radioactivity can be ignored. Similarly, (0.6%), with a half-life of is not usually considered to be radioactive. The other isotopes found in nature are (28%), (4%) and (19%).\n\nSeveral dozen radioisotopes have been characterized. , which has a half-life of 243.66 days, is the least active radioisotope, followed by with a half-life of 46.5 hours. Zinc has 10 nuclear isomers. Zn has the longest half-life, 13.76 h. The superscript \"m\" indicates a metastable isotope. The nucleus of a metastable isotope is in an excited state and will return to the ground state by emitting a photon in the form of a gamma ray. has three excited metastable states and has two. The isotopes , , and each have only one excited metastable state.\n\nThe most common decay mode of a radioisotope of zinc with a mass number lower than 66 is electron capture. The decay product resulting from electron capture is an isotope of copper.\n\nThe most common decay mode of a radioisotope of zinc with mass number higher than 66 is beta decay (β), which produces an isotope of gallium.\n\nZinc has an electron configuration of [Ar]3d4s and is a member of the group 12 of the periodic table. It is a moderately reactive metal and strong reducing agent. The surface of the pure metal tarnishes quickly, eventually forming a protective passivating layer of the basic zinc carbonate, , by reaction with atmospheric carbon dioxide. This layer helps prevent further reaction with air and water.\n\nZinc burns in air with a bright bluish-green flame, giving off fumes of zinc oxide. Zinc reacts readily with acids, alkalis and other non-metals. Extremely pure zinc reacts only slowly at room temperature with acids. Strong acids, such as hydrochloric or sulfuric acid, can remove the passivating layer and subsequent reaction with water releases hydrogen gas.\n\nThe chemistry of zinc is dominated by the +2 oxidation state. When compounds in this oxidation state are formed, the outer shell \"s\" electrons are lost, yielding a bare zinc ion with the electronic configuration [Ar]3d. In aqueous solution an octahedral complex, is the predominant species. The volatilization of zinc in combination with zinc chloride at temperatures above 285 °C indicates the formation of , a zinc compound with a +1 oxidation state. No compounds of zinc in oxidation states other than +1 or +2 are known. Calculations indicate that a zinc compound with the oxidation state of +4 is unlikely to exist.\n\nZinc chemistry is similar to the chemistry of the late first-row transition metals, nickel and copper, though it has a filled d-shell and compounds are diamagnetic and mostly colorless. The ionic radii of zinc and magnesium happen to be nearly identical. Because of this some of the equivalent salts have the same crystal structure, and in other circumstances where ionic radius is a determining factor, the chemistry of zinc has much in common with that of magnesium. In other respects, there is little similarity with the late first-row transition metals. Zinc tends to form bonds with a greater degree of covalency and much more stable complexes with N- and S- donors. Complexes of zinc are mostly 4- or 6- coordinate although 5-coordinate complexes are known.\n\nZinc(I) compounds are rare and need bulky ligands to stabilize the low oxidation state. Most zinc(I) compounds contain formally the [Zn] core, which is analogous to the [Hg] dimeric cation present in mercury(I) compounds. The diamagnetic nature of the ion confirms its dimeric structure. The first zinc(I) compound containing the Zn–Zn bond, (η-CMe)Zn, is also the first dimetallocene. The [Zn] ion rapidly disproportionates into zinc metal and zinc(II), and has been obtained only a yellow glass only by cooling a solution of metallic zinc in molten ZnCl.\n\nBinary compounds of zinc are known for most of the metalloids and all the nonmetals except the noble gases. The oxide ZnO is a white powder that is nearly insoluble in neutral aqueous solutions, but is amphoteric, dissolving in both strong basic and acidic solutions. The other chalcogenides (ZnS, ZnSe, and ZnTe) have varied applications in electronics and optics. Pnictogenides (, , and ), the peroxide (), the hydride (), and the carbide () are also known. Of the four halides, has the most ionic character, while the others (, , and ) have relatively low melting points and are considered to have more covalent character.\n\nIn weak basic solutions containing ions, the hydroxide forms as a white precipitate. In stronger alkaline solutions, this hydroxide is dissolved to form zincates (). The nitrate , chlorate , sulfate , phosphate , molybdate , cyanide , arsenite , arsenate and the chromate (one of the few colored zinc compounds) are a few examples of other common inorganic compounds of zinc. One of the simplest examples of an organic compound of zinc is the acetate ().\n\nOrganozinc compounds are those that contain zinc–carbon covalent bonds. Diethylzinc () is a reagent in synthetic chemistry. It was first reported in 1848 from the reaction of zinc and ethyl iodide, and was the first compound known to contain a metal–carbon sigma bond.\n\nCobalticyanide paper (Rinnmann's test for Zn) can be used as a chemical indicator for zinc. 4 g of KCo(CN) and 1 g of KClO is dissolved on 100 ml of water. Paper is dipped in the solution and dried at 100 °C. One drop of the sample is dropped onto the dry paper and heated. A green disc indicates the presence of zinc.\n\nVarious isolated examples of the use of impure zinc in ancient times have been discovered. Zinc ores were used to make the zinc–copper alloy brass thousands of years prior to the discovery of zinc as a separate element. Judean brass from the 14th to 10th centuries BC contains 23% zinc.\n\nKnowledge of how to produce brass spread to Ancient Greece by the 7th century BC, but few varieties were made. Ornaments made of alloys containing 80–90% zinc, with lead, iron, antimony, and other metals making up the remainder, have been found that are 2,500 years old. A possibly prehistoric statuette containing 87.5% zinc was found in a Dacian archaeological site.\n\nThe oldest known pills were made of the zinc carbonates hydrozincite and smithsonite. The pills were used for sore eyes and were found aboard the Roman ship Relitto del Pozzino, wrecked in 140 BC.\n\nThe manufacture of brass was known to the Romans by about 30 BC. They made brass by heating powdered calamine (zinc silicate or carbonate), charcoal and copper together in a crucible. The resulting calamine brass was then either cast or hammered into shape for use in weaponry. Some coins struck by Romans in the Christian era are made of what is probably calamine brass.\n\nStrabo writing in the 1st century BC (but quoting a now lost work of the 4th century BC historian Theopompus) mentions \"drops of false silver\" which when mixed with copper make brass. This may refer to small quantities of zinc that is a by-product of smelting sulfide ores. Zinc in such remnants in smelting ovens was usually discarded as it was thought to be worthless.\n\nThe Berne zinc tablet is a votive plaque dating to Roman Gaul made of an alloy that is mostly zinc.\n\nThe Charaka Samhita, thought to have been written between 300 and 500 AD, mentions a metal which, when oxidized, produces \"pushpanjan\", thought to be zinc oxide. Zinc mines at Zawar, near Udaipur in India, have been active since the Mauryan period ( and 187 BCE). The smelting of metallic zinc here, however, appears to have begun around the 12th century AD. One estimate is that this location produced an estimated million tonnes of metallic zinc and zinc oxide from the 12th to 16th centuries. Another estimate gives a total production of 60,000 tonnes of metallic zinc over this period. The Rasaratna Samuccaya, written in approximately the 13th century AD, mentions two types of zinc-containing ores: one used for metal extraction and another used for medicinal purposes.\n\nZinc was distinctly recognized as a metal under the designation of \"Yasada\" or Jasada in the medical Lexicon ascribed to the Hindu king Madanapala (of Taka dynasty) and written about the year 1374. Smelting and extraction of impure zinc by reducing calamine with wool and other organic substances was accomplished in the 13th century in India. The Chinese did not learn of the technique until the 17th century.\nAlchemists burned zinc metal in air and collected the resulting zinc oxide on a condenser. Some alchemists called this zinc oxide \"lana philosophica\", Latin for \"philosopher's wool\", because it collected in wooly tufts, whereas others thought it looked like white snow and named it \"nix album\".\n\nThe name of the metal was probably first documented by Paracelsus, a Swiss-born German alchemist, who referred to the metal as \"zincum\" or \"zinken\" in his book \"Liber Mineralium II\", in the 16th century. The word is probably derived from the German , and supposedly meant \"tooth-like, pointed or jagged\" (metallic zinc crystals have a needle-like appearance). \"Zink\" could also imply \"tin-like\" because of its relation to German \"zinn\" meaning tin. Yet another possibility is that the word is derived from the Persian word \"seng\" meaning stone. The metal was also called Indian tin, tutanego, calamine, and spinter.\n\nGerman metallurgist Andreas Libavius received a quantity of what he called \"calay\" of Malabar from a cargo ship captured from the Portuguese in 1596. Libavius described the properties of the sample, which may have been zinc. Zinc was regularly imported to Europe from the Orient in the 17th and early 18th centuries, but was at times very expensive.\n\nMetallic zinc was isolated in India by 1300 AD, much earlier than in the West. Before it was isolated in Europe, it was imported from India in about 1600 CE. Postlewayt's \"Universal Dictionary\", a contemporary source giving technological information in Europe, did not mention zinc before 1751 but the element was studied before then.\n\nFlemish metallurgist and alchemist P. M. de Respour reported that he had extracted metallic zinc from zinc oxide in 1668. By the start of the 18th century, Étienne François Geoffroy described how zinc oxide condenses as yellow crystals on bars of iron placed above zinc ore that is being smelted. In Britain, John Lane is said to have carried out experiments to smelt zinc, probably at Landore, prior to his bankruptcy in 1726.\n\nIn 1738 in Great Britain, William Champion patented a process to extract zinc from calamine in a vertical retort style smelter. His technique resembled that used at Zawar zinc mines in Rajasthan, but no evidence suggests he visited the Orient. Champion's process was used through 1851.\n\nGerman chemist Andreas Marggraf normally gets credit for discovering pure metallic zinc, even though Swedish chemist Anton von Swab had distilled zinc from calamine four years previously. In his 1746 experiment, Marggraf heated a mixture of calamine and charcoal in a closed vessel without copper to obtain a metal. This procedure became commercially practical by 1752.\n\nWilliam Champion's brother, John, patented a process in 1758 for calcining zinc sulfide into an oxide usable in the retort process. Prior to this, only calamine could be used to produce zinc. In 1798, Johann Christian Ruberg improved on the smelting process by building the first horizontal retort smelter. Jean-Jacques Daniel Dony built a different kind of horizontal zinc smelter in Belgium that processed even more zinc.\n\nItalian doctor Luigi Galvani discovered in 1780 that connecting the spinal cord of a freshly dissected frog to an iron rail attached by a brass hook caused the frog's leg to twitch. He incorrectly thought he had discovered an ability of nerves and muscles to create electricity and called the effect \"animal electricity\". The galvanic cell and the process of galvanization were both named for Luigi Galvani, and his discoveries paved the way for electrical batteries, galvanization, and cathodic protection.\n\nGalvani's friend, Alessandro Volta, continued researching the effect and invented the Voltaic pile in 1800. The basic unit of Volta's pile was a simplified galvanic cell, made of plates of copper and zinc separated by an electrolyte and connected by a conductor externally. The units were stacked in series to make the Voltaic cell, which produced electricity by directing electrons from the zinc to the copper and allowing the zinc to corrode.\n\nThe non-magnetic character of zinc and its lack of color in solution delayed discovery of its importance to biochemistry and nutrition. This changed in 1940 when carbonic anhydrase, an enzyme that scrubs carbon dioxide from blood, was shown to have zinc in its active site. The digestive enzyme carboxypeptidase became the second known zinc-containing enzyme in 1955.\n\nZinc is the fourth most common metal in use, trailing only iron, aluminium, and copper with an annual production of about 13 million tonnes. The world's largest zinc producer is Nyrstar, a merger of the Australian OZ Minerals and the Belgian Umicore. About 70% of the world's zinc originates from mining, while the remaining 30% comes from recycling secondary zinc. Commercially pure zinc is known as Special High Grade, often abbreviated \"SHG\", and is 99.995% pure.\n\nWorldwide, 95% of new zinc is mined from sulfidic ore deposits, in which sphalerite (ZnS) is nearly always mixed with the sulfides of copper, lead and iron. Zinc mines are scattered throughout the world, with the main areas being China, Australia, and Peru. China produced 38% of the global zinc output in 2014.\n\nZinc metal is produced using extractive metallurgy. The ore is finely ground, then put through froth flotation to separate minerals from gangue (on the property of hydrophobicity), to get a zinc sulfide ore concentrate consisting of about 50% zinc, 32% sulfur, 13% iron, and 5% .\n\nRoasting converts the zinc sulfide concentrate to zinc oxide:\n\nThe sulfur dioxide is used for the production of sulfuric acid, which is necessary for the leaching process. If deposits of zinc carbonate, zinc silicate, or zinc spinel (like the Skorpion Deposit in Namibia) are used for zinc production, the roasting can be omitted.\n\nFor further processing two basic methods are used: pyrometallurgy or electrowinning. Pyrometallurgy reduces zinc oxide with carbon or carbon monoxide at into the metal, which is distilled as zinc vapor to separate it from other metals, which are not volatile at those temperatures. The zinc vapor is collected in a condenser. The equations below describe this process:\n\nIn electrowinning, zinc is leached from the ore concentrate by sulfuric acid:\n\nFinally, the zinc is reduced by electrolysis.\n\nThe sulfuric acid is regenerated and recycled to the leaching step.\n\nWhen galvanised feedstock is fed to an electric arc furnace, the zinc is recovered from the dust by a number of processes, predominately the Waelz process (90% as of 2014).\n\nRefinement of sulfidic zinc ores produces large volumes of sulfur dioxide and cadmium vapor. Smelter slag and other residues contain significant quantities of metals. About 1.1 million tonnes of metallic zinc and 130 thousand tonnes of lead were mined and smelted in the Belgian towns of La Calamine and Plombières between 1806 and 1882. The dumps of the past mining operations leach zinc and cadmium, and the sediments of the Geul River contain non-trivial amounts of metals. About two thousand years ago, emissions of zinc from mining and smelting totaled 10 thousand tonnes a year. After increasing 10-fold from 1850, zinc emissions peaked at 3.4 million tonnes per year in the 1980s and declined to 2.7 million tonnes in the 1990s, although a 2005 study of the Arctic troposphere found that the concentrations there did not reflect the decline. Anthropogenic and natural emissions occur at a ratio of 20 to 1.\n\nZinc in rivers flowing through industrial and mining areas can be as high as 20 ppm. Effective sewage treatment greatly reduces this; treatment along the Rhine, for example, has decreased zinc levels to 50 ppb. Concentrations of zinc as low as 2 ppm adversely affects the amount of oxygen that fish can carry in their blood.\n\nSoils contaminated with zinc from mining, refining, or fertilizing with zinc-bearing sludge can contain several grams of zinc per kilogram of dry soil. Levels of zinc in excess of 500 ppm in soil interfere with the ability of plants to absorb other essential metals, such as iron and manganese. Zinc levels of 2000 ppm to 180,000 ppm (18%) have been recorded in some soil samples.\n\nMajor applications of zinc include (numbers are given for the US)\n\nZinc is most commonly used as an anti-corrosion agent, and galvanization (coating of iron or steel) is the most familiar form. In 2009 in the United States, 55% or 893,000 tons of the zinc metal was used for galvanization.\n\nZinc is more reactive than iron or steel and thus will attract almost all local oxidation until it completely corrodes away. A protective surface layer of oxide and carbonate ( forms as the zinc corrodes. This protection lasts even after the zinc layer is scratched but degrades through time as the zinc corrodes away. The zinc is applied electrochemically or as molten zinc by hot-dip galvanizing or spraying. Galvanization is used on chain-link fencing, guard rails, suspension bridges, lightposts, metal roofs, heat exchangers, and car bodies.\n\nThe relative reactivity of zinc and its ability to attract oxidation to itself makes it an efficient sacrificial anode in cathodic protection (CP). For example, cathodic protection of a buried pipeline can be achieved by connecting anodes made from zinc to the pipe. Zinc acts as the anode (negative terminus) by slowly corroding away as it passes electric current to the steel pipeline. Zinc is also used to cathodically protect metals that are exposed to sea water. A zinc disc attached to a ship's iron rudder will slowly corrode while the rudder stays intact. Similarly, a zinc plug attached to a propeller or the metal protective guard for the keel of the ship provides temporary protection.\n\nWith a standard electrode potential (SEP) of −0.76 volts, zinc is used as an anode material for batteries. (More reactive lithium (SEP −3.04 V) is used for anodes in lithium batteries ). Powdered zinc is used in this way in alkaline batteries and the case (which also serves as the anode) of zinc–carbon batteries is formed from sheet zinc. Zinc is used as the anode or fuel of the zinc-air battery/fuel cell. The zinc-cerium redox flow battery also relies on a zinc-based negative half-cell.\n\nA widely used zinc alloy is brass, in which copper is alloyed with anywhere from 3% to 45% zinc, depending upon the type of brass. Brass is generally more ductile and stronger than copper, and has superior corrosion resistance. These properties make it useful in communication equipment, hardware, musical instruments, and water valves.\nOther widely used zinc alloys include nickel silver, typewriter metal, soft and aluminium solder, and commercial bronze. Zinc is also used in contemporary pipe organs as a substitute for the traditional lead/tin alloy in pipes. Alloys of 85–88% zinc, 4–10% copper, and 2–8% aluminium find limited use in certain types of machine bearings. Zinc is the primary metal in American one cent coins (pennies) since 1982. The zinc core is coated with a thin layer of copper to give the appearance of a copper coin. In 1994, of zinc were used to produce 13.6 billion pennies in the United States.\n\nAlloys of zinc with small amounts of copper, aluminium, and magnesium are useful in die casting as well as spin casting, especially in the automotive, electrical, and hardware industries. These alloys are marketed under the name Zamak. An example of this is zinc aluminium. The low melting point together with the low viscosity of the alloy makes possible the production of small and intricate shapes. The low working temperature leads to rapid cooling of the cast products and fast production for assembly. Another alloy, marketed under the brand name Prestal, contains 78% zinc and 22% aluminium, and is reported to be nearly as strong as steel but as malleable as plastic. This superplasticity of the alloy allows it to be molded using die casts made of ceramics and cement.\n\nSimilar alloys with the addition of a small amount of lead can be cold-rolled into sheets. An alloy of 96% zinc and 4% aluminium is used to make stamping dies for low production run applications for which ferrous metal dies would be too expensive. For building facades, roofing, and other applications for sheet metal formed by deep drawing, roll forming, or bending, zinc alloys with titanium and copper are used. Unalloyed zinc is too brittle for these manufacturing processes.\n\nAs a dense, inexpensive, easily worked material, zinc is used as a lead replacement. In the wake of lead concerns, zinc appears in weights for various applications ranging from fishing to tire balances and flywheels.\n\nCadmium zinc telluride (CZT) is a semiconductive alloy that can be divided into an array of small sensing devices. These devices are similar to an integrated circuit and can detect the energy of incoming gamma ray photons. When behind an absorbing mask, the CZT sensor array can determine the direction of the rays.\n\nRoughly one quarter of all zinc output in the United States in 2009 was consumed in zinc compounds; a variety of which are used industrially. Zinc oxide is widely used as a white pigment in paints and as a catalyst in the manufacture of rubber to disburse heat. Zinc oxide is used to protect rubber polymers and plastics from ultraviolet radiation (UV). The semiconductor properties of zinc oxide make it useful in varistors and photocopying products. The zinc zinc-oxide cycle is a two step thermochemical process based on zinc and zinc oxide for hydrogen production.\n\nZinc chloride is often added to lumber as a fire retardant and sometimes as a wood preservative. It is used in the manufacture of other chemicals. Zinc methyl () is used in a number of organic syntheses. Zinc sulfide (ZnS) is used in luminescent pigments such as on the hands of clocks, X-ray and television screens, and luminous paints. Crystals of ZnS are used in lasers that operate in the mid-infrared part of the spectrum. Zinc sulfate is a chemical in dyes and pigments. Zinc pyrithione is used in antifouling paints.\n\nZinc powder is sometimes used as a propellant in model rockets. When a compressed mixture of 70% zinc and 30% sulfur powder is ignited there is a violent chemical reaction. This produces zinc sulfide, together with large amounts of hot gas, heat, and light.\n\nZinc sheet metal is used to make zinc bars.\n\n, the most abundant isotope of zinc, is very susceptible to neutron activation, being transmuted into the highly radioactive , which has a half-life of 244 days and produces intense gamma radiation. Because of this, zinc oxide used in nuclear reactors as an anti-corrosion agent is depleted of before use, this is called depleted zinc oxide. For the same reason, zinc has been proposed as a salting material for nuclear weapons (cobalt is another, better-known salting material). A jacket of isotopically enriched would be irradiated by the intense high-energy neutron flux from an exploding thermonuclear weapon, forming a large amount of significantly increasing the radioactivity of the weapon's fallout. Such a weapon is not known to have ever been built, tested, or used.\n\nZinc dithiocarbamate complexes are used as agricultural fungicides; these include Zineb, Metiram, Propineb and Ziram. Zinc naphthenate is used as wood preservative. Zinc in the form of ZDDP, is used as an anti-wear additive for metal parts in engine oil.\n\nOrganozinc chemistry is the science of compounds that contain carbon-zinc bonds, describing the physical properties, synthesis, and chemical reactions.Many organozinc compounds are important. Among important applications are\n\nZinc has found many applications as catalyst in organic synthesis including asymmetric synthesis, being cheap and easily available alternative to precious metal complexes. The results (yield and ee) obtained with chiral zinc catalysts are comparable to those achieved with palladium, ruthenium, iridium and others, and zinc becomes metal catalyst of choice.\n\nIn most single-tablet, over-the-counter, daily vitamin and mineral supplements, zinc is included in such forms as zinc oxide, zinc acetate, or zinc gluconate. Zinc is generally considered to be an antioxidant. However, it is redox inert and thus can serve such a function only indirectly. Generally, zinc supplement is recommended where there is high risk of zinc deficiency (such as low and middle income countries) as a preventive measure.\n\nZinc deficiency has been associated with major depressive disorder (MDD), and zinc supplements may be an effective treatment.\n\nZinc serves as a simple, inexpensive, and critical tool for treating diarrheal episodes among children in the developing world. Zinc becomes depleted in the body during diarrhea, but recent studies suggest that replenishing zinc with a 10- to 14-day course of treatment can reduce the duration and severity of diarrheal episodes and may also prevent future episodes for as long as three months.\n\nA Cochrane review stated that people taking zinc supplement may be less likely to progress to age-related macular degeneration.\n\nZinc supplement is an effective treatment for acrodermatitis enteropathica, a genetic disorder affecting zinc absorption that was previously fatal to affected infants.\n\nGastroenteritis is strongly attenuated by ingestion of zinc, possibly by direct antimicrobial action of the ions in the gastrointestinal tract, or by the absorption of the zinc and re-release from immune cells (all granulocytes secrete zinc), or both.\n\nIn 2011, researchers reported that adding large amounts of zinc to a urine sample masked detection of drugs. The researchers did not test whether orally consuming a zinc dietary supplement could have the same effect.\n\nZinc is a negative allosteric modulator of the GABA receptor.\n\nTopical preparations of zinc include those used on the skin, often in the form of zinc oxide. Zinc preparations can protect against sunburn in the summer and windburn in the winter. Applied thinly to a baby's diaper area (perineum) with each diaper change, it can protect against diaper rash.\n\nChelated zinc is used in toothpastes and mouthwashes to prevent bad breath.\n\nZinc pyrithione is widely included in shampoos to prevent dandruff.\n\nZinc is an essential trace element for humans and other animals, for plants and for microorganisms. Zinc is required for the function of over 300 enzymes and 1000 transcription factors, and is stored and transferred in metallothioneins. It is the second most abundant trace metal in humans after iron and it is the only metal which appears in all enzyme classes.\n\nIn proteins, zinc ions are often coordinated to the amino acid side chains of aspartic acid, glutamic acid, cysteine and histidine. The theoretical and computational description of this zinc binding in proteins (as well as that of other transition metals) is difficult.\n\nRoughly  grams of zinc are distributed throughout the human body. Most zinc is in the brain, muscle, bones, kidney, and liver, with the highest concentrations in the prostate and parts of the eye. Semen is particularly rich in zinc, a key factor in prostate gland function and reproductive organ growth.\n\nIn humans, the biological roles of zinc are ubiquitous. It interacts with \"a wide range of organic ligands\", and has roles in the metabolism of RNA and DNA, signal transduction, and gene expression. It also regulates apoptosis. A 2006 study estimated that about 10% of human proteins (2800) potentially bind zinc, in addition to hundreds more that transport and traffic zinc; a similar \"in silico\" study in the plant \"Arabidopsis thaliana\" found 2367 zinc-related proteins.\n\nIn the brain, zinc is stored in specific synaptic vesicles by glutamatergic neurons and can modulate neuronal excitability. It plays a key role in synaptic plasticity and so in learning. Zinc homeostasis also plays a critical role in the functional regulation of the central nervous system. Dysregulation of zinc homeostasis in the central nervous system that results in excessive synaptic zinc concentrations is believed to induce neurotoxicity through mitochondrial oxidative stress (e.g., by disrupting certain enzymes involved in the electron transport chain, including complex I, complex III, and α-ketoglutarate dehydrogenase), the dysregulation of calcium homeostasis, glutamatergic neuronal excitotoxicity, and interference with intraneuronal signal transduction. L- and D-histidine facilitate brain zinc uptake. SLC30A3 is the primary zinc transporter involved in cerebral zinc homeostasis.\n\nZinc is an efficient Lewis acid, making it a useful catalytic agent in hydroxylation and other enzymatic reactions. The metal also has a flexible coordination geometry, which allows proteins using it to rapidly shift conformations to perform biological reactions. Two examples of zinc-containing enzymes are carbonic anhydrase and carboxypeptidase, which are vital to the processes of carbon dioxide () regulation and digestion of proteins, respectively.\n\nIn vertebrate blood, carbonic anhydrase converts into bicarbonate and the same enzyme transforms the bicarbonate back into for exhalation through the lungs. Without this enzyme, this conversion would occur about one million times slower at the normal blood pH of 7 or would require a pH of 10 or more. The non-related β-carbonic anhydrase is required in plants for leaf formation, the synthesis of indole acetic acid (auxin) and alcoholic fermentation.\n\nCarboxypeptidase cleaves peptide linkages during digestion of proteins. A coordinate covalent bond is formed between the terminal peptide and a C=O group attached to zinc, which gives the carbon a positive charge. This helps to create a hydrophobic pocket on the enzyme near the zinc, which attracts the non-polar part of the protein being digested.\n\nZinc has been recognized as a messenger, able to activate signalling pathways. Many of these pathways provide the driving force in aberrant cancer growth. They can be targeted through ZIP transporters.\n\nZinc serves a purely structural role in zinc fingers, twists and clusters. Zinc fingers form parts of some transcription factors, which are proteins that recognize DNA base sequences during the replication and transcription of DNA. Each of the nine or ten ions in a zinc finger helps maintain the finger's structure by coordinately binding to four amino acids in the transcription factor. The transcription factor wraps around the DNA helix and uses its fingers to accurately bind to the DNA sequence.\n\nIn blood plasma, zinc is bound to and transported by albumin (60%, low-affinity) and transferrin (10%). Because transferrin also transports iron, excessive iron reduces zinc absorption, and vice versa. A similar antagonism exists with copper. The concentration of zinc in blood plasma stays relatively constant regardless of zinc intake. Cells in the salivary gland, prostate, immune system, and intestine use zinc signaling to communicate with other cells.\n\nZinc may be held in metallothionein reserves within microorganisms or in the intestines or liver of animals. Metallothionein in intestinal cells is capable of adjusting absorption of zinc by 15–40%. However, inadequate or excessive zinc intake can be harmful; excess zinc particularly impairs copper absorption because metallothionein absorbs both metals.\n\nThe human dopamine transporter contains a high affinity extracellular zinc binding site which, upon zinc binding, inhibits dopamine reuptake and amplifies amphetamine-induced dopamine efflux \"in vitro\". The human serotonin transporter and norepinephrine transporter do not contain zinc binding sites.\n\nThe U.S. Institute of Medicine (IOM) updated Estimated Average Requirements (EARs) and Recommended Dietary Allowances (RDAs) for zinc in 2001. The current EARs for zinc for women and men ages 14 and up is 6.8 and 9.4 mg/day, respectively. The RDAs are 8 and 11 mg/day. RDAs are higher than EARs so as to identify amounts that will cover people with higher than average requirements. RDA for pregnancy is 11 mg/day. RDA for lactation is 12 mg/day. For infants up to 12 months the RDA is 3 mg/day. For children ages 1–13 years the RDA increases with age from 3 to 8 mg/day. As for safety, the IOM sets Tolerable upper intake levels (ULs) for vitamins and minerals when evidence is sufficient. In the case of zinc the adult UL is 40 mg/day (lower for children). Collectively the EARs, RDAs, AIs and ULs are referred to as Dietary Reference Intakes (DRIs).\n\nThe European Food Safety Authority (EFSA) refers to the collective set of information as Dietary Reference Values, with Population Reference Intake (PRI) instead of RDA, and Average Requirement instead of EAR. AI and UL are defined the same as in United States. For people ages 18 and older the PRI calculations are complex, as the EFSA has set higher and higher values as the phytate content of the diet increases. For women, PRIs increase from 7.5 to 12.7 mg/day as phytate intake increases from 300 to 1200 mg/day; for men the range is 9.4 to 16.3 mg/day. These PRIs are higher than the U.S. RDAs. The EFSA reviewed the same safety question and set its UL at 25 mg/day, which is much lower than the U.S. value.\n\nFor U.S. food and dietary supplement labeling purposes the amount in a serving is expressed as a percent of Daily Value (%DV). For zinc labeling purposes 100% of the Daily Value was 15 mg, but as of May 27, 2016 it has been revised to 11 mg. A table of the old and new adult Daily Values is provided at Reference Daily Intake. Food and supplement companies have until January 1, 2020 to comply with the change.\n\nAnimal products such as meat, fish, shellfish, fowl, eggs, and dairy contain zinc. The concentration of zinc in plants varies with the level in the soil. With adequate zinc in the soil, the food plants that contain the most zinc are wheat (germ and bran) and various seeds, including sesame, poppy, alfalfa, celery, and mustard. Zinc is also found in beans, nuts, almonds, whole grains, pumpkin seeds, sunflower seeds, and blackcurrant. Plant phytates are particularly found in pulses and cereals and interfere with zinc absorption.\n\nOther sources include fortified food and dietary supplements in various forms. A 1998 review concluded that zinc oxide, one of the most common supplements in the United States, and zinc carbonate are nearly insoluble and poorly absorbed in the body. This review cited studies that found lower plasma zinc concentrations in the subjects who consumed zinc oxide and zinc carbonate than in those who took zinc acetate and sulfate salts. For fortification, however, a 2003 review recommended cereals (containing zinc oxide) as a cheap, stable source that is as easily absorbed as the more expensive forms. A 2005 study found that various compounds of zinc, including oxide and sulfate, did not show statistically significant differences in absorption when added as fortificants to maize tortillas.\n\nZinc deficiency is usually due to insufficient dietary intake, but can be associated with malabsorption, acrodermatitis enteropathica, chronic liver disease, chronic renal disease, sickle cell disease, diabetes, malignancy, and other chronic illnesses. Groups at risk of zinc deficiency include the elderly, children in developing countries, and those with renal dysfunction.\n\nIn the United States, a federal survey of food consumption determined that for women and men over the age of 19, average consumption was 9.7 and 14.2 mg/day, respectively. For women, 17% consumed less than the EAR, for men 11%. The percentages below EAR increased with age. The most recent published update of the survey (NHANES 2013–2014) reported lower averages – 9.3 and 13.2 mg/day – again with intake decreasing with age.\n\nSymptoms of mild zinc deficiency are diverse. Clinical outcomes include depressed growth, diarrhea, impotence and delayed sexual maturation, alopecia, eye and skin lesions, impaired appetite, altered cognition, impaired host defense properties, defects in carbohydrate utilization, and reproductive teratogenesis. Mild zinc deficiency depresses immunity, although excessive zinc does also. Animals with a zinc deficiency require twice as much food to attain the same weight gain as animals with sufficient zinc.\n\nDespite some concerns, western vegetarians and vegans do not suffer any more from overt zinc deficiency than meat-eaters. Major plant sources of zinc include cooked dried beans, sea vegetables, fortified cereals, soy foods, nuts, peas, and seeds. However, phytates in many whole-grains and fibers may interfere with zinc absorption and marginal zinc intake has poorly understood effects. The zinc chelator phytate, found in seeds and cereal bran, can contribute to zinc malabsorption. Some evidence suggests that more than the US RDA (15 mg) of zinc daily may be needed in those whose diet is high in phytates, such as some vegetarians. These considerations must be balanced against the paucity of adequate zinc biomarkers, and the most widely used indicator, plasma zinc, has poor sensitivity and specificity. Diagnosing zinc deficiency is a persistent challenge.\n\nNearly two billion people in the developing world are deficient in zinc. In children, it causes an increase in infection and diarrhea and contributes to the death of about 800,000 children worldwide per year. The World Health Organization advocates zinc supplementation for severe malnutrition and diarrhea. Zinc supplements help prevent disease and reduce mortality, especially among children with low birth weight or stunted growth. However, zinc supplements should not be administered alone, because many in the developing world have several deficiencies, and zinc interacts with other micronutrients.\n\nSpecies of \"Calluna\", \"Erica\" and \"Vaccinium\" can grow in zinc metalliferous soils, because translocation of toxic ions is prevented by the action of ericoid mycorrhizal fungi.\n\nZinc deficiency appears to be the most common micronutrient deficiency in crop plants; it is particularly common in high-pH soils. Zinc-deficient soil is cultivated in the cropland of about half of Turkey and India, a third of China, and most of Western Australia. Substantial responses to zinc fertilization have been reported in these areas. Plants that grow in soils that are zinc-deficient are more susceptible to disease. Zinc is added to the soil primarily through the weathering of rocks, but humans have added zinc through fossil fuel combustion, mine waste, phosphate fertilizers, pesticide (zinc phosphide), limestone, manure, sewage sludge, and particles from galvanized surfaces. Excess zinc is toxic to plants, although zinc toxicity is far less widespread.\n\nAlthough zinc is an essential requirement for good health, excess zinc can be harmful. Excessive absorption of zinc suppresses copper and iron absorption. The free zinc ion in solution is highly toxic to plants, invertebrates, and even vertebrate fish. The Free Ion Activity Model is well-established in the literature, and shows that just micromolar amounts of the free ion kills some organisms. A recent example showed 6 micromolar killing 93% of all \"Daphnia\" in water.\n\nThe free zinc ion is a powerful Lewis acid up to the point of being corrosive. Stomach acid contains hydrochloric acid, in which metallic zinc dissolves readily to give corrosive zinc chloride. Swallowing a post-1982 American one cent piece (97.5% zinc) can cause damage to the stomach lining through the high solubility of the zinc ion in the acidic stomach.\n\nEvidence shows that people taking 100–300 mg of zinc daily may suffer induced copper deficiency. A 2007 trial observed that elderly men taking 80 mg daily were hospitalized for urinary complications more often than those taking a placebo. Levels of 100–300 mg may interfere with the utilization of copper and iron or adversely affect cholesterol. Zinc in excess of 500 ppm in soil interferes with the plant absorption of other essential metals, such as iron and manganese. A condition called the zinc shakes or \"zinc chills\" can be induced by inhalation of zinc fumes while brazing or welding galvanized materials. Zinc is a common ingredient of denture cream which may contain between 17 and 38 mg of zinc per gram. Disability and even deaths from excessive use of these products have been claimed.\n\nThe U.S. Food and Drug Administration (FDA) states that zinc damages nerve receptors in the nose, causing anosmia. Reports of anosmia were also observed in the 1930s when zinc preparations were used in a failed attempt to prevent polio infections. On June 16, 2009, the FDA ordered removal of zinc-based intranasal cold products from store shelves. The FDA said the loss of smell can be life-threatening because people with impaired smell cannot detect leaking gas or smoke, and cannot tell if food has spoiled before they eat it.\n\nRecent research suggests that the topical antimicrobial zinc pyrithione is a potent heat shock response inducer that may impair genomic integrity with induction of PARP-dependent energy crisis in cultured human keratinocytes and melanocytes.\n\nIn 1982, the US Mint began minting pennies coated in copper but containing primarily zinc. Zinc pennies pose a risk of zinc toxicosis, which can be fatal. One reported case of chronic ingestion of 425 pennies (over 1 kg of zinc) resulted in death due to gastrointestinal bacterial and fungal sepsis. Another patient who ingested 12 grams of zinc showed only lethargy and ataxia (gross lack of coordination of muscle movements). Several other cases have been reported of humans suffering zinc intoxication by the ingestion of zinc coins.\n\nPennies and other small coins are sometimes ingested by dogs, requiring veterinary removal of the foreign objects. The zinc content of some coins can cause zinc toxicity, commonly fatal in dogs through severe hemolytic anemia and liver or kidney damage; vomiting and diarrhea are possible symptoms. Zinc is highly toxic in parrots and poisoning can often be fatal. The consumption of fruit juices stored in galvanized cans has resulted in mass parrot poisonings with zinc.\n\n\n\n"}
