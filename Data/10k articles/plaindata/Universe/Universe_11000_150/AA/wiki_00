{"id": "18673702", "url": "https://en.wikipedia.org/wiki?curid=18673702", "title": "Alaska Wilderness League", "text": "Alaska Wilderness League\n\nThe Alaska Wilderness League (AWL) is a nonprofit organization that works to protect Alaska’s most significant wild lands from oil and gas drilling and from other industrial threats. Founded in 1993, AWL has its main office in Washington, DC, with additional offices in Anchorage and Juneau, Alaska.\n\nFor the past ten years, AWL has taken the lead in the fight to protect the Arctic National Wildlife Refuge from being opened to oil and gas development, and its leadership has made a significant impact in this area. The organization has rallied public support and successfully stopped numerous attempts by Congress to open the refuge to development.\n\nIn 2001, AWL and its supporters helped fund photographer Subhankar Banerjee’s ground-breaking winter field photography in the Arctic National Wildlife Refuge. The photos he took were published in the book \"Seasons of Life and Land\". Banerjee’s photographs of the Refuge were exhibited at the Smithsonian Institution’s National Museum of Natural History, and controversy erupted when his captions for the photos were altered and the exhibit was moved to a far corner of the museum. Some charged that Alaska Senator Ted Stevens had used political pressure to remove the exhibit from the spotlight because Senator Barbara Boxer had held up Banerjee’s book during a Senate floor debate over oil drilling in the Refuge, but the museum maintained the changes were made “for artistic reasons”. \n\nStarting in 2004, AWL expanded its work to include ecologically significant areas of Alaska’s vast National Petroleum Reserve, the Tongass National Forest, and the outer continental shelf areas of the Beaufort and Chukchi Seas.\n\nAWL's work has had long-standing support from President Jimmy Carter, who remains the Honorary Chairman of the Board of Directors.\n\nAndrew Revkin: Who’s Backing Gingrich’s ‘Drill Here’ Push? (NY Times)\n\n"}
{"id": "3140268", "url": "https://en.wikipedia.org/wiki?curid=3140268", "title": "Bluing (fabric)", "text": "Bluing (fabric)\n\nBluing, laundry blue, dolly blue or washing blue is a household product used to improve the appearance of textiles, especially white fabrics. Used during laundering, it adds a trace of blue dye (often synthetic ultramarine, sometimes Prussian blue) to the fabric.\n\nWhite fabrics acquire a slight color cast after use (usually grey or yellow). Since blue and yellow are complementary colors in the subtractive color model of color perception, adding a trace of blue color to the slightly off-white color of these fabrics makes them appear whiter. Laundry detergents may also use fluorescing agents to similar effect. Many white fabrics are blued during manufacturing. Bluing is not permanent and rinses out over time leaving dingy or yellowed whites. A commercial bluing product allows the consumer to add the bluing back into the fabric to restore whiteness.\n\nOn the same principle, bluing is sometimes used by white-haired people in a blue rinse.\n\nBluing has other miscellaneous household uses, including as an ingredient in rock crystal \"gardens\" (whereby a porous item is placed in a salt solution, the solution then precipitating out as crystals), and to improve the appearance of swimming-pool water. In Australia it was used as a folk remedy to relieve the itching of mosquito and sand fly bites.\n\nLaundry bluing is made of a colloid of ferric ferrocyanide (blue iron salt, also referred to as \"Prussian blue\") in water. \n\nBlue colorings have been added to rinse water for centuries, first in the form of powder blue or smalt, or using small lumps of indigo and starch, called stone blue. After the invention of synthetic ultramarine and Prussian blue it was manufactured by many companies, including Mrs. Stewart's Bluing in the US, Reckitt's Crown Blue in Hull and the Lancashire Ultramarine Company's Dolly Blue at Backbarrow (later purchased by Reckitt & Sons). It was popular until the mid-twentieth century in the UK and US, and is still widely used in India and Pakistan. In many places, it has been replaced by bleach for its primary purpose.\n\nBluing is usually sold in liquid form, but it may also be a solid. Solid bluing is sometimes used by hoodoo doctors to provide the blue color needed for \"mojo hands\" without having to use the toxic compound copper(II) sulfate. Bluing was also used by some Native American tribes to mark their arrows showing tribe ownership.\n\n\n"}
{"id": "230539", "url": "https://en.wikipedia.org/wiki?curid=230539", "title": "Camel train", "text": "Camel train\n\nA camel train or caravan is a series of camels carrying passengers and/or goods on a regular or semi-regular service between points. Although they rarely travelled faster than the walking speed of a person, camels' ability to withstand harsh conditions made them ideal for communication and trade in the desert areas of North Africa and the Arabian Peninsula for centuries. Camel trains were also used sparingly elsewhere around the globe. Since the early 20th century they have been largely replaced by motorized vehicles or air traffic.\n\nBy far the greatest use of camel trains occurs between North and West Africa by the Tuareg, Shuwa and Hassaniyya, as well as by culturally-affiliated groups like the Toubou, Hausa and Songhay. These camel trains conduct trade in and around the Sahara Desert and Sahel. Trains travel as far south as central Nigeria and northern Cameroon in the west, and northern Kenya in the east of the continent. In antiquity, the Arabian Peninsula was an important route for the trade with India and Abyssinia. \n\nCamel trains have also long been used in portions of trans-Asian trade, including the Silk Road. As late as the early twentieth century, camel caravans played an important role connecting the Beijing/Shanxi region of eastern China with Mongolian centers (Urga, Uliastai, Kobdo) and Xinjiang. The routes went across Inner and/or Outer Mongolia. According to Owen Lattimore, who spent five months in 1926 crossing the northern edge of China (from Hohhot to Gucheng, via Inner Mongolia) with a camel caravan, demand for caravan trade was only increased by the arrival of foreign steamships into Chinese ports and the construction of the first railways in eastern China, as they improved access to the world market for such products of western China as wool.\n\nIn the English-speaking world the term \"camel train\" often applies to Australia, notably the service that once connected a railhead at Oodnadatta in South Australia to Alice Springs in the center of the continent. The service ended when the train line was extended to Alice Springs in 1929; that train is called \"the Ghan\", a shortened version of \"Afghan Express\", and its logo is camel and rider, in honor of the \"Afghan cameleers\" who pioneered the route.\n\n\nThe history of camel trains in the United States consists mainly of an experiment by the United States Army. On April 29, 1856, thirty-three camels and five drivers arrived at Indianola, Texas. While camels were suited to the job of transport in the American Southwest, the experiment failed. Their stubbornness and aggressiveness made them unpopular among soldiers, and they frightened horses. Many of the camels were sold to private owners, others escaped into the desert. These feral camels continued to be sighted through the early 20th century, with the last reported sighting in 1941 near Douglas, Texas.\n\n\nCamels were used from 1862 to 1863 in British Columbia, Canada during the Cariboo Gold Rush.\n\nWhile organization of camel caravans varied over time and the territory traversed, Owen Lattimore's account of caravan life in northern China in the 1920s gives a good idea of what camel transport is like. In his \"Desert Road to Turkestan\" he describes mostly camel caravans run by Han Chinese and Hui firms from eastern China (Hohhot, Baotou) or Xinjiang (Qitai (then called Gucheng), Barkol), plying the routes connecting those two regions through the Gobi Desert by way of Inner (or, before Mongolia's independence, Outer) Mongolia. Before Outer Mongolia's effective independence of China (circa 1920) the same firms also ran caravans into Urga, Uliassutai, and other centers of Outer Mongolia, and to the Russian border at Kyakhta, but with the creation of an international border, those routes came into decline. Less important caravan routes served various other areas of northern China, such as most centers in today's Gansu, Ningxia, and northern Qinghai. Some of the oldest Hohhot-based caravan firms had a history dating to the early Qing Dynasty.\n\n Caravans originating from both ends of the Hohhot-Gucheng route were composed of two-humped Bactrian camels, suitable for the climate on the area, although very occasionally one could see single-humped Dromedaries brought to this route by Uighur (\"Turki\", in Lattimore's parlance) caravan people from Hami A caravan would be normally composed of a number of files (, \"lian\"), of up to 18 camels each. Each of the rank-and-file caravan men, known as the \"camel-pullers\" (, \"la luotuo-de\"), was in charge of one such file. On the march, the camel-puller's job was to lead the first camel of his file by a rope tied to a peg attached to its nose, each of the other camels of the file being led by means of similar rope by the camel in front of it. Two files (\"lian\") formed a \"ba\", and the camel-pullers of the two files would help each other when loading cargo on the camels at the beginning of each day's march or unloading it when halted. To do their job properly camel-pullers had to be experts on camels: as Lattimore comments, \"because there is no good doctoring known for him [a camel] when he is sick, they must learn how to keep him well.\" Taking care of camels' health included the ability to find the best available grazing for them and keeping them away from poisonous plants; knowledge of when one should not allow a camel to drink too much water; how to park camels for the night, allowing them to obtain the best possible shelter from wind-blown snow in winter; how to properly distribute the load to prevent it from hurting the animal; and how to treat minor injuries of the camels, such as blisters or pack-sores.\n\nThe loading of camels was described by Mildred Cable and Francesca French in their book \"Through Jade Gate and Central Asia\" (1927): «In the loading of a camel its grumblings commence as the first bale is placed on its back, and continue uninterruptedly until the load is equal to its strength, but as soon as it shows signs of being in excess, the grumbling ceases suddenly, and then the driver says: \"Enough! put no more on this beast!\"»\n\nA caravan could consist of 150 or so camels (8 or more files), with a camel-puller for each file. Besides the camel-pullers the caravan would also include a \"xiansheng\" (先生, literally, \"Sir\", \"Mister\") (typically, an older man with a long experience as a camel-puller, now playing the role of a general manager), one or two cooks, and the caravan master, whose authority over the caravan and its people was as absolute as that of a captain on a ship. If the owner of the caravan did not travel with the caravan himself, he would send along a supercargo--the person who will take care of the disposal of the freight upon arrival, but had no authority during the journey. The caravan could carry a number of paying passengers as well, who would alternate between riding on top of a camel load and walking.\n\nCamel-pullers' salary was quite low (around 2 silver taels a month in 1926, which would not be enough even for shoes and clothing he wore out while walking with his camels), although they were also fed and provided with tent space at the caravan owner's expense. Those people worked not so much for the wages as for the benefit of carrying some cargo—half of a camel load, or a full load—of their own on the caravan's camels; when successfully sold at the destination, it would bring a handy profit. Even more importantly, if a camel-puller could afford to buy a camel or a few of his own, he was allowed to include them into his file, and to collect the carriage-money for the cargo (assigned by the caravan owner) that they would carry. Once the camel-puller got rich enough to own close to a full file of 18 camels, he could join the caravan not as an employee but as a kind of a partner—now instead of earning wages he would be paying money (around 20 taels per round-trip in 1926) to the owner of (the rest of) the caravan for the benefit of joining the caravan, sharing in the food, etc.\n\nThe caravan people's food was mostly based around oat and millet flour, with some animal fat. A sheep would be bought from the Mongols and slaughtered every now and then, and tea was the usual daily drink; as fresh vegetables were scarce, scurvy was a danger. Besides the paid cargo and the food and gear for the men, the camels would also carry a fair amount of fodder for themselves (typically, dried peas when going west, and barley when going east, those being the cheapest types of camel feed in Hohhot and Gucheng, respectively). It was estimated that, when leaving its point of origin, for every 100 loads of merchandise the caravan would carry around 30 loads of fodder. When that was not enough (especially in winter) more fodder could be bought (very expensively) from dealers who would come to the caravan route's popular stopping places from the populated areas of Gansu or Ningxia to the south.\n\nTypical cargo carried by the caravans were commodities such as wool, cotton fabrics, or tea, as well as miscellaneous manufactured goods for sale in Xinjiang and Mongolia. Opium was carried as well, typically by smaller, surreptitious, caravans, usually in winter (since in the hot weather opium would be too easily detected by the smell). More exotic loads could include jade from Khotan, elk antlers prized in Chinese medicine, or even dead bodies of the Shanxi caravan men and traders, who happened to die while in Xinjiang. In the latter case, the bodies had been first \"temporarily\" buried in Gucheng in light-weight coffins, and when, after three or so years in the grave the flesh had been mostly \"consumed away\", the merchant guild sent the bodies to the east by a special caravan. Due to the special nature of the load, higher freight rate was charged for such \"dead passengers\".\n\nAccording to Lattimore's diary, caravan travel in Inner Mongolia did not always follow a regular schedule. Caravans traveled or camped at any time of day or night, depending on weather, local conditions, and the need for rest. Since the caravan traveled at the walking speed of the men, the distance made in a day (a \"stage\") was usually between , depending on road and weather conditions, and distances between water sources. On occasions several days were spent in a camp without going forward, due to bad weather. A one-way trip from Hohhot to Gucheng ( by Lattimore's reckoning) could take anything from three to eight months.\n\nSmaller caravans owned by Mongols of the Alashan (the westernmost Inner Mongolia) and manned by Han Chinese from Zhenfan, were able to make longer marches (and, thus, cover longer distances faster) than the typical Han Chinese or Hui caravans, because the Mongols were able to always use \"fresh\" camels (picked from their large herd for just a single journey), every man was provided with a camel to ride, and loads were much lighter than in the \"standard\" caravans (rarely exceeding . These caravans would typically travel by day, from sunrise to sunset. Such a camel train is described in the accounts of the journey made by Peter Fleming and Ella Maillart in the Gobi Desert in the mid-1930s.\n\nIt was necessary for camels to spend at least two months between long journeys to recuperate, and the best time for that recuperation was in June–July, when camels shed their hair and the grazing is best. Therefore, the best practice was for a caravan to leave Hohhot in August, just after the grazing season; upon reaching Gucheng, weaker camels could stay there until the next summer by grazing whatever vegetation is available in winter, while the stronger ones, after a few weeks of recovery on a grain diet (grain being cheaper in Xinjiang than in eastern China), would be sent back in late winter/early spring, taking along plenty of grain for fodder, and returning to Hohhot before the next grazing season. Vice versa, one could leave Hohhot in the spring, spend the summer grazing season in Xinjiang, and come back in the late fall of the same year. Either way, it would be possible for the caravan people and their best camels to make a full round trip within a year. However, such perfect scheduling was not always possible, and it was often the case that a caravan sent out from Hohhot in August would end up staying on the other end of the route until and through the next grazing season, coming back to Hohhot about a year and a half after its departure.\nOn almost every journey quite a few camels in each caravan would be lost. On a particularly exhausting section of the trip, an animal already worn out by many weeks of walking, or accidentally poisoned by eating a poisonous plant, would kneel down and not rise anymore. Since killing a camel was considered bad karma by the caravan people, the hopeless animal—whose death, if it was owned by an individual camel-puller, would be a huge material loss for its owner—was simply left behind to die, \"thrown on the Gobi\" as the camel men would say.\n\nSince camels moult in the summer, camel owners received additional income from collecting several pounds of hair their animals dropped during the summer grazing (and shedding season); in northern China, the camel hair trade started around the 1880s. Later, caravan men learned the art of knitting and crocheting from the defeated White Russians (in exile in Xinjiang after the Russian Civil War) and the items they had made were transported to eastern China by camel caravan. Although the hair shed by the camels or picked from them was of course considered the property of the camel owners, caravan workers were entitled to make use of some hair for making knitwear for themselves (mostly socks) or for sale. Lattimore in 1926 observed camel-pullers \"knitting on the march; if they ran out of yarn, they would reach back to the first camel of the file they were leading, pluck a handful of hair from the neck, and roll it in their palms into the beginning of a length of yarn; a weight was attached to this, and given a twist to start it spinning, and the man went on feeding wool into the thread until he had spun enough yarn to continue his knitting\".\n\n\"In The Desert\" (\"Верблюды\", lit. = \"camels\") is a \"traditional Russian\" song, performed by Donald Swann. He provides an English-language translation after every line. The song is extremely repetitive (\"Another camel is approaching\"), rendering the translation largely redundant, \"a whole caravan of camels is approaching\".\n\nFritz Mühlenweg wrote a book called \"In geheimer Mission durch die Wüste Gobi\" (part one in English \"Big Tiger and Compass Mountain\"), published in 1950. It was later shortened and translated into English under the title \"Big Tiger and Christian\"; it concerns the adventures of two boys who cross the Gobi Desert.\n\n\n"}
{"id": "36839983", "url": "https://en.wikipedia.org/wiki?curid=36839983", "title": "Canopus Foundation", "text": "Canopus Foundation\n\nThe Canopus Foundation is a registered private charitable institution under German jurisdiction founded in 1997 by Wolfgang Heller and Dr. Peter W. Heller.\n\nExecutive Director of the foundation is Dr. Peter W. Heller. Members of the Board of Trustees are Micaela Heller, Julia T. Heller and Dieter Heller.\n\nThe foundation employs 2 permanent staff and 4 freelancers (status: 2014). The yearly operating expenses are approximately 300.000 Euro.\n\nThe foundation is legally registered in Dreieich, Germany. The office is located in Freiburg im Breisgau, Germany.\n\nThe initial focus after the establishment of the Canopus Foundation in October 1997 was set on the development of a specific entrepreneurial profile for the organization. Between 2000 and 2008 Canopus gained valuable practical experience while working with social entrepreneurs in less developed and emerging countries. \nDuring this time, the foundation supported among others the project \"The Sun Shines for All\" of IDEAAS (Brazil) providing low-income families in Paranà and Rio Grande do Sul with solar power-generated electricity; assisted SELCO Ltd. in Bangalore, India, in certifying their solar systems as “Certified Emission Reduction” (CER) projects; advised the Barefoot College in Tilonia, India, on the establishment of a manual production line of solar lanterns, and cooperated with Grameen Bank, Grameen Shakti and Grameen Cybernet in providing three village schools in Bangladesh with thin-film solar modules for the local power generation.\n\nIn 2008, in cooperation with Ashoka, the Canopus Foundation launched the Solar for All Initiative. The objective of the global initiative is to make solar energy affordable to 1.3 billion people without access to electricity. \nIn 2010, the Indian company Greenlight Planet was awarded with the Solar for All Innovation Prize at Intersolar North America in San Francisco.\nIn 2010 the foundation initiated the development of an energy access Social Impact Investment Fund, the conceptual work was partly funded by the European Investment Bank. In 2011 the Canopus Foundation handed the concept over to Bamboo Finance, Geneva, an investment management company specialised in micro credit and other social impact funds.\nIn 2014 the foundation started the project \"re.Mini\" in close collaboration with iiDevelopment, Frankfurt / Germany, to foster the implementation of renewable energy powered village grids in South Asia and Sub-Saharan Africa.\n\nAccording to its Charter, the core activities of the foundation are poverty mitigation, the protection of climate and environment (international projects in Asia, Africa, South America), and the advancement of scientific research and education on sustainable development (projects in Europe and Germany).\nOver the last 14 years the foundation has focused its project work on the promotion of private social investment and social enterprise in order to fight poverty and environmental degradation, and the provision of business development assistance for social entrepreneurs in developing countries working in the field of clean energy technologies and energy access.\n\nAs a family foundation Canopus pursues the venture philanthropy model which transfers the method and tools of venture capital to the social sector, responding to its growing demand for advanced financial engineering. The foundation promotes the long term allocation of philanthropic risk capital to early stage social enterprises, laying the ground for financial self-sufficiency and the expansion of their social and ecological impact. Towards this end the Canopus Foundation provides additional technical expertise, management skills and market intelligence where needed. Since 2008 the foundation is a member of the European Venture Philanthropy Association (EVPA).\n\nAshoka, Bamboo Finance S.A., IDEAAS (Fabiò Rosa), SELCO Ltd., Elea Foundation for Ethics in Globalization\n\n\n"}
{"id": "6276662", "url": "https://en.wikipedia.org/wiki?curid=6276662", "title": "Chvaletice Power Station", "text": "Chvaletice Power Station\n\nChvaletice Power Station is a large lignite-fired power station at Chvaletice in the Czech Republic, owned by Pavel Tykač. Its installed power output is 4x 200 MW. Blocks 3 + 4 use a chimney 305 metres tall built in 1977 which is the tallest free-standing structure in the Czech Republic.\n\n\n"}
{"id": "56907757", "url": "https://en.wikipedia.org/wiki?curid=56907757", "title": "Concord stagecoach", "text": "Concord stagecoach\n\nConcord stagecoaches are horse-drawn stagecoaches which employ a style of suspension and construction particularly suited to North America's early 19th century roads. Leather thoroughbraces suspend passengers who are in constant motion while the stagecoach is moving. The swaying is accepted by passengers for the shock absorbing action of the leather straps and for the way the special motion eases the stagecoach over very rough patches of roadway. The coaches were first developed and built by coachbuilder J Stephen Abbott and wheelwright Lewis Downing of Concord, New Hampshire. The expense of the high quality of the vehicle's construction had to be set against its structure's good lasting qualities improved by the relative smoothness of its thoroughbrace suspension system. These features attracted passengers.\n\nAbbott and Downing's Concord design replaced European designs which had short useful lives under punishing North American conditions. They were exported to Australia and South Africa.\n\nRailroads replaced stagecoaches in the middle of the 19th century, but Concord coaches remained in commercial use into the 20th century and continue to be used in parades and for publicity purposes by Wells Fargo Bank.\n\nThe smallest six-passenger Concord weighed two and a quarter tons or 2.25 metric tonnes and stood six feet eight inches or two metres tall.\n\nTimber: white oak, ash and basswood braced with iron bands. Iron fittings. Leather and canvas.\n\nThe undercarriage supports the leather thoroughbraces carrying the body. The two axles are tied together by a firm undercarriage braced by three straight perches (lengthwise frame members) and given a relatively slim transom (the transverse members at either end of the perches).\n\nEach end of each transom holds an upright metal standard from which hang the leather thoroughbraces.\n\nThe back wheels have brake blocks acting on the iron tires. The driver controls them with a foot lever to his right at the side of his footboard.\n\nThe body needs to make no contribution to the rigidity of the undercarriage and so is more lightly constructed than was the custom for European vehicles. This lightness also eases progress on the very rough roads.\n\nThere are three bench seats accommodating up to nine people though models to seat six and twelve passengers were available. The benches at the front and back of the body have limited headroom. Passengers on the center bench are given no backrest but steady themselves with a broad leather harness suspended across the coach by straps from the roof.\n\nAnother six passengers can travel in the open air on the body's roof. There is an external luggage compartment or boot at the back of the body and another boot for valuables below the driver's seat at the front.\n\nWindows are glazed, but in the 19th century the glass did not withstand hard overland mail trails of the western United States.\n\nA Concord Coach in Hadley Farm Museum, Massachusetts\nThe leading horses are known as the lead horses. The wheel horses or wheelers are the back pair nearest the coach's wheels. The number of horses, usually four or six, could be even more. Two horses alone would very soon tire.\n\nIt is not possible to guide a Concord coach with European-style precision. The Concord body continuously shifts. The driver or coachman has to sit slightly askew and brace himself with the aid of a steeply angled footboard. He cannot keep his reins in a steady contact with the horses' mouths. He has to bend his arms and elbows to constantly compensate, and his body always leans slightly forward. He holds his left reins in his left hand and his right reins — separated by his middle finger — in his right hand and not all in one hand like a European could. It is easy to slacken an individual rein but much more difficult to shorten it. His right hand also has to control his whip used on the wheel horses. If obliged to make his right hand free, then he must lay all the right hand reins in his left hand unseparated.\n\nThe horses were harnessed very loosely by European standards because without proper roads the horses had to be allowed to avoid their particular obstacles. The Concord pole, though mounted to allow far more play, moved less.\n\nThe result was the coach's direction was straighter than with a European coach, it did not respond to every irregularity in the road.\nConcord stagecoaches were expensive. Abbot-Downing also supplied a much simpler, lighter, and less expensive vehicle which they named \"Overland wagon\" and later \"Western passenger wagon\".\n\nThese are the vehicles which opened up the stage routes of the U.S. West.\n\n"}
{"id": "19277635", "url": "https://en.wikipedia.org/wiki?curid=19277635", "title": "Dam removal", "text": "Dam removal\n\nDam removal is the process of demolishing a dam, leaving a river to flow freely. It is undertaken for a variety of reasons that include environmental rehabilitation, structural weakness and maintenance expense.\n\nAll dams negatively affect the aquatic environments of rivers that they are located on, but those effects are often negated by the stated purpose of the dam, whether it is hydroelectric power, flood control, or other functions. When the negative environmental effects outweigh the benefits, a dam may be considered for removal. This happened on the Elwha River in Olympic National Park in Washington when extraordinarily rich salmon habitat was being disrupted by an out-dated hydroelectric plant. Before dams were built on the Elwha River, 400,000 salmon returned each year to spawn, but that number dropped to fewer than 3,000 after dams were put up. Once the hydroelectric power generating capacities of the dams had outlived their useful lives, the importance of this salmon habitat necessitated the removal of the dams on the Elwha River.\n\nDam failure is another threat that can lead to the removal of dams. Sometimes the fear of failure is due to seismic activity or the placement of the dam. Additionally, the older the dam is, the more likely it is to fail. Around the world, a growing number of 20th-century dam construction projects are reaching the end of their design lives: as of 1996, 5,000 large dams around the world were more than 50 years old.[1] By 2020, in the United States alone, 85% percent of the country’s dams will have exceeded that 50 year threshold.\n\nFinally, economic reasons often provide a convincing reason to remove a dam. The cost of keeping outdated hydroelectric equipment running decades after it was installed or upgrading dam safety systems may not be worth it. Additionally, population centers and industrial facilities that demanded hydropower decades ago can move and leave the dam without a purpose.\n\nSince around 2000, most major dam removal projects, and the largest single project, the $350M removal of two Olympic Peninsula dams as part of the Elwha Ecosystem Restoration, have been driven by restoration of river habitat and fish passages. Inspired by the Elwha project, a French group called \"SOS Loire Vivante\" successfully lobbied for the removal of two dams in the Upper Loire Valley, and the re-engineering of a 1941 hydroelectric dam on the Allier River to restore habitat for Atlantic salmon.\n\nIn the United States roughly 900 dams were removed between 1990 and 2015, with another 50 to 60 more every year. France and Canada have also completed significant removal projects, and Japan's first removal, of the Arase Dam on the Kuma River, began in 2012 and is scheduled to complete in 2018.\n\nMany of the dams in the eastern United States were built for water diversion, agriculture, factory watermills, and other purposes that are no longer seen as useful. Because of the age of these dams, over time the risk for catastrophic failure increases. In addition, many of these dams block anadromous fish runs, such as Atlantic salmon and American shad, and prevent important sediments from reaching estuaries.\n\nMany dams in the western United States were built for agricultural water diversion in the arid country, with hydroelectric power generation being a very significant side benefit. Among the largest of these water diversion projects is the Columbia Basin Project, which diverts water at the Grand Coulee Dam. The Bureau of Reclamation manages many of these water diversion projects.\n\nSome dams in the Pacific Northwest and California block passage for anadromous fish species such as Pacific Salmon and steelhead. Fish ladders and other passage facilities have been largely ineffective in mitigating the negative effects on salmon populations. Bonneville Power Administration manages electricity on 11 dams on the Columbia River and 4 on the Snake River, which were built by the Army Corps of Engineers.\n\nIn the Desert Southwest, dams can change the nature of the river ecosystem. In the particular case of the Glen Canyon Dam, the originally warm, sediment-filled, muddy water, instead runs cold and clear through the Grand Canyon, which has significant impacts on the downstream ecosystems. Three native fish species have become extinct in the Grand Canyon and others are endangered since the dam was completed, including humpback chub and razorback sucker.\n\nSome dam projects, such as those on the Salt River Project in Arizona, eliminate the flow of the river downstream, by diverting the flow into the Arizona Canal system for use in agriculture and urban usage, such that only a dry channel or arroyo heads out across the desert.\n\nSo much water is taken out of the Colorado River for agriculture, urban use, and evaporation behind the dams, that the river no longer flows into the Gulf of California.\n\nThere are several ways dams can be removed and the chosen method will depend on many factors. The size and type of the dam, the amount of sediment behind the dam, the aquatic environment below the dam, who owns the dam and what their priorities are, and the timeframe of dam removal are all factors that affect how the dam will be removed. Removal is costly no matter what and expenses typically rise when greater weight is given to environmental concerns. Fortunately, the cost of dam removal is usually shared by multiple stakeholders such as the dam owner and either the federal, state or local government. Four of the most common dam removal methods are described below.\n\nSediment management is a driving force in all of them. A common problem for dams is how sediment carried naturally by the river is deposited in the reservoir and eventually fills it up with silt. This excess sediment reduces the hydroelectric generating capacity of a reservoir, changes the river channel downstream, traps nutrient-rich sediment behind the dam, and can put a dangerous amount of pressure on the dam itself. Oftentimes the sediment stored in a reservoir is good for the riparian corridor below the dam, can rebuild fish habitat, provide nutrients, and add onto a beach or estuary. Other times, the sediment can increase the turbidity of the river harming fish, scour the landscape, and bury infrastructure.\n\nSediment can be tested before it is released to determine if it will be harmful to the landscape below the dam. Dam removal can have adverse consequences if this is not done. For example, when the Fort Edward Dam on the Hudson River was removed in the 1970s, PCBs in the sediment were released, affecting human and wildlife health downstream.\n\nThe notch and release approach is commonly used because of its ecological benefits. It is a slow method in which the reservoir is drained through notches cut into the dam. New notches are cut in so the water drains out of the reservoir at a consistent flow. The sediment trapped behind the dam flows downstream in a fixed rate that allows the ecosystem to adjust to the changes. This method can take months or even over a year but has proven success with restoring fish species to rivers. The Elwha and Glines Canyon dam removal project used the notch and release approach to great success.\n\nThe rapid release approach is both the quickest and least expensive way to remove a dam, but comes with significant drawbacks. In this approach, a large tunnel is dug through the base of the dam and then connected to the reservoir. The entire body of water will drain through this tunnel in a matter of minutes or hours and the massive release of water and sediment can cause severe flooding and erosion along the river downstream for miles. This can devastate the riparian ecosystem along the river as well as dangerously scour bridge pilings, buried pipes, levees, and other infrastructure. However, if the reservoir held back by the dam is relatively small and quickly drains into a larger river or lake, this approach can be carried out with minimal impact on either the ecosystem or human infrastructure.\n\nThe dig and dewater approach is typically the most expensive dam removal method, but is necessary in some cases. It entails emptying the entire reservoir, allowing the sediment to dry, and then transporting it to a safe location for disposal. It is costly and slow, but if the reservoir is located very near hydroelectric generating facilities that would be greatly impacted by released sediments, it may be necessary. Another situation is if the sediments behind the dam contain toxins. Hauling them away and disposing of them safely is important for the ecological health of the river.\n\nThe retained sediment approach is the final commonly used approach and involves leaving the sediment behind where it is. To do this, the river or creek must be rerouted around the damsite which can prove expensive and challenging. This may be carried out in places where the dig and dewater approach makes sense, but are too remote to be cost-effective.\n\nWhile fewer than 1% of United States dams are being considered for removal, there has been a push in recent years to address the deficiencies in existing dams without removing them. These goals include maximizing the efficiency of existing dams and minimizing their environmental impact. Updating equipment and acknowledging that dams have a limited life span are two ways to achieve those goals. As part of them, a plan for decommissioning the dam and restoring the river should be drawn up long before the dam exceeds its design life. One part of river restoration that does not have to wait until the dam is removed is introducing environmental flow. Having variable amounts of water flow through the dam at different seasons mimics natural seasonal variations in water level from winter and spring storms. Additionally, fish ladders can be added to dams to increase the connectivity of a river and allow fish to reach their spawning grounds. There’s debate about the effectiveness of fish ladders, but generally some fish will make it through as opposed to zero fish spawning in their traditional location.\n\n\nFour dams along the lower Snake River, built and still operated by the United States Army Corps of Engineers, serve as hydroelectric power sources as well as maintaining an inland port at Lewiston, Idaho for agricultural barge traffic. The four are candidates for removal because of millions of cubic yards accumulated behind the dams, which are raising water levels for riverside cities. They include: the 1975 Lower Granite Dam, the 1970 Little Goose Dam, the 1969 Lower Monumental Dam, and the 1962 Ice Harbor Dam.\n\nThree million new cubic yards of sediment are deposited behind the lower four dams on the Snake River annually. The city of Lewiston, Idaho and others along the Snake have built a system of levees maintained by the Army Corps of Engineers. The levees in Lewiston were designed to leave five feet between water levels and the top of the levees. As of 2011, two feet remained. As water levels continue to rise, either some of the dams must be removed or dredged, or the municipal levees will continue to be raised. The Corps admits that the amount of sediment in the riverbed is too great for dredging to be effective, and Lewiston community leaders are worried that higher levees will further cut the town off from its rivers.\n\nThe Corps began dredging behind Lower Granite Dam in 2015.\n\nThe privately owned Rindge Dam on Malibu Creek in the Santa Monica Mountains of California was built in 1924 and has been allowed to completely fill with sediment, making it functionally obsolete but still a potential hazard. Malibu Creek once supported the southernmost steelhead population in the world. But today, steelhead no longer occupy the creek.\n\nThe similar 1947 Matilija Dam near Ojai, California was built against the advice of the U.S. Army Corps of Engineers, among others, and also blocked steelhead trout spawning grounds. After being notched twice and largely silted up, 90% of its design capacity has been lost. As of 2013 stakeholders agree that the dam and its sediment be removed, but no funding source has been identified.\n\nAs resolution of several long-range issues centered on water rights in the Klamath Basin, the multi-party Klamath Basin Restoration Agreement was signed in early 2008. Parties to the agreement included the state of California, the state of Oregon, three Native American tribes, four counties, and 35 other local organizations and individuals.\n\nThis agreement called for the removal of four of the Klamath River Hydroelectric Project dams: the 1958 John C. Boyle Dam, the 1922 Copco Number 1 and Copco Number 2, and the 1964 Iron Gate Dam. All four are privately owned by PacifiCorp. At the time PacifiCorp faced a relicensing cycle with Federal Energy Regulatory Commission, with potentially expensive fixes for salmon passage and to address the growth of the toxic bacteria \"Microcystis aeruginosa\" in the Copco and Iron Gate Reservoirs.\n\nOn September 29, 2009, Pacificorp reached an agreement in principle with the other KBRA parties to remove the four dams, pending Congressional approval.\n\nCongress did not act, so as of February 2016, the states of Oregon and California, the dam owners, federal regulators and other parties reached a further agreement to remove all four Klamath basin dams by the year 2020, contingent only on approval by the Federal Energy Regulatory Commission. The new plan has been endorsed by the governors of California and Oregon, as well as the U.S. Secretary of the Interior.\n\nThe Glen Canyon Dam has been proposed for removal because of the negative effects it has on the water quality and riparian habitat of the Colorado River in Grand Canyon National Park. In addition, the reservoir impounded behind it, Lake Powell has filled all of the canyons for up to above the dam. This lake, while providing recreational opportunities, has eliminated more than of habitat for endangered Colorado River fish species.\n\nThe reservoir also loses more than 6 percent of the total annual flow of the Colorado River to evaporation and seepage. Advocates of dam removal such as the Glen Canyon Institute also cite these losses of stored water as reason to decommission the dam. If it were to be removed, it would dwarf any completed dam removal project in history.\n\nO'Shaughnessy Dam in California was completed in 1923 and represented the first great environmental controversy in the US as it was constructed in a national park. The debate over the dam and reservoir continues today. Preservationist groups such as the Sierra Club lobby for the restoration of the valley, while others argue that leaving the dam in place would be the better economic and environmental decision.\n\nThe two remaining dams on the Kinnickinnic River in River Falls, Wisconsin are being considered for removal in order to completely restore the Kinnickinnic River to its natural state. The Kinnickinnic River, called the Kinni for short, is a river in northwestern Wisconsin in the United States. The Kinni is a cold water fishery supporting a population of native Brook Trout and naturally reproducing Brown Trout. The Kinnickinnic River is officially designated as a Class I trout stream by the WI DNR, indicating it is a \"high quality\" trout water that has sufficient natural reproduction to sustain populations of wild trout, at or near carrying capacity. The Kinnickinnic is also designated as an Outstanding Resource Water (ORW) by the WI DNR both above State HWY 35, and below the Powell Falls Dam, however, the stretch of the Kinni through the City of River Falls is not included in this designation where the river is impounded into two reservoirs which do not support a fishery. This ORW designation indicates the Kinni provides outstanding recreational opportunities, supports valuable fisheries and wildlife habitat, has good water quality, and is not significantly impacted by human activities. This designation indicates that the State of Wisconsin has determined the Kinnickinnic River warrants additional protection from the effects of pollution. These designations are intended to meet federal Clean Water Act obligations requiring Wisconsin to adopt an “antidegradation” policy that is designed to prevent any lowering of water quality – especially in those waters having significant ecological or cultural value.\n\nLocal stakeholder organizations in the FERC relicensing process include the Friends of the Kinni, the Kiap-TU-Wish Chapter of Trout Unlimited, the Kinnickinnic River Land Trust, and the River Alliance of Wisconsin. Government agencies also serving as stakeholder organizations include the Wisconsin DNR, the US Fish and Wildlife Service, and the National Park Service.\n\nDam Removal Europe (\n\n"}
{"id": "11978558", "url": "https://en.wikipedia.org/wiki?curid=11978558", "title": "DeLorean time machine", "text": "DeLorean time machine\n\nThe DeLorean time machine is a fictional automobile-based time travel device featured in the \"Back to the Future\" franchise. In the feature film series, Dr. Emmett Brown builds a time machine based on a DeLorean DMC-12 car, to gain insights into history and the future. Instead, he ends up using it to travel over 30 years of Hill Valley history (from 1985 to 2015) with Marty McFly to change the past for the better and to undo the negative effects of time travel. One of the cars used in filming is on display at Universal Studios Hollywood and the official \"Back to the Future\" DeLorean can be viewed at the Petersen Automotive Museum.\n\nThe control of the time machine is the same in all three films. The operator is seated inside the DeLorean (except the first time, when a remote control is used), and turns on the time circuits, activating a unit containing multiple fourteen- and seven-segment displays that show the destination (red), present (green), and last-departed (yellow) dates and times. After entering a target date, the operator accelerates the car to , which activates the flux capacitor. As it accelerates, several coils around the body glow blue/white while a burst of light appears in front of it. Surrounded by electrical current similar to a Tesla coil, the whole car vanishes in a flash of white/blue light seconds later, leaving a pair of fiery tire tracks. A digital speedometer is attached to the dashboard so that the operator can accurately gauge the car's speed. Various proposals have been brought forth in the past by fans of the movie franchise for why the car has to be moving at 88 mph to achieve temporal displacement, but actually the production crew chose the velocity simply because they liked how it looked on the speedometer.\n\nObservers outside the vehicle see an implosion of plasma as the vehicle disappears, while occupants within the vehicle see a quick flash of light and instantaneously arrive at the target time in the same spatial location (relative to the Earth) as when it departed. In the destination time, immediately before the car's arrival, three large and loud flashes occur at the point from which the car emerges from its time travel. After the trip, the exterior of the DeLorean is extremely cold, and frost forms from atmospheric moisture all over the car's body.\n\nA few technical glitches with the DeLorean hinder time travel for its users. In the first film, the car has starter problems and has a hard time restarting once stopped, much to Marty's repeated frustration. In the second movie, the destination time display malfunctions and shows random dates (mostly January 1, 1885), which partially cause Doc to be sent to 1885. In the third movie, the flying circuits (added by Doc in 2015), fuel line, and fuel injection manifold are damaged, preventing the car from moving under its own power.\n\nThe time machine is electric and requires a power input of 1.21 gigawatts to operate, originally provided by a plutonium-fueled nuclear reactor. In the first movie, Doc has no access to plutonium in 1955, so he outfits the car with a large pole and hook in order to channel the power of a lightning bolt into the flux capacitor and send Marty back to 1985. During Doc's first visit to 2015, he has the machine refitted to hover above ground in addition to standard road driving, and he replaces the nuclear reactor with a \"Mr. Fusion\" generator that uses garbage as fuel.\n\nAlthough the Mr. Fusion unit provides the required power for the time machine, the DeLorean is still powered by an internal combustion engine for propulsion. The fuel line is damaged during Marty's trip to 1885 in \"Back to the Future Part III\"; after he and Doc patch it, they attempt to use whiskey as a replacement fuel since commercial gasoline is not yet available. The test fails, damaging the car's fuel injection manifold and leaving it unable to travel under its own power.\n\nDoc and Marty consider options to reach the required 88 mph (such as pulling it with horses, which fails because the car barely breaks 20 mph) but ultimately settle on pushing the car with a steam locomotive. For the extra power needed to push the DeLorean up to speed, Doc adds his own version of \"Presto Logs\" (a chemically treated mixture of pressed wood and anthracite) to the locomotive's boiler and chooses a location with a straight section of track long enough to achieve 88 mph.\n\nThe power required is pronounced in the film as one point twenty-one \"jigowatts\". While the closed-captioning in home video versions spells the word as it appears in the script, jigowatt, the actual spelling matches the standard prefix and the term for power of \"one billion watts\": \"giga\"watt. Although rarely used, the \"j\" sound at the beginning of the SI prefix \"giga-\" is an acceptable pronunciation for \"gigawatt.\" In the DVD commentary for \"Back to the Future,\" Bob Gale states that he had thought it was pronounced this way because it was how a scientific adviser for the film pronounced it.\n\nThe \"flux capacitor\", which consists of a rectangular-shaped compartment with three flashing Geissler-style tubes arranged in a \"Y\" configuration, is described by Doc as \"what makes time travel possible.\" The device is the core component of the time machine.\n\nAs the time machine nears 88 mph, light coming from the flux capacitor begins pulsing more rapidly until it becomes a steady stream. Doc originally conceived the idea for the flux capacitor on , 1955, when he slipped while hanging a clock in his bathroom and hit his head on the sink. A similar, but more primitive, steam-powered flux capacitor is also seen in the chimney headlamp of Doc's second time machine, the Time Train, at the end of \"Back to the Future Part III.\"\n\nAlthough the films do not describe exactly how the flux capacitor works, Doc mentions at one point that the stainless steel body of the DeLorean has a direct and influential effect on the \"flux dispersal\", but he is interrupted before he can finish the explanation. The flux capacitor requires 1.21 gigawatts of electrical power to operate, which is roughly equivalent to the power produced by 15 typical commercial airplane jet engines.\n\nThe instruction manual for the AMT/ERTL DeLorean model kit says: \"Because the car's stainless steel body improves the flux dispersal generated by the flux capacitor, and this in turn allows the vehicle smooth passage through the space time continuum.\"\n\nThe Mr. Fusion Home Energy Reactor is the name of a power source used by the DeLorean time machine in the \"Back to the Future\" trilogy. It can be seen for the first time at the end of \"Back to the Future\" when \"Doc\" Emmett Brown pulls into the McFly's driveway after a trip to the year 2015. It is a parody of Mr. Coffee machines, which were very popular at the time of filming. The appliance from which the prop was made was actually a Krups \"Coffina\" model coffee grinder.\n\nThe Mr. Fusion Home Energy Reactor converts household waste to power for the time machine's flux capacitor and time circuits using nuclear fusion, presumably cold fusion. In the film, Mr. Fusion allows the DeLorean time machine to generate the required 1.21 gigawatts needed to travel to any point in time. The energy produced by Mr. Fusion replaces plutonium as the primary power source of the DeLorean's time travel, allowing the characters to bypass the arduous power-generation requirements upon which the plot of the first film hinges. The plutonium fission reactor was most likely left installed underneath Mr. Fusion as a backup power source.\n\nThe Mr. Fusion can provide enough power to the flux capacitor but is not used to power up the DeLorean itself, which makes use of an ordinary gasoline combustion engine to reach the 88 mph speed necessary for it to time travel, a limitation that proved itself crucial in the third movie when Doc and Marty find themselves stuck in 1885 and unable to return with the DeLorean out of gas (due to a fuel leak). The vehicle's hover system is powered by Mr. Fusion and is capable of bringing the DeLorean up to the required 88 mph; the combustion engine was also probably left on board as a backup. However, the flight systems were destroyed as a result of a lightning strike, leaving Marty to rely on the original combustion engine, which in turn was disabled.\n\nFor most of the first film, the 1.21 gigawatts are supplied by a plutonium-powered nuclear fission reactor and, with the absence of plutonium, a bolt of lightning channeled directly into the flux capacitor by a long pole and hook in the film's climactic sequence. At the end of the first film, and for the remainder of the trilogy, the plutonium nuclear reactor is replaced by a \"Mr. Fusion Home Energy Reactor\" generator possibly acquired in 2015. The \"Mr. Fusion\" device apparently converts household waste into electrical power; the name suggests nuclear fusion. Due to a \"hover conversion\" made in 2015, the car also becomes capable of hovering and flight, though it lost this ability at the end of the second film.\n\nThe DeLorean returns to 1985 and proceeds to travel to October 21, 2015, to stop Marty's future son from committing a crime. While there, the DeLorean is stolen by Biff who then travels back to November 12, 1955, the same day as the climax of the first film, to give his past self a sports almanac to be used for gambling. Once Biff returns to 2015 without Doc's knowledge, the duo return to 1985, but find themselves in an alternate timeline where Hill Valley is ruled by Biff that Doc described as 1985A (alternate 1985). The DeLorean then travels back to 1955 to restore the timeline, but in the aftermath, it is struck by lightning again in the very same electrical storm, this time by accident. According to writers Bob Gale and Robert Zemeckis, the lightning causes the DeLorean to spin at 88 miles per hour, and Doc later states in a letter to Marty that the bolt caused a \"gigawatt overload\" which \"shorted out the time circuits and destroyed the flying circuits\". The DeLorean then disappears from 1955, travelling back in time to January 1, 1885 (earlier in the film, Doc mentions that the time circuits are not functioning correctly; several instances in the film that show the time circuit display showed 1885 as the destination when the time circuits malfunctioned).\n\nOnce in 1885, the DeLorean is hidden in a mine because suitable replacement parts to replace its destroyed microchip will not be invented until 1947 (presumably referring to the transistor, invented in that year). Doc and Marty recover the DeLorean from the mine in 1955, and Doc builds a vacuum tube circuit assembly to replace the destroyed microchip circuitry and restore the vehicle's time travel capabilities. The tires have disintegrated in storage, so Doc replaces them with whitewalls. The gasoline engine is still functional, but the flying circuits are not.\n\nIn a letter Doc wrote to Marty in 1885, Doc states he is happy in his new life there and requests that Marty not attempt to retrieve him, but instead to return to 1985 and destroy the DeLorean, believing that it has brought them and the world nothing but disaster. However, Marty and the Doc of 1955 learn of tragedy to come Doc's way when he is murdered by Biff's grandfather, Buford \"Mad Dog\" Tannen, on September 7, 1885; therefore, 1955 Doc agrees to send Marty back to the Old West to rescue himself.\n\nWhen Marty arrives in 1885, the DeLorean's fuel line is pierced by an arrow during an Indian attack. He and Doc patch it and attempt to use whiskey as a replacement fuel, since commercial gasoline is not yet available; the test fails, destroying the fuel injection and ignition systems and leaving the car unable to travel under its own power. Its final trip, from 1885 to 1985, is propelled by a steam locomotive that has Doc's version of \"Presto Logs\" (pressed wood treated with anthracite) added to the boiler to provide the extra power needed to push the car up to 88 mph; once this speed is reached, the Mr. Fusion unit provides the power required to activate the flux capacitor and make the jump through time. Doc replaces the 1955-style wheels with cast iron train wheels that fit on the track rails. He uses the old tires and a wooden support to cushion the locomotive's \"cow catcher\" and the car's rear end. Since each of the three \"Presto Logs\" fire at different intervals with increasing power, Doc installs a boiler temperature gauge on the DeLorean's dashboard to indicate when the car will experience a sudden burst of acceleration.\n\nOnce the DeLorean makes its final trip from 1885, it arrives back in 1985 and is immediately destroyed by an oncoming freight train running in the opposite direction. Marty is able to bail out of the car seconds before the train strikes. Later the Time Train, which is Doc's second time machine, appeared in the same spot where the DeLorean was destroyed.\n\nIn the films, the DeLorean time machine is a licensed, registered vehicle in the state of California, where the films take place. The vanity license plate used in the film reads \"OUTATIME\", a deliberate anomaly, as the maximum number of symbols on California plates is seven characters. When Doc returns from 2015, it is a barcode license plate, which implies that by that year license plates have moved to other more sophisticated means of tracking and registering.\nIn \"\", Doc builds another DeLorean into a time machine, restoring most of its features, including Mr. Fusion and the hover conversion (Doc either rebuilds the one destroyed at the end of \"Part III\" or he simply builds a new one). He also seemingly adds the capability to travel through space in addition to time (i.e., appear at a different location from the one it departed), similar to the TARDIS from \"Doctor Who\". The cartoon DeLorean time machine has many add-ons, including a back seat in normal two-door mode, the ability to transform into a four-door, a pop-out covered wagon top, a blimp, a rear video screen, and a voice activated time input.\n\n\"\" features a chronal duplicate of the original DeLorean, which Doc Brown recovered from the timestream after the destruction of the original. This DeLorean is created at the end of \"Back to the Future Part II\", when the original time machine was struck by lightning: while the DeLorean itself is sent to 1885, a fully functional duplicate appears (apparently unmanned and undamaged) in 2025, where Doc retrieves it with the Hover Train. He later traveled to 1931 and sent the DeLorean to 1986 to get Marty to rescue him from 1931 before he can be killed. This duplicate DeLorean is effectively the same as the Part II car, including the occasional glitches in the time controls (mostly affecting the last time departed time display), but with a new automatic retrieval feature that automatically brings the DeLorean to a set time and location of Doc's choosing every time Doc Brown doesn't return to the car in a fixed amount of time.\n\nThis DeLorean is later badly damaged (Marty crashes the DeLorean into a billboard, and after Marty gets out, the DeLorean falls through the billboard and crashes onto the ground) and then restored by an alternate version of Doc Brown who has never developed time travel technology, having access to limited notes about the flux capacitor. As such, the chronal circuits of the duplicate DeLorean become even more glitchy, accumulating errors as severe as the interval of time traveled, with increasing damage with every attempt: as such, Citizen Brown, the Alternate Doc, has to install a diagnostic console made of materials available in 1931 (appearing as a plywood box with a lightbulb and several similar bulbs placed on the coils on the outer body).\n\nApparently, part of the problem is chromium parts becoming unstable during time travel, according to Alternate Doc. This DeLorean is then stolen by Edna Strickland, one of the game's main villains. The more adept Doc Brown returns in another duplicate DeLorean due to earlier events in the game, although it is unknown where it came from. Then Officer Danny Parker nearly arrests Marty and Doc for allegedly having the car that Edna got away in. After they explain to him that there is more than one DeLorean, Marty explains that the other DeLorean had malfunctioning time circuits. To make matters worse, the entire town of Hill Valley disappears around them. They go to \"Mary Pickford's\" house and see that the other DeLorean had been destroyed. After they get information from Mary, who was really Edna, they go to 1876. After they stop the fire that would've burned down Hill Valley, they chase down Edna, who is trying to get away in the first DeLorean. Marty synchronizes the two DeLoreans and hoverboards back to Doc's DeLorean, which is flying behind the one Edna's driving. They all travel back to 1931, with Edna's DeLorean duplicate vanishing because of the time ripples catching up with them, causing \"chronal decay\" (i.e., since the Alternate Doc timeline ceased to exist, the alternated Clone DeLorean was folded back with the real Clone DeLorean).\n\nIt is implied that the Hover Train stays with Clara, Jules and Verne, passingly mentioned as enjoying the same nomadic life around the time-stream of Doc, but it is never seen in the game. The ending introduces a blue DeLorean and a black DeLorean, but it is unknown how these time machines were created.\n\nIn \"\", Doc, who now lives in a lab, had created an 8-passenger DeLorean that can fly just like the original DeLorean (which can be seen in the ride and in the outside display) and the Hover Train (which can only be seen in display outside of the attraction). Unlike the original DeLorean, the flux capacitor is in the front of the cockpit along with a small screen, the time circuits, and the speedometer. The original DeLorean is also shown to have its original \"OUTATIME\" license plate instead of the bar code license plate, but it could just mean that this DeLorean is actually a new one being built into a time machine. However, in a post-credits scene, Clara Clayton, who has built Hover Train with Doc, currently repaired the DeLorean and travels back to 1947 to a farm.\n\nIn \"Doc Brown Saves the World\", there was a repaired DeLorean time machine which included new replacement parts from 2015. The DeLorean is also seen in a video promoting \"Doc Brown Saves the World\", and it is unknown as to whether or not a flux capacitor was inside.\n\nThe time machine went through several variations during production of the first film, \"Back to the Future\". In the first draft of the screenplay, the time machine was a laser device that was housed in a room. At the end of the first draft the device was attached to a refrigerator and taken to an atomic bomb test site. Director Robert Zemeckis said in an interview that the idea was scrapped because he did not want children to start climbing into refrigerators and getting trapped inside. In the third draft of the film the time machine was a car, as Zemeckis reasoned that if you were going to make a time machine, you would want it to be mobile. The specific choice of vehicle was a DeLorean DMC-12 for the purposes of it looking like an alien spaceship due to its characteristic gullwing doors (which were inspired by the Mercedes-Benz 300 SL). However, in order to send Marty back to the future, the vehicle had to drive into a nuclear test site. Ultimately this concept was considered too expensive to film, so the power source was changed to lightning.\n\nWhen the filmmakers arrived at the point where the time machine would be built into a car, the art department was instructed to come up with designs for the DeLorean. Andrew Probert was the first artist to explore the subject (before Ron Cobb joined the production), but his designs were deemed \"too perfect\" for the look the producers wanted, which was to make it look as if it had been built in a garage by Doc Brown. The idea was that it had been constructed with parts found in a hardware and electronics store, so it couldn't look too sophisticated. It also had to look dangerous, as Producer Bob Gale noted in the DVD commentary for \"Back to the Future\". The task was undertaken by Ron Cobb who added the coils to the back of the vehicle. The nuclear reactor was also a design choice made by Cobb. This choice proved to be important, given the direction the script had taken. Cobb complemented the nuclear reactor with one vent on the back of the car, since it was generally known at the time that nuclear reactors had vents. Once Cobb had left the production, the producers wanted to balance the design with another vent, keeping a symmetrical aesthetic. Probert was asked to step in and he brought the design to its final form. At the end of the first film of the trilogy these vents become the propulsion system for the improved DeLorean, which now had hovering abilities and could reach the time-traveling speed of 88 miles per hour flying. The production design team added other buttons and lights inside the car to make it look more appealing and complex in order for the audience to have something attractive to look at.\n\nDifferent parts from three 1982 DeLoreans were used in the first film. Liquid nitrogen was poured onto the car for scenes after it had travelled through time to give the impression that it was cold. The base for the nuclear reactor was made from the hubcap from a Dodge Polara. Aircraft parts and blinking lights were added for effect. In one of the first scenes, carbon dioxide extinguishers were hidden inside the DeLorean to simulate the exhaust effect. Ultimately, five real DeLoreans were used in the filming of the trilogy, plus one \"process\" car built for interior shots. In the off-road scenes in the third film, a modified-for-off-road VW Beetle frame was fitted to the DeLorean with the whitewall tires and baby Moon hubcaps. A seventh DeLorean was also used in the filming; however, this one was merely a full-sized, fiberglass model used for exterior shots where the vehicle hovers above the set as well as when the actors interact with the vehicle.\n\nRather than use the sound of the stock V-6 DeLorean engine in the film, the sound of a DMC-12 with a Legend Twin Turbo VIN 530 was used.\n\nA number of private auto customizers have built replicas of the DeLorean time machine. Starting with a stock DMC-12, they added most, if not all, the props used by the movie producers for the picture cars. In addition to the interior and exterior props, they feature working indicator lights and switches along with the actual sounds which duplicate the ones made by the movie car's controls when activated. These vehicles are, for the most part, roadworthy DeLoreans with stock drivetrains, and are frequently driven to car shows and \"Back to the Future\"-related events.\n\nReplicas can range from a minimal 'obligatory mod' - typically in the form of an operational Flux-Capacitor prop on the rear firewall of an otherwise stock Delorean DMC-12, all the way through to complete replicas, as described above.\n\nAs of March 2017, some 87 DeLorean DMC-12s are known to be converted to \"Back to the Future\" Time Machines.\n\n"}
{"id": "32069451", "url": "https://en.wikipedia.org/wiki?curid=32069451", "title": "Distributed propulsion", "text": "Distributed propulsion\n\nDistributed propulsion (DP) is a type of powered flight propulsion system for fixed-wing aircraft in which engines are distributed about a vessel. Its goal is to increase performance in fuel efficiency, emissions, noise, landing field length and handling performance. DP is typically accomplished by spanwise distribution of partially or fully embedded multiple small engines or fans along the wing. Alternatively, it may involve ducting exhaust gases along the wing's entire trailing edge.\n\nDistributed propulsion spreads thrust around the aircraft, either by using three or more smaller propulsion units (engines, propulsors, thrusters or others) or by ducting exhaust gases to three or more locations. \n\nThe multiple propulsion unit strategy involves three or more propulsion units . These units are arranged in Leader or Follower configurations. They are classified into five intensity classes (A–E) and three thrust-to-weight ratio categories (I-III). They can be arranged within/above/around or across the wing(s)/fuselage(s) or airframe. \n\nLeader arrangements employ propulsion units to directly generate thrust, i.e., distributed engines. The Follower arrangement uses secondary propulsion unit(s), such as multiple fans that are powered by a single engine. In the last case, the power transmission between the fans and engines may be linked by ducting hot gas, mechanical gears, or electric power lines.\n\nIntensity classes group designs according to the number of propulsion units they employ.\n\nFor distributed propulsion, this ratio can be defined as the total aircraft thrust produced divided by the Maximum Take-Off Weight (MTOW) rather than dividing only by the propulsion unit weight, given the strategy's potential to reduce the weight of the rest of the aircraft.\nThus, the following convention describes specific systems:\n\nDP Configuration (L/F)-Intensity class (A-E)-Thrust-to-weight (I-III)-(X)\n\nFor example, the 1945s Blohm and Voss 238 V1 aircraft would be denoted as DPL-B-I,since the aircraft employs six piston engines and has a thrust-to-weight ratio less than 0.10.\n\nIn addition to providing propulsion, distributed propulsion arrangements may provide the following functions:\n\nThese implementations are often proposed in conjunction with blended wing body (BWB) or hybrid wing body (HWB) aircraft.\n\nWhile some of these concepts were tested on full scale aircraft in the 1960 - 1970s, such as the Hunting H.126, they were not fielded in production aircraft. More recently, several full-size and smaller unmanned aerial vehicle (UAV) projects have proposed DP approaches to meet noise abatement, fuel efficiency and landing field length goals. Advancements in materials engineering, cryogenic cooling systems, novel fuels and high fidelity computational fluid dynamics (CFD) modeling and analysis have been credited for the renewed interest.\n\nRecent analytic and experimental distributed propulsion studies suggest several improvements in aircraft performance. They include fuel consumption efficiency, noise abatement, steep climbing for short take off and landing (STOL), novel control approaches (in particular eliminating control surfaces for roll, pitch and yaw moments), and high bypass ratios. It has also been suggested that smaller propulsors will be cheaper to manufacture and easier to handle during assembly and maintenance.\n\n"}
{"id": "684190", "url": "https://en.wikipedia.org/wiki?curid=684190", "title": "Drum (container)", "text": "Drum (container)\n\nA drum is a cylindrical container used for shipping bulk cargo. Drums can be made of steel, dense paperboard (commonly called a fiber drum), or plastics, and are generally used for the transportation and storage of liquids and powders. Drums are often certified for shipment of dangerous goods. Shipped goods must be matched with the make of drum necessary to comply with applicable regulations. Drums are also called barrels in common usage.\n\nIt is common to hear a drum referred to as a barrel and the two terms are used nearly interchangeably. Many drums have a common nominal volume of and nominally measure just under tall with a diameter just under and differ by holding about thirteen gallons more than a barrel of crude oil. In the United States, drums are also in common use and have the same height. This allows easy stacking of mixed pallets. Barrels can be constructed of plastic, laminated paperboard or steel.\n\nThe two common sub-types of drums are the open top and the welded top (with NPS bung holes). The latter are almost universally called \"barrels\" in preference to \"drums\" in the United States. They cannot efficaciously either dispense or be filled with powdered goods, though they might \"store\" them very well, so are not used for such goods, being reserved for liquids transport and storage. Plastic drums are manufactured using injection blow moulding technology and have either a separate lid (similar to those on fiber drums) or a welded type top with the bung holes molded in. Metal drums are manufactured with cold-rolled steel sheets, welded themselves into long pipe-like sections then \"forged\" on a stamping press into drum bodies. A rolled seam is then made for the drum bottom, or bottom and top both.\n\nSome drums have reinforcing rings of thickened metal or plastic, called \"chimes\", at four places: top, bottom, and one each a third of the way from each end ring. This sufficiently strengthens them so that they can readily be turned on their sides and rolled when filled with heavy materials, like liquids. Over short to medium distances, drums can be tipped and rolled on the bottom rim while being held at an angle, balanced, and rotated with a two-handed top grip that also supplies the torque (rotational or rolling force).\n\nThe open-top sub-type is sealed by a mechanical ring clamp (concave inwards) that exerts sufficient pressure to hold many non-volatile liquids and make an airtight seal against a gasket, as it exerts force inward and downward when tightened by a normal three-quarter inch wrench or ratchet wrench. Tops exist with bung holes as above, and these hybrid drums with lid can be used to ship many non-volatile liquids as well as industrial powders. Many drums are used to ship and store powdered products as well as liquids, such as plastic beads for injection moulding, extrusion, and purified industrial grade powders like cleansers (e.g., fertilizers, and powdered aluminum). If used to transport dangerous goods across international boundaries, they may need to have UN certification. In general, drum usage is limited to wholesale distribution of bulk products, which are then further processed or sub-divided in a factory.\n\nThese metal drums have two openings with flanges (2 in NPS and ¾ in NPS). Once the drums are filled, the plugs (bungs) are screwed in the flanges using pneumatic or hand operated bung tightener (plug wrench). To secure the contents of the drums against theft and adulteration during shipment, cap-seals made of metal and other types like metal-plastic laminates are used. These cap-seals sit on top of the flanges and are crimped, using a drum cap-seal crimping tool, also called a drum cap sealer. Once cap-seals are crimped, the plugs can be unscrewed only by breaking these cap-seals. Pneumatic and hand-operated cap-seal crimping tools are available. Pneumatic ones are used in production lines for high production.\n\nThe fiber drums referred to above will easily hold , and are usually coated internally with a urethane or plastic protective coating. They have steel reinforcement rims at their ends, and are sufficiently strong that this is the only type of drum that is not reinforced in the middle third, but that is almost certainly due to the difficulty in creating a \"vee\" rib in a paper layer that essentially spirals out from a single end seam.\n\nA 200-litre drum (known as a 55-gallon drum in the United States and a 44-gallon drum in the United Kingdom) is a cylindrical container with a nominal capacity of 200 litres (55 US or 44 imp gal). The exact capacity varies by manufacturer, purpose, or other factors. Standard drums have inside dimensions of diameter and height. These dimensions yield a volume of about 218,681 dm or , but they are commonly filled to about 200 litres.\n\nThe outside dimensions of a 200-litre drum are typically diameter at the top or bottom rim, diameter at the chines (ridges around drum), and height. Exact dimensions are specified in ANSI MH2.\n\nThe drums are typically made of steel with a ribbed outer wall to improve rigidity and for rolling. The lids can be welded or secured with a head gasket and bolt ring. Drums can also be made of durable plastic or paperboard. They are commonly used for transporting oils, fuels, chemicals, and dry goods. The construction and performance of drums used for shipment of hazardous materials are governed by UN, country, and carrier regulations.\n\nDrums are frequently transported on pallets for ease of handling by a fork truck and for shipping. The drum's size, shape, and weight distribution lends itself to being moved about readily on the loading dock or factory floor with a two-wheeled hand truck. They can be turned on side and rolled. They can also be moved by hand short distances on firm surfaces by tilting and then rolling along the base, or by using a drum handler, which is designed especially for that purpose.\n\nHenry Wehrhahn, employee of Nellie Bly’s Iron Clad Manufacturing Company of New York, received two patents in December 1905 that would lead to the modern 55-gallon steel barrel. Use of 200-litre drums became widespread in World War II, the first war in which trucks, cold rolled steel, stamp or pattern forging machinery and welding were widely available. They were first developed by the Axis powers (Germany and Italy), but were quickly adopted by Allies. The drums helped win the Guadalcanal Campaign in the first U.S. offensive in the South Pacific Theater. The U.S. Navy could not maintain command of the sea long enough to offload aviation gasoline for aircraft ashore, so the drums were often transported to the island on fast ships, such as destroyers, and shoved over the sides (or, time permitting, lowered in cargo nets). Because gasoline's density is much less than that of water, the drums floated. Navy Seabees in small craft corralled the drums.\n\nClosed-head steel barrels and drums used for shipment of chemicals and petroleum products have a standardised bunghole arrangement, with one (DN50) NPT and one (DN20) NPT threaded bunghole on opposite sides of the top head. This arrangement is echoed in many plastic drums in the same size. Various components can be mounted to the drum, such as drum pumps and bung mixers.\n\nIn the past, hazardous waste was often placed in drums of this size and stored in open fields or buried. Over time, some drums would corrode and leak. As a result, these drums have become iconic of pollution problems, even though they have numerous uses and are ubiquitous in commerce. Drums are often cleaned or re-conditioned and then used for storing or shipping various liquids or materials.\n\nDrums are frequently reused for many purposes including as barricades to protect construction workers from oncoming traffic, and perhaps most famously as musical instruments.\n\nAlthough crude oil is sometimes shipped in 55-US-gallon drums, the measurement of oil in barrels is based on the whiskey barrels of the 1870s which measured . The measure of 42 US or wine gallons, corresponds to a wine tierce (third-pipe). A wine barrel, or 1/8 tun, measures .\n\n\n"}
{"id": "44804203", "url": "https://en.wikipedia.org/wiki?curid=44804203", "title": "Dutch cask", "text": "Dutch cask\n\nDutch cask is a UK unit of weight for Butter and Cheese.\n\n112 pounds(avdp.).\n\n1 Dutch cask ≡ 32/21 Tub\n\n1 Dutch cask ≡ 112 pounds(avdp.)\n\n1 Dutch cask ≡ 50.80234544 kg\n"}
{"id": "13891512", "url": "https://en.wikipedia.org/wiki?curid=13891512", "title": "Dynamic frequency scaling", "text": "Dynamic frequency scaling\n\nDynamic frequency scaling (also known as CPU throttling) is a technique in computer architecture whereby the frequency of a microprocessor can be automatically adjusted \"on the fly\" depending on the actual needs, to conserve power and reduce the amount of heat generated by the chip. Dynamic frequency scaling helps preserve battery on mobile devices and decrease cooling cost and noise on quiet computing settings, or can be useful as a security measure for overheated systems (e.g. after poor overclocking). Dynamic frequency scaling is used in all ranges of computing systems, ranging from mobile systems to data centers to reduce the power at the times of low workload. \n\nThe dynamic power (\"switching power\") dissipated per unit of time by a chip is \"C·V·A·f\", where C is the capacitance being switched per clock cycle, V is voltage, A is the Activity Factor indicating the average number of switching events undergone by the transistors in the chip (as a unitless quantity) and f is the switching frequency.\n\nVoltage is therefore the main determinant of power usage and heating. The voltage required for stable operation is determined by the frequency at which the circuit is clocked, and can be reduced if the frequency is also reduced. Dynamic power alone does not account for the total power of the chip, however, as there is also static power, which is primarily because of various leakage currents. Due to static power consumption and asymptotic execution time it has been shown that the energy consumption of a piece of software shows convex energy behavior, i.e., there exists an optimal CPU frequency at which energy consumption is minimal.\nLeakage current has become more and more important as transistor sizes have become smaller and threshold voltage levels lower. A decade ago, dynamic power accounted for approximately two-thirds of the total chip power. The power loss due to leakage currents in contemporary CPUs and SoCs tend to dominate the total power consumption. In the attempt to control the leakage power, high-k metal-gates and power gating have been common methods.\n\nDynamic voltage scaling is another related power conservation technique that is often used in conjunction with frequency scaling, as the frequency that a chip may run at is related to the operating voltage.\n\nThe efficiency of some electrical components, such as voltage regulators, decreases with increasing temperature, so the power usage may increase with temperature. Since increasing power use may increase the temperature, increases in voltage or frequency may increase system power demands even further than the CMOS formula indicates, and vice versa.\n\nDynamic frequency scaling reduces the number of instructions a processor can issue in a given amount of time, thus reducing performance. Hence, it is generally used when the workload is not CPU-bound.\n\nDynamic frequency scaling by itself is rarely worthwhile as a way to conserve switching power. Saving the highest possible amount of power requires dynamic voltage scaling too, because of the V component and the fact that modern CPUs are strongly optimized for low power idle states. In most constant-voltage cases, it is more efficient to run briefly at peak speed and stay in a deep idle state for longer time (called \"race to idle\" or computational sprinting), than it is to run at a reduced clock rate for a long time and only stay briefly in a light idle state. However, reducing voltage along with clock rate can change those tradeoffs.\n\nA related-but-opposite technique is overclocking, whereby processor performance is increased by ramping the processor's (dynamic) frequency beyond the manufacturer's design specifications.\n\nOne major difference between the two is that in modern PC systems overclocking is mostly done over the Front Side Bus (mainly because the multiplier is normally locked), but dynamic frequency scaling is done with the multiplier. Moreover, overclocking is often static, while dynamic frequency scaling is always dynamic. Software can often incorporate overclocked frequencies into the frequency scaling algorithm, if the chip degradation risks are allowable.\n\nIntel's CPU throttling technology, SpeedStep, is used in its mobile and desktop CPU lines.\n\nAMD employs two different CPU throttling technologies. AMD's Cool'n'Quiet technology is used on its desktop and server processor lines. The aim of Cool'n'Quiet is not to save battery life, as it is not used in AMD's mobile processor line, but instead with the purpose of producing less heat, which in turn allows the system fan to spin down to slower speeds, resulting in cooler and quieter operation, hence the name of the technology. AMD's PowerNow! CPU throttling technology is used in its mobile processor line, though some supporting CPUs like the AMD K6-2+ can be found in desktops as well.\n\nVIA Technologies processors use a technology named LongHaul (PowerSaver), while Transmeta's version was called LongRun.\n\nThe 36-processor AsAP 1 chip is among the first multi-core processor chips to support completely unconstrained clock operation (requiring only that frequencies are below the maximum allowed) including arbitrary changes in frequency, starts, and stops. The 167-processor AsAP 2 chip is the first multi-core processor chip which enables individual processors to make fully unconstrained changes to their own clock frequencies.\n\nAccording to the ACPI Specs, the C0 working state of a modern-day CPU can be divided into the so-called \"P\"-states (performance states) which allow clock rate reduction and \"T\"-states (throttling states) which will further throttle down a CPU (but not the actual clock rate) by inserting STPCLK (stop clock) signals and thus omitting duty cycles.\n\nAMD PowerTune and AMD ZeroCore Power are dynamic frequency scaling technologies for GPUs.\n\nPower Saving Technologies:\nPerformance Boosting Technologies:\n"}
{"id": "40506832", "url": "https://en.wikipedia.org/wiki?curid=40506832", "title": "Edison and Swan Electric Light Company", "text": "Edison and Swan Electric Light Company\n\nThe Edison and Swan Electric Light Company Limited was a manufacturer of incandescent lamp bulbs and other electrical goods. It was formed in 1883 with the name Edison & Swan United Electric Light Company with the merger of the Swan United Electric Company and the Edison Electric Light Company.\n\nThomas Edison established the Edison Electric Light Company in 1878. Joseph Swan established the Swan United Electric Light Company in 1881. In 1882 the American Edison Company of Thomas Edison sued Swan, claiming infringement of Edison's U.S. patent of 1879; however, they never made it to court and instead negotiated a merger with Swan's company. The lamps sold in Britain were almost entirely to Swan's design, excepting the filaments. From 1887 or earlier Sir Ambrose Fleming was an adviser to the company, and conducted research at Ponders End.\n\nThe company had offices at 155 Charing Cross Road, London, and factories in Brimsdown, Ponders End and Sunderland. In 1928, the company was acquired by Associated Electrical Industries. In 1956, a new cathode ray tube plant was opened in Sunderland. The company was renamed Siemens Ediswan following the takeover of Siemens Brothers by AEI in 1957. In 1964, AEI merged its lamp and radio valve manufacturing interests with those of Thorn Electrical Industries to form British Lighting Industries Ltd.\n\nEdison Swan (or later Siemens Edison Swan) produced a wide range of vacuum tubes and cathode ray tubes under the names \"Ediswan\" or \"Mazda\" and the 1964 Mazda Valve Data Book claimed: \"Professor Sir. Ambrose Fleming... was Technical Consultant to the Edison Swan Company at the time. It was this close co-operation between University and Factory which resulted in the first radio valve in the world.\"\n\n\n"}
{"id": "533487", "url": "https://en.wikipedia.org/wiki?curid=533487", "title": "Energy development", "text": "Energy development\n\nEnergy development is the field of activities focused on obtaining sources of energy from natural resources. These activities include production of conventional, alternative and renewable sources of energy, and for the recovery and reuse of energy that would otherwise be wasted. Energy conservation and efficiency measures reduce the demand for energy development, and can have benefits to society with improvements to environmental issues.\n\nSocieties use energy for transportation, manufacturing, illumination, heating and air conditioning, and communication, for industrial, commercial, and domestic purposes. Energy resources may be classified as primary resources, where the resource can be used in substantially its original form, or as secondary resources, where the energy source must be converted into a more conveniently usable form. Non-renewable resources are significantly depleted by human use, whereas renewable resources are produced by ongoing processes that can sustain indefinite human exploitation.\n\nThousands of people are employed in the energy industry. The conventional industry comprises the petroleum industry, the natural gas industry, the electrical power industry, and the nuclear industry. New energy industries include the renewable energy industry, comprising alternative and sustainable manufacture, distribution, and sale of alternative fuels.\n\nEnergy resources may be classified as primary resources, suitable for end use without conversion to another form, or secondary resources, where the usable form of energy required substantial conversion from a primary source. Examples of primary energy resources are wind power, solar power, wood fuel, fossil fuels such as coal, oil and natural gas, and uranium. Secondary resources are those such as electricity, hydrogen, or other synthetic fuels.\n\nAnother important classification is based on the time required to regenerate an energy resource. \"Renewable\" resources are those that recover their capacity in a time significant by human needs. Examples are hydroelectric power or wind power, when the natural phenomena that are the primary source of energy are ongoing and not depleted by human demands. Non-renewable resources are those that are significantly depleted by human usage and that will not recover their potential significantly during human lifetimes. An example of a non-renewable energy source is coal, which does not form naturally at a rate that would support human use.\n\nFossil fuel (\"primary non-renewable fossil\") sources burn coal or hydrocarbon fuels, which are the remains of the decomposition of plants and animals. There are three main types of fossil fuels: coal, petroleum, and natural gas. Another fossil fuel, liquefied petroleum gas (LPG), is principally derived from the production of natural gas. Heat from burning fossil fuel is used either directly for space heating and process heating, or converted to mechanical energy for vehicles, industrial processes, or electrical power generation. These fossil fuels are part of the carbon cycle and thus allow stored solar energy to be used today.\n\nThe use of fossil fuels in the 18th and 19th Century set the stage for the Industrial Revolution.\n\nFossil fuels make up the bulk of the world's current primary energy sources. In 2005, 81% of the world's energy needs was met from fossil sources. The technology and infrastructure already exist for the use of fossil fuels. Liquid fuels derived from petroleum deliver a great deal of usable energy per unit of weight or volume, which is advantageous when compared with lower energy density sources such as a battery. Fossil fuels are currently economical for decentralised energy use.\n\nEnergy dependence on imported fossil fuels creates energy security risks for dependent countries. Oil dependence in particular has led to war, funding of radicals, monopolization, and socio-political instability.\n\nFossil fuels are non-renewable resources, which will eventually decline in production and become exhausted. While the processes that created fossil fuels are ongoing, fuels are consumed far more quickly than the natural rate of replenishment. Extracting fuels becomes increasingly costly as society consumes the most accessible fuel deposits. Extraction of fossil fuels results in environmental degradation, such as the strip mining and mountaintop removal of coal.\n\nFuel efficiency is a form of thermal efficiency, meaning the efficiency of a process that converts chemical potential energy contained in a carrier fuel into kinetic energy or work. The fuel economy is the energy efficiency of a particular vehicle, is given as a ratio of distance travelled per unit of fuel consumed. Weight-specific efficiency (efficiency per unit weight) may be stated for freight, and passenger-specific efficiency (vehicle efficiency per passenger). The inefficient atmospheric combustion (burning) of fossil fuels in vehicles, buildings, and power plants contributes to urban heat islands.\n\nConventional production of oil has peaked, conservatively, between 2007 and 2010. In 2010, it was estimated that an investment in non-renewable resources of $8 trillion would be required to maintain current levels of production for 25 years. In 2010, governments subsidized fossil fuels by an estimated $500 billion a year. Fossil fuels are also a source of greenhouse gas emissions, leading to concerns about global warming if consumption is not reduced.\n\nThe combustion of fossil fuels leads to the release of pollution into the atmosphere. The fossil fuels are mainly carbon compounds. During combustion, carbon dioxide is released, and also nitrogen oxides, soot and other fine particulates. Man-made carbon dioxide according to the IPCC contributes to global warming.\nOther emissions from fossil fuel power station include sulfur dioxide, carbon monoxide (CO), hydrocarbons, volatile organic compounds (VOC), mercury, arsenic, lead, cadmium, and other heavy metals including traces of uranium.\n\nA typical coal plant generates billions of kilowatt hours per year.\n\nNuclear power is the use of nuclear fission to generate useful heat and electricity. Fission of uranium produces nearly all economically significant nuclear power. Radioisotope thermoelectric generators form a very small component of energy generation, mostly in specialized applications such as deep space vehicles.\n\nNuclear power plants, excluding naval reactors, provided about 5.7% of the world's energy and 13% of the world's electricity in 2012.\n\nIn 2013, the IAEA report that there are 437 operational nuclear power reactors, in 31 countries, although not every reactor is producing electricity. In addition, there are approximately 140 naval vessels using nuclear propulsion in operation, powered by some 180 reactors. As of 2013, attaining a net energy gain from sustained nuclear fusion reactions, excluding natural fusion power sources such as the Sun, remains an ongoing area of international physics and engineering research. More than 60 years after the first attempts, commercial fusion power production remains unlikely before 2050.\nThere is an ongoing debate about nuclear power. Proponents, such as the World Nuclear Association, the IAEA and Environmentalists for Nuclear Energy contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions. Opponents, such as Greenpeace International and NIRS, contend that nuclear power poses many threats to people and the environment.\nNuclear power plant accidents include the Chernobyl disaster (1986), Fukushima Daiichi nuclear disaster (2011), and the Three Mile Island accident (1979). There have also been some nuclear submarine accidents. In terms of lives lost per unit of energy generated, analysis has determined that nuclear power has caused less fatalities per unit of energy generated than the other major sources of energy generation. Energy production from coal, petroleum, natural gas and hydropower has caused a greater number of fatalities per unit of energy generated due to air pollution and energy accident effects. However, the economic costs of nuclear power accidents is high, and meltdowns can take decades to clean up. The human costs of evacuations of affected populations and lost livelihoods is also significant.\n\nComparing Nuclear's \"latent\" cancer deaths, such as cancer with other energy sources \"immediate\" deaths per unit of energy generated(GWeyr). This study does not include fossil fuel related cancer and other indirect deaths created by the use of fossil fuel consumption in its \"severe accident\" classification, which would be an accident with more than 5 fatalities.\n\nNuclear power is a low carbon power generation method of producing electricity, with an analysis of the literature on its total life cycle emission intensity finding that it is similar to renewable sources in a comparison of greenhouse gas(GHG) emissions per unit of energy generated. Since the 1970s, nuclear fuel has displaced about 64 gigatonnes of carbon dioxide equivalent(GtCO2-eq) greenhouse gases, that would have otherwise resulted from the burning of oil, coal or natural gas in fossil-fuel power stations.\nAs of 2012, according to the IAEA, worldwide there were 68 civil nuclear power reactors under construction in 15 countries, approximately 28 of which in the People's Republic of China (PRC), with the most recent nuclear power reactor, as of May 2013, to be connected to the electrical grid, occurring on February 17, 2013 in Hongyanhe Nuclear Power Plant in the PRC. In the United States, two new Generation III reactors are under construction at Vogtle. U.S. nuclear industry officials expect five new reactors to enter service by 2020, all at existing plants. In 2013, four aging, uncompetitive, reactors were permanently closed.\n\nJapan's 2011 Fukushima Daiichi nuclear accident, which occurred in a reactor design from the 1960s, prompted a rethink of nuclear safety and nuclear energy policy in many countries. Germany decided to close all its reactors by 2022, and Italy has banned nuclear power. Following Fukushima, in 2011 the International Energy Agency halved its estimate of additional nuclear generating capacity to be built by 2035.\n\nRecent experiments in extraction of uranium use polymer ropes that are coated with a substance that selectively absorbs uranium from seawater. This process could make the considerable volume of uranium dissolved in seawater exploitable for energy production. Since ongoing geologic processes carry uranium to the sea in amounts comparable to the amount that would be extracted by this process, in a sense the sea-borne uranium becomes a sustainable resource.\n\nThe economics of new nuclear power plants is a controversial subject, since there are diverging views on this topic, and multibillion-dollar investments ride on the choice of an energy source. Nuclear power plants typically have high capital costs for building the plant, but low direct fuel costs.\n\nIn recent years there has been a slowdown of electricity demand growth and financing has become more difficult, which affects large projects such as nuclear reactors, with very large upfront costs and long project cycles which carry a large variety of risks. In Eastern Europe, a number of long-established projects are struggling to find finance, notably Belene in Bulgaria and the additional reactors at Cernavoda in Romania, and some potential backers have pulled out. Where cheap gas is available and its future supply relatively secure, this also poses a major problem for nuclear projects.\n\nAnalysis of the economics of nuclear power must take into account who bears the risks of future uncertainties. To date all operating nuclear power plants were developed by state-owned or regulated utility monopolies where many of the risks associated with construction costs, operating performance, fuel price, and other factors were borne by consumers rather than suppliers. Many countries have now liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.\n\nTwo of the four EPRs under construction (in Finland and France) are significantly behind schedule and substantially over cost. Following the 2011 Fukushima Daiichi nuclear disaster, costs are likely to go up for currently operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats. While first of their kind designs, such as the EPRs under construction are behind schedule and over-budget, of the seven South Korean APR-1400s presently under construction worldwide, two are in S.Korea at the Hanul Nuclear Power Plant and four are at the largest nuclear station construction project in the world as of 2016, in the United Arab Emirates at the planned Barakah nuclear power plant. The first reactor, Barakah-1 is 85% completed and on schedule for grid-connection during 2017.\n\nRenewable energy is generally defined as energy that comes from resources which are naturally replenished on a human timescale such as sunlight, wind, rain, tides, waves and geothermal heat. Renewable energy replaces conventional fuels in four distinct areas: electricity generation, hot water/space heating, motor fuels, and rural (off-grid) energy services.\n\nAbout 16% of global final energy consumption presently comes from renewable resources, with 10% of all energy from traditional biomass, mainly used for heating, and 3.4% from hydroelectricity. New renewables (small hydro, modern biomass, wind, solar, geothermal, and biofuels) account for another 3% and are growing rapidly. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. National renewable energy markets are projected to continue to grow strongly in the coming decade and beyond. Wind power, for example, is growing at the rate of 30% annually, with a worldwide installed capacity of 282,482 megawatts (MW) at the end of 2012.\n\nRenewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency is resulting in significant energy security, climate change mitigation, and economic benefits. In international public opinion surveys there is strong support for promoting renewable sources such as solar power and wind power.\n\nWhile many renewable energy projects are large-scale, renewable technologies are also suited to rural and remote areas and developing countries, where energy is often crucial in human development. United Nations' Secretary-General Ban Ki-moon has said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.\n\nHydroelectricity is electric power generated by hydropower; the force of falling or flowing water. In 2015 hydropower generated 16.6% of the world's total electricity and 70% of all renewable electricity and is expected to increase about 3.1% each year for the next 25 years.\n\nHydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010. China is the largest hydroelectricity producer, with 721 terawatt-hours of production in 2010, representing around 17 percent of domestic electricity use. There are now three hydroelectricity plants larger than 10 GW: the Three Gorges Dam in China, Itaipu Dam across the Brazil/Paraguay border, and Guri Dam in Venezuela.\n\nThe cost of hydroelectricity is relatively low, making it a competitive source of renewable electricity. The average cost of electricity from a hydro plant larger than 10 megawatts is 3 to 5 U.S. cents per kilowatt-hour. Hydro is also a flexible source of electricity since plants can be ramped up and down very quickly to adapt to changing energy demands. However, damming interrupts the flow of rivers and can harm local ecosystems, and building large dams and reservoirs often involves displacing people and wildlife. Once a hydroelectric complex is constructed, the project produces no direct waste, and has a considerably lower output level of the greenhouse gas carbon dioxide than fossil fuel powered energy plants.\n\nWind power harnesses the power of the wind to propel the blades of wind turbines. These turbines cause the rotation of magnets, which creates electricity. Wind towers are usually built together on wind farms. There are offshore and onshore wind farms. Global wind power capacity has expanded rapidly to 336 GW in June 2014, and wind energy production was around 4% of total worldwide electricity usage, and growing rapidly.\n\nWind power is widely used in Europe, Asia, and the United States. Several countries have achieved relatively high levels of wind power penetration, such as 21% of stationary electricity production in Denmark, 18% in Portugal, 16% in Spain, 14% in Ireland, and 9% in Germany in 2010. By 2011, at times over 50% of electricity in Germany and Spain came from wind and solar power. As of 2011, 83 countries around the world are using wind power on a commercial basis.\n\nMany of the world's largest onshore wind farms are located in the United States, China, and India. Most of the world's largest offshore wind farms are located in Denmark, Germany and the United Kingdom. The two largest offshore wind farm are currently the 630 MW London Array and Gwynt y Môr.\n\nSolar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, solar photovoltaics, solar thermal electricity, solar architecture and artificial photosynthesis.\n\nSolar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert and distribute solar energy. Active solar techniques include the use of photovoltaic panels and solar thermal collectors to harness the energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air.\n\nIn 2011, the International Energy Agency said that \"the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries’ energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared\". More than 100 countries use solar PV.\n\nPhotovoltaics (PV) is a method of generating electrical power by converting solar radiation into direct current electricity using semiconductors that exhibit the photovoltaic effect. Photovoltaic power generation employs solar panels composed of a number of solar cells containing a photovoltaic material. Materials presently used for photovoltaics include monocrystalline silicon, polycrystalline silicon, amorphous silicon, cadmium telluride, and copper indium gallium selenide/sulfide. Due to the increased demand for renewable energy sources, the manufacturing of solar cells and photovoltaic arrays has advanced considerably in recent years.\n\nSolar photovoltaics is a sustainable energy source. By the end of 2011, a total of 71.1 GW had been installed, sufficient to generate 85 TWh/year. And by end of 2012, the 100 GW installed capacity milestone was achieved. Solar photovoltaics is now, after hydro and wind power, the third most important renewable energy source in terms of globally installed capacity. In 2016, after another year of rapid growth, solar generated 1.3% of global power.\n\nDriven by advances in technology and increases in manufacturing scale and sophistication, the cost of photovoltaics has declined steadily since the first solar cells were manufactured, and the levelised cost of electricity (LCOE) from PV is competitive with conventional electricity sources in an expanding list of geographic regions. Net metering and financial incentives, such as preferential feed-in tariffs for solar-generated electricity, have supported solar PV installations in many countries. The Energy Payback Time (EPBT), also known as \"energy amortization\", depends on the location's annual solar insolation and temperature profile, as well as on the used type of PV-technology. For conventional crystalline silicon photovoltaics, the EPBT is higher than for thin-film technologies such as CdTe-PV or CPV-systems. Moreover, the payback time decreased in the recent years due to a number of improvements such as solar cell efficiency and more economic manufacturing processes. As of 2014, photovoltaics recoup on average the energy needed to manufacture them in 0.7 to 2 years. This results in about 95% of net-clean energy produced by a solar rooftop PV system over a 30-year life-time. Installations may be ground-mounted (and sometimes integrated with farming and grazing) or built into the roof or walls of a building (either building-integrated photovoltaics or simply rooftop).\n\nA biofuel is a fuel that contains energy from geologically recent carbon fixation. These fuels are produced from living organisms. Examples of this carbon fixation occur in plants and microalgae. These fuels are made by a biomass conversion (biomass refers to recently living organisms, most often referring to plants or plant-derived materials). This biomass can be converted to convenient energy containing substances in three different ways: thermal conversion, chemical conversion, and biochemical conversion. This biomass conversion can result in fuel in solid, liquid, or gas form. This new biomass can be used for biofuels. Biofuels have increased in popularity because of rising oil prices and the need for energy security.\n\nBioethanol is an alcohol made by fermentation, mostly from carbohydrates produced in sugar or starch crops such as corn or sugarcane. Cellulosic biomass, derived from non-food sources, such as trees and grasses, is also being developed as a feedstock for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the USA and in Brazil. Current plant design does not provide for converting the lignin portion of plant raw materials to fuel components by fermentation.\n\nBiodiesel is made from vegetable oils and animal fats. Biodiesel can be used as a fuel for vehicles in its pure form, but it is usually used as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. However, research is underway on producing renewable fuels from decarboxylation\n\nIn 2010, worldwide biofuel production reached 105 billion liters (28 billion gallons US), up 17% from 2009, and biofuels provided 2.7% of the world's fuels for road transport, a contribution largely made up of ethanol and biodiesel. Global ethanol fuel production reached 86 billion liters (23 billion gallons US) in 2010, with the United States and Brazil as the world's top producers, accounting together for 90% of global production. The world's largest biodiesel producer is the European Union, accounting for 53% of all biodiesel production in 2010. As of 2011, mandates for blending biofuels exist in 31 countries at the national level and in 29 states or provinces. The International Energy Agency has a goal for biofuels to meet more than a quarter of world demand for transportation fuels by 2050 to reduce dependence on petroleum and coal.\n\nGeothermal energy is thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. The geothermal energy of the Earth's crust originates from the original formation of the planet (20%) and from radioactive decay of minerals (80%). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective \"geothermal\" originates from the Greek roots \"γη (ge)\", meaning earth, and \"θερμος (thermos)\", meaning hot.\n\nEarth's internal heat is thermal energy generated from radioactive decay and continual heat loss from Earth's formation. Temperatures at the core-mantle boundary may reach over 4000 °C (7,200 °F). The high temperature and pressure in Earth's interior cause some rock to melt and solid mantle to behave plastically, resulting in portions of mantle convecting upward since it is lighter than the surrounding rock. Rock and water is heated in the crust, sometimes up to 370 °C (700 °F).\n\nFrom hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times, but it is now better known for electricity generation. Worldwide, 11,400 megawatts (MW) of geothermal power is online in 24 countries in 2012. An additional 28 gigawatts of direct geothermal heating capacity is installed for district heating, space heating, spas, industrial processes, desalination and agricultural applications in 2010.\n\nGeothermal power is cost effective, reliable, sustainable, and environmentally friendly, but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have dramatically expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.\n\nThe Earth's geothermal resources are theoretically more than adequate to supply humanity's energy needs, but only a very small fraction may be profitably exploited. Drilling and exploration for deep resources is very expensive. Forecasts for the future of geothermal power depend on assumptions about technology, energy prices, subsidies, and interest rates. Pilot programs like EWEB's customer opt in Green Power Program show that customers would be willing to pay a little more for a renewable energy source like geothermal. But as a result of government assisted research and industry experience, the cost of generating geothermal power has decreased by 25% over the past two decades. In 2001, geothermal energy cost between two and ten US cents per kWh.\n\nMarine energy or marine power (also sometimes referred to as ocean energy, ocean power, or marine and hydrokinetic energy) refers to the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries.\n\nThe term marine energy encompasses both wave power i.e. power from surface waves, and tidal power i.e. obtained from the kinetic energy of large bodies of moving water. Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water.\nThe oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.\n\nThe incentive to use 100% renewable energy, for electricity, transport, or even total primary energy supply globally, has been motivated by global warming and other ecological as well as economic concerns. Renewable energy use has grown much faster than anyone anticipated. The Intergovernmental Panel on Climate Change has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply. Also, Professors S. Pacala and Robert H. Socolow have developed a series of \"stabilization wedges\" that can allow us to maintain our quality of life while avoiding catastrophic climate change, and \"renewable energy sources,\" in aggregate, constitute the largest number of their \"wedges.\" \n\nMark Z. Jacobson says producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs with a wind, solar, water system should be similar to today's energy costs.\n\nSimilarly, in the United States, the independent National Research Council has noted that \"sufficient domestic renewable resources exist to allow renewable electricity to play a significant role in future electricity generation and thus help confront issues related to climate change, energy security, and the escalation of energy costs … Renewable energy is an attractive option because renewable resources available in the United States, taken collectively, can supply significantly greater amounts of electricity than the total current or projected domestic demand.\" .\n\nCritics of the \"100% renewable energy\" approach include Vaclav Smil and James E. Hansen. Smil and Hansen are concerned about the variable output of solar and wind power, but Amory Lovins argues that the electricity grid can cope, just as it routinely backs up nonworking coal-fired and nuclear plants with working ones.\n\nGoogle spent $30 million on their RE<C project to develop renewable energy and stave off catastrophic climate change. The project was cancelled after concluding that a best-case scenario for rapid advances in renewable energy could only result in emissions 55 percent below the fossil fuel projections for 2050.\n\nAlthough increasing the efficiency of energy use is not energy development per se, it may be considered under the topic of energy development since it makes existing energy sources available to do work.\n\nEfficient energy use reduces the amount of energy required to provide products and services. For example, insulating a home allows a building to use less heating and cooling energy to maintain a comfortable temperature. Installing fluorescent lamps or natural skylights reduces the amount of energy required for illumination compared to incandescent light bulbs. Compact fluorescent lights use two-thirds less energy and may last 6 to 10 times longer than incandescent lights. Improvements in energy efficiency are most often achieved by adopting an efficient technology or production process.\n\nReducing energy use may save consumers money, if the energy savings offsets the cost of an energy efficient technology. Reducing energy use reduces emissions. According to the International Energy Agency, improved energy efficiency in buildings, industrial processes and transportation could reduce the world's energy needs in 2050 by one third, and help control global emissions of greenhouse gases.\n\nEnergy efficiency and renewable energy are said to be the \"twin pillars\" of sustainable energy policy. In many countries energy efficiency is also seen to have a national security benefit because it can be used to reduce the level of energy imports from foreign countries and may slow down the rate at which domestic energy resources are depleted.\n\nIt's been discovered \"that for OECD countries, wind, geothermal, hydro and nuclear have the lowest hazard rates among energy sources in production\".\n\nWhile new sources of energy are only rarely discovered or made possible by new technology, distribution technology continually evolves. The use of fuel cells in cars, for example, is an anticipated delivery technology. This section presents the various delivery technologies that have been important to historic energy development. They all rely in way on the energy sources listed in the previous section.\n\nCoal, petroleum and their derivatives are delivered by boat, rail, or road. Petroleum and natural gas may also be delivered by pipeline, and coal via a Slurry pipeline. Fuels such as gasoline and LPG may also be delivered via aircraft. Natural gas pipelines must maintain a certain minimum pressure to function correctly. The higher costs of ethanol transportation and storage are often prohibitive.\n\nElectricity grids are the networks used to transmit and distribute power from production source to end user, when the two may be hundreds of kilometres away. Sources include electrical generation plants such as a nuclear reactor, coal burning power plant, etc. A combination of sub-stations and transmission lines are used to maintain a constant flow of electricity. Grids may suffer from transient blackouts and brownouts, often due to weather damage. During certain extreme space weather events solar wind can interfere with transmissions. Grids also have a predefined carrying capacity or load that cannot safely be exceeded. When power requirements exceed what's available, failures are inevitable. To prevent problems, power is then rationed.\n\nIndustrialised countries such as Canada, the US, and Australia are among the highest per capita consumers of electricity in the world, which is possible thanks to a widespread electrical distribution network. The US grid is one of the most advanced, although infrastructure maintenance is becoming a problem. CurrentEnergy provides a realtime overview of the electricity supply and demand for California, Texas, and the Northeast of the US. African countries with small scale electrical grids have a correspondingly low annual per capita usage of electricity. One of the most powerful power grids in the world supplies power to the state of Queensland, Australia.\n\nWireless power transfer is a process whereby electrical energy is transmitted from a power source to an electrical load that does not have a built-in power source, without the use of interconnecting wires. Currently available technology is limited to short distances and relatively low power level.\n\nOrbiting solar power collectors would require wireless transmission of power to Earth. The proposed method involves creating a large beam of microwave-frequency radio waves, which would be aimed at a collector antenna site on the Earth. Formidable technical challenges exist to ensure the safety and profitability of such a scheme.\n\nEnergy storage is accomplished by devices or physical media that store energy to perform useful operation at a later time. A device that stores energy is sometimes called an accumulator.\n\nAll forms of energy are either potential energy (e.g. Chemical, gravitational, electrical energy, temperature differential, latent heat, etc.) or kinetic energy (e.g. momentum). Some technologies provide only short-term energy storage, and others can be very long-term such as power to gas using hydrogen or methane and the storage of heat or cold between opposing seasons in deep aquifers or bedrock. A wind-up clock stores potential energy (in this case mechanical, in the spring tension), a battery stores readily convertible chemical energy to operate a mobile phone, and a hydroelectric dam stores energy in a reservoir as gravitational potential energy. Ice storage tanks store ice (thermal energy in the form of latent heat) at night to meet peak demand for cooling. Fossil fuels such as coal and gasoline store ancient energy derived from sunlight by organisms that later died, became buried and over time were then converted into these fuels. Even food (which is made by the same process as fossil fuels) is a form of energy stored in chemical form.\n\nSince prehistory, when humanity discovered fire to warm up and roast food, through the Middle Ages in which populations built windmills to grind the wheat, until the modern era in which nations can get electricity splitting the atom. Man has sought endlessly for energy sources.\n\nExcept nuclear, geothermal and tidal, all other energy sources are from current solar isolation or from fossil remains of plant and animal life that relied upon sunlight. Ultimately, solar energy itself is the result of the Sun's nuclear fusion. Geothermal power from hot, hardened rock above the magma of the Earth's core is the result of the decay of radioactive materials present beneath the Earth's crust, and nuclear fission relies on man-made fission of heavy radioactive elements in the Earth's crust; in both cases these elements were produced in supernova explosions before the formation of the solar system.\n\nSince the beginning of the Industrial Revolution, the question of the future of energy supplies has been of interest. In 1865, William Stanley Jevons published \"The Coal Question\" in which he saw that the reserves of coal were being depleted and that oil was an ineffective replacement. In 1914, U.S. Bureau of Mines stated that the total production was . In 1956, Geophysicist M. King Hubbert deduces that U.S. oil production would peak between 1965 and 1970 and that oil production will peak \"within half a century\" on the basis of 1956 data. In 1989, predicted peak by Colin Campbell In 2004, OPEC estimated, with substantial investments, it would nearly double oil output by 2025\n\nThe environmental movement has emphasized sustainability of energy use and development. Renewable energy is sustainable in its production; the available supply will not be diminished for the foreseeable future - millions or billions of years. \"Sustainability\" also refers to the ability of the environment to cope with waste products, especially air pollution. Sources which have no direct waste products (such as wind, solar, and hydropower) are brought up on this point. With global demand for energy growing, the need to adopt various energy sources is growing. Energy conservation is an alternative or complementary process to energy development. It reduces the demand for energy by using it efficiently.\n\nSome observers contend that idea of \"energy independence\" is an unrealistic and opaque concept. The alternative offer of \"energy resilience\" is a goal aligned with economic, security, and energy realities. The notion of resilience in energy was detailed in the 1982 book \"Brittle Power: Energy Strategy for National Security\". The authors argued that simply switching to domestic energy would not be secure inherently because the true weakness is the interdependent and vulnerable energy infrastructure of the United States. Key aspects such as gas lines and the electrical power grid are centralized and easily susceptible to disruption. They conclude that a \"resilient energy supply\" is necessary for both national security and the environment. They recommend a focus on energy efficiency and renewable energy that is decentralized.\n\nIn 2008, former Intel Corporation Chairman and CEO Andrew Grove looked to energy resilience, arguing that complete independence is unfeasible given the global market for energy. He describes energy resilience as the ability to adjust to interruptions in the supply of energy. To that end, he suggests the U.S. make greater use of electricity. Electricity can be produced from a variety of sources. A diverse energy supply will be less affected by the disruption in supply of any one source. He reasons that another feature of electrification is that electricity is \"sticky\" – meaning the electricity produced in the U.S. is to stay there because it cannot be transported overseas. According to Grove, a key aspect of advancing electrification and energy resilience will be converting the U.S. automotive fleet from gasoline-powered to electric-powered. This, in turn, will require the modernization and expansion of the electrical power grid. As organizations such as The Reform Institute have pointed out, advancements associated with the developing smart grid would facilitate the ability of the grid to absorb vehicles \"en masse\" connecting to it to charge their batteries.\n\nExtrapolations from current knowledge to the future offer a choice of energy futures. Predictions parallel the Malthusian catastrophe hypothesis. Numerous are complex models based scenarios as pioneered by \"Limits to Growth\". Modeling approaches offer ways to analyze diverse strategies, and hopefully find a road to rapid and sustainable development of humanity. Short term energy crises are also a concern of energy development. Extrapolations lack plausibility, particularly when they predict a continual increase in oil consumption.\n\nEnergy production usually requires an energy investment. Drilling for oil or building a wind power plant requires energy. The fossil fuel resources that are left are often increasingly difficult to extract and convert. They may thus require increasingly higher energy investments. If investment is greater than the value of the energy produced by the resource, it is no longer an effective energy source. These resources are no longer an energy source but may be exploited for value as raw materials. New technology may lower the energy investment required to extract and convert the resources, although ultimately basic physics sets limits that cannot be exceeded.\n\nBetween 1950 and 1984, as the Green Revolution transformed agriculture around the globe, world grain production increased by 250%. The energy for the Green Revolution was provided by fossil fuels in the form of fertilizers (natural gas), pesticides (oil), and hydrocarbon fueled irrigation. The peaking of world hydrocarbon production (peak oil) may lead to significant changes, and require sustainable methods of production. One vision of a sustainable energy future involves all human structures on the earth's surface (i.e., buildings, vehicles and roads) doing artificial photosynthesis (using sunlight to split water as a source of hydrogen and absorbing carbon dioxide to make fertilizer) efficiently than plants.\n\nWith contemporary space industry's economic activity and the related private spaceflight, with the manufacturing industries, that go into Earth's orbit or beyond, delivering them to those regions will require further energy development. Researchers have contemplated space-based solar power for collecting solar power for use on Earth. Space-based solar power has been in research since the early 1970s. Space-based solar power would require construction of collector structures in space. The advantage over ground-based solar power is higher intensity of light, and no weather to interrupt power collection.\n\n\n\n\n\n\n\n\n"}
{"id": "624224", "url": "https://en.wikipedia.org/wiki?curid=624224", "title": "Getter", "text": "Getter\n\nA getter is a deposit of reactive material that is placed inside a vacuum system, for the purpose of completing and maintaining the vacuum. When gas molecules strike the getter material, they combine with it chemically or by absorption. Thus the getter removes small amounts of gas from the evacuated space.\n\nThe getter is usually a coating applied to a surface within the evacuated chamber.\n\nA vacuum is initially created by connecting a closed container to a vacuum pump. After achieving a vacuum, the container can be sealed, or the vacuum pump can be left running. Getters are especially important in sealed systems, such as vacuum tubes, including cathode ray tubes (CRTs), and vacuum insulated panels, which must maintain a vacuum for a long time. This is because the inner surfaces of the container release absorbed gases for a long time after the vacuum is established. The getter continually removes this residual gas as it is produced. Even in systems which are continually evacuated by a vacuum pump, getters are also used to remove residual gas, often to achieve a higher vacuum than the pump could achieve alone. Although it weighs almost nothing and has no moving parts, a getter is itself a vacuum pump.\n\nGetters cannot react permanently with inert gases, though some getters will adsorb them in a reversible fashion. Also, hydrogen is usually handled by adsorption rather than reaction.\n\nSmall amounts of gas within a vacuum tube will ionize, causing undesired conduction leading to major malfunction. Small amounts of gas within a vacuum insulated panel can greatly compromise its insulation value. Getters help to maintain the vacuum.\n\nTo avoid being contaminated by the atmosphere, the getter must be introduced into the vacuum system in an inactive form during assembly, and activated after evacuation. This is usually done by heat. Different types of getter use different ways of doing this:\n\nFlashed getters are prepared by arranging a reservoir of volatile and reactive material inside the vacuum system. Once the system is evacuated and sealed, the material is heated (usually by radio frequency induction heating). After evaporating, it is deposited as a coating on the interior surfaces of the system. Flashed getters (typically made with barium) are commonly used in vacuum tubes. The getter will usually be seen as a silvery metallic spot on the inside of the tube's glass envelope. Large transmission tubes and specialty systems often use more exotic getters, including aluminium, magnesium, calcium, sodium, strontium, caesium and phosphorus.\n\nIf the getter is exposed to atmospheric air (for example, if the tube breaks or develops a leak), it turns white and becomes useless. For this reason, flashed getters are only used in sealed systems. A functioning phosphorus getter looks very much like an oxidised metal getter, although it has an iridescent pink or orange appearance which oxidised metal getters lack. Phosphorus was frequently used before metallic getters were developed.\n\nIn systems which need to be opened to air for maintenance, a titanium sublimation pump provides similar functionality to flashed getters, but can be flashed repeatedly. Alternatively, nonevaporable getters may be used.\n\nThose unfamiliar with sealed vacuum devices, such as vacuum tubes/thermionic valves, high pressure sodium lamps or some types of metal-halide lamps, are often mistaken into thinking the flash getter deposit is caused as a result of the failure of the device. Note that contemporary high intensity discharge lamps tend to use non-evaporable getters rather than flash getters.\n\nThose familiar with such devices can often make qualitative assessments as to the hardness or quality of the vacuum within by the appearance of the flash getter deposit, a shiny deposit indicating a good vacuum. As the getter is used, the deposit often becomes thin and translucent particularly at the edges. It can take on a brownish-red semi translucent appearance and this indicates poor seals or extensive use of the device at elevated temperatures. A white deposit, usually of barium oxide indicates total failure of the seal on the vacuum system. as depicted in the fluorescent display module depicted above.\n\nThe typical flashed getter used in small vacuum tubes \"(seen in 12AX7 tube, top)\" consists of a ring shaped structure made from a long strip of nickel, bent up into a long, narrow trough and then folded into the ring shape with the trough opening facing upwards in the specific case depicted above. The trough is filled with a mixture of barium azide and powdered glass.\n\nDuring activation, whilst the bulb is still on the pump, an R.F. induction heating coil connected to a powerful R.F. oscillator operating in the 27 MHz or 40.68 MHz ISM band is positioned around the bulb in the plane of the ring. The coil acts as the primary of a transformer and the ring as a single shorted turn. Large R. F. currents flow in the ring, heating it. The coil is moved along the axis of the bulb so as not to overheat and melt the ring. Once the ring is heated the barium azide decomposes into barium vapor and nitrogen. The nitrogen is pumped out and the barium condenses on the bulb above the plane of the ring forming a mirror like deposit with a large surface area. The powdered glass in the ring melts and entraps any particles which could otherwise escape loose inside the bulb causing later problems. The barium combines with any free gas when activated and continues to act after the bulb is sealed off from the pump. During use, the internal electrodes and other parts of the tube get hot. This can cause adsorbed gases to be released from metallic parts, such as anodes (plates), grids or non metallic—but porous—parts, such as sintered ceramic parts. The gas is trapped on the large area of reactive barium on the bulb wall and removed from the tube.\n\n\"Non-evaporable getters\" which work at high temperature generally consist of a film of a special alloy, often primarily zirconium; the requirement is that the alloy materials must form a passivation layer at room temperature which disappears when heated.\nCommon alloys have names of the form St (Stabil) followed by a number:\nIn tubes used in electronics, the getter material coats plates within the tube which are heated in normal operation; when getters are used within more general vacuum systems, such as in semiconductor manufacturing, they are introduced as separate pieces of equipment in the vacuum chamber, and turned on when needed.\nDeposited and patterned getter material is being used in microelectronics packaging to provide an ultra-high vacuum in a sealed cavity, To enhance the getter pumping capacity, the activation temperature must be maximized, considering the process limitations . It is, of course, important not to heat the getter when the system is not already in a good vacuum.\n\n\n\n"}
{"id": "31924773", "url": "https://en.wikipedia.org/wiki?curid=31924773", "title": "Gobius ateriformis", "text": "Gobius ateriformis\n\nGobius ateriformis is a species of goby native to the Atlantic Ocean along the coast of Cape Verde where it occurs in tide pools to a depth of about and is harmless to humans. The species were described by Alberto Brito and Peter J. Miller in 2001.\n\nThis species can reach a length of TL.\n\n"}
{"id": "48482851", "url": "https://en.wikipedia.org/wiki?curid=48482851", "title": "Heladhanavi Power Station", "text": "Heladhanavi Power Station\n\nThe Heladhanavi Power Station is a thermal power station in Puttalam, Sri Lanka. The fuel oil-run power station was commissioned in August 2004, and was operated by , a subsidiary of Hemas Holdings. It was decommissioned in 2015 by request from the Ministry of Power and Energy. The power station consisted of six generation units of each.\n\n"}
{"id": "18298475", "url": "https://en.wikipedia.org/wiki?curid=18298475", "title": "International Journal of River Basin Management", "text": "International Journal of River Basin Management\n\nInternational Journal of River Basin Management is a quarterly academic journal issued for the first time during the Third World Water Forum in Kyoto, March 2003. It is published in print and electronic format by Taylor & Francis, on behalf of the International Association of Hydraulic Engineering and Research, the International Association of Hydrological Sciences and the International Network of Basin Organizations.\n\n\n"}
{"id": "58650000", "url": "https://en.wikipedia.org/wiki?curid=58650000", "title": "Jaan Kalviste", "text": "Jaan Kalviste\n\nJaan Kalviste (3 April 1898 – 15 June 1936) was an Estonian chemist, mineralogist, educator, and translator.\n\nJaan Kalviste was born Jaan Kranig on Mikko farm in the small village of Läste in present-day Lääne-Viru County to railway worker Ado Kranig and his wife Kadri (\"née\" Kuulmata). He was the second eldest of five siblings. He attended primary school in rural Lehtse Parish before studying at secondary school in Tallinn.\n\nDuring World War I he was conscripted and served a year in the Imperial Russian Army, then enlisted in the Estonian Land Forces at age twenty and fought in the Estonian War of Independence.\n\nFollowing the end of the war, he enrolled at the University of Tartu in 1920; graduating with a master's degree in chemistry in 1925 with the thesis \"Investigation of Alkyl Carbonate Constants\". Kalviste was a founding member of Students' Society Raimla (\"Üliõpilaste Selts Raimla\", or \"ÜS Raimla\").\n\nIn 1926, he relocated to France as a scholarship holder and received his Doctor of Science degree from the University of Paris in 1929 following the publication of his thesis \"Contribution to the Development of Complexes of Oxalics and Carbonics in Trivalent Cobalt\" by Masson publishing house.\n\nKalviste returned to Estonia in 1929 and taught chemistry and minerology at the University of Tartu as a docent until 1933. From 1933, he worked as a senior chemist at the Kohtla-Järve Oil Manufactory where he experimented with the study of oil shale products (phenols, gasoline, etc.) using spectrometric methods and in photochemistry. In 1935, he changed his surname from Kranig to Kalviste. In 1936, he worked as a chemist of the State Oil Shale Industry Laboratory and concurrently as a teacher at the Virumaa Mining School in Jõhvi.\n\nFluent in several languages, Jaan Kalviste translated mathematician Henri Poincaré's 1902 book \"Science and Hypothesis\" from French into Estonian (\"Teadus ja hüpotees\") in 1936.\n\nOn 15 June 1936, Jaan Kalviste was part of a group of approximately ten chemists at a seminar organized at the laboratory of the Männiku military ammunition stores in the Tallinn district of Nõmme when a massive explosion occurred, destroying the entire site and starting a blaze in the nearby heath and pine forest. Kalviste was among the 63 individuals killed in the explosion; the cause of which has, to date, never been fully determined. He was 38-years-old.\n\nKalviste was buried at the Rahumäe cemetery in Tallinn.\n\nJaan Kalviste was married to Alma Ennok on 21 February 1931 and had two sons, Aavo and Jüri, who were both under the age of five when he was killed. Following Kalviste's death, his widow Alma emigrated to the United States in 1949 and later remarried.\n"}
{"id": "5653446", "url": "https://en.wikipedia.org/wiki?curid=5653446", "title": "Johannesburg Declaration", "text": "Johannesburg Declaration\n\nThe Johannesburg Declaration on Sustainable Development was adopted at the World Summit on Sustainable Development (WSSD), sometimes referred to as Earth Summit 2002, at which the Plan of Implementation of the World Summit on Sustainable Development was also agreed upon.\n\nThe Johannesburg Declaration builds on earlier declarations made at the United Nations Conference on the Human Environment at Stockholm in 1972, and the Earth Summit in Rio de Janeiro in 1992. While committing the nations of the world to sustainable development, it also includes substantial mention of multilateralism as the path forward.\n\nIn terms of the political commitment of parties, the Declaration is a more general statement than the Rio Declaration. It is an agreement to focus particularly on \"the worldwide conditions that pose severe threats to the sustainable development of our people, which include: chronic hunger; malnutrition; foreign occupation; armed conflict; illicit drug problems; organized crime; corruption; natural disasters; illicit arms trafficking; trafficking in persons; terrorism; intolerance and incitement to racial, ethnic, religious and other hatreds; xenophobia; and endemic, communicable and chronic diseases, in particular HIV/AIDS, malaria and tuberculosis.\" Johannesburg Declaration 19. \n"}
{"id": "26175211", "url": "https://en.wikipedia.org/wiki?curid=26175211", "title": "Kashima Power Station", "text": "Kashima Power Station\n\nThe facility operates with an installed capacity of 5,660 MW, making it the largest fossil-fueled power station in the world. \nThe plant includes four oil-fired steam turbines rated at 600 MW, two oil-fired steam turbines rated at 1,000 MW, and three advanced combined cycle gas turbines rated at 420 MW added in 2014. \nAs of April 2016, the four oil-fired 600 MW turbines have been suspended indefinitely.\n\n"}
{"id": "26781338", "url": "https://en.wikipedia.org/wiki?curid=26781338", "title": "Kurakhove Power Station", "text": "Kurakhove Power Station\n\nKurakhove Power Station (also: Kurakhivska TES or Kurakhovskaya TES, ) is a thermal power plant on Volycha river from Kurakhove in Donetsk Oblast, Ukraine.\n\nThe first Kurakhove power station went into service on July 6, 1941, but was soon rendered inoperable by German troops during World War II. On October 20, 1941 it was occupied by German troops, who destroyed it in their retraction in 1943. It was repaired soon thereafter and on August 12, 1946 it went into operation again. Further units were installed resulting in an output power of 400 MW. Between 1972 and 1975 7 new units, one with 200 MW, 6 with 210 MW went in service, which replaced the old one and which are after modernization in 2006 still in use. The chimney of the power plant is tall.\n\nKurakhovka power station is startpoint of a 330 kV-double-circuit power line of unusual design to Zaporozhye. The 330 kV-Line Donbass-Dnepr is a double-circuit 330 kV three-phase AC power line connecting the 330 kV-substation DD-330 in Zaporozhye with Kurakhove Power Station. The line, which was built in 1964, consists in most parts of its length of three parallel power lines, each consisting of pylons with two conductors. Hereby one three-phase AC system consists of the outermost power line and the conductor of the innermost power line, which is closest to it.\n\n"}
{"id": "47115467", "url": "https://en.wikipedia.org/wiki?curid=47115467", "title": "Kysinga Hydroelectric Power Station", "text": "Kysinga Hydroelectric Power Station\n\nThe Kysinga Hydroelectric Power Station () is a hydroelectric power station in the municipality of Rindal in Møre og Romsdal county, Norway. It is a run-of-river hydro power station utilizing a drop of on a tributary of the Surna River. Permission was granted for construction in 2009 and the plant came into operation in 2010. It is operated by Kysinga Kraft AS. It operates at an installed capacity of , with an average annual production of about 6.5 GWh.\n"}
{"id": "6188802", "url": "https://en.wikipedia.org/wiki?curid=6188802", "title": "Leader (spark)", "text": "Leader (spark)\n\nA leader is a hot, highly conductive channel of plasma that plays a critical part during dielectric breakdown within a long electric spark.\n\nWhen a gas is subjected to high voltage stress, the electric field is often quite non-uniform near one, or both, of the high voltage electrodes making up a spark gap. Breakdown initially begins with the formation of corona discharges near the electrode with the highest electrical stress. If the electrical field is further increased, longer length \"cold\" discharges (called \"streamers\" or \"burst corona\") sporadically form near the stressed electrode. Streamers attract multiple electron avalanches into a single channel, propagating forward quickly via photon emission which leads to photoelectrons producing new avalanches. Streamers redistribute charge within the surrounding gas, temporarily forming regions of excess charge (space charges) in the regions surrounding the discharges.\n\nIf the electrical field is sufficiently high, the individual currents from multitudes of streamers combine to create a hot, highly conductive path that projects from the electrode, going some distance into the gap. The projecting channel of hot plasma is called a leader, and it can have an electrical conductivity approaching that of an electric arc. The leader effectively \"projects\" the electrical field from the nearby electrode further into the gap, similar to introducing a short length of wire into the gap. The tip of the conductive leader now forms a new region from which streamers can entend even further into the gap. As new streamer discharges feed the tip of the leader, the streamer currents help to keep the leader hot and conductive. Under sufficiently high voltages, the leader will continue to extend itself further into the gap, doing so in a series of jumps until the entire gap has been bridged. Although leaders are most often associated with the initial formative stages of a lightning stroke, they are characteristic of the development of all long sparks. In the case of a lightning leader, each extension (called a step leader) is typically 10 – 50 meters in length. The branching, visible blue-white discharges from a Tesla Coil are also examples of leaders.\n\n\n"}
{"id": "7389351", "url": "https://en.wikipedia.org/wiki?curid=7389351", "title": "MEMS thermal actuator", "text": "MEMS thermal actuator\n\nA MEMS thermal actuator is a micromechanical device that typically generates motion by thermal expansion amplification. A small amount of thermal expansion of one part of the device translates to a large amount of deflection of the overall device. Usually fabricated out of doped Single Crystal Silicon or Polysilicon as a complex compliant member, the increase in temperature can be achieved internally by electrical resistive heating or something by a heat source capable of locally introducing heat. Microfabricated thermal actuators can be integrated into micromotors.\n\n\n"}
{"id": "24322531", "url": "https://en.wikipedia.org/wiki?curid=24322531", "title": "MIS capacitor", "text": "MIS capacitor\n\nA MIS capacitor is a capacitor formed from a layer of metal, a layer of insulating material and a layer of semiconductor material. It gets its name from the initials of the metal-insulator-semiconductor structure. As with the MOS field-effect transistor structure, for historical reasons, this layer is also often referred to as a MOS capacitor, but this specifically refers to an oxide insulator material.\n\nThe maximum capacitance, \"C\" is calculated analogously to the plate capacitor:\n\nwhere :\n\nThe production method depends on materials used (it is even possible that polymers can be used as the insulator). We will consider an example of a MOS capacitor based on silicon and silicon dioxide. On the semiconductor substrate, a thin layer of oxide (silicon dioxide) is applied (by, for example, thermal oxidation, or chemical vapour deposition) and then coated with a metal.\n\nThis structure and thus a capacitor of this type is present in every MIS field-effect transistor, such as MOSFETs. For the steady reduction of the size of structures in microelectronics, the following facts are clear. From the formula above it follows that capacitance increases with ever thinner layers of insulation. For all MIS devices the insulation thickness cannot fall below a minimum of around 10 nm. Using thinner insulation than this leads to the occurrence of tunneling through the insulating material (dielectric). Due to this, the use of so-called high-k materials as the insulator material is being investigated (as of 2009).\n"}
{"id": "12631551", "url": "https://en.wikipedia.org/wiki?curid=12631551", "title": "MT Independența", "text": "MT Independența\n\nMT \"Independența\" (\"Independence\") was a large Romanian crude oil carrier. She collided in 1979 with a Greek freighter at the southern entrance of Bosphorus, Turkey, and exploded. She caught fire and grounded. Almost all of the tanker's crew members died. The wreck of the \"Independența\" burned for weeks, causing heavy air and sea pollution in the Istanbul area and the Sea of Marmara.\n\nMT \"Independența\" was a 1978–built Romanian-flagged crude oil carrier, the biggest ship of her country's commercial fleet at that time. She was long, had a beam of and a depth of .\n\nBy mid November 1979, the \"Independența\", carrying 94,000 tons (714,760 barrels) of crude oil from Es Sider, Libya to Constanța, Romania dropped anchor about 4 nautical miles off the Haydarpaşa Breakwater at the southern entrance of the Istanbul Strait. She was waiting for a maritime pilot for the guidance of her 19th passage through the strait. The Greek cargo ship M/V \"Evriali\" (10,000 DWT) was transporting 7,400 tons steel from Mariupol, Ukraine (formerly Zhdanov) to Italy, and had already passed the strait southwards.\n\nEarly in the morning of 15 November at 04:35, Evriali collided with Independenta hitting the Romanian ship between the number 3 and 4 tanks, on the starboard side. The Turkish pilot that helped Evriali navigate the strait, Dincer Sumerkan, got off the Greek ship earlier, instructing them to follow a course at 260 degrees right, however the Greek cargo ship took a course of 160 degrees left.\n\nThe sparks from the collision at 04:35 caused a big explosion on Independenta followed 3 minutes later by another explosion. A third explosion occurred at 04:47. The \"Independența\" ran aground half a mile off the Port of Haydarpaşa. 43 members of the tanker's crew lost their lives; only 3 survived the catastrophic accident. Almost all the victims lost their lives in the water when burning oil on the surface was driven towards the shore by the wind; the sailors who survived had jumped on the other side of the ship, against the wind, being later rescued by boats.\n\nThe Turkish Navy immediately attempted to extinguish the fire. However, these efforts had to be abandoned due to the intensity of the fire. The Director of the Sea of Marmara District took over the spill on 19 November, and the Navy withdrew.\n\nThe Greek captain of the Evriali, Alekos Adamopulos (29 years old at that time), was sentenced to 20 months in jail. The difference of time between his arrest and the 20 months, was converted into an $850 fine.\n\nThe Istanbul Strait remained closed for weeks. The wreck affected the area for some years until it was broken up and salvaged to a shipyard in Tuzla.\n\nThe exact cause of accident is not clear, even today. The Romanian shipping company \"Navrom\" claimed the insurance payout, amounting to some tens of millions of US dollars. This action resulted in a thorough inspection of the ship's wreck by independent survey teams employed by the insurance company from Lloyd's Register of Shipping, Japan. It is hinted that it was an inside job so that they could acquire more money.\n\nAfter the accident, the court charged the captain of the Greek ship, Alekos Adamopoulos, and the seven crew with being careless and negligent, disobeying international maritime regulations and jeopardizing security of Istanbul. He was sentenced to 20 months in prison, but was released in respect to his 7-months jail during the trial.\n\nFrom 17 to 27 November, there was slight leakage from the tanker. Another major explosion occurred on board the vessel on the night of 6 December at 10:40 p.m., which resulted in more oil spillage. The slick from the vessel drifted towards the Port of Haydarpaşa, and the booms across its entrance could not prevent approximately 50 tons of oil entering the harbor. The tanker continued to burn until 14 December.\n\nThe maximum accumulation of harmful particles in the air during the fire exceeded by four times the permissible limit set for human health. Heavy oil contamination formed on the surface of the sea and on the heavily built-up shores and the recreational beaches of the Sea of Marmara and the Istanbul Strait.\n\nIt was estimated that 30,000 tons of crude oil burned and that the remaining 64,000 tons spilled into the sea. Because of the rapid evaporation of the light components, the crude oil quickly sank to the bottom of the sea in an area approximately 5.5 km in diameter.\n\nM/T \"Independența\" was the first of a series of five Romanian supertankers which were constructed by Santierul Naval Constanța in the 1980s. The sister ships were as follows:\n\nM/T \"Unirea\" (Union) – broke up and sank at the beginning of the 1980s in Bulgarian waters of the Black Sea. Official reports claim that the accident was caused by a collision with a World War II mine. A different opinion (unofficial) came from some naval architects and marine engineers stating that the ship broke up due to incorrect ballasting (the ship had no cargo at the moment of the accident).\n\nM/T \"Biruința\" (Victory) – owned and managed by the then Romanian state owned Shipping Company (Navrom). In the 1990s the ship was passed to the Romanian private shipping company \"Petromin\", changing its name to M/T \"Iris Star\". Finally the ship was bought by the Romanian shipping company Histria Shipmanagement having its name changed again to M/T \"Histria Crown\". After an extensive refit in Keppel Shipyard (Singapore) finished in 2009, the ship was converted in into a FPSO (Floating production storage and offloading) and given a new name, Armada Perdana. It is still in use of the coast of Nigeria (Oyo Oil Field) as of 2018.\n\nM/T \"Libertatea\" (Liberty) – had the same history (including ownership) as her older sister M/T \"Biruinta\". The ship, known then as M/T \"Histria Prestige\", was broken up in 2005.\n\nM/T \"Pacea\" (Peace) – was never fully completed. At the end of the 1980s it was passed to Czechoslovakia as a part of Romania's foreign debt.\n\nThe sister ship of \"Independența\", the M/T \"Iris Star\" lost power due to engine failure during her passage through the Bosphorus on 27 July 2000, and drifted towards Kandilli point. There was no extensive damage reported.\n"}
{"id": "16234726", "url": "https://en.wikipedia.org/wiki?curid=16234726", "title": "Mesa Boogie Mark Series", "text": "Mesa Boogie Mark Series\n\nThe Mesa Boogie Mark Series is a series of guitar amplifier made by Mesa Engineering (more commonly known as \"Mesa/Boogie\"). Originally just referred to as \"Boogies,\" the product line took on the moniker \"Mark Series\" as newer revisions were put into production. The Mark Series amplifier was Mesa's flagship product until the introduction of the Rectifier series, and the amplifiers are very collectable.\n\nRandall Smith began Mesa/Boogie with a practical joke: he borrowed a Fender Princeton (a small 12-watt amplifier) from his friend, Barry Melton of Country Joe and the Fish, and \"hotrodded\" it by replacing the amplifier section with a powerful Fender Bassman amp and installing a 12-inch speaker instead of the original 10-inch. The resulting amplifier proved to be loud and successful, and Smith made more than 200 of these Princeton \"Boogies\"—a name allegedly provided by Carlos Santana, who is to have exclaimed \"Man, that little thing really boogies!\"\n\nA second important improvement was in developing an extra gain stage for the guitar input. Smith added an extra tube gain stage to the preamp, with three variable gain controls at different points in the circuit (this is now called a \"cascaded\" design), creating the first high-gain amplifier. He set about designing a guitar amplifier around the new principle, and in 1972 the Mark I was released.\n\nOne of the more notable amps in the series was built in 1977, with serial number A804: this is the amp built for Keith Richards, the first one in a long collaboration between Smith and the Rolling Stones, a collaboration which started somewhat inauspiciously when the Stones manager asked Smith for some free amps (\"We're the Rolling Stones; we don't pay for amps\"), and Smith refused. (Richards had played Santana's Boogie and decided he wanted one too.) Finally, Smith talked to Richards and they agreed that he would send them an amp, and that the Stones would pay for it or return it. Richards ended up using the amp for the El Mocambo show (as one of six), and the Stones, over the years, received and paid for over forty of Smith's amps.\n\nThe first Boogies are referred to as Mark I's, though they were not given this name until the Mark II was released. They were 60 or 100 watt combo amps with a 12-inch speaker, primarily Altec-Lansing 417-8H Series II. The Mark I had two channels: The \"Input 2\" channel, voiced like the Fender Bassman, and the high gain \"Input 1\" channel, which produced the overdriven \"Boogie lead\" sound used most notably by Carlos Santana on side 2 of Caravanserai, and by The Rolling Stones' Keith Richards and Ron Wood, who used the amps live and in the studio from 1977 until 1993. Examples of this amp in its original form and in good condition are sought after by collectors and guitar aficionados. Reverb was optional, and not present on many early Boogies. Later, Mark I models were available with reverb and/or graphic EQ.\n\nThe S.O.B. was introduced in the Mark II era. This was Mesa/Boogie's first attempt at having a reissue of the Mark I. It had two cascading gain inputs and its controls were Volume(gain) 1, Volume(gain) 2, Master, High, Middle, Low, Limit or Presence (depending on the version). No foot-switching available, however an A/B splitter pedal could be used to select input 1 or input 2 separately. There was also a reverb option which replaced the Middle knob with the reverb control knob. These amps had a point-to-point wired power section and had large transformers with great headroom, even in 50 watt trim. SOB chassis were shared with other heads, but had different front and rear plates. EQ, slider cutouts and other 1/4\" jack cutouts can be seen from inside the chassis.\n\nThe Mark II introduced channel footswitching. It was not referred to as the \"Mark IIA\" until the Mark IIB was issued. It was also available as a head (a standalone amplifier), which could be hooked up to a number of different speaker combinations, although a 1x12\" cabinet was the most common. The preamp gain on the Mark IIs occurs after the tone controls and so, according to Mesa/Boogie, the IIA has a \"tighter, more focused sound\" than the Mark I. The Mark IIA's control panel was extended from the Mark I's to include a separate master volume control for the lead mode, and various push/pull switches including Pull Bright, Pull Treble Shift, Pull Gain Boost, a separate Pull Bright for the lead mode, and of course, Pull Lead. The 1/4\" jack previously marked \"1\" was changed to just \"Input,\" and \"2\" was changed to \"Foot Switch.\" The Mark IIA was a great improvement over the Mark I, however it had a few major flaws that it received criticism for among collectors. The new footswitching system relied on a relay, which made an audible popping noise when switching modes. The reverb circuit was also noise-ridden on some models. The IIA and IIB, and some late-model Mark I amps, used a JFET-based device called fetron in place of the input stage 12AX7 (V1), and included a switch for configuring the amp for either Fetron or 12AX7 operation. The reason for using a fetron was to address some of the problems associated with microphonic 12AX7 tubes in a high-gain situation; its use was later discontinued as newer production tubes were able to withstand the extreme conditions within the amplifier.\n\nThe Mark IIB is credited as the first guitar amplifier with a tube-buffered effects loop. However, the loop was placed between two critical gain stages, and tended to overdrive some instrument level effects, and also caused volume pedals to act as remote gain controls for the lead mode. Mesa later implemented a mod that caused the effects loop to become more transparent, and smoothed out the lead channel, similar to the IIC+'s lead channel. More importantly, it marked the introduction of Mesa/Boogie's \"Simul-Class\" system, where two of the power tubes (always 6L6s) run in class AB pentode while the other two tubes (either 6L6s or EL34s) run in class A triode. In a simul-class amp, running all four tubes generates approximately 75 watts RMS of power; running only the class A tubes produces about 15 watts. Also available were non-simul-class Mark IIBs in both a 60 watt version and a 100 watt version that allowed shifting down to 60 watts by turning off a pair of power tubes.\n\nThe Mark IIB's front control panel is identical to that of the IIA. The two input jacks on the front panel are marked \"Input\" and \"Foot Switch.\" The front panels read Volume, Treble, Bass, Middle, Master, Lead Drive and Master. It has \"Pull Bright\" on the Volume, \"Pull Shift\" on the Treble, and \"Pull Bright\" on the Master. The Rear control panel was altered to accommodate the FX Send and Return jacks.\n\nThe Mark IIC finally remedied the two major problems of the IIA and the IIB: the previously noisy reverb circuit and a footswitching system that produced a popping noise when activated. The Mark IIC featured a quieter footswitching system based on optocouplers to reroute the signal, and a new mod to the reverb circuit. The reverb modification involved resistor swaps and a change in ground lead placement. That mod[ification] is still on the books of 'official' mods, which they send to their authorized techs; it runs about $50.\" Mesa/Boogie no longer does this modification at its own factory. The Mark IIC also featured a new Pull Bass shift on the front panel, which slightly extended the low frequencies in the preamp.\n\nThe Mark IIC+ was the last of the Mark II series and featured a more sensitive lead channel - due to its featuring a dual cascading drive stage - whereas the IIA and IIB had a single-stage drive circuit. The IIC+ also had an improved effects loop. Unlike earlier Mark II models, pedals configured for instrument-level input signal could be used without the amp's signal overloading their inputs. However, the volume pedal option on the Mark IIB cannot be implemented on Mark IIC+s.\n\nSome owners/dealers/sellers say the \"+\" refers to an amp having an EQ, but they are mistaken. The mistake may have originated in the mid 1980s, when Mesa/Boogie issued their Studio .22 model and then changed the name to Studio .22+, which featured improved wiring, etc. All the Mark II models could be made with EQ as an option, but not all of them did. A Mark IIC+ could, for example, refer to a 100 watt amp without EQ or reverb.\n\nOne can tell if a particular amp is a \"+\" by looking for a hand-written black \"+\" mark directly above where the power cord attaches to the back of the amp. Many dealers increase the price on a Mark IIC+ but often don't know anything about what the \"+\" means - they often don't even know where to find the \"+\" mark. Indeed, the mark itself can be forged. An owner can call Mesa/Boogie and ask them to check his or her serial number against their records. Mesa/Boogie only made about 1,400 Mark IIC amps before moving to the Mark IIC+.\nAnother cosmetic way of distinguishing a IIC from a IIC+ is the front panel. A IIC has the traditional \"Gain Boost\" pull switch integrated into the master volume, while a IIC+ replaced the switch with a Pull Deep bass booster. Some Coliseum series IIC+s retained the surplus \"Gain Boost\" faceplates, though. IIC+s upgraded from earlier IICs also retain their original Gain Boost faceplates. However, there are some very early \"transitional\" C+s (in the 133xx range) with gain boost on the panel, but it works the same as the pull Deep on the later versions.\n\nA practical, non-cosmetic method for determining whether a Mark IIC is a C+ is the \"Loop Test\":\n\n1) Plug your guitar into the Effects Return jack\n2) Switch to lead mode\n3) Turn the Lead Drive and Gain controls with a note ringing.\n4) If they have NO effect on the volume and sound you have a “+”.\n\nThis method should remove all doubt as to whether the preamp circuit board is a IIC or a IIC+.\n\nThe Mark IIC+ is currently the most coveted vintage Boogie, selling for twice its original price on average, because of its much praised \"Liquid Lead\" mode, and also its warm, clean rhythm mode.\n\nThe Mark III was launched by Mesa/Boogie in 1985. It introduced a third channel, a \"crunch\" rhythm sound right in between the rhythm and lead channels. This amp has a dual footswitch system: one footswitch alternates between the current rhythm mode and the lead mode, and the other selects either the clean rhythm mode or the crunch rhythm mode. The two rhythm modes share all of their controls, while the lead mode only shares the rhythm modes' tone stack, featuring independent gain and master volume controls. The physical switch for the crunch rhythm mode was implemented as a push/pull switch above the Middle frequency control. Most Mark IIIs have presence and reverb on the back (except for long chassis') unless not desired by the buyer; Graphic EQ was also optional all in either head or combo format.\n\nThe Mark III went through multiple revisions, similar to the Mark II. Each revision had a slightly different voicing, but identical functionality. Non-Simulclass versions of the Mark III came in either 60w RMS with two 6L6s or 60w/100w with four 6L6s in the power section. Mark IIIs contain either four or five 12AX7 tubes in the pre-amp section, depending on if they have the reverb option. Simul-class Mark IIIs usually contain two 6L6s in the inner sockets and two EL34s in the outer sockets for 15w/75w use.\n\nThese are distinguished by either the absence of a marking, a black dot, or a black marker stripe above the power cord entry. Early Black Stripes retained the same power transformer as the IIC+, which is easily distinguished by its larger physical size than the later-introduced Mark III transformer.\n\nEarly Black Stripes also re-used the faceplate from the Mark II series. This resulted in the pull function of the Master 1 knob being mislabelled as Gain Boost instead of the correct Pull Deep name. Furthermore, the pull function labels above the Bass and Middle knobs were hand-etched onto the face plate resulting in a slightly different look than the other labels on the faceplate.\n\nThe second revision was the \"Purple Stripe\" Mark III, which featured a purple marker stripe above the power cord. This amplifier was voiced with a more mellow lead and crunch modes, with slightly reduced gain.\n\nThe third revision was the \"Red Stripe\" Mark III which featured a red marker stripe above the power cord. The amplifier had increased gain over the purple stripe, and lead mode circuitry almost identical to the IIC+.\n\nThe fourth revision was the \"Blue Stripe\" Mark III which featured a blue marker stripe above the power cord. The amplifier was voiced so brightly, it is considered to be the most aggressive Mark Series Boogie ever introduced. The power amp was also altered to mirror that of the IIC+.\n\nThe final revision was the \"Green Stripe\" Mark III, which was only available in a Simul-Class format. It was identical to the Blue Stripe, except for the wiring of the Class A power amp tubes, which were switched to Pentode operation instead of Triode for a 10w RMS increase over previous Simul-Class amplifiers (15w/75w) making 25w/85w.\nMesa ultimately ended the Mark III's production in the company's largest marketing failure, since it overlapped with production of its successor, the Mark IV, which was introduced in 1990. Mark IIIs were still in steady production around 1994, and finally ceased as late as 1997, 11 years after its launch.\n\nThe Mark IV was launched by Mesa/Boogie in 1990 as a three-channel amp - with independent controls for all three channels, except bass and mid, which are the same for both Rhythm 1 (clean) and Rhythm 2 (crunch). The \"crunch\" channel is designed for use by hard rock and heavy metal rhythm guitarists. There were two versions of this amp. Mark IVs built from the start of production until about September 1993 are referred to as version A; amplifiers made from late 1993 until the end of production in 2008 are known as version B. Early Bs have an attached power cord, like the A version. Some differences: version A has no footswitch for reverb or stereo effects loop, and the lead channel is much like the Mark IIC+‘s. Version B has switchable reverb, a single switchable stereo effects loop, and an output to drive another power amp. Its voicings are altered slightly. Both versions are highly regarded; production of the Mark IV ceased in 2008.\n\nThe Mark V was introduced in early 2009. Much like its close cousin, the Triaxis Preamp, it features many voicings based on previous Mark Series amplifiers. It has three distinct channels, each with their own pre-gain three band tone stack, gain, master, and presence controls. Each channel also has three modes, which control several of the amplifier's 49 relays to mirror the circuit being modelled. The Mark V introduced a channel-assignable graphic EQ. Older Boogies were equipped with graphic equalizers, but these did not allow the same flexibility. Each channel has a toggle switch able to select between EQ active, off, or footswitched. Similar to the Express and F-series amplifiers, the graphic EQ also had channel-assignable contour knobs.\n\nThe Mark V - like its predecessor - comes standard in a Simul-Class format, but with a twist: early Simul-Class power amps were configured for SC-75 watt operation, or A-15 watt operation, or an increase of 10 watts when in pentode mode. The Mark V is biased warmer to produce an output of SC-90 watts, AB-45 watts, and Single-Ended A-10 watts, similar to the Lone Star. Channel-specific Multi-Watt toggles dictate the power amplifier's operation class.\n\nThe Mark 5:25 was introduced in 2014. It is a smaller, two-channel version of the Mark V. The output section contains two EL84 tubes which can be switched between 10 and 25 Watts. It also features a built-in CabClone which can be used to emulate a speaker cabinet while driving headphones for silent playing, or a direct-in (DI) box for recording or sound reinforcement applications.\n\nThe Mark 5:35 was introduced in 2015. It is based on the Mark 5:25 and features two channels. The output section contains four EL84 tubes which can be switched between 10, 25, and 35 Watts. Additional solo controls were added for independent volume switching. It is also available as a combo and a head. The combo is a similar size to the Mark I combo. It also features cab clone.\n\nThe JP-2C was introduced in 2016. It is a 3-channel 100-watt \"revival\" of the Mesa Mark IIC+ designed in part with Dream Theater guitarist John Petrucci, who is well-known for using a Mark IIC+ in the studio. The three channels are based upon the original channels of the Mark IIC+, but with two identical gain channels based on the IIC+'s lead channel. It also features two separate graphic EQs, which can be selected for each channel via mini toggles on each channel's front panel. It is also the first Mesa amp to feature MIDI connections, which can control the channel switching, as well as controlling the FX loop and graphic EQs. Much like the Mark 5:25, the JP-2C features a built-in CabClone. The amp also features a switch to drop the amp's wattage down to 60 watts for use in smaller venues or recording.\n"}
{"id": "25154834", "url": "https://en.wikipedia.org/wiki?curid=25154834", "title": "Naval museum complex Balaklava", "text": "Naval museum complex Balaklava\n\nNaval museum complex Balaklava (, Russian: Музей холодной войны, \"The Cold War Museum\", designation K-825) is an underground submarine base in Balaklava, Crimea, (originally known as \"Object 825 GTS\"). It was a top-secret military facility during the Cold War, located in Balaklava Bay.\n\nToday it serves as a museum and also houses a museum about the Crimean War.\n\nThe complex is built accordingly to withstand a category-I (nuclear yield of 100kt) nuclear explosion, and includes an underground network of water channels complete with a dry dock, repair shops, warehouses for torpedoes and other weapons. Additionally it could protect personnel from nuclear fallout. The complex is located in the mountain of Tavros, on both sides of which are exits. Caisson gates could be used if necessary to seal the entire complex. An exit to the open sea is provided on the northern side of the mountain. The holes in the rock are neatly covered with camouflage devices and networks.\n\nObject 825 GTS was intended to house, repair and maintain Project 613 and 633(known as Whiskey and Romeo-class respectively) submarines. The central water channel of the facility, at a length 602 meters, could accommodate up to 7 submarines if necessary, and up to 14 submarines of different classes in all water channels. The water channels have depths up to 8 meters, with widths ranging from 12 to 22 meters. The total area of all facilities in the complex is around 9600 m, while the total surface area of water stands at 5200. Equipment loading in peacetime was carried out on the pier, then conducted while watching out for the movements of spy satellites of possible military adversaries. A special tunnel is used for loading equipment into the base in wartime. The complex also includes a repair and technical base, codenamed Object 280, designed for storing and maintaining nuclear arsenal. Submarines could enter and exit the base completely submerged through its underwater access point. Temperature inside the base is kept around 15 degrees Celsius.\n\nThe Soviet Navy trained dolphins at this facility to attach underwater beacons and explosives to submarines and ships.\n\nIn the period after the Second World War, the two superpowers, the USSR and the US, stepped up their nuclear arsenal, threatening each other with pre-emptive strikes and retaliatory strikes. It was then when Joseph Stalin gave Lavrentiy Beria (who was responsible at that time for nuclear projects), a secret directive; to find a place where they could house submarines for a retaliatory nuclear strike. Several years of research pinpointed the quiet Balaklava as the location, and the city was immediately coded and got merged into the city of Sevastopol as a city district. Balaklava sits on a narrow winding inlet with a width of only 200–400 meters. This small inlet protects the city not only from storms, but also from reconnaissance as it is not visible from any angle from the open sea. Additionally, the site is close to Sevastopol, a major naval base still used by the Russian Navy's Black Sea Fleet today.\n\nIn 1957 a special construction department coded as No. 528 was created, which handled the construction of underground facilities. The construction of the underground complex lasted eight years, from 1953 to 1961. About 120 thousand tons of rock were removed from the Tavros mountain throughout the process. To ensure secrecy supplies were transported at night on a barge in the open sea. After the complex was closed in 1993, most of the complex is unguarded. The abandoned facility was handed over to the naval forces of the Armed Forces of Ukraine in 2000.\n\nHowever, the former base was frequently plundered during the unguarded period from 1993 to 2003, with all metal structures scavenged for the metal.\n\nThe Sevastopol \"Marine Commission\" led by Vladimir Stefanovsky proposed the construction of the current museum. The museum would have themed exhibition halls, which were former repair shops and arsenals, a submarine standing by the underground pier, a tourist center, a cinema room with a chronicle of the time of active military confrontation between the two superpowers, and finally, an underground memorial dedicated to submariners who died at sea.\n\nThe 10th anniversary of the museum was celebrated in June 2013. Submarine veterans, former employees of the base as well as representatives from the authorities, armed forces and students attended the ceremony.\n\nThe facility was placed under the jurisdiction of Russia and the southern area of the Military History Museum of fortification structures of the Russian Federation in 2014, after the annexation of Crimea.\n\nAccording to press information from March 2014, Russia is considering the possibility of restoring the submarine base in Balaklava.\n\nThe base sits at No.22 Tavricheskaya Naberezhnaya street, Balaklava district, Sevastopol, Crimea.\n\n\nThe 2012 American action film, \"Soldiers of Fortune\", filmed parts of it near the base.\n\n\n"}
{"id": "21272", "url": "https://en.wikipedia.org/wiki?curid=21272", "title": "Neutron", "text": "Neutron\n\nThe neutron is a subatomic particle, symbol or , with no net electric charge and a mass slightly larger than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one atomic mass unit, they are both referred to as nucleons. Their properties and interactions are described by nuclear physics.\n\nThe chemical and nuclear properties of the nucleus are determined by the number of protons, called the atomic number, and the number of neutrons, called the neutron number. The atomic mass number is the total number of nucleons. For example, carbon has atomic number 6, and its abundant carbon-12 isotope has 6 neutrons, whereas its rare carbon-13 isotope has 7 neutrons. Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes.\n\nWithin the nucleus, protons and neutrons are bound together through the nuclear force. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen atom. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.\n\nThe neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932, neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938, it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, etc., in a cascade known as a nuclear chain reaction. These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).\n\nFree neutrons, while not directly ionizing atoms, cause ionizing radiation. As such they can be a biological hazard, depending upon dose. A small natural \"neutron background\" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust. Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.\n\nAtomic nuclei are formed by a number of protons, Z the atomic number, and a number of neutrons, N the neutron number, bound together by the nuclear force. The atomic number defines the chemical properties of the atom, and the neutron number determines the isotope or nuclide. The terms isotope and nuclide are often used synonymously, but they refer to chemical and nuclear properties, respectively. Strictly speaking, isotopes are two or more nuclides with the same number of protons; nuclides with the same number of neutrons are called isotones. The atomic mass number, symbol A, equals Z+N. Nuclides with the same atomic mass number are called isobars. The nucleus of the most common isotope of the hydrogen atom (with the chemical symbol H) is a lone proton. The nuclei of the heavy hydrogen isotopes deuterium (D or H) and tritium (T or H) contain one proton bound to one and two neutrons, respectively. All other types of atomic nuclei are composed of two or more protons and various numbers of neutrons. The most common nuclide of the common chemical element lead, Pb, has 82 protons and 126 neutrons, for example. The table of nuclides comprises all the known nuclides. Even though it is not a chemical element, the neutron is included in this table.\n\nThe free neutron has a mass of 939,565,413.3 eV/c, or , or . The neutron has a mean square radius of about , or 0.8 fm, and it is a spin-½ fermion.\nThe neutron has no measurable electric charge. With its positive electric charge, the proton is directly influenced by electric fields, whereas the neutron is unaffected by electric fields. The neutron has a magnetic moment, however, so the neutron is influenced by magnetic fields. The neutron's magnetic moment has a negative value, because its orientation is opposite to the neutron's spin.\n\nA free neutron is unstable, decaying to a proton, electron and antineutrino with a mean lifetime of just under 15 minutes (). This radioactive decay, known as beta decay, is possible because the mass of the neutron is slightly greater than the proton. The free proton is stable. Neutrons or protons bound in a nucleus can be stable or unstable, however, depending on the nuclide. Beta decay, in which neutrons decay to protons, or vice versa, is governed by the weak force, and it requires the emission or absorption of electrons and neutrinos, or their antiparticles.\nProtons and neutrons behave almost identically under the influence of the nuclear force within the nucleus. The concept of isospin, in which the proton and neutron are viewed as two quantum states of the same particle, is used to model the interactions of nucleons by the nuclear or weak forces. Because of the strength of the nuclear force at short distances, the binding energy of nucleons is more than seven orders of magnitude larger than the electromagnetic energy binding electrons in atoms. Nuclear reactions (such as nuclear fission) therefore have an energy density that is more than ten million times that of chemical reactions. Because of the mass–energy equivalence, nuclear binding energies add or subtract from the mass of nuclei. Ultimately, the ability of the nuclear force to store energy arising from the electromagnetic repulsion of nuclear components is the basis for most of the energy that makes nuclear reactors or bombs possible. In nuclear fission, the absorption of a neutron by a heavy nuclide (e.g., uranium-235) causes the nuclide to become unstable and break into light nuclides and additional neutrons. The positively charged light nuclides then repel, releasing electromagnetic potential energy.\n\nThe neutron is classified as a \"hadron\", because it is a composite particle made of quarks. The neutron is also classified as a \"baryon\", because it is composed of three valence quarks. The finite size of the neutron and its magnetic moment indicates that the neutron is a composite particle, as opposed to being an elementary particle. A neutron contains two down quarks with charge − \"e\" and one up quark with charge + \"e\".\n\nLike protons, the quarks of the neutron are held together by the strong force, mediated by gluons. The nuclear force results from secondary effects of the more fundamental strong force.\n\nThe story of the discovery of the neutron and its properties is central to the extraordinary developments in atomic physics that occurred in the first half of the 20th century, leading ultimately to the atomic bomb in 1945. In the 1911 Rutherford model, the atom consisted of a small positively charged massive nucleus surrounded by a much larger cloud of negatively charged electrons. In 1920, Rutherford suggested that the nucleus consisted of positive protons and neutrally-charged particles, suggested to be a proton and an electron bound in some way. Electrons were assumed to reside within the nucleus because it was known that beta radiation consisted of electrons emitted from the nucleus. Rutherford called these uncharged particles \"neutrons\", by the Latin root for \"neutralis\" (neuter) and the Greek suffix \"-on\" (a suffix used in the names of subatomic particles, i.e. \"electron\" and \"proton\"). References to the word \"neutron\" in connection with the atom can be found in the literature as early as 1899, however.\n\nThroughout the 1920s, physicists assumed that the atomic nucleus was composed of protons and \"nuclear electrons\" but there were obvious problems. It was difficult to reconcile the proton–electron model for nuclei with the Heisenberg uncertainty relation of quantum mechanics. The Klein paradox, discovered by Oskar Klein in 1928, presented further quantum mechanical objections to the notion of an electron confined within a nucleus. Observed properties of atoms and molecules were inconsistent with the nuclear spin expected from the proton–electron hypothesis. Since both protons and electrons carry an intrinsic spin of ½ \"ħ\", there is no way to arrange an odd number of spins ±½ \"ħ\" to give a spin integer multiple of \"ħ\". Nuclei with integer spin are common, e.g., N.\n\nIn 1931, Walther Bothe and Herbert Becker found that if alpha particle radiation from polonium fell on beryllium, boron, or lithium, an unusually penetrating radiation was produced. The radiation was not influenced by an electric field, so Bothe and Becker assumed it was gamma radiation. The following year Irène Joliot-Curie and Frédéric Joliot-Curie in Paris showed that if this \"gamma\" radiation fell on paraffin, or any other hydrogen-containing compound, it ejected protons of very high energy. Neither Rutherford nor James Chadwick at the Cavendish Laboratory in Cambridge were convinced by the gamma ray interpretation. Chadwick quickly performed a series of experiments that showed that the new radiation consisted of uncharged particles with about the same mass as the proton. These particles were neutrons. Chadwick won the Nobel Prize in Physics for this discovery in 1935.\n\nModels for atomic nucleus consisting of protons and neutrons were quickly developed by Werner Heisenberg and others. The proton–neutron model explained the puzzle of nuclear spins. The origins of beta radiation were explained by Enrico Fermi in 1934 by the process of beta decay, in which the neutron decays to a proton by \"creating\" an electron and a (as yet undiscovered) neutrino. In 1935 Chadwick and his doctoral student Maurice Goldhaber, reported the first accurate measurement of the mass of the neutron.\n\nBy 1934, Fermi had bombarded heavier elements with neutrons to induce radioactivity in elements of high atomic number. In 1938, Fermi received the Nobel Prize in Physics \"for his demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons\". In 1938 Otto Hahn, Lise Meitner, and Fritz Strassmann discovered nuclear fission, or the fractionation of uranium nuclei into light elements, induced by neutron bombardment. In 1945 Hahn received the 1944 Nobel Prize in Chemistry \"for his discovery of the fission of heavy atomic nuclei.\" The discovery of nuclear fission would lead to the development of nuclear power and the atomic bomb by the end of World War II.\n\nUnder the Standard Model of particle physics, the only possible decay mode for the neutron that conserves baryon number is for one of the neutron's quarks to change flavour via the weak interaction. The decay of one of the neutron's down quarks into a lighter up quark can be achieved by the emission of a W boson. By this process, the Standard Model description of beta decay, the neutron decays into a proton (which contains one down and two up quarks), an electron, and an electron antineutrino.\n\nSince interacting protons have a mutual electromagnetic repulsion that is stronger than their attractive nuclear interaction, neutrons are a necessary constituent of any atomic nucleus that contains more than one proton (see diproton and neutron–proton ratio). Neutrons bind with protons and one another in the nucleus via the nuclear force, effectively moderating the repulsive forces between the protons and stabilizing the nucleus.\n\nOutside the nucleus, free neutrons are unstable and have a mean lifetime of (about 14 minutes, 42 seconds); therefore the half-life for this process (which differs from the mean lifetime by a factor of ) is (about 10 minutes, 11 seconds). Beta decay of the neutron, described above, can be denoted by the radioactive decay:\n\nwhere , , and denote the proton, electron and electron antineutrino, respectively.\nFor the free neutron the decay energy for this process (based on the masses of the neutron, proton, and electron) is 0.782343 MeV. The maximal energy of the beta decay electron (in the process wherein the neutrino receives a vanishingly small amount of kinetic energy) has been measured at 0.782 ± 0.013 MeV. The latter number is not well-enough measured to determine the comparatively tiny rest mass of the neutrino (which must in theory be subtracted from the maximal electron kinetic energy) as well as neutrino mass is constrained by many other methods.\n\nA small fraction (about one in 1000) of free neutrons decay with the same products, but add an extra particle in the form of an emitted gamma ray:\n\nThis gamma ray may be thought of as an \"internal bremsstrahlung\" that arises from the electromagnetic interaction of the emitted beta particle with the proton. Internal bremsstrahlung gamma ray production is also a minor feature of beta decays of bound neutrons (as discussed below).\n\nA very small minority of neutron decays (about four per million) are so-called \"two-body (neutron) decays\", in which a proton, electron and antineutrino are produced as usual, but the electron fails to gain the 13.6 eV necessary energy to escape the proton (the ionization energy of hydrogen), and therefore simply remains bound to it, as a neutral hydrogen atom (one of the \"two bodies\"). In this type of free neutron decay, almost all of the neutron decay energy is carried off by the antineutrino (the other \"body\"). (The hydrogen atom recoils with a speed of only about (decay energy)/(hydrogen rest energy) times the speed of light, or 250 km/s.)\n\nThe transformation of a free proton to a neutron (plus a positron and a neutrino) is energetically impossible, since a free neutron has a greater mass than a free proton. But a high-energy collision of a proton and an electron or neutrino can result in a neutron.\n\nWhile a free neutron has a half life of about 10.2 min, most neutrons within nuclei are stable. According to the nuclear shell model, the protons and neutrons of a nuclide are a quantum mechanical system organized into discrete energy levels with unique quantum numbers. For a neutron to decay, the resulting proton requires an available state at lower energy than the initial neutron state. In stable nuclei the possible lower energy states are all filled, meaning they are each occupied by two protons with spin up and spin down. The Pauli exclusion principle therefore disallows the decay of a neutron to a proton within stable nuclei. The situation is similar to electrons of an atom, where electrons have distinct atomic orbitals and are prevented from decaying to lower energy states, with the emission of a photon, by the exclusion principle.\n\nNeutrons in unstable nuclei can decay by beta decay as described above. In this case, an energetically allowed quantum state is available for the proton resulting from the decay. One example of this decay is carbon-14 (6 protons, 8 neutrons) that decays to nitrogen-14 (7 protons, 7 neutrons) with a half-life of about 5,730 years.\n\nInside a nucleus, a proton can transform into a neutron via inverse beta decay, if an energetically allowed quantum state is available for the neutron. This transformation occurs by emission of a positron and an electron neutrino:\n\nThe transformation of a proton to a neutron inside of a nucleus is also possible through electron capture:\nPositron capture by neutrons in nuclei that contain an excess of neutrons is also possible, but is hindered because positrons are repelled by the positive nucleus, and quickly annihilate when they encounter electrons.\n\nThree types of beta decay in competition are illustrated by the single isotope copper-64 (29 protons, 35 neutrons), which has a half-life of about 12.7 hours. This isotope has one unpaired proton and one unpaired neutron, so either the proton or the neutron can decay. This particular nuclide is almost equally likely to undergo proton decay (by positron emission, 18% or by electron capture, 43%) or neutron decay (by electron emission, 39%).\n\nThe mass of a neutron cannot be directly determined by mass spectrometry due to lack of electric charge. However, since the masses of a proton and of a deuteron can be measured with a mass spectrometer, the mass of a neutron can be deduced by subtracting proton mass from deuteron mass, with the difference being the mass of the neutron plus the binding energy of deuterium (expressed as a positive emitted energy). The latter can be directly measured by measuring the energy (formula_1) of the single gamma photon emitted when neutrons are captured by protons (this is exothermic and happens with zero-energy neutrons), plus the small recoil kinetic energy (formula_2) of the deuteron (about 0.06% of the total energy).\n\nThe energy of the gamma ray can be measured to high precision by X-ray diffraction techniques, as was first done by Bell and Elliot in 1948. The best modern (1986) values for neutron mass by this technique are provided by Greene, et al. These give a neutron mass of:\n\nThe value for the neutron mass in MeV is less accurately known, due to less accuracy in the known conversion of u to MeV:\n\nAnother method to determine the mass of a neutron starts from the beta decay of the neutron, when the momenta of the resulting proton and electron are measured.\n\nThe total electric charge of the neutron is . This zero value has been tested experimentally, and the present experimental limit for the charge of the neutron is ,   or . This value is consistent with zero, given the experimental uncertainties (indicated in parentheses). By comparison, the charge of the proton is .\n\nEven though the neutron is a neutral particle, the magnetic moment of a neutron is not zero. The neutron is not affected by electric fields, but it is affected by magnetic fields. The magnetic moment of the neutron is an indication of its quark substructure and internal charge distribution.\nThe value for the neutron's magnetic moment was first directly measured by Luis Alvarez and Felix Bloch at Berkeley, California, in 1940, using an extension of the magnetic resonance methods developed by Rabi. Alvarez and Bloch determined the magnetic moment of the neutron to be , where \"μ\" is the nuclear magneton.\n\nIn the quark model for hadrons, the neutron is composed of one up quark (charge +2/3 \"e\") and two down quarks (charge −1/3 \"e\"). The magnetic moment of the neutron can be modeled as a sum of the magnetic moments of the constituent quarks. The calculation assumes that the quarks behave like pointlike Dirac particles, each having their own magnetic moment. Simplistically, the magnetic moment of the neutron can be viewed as resulting from the vector sum of the three quark magnetic moments, plus the orbital magnetic moments caused by the movement of the three charged quarks within the neutron.\n\nIn one of the early successes of the Standard Model (SU(6) theory, now understood in terms of quark behavior), in 1964 Mirza A. B. Beg, Benjamin W. Lee, and Abraham Pais theoretically calculated the ratio of proton to neutron magnetic moments to be −3/2, which agrees with the experimental value to within 3%. The measured value for this ratio is . A contradiction of the quantum mechanical basis of this calculation with the Pauli exclusion principle, led to the discovery of the color charge for quarks by Oscar W. Greenberg in 1964.\n\nThe above treatment compares neutrons with protons, allowing the complex behavior of quarks to be subtracted out between models, and merely exploring what the effects would be of differing quark charges (or quark type). Such calculations are enough to show that the interior of neutrons is very much like that of protons, save for the difference in quark composition with a down quark in the neutron replacing an up quark in the proton.\n\nAttempts have been made to quantitatively recover the neutron magnetic moment from first principles. From the nonrelativistic, quantum mechanical wavefunction for baryons composed of three quarks, a straightforward calculation gives fairly accurate estimates for the magnetic moments of neutrons, protons, and other baryons. For a neutron, the end result of this calculation is that the magnetic moment of the neutron is given by , where \"μ\" and \"μ\" are the magnetic moments for the down and up quarks, respectively. This result combines the intrinsic magnetic moments of the quarks with their orbital magnetic moments, and assumes the three quarks are in a particular, dominant quantum state.\n\nThe results of this calculation are encouraging, but the masses of the up or down quarks were assumed to be 1/3 the mass of a nucleon. The masses of the quarks are actually only about 1% that of a nucleon. The discrepancy stems from the complexity of the Standard Model for nucleons, where most of their mass originates in the gluon fields, virtual particles, and their associated energy that are essential aspects of the strong force. Furthermore, the complex system of quarks and gluons that constitute a neutron requires a relativistic treatment. The nucleon magnetic moment has been successfully computed numerically from first principles, however, including all the effects mentioned and using more realistic values for the quark masses. The calculation gave results that were in fair agreement with measurement, but it required significant computing resources.\n\nThe neutron is a spin 1/2 particle, that is, it is a fermion with intrinsic angular momentum equal to 1/2 , where is the reduced Planck constant. For many years after the discovery of the neutron, its exact spin was ambiguous. Although it was assumed to be a spin 1/2 Dirac particle, the possibility that the neutron was a spin 3/2 particle lingered. The interactions of the neutron's magnetic moment with an external magnetic field were exploited to finally determine the spin of the neutron. In 1949, Hughes and Burgy measured neutrons reflected from a ferromagnetic mirror and found that the angular distribution of the reflections was consistent with spin 1/2. In 1954, Sherwood, Stephenson, and Bernstein employed neutrons in a Stern–Gerlach experiment that used a magnetic field to separate the neutron spin states. They recorded two such spin states, consistent with a spin 1/2 particle.\n\nAs a fermion, the neutron is subject to the Pauli exclusion principle; two neutrons cannot have the same quantum numbers. This is the source of the degeneracy pressure which makes neutron stars possible.\nAn article published in 2007 featuring a model-independent analysis concluded that the neutron has a negatively charged exterior, a positively charged middle, and a negative core. In a simplified classical view, the negative \"skin\" of the neutron assists it to be attracted to the protons with which it interacts in the nucleus. (However, the main attraction between neutrons and protons is via the nuclear force, which does not involve electric charge.)\n\nThe simplified classical view of the neutron's charge distribution also \"explains\" the fact that the neutron magnetic dipole points in the opposite direction from its spin angular momentum vector (as compared to the proton). This gives the neutron, in effect, a magnetic moment which resembles a negatively charged particle. This can be reconciled classically with a neutral neutron composed of a charge distribution in which the negative sub-parts of the neutron have a larger average radius of distribution, and therefore contribute more to the particle's magnetic dipole moment, than do the positive parts that are, on average, nearer the core.\n\nThe Standard Model of particle physics predicts a tiny separation of positive and negative charge within the neutron leading to a permanent electric dipole moment. The predicted value is, however, well below the current sensitivity of experiments. From several unsolved puzzles in particle physics, it is clear that the Standard Model is not the final and full description of all particles and their interactions. New theories going beyond the Standard Model generally lead to much larger predictions for the electric dipole moment of the neutron. Currently, there are at least four experiments trying to measure for the first time a finite neutron electric dipole moment, including:\n\nThe antineutron is the antiparticle of the neutron. It was discovered by Bruce Cork in the year 1956, a year after the antiproton was discovered. CPT-symmetry puts strong constraints on the relative properties of particles and antiparticles, so studying antineutrons provides stringent tests on CPT-symmetry. The fractional difference in the masses of the neutron and antineutron is . Since the difference is only about two standard deviations away from zero, this does not give any convincing evidence of CPT-violation.\n\nThe existence of stable clusters of 4 neutrons, or tetraneutrons, has been hypothesised by a team led by Francisco-Miguel Marqués at the CNRS Laboratory for Nuclear Physics based on observations of the disintegration of beryllium-14 nuclei. This is particularly interesting because current theory suggests that these clusters should not be stable.\n\nIn February 2016, Japanese physicist Susumu Shimoura of the University of Tokyo and co-workers reported they had observed the purported tetraneutrons for the first time experimentally. Nuclear physicists around the world say this discovery, if confirmed, would be a milestone in the field of nuclear physics and certainly would deepen our understanding of the nuclear forces.\n\nThe dineutron is another hypothetical particle. In 2012, Artemis Spyrou from Michigan State University and coworkers reported that they observed, for the first time, the dineutron emission in the decay of Be. The dineutron character is evidenced by a small emission angle between the two neutrons. The authors measured the two-neutron separation energy to be 1.35(10) MeV, in good agreement with shell model calculations, using standard interactions for this mass region.\n\nAt extremely high pressures and temperatures, nucleons and electrons are believed to collapse into bulk neutronic matter, called neutronium. This is presumed to happen in neutron stars.\n\nThe extreme pressure inside a neutron star may deform the neutrons into a cubic symmetry, allowing tighter packing of neutrons.\n\nThe common means of detecting a charged particle by looking for a track of ionization (such as in a cloud chamber) does not work for neutrons directly. Neutrons that elastically scatter off atoms can create an ionization track that is detectable, but the experiments are not as simple to carry out; other means for detecting neutrons, consisting of allowing them to interact with atomic nuclei, are more commonly used. The commonly used methods to detect neutrons can therefore be categorized according to the nuclear processes relied upon, mainly neutron capture or elastic scattering.\n\nA common method for detecting neutrons involves converting the energy released from neutron capture reactions into electrical signals. Certain nuclides have a high neutron capture cross section, which is the probability of absorbing a neutron. Upon neutron capture, the compound nucleus emits more easily detectable radiation, for example an alpha particle, which is then detected. The nuclides , , , , , , and are useful for this purpose. \n\nNeutrons can elastically scatter off nuclei, causing the struck nucleus to recoil. Kinematically, a neutron can transfer more energy to a light nucleus such as hydrogen or helium than to a heavier nucleus. Detectors relying on elastic scattering are called fast neutron detectors. Recoiling nuclei can ionize and excite further atoms through collisions. Charge and/or scintillation light produced in this way can be collected to produce a detected signal. A major challenge in fast neutron detection is discerning such signals from erroneous signals produced by gamma radiation in the same detector.\n\nFast neutron detectors have the advantage of not requiring a moderator, and are therefore capable of measuring the neutron's energy, time of arrival, and in certain cases direction of incidence.\n\nFree neutrons are unstable, although they have the longest half-life of any unstable subatomic particle by several orders of magnitude. Their half-life is still only about 10 minutes, however, so they can be obtained only from sources that produce them continuously.\n\nNatural neutron background. A small natural background flux of free neutrons exists everywhere on Earth. In the atmosphere and deep into the ocean, the \"neutron background\" is caused by muons produced by cosmic ray interaction with the atmosphere. These high-energy muons are capable of penetration to considerable depths in water and soil. There, in striking atomic nuclei, among other reactions they induce spallation reactions in which a neutron is liberated from the nucleus. Within the Earth's crust a second source is neutrons produced primarily by spontaneous fission of uranium and thorium present in crustal minerals. The neutron background is not strong enough to be a biological hazard, but it is of importance to very high resolution particle detectors that are looking for very rare events, such as (hypothesized) interactions that might be caused by particles of dark matter. Recent research has shown that even thunderstorms can produce neutrons with energies of up to several tens of MeV. Recent research has shown that the fluence of these neutrons lies between 10 and 10 per ms and per m depending on the detection altitude. The energy of most of these neutrons, even with initial energies of 20 MeV, decreases down to the keV range within 1 ms.\n\nEven stronger neutron background radiation is produced at the surface of Mars, where the atmosphere is thick enough to generate neutrons from cosmic ray muon production and neutron-spallation, but not thick enough to provide significant protection from the neutrons produced. These neutrons not only produce a Martian surface neutron radiation hazard from direct downward-going neutron radiation but may also produce a significant hazard from reflection of neutrons from the Martian surface, which will produce reflected neutron radiation penetrating upward into a Martian craft or habitat from the floor.\n\nSources of neutrons for research. These include certain types of radioactive decay (spontaneous fission and neutron emission), and from certain nuclear reactions. Convenient nuclear reactions include tabletop reactions such as natural alpha and gamma bombardment of certain nuclides, often beryllium or deuterium, and induced nuclear fission, such as occurs in nuclear reactors. In addition, high-energy nuclear reactions (such as occur in cosmic radiation showers or accelerator collisions) also produce neutrons from disintegration of target nuclei. Small (tabletop) particle accelerators optimized to produce free neutrons in this way, are called neutron generators.\n\nIn practice, the most commonly used small laboratory sources of neutrons use radioactive decay to power neutron production. One noted neutron-producing radioisotope, californium-252 decays (half-life 2.65 years) by spontaneous fission 3% of the time with production of 3.7 neutrons per fission, and is used alone as a neutron source from this process. Nuclear reaction sources (that involve two materials) powered by radioisotopes use an alpha decay source plus a beryllium target, or else a source of high-energy gamma radiation from a source that undergoes beta decay followed by gamma decay, which produces photoneutrons on interaction of the high-energy gamma ray with ordinary stable beryllium, or else with the deuterium in heavy water. A popular source of the latter type is radioactive antimony-124 plus beryllium, a system with a half-life of 60.9 days, which can be constructed from natural antimony (which is 42.8% stable antimony-123) by activating it with neutrons in a nuclear reactor, then transported to where the neutron source is needed.\n\nNuclear fission reactors naturally produce free neutrons; their role is to sustain the energy-producing chain reaction. The intense neutron radiation can also be used to produce various radioisotopes through the process of neutron activation, which is a type of neutron capture.\n\nExperimental nuclear fusion reactors produce free neutrons as a waste product. However, it is these neutrons that possess most of the energy, and converting that energy to a useful form has proved a difficult engineering challenge. Fusion reactors that generate neutrons are likely to create radioactive waste, but the waste is composed of neutron-activated lighter isotopes, which have relatively short (50–100 years) decay periods as compared to typical half-lives of 10,000 years for fission waste, which is long due primarily to the long half-life of alpha-emitting transuranic actinides.\n\nFree neutron beams are obtained from neutron sources by neutron transport. For access to intense neutron sources, researchers must go to a specialized neutron facility that operates a research reactor or a spallation source.\n\nThe neutron's lack of total electric charge makes it difficult to steer or accelerate them. Charged particles can be accelerated, decelerated, or deflected by electric or magnetic fields. These methods have little effect on neutrons. However, some effects may be attained by use of inhomogeneous magnetic fields because of the neutron's magnetic moment. Neutrons can be controlled by methods that include moderation, reflection, and velocity selection. Thermal neutrons can be polarized by transmission through magnetic materials in a method analogous to the Faraday effect for photons. Cold neutrons of wavelengths of 6–7 angstroms can be produced in beams of a high degree of polarization, by use of magnetic mirrors and magnetized interference filters.\n\nThe neutron plays an important role in many nuclear reactions. For example, neutron capture often results in neutron activation, inducing radioactivity. In particular, knowledge of neutrons and their behavior has been important in the development of nuclear reactors and nuclear weapons. The fissioning of elements like uranium-235 and plutonium-239 is caused by their absorption of neutrons.\n\n\"Cold\", \"thermal\", and \"hot\" neutron radiation is commonly employed in neutron scattering facilities, where the radiation is used in a similar way one uses X-rays for the analysis of condensed matter. Neutrons are complementary to the latter in terms of atomic contrasts by different scattering cross sections; sensitivity to magnetism; energy range for inelastic neutron spectroscopy; and deep penetration into matter.\n\nThe development of \"neutron lenses\" based on total internal reflection within hollow glass capillary tubes or by reflection from dimpled aluminum plates has driven ongoing research into neutron microscopy and neutron/gamma ray tomography.\n\nA major use of neutrons is to excite delayed and prompt gamma rays from elements in materials. This forms the basis of neutron activation analysis (NAA) and prompt gamma neutron activation analysis (PGNAA). NAA is most often used to analyze small samples of materials in a nuclear reactor whilst PGNAA is most often used to analyze subterranean rocks around bore holes and industrial bulk materials on conveyor belts.\n\nAnother use of neutron emitters is the detection of light nuclei, in particular the hydrogen found in water molecules. When a fast neutron collides with a light nucleus, it loses a large fraction of its energy. By measuring the rate at which slow neutrons return to the probe after reflecting off of hydrogen nuclei, a neutron probe may determine the water content in soil.\n\nBecause neutron radiation is both penetrating and ionizing, it can be exploited for medical treatments. Neutron radiation can have the unfortunate side-effect of leaving the affected area radioactive, however. Neutron tomography is therefore not a viable medical application.\n\nFast neutron therapy utilizes high-energy neutrons typically greater than 20 MeV to treat cancer. Radiation therapy of cancers is based upon the biological response of cells to ionizing radiation. If radiation is delivered in small sessions to damage cancerous areas, normal tissue will have time to repair itself, while tumor cells often cannot. Neutron radiation can deliver energy to a cancerous region at a rate an order of magnitude larger than gamma radiation\n\nBeams of low-energy neutrons are used in boron capture therapy to treat cancer. In boron capture therapy, the patient is given a drug that contains boron and that preferentially accumulates in the tumor to be targeted. The tumor is then bombarded with very low-energy neutrons (although often higher than thermal energy) which are captured by the boron-10 isotope in the boron, which produces an excited state of boron-11 that then decays to produce lithium-7 and an alpha particle that have sufficient energy to kill the malignant cell, but insufficient range to damage nearby cells. For such a therapy to be applied to the treatment of cancer, a neutron source having an intensity of the order of a thousand million (10) neutrons per second per cm is preferred. Such fluxes require a research nuclear reactor.\n\nExposure to free neutrons can be hazardous, since the interaction of neutrons with molecules in the body can cause disruption to molecules and atoms, and can also cause reactions that give rise to other forms of radiation (such as protons). The normal precautions of radiation protection apply: Avoid exposure, stay as far from the source as possible, and keep exposure time to a minimum. Some particular thought must be given to how to protect from neutron exposure, however. For other types of radiation, e.g., alpha particles, beta particles, or gamma rays, material of a high atomic number and with high density make for good shielding; frequently, lead is used. However, this approach will not work with neutrons, since the absorption of neutrons does not increase straightforwardly with atomic number, as it does with alpha, beta, and gamma radiation. Instead one needs to look at the particular interactions neutrons have with matter (see the section on detection above). For example, hydrogen-rich materials are often used to shield against neutrons, since ordinary hydrogen both scatters and slows neutrons. This often means that simple concrete blocks or even paraffin-loaded plastic blocks afford better protection from neutrons than do far more dense materials. After slowing, neutrons may then be absorbed with an isotope that has high affinity for slow neutrons without causing secondary capture radiation, such as lithium-6.\n\nHydrogen-rich ordinary water affects neutron absorption in nuclear fission reactors: Usually, neutrons are so strongly absorbed by normal water that fuel enrichment with fissionable isotope is required. The deuterium in heavy water has a very much lower absorption affinity for neutrons than does protium (normal light hydrogen). Deuterium is, therefore, used in CANDU-type reactors, in order to slow (moderate) neutron velocity, to increase the probability of nuclear fission compared to neutron capture.\n\nA \"thermal neutron\" is a free neutron that is Boltzmann distributed with kT= () at room temperature. This gives characteristic (not average, or median) speed of 2.2 km/s. The name 'thermal' comes from their energy being that of the room temperature gas or material they are permeating. (see \"kinetic theory\" for energies and speeds of molecules). After a number of collisions (often in the range of 10–20) with nuclei, neutrons arrive at this energy level, provided that they are not absorbed.\n\nIn many substances, thermal neutron reactions show a much larger effective cross-section than reactions involving faster neutrons, and thermal neutrons can therefore be absorbed more readily (i.e., with higher probability) by any atomic nuclei that they collide with, creating a heavier — and often unstable — isotope of the chemical element as a result.\n\nMost fission reactors use a neutron moderator to slow down, or \"thermalize\" the neutrons that are emitted by nuclear fission so that they are more easily captured, causing further fission. Others, called fast breeder reactors, use fission energy neutrons directly.\n\n\"Cold neutrons\" are thermal neutrons that have been equilibrated in a very cold substance such as liquid deuterium. Such a \"cold source\" is placed in the moderator of a research reactor or spallation source. Cold neutrons are particularly valuable for neutron scattering experiments.\n\nUltracold neutrons are produced by elastically scattering cold neutrons in substances with a temperature of a few kelvins, such as solid deuterium or superfluid helium. An alternative production method is the mechanical deceleration of cold neutrons.\n\nA \"fast neutron\" is a free neutron with a kinetic energy level close to (), hence a speed of ~ (~5% of the speed of light). They are named \"fission energy\" or \"fast\" neutrons to distinguish them from lower-energy thermal neutrons, and high-energy neutrons produced in cosmic showers or accelerators. Fast neutrons are produced by nuclear processes such as nuclear fission. Neutrons produced in fission, as noted above, have a Maxwell–Boltzmann distribution of kinetic energies from 0 to ~14 MeV, a mean energy of 2 MeV (for U-235 fission neutrons), and a mode of only 0.75 MeV, which means that more than half of them do not qualify as fast (and thus have almost no chance of initiating fission in fertile materials, such as U-238 and Th-232).\n\nFast neutrons can be made into thermal neutrons via a process called moderation. This is done with a neutron moderator. In reactors, typically heavy water, light water, or graphite are used to moderate neutrons.\n\nD–T (deuterium–tritium) fusion is the fusion reaction that produces the most energetic neutrons, with 14.1 MeV of kinetic energy and traveling at 17% of the speed of light. D–T fusion is also the easiest fusion reaction to ignite, reaching near-peak rates even when the deuterium and tritium nuclei have only a thousandth as much kinetic energy as the 14.1 MeV that will be produced.\n\n14.1 MeV neutrons have about 10 times as much energy as fission neutrons, and are very effective at fissioning even non-fissile heavy nuclei, and these high-energy fissions produce more neutrons on average than fissions by lower-energy neutrons. This makes D–T fusion neutron sources such as proposed tokamak power reactors useful for transmutation of transuranic waste. 14.1 MeV neutrons can also produce neutrons by knocking them loose from nuclei.\n\nOn the other hand, these very high-energy neutrons are less likely to simply be captured without causing fission or spallation. For these reasons, nuclear weapon design extensively utilizes D–T fusion 14.1 MeV neutrons to cause more fission. Fusion neutrons are able to cause fission in ordinarily non-fissile materials, such as depleted uranium (uranium-238), and these materials have been used in the jackets of thermonuclear weapons. Fusion neutrons also can cause fission in substances that are unsuitable or difficult to make into primary fission bombs, such as reactor grade plutonium. This physical fact thus causes ordinary non-weapons grade materials to become of concern in certain nuclear proliferation discussions and treaties.\n\nOther fusion reactions produce much less energetic neutrons. D–D fusion produces a 2.45 MeV neutron and helium-3 half of the time, and produces tritium and a proton but no neutron the rest of the time. D–He fusion produces no neutron.\n\nA fission energy neutron that has slowed down but not yet reached thermal energies is called an epithermal neutron.\n\nCross sections for both capture and fission reactions often have multiple resonance peaks at specific energies in the epithermal energy range.\nThese are of less significance in a fast neutron reactor, where most neutrons are absorbed before slowing down to this range, or in a well-moderated thermal reactor, where epithermal neutrons interact mostly with moderator nuclei, not with either fissile or fertile actinide nuclides.\nHowever, in a partially moderated reactor with more interactions of epithermal neutrons with heavy metal nuclei, there are greater possibilities for transient changes in reactivity that might make reactor control more difficult.\n\nRatios of capture reactions to fission reactions are also worse (more captures without fission) in most nuclear fuels such as plutonium-239, making epithermal-spectrum reactors using these fuels less desirable, as captures not only waste the one neutron captured but also usually result in a nuclide that is not fissile with thermal or epithermal neutrons, though still fissionable with fast neutrons. The exception is uranium-233 of the thorium cycle, which has good capture-fission ratios at all neutron energies.\n\nHigh-energy neutrons have much more energy than fission energy neutrons and are generated as secondary particles by particle accelerators or in the atmosphere from cosmic rays. These high-energy neutrons are extremely efficient at ionization and far more likely to cause cell death than X-rays or protons.\n\n\n\n\n"}
{"id": "4119350", "url": "https://en.wikipedia.org/wiki?curid=4119350", "title": "Nickel(III) oxide", "text": "Nickel(III) oxide\n\nNickel(III) oxide is the inorganic compound with the formula NiO. It is not well characterised, sometimes referred to as black nickel oxide. Traces of NiO on nickel surfaces have been mentioned. A related, better characterized material is nickel oxide hydroxide (NiOOH), which is likely the reagent employed in organic synthesis since it is generated in aqueous media.\n"}
{"id": "24341652", "url": "https://en.wikipedia.org/wiki?curid=24341652", "title": "Nimo tube", "text": "Nimo tube\n\nNimo was the trademark of a family of very small non-standard CRTs manufactured by Industrial Electronics Engineers around mid-1960s, with 10 electron guns with stencils which shaped the electron beam as digits. The Nimo tube operated on a similar principle as the charactron, but used a much simpler design. They were intended as single digit, simple displays, or as 4 or 6 digits by means of a special horizontal magnetic deflection system. Having only 3 electrode types (a filament, an anode and 10 different grids), the driving circuit for this tube was very simple, and as the image was projected on the glass face, it allowed a much wider viewing angle than for example nixie tubes which Nimo tried to replace.\n\nThe tube required 1750 volts DC for the anode and also required 1.1 volts AC for the filaments as well as a cathode bias for the filaments. \n\nThe German tube manufacturer Telefunken tried to sell an unlicensed copy of the design under the type number XM1000, but was sued by IEE in 1969 and lost, having to destroy all tubes already produced. Only a few survived, most of them not yet labeled.\n\n\n"}
{"id": "595605", "url": "https://en.wikipedia.org/wiki?curid=595605", "title": "Nuclear Energy Agency", "text": "Nuclear Energy Agency\n\nThe Nuclear Energy Agency (NEA) is an intergovernmental agency that is organized under the Organisation for Economic Co-operation and Development (OECD). Originally formed on 1 February 1958 with the name European Nuclear Energy Agency (ENEA)—the United States participated as an Associate Member—the name was changed on 20 April 1972 to its current name after Japan became a member.\n\nThe mission of the NEA is to \"assist its member countries in maintaining and further developing, through international co-operation, the scientific, technological and legal bases required for the safe, environmentally friendly and economical use of nuclear energy for peaceful purposes.\" \n\nNEA currently consists of 33 countries from Europe, North America and the Asia-Pacific region. In 2017 Argentina and Romania accessioned to NEA as its most recent members.\nTogether they account for approximately 85% of the world’s installed nuclear capacity. Nuclear power accounts for almost a quarter of the electricity produced in NEA Member countries. The NEA works closely with the International Atomic Energy Agency (IAEA) in Vienna and with the European Commission in Brussels.\n\nWithin the OECD, there is close co-ordination with the International Energy Agency and the Environment Directorate, as well as contacts with other directorates, as appropriate.\n\n\nSince 1 September 2014, the Director-General of the NEA is William D Magwood, IV, who replaced Luis E. Echávarri on this post. The NEA Secretariat serves seven specialised standing technical committees under the leadership of the Steering Committee for Nuclear Energythe governing body of the NEAwhich reports directly to the OECD Council.\n\nThe standing technical committees, representing each of the seven major areas of the Agency's programme, are composed of member country experts who are both contributors to the programme of work and beneficiaries of its results. The approach is highly cost-efficient as it enables the Agency to pursue an ambitious programme with a relatively small staff that co-ordinates the work. The substantive value of the standing technical committees arises from the numerous important functions they perform, including: providing a forum for in-depth exchanges of technical and programmatic information; stimulating development of useful information by initiating and carrying out co-operation/research on key problems; developing common positions, including \"consensus opinions\", on technical and policy issues; identifying areas where further work is needed and ensuring that NEA activities respond to real needs; organising joint projects to enable interested countries to carry out research on particular issues on a cost-sharing basis.\n\n\n"}
{"id": "701333", "url": "https://en.wikipedia.org/wiki?curid=701333", "title": "Nuclear fission product", "text": "Nuclear fission product\n\nNuclear fission products are the atomic fragments left after a large atomic nucleus undergoes nuclear fission. Typically, a large nucleus like that of uranium fissions by splitting into two smaller nuclei, along with a few neutrons, the release of heat energy (kinetic energy of the nuclei), and gamma rays. The two smaller nuclei are the \"fission products\". (See also Fission products (by element)).\n\nAbout 0.2% to 0.4% of fissions are ternary fissions, producing a third light nucleus such as helium-4 (90%) or tritium (7%).\n\nThe fission products themselves are usually unstable and therefore radioactive; due to being relatively neutron-rich for their atomic number, many of them quickly undergo beta decay. This releases additional energy in the form of beta particles, antineutrinos, and gamma rays. Thus, fission events normally result in beta and gamma radiation, even though this radiation is not produced directly by the fission event itself.\n\nThe produced radionuclides have varying half-lives, and therefore vary in radioactivity. For instance, strontium-89 and strontium-90 are produced in similar quantities in fission, and each nucleus decays by beta emission. But Sr has a 30-year half-life, and Sr a 50.5-day half-life. Thus in the 50.5 days it takes half the Sr atoms to decay, emitting the same number of beta particles as there were decays, less than 0.4% of the Sr atoms have decayed, emitting only 0.4% of the betas. The radioactive emission rate is highest for the shortest lived radionuclides, although they also decay the fastest. Additionally, less stable fission products are less likely to decay to stable nuclides, instead decaying to other radionuclides, which undergo further decay and radiation emission, adding to the radiation output. It is these short lived fission products that are the immediate hazard of spent fuel, and the energy output of the radiation also generates significant heat which must be considered when storing spent fuel. As there are hundreds of different radionuclides created, the initial radioactivity level fades quickly as short lived radionuclides decay, but never ceases completely as longer lived radionuclides make up more and more of the remaining unstable atoms.\n\nThe sum of the atomic mass of the two atoms produced by the fission of one fissile atom is always less than the atomic mass of the original atom. This is because some of the mass is lost as free neutrons, and once kinetic energy of the fission products has been removed (i.e., the products have been cooled to extract the heat provided by the reaction), then the mass associated with this energy is lost to the system also, and thus appears to be \"missing\" from the cooled fission products.\n\nSince the nuclei that can readily undergo fission are particularly neutron-rich (e.g. 61% of the nucleons in uranium-235 are neutrons), the initial fission products are often more neutron-rich than stable nuclei of the same mass as the fission product (e.g. stable zirconium-90 is 56% neutrons compared to unstable strontium-90 at 58%). The initial fission products therefore may be unstable and typically undergo beta decay to move towards a stable configuration, converting a neutron to a proton with each beta emission. (Fission products do not decay via alpha decay.)\n\nA few neutron-rich and short-lived initial fission products decay by ordinary beta decay (this is the source of perceptible half life, typically a few tenths of a second to a few seconds), followed by immediate emission of a neutron by the excited daughter-product. This process is the source of so-called delayed neutrons, which play an important role in control of a nuclear reactor.\n\nThe first beta decays are rapid and may release high energy beta particles or gamma radiation. However, as the fission products approach stable nuclear conditions, the last one or two decays may have a long half-life and release less energy.\n\nFission products have half-lives of 90 years (samarium-151) or less, except for seven long-lived fission products that have half lives of 211,100 years (technetium-99) and more. Therefore, the total radioactivity of a mixture of pure fission products decreases rapidly for the first several hundred years (controlled by the short-lived products) before stabilizing at a low level that changes little for hundreds of thousands of years (controlled by the seven long-lived products).\n\nThis behavior of pure fission products with actinides removed, contrasts with the decay of fuel that still contains actinides. This fuel is produced in the so-called \"open\" (i.e., no nuclear reprocessing) nuclear fuel cycle. A number of these actinides have half lives in the missing range of about 100 to 200,000 years, causing some difficulty with storage plans in this time-range for open cycle non-reprocessed fuels.\n\nProponents of nuclear fuel cycles which aim to consume all their actinides by fission, such as the Integral Fast Reactor and molten salt reactor, use this fact to claim that within 200 years, their fuel wastes are no more radioactive than the original uranium ore.\n\nFission products emit beta radiation, while actinides primarily emit alpha radiation. Many of each also emit gamma radiation.\n\nEach fission of a parent atom produces a different set of fission product atoms. However, while an individual fission is not predictable, the fission products are statistically predictable. The amount of any particular isotope produced per fission is called its yield, typically expressed as percent per parent fission; therefore, yields total to just over 200% (because of ternary fissions), not 100%.\n\nWhile fission products include every element from zinc through the lanthanides, the majority of the fission products occur in two peaks. One peak occurs at about (expressed by atomic number) strontium to ruthenium while the other peak is at about tellurium to neodymium. The yield is somewhat dependent on the parent atom and also on the energy of the initiating neutron.\n\nIn general the higher the energy of the state that undergoes nuclear fission, the more likely that the two fission products have similar mass. Hence as the neutron energy increases and/or the energy of the fissile atom increases, the valley between the two peaks becomes more shallow.\nFor instance, the curve of yield against mass for Pu-239 has a more shallow valley than that observed for U-235 when the neutrons are thermal neutrons. The curves for the fission of the later actinides tend to make even more shallow valleys. In extreme cases such as Fm, only one peak is seen.\n\nThe adjacent figure shows a typical fission product distribution from the fission of uranium. Note that in the calculations used to make this graph, the activation of fission products was ignored and the fission was assumed to occur in a single moment rather than a length of time. In this bar chart results are shown for different cooling times — time after fission.\nBecause of the stability of nuclei with even numbers of protons and/or neutrons, the curve of yield against element is not a smooth curve but tends to alternate. Note that the curve against mass number is smooth.\n\nSmall amounts of fission products are naturally formed as the result of either spontaneous fission of natural uranium, which occurs at a low rate, or as a result of neutrons from radioactive decay or reactions with cosmic ray particles. The microscopic tracks left by these fission products in some natural minerals (mainly apatite and zircon) are used in fission track dating to provide the cooling (crystallization) ages of natural rocks. The technique has an effective dating range of 0.1 Ma to >1.0 Ga depending on the mineral used and the concentration of uranium in that mineral.\n\nAbout 1.5 billion years ago in a uranium ore body in Africa, a natural nuclear fission reactor operated for a few hundred thousand years and produced approximately 5 tonnes of fission products. These fission products were important in providing proof that the natural reactor had occurred.\nFission products are produced in nuclear weapon explosions, with the amount depending on the type of weapon.\nThe largest source of fission products is from nuclear reactors. In current nuclear power reactors, about 3% of the uranium in the fuel is converted into fission products as a by-product of energy generation. Most of these fission products remain in the fuel unless there is fuel element failure or a nuclear accident, or the fuel is reprocessed.\n\nIn commercial nuclear fission reactors, the system is operated in the otherwise self-extinguishing prompt subcritical state. The reactor specific physical phenomena that nonetheless maintains the temperature above the decay heat level, are the predictably delayed, and therefore easily controlled, transformations or movements of a vital class of fission product, or reaction ember, as they decay, with Bromine-87 being one such long-lived \"ember\", with a half-life of about a minute and thus it emits a \"delayed\" neutron upon decay. Operating in this delayed critical state, the dependence on the inherently delayed transformation or movement of fission products/embers to maintain the temperature, is a process that occurs slow enough to permit human feedback on the temperature control. In an analogous manner to fire dampers varying the opening to control the movement of wood embers towards new fuel, control rods are comparatively varied up or down, as the nuclear fuel burns up over time.\n\nIn a nuclear power reactor, the main sources of radioactivity are fission products, alongside actinides and activation products. Fission products are the largest source of radioactivity for the first several hundred years, while actinides are dominant roughly 10 to 10 years after fuel use.\n\nFission occurs in the nuclear fuel, and the fission products are primarily retained within the fuel close to where they are produced. These fission products are important to the operation of the reactor because some fission products contribute delayed neutrons that are useful for reactor control while others are neutron poisons that tend to inhibit the nuclear reaction. The buildup of the fission product poisons is a key factor in determining the maximum duration a given fuel element can be kept within the reactor. The decay of short-lived fission products also provide a source of heat within the fuel that continues even after the reactor has been shut down and the fission reactions stopped. It is this decay heat that sets the requirements for cooling of a reactor after shutdown.\n\nIf the fuel cladding around the fuel develops holes, then fission products can leak into the primary coolant. Depending on the fission product chemistry, it may settle within the reactor core or travel through the coolant system. Coolant systems include chemistry control systems that tend to remove such fission products. In a well-designed power reactor running under normal conditions, the radioactivity of the coolant is very low.\n\nIt is known that the isotope responsible for the majority of the gamma exposure in fuel reprocessing plants (and the Chernobyl site in 2005) is Cs-137. Iodine-129 is one of the major radioactive elements released from reprocessing plants. In nuclear reactors both Cs-137 and strontium-90 are found in locations remote from the fuel. This is because these isotopes are formed by the beta decay of noble gases (xenon-137 {halflife of 3.8 minutes} and krypton-90 {halflife 32 seconds}) which enable these isotopes to be deposited in locations remote from the fuel (e.g. on control rods).\n\nSome fission products decay with the release of a neutron. Since there may be a short delay in time between the original fission event (which releases its own prompt neutrons immediately) and the release of these neutrons, the latter are termed \"delayed neutrons\". These delayed neutrons are important to nuclear reactor control.\n\nSome of the fission products, such as xenon-135 and samarium-149, have a high neutron absorption cross section. Since a nuclear reactor depends on a balance in the neutron production and absorption rates, those fission products that remove neutrons from the reaction will tend to shut the reactor down or \"poison\" the reactor. Nuclear fuels and reactors are designed to address this phenomenon through such features as burnable poisons and control rods. Build-up of xenon-135 during shutdown or low-power operation may poison the reactor enough to impede restart or to interfere with normal control of the reaction during restart or restoration of full power, possibly causing or contributing to an accident scenario.\n\nNuclear weapons use fission as either the partial or the main energy source. Depending on the weapon design and where it is exploded, the relative importance of the fission product radioactivity will vary compared to the activation product radioactivity in the total fallout radioactivity.\n\nThe immediate fission products from nuclear weapon fission are essentially the same as those from any other fission source, depending slightly on the particular nuclide that is fissioning. However, the very short time scale for the reaction makes a difference in the particular mix of isotopes produced from an atomic bomb.\n\nFor example, the Cs/Cs ratio provides an easy method of distinguishing between fallout from a bomb and the fission products from a power reactor. Almost no Cs-134 is formed by nuclear fission (because xenon-134 is stable). The Cs is formed by the neutron activation of the stable Cs which is formed by the decay of isotopes in the isobar (A = 133). So in a momentary criticality by the time that the neutron flux becomes zero too little time will have passed for any Cs to be present. While in a power reactor plenty of time exists for the decay of the isotopes in the isobar to form Cs, the Cs thus formed can then be activated to form Cs only if the time between the start and the end of the criticality is long.\n\nAccording to Jiri Hala's textbook, the radioactivity in the fission product mixture in an atom bomb is mostly caused by short-lived isotopes such as I-131 and Ba-140. After about four months Ce-141, Zr-95/Nb-95, and Sr-89 represent the largest share of radioactive material. After two to three years, Ce-144/Pr-144, Ru-106/Rh-106, and Promethium-147 are the bulk of the radioactivity. After a few years, the radiation is dominated by strontium-90 and caesium-137, whereas in the period between 10,000 and a million years it is technetium-99 that dominates.\n\nSome fission products (such as Cs-137) are used in medical and industrial radioactive sources.\nTcO ion can react with steel surfaces to form a corrosion resistant layer. In this way these metaloxo anions act as anodic corrosion inhibitors - it renders the steel surface passive. The formation of TcO on steel surfaces is one effect which will retard the release of Tc from nuclear waste drums and nuclear equipment which has become lost prior to decontamination (e.g. nuclear submarine reactors which have been lost at sea).\n\nIn a similar way the release of radio-iodine in a serious power reactor accident could be retarded by adsorption on metal surfaces within the nuclear plant. Much of the other work on the iodine chemistry which would occur during a bad accident has been done.\n\nFor fission of uranium-235, the predominant radioactive fission products include isotopes of iodine, caesium, strontium, xenon and barium. The threat becomes smaller with the passage of time. Locations where radiation fields once posed immediate mortal threats, such as much of the Chernobyl Nuclear Power Plant on day one of the accident and the ground zero sites of U.S. atomic bombings in Japan (6 hours after detonation) are now relatively safe because the radioactivity has decayed to a low level.\nMany of the fission products decay through very short-lived isotopes to form stable isotopes, but a considerable number of the radioisotopes have half-lives longer than a day.\n\nThe radioactivity in the fission product mixture is initially mostly caused by short lived isotopes such as Iodine-131 and Ba; after about four months Ce, Zr/Nb and Sr take the largest share, while after about two or three years the largest share is taken by Ce/Pr, Ru/Rh and Pm. Later Sr and Cs are the main radioisotopes, being succeeded by Tc. In the case of a release of radioactivity from a power reactor or used fuel, only some elements are released; as a result, the isotopic signature of the radioactivity is very different from an open air nuclear detonation, where all the fission products are dispersed.\n\nThe purpose of radiological emergency preparedness is to protect people from the effects of radiation exposure after a nuclear accident or bomb. Evacuation is the most effective protective measure. However, if evacuation is impossible or even uncertain, then local fallout shelters and other measures provide the best protection.\n\nAt least three isotopes of iodine are important. I, I (radioiodine) and I. Open air nuclear testing and the Chernobyl disaster both released iodine-131.\n\nThe short-lived isotopes of iodine are particularly harmful because the thyroid collects and concentrates iodide – radioactive as well as stable. Absorption of radioiodine can lead to acute, chronic, and delayed effects. Acute effects from high doses include thyroiditis, while chronic and delayed effects include hypothyroidism, thyroid nodules, and thyroid cancer. It has been shown that the active iodine released from Chernobyl and Mayak has resulted in an increase in the incidence of thyroid cancer in the former Soviet Union.\n\nOne measure which protects against the risk from radio-iodine is taking a dose of potassium iodide (KI) before exposure to radioiodine. The non-radioactive iodide 'saturates' the thyroid, causing less of the radioiodine to be stored in the body.\nAdministering potassium iodide reduces the effects of radio-iodine by 99% and is a prudent, inexpensive supplement to fallout shelters. A low-cost alternative to commercially available iodine pills is a saturated solution of potassium iodide. Long-term storage of KI is normally in the form of reagent grade crystals.\n\nThe administration of known goitrogen substances can also be used as a prophylaxis in reducing the bio-uptake of iodine, (whether it be the nutritional non-radioactive iodine-127 or radioactive iodine, radioiodine - most commonly iodine-131, as the body cannot discern between different iodine isotopes).\nPerchlorate ions, a common water contaminant in the USA due to the aerospace industry, has been shown to reduce iodine uptake and thus is classified as a goitrogen. Perchlorate ions are a competitive inhibitor of the process by which iodide is actively deposited into thyroid follicular cells. Studies involving healthy adult volunteers determined that at levels above 0.007 milligrams per kilogram per day (mg/(kg·d)), perchlorate begins to temporarily inhibit the thyroid gland’s ability to absorb iodine from the bloodstream (\"iodide uptake inhibition\", thus perchlorate is a known goitrogen).\nThe reduction of the iodide pool by perchlorate has dual effects – reduction of excess hormone synthesis and hyperthyroidism, on the one hand, and reduction of thyroid inhibitor synthesis and hypothyroidism on the other. Perchlorate remains very useful as a single dose application in tests measuring the discharge of radioiodide accumulated in the thyroid as a result of many different disruptions in the further metabolism of iodide in the thyroid gland.\n\nTreatment of thyrotoxicosis (including Graves' disease) with 600-2,000 mg potassium perchlorate (430-1,400 mg perchlorate) daily for periods of several months or longer was once common practice, particularly in Europe, and perchlorate use at lower doses to treat thryoid problems continues to this day. Although 400 mg of potassium perchlorate divided into four or five daily doses was used initially and found effective, higher doses were introduced when 400 mg/day was discovered not to control thyrotoxicosis in all subjects.\n\nCurrent regimens for treatment of thyrotoxicosis (including Graves' disease), when a patient is exposed to additional sources of iodine, commonly include 500 mg potassium perchlorate twice per day for 18–40 days.\n\nProphylaxis with perchlorate-containing water at concentrations of 17 ppm, which corresponds to 0.5 mg/kg-day personal intake, if one is 70 kg and consumes 2 litres of water per day, was found to reduce baseline radioiodine uptake by 67% This is equivalent to ingesting a total of just 35 mg of perchlorate ions per day. In another related study where subjects drank just 1 litre of perchlorate-containing water per day at a concentration of 10 ppm, i.e. daily 10 mg of perchlorate ions were ingested, an average 38% reduction in the uptake of iodine was observed.\n\nHowever, when the average perchlorate absorption in perchlorate plant workers subjected to the highest exposure has been estimated as approximately 0.5 mg/kg-day, as in the above paragraph, a 67% reduction of iodine uptake would be expected. Studies of chronically exposed workers though have thus far failed to detect any abnormalities of thyroid function, including the uptake of iodine. this may well be attributable to sufficient daily exposure or intake of healthy iodine-127 among the workers and the short 8 hr biological half life of perchlorate in the body.\n\nTo completely block the uptake of iodine-131 by the purposeful addition of perchlorate ions to a populace's water supply, aiming at dosages of 0.5 mg/kg-day, or a water concentration of 17 ppm, would therefore be grossly inadequate at truly reducing radioiodine uptake. Perchlorate ion concentrations in a region's water supply would need to be much higher, at least 7.15 mg/kg of body weight per day, or a water concentration of 250 ppm, assuming people drink 2 liters of water per day, to be truly beneficial to the population at preventing bioaccumulation when exposed to a radioiodine environment, independent of the availability of iodate or iodide drugs.\n\nThe continual distribution of perchlorate tablets or the addition of perchlorate to the water supply would need to continue for no less than 80–90 days, beginning immediately after the initial release of radioiodine was detected. After 80–90 days passed, released radioactive iodine-131 would have decayed to less than 0.1% of its initial quantity, at which time the danger from biouptake of iodine-131 is essentially over.\n\nIn the event of a radioiodine release, the ingestion of prophylaxis potassium iodide, if available, or even iodate, would rightly take precedence over perchlorate administration, and would be the first line of defense in protecting the population from a radioiodine release. However, in the event of a radioiodine release too massive and widespread to be controlled by the limited stock of iodide and iodate prophylaxis drugs, then the addition of perchlorate ions to the water supply, or distribution of perchlorate tablets would serve as a cheap, efficacious, second line of defense against carcinogenic radioiodine bioaccumulation.\n\nThe ingestion of goitrogen drugs is, much like potassium iodide also not without its dangers, such as hypothyroidism. In all these cases however, despite the risks, the prophylaxis benefits of intervention with iodide, iodate, or perchlorate outweigh the serious cancer risk from radioiodine bioaccumulation in regions where radioiodine has sufficiently contaminated the environment.\n\nThe Chernobyl accident released a large amount of cesium isotopes which were dispersed over a wide area. Cs is an isotope which is of long-term concern as it remains in the top layers of soil. Plants with shallow root systems tend to absorb it for many years. Hence grass and mushrooms can carry a considerable amount of Cs, which can be transferred to humans through the food chain.\n\nOne of the best countermeasures in dairy farming against Cs is to mix up the soil by deeply ploughing the soil. This has the effect of putting the Cs out of reach of the shallow roots of the grass, hence the level of radioactivity in the grass will be lowered. Also the removal of top few centimeters of soil and its burial in a shallow trench will reduce the dose to humans and animals as the gamma photons from Cs will be attenuated by their passage through the soil. The deeper and more remote the trench is, the better the degree of protection.\nFertilizers containing potassium can be used to dilute cesium and limit its uptake by plants.\n\nIn livestock farming, another countermeasure against Cs is to feed to animals prussian blue. This compound acts as an ion-exchanger. The cyanide is so tightly bonded to the iron that it is safe for a human to consume several grams of prussian blue per day. The prussian blue reduces the biological half-life (different from the nuclear half-life) of the cesium. The physical or nuclear half-life of Cs is about 30 years. Cesium in humans normally has a biological half-life of between one and four months. An added advantage of the prussian blue is that the cesium which is stripped from the animal in the droppings is in a form which is not available to plants. Hence it prevents the cesium from being recycled. The form of prussian blue required for the treatment of animals, including humans is a special grade. Attempts to use the pigment grade used in paints have not been successful.\n\nThe addition of lime to soils which are poor in calcium can reduce the uptake of strontium by plants. Likewise in areas where the soil is low in potassium, the addition of a potassium fertilizer can discourage the uptake of cesium into plants. However such treatments with either lime or potash should not be undertaken lightly as they can alter the soil chemistry greatly, so resulting in a change in the plant ecology of the land.\n\nFor introduction of radionuclides into organism, ingestion is the most important route. Insoluble compounds are not absorbed from the gut and cause only local irradiation before they are excreted. Soluble forms however show wide range of absorption percentages.\n\n\nPaul Reuss, \"Neutron Physics\", chp 2.10.2, p 75\n\n"}
{"id": "23896384", "url": "https://en.wikipedia.org/wiki?curid=23896384", "title": "Office of Seed and Plant Introduction", "text": "Office of Seed and Plant Introduction\n\nThe Office of Seed and Plant Introduction was a branch of the United States Department of Agriculture which introduced over 200,000 species and varieties of non-native plants to the United States. It was established in 1898, under the direction of David Fairchild. The department employed agricultural explorers to seek out economically useful plant species to import to the United States. One of the agency's explorers, Frank Nicholas Meyer, introduced the variety of lemon which was later named after him.\n"}
{"id": "67420", "url": "https://en.wikipedia.org/wiki?curid=67420", "title": "On Deadly Ground", "text": "On Deadly Ground\n\nOn Deadly Ground is a 1994 environmental action-adventure film, directed, co-produced by and starring Steven Seagal, and co-starring Michael Caine, Joan Chen, John C. McGinley and R. Lee Ermey. It is, to date, Seagal's only directorial effort, and features a minor appearance by Billy Bob Thornton in one of his early roles.\n\n\"On Deadly Ground\" earned $38.6 million during its theatrical run, failing to bring back its reported $50 million budget and received negative reviews.\n\nAegis Oil operates Aegis 1, an oil refinery and several oil rigs in Alaska. They purchased the oil rights from the local Alaskan Natives 20 years ago, but stand to lose them if the refinery isn’t on-line by a certain deadline. With 13 days to go, and billions of dollars at stake, the company cuts corners and uses faulty equipment. Hugh Palmer, a rig foreman, is aware of this; as he predicts, his rig catches fire. It takes Forrest Taft (Seagal), a specialist in dealing with oil drilling-related fires, to extinguish the fire. Taft refuses to believe Hugh’s story of faulty equipment at first, but later discovers that it is true after accessing the company’s computer records and finding that the next shipments of new, adequate equipment have been delayed way past the deadline. Michael Jennings (Michael Caine), the ruthless CEO of Aegis, deludedly believes that Hugh's carelessness is to blame for the rig fire and, after discovering his efforts to alert the EPA about the use of substandard equipment, arranges for him to be ‘dealt with’ by his henchmen MacGruder (John C. McGinley) and Otto (Sven-Ole Thorsen).\n\nJennings is alerted to Taft's activities and orders that Taft be eliminated as well. MacGruder and Otto brutally ransack Palmer's cabin for the evidence against Jennings, and torture and murder Palmer without finding it. Taft is set up for a trap by investigating a supposedly damaged pump station. He is badly wounded by an explosion, but survives and is rescued by Masu (Joan Chen), the daughter of Silook, the chief of her tribe.\n\nMacGruder and Otto are unable to locate Taft's body, and Jennings assumes that he is still alive. Taft is being cared for by Silook's tribe. After unsuccessfully trying to leave using a dogsled, Silook has Taft undergo a vision quest in which he sees the truth. When made to choose between two women, Taft opts for the elderly, clothed grandmother, forgoing the erotically-charged nude Iñupiaq seductress. The grandmother warns Taft that time is running out for those who pollute the world. Taft realizes that his only option is to see the refinery closed. He takes off, with MacGruder and Otto hot on his trail.\n\nAt Silook's village, they demand to know where Taft is. Silook refuses to give the information and is fatally shot by MacGruder. Jennings berates MacGruder for killing Silook in front of his entire tribe. They bring in a group of New Orleans-based mercenaries led by Stone (R. Lee Ermey) to finish off Taft before Aegis 1 can go on-line. They also have an FBI Anti-Terrorist Unit at the refinery.\n\nAccompanied by Masu, Taft (who is probably ex-CIA and an expert on sabotage and demolition), collects weapons and explosives and manages to enter the refinery complex, and begins to effectively sabotage the refinery. MacGruder (who is killed by Taft in the process of getting thrown into the helicopter's tail rotor blades for killing Hugh and Silook), Otto (who was killed earlier at Hugh's cabin), Stone (who is killed by Taft with his own shotgun when he confronts him at the platform) and Jennings’ ruthlessly efficient female assistant Liles (who crashes her truck into a gasoline tank in an escape attempt), are powerless to defeat him and are all killed in various gruesome ways; the FBI also pulls out, revealing in the process that Taft might be ex-CIA.\nTaft and Masu confront Jennings, string him up, and drop him into a pool of oil, effectively drowning Jennings in his own wealth. They then escape as a series of explosions destroy the rest of Aegis 1.\n\nAs an epilogue, Taft, far from being arrested for sabotage and multiple murders (self defense), is asked to deliver a speech at the Alaska State Capitol about the dangers of oil pollution, and the companies that are endangering the ecosystem. During the speech they show a scene of one of the first commercial hydrogen fuel cell systems developed by Perry Energy Systems.\n\n\n\"On Deadly Ground\" has received negative reviews from critics. The film bears a 10% freshness rating on the review aggregate site Rotten Tomatoes based on 30 reviews. Audiences polled by CinemaScore gave the film an average grade of \"B+\" on an A+ to F scale.\n\nAt the time of its release, Gene Siskel included the film in his \"Worst of\" list for 1994, singling out the melancholy tone of the film, and the quality of Seagal's dialogue. On their syndicated TV show Siskel & Ebert, Siskel called the film's pyrotechnics \"low rent\" and stated that he \"didn't think the fight sequences were anything special.\" However, he noted that Seagal's speech at the end was \"more interesting than the actual fighting.\" Roger Ebert, for his part, called the speech \"absurd\" and \"shameless\" but opined that while \"it doesn't pay to devote close attention to the plot,\" \"if you like to see lots of stuff blowed up real good, this’d be a movie for you.\" The film is listed in Golden Raspberry Award founder John Wilson's book \"The Official Razzie Movie Guide\" as one of The 100 Most Enjoyably Bad Movies Ever Made.\n\n\"Variety\" film critic Leonard Klady referred to the film as \"a vanity production parading as a social statement\" and commented that the film seemingly borrowed heavily from the earlier film, \"Billy Jack\" but opined that Seagal lacked \"acting technique and the ability behind the camera to keep the story simple and direct\" that \"Billy Jack\" star Tom Laughlin, exhibited. Like Siskel, Klady also singled out the speech by Seagal's character at the end of the film.\n\n\"Seagalogy\" author Vern considers \"On Deadly Ground\" to be one of Seagal's defining works, writing, \"It's the corniest, most unintentionally hilarious movie of his career... But it's also Seagal's most sincere and his most ballsy,\" going on to claim, \"You can't understand Seagal if you haven't seen \"On Deadly Ground\".\" He points out that many of the most important themes and motifs that define Seagal's work are present here more overtly than in any of his other films.\n\n"}
{"id": "57150908", "url": "https://en.wikipedia.org/wiki?curid=57150908", "title": "PETase", "text": "PETase\n\nPETases are an esterase class of enzymes that catalyze the hydrolysis of poly (ethylene terephthalate) PET plastic to monomeric mono-2-hydroxyethyl terephthalate (MHET). The idealized chemical reaction is (where n is the number of monomers in the polymer chain):\n\nTrace amount of the PET breaks down to bis(2-hydroxyethyl) terephthalate (BHET). PETases can also break down PEF-plastic (polyethylene-2,5-furandicarboxylate), which is a bioderived PET replacement. PETases can't catalyze the hydrolysis of aliphatic polyesters like polybutylene succinate or polylactic acid.\n\nThe first PETase enzyme was discovered in 2016 from \"Ideonella sakaiensis\" strain 201-F6 bacteria found from sludge samples collected close to a Japanese PET bottle recycling site. Scientists suggested that the PETase enzyme may have had past enzymatic activity associated with degradation of a waxy coating on plants. Normally the natural degradation of PET without PETase will take hundred of years. PET (polyethylene terephthalate-plastics)is a very common source of many plastic items used in the daily life. PETase can degrade PET in a way that is not harmful to the environment. Other types of PET degrading hydolases have been known before this discovery. These include hydrolases such as: lipases, esterases, and cutinases. Discoveries of polyester degrading enzymes date at least as far back as 1975 (α-chymotrypsin) and 1977 (lipase) for example. PET plastic was put into widespread use in the 1970s and it has been suggested that PETases in bacteria evolved only recently.\n\nPETase hydrolyses PET (polyethylene terephthalate) into soluble building blocks due to reaction with water which is a bioconversion of plastics. PET is a polymer composed of ester bond-linked terephthalate (TPA) and ethylene glycol (EG). A high molecular weight and other properties make PET a great utilizing plastic. By the hydrolysis reaction PET hydrolyzing enzymes decompose PET into building blocks which is helpful for the environment. During the hydrolyzing PET, the enzyme produces mono-(2-hydroxyethyl) terephthalic acid (MHET), TPA ,and bis-2(hydroxyethyl) TPA (BHET). The novel bacteria called \"Ideonella sakaieensis\" is isolating and it utilize PET as an energy and carbon source. The \"deonella sakaieensis\" sticks to the surface of PET and keep a cutinase enzyme that allow PET to degrade. That reaction allows to degrade PET, and make it less harmful for the environment. \n\nMHET is broken down in \"I. sakaiensis\" by the action of MHETase enzyme to terephthalic acid and ethylene glycol. These are environmentally harmless as they are broken down further to produce carbon dioxide and water. The \"I. sakaiensis\" bacterium adhere to the PET surface and release a unique enzyme, similar to cutinase, with the ability to degrade PET. These are environmentally harmless as they are broken down further to produce carbon dioxide and water.\n\nAs of April 2018, there were 13 known three-dimensional crystal structures of PETases: 6EQD, 6EQE, 6EQF, 6EQG, 6EQH, 6ANE, 5XJH, 5YNS, 5XFY, 5XFZ, 5XG0, 5XH2 and 5XH3. PETase exhibits shared qualities with both lipases and cutinases in that it possesses an α/β-hydrolase fold; although, the active-site cleft observed in PETase is more open than in cutinases. Scientists revolutionized the degradation rate of PET by PETase as a result of narrowing of the binding site through mutation of two active-site residues, although there are three comprising the active site. In the location of the active site, a catalytic triad is formed by the three residues Ser160, Asp206, and His237. \n\nIn 2018 scientists from the University of Portsmouth with the collaboration of the National Renewable Energy Laboratory of the United States Department of Energy developed a mutant of this PETase that degrades PET faster than the one in its natural state. In this study it was also shown that PETases can degrade polyethylene 2,5-furandicarboxylate (PEF).\n\nThere are approximately 69 PETase-like enzymes comprising a variety of diverse organisms, and there are two classifications of these enzymes including type I and type II. It is suggested that 57 enzymes fall into the type I category whereas the rest fall into the type II group, including the PETase enzyme found in the \"Ideonella sakaiensis.\" Within all 69 PETase-like enzymes, there exists the same three residues within the active site, suggesting that the catalytic mechanism is the same in all forms of PETase-like enzymes. \n\n"}
{"id": "2966831", "url": "https://en.wikipedia.org/wiki?curid=2966831", "title": "Pachyrhizus erosus", "text": "Pachyrhizus erosus\n\nPachyrhizus erosus, commonly known as jicama ( or ; Spanish \"jícama\" ; from Nahuatl \"xīcamatl\", ), Mexican yam bean, or Mexican turnip, is the name of a native Mexican vine, although the name most commonly refers to the plant's edible tuberous root. Jícama is a species in the genus \"Pachyrhizus\" in the bean family (Fabaceae). Plants in this genus are commonly referred to as yam bean, although the term \"yam bean\" can be another name for jícama. The other major species of yam beans are also indigenous within the Americas. \"Pachyrhizus tuberosus\" and \"Pachyrhizus ahipa\" are the other two cultivated species. The naming of this group of edible plants seems confused, with much overlap of similar or the same common names.\n\nFlowers, either blue or white, and pods similar to lima beans, are produced on fully developed plants. Several species of jicama occur, but the one found in many markets is \"P. erosus\". The two cultivated forms of \"P. erosus\" are \"jicama de agua\" and \"jicama de leche\", both named for the consistency of their juice. The \"leche\" form has an elongated root and milky juice, while the \"agua\" form has a top-shaped to oblate root and a more watery, translucent juice, and is the preferred form for market.\n\nOther names for jicama include Mexican potato, \"ahipa\", \"saa got\", Chinese potato, and sweet turnip. In Ecuador and Peru, the name \"jicama\" is used for the unrelated \"yacón\" or Peruvian ground apple, a plant of the sunflower family whose tubers are also used as food.\n\nThe jícama vine can reach a height of 4–5 m given suitable support. Its root can attain lengths up to 2 m and weigh up to 20 kg. The heaviest jícama root ever recorded weighed 23 kg and was found in 2010 in the Philippines (where they are called \"singkamas\").\nJicama is frost-tender and requires 9 months without frost for a good harvest of large tubers or to grow it commercially. It is worth growing in cooler areas that have at least 5 months without frost, as it will still produce tubers, but they will be smaller.\nWarm, temperate areas with at least 5 months without frost can start seed 8 to 10 weeks before the last spring frost. Bottom heat is recommended, as the seeds require warm temperatures to germinate, so the pots will need to be kept in a warm place. Jicama is unsuitable for areas with a short growing season unless cultured in a greenhouse. Growers in tropical areas can sow seed at any time of the year. Those in subtropical areas should sow seed once the soil has warmed in the spring.\n\nThe root's exterior is yellow and papery, while its inside is creamy white with a crisp texture that resembles raw potato or pear. The flavor is sweet and starchy, reminiscent of some apples or raw green beans, and it is usually eaten raw, sometimes with salt, lemon, or lime juice, \"alguashte\", and chili powder. It is also cooked in soups and stir-fried dishes.\nJícama is often paired with chili powder, cilantro, ginger, lemon, lime, orange, red onion, salsa, sesame oil, grilled fish, and soy sauce. It can be cut into thin wedges and dipped in salsa. In Mexico, it is popular in salads, fresh fruit combinations, fruit bars, soups, and other cooked dishes. In contrast to the root, the remainder of the jícama plant is very poisonous; the seeds contain the toxin rotenone, which is used to poison insects and fish.\n\nSpaniards spread cultivation of jícama from Mexico to the Philippines (where it is known as \"singkamas\", from Nahuatl \"xicamatl\"), from there it went to China and other parts of Southeast Asia, where notable uses of raw jícama include \"popiah, \" fresh \"lumpia\" in the Philippines, and salads in Indonesia, Singapore, and Malaysia such as \"yusheng\" and \"rojak\".\n\nIn the Philippines, jícama is usually eaten fresh with condiments such as rice vinegar and sprinkled with salt, or with \"bagoong\" (shrimp paste). In Malay, it is known by the name \"ubi sengkuang\". In Indonesia, jícama is known as \"bengkuang\". This root crop is also known by people in Sumatra and Java, and eaten at fresh fruit bars or mixed in the \"rojak\" (a kind of spicy fruit salad). Padang, a city in West Sumatra, is called \"the city of \"bengkuang\"\". Local people might have thought that this jícama is the \"indigenous crop\" of Padang. The crop has been grown everywhere in this city and it has become a part of their culture.\n\nIt is known by its Chinese name \"bang kuang\" to the ethnic Chinese in Southeast Asia. In Mandarin Chinese, it is known as \"dòushǔ\"(豆薯) or \"liáng shǔ\" (涼薯), as \"sa1 got\" 沙葛 (same as \"turnip\") in Yue Chinese/Cantonese, and as \"mang-guang\" 芒光 in Teochew, where the word is borrowed from the Malay, and as \"dìguā\" 地瓜 in Guizhou province and several neighboring provinces of China, the latter term being shared with sweet potatoes. Jícama has become popular in Vietnamese food as an ingredient in pie, where it is called \"cây củ đậu\" (in northern Vietnam) or \"củ sắn\" or \"sắn nước\" (in southern Vietnam).\n\nIn Japanese, it is known as 葛芋 (\"kuzu-imo\"). In Myanmar, it is called စိမ်းစားဥ (\"sane-saar-u\"). Its Thai name is มันแกว (\"man kaeo\"). In Cambodia, it is known as ដំឡូងរលួស /dɑmlɔoŋ rəluəh/ or under its Chinese name as ប៉ិកួៈ ~ ប៉ិគក់ /peʔkŭəʔ/. In Bengali, it is known as \"shankhalu\" (শাঁখ আলু), literally translating to \"conch (\"shankha\", শাঁখ) potato (\"alu\", আলু)\" for its shape, size, and colour. In Hindi, it is known as \"mishrikand\" (मिश्रीकंद). It is eaten during fast (उपवास) in Bihar (India) and is known as \"kesaur\" (केसौर). In Odia, it is known as (ଶଙ୍ଖ ସାରୁ) \"shankha saru\". In Laos, it is called \"man phao\" (ມັນເພົາ), smaller and tastes a little sweeter than the Mexican type. It is used as a snack by peeling off the outer layer of the skin, then cutting into bite sizes for eating like an apple or a pear.\n\nJícama is high in carbohydrates in the form of dietary fiber (notably inulin). It is composed of 86–90% water; it contains only trace amounts of protein and lipids. Its sweet flavor comes from the oligofructose inulin (also called fructo-oligosaccharide), which is a prebiotic. Jícama is very low in saturated fat and sodium. It is also a good source of vitamin C.\n\nJícama should be stored dry, between 12 and 16°C (53 and 60°F). As colder temperatures will damage the roots, whole unpeeled jicama root should not be refrigerated. A fresh root stored at an appropriate temperature will keep for a month or two.\n\n"}
{"id": "13871411", "url": "https://en.wikipedia.org/wiki?curid=13871411", "title": "Plasma-immersion ion implantation", "text": "Plasma-immersion ion implantation\n\nPlasma-immersion ion implantation (PIII) or pulsed-plasma doping (pulsed PIII) is a surface modification technique of extracting the accelerated ions from the plasma by applying a high voltage pulsed DC or pure DC power supply and targeting them into a suitable substrate or electrode with a semiconductor wafer placed over it, so as to implant it with suitable dopants. The electrode is a cathode for an electropositive plasma, while it is an anode for an electronegative plasma. Plasma can be generated in a suitably designed vacuum chamber with the help of various plasma sources such as Electron Cyclotron Resonance plasma source which yields plasma with the highest ion density and lowest contamination level, helicon plasma source, capacitively coupled plasma source, inductively coupled plasma source, DC glow discharge and metal vapor arc(for metallic species). The vacuum chamber can be of two types - \"diode\" and \"triode\" type depending upon whether the power supply is applied to the substrate as in the former case or to the perforated grid as in the latter.\n\nIn a conventional \"immersion\" type of PIII system, also called as the \"diode\" type configuration, the wafer is kept at a negative potential since the positively charged ions of the electropositive plasma are the ones who get extracted and implanted. The wafer sample to be treated is placed on a sample holder in a vacuum chamber. The sample holder is connected to a high voltage power supply and is electrically insulated from the chamber wall. By means of pumping and gas feed systems, an atmosphere of a working gas at a suitable pressure is created.\n\nWhen the substrate is biased to a negative voltage (few KV's), the resultant electric field drives electrons away from the substrate in the time scale of the inverse electron plasma frequency ω ( ~ l0 sec). Thus an ion matrix Debye sheath which is depleted of electrons forms around it. The negatively biased substrate will accelerate the ions within a time scale of the inverse ion plasma frequency ω ( ~ 10 sec). This ion movement lowers the ion density in the bulk, which causes the sheath-plasma boundary to expand in order to sustain the applied potential drop,in the process exposing more ions. The plasma sheath expands until either a steady-state condition is reached, which is called Child Langmuir law limit; or the high voltage is switched off as in the case of Pulsed DC biasing. Pulse biasing is preferred over DC biasing because it creates less damage during the pulse ON time and neutralization of unwanted charges accumulated on the wafer in the afterglow period (i.e. after the pulse has ended). In case of pulsed biasing the T time of the pulse is generally kept at 20-40 µs, while the T is kept at 0.5-2 ms i.e. a duty cycle of 1-8%. The power supply used is in range of 500 V to hundreds of KV and the pressure in the range of 1-100 mTorr. This is the basic principle of the operation of \"immersion\" type PIII.\n\nIn case of a \"triode\" type configuration, a suitable perforated grid is placed in between the substrate and the plasma and a pulsed DC bias is applied to this grid. Here the same theory applies as previously discussed, but with a difference that the extracted ions from the grid holes bombard the substrate, thus causing implantation. In this sense a \"triode\" type PIII implanter is a crude version of ion implantation because it does not contain plethora of components like ion beam steering, beam focusing, additional grid accelerators etc.\n\n\nC.R. Viswanathan, \"Plasma induced damage,\" \"Microelectronic Engineering\", Vol. 49, No. 1-2, November 1999, pp. 65–81.\n"}
{"id": "72227", "url": "https://en.wikipedia.org/wiki?curid=72227", "title": "Plywood", "text": "Plywood\n\nPlywood is a material manufactured from thin layers or \"plies\" of wood veneer that are glued together with adjacent layers having their wood grain rotated up to 90 degrees to one another. It is an engineered wood from the family of manufactured boards which includes medium-density fibreboard (MDF) and particle board (chipboard).\n\nAll plywoods bind resin and wood fibre sheets (cellulose cells are long, strong and thin) to form a composite material. This alternation of the grain is called \"cross-graining\" and has several important benefits: it reduces the tendency of wood to split when nailed in at the edges; it reduces expansion and shrinkage, providing improved dimensional stability; and it makes the strength of the panel consistent across all directions. There is usually an odd number of plies, so that the sheet is balanced—this reduces warping. Because plywood is bonded with grains running against one another and with an odd number of composite parts, it has high stiffness perpendicular to the grain direction of the surface ply. \n\nSmaller, thinner, and lower-quality plywoods may only have their plies (layers) arranged at right angles to each other. Some better-quality plywood products will by design have five plies in steps of 45 degrees (0, 45, 90, 135, and 180 degrees), giving strength in multiple axes.\n\nThe word \"ply\" derives from the French verb , \"to fold\", from the Latin verb , from the ancient Greek verb .\n\nIn 1797 Samuel Bentham applied for patents covering several machines to produce veneers. In his patent applications, he described the concept of laminating several layers of veneer with glue to form a thicker piece – the first description of what we now call plywood. Bentham was a British naval engineer with many shipbuilding inventions to his credit. Veneers at the time of Bentham were flat sawn, rift sawn or quarter sawn; i.e. cut along or across the log manually in different angles to the grain and thus limited in width and length.\n\nAbout fifty years later Immanuel Nobel, father of Alfred Nobel, realized that several thinner layers of wood bonded together would be stronger than a single thick layer of wood. Understanding the industrial potential of laminated wood, he invented the rotary lathe. \n\nThere is little record of the early implementation of the rotary lathe and the subsequent commercialization of plywood as we know it today, but in its 1870 edition, the French dictionary \"Robert\" describes the process of rotary lathe veneer manufacturing in its entry \"Déroulage\". One can thus presume that rotary lathe plywood manufacturer was an established process in France in the 1860s. Plywood was introduced into the United States in 1865 and industrial production started shortly after. In 1928, the first standard-sized 4 ft by 8 ft (1.2 m by 2.4 m) plywood sheets were introduced in the United States for use as a general building material.\n\nArtists use plywood as a support for easel paintings to replace traditional canvas or cardboard. Ready-made \"artist boards\" for oil painting in three-layered plywood (3-ply) were produced and sold in New York as early as 1880.\n\nA typical plywood panel has face veneers of a higher grade than the core veneers. The principal function of the core layers is to increase the separation between the outer layers where the bending stresses are highest, thus increasing the panel's resistance to bending. As a result, thicker panels can span greater distances under the same loads. In bending, the maximum stress occurs in the outermost layers, one in tension, the other in compression. Bending stress decreases from the maximum at the face layers to nearly zero at the central layer. Shear stress, by contrast, is higher in the center of the panel, and at the outer fibres.\n\nDifferent varieties of plywood exist for different applications:\n\nSoftwood plywood is usually made either of cedar, Douglas fir or spruce, pine, and fir (collectively known as spruce-pine-fir or SPF) or redwood and is typically used for construction and industrial purposes.\n\nThe most common dimension is or the slightly larger imperial dimension of 4 feet × 8 feet. Plies vary in thickness from 1.4 mm to 4.3 mm. The number of plies—which is always odd—depends on the thickness and grade of the sheet. Roofing can use the thinner plywood. Subfloors are at least thick, the thickness depending on the distance between floor joists. Plywood for flooring applications is often tongue and groove (T&G); This prevents one board from moving up or down relative to its neighbor, providing a solid-feeling floor when the joints do not lie over joists. T&G plywood is usually found in the range.\n\nHardwood plywood is made out of wood from angiosperm trees and used for demanding end uses. Hardwood plywood is characterized by its excellent strength, stiffness and resistance to creep. It has a high planar shear strength and impact resistance, which make it especially suitable for heavy-duty floor and wall structures. Oriented plywood construction has a high wheel-carrying capacity. Hardwood plywood has excellent surface hardness, and damage- and wear-resistance.\n\nTropical plywood is made of mixed species of tropical timber. Originally from the Asian region, it is now also manufactured in African and South American countries. Tropical plywood is superior to softwood plywood due to its density, strength, evenness of layers, and high quality. It is usually sold at a premium in many markets if manufactured with high standards. Tropical plywood is widely used in the UK, Japan, United States, Taiwan, Korea, Dubai, and other countries worldwide. It is used for construction purposes in many regions due to its low cost. However, many countries’ forests have been over-harvested, including the Philippines, Malaysia and Indonesia, largely due to the demand for plywood production and export.\n\nHigh-strength plywood, also known as aircraft plywood, is made from mahogany, spruce and/or birch using adhesives with an increased resistance to heat and humidity. It was used in the construction of air assault gliders during World War II and also several fighter aircraft, most notably the multi-role British Mosquito. Nicknamed \"The Wooden Wonder\" plywood was used for the wing surfaces, and also flat sections such as bulkheads and the webs of the wing spars. The fuselage had exceptional rigidity from the bonded ply-balsa-ply ‘sandwich’ of its monocoque shell; elliptical in cross-section, it was formed in two separate mirror-image halves, using curved moulds.\n\nStructural aircraft-grade plywood is most commonly manufactured from African mahogany, spruce or birch veneers that are bonded together in a hot press over hardwood cores of basswood or poplar or from European Birch veneers throughout. Basswood is another type of aviation-grade plywood that is lighter and more flexible than mahogany and birch plywood but has slightly less structural strength. Aviation-grade plywood is manufactured to a number of specifications including those outlined since 1931 in the Germanischer Lloyd Rules for Surveying and Testing of Plywood for Aircraft and MIL-P-607, the latter of which calls for shear testing after immersion in boiling water for three hours to verify the adhesive qualities between the plies and meets specifications.\n\nUsually faced with hardwood, including ash, oak, red oak, birch, maple, mahogany, Philippine mahogany (often called lauan, luan or meranti and having no relation to true mahogany), rosewood, teak and a large number of other hardwoods.\n\nFlexible plywood is designed for making curved parts, a practice which dates back to the 1850s in furniture making.\n\nAircraft grade plywood, often Baltic birch, is made from three or more plies of birch, as thin as thick in total, and is extremely strong and light. At thick, mahogany three-ply \"wiggle board\" or \"bendy board\" come in sheets with a very thin cross-grain central ply and two thicker exterior plies, either long grain on the sheet, or cross grain. Wiggle board is often glued together in two layers once it is formed into the desired curve, so that the final shape will be stiff and resist movement. Often, decorative wood veneers are added as a surface layer.\n\nIn the United Kingdom single-ply sheets of veneer were used to make stovepipe hats in Victorian times, so flexible modern plywood is sometimes known there as \"hatters ply\", although the original material was not strictly plywood, but a single sheet of veneer.\n\nMarine plywood is manufactured from durable face and core veneers, with few defects so it performs longer in both humid and wet conditions and resists delaminating and fungal attack. Its construction is such that it can be used in environments where it is exposed to moisture for long periods. Each wood veneer will be from tropical hardwoods, have negligible core gap, limiting the chance of trapping water in the plywood and hence providing a solid and stable glue bond. It uses an exterior Water and Boil Proof (WBP) glue similar to most exterior plywoods.\n\nMarine plywood can be graded as being compliant with BS 1088, which is a British Standard for marine plywood. There are few international standards for grading marine plywood and most of the standards are voluntary. Some marine plywood has a Lloyd's of London stamp that certifies it to be BS 1088 compliant. Some plywood is also labeled based on the wood used to manufacture it. Examples of this are Okoumé or Meranti.\n\nOther types of plywoods include fire-retardant, moisture-resistant, wire mesh, sign-grade, and pressure-treated. However, the plywood may be treated with various chemicals to improve the plywood's fireproofing. Each of these products is designed to fill a need in industry.\n\nBaltic Birch plywood is a product of an area around the Baltic Sea. Originally manufactured for European cabinet makers but now popular in the United States as well. It is very stable composed of an inner void-free core of cross-banded birch plys with an exterior grade adhesive. The face veneers are thicker than traditional cabinet grade plywood.\n\nPlywood production requires a good log, called a peeler, which is generally straighter and larger in diameter than one required for processing into dimensioned lumber by a sawmill. The log is laid horizontally and rotated about its long axis while a long blade is pressed into it, causing a thin layer of wood to peel off (much as a continuous sheet of paper from a roll). An adjustable nosebar, which may be solid or a roller, is pressed against the log during rotation, to create a \"gap\" for veneer to pass through between the knife and the nosebar. The nosebar partly compresses the wood as it is peeled; it controls vibration of the peeling knife; and assists in keeping the veneer being peeled to an accurate thickness. In this way the log is peeled into sheets of veneer, which are then cut to the desired oversize dimensions, to allow it to shrink (depending on wood species) when dried. The sheets are then patched, graded, glued together and then baked in a press at a temperature of at least , and at a pressure of up to (but more commonly 200 psi) to form the plywood panel. The panel can then be patched, have minor surface defects such as splits or small knot holes filled, re-sized, sanded or otherwise refinished, depending on the market for which it is intended.\n\nPlywood for indoor use generally uses the less expensive urea-formaldehyde glue, which has limited water resistance, while outdoor and marine-grade plywood are designed to withstand moisture, and use a water-resistant phenol-formaldehyde glue to prevent delamination and to retain strength in high humidity.\n\nAnti-fungal additives such as Xyligen (Furmecyclox) may sometimes be added to the glueline to provide added resistance to fungal attack.\n\nThe adhesives used in plywood have become a point of concern. Both urea formaldehyde and phenol formaldehyde are carcinogenic in very high concentrations. As a result, many manufacturers are turning to low formaldehyde-emitting glue systems, denoted by an \"E\" rating. Plywood produced to \"E0\" has effectively zero formaldehyde emissions.\n\nIn addition to the glues being brought to the forefront, the wood resources themselves are becoming the focus of manufacturers, due in part to energy conservation, as well as concern for natural resources. There are several certifications available to manufacturers who participate in these programs. Programme for the Endorsement of Forest Certification (PEFC) Forest Stewardship Council (FSC), Leadership in Energy and Environmental Design (LEED), Sustainable Forestry Initiative (SFI), and Greenguard are all certification programs that ensure that production and construction practices are sustainable. Many of these programs offer tax benefits to both the manufacturer and the end user.\n\nThe most commonly used thickness range is from . The sizes of the most commonly used plywood sheets are 4 x 8 feet (1220 x 2440 mm) which was first used by the Portland Manufacturing Company, who developed what we know of as modern veneer core plywood for the 1905 Portland World Fair. A common metric size for a sheet of plywood is 1200 x 2400 mm. is also a common European size for Baltic birch ply, and aircraft ply.\n\nSizes on specialised plywood for concrete-forming can range from , and a multitude of formats exist, though 15 ×  (19/32in × 2 ft-6in × 4 ft-11in) is very commonly used.\n\nAircraft plywood is available in thicknesses of (3 ply construction) and upwards; typically aircraft plywood uses veneers of 0.5 mm (approx 1/64 in) thickness although much thinner veneers such as 0.1 mm are also used in construction of some of the thinner panels.\n\nGrading rules differ according to the country of origin. The most popular standards are the British Standard (BS) and the American Standard (ASTM). Joyce (1970), however, list some general indication of grading rules:\nJPIC Standards\n\nPlywood is used in many applications that need high-quality, high-strength sheet material. Quality in this context means resistance to cracking, breaking, shrinkage, twisting and warping.\n\nExterior glued plywood is suitable for outdoor use, but because moisture affects the strength of wood, optimal performance is achieved where the moisture content remains relatively low. Subzero conditions do not affect the dimensional or strength properties of plywood, making some special applications possible.\n\nPlywood is also used as an engineering material for stressed-skin applications. It has been used for marine and aviation applications since WWII. Most notable is the British de Havilland Mosquito bomber, with a fuselage made of birch plywood sandwiching a balsa core, and using plywood extensively for the wings. Plywood was also used for the hulls in the hard-chine Motor Torpedo Boats (MTB) and Motor Gun Boats (MGB) built by the British Power Boat Company and Vosper's. Plywood is currently successfully used in stressed-skin applications. The American designers Charles and Ray Eames are known for their plywood-based furniture, as is Finnish Architect Alvar Aalto and his firm Artek, while Phil Bolger has designed a wide range of boats built primarily of plywood. Jack Köper of Cape Town designed the plywood Dabchick sailing dinghy, which is still sailed by large numbers of teenagers.\n\nPlywood is often used to create curved surfaces because it can easily bend with the grain. Skateboard ramps often utilize plywood as the top smooth surface over bent curves to create transition that can simulate the shapes of ocean waves.\n\nTypical end uses of spruce plywood are:\n\nThere are coating solutions available that mask the prominent grain structure of spruce plywood. For these coated plywoods there are some end uses where reasonable strength is needed but the lightness of spruce is a benefit e.g.:\n\nPhenolic resin film coated (Film Faced) plywood is typically used as a ready-to-install component e.g.:\n\nBirch plywood is used as a structural material in special applications e.g.:\n\nSmooth surface and accurate thickness combined with the durability of the material makes birch plywood a favorable material for many special end uses e.g.:\n\nTropical plywood is widely available from the South-East Asia region, mainly from Malaysia and Indonesia.\n\n\n"}
{"id": "43051375", "url": "https://en.wikipedia.org/wiki?curid=43051375", "title": "Sikmogil", "text": "Sikmogil\n\nSikmogil is a holiday in South Korea, the Korean Arbor Day. It is celebrated annually on April 5. This day was designated by presidential decree in 1949 following legislation by the National Assembly.\n\nIn 1960, Sikmogil's status as a holiday was abolished and April 5 was treated as any other day. However, the following year the official status of the holiday was restored. In 2006, Sikmogil's holiday status was abolished again.\n\nThe idea of Sikmogil was to celebrate forestry and the development of national history. The day of April 5 was chosen for its historical significance. On April 5, Silla achieved the unification of the Three Kingdoms of Korea.\n\nOn Sikmogil, South Korean people plant trees that are appropriate for the region's climate. Government offices help people plant trees. During the month of Sikmogil, the government encourages the economical utilization of forestry by designating a \"National Planting Period.\" Even though Sikmogil was abolished in 2006 as a holiday, the South Korean public continues to take part in meaningful activities.\n\n"}
{"id": "24163839", "url": "https://en.wikipedia.org/wiki?curid=24163839", "title": "The Conservation Society", "text": "The Conservation Society\n\nThe Conservation Society was an early British environmental organisation founded by Douglas M. C. MacEwan in 1966, in response to what were seen to be fundamental ecological constraints on continued economic growth and population growth in the UK.\n\nThe Society received particular press attention in November 1969 for its submission to the House of Commons Select Committee on Science and Technology of a paper entitled \"Why Britain Needs a Population Policy\".\n\nThe Society was directed by John Davoll from 1970 until its winding-up in 1987.\n\nThe Society published, in association with Volturna Press, in 1968 and 1969, three compilations of articles sympathetic to its aims as 'Conservation Society Reprints' Volumes I, II and 3. This was succeeded by 'Conservation Topics' with original material. \n\nNote: As a past (non activist) member of the CS I recollect that the society broke up due to disagreements over nuclear issues. \n"}
{"id": "9209712", "url": "https://en.wikipedia.org/wiki?curid=9209712", "title": "Thermoelectric generator", "text": "Thermoelectric generator\n\nA thermoelectric generator (TEG), also called a Seebeck generator, is a solid state device that converts heat flux (temperature differences) directly into electrical energy through a phenomenon called the Seebeck effect (a form of thermoelectric effect). Thermoelectric generators function like heat engines, but are less bulky and have no moving parts. However, TEGs are typically more expensive and less efficient.\n\nThermoelectric generators could be used in power plants in order to convert waste heat into additional electrical power and in automobiles as automotive thermoelectric generators (ATGs) to increase fuel efficiency. Another application is radioisotope thermoelectric generators which are used in space probes, which has the same mechanism but use radioisotopes to generate the required heat difference.\n\nIn 1821, Thomas Johann Seebeck rediscovered that a thermal gradient formed between two dissimilar conductors can produce electricity. At the heart of the thermoelectric effect is the fact that a temperature gradient in a conducting material results in heat flow; this results in the diffusion of charge carriers. The flow of charge carriers between the hot and cold regions in turn creates a voltage difference. In 1834, Jean Charles Athanase Peltier discovered the reverse effect, that running an electric current through the junction of two dissimilar conductors could, depending on the direction of the current, cause it to act as a heater or cooler.\n\nThermoelectric power generators consist of three major components: thermoelectric materials, thermoelectric modules and thermoelectric systems that interface with the heat source.\n\nThermoelectric materials generate power directly from heat by converting temperature differences into electric voltage. These materials must have both high electrical conductivity (σ) and low thermal conductivity (κ) to be good thermoelectric materials. Having low thermal conductivity ensures that when one side is made hot, the other side stays cold, which helps to generate a large voltage while in a temperature gradient. The measure of the magnitude of electrons flow in response to a temperature difference across that material is given by the Seebeck coefficient (S). The efficiency of a given material to produce a thermoelectric power is governed by its “figure of merit” zT = SσT/κ.\n\nFor many years, the main three semiconductors known to have both low thermal conductivity and high power factor were bismuth telluride (BiTe), lead telluride (PbTe), and silicon germanium (SiGe). These materials have very rare elements which make them very expensive compounds.\n\nToday, the thermal conductivity of semiconductors can be lowered without affecting their high electrical properties using nanotechnology. This can be achieved by creating nanoscale features such as particles, wires or interfaces in bulk semiconductor materials. However, the manufacturing processes of nano-materials is still challenging. \n\nA thermoelectric module is a circuit containing thermoelectric materials which generates electricity from heat directly. A thermoelectric module consists of two dissimilar thermoelectric materials joined at their ends: an n-type (negatively charged), and a p-type (positively charged) semiconductor. A direct electric current will flow in the circuit when there is a temperature difference between the ends of the materials. Generally, the current magnitude is directly proportional to the temperature difference. \n\nIn application, thermoelectric modules in power generation work in very tough mechanical and thermal conditions. Because they operate in a very high temperature gradient, the modules are subject to large thermally induced stresses and strains for long periods of time. They also are subject to mechanical fatigue caused by large number of thermal cycles.\n\nThus, the junctions and materials must be selected so that they survive these tough mechanical and thermal conditions. Also, the module must be designed such that the two thermoelectric materials are thermally in parallel, but electrically in series. The efficiency of a thermoelectric module is greatly affected by the geometry of its design.\n\nUsing thermoelectric modules, a thermoelectric system generates power by taking in heat from a source such as a hot exhaust flue. In order to do that, the system needs a large temperature gradient, which is not easy in real-world applications. The cold side must be cooled by air or water. Heat exchangers are used on both sides of the modules to supply this heating and cooling.\n\nThere are many challenges in designing a reliable TEG system that operates at high temperatures. Achieving high efficiency in the system requires extensive engineering design in order to balance between the heat flow through the modules and maximizing the temperature gradient across them. To do this, designing heat exchanger technologies in the system is one of the most important aspects of TEG engineering. In addition, the system requires to minimize the thermal losses due to the interfaces between materials at several places. Another challenging constraint is avoiding large pressure drops between the heating and cooling sources.\n\nAfter the DC power from the TE modules passes through an inverter, the TEG produces AC power, which in turn, requires an integrated power electronics system to deliver it to the customer.\n\nOnly a few known materials to date are identified as thermoelectric materials. Most thermoelectric materials today have a zT, the figure of merit, value of around 1, such as in Bismuth Telluride (BiTe) at room temperature and lead telluride (PbTe) at 500-700K. However, in order to be competitive with other power generation systems, TEG materials should have a zT of 2-3. Most research in thermoelectric materials has focused on increasing the Seebeck coefficient (S) and reducing the thermal conductivity, especially by manipulating the nanostructure of the thermoelectric materials. Because the thermal and electrical conductivity correlate with the charge carriers, new means must be introduced in order to conciliate the contradiction between high electrical conductivity and low thermal conductivity as indicated.\n\nWhen selecting materials for thermoelectric generation, a number of other factors need to be considered. During operation, ideally the thermoelectric generator has a large temperature gradient across it. Thermal expansion will then introduce stress in the device which may cause fracture of the thermoelectric legs, or separation from the coupling material. The mechanical properties of the materials must be considered and the coefficient of thermal expansion of the n and p-type material must be matched reasonably well. In segmented thermoelectric generators, the material's compatibility must also be considered.\n\nA material's compatibility factor is defined as\n\nformula_1.\n\nWhen the compatibility factor from one segment to the next differs by more than a factor of about two, the device will not operate efficiently. The material parameters determining s (as well as zT) are temperature dependent, so the compatibility factor may change from the hot side to the cold side of the device, even in one segment. This behavior is referred to as self-compatibility and may become important in devices design for low temperature operation.\n\nIn general, thermoelectric materials can be categorized into conventional and new materials:\n\nMany TEG materials are employed in commercial applications today. These materials can be divided into three groups based on the temperature range of operation:\nAlthough these materials still remain the cornerstone for commercial and practical applications in thermoelectric power generation, significant advances have been made in synthesizing new materials and fabricating material structures with improved thermoelectric performance. Recent research has focused on improving the material’s figure-of-merit (zT), and hence the conversion efficiency, by reducing the lattice thermal conductivity.\n\nResearchers are trying to develop new thermoelectric materials for power generation by improving the figure-of-merit zT. One example of these materials is the semiconductor compound ß-ZnSb, which possesses an exceptionally low thermal conductivity and exhibits a maximum zT of 1.3 at a temperature of 670K. This material is also relatively inexpensive and stable up to this temperature in a vacuum, and can be a good alternative in the temperature range between materials based on BiTe and PbTe.\n\nBeside improving the figure-of-merit, there is increasing focus to develop new materials by increasing the electrical power output, decreasing cost and developing environmentally friendly materials. For example, when the fuel cost is low or almost free, such as in waste heat recovery, then the cost per watt is only determined by the power per unit area and the operating period. As a result, it has initiated a search for materials with high power output rather than conversion efficiency. For example, the rare earth compounds YbAl has a low figure-of-merit, but it has a power output of at least double that of any other material, and can operate over the temperature range of a waste heat source.\n\nIn order to increase the figure of merit (zT), a material’s thermal conductivity should be minimized while its electrical conductivity and Seebeck coefficient is maximized. In most cases, methods to increase or decrease one property result in the same effect on other properties due to their interdependence. A novel processing technique exploits the scattering of different phonon frequencies to selectively reduce lattice thermal conductivity without the typical negative effects on electrical conductivity from the simultaneous increased scattering of electrons. In a bismuth antimony tellurium ternary system, liquid-phase sintering is used to produce low-energy semicoherent grain boundaries, which do not have a significant scattering effect on electrons. The breakthrough is then applying a pressure to the liquid in the sintering process, which creates a transient flow of the Te rich liquid and facilitates the formation of dislocations that greatly reduce the lattice conductivity. The ability to selectively decrease the lattice conductivity results in reported zT values of 1.86 ± .15 which are a significant improvement over current commercial thermoelectric generators which have typical figures of merit closer to .3-.6. These improvements highlight the fact that in addition to development of novel materials for thermoelectric applications, using different processing techniques to design microstructure is a viable and worthwhile effort. In fact, it often makes sense to work to optimize both composition and microstructure.\n\nThe typical efficiency of TEGs is around 5–8%. Older devices used bimetallic junctions and were bulky. More recent devices use highly doped semiconductors made from bismuth telluride (BiTe), lead telluride (PbTe), calcium manganese oxide (CaMnO), or combinations thereof, depending on temperature. These are solid-state devices and unlike dynamos have no moving parts, with the occasional exception of a fan or pump. For a discussion of the factors determining and limiting efficiency as well as ongoing efforts to improve the efficiency, see the article Thermoelectric materials (device efficiency).\n\nThermoelectric generators have a variety of applications. Frequently, thermoelectric generators are used for low power remote applications or where bulkier but more efficient heat engines such as Stirling engines would not be possible. Unlike heat engines, the solid state electrical components typically used to perform thermal to electric energy conversion have no moving parts. The thermal to electric energy conversion can be performed using components that require no maintenance, have inherently high reliability, and can be used to construct generators with long service-free lifetimes. This makes thermoelectric generators well suited for equipment with low to modest power needs in remote uninhabited or inaccessible locations such as mountaintops, the vacuum of space, or the deep ocean. \n\nBesides low efficiency and relatively high cost, practical problems exist in using thermoelectric devices in certain types of applications resulting from a relatively high electrical output resistance, which increases self-heating, and a relatively low thermal conductivity, which makes them unsuitable for applications where heat removal is critical, as with heat removal from an electrical device such as microprocessors. \n\nWhile TEG technology has been used in military and aerospace applications for decades, new TE materials and systems are being developed to generate power using low or high temperatures waste heat, and that could provide a significant opportunity in the near future. These systems can also be scalable to any size and have lower operation and maintenance cost.\n\nIn general, investing in TEG technology is increasing rapidly. The global market for thermoelectric generators is estimated to be US$320 million in 2015. A recent study estimated that TEG is expected to reach $720 million in 2021 with a growth rate of 14.5%. Today, North America capture 66% of the market share and it will continue to be the biggest market in the near future. However, Asia-Pacific and European countries are projected to grow at relatively higher rates. A study found that the Asia-Pacific market would grow at a Compound Annual Growth Rate (CAGR) of 18.3% in the period from 2015 to 2020 due to the high demand of thermoelectric generators by the automotive industries to increase overall fuel efficiency, as well as the growing industrialization in the region.\n\nLow power TEG or \"sub-watt\" (i.e. generating up to 1 Watt peak) market is a growing part of the TEG market, capitalizing on latest technologies. Main applications are sensors, low power applications and more globally Internet of things applications. A specialized market research company indicated that 100 000 units have been shipped in 2014 and expects 9 million units per year by 2020.\n\n"}
{"id": "17731917", "url": "https://en.wikipedia.org/wiki?curid=17731917", "title": "Tricritical point", "text": "Tricritical point\n\nIn condensed matter physics, dealing with the macroscopic physical properties of matter, a tricritical point is a point in the phase diagram of a system at which three-phase coexistence terminates. This definition is clearly parallel to the definition of an ordinary critical point as the point at which two-phase coexistence terminates.\n\nA point of three-phase coexistence is termed a triple point for a one-component system, since, from Gibbs' phase rule, this condition is only achieved for a single point in the phase diagram (\"F\" = 2-3+1 =0). For tricritical points to be observed, one needs a mixture with more components. It can be shown that three is the \"minimum\" number of components for which these points can appear. In this case, one may have a two-dimensional region of three-phase coexistence (\"F\" = 2-3+3 =2) (thus, each point in this region corresponds to a triple point). This region (F=2, P=3) will terminate in two critical lines of two-phase coexistence; these two critical lines (F=1, P=2) may then terminate at a single tricritical point. This point (F=0, P=1) is therefore \"twice critical\", since it belongs to two critical branches.<br>\nIndeed, its critical behavior is different from that of a conventional critical point: the upper critical dimension is lowered from d=4 to d=3 so the classical exponents turn out to apply for real systems in three dimensions (but not for systems whose spatial dimension is 2 or lower).\n\nIt seems more convenient experimentally to consider mixtures with four components for which one thermodynamic variable (usually the pressure or the volume) is kept fixed. The situation then reduces to the one described for mixtures of three components.\n\nHistorically, it was for a long time unclear whether a superconductor undergoes a first- or a second-order phase transition. The question was finally settled in 1982. If the Ginzburg-Landau parameter formula_1 that distinguishes type-I and type-II superconductors (see also here) is large enough, vortex fluctuations become important which drive the transition to \"second\" order.\nThe tricritical point lies at roughly formula_2, i.e., slightly below the value formula_3 where type-I goes over into type-II superconductor. The prediction was confirmed in 2002 by Monte Carlo computer simulations.\n"}
{"id": "53265389", "url": "https://en.wikipedia.org/wiki?curid=53265389", "title": "UMaine Deepwater Offshore Wind Test Site", "text": "UMaine Deepwater Offshore Wind Test Site\n\nThe University of Maine (UMaine) Deepwater Offshore Wind Test Site, located nearly 3 miles south west of Monhegan Island, Maine is available for use by commercial and non-commercial entities in partnership with the University of Maine, or the University of Maine itself, to research and develop ocean energy devices, such as floating wind turbines or wave energy converters.\n\nThe University of Maine asserts that the test site is among the most extensively studied in the Gulf of Maine.\n\nThe site was established under Maine law in 2009. The legislation formally designates the test site as the \"Maine Offshore Wind Energy Research Center.\"\n\nMaine Aqua Ventus I, GP, LLC, is pursuing a 12 MW demonstration offshore wind pilot project in the Test Site, with deployment and commercial operations planned for 2020.\n\nThe following research and development efforts are allowable in the UMaine Deepwater Offshore Wind Test Site, per LD 1465:\nThere are no enhanced limitations placed on traditional commercial or recreational uses within the test site.\n\nThe University of Maine and its partners conducted a variety of ecological and environmental studies to characterize the UMaine Deepwater Offshore Wind Test Site, primarily between 2010 and 2013. Studies include baseline data on benthic invertebrates, marine mammals, birds, fish, and others.\n\nSurvey results were published in 2011 in a University of Maine-published report, entitled: \"Maine Offshore Wind Report.\" The report was funded by the U.S. Department of Energy, National Science Foundation, Rockefeller Brothers Fund, and others.\n\nHourly buoy data near the UMaine Deepwater Offshore Wind Test Site is available viewing and download from the Northeastern Regional Association of Coastal Ocean Observing Systems (NERACOOS) buoy E01. Available real-time weather and ocean data includes winds, waves, visibility, air temperature, and water temperature at the UMaine Deepwater Offshore Wind Test Site.\n\nThe UMaine Deepwater Offshore Wind Test Site was designated to the University of Maine by the 124th Legislature of the State of Maine in 2009. As a result of recommendations from Governor John E. Baldacci's Ocean Energy Task Force, the Maine Department of Conservation identified three locations off the coast of Maine as Wind Energy Test Areas. The sites were chosen following a public outreach process led by the Maine State Planning Office in cooperation with the Maine State Planning Office, Department of Environmental Protection, Department of Marine Resources, and the University of Maine. In December 2009, the Maine Department of Conservation designated the location off of Monhegan Island based on its distance from the mainland shore and greater water depths.\n\nIn September 2009, the Maine Department of Conservation released a document regarding the Maine Offshore Energy Demonstration Area Siting Initiative, which worked to establish several areas within Maine state waters that would be technically appropriate for an offshore wind demonstration area. The document stated that demonstration areas would be identified as a \"three pronged approach,\" that would consider: geographical information system \"showstoppers,\" human use and activities, and environmental considerations.\n\nFurther, the document states: \"Through a combination of anthropogenic, environmental, and geophysical analyses, the State of Maine hopes to site these ocean energy testing and demonstration projects in areas with the fewest amount of conflicts, or at the very least, minimize what conflicts there will be. Adaptive Management will play a critical role in the duration of a project, or may affect how the project operates, once it is built.\"\n\nIn October 2009, the Maine Department of Conservation and Maine State Planning Office released a document that detailed site selection methodology for the Maine Offshore Energy Demonstration Area Siting Initiative.\n\nThe document outlines two phases. The first phrase focused on technical identification, public scoping sessions, and state consultation with state agencies. The second phase focused on map analysis (of ecological concerns, geology, obstructions, infrastructure, human uses of the marine environment, and viewshed) followed by a ranking process that gathered written and oral comments from a variety of sources, including fishermen, state agencies, and other interested parties.\n\nMatthew Nixon of the Maine State Planning Office provided a culminating presentation at a \"Developer's Summit,\" sponsored by the University of Maine in February 2011, describing the technical identification process and more than 40 public scoping sessions held in 2009.\n\nMaine Aqua Ventus I, GP, LLC, is pursuing a 12 MW demonstration project in the test site. The project would feature two 6 MW wind turbines mounted on a University of Maine-developed VolturnUS floating concrete hull.\n\nSections of the VolturnUS will be fabricated in Hampden, Maine, and Searsport, Maine, and towed to the test site. An AC cable will join the turbines, and a subsea power cable will connect the test site to a point onshore. The project has a planned 20 year duration, as indicated in the project's Power Purchase Agreement draft term sheet with the Maine Public Utilities Commission.\n\nProject participants include Emera, Inc., Cianbro Corporation, DCNS, and the University of Maine. Deployment and commercial operation of the two units is currently planned for 2019.\n\nMonhegan Island is located 12 nautical miles off the mainland. The population was 75 at the 2000 census. Monhegan Plantation created and authorized the Monhegan Energy Task Force to, \"represent Monhegan in communications and negotiations with Maine Aqua Ventus regarding all aspects of the proposed offshore wind project, including the planning construction, operation, and decommissioning phases.\"\n\nAs part of their proposed contract with the Maine Public Utilities Commission through a Power Purchase Agreement, Maine Aqua Ventus has an obligation to provide local benefit to Monhegan through a subsea cable interconnection with Central Maine Power or an alternative negotiated benefit. As of November 1, 2016, the Monhegan Energy Task Force contracted with law firm Eaton Peabody to negotiate the community benefits agreement with Maine Aqua Ventus.\n\nThe test site and proposed project has created ongoing discussion on the Island, with residents considering potential impacts to their community.\n\nIn early July 2016, the Monhegan Energy Task Force conducted a survey to gauge community interest and sentiment towards Maine Aqua Ventus and the proposed offshore wind demonstration project. The survey broke results down between all respondents (registered voter, property owner, year-round resident, or seasonal worker) and exclusively Monhegan registered voters. The survey found that respondents were more supportive of the project (43.7%) versus opposed (40.2%) with a small percentage of undecided (16%). This support was found to be stronger among Monhegan Registered Voters.\n\nThe same survey asked if Monhegan Plantation should cease to engage with Maine Aqua Ventus overall. The Monhegan Energy Task Force reported that nearly 80% of Registered Voters should engage with Maine Aqua Ventus and pursue a community benefit agreement.\n\nIn late July 2016, Monhegan Plantation held a special town meeting to see if the plantation to pursue development of a community benefits agreement with Maine Aqua Ventus. This vote passed with 31 to 1 registered voters in favor.\n\nA group called \"Protect Monhegan,\" formed in November 2016, led by Monhegan resident Travis Dow, to express opposition to the Maine Aqua Ventus project.\n\nOn February 8, 2017, Maine State Senator Dana Dow (R-Damariscotta) issued a statement suggesting he would introduce a bill on behalf of \"Protect Monhegan,\" to prohibit a wind energy test area within 10 miles of Monhegan Island's lobster conservation area. Formal bill language was not issued with the Senator's statement, made through the State's Republican party website.\n\nOn February 7, 2017, during a briefing of the State's joint legislative committee on Energy, Utilities, and Technology (EUT Committee), Dr. Habib Dagher of the University of Maine asserted that moving the test site would \"kill the project.\"\n\nThe EUT committee voted Ought Not to Pass Pursuant To Joint Rule 310, May 23, 2017.\n\n\n__NOEDITSECTION__\n"}
{"id": "48458656", "url": "https://en.wikipedia.org/wiki?curid=48458656", "title": "Unstoppable: Harnessing Science to Change the World", "text": "Unstoppable: Harnessing Science to Change the World\n\nUnstoppable: Harnessing Science to Change the World is a 2015 book written by Bill Nye and edited by Corey S. Powell. Published by St. Martin's Press, it is Nye's second book, after \"\", also with Powell, which was also published by St. Martin's Press in 2014. The book is about how to use science to improve the environment and the challenges faced with global warming as well as raising the standard of living.\n\nBBC science columnist Rose Eveleth wrote a joint review in \"The New York Times\" of the audiobook versions of \"Unstoppable\" and Leonard Mlodinow's 2015 book \"The Upright Thinkers\". Eveleth found Nye's audiobook \"easy to listen to\", noting the familiarity of Nye's voice to those who have watched the children's television series \"Bill Nye the Science Guy\". However, Eveleth wrote that both books \"stumble\" because they \"recycle well-known material\", describing how Nye \"spends a good chunk of his book talking about the setup of his extremely energy-efficient home ... Some of these stories are relevant and interesting. Other times, they feel forced, as if both authors knew that without a personal touch, their books would read like any other general primer on climate change or great men in science.\" Simon Warthall, in the introduction to an interview with Nye for \"National Geographic\"s \"Book Talk\" column, commented that \"Unstoppable\" \"mixes science and [Nye's] trademark humor\". Jennifer Kay, in a review for the Associated Press, commented on the clarity of Nye's language, writing how \"Nye explains the basics of climate science without making 'Unstoppable' feel like a textbook.\"\n"}
{"id": "4721893", "url": "https://en.wikipedia.org/wiki?curid=4721893", "title": "VT-1 reactor", "text": "VT-1 reactor\n\nThe VT-1 reactor was the nuclear fission reactor used in a pair to power Soviet submarine K-27 as part of the Soviet Navy's Project 645 Кит-ЖМТ. It is a liquid metal cooled reactor (LMR), using highly enriched uranium-235 fuel to produce 73 MW of power.\n\nK-27 was a November class first generation nuclear submarine, and the only one of its class fitted with liquid metal cooled reactors. However the seven-member Alfa class were subsequently fitted with liquid metal cooled reactors.\n\nIt was developed by OKB Gidropress in cooperation with IPPE.\n\n"}
{"id": "33306", "url": "https://en.wikipedia.org/wiki?curid=33306", "title": "Water", "text": "Water\n\nWater is a transparent, tasteless, odorless, and nearly colorless chemical substance, which is the main constituent of Earth's streams, lakes, and oceans, and the fluids of most living organisms. It is vital for all known forms of life, even though it provides no calories or organic nutrients. Its chemical formula is HO, meaning that each of its molecules contains one oxygen and two hydrogen atoms connected by covalent bonds. Water is the name of the liquid state of HO at standard ambient temperature and pressure. It forms precipitation in the form of rain and aerosols in the form of fog. Clouds are formed from suspended droplets of water and ice, its solid state. When finely divided, crystalline ice may precipitate in the form of snow. The gaseous state of water is steam or water vapor. Water moves continually through the water cycle of evaporation, transpiration (evapotranspiration), condensation, precipitation, and runoff, usually reaching the sea.\n\nWater covers 71% of the Earth's surface, mostly in seas and oceans. Small portions of water occur as groundwater (1.7%), in the glaciers and the ice caps of Antarctica and Greenland (1.7%), and in the air as vapor, clouds (formed of ice and liquid water suspended in air), and precipitation (0.001%).\n\nWater plays an important role in the world economy. Approximately 70% of the freshwater used by humans goes to agriculture. Fishing in salt and fresh water bodies is a major source of food for many parts of the world. Much of long-distance trade of commodities (such as oil and natural gas) and manufactured products is transported by boats through seas, rivers, lakes, and canals. Large quantities of water, ice, and steam are used for cooling and heating, in industry and homes. Water is an excellent solvent for a wide variety of chemical substances; as such it is widely used in industrial processes, and in cooking and washing. Water is also central to many sports and other forms of entertainment, such as swimming, pleasure boating, boat racing, surfing, sport fishing, and diving.\n\nThe word \"water\" comes from Old English \"wæter\", from Proto-Germanic \"*watar\" (source also of Old Saxon \"watar\", Old Frisian \"wetir\", Dutch \"water\", Old High German \"wazzar\", German \"Wasser\", Old Norse \"vatn\", Gothic \"wato\"), from Proto-Indoeuropean \"*wod-or\", suffixed form of root \"*wed-\" (\"water\"; \"wet\").\n\nWater () is a polar inorganic compound that is at room temperature a tasteless and odorless liquid, nearly colorless with a hint of blue. This simplest hydrogen chalcogenide is by far the most studied chemical compound and is described as the \"universal solvent\" for its ability to dissolve many substances. This allows it to be the \"solvent of life\". It is the only common substance to exist as a solid, liquid, and gas in normal terrestrial conditions.\n\nWater is a liquid at the temperatures and pressures that are most adequate for life. Specifically, at a standard pressure of 1 atm, water is a liquid between . Increasing the pressure slightly lowers the melting point, which is about at 600 atm and at 2100 atm. This effect is relevant, for example, to ice skating, to the buried lakes of Antarctica, and to the movement of glaciers. (At pressures higher than 2100 atm the melting point rapidly increases again, and ice takes several exotic forms that do not exist at lower pressures.)\n\nIncreasing the pressure has a more dramatic effect on the boiling point, that is about at 220 atm. This effect is important in, among other things, deep-sea hydrothermal vents and geysers, pressure cooking, and steam engine design. At the top of Mount Everest, where the atmospheric pressure is about 0.34 atm, water boils at .\n\nAt very low pressures (below about 0.006 atm), water cannot exist in the liquid state and passes directly from solid to gas by sublimation—a phenomenon exploited in the freeze drying of food. At very high pressures (above 221 atm), the liquid and gas states are no longer distinguishable, a state called supercritical steam.\n\nWater also differs from most liquids in that it becomes less dense as it freezes. The maximum density of water in its liquid form (at 1 atm) is ; that occurs at . The density of ice is . Thus, water expands 9% in volume as it freezes, which accounts for the fact that ice floats on liquid water.\n\nThe details of the exact chemical nature of liquid water are not well understood; some theories suggest that water's unusual behaviour is as a result of it having 2 liquid states.\n\nPure water is usually described as tasteless and odorless, although humans have specific sensors that can feel the presence of water in their mouths, and frogs are known to be able to smell it. However, water from ordinary sources (including bottled mineral water) usually has many dissolved substances, that may give it varying tastes and odors. Humans and other animals have developed senses that enable them to evaluate the potability of water by avoiding water that is too salty or putrid.\n\nThe apparent color of natural bodies of water (and swimming pools) is often determined more by dissolved and suspended solids, or by reflection of the sky, than by water itself.\n\nLight in the visible electromagnetic spectrum can traverse a couple meters of pure water (or ice) without significant absorption, so that it looks transparent and colorless. Thus aquatic plants, algae, and other photosynthetic organisms can live in water up to hundreds of meters deep, because sunlight can reach them. Water vapour is essentially invisible as a gas.\n\nThrough a thickness of or more, however, the intrinsic color of water (or ice) is visibly turquoise (greenish blue), as its absorption spectrum has a sharp minimum at the corresponding color of light (1/227 m at 418 nm). The color becomes increasingly stronger and darker with increasing thickness. (Practically no sunlight reaches the parts of the oceans below of depth.) Infrared and ultraviolet light, on the other hand, is strongly absorbed by water.\n\nThe refraction index of liquid water (1.333 at ) is much higher than that of air (1.0), similar to those of alkanes and ethanol, but lower than those of glycerol (1.473), benzene (1.501), carbon disulfide (1.627), and common types of glass (1.4 to 1.6). The refraction index of ice (1.31) is lower than that of liquid water.\n\nSince the water molecule is not linear and the oxygen atom has a higher electronegativity than hydrogen atoms, it is a polar molecule, with an electrical dipole moment: the oxygen atom carries a slight negative charge, whereas the hydrogen atoms are slightly positive. Water is a good polar solvent, that dissolves many salts and hydrophilic organic molecules such as sugars and simple alcohols such as ethanol. Most acids dissolve in water to yield the corresponding anions. Many substances in living organisms, such as proteins, DNA and polysaccharides, are dissolved in water. Water also dissolves many gases, such as oxygen and carbon dioxide—the latter giving the fizz of carbonated beverages, sparkling wines and beers.\n\nOn the other hand, many organic substances (such as fats and oils and alkanes) are hydrophobic, that is, insoluble in water. Many inorganic substances are insoluble too, including most metal oxides, sulfides, and silicates.\n\nBecause of its polarity, a molecule of water in the liquid or solid state can form up to four hydrogen bonds with neighboring molecules. These bonds are the cause of water's high surface tension and capillary forces. The capillary action refers to the tendency of water to move up a narrow tube against the force of gravity. This property is relied upon by all vascular plants, such as trees.\n\nThe hydrogen bonds are also the reason why the melting and boiling points of water are much higher than those of other analogous compounds like hydrogen sulfide (). They also explain its exceptionally high specific heat capacity (about 4.2 J/g/K), heat of fusion (about 333 J/g), heat of vaporization (), and thermal conductivity (between 0.561 and 0.679 W/m/K). These properties make water more effective at moderating Earth's climate, by storing heat and transporting it between the oceans and the atmosphere. The hydrogen bonds of water are of moderate strength, around 23 kJ/mol (compared to a covalent O-H bond at 492 kJ/mol). Of this, it is estimated that 90% of the hydrogen bond is attributable to electrostatics, while the remaining 10% reflects partial covalent character.\n\nPure water has a low electrical conductivity, which increases with the dissolution of a small amount of ionic material such as common salt.\n\nLiquid water can be split into the elements hydrogen and oxygen by passing an electric current through it—a process called electrolysis. The decomposition requires more energy input than the heat released by the inverse process (285.8 kJ/mol, or 15.9 MJ/kg).\n\nLiquid water can be assumed to be incompressible for most purposes: its compressibility ranges from 4.4 to in ordinary conditions. Even in oceans at 4 km depth, where the pressure is 400 atm, water suffers only a 1.8% decrease in volume.\n\nThe viscosity of water is about 10 Pa·s or 0.01 poise at , and the speed of sound in liquid water ranges between depending on temperature. Sound travels long distances in water with little attenuation, especially at low frequencies (roughly 0.03 dB/km for 1 kHz), a property that is exploited by cetaceans and humans for communication and environment sensing (sonar).\n\nMetallic elements which are more electropositive than hydrogen such as lithium, sodium, calcium, potassium and caesium displace hydrogen from water, forming hydroxides and releasing hydrogen. At high temperatures, carbon reacts with steam to form carbon monoxide.\n\nHydrology is the study of the movement, distribution, and quality of water throughout the Earth. The study of the distribution of water is hydrography. The study of the distribution and movement of groundwater is hydrogeology, of glaciers is glaciology, of inland waters is limnology and distribution of oceans is oceanography. Ecological processes with hydrology are in focus of ecohydrology.\n\nThe collective mass of water found on, under, and over the surface of a planet is called the hydrosphere. Earth's approximate water volume (the total water supply of the world) is .\n\nLiquid water is found in bodies of water, such as an ocean, sea, lake, river, stream, canal, pond, or puddle. The majority of water on Earth is sea water. Water is also present in the atmosphere in solid, liquid, and vapor states. It also exists as groundwater in aquifers.\n\nWater is important in many geological processes. Groundwater is present in most rocks, and the pressure of this groundwater affects patterns of faulting. Water in the mantle is responsible for the melt that produces volcanoes at subduction zones. On the surface of the Earth, water is important in both chemical and physical weathering processes. Water, and to a lesser but still significant extent, ice, are also responsible for a large amount of sediment transport that occurs on the surface of the earth. Deposition of transported sediment forms many types of sedimentary rocks, which make up the geologic record of Earth history.\n\nThe water cycle (known scientifically as the hydrologic cycle) refers to the continuous exchange of water within the hydrosphere, between the atmosphere, soil water, surface water, groundwater, and plants.\n\nWater moves perpetually through each of these regions in the \"water cycle\" consisting of following transfer processes:\nMost water vapor over the oceans returns to the oceans, but winds carry water vapor over land at the same rate as runoff into the sea, about 47 Tt per year. Over land, evaporation and transpiration contribute another 72 Tt per year. Precipitation, at a rate of 119 Tt per year over land, has several forms: most commonly rain, snow, and hail, with some contribution from fog and dew. Dew is small drops of water that are condensed when a high density of water vapor meets a cool surface. Dew usually forms in the morning when the temperature is the lowest, just before sunrise and when the temperature of the earth's surface starts to increase. Condensed water in the air may also refract sunlight to produce rainbows.\n\nWater runoff often collects over watersheds flowing into rivers. A mathematical model used to simulate river or stream flow and calculate water quality parameters is a hydrological transport model. Some water is diverted to irrigation for agriculture. Rivers and seas offer opportunity for travel and commerce. Through erosion, runoff shapes the environment creating river valleys and deltas which provide rich soil and level ground for the establishment of population centers. A flood occurs when an area of land, usually low-lying, is covered with water. It is when a river overflows its banks or flood comes from the sea. A drought is an extended period of months or years when a region notes a deficiency in its water supply. This occurs when a region receives consistently below average precipitation.\n\nSome runoff water is trapped for periods of time, for example in lakes. At high altitude, during winter, and in the far north and south, snow collects in ice caps, snow packs and glaciers. Water also infiltrates the ground and goes into aquifers. This groundwater later flows back to the surface in springs, or more spectacularly in hot springs and geysers. Groundwater is also extracted artificially in wells. This water storage is important, since clean, fresh water is essential to human and other land-based life. In many parts of the world, it is in short supply.\n\nSea water contains about 3.5% sodium chloride on average, plus smaller amounts of other substances. The physical properties of sea water differ from fresh water in some important respects. It freezes at a lower temperature (about ) and its density increases with decreasing temperature to the freezing point, instead of reaching maximum density at a temperature above freezing. The salinity of water in major seas varies from about 0.7% in the Baltic Sea to 4.0% in the Red Sea. (The Dead Sea, known for its ultra-high salinity levels of between 30–40%, is really a salt lake.)\n\nTides are the cyclic rising and falling of local sea levels caused by the tidal forces of the Moon and the Sun acting on the oceans. Tides cause changes in the depth of the marine and estuarine water bodies and produce oscillating currents known as tidal streams. The changing tide produced at a given location is the result of the changing positions of the Moon and Sun relative to the Earth coupled with the effects of Earth rotation and the local bathymetry. The strip of seashore that is submerged at high tide and exposed at low tide, the intertidal zone, is an important ecological product of ocean tides.\n\nFrom a biological standpoint, water has many distinct properties that are critical for the proliferation of life. It carries out this role by allowing organic compounds to react in ways that ultimately allow replication. All known forms of life depend on water. Water is vital both as a solvent in which many of the body's solutes dissolve and as an essential part of many metabolic processes within the body. Metabolism is the sum total of anabolism and catabolism. In anabolism, water is removed from molecules (through energy requiring enzymatic chemical reactions) in order to grow larger molecules (e.g. starches, triglycerides and proteins for storage of fuels and information). In catabolism, water is used to break bonds in order to generate smaller molecules (e.g. glucose, fatty acids and amino acids to be used for fuels for energy use or other purposes). Without water, these particular metabolic processes could not exist.\n\nWater is fundamental to photosynthesis and respiration. Photosynthetic cells use the sun's energy to split off water's hydrogen from oxygen. Hydrogen is combined with CO (absorbed from air or water) to form glucose and release oxygen. All living cells use such fuels and oxidize the hydrogen and carbon to capture the sun's energy and reform water and CO in the process (cellular respiration).\n\nWater is also central to acid-base neutrality and enzyme function. An acid, a hydrogen ion (H, that is, a proton) donor, can be neutralized by a base, a proton acceptor such as a hydroxide ion (OH) to form water. Water is considered to be neutral, with a pH (the negative log of the hydrogen ion concentration) of 7. Acids have pH values less than 7 while bases have values greater than 7.\n\nEarth surface waters are filled with life. The earliest life forms appeared in water; nearly all fish live exclusively in water, and there are many types of marine mammals, such as dolphins and whales. Some kinds of animals, such as amphibians, spend portions of their lives in water and portions on land. Plants such as kelp and algae grow in the water and are the basis for some underwater ecosystems. Plankton is generally the foundation of the ocean food chain.\n\nAquatic vertebrates must obtain oxygen to survive, and they do so in various ways. Fish have gills instead of lungs, although some species of fish, such as the lungfish, have both. Marine mammals, such as dolphins, whales, otters, and seals need to surface periodically to breathe air. Some amphibians are able to absorb oxygen through their skin. Invertebrates exhibit a wide range of modifications to survive in poorly oxygenated waters including breathing tubes (see insect and mollusc siphons) and gills (\"Carcinus\"). However as invertebrate life evolved in an aquatic habitat most have little or no specialisation for respiration in water.\n\nCivilization has historically flourished around rivers and major waterways; Mesopotamia, the so-called cradle of civilization, was situated between the major rivers Tigris and Euphrates; the ancient society of the Egyptians depended entirely upon the Nile. Rome was also founded on the banks of the Italian river Tiber. Large metropolises like Rotterdam, London, Montreal, Paris, New York City, Buenos Aires, Shanghai, Tokyo, Chicago, and Hong Kong owe their success in part to their easy accessibility via water and the resultant expansion of trade. Islands with safe water ports, like Singapore, have flourished for the same reason. In places such as North Africa and the Middle East, where water is more scarce, access to clean drinking water was and is a major factor in human development.\n\nWater fit for human consumption is called drinking water or potable water. Water that is not potable may be made potable by filtration or distillation, or by a range of other methods.\n\nWater that is not fit for drinking but is not harmful for humans when used for swimming or bathing is called by various names other than potable or drinking water, and is sometimes called safe water, or \"safe for bathing\". Chlorine is a skin and mucous membrane irritant that is used to make water safe for bathing or drinking. Its use is highly technical and is usually monitored by government regulations (typically 1 part per million (ppm) for drinking water, and 1–2 ppm of chlorine not yet reacted with impurities for bathing water). Water for bathing may be maintained in satisfactory microbiological condition using chemical disinfectants such as chlorine or ozone or by the use of ultraviolet light.\n\nIn the US, non-potable forms of wastewater generated by humans may be referred to as greywater, which is treatable and thus easily able to be made potable again, and blackwater, which generally contains sewage and other forms of waste which require further treatment in order to be made reusable. Greywater composes 50–80% of residential wastewater generated by a household's sanitation equipment (sinks, showers and kitchen runoff, but not toilets, which generate blackwater.) These terms may have different meanings in other countries and cultures.\n\nThis natural resource is becoming scarcer in certain places, and its availability is a major social and economic concern. Currently, about a billion people around the world routinely drink unhealthy water. Most countries accepted the goal of halving by 2015 the number of people worldwide who do not have access to safe water and sanitation during the 2003 G8 Evian summit. Even if this difficult goal is met, it will still leave more than an estimated half a billion people without access to safe drinking water and over a billion without access to adequate sanitation. Poor water quality and bad sanitation are deadly; some five million deaths a year are caused by polluted drinking water. The World Health Organization estimates that safe water could prevent 1.4 million child deaths from diarrhea each year.\n\nWater, however, is not a finite resource (meaning the availability of water is limited), but rather re-circulated as potable water in precipitation in quantities many orders of magnitude higher than human consumption. Therefore, it is the relatively small quantity of water in reserve in the earth (about 1% of our drinking water supply, which is replenished in aquifers around every 1 to 10 years), that is a non-renewable resource, and it is, rather, the distribution of potable and irrigation water which is scarce, rather than the actual amount of it that exists on the earth. Water-poor countries use importation of goods as the primary method of importing water (to leave enough for local human consumption), since the manufacturing process uses around 10 to 100 times products' masses in water.\n\nIn the developing world, 90% of all wastewater still goes untreated into local rivers and streams. Some 50 countries, with roughly a third of the world's population, also suffer from medium or high water stress, and 17 of these extract more water annually than is recharged through their natural water cycles. The strain not only affects surface freshwater bodies like rivers and lakes, but it also degrades groundwater resources.\n\nThe most important use of water in agriculture is for irrigation, which is a key component to produce enough food. Irrigation takes up to 90% of water withdrawn in some developing countries and significant proportions in more economically developed countries (in the United States, 30% of freshwater usage is for irrigation).\n\nFifty years ago, the common perception was that water was an infinite resource. At the time, there were fewer than half the current number of people on the planet. People were not as wealthy as today, consumed fewer calories and ate less meat, so less water was needed to produce their food. They required a third of the volume of water we presently take from rivers. Today, the competition for the fixed amount of water resources is much more intense, giving rise to the concept of peak water. This is because there are now nearly seven billion people on the planet, their consumption of water-thirsty meat and vegetables is rising, and there is increasing competition for water from industry, urbanisation and biofuel crops. In future, even more water will be needed to produce food because the Earth's population is forecast to rise to 9 billion by 2050.\n\nAn assessment of water management in agriculture was conducted in 2007 by the International Water Management Institute in Sri Lanka to see if the world had sufficient water to provide food for its growing population. It assessed the current availability of water for agriculture on a global scale and mapped out locations suffering from water scarcity. It found that a fifth of the world's people, more than 1.2 billion, live in areas of physical water scarcity, where there is not enough water to meet all demands. A further 1.6 billion people live in areas experiencing economic water scarcity, where the lack of investment in water or insufficient human capacity make it impossible for authorities to satisfy the demand for water. The report found that it would be possible to produce the food required in future, but that continuation of today's food production and environmental trends would lead to crises in many parts of the world. To avoid a global water crisis, farmers will have to strive to increase productivity to meet growing demands for food, while industry and cities find ways to use water more efficiently.\n\nWater scarcity is also caused by production of cotton: 1 kg of cotton—equivalent of a pair of jeans—requires water to produce. While cotton accounts for 2.4% of world water use, the water is consumed in regions which are already at a risk of water shortage. Significant environmental damage has been caused, such as disappearance of the Aral Sea.\n\nOn 7 April 1795, the gram was defined in France to be equal to \"the absolute weight of a volume of pure water equal to a cube of one hundredth of a meter, and at the temperature of melting ice\". For practical purposes though, a metallic reference standard was required, one thousand times more massive, the kilogram. Work was therefore commissioned to determine precisely the mass of one liter of water. In spite of the fact that the decreed definition of the gram specified water at —a highly reproducible \"temperature\"—the scientists chose to redefine the standard and to perform their measurements at the temperature of highest water \"density\", which was measured at the time as .\n\nThe Kelvin temperature scale of the SI system is based on the triple point of water, defined as exactly . The scale is an absolute temperature scale with the same increment as the Celsius temperature scale, which was originally defined according to the boiling point (set to ) and melting point (set to ) of water.\n\nNatural water consists mainly of the isotopes hydrogen-1 and oxygen-16, but there is also a small quantity of heavier isotopes such as hydrogen-2 (deuterium). The amount of deuterium oxides or heavy water is very small, but it still affects the properties of water. Water from rivers and lakes tends to contain less deuterium than seawater. Therefore, standard water is defined in the Vienna Standard Mean Ocean Water specification.\n\nThe human body contains from 55% to 78% water, depending on body size. To function properly, the body requires between of water per day to avoid dehydration; the precise amount depends on the level of activity, temperature, humidity, and other factors. Most of this is ingested through foods or beverages other than drinking straight water. It is not clear how much water intake is needed by healthy people, though most specialists agree that approximately 2 liters (6 to 7 glasses) of water daily is the minimum to maintain proper hydration. Medical literature favors a lower consumption, typically 1 liter of water for an average male, excluding extra requirements due to fluid loss from exercise or warm weather.\n\nFor those who have healthy kidneys, it is rather difficult to drink too much water, but (especially in warm humid weather and while exercising) it is dangerous to drink too little. People can drink far more water than necessary while exercising, however, putting them at risk of water intoxication (hyperhydration), which can be fatal. The popular claim that \"a person should consume eight glasses of water per day\" seems to have no real basis in science. Studies have shown that extra water intake, especially up to at mealtime was conducive to weight loss. Adequate fluid intake is helpful in preventing constipation.\nAn original recommendation for water intake in 1945 by the Food and Nutrition Board of the United States National Research Council read: \"An ordinary standard for diverse persons is 1 milliliter for each calorie of food. Most of this quantity is contained in prepared foods.\" The latest dietary reference intake report by the United States National Research Council in general recommended, based on the median total water intake from US survey data (including food sources): for men and of water total for women, noting that water contained in food provided approximately 19% of total water intake in the survey.\n\nSpecifically, pregnant and breastfeeding women need additional fluids to stay hydrated. The Institute of Medicine (US) recommends that, on average, men consume and women ; pregnant women should increase intake to and breastfeeding women should get 3 liters (12 cups), since an especially large amount of fluid is lost during nursing. Also noted is that normally, about 20% of water intake comes from food, while the rest comes from drinking water and beverages (caffeinated included). Water is excreted from the body in multiple forms; through urine and feces, through sweating, and by exhalation of water vapor in the breath. With physical exertion and heat exposure, water loss will increase and daily fluid needs may increase as well.\n\nHumans require water with few impurities. Common impurities include metal salts and oxides, including copper, iron, calcium and lead, and/or harmful bacteria, such as \"Vibrio\". Some solutes are acceptable and even desirable for taste enhancement and to provide needed electrolytes.\n\nThe single largest (by volume) freshwater resource suitable for drinking is Lake Baikal in Siberia.\n\nThe propensity of water to form solutions and emulsions is useful in various washing processes. Washing is also an important component of several aspects of personal body hygiene. Most of personal water use is due to showering, doing the laundry and dishwashing, reaching hundreds of liters per day in developed countries.\n\nThe use of water for transportation of materials through rivers and canals as well as the international shipping lanes is an important part of the world economy.\n\nWater is widely used in chemical reactions as a solvent or reactant and less commonly as a solute or catalyst. In inorganic reactions, water is a common solvent, dissolving many ionic compounds, as well as other polar compounds such as ammonia and compounds closely related to water. In organic reactions, it is not usually used as a reaction solvent, because it does not dissolve the reactants well and is amphoteric (acidic \"and\" basic) and nucleophilic. Nevertheless, these properties are sometimes desirable. Also, acceleration of Diels-Alder reactions by water has been observed. Supercritical water has recently been a topic of research. Oxygen-saturated supercritical water combusts organic pollutants efficiently. Water vapor is used for some processes in the chemical industry. An example is the production of acrylic acid from acrolein, propylene and propane. The possible effect of water in these reactions includes the physical-, chemical interaction of water with the catalyst and the chemical reaction of water with the reaction intermediates.\n\nWater and steam are a common fluid used for heat exchange, due to its availability and high heat capacity, both for cooling and heating. Cool water may even be naturally available from a lake or the sea. It's especially effective to transport heat through vaporization and condensation of water because of its large latent heat of vaporization. A disadvantage is that metals commonly found in industries such as steel and copper are oxidized faster by untreated water and steam. In almost all thermal power stations, water is used as the working fluid (used in a closed loop between boiler, steam turbine and condenser), and the coolant (used to exchange the waste heat to a water body or carry it away by evaporation in a cooling tower). In the United States, cooling power plants is the largest use of water.\n\nIn the nuclear power industry, water can also be used as a neutron moderator. In most nuclear reactors, water is both a coolant and a moderator. This provides something of a passive safety measure, as removing the water from the reactor also slows the nuclear reaction down. However other methods are favored for stopping a reaction and it is preferred to keep the nuclear core covered with water so as to ensure adequate cooling.\n\nWater has a high heat of vaporization and is relatively inert, which makes it a good fire extinguishing fluid. The evaporation of water carries heat away from the fire. It is dangerous to use water on fires involving oils and organic solvents, because many organic materials float on water and the water tends to spread the burning liquid.\n\nUse of water in fire fighting should also take into account the hazards of a steam explosion, which may occur when water is used on very hot fires in confined spaces, and of a hydrogen explosion, when substances which react with water, such as certain metals or hot carbon such as coal, charcoal, or coke graphite, decompose the water, producing water gas.\n\nThe power of such explosions was seen in the Chernobyl disaster, although the water involved did not come from fire-fighting at that time but the reactor's own water cooling system. A steam explosion occurred when the extreme overheating of the core caused water to flash into steam. A hydrogen explosion may have occurred as a result of reaction between steam and hot zirconium.\n\nHumans use water for many recreational purposes, as well as for exercising and for sports. Some of these include swimming, waterskiing, boating, surfing and diving. In addition, some sports, like ice hockey and ice skating, are played on ice. Lakesides, beaches and water parks are popular places for people to go to relax and enjoy recreation. Many find the sound and appearance of flowing water to be calming, and fountains and other water features are popular decorations. Some keep fish and other life in aquariums or ponds for show, fun, and companionship. Humans also use water for snow sports i.e. skiing, sledding, snowmobiling or snowboarding, which require the water to be frozen.\n\nThe water industry provides drinking water and wastewater services (including sewage treatment) to households and industry. Water supply facilities include water wells, cisterns for rainwater harvesting, water supply networks, and water purification facilities, water tanks, water towers, water pipes including old aqueducts. Atmospheric water generators are in development.\n\nDrinking water is often collected at springs, extracted from artificial borings (wells) in the ground, or pumped from lakes and rivers. Building more wells in adequate places is thus a possible way to produce more water, assuming the aquifers can supply an adequate flow. Other water sources include rainwater collection. Water may require purification for human consumption. This may involve removal of undissolved substances, dissolved substances and harmful microbes. Popular methods are filtering with sand which only removes undissolved material, while chlorination and boiling kill harmful microbes. Distillation does all three functions. More advanced techniques exist, such as reverse osmosis. Desalination of abundant seawater is a more expensive solution used in coastal arid climates.\n\nThe distribution of drinking water is done through municipal water systems, tanker delivery or as bottled water. Governments in many countries have programs to distribute water to the needy at no charge.\n\nReducing usage by using drinking (potable) water only for human consumption is another option. In some cities such as Hong Kong, sea water is extensively used for flushing toilets citywide in order to conserve fresh water resources.\n\nPolluting water may be the biggest single misuse of water; to the extent that a pollutant limits other uses of the water, it becomes a waste of the resource, regardless of benefits to the polluter. Like other types of pollution, this does not enter standard accounting of market costs, being conceived as externalities for which the market cannot account. Thus other people pay the price of water pollution, while the private firms' profits are not redistributed to the local population, victims of this pollution. Pharmaceuticals consumed by humans often end up in the waterways and can have detrimental effects on aquatic life if they bioaccumulate and if they are not biodegradable.\n\nMunicipal and industrial wastewater are typically treated at wastewater treatment plants. Mitigation of polluted surface runoff is addressed through a variety of prevention and treatment techniques. (\"See\" Surface runoff#Mitigation and treatment.)\n\nMany industrial processes rely on reactions using chemicals dissolved in water, suspension of solids in water slurries or using water to dissolve and extract substances, or to wash products or process equipment. Processes such as mining, chemical pulping, pulp bleaching, paper manufacturing, textile production, dyeing, printing, and cooling of power plants use large amounts of water, requiring a dedicated water source, and often cause significant water pollution.\n\nWater is used in power generation. Hydroelectricity is electricity obtained from hydropower. Hydroelectric power comes from water driving a water turbine connected to a generator. Hydroelectricity is a low-cost, non-polluting, renewable energy source. The energy is supplied by the motion of water. Typically a dam is constructed on a river, creating an artificial lake behind it. Water flowing out of the lake is forced through turbines that turn generators.\n\nPressurized water is used in water blasting and water jet cutters. Also, very high pressure water guns are used for precise cutting. It works very well, is relatively safe, and is not harmful to the environment. It is also used in the cooling of machinery to prevent overheating, or prevent saw blades from overheating.\n\nWater is also used in many industrial processes and machines, such as the steam turbine and heat exchanger, in addition to its use as a chemical solvent. Discharge of untreated water from industrial uses is pollution. Pollution includes discharged solutes (chemical pollution) and discharged coolant water (thermal pollution). Industry requires pure water for many applications and utilizes a variety of purification techniques both in water supply and discharge.\n\nBoiling, steaming, and simmering are popular cooking methods that often require immersing food in water or its gaseous state, steam. Water is also used for dishwashing. Water also plays many critical roles within the field of food science. It is important for a food scientist to understand the roles that water plays within food processing to ensure the success of their products.\n\nSolutes such as salts and sugars found in water affect the physical properties of water. The boiling and freezing points of water are affected by solutes, as well as air pressure, which is in turn affected by altitude. Water boils at lower temperatures with the lower air pressure that occurs at higher elevations. One mole of sucrose (sugar) per kilogram of water raises the boiling point of water by , and one mole of salt per kg raises the boiling point by ; similarly, increasing the number of dissolved particles lowers water's freezing point.\n\nSolutes in water also affect water activity that affects many chemical reactions and the growth of microbes in food. Water activity can be described as a ratio of the vapor pressure of water in a solution to the vapor pressure of pure water. Solutes in water lower water activity—this is important to know because most bacterial growth ceases at low levels of water activity. Not only does microbial growth affect the safety of food, but also the preservation and shelf life of food.\n\nWater hardness is also a critical factor in food processing and may be altered or treated by using a chemical ion exchange system. It can dramatically affect the quality of a product, as well as playing a role in sanitation. Water hardness is classified based on concentration of calcium carbonate the water contains. Water is classified as soft if it contains less than 100 mg/l (UK) or less than 60 mg/l (US).\n\nAccording to a report published by the Water Footprint organization in 2010, a single kilogram of beef requires of water; however, the authors also make clear that this is a global average and circumstantial factors determine the amount of water used in beef production.\n\nWater for injection is on the World Health Organization's list of essential medicines.\n\nMuch of the universe's water is produced as a byproduct of star formation. The formation of stars is accompanied by a strong outward wind of gas and dust. When this outflow of material eventually impacts the surrounding gas, the shock waves that are created compress and heat the gas. The water observed is quickly produced in this warm dense gas.\n\nOn 22 July 2011, a report described the discovery of a gigantic cloud of water vapor containing \"140 trillion times more water than all of Earth's oceans combined\" around a quasar located 12 billion light years from Earth. According to the researchers, the \"discovery shows that water has been prevalent in the universe for nearly its entire existence\".\n\nWater has been detected in interstellar clouds within our galaxy, the Milky Way. Water probably exists in abundance in other galaxies, too, because its components, hydrogen and oxygen, are among the most abundant elements in the universe. Based on models of the formation and evolution of the Solar System and that of other star systems, most other planetary systems are likely to have similar ingredients.\n\nWater is present as vapor in:\n\nLiquid water is present on Earth, covering 71% of its surface. Liquid water is also occasionally present in small amounts on Mars. Scientists believe liquid water is present in the Saturnian moons of Enceladus, as a 10-kilometre thick ocean approximately 30–40 kilometres below Enceladus' south polar surface, and Titan, as a subsurface layer, possibly mixed with ammonia. Jupiter's moon Europa has surface characteristics which suggest a subsurface liquid water ocean. Liquid water may also exist on Jupiter's moon Ganymede as a layer sandwiched between high pressure ice and rock.\n\nWater is present as ice on:\n\n\nAnd is also likely present on:\n\nWater and other volatiles probably comprise much of the internal structures of Uranus and Neptune and the water in the deeper layers may be in the form of ionic water in which the molecules break down into a soup of hydrogen and oxygen ions, and deeper still as superionic water in which the oxygen crystallises but the hydrogen ions float about freely within the oxygen lattice.\n\nThe existence of liquid water, and to a lesser extent its gaseous and solid forms, on Earth are vital to the existence of life on Earth as we know it. The Earth is located in the habitable zone of the solar system; if it were slightly closer to or farther from the Sun (about 5%, or about 8 million kilometers), the conditions which allow the three forms to be present simultaneously would be far less likely to exist.\n\nEarth's gravity allows it to hold an atmosphere. Water vapor and carbon dioxide in the atmosphere provide a temperature buffer (greenhouse effect) which helps maintain a relatively steady surface temperature. If Earth were smaller, a thinner atmosphere would allow temperature extremes, thus preventing the accumulation of water except in polar ice caps (as on Mars).\n\nThe surface temperature of Earth has been relatively constant through geologic time despite varying levels of incoming solar radiation (insolation), indicating that a dynamic process governs Earth's temperature via a combination of greenhouse gases and surface or atmospheric albedo. This proposal is known as the \"Gaia hypothesis\".\n\nThe state of water on a planet depends on ambient pressure, which is determined by the planet's gravity. If a planet is sufficiently massive, the water on it may be solid even at high temperatures, because of the high pressure caused by gravity, as it was observed on exoplanets Gliese 436 b and GJ 1214 b.\n\nWater politics is politics affected by water and water resources. For this reason, water is a strategic resource in the globe and an important element in many political conflicts. It causes health impacts and damage to biodiversity.\n\nAccess to safe drinking water has improved over the last decades in almost every part of the world, but approximately one billion people still lack access to safe water and over 2.5 billion lack access to adequate sanitation. However, some observers have estimated that by 2025 more than half of the world population will be facing water-based vulnerability. A report, issued in November 2009, suggests that by 2030, in some developing regions of the world, water demand will exceed supply by 50%.\n\n1.6 billion people have gained access to a safe water source since 1990. The proportion of people in developing countries with access to safe water is calculated to have improved from 30% in 1970 to 71% in 1990, 79% in 2000 and 84% in 2004. This trend is projected to continue. To halve, by 2015, the proportion of people without sustainable access to safe drinking water is one of the Millennium Development Goals. This goal is projected to be reached.\n\nA 2006 United Nations report stated that \"there is enough water for everyone\", but that access to it is hampered by mismanagement and corruption. In addition, global initiatives to improve the efficiency of aid delivery, such as the Paris Declaration on Aid Effectiveness, have not been taken up by water sector donors as effectively as they have in education and health, potentially leaving multiple donors working on overlapping projects and recipient governments without empowerment to act.\n\nThe authors of the 2007 Comprehensive Assessment of Water Management in Agriculture cited poor governance as one reason for some forms of water scarcity. Water governance is the set of formal and informal processes through which decisions related to water management are made. Good water governance is primarily about knowing what processes work best in a particular physical and socioeconomic context. Mistakes have sometimes been made by trying to apply 'blueprints' that work in the developed world to developing world locations and contexts. The Mekong river is one example; a review by the International Water Management Institute of policies in six countries that rely on the Mekong river for water found that thorough and transparent cost-benefit analyses and environmental impact assessments were rarely undertaken. They also discovered that Cambodia's draft water law was much more complex than it needed to be.\n\nThe UN World Water Development Report (WWDR, 2003) from the World Water Assessment Program indicates that, in the next 20 years, the quantity of water available to everyone is predicted to decrease by 30%. 40% of the world's inhabitants currently have insufficient fresh water for minimal hygiene. More than 2.2 million people died in 2000 from waterborne diseases (related to the consumption of contaminated water) or drought. In 2004, the UK charity WaterAid reported that a child dies every 15 seconds from easily preventable water-related diseases; often this means lack of sewage disposal; see toilet.\n\nOrganizations concerned with water protection include the International Water Association (IWA), WaterAid, Water 1st, and the American Water Resources Association. The International Water Management Institute undertakes projects with the aim of using effective water management to reduce poverty. Water related conventions are United Nations Convention to Combat Desertification (UNCCD), International Convention for the Prevention of Pollution from Ships, United Nations Convention on the Law of the Sea and Ramsar Convention. World Day for Water takes place on 22 March and World Ocean Day on 8 June.\n\nWater is considered a purifier in most religions. Faiths that incorporate ritual washing (ablution) include Christianity, Hinduism, Islam, Judaism, the Rastafari movement, Shinto, Taoism, and Wicca. Immersion (or aspersion or affusion) of a person in water is a central sacrament of Christianity (where it is called baptism); it is also a part of the practice of other religions, including Islam (\"Ghusl\"), Judaism (\"mikvah\") and Sikhism (\"Amrit Sanskar\"). In addition, a ritual bath in pure water is performed for the dead in many religions including Islam and Judaism. In Islam, the five daily prayers can be done in most cases after completing washing certain parts of the body using clean water (\"wudu\"), unless water is unavailable (see \"Tayammum\"). In Shinto, water is used in almost all rituals to cleanse a person or an area (e.g., in the ritual of \"misogi\").\n\nIn Christianity, holy water is water that has been sanctified by a priest for the purpose of baptism, the blessing of persons, places, and objects, or as a means of repelling evil.\n\nIn Zoroastrianism, water (\"āb\") is respected as the source of life.\n\nThe Ancient Greek philosopher Empedocles held that water is one of the four classical elements along with fire, earth and air, and was regarded as the ylem, or basic substance of the universe. Thales, who was portrayed by Aristotle as an astronomer and an engineer, theorized that the earth, which is denser than water, emerged from the water. Thales, a monist, believed further that all things are made from water. Plato believed the shape of water is an icosahedron which accounts for why it is able to flow easily compared to the cube-shaped earth.\n\nIn the theory of the four bodily humors, water was associated with phlegm, as being cold and moist. The classical element of water was also one of the five elements in traditional Chinese philosophy, along with earth, fire, wood, and metal.\n\nWater is also taken as a role model in some parts of traditional and popular Asian philosophy. James Legge's 1891 translation of the \"Dao De Jing\" states, \"The highest excellence is like (that of) water. The excellence of water appears in its benefiting all things, and in its occupying, without striving (to the contrary), the low place which all men dislike. Hence (its way) is near to (that of) the Tao\" and \"There is nothing in the world more soft and weak than water, and yet for attacking things that are firm and strong there is nothing that can take precedence of it—for there is nothing (so effectual) for which it can be changed.\" \"Guanzi\" in \"Shui di\" 水地 chapter further elaborates on symbolism of water, proclaiming that \"man is water\" and attributing natural qualities of the people of different Chinese regions to the character of local water resources.\n\nWater's technically correct but rarely used chemical name, \"dihydrogen monoxide\", has been used in a series of hoaxes and pranks that mock scientific illiteracy. This began in 1983, when an April Fools' Day article appeared in a newspaper in Durand, Michigan. The false story consisted of safety concerns about the substance.\n\n\n"}
{"id": "9123968", "url": "https://en.wikipedia.org/wiki?curid=9123968", "title": "Wind Over Water", "text": "Wind Over Water\n\nWind Over Water: The Cape Wind Story is an independent documentary film which chronicles the controversy over the Cape Wind Project. The film, by journalist Ole Tangen Jr. documented both sides of the debate as it unfolded on the Cape. An independent production, the filmmaker interviewed numerous people involved with the project including Jim Gordon of Cape Wind and Isaac Rosen, then director of the Alliance to Protect Nantucket Sound.\n\nFocusing also on wind power in general, \"Wind Over Water\" features aerial footage of the offshore wind farm at Horns Rev in Denmark and footage from various wind farms in the US.\n\nThe film had its world premiere on December 6, 2003, at Lillie Auditorium, Woods Hole, MA. It has also been screened across the USA and in Canada and New Zealand.\n\n\n"}
