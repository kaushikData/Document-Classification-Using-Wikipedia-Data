{"id": "15132888", "url": "https://en.wikipedia.org/wiki?curid=15132888", "title": "2008 Kazan gas explosion", "text": "2008 Kazan gas explosion\n\nThe Kazan gas explosion occurred on January 9, 2008, destroying an entire corner of a three-story khrushchyovka-style apartment building on Malaya Pechyorskaya Street in the Aviastroitelny District of Kazan, Tatarstan, Russia. The explosion took place at 0:28 a.m. local time (UTC +3).\n\nThe blast destroyed 12 apartments and killed ten residents, including one woman who died of blast-related trauma in a hospital after being rescued from the rubble. Additionally, two were non-fatally wounded.\n\nRescue efforts were complicated by low temperatures of -30 °C, raising concerns that victims trapped in the destroyed building might freeze before being reached by rescuers.\n\nThree of the building residents, including one child, were not immediately accounted for. However, several body parts were found in the ruins, and subsequent DNA analyses determined that the young girl and her grandfather were both among those killed in the explosion. The girl's grandmother, also thought to have been in the apartment, is still counted missing.\n\nThe exact cause of the blast has not been established. While a criminal investigation was initiated, city authorities suspect that the cause may have been improper use of gas appliances.\n\nThe Kazan city government arranged for residents of the building destroyed in the blast to receive apartments in newly constructed buildings. On July 10, 2008, the first nine affected families received keys to new apartments in a building on Amirkhan Street. The residents received the apartments in exchange for their destroyed ones, an arrangement set up by the city-held \"OAO Residential Investment Company of the City of Kazan\" and supported by Kazan mayor Ilsur Metshin and the district administration. Families were also given the opportunity to pay for larger apartments at a discounted price per square meter.\n"}
{"id": "46206451", "url": "https://en.wikipedia.org/wiki?curid=46206451", "title": "After the Wind", "text": "After the Wind\n\nAfter the Wind: 1996 Everest Tragedy—One Survivor’s Story is a book by Lou Kasischke that details his experiences as a client on Rob Hall’s expedition during the 1996 Mount Everest tragedy. The accident killed eight climbers—including four from the Hall expedition—and remained the worst climbing accident on Everest until the 2014 Mount Everest avalanche. The book features 55 illustrations by Jane Cardinal and was published in 2014 by Good Hart Publishing.\n\nInitially written in the aftermath of the tragedy, Kasischke waited 16 years to release \"After the Wind\". The book focuses on the events leading up to the ascent, Kasischke’s experiences on the mountain, and his motivation for turning around 400 feet from the summit, as well as the close relationship he shares with his wife, Sandy.\n\nUsing the analytical skills acquired from his many years as a corporate attorney, Kasischke also offers a fresh analysis of how the tragedy unfolded and the decisions—both individual and collective—that contributed to it. Though Kasischke avoids blaming any single person for the tragedy, he does note that the presence of journalist Jon Krakauer may have caused guide Rob Hall, whose Adventure Consultants business was in competition with Scott Fischer’s Mountain Madness, to take uncharacteristic risks on the mountain in an effort to get as many clients as possible to the summit and subsequently receive favorable press from Krakauer's report.\n\n\"After the Wind\" received favorable reviews from critics. \"Kirkus Reviews\" gave the book a starred review, describing it as “a vivid, intimate memoir that, with great clarity and attention to detail, tells an unforgettable survival story.” Blue Ink Reviews labelled the book a “thoughtful, well-written love story” and “an edge-of-your-seat description of navigating and mountaineering Everest,” and awarded the book a starred review. It was also nominated for the 2014 Kirkus Prize for non-fiction and was a finalist for Eric Hoffer Grand Prize and First Horizon Book Awards.\n\nA reviewer of \"Publishers Weekly\" commented \"His narrative is stark and accessible, bringing the mountaineer’s journey to life in accessible language, though the internal monologue veers toward self-indulgent reflection and recriminations... Regardless of why the tragedy occurred, Kasischke’s account provides an eye-opening look at the perils and extreme conditions on Everest. Evocative illustrations by Jane Cardinal further enhance the text, and include maps and time lines\".\n\n\n"}
{"id": "297395", "url": "https://en.wikipedia.org/wiki?curid=297395", "title": "Autecology", "text": "Autecology\n\nAutecology is an approach in ecology that seeks to explain the distribution and abundance of species by studying interactions of individual organisms with their environments. An autecological approach differs from both community ecology (synecology) and population ecology by greater recognition of the species-specific adaptations of individual animals, plants or other organisms, and of environmental over density-dependent influences on species distributions. Autecological theory relates the species-specific requirements and environmental tolerances of individuals to the geographic distribution of the species, with individuals tracking suitable conditions, having the capacity for migration at at least one stage in their life cycles. Autecology has a strong grounding in evolutionary theory,including the theory of punctuated equilibrium and the recognition concept of species.\nAutecology was pioneered by German field botanists in the late 19th century. During the 20th century, autecology continued to exist mainly as a descriptive science rather than one with supporting theory and the most notable proponents of an autecological approach, Herbert Andrewartha and Charles Birch, avoided the term autecology when referring to species-focused ecological investigation with emphasis on density-independent processes. Part of the problem with deriving a theoretical structure for autecology is that individual species are unique in their life history and behaviour, making it difficult to draw broad generalisations across them without losing the crucial information that is gained by studying biology at a species level. Progress has been made in more recent times with Paterson’s recognition concept of species and the concept of habitat tracking by organisms . The most recent attempt at deriving a theoretical structure for autecology was published in 2014 by ecologists Gimme Walter and Rob Hengeveld.\n\nAutecological theory is focused on species as the most important unit of biological organisation, as individuals across all populations of a particular species share species-specific adaptations that influence their ecology. This particularly relates to reproduction, as individuals of a sexual species share unique adaptations (e.g. courtship songs, pheromones) for recognising potential mates, and share a fertilisation mechanism that differs from those in all other species. This recognition concept of species differs from the biological species concept (or isolation concept) which defines species by cross-mating sterility, which in allopatric speciation is merely a consequence of adaptive change in a new species' fertilisation mechanism to suit a different environment.\n\nIndividuals from across a species' range tend to be relatively uniform in terms of their dietary and habitat requirements and the range of environmental conditions they can tolerate. These differ from those of other species. Individuals of a species likewise share specific sensory adaptations for recognising suitable habitat. Seasonal changes and variability in climate mean that the spatial and/or temporal distribution of suitable habitat for a species also varies.In response, organisms track suitable conditions, for example by migrating in order to remain within suitable habitat, for which there is evidence in the fossil record. By determining the requirements and tolerances of a particular species, it is possible to predict how individuals of that species will respond to specific environmental changes \n\nAutecological theory predicts that populations will reproduce at around replacement level unless a period of environmental change causing unusually high or low survival causes the population to grow or shrink before restabilising at replacement level again . Population numbers may be reduced by introduction of new predation pressure, such as with poor fisheries management or introduction of a biological control agent to control an invasive species, such that net reproductive rate, R0, drops below replacement level. The species being preyed upon in each case may stabilise at a lower population density where it is more difficult for individuals of the higher trophic level to locate the prey species, but at this point relieving predation tends to make little difference to population size, as individuals continue to reproduce around replacement level as they were at a higher density prior to the introduction of a higher trophic level .\n\nKnowledge of species-level interactions, tolerances and habitat requirements is valuable for conservation of an endangered plant or animal species by ensuring its particular ecological requirements are met. \n\nWith focus on individual organism, autecology has mechanistic links to several other biological fields, including ethology, evolution, genetics and physiology\n"}
{"id": "2161298", "url": "https://en.wikipedia.org/wiki?curid=2161298", "title": "Bioglass", "text": "Bioglass\n\nBioglass 45S5, commonly referred to by its commercial name Bioglass, is a glass specifically composed of 45 wt% SiO, 24.5 wt% CaO, 24.5 wt% NaO, and 6.0 wt% PO.  Glasses are non-crystalline amorphous solids that are commonly composed of silica-based materials with other minor additives.  Compared to soda-lime glass (commonly used, as in windows or bottles), Bioglass 45S5 contains less silica and higher amounts of calcium and phosphorus.  The 45S5 name signifies glass with 45 weight % of SiO and 5:1 molar ratio of calcium to phosphorus.  This high ratio of calcium to phosphorus promotes formation of apatite crystals; calcium and silica ions can act as crystallization nuclei.  Lower Ca:P ratios do not bond to bone.  Bioglass 45S5's specific composition is optimal in biomedical applications because of its similar composition to that of hydroxyapatite, the mineral component of bone. This similarity provides Bioglass' ability to be integrated with living bone.\n\nThis composition of bioactive glass is comparatively soft in comparison to other glasses. It can be machined, preferably with diamond tools, or ground to powder. Bioglass has to be stored in a dry environment, as it readily absorbs moisture and reacts with it. Bioglass 45S5 is the first formulation of an artificial material that was found to chemically bond with bone. One of its main medical advantages is its biocompatibility, seen in its ability to avoid an immune reaction and fibrous encapsulation. Its primary application is the repair of bone injuries or defects too large to be regenerated by the natural process.\n\nThe first successful surgical use of Bioglass 45S5 was in replacement of ossicles in the middle ear, as a treatment of conductive hearing loss. Other uses include cones for implantation into the jaw following a tooth extraction. Composite materials made of Bioglass 45S5 and patient's own bone can be used for bone reconstruction. Further research is being conducted for the development of new processing techniques to allow for more applications of Bioglass.\n\nBioglass is important to the field of biomaterials as one of the first completely synthetic materials that seamlessly bonds to bone. It was developed by Larry L. Hench in the late 1960s. The idea for the material came to him during a bus ride in 1967. While working as an assistant professor at the University of Florida, Dr. Hench decided to attend the U.S. Army Materials Research Conference held in Sagamore, New York, where he planned to talk about radiation resistant electronic materials. He began discussing his research with a fellow traveller on the bus, Colonel Klinker, who had recently returned to the United States after serving as an Army medical supply officer in Vietnam.\n\nAfter listening to Dr. Hench's description of his research, the Colonel asked, “If you can make a material that will survive exposure to high energy radiation can you make a material that will survive exposure to the human body?” Klinker then went on to describe the amputations that he had witnessed in Vietnam, which resulted from the body's rejection of metal and plastic implants. Hench realized that there was a need for a novel material that could form a living bond with tissues in the body.\n\nWhen Hench returned to Florida after the conference, he submitted a proposal to the U.S. Army Medical Research and Design Command. He received funding in 1968, and in November 1969 Hench began to synthesize small rectangles of what he called 45S5 glass. Ted Greenlee, Assistant Professor of Orthopaedic Surgery at the University of Florida, implanted them in rat femurs at the VA Hospital in Gainesville. Six weeks later, Greenlee called Hench asking, \"Larry, what are those samples you gave me? They will not come out of the bone. I have pulled on them, I have pushed on them, I have cracked the bone and they are still bonded in place.\"\n\nWith this first successful experiment, Bioglass was born and the first compositions studied. Hench published his first paper on the subject in 1971 in the Journal of Biomedical Materials Research, and his lab continued to work on the project for the next 10 years with continued funding from the U.S. Army. By 2006, there were over 500 papers published on the topic of bioactive glasses from different laboratories and institutions around the world. The first successful surgical use of Bioglass 45S5 was in replacement of ossicles in middle ear as a treatment of conductive hearing loss, and the material continues to be used in bone reconstruction applications today.\n\nBioactive glass offers good osteoconductivity and bioactivity, it can deliver cells and is biodegradable. This makes it an excellent candidate to be used in tissue engineering applications. Although this material is known to be brittle, it is still used extensively to enhance the growth of bone since new forms of bioactive glasses are based on borate and borosilicate compositions. Bioglass can also be doped with varying quantities of elements like Copper, Zinc, or Strontium which can allow the growth and formation of healthy bone. The formation of neocartilage can also be induced with bioactive glass by using an in vitro culture of chondrocyte-seeded hydrogels and can serve as a subchondral substrate for tissue-engineered osteochondral constructs.\n\nThe borate-based bioactive glass has controllable degradation rates in order to match the rate at which actual bone is formed. Bone formation has been shown to enhance when using this type of material. When implanted into rabbit femurs, the 45S5 bioactive glass showed that it could induce bone proliferation at a much quicker rate than synthetic hydroxyapatite (HA). 45S5 glass can also be osteoconductive and osteoinductive because it allows for new bone growth along the bone-implant interface as well as within the bone-implant interface. Studies have been conducted to determine the process by which it can induce bone formation. It was shown that 45S5 glass degrades and releases sodium ions, as well as soluble silica, the combination of all these ions is said to produce new bone. Borate bioglass has proven that it can support cell proliferation and differentiation in vitro and in vivo. It also has shown that it is suitable to be used as a substrate for drug release when treating bone infection. However, there has been a concern as to whether or not the release of boron into a solution as borate ions will be toxic to the body. It has been shown that in static cell culture conditions, borate glasses were toxic to cells, but not in dynamic culture conditions.\n\nAnother area in which bioactive glass has been investigated to use is enamel reconstruction, which has proven to be a difficult task in the field of dentistry. Enamel is made up of a very organized hierarchical microstructure of carbonated hydroxyapatite nanocrystals. It has been reported that Bioglass 45S5-phosphoric acid paste can be used to form an interaction layer that can obstruct dentinal tubule orifices and can therefore be useful in the treatment of dentin hypersensitivity lesions. This material in an aqueous environment could have an antibacterial property that is advantageous in periodontal surgical procedures. In a study done with 45S5 Bioglass, control biofilms of S. sanguis were grown on inactive glass particulates and the biofilm grown on the Bioglass was significantly lower than those that were on the inactive glass. It was concluded that Bioglass can reduce surface bacterial formation, which could benefit post-surgical periodontal wound healing. The most effective antibacterial bioactive glass is S53P4, which has exhibited a growth-inhibitory effect on the pathogens that was tested on it. Bioactive glasses that are sol-gel derived, such as CaPSiO and CaPSiO II, have also exhibited antibacterial property. Studies done with S. epidermidis and E. coli cultured with bioactive glass have shown that the 45S5 bioactive glass have a very high antibacterial resistance. It was also observed in the experiment that there were needle-like bioglass debris which could have ruptured the cell walls of the bacteria and rendered them inactive.\n\nBioactive glass has even been applied to medical devices to help restore the hearing to a deaf patient using Bioglass 45S5 in 1984. The patient went deaf due to at ear infection that degraded two of the three bones in her middle ear. An implant was designed to replace the damaged bone and carry sound from the eardrum to the cochlea, restoring the patient's hearing. Before this material was available, plastics and metals would be used because they did not produce a reaction in the body, however they eventually failed because tissue would grow around them after implantation. A prosthesis made up of Bioglass 45S5 was made to fit the patient and most of the prosthesis that were made were able to maintain functionality after 10 years. The Endosseous Ridge Maintenance Implant made of Bioglass 45S5 was another device that could be inserted into tooth extraction sites that would repair tooth roots and allow for a stable ridge for dentures.\n\nThis material has also been used in jaw and orthopedics applications, in this way it dissolves and can stimulate the natural bone to repair itself. GlaxoSmithKline is using this material as an active ingredient in toothpaste  under the commercial name NovaMin, which can help repair tiny holes and decrease tooth sensitivity. Currently, bioactive glass is still be researched and has yet to reach its full capacity of use .\n\nWhen implanted, Bioglass 45S5 reacts with the surrounding physiological fluid, causing the formation of a hydroxyl carbonated apatite (HCA) layer at the material surface. The HCA layer has a similar composition to hydroxyapatite, the mineral phase of bone, a quality which allows for strong interaction and integration with bone. The process by which this reaction occurs can be separated into 12 steps. The first 5 steps are related to the Bioglass response to the environment within the body, and occur rapidly at the material surface over several hours. Reaction steps 6-10 detail the reaction of the body to the integration of the biomaterial, and the process of integration with bone. These stages occur over the scale of several weeks or months. The steps are separated as follows:\n\n1. Alkali ions (ex. Na and Ca) on the glass surface rapidly exchange with hydrogen ions or hydronium from surrounding bodily fluids. The reaction below shows this process, which causes hydrolysis of silica groups. As this occurs, the pH of the solution increases.\n\nSi⎯O⎯Na + H + OH → Si⎯OH + Na (aq) + OH\n\n2. Due to an increase in the hydroxyl (OH) concentration at the surface (a result of step 1), a dissolution of the silica glass network occurs, seen by the breaking of Si⎯O⎯Si bonds. Soluble silica is transformed to the form of Si(OH) and silanols (Si⎯OH) creation occurs at the material surface. The reaction occurring in this stage is shown below:\n\nSi⎯O⎯Si + HO→ Si⎯OH + OH⎯Si\n\n3. The silanol groups at the material surface condense and re-polymerize to form a silica-gel layer at the surface of Bioglass. As a result of the first steps, the surface contains very little alkali content. The condensation reaction is shown below:\n\nSi⎯OH + Si⎯OH → Si⎯O⎯Si\n\n4. Amorphous Ca and PO gather at the silica-rich layer (created in step 3) from both the surrounding bodily fluid and the bulk of the Bioglass. This creates a layer composed primarily of CaO⎯PO on top of the silica layer.\n\n5. The CaO⎯PO film created in step 4 incorporates OH and CO from the bodily solution, causing it to crystallize. This layer is called a mixed carbonated hydroxyl apatite (HCA).\n\n6. Growth factors adsorb to the surface of Bioglass due to its structural and chemical similarities to hydroxyapatite.\n\n7. Adsorbed growth factors cause the activation of M2 macrophages. M2 macrophages tend to promote wound healing and the initiate the migration of progenitor cells to an injury site. In contrast, M1 macrophages become activated when a non-biocompatible material is implanted, triggering an inflammatory response.\n\n8. Triggered by M2 macrophage activation, mesenchymal stem cells and osteoprogenitor cells migrate to the Bioglass surface and attach to the HCA layer.\n\n9. Stem cells and osteoprogenitor cells at the HCA surface differentiate to become osteogenic cells typically present in bone tissue, particularly osteoblasts.\n\n10. The attached and differentiated osteoblasts generate and deposit extracellular matrix (ECM) components, primarily type I collagen, the main protein component of bone.\n\n11. The collagen ECM becomes mineralized as normally occurs in native bone. Nanoscale hydroxyapatite crystals form a layered structure with the deposited collagen at the surface of the implant.\n\n12. Following these reactions, bone growth continues as the newly recruited cells continue to function and facilitate tissue growth and repair. The Bioglass implant continues to degrade and be converted to new ECM material.\n\nThere are three main manufacturing techniques that are used for the synthesis of Bioglass. The first is melt quench synthesis, which is the conventional glass-making technology used by Larry Hench when he first manufactured the material in 1969. This method includes melting a mixture of oxides such as SiO, NaO, CaO and PO at high temperatures generally above 1100-1300 °C. Platinum or platinum alloy crucibles to are used avoid contamination, which would interfere with the product's chemical reactivity in organism. Annealing is a crucial step in forming bulk parts, due to high thermal expansion of the material. Heat treatment of Bioglass reduces the volatile alkali metal oxide content and precipitates apatite crystals in the glass matrix. However, the scaffolds that result from melt quench techniques are much less porous compared to other manufacturing methods, which may lead to defects in tissue integration when implanted in vivo.\n\nThe second method is sol-gel synthesis of Bioglass. This process is carried out at much lower temperatures than the traditional melting methods. It involves the creation of a solution (sol), which is composed of metal-organic and metal salt precursors. A gel is then formed through hydrolysis and condensation reactions, and it undergoes thermal treatment for drying, oxide formation, and organic removal. Because of the lower fabrication temperatures used in this method, there is a greater level of control on the composition and homogeneity of the product. In addition, sol-gel bioglasses have much higher porosity, which leads to a greater surface area and degree of integration in the body.\n\nThe third method is microwave synthesis of Bioglass, which has been gaining attention in recent years. Microwave synthesis is a rapid and low-cost powder synthesis method in which precursors are dissolved in water, transferred to an ultrasonic bath, and irradiated. The resulting amorphous powder is then filtered, dried  at 80 °C, and calcined at 700 °C.\n\nA setback to using Bioglass 45S5 is that it is difficult to process into porous 3D scaffolds. These porous scaffolds are usually prepared by sintering glass particles that are already formed into the 3D geometry and allowing them to bond to the particles into a strong glass phase made up of a network of pores. Since this particular type of bioglass cannot fully sinter by viscous flow above its Tg, and its Tg is close to the onset of crystallization, it is hard to sinter this material into a dense network.\n\n45S5 glass also has a slow degradation and rate of conversion to an HA-like material. This setback makes it more difficult for the degradation rate of the scaffold to coincide with the rate of tissue formation. Another limitation is that the biological environment can be easily influenced by its degradation. Increases in the sodium and calcium ions and changing pH is due to its degradation. However, the roles of these ions and their toxicity to the body have not been fully researched.\n\nSeveral studies have investigated methods to improve the mechanical strength and toughness of Bioglass 45S5. These include creating polymer-glass composites, which combine the bioactivity of Bioglass with the relative flexibility and wear resistance of different polymers. Another solution is coating a metallic implant with Bioglass, which takes advantage of the mechanical strength of the implant's bulk material while retaining bioactive effects at the surface. Some of the most notable modifications have used various forms of carbon to improve the properties of 45S5 glass.\n\nFor example, Touri et al. developed a method to incorporate carbon nanotubes (CNTs) into the structure without interfering with the material's bioactive properties. CNTs were chosen because of their large aspect ratio and high strength. By synthesizing Bioglass 45S5 on a CNT scaffold, the researchers were able to create a composite that more than doubled the compressive strength and the elastic modulus when compared to the pure glass.\n\nAnother study carried out by Li et al. looked into different properties, such as the fracture toughness and wear resistance of Bioglass 45S5. The authors loaded graphene nanoplatelets (GNP) into the glass structure through a spark plasma sintering method. Graphene was chosen because of its high specific surface area and strength, as well as its cytocompatibility and lack of interference with Bioglass 45S5's bioactivity. The composites that were created in this experiment achieved a fracture toughness of more than double the control. In addition, the tribological properties of the material were greatly improved.\n\n"}
{"id": "1067485", "url": "https://en.wikipedia.org/wiki?curid=1067485", "title": "Bismanol", "text": "Bismanol\n\nBismanol is an magnetic alloy of bismuth and manganese (manganese bismuthide) developed by the US Naval Ordnance Laboratory. \n\n\"Bismanol\", a permanent magnet made from powder metallurgy of Manganese Bismuthide, was developed by the US Naval Ordnance Laboratory in the early 1950s - at the time of invention it was one of the highest coercive force permanent magnets available, at 3000 oersteds. Coercive force reached 3650 oersteds and flux density 4800 by the mid 1950s. The material was generally strong, and stable to shock and vibration, but had a tendency to chip. Slow corrosion of the material occurred under normal conditions.\n\nThe material was used to make permanent magnets for use in small electric motors.\n\nBismanol magnets have been replaced by Neodymium magnets which are both cheaper and superior in other ways, Samarium-Cobalt magnets in more critical applications, and Alnico magnets.\n"}
{"id": "30015793", "url": "https://en.wikipedia.org/wiki?curid=30015793", "title": "Blue Sky Solar Racing", "text": "Blue Sky Solar Racing\n\nBlue Sky Solar Racing is a student-run initiative at the University of Toronto. The team aims to promote environmentally friendly technologies through the design, construction and showcasing of world class solar powered vehicles. The project encourages students to participate in a long-term multidisciplinary project to augment their skills learned in class, and to make a positive impact on their community.\n\nSince its foundation in 1997, the team has produced eight vehicles that competed in a total of nine international solar car racing competitions. Their most recent vehicle, Horizon, was ranked 12th at World Solar Challenge 2015, and 1st in Canada. In 2016, Horizon raced in the American Solar Challenge and finished 3rd place overall, and 1st in Canada.\n\nIn addition to racing, the Blue Sky team also actively participates in community events to promote environmental awareness and innovation in technology. Through attending showcase events in schools around Toronto, the team has generated a great deal of public interest in science and engineering while also educating students about environmental issues.\n\n\nThe Blue Sky Solar Racing Team is a multidisciplinary team composed of undergraduate, graduate students and alumni volunteers from the Faculty of Applied Science & Engineering, the Faculty of Arts & Science, and the Joseph L. Rotman School of Management at the University of Toronto. With over 1000 members from a diverse background since the start of the team, the team is able to offer a unique perspective on issues of sustainability and technological advancement. Team members are committed to demonstrating the viability of alternative energy technology and the practical benefits of a multidisciplinary approach to solving problems.\n\nIn 1997, the Blue Sky Solar Racing Team produced its first solar powered vehicle named Blue Sky Project. Being the very first car, Blue Sky Project served as a test bed to work out the organizational and operational relationships required to build a car that was capable of qualifying and participating in competition. The project was deemed as a success. The successful freshman effort gave the team a firm foundation to further expand its capabilities to compete with other well-established Canadian solar car racing teams such as Midnight Sun Solar Race Team and Queen's University Solar Vehicle Team.\n\nAs Blue Sky Solar Racing Team's freshman effort in the competitive scene of solar car racing, Florida SunDay Challenge 1997 served as a proof that the University of Toronto had the capability to produce world-class solar vehicles despite their inexperience. Ranking 3rd overall in the competition with their first vehicle – Blue Sky Project, the rookie team's impressive results fueled their expansion and development. Participation in the race also gave the team an invaluable learning experience which formed the foundation of the team's later innovations.\n\nBlue Sky Solar Racing Team's Blue II competed in SunRayce '99 in Orlando, Florida. Finishing 9th in the qualifying round, Blue II showed great promise. However, the race was struck with poor weather conditions throughout the competition and recharging the solar vehicle became a great challenge for the team. The team was forced to adjust their racing strategy and optimize their power usage under the new weather constraints so they did not exhaust the battery. Despite such poor weather conditions, Blue II traveled over 2250 km over a span of 9 days and ranked 20th overall in the competition. The team was also awarded \"Best Rookie Team\" in recognition of their achievements in the race.\n\nWith the team's third generation solar vehicle, Faust, the team took on the world stage for the first time at the World Solar Challenge. Faust ran a perfect race without a single mishap and not once was there an emergency during the racing that required Faust to be pulled over. For 5 days straight, the team drove for 8 hours through the Australian continent covering 3010 km in total. The team ranked 14th overall in the competition, besting many teams with years of experience; an excellent achievement for a rookie team.\n\nThe American Solar Challenge 2001 served as one of the greatest challenges that Blue Sky Solar Racing Team has faced to date. Crossing seven states, Blue Sky's third generation vehicle, Faust, was tasked with overcoming long stretches of rough terrain. This put the vehicle's handling and stability to the test as solar vehicles are rarely designed to withstand such a beating. The rough road conditions of the Great Plains and Rocky Mountains caused havoc to the suspension system in the vehicle and caused a cascade effect throughout other systems. Quick, temporary fixes were implemented when breakdowns occurred during official race hours, and long nights of repair would ensue at the end of each race day. After several sleepless nights, the team completed the 3595 km race in 79 hours 8 minutes and 57 seconds, which included 10 hours of penalties. The team ranked 12th in the competition and was one of the few teams that were able to fully complete the race.\n\nWith experience of the competition's grueling road conditions, Blue Sky Solar Racing Team returned to the American Solar Challenge in 2003 with Faust II, the successor to Blue Sky's vehicle in their first American Solar Challenge attempt. Traveling over across 8 states from Chicago, Illinois, to Los Angeles, California, Faust II proved to be an extremely capable race car. In contrast to American Solar Challenge 2001, Faust II was able to traverse the grueling terrain with no major on-road difficulties due to the improvements made to the vehicle. Logging 79 hours, 51 minutes, and 39 seconds of race time, Blue Sky captured 11th place overall and placed first out of all teams of equivalent solar array technology. The team was also presented with the American Solar Challenge Safety Award for outstanding safety practices during the competition.\n\nBlue Sky Solar Racing Team's fifth generation solar vehicle, Cerulean, participated in the Panasonic World Solar Challenge 2007 and finished with spectacular results. Traveling 3000 km across the Australian outback, Cerulean completed the race in 46 hours and 19 minutes with a top speed of 118 km/h. The team ranked 5th in the Adventure class which was the highest placing amongst all Canadian teams in the competition.\n\nIn 2011, Blue Sky Solar entered the World Solar Challenge for the first time in four years. Their sixth generation vehicle, Azure, placed 24th in the Challenger class of the race.\n\nB-7, the team's seventh generation vehicle, placed eighth in the World Solar Challenge 2013, 2nd among North American teams, and first among Canadian teams, in the Challenger class.\n\nHorizon, the eighth generation vehicle competed in the World Solar Challenge 2015 and finished 12th.\n\nFor the first time in 13 years, Blue Sky Solar Racing took part in the American Solar Challenge. Racing with their eight generation car, Horizon, Blue Sky Solar Racing completed the race with a 3rd-place finish.\n\nAs Blue Sky Solar Racing's first project, Blue Sky Project served as an experimental effort for the team to learn more about the production of solar vehicles. The vehicle was designed as a single-seated four-wheeler with a weight of 850 lbs. The project was completed in 1997 and was immediately put to test at the Florida SunDay Challenge 1997. The vehicle achieved extremely encouraging results for the rookie team and created a solid foundation and strong motivation for the team to continue developing solar vehicles.\n\nWeight: 850 lbsPower: 650 watts, 14% silicon solar cellsBatteries: Lead-acidCompetitions: Florida SunDay Challenge 1997 – 3rd Place\n\nUpon the success of the Blue Sky Project, Blue Sky Solar Racing decided to immediately begin production of an improved solar vehicle – Blue II. With a much greater power output than the Blue Sky Project, Blue II was capable of achieving a top speed of 110 km/h and it was as a key milestone in the technical development of the team. Blue II was completed in 1999 and competed in SunRayce '99. After the vehicle was decommissioned, it served for several years as the main display in the team's community education events in recognition of its milestone achievements.\n\nWeight: 968 lbs\nPower: 750 watts, 15% silicon solar cells\nBatteries: Lead-acid\nCompetitions: SunRayce '99 – 20th Place + Top Rookie Award\n\nAfter learning from the experiences of their previous experimental vehicles, Blue Sky Solar Racing was prepared to make a splash on the international stage. In the design of their third generation vehicle – Faust – the team focused on several key concepts to produce a solar vehicle that was competitive in international competitions; mainly, to minimize drag and weight. This led to several drastic departures in their design traditions. First of all, in contrast to Blue II, Faust was designed as a three-wheeler to minimize rolling resistance and decrease the overall weight of the vehicle. Secondly, the chassis for Faust was made of an extremely lightweight hollow tube aluminum space frame which decreased the overall weight of the vehicle to only 440 lbs, nearly half of the weight of Blue II. These departures in design lead to vast performance improvements which made it possible for the vehicle to achieve a top speed of 140 km/h. Construction Faust was completed in 2001 and it participated in two competitions in the same year.\n\nWeight: 620 lbs\nPower: 950 watts, 16.2% silicon solar cells\nBatteries: Lithium ion-polymer\nCompetitions: World Solar Challenge 2001 – 12th place, American Solar Challenge 2001 – 14th place\n\nLooking to improve upon the success of Faust, its successor – Faust II – aimed primarily to strengthen the build of the body and reduce weight, thus allowing it to achieve faster speeds. Learning from the damage that the grueling road conditions in the American Solar Challenge 2001 brought to Faust, the team fitted Faust II with a composite fiber cloth body which made it extremely durable while also reducing the vehicle's body weight by 40 lbs. Several other improvements were also made to the battery and motor of the vehicle. Production of Faust II was completed in 2003 and it participated in the American Solar Challenge 2003.\n\nWeight: 600 lbs\nPower: 1050 watts, 16.8% silicon solar cells\nBatteries: Lithium ion-polymer\nCompetitions: American Solar Challenge 2003 – 11th Place + Safety Award\n\nWhile in the production phase of Faust II, Blue Sky Solar Racing began design on their fifth generation vehicle – Cerulean. Unlike its predecessors, Cerulean was designed as a double-seater vehicle; this allowed the team challenge themselves by attempting a new solar vehicle design with entirely different specifications and constraints.\n\nAs a double-seater vehicle, the team was allowed to double the solar array output to 2300W and the motor power output to 13.4 hp. The team was also capable of keeping the vehicle lightweight by constructing a chassis from carbon board rails, thus minimizing the weight at 500 lbs. Despite being their first attempt at constructing a double-seater solar vehicle, Cerulean proved to be their most successful vehicle to date. With construction complete in 2007, Cerulean participated in the World Solar Challenge 2007 and placed 5th in its class, ranking it the highest amongst all Canadian teams.\n\nWeight: 230 kg (without driver)\nPower: 2000 W 27% GaAs solar cells\nBatteries: Lithium polymer\nCompetitions: World Solar Challenge 2007 – 5th Place in Adventure Class\n\nBlue Sky Solar Racing's sixth generation vehicle, Azure, placed 24th in the World Solar Challenge 2011, in the Challenger Class. The design of the vehicle was guided by three requirements: The vehicle must be able to achieve high speeds yet have low power consumption, exhibit great stability at high speeds yet have a low drag value, and have a great safety margin in critical components yet remain lightweight.\nThe vehicle was officially unveiled on August 6, 2011.\n\nWeight: 250 kg (without driver)\nPower: 22% silicon solar cells\nBatteries: Lithium ion-polymer\nCompetitions: World Solar Challenge 2011 - 24th Place in Challenger Class\n\nOn July 28, 2013, Blue Sky Solar unveiled its seventh generation vehicle, B-7. The car features a five-fairing design. B-7 placed eighth in the World Solar Challenge 2013, 2nd among North American teams, and first among Canadian teams, in the Challenger class.\n\nWeight: 230 kg (without driver)\nPower: 1.3 kW, 22.5% silicon monocrystalline solar cells\nBatteries: Lithium ion-polymer\nCompetitions: World Solar Challenge 2013 - 8th Place in Challenger Class\n\nBlue Sky Solar Racing eighth generation solar powered vehicle - Horizon has an asymmetrical design and a catamaran-shaped aerobody. Horizon placed 12th in the Challenger class in the World Solar Challenge 2015. 3rd in North America and 1st in Canada. In the American Solar Challenge 2016, Horizon placed 3rd overall and 1st in Canada.\n\nWeight: 239 kg (without driver)\nPower: 1.4 kW, 23.9% silicon monocrystalline solar cells\nBatteries: Lithium ion-polymer\nCompetitions: World Solar Challenge 2015 - 12th Place in Challenger Class, American Solar Challenge 2016 - 3rd Place\n\nSimilar to Horizon, the ninth generation solar powered vehicle - Polaris has an asymmetrical design and a catamaran-shaped aerobody.\n\n\n"}
{"id": "241030", "url": "https://en.wikipedia.org/wiki?curid=241030", "title": "Bottom quark", "text": "Bottom quark\n\nThe bottom quark or b quark, also known as the beauty quark, is a third-generation quark with a charge of − \"e\".\n\nAll quarks are described in a similar way by electroweak and quantum chromodynamics, but the bottom quark has exceptionally low rates of transition to lower-mass quarks. The bottom quark is also notable because it is a product in almost all top quark decays, and is a frequent decay product of the Higgs boson.\n\nThe bottom quark was first described theoretically in 1973 by physicists Makoto Kobayashi and Toshihide Maskawa to explain CP violation. The name \"bottom\" was introduced in 1975 by Haim Harari.\n\nThe bottom quark was discovered in 1977 by the Fermilab E288 experiment team led by Leon M. Lederman, when collisions produced bottomonium. Kobayashi and Maskawa won the 2008 Nobel Prize in Physics for their explanation of CP-violation.\n\nOn its discovery, there were efforts to name the bottom quark \"beauty\", but \"bottom\" became the predominant usage, by analogy of \"top\" and \"bottom\" to \"up\" and \"down\".\n\nThe bottom quark's \"bare\" mass is around – a bit more than four times the mass of a proton, and many orders of magnitude larger than common \"light\" quarks.\n\nAlthough it almost-exclusively transitions from or to a top quark, the bottom quark can decay into either an up quark or charm quark via the weak interaction. CKM matrix elements \"V\" and \"V\" specify the rates, where both these decays are suppressed, making lifetimes of most bottom particles (~10 s) somewhat higher than those of charmed particles (~10 s), but lower than those of strange particles (from ~10 to ~10 s).\n\nThe combination of high mass and low transition-rate gives experimental collision byproducts containing a bottom quark a distinctive signature that makes them relatively easy to identify using a technique called \"B-tagging\". For that reason, mesons containing the bottom quark are exceptionally long-lived for their mass, and are the easiest particles to use to investigate CP violation. Such experiments are being performed at the BaBar, Belle and LHCb experiments.\n\nSome of the hadrons containing bottom quarks include:\n\n\n\n"}
{"id": "346585", "url": "https://en.wikipedia.org/wiki?curid=346585", "title": "Brazier", "text": "Brazier\n\nA brazier () is a container for hot coals, generally taking the form of an upright standing or hanging metal bowl or box. Used for burning solid fuel, usually charcoal, braziers principally provide heat, but may also be used for cooking and cultural rituals. Braziers have been recovered from many early archaeological sites like the Nimrud brazier, recently excavated by the Iraqi National Museum, which dates back to at least 824 BC.\n\nBraziers are mentioned in the Bible. The Hebrew word for brazier is of Egyptian origin, suggesting that it was imported from Egypt. There are two references to it in the Bible. The first is found in Genesis 15:17, whereby God Himself sent and provided a \"smoking brazier\" for the sacrifice which Abram prepared. The second is in Jeremiah 36:22–23, with braziers heating the winter palace of King Jehoiakim.\n\nThe Roman Emperor Jovian was poisoned by the fumes from a brazier in his tent in 364, ending the line of Constantine.\n\nDespite the risks associated with burning charcoal on open fires, braziers were widely adopted as a source of domestic heat, particularly in the Spanish-speaking world. Fernando de Alva Cortés Ixtlilxochitl notes that the Tepanec Tlatoani in New Spain slept between two braziers because he was so old he produced no natural heat. Nineteenth century British travellers such as diplomat and scientist Woodbine Parish and the writer Richard Ford, author of \"A Handbook for Travellers in Spain\", state that in many areas braziers were considered healthier than fireplaces and chimneys.\n\nThe brazier did not just sit out in the open, in a room; often it was incorporated into furniture. Many cultures developed their own variants of a low table, with a heat source underneath and blankets to capture the warmth: the \"kotatsu\" in Japan, the \"korsi\" in Iran, the \"sandali\" in Afghanistan, and the foot stove in northern Europe. In Spain the \"brasero\" continued to be one of the primary means of heating houses until the early 20th century; Gerald Brenan described in his memoir \"South from Granada\" the widespread habit there in the 1920s of placing a brazier beneath a cloth-covered table to keep the legs and feet of the family warm on winter evenings.\n\nWhen burned, moist rose and grapevine trimmings produce a pungent, sweet-smelling smoke, and make very good charcoal. When the charcoal fumes became overbearing, however, aromatics (lavender seeds, orange peel) were sometimes added to the embers in the brazier. A brazier used exclusively for burning aromatics (incense) is known as a censer or thurible.\n\nIn some churches a brazier is used to create a small fire, called new fire, which is then used to light the Paschal candle during the Easter Vigil.\n\nBraziers used to be a commonplace sight on industrial picket lines, although as strikes in the UK have become increasingly white collar, they are seen less and less.\n\nIn Japanese, a brazier is called a \"hibachi\". They are used principally for cooking and in cultural rituals such as the Japanese tea ceremony.\n\n"}
{"id": "7739", "url": "https://en.wikipedia.org/wiki?curid=7739", "title": "Carbide", "text": "Carbide\n\nIn chemistry, a carbide is a compound composed of carbon and a less electronegative element. Carbides can be generally classified by the chemical bonds type as follows: (i) salt-like, (ii) covalent compounds, (iii) interstitial compounds, and (iv) \"intermediate\" transition metal carbides. Examples include calcium carbide (CaC), silicon carbide (SiC), tungsten carbide (WC; often called, simply, \"carbide\" when referring to machine tooling), and cementite (FeC), each used in key industrial applications. The naming of ionic carbides is not systematic.\n\nSalt-like carbides are composed of highly electropositive elements such as the alkali metals, alkaline earth metals, and group 3 metals, including scandium, yttrium, and lanthanum. Aluminium from group 13 forms carbides, but gallium, indium, and thallium do not. These materials feature isolated carbon centers, often described as \"C\", in the methanides or methides; two-atom units, \", in the acetylides; and three-atom units, \", in the sesquicarbides. The graphite intercalation compound KC, prepared from vapour of potassium and graphite, and the alkali metal derivatives of C are not usually classified as carbides.\n\nCarbides of this class decompose in water producing methane. Three such examples are aluminium carbide , magnesium carbide and beryllium carbide .\n\nTransition metal carbides are not saline carbides but their reaction with water is very slow and is usually neglected. For example, depending on surface porosity, 5–30 atomic layers of titanium carbide are hydrolyzed, forming methane within 5 minutes at ambient conditions, following by saturation of the reaction.\n\nNote that methanide in this context is a trivial historical name. According to the IUPAC systematic naming conventions, a compound such as NaCH would be termed a \"methanide\", although this compound is often called methylsodium.\n\nSeveral carbides are assumed to be salts of the acetylide anion C (also called percarbide), which has a triple bond between the two carbon atoms. Alkali metals, alkaline earth metals, and lanthanoid metals form acetylides, e.g., sodium carbide NaC, calcium carbide CaC, and LaC. Lanthanides also form carbides (sesquicarbides, see below) with formula MC. Metals from group 11 also tend to form acetylides, such as copper(I) acetylide and silver acetylide. Carbides of the actinide elements, which have stoichiometry MC and MC, are also described as salt-like derivatives of .\n\nThe C-C triple bond length ranges from 119.2 pm in CaC (similar to ethyne), to 130.3 pm in LaC and 134 pm in UC. The bonding in LaC has been described in terms of La with the extra electron delocalised into the antibonding orbital on , explaining the metallic conduction.\n\nThe polyatomic ion , sometimes called sesquicarbide or allylenide, is found in LiC and MgC. The ion is linear and is isoelectronic with CO. The C-C distance in MgC is 133.2 pm. MgC yields methylacetylene, CHCCH, and propadiene, CHCCH, on hydrolysis, which was the first indication that it contains .\n\nThe carbides of silicon and boron are described as \"covalent carbides\", although virtually all compounds of carbon exhibit some covalent character. Silicon carbide has two similar crystalline forms, which are both related to the diamond structure. Boron carbide, BC, on the other hand, has an unusual structure which includes icosahedral boron units linked by carbon atoms. In this respect boron carbide is similar to the boron rich borides. Both silicon carbide (also known as \"carborundum\") and boron carbide are very hard materials and refractory. Both materials are important industrially. Boron also forms other covalent carbides, e.g. BC.\n\nThe carbides of the group 4, 5 and 6 transition metals (with the exception of chromium) are often described as interstitial compounds. These carbides have metallic properties and are refractory. Some exhibit a range of stoichiometries, e.g. titanium carbide, TiC. Titanium carbide and tungsten carbide are important industrially and are used to coat metals in cutting tools.\n\nThe long-held view is that the carbon atoms fit into octahedral interstices in a close-packed metal lattice when the metal atom radius is greater than approximately 135 pm:\n\nThe following table shows actual structures of the metals and their carbides. (N.B. the body centered cubic structure adopted by vanadium, niobium, tantalum, chromium, molybdenum and tungsten is not a close-packed lattice.) The notation \"h/2\" refers to the MC type structure described above, which is only an approximate description of the actual structures. The simple view that the lattice of the pure metal \"absorbs\" carbon atoms can be seen to be untrue as the packing of the metal atom lattice in the carbides is different from the packing in the pure metal, although it is technically correct that the carbon atoms fit into the octahedral interstices of a close-packed metal lattice.\n\nFor a long time the non-stoichiometric phases were believed to be disordered with a random filling of the interstices, however short and longer range ordering has been detected.\n\nIn these carbides, the transition metal ion is smaller than the critical 135 pm, and the structures are not interstitial but are more complex. Multiple stoichiometries are common; for example, iron forms a number of carbides, FeC, FeC and FeC. The best known is cementite, FeC, which is present in steels.\nThese carbides are more reactive than the interstitial carbides; for example, the carbides of Cr, Mn, Fe, Co and Ni are all hydrolysed by dilute acids and sometimes by water, to give a mixture of hydrogen and hydrocarbons. These compounds share features with both the inert interstitials and the more reactive salt-like carbides.\n\nMetal complexes containing C are known as metal carbido complexes. Most common are carbon-centered octahedral clusters, such as [AuC(PPh)] and [FeC(CO)]. Similar species are known for the metal carbonyls and the early metal halides. A few terminal carbides have been isolated, e.g., CRuCl(P(CH)).\n\nMetallocarbohedrynes (or \"met-cars\") are stable clusters with the general formula where M is a transition metal (Ti, Zr, V, etc.).\n\nSome metals, such as lead and tin, are believed not to form carbides under any circumstances. There exists however a mixed titanium-tin carbide, which is a two-dimensional conductor. (In 2007, there were two reports of a lead carbide PbC, apparently of the acetylide type; but these claims have yet to be published in reviewed journals.)\n\nIn addition to the carbides, other groups of related carbon compounds exist:\n"}
{"id": "43977871", "url": "https://en.wikipedia.org/wiki?curid=43977871", "title": "Chamber of Traditional Artisanship - Meknes", "text": "Chamber of Traditional Artisanship - Meknes\n\nThe Meknes Chamber of Traditional Artisanship () is a Moroccan government organization charged with the promotion of traditional Moroccan handicrafts though the training of artisans and craftsmen in the Meknes-Tafilalet region. Although the main campus is located in the city of Meknes, there are four satellite campuses spread around the region in Hajeb, Ifrane, Khenifra and Rashidiyya.\n\nThe Chamber provides training in many of the traditional Moroccan handicrafts including: woodcarving, carpet making, decorative iron working, leather tanning, pottery, and traditional garment making.\n"}
{"id": "31242278", "url": "https://en.wikipedia.org/wiki?curid=31242278", "title": "Chernobyl: Consequences of the Catastrophe for People and the Environment", "text": "Chernobyl: Consequences of the Catastrophe for People and the Environment\n\nChernobyl: Consequences of the Catastrophe for People and the Environment is a translation of a 2007 Russian publication by , Vassily B. Nesterenko, and Alexey V. Nesterenko, edited by Janette D. Sherman-Nevinger, and originally published by the New York Academy of Sciences in 2009 in their \"Annals of the New York Academy of Sciences\" series.\n\nThe book presents an analysis of scientific literature and concludes that medical records between 1986, the year of the Chernobyl disaster, and 2004 reflect 985,000 premature deaths as a result of the radioactivity released. The literature analysis draws on over 1,000 published titles and over 5,000 internet and printed publications, primarily in Slavic languages (i.e. not translated in English), discussing the consequences of the Chernobyl disaster. \n\nThe book was not peer reviewed by the New York Academy of Sciences. Five reviews were published in the academic press, with four of them considering the book severely flawed and contradictory, and one praising it while noting some shortcomings.\n\nThe primary author, the biologist Alexey V. Yablokov, was a member of the Russian Academy of Science. Consulting editor, Janette Sherman, MD, has researched the health effects of nuclear radiation and illnesses such as cancer and birth defects.\n\nThe primary author, the late biologist Alexey V. Yablokov, was a member of the Russian Academy of Science, and was deputy chair of the commission of ecology of the USSR' Parliament (1989-1991), councillor on ecology and public health to the President of the Russian Federation (1991-1993) and chair of the state commission on dumping of radioactive wastes in seas surrounding the Russian Federation (1992-1993). He is also a co-founder of Greenpeace Russia. From 1977 to 1987, the late Prof. Vassily B. Nesterenko was the director of the Institute of Nuclear Energy at the National Academy of Sciences of Belarus. The foreword of the book is authored by Dimitro M. Grodzinsky, chairman of the Ukrainian National Commission on Radiation Protection and chairman of the Department of General Biology at the Ukrainian National Academy of Sciences. Consulting editor, Janette Sherman, MD, has a background in medicine and toxicology, with special reference to the health effects of nuclear radiation and illnesses such as cancer and birth defects.\n\nThe book presents an analysis of scientific literature and concludes that medical records between 1986, the year of the Chernobyl disaster, and 2004 reflect 985,000 premature deaths as a result of the radioactivity released. The authors suggest that most of the deaths were in Russia, Belarus and Ukraine, though others occurred worldwide throughout the many countries that were struck by radioactive fallout from Chernobyl. The literature analysis draws on over 1,000 published titles and over 5,000 internet and printed publications, primarily in Slavic languages (i.e. not translated in English), discussing the consequences of the Chernobyl disaster. The authors contend that those publications and papers were written by leading Eastern European authorities and have largely been downplayed or ignored by the IAEA and UNSCEAR. The claim was made, notwithstanding the fact that 13 of the authors of the Chernobyl Forum were from Ukraine, Russia or Belarus.\n\nThe book was not peer reviewed by the New York Academy of Sciences. Five reviews were published in the academic press, with four of them considering the book severely flawed and contradictory, and one praising it while noting some shortcomings. \n\nTwo expert reviews of the book were commissioned by the Oxford journal \"Radiation Protection Dosimetry\". The first, by Dr. Ian Fairlie, generally endorses the book's conclusions. Dr. Fairlie, a radiation biologist, was a scientific secretary to UK Government’s Committee Examining Radiation Risks from Internal Emitters and one of two authors of the TORCH report commissioned by the European Green Party. He greets the book as a\n\n\"... welcome addition to the literature in English. The New York Academy of Sciences [is] to be congratulated for publishing this volume. [...] In the opinion of the reviewer, this volume makes it clear that international nuclear agencies and some national authorities remain in denial about the scale of the health disasters in their countries due to Chernobyl's fallout. This is shown by their reluctance to acknowledge contamination and health outcomes data, their ascribing observed morbidity/mortality increases to non-radiation causes, and their refusal to devote resources to rehabilitation and disaster management.\"\nFairlie notes two shortcomings of the book: that it does not sufficiently investigate the large decrease in average male life spans throughout Belarus, Russia and Ukraine, in both contaminated and uncontaminated areas; and that it does not make enough effort to reconstruct estimated doses of contamination and discuss their implications for eastern and western Europe (though Fairlie agrees with the authors that studies should not be rejected for failing to contain dose estimates—a criterion commonly applied by western nuclear agencies such as the IAEA).\n\nFairlie specifically concurs with Yablakov \"et al.\" on three points:\n\n\nThe second review (in the same volume), by Dr. Monty Charles, is largely critical, noting several problems:\nWhile Charles agrees with the importance of making eastern research more available in the west, he states that he cannot tell which of the publications referred to by the book would sustain critical peer-review in western scientific literature, and that verifying these sources would require considerable effort. Charles sees the book as representing one end of a spectrum of views, and believes that works from the entire spectrum must be critically evaluated in order to develop an informed opinion.\n\nA third review by Mona Dreicer was published in the journal Environmental Health Perspectives. It was highly critical of the book's methodology:\n\n\"... by discounting the widely accepted scientific method for associating cause and effect (while taking into account the uncertainties of dose assessment and measurement of impacts), the authors leave us with only with their assertion that the data in this volume \"document the true scale of the consequences of the Chernobyl catastrophe.\"\"\nThe New York Academy of Sciences published a fourth review, by M. I. Balonov of Institute of Radiation Hygiene, St. Petersburg, Russia. The reviewer condemned the book for completely discounting dosimetry and radiation dose reconstruction, relying instead on inferior, simplistic methodologies, such as ecological and geographical techniques and tracking health indicators over time, which are known to give erroneous conclusions. He also noted the inexplicable selection of publications for analysis, which included media reports, websites of public organizations and even unidentified persons. At the same time, a lot of respected, peer-reviewed work from Russian-language authors was ignored. Balonov's review concludes that the value of the report is negative, because it has very little scientific merit while being highly misleading to the lay reader. It also characterized the estimate of nearly a million deaths as more in the realm of science fiction than science.\n\nA fifth review, by Sergei V. Jargin, was published in the journal \"Radiation and Environmental Biophysics\" which described \"Consequences\" as overestimating the health impacts and containing \"poorly substantiated information\". A reply to this by Yablokov and A. Nesterenko was also published in the same issue.\n\n\n"}
{"id": "706590", "url": "https://en.wikipedia.org/wiki?curid=706590", "title": "Dalecarlian horse", "text": "Dalecarlian horse\n\nA Dalecarlian horse or Dala horse (; Swedish: Dalahäst) is a traditional carved, painted wooden statue of a horse originating in the Swedish province of Dalarna (Dalecarlia). In the old days the Dalecarlian horse was mostly used as a toy for children; in modern times it has become a symbol of Dalarna, as well as of Sweden in general. Several different types of Dalecarlian horses are made, with distinguishing features common to the locality of the site where they are produced. One particular style has, however, become much more common and widespread than others. It is stoutly carved and painted bright red with details and a harness in white, green, yellow and blue.\n\nIt was in the small log cabins deep in the forests during the long winter nights in front of a log fire that the forerunner of the Dala horse was born. Using simple tools, generally only a knife, woodcarvers made toys for their children. It was only natural that many of these toys were horses, because the horse was invaluable in those days, as a trusty friend and worker who could pull great loads of timber from the forests during the winter months, and in the summer could be of just as much use on the farm.\n\nThe art of carving and painting the small horses quickly flourished in the 19th century, as economic hardship in the region inspired greater production of the small horses, and they became an important item of barter. Horse-making may have started as something to do during the long dark winter months, but soon the Dala horses were traded in exchange for household goods and their carving and painting blossomed into a full-fledged cottage industry. The rural families depended on horse production to help keep food on the table, as the skills of horse carving and painting were passed on from generation to generation.\nThe carving of Dala horses as a livelihood is thought to have started in the village of Bergkarlås in central Sweden, though the nearby \"horse\" villages of Risa, Vattnäs, and Nusnäs were also centres of horse-making. The villages were involved in the art of furniture and clock-making, and it is likely the leftover scraps of wood were put to use in the production of Dala horses. Many early Dala horses were not painted at all, but in the beginning of the 19th century painting them in a single color, white or red, became common practice. The decoration of the Dala horse has its roots in furniture painting and was perfected over the years. According to a local tale, a wandering painter in the style of \"kurbits\" came across one of these Dala horses in a farm he was decorating. When asked by one of the children why that horse was not as beautifully painted as the ones in the decorations, he painted the Dala horse in the same style. This tradition was then carried on in order to raise the market value of the Dala horses.\n\nThe earliest references to wooden horses for sale are from 1623—nearly 400 years ago. In the 19th century, Stikå-Erik Hansson from the village Risa in the parish of Mora introduced the technique of painting with two colours on the same brush, still used today. In the book \"The Wooden Horses of Sweden,\" the author mentions that this famous Dala painter is buried in a small churchyard in Nebraska after having immigrated to the Midwest in 1887 at the age of 64.) He changed his name to Erik Erikson upon coming to America and is buried at Bega Cemetery in Stanton County Nebraska, outside of Norfolk.\n\nWhile there were many horse whittlers in the early production of Dala horses, there were comparatively few horse painters. The large number of whittlers and a lack of distinguishing features makes it difficult to distinguish between different whittlers. Early painters very rarely signed their work, but they did have their own distinct pattern from which it is often possible to identify who painted a particular horse. In the 1930s (especially after the World's Expo in Paris 1937 and World's Fair in New York 1939 in which Dala horses were shown) mass production of Dala horses started. This marks the beginning of a new era for the Dala horse, transitioning from toy to a national symbol and popular souvenir.\n\nThe Dalecarlian horse of today is still a handcrafted article, made of pine, and its pattern is about 150 years old. At least nine different people contribute their skills to create each horse. The distinctive shape of the horse is due to the usage of flat-plane style carving.\n\nAn apocryphal legend of the Dalecarlian horse is that they became the national toy in 1716. According to the legend, soldiers loyal to King Charles XII were quartered in the Dalecarlian region and carved the toys as gifts for their hosts.\n\nEarly production of Dala horses was concentrated to four villages: Bergkarlås, Risa, Vattnäs, and Nusnäs, all in the parish of Mora. Production is believed to have started in\nBergkarlås and later spread to nearby villages Risa and Vattnäs through kindred. At about the same time production started independently in Nusnäs, and being farther away their style was less influenced by those of the other villages. The individual painters each had their particular style, and the few who are old enough to remember first- or second-hand the history can often tell which village, and in some cases which carver or painter, turned out a particular horse. The distinguishing features of many early painters from these villages have now been documented.\n\nMany of the works by the earliest horse makers are no longer in existence but those that remain are cherished by their owners and have often been passed down through generations. These are also coveted by collectors, and their value has risen markedly over the years.\n\nToday, many of the villages in Dalarna county make Dala horses with individual styles representing the district of origin. These horses also have distinctive shapes and often come in different sizes. Some horses like the Nusnäs horse are stocky work horses; others are lean and upright with stately countenance like the Rättvik horse. Many of these can be seen at a yearly exhibition (since the year 2000), which is held at Klockargården in Leksand, Sweden. Currently about forty present carvers and painters show their Dala horses and put them up for sale.\n\nToday, Nusnäs is the centre of Dala horse production with the most famous being the Nils Olsson and Grannas Olsson workshops. Grannas A. Olssons Hemslöjd AB, founded in 1922, is the oldest company which still makes Dalecarlian horses. Nils Olssons Hemslöjd is almost as old. The horses made at these workshops share the same familiar pattern which most people associate to Dala horses. Apart from these, old-style horses are also hand-carved and painted in limited editions to replicate the style of the antique horses found in Swedish museums or held in private family collections. These are numbered and signed, so that they cannot be confused with an original early Dala horse.\n\nThe wood from which the figures are carved comes from the slow-growing pine forests around Lake Siljan. The wood is ideal for carpentry and carving. The trees to be made into horses are marked out while still standing in the forest. Only the best timber will be selected for carving into horses. The trees are felled and sawed into pieces of a suitable size for the blanks that will eventually be made into Dalecarlian horses.\n\nThe blanks are sawed and carved by hand. Because of this, no two horses are exactly alike. The horses are dipped in primer immediately after carving, to reveal any defects in the wood that may need to be fixed. After priming, any cavities in the wood are filled in to ensure extra smoothness. The horses are polished to give them a smooth, attractive finish.\n\nAfter sanding, the horses are dipped into paint of the appropriate colour. The traditional pattern is painted free-hand by practised \"ripple\" painters. The art of rippling requires great skill and takes many years to learn. Finally, \"all the fine horses\" are varnished and sent out from Nusnäs to serve as a symbol of Sweden in the outside world.\n\nDala horses were introduced to North America at the 1939 World's Fair in New York. The architects Sven Markelius and Anders Beckman chose the dala horse because they were searching for a powerful symbol for the Swedish pavilion at the fair. \n\nA similar dala horse can be seen in Andersonville, Chicago, donated by the Swedish American Women's Educational Association (SWEA) to the Swedish American Museum. SWAE commissioned the Swedish Artist Lars Gillis to paint the dala horse in a way that represents the connection between Sweden and Chicago. After eight years, the original dala horse could not withstand the harsh Chicago weather conditions and was removed in 2013 for restoration that took a year. It is now kept in the lobby of the Swedish American Museum, which provided a fiberglass replica placed on the corner of Clark and Farragut streets.\n\n"}
{"id": "19055623", "url": "https://en.wikipedia.org/wiki?curid=19055623", "title": "Deborah Tabart", "text": "Deborah Tabart\n\nDeborah Tabart, OAM is the CEO of the Australian Koala Foundation. She has worked with this organisation since 1988 and is known internationally as the Koala Woman.\n\nIn 1988, Deborah Tabart was asked by the Australian Koala Foundation Board of Directors to “raise funds to save the koala”. She quickly realised that the world of koala conservation was much more complicated than that. She faced a steep learning curve in the worlds of science, politics, development and conservation.\n\nUnder her guidance, the Australian Koala Foundation has grown from a fledgling group of koala advocates into an award-winning, international scientific organisation that has funded millions of dollars worth of research and conservation.\n\nIn January 2008, Tabart was awarded the Medal of the Order of Australia (OAM) in acknowledgement of her contributions to the protection and management koalas and their habitat, and her service to Australia and humanity.\n\nShe has led the Australian Koala Foundation in its key achievements of the Koala Habitat Atlas and Koala Beach, and is currently formulating a National Koala Act in the hope of legislating protection for koala habitat.\n\nDeborah Tabart now resides in Queensland, Australia. Besides her day job at the AKF, she also practices permaculture and grows her own vegetables.\n\n"}
{"id": "29871228", "url": "https://en.wikipedia.org/wiki?curid=29871228", "title": "Ecopower (cooperative)", "text": "Ecopower (cooperative)\n\nEcopower is a Belgian cooperative founded in 1991, financing renewable energy projects in Flanders. Since the liberalisation of the energy market in Flanders (July 2003) Ecopower became a supplier of green electricity produced in Belgium. At the end of 2010 1% of the Flemish households were supplied by Ecopower. Projects in recent years include the placement of wind turbines in Eeklo, Gistel and Ghent, and the placement of hydroelectric turbines in Rotselaar, Hoegaarden and Overijse.\n\nEcopower is member of the Belgium group of renewable cooperatives REScoop.be and the European group of renewable cooperatives REScoop.eu.\n"}
{"id": "9598", "url": "https://en.wikipedia.org/wiki?curid=9598", "title": "Electronvolt", "text": "Electronvolt\n\nIn physics, the electronvolt (symbol eV, also written electron-volt and electron volt) is a unit of energy equal to approximately joules (symbol J) in SI units. \n\nHistorically, the electronvolt was devised as a standard unit of measure through its usefulness in electrostatic particle accelerator sciences, because a particle with charge \"q\" has an energy after passing through the potential \"V\"; if \"q\" is quoted in integer units of the elementary charge and the terminal bias in volts, one gets an energy in eV.\n\nLike the elementary charge on which it is based, it is not an independent quantity but is equal to . It is a common unit of energy within physics, widely used in solid state, atomic, nuclear, and particle physics. It is commonly used with the metric prefixes milli-, kilo-, mega-, giga-, tera-, peta- or exa- (meV, keV, MeV, GeV, TeV, PeV and EeV respectively). In some older documents, and in the name Bevatron, the symbol BeV is used, which stands for billion (10) electronvolts; it is equivalent to the GeV.\n\nAn electronvolt is the amount of energy gained (or lost) by the charge of a single electron moving across an electric potential difference of one volt. Hence, it has a value of one volt, , multiplied by the electron's elementary charge \"e\", Therefore, one electronvolt is equal to The electronvolt is not an SI unit, and its definition is empirical (unlike the litre, the light-year and other such non-SI units), where its value in SI units must be obtained experimentally.\n\nBy mass–energy equivalence, the electronvolt is also a unit of mass. It is common in particle physics, where units of mass and energy are often interchanged, to express mass in units of eV/\"c\", where \"c\" is the speed of light in vacuum (from ). It is common to simply express mass in terms of \"eV\" as a unit of mass, effectively using a system of natural units with \"c\" set to 1. The mass equivalent of is\n\nFor example, an electron and a positron, each with a mass of , can annihilate to yield of energy. The proton has a mass of . In general, the masses of all hadrons are of the order of , which makes the GeV (gigaelectronvolt) a convenient unit of mass for particle physics:\n\nThe unified atomic mass unit (u), 1 gram divided by Avogadro's number, is almost the mass of a hydrogen atom, which is mostly the mass of the proton. To convert to megaelectronvolts, use the formula:\n\nIn high-energy physics, the electronvolt is often used as a unit of momentum. A potential difference of 1 volt causes an electron to gain an amount of energy (i.e., ). This gives rise to usage of eV (and keV, MeV, GeV or TeV) as units of momentum, for the energy supplied results in acceleration of the particle.\n\nThe dimensions of momentum units are . The dimensions of energy units are . Then, dividing the units of energy (such as eV) by a fundamental constant that has units of velocity (), facilitates the required conversion of using energy units to describe momentum. In the field of high-energy particle physics, the fundamental velocity unit is the speed of light in vacuum \"c\". \n\nBy dividing energy in eV by the speed of light, one can describe the momentum of an electron in units of eV/\"c\".\nThe fundamental velocity constant \"c\" is often \"dropped\" from the units of momentum by way of defining units of length such that the value of \"c\" is unity. For example, if the momentum \"p\" of an electron is said to be , then the conversion to MKS can be achieved by:\n\nIn particle physics, a system of \"natural units\" in which the speed of light in vacuum \"c\" and the reduced Planck constant \"ħ\" are dimensionless and equal to unity is widely used: . In these units, both distances and times are expressed in inverse energy units (while energy and mass are expressed in the same units, see mass–energy equivalence). In particular, particle scattering lengths are often presented in units of inverse particle masses.\n\nOutside this system of units, the conversion factors between electronvolt, second, and nanometer are the following:\n\nThe above relations also allow expressing the mean lifetime \"τ\" of an unstable particle (in seconds) in terms of its decay width \"Γ\" (in eV) via . For example, the B meson has a lifetime of 1.530(9) picoseconds, mean decay length is , or a decay width of .\n\nConversely, the tiny meson mass differences responsible for meson oscillations are often expressed in the more convenient inverse picoseconds.\n\nEnergy in electronvolts is sometimes expressed through the wavelength of light with photons of the same energy: 1 eV = 8065.544005(49) cm. \n\nIn certain fields, such as plasma physics, it is convenient to use the electronvolt as a unit of temperature. The conversion to the Kelvin scale is defined by using \"k\", the Boltzmann constant:\n\nFor example, a typical magnetic confinement fusion plasma is , or 170 MK.\n\nAs an approximation: \"k\"\"T\" is about (≈ ) at a temperature of .\n\nThe energy \"E\", frequency \"v\", and wavelength λ of a photon are related by\n\nwhere \"h\" is the Planck constant, \"c\" is the speed of light. This reduces to\n\nA photon with a wavelength of (green light) would have an energy of approximately . Similarly, would correspond to an infrared photon of wavelength or frequency .\n\nIn a low-energy nuclear scattering experiment, it is conventional to refer to the nuclear recoil energy in units of eVr, keVr, etc. This distinguishes the nuclear recoil energy from the \"electron equivalent\" recoil energy (eVee, keVee, etc.) measured by scintillation light. For example, the yield of a phototube is measured in phe/keVee (photoelectrons per keV electron-equivalent energy). The relationship between eV, eVr, and eVee depends on the medium the scattering takes place in, and must be established empirically for each material.\n\n\nOne mole of particles given 1 eV of energy has approximately 96.5 kJ of energy – this corresponds to the Faraday constant (\"F\" ≈ ) where the energy in joules of \"N\" moles of particles each with energy \"X\" eV is \"X\"·\"F\"·\"N\".\n\n\n"}
{"id": "2478755", "url": "https://en.wikipedia.org/wiki?curid=2478755", "title": "FLiNaK", "text": "FLiNaK\n\nFLiNaK is the name of the ternary eutectic alkaline metal fluoride salt mixture LiF-NaF-KF (46.5-11.5-42 mol %). It has a melting point of 454 °C and a boiling point of 1570 °C. It is used as electrolyte for the electroplating of refractory metals and compounds like titanium, tantalum, hafnium, zirconium and their borides. FLiNaK also could see potential use as a coolant in the very high temperature reactor, a type of nuclear reactor.\n\nFLiNaK salt was researched heavily during the late 1950s by Oak Ridge National Laboratory as potential candidate for a coolant in the molten salt reactor because of its low melting point, its high heat capacity, and its chemical stability at high temperatures. Ultimately, its sister salt, FLiBe, was chosen as the solvent salt for the molten salt reactor due to a more desirable nuclear cross section. FLiNaK still gathers interest as an intermediate coolant for a high-temperature molten salt reactor where it could transfer heat without being in the presence of the fuel.\n\nFluoride salts, like all salts, cause corrosion in most metals and alloys. FliNak is different from FLiBe in the sense that is a basic melt—or it has an excess of fluorine ions. As FLiNak melts, all three components are alkali fluorides and therefore disassociate into positive and negative ions. The concentration of molten fluorine ions are able to corrode any metallic structures if it is energetically favorable. This is in contrast to FLiBe, which in a 66-34 mol% mixture will be a chemically neutral mix, as fluorine ions from LiF are donated to BeF to create the tetrafluoroberyllate ion BeF.\n\n"}
{"id": "17956189", "url": "https://en.wikipedia.org/wiki?curid=17956189", "title": "Foreign Reports", "text": "Foreign Reports\n\nForeign Reports Inc. is a Washington, D.C.-based consulting firm for the oil industry, founded in 1956. Foreign Reports advises energy companies, governments, and financial institutions on world energy issues, with a specialization on the Middle East. The President of the firm is Nathaniel Kern.\n\nForeign Reports Inc. has been in this business for more than 50 years and counts among its subscribers many of the world’s largest oil companies—both international and national—as well as many other financial institutions. It reports on political developments that are highly relevant to oil markets, crude oil price formation, and related macroeconomic variables.\n\nIn providing political intelligence and analysis of world oil markets, Foreign Reports uses three simple tools:\n\nForeign Reports Inc. was founded in 1956 by Harry Kern, who had previously been Foreign Editor of Newsweek, in which capacity he traveled extensively throughout the world, but especially in the Far and Middle East.\n\nNewsweek and Time Magazine during that period were practically the sole elements of the U.S. news media reporting on world activities in a timely fashion. As Foreign Editor, Harry Kern also was Editor-in-Chief of the magazine’s International Edition and thus had the privilege of picking who or what would adorn the cover of those editions. Since various foreign leaders, or aspiring ones, angled to get their pictures on the front of Newsweek, Kern was a popular visitor in many foreign capitals. In the process, he managed to befriend both current and future leaders and to gain insights into how their policies were developed.\n\nForeign Reports grew out of these unique circumstances, as Kern saw a need among growing multinational companies with sizable stakes around the world for a level of international political reporting that surpassed what was then being carried in the daily newspapers of the period. From Newsweek, he brought with him to Foreign Reports two bureau chiefs, one in Beirut and one in Tokyo. From these \"bureaus\" of Foreign Reports came a steady stream of insightful reporting on the regions they covered. Among its initial major subscribers were the world’s major oil companies, but also other industrial and banking concerns.\n\nIn the year of its founding, Foreign Reports benefited from one of the first “oil crises” that have afflicted the Middle East over the years—the 1956 Suez Crisis, with its concomitant closure of the Suez Canal, which was a great boon to notable tanker owners of the time, who were avid clients of Foreign Reports.\n\nSince that time, Foreign Reports has closely covered for its subscribers all the major and minor crises that have bedeviled world oil markets ever since, as well as the broad geopolitical trends that have affected markets and business conditions. The methods it uses to anticipate the unanticipated are relatively straightforward and avoid being unduly alarmist. They are methods that have been refined over time.\n\nMost every crisis begins with a series of rumbles, and the rumbles have to be distinguished from mere bluster and bombast. Knowing who the players are, how they think, what they confide in others, their history of risk-taking and their own domestic political requirements is essential. As any potential crisis builds, often over a period of months, Foreign Reports writes up a contemporaneous narrative, covering the story as it develops, often focusing on key details, which, only later, historians pick up on and piece together.\n\nThe Middle East, with its vast reserves of petroleum, was an obvious early focus of Foreign Reports, especially as the firm’s subscribers had substantial equity interests in oil concessions in that volatile part of the world, where Kern remained a frequent visitor to many of the key players—the Shah of Iran, Gamal Abdul Nasser of revolutionary Egypt, Crown Prince Faisal of Saudi Arabia, etc. Kern also maintained close relationships with the leading foreign policy actors in the Eisenhower Administration, notably Secretary of State John Foster Dulles and his brother, CIA Director Allen Dulles, forging a long relationship with U.S. intelligence, both in Washington and in the Agency’s foreign “stations.”\n\nNathaniel Kern (also Nat Kern) joined his father at Foreign Reports in 1972 after graduating from Princeton University and attending the University of Riyadh from 1970-71 as the first non-Arab student. By the time he graduated and joined the firm, rumblings of the first full-scale “energy crisis” had begun and the role of Saudi Arabia on the world scene began to be transformed.\n\nWithin two years of Nat's joining the firm, the world of oil and the Middle East had changed dramatically, with prices skyrocketing and the volumes of crude oil being produced in Saudi Arabia growing steadily. The firm’s business branched out from providing political reporting on oil in the Middle East into also providing business development assistance to firms wishing to break into new markets in the Middle East, primarily, though not exclusively, in Saudi Arabia. The main areas the firm concentrated in were competitive bidding opportunities in the power and desalination markets. This required an understanding of the technologies, engineering and procurement issues inherent in complex projects, and Foreign Reports brought on board the necessary skilled individuals in these areas.\n\nNat Kern was a frequent visitor to Iraq during the 1980-1988 Iran-Iraq war, at a time when U.S.-Iraqi relations were improving, and was tasked by the U.S. government with maintaining ties with certain key Iraqi officials from 1991 onwards, at a time when the U.S. government maintained a policy of shunning any official contact with the Iraqi government.\n\nBy the early 1980s, the nature of the world oil business began to change in a number of different ways, all of which affected how Foreign Reports would be able to continue to provide services to its client base. The major international oil companies were gradually losing their equity ownership of Middle East oil production and many needed to forge different kinds of relationships with producing governments. In addition, a new class of players in the oil market was gradually emerging as interest and liquidity grew in the futures market. World oil prices had been practically a secret in the early days of Foreign Reports and had been remarkably stable in general during the firm’s first 16 years, but it would be another ten years before price volatility would become a major reason for the firm to develop another service for its clients.\n\nOPEC did not institute its first quotas until 1982, just as crude oil prices were beginning to come under downward pressure in the market. When prices did eventually start to crash in late November 1985, no other reporting service in the industry had so closely chronicled how that crash would materialize as Foreign Reports had done. The firm had watched intensely how then Saudi Petroleum Minister Ahmed Zaki Yamani had wrestled over new ways to price Saudi Arabia’s oil as he cruised the Mediterranean on his yacht during August 1985. Foreign Reports was the first to report that Yamani, just before that Labor Day, had got off his yacht and signed “net-back pricing deals” with his main international customers. These deals that would cut all previous supports for crude oil prices and lead prices from the high $20s to the single digits within nine months. Incredibly, in those early days of the NYMEX, futures prices did not start to decline until the day after Thanksgiving.\n\nAs the pace and sophistication of NYMEX trading has accelerated greatly since those days, and as access to the incredible amounts of information over the internet has exploded, the services that Foreign Reports has offered have also changed, while still staying with time-tested methods: follow the narrative; know the actors; know their characters; understand the rules; understand cultures and histories; pay ever increasing attention to separating the wheat from the chaff in an information-laden age; and communicate concisely and clearly.\n\nForeign Reports continues to report on political developments that are highly relevant to oil markets, crude oil price formation, and related macroeconomic variables. It closely monitors and reports on the political and economic situations in places such as: Iraq, Iran, Saudi Arabia, Nigeria, and Venezuela. The firm also reports on OPEC politics and examines what oil production decisions might be looming in the near future. Executive and legislative activities in the U.S. which affect world oil markets are also often reported on.\n\n\n\n"}
{"id": "15368022", "url": "https://en.wikipedia.org/wiki?curid=15368022", "title": "Gary Yohe", "text": "Gary Yohe\n\nGary Wynn Yohe is the Huffington Foundation Professor of Economics and Environmental Studies at Wesleyan University, Middletown, Connecticut. He holds a PhD from Yale University.\n\nYohe specializes in Microeconomic theory, Natural Resources, and Environmental Economics. He is a researcher on the economics of climate change and integrated assessment modelling. Among other works, he is an editor of the book \"Avoiding Dangerous Climate Change\" and co-author (with Edwin Mansfield) of \"Microeconomics| Microeconomics: Theory and Applications\". He is a senior member of the United Nations Intergovernmental Panel on Climate Change (IPCC) that was awarded a share of the 2007 Nobel Peace Prize with Al Gore. He has been involved with the IPCC since the mid-1990s, has served, among other capacities, as a Lead Author for four different chapters in the IPCC Third Assessment Report, and as Convening Lead Author for the last chapter of the contribution of Working Group II to the IPCC Fourth Assessment Report. Yohe also worked with the Core Writing Team to prepare the overall Synthesis Report for the entire Assessment.\n\nYohe is also a member of the New York City Panel on Climate Change and the standing Committee on the Human Dimensions of Global Change of the National Academy of Sciences. He is also a standing member of the National Academy of Sciences' Committee on the Human Dimensions of Global Change. He was a vice-chair of the Third National Climate Assessment.\n\nHe is one of the four co-signers of an open letter, dated March 12, 2010, regarding possible errors in the IPCC Fourth Assessment Report and regularly advises the US government.\n\n"}
{"id": "46798384", "url": "https://en.wikipedia.org/wiki?curid=46798384", "title": "Geysir Green Energy", "text": "Geysir Green Energy\n\nGeysir Green Energy is an Icelandic energy company. It is part-owned by Atorka, Íslandsbanki, VGK-Invest and Reykjanesbæjar and specializes in projects related to geothermal energy production and maximising potential in low-temperature areas. Geysir Green Energy was established on January 5, 2007, and on April 30, 2007 the company purchased a 15.2% stake in Hitaveita Suðurnesja for 7.6 billion króna. Among the major projects of the company is ENEX, a subsidiary which develops power plants that use geothermal energy. Enex-China is a joint project between Enex and the Chinese Shaanxi Geothermal Energy Development Corporation. The company also owns stake in Ram Power Inc., which is involved in the development of geothermal energy in the United States.\n"}
{"id": "39435562", "url": "https://en.wikipedia.org/wiki?curid=39435562", "title": "Grid balancing", "text": "Grid balancing\n\nGrid balancing has become an important aspect for the power grid in matching the supply of energy to demand. In more recent years this has become less predictable with more renewable energy being installed into the grid.\n\nThis has resulted in wind farms being turned off at night time, when it is windy, but there is no demand. In Scotland this has resulted in payouts, most recently over £6m in 33 days has been paid by the grid to wind farms to not generate electricity.\n\nConstraint payments are made to other electricity suppliers as well as wind. In 2011/2012, payments by the National Grid in the UK totaled £324 million of which £31 million went to wind. In 2012/2013, thanks to improved transmission capability, they were £130 million of which only £7 million were for wind.\n\nThis temporarily excess electric energy could alternatively be used in electrolysis of water to make high purity hydrogen fuel used in fuel cells. In areas with little hydroelectricity, pumped storage systems such as the Dinorwig Power Station can allow the energy to be used for operational reserve or at times of peak demand rather than run a natural gas peaking power plant.\n\n"}
{"id": "30890402", "url": "https://en.wikipedia.org/wiki?curid=30890402", "title": "Hong Kong Bird Watching Society", "text": "Hong Kong Bird Watching Society\n\nThe Hong Kong Bird Watching Society (HKBWS) is an environmental non-governmental organization dedicated to the conservation of birds and their habitats in Hong Kong, a territory on the southern coast of China. It is a BirdLife International affiliated organisation. The emblem of the HKBWS is the Chinese egret which visits Hong Kong on migration and used to breed in the territory.\n\nThe HKBWS was formed in 1957. It publishes the annual Hong Kong Bird Report as well as regular bulletins. As of 2013 it has over 1800 members, employs four full-time staff, holds regular meetings, conducts surveys and organises birdwatching tours. It is involved in the conservation management of the Long Valley agricultural wetland where it monitors the birds. Its research programs include monitoring waterfowl at the Mai Po Marshes and Deep Bay, reviewing Hong Kong bird records and maintaining a checklist, studying the wintering ecology of black-faced spoonbills and conducting breeding bird surveys at the Tai Po Kau Forest Reserve.\n\n"}
{"id": "1048332", "url": "https://en.wikipedia.org/wiki?curid=1048332", "title": "Hotspot (geology)", "text": "Hotspot (geology)\n\nIn geology, the places known as hotspots or hot spots are volcanic regions thought to be fed by underlying mantle that is anomalously hot compared with the surrounding mantle. Their position on the Earth's surface is independent of tectonic plate boundaries. There are two hypotheses that attempt to explain their origins. One suggests that hotspots are due to mantle plumes that rise as thermal diapirs from the core–mantle boundary. The other hypothesis is that lithospheric extension permits the passive rising of melt from shallow depths. This hypothesis considers the term \"hotspot\" to be a misnomer, asserting that the mantle source beneath them is, in fact, not anomalously hot at all. Well-known examples include the Hawaii, Iceland and Yellowstone hotspots.\n\nThe origins of the concept of hotspots lie in the work of J. Tuzo Wilson, who postulated in 1963 that the formation of the Hawaiian Islands resulted from the slow movement of a tectonic plate across a hot region beneath the surface. It was later postulated that hotspots are fed by narrow streams of hot mantle rising from the Earth's core–mantle boundary in a structure called a mantle plume. Whether or not such mantle plumes exist is the subject of a major controversy in Earth science. Estimates for the number of hotspots postulated to be fed by mantle plumes have ranged from about 20 to several thousands, over the years, with most geologists considering a few tens to exist. Hawaii, Réunion, Yellowstone, Galápagos, and Iceland are some of the most active volcanic regions to which the hypothesis is applied.\n\nMost hotspot volcanoes are basaltic (e.g., Hawaii, Tahiti). As a result, they are less explosive than subduction zone volcanoes, in which water is trapped under the overriding plate. Where hotspots occur in continental regions, basaltic magma rises through the continental crust, which melts to form rhyolites. These rhyolites can form violent eruptions. For example, the Yellowstone Caldera was formed by some of the most powerful volcanic explosions in geologic history. However, when the rhyolite is completely erupted, it may be followed by eruptions of basaltic magma rising through the same lithospheric fissures (cracks in the lithosphere). An example of this activity is the Ilgachuz Range in British Columbia, which was created by an early complex series of trachyte and rhyolite eruptions, and late extrusion of a sequence of basaltic lava flows.\n\nThe hotspot hypothesis is now closely linked to the mantle plume hypothesis.\n\nHotspot volcanoes are considered to have a fundamentally different origin from island arc volcanoes. The latter form over subduction zones, at converging plate boundaries. When one oceanic plate meets another, the denser plate is forced downward into a deep ocean trench. This plate, as it is subducted, releases water into the base of the over-riding plate, and this water mixes with the rock, thus changing its composition causing some rock to melt and rise. It is this that fuels a chain of volcanoes, such as the Aleutian Islands, near Alaska.\n\nThe joint mantle plume/hotspot hypothesis envisages the feeder structures to be fixed relative to one another, with the continents and seafloor drifting overhead. The hypothesis thus predicts that time-progressive chains of volcanoes are developed on the surface. Examples are Yellowstone, which lies at the end of a chain of extinct calderas, which become progressively older to the west. Another example is the Hawaiian archipelago, where islands become progressively older and more deeply eroded to the northwest.\n\nGeologists have tried to use hotspot volcanic chains to track the movement of the Earth's tectonic plates. This effort has been vexed by the lack of very long chains, by the fact that many are not time-progressive (e.g. the Galápagos) and by the fact that hotspots do not appear to be fixed relative to one another (e.g. Hawaii and Iceland.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "54965789", "url": "https://en.wikipedia.org/wiki?curid=54965789", "title": "IEC 63110", "text": "IEC 63110\n\nIEC 63110 is an international standard defining a protocol for the management of electric vehicles charging and discharging infrastructures, which is currently under development. IEC 63110 is one of the International Electrotechnical Commission's group of standards for electric road vehicles and electric industrial trucks, and is the responsibility of Joint Working Group 11 (JWG11) of IEC Technical Committee 69 (TC69).\n\nIEC 63110 consists of the following parts, detailed in separate IEC 63110 standard documents:\n"}
{"id": "40414239", "url": "https://en.wikipedia.org/wiki?curid=40414239", "title": "Janggunite", "text": "Janggunite\n\nJanggunite is a rare manganese oxide mineral with formula Mn(Mn,Fe)O(OH).\n\nIt was first described in 1975 for an occurrence in the Janggun mine, Bonghwa-gun, Gyeongsangbuk-do, South Korea and named for the locality.\n"}
{"id": "3779296", "url": "https://en.wikipedia.org/wiki?curid=3779296", "title": "Knickerbocker storm", "text": "Knickerbocker storm\n\nThe Knickerbocker storm was a blizzard that occurred on January 27–28, 1922 in the upper South and middle Atlantic United States. The storm took its name from the resulting collapse of the Knickerbocker Theatre in Washington, D.C. shortly after 9 p.m. on January 28 which killed 98 people and injured 133.\n\nAn Arctic airmass was in place across the northeast United States; Washington, D.C. had been below freezing since the afternoon of January 23. The front that spawned the cyclone was almost completely dry until after it crossed the Gulf of Mexico, making this storm unique among large southeast snowstorms. Despite the slow start, a low formed and deepened rapidly off the Georgia coast as the cold front reached the Gulf Stream on January 27. Heavy snow quickly developed from the Carolinas to Pennsylvania as the low drifted north to the outer Banks of North Carolina on the 28th. A strong high pressure to the north helped to cut the system off from the jet stream. As a result, the cyclone took three days to move up the Eastern Seaboard, which was double the normal time used by forecasters of that era to move storm systems up the coast. Snow reached Philadelphia and Washington, D.C. by noon on January 28, and continued into the morning of January 29. Temperatures remained in the 20s Fahrenheit (-5 °C) through much of the event.\n\nAn estimated 22,400 square miles (58,000 km²) of the northeast United States were affected by 20 inches (51 cm) of snow from this cyclone, which was over one-fifth of the total area that received over 4 in (10 cm) of snow. Snowfall was quite heavy in Maryland and Virginia. Richmond, Virginia recorded . Baltimore, Maryland was paralyzed as it received the most snowfall within 24 hours since 1872. Parts of North Carolina also received over one foot of snow.\n\nThe measured snow depth at the main observing site in Washington, D.C. reached 28 in (71 cm) while an observer in Rock Creek Park a few miles to the north measured 33 in (84 cm) with 3.02 in (76.7 mm) of liquid equivalent. Railroad lines between Philadelphia, Pennsylvania and Washington, D.C. were covered by at least 36 in (91 cm) of snow, with drifts as high as 16 ft (4.9 m). This snowstorm is the biggest in the history of Washington, D.C. since official record keeping began in 1885 (although it is dwarfed by the of snow in the Washington–Jefferson Storm of January 1772). Among other disruptions, Congress adjourned as a result of the storm.\n\nThe Knickerbocker Theatre was the largest and newest movie house in Washington, D.C., built in 1917 and owned by Harry M. Crandall. The roof was flat, which allowed the snow which had recently fallen to remain on the roof. During the movie's (\"Get-Rich-Quick Wallingford\") intermission, the weight of the heavy, wet snow became too much for the roof to bear. The roof split down the middle, bringing down the balcony seating as well as a portion of the brick wall. Dozens were buried. The disaster ranks as one of the worst in Washington, D.C. history. Congressman Andrew Jackson Barchfeld was among those killed in the theater. The theater's architect, Reginald Wyckliffe Geare, and owner, Harry M. Crandall, later committed suicide; Geare in 1927 and Crandall in 1937.\n\n\n\n"}
{"id": "9020273", "url": "https://en.wikipedia.org/wiki?curid=9020273", "title": "Kula (unit)", "text": "Kula (unit)\n\nA kula is an obsolete unit of measurement. \n\n\n"}
{"id": "1836020", "url": "https://en.wikipedia.org/wiki?curid=1836020", "title": "Laser ablation", "text": "Laser ablation\n\nLaser ablation or photoablation is the process of removing material from a solid (or occasionally liquid) surface by irradiating it with a laser beam. At low laser flux, the material is heated by the absorbed laser energy and evaporates or sublimates. At high laser flux, the material is typically converted to a plasma. Usually, laser ablation refers to removing material with a pulsed laser, but it is possible to ablate material with a continuous wave laser beam if the laser intensity is high enough. Excimer lasers of deep ultra-violet light are mainly used in photoablation; the wavelength of laser used in photoablation is approximately 200 nm.\n\nThe depth over which the laser energy is absorbed, and thus the amount of material removed by a single laser pulse, depends on the material's optical properties and the laser wavelength and pulse length. The total mass ablated from the target per laser pulse is usually referred to as ablation rate. Such features of laser radiation as laser beam scanning velocity and the covering of scanning lines can significantly influence the ablation process.\n\nLaser pulses can vary over a very wide range of duration (milliseconds to femtoseconds) and fluxes, and can be precisely controlled. This makes laser ablation very valuable for both research and industrial applications.\n\nThe simplest application of laser ablation is to remove material from a solid surface in a controlled fashion. Laser machining and particularly laser drilling are examples; pulsed lasers can drill extremely small, deep holes through very hard materials. Very short laser pulses remove material so quickly that the surrounding material absorbs very little heat, so laser drilling can be done on delicate or heat-sensitive materials, including tooth enamel (laser dentistry). Several workers have employed laser ablation and gas condensation to produce nano particles of metal, metal oxides and metal carbides.\n\nAlso, laser energy can be selectively absorbed by coatings, particularly on metal, so CO or pulsed lasers can be used to clean surfaces, remove paint or coating, or prepare surfaces for painting without damaging the underlying surface. High power lasers clean a large spot with a single pulse. Lower power lasers use many small pulses which may be scanned across an area.\n\nOne of the advantages is that no solvents are used, therefore it is environmentally friendly and operators are not exposed to chemicals (assuming nothing harmful is vaporized). It is relatively easy to automate. The running costs are lower than dry media or dry-ice blasting, although the capital investment costs are much higher. The process is gentler than abrasive techniques, e.g. carbon fibres within a composite material are not damaged. Heating of the target is minimal.\n\nAnother class of applications uses laser ablation to process the material removed into new forms either not possible or difficult to produce by other means. A recent example is the production of carbon nanotubes.\n\nLaser ablation is also used in removing rust from iron objects easily.\n\nIn March 1995 Guo et al. were the first to report the use of a laser to ablate a block of pure graphite, and later graphite mixed with catalytic metal. The catalytic metal can consist of elements such as cobalt, niobium, platinum, nickel, copper, or a binary combination thereof. The composite block is formed by making a paste of graphite powder, carbon cement, and the metal. The paste is next placed in a cylindrical mold and baked for several hours. After solidification, the graphite block is placed inside an oven with a laser pointed at it, and argon gas is pumped along the direction of the laser point. The oven temperature is approximately 1200 °C. As the laser ablates the target, carbon nanotubes form and are carried by the gas flow onto a cool copper collector. Like carbon nanotubes formed using the electric-arc discharge technique, carbon nanotube fibers are deposited in a haphazard and tangled fashion. Single-walled nanotubes are formed from the block of graphite and metal catalyst particles, whereas multi-walled nanotubes form from the pure graphite starting material.\n\nA variation of this type of application is to use laser ablation to create coatings by ablating the coating material from a source and letting it deposit on the surface to be coated; this is a special type of physical vapor deposition called pulsed laser deposition (PLD), and can create coatings from materials that cannot readily be evaporated any other way. This process is used to manufacture some types of high temperature superconductor and laser crystals.\n\nRemote laser spectroscopy uses laser ablation to create a plasma from the surface material; the composition of the surface can be determined by analyzing the wavelengths of light emitted by the plasma.\n\nLaser ablation is also used to create pattern, removing selectively coating from dichroic filter. This products are used in stage lighting for high dimensional projections, or for calibration of machine vision's instruments.\n\nFinally, laser ablation can be used to transfer momentum to a surface, since the ablated material applies a pulse of high pressure to the surface underneath it as it expands. The effect is similar to hitting the surface with a hammer. This process is used in industry to work-harden metal surfaces, and is one damage mechanism for a laser weapon. It is also the basis of pulsed laser propulsion for spacecraft.\n\nThe laser ablation of electronic semiconductors and microprocessors is now being pioneered in the UK to keep electronic manufacturers' designs confidential. The main reason is that it greatly reduces the risk of copying infringements.\n\nProcesses are currently being developed to use laser ablation in the removal of thermal barrier coating on high-pressure gas turbine components. Due to the low heat input, TBC removal can be completed with minimal damage to the underlying metallic coatings and parent material.\n\nLaser ablation is used as a sampling method for inductively coupled plasma mass spectrometry with forensic and biological applications.\n\nLaser ablation is used in science to destroy nerves and other tissues to study their function. For example, a species of pond snail, \"Helisoma trivolvis\", can have their sensory neurons laser ablated off when the snail is still an embryo to prevent use of those nerves.\n\nAnother example is the trochophore larva of \"Platynereis dumerilii\", where the larval eye was ablated and the larvae was not phototactic, anymore. However phototaxis in the nectochaete larva of \"Platynereis dumerilii\" is not mediated by the larval eyes, because the larva is still phototactic, even if the larval eyes are ablated. But if the adult eyes are ablated, then the nectochaete is not phototactic anymore and thus phototaxis in the nectochaete larva is mediated by the adult eyes.\n\nLaser ablation can also be used to destroy individual cells during embryogenesis of an organism, like \"Platynereis dumerilii\", to study the effect of missing cells during development.\n\nThere are several laser types used in medicine for ablation, including argon, carbon dioxide (CO), dye, , excimer, , and others. Laser ablation is used in a variety of medical specialties including ophthalmology, general surgery, neurosurgery, ENT, dentistry, oral and maxillofacial surgery, and veterinary. Laser scalpels are used for ablation in both hard- and soft-tissue surgeries. Some of the most common procedures where laser ablation is used include LASIK, skin resurfacing, cavity preparation, biopsies, and tumor and lesion removal. In soft-tissue surgeries, the CO laser beam ablates and cauterizes simultaneously, making it the most practical and most common soft-tissue laser.\n\nLaser ablation can be used on benign and malignant lesions in various organs, which is called laser-induced interstitial thermotherapy. The main applications currently involve the reduction of benign thyroid nodules and destruction of primary and secondary malignant liver lesions.\n\nLaser ablation is also used to treat chronic venous insufficiency.\n\n\n"}
{"id": "337010", "url": "https://en.wikipedia.org/wiki?curid=337010", "title": "Lifemapper", "text": "Lifemapper\n\nLifemapper is building a species diversity map of the world. It is similar to the SETI@Home client, in that it uses a distributed computing client running primarily on home user's computers to correlate georeferenced biological samples with environmental models of the Earth. It is an experimental GIS, or Geographic Information System, that uses a special genetic algorithm to see if predicted rules about where a species lives match up with the species' observed natural settings. It is hoped that this technique will be able to both represent a current \"map\" of all organisms habitats on Earth as well as predict where organisms may possibly thrive or face extinction due to climate change and other ecological transformations.\n\n\n"}
{"id": "31235916", "url": "https://en.wikipedia.org/wiki?curid=31235916", "title": "Lingen Nuclear Power Plant", "text": "Lingen Nuclear Power Plant\n\nLingen Nuclear Power Plant is an inactive nuclear power plant in Germany, close to Emsland Nuclear Power Plant.\n\nIt once belonged to VEW, and now belongs to RWE Power AG.\n"}
{"id": "20051", "url": "https://en.wikipedia.org/wiki?curid=20051", "title": "Mach number", "text": "Mach number\n\nIn fluid dynamics, the Mach number (M or Ma) (; ) is a dimensionless quantity representing the ratio of flow velocity past a boundary to the local speed of sound.\n\nwhere:\n\nBy definition, at Mach1, the local flow velocity is equal to the speed of sound. At Mach0.65, is 65% of the speed of sound (subsonic), and, at Mach1.35, is 35% faster than the speed of sound (supersonic).\n\nThe local speed of sound, and thereby the Mach number, depends on the condition of the surrounding medium, in particular the temperature. The Mach number is primarily used to determine the approximation with which a flow can be treated as an incompressible flow. The medium can be a gas or a liquid. The boundary can be traveling in the medium, or it can be stationary while the medium flows along it, or they can both be moving, with different velocities: what matters is their relative velocity with respect to each other. The boundary can be the boundary of an object immersed in the medium, or of a channel such as a nozzle, diffusers or wind tunnels channeling the medium. As the Mach number is defined as the ratio of two speeds, it is a dimensionless number. If  < 0.2–0.3 and the flow is quasi-steady and isothermal, compressibility effects will be small and simplified incompressible flow equations can be used.\n\nThe Mach number is named after Austrian physicist and philosopher Ernst Mach, and is a designation proposed by aeronautical engineer Jakob Ackeret. As the Mach number is a dimensionless quantity rather than a unit of measure, with Mach, the number comes \"after\" the unit; the second Mach number is \"Mach2\" instead of \"2Mach\" (or Machs). This is somewhat reminiscent of the early modern ocean sounding unit \"mark\" (a synonym for fathom), which was also unit-first, and may have influenced the use of the term Mach. In the decade preceding faster-than-sound human flight, aeronautical engineers referred to the speed of sound as \"Mach's number\", never \"Mach 1\".\n\nMach number is useful because the fluid behaves in a similar manner at a given Mach number, regardless of other variables. As modeled in the International Standard Atmosphere, dry air at mean sea level, standard temperature of , the speed of sound is . The speed of sound is not a constant; in a gas, it increases as the absolute temperature increases, and since atmospheric temperature generally decreases with increasing altitude between sea level and , the speed of sound also decreases. For example, the standard atmosphere model lapses temperature to at altitude, with a corresponding speed of sound (Mach1) of , 86.7% of the sea level value.\n\nWhile the terms \"subsonic\" and \"supersonic\", in the purest sense, refer to speeds below and above the local speed of sound respectively, aerodynamicists often use the same terms to talk about particular ranges of Mach values. This occurs because of the presence of a \"transonic regime\" around M = 1 where approximations of the Navier-Stokes equations used for subsonic design no longer apply; the simplest explanation is that the flow locally begins to exceed M = 1 even though the freestream Mach number is below this value.\n\nMeanwhile, the \"supersonic regime\" is usually used to talk about the set of Mach numbers for which linearised theory may be used, where for example the (air) flow is not chemically reacting, and where heat-transfer between air and vehicle may be reasonably neglected in calculations.\n\nIn the following table, the \"regimes\" or \"ranges of Mach values\" are referred to, and not the \"pure\" meanings of the words \"subsonic\" and \"supersonic\".\n\nGenerally, NASA defines \"high\" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Aircraft operating in this regime include the Space Shuttle and various space planes in development.\n\nFlight can be roughly classified in six categories:\n\nFor comparison: the required speed for low Earth orbit is approximately 7.5 km/s = Mach 25.4 in air at high altitudes.\n\nAt transonic speeds, the flow field around the object includes both sub- and supersonic parts. The transonic period begins when first zones of M > 1 flow appear around the object. In case of an airfoil (such as an aircraft's wing), this typically happens above the wing. Supersonic flow can decelerate back to subsonic only in a normal shock; this typically happens before the trailing edge. (Fig.1a)\n\nAs the speed increases, the zone of M > 1 flow increases towards both leading and trailing edges. As M = 1 is reached and passed, the normal shock reaches the trailing edge and becomes a weak oblique shock: the flow decelerates over the shock, but remains supersonic. A normal shock is created ahead of the object, and the only subsonic zone in the flow field is a small area around the object's leading edge. (Fig.1b)\n\nFig. 1. \"Mach number in transonic airflow around an airfoil; M < 1 (a) and M > 1 (b).\"\n\nWhen an aircraft exceeds Mach 1 (i.e. the sound barrier), a large pressure difference is created just in front of the aircraft. This abrupt pressure difference, called a shock wave, spreads backward and outward from the aircraft in a cone shape (a so-called Mach cone). It is this shock wave that causes the sonic boom heard as a fast moving aircraft travels overhead. A person inside the aircraft will not hear this. The higher the speed, the more narrow the cone; at just over M = 1 it is hardly a cone at all, but closer to a slightly concave plane.\n\nAt fully supersonic speed, the shock wave starts to take its cone shape and flow is either completely supersonic, or (in case of a blunt object), only a very small subsonic flow area remains between the object's nose and the shock wave it creates ahead of itself. (In the case of a sharp object, there is no air between the nose and the shock wave: the shock wave starts from the nose.)\n\nAs the Mach number increases, so does the strength of the shock wave and the Mach cone becomes increasingly narrow. As the fluid flow crosses the shock wave, its speed is reduced and temperature, pressure, and density increase. The stronger the shock, the greater the changes. At high enough Mach numbers the temperature increases so much over the shock that ionization and dissociation of gas molecules behind the shock wave begin. Such flows are called hypersonic.\n\nIt is clear that any object traveling at hypersonic speeds will likewise be exposed to the same extreme temperatures as the gas behind the nose shock wave, and hence choice of heat-resistant materials becomes important.\n\nAs a flow in a channel becomes supersonic, one significant change takes place. The conservation of mass flow rate leads one to expect that contracting the flow channel would increase the flow speed (i.e. making the channel narrower results in faster air flow) and at subsonic speeds this holds true. However, once the flow becomes supersonic, the relationship of flow area and speed is reversed: expanding the channel actually increases the speed.\n\nThe obvious result is that in order to accelerate a flow to supersonic, one needs a convergent-divergent nozzle, where the converging section accelerates the flow to sonic speeds, and the diverging section continues the acceleration. Such nozzles are called de Laval nozzles and in extreme cases they are able to reach hypersonic speeds ( at 20 °C).\n\nAn aircraft Machmeter or electronic flight information system (EFIS) can display Mach number derived from stagnation pressure (pitot tube) and static pressure.\n\nThe Mach number at which an aircraft is flying can be calculated by\n\nwhere:\n\nNote that the dynamic pressure can be found as:\n\nAssuming air to be an ideal gas, the formula to compute Mach number in a subsonic compressible flow is derived from Bernoulli's equation for M < 1:\n\nwhere:\n\nThe formula to compute Mach number in a supersonic compressible flow is derived from the Rayleigh supersonic pitot equation:\n\nMach number is a function of temperature and true airspeed.\nAircraft flight instruments, however, operate using pressure differential to compute Mach number, not temperature.\n\nAssuming air to be an ideal gas, the formula to compute Mach number in a subsonic compressible flow is found from Bernoulli's equation for (above):\n\nThe formula to compute Mach number in a supersonic compressible flow can be found from the Rayleigh supersonic pitot equation (above) using parameters for air:\n\nwhere:\n\nAs can be seen, M appears on both sides of the equation, and for practical purposes a root-finding algorithm must be used for a numerical solution. It is first determined whether M is indeed greater than 1.0 by calculating M from the subsonic equation. If M is greater than 1.0 at that point, then the value of M from the subsonic equation is used as the initial condition for fixed point iteration of the supersonic equation, which usually converges in just a few iterations. Alternatively, Newton's method can also be used.\n\n\n"}
{"id": "13869206", "url": "https://en.wikipedia.org/wiki?curid=13869206", "title": "Mead acid", "text": "Mead acid\n\nMead acid is an omega-9 fatty acid, first characterized by James F. Mead. As with some other omega-9 polyunsaturated fatty acids, animals can make Mead acid \"de novo\". Its elevated presence in the blood is an indication of essential fatty acid deficiency. Mead acid is found in large quantities in cartilage.\n\nMead acid, also referred to as eicosatrienoic acid, is chemically a carboxylic acid with a 20-carbon chain and three methylene-interrupted \"cis\" double bonds. The first double bond is located at the ninth carbon from the omega end. In physiological literature, it is given the name 20:3 (n-9). (See Fatty Acid#Nomenclature for an explanation of the naming system.) In the presence of lipoxygenase, cytochrome p450 or cyclooxygenase Mead acid can form various hydroxyeicosatetraenoic acid (HETE) and hydroperoxy (HpETE) products.\n\nTwo fatty acids, linoleic acid and alpha-linolenic acid, are considered essential fatty acids (EFAs) in humans and other mammals. Both are 18 carbon fatty acids unlike mead acid, which has 20 carbons. Linoleic is an ω-6 fatty acid whereas linolenic is ω-3 and mead is ω-9. One study examined patients with intestinal fat malabsorption and suspected EFA deficiency; they were found to have blood-levels of Mead acid 1263% higher than reference subjects. Under severe conditions of essential fatty acid deprivation, mammals will elongate and desaturate oleic acid to make mead acid, (20:3, \"n\"−9). This has been documented to a lesser extent in vegetarians and semi-vegetarians following an unbalanced diet.\n\nMead acid has been found to decrease osteoblastic activity. This may be important in treating conditions where inhibition of bone formation is desired.\n\nCyclooxygenases are enzymes known to play a large role in inflammatory processes through oxidation of unsaturated fatty acids, most notably, the formation of prostaglandin H2 from arachidonic acid which is very similar in structure to mead acid. When physiological levels of arachidonic acid are low, other unsaturated fatty acids including mead and linoleic acid are oxidized by COX.\n\nMead acid is also converted to leukotrienes C3 and D3.\n\nMead acid is metabolized by 5-lipoxygenase to 5-hydroxyeicosatrieonic acid (5-HETrE) and then by 5-Hydroxyeicosanoid dehydrogenase to 5-oxoeicosatrienoic acid (5-oxo-ETrE).\n5-Oxo-ETrE is as potent as its arachidonic acid-derived analog, 5-oxo-eicosatetraenoic acid (5-oxo-ETE), in stimulating human blood eosinophils and neutrophils;\nit presumably does so by binding to the 5-oxo-ETE receptor (OXER1) and therefore may be, like 5-oxo-ETE, a mediator of human allergic and inflammatory reactions.\n\n"}
{"id": "13973542", "url": "https://en.wikipedia.org/wiki?curid=13973542", "title": "Melhus Energi", "text": "Melhus Energi\n\nMelhus Energi is a defunct power company that operated hydro electric power plants and the power grid in Melhus, Norway. Until it merged with TrønderEnergi in 2001 it was owned entirely by the municipality of Melhus.\n"}
{"id": "24223007", "url": "https://en.wikipedia.org/wiki?curid=24223007", "title": "Montara oil spill", "text": "Montara oil spill\n\nThe Montara oil spill was an oil and gas leak and subsequent slick that took place in the Montara oil field in the Timor Sea, off the northern coast of Western Australia. It is considered one of Australia's worst oil disasters. The slick was released following a blowout from the Montara wellhead platform on 21 August 2009, and continued leaking until 3 November 2009 (in total 74 days), when the leak was stopped by pumping mud into the well and the wellbore cemented thus \"capping\" the blowout. The West Atlas rig is owned by the Norwegian-Bermudan Seadrill, and operated by PTTEP Australasia (PTTEPAA), a subsidiary of PTT Exploration and Production (PTTEP) which is in turn a subsidiary of PTT, the Thai state-owned oil and gas company was operating over on adjacent well on the Montara platform. Houston-based Halliburton was involved in cementing the well. The Montara field is located off the Kimberley coast, north of Truscott airbase, and west of Darwin. Sixty-nine workers were safely evacuated from the West Atlas jackup drilling rig when the blowout occurred.\n\nThe Australian Department of Resources, Energy and Tourism estimated that the Montara oil leak could be as high as /day, five times the /day estimated by PTTEP Australasia. A spokesman for Resources Minister, Martin Ferguson, said the referred to the amount of oil that the well could produce when brought into peak production. After flying over the spill site, Australian Greens Senator Rachel Siewert claimed the spill was far greater than had originally been reported. WWF-Australia also claimed that the spill was worse than originally expected.\n\nThe first four attempts to plug the oil leak by PTTEPAA failed, but the fifth attempt succeeded on 3 November 2009, when PTTEPAA pumped approximately of mud into a relief well to stop the leak.\n\nOn 1 November 2009, during an attempt to stop the leak, a fire broke out on the West Atlas drilling rig. On 2 November, PTTEPAA said that the fire appeared to be burning off the oil and thereby preventing further leakage into the sea. The fire was largely extinguished when the leak was stopped. Once safety criteria were met, a specialist team boarded the Montara wellhead platform and the West Atlas to assess the damages.\n\nThe operation later in November 2009 to finally plug the well after the leak was stopped involved pumping a 1,400 metre cement plug from the West Triton rig down the relief well to the bottom of the 2.5 kilometre well. Once completed, the West Triton relief rig was demobilised and returned to Singapore.\n\nIn December 2009, a team from PTTEPAA and Alert Well Control returned to the Montara field to complete the final stages of the operation, which involved inserting two mechanical barriers at depth above the cement plug into the well.\nOperations were completed in January 2010 when the reboarding team installed a capping assembly on the well.\n\nWork to safely remove the West Atlas drilling rig from the Montara well head platform (WHP) in the Timor Sea started in August 2010.\n\nThe offshore construction vessel \"Jascon 25\" equipped with an 800-tonne crane was mobilised for the salvage operations. This work was expected to take about three months and involve cleaning and removal of debris from the rig as well as the cantilever drill floor which was left extended over the WHP helideck after the fire in November 2009. After the debris removal work was completed, this was followed by the jacking down of the West Atlas rig to be towed to Singapore.\n\nPTTEPAA announced a major transformation of its Australian drilling operations on 24 November 2010 following the release of the Australian Government’s Montara Commission of Inquiry report into causes of the incident.\nThe company said it was implementing a nine-point Action Plan to embed the highest standards of oil field practice and safety in its operations. PTTEPAA said it regretted the Montara incident and acknowledged there were deficiencies identified in the company’s operations in the Commission of Inquiry’s report.\nThe company said drilling supervisors and management associated with the incident have been removed from their positions with the company.\nAccording to a company spokesman, the Action Plan will \"ensure the full accountability of key personnel to give greater oversight for reporting and checking of all critical offshore operations. This will strengthen the integrity and safety of drilling operations\".\n\nThe leak initially emanated from the Montara Wellhead platform on 21 August 2009. The depth of water was approximately and depth of hole was approximately below the seabed. Sixty-nine workers on the rig were safely evacuated with no injuries or fatalities. By 24 August, the oil slick resulting from the spill was estimated to be long and wide. On 29 August, the slick was estimated at at a minimum, measured east to west.\n\nBy 3 September 2009, the Australian Maritime Safety Authority (AMSA) reported that the slick was from the coast of Western Australia, and moving closer to the shore. The slick was also reported to have spread over of ocean with evidence that the oil was killing marine life. Reports that the slick had spread to within of the Northern Territory coastline were dismissed as incorrect, with the AMSA stating that the discoloured water was likely to be a natural phenomenon, such as an algal bloom or coral spawn. Daily overflights by the Australian authorities in September and October 2009 identified isolated patches of weathered oil and sheen within Indonesian waters with small patches seen 94 kilometres south east of Roti Island. At the time the main spill was located about 248 kilometres from the Indonesian coastline.\n\nThe West Australian newspaper online reported that no oil had reached the Kimberley coastline in northern Western Australia. The report was based on a West Australian Environmental Protection Authority study conducted in October 2009 during the spill. The study, released in July 2010 stated no traces of hydrocarbons were found in water or shoreline sediments in areas sampled between Camden Sound and the Stewart Islands.\n\nThe Australian Marine Oil Spill Centre began mobilising aircraft and equipment on 21 August 2009. On 23 August 2009, a Hercules aircraft sprayed 10,000 litres of chemical dispersant onto parts of the slick, with ongoing aerial spraying with dispersants being the primary early response to the spill. Spraying from vessels commenced on 30 August and continued until 1 November utilising 118,000 litres of dispersant. Six different chemical dispersants were used: Slickgone NS, Slickgone LTSW, Ardrox 6120, Tergo R40, Corexit 9500 and Corexit 9527. In total, 184,135 litres of chemical dispersants were sprayed from aircraft or vessels between 23 August and 1 November.\n\nThe West Triton jackup drilling rig arrived at the Montara Field in an attempt to plug the oil leak on 11 September 2009. Oil and gas producer, Woodside Petroleum Ltd offered to assist PTTEPAA in cleaning up the oil spill with the use of a rig closer to the spill site. However, PTTEPAA rejected the Woodside offer on the basis of \"safety reasons\". The Woodside rig was a semisubmersible drilling rig and, as it floats on the sea surface, was not deemed as a suitable platform for the relief well. A jackup rig was required because it could be secured to the sea floor giving better stability and had the capacity to pump large volumes of heavy mud needed to stop the leak. On 6 September, the plugging of the oil leak was delayed further by a broken towline to the mobile oil rig being towed in from Indonesia by PTTEPAA. On 7 September, the Australian Federal Government announced that it was suspending the normal approval process to fast track stopping the leak at the West Atlas oil rig. PTTEP Australasia initially said it could be just days before the leak was brought under control, but then said that the oil leak would continue for eight weeks until they could bring in another mobile offshore rig, West Triton, to drill a hole into the leaking oil well, and pump mud in to alleviate pressure to stop the oil flow.\n\nOn 1 November 2009, the West Triton rig successfully drilled the relief well to intercept the leaking well. During operations to kill the leak by pumping heavy mud down the relief well, a fire broke out from the H1 well on the well head platform. This was expected to delay further work on resolving the spill. All eight non-essential personnel were taken off the West Triton rig.\n\nOn 1 November 2009, the fifth attempt to intercept the well succeeded. Approximately of heavy mud were subsequently pumped down the relief well on 3 November 2009 thereby stopping the leak and extinguishing the fire. PTTEPAA continued to pump a mixture of heavy mud and brine into the relief to maintain a stable condition before it was eventually cemented. Once the leak was killed, the main fire on the Montara Wellhead platform was extinguished. Some material on the topside of the West Atlas rig remained on fire but had extinguished by 3 November 2009 as the fuel source burnt out.\n\nIn July 2010, a PTTEPAA team reboarded the well head platform to pressure test all the wells. Following a three-week operation, the company confirmed all wells were secure.\n\nPTTEPAA estimates that it has spent $170 million on the gas and oil leak up to 3 November 2009. The environmental clean-up cost $5.3 million. Since the spill originated directly from an oil well, it is difficult to determine its total size with any precision. Estimates range from to more than , or about 4,000 tonnes to 30,000 tonnes.\n\nThe Australian Government in November 2010 released the first results of an independent scientific studies conducted under the long-term environmental monitoring program being funded by PTTEPAA under an agreement with the Australian Government announced in October 2009.\nThe scientific studies issued by the Australian Department of Sustainability, Environment, Water, Population and Communities (DSEWPaC) on 19 November 2010 found that no oil reached the Australian mainland or Indonesian coast and the maximum surface area of the ocean that had hydrocarbons on it on any one given day during the spill was 11,183 square kilometres.\nThe Australian Government sought independent expert advice on the detail of the environmental monitoring program from organisations such as the Australian Institute of Marine Science, the Commonwealth Scientific and Industrial Research Organisation (CSIRO) and State and Territory agencies.\nEach scientific study conducted under the program is subject to a detailed review by a DSEWPaC technical advisory board.\nPTTEPAA will fund the comprehensive monitoring program for at least two years, with some studies expected to continue up to five years or longer if required to provide data to measure and address any longer-term impacts should they occur.\nA PTTEPAA spokesperson said \"the commitment to run some of the studies beyond two years does not necessarily indicate long-term impacts but supports the recommendations of independent experts to ensure the best science is in place to monitor the marine environment\".\nThe spokesperson said the studies are creating a body of high quality scientific baseline data in key areas of the Timor Sea marine environment which will be an asset for the industry and the community in managing activities in the region.\n\nBiologists said that the effects of the Montara oil spill could be catastrophic for marine ecosystems, with claims that although it is a lightweight crude oil spilling from the platform, it can still have toxic effects on birds, marine invertebrates, coral and marine algae. The Wilderness Society described the area as a \"marine superhighway\", and whales and endangered flatback turtles observed in the area are at risk from the spill. By 3 September 2009, fishers observed sick and dying marine life, and an absence of birds in the spill area. The World Wildlife Fund (WWF) observed spinner dolphins, sooty terns, spotted sea snake and threatened hawksbill and flatback turtles swimming in the oil slick, and expressed concern about long-term effects. WWF also observed a wax-like residue from the oil spill. The Australian government has acknowledged treating a small number of birds as a result of the spill, including common noddies, brown boobies and a sooty tern.\n\nIndonesian fishermen have claimed that the spill and response polluted their national waters, killed thousands of fish and caused skin diseases and loss of human life. NGOs in Indonesia expressed concern about the oil spill's effects on the Indonesian environment and traditional fishing grounds as the oil spill drifted towards the islands of Timor and Sumba. The Montara oil field is situated south of Indonesia's Pasir Island, a popular fishing location for many East Nusa Tenggara fishermen.\n\nThe extent of the spread of the oil slick and sheen into Indonesian waters is disputed. Maps obtained by the Australian Lawyers Alliance under the Freedom of Information Act suggested that oil could have come as close as 37 km to the southern coast of Rote. The Australian Maritime Safety Authority advised that while figures on the maps were accurate, the maps were not drawn to scale so distances could not be visually interpreted. PTTEP maintains that the slick remained 94 km distant from Indonesia and was primarily contained within a 23 km radius of the drilling platform.\n\nIn October 2010, fishermen claimed to have observed dramatic declines in the number of red snapper caught by Indonesians, with 7,000 fishers impacted by loss of income, including cases of bankruptcy. Declining fish catches have forced thousands of fishermen to find new livelihoods on other islands as distant as the Bangka–Belitung Islands. On 10 November, video emerged of Indonesian fisherman and scuba divers holding dead fish whilst in the water amongst the oil slick. The footage was allegedly taken whilst in Indonesian waters.\n\nThe West Timor Care Foundation received reports on the death of eight people and 30 poisonous cases after the consumption of fish in the waters around areas allegedly contaminated by oil and chemical dispersant. As of October 2010, the foundation was the only Indonesian NGO to file a legal action with the Australian independent investigation commission into the Montara spill.\n\nThe Australian Lawyers Alliance has argued that in absence of sub-surface sampling of oil and chemical disperant, the spread of contamination and the environmental consequences of the spill can not be fully understood. The organisation has called for further investigation, suggesting that if Australian livelihoods had been affected there would be public outrage.\n\nEast Nusa Tenggara governor Frans Leburaya said Australia and the operator of the oil field should be held responsible for any environmental damage caused by the oil spill. In June 2010, the East Nusa Tenggara provincial government estimated that economic losses due to the pollution of the Timor Sea resulting from the Montara spill amounted to more than 2.5-3 trillion rupiah (Rp) or $318–382 million (AUD). The loss was born exclusively by the fishermen and coastal residents whose living was dependent on marine resources.\n\nThe Indonesian president, Susilo Bambang Yudhoyono announced that the Indonesian government would seek compensation from PTTEPAA because he believed the lives of Indonesian seaweed farmers and fishermen had been affected by the oil spill.\n\nPTTEP Australia has stated that independent studies published by the Australian Department for the Environment found that 98.6 per cent of Montara oil stayed in Australian waters, and that the company has received \"no credible evidence of damage to the environment in West Timor.\"\n\nEast Timorese President, Dr José Ramos-Horta, said that the Australian government and the Thai company that own the platform are responsible for the spill, and that he will seek compensation for damage caused by the spill to his country's environment. Ramos-Horta called for Australian environmental organisations to help assess whether the spill has caused damage to East Timor's maritime area.\n\nTo assist in planning and executing the spill response, the Australian Government commenced an environmental monitoring program comprising a series of operational studies, including a wildlife monitoring program.\nThe wildlife monitoring program sought to locate, assess and treat oil affected wildlife found in the region of the spill. Carcasses of oil affected wildlife were retained and tested to determine their cause of death.\nStudies undertaken under this program by Associate Professor Marthe Monique Gagnon and Dr Christopher Rawson from Curtin University in Western Australia, involved testing of four fish specimens collected in the vicinity of the spill. Tests were also conducted on 16 birds, two sea snakes and one green turtle collected in the region of the oil spill.\nResults announced in November 2010 found that two of the birds (both common noddies) had been affected by oil, one internally and the second both internally and externally. The study also found that the 14 remaining birds had no traces of oil. The poor physical condition of the remaining birds suggests that they likely died of natural causes.\nTests on the horned sea snake indicated it had been affected by oil; however, positive hydrocarbon readings were only detected in its stomach contents. This suggests the sea snake was indirectly exposed to oil through ingesting prey.\nTests on the sea turtle found no positive results for hydrocarbons, which indicated oil exposure was not the cause of death.\n\nShoreline ecological assessment – Study S2\nThis independent study, led by Dr Norm Duke of the University of Queensland, aimed to collect baseline information on habitats and species found along Australia’s north-west coast between Broome in Western Australia and Darwin in the Northern Territory.\nDuring November 2009, aerial surveys were undertaken covering more than 5,000 km of shoreline.\nThe study findings are consistent with those released in July 2010 by the Environmental Protection Authority of Western Australia (EPA) from an independent survey of the Kimberley shoreline conducted in October–November 2009 which found no oil issued during the Montara incident reached the Western Australian coast. The EPA surveyed 16 Kimberley island shorelines and two mainland promontories between Camden Sound and the Stewart Islands. The EPA found that no hydrocarbons were detected in any water or shoreline samples and analyses of rock oyster and cultured pearl oyster show no evidence of in situ contamination.\n\nOil Fate and Effects Assessment - Trajectory Analysis Study S7.1\nThis study was undertaken by Asia-Pacific ASA, an Australian company that specialises in modelling, mapping and assessing spill events throughout the world.\nHeaded by senior oceanographer Dr Brian King and senior chemist and environmental scientist Trevor Gilbert, the S7.1 study combined sightings of oil with modelling data to produce a picture of the likely extent of the spill. The results of this study will be used so that future scientific monitoring covers appropriate areas. This study used trajectory modelling, overflight data and satellite images to gain an almost hourly understanding of the spill.\nThis study reported, in part, that no oil reached the Australian mainland or Indonesian coast. The greatest occurrence of oil was within 22.8 kilometres of the release site. Beyond 22.8 kilometres, the hydrocarbons were predominantly sheens/waxy films and of short duration. 98.6 per cent of occurrences of hydrocarbons on surface were within Australian waters.\nKing’s oil fate and effects studies also used reports from the World Wildlife Fund (WWF), all submissions made to the Montara Commission of Inquiry and related transcripts, as well as Australian and Indonesian media reports of oil locations.\nCombination of all these datasets with scientific modelling ensured the most accurate and objective description of the movement of hydrocarbons in the Timor Sea throughout the incident, Dr King said. Satellite imagery used in Asia-Pacific ASA’s assessments were the same used by the WWF and oil spill tracker Skytruth, with additional high resolution imagery from LANDSAT. These combined techniques gave Dr King the highest degree of confidence in providing detailed modelling, analysis and mapping of the spill event.\n\nOil Fate and Effects Assessment - Dispersant Oil Modelling Study S7.2\nAfter using chemical dispersants to accelerate the natural breakdown of oil during the Montara incident, the Australian Government sought to find out the fate of the dispersed oil once it was in the\nwater column. This study aimed to determine the potential concentrations of dispersed oil under the water surface using computer modelling.\n\nThe S7.2 study undertaken by specialist company ASA-Pacific reported, in part the chemical dispersant application caused increased hydrocarbon concentrations in the water column, mostly within the first metre of the water column. However, these concentrations reduced quickly with time, depth and distance from the dispersant application site and no dispersed oil reached the Australian mainland or Indonesian coast.\n\nThe study found that under the worst-case scenario dispersed oil from three of the spraying operations may have reached the Goerree and Barracouta shoals. Because of these findings, a further study—the Scientific Monitoring Study 5 Offshore Banks Assessment Survey—will be undertaken.\nDispersant Study S7.2 research relied on the Australian Maritime Safety Authority’s field monitoring of dispersed oil concentrations to validate S7.2 modelling. Where field data was unavailable, Asia-Pacific ASA took a \"conservative approach\" to modelling was taken and potential dispersed oil concentrations were overestimated to ensure further investigation by independent field monitoring teams from Western Australian Department of Fisheries and the Australian Institute of Marine Science.\n\nFurther studies\nThe scientific studies triggered under the Montara Long Term Environmental Monitoring Program comprise:\nShoreline Ecological Assessment Aerial Surveys (study S2);\nAssessments of Fish Catch for the Presence of Oil (study S3);\nAssessments of Effects on Timor Sea Fish and Fisheries (study S4);\nOffshore Banks Assessment Surveys (study S5);\nShoreline Ecological Ground Surveys (study S6); and\nOil Fate and Effects Assessments (study S7). Marine Megafauna Aerial Assessment Surveys (study S1) has not been triggered.\n\nThe Australian Government released the initial scientific reports S7 and S2 in November 2010 and further studies are expected to be released in the near future.\n\nPTTEPAA stated shortly after the leak was plugged that they had a theory about the cause of the leak but would not disclose nor confirm the cause until they had access to the Montara Wellhead Platform and could present a thorough appraisal.\n\nOn 5 November 2009, a Commission of Inquiry into the oil leak was announced. The inquiry, led by David Borthwick, had nearly all the powers of a Royal Commission. The report was to be presented by the end of April 2010; the commission delayed their report, however, until 18 June to further examine the causes and effects of the spill.\n\nSince the spill in August 2009 Martin Ferguson has signed 120 new gas and oil exploration licenses but has still not imposed any regulations on deep sea oil drilling.\n\nElmer Danenberger, who used to be in charge of regulatory affairs for the U.S. Minerals Management Service, claimed that Halliburton had done a poor job cementing, a process that is supposed to fill the gaps around the casing with cement to prevent leaks of oil and gas, probably causing the spill.\nOn 24 November 2010, Australian Resources and Energy Minister Martin Ferguson released the Report of the Montara Commission of Inquiry and a draft response from the Australian Government.\nThe Report contained 100 findings and 105 recommendations. The Australian Government proposed accepting 92, noting 10, and not accepting three of the Report’s recommendations.\nCommissioner David Borthwick’s final report stated that while the source of the blowout was largely uncontested it was most likely that hydrocarbons entered the H1 Well through its 9⅝\" cemented casing shoe and flowed up the inside of its 9⅝\" casing. The Inquiry found that the primary well control barrier – the 9⅝\" cemented casing shoe – failed.\nThe final report by the Commission commended the spill response efforts by PTTEPAA, the Australian Maritime Safety Authority in its role as Combat Agency and the then Department of Environment, Water, Heritage and the Arts for its role as environmental regulator.\n\nMinister Ferguson said the failure of the operator and regulator to adhere to Australia’s oil and gas regulatory regime was a key factor in the Montara incident.\n\nRepresentatives from PTTEPAA held meetings with Indonesian government officials in Perth Western Australia on 27 July and 26 August 2010, to discuss the Indonesian government’s claim for compensation. On 2 September 2010, PTTEPAA stated it did not accept any claim because no verifiable scientific evidence had been presented to the company to support the summary of claims presented by the Indonesian government. In October 2010, PTTEPAA announced its commitment to the Australian Government to fund a range of scientific studies aimed at determining any environmental impacts from the incident. The Commission during its inquiry took into account measures which could have been implemented to mitigate environmental damage.\n\nIn February 2011 PTT Exploration & Production Pcl (PTTEP) was cleared to keep operating in Australian waters after satisfying the Australian government that it had taken steps to prevent a repeat of the Montara blowout.\n\nIn 2012, PTTEP Australasia pleaded guilty to charges under the \"Offshore Petroleum and Greenhouse Gas Storage Act\" and was fined $510,000.\n\nAs of 2014, the fishermen of West Timor are represented by lawyer Greg Phelps with the support of the Indonesian government. Phelps believes an independent investigation into the oil spill's environmental and economic impact in Indonesia is necessary.\n\nIn August 2016, a class action suit was filed in Sydney, Australia representing the interests of over 13,000 Indonesian seaweed farmers whose livelihoods were affected by the spill and subsequent clean-up activities.\n\n\n"}
{"id": "43070167", "url": "https://en.wikipedia.org/wiki?curid=43070167", "title": "N-Acylamides", "text": "N-Acylamides\n\nN-acyl amides are a general class of endogenous fatty acid compounds characterized by a fatty acyl group linked to a primary amine metabolite by an amide bond. Broadly speaking, N-acyl amides fall into several categories: amino acid conjugates (e.g., N-arachidonoyl-glycine), neurotransmitter conjugates (e.g., N-arachidonoyl-serotonin), ethanolamine conjugates (e.g., anandamide), and taurine conjugates (e.g., N-palmitoyl-taurine). N-acyl amides have pleiotropic signaling functions in physiology, including in cardiovascular function, metabolic homeostasis, memory, cognition, pain, motor control and others. Initial attention focused on N-acyl amides present in mammalian organisms, however recently lipid signaling systems consisting of N-acyl amides have also been found to be present in invertebrates, such as Drosophila melanogaster. N-acyl amides play important roles in many biochemical pathways involved in a variety of physiological and pathological processes, as well as the metabolic enzymes, transporters, and receptors that regulate their signaling.\n\n†-Compound found in mammalian species\n\n<nowiki>#</nowiki>-Compound found in invertebrate (\"Drosophila melanogaster\") species<ref name=\"Salzet 1\" /\n\n<nowiki>*</nowiki>-Compound found in plant species\n\nThe enzymatic biosynthesis of the N-acyl amide class of metabolites is a topic of active research with various pathways being discovered for specific N-acyl amides. For example, a proposed biosynthetic pathway for the N-acyl ethanolamines (NAEs) has been the hydrolysis of an unusual phospholipid precursor, N-acyl-phosphatidylethanolamine (NAPE), by a phospholipase D activity to liberate NAE and, as a byproduct, phosphatidic acid. Mice deficient in the enzyme NAPE-PLD show decreased in a subset of brain NAEs, providing genetic evidence for this proposal, at least for a subset of the NAEs. Other biosynthetic pathways likely exist, but the enzymes catalyzing these reactions have not yet been identified.\n\nThe degradation of NAEs in vivo is largely mediated by an enzyme called fatty acid amide hydrolase (FAAH), which catalyzes the hydrolysis of NAEs into fatty acids and ethanolamine. Mice deficient in FAAH show complete loss of NAE degradation activity in tissues and dramatic elevations in tissue levels of NAEs.\n\nFAAH also mediates the degradation of a separate class of N-acyl amides, the N-acyl taurines (NATs). FAAH knockout mice also show dramatic increases in tissue and blood NATs. The enzymatic biosynthesis of NATs remains unknown.\n\nA distinct circulating enzyme, peptidase M20 domain containing 1 (PM20D1), can catalyze the bidirectional the condensation and hydrolysis of a variety of N-acyl amino acids in vitro. In vivo, PM20D1 overexpression increases the levels of various N-acyl amino acids in blood, demonstrating that this enzyme can contribute to N-acyl amino acid biosynthesis. PM20D1 knockout mice have complete loss of N-acyl amino acid hydrolysis activity in blood and tissues with concomitant bidirectional dysregulation of endogenous N-acyl amino acids.\n\nN-acyl amides have been shown to play an important role in a variety of physiological functions as lipid signaling molecule. Apart from the aforementioned roles in cardiovascular function, memory, cognition, pain, and motor control, the compounds have also been shown to play a role in cell migration, inflammation and certain pathological conditions such as diabetes, cancer, neurodegenerative disease, and obesity.\n\nIn a more general sense, one of the key characteristics of the N-acyl amide group of compounds is their ubiquitous nature. Research has shown the presence of the compounds in mice, \"Drosophila melanogaster, Arabidopsis, C. Elegans, Cerevisiae (yeast), Pseudomonas Syringae\", olive oil and PYD media\n. This diverse presence of N-acyl amides attests to their importance in multiple biological systems and also shows that the detected presence of specific N-acyl amides in a number of species, including humans, may be endogenous or exogenous.\n\nN-acyl amides are primarily involved in cell-to-cell communication in biological systems. An example of this is the lipid signaling system involving transient receptor potential channels (TRP), which interact with N-acyl amides such as N-arachidonoyl ethanolamide (Anandamide), N-arachidonoyl dopamine and others in an opportunistic fashion. This signaling system has been shown to play a role in the physiological processes involved in inflammation. Other N-acyl amides, including N-oleoyl-glutamine, have also been characterized as TRP channel antagonists.\n\nAn application of N-acyl amides that is currently at the forefront of related research is the correlation between oleoyl serine and bone remodeling. Recent research has shown that oleoyl serine, an N-acyl amide found in olive oil amongst other sources, plays a role in the proliferation of osteoblast activity and the inhibition of osteoclast activity. Further research regarding this application of oleoyl serine is set to take place to explore the possible correlation between the consumption of the compound by individuals at risk for osteoporosis.\n\nCertain N-acyl amino acids can act as chemical uncouplers and directly stimulate mitochondrial respiration. These N-acyl amino acids are characterized by medium chain, unsaturated fatty acyl chains and neutral amino acid head groups. Administration of these N-acyl amino acids to mice elevates energy expenditure leading to profound body weight loss and improvement of glucose homeostasis.\n\nOverall, the applications of N-acyl amides in biological settings are abundant. As mentioned, their importance in cell signaling in a variety systems leading to various physiological roles and in turn therapeutic capabilities, which gives all the more reason to continue the extensive research being conducted on the compounds today.\n\nSeveral N-acyl amides have been demonstrated to physiologically activate G-protein coupled receptors. Anandamide activates the cannabinoid receptors CB1 and CB2. FAAH knockout mice show increased anandamide levels in vivo and cannabinoid-receptor dependent behaviors including antinociception and anxiolysis. GPR18, GPR55, GPR92 have also been proposed to be activated by various N-acyl amides, though the physiological relevance of these assignments remains unknown.\n\n"}
{"id": "31537144", "url": "https://en.wikipedia.org/wiki?curid=31537144", "title": "Oil on Ice", "text": "Oil on Ice\n\nOil on Ice is a 2004 documentary film directed by Bo Boudart and Dale Djerassi. It explores the Arctic Refuge drilling controversy in the Arctic National Wildlife Refuge (ANWR) and the impact of oil and gas development on the land, wildlife, and lives of the Gwich'in Athabascan Indians and Inupiat Eskimos.\n\nThe film was narrated by Peter Coyote and features interviews with and footage of environmentalists Amory Lovins, Celia Hunter, Sarah James, Norma Kassi, former Alaska Governor Tony Knowles, former Alaska Senator Ted Stevens, California Senator Barbara Boxer, former Sierra Club director Carl Pope, Ken Whitten, David Klein, former Fairbanks North Star Borough Mayor Jim Whitaker, former North Slope Borough Mayor George N. Ahmaogak, and Inupiaq activist and former Nuiqsut mayor Rosemary Ahtuangaruak.\n\n\"Oil on Ice\" was sponsored by Northern Alaska Environmental Center (NAEC) and was filmed on location in Alaska, San Francisco, and Washington, DC. To promote and market the film, Steve Michelson engaged fourteen non-profit organizations, including the Sierra Club, members of which hosted thousands of house parties to screen and distribute materials about the film, and to promote grassroots efforts to prevent ANWR drilling.\n\nThe film premiered at Mountainfilm in Telluride on May 31, 2004, and was released on DVD September 6, 2005. The soundtrack CD \"Oil on Ice\" by William Susman, featuring cellist Joan Jeanrenaud, was released on October 16, 2007.\n\nFilmCritic.com reviewer Eric Meyerson described the film as \"unabashed counter-propaganda to the pro-drilling forces\" seeking access to wilderness land. He credits its professional production and its \"powerful story, with astonishing wildlife photography and fascinating and tragic tales of the plights of local fishermen and native tribes.\" Meyerson found the interviews with Gwich'in Indian Adeline Peter Raboff \"particularly affecting.\" He noted that the film is \"as one-sided as the \"O'Reilly Factor\"\" in that it \"failed to address any positive economic impacts that the oil industry has had on Alaskans.\" But he termed the film \"well-made counter-propaganda. If anything, \"Oil on Ice\" is worth seeing just to see exactly what ExxonMobil and CononoPhillips are getting ready to tear into.\"\n\nRussell Engebretson, writing for DVDVerdict, wrote of its \"beautiful Alaskan wildlife cinematography, including one truly stunning shot of a grazing Caribou herd that must have numbered in the thousands. For contrast, we get an aerial view of the massive, grotesque Prudhoe Bay drilling operation that abuts ANWR to the west.\" He noted that while the \"wilderness scenery is delightful, there is ample interview material as well.\" Of the many interviewees, he found that two, Pope and Levin, \"drag down the film\", and that Levin \"steers the film away from its central thesis — the exploitation of ANWR.\" Engebretson felt that the documentary erred in two ways: \"by lionizing people who don't deserve such treatment\", and \"in presenting [John] Kerry as a foe of Big Oil\". Summarizing, he found the film to be \"a good, basic introduction (from an anti-drilling point-of-view) to the oil extraction debate.\"\n\n\"Oil on Ice\" received several awards: the 2004 International Documentary Association \"Pare Lorentz Award\" for best representing the \"democratic sensibility, activist spirit and lyrical vision\" of Pare Lorentz, the 2005 Missoula International Wildlife Film Festival \"Festival Prize\", the 2005 Moondance International Film Festival \"Calypso Award\" for feature documentary and \"Seahorse Award\" for best film score, and the 2006 Park City Film Music Festival \"Gold Medal for Excellence\" in the category Documentary, Jury Choice: Best Impact of Music.\n\n\n"}
{"id": "147566", "url": "https://en.wikipedia.org/wiki?curid=147566", "title": "Otto cycle", "text": "Otto cycle\n\nAn Otto cycle is an idealized thermodynamic cycle that describes the functioning of a typical spark ignition piston engine. It is the thermodynamic cycle most commonly found in automobile engines.\n\nThe Otto cycle is a description of what happens to a mass of gas as it is subjected to changes of pressure, temperature, volume, addition of heat, and removal of heat. The mass of gas that is subjected to those changes is called the system. The system, in this case, is defined to be the fluid (gas) within the cylinder. By describing the changes that take place within the system, it will also describe in inverse, the system's effect on the environment. In the case of the Otto cycle, the effect will be to produce enough net work from the system so as to propel an automobile and its occupants in the environment.\n\nThe Otto cycle is constructed from:\n\nThe isentropic process of compression or expansion implies that there will be no inefficiency (loss of mechanical energy), and there be no transfer of heat into or out of the system during that process. Hence the cylinder, and piston are assumed impermeable to heat during that time. Work is performed on the system during the lower isentropic compression process. Heat flows into the Otto cycle through the left pressurizing process and some of it flows back out through the right depressurizing process. The summation of the work added to the system plus the heat added minus the heat removed yields the net mechanical work generated by the system.\n\nThe processes are described by:\n\nThe Otto cycle consists of isentropic compression, heat addition at constant volume, isentropic expansion, and rejection of heat at constant volume. In the case of a four-stroke Otto cycle, technically there are two additional processes: one for the exhaust of waste heat and combustion products at constant pressure (isobaric), and one for the intake of cool oxygen-rich air also at constant pressure; however, these are often omitted in a simplified analysis. Even though those two processes are critical to the functioning of a real engine, wherein the details of heat transfer and combustion chemistry are relevant, for the simplified analysis of the thermodynamic cycle, it is more convenient to assume that all of the waste-heat is removed during a single volume change.\n\nThe four-stroke engine was first patented by Alphonse Beau de Rochas in 1861. Before, in about 1854–57, two Italians (Eugenio Barsanti and Felice Matteucci) invented an engine that was rumored to be very similar, but the patent was lost.\n\nThe first person to build a working four-stroke engine, a stationary engine using a coal gas-air mixture for fuel (a gas engine), was German engineer Nikolaus Otto. This is why the four-stroke principle today is commonly known as the Otto cycle and four-stroke engines using spark plugs often are called Otto engines.\n\nThe system is defined to be the mass of air that is drawn from the atmosphere into the cylinder, compressed by the piston, heated by the spark ignition of the added fuel, allowed to expand as it pushes on the piston, and finally exhausted back into the atmosphere. The mass of air is followed as its volume, pressure and temperature change during the various thermodynamic steps. As the piston is capable of moving along the cylinder, the volume of the air changes with its position in the cylinder. The compression and expansion processes induced on the gas by the movement of the piston are idealised as reversible, i.e., no useful work is lost through turbulence or friction and no heat is transferred to or from the gas during those two processes. Energy is added to the air by the combustion of fuel. Useful work is extracted by the expansion of the gas in the cylinder. After the expansion is completed in the cylinder, the remaining heat is extracted and finally the gas is exhausted to the environment. Useful mechanical work is produced during the expansion process and some of that used to compress the air mass of the next cycle. The useful mechanical work produced minus that used for the compression process is the net work gained and that can be used for propulsion or for driving other machines. Alternatively the useful work gained is the difference between the heat added and the heat removed.\nA mass of air (working fluid) is drawn into the cylinder, from 0 to 1, at atmospheric pressure (constant pressure) through the open intake valve, while the exhaust valve is closed during this process. The intake valve closes at point 1.\n\nPiston moves from crank end (BDC, bottom dead centre and maximum volume) to cylinder head end (\"TDC\", top dead centre and minimum volume) as the working gas with initial state 1 is compressed isentropically to state point 2, through compression ratio . Mechanically this is the isentropic compression of the air/fuel mixture in the cylinder, also known as the compression stroke. This isentropic process assumes that no mechanical energy is lost due to friction and no heat is transferred to or from the gas, hence the process is reversible. The compression process requires that mechanical work be added to the working gas. Generally the compression ratio is around 9–10:1 for a typical engine.\n\nThe piston is momentarily at rest at \"TDC\". During this instant, which is known as the ignition phase, the air/fuel mixture remains in a small volume at the top of the compression stroke. Heat is added to the working fluid by the combustion of the injected fuel, with the volume essentially being held constant. The pressure rises and the ratio formula_1 is called the \"explosion ratio\".\n\nThe increased high pressure exerts a force on the piston and pushes it towards the \"BDC\". Expansion of working fluid takes place isentropically and work is done by the system on the piston. The volume ratio formula_2 is called the \"isentropic expansion ratio\". (For the Otto cycle is the same as the compression ratio formula_3). Mechanically this is the expansion of the hot gaseous mixture in the cylinder known as expansion (power) stroke.\n\nThe piston is momentarily at rest at \"BDC\". The working gas pressure drops instantaneously from point 4 to point 1 during a constant volume process as heat is removed to an idealized external sink that is brought into contact with the cylinder head. The gas has returned to state 1.\n\nThe exhaust valve opens at point 1. As the piston moves from \"BDC\" (point 1) to \"TDC\" (point 0) with the exhaust valve opened, the gaseous mixture is vented to the atmosphere and the process starts anew.\n\nIn the processes 1–2 the piston does work on the gas and in process 3–4 the gas does work on the piston during those isentropic compression and expansion processes, respectively. Processes 2–3 and 4–1 are isochoric processes; heat is transferred into the system from 2—3 and out of the system from 4—1 but no work is done on the system or extracted from the system during those processes. No work is done during an isochoric (constant volume) process because addition or removal of work from a system requires the movement of the boundaries of the system; hence, as the cylinder volume does not change, no shaft work is added to or removed from the system.\n\nFour different equations are used to describe those four processes. A simplification is made by assuming changes of the kinetic and potential energy that take place in the system (mass of gas) can be neglected and then applying the first law of thermodynamics (energy conservation) to the mass of gas as it changes state as characterized by the gas's temperature, pressure, and volume.\n\nDuring a complete cycle, the gas returns to its original state of temperature, pressure and volume, hence the net internal energy change of the system (gas) is zero. As a result, the energy (heat or work) added to the system must be offset by energy (heat or work) that leaves the system. In the analysis of thermodynamic systems, the convention is to account energy that enters the system as positive and energy that leaves the system is accounted as negative.\nEquation 1a.\n\nDuring a complete cycle, the net change of energy of the system is zero:\n\nThe above states that the system (the mass of gas) returns to the original thermodynamic state it was in at the start of the cycle.\n\nWhere formula_5is energy added to the system from 1–2–3 and formula_6 is energy is removed from 3–4–1. In terms of work and heat added to the system\nEquation 1b:\n\nEach term of the equation can be expressed in terms of the internal energy of the gas at each point in the process:\n\nThe energy balance Equation 1b becomes\n\nTo illustrate the example we choose some values to the points in the illustration:\nThese values are arbitrarily but rationally selected. The work and heat terms can then be calculated.\n\nThe energy added to the system as work during the compression from 1 to 2 is\n\nThe energy added to the system as heat from point 2 to 3 is\n\nThe energy removed from the system as work during the expansion from 3 to 4 is\n\nThe energy removed from the system as heat from point 4 to 1 is\n\nThe energy balance is\n\nNote that energy added to the system is counted as positive and energy leaving the system is counted as negative and the summation is zero as expected for a complete cycle that returns the system to its original state.\n\nFrom the energy balance the work out of the system is:\n\nThe net energy out of the system as work is -1, meaning the system has produced one net unit of energy that leaves the system in the form of work.\n\nThe net heat out of the system is:\n\nAs energy added to the system as heat is positive. From the above it appears as if the system gained one unit of heat. This matches the energy produced by the system as work out of the system.\nThermal efficiency is the quotient of the net work from the system, to the heat added to system.\nEquation 2:\n\nAlternatively, thermal efficiency can be derived by strictly heat added and heat rejected.\n\nSupplying the fictitious values\n\nformula_27\n\nIn the Otto cycle, there is no heat transfer during the process 1–2 and 3–4 as they are isentropic processes. Heat is supplied only during the constant volume processes 2–3 and heat is rejected only during the constant volume processes 4–1.\nThe above values are absolute values that might, for instance, have units of joules (assuming the MKS system of units are to be used) and would be of use for a particular engine with particular dimensions. In the study of thermodynamic systems the extensive quantities such as energy, volume, or entropy (versus intensive quantities of temperature and pressure) are placed on a unit mass basis, and so too are the calculations, making those more general and therefore of more general use. Hence, each term involving an extensive quantity could be divided by the mass, giving the terms units of joules/kg (specific energy), meters/kg (specific volume), or joules/(kelvin·kg) (specific entropy, heat capacity) etc. and would be represented using lower case letters, u, v, s, etc.\nEquation 1 can now be related to the specific heat equation for constant volume. The specific heats are particularly useful for thermodynamic calculations involving the ideal gas model.\n\nRearranging yields:\n\nInserting the specific heat equation into the thermal efficiency equation (Equation 2) yields.\n\nUpon rearrangement:\n\nNext, noting from the diagrams formula_32 (see isentropic relations for an ideal gas), thus both of these can be omitted. The equation then reduces to:\nEquation 2:\n\nSince the Otto cycle uses isentropic processes during the compression (process 1 to 2) and expansion (process 3 to 4) the isentropic equations of ideal gases and the constant pressure/volume relations can be used to yield Equations 3 & 4.\nEquation 3:\nEquation 4:\n\nFurther simplifying Equation 4, where formula_43 is the compression ratio formula_44:\nEquation 5:\n\nFrom inverting Equation 4 and inserting it into Equation 2 the final thermal efficiency can be expressed as: name=\"Fundamentals\" />\nEquation 6:\n\nFrom analyzing equation 6 it is evident that the Otto cycle efficiency depends directly upon the compression ratio formula_43. Since the formula_48 for air is 1.4, an increase in formula_43 will produce an increase in formula_50. However, the formula_51 for combustion products of the fuel/air mixture is often taken at approximately 1.3.\nThe foregoing discussion implies that it is more efficient to have a high compression ratio. The standard ratio is approximately 10:1 for typical automobiles. Usually this does not increase much because of the possibility of autoignition, or \"knock\", which places an upper limit on the compression ratio. During the compression process 1–2 the temperature rises, therefore an increase in the compression ratio causes an increase in temperature. Autoignition occurs when the temperature of the fuel/air mixture becomes too high before it is ignited by the flame front. The compression stroke is intended to compress the products before the flame ignites the mixture. If the compression ratio is increased, the mixture may auto-ignite before the compression stroke is complete, leading to \"engine knocking\". This can damage engine components and will decrease the brake horsepower of the engine.\n\nThe power produced by the Otto cycle is the energy developed per unit of time. The Otto engines are called four-stroke engines. \nThe intake stroke and compression stroke require one rotation of the engine crankshaft. The power stroke and exhaust stroke require another rotation. For two rotations there is one work generating stroke.\n\nFrom the above cycle analysis the net work produced by the system :\n\nIf the units used were MKS the cycle would have produced one joule of energy in the form of work. For an engine of a particular displacement, such as one liter, the mass of gas of the system can be calculated assuming the engine is operating at standard temperature (20 °C) and pressure (1 atm). Using the Universal Gas Law the mass of one liter of gas is at room temperature and sea level pressure:\n\nAt an engine speed of 3000 RPM there are 1500 work-strokes/minute or 25 work-strokes/second.\n\nPower is 25 times that since there are 25 work-strokes/second\n\nIf the engine is multi-cylinder, the result would be multiplied by that factor. If each cylinder is of a different liter displacement, the results would also be multiplied by that factor. These results are the product of the values of the internal energy that were assumed for the four states of the system at the end each of the four strokes (two rotations). They were selected only for the sake of illustration, and are obviously of low value. Substitution of actual values from an actual engine would produce results closer to that of the engine. Whose results would be higher than the actual engine as there are many simplifying assumptions made in the analysis that overlook inefficiencies. Such results would overestimate the power output.\n\nThe difference between the exhaust and intake pressures and temperatures means that some increase in efficiency can be gained by use of a turbocharger, removing from the exhaust flow some part of the remaining energy and transferring that to the intake flow to increase the intake pressure. A gas turbine can extract useful work energy from the exhaust stream and that can then be used to pressurize the intake air. The pressure and temperature of the exhausting gases would be reduced as they expand through the gas turbine and that work is then applied to the intake gas stream, increasing its pressure and temperature. The transfer of energy amounts to an efficiency improvement and the resulting power density of the engine is also improved. The intake air is typically cooled so as to reduce its volume as the work produced per stroke is a direct function of the amount of mass taken into the cylinder; denser air will produce more work per cycle. Practically speaking the intake air mass temperature must also be reduced to prevent premature ignition in a petrol fueled engine; hence, an intercooler is used to remove some energy as heat and so reduce the intake temperature. Such a scheme both increases the engine's efficiency and power.\n\nThe application of a supercharger driven by the crankshaft does increase the power output (power density) but does not increase efficiency as it uses some of the net work produced by the engine to pressurize the intake air and fails to extract otherwise wasted energy associated with the flow of exhaust at high temperature and a pressure to the ambient.\n"}
{"id": "35013287", "url": "https://en.wikipedia.org/wiki?curid=35013287", "title": "Raketa mena", "text": "Raketa mena\n\nRaketa mena is a 2007 documentary film.\n\nOn the coast of Androy, the southernmost point of Madagascar, the weather conditions don’t allow the fishermen to go fishing very often. The dunes build up, day by day, over fertile land. But that’s not the worst. The population is missing the most important element, water. To calm their thirst and hunger, many villages eat \"raketa mena\", a cactus whose scientific name is \"Opuntia stricta\". But this cactus is an invader that dries out the land. What is the solution?\n\n"}
{"id": "21300173", "url": "https://en.wikipedia.org/wiki?curid=21300173", "title": "Raygun", "text": "Raygun\n\nA raygun is a science fiction particle-beam weapon that fires what is usually destructive energy. They have various alternate names: ray gun, death ray, beam gun, blaster, laser gun, laser pistol, phaser, zap gun, etc. In most stories, when activated, a raygun emits a ray, typically visible, usually lethal if it hits a human target, often destructive if it hits mechanical objects, with properties and other effects unspecified or varying.\n\nReal-life analogues are directed-energy weapons or electrolasers, electroshock weapons which send current along an electrically conductive laser-induced plasma channel.\n\nA very early example of a raygun is the Heat-Ray featured in H. G. Wells' novel \"The War of the Worlds\" (1898). Science fiction during the 1920s described death rays. Early science fiction often described or depicted raygun beams making bright light and loud noise like lightning or large electric arcs.\n\nAccording to \"The Encyclopedia of Science Fiction\", the word \"ray gun\" was first used by Victor Rousseau in 1917, in a passage from \"The Messiah of the Cylinder\":\n\nAll is not going well, Arnold: the ray-rods are emptying fast, and our attack upon the lower level of the wing has failed. Sanson has placed a ray-gun there. All depends on the air-scouts, and we must hold our positions until the battle-planes arrive.\n\nThe variant \"ray projector\" was used by John W. Campbell in \"The Black Star Passes\" in 1930. Related terms \"disintegrator ray\" dates to 1898 in Garrett P. Serviss' \"Edison's Conquest of Mars\"; \"blaster\" dates to 1925 in Nictzin Dyalhis' story \"When the Green Star Waned;\" and \"needle ray\" and \"needler\" date to 1934 in E.E. Smith's \"The Skylark of Valeron\".\n\nRay guns were so common on magazine covers during the Golden Age of Science Fiction that Campbell's \"Astounding\" was unusual for not depicting them. The term \"ray gun\" had already become cliché by the 1940s, in part due to association with the comic strips (and later film serials) Buck Rogers and Flash Gordon.\nSoon after the invention of lasers during 1960, such devices became briefly fashionable as a directed-energy weapon for science fiction stories. For instance, characters of the \"Lost in Space\" TV series (1965–1968) and of the \"\" pilot episode \"\" (1964) carried handheld laser weapons.\n\nBy the late 1960s and 1970s, as the laser's limits as a weapon became evident, rayguns were dubbed \"phasers\" (for \"Star Trek\"), \"blasters\" (\"Star Wars\"), \"pulse rifles\", \"plasma rifles\", and so forth.\n\nIn his book \"Physics of the Impossible\", Michio Kaku used gamma ray bursts as an evidence to illustrate that extremely powerful rayguns such as the Death Star's primary weapon in the \"Star Wars\" franchise do not violate known physical laws and theories. He further analyses the problem of rayguns' power sources.\n\nRay guns as described by science fiction do not have the disadvantages that have, so far, made directed-energy weapons largely impractical as weapons in real life, needing a suspension of disbelief by a technologically educated audience:\n\nSome of the effects are what would be expected from a powerful directed-energy beam if it could be generated in reality:\n\nBut sometimes not:\n\nUltimately, rayguns have whatever properties are required for their dramatic purpose. They bear little resemblance to real-world directed-energy weapons, even if they are given the names of existing technologies such as lasers, masers, or particle beams. This can be compared with real-type firearms as commonly depicted by action movies, as tending infallibly to hit whatever they are aimed at (when wielded by the heroes) and seldom depleting their ammunition.\n\nRayguns by their various names have various sizes and forms: pistol-like; two-handed (often called a rifle); mounted on a vehicle; artillery-sized mounted on a spaceship or space base or asteroid or planet.\n\nRayguns have a great variety of shapes and sizes, according to the imagination of the story writers or movie prop makers. Most pistol rayguns have a conventional grip and trigger, but some (e.g. \"\" phasers) do not. Sometimes the end of the barrel expands into a shield, as if to protect the user from back-flash from the emitted beam.\n\nThe ray is usually stated to be one of the following:\n\nRayguns are often one-handed, sometimes two-handed, and often artillery-sized fastened to a spaceship.\n\nRayguns powered by a backpack powerpack are described from time to time in science fiction.\n\nThe following is a list of notable rayguns.\n\n\n\n\n\n\n"}
{"id": "772834", "url": "https://en.wikipedia.org/wiki?curid=772834", "title": "Selective surface", "text": "Selective surface\n\nIn solar thermal collectors, a selective surface or selective absorber is a means of increasing its operation temperature and/or efficiency. The selectivity is defined as the ratio of solar radiation absorption (α) to thermal infrared radiation emission (ε).\n\nSelective surfaces take advantage of the differing wavelengths of incident solar radiation and the emissive radiation from the absorbing surface:\n\nNormally, a combination of materials is used. One of the first selective surfaces investigated was a semiconductor-metal tandem – simply copper with a layer of black cupric oxide. Silicon on metal is also another example. A different design has ceramic–metal composites (cermets) on metal substrates. Black chromium (\"black chrome\") and nickel-plated anodized aluminum is another selective surface that is very durable, highly resistant to humidity or oxidizing atmospheres and extreme temperatures, while being able to retain its selective properties, but expensive. One of the more popular designs – a multi-layer broadband solar absorber – consists of a metal substrate coated with multiple layers of metal and dielectric materials. While those have to be vacuum-deposited, they have been widely adopted due to their suitability for vacuum tubes.\n\nAlthough ordinary black paint has high solar absorption, it also has high thermal emissivity, and thus it is not a selective surface.\n\nTypical values for a selective surface might be 0.90 solar absorption and 0.10 thermal emissivity, but can range from 0.8/0.3 for paints on metal to 0.96/0.05 for commercial surfaces. Thermal emissivities as low as 0.02 have been obtained in laboratories.\n\nSelective surfaces are used for other applications than solar thermal collectors, such as low emissivity surfaces used in window glasses, which reflect thermal radiation and have high transmittance factors (being transparent) for visible sunlight.\n\n\n"}
{"id": "476993", "url": "https://en.wikipedia.org/wiki?curid=476993", "title": "Thermoelectric materials", "text": "Thermoelectric materials\n\nThermoelectric materials show the thermoelectric effect in a strong or convenient form.\n\nThe \"thermoelectric effect\" refers to phenomena by which either a temperature difference creates an electric potential or an electric potential creates a temperature difference. These phenomena are known more specifically as the Seebeck effect (converting temperature to current), Peltier effect (converting current to temperature), and Thomson effect (conductor heating/cooling). While all materials have a nonzero thermoelectric effect, in most materials it is too small to be useful. However, low-cost materials that have a sufficiently strong thermoelectric effect (and other required properties) could be used in applications including power generation and refrigeration. A commonly used thermoelectric material in such applications is bismuth telluride ().\n\nThermoelectric materials are used in thermoelectric systems for cooling or heating in niche applications, and are being studied as a way to regenerate electricity from waste heat.\n\nThe usefulness of a material in thermoelectric systems is determined by the two factors device efficiency and power factor. These are determined by the material's electrical conductivity, thermal conductivity, Seebeck coefficient and behavior under changing temperatures.\n\nThe efficiency of a thermoelectric device for electricity generation is given by formula_1, defined as\nThe ability of a given material to efficiently produce thermoelectric power is related to its dimensionless figure of merit given by\nwhich depends on the Seebeck coefficient \"S\", thermal conductivity \"κ\", electrical conductivity \"σ\", and temperature \"T\".\n\nIn an actual thermoelectric device, two materials are used. The maximum efficiency formula_4 is then given by\nwhere formula_6 is the temperature at the hot junction and formula_7 is the temperature at the surface being cooled. formula_8 is the modified dimensionless figure of merit, which takes into consideration the thermoelectric capacity of both thermoelectric materials being used in the device and, after geometrical optimization regarding the legs sections, is defined as\nwhere formula_10 is the electrical resistivity, formula_11 is the average temperature between the hot and cold surfaces and the subscripts n and p denote properties related to the n- and p-type semiconducting thermoelectric materials, respectively. Since thermoelectric devices are heat engines, their efficiency is limited by the Carnot efficiency, hence the formula_6 and formula_7 terms in formula_4. Regardless, the coefficient of performance of current commercial thermoelectric refrigerators ranges from 0.3 to 0.6, one-sixth the value of traditional vapor-compression refrigerators.\n\nTo determine the usefulness of a material in a thermoelectric generator or a thermoelectric cooler the power factor is calculated by its Seebeck coefficient and its electrical conductivity under a given temperature difference:\n\nwhere \"S\" is the Seebeck coefficient, and \"σ\" is the electrical conductivity.\n\nMaterials with a high power factor are able to 'generate' more energy (move more heat or extract more energy from that temperature difference) in a space-constrained application, but are not necessarily more efficient in generating this energy.\n\nFor good efficiency, materials with high electrical conductivity, low thermal conductivity and high Seebeck coefficient are needed.\n\nThe band structure of semiconductors offers better thermoelectric effects than the band structure of metals.\n\nThe Fermi energy is below the conduction band causing the state density to be asymmetric around the Fermi energy. Therefore, the average electron energy of the conduction band is higher than the Fermi energy, making the system conducive for charge motion into a lower energy state. By contrast, the Fermi energy lies in the conduction band in metals. This makes the state density symmetric about the Fermi energy so that the average conduction electron energy is close to the Fermi energy, reducing the forces pushing for charge transport. Therefore, semiconductors are ideal thermoelectric materials. Due to the small Seebeck coefficient metals have a very limited performance and the main materials of interest are Semiconductors.\n\nIn the efficiency equations above, thermal conductivity and electrical conductivity compete.\n\nThe thermal conductivity \"κ\" has mainly two components: \nAccording to the Wiedemann–Franz law, the higher the electrical conductivity, the higher \"κ\" becomes. Thus in metals the ratio of thermal to electrical conductivity is about fixed, as the electron part dominates.\nIn semiconductors, the phonon part is important and can not be neglected. It reduces the efficiency. For good efficiency a low ratio of \"κ\" / \"κ\" is desired.\n\nTherefore, it is necessary to minimize \"κ\" and keep the electrical conductivity high. Thus semiconductors should be highly doped.\n\nG. A. Slack proposed that in order to optimize the figure of merit, phonons, which are responsible for thermal conductivity must experience the material as a glass (experiencing a high degree of phonon scattering—lowering thermal conductivity) while electrons must experience it as a crystal (experiencing very little scattering—maintaining electrical conductivity). The figure of merit can be improved through the independent adjustment of these properties.\n\nThe maximum formula_8 of a material is given by the material's Quality Factor:\n\nformula_17\n\nwhere formula_18 is the Boltzmann constant, formula_19 is the reduced Planck constant, formula_20 is the number of degenerated valleys for the band, formula_21 is the average longitudinal elastic moduli, formula_22 is the inertial effective mass, formula_23 is the deformation potential coefficient, formula_24 is the lattice thermal conduction, and formula_25 is temperature. The figure of merit, formula_8, depends on doping concentration and temperature of the material of interest. The material Quality Factor: formula_27 is useful because it allows for an intrinsic comparisons of possible efficiency between different materials. This relation shows that improving the electronic component formula_28, which primarily affects the Seebeck coefficient, will increase the quality factor of a material. A large density of states can be created due to a large number of conducting bands (formula_20) or by flat bands giving a high band effective mass (formula_30). For isotropic materials formula_31. Therefore, it is desirable for thermoelectric materials to have high valley degeneracy in a very sharp band structure. Other complex features of the electronic structure are important. These can be partially quantified using an electronic fitness function.\n\nStrategies to improve thermoelectrics include both advanced bulk materials and the use of low-dimensional systems. Such approaches to reduce lattice thermal conductivity fall under three general material types: (1) Alloys: create point defects, vacancies, or rattling structures (heavy-ion species with large vibrational amplitudes contained within partially filled structural sites) to scatter phonons within the unit cell crystal; (2) Complex crystals: separate the phonon glass from the electron crystal using approaches similar to those for superconductors (the region responsible for electron transport should be an electron crystal of a high-mobility semiconductor, while the phonon glass should ideally house disordered structures and dopants without disrupting the electron crystal, analogous to the charge reservoir in high-T superconductors); (3) Multiphase nanocomposites: scatter phonons at the interfaces of nanostructured materials, be they mixed composites or thin film superlattices.\n\nMaterials under consideration for thermoelectric device applications include:\n\nMaterials such as and comprise some of the best performing room temperature thermoelectrics with a temperature-independent figure-of-merit, ZT, between 0.8 and 1.0. Nanostructuring these materials to produce a layered superlattice structure of alternating and layers produces a device within which there is good electrical conductivity but perpendicular to which thermal conductivity is poor. The result is an enhanced ZT (approximately 2.4 at room temperature for p-type). Note that this high value of ZT has not been independently confirmed due to the complicated demands on the growth of such superlattices and device fabrication; however the material ZT values are consistent with the performance of hot-spot coolers made out of these materials and validated at Intel Labs.\n\nBismuth telluride and its solid solutions are good thermoelectric materials at room temperature and therefore suitable for refrigeration applications around 300 K. The Czochralski method has been used to grow single crystalline bismuth telluride compounds. These compounds are usually obtained with directional solidification from melt or powder metallurgy processes. Materials produced with these methods have lower efficiency than single crystalline ones due to the random orientation of crystal grains, but their mechanical properties are superior and the sensitivity to structural defects and impurities is lower due to high optimal carrier concentration.\n\nThe required carrier concentration is obtained by choosing a nonstoichiometric composition, which is achieved by introducing excess bismuth or tellurium atoms to primary melt or by dopant impurities. Some possible dopants are halogens and group IV and V atoms. Due to the small bandgap (0.16 eV) BiTe is partially degenerate and the corresponding Fermi-level should be close to the conduction band minimum at room temperature. The size of the band-gap means that BiTe has high intrinsic carrier concentration. Therefore, minority carrier conduction cannot be neglected for small stoichiometric deviations. Use of telluride compounds is limited by the toxicity and rarity of tellurium.\n\nIn 2008 Joseph Heremans and his colleagues demonstrated that thallium-doped lead telluride alloy (PbTe) achieves a ZT of 1.5 at 773 K. Later, Snyder and his colleagues reported ZT~1.4 at 750 K in sodium-doped PbTe, and ZT~1.8 at 850 K in sodium-doped PbTeSe alloy. Snyder's group determined that both thallium and sodium alter the electronic structure of the crystal increasing electronic conductivity. They also claim that selenium increases electric conductivity and reduces thermal conductivity.\n\nIn 2012 another team used lead telluride to convert 15 to 20 percent of waste heat to electricity, reaching a ZT of 2.2, which they claimed was the highest yet reported.\n\nInorganic clathrates have the general formula ABC (type I) and ABC (type II), where B and C are group III and IV elements, respectively, which form the framework where “guest” A atoms (alkali or alkaline earth metal) are encapsulated in two different polyhedra facing each other. The differences between types I and II come from the number and size of voids present in their unit cells. Transport properties depend on the framework's properties, but tuning is possible by changing the “guest” atoms.\n\nThe most direct approach to synthesize and optimize the thermoelectric properties of semiconducting type I clathrates is substitutional doping, where some framework atoms are replaced with dopant atoms. In addition, powder metallurgical and crystal growth techniques have been used in clathrate synthesis. The structural and chemical properties of clathrates enable the optimization of their transport properties as a function of stoichiometry. The structure of type II materials allows a partial filling of the polyhedra, enabling better tuning of the electrical properties and therefore better control of the doping level. Partially filled variants can be synthesized as semiconducting or even insulating.\n\nBlake \"et al.\" have predicted ZT~0.5 at room temperature and ZT~1.7 at 800 K for optimized compositions. Kuznetsov \"et al.\" measured electrical resistance and Seebeck coefficient for three different type I clathrates above room temperature and by estimating high temperature thermal conductivity from the published low temperature data they obtained ZT~0.7 at 700 K for BaGaGe and ZT~0.87 at 870 K for BaGaSi.\n\nMgB (B=Si, Ge, Sn) compounds and their solid solutions are good thermoelectric materials and their ZT values are comparable with those of established materials. The appropriate production methods are based on direct co-melting, but mechanical alloying has also been used. During synthesis, magnesium losses due to evaporation and segregation of components (especially for MgSn) need to be taken into account. Directed crystallization methods can produce single crystalline material. Solid solutions and doped compounds have to be annealed in order to produce homogeneous samples - with the same properties throughout. At 800 K, MgSiSnGeBi has been reported to have a figure of merit about 1.4, the highest ever reported for these compounds.\n\nHigher silicides display ZT levels with current materials. They are mechanically and chemically strong and therefore can often be used in harsh environments without protection. Possible fabrication methods include Czochralski and floating zone for single crystals and hot pressing and sintering for polycrystalline.\n\nRecently, skutterudite materials have sparked the interest of researchers in search of new thermoelectrics. These structures are of the form and are cubic with space group Im3. Unfilled, these materials contain voids into which low-coordination ions (usually rare-earth elements) can be inserted in order to alter thermal conductivity by producing sources for lattice phonon scattering and decrease thermal conductivity due to the lattice without reducing electrical conductivity. Such qualities make these materials exhibit PGEC behavior. However, recently Khan et al. showed that it is possible to reduce the thermal conductivity without filling these voids and enhance the figure of merit by 100%, with special architecture containing nano and micro pores.\n\nSkutterudites have the chemical formula LMX, where L is a rare-earth metal, M a transition metal and X a metalloid, a group V element or pnictogen whose properties lie between those of a metal and nonmetal such as phosphorus, antimony, or arsenic. These materials could be potential in multistage thermoelectric devices as it has been shown that they have ZT>1.0, but their properties are not well known.\n\nHomologous oxide compounds (such as those of the form ()—the Ruddlesden-Popper phase) have layered superlattice structures that make them promising candidates for use in high-temperature thermoelectric devices. These materials exhibit low thermal conductivity perpendicular to the layers while maintaining good electronic conductivity within the layers. ZT values are relatively low (~0.34 at 1,000K), but their enhanced thermal stability, as compared to conventional high-ZT bismuth compounds, makes them superior for use in high-temperature applications.\n\nInterest in oxides as thermoelectric materials was reawakened in 1997 when NaCoO was found to exhibit good thermoelectric behavior. In addition to their thermal stability, other advantages of oxides are their non-toxicity and high oxidation resistance. Simultaneously controlling both the electric and phonon systems may require nanostructured materials. Some layered oxide materials are thought to have ZT~2.7 at 900 K. If the layers in a given material have the same stoichiometry, they will be stacked so that the same atoms will not be positioned on top of each other, impeding phonon conductivity perpendicular to the layers. Recently, oxide thermoelectrics have gained a lot of attention so that the range of promising phases increased drastically. Novel members of this family include ZnO, MnO, and NbO, to name but a few.\n\nHalf Heusler alloys have a great potential for high- temperature power generation applications. Half-Heusler (HH) are alloys with a Formula ABX . Examples of Half-Heusler include NbFeSb, NbCoSn and VFeSb. HH possesses cubic MgAgAs type structure, forming three interpenetrating face-centered-cubic (FCC). The ability to substitute any of these three sublattices opens the door for wide variety of HH compounds to be synthesized. A and B sites substitutions are employed to reduce the thermal conductivity, while the X site substitution is used to enhance the carrier concentration and thus the electrical conductivity. \nPreviously, ZT could not peak more than 0.5 for p-type and 0.8 for n-type HH compound. However, in the past few years, researchers were able to achieve ZT≈1 for both n-type and p-type. Nano-sized grains is one of the approaches used to lower the thermal conductivity via grain boundaries- assisted phonon scattering. Other approach was to utilize the principles of nanocomposites, by which certain combination of metals were favored on others due to the atomic size difference. For instance, Hf and Ti is more effective than Hf and Zr, when reduction of thermal conductivity is of concern, since the atomic size difference between the former is larger than that of the latter.\n\nSome electrically conducting organic materials may have a higher figure of merit than existing inorganic materials. Seebeck coefficient can be even millivolts per Kelvin but electrical conductivity is usually low, resulting in small ZT values. Quasi-one-dimensional organic crystals are formed from linear chains or stacks of molecules that are packed into a 3D crystal. Under certain conditions some Q1D organic crystals may have ZT~20 at room temperature for both p- and n-type materials. This has been credited to an unspecified interference between two main electron-phonon interactions leading to the formation of narrow strip of states in the conduction band with a significantly reduced scattering rate as the mechanism compensate each other, yielding high ZT.\n\nSilicon-germanium alloys are currently the best thermoelectric materials around 1000 ℃ and are therefore used in some radioisotope thermoelectric generators (RTG) (notably the MHW-RTG and GPHS-RTG) and some other high temperature applications, such as waste heat recovery. Usability of silicon-germanium alloys is limited by their price and mid-range ZT (~0.7).\n\nExperiments on crystals of sodium cobaltate, using X-ray and neutron scattering experiments carried out at the European Synchrotron Radiation Facility (ESRF) and the Institut Laue-Langevin (ILL) in Grenoble were able to suppress thermal conductivity by a factor of six compared to vacancy-free sodium cobaltate. The experiments agreed with corresponding density functional calculations. The technique involved large anharmonic displacements of contained within the crystals.\n\nIn 2002, Nolas and Goldsmid have come up with a suggestion that systems with the phonon mean free path larger than the charge carrier mean free path can exhibit an enhanced thermoelectric efficiency. This can be realized in amorphous thermoelectrics and soon they became a focus of many studies. This ground-breaking idea was accomplished in Cu-Ge-Te, NbO, In-Ga-Zn-O, Zr-Ni-Sn, Si-Au, and Ti-Pb-V-O amorphous systems. It should be mentioned that modelling of transport properties is challenging enough without breaking the long-range order so that design of amorphous thermoelectrics is at its infancy. Naturally, amorphous thermoelectrics give rise to extensive phonon scattering, which is still a challenge for crystalline thermoelectrics. A bright future is expected for these materials.\n\nFunctionally graded materials make it possible to improve the conversion efficiency of existing thermoelectrics. These materials have a non-uniform carrier concentration distribution and in some cases also solid solution composition. In power generation applications the temperature difference can be several hundred degrees and therefore devices made from homogeneous materials have some part that operates at the temperature where ZT is substantially lower than its maximum value. This problem can be solved by using materials whose transport properties vary along their length thus enabling substantial improvements to the operating efficiency over large temperature differences. This is possible with functionally graded materials as they have a variable carrier concentration along the length of the material, which is optimized for operations over specific temperature range.\n\nIn addition to nanostructured / superlattice thin films, other nanostructured materials, including nanowires, nanotubes and quantum dots show potential in improving thermoelectric properties.\n\nAnother example of a superlattice involves a PbTe/PbSeTe quantum dot superlattices provides an enhanced ZT (approximately 1.5 at room temperature) that was higher than the bulk ZT value for either PbTe or PbSeTe (approximately 0.5).\n\nNot all nanocrystalline materials are stable, because the crystal size can grow at high temperatures, ruining the materials' desired characteristics.\n\nNanocrystalline materials have many interfaces between crystals, which Physics of SASER scatter phonons so the thermal conductivity is reduced. Phonons are confined to the grain, if their mean free path is larger than the material grain size.\n\nMeasured lattice thermal conductivity in \"nanowires\" is known to depend on roughness, the method of synthesis and properties of the source material.\n\nNanocrystalline transition metal silicides are a promising material group for thermoelectric applications, because they fulfill several criteria that are demanded from the commercial applications point of view. In some nanocrystalline transition metal silicides the power factor is higher than in the corresponding polycrystalline material but the lack of reliable data on thermal conductivity prevents the evaluation of their thermoelectric efficiency.\n\nSkutterudites, a cobalt arsenide mineral with variable amounts of nickel and iron, can be produced artificially, and are candidates for better thermoelectric materials.\nOne advantage of nanostructured skutterudites over normal skutterudites is their reduced thermal conductivity, caused by grain boundary scattering. ZT values of ~0.65 and > 0.4 have been achieved with CoSb based samples; the former values were 2.0 for Ni and 0.75 for Te-doped material at 680 K and latter for Au-composite at .\n\nEven greater performance improvements can be achieved by using composites and by controlling the grain size, the compaction conditions of polycrystalline samples and the carrier concentration.\n\nGraphene is known for its high electrical conductivity and Seebeck coefficient at room temperature. However, from thermoelectric perspective, its thermal conductivity is notably high, which in turn limits its ZT. Several approaches were suggested to reduce the thermal conductivity of graphene without altering much its electrical conductivity. These include, but not limited to, the following:\n\n1- Doping with carbon isotopes to form isotopic heterojunction such as that of 12C and 13C. Basically, those isotopes possess different phonon frequency mismatch, which ultimately lead to the scattering of the heat carriers (the phonons). Fortunately, this approach has been shown to not affecting neither the power factor nor the electrical conductivity.\n\n2- Wrinkles and cracks in the graphene structure were shown to contribute to the reduction in the thermal conductivity. Reported values of thermal conductivity of suspended graphene of size 3.8 µm show a wide spread from 1500 to 5000 W/mK. A recent study attributed that to the microstructural defects present in graphene, such as wrinkles and cracks, which can drop the thermal conductivity by 27%. These defects help scatter phonons.\n\n3- Introduction of defects with techniques such as oxygen plasma treatment. A more systemic way of introducing defects in graphene structure is done through O plasma treatment. Ultimately, the graphene sample will contain prescribed-holes spaced and numbered according to the plasma intensity. People were able to improve ZT of graphene from 1 to a value of 2.6 when the defect density is raised from 0.04 to 2.5 (this number is an index of defect density and usually understood when compared to the corresponding value of the un-treated graphene, 0.04 in our case). Nevertheless, this technique would lower the electrical conductivity as well, which can be kept unchanged if the plasma processing parameters are optimized.\n\n4- Functionalization of graphene by oxygen. The thermal behavior of graphene oxide has not been investigated extensively as compared to its counterpart; graphene. However, it was shown theoretically by Density Functional Theory (DFT) model that adding oxygen into the lattice of graphene reduces a lot its thermal conductivity due to phonon scattering effect. Scattering of phonons result from both acoustic mismatch and reduced symmetry in graphene structure after doping with oxygen. The reduction of thermal conductivity can easily exceed 50% with this approach.\n\nSuperlattices - nano structured thermocouples, are considered a good candidate for better thermoelectric device manufacturing, with materials that can be used in manufacturing this structure.\n\nTheir production is expensive for general-use due to fabrication processes based on expensive thin-film growth methods. However, since the amount of thin-film materials required for device fabrication with superlattices, is so much less than thin-film materials in bulk thermoelectric materials (almost by a factor of 1/10,000) the long-term cost advantage is indeed favorable.\n\nThis is particularly true given the limited availability of tellurium causing competing solar applications for thermoelectric coupling systems to rise.\n\nSuperlattice structures also allow the independent manipulation of transport parameters by adjusting the structure itself, enabling research for better understanding of the thermoelectric phenomena in nanoscale, and studying the phonon-blocking electron-transmitting structures - explaining the changes in electric field and conductivity due to the material's nano-structure.\n\nMany strategies exist to decrease the superlattice thermal conductivity that are based on engineering of phonon transport. The thermal conductivity along the film plane and wire axis can be reduced by creating diffuse interface scattering and by reducing the interface separation distance, both which are caused by interface roughness.\n\nInterface roughness can naturally occur or may be artificially induced.\n\nIn nature, roughness is caused by the mixing of atoms of foreign elements. Artificial roughness can be created using various structure types, such as quantum dot interfaces and thin-films on step-covered substrates.\n\nReduced electrical conductivity: \nReduced phonon-scattering interface structures often also exhibit a decrease in electrical conductivity.\n\nThe thermal conductivity in the cross-plane direction of the lattice is usually very low, but depending on the type of superlattice, the thermoelectric coefficient may increase because of changes to the band structure.\n\nLow thermal conductivity in superlattices is usually due to strong interface scattering of phonons. Minibands are caused by the lack of quantum confinement within a well. The mini-band structure depends on the superlattice period so that with a very short period (~1 nm) the band structure approaches the alloy limit and with a long period (≥ ~60 nm) minibands become so close to each other that they can be approximated with a continuum.\n\nSuperlatice structure countermeasures: \nCounter measures can be taken which practically eliminate the problem of decreased electrical conductivity in a reduced phonon-scattering interface. These measures include the proper choice of superlattice structure, taking advantage of mini-band conduction across superlattices, and avoiding quantum-confinement. It has been shown that because electrons and phonons have different wavelengths, it is possible to engineer the structure in such a way that phonons are scattered more diffusely at the interface than electrons.\n\nPhonon confinement countermeasures: \nAnother approach to overcome the decrease in electrical conductivity in reduced phonon-scattering structures is to increase phonon reflectivity and therefore decrease the thermal conductivity perpendicular to the interfaces.\n\nThis can be achieved by increasing the mismatch between the materials in adjacent layers, including density, group velocity, specific heat, and the phonon-spectrum.\n\nInterface roughness causes diffuse phonon scattering, which either increases or decreases the phonon reflectivity at the interfaces. A mismatch between bulk dispersion relations confines phonons, and the confinement becomes more favorable as the difference in dispersion increases.\n\nThe amount of confinement is currently unknown as only some models and experimental data exist. As with a previous method, the effects on the electrical conductivity have to be considered.\n\nAttempts to Localize long wavelength phonons by aperiodic superlattices or composite superlattices with different periodicities have been made. In addition, defects, especially dislocations, can be used to reduce thermal conductivity in low dimensional systems.\n\nParasitic heat: \nParasitic heat conduction in the barrier layers could cause significant performance loss. It has been proposed but not tested that this can be overcome by choosing a certain correct distance between the quantum wells.\nThe Seebeck coefficient can change its sign in superlattice nanowires due to the existence of minigaps as Fermi energy varies. This indicates that superlattices can be tailored to exhibit n or p-type behavior by using the same dopants as those that are used for corresponding bulk materials by carefully controlling Fermi energy or the dopant concentration. With nanowire arrays, it is possible to exploit semimetal-semiconductor transition due to the quantum confinement and use materials that normally would not be good thermoelectric materials in bulk form. Such elements are for example bismuth. The Seebeck effect could also be used to determine the carrier concentration and Fermi energy in nanowires.\n\nIn quantum dot thermoelectrics, unconventional or nonband transport behavior (e.g. tunneling or hopping) is necessary to utilize their special electronic band structure in the transport direction. It is possible to achieve ZT>2 at elevated temperatures with quantum dot superlattices, but they are almost always unsuitable for mass production.\n\nHowever, in superlattices, where quantum-effects are not involved, with film thickness of only a few micrometers (µm) to about 15 µm, BiTe/SbTe superlattice material has been made into high-performance microcoolers and other devices. The performance of hot-spot coolers are consistent with the reported ZT~2.4 of superlattice materials at 300 K.\n\nNanocomposites are promising material class for bulk thermoelectric devices, but several challenges have to be overcome to make them suitable for practical applications. It is not well understood why the improved thermoelectric properties appear only in certain materials with specific fabrication processes.\n\nSrTe nanocrystals can be embedded in a bulk PbTe matrix so that rocksalt lattices of both materials are completely aligned (endotaxy) with optimal molar concentration for SrTe only 2%. This can cause strong phonon scattering but would not affect charge transport. In such case, ZT~1.7 can be achieved at 815 K for p-type material.\n\nIn 2014, researchers at Northwestern University discovered that tin selenide (SnSe) has a ZT of 2.6 along the b axis of the unit cell. This is the highest value reported to date. This high ZT figure of merit has been attributed to an extremely low thermal conductivity found in the SnSe lattice. Specifically, SnSe demonstrated a lattice thermal conductivity of 0.23 W·m·K, which is much lower than previously reported values of 0.5 W·m·K and greater.\nThis SnSe material also exhibited a ZT of along the c-axis and along the a-axis. These excellent figures of merit were obtained by researchers working at elevated temperatures, specifically . As shown by the figures below, SnSe performance metrics were found to significantly improve at higher temperatures; this is due to a structural change that is discussed below. Power factor, conductivity, and thermal conductivity all reach their optimal values at or above 750 K, and appear to plateau at higher temperatures. However, these reports have become controversial as reported in Nature because other groups have not been able to reproduce the reported bulk thermal conductivity data.\n\nAlthough it exists at room temperature in an orthorhombic structure with space group Pnma, SnSe has been shown to undergo a transition to a structure with higher symmetry, space group Cmcm, at higher temperatures. This structure consists of Sn-Se planes that are stacked upwards in the a-direction, which accounts for the poor performance out-of-plane (along a-axis). Upon transitioning to the Cmcm structure, SnSe maintains its low thermal conductivity but exhibits higher carrier mobilities, leading to its excellent ZT value.\n\nOne particular impediment to further development of SnSe is that it has a relatively low carrier concentration: approximately 10 cm. Further compounding this issue is the fact that SnSe has been reported to have low doping efficiency.\nHowever, such single crystalline materials suffer from inability to make useful devices due to their brittleness as well as narrow range of temperatures, where ZT is reported to be high. Further, polycrystalline materials made out of these compounds by several investigators have not confirmed the high ZT of these materials.\n\nProduction methods for these materials can be divided into powder and crystal growth based techniques. Powder based techniques offer excellent ability to control and maintain desired carrier distribution. In crystal growth techniques dopants are often mixed with melt, but diffusion from gaseous phase can also be used. In the zone melting techniques disks of different materials are stacked on top of others and then materials are mixed with each other when a traveling heater causes melting. In powder techniques, either different powders are mixed with a varying ratio before melting or they are in different layers as a stack before pressing and melting.\n\nThere are applications, such as cooling of electronic circuits, where thin films are required. Therefore, thermoelectric materials can also be synthesized using physical vapor deposition techniques. Another reason to utilize these methods is to design these phases and provide guidance for bulk applications.\n\nThermoelectric materials can be used as refrigerators, called \"thermoelectric coolers\", or \"Peltier coolers\" after the Peltier effect that controls their operation. As a refrigeration technology, Peltier cooling is far less common than vapor-compression refrigeration. The main advantages of a Peltier cooler (compared to a vapor-compression refrigerator) are its lack of moving parts or refrigerant, and its small size and flexible shape (form factor).\n\nThe main \"disadvantage\" of Peltier coolers is low efficiency. It is estimated that materials with ZT>3 (about 20–30% Carnot efficiency) would be required to replace traditional coolers in most applications. Today, Peltier coolers are only used in niche applications, especially small scale, where efficiency is not important.\n\nThermoelectric efficiency depends on the \"figure of merit\", ZT. There is no theoretical upper limit to ZT, and as ZT approaches infinity, the thermoelectric efficiency approaches the Carnot limit. However, no known thermoelectrics have a ZT>3. As of 2010, thermoelectric generators serve application niches where efficiency and cost are less important than reliability, light weight, and small size.\n\nInternal combustion engines capture 20–25% of the energy released during fuel combustion. Increasing the conversion rate can increase mileage and provide more electricity for on-board controls and creature comforts (stability controls, telematics, navigation systems, electronic braking, etc.) It may be possible to shift energy draw from the engine (in certain cases) to the electrical load in the car, e.g. electrical power steering or electrical coolant pump operation.\n\nCogeneration power plants use the heat produced during electricity generation for alternative purposes. Thermoelectrics may find applications in such systems or in solar thermal energy generation.\n\n\n\n"}
{"id": "6169592", "url": "https://en.wikipedia.org/wiki?curid=6169592", "title": "Unimorph", "text": "Unimorph\n\nA unimorph or monomorph is a cantilever that consists of one active layer and one inactive layer. In the case where active layer is piezoelectric, deformation in that layer may be induced by the application of an electric field. This deformation induces a bending displacement in the cantilever. The inactive layer may be fabricated from a non-piezoelectric material.\n\n"}
{"id": "24222642", "url": "https://en.wikipedia.org/wiki?curid=24222642", "title": "Upstream (newspaper)", "text": "Upstream (newspaper)\n\nUpstream is an independent oil and gas industry upstream sector weekly newspaper and a daily internet news site. The newspaper is owned by NHST Media Group. It is headquartered in Oslo, Norway. The newspaper covers the upstream sector of the global oil and gas industry with full-time staff correspondents in all the major centres of the industry. It is published every Friday. Upstream had full-time reporters based in its head office in Oslo, as well as bureaux and correspondents in London, Moscow, Accra, New Delhi, Singapore, Wellington, Rio de Janeiro and Houston. Its editor in chief is Erik Means.\n\nThe newspaper was founded in 1996 to compete with well-established rivals including \"Oil & Gas Journal\", \"Petroleum Intelligence Weekly\", and \"Offshore Engineer\". It covers all aspects of the upstream industry, but focuses especially on news related to business, policy and the sector's key players as well as the commercial side of the industry. Coverage includes exploration, field development, contracts, company news, technological developments and the liquefied natural gas sector, as well as political and financial news which affects the exploration and production sector.\n\nIn 2001, Upstream added an online edition, upstreamonline.com, which is updated 24 hours a day, five days a week from London, Oslo, Houston and Perth. It provides breaking news and online feature articles written by a dedicated online editorial team, as well as giving access to digital versions of the newspaper in various formats.\n"}
{"id": "31493512", "url": "https://en.wikipedia.org/wiki?curid=31493512", "title": "Ursula Sladek", "text": "Ursula Sladek\n\nUrsula Sladek (1946-09-06 ) owns a small local power company, Schönau Power Supply, located in Schönau im Schwarzwald, Germany, that provides electricity from renewable energy sources to the German electricity grid. Her company \"gets much of its energy from small local energy producers, including a handful of hydropower operations, solar panels, some wind turbines, and about 20 washing-machine-size co-generation plants in people’s homes that produce both heat for the home and electricity for the grid\". Sladek has also been interested in finding ways of rendering nuclear power unnecessary in Germany: Sladek won a Goldman Environmental Prize in 2011.\n\nIn 1986, she was a homemaker and the mother of five school-age children when some radioactive isotopes blown into the air by the explosion at the Chernobyl nuclear power plant in the Ukraine landed around her town, Schönau, in the Black Forest in western Germany. Her children could not play outdoors for two weeks; 25 years later, that forest’s mushrooms are still considered unsafe. \nTrained as a schoolteacher, Sladek began to study the energy industry in Germany to see if there were ways to decrease dependence on nuclear power. Together with her husband, Michael Sladek, she formed a group called \"Parents for a Nuclear Free Future\" to promote energy efficiency in the Black Forest region of Germany, and return control of energy production and distribution to the community. In 1991, when the previous power company's lease to supply power to the Schönau region was up for renewal, Sladek and her partners began a nationwide fundraising effort to enable them to take ownership of the local power grid. They were able to raise 6 million DM (about 3 million Euros) and by 1997 had established the Schönau Power Supply as a community operated energy provider committed to a sustainabie energy future. The Schönau Power Supply uses a decentralized approach to power generation, and makes use of renewable energy sources, including solar, hydroelectric, wind power, and biomass. The company is operated as a cooperative; while the cooperative owners receive dividends, the majority of the profits are re-invested in renewable energy sources. Total revenues reached 67 million Euros in 2009\n\nUrsula Sladek has won many awards for her work in the fields of energy conservation and renewable energy production, including the German Federal Cross of Merit, the Henry Ford European Conservation Award, the German Founder of the Year Award, the International Nuclear-Free Future Award, the German Energy Prize, and the European Solar Prize, and was elected an Ashoka Fellow in 2008. Sladek won a Goldman Environmental Prize in 2011.\n\n"}
{"id": "19277487", "url": "https://en.wikipedia.org/wiki?curid=19277487", "title": "Victoria County Station", "text": "Victoria County Station\n\nThe Victoria County Station was a proposed nuclear power plant, in Victoria County, 13.3 miles south of Victoria, Texas. The plant, consisting of two 1535 MWe General-Electric-Hitachi economic simplified boiling water reactors was applied for by the Exelon Nuclear Texas Holdings, LLC on September 2, 2008. The project was canceled in August 2012.\n\nExelon filed a Combined Construction and Operating License (COL) application for the plant on September 3, 2008 with the Nuclear Regulatory Commission (NRC). In July 2009, Exelon announced that it was suspending its COL application. In March 2010, Exelon announced that it was formally withdrawing its COL application, while submitting its application for an early site permit (ESP).\n\nOn August 28, 2012, Exelon announced that it had notified the Nuclear Regulatory Commission that it was withdrawing its ESP application, which brought to an end all project activity.\n\n"}
{"id": "5928405", "url": "https://en.wikipedia.org/wiki?curid=5928405", "title": "Voices of the Apalachicola", "text": "Voices of the Apalachicola\n\nVoices of the Apalachicola is a book by Faith Eidse chronicling the history of the Apalachicola River in Northern Florida, United States. It was the winner of the 2007 Samuel Proctor Award.\n\nIts release of publication in 2006 coincided with increased awareness of the Florida-Georgia dispute over use of the river's resources, and the dying way of life of oyster fisherman in the Apalachicola River Basin, both of which were covered extensively by local press. The book contains oral histories of people who have worked and lived by the river, and builds a history of the culture and environment of the region, which is often regarded as one of the most endangered river systems in the United States.\n\n\"Voices of the Apalachicola\" () is published by University Press of Florida, 352 pages.\n"}
{"id": "7540036", "url": "https://en.wikipedia.org/wiki?curid=7540036", "title": "Waterberg Biosphere", "text": "Waterberg Biosphere\n\nThe Waterberg () is a mountainous massif of approximately in north Limpopo Province, South Africa. \nThe average height of the mountain range is 600 m with a few peaks rising up to 2000 m above sea level. Vaalwater town is located just north of the mountain range. The extensive rock formation was shaped by hundreds of millions of years of riverine erosion to yield diverse bluff and butte landform. The ecosystem can be characterised as a dry deciduous forest or Bushveld. Within the Waterberg there are archaeological finds dating to the Stone Age, and nearby are early evolutionary finds related to the origin of humans.\n\nWaterberg (Thaba Meetse) is the first region in the northern part of South Africa to be named as a Biosphere Reserve by UNESCO.\n\nThe underlying rock formation derives from the Kaapvaal Craton, formed as a precursor island roughly 2.7 billion years ago. This crustal formation became the base of the Waterberg, which was further transformed by upward extrusion of igneous rocks. These extruded rocks, containing minerals such as vanadium and platinum, are called the Bushveld Igneous Complex. The original extent of this rock upthrust involved about 250,000 square kilometers, and is sometimes called the Waterberg Supergroup.\n\nSedimentary deposition from rivers cutting through Waterberg endured until roughly 1.5 billion years ago. In more recent time (around 250 million years ago) the Kaapvaal craton collided with the supercontinent Gondwana, and split Gondwana into its modern-day continents. Waterberg today contains mesas, buttes and some kopje outcrops. Some of cliffs stand up to 550 meters above the plains, with exposed multi-coloured sandstone.\n\nThe sandstone formations could retain groundwater sufficient to make a suitable environment for primitive man. The cliff overhangs offered natural shelters for these early humans. The first human ancestors may have been at Waterberg as early as three million years ago, since Makapansgat, 40 kilometers distant, has yielded skeletons of \"Australopithecus africanus\". Hogan suggests that Homo erectus, whose evidence remains were also discovered in Makapansgat, \"may have purposefully moved into the higher areas of the Waterberg for summer (December to March) game\".\n\nBushmen entered Waterberg around two thousand years ago. They produced rock paintings at Lapalala within the Waterberg, including depictions of rhinoceros and antelope. Early Iron Age settlers in Waterberg were Bantu, who had brought cattle to the region. The Bantu created a problem in Waterberg, since cattle reduced grassland caused invasion of brush species leading to an outbreak of the tse-tse fly. The ensuing epidemic of sleeping sickness depopulated the plains, but at higher elevations man survived, because the fly cannot survive above 600 meters.\n\nLater people left the first Stone Age artifacts recovered in northern South Africa. Starting about the year 1300 AD, Nguni settlers arrived with new technologies, including the ability to build dry-stone walls, which techniques were then used to add defensive works to their Iron Age forts, some of which walls survive to today. Archaeologists continue to excavate Waterberg to shed light on the Nguni culture and the associated dry-stone architecture.\n\nThe first white settlers arrived in Waterberg in 1808 and the first naturalist a Swede appeared just before mid 19th century. Around the mid 19th century, a group of Dutch travelers set out from Cape Town in search of Jerusalem. Arriving in Waterberg, they mis-estimated their distance and thought they had reached Egypt.\n\nAfter battles between Dutch settlers and tribesmen, the races co-existed until around 1900. The Dutch brought further cattle grazing, multiplying the impacts of indigenous tribes. By the beginning of the 20th century there were an estimated 200 western inhabitants of the Waterberg, and grassland loss began to have a severe impact upon native wildlife populations.\n\nThere are several sub-habitats within the Waterberg Biosphere, which is fundamentally a dry deciduous forest; according to Hogan: \"These sub-habitats include \"high plateau savanna\", \"specialized shaded cliff vegetation system\" and \"riparian zone habitat\" with associated marshes\".\n\nThe savanna consists of rolling grasslands and a semi-deciduous forest, with trees such as mountain syringa, silver cluster-leaf and lavender tree. The canopy is mostly leafless during the dry winter. Native grasses include signal grass, goose grass and heather-topped grass. Indigenous grasses provide graze to support native species including impala, kudu, klipspringer and blue wildebeest. Some Pachypodium habitats are found especially in isolated kopje formations.\n\nOther indigenous mammals include giraffe, white rhinoceros and warthog. Snakes include the black mamba and spitting cobra. In 1905 Eugene Marais studied these snakes of the Waterberg. Some birds seen are the black-headed oriole and the white-backed vulture. Predators include the leopard, hyena and lion.\n\nVegetative cliff habitats are abundant in the Waterberg due to the extensive historic riverine erosion. The African porcupine uses the protection of these cliffside caves. Some trees cling to the cliff areas, including the paperbark false-thorn, that have flaking bark hanging from their thick trunks. Another tree in this habitat is the fever tree, thought by Bushmen to have special power to allow communication with the dead. It is found on cliffs above the Palala River including one site used for prehistoric ceremonies, which is also a location of some intact rock paintings.\n\nRiparian zones are associated with various rivers that cut through Waterberg. These surface waters all drain to the Limpopo River which flows easterly to discharge into the Indian Ocean. The river bushwillow is a riparian tree in this habitat. These riparian zones offer habitat for birds, reptiles and mammals that require more water than plateau species. The riverine areas are inhabited by the apex predator Nile crocodile and the hippopotamus. These wet habitats have reduced numbers of water-living insects, and the Waterberg is thus considered an almost malaria-free region.\n\nAs of 2006 about 80,000 people lived on the Waterberg plateau, which is part of the Bushveld district of Limpopo Province of South Africa. After cattle grazing wrought ecological havoc in the mid 1900s, the land owners of the region became aware of the benefits of restoring habitat to attract and protect the original species of antelope, white rhino, giraffe, hippopotami, and other species whose numbers dropped in the era of intense cattle grazing.\n\nThe rise in eco-tourism has stimulated interest in soil conservation practices to restore original grass species to the Waterberg. The land management practises required are expensive, but repay the landowner with added value in wildlife habitat. There is also a trend for larger farms and open space areas with the resultant advantage of fence removal. This outcome not only benefits large mammal migration, but yields an improved gene pool.\n\nThe Welgevonden Game Reserve covers 37,500 hectares of the plateau. The Kololo Game Reserve covers 3000 hectares, part of which is in the Welgevonden Private Game Reserve, and part of which is completely protected.\n\n\n"}
{"id": "4502637", "url": "https://en.wikipedia.org/wiki?curid=4502637", "title": "West Coast lumber trade", "text": "West Coast lumber trade\n\nThe West Coast lumber trade was a maritime trade route on the West Coast of the United States. It carried lumber from the coasts of Northern California, Oregon, and Washington mainly to the port of San Francisco. The trade included direct foreign shipment from ports of the Pacific Northwest and might include another product characteristic of the region, salmon, as in the schooner \"Henry Wilson\" sailing from Washington state for Australia with \"around 500,000 feet of lumber and canned salmon\" in 1918.\n\nThe trade was instrumental in founding shipping empires such as the Dollar Steamship Company in which its founder, Captain Robert Dollar, emigrated from Scotland, worked in the lumber camps of Canada and, after moving to San Francisco in 1888 and buying timber tracts, founded a shipping line that extended to China.\n\nAs late as the California Gold Rush, New England lumber was still carried 13,000 miles around Cape Horn to San Francisco. But that started to change when Captain Stephen Smith (of the bark \"George Henry\") established one of the first west coast lumber mill in a redwood forest near Bodega, California, in 1843. The first lumber mill on the west coast was established by John B. R. Cooper in Rancho El Molino near present-day Forestville, California. By the mid-1880s, more than 400 such mills operated within the forests of California's Humboldt County and along the shores of Humboldt Bay alone.\n\nAt first, the lumber was shipped in old square-riggers, but these aging ships were inefficient as they required a large crew to operate and were hard to load. Soon local shipyards opened to supply specialist vessels. In 1865 Hans Ditlev Bendixsen opened one of these yards at Fairhaven, California on Humboldt Bay adjacent to Eureka. Bendixsen built many vessels for the lumber trade, including the \"C.A. Thayer\", now preserved at the San Francisco Maritime National Historical Park. He constructed 92 sailing vessels between 1869 and 1901, including 35 three-masters.\n\nThe lumber schooners were built of the same Douglas fir as the planks they carried. (Schooner \"Oregon Pine\" was named after the tree.) They had shallow drafts for crossing coastal bars, uncluttered deck arrangements for ease of loading, and were especially handy for maneuvering into the tiny, Northern California ports. Many West Coast lumber schooners were also rigged without topsails, a configuration referred to as being \"baldheaded\". This rig simplified tacking into the strong westerlies when bound north. Crews liked baldheaders because no topmast meant no climbing aloft to shift or furl the sails. If more sail was desired then it could be set by being hoisted from the deck.\n\nThe demands of navigating the Redwood Coast, however, and a boom in the lumber industry in the 1860s called for the development of handy two-masted schooners able to operate in the tiny dog-hole ports that served the sawmills. Many sites along this stretch of coast utilized chutes and wire trapeze rigging to load the small coastal schooners with lumber. Most of these ports were so small they were called dog-hole ports—since they supposedly were just big enough to allow a dog to get in and out. Dozens of these were built, and almost any small cove or river outlet was a prime candidate for a chute. Each dog-hole was unique, which was why schooner captains often sailed back and forth to the same ports to load. The mariners were often forced to load right among the rocks and cliffs in the treacherous surf.\n\nThe schooner rig dominated the lumber trade, since its fore-and-aft rigging permitted sailing closer to the wind, easier entry to small ports, and smaller crews than square-rigged vessels. These ships needed to return to the lumber ports without the expense of loading ballast. Shipyards built some smaller schooners with centerboards that retracted. This helped the flat-bottomed vessels to enter shallow water.\n\nAt the time of the construction of the barque \"Hesperus\" in 1882, Jackson writes, \"the form of the West Coast lumber vessels had become well established and were a radical departure from the New England built ships.\" Because lumber is a bulk cargo that does not require shelter, and is difficult to stow below decks, lumber ships from yards such as the Hall Brothers in Port Blakely, Washington were built without the between decks of the New England \"Downeasters.\" \"Close to half of their cargo was stowed as deckload – that is above deck.\"\n\nJackson also writes that a triangle trade had developed at this time, with \"lumber out to Australia, coal to Hawaii, and sugar to San Francisco. It should be noted that the return cargoes were compact and heavy, thus no need for the conventional deep hull form.\".\n\nRecently, evidence of the local trade in Northern California was unearthed when a historic oven used in Fort Bragg from 1909 until 2003 was discovered to be built with \"hundreds of century-old bricks, many stamped with the name of the California brick factory from which they had come: Richmond, Stockton and Corona.\" Press coverage states that \"these bricks had come north from San Francisco as ballast on lumber ships. In the years after the 1906 quake, Fort Bragg sent tons of timber to the city to be used in rebuilding. Coming home, the ships used bricks from Bay Area factories for weight and for new construction in Fort Bragg.\"\n\nEventually, however, steam-powered vessels proved more dependable than sail, and railroads gained greater penetration of the coastal regions. Sailing vessels continued to compete with steamships and railroads well into the 20th century, but the last purpose-built sailing lumber schooner was launched in 1905.\n\nSoon steam schooners (wooden but powered) replaced the small two-masters in the dog-hole trade and larger schooners, such as the still existing \"C.A. Thayer\" and the \"Wawona\", were built for longer voyages and bigger cargo. West Coast shipyards continued to build sail-rigged lumber schooners until 1905 and wooden steam schooners until 1923. In 1907 observers noted the increase in size of schooners. The first three-masted schooner built on the Coast was launched in 1875. It was also the first lumber schooner to exceed 300 tons. Ship wrights built the first four-master in 1886 and the first five-master in 1896. The later were more generally involved in the overseas trade. Sail schooners grew from fifty to 1,100 tons during this period. More than 50 major shipbuilders operated on the Pacific Coast during the era of the coast wise schooners. Demand for coastwise lumber shipping continued until after the First World War and total lumber transported by the railroads did not exceed its seaborne competition until about 1905. Even in the 1870s mills shipped lumber directly from some dog-holes to Asia and South America.\nThe last wooden steam lumber schooner built was constructed by Matthews Shipbuilding, Hoquiam, Washington in 1923 for the A. B. Johnson Lumber Company. \"Esther Johnson\" was of wooden construction with planking of three inch Douglas fir, long by and with draft. On 29 March 1943 the ship was purchased by the War Shipping Administration and by June had arrived in Australia to become part of the U.S. Army's Southwest Pacific Area fleet as X-9. \"Esther Johnson\" arrived in Milne Bay on 4 October 1943 and, capable of transporting wooden piles sufficient to build an entire pier, was instrumental in building piers at the bases at Lae, Finschhafen in New Guinea and Tacloban in the Philippines. The ship was bombed on arrival at Lae and both bombed and strafed at Tacloban and at war's end was badly damaged by shipworms. The badly leaking ship returned from Manila to Melbourne for repairs and then returned to the Philippines going into the reserve fleet on 20 December 1947 at Subic Bay before being sold to the Philippine government on 23 February 1948. The older and slightly larger \"Barbara C\", built as \"Pacific\", also served in the Southwest Pacific in the same role.\n\n\n\n"}
{"id": "1838622", "url": "https://en.wikipedia.org/wiki?curid=1838622", "title": "World Environment Day", "text": "World Environment Day\n\nWorld Environment Day (WED) is celebrated on the 5th of June every year, and is the United Nation's principal vehicle for encouraging awareness and action for the protection of our environment. First held in 1974, it has been a flagship campaign for raising awareness on emerging environmental issues from marine pollution, human overpopulation, and global warming, to sustainable consumption and wildlife crime. WED has grown to become a global platform for public outreach, with participation from over 143 countries annually. Each year, WED has a new theme that major corporations, NGOs, communities, governments and celebrities worldwide adopt to advocate environmental causes.\n\nWorld Environment Day [WED] was established by the UN General Assembly in 1972 on the first day of the Stockholm Conference on the Human Environment, resulting from discussions on the integration of human interactions and the environment. Two years later, in 1974 the first WED was held with the theme \"Only One Earth\". Even though WED celebration have been held annually since 1974, in 1987 the idea for rotating the center of these activities through selecting different host countries began.\n\nFor almost five decades, World Environment Day has been raising awareness, supporting action, and driving change. Here is a timeline of key accomplishments in WEDs’ history:\nThe theme for 2018 is \"Beat Plastic Pollution\". The host nation is India. By choosing this Theme, it is aimed that people may strive to change their everyday lives to reduce the heavy burden of plastic pollution. People should be free from the over-reliance on single-use or disposables, as they have severe environmental consequences. We should liberate our natural places, our wildlife - and our own health from plastics. Indian government pledged to eliminate all single use of plastic in India by 2022.\n\nThe theme for 2017 was 'Connecting People to Nature – in the city and on the land, from the poles to the equator'. The host nation was Canada.\n\nThe 2016 WED was organized under the theme \"Go wild for life\". This edition of the WED aims to reduce and prevent the illegal trade in wildlife. Angola was chosen as the host country of the 2016 WED during the COP21 in Paris.\n\nThe Slogan of the 2015 edition of the World Environment Day is \"Seven Billion Dreams. One Planet. Consume with Care\". The slogan was picked through a voting process on social media. In Saudi Arabia, 15 women recycled 2000 plastic bags to crochet a mural in support of the WED 2015. In India, Narendra Modi planted a Kadamb sapling to celebrate the World Environment Day and raise awareness for Environment. Italy is the host country of the 43rd edition of the WED. The celebrations took place as part of Milan Expo around the theme: Feeding the Planet - Energy for Life. \n\nThe Theme of the 2014 WED is : International Year of Small Islands Developing States (SIDS). By choosing this Theme the UN General Assembly aimed to highlight the development Challenges and successes of the SIDS. In 2014, the World Environment Day focused on global warming and its impact on ocean levels. The Slogan of the WED 2014 is \"Raise your voice not the sea level\", as Barbados hosted the global celebrations of the 42nd edition of the World Environment Day. The UN Environement Programme named actor Ian Somerhalder as the official Goodwill ambassador of the WED 2014.\n\nThe 2013 theme for World Environment Day was \"Think.Eat.Save.\"\n\nThe campaign addressed the huge annual wastage and losses in food, which, if conserved, would release a large quantity of food as well as reduce the overall carbon footprint. The campaign aimed to bring about awareness in countries with lifestyles resulting in food wastage. It also aimed to empower people to make informed choices about the food they eat so as to reduce the overall ecological impact due to the worldwide production of food..The host country for the year's celebrations was Mongolia.\n\nThe theme for the 2012 World Environment Day was \"Green Economy: Does it include you?\"\n\nThe theme aimed to invite people to examine their activities and lifestyle and see how the concept of a \"Green Economy\" fits into it. The host country for the year's celebrations was Brazil.\n\nThe theme for 2011 was Forests-Nature At Your Service. Thousands of activities were organized worldwide, with beach clean-ups, concerts, exhibits, film festivals, community events and much more. That year's global host, India, is a country of wide biodiversity.\n\n'Many Species. One Planet. One Future', was the theme of 2010.\n\nIt celebrated the diversity of life on Earth as part of the 2010 International Year of Biodiversity. It was hosted in Rwanda. Thousands of activities were organized worldwide, with beach clean-ups, concerts, exhibits, film festivals, community events and much more. Each continent (except Antarctica) had a \"regional host city\", the U.N. chose Pittsburgh, Pennsylvania as the host for all North.\n\nThe theme for WED 2009 was 'Your Planet Needs You – UNite to Combat Climate Change', and Michael Jackson's 'Earth Song' was declared 'World Environment Day Song'. It was hosted in Mexico.\n\nThe host for World Environment Day 2008 was New Zealand, with the main international celebrations scheduled for Wellington. The slogan for 2008 was \"CO, Kick the Habit! Towards a Low Carbon Economy.\" New Zealand was one of the first countries to pledge to achieve carbon neutrality, and will also focus on forest management as a tool for reducing greenhouse gases. \n\nThe Chicago Botanic Garden served as the North American host for World Environment Day on 5 June 2008.\n\nThe topic for World Environment Day for 2007 was \"Melting Ice – a Hot Topic?\" During International Polar Year, WED 2007 focused on the effects that climate change is having on polar ecosystems and communities, on other ice- and snow-covered areas of the world, and the resulting global impacts.\n\nThe main international celebrations of the WED 2007 were held in the city of Tromsø, Norway, a city north of the Arctic Circle.\n\nThe topic for WED 2006 was Deserts and Desertification and the slogan was \"Don't desert drylands\".\n\nThe slogan emphasised the importance of protecting drylands. The main international celebrations of the World Environment Day 2006 were held in Algeria.\n\nThe theme for the 2005 World Environment Day was \"Green Cities\" and the slogan was \"Plan for the Planet!\".\n\nWorld Environment Day celebrations have been (and will be) hosted in the following cities:\nAn Earth Anthem penned by poet Abhay K is sung to celebrate World Environment Day.\n\n<poem>\nOur cosmic oasis, cosmic blue pearl\nthe most beautiful planet in the universe\nall the continents and the oceans \nunited we stand as flora and fauna\nunited we stand as species of one earth\ndifferent cultures, beliefs and ways\nwe are humans, the earth is our home\nall the people and the nations of the world\nall for one and one for all\nunited we unfurl the blue flag.\n</poem>\n\nIt was launched in June 2013 on the occasion of the World Environment Day by Kapil Sibal and Shashi Tharoor, then Union Ministers of India, at a function organized by the Indian Council of Cultural Relations in New Delhi. It is supported by the global organization Habitat For Humanity.\n\n\n"}
{"id": "46981574", "url": "https://en.wikipedia.org/wiki?curid=46981574", "title": "World Solar Challenge 2007", "text": "World Solar Challenge 2007\n\nThe 2007 World Solar Challenge was one of a biennial series of solar-powered car races, covering through the Australian Outback, from Darwin, Northern Territory to Adelaide, South Australia.\n\nIn the Challenge class 19 teams started, of which 10 completed the course, and the winner was a Nuna car built by Nuon of the Netherlands. In the Adventure class 18 teams started and eight completed the course, the winner being Ashiya University of Japan.\n\n"}
{"id": "5557685", "url": "https://en.wikipedia.org/wiki?curid=5557685", "title": "Zellweger off-peak", "text": "Zellweger off-peak\n\nZellweger is the brand name of an electric switching device used for to control off-peak electrical loads such as water heaters. It is an example of carrier current signaling. Power stations transmit a ripple signal on the main transmission lines when off-peak rates start (often around 10 pm). This ripple noise is picked up by the Zellweger, which after a random delay turns the hot water heater on. The noise is often picked up by other equipment, especially audio amplifiers and stereos and the noise can cause problems with other electrical devices. It is especially audible from ceiling fans running at low speed. Even some telephone lines can pick up the noise. The noise can be particularly obtrusive from some fluorescent light systems.\n\nNewer electrical meters incorporate this technology into the meter. \"Time of use\" meters charge electricity to the current tariff within half an hour, giving customers incentive to run appliances such as dishwashers, pool pumps and clothes dryers during the night.\n\nPower stations have plenty of unused capacity late at night but must keep running as they take days to shut down. Off-peak rates are used as an incentive for customers to use this surplus capacity and to reduce the amount of peak demand. This can produce cheaper power by delaying the need to build new power stations and reduce environmental impact.\nThe random time delay in the Zellweger means that the power stations aren't hit with a huge demand when all the hot water systems turn on at the same time; rather, the load is spread over a greater time period.\n\nOriginally time-clocks were used; however, they can easily lose accurate time and are not easily adjusted for daylight saving time. Zellwegers were first introduced in Australia in 1953, but were not compliant with modern harmonic disturbance standards. The second generation was introduced in the 1970s and was more reliable. A variety of devices can still be seen across Australia.\n\nIn at least some parts of Sydney, the ripple frequency is 1042 Hz. The signal usually consists of several bursts of a few seconds on and off, followed by a period of up to 50 seconds on. This is coded to affect only selected equipment. Occurrences are very frequent, sometimes several times an hour throughout the day, not just at evening and morning off-peak times.\n\nZellweger ZE22/3 contain low-risk radioactive material, and must be only handled by authorised people as breakage of the glass tube could cause a dangerous situation by releasing the radioactive material. The device reportedly contains a 'glow tube' containing tritium and radium.\n\nA few images are attached from an older electromechanical Zellweger ripple plant near Silverdale. Many of these Zellweger plants are still in use in New Zealand. Frequency used is 1050 Hz, superimposed upon the 50 Hz mains. Solid state Zellweger equipment is used as well, which can directly inject into the 22 kV or 33 kV sub transmission mains instead of the 11 kV mains as with these older Zellweger plants.\n"}
