{"id": "40333829", "url": "https://en.wikipedia.org/wiki?curid=40333829", "title": "3,10-Dihydroxydecanoic acid", "text": "3,10-Dihydroxydecanoic acid\n\n3,10-Dihydroxydecanoic acid is a chemical found in royal jelly.\n\n"}
{"id": "2574259", "url": "https://en.wikipedia.org/wiki?curid=2574259", "title": "Aggregated diamond nanorod", "text": "Aggregated diamond nanorod\n\nAggregated diamond nanorods, or ADNRs, are a nanocrystalline form of diamond, also known as nanodiamond or hyperdiamond.\n\nNanodiamond or hyperdiamond was convincingly demonstrated to be produced by compression of graphite in 2003 and in the same work found to be much harder than bulk diamond. Later it was also produced by compression of fullerene and confirmed to be the hardest and least compressible known material, with an isothermal bulk modulus of 491 gigapascals (GPa), while a conventional diamond has a modulus of 442–446 GPa; these results were inferred from X-ray diffraction data, which also indicated that ADNRs are 0.3% denser than regular diamond. The same group later described ADNRs as \"having a hardness and Young's modulus comparable to that of natural diamond, but with 'superior wear resistance'\".\n\nA <111> surface (normal to the largest diagonal of a cube) of pure diamond has a hardness value of 167±6 GPa when scratched with a nanodiamond tip, while the nanodiamond sample itself has a value of 310 GPa when tested with a nanodiamond tip. However, the test only works properly with a tip made of harder material than the sample being tested. This means that the true value for nanodiamond is likely somewhat lower than 310 GPa.\n\nADNRs (hyper diamonds / nano diamonds) are produced by compressing fullerite powder—a solid form of allotropic carbon fullerene—by any of two somewhat similar methods. One uses a diamond anvil cell and applied pressure ~37 GPa without heating the cell. In another method, fullerite is compressed to lower pressures (2–20 GPa) and then heated to a temperature in the range of . Extreme hardness of what now appears likely to have been nanodiamonds was reported by researchers in the 1990s. The material is a series of interconnected diamond nanorods, with diameters of between 5 and 20 nanometres and lengths of around 1 micrometre each.\n\nNanodiamond aggregates ca. 1 mm in size also form in nature, from graphite upon meteoritic impact, such as that of the Popigai crater in Siberia, Russia.\n\n\n"}
{"id": "4100725", "url": "https://en.wikipedia.org/wiki?curid=4100725", "title": "Air flow bench", "text": "Air flow bench\n\nAn air flow bench is a device used for testing the internal aerodynamic qualities of an engine component and is related to the more familiar wind tunnel.\n\nIt is used primarily for testing the intake and exhaust ports of cylinder heads of internal combustion engines. It is also used to test the flow capabilities of any component such as air filters, carburetors, manifolds or any other part that is required to flow gas. A flow bench is one of the primary tools of high performance engine builders, and porting cylinder heads would be strictly hit or miss without it.\n\nA flow bench consists of an air pump of some sort, a metering element, pressure and temperature measuring instruments such as manometers, and various controls. The test piece is attached in series with the pump and measuring element and air is pumped through the whole system. Therefore, all the air passing through the metering element also passes through the test piece. Because the volume flow rate through the metering element is known and the flow through the test piece is the same, it is also known. The mass flow rate can be calculated using the known pressure and temperature data to calculate air densities, and multiplying by the volume flow rate.\n\nThe air pump used must be able to deliver the volume required at the pressure required. Most flow testing is done at 10 and 28 inches of water pressure (2.5 to 7 kilopascals). Although other test pressures will work, the results would have to be converted for comparison to the work of others. The pressure developed must account for the test pressure plus the loss across the metering element plus all other system losses. The greater the accuracy of the metering element the greater is the loss. Flow volume of between 100 and 600 cubic feet per minute (0.05 to 0.28 m³/s) would serve almost all applications depending on the size of the engine under test.\n\nAny type of pump that can deliver the required pressure difference and flow volume can be used. Most often used is the dynamic-compression centrifugal type compressor, which is familiar to most as being used in vacuum cleaners and turbochargers, but multistaged axial-flow compressor types, similar to those used in most jet engines, could work as well, although there would be little need for the added cost and complexities involved, as they typically don't require such a high flow rate as a jet engine, nor are they limited by the aerodynamic drag considerations which makes a narrow-diameter axial compressor more effective in jet engines than a centrifugal compressor of equal air flow. Positive displacement types such as piston compressors, or rotary types such as a Roots blower could also be used with suitable provisions for damping the pulsations in the air flow (however, other rotary types such as twin screw compressors are capable of providing a steady supply of compressed fluid). The pressure ratio of a single fan blade is too low and cannot be used.\n\nThere are several possible types of metering element in use. Flow benches ordinarily use one of three types: orifice plate, venturi meter and pitot/static tube, all of which deliver similar accuracy. Most commercial machines use orifice plates due to their simple construction and the ease of providing multiple flow ranges. Although the venturi offers substantial improvements in efficiency, its cost is higher.\n\nAir flow conditions must be measured at two locations, across the test piece and across the metering element. The pressure difference across the test piece allows the standardization of tests from one to another. The pressure across the metering element allows calculation of the actual flow through the whole system.\n\nThe pressure across the test piece is typically measured with a U tube manometer while, for increased sensitivity and accuracy, the pressure difference across the metering element is measured with an inclined manometer. One end of each manometer is connected to its respective plenum chamber while the other is open to the atmosphere.\n\nOrdinarily all flow bench manometers measure in inches of water although the inclined manometer's scale is usually replaced with a logarithmic scale reading in percentage of total flow of the selected metering element which makes flow calculation simpler.\n\nTemperature must also be accounted for because the air pump will heat the air passing through it making the air down stream of it less dense and more viscous. This difference must be corrected for. Temperature is measured at the test piece plenum and at the metering element plenum. Correction factors are then applied during flow calculations. Some flow bench designs place the air pump after the metering element so that heating by the air pump is not as large a concern.\n\nAdditional manometers can be installed for use with hand held probes, which are used to explore local flow conditions in the port.\n\nThe air flow bench can give a wealth of data about the characteristics of a cylinder head or whatever part is tested. The result of main interest is bulk flow. It is the volume of air that flows through the port in a given time. Expressed in cubic feet per minute or cubic meters per second/minute.\n\nValve lift can be expressed as an actual dimension in decimal inches or mm. It can also be specified as a ratio between a characteristic diameter and the lift \"L\"/\"D\". Most often used is the valve head diameter. Normally engines have an \"L\"/\"D\" ratio from 0 up to a maximum of .35. For example, a valve would be lifted a maximum of 0.350 inch. During flow testing the valve would be set at \"L\"/\"D\" .05 .1 .15 .2 .25 .3 and readings taken successively. This allows the comparison of efficiencies of ports with other valve sizes, as the valve lift is proportional rather than absolute. For comparison with tests by others the characteristic diameter used to determine lift must be the same.\n\nFlow coefficients are determined by comparing the actual flow of a test piece to the theoretical flow of a perfect orifice of equal area. Thus the flow coefficient should be a close measure of efficiency. It cannot be exact because the \"L\"/\"D\" does not indicate the actual minimum size of the duct.\n\nAn orifice with a flow coefficient of .59 would flow the same amount of fluid as a perfect orifice with 59% of its area or 59% of the flow of a perfect orifice with the same area (orifice plates of the type shown would have a coefficient of between .58 and .62 depending on the precise details of construction and the surrounding installation).\n\nValve/port coefficient is non dimensional and is derived by multiplying a characteristic physical area of the port and by the bulk flow figures and comparing the result to an ideal orifice of the same area. It is here that air flow bench norms differ from fluid dynamics or aerodynamics at large. The coefficient may be based on the inner valve seat diameter, the outer valve head diameter, the port throat area or the valve open curtain area. Each of these methods are valid for some purpose but none of them represents the true minimum area for the valve/port in question and each results in a different flow coefficient. The great difficulty of measuring the actual minimum area at all the various valve lifts precludes using this as a characteristic measurement. This is due to the minimum area changing shape and location throughout the lift cycle. Because of this non standardization, port flow coefficients are not \"true\" flow coefficients, which would be based on the actual minimum area in the flow path. Which method to choose depends on what use is intended for the data. Engine simulation applications each require their own specification. If the result is to be compared to the work of others then the same method would have to be selected.\nUsing extra instrumentation (manometers and probes) the detailed flow through the port can be mapped by measuring multiple points within the port with probes. Using these tools, the velocity profile throughout the port can be mapped which gives insight into what the port is doing and what might be done to improve it.\n\nOf less interest is mass flow per minute or second since the test is not of a running engine which would be affected by it. It is the weight of air that flows through the port in a given time. Expressed in pounds per minute/hour or kilograms per second/minute. Mass flow is derived from the volume flow result to which a density correction is applied.\n\nWith the information gathered on the flow bench, engine power curve and system dynamics can be roughly estimated by applying various formulae. With the advent of accurate engine simulation software, however, it is much more useful to use flow data to create an engine model for a simulator.\n\nDetermining air velocity is a useful part of flow testing. It is calculated as follows:\n\nFor one set of English units\n\nWhere:\n\nFor SI units\n\nWhere:\n\nThis represents the highest speed of the air in the flow path, at or near the section of minimum area (\"through the valve seat at low values of L/D for instance\").\n\nOnce velocity has been calculated, the volume can be calculated by multiplying the velocity by the orifice area times its flow coefficient.\n\nA flow bench is capable of giving flow data which is closely but not perfectly related to actual engine performance. There are a number of limiting factors which contribute to the discrepancy.\n\nA flow bench tests ports under a steady pressure difference while in the actual engine the pressure difference varies widely during the whole cycle. The exact flow conditions existing in the flow bench test exist only fleetingly if at all in an actual running engine. Running engines cause the air to flow in strong waves rather than the steady stream of the flow bench. This acceleration/deceleration of the fuel/air column causes effects not accounted for in flow bench tests.\nThis graph, generated with an engine simulation program, shows how widely the pressures vary in a running engine vs. the steady test pressure of the flow bench.\n\n(Note, on the graph, that, in this case, when the intake valve opens, the cylinder pressure is above atmospheric (nearly 50% above or 1.5 bar or 150 kPa). This will cause reverse flow into the intake port until pressure in the cylinder falls below the ports pressure).\n\nThe coefficient of the port may change somewhat at different pressure differentials due to changes in Reynolds number regime leading to a possible loss of dynamic similitude.\nFlow bench test pressure are typically conducted at 10 to 28 inches of water (2.5 to 7 kPa) while a real engine may see 190 inches of water (47 kPa) pressure difference.\n\nThe flow bench tests using only air while a real engine usually uses air mixed with fuel droplets and fuel vapor, which is significantly different. Evaporating fuel passing through the port-runner has the effect of adding gas to and lowering the temperature of the air stream along the runner and giving the outlet flow rate slightly higher than the flow rate entering the port-runner. A port which flows dry air well might cause fuel droplets to fall out of suspension causing a loss of power not indicated by flow figures alone.\n\nLarge ports and valves can show high flow rates on a flow bench but the velocity can be lowered to the point that the gas dynamics of a real engine are ruined. Overly large ports also contribute to fuel fall out.\n\nA running engine is much hotter than room temperature and the temperature in various parts of the system vary widely. This affects the actual flow, fuel effects as well as the dynamic wave effects in the engine which do not exist on the flow bench.\n\nThe proximity, shape and movement of the piston as well as the movement of the valve itself significantly alters the flow conditions in a real engine that do not exist in flow bench tests.\n\nThe flow simulated on a flow bench bears almost no similarity to the flow in a real exhaust port. Here even the coefficients measured on flow benches are inaccurate. This is due to the very high and wide-ranging pressures and temperatures. From the graph above it can be seen that the pressure in the port reaches 2.5 bar (250 kPa) and the cylinder pressure at opening is 6 bar (600 kPa) and more. This is many times more than the capabilities of a typical flow bench of 0.06 bar (6 kPa).\n\nThe flow in a real exhaust port can easily be sonic with choked flow occurring and even supersonic flow in areas. The very high temperature causes the viscosity of the gas to increase, all of which alters the Reynolds number drastically.\n\nAdded to the above is the profound effect that downstream elements have on the flow of the exhaust port. Far more than upstream elements found on the intake side.\n\nExhaust port size and flow information might be considered as vague, but there are certain guidelines which are used when creating a base-line to optimum performance. This base line, of course, is further tuned and qualified through a dynamometer.\n\n\n"}
{"id": "370528", "url": "https://en.wikipedia.org/wiki?curid=370528", "title": "Athabasca oil sands", "text": "Athabasca oil sands\n\nThe Athabasca oil sands (or tar sands) are large deposits of bitumen or extremely heavy crude oil, located in northeastern Alberta, Canada – roughly centred on the boomtown of Fort McMurray. These oil sands, hosted primarily in the McMurray Formation, consist of a mixture of crude bitumen (a semi-solid rock-like form of crude oil), silica sand, clay minerals, and water. The Athabasca deposit is the largest known reservoir of crude bitumen in the world and the largest of three major oil sands deposits in Alberta, along with the nearby Peace River and Cold Lake deposits (the latter stretching into Saskatchewan).\n\nTogether, these oil sand deposits lie under of boreal forest and muskeg (peat bogs) and contain about of bitumen in-place, comparable in magnitude to the world's total proven reserves of conventional petroleum. The International Energy Agency (IEA) lists the economically recoverable reserves, at 2007 prices and modern unconventional oil production technology, to be , or about 10% of these deposits. These contribute to Canada's total proven reserves being the third largest in the world, after Saudi Arabia and Venezuela's Orinoco Belt.\n\nBy 2009, the two extraction methods used were \"in situ\" extraction, when the bitumen occurs deeper within the ground, (which will account for 80 percent of oil sands development) and surface or open-pit mining, when the bitumen is closer to the surface. Only 20 percent of bitumen can be extracted using open pit mining methods, which involves large scale excavation of the land with huge hydraulic power shovels and 400-ton heavy hauler trucks. Surface mining leaves toxic tailings ponds. In contrast, \"in situ\" uses more specialized techniques such as steam-assisted gravity drainage (SAGD). \"Eighty percent of the oil sands will be developed \"in situ\" which accounts for 97.5 percent of the total surface area of the oil sands region in Alberta.\" In 2006 the Athabasca deposit was the only large oil sands reservoir in the world which was suitable for large-scale surface mining, although most of this reservoir can only be produced using more recently developed in-situ technology.\n\nCritics contend that government and industry measures taken to reduce environmental and health risks posed by large-scale mining operations are inadequate, causing unacceptable damage to the natural environment and human welfare.\nObjective discussion of the environmental impacts has often been clouded by polarized arguments from industry and from advocacy groups.\n\nThe Athabasca oil sands are named after the Athabasca River which cuts through the heart of the deposit, and traces of the heavy oil are readily observed on the river banks. Historically, the bitumen was used by the indigenous Cree and Dene Aboriginal peoples to waterproof their canoes. The oil deposits are located within the boundaries of Treaty 8, and several First Nations of the area are involved with the sands.\n\nThe Athabasca oil sands first came to the attention of European fur traders in 1719 when Wa-pa-su, a Cree trader, brought a sample of bituminous sands to the Hudson's Bay Company post at York Factory on Hudson Bay where Henry Kelsey was the manager. In 1778, Peter Pond, another fur trader and a founder of the rival North West Company, became the first European to see the Athabasca deposits after exploring the Methye Portage which allowed access to the rich fur resources of the Athabasca River system from the Hudson Bay watershed.\n\nIn 1788, fur trader Alexander Mackenzie, after whom the Mackenzie River was later named, traveled along routes to both the Arctic and Pacific Ocean wrote: \"At about from the fork (of the Athabasca and Clearwater Rivers) are some bituminous fountains into which a pole of long may be inserted without the least resistance. The bitumen is in a fluid state and when mixed with gum, the resinous substance collected from the spruce fir, it serves to gum the Indians' canoes.\" He was followed in 1799 by mapmaker David Thompson and in 1819 by British Naval officer John Franklin.\n\nJohn Richardson did the first serious scientific assessment of the oil sands in 1848 on his way north to search for Franklin's lost expedition. The first government-sponsored survey of the oil sands was initiated in 1875 by John Macoun, and in 1883, G. C. Hoffman of the Geological Survey of Canada tried separating the bitumen from oil sand with the use of water and reported that it separated readily. In 1888, Robert Bell, the director of the Geological Survey of Canada, reported to a Senate Committee that \"The evidence … points to the existence in the Athabasca and Mackenzie valleys of the most extensive petroleum field in America, if not the world.\"\n\nCount Alfred von Hammerstein (1870–1941), who arrived in the region in 1897, promoted the Athabaska oil sands for over forty years, taking photos with descriptive titles such as \"Tar Sands and Flowing Asphaltum in the Athabasca District,\" that are now in the National Library and National Archives Canada. Photos of the Athabasca oil sands were also featured in Canadian writer and adventurer, Agnes Deans Cameron's, best-selling book entitled \"The New North: Being Some Account of a Woman’s Journey through Canada to the Arctic\" which recounted her roundtrip to the Arctic Ocean. Following this journey and the publication of her book, she travelled extensively as lecturer, with magic lantern slides of her Kodak images, promoting immigration to western Canada at Oxford, Cambridge, St. Andrew’s University and the Royal Geographical Society. Her photographs were reproduced in 2011-2012 in an exhibit at the Canadian Museum of Civilization in Ottawa, Ontario, Canada. Cameron was particularly enthusiastic about the Athabaska region and the Athabaska oil sands which included photos of Count Alfred Von Hammerstein's oil drill works along the Athabasca River. \"While the Count was unsuccessful drilling for \"elephant pools of oil,\" Cameron’s book and its images... made her a media celebrity.\" \"In all Canada there is no more interesting stretch of waterway than that upon which we are entering. An earth-movement here has created a line of fault clearly visible for seventy or eighty miles along the river-bank, out of which oil oozes at frequent intervals. […] Tar there is ... in plenty. ... It oozes from every fissure, and into some bituminous tar well we can poke a twenty-foot pole and find no resistance. cited in \n\nIn 1926, Karl Clark of the University of Alberta received a patent for a hot water separation process which was the forerunner of today's thermal extraction processes. Several attempts to implement it had varying degrees of success.\n\nA pioneer in the discovery and use of natural gas was Georg Naumann. He used natural gas as early as about 1940.\n\nProject Oilsand, also known as Project Oilsands, was a 1958 proposal to exploit the Athabasca oil sands using the underground detonation of nuclear explosives; hypothetically, the heat and pressure created by an underground detonation would boil the bitumen deposits, reducing their viscosity to the point that standard oilfield techniques could be used. The general means by which the plan was to work was discussed in the October 1976 \"Bulletin of the Atomic Scientists\" issue. A patent was granted for the process that was intended: \"The Process for Stimulating Petroliferous Subterranean Formations with Contained Nuclear Explosions\" by Bray, Knutson, and Coffer, which was first submitted in 1964. With the nuclear heating option considered a forerunner to some of the nascent conventional heating ideas that are presently suggested and in use extracting oil from the Alberta regions Athabasca oil sands.\n\nThe proposal, originally known as \"Project Cauldron\", was devised by geologist Manley L. Natland at Los Angeles-based Richfield Oil Corporation. Natland believed that an underground blast was the most efficient way to generate the heat needed to liquefy the viscous bitumen so that it could be pumped to the surface by conventional wells. The project was conceived of as part of Operation Plowshare, a United States project to harness the nuclear explosions for peaceful applications. However, some experts had doubts. In 1959, oil sands pioneer Robert Fitzsimmons of the International Bitumen Company wrote a letter to the Edmonton Journal, saying \"While the writer does not know anything about nuclear energy and is therefore not qualified to make any definite statement as to it’s [sic] results he does know something about the effect dry heat has on those sands and ventures a guess that if it does not turn the whole deposit into a burning inferno it is almost sure to fuse it into a solid mass of semi glass or coke.\"\n\nIn April 1959, the Federal Mines Department approved Project Oilsand. However, before the project could continue beyond preliminary steps, the Canadian government's stance on the use of nuclear devices changed. In April 1962, the Canadian Secretary of State for External Affairs said \"Canada is opposed to nuclear tests, period\". These 1962 changes in Canadian public opinion is regarded by historian Michael Payne to be due to the shift in public perception of nuclear explosives following the 1962 Cuban Missile Crisis, Project Oilsand was subsequently cancelled. Prime Minister John Diefenbaker told Parliament that the decision to detonate an atomic bomb on or under Canadian soil would be made by Canada, not the United States, and ordered Project Cauldron/Oilsand placed on permanent hold, citing the risk of upsetting the Soviet Union during nuclear disarmament negotiations being conducted in Geneva.\n\nThe United States government continued with exploring the peaceful uses of nuclear detonations with Operation Plowshare, but was likewise eventually terminated in 1977. While social scientist, Benjamin Sovacool contends that the main problem was that the produced oil and gas was radioactive, which caused consumers to reject it. In contrast, oil and gas are sometimes considerably \"naturally\" radioactive to begin with and the industry is set up to deal with this, moreover in contrast to earlier stimulation efforts, contamination from many \"later\" tests was not a showstopping issue, it was primarily changing public opinion due to the societal fears caused by events such as the Cuban Missile Crisis, that resulted in protests, court cases and general hostility that ended the US exploration. Furthermore, as the years went by without further development and the closing/curtailment in US nuclear weapons factories, this began to evaporate the economies of scale advantage that had earlier existed, with this, it was increasingly found that most US fields could instead be stimulated by non-nuclear techniques which were found to be likely cheaper. The most successful and profitable nuclear stimulation effort that did not result in customer product contamination issues was the 1976 Project Neva on the Sredne-Botuobinsk gas field in the Soviet Union, made possible by multiple cleaner stimulation explosives, favourable rock strata and the possible creation of an underground contaminant storage cavity.\n\nThe oil sands, which are typically thick and sit on top of relatively flat limestone, are relatively easy to access. They lie under of waterlogged muskeg, of clay and barren sand. As a result of the easy accessibility, the world's first oil-sands mine was in the Athabasca oil sands.\n\nCommercial production of oil from the Athabasca oil sands began in 1967, with the opening of the Great Canadian Oil Sands (GCOS) plant in Fort McMurray. It was the first operational oil sands project in the world, owned and operated by the American parent company, Sun Oil Company. When the US$240 million plant officially opened with a capacity of , it marked the beginning of commercial development of the Athabasca oil sands. In 2013 McKenzie-Brown listed industrialist J. Howard Pew as one of the six visionaries who built the Athabasca oil sands. By the time of his death in 1971, the Pew family were ranked by \"Forbes\" magazine as one of the half-dozen wealthiest families in America. The Great Canadian Oil Sands Limited (then a subsidiary of Sun Oil Company but now incorporated into an independent company known as Suncor Energy Inc.) produced of synthetic crude oil.\n\nIn 1979, Sun formed Suncor by merging its Canadian refining and retailing interests with Great Canadian Oil Sands and its conventional oil and gas interests. In 1981, the Government of Ontario purchased a 25% stake in the company but divested it in 1993. In 1995 Sun Oil also divested its interest in the company, although Suncor maintained the Sunoco retail brand in Canada. Suncor took advantage of these two divestitures to become an independent, widely held public company.\n\nSuncor continued to grow and continued to produce more and more oil from its oil sands operations regardless of fluctuating market prices, and eventually became bigger than its former parent company. In 2009, Suncor acquired the formerly Canadian government owned oil company, Petro-Canada, which turned Suncor into the largest petroleum company in Canada and one of the biggest Canadian companies. Suncor Energy is now a Canadian company completely unaffiliated with its former American parent company. Sun Oil Company became known as Sunoco, but later left the oil production and refining business, and has since become a retail gasoline distributor owned by Energy Transfer Partners of Dallas, Texas. In Canada, Suncor Energy converted all of its Sunoco stations (which were all in Ontario) to Petro-Canada sites in order to unify all of its downstream retail operations under the Petro-Canada banner and discontinue paying licensing fees for the Sunoco brand. Nationwide, Petro-Canada's upstream product supplier and parent company is Suncor Energy. Suncor Energy continues to operate just one Sunoco retail site in Ontario.\n\nThe true size of the Canadian oil sands deposits became known in the 1970s. The Syncrude mine opened in 1978 and is now the largest mine (by area) in the world, with mines potentially covering . (Although there is oil underlying , which may be disturbed by drilling and in situ extraction, only may potentially be surface mined, and has to date been mined.)\n\nDevelopment was inhibited by declining world oil prices, and the second mine, operated by the Syncrude consortium, did not begin operating until 1978, after the 1973 oil crisis sparked investor interest.\n\nHowever the price of oil subsided afterwards and although the 1979 energy crisis caused oil prices to peak again, during the 1980s, oil prices declined to very low levels causing considerable retrenchment in the oil industry.\n\nAt the turn of the 21st century, oil sands development in Canada started to take off, with an expansion at the Suncor mine, a new mine and expansion at Syncrude, and a new mine by Royal Dutch Shell associated with their new Scotford Upgrader near Edmonton. Three new large steam assisted gravity drainage (SAGD) projects were added – Foster Creek, Surmont, and MacKay River – by different companies, all of which have since been bought by larger companies.\n\nShell Canada's third mine began operating in 2003. However, as a result of oil price increases since 2003, the existing mines have been greatly expanded and new ones are being planned.\n\nAccording to the Alberta Energy and Utilities Board, 2005 production of crude bitumen in the Athabasca oil sands was as follows:\n\nAs of 2006, oil sands production had increased to . Oil sands were the source of 62% of Alberta's total oil production and 47% of all oil produced in Canada. As of 2010, oil sands production had increased to over , where 53% of this was produced by surface mining and 47% by in-situ. The Alberta government believes this level of production could reach by 2020 and possibly by 2030.\n\nIn 2012, the actual oil production from oil sands was .\n\nCanada is the largest source of oil imported by the United States, supplying nearly from oil sand sources. Keystone XL, a pipeline from Alberta to Gulf coast refineries, is under consideration, as is the North Gateway project to Kitimat, British Columbia, which would be built by Enbridge, operator of the Enbridge Pipeline System which also serves the area. Industry observers believe there may be excess pipeline capacity. Kinder Morgan has made another proposal for a west coast pipeline while Enbridge also proposes Eastern Access, a pipeline to refineries in Montreal and possibly to a terminal in Portland, Maine, as well as expansion of an existing pipeline to Chicago. Environmental and First Nations opposition to all these projects is anticipated, and planned.\n\nAs of December 2008, the Canadian Association of Petroleum Producers revised its 2008–2020 crude oil forecasts to account for project cancellations and cutbacks as a result of the price declines in the second half of 2008. The revised forecast predicted that Canadian oil sands production would continue to grow, but at a slower rate than previously predicted. There would be minimal changes to 2008–2012 production, but by 2020 production could be less than its prior predictions. This would mean that Canadian oil sands production would grow from in 2008 to in 2020, and that total Canadian oil production would grow from in 2020. Even accounting for project cancellations, this would place Canada among the four or five largest oil-producing countries in the world by 2020.\n\nIn early December 2007, London-based BP and Calgary-based Husky Energy announced a 50–50 joint venture to produce and refine bitumen from the Athabasca oil sands. BP would contribute its Toledo, Ohio refinery to the joint venture, while Husky would contribute its Sunrise oil sands project. Sunrise was planned to start producing of bitumen in 2012 and may reach by 2015–2020. BP would modify its Toledo refinery to process of bitumen directly to refined products. The joint venture would solve problems for both companies, since Husky was short of refining capacity, and BP had no presence in the oil sands. It was a change of strategy for BP, since the company historically has downplayed the importance of oil sands.\n\nIn mid December 2007, ConocoPhillips announced its intention to increase its oil sands production from to over the next 20 years, which would make it the largest private sector oil sands producer in the world. ConocoPhillips currently holds the largest position in the Canadian oil sands with over under lease. Other major oil sands producers planning to increase their production include Royal Dutch Shell (to ); Syncrude Canada (to ); Suncor Energy (to ) and Canadian Natural Resources (to ). If all these plans come to fruition, these five companies will be producing over of oil from oil sands by 2028.\n\nThe governance of the Alberta oil sands is focused on economic development, and has historically been dominated by the interests of two primary actors; government (federal and provincial) and industry. Canadian federalism forms the functions and roles of each level of government, in that constitutional power is split so that neither is superior to the other. The Constitution Act, 1867, Section 109 ensures the province full ownership of the lands and resources within its borders. The province acts as the landowner and the federal government oversees jurisdiction over trade, commerce and taxation. There is a clear overlap, as resource management influences trade, and trade management influences resources. As of the 1990s, both the federal and provincial government have been aligned, focusing on regulation, technology and the development of new export markets. The majority of \"ground-level\" governance is carried out by a number of provincial institutions.\n\nOttawa has avoided direct investment, preferring to improve the investment climate. A prime example of this occurred in 1994, when the federal government rolled out tax breaks allowing 100% of oil sands capital investments to be written off as accelerated capital cost allowances. The provincial government had a much more direct role in development; investing directly in numerous pilot projects, undertaking joint ventures with the industry and consistently making massive investments in research and development. Some people have claimed that Alberta features one of the lowest royalty rates in the world. Since Alberta, unlike US states, owns the vast majority of oil under its surface it can exercise more control over it, whereas US states are limited to severance taxes. This industry-centric royalty system has been criticised for \"promoting a runaway pace of development\".\n\nIndustry is the core force of oil sands development. The first major players, Suncor Energy and Syncrude, dominated the market until the 1990s. Currently there are 64 companies operating several hundred projects. The majority of production now comes from foreign-owned corporations, and the maintenance of a favourable climate for these corporations grants them strong influence; much stronger than that of non-productive stakeholders, such as citizens and environmental groups.\n\nGovernance (policy, administration, regulation) over the oil sands is held almost entirely by the Ministry of Energy (Alberta) and its various departments. Critics noted a clear and systemic lack of public involvement at all key stages of the governance process. In answer to this, the province initiated the Oil Sands Consultations Multistakeholder Committee (MSC) in 2006. The MSC represents four organisations: the Cumulative Environmental Management Association (CEMA), the Wood Buffalo Environmental Association (WBEA), the Canadian Oil Sands Network for Research and Development (CONRAD) and the Athabasca Regional Issues Working Group (RIWG). The role of the MSC is to consult and make recommendations on management principles. The recommendations contained in the MSC’s first 2007 Final Report were lauded by several ministers and government representatives, but none have yet been effectively passed into law.\n\nOn October 17, 2012, the Alberta government announced it would follow the recommendations of a working group to develop an agency that would monitor the environmental impact of the oil sands. \"The new science-based agency will begin work in the oil sands region and will focus on what is monitored, how it’s monitored and where it’s monitored. This will include integrated and coordinated monitoring of land, air, water and biodiversity,\" said a press release from Diana McQueen's office, the Minister of Energy and Sustainable Development. The provincial government moved to develop the agency after widespread public criticism by environmentalists, aboriginal groups and scientists, who claimed the oil sands would have a devastating, long-term effect on the environment if left unchecked.\n\nOn 17 June 2013 the newly formed corporation, Alberta Energy Regulator (AER) was phased in with a mandate to regulate oil, gas and coal development in Alberta including the Athabasca oil sands. The AER brings together \"the regulatory functions from the Energy Resources Conservation Board and the Alberta Ministry of Environment and Sustainable Resource Development into a one-stop shop\" The Alberta Energy Regulator is now \"responsible for all projects from application to reclamation.\" They will respond to project proponents, landowners and industry regarding energy regulations in Alberta. The Responsible Energy Development Act gave the Alberta Energy Regulator \"the authority to administer the Public Lands Act, the Environmental Protection and Enhancement Act and the Water Act, with regards to energy development.\" The Alberta Energy Regulator will enforce environmental laws and issue environmental and water permits, responsibilities formerly the mandate of Alberta Environment.\n\nThe key characteristic of the Athabasca deposit is that it is the only one shallow enough to be suitable for surface mining. About 10% of the Athabasca oil sands are covered by less than of overburden. Until 2009, the surface mineable area (SMA) was defined by the ERCB, an agency of the Alberta government, to cover 37 contiguous townships (about ) north of Fort McMurray. In June 2009, the SMA was expanded to townships, or about . This expansion pushes the northern limit of the SMA to within of Wood Buffalo National Park, a UNESCO World Heritage Site.\n\nThe Albian Sands mine (operated by Shell Canada) opened in 2003. All three of these mines are associated with bitumen upgraders that convert the unusable bitumen into synthetic crude oil for shipment to refineries in Canada and the United States. For Albian, the upgrader is located at Scotford, 439 km south. The bitumen, diluted with a solvent, is transferred there in a corridor pipeline.\n\nThe Energy Resource Conservation Board has approved over 100 mining and in-situ projects despite the negative environmental impacts. As of 2012, there was 9 active open mining projects, more than 50 approved in-situ projects as well as 190 primary recovery projects extracting bitumen that is free flowing. The ERCB has also approved 20 projects that are testing unproven technology as well as new versions of existing technologies.\n\nSince Great Canadian Oil Sands (now Suncor) started operation of its mine in 1967, bitumen has been extracted on a commercial scale from the Athabasca Oil Sands by surface mining. In the Athabasca sands there are very large amounts of bitumen covered by little overburden, making surface mining the most efficient method of extracting it. The overburden consists of water-laden muskeg (peat bog) over top of clay and barren sand. The oil sands themselves are typically deep, sitting on top of flat limestone rock. Originally, the sands were mined with draglines and bucket-wheel excavators and moved to the processing plants by conveyor belts.\n\nThese early mines had a steep learning curve to deal with before their bitumen mining techniques became efficient. In the intervening years, more effective in-situ production techniques were developed, particularly steam assisted gravity drainage (SAGD). In-situ methods became increasingly important because only about 20% of the Athabasca oil sands were shallow enough to recover by surface mining, and the SAGD method in particular was very efficient at recovering large amounts of bitumen at a reasonable cost.\n\nIn recent years, companies such as Syncrude and Suncor have switched to much cheaper shovel-and-truck operations using the biggest power shovels (at least ) and dump trucks () in the world. This has held production costs to around US$27 per barrel of synthetic crude oil despite rising energy and labour costs.\n\nAfter excavation, hot water and caustic soda (sodium hydroxide) is added to the sand, and the resulting slurry is piped to the extraction plant where it is agitated and the oil skimmed from the top. Provided that the water chemistry is appropriate to allow bitumen to separate from sand and clay, the combination of hot water and agitation releases bitumen from the oil sand, and allows small air bubbles to attach to the bitumen droplets. The bitumen froth floats to the top of separation vessels, and is further treated to remove residual water and fine solids.\n\nAbout of oil sands are required to produce one barrel () of oil. Originally, roughly 75% of the bitumen was recovered from the sand. However, recent enhancements to this method include Tailings Oil Recovery (TOR) units which recover oil from the tailings, Diluent Recovery Units to recover naphtha from the froth, inclined plate settlers (IPS) and disc centrifuges. These allow the extraction plants to recover well over 90% of the bitumen in the sand. After oil extraction, the spent sand and other materials are then returned to the mine, which is eventually reclaimed.\n\nAlberta Taciuk Process technology extracts bitumen from oil sands through a dry retorting. During this process, oil sand is moved through a rotating drum, cracking the bitumen with heat and producing lighter hydrocarbons. Although tested, this technology is not in commercial use yet.\n\nThe original process for extraction of bitumen from the sands was developed by Dr. Karl Clark, working with the Alberta Research Council in the 1920s. Today, all of the producers doing surface mining, such as Syncrude Canada, Suncor Energy and Albian Sands Energy etc., use a variation of the Clark Hot Water Extraction (CHWE) process. In this process, the ores are mined using open-pit mining technology. The mined ore is then crushed for size reduction. Hot water at is added to the ore and the formed slurry is transported using hydrotransport line to a primary separation vessel (PSV) where bitumen is recovered by flotation as bitumen froth. The recovered bitumen froth consists of 60% bitumen, 30% water and 10% solids by weight.\n\nThe recovered bitumen froth needs to be cleaned to reject the contained solids and water to meet the requirement of downstream upgrading processes. Depending on the bitumen content in the ore, between 90 and 100% of the bitumen can be recovered using modern hot water extraction techniques. After oil extraction, the spent sand and other materials are then returned to the mine, which is eventually reclaimed.\n\nSteam Assisted Gravity Drainage (SAGD) is an enhanced oil recovery technology for producing heavy crude oil and bitumen. It is an advanced form of steam stimulation in which a pair of horizontal wells are drilled into the oil reservoir, one a few metres above the other. High pressure steam is continuously injected into the upper wellbore to heat the oil and reduce its viscosity, causing the heated oil to drain into the lower wellbore, where it is pumped out to a bitumen recovery facility. Dr. Roger Butler, engineer at Imperial Oil from 1955 to 1982, invented steam-assisted gravity drainage (SAGD) in the 1970s. Butler \"developed the concept of using horizontal pairs of wells and injected steam to develop certain deposits of bitumen considered too deep for mining.\"\n\nMore recently, \"in situ\" methods like steam-assisted gravity-drainage (SAGD) and cyclic steam stimulation (CSS) have been developed to extract bitumen from deep deposits by injecting steam to heat the sands and reduce the bitumen viscosity so that it can be pumped out like conventional crude oil.\n\nThe standard extraction process requires huge amounts of natural gas. As of 2007, the oil sands industry used about 4% of the Western Canada Sedimentary Basin natural gas production. By 2015, this may increase two-and-a-half-fold.\n\nAccording to the National Energy Board, it requires about of natural gas to produce one barrel of bitumen from \"in situ\" projects and about for integrated projects. Since a barrel of oil equivalent is about of gas, this represents a large gain in energy. That being the case, it is likely that Alberta regulators will reduce exports of natural gas to the United States in order to provide fuel to the oil sands plants. As gas reserves are exhausted, however, oil upgraders will probably turn to bitumen gasification to generate their own fuel. In much the same way as bitumen can be converted into synthetic crude oil, it can also be converted into synthetic natural gas.\n\nCritics contend that government and industry measures taken to reduce environmental and health risks posed by large-scale mining operations are inadequate, causing unacceptable damage to the natural environment and human welfare.\nObjective discussion of the environmental impacts has often been clouded by polarized arguments from industry and from advocacy groups.\n\nApproximately 20% of Alberta's oil sands are recoverable through open-pit mining, while 80% require \"in situ\" extraction technologies (largely because of their depth). Open pit mining destroys the boreal forest and muskeg, while \"in situ\" extraction technologies cause less significant damage. Approximately 0.19% of the Alberta boreal forest has been disturbed by open pit mining. The Alberta government requires companies to restore the land to \"equivalent land capability\". This means that the ability of the land to support various land uses after reclamation is similar to what existed, but that the individual land uses may not necessarily be identical.\n\nIn some particular circumstances the government considers agricultural land to be equivalent to forest land. Oil sands companies have reclaimed mined land to use as pasture for wood bison instead of restoring it to the original boreal forest and muskeg. Syncrude asserts they have reclaimed 22% of their disturbed land, a figure disputed by other sources, who assess Syncrude more accurately reclaimed only 0.2% of its disturbed land.\n\nA Pembina Institute report stated \"To produce one cubic metre (m) [] of synthetic crude oil (SCO) (upgraded bitumen) in a mining operation requires about of water (net figures). Approved oil sands mining operations are currently licensed to divert 359 million m from the Athabasca River, or more than twice the volume of water required to meet the annual municipal needs of the City of Calgary.\" It went on to say \"...the net water requirement to produce a cubic metre of oil with \"in situ\" production may be as little as , depending on how much is recycled\".\n\nThe Athabasca River runs from the Athabasca Glacier in west-central Alberta to Lake Athabasca in northeastern Alberta. The average annual flow just downstream of Fort McMurray is with its highest daily average measuring 1,200 cubic metres per second.\n\nWater licence allocations total about 1% of the Athabasca River average annual flow, though actual withdrawals for all uses, in 2006, amount to about 0.4%. In addition, the Alberta government sets strict limits on how much water oil sands companies can remove from the Athabasca River. According to the Water Management Framework for the Lower Athabasca River, during periods of low river flow water consumption from the Athabasca River is limited to 1.3% of annual average flow. The province of Alberta is also looking into cooperative withdrawal agreements between oil sands operators.\n\nSince the beginning of the oil sands development, there have been several leaks into the Athabasca River polluting it with oil and tailing pond water. The close proximity of the tailing ponds to the river drastically increases the likelihood of contamination due to ground water leakages. In 1997, Suncor admitted that their tailing ponds had been leaking of toxic water into the river a day. This water contains naphthenic acid, trace metals such as mercury and other pollutants. The Athabasca River is the largest freshwater delta in the world but with Suncor and Syncrude leaking tail ponds the amount of polluted water will exceed 1 billion cubic meters by 2020.\n\nNatural toxicants derived from bitumen in Northern Alberta pose potential ecological and human health risks to northerners living in the area. Oil sands development contributes arsenic, cadmium, chromium, lead, mercury, nickel and other metal elements toxic at low concentrations to the tributaries and rivers of the Athabasca.<ref name=\"doi: 10.1073/pnas.1008754107\"></ref>\n\nThe processing of bitumen into synthetic crude requires energy, which is currently being generated by burning natural gas. In 2007, the oil sands used around of natural gas per day, around 40% of Alberta's total usage. Based on gas purchases, natural gas requirements are given by the Canadian Energy Resource Institute as 2.14 GJ (2.04 thousand cu ft) per barrel for cyclic steam stimulation projects, 1.08 GJ (1.03 thousand cu ft) per barrel for SAGD projects, 0.55 GJ (0.52 thousand cu ft) per barrel for bitumen extraction in mining operations not including upgrading or 1.54 GJ (1.47 thousand cu ft) per barrel for extraction and upgrading in mining operations.\n\nA 2009 study by CERA estimated that production from Canada's oil sands\nemits \"about 5 percent to 15 percent more carbon dioxide, over the\n\"well-to-wheels\" lifetime analysis of the fuel, than average crude\noil.\" Author and investigative journalist David Strahan that same year stated that IEA figures show that carbon dioxide emissions from the oil sands are 20% higher than average emissions from oil, explaining the discrepancy as the difference between upstream emissions and life cycle emissions. He goes on to say that a US government report in 2005 suggested with current technology conventional oil releases 40 kg of carbon dioxide per barrel while non-conventional oil releases 80–115 kg of carbon dioxide. Alberta energy suggests lower releases of carbon with improving technology, giving a value of 39% drop in emissions per barrel between 1990 and 2008, however only a 29% reduction between 1990 and 2009.\n\nThe forecast growth in synthetic oil production in Alberta also threatens Canada's international commitments. In ratifying the Kyoto Protocol, Canada agreed to reduce, by 2012, its greenhouse gas emissions by 6% with respect to 1990. In 2002, Canada's total greenhouse gas emissions had increased by 24% since 1990. \n\nRanked as the world's eighth largest emitter of greenhouse gases, Canada is a relatively large emitter given its population and is missing its Kyoto targets. A major Canadian initiative called the Integrated CO2 Network (ICO2N) promotes the development of large scale capture, transport and storage of carbon dioxide (CO) as a means of helping Canada to help meet climate change objectives while supporting economic growth. ICO2N members represent a group of industry participants, many oil sands producers, providing a framework for carbon capture and storage development in Canada.\n\nIn Northern Alberta, oil development activities bring an enormous number of people into a fragile ecosystem. Historically, population figures have been very low for this region. Water is easily polluted because the water table reaches the surface in most areas of muskeg. With the ever-increasing development and extraction of resources, wildlife are recipient to both direct and indirect effects of pollution. Woodland Caribou are particularly sensitive to human activities, and as such are pushed away from their preferred habitat during the time of year when their caloric needs are greatest and food is the most scarce. Humans' effect on the Caribou is compounded by road construction and habitat fragmentation that open the area up to deer and wolves.\n\nWildlife living near the Athabasca River have been greatly impacted due to pollutants entering the water system. An unknown number of birds die each year. Particularly visible and hard hit are migrating birds that stop to rest at tailing ponds. There have been numerous reports of large flocks of ducks landing in tailing ponds and perishing soon after. Data has been recorded since the 1970s on the number of birds found on tailing ponds.\n\nThere has also been a large impact on the fish that live and spawn in the area. As toxins accumulate in the river due to the oil sands, bizarre mutations, tumors, and deformed fish species have begun to appear. A study commissioned by the region's health authority found that several known toxins and carcinogens were elevated. Aboriginal communities that live around the river are becoming increasingly worried about how the animals they eat and their drinking water are being affected.\n\nWhile there has been no link yet made between the oil sands and health issues, Matt Price of Environmental Defense says the connection makes common sense. Deformities in fish and high concentrations of toxic substances in animals have also been identified.\n\nLarge volumes of tailings are a byproduct of bitumen extraction from the oil sands and managing these tailings is one of the most difficult environmental challenges facing the oil sands industry. Tailings ponds are engineered dam and dyke systems that contain solvents used in the separation process as well as residual bitumen, salts and soluble organic compounds, fine silts and water. The concentrations of chemicals may be harmful to fish and oil on the surface harmful to birds. These settling basins were meant to be temporary. A major hindrance to the monitoring of oil sands produced waters has been the lack of identification of individual compounds present. By better understanding the nature of the highly complex mixture of compounds, including naphthenic acids, it may be possible to monitor rivers for leachate and also to remove toxic components. Such identification of individual acids has for many years proved to be impossible but a breakthrough in 2011 in analysis began to reveal what is in the oil sands tailings ponds. Ninety percent of the tailings water can be reused for oil extraction. By 2009 as tailing ponds continued to proliferate and volumes of fluid tailings increased, the Alberta Energy Resources Conservation Board issued Directive 074 to force oil companies to manage tailings based on new aggressive criteria. The Government of Alberta reported in 2013 that tailings ponds in the Alberta oil sands covered an area of about . The Tailings Management Framework for Mineable Oil Sands is part of Alberta’s Progressive Reclamation Strategy for the oil sands to ensure that tailings are reclaimed as quickly as possible.\n\nSuncor invested $1.2 billion in their Tailings Reduction Operations (TROTM) method that treats mature fine tails (MFT) from tailings ponds with chemical flocculant, an anionic Polyacrylamide, commonly used in water treatment plants to improve removal of total organic content (TOC), to speed their drying into more easily reclaimable matter. Mature tailings dredged from a pond bottom in suspension were mixed with a polymer flocculant and spread over a \"beach\" with a shallow grade where the tailings would dewater and dry under ambient conditions. The dried MFT can then be reclaimed in place or moved to another location for final reclamation. Suncor hoped this would reduce the time for water reclamation from tailings to weeks rather than years, with the recovered water being recycled into the oil sands plant. Suncor claimed the mature fines tailings process would reduce the number of tailing ponds and shorten the time to reclaim a tailing pond from 40 years at present to 7–10 years, with land rehabilitation continuously following 7 to 10 years behind the mining operations. For the reporting periods from 2010 to 2012, Suncor had a lower-than-expected fines capture performance from this technology.\nSyncrude used the older composite tailings (CT) technology to capture fines at its Mildred Lake project. Syncrude had a lower-than-expected fines capture performance in 2011/2012 but exceeded expectations in 2010/2011. Shell used atmospheric fines drying (AFD) technology combined \"fluid tailings and flocculants and deposits the mixture in a sloped area to allow the water to drain and the deposit to dry\" and had a lower-than-expected fines capture performance.\n\nBy 2010 Suncor had transformed their first tailings pond, Pond One, into Wapisiw Lookout, the first reclaimed settling basin in the oil sands. In 2007 the area was a 220-hectare pond of toxic effluent but several years later there was firm land planted with black spruce and trembling aspen. Wapisiw Lookout represents only one percent of tailings ponds in 2011 but Pond One was the first effluent pond in the oil sands industry in 1967 and was used until 1997. By 2011 only 65 square kilometres were cleaned up and about one square kilometre was certified by Alberta as a self-sustaining natural environment. Wapisiw Lookout has not yet been certified. Closure operations of Pond One began in 2007. The jello-like mature fine tails (MFT) were pumped and dredged out of the pond and relocated to another tailings pond for long-term storage and treatment. The MFT was then replaced with 30 million tonnes clean sand and then topsoil that had been removed from the site in the 1960s. The of topsoil over the surface, to a depth of , was placed on top of the sand in the form of hummocks and swales. It was then planted with reclamation plants.\n\nIn March 2012 an alliance of oil companies called Canada’s Oil Sands Innovation Alliance (COSIA) was launched with a mandate to share research and technology to decrease the negative environmental impact of oil sands production focusing on tailings ponds, greenhouse gases, water and land. Almost all the water used to produce crude oil using steam methods of production ends up in tailings ponds. Recent enhancements to this method include Tailings Oil Recovery (TOR) units which recover oil from the tailings, Diluent Recovery Units to recover naphtha from the froth, Inclined Plate Settlers (IPS) and disc centrifuges. These allow the extraction plants to recover well over 90% of the bitumen in the sand.\n\nIn January 2013, scientists from Queen's University published a report analyzing lake sediments in the Athabasca region over the past fifty years. They found that levels of polycyclic aromatic hydrocarbons (PAHs) had increased as much as 23-fold since bitumen extraction began in the 1960s. Levels of carcinogenic, mutagenic, and teratogenic PAHs were substantially higher than guidelines for lake sedimentation set by the Canadian Council of Ministers of the Environment in 1999. The team discovered that the contamination spread farther than previously thought.\n\nThe Pembina Institute suggested that the huge investments by many companies in Canadian oil sands leading to increased production results in excess bitumen with no place to store it. It added that by 2022 a month’s output of waste-water could result in a toxic reservoir the size of New York City's Central Park [840.01 acres (339.94 ha) (3.399 km)].\n\nThe oil sands industry may build a series of up to thirty lakes by pumping water into old mine pits when they have finished excavation leaving toxic effluent at their bottoms and letting biological processes restore it to health. It is less expensive to fill abandoned open pit mines with water instead of dirt. In 2012 the Cumulative Environmental Management Association (CEMA) described End Pit Lakes (EPL) as \n\nCEMA acknowledged that the \"main concern is the potential for EPLs to develop a legacy of toxicity and thus reduce the land use value of the oil sands region in the future.\" Syncrude Canada was planning the first end pit lake in 2013 with the intention of \"pumping fresh water over 40 vertical metres of mine effluent that it has deposited in what it calls 'base mine lake.'\" David Schindler argued that no further end pit lakes should be approved until we \"have some assurance that they will eventually support a healthy ecosystem.\" There is to date no \"evidence to support their viability, or the ‘modelled’ results suggesting that outflow from the lakes will be non-toxic.\"\n\nSee main article Long Lake\n\nIn July 2015, one of the largest leaks in Canada’s history spilled 5,000 cubic metres of emulsion — about 5 million litres of bitumen, sand and wastewater — from a Nexen Energy pipeline at a Long Lake oil sands facility, south of Fort McMurray. The subsidiary of China’s CNOOC Ltd. automated safety systems had not detected the pipeline fault that caused the spill to cover an area of about 16,000 square metres prior to manual inspection. Alberta Energy Regulator (AER) revealed the number of pipeline \"incidents\" in Alberta increased 15% last year, despite the regulator’s well-publicized efforts to reduce ruptures and spills.\n\nAn explosion left one worker dead and another seriously injured at the Chinese-owned Nexen Energy facility in the Long Lake oil sands near Anzac, south of Fort McMurray The two maintenance workers involved were found near natural gas compression equipment used for a hydrocracker, which turns heavy oil into lighter crude, at the plant’s main processing facility, known as an upgrader.\n\nThe Athabasca oil sands are located in the northeastern portion of the Canadian province of Alberta, near the city of Fort McMurray. The area is only sparsely populated, and in the late 1950s, it was primarily a wilderness outpost of a few hundred people whose main economic activities included fur trapping and salt mining. From a population of 37,222 in 1996, the boomtown of Fort McMurray and the surrounding region (known as the Regional Municipality of Wood Buffalo) grew to 79,810 people as of 2006, including a \"shadow population\" of 10,442 living in work camps, leaving the community struggling to provide services and housing for migrant workers, many of them from Eastern Canada, especially Newfoundland. Fort McMurray ceased to be an incorporated city in 1995 and is now an urban service area within Wood Buffalo.\n\nThe Alberta government's Energy and Utilities Board (EUB) estimated in 2007 that about of crude bitumen were economically recoverable from the three Alberta oil sands areas based on then-current technology and price projections from the 2006 market prices of $62 per barrel for benchmark West Texas Intermediate (WTI), rising to a projected $69 per barrel. This was equivalent to about 10% of the estimated of bitumen-in-place. Alberta estimated that the Athabasca deposits alone contain of surface mineable bitumen and of bitumen recoverable by in-situ methods. These estimates of Canada's reserves were doubted when they were first published but are now largely accepted by the international oil industry. This volume placed Canadian proven reserves second in the world behind those of Saudi Arabia.\n\nOnly 3% of the initial established crude bitumen reserves have been produced since commercial production started in 1967. At rate of production projected for 2015, about , the Athabasca oil sands reserves would last over 170 years. However those production levels require an influx of workers into an area that until recently was largely uninhabited. By 2007 this need in northern Alberta drove unemployment rates in Alberta and adjacent British Columbia to the lowest levels in history. As far away as the Atlantic Provinces, where workers were leaving to work in Alberta, unemployment rates fell to levels not seen for over one hundred years.\n\nThe Venezuelan Orinoco Oil Sands site may contain more oil sands than Athabasca. However, while the Orinoco deposits are less viscous and more easily produced using conventional techniques (the Venezuelan government prefers to call them \"extra-heavy oil\"), they are too deep to access by surface mining.\n\nDespite the large reserves, the cost of extracting the oil from bituminous sands has historically made production of the oil sands unprofitable—the cost of selling the extracted crude would not cover the direct costs of recovery; labour to mine the sands and fuel to extract the crude.\nIn mid-2006, the National Energy Board of Canada estimated the operating cost of a new mining operation in the Athabasca oil sands to be C$9 to C$12 per barrel, while the cost of an in-situ SAGD operation (using dual horizontal wells) would be C$10 to C$14 per barrel.\nThis compares to operating costs for conventional oil wells which can range from less than one dollar per barrel in Iraq and Saudi Arabia to over six in the United States and Canada's conventional oil reserves.\n\nThe capital cost of the equipment required to mine the sands and haul it to processing is a major consideration in starting production. The NEB estimates that capital costs raise the total cost of production to C$18 to C$20 per barrel for a new mining operation and C$18 to C$22 per barrel for a SAGD operation. This does not include the cost of upgrading the crude bitumen to synthetic crude oil, which makes the final costs C$36 to C$40 per barrel for a new mining operation.\n\nTherefore, although high crude prices make the cost of production very attractive, sudden drops in price leaves producers unable to recover their capital costs—although the companies are well financed and can tolerate long periods of low prices since the capital has already been spent and they can typically cover incremental operating costs.\n\nHowever, the development of commercial production is made easier by the fact that exploration costs are very low. Such costs are a major factor when assessing the economics of drilling in a traditional oil field. The location of the oil deposits in the oil sands are well known, and an estimate of recovery costs can usually be made easily. There is not another region in the world with energy deposits of comparable magnitude where it would be less likely that the installations would be confiscated by a hostile national government, or be endangered by a war or revolution.\n\nAs a result of the oil price increases since 2003, the economics of oil sands have improved dramatically. At a world price of US$50 per barrel, the NEB estimated an integrated mining operation would make a rate return of 16 to 23%, while a SAGD operation would return 16 to 27%. Prices since 2006 have risen, exceeding US$145 in mid-2008 but falling back to less than 40 US$ as a result of the worldwide financial crisis, the oil price recovered slowly and many of the planned projects (expected to exceed C$100 billion between 2006 and 2015) were stopped or scheduled. In 2012 and 2013 the oil price was high again, but the US production is increasing due to new technologies, while the gasoline demand is falling, so there is an overproduction of oil. But recovering economy can change this in a few years.\n\nAt present the area around Fort McMurray has seen the most effect from the increased activity in the oil sands. Although jobs are plentiful, housing is in short supply and expensive. People seeking work often arrive in the area without arranging accommodation, driving up the price of temporary accommodation. The area is isolated, with only a two-lane road, Alberta Highway 63, connecting it to the rest of the province, and there is pressure on the government of Alberta to improve road links as well as hospitals and other infrastructure.\n\nDespite the best efforts of companies to move as much of the construction work as possible out of the Fort McMurray area, and even out of Alberta, the shortage of skilled workers is spreading to the rest of the province. Even without the oil sands, the Alberta economy would be very strong, but development of the oil sands has resulted in the strongest period of economic growth ever recorded by a Canadian province.\n\nThe Athabasca oil sands are often a topic in international trade talks, with energy rivals China and the United States negotiating with Canada for a bigger share of the rapidly increasing output. Production is expected to quadruple between 2005 and 2015, reaching a day, with increasing political and economic importance. Currently, most of the oil sands production is exported to the United States.\n\nAn agreement has been signed between PetroChina and Enbridge to build a pipeline from Edmonton, Alberta, to the west coast port of Kitimat, British Columbia. If it is built, the pipeline will help export synthetic crude oil from the oil sands to China and elsewhere in the Pacific. However, in 2011, First Nations and environmental groups protested the proposed pipeline, stating that its construction and operation would be destructive to the environment. First Nations groups also claim that the development of the proposed pipeline is in violation of commitments that the Government of Canada has made through various Treaties and the UN Declaration of the Rights of Indigenous Peoples. A smaller pipeline will also be built alongside to import condensate to dilute the bitumen. Sinopec, the largest refining and chemical company in China, and China National Petroleum Corporation have bought or are planning to buy shares in major oil sands development.\n\nOn August 20, 2009, the U.S. State Department issued a presidential permit for an Alberta Clipper Pipeline that will run from Hardisty, Alberta to Superior, Wisconsin. The pipeline will be capable of carrying up to of crude oil a day to refineries in the U.S.\n\nIndigenous peoples of the area include the Fort McKay First Nation. The oil sands themselves are located within the boundaries of Treaty 8, signed in 1899, which states:\n\nThe Fort McKay First Nation has formed several companies to service the oil sands industry and will be developing a mine on their territory.\nOpposition remaining within the First Nation focuses on environmental stewardship, land rights, and health issues, like elevated cancer rates in Fort Chipewyan and deformed fish being found by commercial fishermen in Lake Athabasca.\n\nThe Alberta Cancer Board published research of the cancer rates of those living in Fort Chipewyan, Alberta in 2009. While many companies argue that there are not enough chemicals and toxic material in the water due to the development of the oil sands, this report indicates that there is coincidentally a significantly higher rate of cancer within this community. There have been many speculations as to why there is a higher rate of cancer in this community; some of those speculations are contamination with the river and the oil sands as well as uranium mining that is currently in progress. The world’s largest production of uranium is produced in this area as well as along the Athabasca River, allowing for easy contamination of the river.\n\nPipeline development poses significant risks to the cultural, social, and economic way of life of Canada’s Indigenous populations. Historically, many Indigenous groups have opposed pipeline development for two primary reasons: 1) the inherent environmental risks associated with transporting harmful oil and gas products, and 2) failure by the federal government to properly consider and mitigate Indigenous groups’ concerns regarding resource development on their lands. For instance, many Indigenous groups rely heavily on local wildlife and vegetation for their survival. Increased oil production in Canada requires greater oil transport through their traditional lands, which poses serious threats to the survival and traditional way of life of Indigenous groups, as well as the safety and preservation of the surrounding ecosystems. As well, First Nation's in Alberta have called particular attention to adverse health impacts related to oil sands emissions, asserting that the water quality testing for specific chemicals (heavy metals) has been insufficient.\n\nAside from environmental concerns, many Indigenous groups have pushed back against pipeline development due to inadequate consultation processes by the federal government. As per Section 35 of the Canadian Constitution Act Indigenous peoples in Canada are guaranteed the right to be meaningfully consulted with and accommodated when the Crown is contemplating resource development on their lands - see Duty to Consult. Through a series of Supreme Court of Canada rulings and political protests from Indigenous peoples (see Haida Nation v. British Columbia [Minister of Forests], Taku River Tlingit First Nation v British Columbia, and Tsilhqot’in Nation v British Columbia), among others, the courts have attempted to further define the Crown’s consultation responsibilities and give legal recognition to Indigenous traditional territory and rights regarding resource development.\n\nContrarily, oil sands development also presents many positive impacts and opportunities for Indigenous groups, particularly in Western Canada. In fact, over the past two decades, First Nations participation in the energy sector has increased dramatically, from employment and business opportunities to project approval processes and environmental evaluation. Increased Indigenous participation has been encouraged by numerous collaboration agreements with industry, typically in the form of impact benefit agreements (IBAs), which provide not only employment and business ventures, but also job training and community benefits. Enhanced participation in the energy sector has empowered many Indigenous groups to push for wider involvement by negotiating ownership stakes in proposed pipelines and bitumen storage projects. Perhaps the best example of such partnering in Alberta is the agreement between Suncor and Fort McKay and Mikisew Cree First Nations. The two First Nations acquired a 49% ownership in Suncor’s East Tank Farm Development with shares valued at about $500 million making it the largest business investment to date by a First Nation entity in Canada.\n\nSupport for resource development and desire for direct involvement is further illustrated by the First Nations’ led $17-billion Eagle Spirit Energy Holding Ltd. pipeline and energy corridor between Alberta and the northern B.C. coast (with a back-up plan to site its terminal in Alaska to get around the tanker ban in B.C.). The project has secured support from 35 First Nations along the proposed route; the bands are entitled to at least 35% ownership in exchange for the land use.\n\nThere are currently three large oil sands mining operations in the area run by Syncrude Canada Limited, Suncor Energy and Albian Sands owned by Shell Canada, Chevron, and Marathon Oil Corp.\n\nMajor producing or planned developments in the Athabasca Oil Sands include the following projects:\n\n\nFor improper diversion of water in 2008/2009, Statoil Canada Ltd. was ordered in 2012 to pay a fine of $5000 and to allocate $185,000 for a training project (The verdict was handed down by the Provincial Court of Alberta, Criminal Division).\n\n\n\n\n"}
{"id": "8811111", "url": "https://en.wikipedia.org/wiki?curid=8811111", "title": "Bizerba", "text": "Bizerba\n\nBizerba SE & Co. KG is a German provider of weighing and slicing technologies for industry and trade and is a worldwide leading specialist in industrial weighing and labeling technologies.\n\nIt was founded in 1866 by the Bizer brothers in Balingen. Its name is a composite of the city's name and Bizer. Bizerba currently has over 100 divisions worldwide and a headquarters located in Balingen, Germany.\n\nBizerba has approximately 4100 employees worldwide, including its headquarters and 3 production facilities in Germany as well as 41 subsidiaries and 69 national representatives.\n\nBizerba USA, Inc. has been incorporated in 1984 as a subsidiary of parent company Bizerba SE & Co. KG (Germany). Its corporate office is located in Joppa, MD. Bizerba is working with factory authorized distributors in all 50 states and Puerto Rico. Bizerba Engineered Solutions is a new subsidiary located in Richmond USA.\n\nBizerba is well known in the food industry for pioneering some very important developments and achievements:\n\n\n"}
{"id": "1008539", "url": "https://en.wikipedia.org/wiki?curid=1008539", "title": "Calendering (textiles)", "text": "Calendering (textiles)\n\nCalendering of textiles is a finishing process used to smooth, coat, or thin a material. With textiles, fabric is passed between calender rollers at high temperatures and pressures. Calendering is used on fabrics such as moire to produce its watered effect and also on cambric and some types of sateens.\n\nIn preparation for calendering, the fabric is folded lengthwise with the front side, or face, inside, and stitched together along the edges. The fabric can be folded together at full width, however this is not done as often as it is more difficult. The fabric is then run through rollers that polish the surface and make the fabric smoother and more lustrous. High temperatures and pressure are used as well. Fabrics that go through the calendering process feel thin, glossy and papery.\n\nThe wash durability of a calendared finish on thermoplastic fibers like polyester is higher than on cellulose fibers such as cotton. On blended fabrics such as Polyester/Cotton the durability depends largely on the proportion of synthetic fiber component present as well as the amount and type of finishing additives used and the machinery and process conditions employed.\n\nSeveral different finishes can be achieved through the calendering process by varying different parts. The main different types of finishes are \"beetling\", \"watered\", \"embossing\" and \"Schreiner\".\n\nBeetling is a finish given to cotton and linen cloth, and makes it look like satin. In the beetling process the fabric goes over wooden rollers and is beaten with wooden hammers.\n\nThe watered finish, also known as moire, is produced by using ribbed rollers. These rollers compress the cloth and the ribs produce the characteristic watermark effect by moving aside threads as well as compressing them. This leaves some of the threads round while others get compressed and become flat.\n\nIn the embossing process the rollers have engraved patterns on them, and the patterns become stamped onto the fabric. The end result is a raised or sunken pattern, depending on the roller. This works best with soft fabrics.\n\nSimilar to the watered process, in the Schreiner process the rollers are ribbed, only in the Schreiner process the ribs are very fine, with as many as six hundred ribs per inch under extremely high pressure. The threads are pressed flat with little lines in them, which causes the fabric to reflect the light better than a flat surface would. Cloth finished with the Schreiner method has a very high lustre, which is made more lasting by heating the rollers.\n\n"}
{"id": "27324966", "url": "https://en.wikipedia.org/wiki?curid=27324966", "title": "Carbon Nation", "text": "Carbon Nation\n\nCarbon Nation is a 2010 documentary film by Peter Byck about technological- and community-based energy solutions to the growing worldwide carbon footprint. The film is narrated by Bill Kurtis. ASIN: B0055T46LA (Rental) and B0055T46G0 (Purchase).\n\nRather than highlighting the problems with use of fossil fuels, \"Carbon Nation\" presents a series of ways in which the 16 terawatts of energy the world consumes can be met while reducing or eliminating carbon-based sources. It contains optimistic interviews with experts in various fields, business CEOs, and sustainable energy supporters to present a compelling case for change while having a neutral, matter-of-fact explanation.\n\nAmong those interviewed are Richard Branson, former CIA Director R. James Woolsey, Earth Day founder Denis Hayes and environmental advocate Van Jones.\n\nMuch of the content of the film consists of interviews, some are listed above. The list of interviewees also includes\n\n\n"}
{"id": "7578885", "url": "https://en.wikipedia.org/wiki?curid=7578885", "title": "Chappell Hayes", "text": "Chappell Hayes\n\nChappell Hayes (1949–1994) was a political activist in West Oakland.\n\nHayes campaigned against polluting industries in West Oakland. Hayes fought against reconstruction of the Cypress Street Viaduct through the center of West Oakland, forced the port to spend millions on environmental initiatives and helped get like-minded residents elected to organizations throughout the Bay Area.\n\nWest Oakland's McClymonds High School's on-campus health clinic is named in his honor.\n\nHayes was married to Oakland City Council member and 2006 mayoral candidate Nancy Nadel.\n\n"}
{"id": "21357453", "url": "https://en.wikipedia.org/wiki?curid=21357453", "title": "Clearance cairn", "text": "Clearance cairn\n\nA clearance cairn is an irregular and unstructured collection of fieldstones which have been removed from arable land or pasture to allow for more effective agriculture and collected into a usually low mound or cairn. Commonly of Bronze Age origins, these cairns may be part of a cairnfield (a collection of closely spaced cairns) where some cairns might be funerary. Clearance cairns are a worldwide phenomenon wherever organised agriculture has been practised.\n\nBy removing large and moderate size stones from the surface and sub-surface, ploughing can take place with much less potential damage taking place to the plough blade. Stones also prevent the growth of plants where they physically block access to the soil and their removal allows for a greater surface area for crops to grow and to allow for plants to grow to their full potential; Stones also increase drainage and may therefore deprive plants of moisture. Stones were removed from fields to allow for the efficient use of hand tools, animal powered machines on such fields in the past and for tractors, etc. in more recent times. Some prehistoric clearance cairns may also contain burials. A few clearance cairns may have been additionally created as boundary markers. As ploughing developed and specialised the greater depth tilled and the increased power of mechanised ploughing resulted in more and larger stones being brought to the surface and these too had to be removed.\n\nField clearance cairns sometimes survive long after cleared fields have fallen out of use and are often the only surviving evidence of past agricultural activity in areas of woodland, upland areas, etc. The existence of these cairns was once regarded as suggesting pastoral farming. Many now consider them the result of arable or mixed agricultural exploitation enabling ploughing or hand tillage to work more efficiently.\n\nThe possibility exists that the stone piles were built in the middle of the enclosures, with an additional purpose of maximising the effects of daytime radiance; the holding in of heat caught during the day, and its release at night would increase the temperature of the field very slightly. This effect would encourage crop growth and reduce the risk of frost. Walled gardens produce the same effect.\n\nMany clearance stones were used in the construction of defensive structures, houses, farm buildings, walls, drainage ditches, road metalling, etc. Where permanent clearance cairns were formed it was predominately on waste land, such as steep slopes, edges of woodland, field corners, and around earth-fast boulders.\n\nThe amount of stone which needed to be cleared from fields depended upon the local geology, glacial deposits, glacial erratics, etc. Many cairns are mainly composed of stones which could have been carried by a single person who would have followed the plough as it turned up small boulders, etc.\nSome surviving cairns are of considerable antiquity as field clearance has been practised since the beginnings of agriculture in the Stone Age, such as at the Gardberg site in Vestre Slidre, Oppland, Norway. A Bronze Age origin for clearance cairns is more common.\n\nCairns may be discrete, in large groups (cairnfields) or as linear formations—linear cairns. Many stones in clearance cairns show plough-marks or ard-marks at various angles to each other, typical of the Bronze Age ploughing methods. These ard-marks indicate that the boulders were obstructing the ploughing process and were in situ for some time before removal.\n\nOther items such as bog-wood and grubbed out tree stumps were sometimes added to the cairns as a result of tree felling and the process of creation of pasture from woodland. Bog-wood stumps and trunks can be thousands of years old, are darker in colour due to the tannins from the peat and do not show typical wood decomposition characteristics due to their preservation within peat bogs. The absence of saw marks indicates felling using axes, providing further information about the age of the wood involved.\n\nSome clearance cairns are linear in form. Many of the smaller cairns were probably created by family groups, whilst larger ones would have required organised labour.\n\nCairnfields have on occasions been confused with various other classes of monuments, such as round barrow cemeteries and groups of round barrows, stone hut circles, ring cairns, or burnt mounds. In general round barrows are larger, more regular, and may contain visible traces of a cist or kerb; stone hut circles have distinct entrances; ring cairns have a hollow centre; and burnt mounds contain a high proportion of fire-crazed stones of rather smaller size than appear in the average clearance cairns within cairnfields.\n\nNatural deposits such as so called 'clitter agglomerates', moraines can also on occasion look rather similar to groups of clearance cairns, but their context, position and regular form usually betrays their true origins.\n\nClearance cairn material is often heterogeneous, typically consisting of both edged and rounded small stones, sometimes mixed with sand, clay or silt, lifted from the field nearby. Agricultural and clearance cairns are sometimes located on slopes, whilst burial cairns rarely are.\n\nClearance cairns constructed using mechanised methods with horse or motor powered assistance may appear superficially similar, however the stones are generally much larger and they are more randomly tipped rather than hand placed.\nDue to the age and 'abandoned' state of many clearance cairns they are often good sites for the growth and survival of lichens, mosses, ferns and other plants; the actual species being dependent on the rock type. Cairns are relatively undisturbed and emulate old walls for the micro-habitats they produce.\n\nIn some areas which are now unproductive moorland, aerial photographs show the presence of numerous prehistoric clearance cairns and associated dwellings such as hut-circles. This clear evidence of previous ancient habitation and cultivation shows that the climate worsened until the farmers were forced to move to lower ground.\n\nStone carry or the stone walk is a traditional Scottish athletic event involving the carrying of large stones down the field of competition; the competitors pick up two heavy stones with iron handles, and carry the pair as far as they can. The sport may have grown out of the act of clearing stones from fields to create pastures and arable land.\n\n"}
{"id": "30651268", "url": "https://en.wikipedia.org/wiki?curid=30651268", "title": "Climax Uranium Mill", "text": "Climax Uranium Mill\n\nClimax Uranium Mill is a decommissioned uranium mill near Grand Junction, CO.\n\nThe mill, that processed vanadium as well as uranium, was incorporated May 11, 1950. It was constructed on city-owned property next to the Colorado River which was once the Grand Junction sugar beet mill. Climax Uranium Company gutted the former sugar beet mill, removing any remaining equipment and stabilizing weak walls, and began uranium and vanadium milling operations. The mill soon grew to be 12 buildings large and processed 2 million tons of ore, mostly for the United States Atomic Energy Commission.\n\nFrom the early 1950s to 1966, Climax donated approximately 300,000 tons of radioactive uranium tailings to the city of Grand Junction for use as a construction material. The tailings were used in sewer and road construction. In 1972, Congress gave the Atomic Energy Commission authority to cooperate with Grand Junction in a cleanup operation, with the federal government covering 75% of the project's cost.\n\nThe mill was decommissioned in 1970 under the US Department of Energy's Uranium Mill Tailings Remedial Action project. The mill site was designated for environmental remediation by the Uranium Mill Tailings Radiation Control Act in 1978.\n\nThe Nuclear Regulatory Commission published a Remedial Action Plan for the site in 1994, and the removal of 4.5 million cubic yards of contaminated materials from the site to a disposal cell was completed that spring. Reseeding and wetlands establishment was completed in August 1994.\n"}
{"id": "52392767", "url": "https://en.wikipedia.org/wiki?curid=52392767", "title": "Cobblestone Street (Boonville, Missouri)", "text": "Cobblestone Street (Boonville, Missouri)\n\nCobblestone Street, also known as Fifth Street and Main Street, is a historic cobblestone street located at Boonville, Cooper County, Missouri. It was built about 1832, and was part of the original Fifth or Main Street. It is located beneath the Boonville Road Bridge and is constructed of cobblestones of varying sizes. The street remnant is approximately 20 feet wide and approximately 200 feet long. The street connected the main commercial district of Boonville with the wharves along the Missouri River.\n\nIt was listed on the National Register of Historic Places in 1990.\n"}
{"id": "16892128", "url": "https://en.wikipedia.org/wiki?curid=16892128", "title": "Come By Chance Refinery", "text": "Come By Chance Refinery\n\nCome By Chance Refinery (also known as North Atlantic Refinery) is a crude oil refinery located in Come By Chance in Newfoundland and Labrador, Canada. It has a refinery capacity of . It is operated by North Atlantic Refining and was owned by Korea National Oil Corporation until 2014, when SilverRange Capital Partners, a New-York Based commodity merchant bank, bought the refinery. SilverRange is now Silverpeak Strategic Partners.\n\nThe refinery was built by John Shaheen's Shaheen Resources in 1971–1973. It was operated in 1973–1976 until the bankruptcy of Shaheen Resources. The refinery was restarted in 1986 by Newfoundland Processing Ltd. In August 1994, the Vitol Group purchased the refinery and the operating company North Atlantic Refining was founded. In October 2006, Calgary-based Harvest Energy Trust purchased North Atlantic Refining for $1.6 billion, and in October 2009, the company was purchased by Korea National Oil Corporation. The refinery was expanded to a capacity of 130,000 bbl after a revamp by Silverpeak in 2016. \n\n"}
{"id": "12703254", "url": "https://en.wikipedia.org/wiki?curid=12703254", "title": "Custody transfer", "text": "Custody transfer\n\nCustody Transfer in the oil and gas industry refers to the transactions involving transporting physical substance from one operator to another. This includes the transferring of raw and refined petroleum between tanks and tankers; tankers and ships and other transactions. Custody transfer in fluid measurement is defined as a metering point (location) where the fluid is being measured for sale from one party to another. During custody transfer, accuracy is of great importance to both the company delivering the material and the eventual recipient, when transferring a material.\n\nThe term \"fiscal metering\" is often interchanged with custody transfer, and refers to metering that is a point of a commercial transaction such as when a change in ownership takes place. Custody transfer takes place any time fluids are passed from the possession of one party to another.\n\nCustody transfer generally involves:\n\nDue to the high level of accuracy required during custody transfer applications, the flowmeters which are used to perform this are subject to approval by an organization such as the American Petroleum Institute (API).\nCustody transfer operations can occur at a number of points along the way; these may include operations, transactions or transferring of oil from an oil production platform to a ship, barge, railcar, truck and also to the final destination point, such as a refinery.\n\nCustody transfer is one of the most important applications for flow measurement. Many flow measurement technologies are used for custody transfer applications; these include differential pressure (DP) flowmeters, turbine flowmeters, positive displacement flowmeters, Coriolis flowmeters and ultrasonic flowmeters.\n\nDifferential pressure (DP) flowmeters are used for the \"custody transfer\" of liquid and gas to measure the flow of liquid, gas, and steam. The DP flowmeter consist of a differential pressure transmitter and a primary element. The primary element places a constriction in a flow stream, while the DP transmitter measures the difference in pressure upstream and downstream of the constriction.\n\nIn many cases, pressure transmitters and primary elements are bought by the end-users from different suppliers. However, several vendors have integrated the pressure transmitter with the primary element to form a complete flowmeter. The advantage of this is that they can be calibrated with the primary element and DP transmitter already in place.\n\nStandards and criteria for the use of DP flowmeters for custody transfer applications are specified by the American Gas Association (AGA) and the American Petroleum Institute (API).\n\nAn advantage of using a DP flowmeters is that they are the most studied and best understood type of flowmeter. A disadvantage of using a DP flowmeters is that they introduce a pressure drop into the flowmeter line. This is a necessary result of the constriction in the line that is required to make the DP flow measurement.\n\nOne important development in the use of DP flowmeters for custody transfer applications has been the development of single and dual chamber orifice fittings.\n\nThe first turbine flowmeter was invented by Reinhard Woltman, a German engineer in 1790. Turbine flowmeters consist of a rotor with propeller-like blades that spins as water or some other fluid passes over it. The rotor spins in proportion to flow rate (see turbine meters) . There are many types of turbine meters, but many of those used for gas flow are called axial meters. \n\nThe turbine flowmeter is most useful when measuring clean, steady, high-speed flow of low-viscosity fluids. In comparison to other flowmeters, the turbine flowmeter has a significant cost advantage over ultrasonic flowmeters, especially in the larger line sizes, and it also has a favourable price compared to the prices of DP flowmeters, especially in cases where one turbine meter can replace several DP meters.\n\nThe disadvantage of turbine flowmeters is that they have moving parts that are subject to wear. To prevent wear and inaccuracy, durable materials are used, including ceramic ball bearings.\n\nPositive displacement (PD) flowmeters are highly accurate meters that are widely used for custody transfer of commercial and industrial water, as well as for custody transfer of many other liquids. PD flowmeters have the advantage that they have been approved by a number of regulatory bodies for this purpose, and they have not yet been displaced by other applications.\n\nPD meters excel at measuring low flows, and also at measuring highly viscous flows, because PD meters captures the flow in a container of known volume. Speed of flow doesn’t matter when using a PD meter.\n\nCoriolis flowmeters have been around for more than 30 years and are preferred in process industries such as chemical and food and beverage. Coriolis technology offers accuracy and reliability in measuring material flow, and is often hailed as among the best flow measurement technologies due to direct mass flow, fluid density, temperature, and precise calculated volume flow rates. Coriolis meters do not have any moving parts and provide long term stability, repeatability, and reliability. Because they are direct mass flow measurement devices, Coriolis meters can handle the widest range of fluids from gases to heavy liquids and are not impacted by viscosity or density changes that often effect velocity based technologies (PD, Turbine, Ultrasonic). With the widest flow range capability of any flow technology, Coriolis can be sized for low pressure drop. This combined with the fact that they are not flow profile dependent helps eliminate the need for straight runs and flow conditioning which enables custody transfer systems to be designed with minimal pressure drop. \n\nIt has to be mentioned that any measurement instrument that relies on one measurement principle only will show a higher measurement uncertainty under two-phase flow conditions. Conventional measurement principles, like positive displacement, turbine meters, orifice plates will seemingly continue to measure, but will not be able to inform the user about the occurrence of two-phase flow. Yet modern principles based on the Coriolis effect or ultrasonic flow measurement will inform the user by means of diagnostic functions.\n\nFlow is measured using Coriolis meters by analyzing the changes in the Coriolis force of a flowing substance. The force is generated in a mass moving within a rotating frame of reference. An angular, outward acceleration, which is factored with linear velocity is produced due to the rotation. With a fluid mass, the Coriolis force is proportional to the mass flow rate of that fluid.\n\nA Coriolis meter has two main components: an oscillating flow tube equipped with sensors and drivers, and an electronic transmitter that controls the oscillations, analyzes the results, and transmits the information. The Coriolis principle for flow measurement requires the oscillating section of a rotating pipe to be exploited. Oscillation produces the Coriolis force, which traditionally is sensed and analyzed to determine the rate of flow. Modern coriolis meters utilize the phase difference measured at each end of the oscillating pipe.\n\nUltrasonic flowmeters were first introduced into industrial markets in 1963 by Tokyo Keiki (now Tokimec) in Japan. Custody transfer measurements have been around for a long time, and over the past ten years, Coriolis and ultrasonic meters have become the flowmeters of choice for custody transfer in the oil and gas industry.\n\nUltrasonic meters provide volumetric flow rate. They typically use the transit-time method, where sounds waves transmitted in the direction of fluid flow travel faster than those travelling upstream. The transit time difference is proportional to fluid velocity. Ultrasonic flow meters have negligible pressure drop if recommended installation is followed, have high turndown capability, and can handle a wide range of applications. Crude oil production, transportation, and processing are typical applications for this technology.\n\nThe use of ultrasonic flowmeters is continuing to grow for custody transfer. Unlike PD and turbine meters, ultrasonic flowmeters do not have moving parts. Pressure drop is much reduced with an ultrasonic meter when compared to PD, turbine, and DP meters. Installation of ultrasonic meters is relatively straightforward, and maintenance requirements are low.\n\nIn June 1998, The American Gas Association published a standard called AGA-9. This standard lays out the criteria for the use of Ultrasonic flowmeters for Custody Transfer of Natural Gas.\n\nCustody transfer requires an entire metering system that is designed and engineered for the application, not just flowmeters. Components of a custody transfer system typically include: \nA typical liquid custody transfer skid includes multiple flowmeters and meter provers. Provers are used to calibrate meters in-situ and are performed frequently; typically before, during, and after a batch transfer for metering assurance. A good example of this is a Lease Automatic Custody Transfer(LACT) unit in a crude oil production facility.\n\nIn the ISO 5725-1 standard accuracy for measuring instruments is defined as “the closeness of agreement between a test result and the accepted reference value”. This term “accuracy” includes both the systematic error and the bias component. Each device has its manufacturer stated accuracy specification and its tested accuracy. Uncertainty takes all the metering system factors that impact measurement accuracy into account. The accuracy of flowmeters could be used in two different metering systems that ultimately have different calculated uncertainties due to other factors in the system that affect flow calculations. Uncertainty even includes such factors as the flow computer's A/D converter accuracy. The quest for accuracy in a custody transfer system requires meticulous attention to detail.\n\nCustody transfer metering systems must meet requirements set by industry bodies such as AGA, API, or ISO, and national metrology standards such as OIML (International), NIST (U.S.), PTB (Germany), CMC (China), and GOST (Russia), among others. These requirements can be of two types: Legal and Contract.\n\nThe national Weights & Measures codes and regulations control the wholesale and retail trade requirements to facilitate fair trade. The regulations and accuracy requirements vary widely between countries and commodities, but they all have one common characteristic - “traceability”. There is always a procedure that defines the validation process where the duty meter is compared to a standard that is traceable to the legal metrology agency of the respective region.\n\nA contract is a written agreement between buyers and sellers that defines the measurement requirements. These are large-volume sales between operating companies where refined products and crude oils are transported by marine, pipeline or rail. Custody transfer measurement must be at the highest level of accuracy possible because a small error in measurement can amount to a large financial difference. Due to these critical natures of measurements, petroleum companies around the world have developed and adopted standards to meet the industry's needs.\n\nIn Canada, for instance, all measurement of a custody transfer nature falls under the purview of Measurement Canada. In the USA, the Federal Energy Regulatory Commission (FERC) controls the standards which must be met for interstate trade.\n\nCustody transfer of liquid flow measurement follow guidelines set by the ISO. By industrial consensus, liquid flow measurement is defined as having an overall uncertainty of ±0.25% or better. The overall uncertainty is derived from an appropriate statistical combination of the component uncertainties in the measurement system.\n\nLiquid flow measurements are usually in volumetric or mass unit. Volume is normally used for stand-alone field tanker loading operations, while mass is used for multi-field pipeline or offshore pipeline with an allocation requirement.\n\nMass measurement and reporting are achieved by\n\nAn automatic flow-proportional sampling system is used in flow measurement to determine the average water content, average density and for analysis purposes. Sampling systems should be broadly in accordance with ISO 3171.\nThe sampling system is a critical section during flow measurement. Any errors introduced through sampling error will generally have a direct, linear effect on the overall measurement.\n\nTemperature and pressure measurement are important factors to consider when taking flow measurements of liquids. Temperature and pressure measurement points should be situated as close to the meter as possible, in reference to their conditions at the meter inlet. Temperature measurements that affect the accuracy of the metering system should have an overall loop accuracy of 0.5°C or better, and the corresponding readout should have a resolution of 0.2°C or better.\n\nTemperature checks are performed by certified thermometers with the aid of Thermowells\n\nPressure measurements that affect the accuracy of the metering system should have an overall loop accuracy of 0.5 bar or better and the corresponding readout should have a resolution of 0.1 bar or better.\n\nCustody transfer of gaseous flow measurement follow guidelines set by the international bodies. By industrial consensus, gaseous flow measurement is defined as mass flow measurement with an overall uncertainty of ±1.0% or better. The overall uncertainty is derived from an appropriate statistical combination of the component uncertainties in the measurement system.\n\nAll gaseous flow measurement must be made on single-phase gas streams, having measurements in either volumetric or mass units.\n\nSampling is an important aspect, as they help to ascertain accuracy. Apt facilities should be provided for the purpose of obtaining representative samples. The type of instrumentation and the measuring system may influence this requirement.\n\nGas density at the meter may be determined either by:\nMost industries prefer to use the continuous measurement of gas density. However, both methods may be used simultaneously, and the comparison of their respective results may provide additional confidence in the accuracy of each method.\n\nIn any custody transfer application, a true random uncertainty has an equal chance of favouring either party, the net impact should be zero to both parties, and measurement accuracy and repeatability should not be valued. Measurement accuracy and repeatability are of high value to most seller because many users install check meters.\nThe first step in designing any custody transfer system is to determine the mutual measurement performance expectations of the supplier and the user over the range of flow rates. This determination of mutual performance expectations should be made by individuals who have a clear understanding of all of the costs of measurement disputes caused by poor repeatability.\nThe second step is to quantify the operating conditions which are not controllable. For a flow measurement, these can include:\nThe third and final step is to select hardware, installation and maintenance procedures which will ensure that the measurement provides the required installed performance under the expected (uncontrollable) operating conditions. For example, the user can:\nWhile the first and second steps involve gathering data, the third step may require calculations and/or testing.\n\nThe formula for calculating the LNG transferred depends on the contractual sales conditions. These can relate to three types of sale contract as defined by Incoterms 2000: an FOB sale, a CIF sale or a DES sale. \n\nIn the case of an FOB (Free On Board) sale, the determination of the energy transferred and invoiced for will be made in the loading port. \n\nIn the case of a CIF (Cost Insurance & Freight) or a DES (Delivered Ex Ship) sale, the energy transferred and invoiced for will be determined in the unloading port. \n\nIn FOB contracts, the buyer is responsible to provide and maintain the custody transfer measurement systems on board the vessel for volume, temperature and pressure determination and the seller is responsible to provide and maintain the custody transfer measurement systems at the loading terminal such as the sampling and gas analysis. For CIF and DES contracts the responsibility is reversed. \n\nBoth buyer and seller have the right to verify the accuracy of each system that is provided, maintained and operated by the other party. \nThe determination of the transferred energy usually happens in the presence of one or more surveyors, the ship’s cargo officer and a representative of the LNG terminal operator. A representative of the buyer can also be present.\n\nIn all cases, the transferred energy can be calculated with the following formula: \n\nE =(VLNG × DLNG × GVCLNG) - Egas displaced ± Egas to ER (if applicable)\n\nWhere: \n\nE = the total net energy transferred from the loading facilities to the LNG carrier, or from the LNG carrier to the unloading facilities.\nVLNG= the volume of LNG loaded or unloaded in m3.\nDLNG = the density of LNG loaded or unloaded in kg/m3.\nGCVLNG = the gross calorific value of the LNG loaded or unloaded in MMBTU/kg\n\nE gas displaced = The net energy of the displaced gas, also in MMBTU, which is either: \n\nE(gas to ER) = If applicable, the energy of the gas consumed in the LNG carrier’s engine room during the time between opening and closing custody transfer surveys, i.e. used by the vessel at the port, which is:\n\n\"+ For an LNG loading transfer\" or \n\n\"- For an LNG unloading transfer\"\n\n"}
{"id": "46797", "url": "https://en.wikipedia.org/wiki?curid=46797", "title": "Death Valley", "text": "Death Valley\n\nDeath Valley is a desert valley located in Eastern California, in the northern Mojave Desert bordering the Great Basin Desert. It is one of the hottest places in the world at the height of summertime along with deserts in the Middle East.\n\nDeath Valley's Badwater Basin is the point of the lowest elevation in North America, at below sea level. This point is east-southeast of Mount Whitney, the highest point in the contiguous United States with an elevation of 14,505 feet (4,421 m). Death Valley's Furnace Creek holds the record for the highest reliably recorded air temperature on Earth at 134 °F (56.7 °C) on July 10, 1913, as well as the highest recorded natural ground surface temperature on Earth at 201 °F (93.9 °C) on July 15, 1972.\n\nLocated near the border of California and Nevada, in the Great Basin, east of the Sierra Nevada mountains, Death Valley constitutes much of Death Valley National Park and is the principal feature of the Mojave and Colorado Deserts Biosphere Reserve. It is located mostly in Inyo County, California. It runs from north to south between the Amargosa Range on the east and the Panamint Range on the west; the Grapevine Mountains and the Owlshead Mountains form its northern and southern boundaries, respectively. It has an area of about . The highest point in Death Valley itself is Telescope Peak in the Panamint Range, which has an elevation of .\n\nDeath Valley is an excellent example of a graben, or a downdropped block of land between two mountain ranges. It lies at the southern end of a geological trough known as Walker Lane, which runs north to Oregon. The valley is bisected by a right lateral strike slip fault system, represented by the Death Valley Fault and the Furnace Creek Fault. The eastern end of the left lateral Garlock Fault intersects the Death Valley Fault. Furnace Creek and the Amargosa River flow through the valley but eventually disappear into the sands of the valley floor.\n\nDeath Valley also contains salt pans. According to current geological consensus, at various times during the middle of the Pleistocene era, which ended roughly 10,000–12,000 years ago, an inland lake referred to as Lake Manly formed in Death Valley. Lake Manly was nearly long and deep, the end-basin in a chain of lakes that began with Mono Lake in the north and continued through multiple basins down the Owens River Valley through Searles and China Lakes and the Panamint Valley to the immediate west.\n\nAs the area turned to desert, the water evaporated, leaving the abundance of evaporitic salts such as common sodium salts and borax, which were later exploited during the modern history of the region, primarily 1883 to 1907.\nDeath Valley has a subtropical, hot desert climate (Köppen: \"BWh\"), with long, extremely hot summers and short, mild winters, as well as little rainfall. As a general rule, lower altitudes tend to have higher temperatures. When the sun heats the ground, that heat is then radiated upward, but the dense below-sea-level air absorbs some of this radiation and radiates some of it back towards the ground. In addition, the high valley walls trap rising hot air and recycle it back down to the valley floor, where it is heated by compression.\n\nThis process is especially important in Death Valley, as it provides its specific climate and geography. The valley is surrounded by mountains, while its surface is mostly flat and devoid of plants, so much of the sun's heat can reach the ground, absorbed by soil and rock. When air at ground level is heated, it begins to rise, moving up past steep, high mountain ranges, which then cools slightly, sinking back down towards the valley more compressed. This air is then reheated by the sun to a higher temperature, moving up the mountain again, whereby the air moves up and down in a circular motion in cycles, similar to how a convection oven works. This heated air increases ground temperature markedly, forming the hot wind currents that are trapped by atmospheric pressure and mountains, thus stays mostly within the valley. Such hot wind currents contribute to perpetual drought-like conditions in Death Valley and prevent much cloud formation from passing through the confines of the valley, where precipitation is often in the form of a virga. Death Valley holds temperature records because it has an unusually high number of factors that lead to high atmospheric temperatures.\n\nThe depth and shape of Death Valley influence its summer temperatures. The valley is a long, narrow basin below sea level, yet is walled by high, steep mountain ranges. The clear, dry air and sparse plant cover allow sunlight to heat the desert surface. Summer nights provide little relief, as overnight lows may only dip into the range. Moving masses of super-heated air blow through the valley creating extremely high temperatures.\nThe hottest air temperature ever recorded in Death Valley was on July 10, 1913, at Furnace Creek, which is the highest atmospheric temperature ever recorded on earth. During the heat wave that peaked with that record, five consecutive days reached or above. Some meteorologists dispute the accuracy of the 1913 temperature measurement.\n\nThe highest surface temperature ever recorded in Death Valley was on July 15, 1972, at Furnace Creek, which is the highest ground surface temperature ever recorded on earth, as well as the only recorded surface temperature of above .\n\nThe greatest number of consecutive days with a maximum temperature of or above was 154 days in the summer of 2001. The summer of 1996 had 40 days over , and 105 days over . The summer of 1917 had 52 days where the temperature reached or above with 43 of them consecutive. Four major mountain ranges lie between Death Valley and the ocean, each one adding to an increasingly drier rain shadow effect, and in 1929, 1953, and 1989, no rain was recorded for the whole year. The period from 1931 to 1934 was the driest stretch on record with only of rain over a 40-month period. On June 30, 2013, during the 2013 extreme heat wave, the mercury reached 129 °F (54 °C) at Furnace Creek station, which is the all-time highest air temperature recorded for June.\n\nThe mean annual temperature for Death Valley (Furnace Creek Weather Station) is with an average high of in December, in January, and in July. From 1934 to 1961, the weather station at Cow Creek recorded a mean annual temperature of .\n\nThe longest number of consecutive days where temperatures reached or more was 205 from April to October 1992. On average, 192 days per year in Death Valley have temperatures that reach 90 °F or more. Before being moved to Furnace Creek, the weather station at Greenland Ranch averaged 194.4 days annually where temperatures reached 90 °F or more.\nThe lowest temperature recorded at Greenland Ranch was in January 1913.\n\nThe period from July 17–19, 1959, was the longest string of consecutive days where nighttime low temperatures did not drop below . The highest overnight or low temperature recorded in Death Valley is , recorded on July 5, 1918, and the current world record for highest overnight low. As recently as July 12, 2012, the low temperature at Death Valley dropped to just after a high of on the previous day. The only other location which matches Death Valley's overnight low temperature of 107 °F in recent years is Khasab Airport in Oman, which also recorded a low of on June 27, 2012, and later one of on June 21, 2017. Also on July 12, 2012, the mean 24-hour temperature recorded at Death Valley was , which makes it the world's warmest 24-hour temperature on record.\n\nThe average annual precipitation in Death Valley is , while the Greenland Ranch station averaged . The wettest month on record is January 1995, when fell on Death Valley. The wettest period on record was mid-2004 to mid-2005, in which nearly of rain fell in total, leading to ephemeral lakes in the valley and the region and tremendous wildflower blooms. Snow with accumulation has only been recorded in January 1922, while scattered flakes have been recorded on other occasions.\n\nIn 2005, Death Valley received four times its average annual rainfall of . As it has done before for hundreds of years, the lowest spot in the valley filled with a wide, shallow lake, but the extreme heat and aridity immediately began evaporating the ephemeral lake.\n\nThe pair of images (seen at right) from NASA's Landsat 5 satellite documents the short history of Death Valley's Lake Badwater: formed in February 2005 (top) and evaporated by February 2007 (bottom). In 2005, a big pool of greenish water stretched most of the way across the valley floor. By May 2005 the valley floor had resumed its more familiar role as Badwater Basin, a salt-coated salt flats. In time, this freshly dissolved and recrystallized salt will darken.\n\nThe western margin of Death Valley is traced by alluvial fans. During flash floods, rainfall from the steep mountains to the west pours through narrow canyons, picking up everything from fine clay to large rocks. When these torrents reach the mouths of the canyons, they widen and slow, branching out into distributary channels. The paler the fans, the younger they are.\n\nIn spite of the overwhelming heat and sparse rainfall, Death Valley exhibits considerable biodiversity. Wildflowers, watered by snowmelt, carpet the desert floor each spring, continuing into June. Bighorn sheep, red-tailed hawks, and wild burros may be seen. Death Valley has over 600 springs and ponds. Salt Creek, a mile-long shallow depression in the center of the valley, supports pupfish. These isolated pupfish populations are remnants of the wetter Pleistocene climate.\n\nDarwin Falls, on the western edge of Death Valley Monument, falls into a large pond surrounded by willows and cottonwood trees. Over 80 species of birds have been spotted around the pond.\n\nDeath Valley is home to the Timbisha tribe of Native Americans, formerly known as the Panamint Shoshone, who have inhabited the valley for at least the past millennium. The Timbisha name for the valley, \"tümpisa\", means \"rock paint\" and refers to the red ochre paint that can be made from a type of clay found in the valley. Some families still live in the valley at Furnace Creek. Another village was in Grapevine Canyon near the present site of Scotty's Castle. It was called in the Timbisha language \"maahunu\", whose meaning is uncertain, although it is known that \"hunu\" means \"canyon\".\nThe valley received its English name in 1849 during the California Gold Rush. It was called Death Valley by prospectors and others who sought to cross the valley on their way to the gold fields, after 13 pioneers perished from one early expedition of wagon trains. During the 1850s, gold and silver were extracted in the valley. In the 1880s, borax was discovered and extracted by mule-drawn wagons.\n\nOn the afternoon of July 10, 1913, the United States Weather Bureau recorded a high temperature of 134 °F (56.7 °C) at Greenland Ranch (now Furnace Creek) in Death Valley. This temperature stands as the highest ambient air temperature ever recorded at the surface of the Earth. (A report of a temperature of 58 °C (136.4 °F) recorded in Libya in 1922 was later determined to be inaccurate.)\n\nDeath Valley National Monument was proclaimed on February 11, 1933, by President Herbert Hoover, placing the area under federal protection. In 1994, the monument was redesignated as Death Valley National Park, as well as being substantially expanded to include Saline and Eureka Valleys.\n\nA number of movies have been filmed in Death Valley, such as the following:\n\n\n\n\n"}
{"id": "1916136", "url": "https://en.wikipedia.org/wiki?curid=1916136", "title": "Dielectric heating", "text": "Dielectric heating\n\nDielectric heating, also known as electronic heating, RF (radio frequency) heating, and high-frequency heating, is the process in which a radio frequency alternating electric field, or radio wave or microwave electromagnetic radiation heats a dielectric material. At higher frequencies, this heating is caused by molecular dipole rotation within the dielectric.\n\nRF dielectric heating at intermediate frequencies, due to its greater penetration over microwave heating, shows greater promise than microwave systems as a method of very rapidly heating and uniformly preparing certain food items, and also killing parasites and pests in certain harvested crops.\n\nMolecular rotation occurs in materials containing polar molecules having an electrical dipole moment, with the consequence that they will align themselves in an electromagnetic field. If the field is oscillating, as it is in an electromagnetic wave or in a rapidly oscillating electric field, these molecules rotate continuously by aligning with it. This is called dipole rotation, or dipolar polarisation. As the field alternates, the molecules reverse direction. Rotating molecules push, pull, and collide with other molecules (through electrical forces), distributing the energy to adjacent molecules and atoms in the material. The process of energy transfer from the source to the sample is a form of radiative heating.\n\nTemperature is related to the average kinetic energy (energy of motion) of the atoms or molecules in a material, so agitating the molecules in this way increases the temperature of the material. Thus, dipole rotation is a mechanism by which energy in the form of electromagnetic radiation can raise the temperature of an object. There are also many other mechanisms by which this conversion occurs.\n\nDipole rotation is the mechanism normally referred to as dielectric heating, and is most widely observable in the microwave oven where it operates most efficaciously on liquid water, and also, but much less so, on fats and sugars. This is because fats and sugar molecules are far less polar than water molecules, and thus less affected by the forces generated by the alternating electromagnetic fields. Outside of cooking, the effect can be used generally to heat solids, liquids, or gases, provided they contain some electric dipoles.\n\nDielectric heating involves the heating of electrically insulating materials by dielectric loss. A changing electric field across the material causes energy to be dissipated as the molecules attempt to line up with the continuously changing electric field. This changing electric field may be caused by an electromagnetic wave propagating in free space (as in a microwave oven), or it may be caused by a rapidly alternating electric field inside a capacitor. In the latter case, there is no freely-propagating electromagnetic wave, and the changing electric field may be seen as analogous to the electric component of an antenna near field. In this case, although the heating is accomplished by changing the electric field inside the capacitive cavity at radio-frequency (RF) frequencies, no actual radio waves are either generated or absorbed. In this sense, the effect is the direct electrical analog of magnetic induction heating, which is also near-field effect (thus not involving radio waves).\n\nFrequencies in the range of 10–100 MHz are necessary to cause dielectric heating, although higher frequencies work equally well or better, and in some materials (especially liquids) lower frequencies also have significant heating effects, often due to more unusual mechanisms. For example, in conductive liquids such as salt water, \"ion-drag\" causes heating, as charged ions are \"dragged\" more slowly back and forth in the liquid under influence of the electric field, striking liquid molecules in the process and transferring kinetic energy to them, which is eventually translated into molecular vibrations and thus into thermal energy.\n\nDielectric heating at low frequencies, as a near-field effect, requires a distance from electromagnetic radiator to absorber of less than ≈ of a wavelength. It is thus a contact process or near-contact process, since it usually sandwiches the material to be heated (usually a non-metal) between metal plates taking the place of the dielectric in what is effectively a very large capacitor. However, actual electrical contact is not necessary for heating a dielectric inside a capacitor, as the electric fields that form inside a capacitor subjected to a voltage do not require electrical contact of the capacitor plates with the (non-conducting) dielectric material between the plates. Because lower frequency electrical fields penetrate non-conductive materials far more deeply than do microwaves, heating pockets of water and organisms deep inside dry materials like wood, it can be used to rapidly heat and prepare many non-electrically conducting food and agricultural items, so long as they fit between the capacitor plates.\n\nAt very high frequencies, the wavelength of the electromagnetic field becomes shorter than the distance between the metal walls of the heating cavity, or than the dimensions of the walls themselves. This is the case inside a microwave oven. In such cases, conventional far-field electromagnetic waves form (the cavity no longer acts as a pure capacitor, but rather as an antenna), and are absorbed to cause heating, but the dipole-rotation mechanism of heat deposition remains the same. However, microwaves are not efficient at causing the heating effects of low frequency fields that depend on slower molecular motion, such as those caused by ion-drag.\n\nDielectric heating must be distinguished from Joule heating of conductive media, which is caused by induced electric currents in the media. For dielectric heating, the generated power density per volume is given by:\n\nwhere \"ω\" is the angular frequency of the exciting radiation, \"ε\"″ is the imaginary part of the complex relative permittivity of the absorbing material, \"ε\" is the permittivity of free space and \"E\" the electric field strength. The imaginary part of the (frequency-dependent) relative permittivity is a measure for the ability of a dielectric material to convert electromagnetic field energy into heat.\n\nIf the conductivity \"σ\" of the material is small, or the frequency is high, such that (with ), then dielectric heating is the dominant mechanism of loss of energy from the electromagnetic field into the medium.\n\nMicrowave frequencies penetrate conductive materials, including semi-solid substances like meat and living tissue, to a distance defined by the skin effect. The penetration essentially stops where all the penetrating microwave energy has been converted to heat in the tissue. Microwave ovens used to heat food are not set to the frequency for optimal absorption by water. If that were so, then the piece of food or liquid in question would absorb all microwave radiation in its outer layer, leading to a cool, unheated centre and a superheated surface. Instead, the frequency selected allows energy to penetrate deeper into the heated food. The frequency of a household microwave oven is 2.45 GHz, while the frequency for optimal absorbency by water is around 10 GHz.\n\nThe use of high-frequency electric fields for heating dielectric materials had been proposed in the 1930s. For example, (application by Bell Telephone Laboratories, dated 1937) states \"This invention relates to heating systems for dielectric materials and the object of the invention is to heat such materials uniformly and substantially simultaneously throughout their mass. It has been proposed therefore to heat such materials simultaneously throughout their mass by means of the dielectric loss produced in them when they are subjected to a high voltage, high frequency field.\" This patent proposed radio frequency (RF) heating at 10 to 20 megahertz (wavelength 15 to 30 meters). Such wavelengths were far longer than the cavity used, and thus made use of near-field effects and not electromagnetic waves. (Commercial microwave ovens use wavelengths only 1% as long.)\n\nIn agriculture, RF dielectric heating has been widely tested and is increasingly used as a way to kill pests in certain food crops after harvest, such as walnuts still in the shell. Because RF heating can heat foods more uniformly than is the case with microwave heating, RF heating holds promise as a way to process foods quickly.\n\nIn medicine, the RF heating of body tissues, called diathermy, is used for muscle therapy Heating to higher temperatures, called hyperthermia therapy, is used to kill cancer and tumor tissue.\n\nMicrowave heating, as distinct from RF heating, is a sub-category of dielectric heating at frequencies above 100 MHz, where an electromagnetic wave can be launched from a small dimension emitter and guided through space to the target. Modern microwave ovens make use of electromagnetic waves with electric fields of much higher frequency and shorter wavelength than RF heaters. Typical domestic microwave ovens operate at 2.45 GHz, but 915 MHz ovens also exist. This means that the wavelengths employed in microwave heating are 0.1 cm to 10 cm. This provides for highly efficient, but less penetrative, dielectric heating.\n\nAlthough a capacitor-like set of plates can be used at microwave frequencies, they are not necessary, since the microwaves are already present as far field type EM radiation, and their absorption does not require the same proximity to a small antenna as does RF heating. The material to be heated (a non-metal) can therefore simply be placed in the path of the waves, and heating takes place in a non-contact process which does not require capacitative conductive plates.\n\nMicrowave volumetric heating is a commercially available method of heating liquids, suspensions, or solids in a continuous flow on an industrial scale. Microwave volumetric heating has a greater penetration depth, of up to , which is an even penetration through the entire volume of the flowing product. This is advantageous in commercial applications where increased shelf-life can be achieved, with increased microbial kill at temperatures lower than when using conventional heating systems.\n\nApplication for microwave volumetic heating:\n\nIn drying of foods, dielectric heating is usually combined with conventional heating. It may be used to preheat the feed to a hot-air drier. By raising the temperature of the feed quickly and causing moisture to move to the surface, it can decrease the overall drying time. Dielectric heating may be applied part-way through the drying cycle, when the food enters the falling rate period. This can boost the rate of drying. If dielectric heating is applied near the end of hot-air drying it can also shorten the drying time significantly and hence increase the throughput of the drier. It is more usual to use dielectric heating in the later stages of drying. One of the major applications of RF heating is in the postbaking of biscuits. The objectives in baking biscuits are to produce a product of the right size, shape, color, and moisture content. In a conventional oven, reducing the moisture content to the desired level can take up a large part of the total baking time. The application of RF heating can shorten the baking time. The oven is set to produce biscuits of the right size, shape, and color, but the RF heating is used to remove the remaining moisture, without excessive heating of the already dry sections of the biscuit. The capacity of an oven can be increased by more than 50% by the use of RF heating. Postbaking by RF heating has also been applied to breakfast cereals and cereal-based baby foods.\n\nFood quality is maximized and better retained using electromagnetic energy than conventional heating. Conventional heating results in large disparity in temperature and longer processing times which can cause overprocessing on the food surface and impairment of the overall quality of the product. Electromagnetic energy can achieve higher processing temperatures in shorter times, therefore, more nutritional and sensory properties are conserved.\n\n\n"}
{"id": "34521762", "url": "https://en.wikipedia.org/wiki?curid=34521762", "title": "Doi Pha Hom Pok National Park", "text": "Doi Pha Hom Pok National Park\n\nDoi Pha Hom Pok National Park (), formerly known as Mae Fang National Park and Doi Fa Hom Pok National Park, is the northernmost national park in Thailand. It straddles Fang, Mae Ai, and Chai Prakan Districts of Chiang Mai Province. The park covers 524 km of the mountain area of the Daen Lao Range, at the border with Myanmar. The tallest peak is Doi Pha Hom Pok at , the second highest in Thailand.\n\nDoi Pha Hom Pok National Park is mostly covered with forest, where tree species such as \"Hopea odorata\" predominate with rare plant species such as \"Impatiens jurpioides\" and butterflies such as \"Meandrusa sciron\". Doi Lang, located within the park, is an excellent area for birdwatching. At Fang Hot Springs, park HQ, are many hot mineral springs in a 10 rai area (16,000 m). Water temperature ranges from 90-130° C. The largest pond has hot steam rising 40-50 meters above the ground.\n\n"}
{"id": "53217", "url": "https://en.wikipedia.org/wiki?curid=53217", "title": "Exotic atom", "text": "Exotic atom\n\nAn exotic atom is an otherwise normal atom in which one or more sub-atomic particles have been replaced by other particles of the same charge. For example, electrons may be replaced by other negatively charged particles such as muons (muonic atoms) or pions (pionic atoms). Because these substitute particles are usually unstable, exotic atoms typically have very short lifetimes.\n\nIn a \"muonic atom\" (previously called a \"mu-mesic\" atom, now known to be a misnomer as muons are not mesons), an electron is replaced by a muon, which, like the electron, is a lepton. Since leptons are only sensitive to weak, electromagnetic and gravitational forces, muonic atoms are governed to very high precision by the electromagnetic interaction. \n\nSince a muon is more massive than an electron, the Bohr orbits are closer to the nucleus in a muonic atom than in an ordinary atom, and corrections due to quantum electrodynamics are more important. Study of muonic atoms' energy levels as well as transition rates from excited states to the ground state therefore provide experimental tests of quantum electrodynamics.\n\nMuon-catalyzed fusion is a technical application of muonic atoms.\n\nA \"hadronic atom\" is an atom in which one or more of the orbital electrons is replaced by a negatively charged hadron. Possible hadrons include mesons such as the pion or kaon, yielding a \"pionic atom\" or a \"kaonic atom\" (see Kaonic hydrogen), collectively called \"mesonic atoms\"; antiprotons, yielding an \"antiprotonic atom\"; and the particle, yielding a or \"sigmaonic atom\".\n\nUnlike leptons, hadrons can interact via the strong force, so the orbitals of hadronic atoms are influenced by nuclear forces between the nucleus and the hadron. Since the strong force is a short-range interaction, these effects are strongest if the atomic orbital involved is close to the nucleus, when the energy levels involved may broaden or disappear because of the absorption of the hadron by the nucleus. Hadronic atoms, such as pionic hydrogen and kaonic hydrogen, thus provide experimental probes of the theory of strong interactions, quantum chromodynamics.\n\nAn \"onium\" (plural: \"onia\" ) is the bound state of a particle and its antiparticle. The classic onium is positronium, which consists of an electron and a positron bound together as a long-lived metastable state. Positronium has been studied since the 1950s to understand bound states in quantum field theory. A recent development called non-relativistic quantum electrodynamics (NRQED) used this system as a proving ground.\n\nPionium, a bound state of two oppositely-charged pions, is useful for exploring the strong interaction. This should also be true of protonium, which is a proton-antiproton bound state. Understanding bound states of pionium and protonium is important in order to clarify notions related to exotic hadrons such as mesonic molecules and pentaquark states. Kaonium, which is a bound state of two oppositely charged kaons, has not been observed experimentally yet.\n\nThe true analogs of positronium in the theory of strong interactions, however, are not exotic atoms but certain mesons, the \"quarkonium states\", which are made of a heavy quark such as the charm or bottom quark and its antiquark. (Top quarks are so heavy that they decay through the weak force before they can form bound states.) Exploration of these states through non-relativistic quantum chromodynamics (NRQCD) and lattice QCD are increasingly important tests of quantum chromodynamics.\n\nMuonium, despite its name, is \"not\" an onium containing a muon and an antimuon, because IUPAC assigned that name to the system of an antimuon bound with an electron. However, the production of a muon/antimuon bound state, which is an onium, has been theorized.\n\nAtoms may be composed of electrons orbiting a hypernucleus that includes strange particles called hyperons. Such hypernuclear atoms are generally studied for their nuclear behaviour, falling into the realm of nuclear physics rather than atomic physics.\n\nIn condensed matter systems, specifically in some semiconductors, there are states called excitons which are bound states of an electron and an electron hole.\n\n"}
{"id": "40047772", "url": "https://en.wikipedia.org/wiki?curid=40047772", "title": "Finngulf LNG", "text": "Finngulf LNG\n\nFinngulf LNG was a liquefied natural gasterminal project in Ingå, Finland. It was an initiative by Finnish natural gas company Gasum. The project was on the list of projects submitted to be considered as potential EU projects of common interest in energy infrastructure. One of the criteria for the EU aid is that the terminal must service more than one EU Member State. This means that another prerequisite was the proposed Balticconnector pipeline, which would connect Estonian and Finnish gas grids.\n\nFrom the terminal, gas would be injected into the Finnish natural gas network or transported in the form of LNG for use as vessel fuel or for industrial uses. The terminal area would also include a large LNG storage tank.\n\nAn environmental impact assessment report for the terminal was completed in spring 2013.\n\nA full-scale terminal could be completed in 2018. In December 2013, the project management services contract was awarded to Neste Jacobs.\n\nIn October 2015, Gasum abandoned the project due to commercial viability.\n\n"}
{"id": "12250231", "url": "https://en.wikipedia.org/wiki?curid=12250231", "title": "Green Dot (symbol)", "text": "Green Dot (symbol)\n\nThe Green Dot () is the license symbol of a European network of industry-funded systems for recycling the packaging materials of consumer goods. The logo is trademark protected worldwide.\n\nThe Green Dot was a system thought up by Klaus Töpfer, Germany's environment minister in the early 1990s. The basic idea of the Green Dot is that consumers who see the logo know that the manufacturer of the product contributes to the cost of recovery and recycling. This can be with household waste collected by the authorities (e.g. in special bags - in Germany these are yellow), or in containers in public places such as car parks and outside supermarkets.\n\nThe system is financed by the green dot licence fee paid by the producers of the products. Fees vary by country and are based on the material used in packaging (e.g. paper, plastic, metal, wood, cardboard). Each country also has different fees for joining the scheme and ongoing fixed and variable fees. Fees also take into account the cost of collection, sorting and recycling methods.\n\nIn simple terms, the system encourages manufacturers to cut down on packaging as this saves them the cost of licence fees.\n\nIn 1991, the German government passed a packaging law (Verpackungsverordnung) that requires manufacturers to take care of the recycling or disposal of any packaging material they sell. As a result of this law, German industry set up a \"dual system\" of waste collection, which picks up household packaging in parallel to the existing municipal waste-collection systems. This industry-funded system is operated in Germany by the Duales System Deutschland GmbH (German for \"Dual System Germany Ltd\") corporation, or short DSD.\n\nDSD only collects packaging material from manufacturers who pay a license fee to DSD. DSD license fee payers can then add the Green Dot logo to their package labeling to indicate that this package should be placed into the separate yellow bags or yellow wheelie bins that will then be collected and emptied by DSD-operated waste collection vehicles and sorted (and where possible recycled) in DSD facilities.\n\nGerman licence fees are calculated using the weight of packs, each material type used and the volumes of product produced per.\n\nWorldwide stewardship of the Green Dot logo is managed by PRO Europe (Packaging Recovery Organisation Europe) on behalf of the various national Green dot organizations across Europe.\n\nThe design of the Green Dot symbol has obvious links with the Chinese Taijitu (yin and yang) symbol and Gary Anderson's recycling symbol. Where full-color printing is available, its official form is printed in a light and a dark shade of green (Pantone 366C and 343C). For cost reasons or to avoid a visual clash with other symbols, many manufacturers chose a black-and-white or other color combination on their packages.\n\nThe Green Dot logo merely indicates that a company has joined the Green Dot scheme, and \"not\" necessarily that the package is fully recyclable. The logo is often confused with the recycling logo.\n\nIn Malta, Green Dot Malta Limited, a waste recovery company that is licensed from Der Grune Punkt Duales System Deutschland GmbH to use the Green Dot trademark in Malta, has successfully petitioned the Maltese courts on a number of occasions to protect the mark from free-riders and from competitors seeking to obtain unfair advantage from the international reputation and goodwill that it enjoys.\n\nIn February 2009, Smart Supermarket, a well established local supermarket based in Birkirkara, Malta, was ordered by a judge not to sell, manufacture or pack products bearing the trademark Green Dot without the necessary licenses. Green Dot Malta Limited had argued in court that the supermarket was not only blatantly infringing the trademark and contravening the Trademarks Act but was also taking unfair advantage of Green Dot's reputation and goodwill without having its authorisation. The First Hall of the Civil Court, presided over by Madam Justice Abigail Lofaro, upheld Green Dot Malta Limited's request and issued a warrant of prohibitory injunction. Lawyers Antoine Naudi and Victor G. Axiak appeared for Green Dot Malta Limited. Following this injunction, the two parties reached an agreement whereby Smart Supermarket entered into a royalty license agreement with Green Dot Malta Limited and registered all its own branded food items such as confectionery and freshly packed products. they agreed to cooperate. The two companies also agreed to cooperate in ensuring that the intellectual property rights relating to the Green Dot mark in Malta will be further protected from any unlawful use by third parties including suppliers of the various goods to the supermarket.\n\nIn April 2009, following a similar request for an injunction filed by Green Dot Malta Limited, the company Zamco Caterware Limited declared in open court that it was binding itself not to circulate any products in the market with the Green Dot symbol on their packaging without the required license. The company also declared that it would not be importing any product bearing the Green Dot mark unless the relative royalty contributions have been paid and unless they prove that the imported product will be recycled in terms of applicable environmental legislation.\n\nIn September 2009, Karta Converters Limited, a company which produces and distributes articles made of paper, cardboard and plastic, was ordered by the First Hall of the Civil Court not to manufacture, pack, sell or otherwise continue circulating in the local market products bearing the trademark “Green Dot” without the necessary licenses. Presiding Judge Dr. Geoffrey Valencia accepted Green Dot Malta Limited’s assertion that the company was conducting itself as a “free-rider” in the local market and that it was taking an unjust and unfair advantage of Green Dot’s reputation without having its consent. Following this injunction, Karta Converters Limited issued a press release whereby it declared that it would be adopting the internationally recognised Green Dot symbol on its paper, carton and plastic products in accordance with the law. Karta Converters also declared that it would be joining some three hundred other companies participating in GreenPak, a waste recovery scheme operated by Green Dot Malta Limited, for the recycling of its packaging.\n\nOut of court settlements have been reached on a number of occasions with various other Maltese companies to ensure that the international reputation of the mark is safeguarded.\n\nIn May 2010, Green Dot Malta Limited won a court case against Green.Mt Ltd, a local competitor which operates another waste recovery scheme in Malta. The latter was incorporated in 2007, a few years after Green Dot Malta Limited’s registration. Green Dot Malta Limited petitioned the Court to declare that the name Green.Mt Ltd amounted to unfair competition in terms of law since the use of the “.” coupled with the words “Green Mt” was intended to cause confusion in the market.\n\nIn delivering judgment, Mr. Justice Raymond Pace stated that the two schemes were obviously direct competitors operating in the same field of activity and that the name Green.Mt Ltd was creating confusion in terms of article 32 of Chapter 13 of the Laws of Malta. The court held that the law laid down that businesses could not use any name, mark or symbol which could create confusion with another name, mark or symbol used legally by others. The imitation did not need to be perfect. It was enough that in its entirety it could deceive a consumer. The Judge said that an examination of the words used left him in no doubt that the names could and indeed were creating confusion amongst consumers. The Court thus ordered Green.Mt Ltd to pay Green Dot Malta Limited a nominal sum by way of penalty and to destroy any offending material in its possession bearing the name Green.Mt Ltd within 30 days from date of judgment. Dr. Antoine Naudi and Dr. Victor G. Axiak appeared for Green Dot Malta Limited. Before final judgment was delivered, Green.Mt Ltd changed its name to Green MT Ltd.\n\n\n"}
{"id": "22257994", "url": "https://en.wikipedia.org/wiki?curid=22257994", "title": "Grzywna (unit)", "text": "Grzywna (unit)\n\nThe grzywna (from Polish) was a measure of weight, mainly for silver, commonly used throughout medieval central and eastern Europe, particularly in the Kingdom of Poland and Kingdom of Bohemia ().\n\nGrzywna was also a unit of measure of a unit of exchange, and as such used as money in the 10th–15th centuries. Silver ingots acted as commodity money before the widespread use of minted coins. Several different grzywnas developed with their own system of weight and exchange, such as the Kulm grzywna and the Kraków grzywna.\n\nThe name is derived from from .\n\nThe Kraków grzywna, used in Poland, weighed anywhere from 196.26 g to 201.86 g, depending on the timeframe. In the 14th century, it was equal to 196.26 g, while in the beginning of the 16th century in weighed 197.684 g, but after 1558 it was equivalent to 201.802 g and after 1650 it was 201.86 g.\n\nThe Kraków grzywna was subdivided thus: 4 wiarduneks or quarters = 8 ounces = 16 drams = 24 skojecs = 96 grains = 240 denarii = 480 obols\n\nAs a measure of unit of exchange, the Kraków grzywna was equal to 48 Prague groschen. During the rule of Władysław I the Elbow-high 576 denarii were struck from one Kraków grzywna of silver. During the rule of his son Casimir the Great, 768 denarii were struck from it and during the reign of Władysław II Jagiełło, it was 864 denarii.\n\n"}
{"id": "8205082", "url": "https://en.wikipedia.org/wiki?curid=8205082", "title": "High-leg delta", "text": "High-leg delta\n\nHigh-leg delta (also known as wild-leg, stinger leg, bastard leg, high-leg, orange-leg, or red-leg delta) is a type of electrical service connection for three-phase electric power installations. It is used when both single and three-phase power is desired to be supplied from a three phase transformer (or transformer bank). The three-phase power is connected in the delta configuration, and the center point of one phase is grounded. This creates both a split-phase single phase supply (L1 or L2 to neutral on diagram at right) and three-phase (L1-L2-L3 at right). It is called \"orange leg\" because the wire is color-coded orange. By convention, the high leg is usually set in the center (B phase) lug in the involved panel, regardless of the L1-L2-L3 designation at the transformer. \n\nHigh-leg delta service is supplied in one of two ways. One is by a transformer having four wires coming out of the secondary, the three phases, plus a neutral connected as a center-tap on one of the windings. Another method requires two transformers. One transformer is connected to one phase of the overhead primary distribution circuit to provide the 'lighting' side of the circuit (this will be the larger of the two transformers), and a second transformer is connected to another phase on the circuit and its secondary is connected to one side of the 'lighting' transformer secondary, and the other side of this transformer is brought out as the 'high leg'. The voltages between the three phases are the same in magnitude, however the voltage magnitudes between a particular phase and the neutral vary. The phase-to-neutral voltage of two of the phases will be half of the phase-to-phase voltage. The remaining phase-to-neutral voltage will be /2 the phase-to-phase voltage. So if A-B, B-C and C-A are all 240 volts, then A-N and C-N will both be 120 volts, but B-N will be 208 volts.\n\nOther types of three-phase supplies are wye connections, ungrounded delta connections, \"ghost\" leg configuration delta connections (two transformers supplying three phase power), or corner-grounded delta connections. These connections do not supply split single-phase power, and do not have a high leg.\n\nConsider the low-voltage side of a 120/240 V high leg delta connected transformer, where the 'b' phase is the 'high' leg. The line-to-line voltage magnitudes are all the same:\n\nBecause the winding between the 'a' and 'c' phases is center-tapped, the line-to-neutral voltages for these phases are as follows:\n\nBut the phase-neutral voltage for the 'b' phase is different:\n\nThis can be proven by writing a KVL equation, using angle notation, starting from the grounded neutral:\n\nor:\n\nIf the \"high leg\" is not used, the system acts like a split single-phase system, which is familiar.\n\nBoth three-phase and single split-phase power can be supplied from a single transformer bank.\n\nWhere the three-phase load is small relative to the total load, two individual transformers may be used instead of the three for a \"full delta\" or a three-phase transformer, thus providing a variety of voltages at reduced cost. This is called \"open-delta high-leg\", and has a reduced capacity relative to a full delta.\n\nIn cases where the single-phase load is much greater than the three-phase load, load balancing will be poor. Generally, these cases are identified by three transformers supplying the service, two of which are sized significantly smaller than the third, and the third larger transformer will be center tap grounded.\n\nOne of the phase-to-neutral voltage (usually phase \"B\") is higher than the other two. The hazard of this is that if single phase loads are connected to the high leg (with the connecting person unaware that that leg is higher voltage), excess voltage is supplied to that load. This can easily cause failure of the load.\n\nCommonly there is a high-leg to neutral load limit when only two transformers are used. One transformer manufacturer's page suggests that High-leg to neutral loading to not exceed 5% of transformer capacity.\n\nIt is often found in older and rural installations. This type of service is usually supplied using 240 V line-to-line and 120 V line-to-neutral. In some ways, the high leg delta service provides the best of both worlds: a line-to-line voltage that is higher than the usual 208 V that most three-phase services have, and a line-to-neutral voltage (on two of the phases) sufficient for connecting appliances and lighting. Thus, large pieces of equipment will draw less current than with 208 V, requiring smaller wire and breaker sizes. Lights and appliances requiring 120 V can be connected to phases 'A' and 'C' without requiring an additional step-down transformer.\n\nEven when unmarked, it is generally easy to identify this type of system, because the \"B\" phase (circuits #3 and #4) and every third circuit afterwards will be either a three-pole breaker or a blank. \n\nCurrent practice is to give separate services for single-phase and three-phase loads, e.g., 120 V split-phase (lighting etc.) and 240 V to 600 V three-phase (for large motors). However, many jurisdictions forbid more than one class for a premises' service, and the choice may come down to 120/240 split-phase, 208 single-phase or three-phase (delta), 120/208 three-phase (wye), or 277/480 three-phase (wye) (or 347/600 three-phase (wye) in Canada).\n\n\n"}
{"id": "28932246", "url": "https://en.wikipedia.org/wiki?curid=28932246", "title": "Index of recycling articles", "text": "Index of recycling articles\n\nThis is an index of recycling topics.\n- Aircraft Fleet Recycling Association\n\n- Battery recycling\n\n- Computer recycling\n- Computer technology for developing areas\n- Container-deposit legislation\n- Creative reuse\n\n- Design for Environment\n- Digger gold\n- Downcycling\n\n- E-Cycling (recycling of electronic components)\n- Electronic Waste Recycling Fee\n- eDay\n- Extended producer responsibility\n\n- Freecycling\n- Front of Store Recycling\n- Full Depth Recycling\n\n- Green Dot (symbol)\n- Green Gifting\n\n\n- I-recycle\n\n- Land recycling\n- Landfill mining\n\n- Pay as you throw\n- PullApart (grading packaging for the environment, consumers and local people).\n\n- Recycling\n- Recycling bin\n- Recycling codes\n- Recycling cooperative\n- Recycling symbol\n- Recycle It, Don't Trash It!\n- Regiving\n- Retrocomputing\n- Reuse\n- Reverse logistics\n- Reverse vending machine\n\n- Ship-Submarine recycling program\n- Single-stream recycling\n- SIRUM (organization) (Supporting Initiatives to Redistribute Unused Medicine)\n\n- Thermal depolymerization\n- Trashware\n\n- Urban mining\n\n- Waste hierarchy\n- Waste minimisation\n- WEEE directive\n"}
{"id": "13651338", "url": "https://en.wikipedia.org/wiki?curid=13651338", "title": "Interface and colloid science", "text": "Interface and colloid science\n\nInterface and colloid science is an interdisciplinary intersection of branches of chemistry, physics, nanoscience and other fields dealing with colloids, heterogeneous systems consisting of a mechanical mixture of particles between 1 nm and 1000 nm dispersed in a continuous medium. A colloidal solution is a heterogeneous mixture in which the particle size of the substance is intermediate between a true solution and a suspension, i.e. between 1–1000 nm. Smoke from a fire is an example of a colloidal system in which tiny particles of solid float in air. Just like true solutions, colloidal particles are small and cannot be seen by the naked eye. They easily pass through filter paper. But colloidal particles are big enough to be blocked by parchment paper or animal membrane.\n\nInterface and colloid science has applications and ramifications in the chemical industry, pharmaceuticals, biotechnology, ceramics, minerals, nanotechnology, and microfluidics, among others.\n\nThere are many books dedicated to this scientific discipline, and there is a glossary of terms \"Nomenclature in Dispersion Science and Technology,\" published by the US National Institute of Standards and Technology (NIST).\n\n\n"}
{"id": "5227127", "url": "https://en.wikipedia.org/wiki?curid=5227127", "title": "Lanthanum hexaboride", "text": "Lanthanum hexaboride\n\nLanthanum hexaboride (LaB, also called lanthanum boride and LaB) is an inorganic chemical, a boride of lanthanum. It is a refractory ceramic material that has a melting point of 2210 °C, and is insoluble in water and hydrochloric acid. It has a low work function and one of the highest electron emissivities known, and is stable in vacuum. Stoichiometric samples are colored intense purple-violet, while boron-rich ones (above LaB) are blue. Ion bombardment changes its color from purple to emerald green.\n\nThe principal use of lanthanum hexaboride is in hot cathodes, either as a single crystal or as a coating deposited by physical vapor deposition. Hexaborides, such as lanthanum hexaboride (LaB) and cerium hexaboride (CeB), have low work functions, around 2.5 eV. They are also somewhat resistant to cathode poisoning. Cerium hexaboride cathodes have a lower evaporation rate at 1700 K than lanthanum hexaboride, but they become equal at temperatures above 1850 K. Cerium hexaboride cathodes have one and half the lifetime of lanthanum hexaboride, due to the former's higher resistance to carbon contamination. Hexaboride cathodes are about ten times \"brighter\" than tungsten cathodes, and have 10–15 times longer lifetime. Devices and techniques in which hexaboride cathodes are used include electron microscopes, microwave tubes, electron lithography, electron beam welding, X-ray tubes, and free electron lasers. Lanthanum hexaboride slowly evaporates from the heated cathodes and forms deposits on the Wehnelt cylinders and apertures. LaB is also used as a size/strain standard in X-ray powder diffraction to calibrate instrumental broadening of diffraction peaks.\n\nLaB is a superconductor with a relatively low transition temperature of 0.45 K.\n"}
{"id": "11020357", "url": "https://en.wikipedia.org/wiki?curid=11020357", "title": "List of Japanese World War II explosives", "text": "List of Japanese World War II explosives\n\nThis is a complete list of Japanese explosives used during the Second World War it is sorted by application\n"}
{"id": "36201494", "url": "https://en.wikipedia.org/wiki?curid=36201494", "title": "List of Nepenthes endophyte species", "text": "List of Nepenthes endophyte species\n\nThis list of \"Nepenthes\" endophytes is a listing of endophytes recorded from the internal tissues of \"Nepenthes\" pitcher plants; that is, organisms that live within the plants for at least part of their life cycles without causing apparent disease.\n\nThe endophyte species are listed alphabetically and grouped by genus, family, and phylum. Additional information is included in brackets after the strain designation, namely: the host \"Nepenthes\" species from which the endophyte has been recorded; the geographical source of the record; and the type of tissue sampled.\n\n\n\n\n\n\n\n\n"}
{"id": "13240688", "url": "https://en.wikipedia.org/wiki?curid=13240688", "title": "List of largest hydroelectric power stations", "text": "List of largest hydroelectric power stations\n\nThis article provides a list of the largest hydroelectric power stations by generating capacity. Only plants with capacity larger than 2,000 MW are listed.\n\nThe Three Gorges Dam in Hubei, China, has the world's largest instantaneous generating capacity (22,500 MW), with the Itaipu Dam in Brazil/Paraguay in second place (14,000 MW). Despite the large difference in installed capacity these two power stations generate nearly equal amounts of electrical energy during the course of an entire year - Itaipu 103.1 TWh in 2016 and Three Gorges 98.8 TWh in 2014, because the Three Gorges experiences six months per year when there is very little water available to generate power, while the Paraná River that feeds the Itaipu has a much lower seasonal variance in flow. Power output of the Three Gorges reaches 125 TWh in years of high feed availability.\n\nThe Three Gorges (22,500 MW - 32 × 700 MW and 2 × 50 MW) is operated jointly with the much smaller Gezhouba Dam (2,715 MW), the total generating capacity of this two-dam complex is 25,215 MW. The Itaipu on the Brazil–Paraguay border has 20 generator units with overall 14,000 MW of installed capacity, however the maximum number of generating units allowed to operate simultaneously cannot exceed 18 (12,600 MW).\n\nThe Jinsha River (the upper stream of Yangtze River) complex is the largest hydroelectric generating system currently under construction. It has three phases. Phase one includes four dams on the downstream of the Jinsha River. They are Wudongde Dam, Baihetan Dam, Xiluodu Dam, and Xiangjiaba Dam, with generating capacity of 10,200 MW, 16,000 MW, 13,860 MW, and 6,448 MW respectively. Phase two includes eight dams on the middle stream of the Jinsha River. The total generating capacity is 21,150 MW. Phase three includes eight dams on the upper stream of the Jinsha River. The total generating capacity is 8,980 MW. The total combined capacity of the Jinsha complex with the Three Gorges complex will be 101,853 MW.\n\nPreliminary plans exist for the construction of the next largest hydroelectric power station with an installed capacity of 39,000 MW. The Project is called Grand Inga and is planned to be realised on the lower Congo River. China is said to have been working on a 50,000 MW dam as part of the Yarlung Tsangpo Hydroelectric and Water Diversion Project\n\nAnother proposal, Penzhin Tidal Power Plant, presumes an installed capacity up to .\n\nThe largest hydroelectric power stations top the list of the largest power stations of any kind, are among the largest hydraulic structures and are some of the in the world.\n\nOnly operational power stations with an installed capacity of at least 2,000 MW. Some of these may have additional units under construction, but only current installed capacity is listed.\n\nThis table lists stations under construction with expected installed capacity at least 2,000 MW.\n\n"}
{"id": "28828990", "url": "https://en.wikipedia.org/wiki?curid=28828990", "title": "Magnetoelectric effect", "text": "Magnetoelectric effect\n\nIn its most general form, the magnetoelectric effect (ME) denotes any coupling between the magnetic and the electric properties of a material. The first example of such an effect was described by Wilhelm Röntgen in 1888, who found that a dielectric material moving through an electric field would become magnetized. A material where such a coupling is intrinsically present is called a magnetoelectric.\n\nHistorically, the first and most studied example of this effect is the linear magnetoelectric effect. Mathematically, while the electric susceptibility formula_1 and magnetic susceptibility formula_2 describe the electric and magnetic polarization responses to an electric, resp. a magnetic field, there is also the possibility of a magnetoelectric susceptibility formula_3 which describes a linear response of the electric polarization to a magnetic field, and vice versa:\nThe tensor formula_6 must be the same in both equations. Here, P is the electric polarization, M the magnetization, E and H the electric and magnetic fields. The SI Unit of α is [s/m] which can be converted to the practical unit [V/(cm Oe)] by [s/m]=1.1 x10 ε [V/(cm Oe)]. For the CGS unit, [unitless] = 3 x 10 [s/m]/(4 x π)\n\nThe first material where an intrinsic linear magnetoelectric effect was predicted theoretically and confirmed experimentally is CrO. This is a single-phase material. Multiferroics are another example of single-phase materials that can exhibit a general magnetoelectric effect if their magnetic and electric orders are coupled. Composite materials are another way to realize magnetoelectrics. There, the idea is to combine, say a magnetostrictive and a piezoelectric material. These two materials interact by strain, leading to a coupling between magnetic and electric properties of the compound material.\n\nSome promising applications of the ME effect are sensitive detection of magnetic fields, advanced logic devices and tunable microwave filters.\n\nThe first example of a magnetoelectric effect was discussed in 1888 by Wilhelm Röntgen, who showed that a dielectric material moving through an electric field would become magnetized. The possibility of an intrinsic magnetoelectric effect in a (non-moving) material was conjectured by P. Curie in 1894, while the term \"magnetoelectric\" was coined by P. Debye in 1926.\nA mathematical formulation of the linear magnetoelectric effect was included in L. D. Landau and E. Lifshitz famous book series on theoretical physics. Only in 1959, I. Dzyaloshinskii, using an elegant symmetry argument, derived the form of a linear magnetoelectric coupling in CrO.\nThe experimental confirmation came just few months later when the effect was observed for the first time by D. Astrov. The general excitement which followed the measurement of the linear magnetoelectric effect lead to the organization of the series of MEIPIC (Magnetoelectric Interaction Phenomena in Crystals) conferences. Between the prediction of I. Dzialoshinskii and the MEIPIC first edition (1973), more than 80 linear magnetoelectric compounds were found. Recently, technological and theoretical progress, driven in large part by the advent of multiferroic materials, triggered a renaissance of these studies and magnetoelectric effect is still heavily investigated.\n\nIf the coupling between magnetic and electric properties is analytic, then the magnetoelectric effect can be described by an expansion of the free energy as a power series in the electric and magnetic fields formula_7 and formula_8:\nDifferentiating the free energy will then give the electric polarization formula_10 and the magnetization formula_11.\nHere, formula_12 and formula_13 are the static polarization, resp. magnetization of the material, whereas formula_1 and formula_2 are the electric, resp. magnetic susceptibilities. The tensor formula_6 describes the linear magnetoelectric effect, which corresponds to a polarization induced linearly by a magnetic field, and vice versa. The higher terms with coefficients formula_17 and formula_18 describe quadratic effects. For instance, the tensor formula_18 describes a linear magnetoelectric effect which is, in turn, induced by an electric field.\n\nThe possible terms appearing in the expansion above are constrained by symmetries of the material. Most notably, the tensor formula_6 must be antisymmetric under time-reversal symmetry. Therefore, the linear magnetoelectric effect may only occur if time-reversal symmetry is explicitly broken, for instance by the explicit motion in Röntgens' example, or by an intrinsic magnetic ordering in the material. In contrast, the tensor formula_17 may be non-vanishing in time-reversal symmetric materials.\n\nThere are several ways in which a magnetoelectric effect can arise microscopically in a material.\n\nIn crystals, spin-orbit coupling is responsible for single-ion magnetocrystalline anisotropy which determines preferential axes for the orientation of the spins (such as easy axes). An external electric field may change the local symmetry seen by magnetic ions and affect both the strength of the anisotropy and the direction of the easy axes. Thus, single-ion anisotropy can couple an external electric field to spins of magnetically ordered compounds.\n\nThe main interaction between spins of transition metal ions in solids is usually provided by superexchange. This interaction depends on details of the crystal structure such as the bond length between magnetic ions and the angle formed by the bonds between magnetic and ligand ions. Symmetric exchange can be both positive and negative and is the main responsible of magnetic ordering.\nAs the strength of symmetric exchange depends on the relative position of the ions, it couples spins to collective lattice distortions, called phonons.\nCoupling of spins to collective distortion with a net electric dipole can happen if the magnetic order breaks inversion symmetry. Thus, symmetric exchange can provide a handle to control magnetic properties through an external electric field.\n\nBecause materials exist that couple strain to electrical polarization (piezoelectrics, electrostrictives, and ferroelectrics) and that couple strain to magnetization (magnetostrictive/magnetoelastic/ferromagnetic materials), it is possible to couple magnetic and electric properties indirectly by creating composites of these materials that are tightly bonded so that strains transfer from one to the other.\n\nThin film strategy enables achievement of interfacial multiferroic coupling through a mechanical channel in heterostructures consisting of a magnetoelastic and a piezoelectric component. This type of heterostructure is composed of an epitaxial magnetoelastic thin film grown on a piezoelectric substrate. For this system, application of a magnetic field will induce a change in the dimension of the magnetoelastic film. This process, called magnetostriction, will alter residual strain conditions in the magnetoelastic film, which can be transferred through the interface to the piezoelectric substrate. Consequently, a polarization is introduced in the substrate through the piezoelectric process. The overall effect is that the polarization of the ferroelectric substrate is manipulated by an application of a magnetic field, which is the desired magnetoelectric effect (the reverse is also possible). In this case, the interface plays an important role in mediating the responses from one component to another, realizing the magnetoelectric coupling. For an efficient coupling, a high-quality interface with optimal strain state is desired. In light of this interest, advanced deposition techniques have been applied to synthesize these types of thin film heterostructures. Molecular beam epitaxy has been demonstrated to be capable of depositing structures consisting of piezoelectric and magnetostrictive components. Materials systems studied included cobalt ferrite, magnetite, SrTiO3, BaTiO3, PMNT.\n\nMagnetically driven ferroelectricity is also caused by inhomogeneous magnetoelectric interaction. This effect appears due to the coupling between inhomogeneous order parameters. It was also called as flexomagnetoelectric effect. Usually it is describing using the Lifshitz invariant (i.e. single-constant coupling term). It was shown that in general case of cubic hexoctahedral crystal the four phenomenological constants approach is correct. The flexomagnetoelectric effect appears in spiral multiferroics or micromagnetic structures like domain walls and magnetic vortexes. Ferroelectricity developed from micromagnetic structure can appear in any magnetic material even in centrosymmetric one. Building of symmetry classification of domain walls leads to determination of the type of electric polarization rotation in volume of any magnetic domain wall. Existing symmetry classification of magnetic domain walls was applied for predictions of electric polarization spatial distribution in their volumes. The predictions for almost all symmetry groups conform with phenomenology in which inhomogeneous magnetization couples with homogeneous polarization. The total synergy between symmetry and phenomenology theory appears if energy terms with electrical polarization spatial derivatives are taken into account.\n\n"}
{"id": "27249202", "url": "https://en.wikipedia.org/wiki?curid=27249202", "title": "Maximum landing weight", "text": "Maximum landing weight\n\nThe maximum landing weight (MLW) is the maximum aircraft gross weight due to design or operational limitations at which an aircraft is permitted to land.\n\nThe operation landing weight may be limited to a weight lower than the maximum landing weight by the most restrictive of the following requirements:\n\n\nIf the flight has been of short duration (for example an emergency just after takeoff), it may be necessary to dump fuel to reduce the landing weight.\n\nWhere aircraft overweight landing is permitted, a structural inspection or evaluation of the touch-down loads before the next aircraft operation will be required in case damage has occurred.\n"}
{"id": "12805568", "url": "https://en.wikipedia.org/wiki?curid=12805568", "title": "Novovoronezh Nuclear Power Plant", "text": "Novovoronezh Nuclear Power Plant\n\nThe Novovoronezh nuclear power station ( []) is a nuclear power station close to Novovoronezh in Voronezh Oblast, central Russia. The site was vital to the development of the VVER design; every unit built was essentially a prototype of its design. On this site is built the Novovoronezh Nuclear Power Plant II.\n\nIn 2002 Novovoronezh-3 was modernised and life extended, including new safety systems.\n\nIn 2010 Novovoronezh-5 was shut down for modernization to extend its operating life for an additional 25 years, the first VVER-1000 to undergo such an operating life extension. The works include the modernization of management, protection and emergency systems, and improvement of security and radiation safety systems.\n\nThe Novovoronezh Nuclear Power Plant has five units:\n\n"}
{"id": "165201", "url": "https://en.wikipedia.org/wiki?curid=165201", "title": "Nuclear Regulatory Commission", "text": "Nuclear Regulatory Commission\n\nThe Nuclear Regulatory Commission (NRC) is an independent agency of the United States government tasked with protecting public health and safety related to nuclear energy. Established by the Energy Reorganization Act of 1974, the NRC began operations on January 19, 1975 as one of two successor agencies to the United States Atomic Energy Commission. Its functions include overseeing reactor safety and security, administering reactor licensing and renewal, licensing radioactive materials, radionuclide safety, and managing the storage, security, recycling, and disposal of spent fuel.\n\nPrior to 1975 the Atomic Energy Commission was in charge of matters regarding radionuclides. The AEC was dissolved, because it was perceived as unduly favoring the industry it was charged with regulating. The NRC was formed as an independent commission to oversee nuclear energy matters, oversight of nuclear medicine, and nuclear safety and security.\n\nThe U.S. AEC became the Energy Research and Development Administration (ERDA) in 1975, responsible for development and oversight of nuclear weapons. Research and promotion of civil uses of radioactive materials, such as for nuclear non-destructive testing, nuclear medicine, and nuclear power, was split into the Office of Nuclear Energy, Science & Technology within ERDA by the same act. In 1977, ERDA became the United States Department of Energy (DOE). In 2000, the National Nuclear Security Administration was created as a subcomponent of DOE, responsible for nuclear weapons.\n\nTwelve years into NRC operations, a 1987 Congressional report entitled \"NRC Coziness with Industry\" concluded, that the NRC \"has not maintained an arms length regulatory posture with the commercial nuclear power industry... [and] has, in some critical areas, abdicated its role as a regulator altogether\". To cite three examples:\nA 1986 Congressional report found that NRC staff had provided valuable technical assistance to the utility seeking an operating license for the controversial Seabrook plant. In the late 1980s, the NRC 'created a policy' of non-enforcement by asserting its discretion not to enforce license conditions; between September 1989 and 1994, the 'NRC has either waived or chosen not to enforce regulations at nuclear power reactors over 340 times'. Finally, critics charge that the NRC has ceded important aspects of regulatory authority to the industry's own Institute for Nuclear Power Operations (INPO), an organization formed by utilities in response to the Three Mile Island Accident.\nThe origins and development of NRC regulatory processes and policies are explained in five volumes of history published by the University of California Press. These are:\n\nThe NRC has produced a booklet, \"A Short History of Nuclear Regulation 1946–2009\", which outlines key issues in NRC history. Thomas Wellock, a former academic, is the NRC historian. Before joining the NRC, Wellock wrote \"\".\n\nThe NRC's mission is to regulate the nation's civilian use of byproduct, source, and special nuclear materials to ensure adequate protection of public health and safety, to promote the common defense and security, and to protect the environment.\nThe NRC's regulatory mission covers three main areas:\n\nThe NRC is headed by five Commissioners appointed by the President of the United States and confirmed by the United States Senate for five-year terms. One of them is designated by the President to be the Chairman and official spokesperson of the Commission.\n\nThe current chairman is Kristine Svinicki. President Donald Trump designated Svinicki as Chairman of the NRC effective January 23, 2017.\n\nThe NRC consists of the Commission on the one hand and offices of the Executive Director for Operations on the other.\nThe Commission is divided into two committees (Advisory Committee on Reactor Safeguards and Advisory Committee on the Medical Uses of Isotopes) and one Board, the Atomic Safety and Licensing Board Panel, as well as eight commission staff offices (Office of Commission Appellate Adjudication, Office of Congressional Affairs, Office of the General Counsel, Office of International Programs, Office of Public Affairs, Office of the Secretary, Office of the Chief Financial Officer, Office of the Executive Director for Operations).\n\nKristine Svinicki is the chairman of the NRC. There are altogether 17 Executive Director for Operations offices: Office of Federal and State Materials and Environmental Management Programs,\nOffice of New Reactors, Office of Nuclear Material Safety and Safeguards, Office of Nuclear Reactor Regulation, Office of Nuclear Regulatory Research, Office of Enforcement, which investigates reports by nuclear power whistleblowers, specifically the Allegations Program, Office of Investigations, Office of Nuclear Security and Incident Response, Region I, Region II, Region III, Region IV, Office of Information Services, Computer Security Office, Office of Administration, Office of the Chief Human Capital Officer, and Office of Small Business and Civil Rights.\n\nOf these operations offices, NRC's major program components are the first five offices mentioned above.\n\nNRC's proposed FY 2015 budget is $1,059.5 million, with 3,895.9 full-time equivalents (FTE), 90 percent of which is recovered by fees. This is an increase of $3.6 million, including 65.1 FTE, compared to FY 2014.\n\nNRC headquarters offices are located in unincorporated North Bethesda, Maryland (although the mailing address for two of the three main buildings in the complex list the city as Rockville, MD), and there are four regional offices.\n\nThe NRC territory is broken down into four geographical regions; until the late 1990s, there was a Region V office in Walnut Creek, California which was absorbed into Region IV, and Region V was dissolved.\n\n\nIn these four regions NRC oversees the operation of US nuclear reactors, namely 104 power-producing reactors, and 36 non-power-producing reactors. Oversight is done on several levels. For example:\n\n\nNRC has a library, which also contains online document collections. In 1999 it started an electronic repository called ADAMS, the Agencywide Documents Access and Management System. for its public inspection reports, correspondence, and other technical documents written by NRC staff, contractors, and licensees. It has been upgraded in October 2010 and is now webbased. Of documents from 1980 to 1999 only some have abstracts and/or full text, most are citations. Documents from before 1980 are available in paper or microfiche formats. Copies of these older documents or classified documents can be applied for with a FOIA request.\n\nNRC conducts audits and training inspections, observes the National Nuclear Accrediting Board meetings, and nominates some members.\n\nThe 1980 Kemeny Commission's report after the Three Mile Island accident recommended that the nuclear energy industry \"set and police its own standards of excellence\". The nuclear industry founded the Institute of Nuclear Power Operations (INPO) within 9 months to establish personnel training and qualification. The industry through INPO created the 'National Academy for Nuclear Training Program' either as early as 1980 or in September 1985 per the International Atomic Energy Agency. INPO refers to NANT as \"our National Academy for Nuclear Training\" on its website. NANT integrates and standardizes the training programs of INPO and US nuclear energy companies, offers training scholarships and interacts with the 'National Nuclear Accrediting Board'. This Board is closely related to the 'National Academy for Nuclear Training', not a government body, and referred to as independent by INPO, the Nuclear Energy Institute, and nuclear utilities. but not by the NRC, all of whom are represented on the Board\n\nThe 1982 Nuclear Waste Policy Act directed NRC in Section 306 to issue regulations or \"other appropriate regulatory guidance\" on training of nuclear plant personnel. Since the nuclear industry already had developed training and accreditation, NRC issued a policy statement in 1985, endorsing the INPO program. NRC has a memorandum of agreement with INPO and \"monitors INPO activities by observing accreditation team visits and the monthly NNAB meetings\".\n\nIn 1993, NRC endorsed the industry's approach to training that had been used for nearly a decade through its 'Training Rule'. In February 1994, NRC passed the 'Operator Requalification Rule' 59 FR 5938, Feb. 9, 1994, allowing each nuclear power plant company -rather than the agency- to conduct the operator licensing renewal examination every six years, eliminating the requirement of NRC-administered written requalification examination.\n\nIn 1999, NRC issued a final rule on operator initial licensing examination, that allows, but does not require, companies to \"prepare, proctor, and grade\" their own operator initial licensing examinations. Facilities can \"upon written request\" continue to have the examinations prepared and administered by NRC staff, but if a company volunteers to prepare the examination, NRC continues to approve and administer it.\n\nSince 2000 meetings between NRC and applicants or licensees have been open to the public.\n\nTerrorist attacks such as those executed by al-Qaeda on New York City and Washington, D.C. on September 11, 2001 and in London on July 7, 2005 have prompted fears that extremist groups might use radioactive dirty bombs in further attacks in the United States and elsewhere.\nIn March 2007, undercover investigators from the Government Accountability Office set up a false company and obtained a license from the Nuclear Regulatory Commission that would have allowed them to buy the radioactive materials needed for a dirty bomb. According to the GAO report, NRC officials did not visit the company or attempt to personally interview its executives. Instead, within 28 days, the NRC mailed the license to the West Virginia postal box. Upon receipt of the license, GAO officials were able to easily modify its stipulations, and remove a limit on the amount of radioactive material they could buy. A spokesman for the NRC said that the agency considered the radioactive devices a \"lower-level threat\"; a bomb built with the materials could have contaminated an area about the length of a city block, but would not have presented an immediate health hazard.\n\nBetween 2007 and 2009, 13 companies applied to the Nuclear Regulatory Commission for construction and operating licenses to build 25 new nuclear power reactors in the United States.\nHowever, the case for widespread nuclear plant construction was eroded due to abundant natural gas supplies, slow electricity demand growth in a weak U.S. economy, lack of financing, and uncertainty following the Fukushima nuclear disaster. Many license applications for proposed new reactors were suspended or cancelled. Only a few new reactors will enter service by 2020. These will not be the cheapest energy options available, but they are an attractive investment for utilities because the government mandates that taxpayers pay for construction in advance. In 2013, four aging reactors were permanently closed: San Onofre 2 and 3 in California, Crystal River 3 in Florida, and Kewaunee in Wisconsin. Vermont Yankee, in Vernon, was shut down on December 29, 2014, following many protests. New York State is seeking to close Indian Point Energy Center, in Buchanan, 30 miles from New York City.\n\nByrne and Hoffman wrote in 1996, that since the 1980s the NRC has generally favored the interests of nuclear industry, and been unduly responsive to industry concerns, while failing to pursue tough regulation. The NRC has often sought to hamper or deny public access to the regulatory process, and created new barriers to public participation.\n\nBarack Obama, when running for president in 2007, said that the five-member NRC had become \"captive of the industries that it regulates\"\n\nNumerous different observers have criticized the NRC as an example of regulatory capture The NRC has been accused of having conflicting roles as regulator and \"salesman\" in a 2011 Reuters article, doing an inadequate job by the Union of Concerned Scientists. and has the agency approval process has been called a \"rubber stamp\".\n\nFrank N. von Hippel wrote in March 2011, that despite the 1979 Three Mile Island accident in Pennsylvania, the NRC has often been too timid in ensuring that America's 104 commercial reactors are operated safely:\n\nNuclear power regulation is a textbook example of the problem of \"regulatory capture\" — in which an industry gains control of an agency meant to regulate it. Regulatory capture can be countered only by vigorous public scrutiny and Congressional oversight, but in the 32 years since Three Mile Island, interest in nuclear regulation has declined precipitously.\nAn article in the Bulletin of the Atomic Scientists stated that many forms of NRC regulatory failure exist, including regulations ignored by the common consent of NRC and industry:\n\nA worker (named George Galatis) at the Millstone Nuclear Power Plant in Connecticut kept warning management, that the spent fuel rods were being put too quickly into the spent storage pool and that the number of rods in the pool exceeded specifications. Management ignored him, so he went directly to the NRC, which eventually admitted that it knew of both of the forbidden practices, which happened at many plants, but chose to ignore them. The whistleblower was fired and blacklisted.\nIn Vermont, the day before the 2011 Tōhoku earthquake and tsunami that damaged Japan's Fukushima Daiichi Nuclear Power Plant, the NRC approved a 20-year extension for the license of Vermont Yankee Nuclear Power Plant, although the Vermont state legislature voted overwhelmingly to deny an extension. The plant had been found to be leaking radioactive materials through a network of underground pipes, which Entergy had denied under oath even existed. At a hearing in 2009 Tony Klein, chairman of the Vermont House Natural Resources and Energy Committee had asked the NRC about the pipes and the NRC also did not know they existed.\n\nIn March 2011, the Union of Concerned Scientists released a study critical of the NRC's 2010 performance as a regulator. The UCS said that over the years, it had found the NRC's enforcement of safety rules has not been \"timely, consistent, or effective\" and it cited 14 \"near-misses\" at U.S. plants in 2010 alone.\n\nIn April 2011, Reuters reported that diplomatic cables showed NRC sometimes being used as a sales tool to help push American technology to foreign governments, when \"lobbying for the purchase of equipment made by Westinghouse Electric Company and other domestic manufacturers\". This gives the appearance of a regulator which is acting in a commercial capacity, \"raising concerns about a potential conflict of interest\".\n\nSan Clemente Green, an environmental group opposed to the continued operation of the San Onofre Nuclear Plant, said in 2011 that instead of being a watchdog, the NRC too often rules in favor nuclear plant operators.\n\nCritics have long charged that NRC has intentionally mislead the public by dismissing critical nuclear safety and environmental issues. One example involves the license renewal program that NRC initiated to extend the operating licenses for the nation's fleet of aging commercial nuclear reactors. Environmental impact statements (EIS) were prepared for each reactor to extend the operational period from 40 to 60 years. One study examined the EISs and found significant flaws, included failure to consider significant issues of concern. It also found that the NRC management had significantly underestimated risk of dismissal of significant issues and failure to significantly underestimated the risk and consequences posed by a severe reactor accident such as a full-scale nuclear meltdown. NRC management asserted, without scientific evidence, that the risk of such accidents were so \"Small\" that the impacts could be dismissed and therefore no analysis of human and environmental was even performed. Such a conclusion is scientifically indefensible given the experience of the Three Mile Island, Chernobyl, and Fukushima accidents. Another finding was that NRC had concealed the risk posed to the public at large by disregarding one of the most important EIS requirements, mandating that cumulative impacts be assessed (40 Code of Federal Regulations §1508.7). By disregarding this basic requirement, NRC effectively misrepresented the risk posed to the nation by approximately two orders of magnitude (i.e., the true risk is about 100 greater than NRC represented). These findings were collaborated in a final report prepared by a special Washington State Legislature Nuclear Power Task Force, titled, \"Doesnt NRC Address Consequences of Severe Accidents in EISs for re-licensing?\" \n\nGregory Jaczko was Chairman of the NRC when the 2011 Fukushima disaster occurred in Japan. Jaczko looked for lessons for the US, and strengthened security regulations for nuclear power plants. For example, he supported the requirement that new plants to be able to withstand an aircraft crash. On February 9, 2012 Jaczko cast the lone dissenting vote on plans to build the first new nuclear power plant in more than 30 years when the NRC voted 4–1 to allow Atlanta-based Southern Co to build and operate two new nuclear power reactors at its existing Vogtle Electric Generating Plant in Georgia. He cited safety concerns stemming from Japan's 2011 Fukushima nuclear disaster, saying \"I cannot support issuing this license as if Fukushima never happened\". In July 2011, Mark Cooper said that the Nuclear Regulatory Commission is \"on the defensive to prove it is doing its job of ensuring safety\". In October 2011, Jaczko described \"a tension between wanting to move in a timely manner on regulatory questions, and not wanting to go too fast\".\n\nIn 2011 Edward J. Markey, Democrat of Massachusetts, criticized the NRC's response to the Fukushima Daiichi nuclear disaster and the decision-making on the proposed Westinghouse AP1000 reactor design.\n\nIn 2011, a total of 45 groups and individuals from across the nation formally asked the NRC to suspend all licensing and other activities at 21 proposed nuclear reactor projects in 15 states until the NRC completed a thorough post-Fukushima nuclear disaster examination:\n\nThe petitioners asked the NRC to supplement its own investigation by establishing an independent commission comparable to that set up in the wake of the less severe 1979 Three Mile Island accident. The petitioners included Public Citizen, Southern Alliance for Clean Energy, and San Luis Obispo Mothers for Peace.\n\nThe flooding of Fukushima led to the meltdown of three reactor cores and release of radiation so high that 100,000 citizens were forced to evacuate. Following the Fukushima disaster, the NRC prepared a report in 2011 to examine the risk that dam failures posed on the nation's fleet of nuclear reactors. A redacted version of NRC's report on dam failures was posted on the NRC website on March 6. The original, un-redacted version was leaked to the public.\n\nThe un-redacted version which was leaked to the public highlights the threat that flooding poses to nuclear power plants located near large dams and substantiates claims that NRC management has intentionally misled the public for years about the severity of the flooding.\n\nThe leaked version of the report concluded that one-third of the U.S. nuclear fleet (34 plants) may face flooding hazards greater than they were designed to withstand. It also shows that NRC management was aware of some aspects of this risk for 15 years and yet it had done nothing to effectively address the problem. Some flooding events are so serious that they could result in a \"severe\" nuclear accident, up to, and including, a nuclear meltdown.\n\nThis criticism is collaborated by two NRC whistleblowers who accused their management of deliberately covering up information concerning the vulnerability of flooding, and of failing to take corrective actions despite being aware of these risks for years. Mr. Richard Perkins, a second risk engineer with the NRC and the lead author of the leaked report, filed a complaint with the agency's Inspector General, asserting that NRC staff had improperly redacted information from the public version of his report \"to prevent the disclosure of this safety information to the public because it will embarrass the agency.\" Perkins wrote. \"Concurrently, the NRC concealed the information from the public.\"\n\nMr. Larry Criscione, a second NRC risk engineer also raised concerns about the NRC withholding information concerning the risk of flooding. He stated that assertions by NRC's management that plants are \"currently able to mitigate flooding events,\" was false.\n\nDavid Lochbaum, a nuclear engineer and safety advocate with the Union of Concerned Scientists: \"The redacted information shows that the NRC is lying to the American public about the safety of U.S. reactors,\" \n\nThe Oconee Nuclear Station has been shown to be at particular risk from flooding. An NRC letter dated 2009 states that \"a Jocassee Dam failure is a credible event\" It goes on to state that \"NRC staff expressed concerns that Duke has not demonstrated that the [null Oconee Nuclear Station] units will be adequately protected.\"\n\nNRC's 2011 leaked report notes that \"dam failure incidents are common\". NRC estimated the odds that dams constructed like Jocassee will fail is about 1 in 3,600 failures per year. Oconee is licensed to operate for another 20 years. The odds of the Jocassee Dam failing over that period are 1 in 180. NRC requires risks to be investigated if they have a frequency of more than 1 in 10,000 years. For a reactor operating over a period of 40 years, these risks must be evaluated if they have a chance greater than a 1 in 250 of occurring.\n\nNRC identified 34 reactors that lie downstream from a total of more than 50 dams. More than half of these dams are roughly the size of the Jocassee dam. Assuming the NRC's failure rate applies to all of these dams, the chance that one will fail over the next 40 years is about 1 in 4 or 25 percent chance. This dam failure rate does not include risks posed by earthquakes or terrorism. Thus, the true probability may be much higher. \n\nThis raised a second and potentially larger issue. NRC recently completed its license renewal program which extended the operating licenses of the nation's fleet of nuclear reactors for an additional 20 years. NRC stated that the probability of a severe accident is so incredible that the consequences can be dismissed from the analysis of impacts in its relicensing environmental impact statements (EIS).Environmental impact statement Yet this conflicts with NRC's internal analyses which concluded that flooding presented a serious human and environmental risk. Critics charge that if these relicensing EISs failed to evaluate the risks of flooding, then how can the public be confident that NRC did not mislead stakeholders concerning other risks such as the potential for a nuclear meltdown.\n\nNRC officials stated in June 2011 that US nuclear safety rules do not adequately weigh the risk of a single event that would knock out electricity from the grid and from emergency generators, as a quake and tsunami did in Japan. , and NRC instructed agency staff to move forward with seven of the 12 safety recommendations put forward by a federal task force in July 2011. The recommendations include \"new standards aimed at strengthening operators' ability to deal with a complete loss of power, ensuring plants can withstand floods and earthquakes and improving emergency response capabilities\". The new safety standards will take up to five years to fully implement.\n\n, Jaczko warned power companies against complacency and said the agency must \"push ahead with new rules prompted by the nuclear crisis in Japan, while also resolving long-running issues involving fire protection and a new analysis of earthquake risks\".\n\n"}
{"id": "97830", "url": "https://en.wikipedia.org/wiki?curid=97830", "title": "Nuclear technology", "text": "Nuclear technology\n\nNuclear technology is technology that involves the nuclear reactions of atomic nuclei. Among the notable nuclear technologies are nuclear reactors, nuclear medicine and nuclear weapons. It is also used, among other things, in smoke detectors and gun sights.\n\nThe vast majority of common, natural phenomena on Earth only involve gravity and electromagnetism, and not nuclear reactions. This is because atomic nuclei are generally kept apart because they contain positive electrical charges and therefore repel each other.\n\nIn 1896, Henri Becquerel was investigating phosphorescence in uranium salts when he discovered a new phenomenon which came to be called radioactivity. He, Pierre Curie and Marie Curie began investigating the phenomenon. In the process, they isolated the element radium, which is highly radioactive. They discovered that radioactive materials produce intense, penetrating rays of three distinct sorts, which they labeled alpha, beta, and gamma after the first three Greek letters. Some of these kinds of radiation could pass through ordinary matter, and all of them could be harmful in large amounts. All of the early researchers received various radiation burns, much like sunburn, and thought little of it.\n\nThe new phenomenon of radioactivity was seized upon by the manufacturers of quack medicine (as had the discoveries of electricity and magnetism, earlier), and a number of patent medicines and treatments involving radioactivity were put forward.\n\nGradually it was realized that the radiation produced by radioactive decay was ionizing radiation, and that even quantities too small to burn could pose a severe long-term hazard. Many of the scientists working on radioactivity died of cancer as a result of their exposure. Radioactive patent medicines mostly disappeared, but other applications of radioactive materials persisted, such as the use of radium salts to produce glowing dials on meters.\n\nAs the atom came to be better understood, the nature of radioactivity became clearer. Some larger atomic nuclei are unstable, and so decay (release matter or energy) after a random interval. The three forms of radiation that Becquerel and the Curies discovered are also more fully understood. Alpha decay is when a nucleus releases an alpha particle, which is two protons and two neutrons, equivalent to a helium nucleus. Beta decay is the release of a beta particle, a high-energy electron. Gamma decay releases gamma rays, which unlike alpha and beta radiation are not matter but electromagnetic radiation of very high frequency, and therefore energy. This type of radiation is the most dangerous and most difficult to block. All three types of radiation occur naturally in certain elements.\n\nIt has also become clear that the ultimate source of most terrestrial energy is nuclear, either through radiation from the Sun caused by stellar thermonuclear reactions or by radioactive decay of uranium within the Earth, the principal source of geothermal energy.\n\nIn natural nuclear radiation, the byproducts are very small compared to the nuclei from which they originate. Nuclear fission is the process of splitting a nucleus into roughly equal parts, and releasing energy and neutrons in the process. If these neutrons are captured by another unstable nucleus, they can fission as well, leading to a chain reaction. The average number of neutrons released per nucleus that go on to fission another nucleus is referred to as \"k\". Values of \"k\" larger than 1 mean that the fission reaction is releasing more neutrons than it absorbs, and therefore is referred to as a self-sustaining chain reaction. A mass of fissile material large enough (and in a suitable configuration) to induce a self-sustaining chain reaction is called a critical mass.\n\nWhen a neutron is captured by a suitable nucleus, fission may occur immediately, or the nucleus may persist in an unstable state for a short time. If there are enough immediate decays to carry on the chain reaction, the mass is said to be prompt critical, and the energy release will grow rapidly and uncontrollably, usually leading to an explosion.\n\nWhen discovered on the eve of World War II, this insight led multiple countries to begin programs investigating the possibility of constructing an atomic bomb — a weapon which utilized fission reactions to generate far more energy than could be created with chemical explosives. The Manhattan Project, run by the United States with the help of the United Kingdom and Canada, developed multiple fission weapons which were used against Japan in 1945 at Hiroshima and Nagasaki. During the project, the first fission reactors were developed as well, though they were primarily for weapons manufacture and did not generate electricity.\n\nIn 1951, the first nuclear fission power plant was the first to produce electricity at the Experimental Breeder Reactor No. 1 (EBR-1), in Arco, Idaho, ushering in the \"Atomic Age\" of more intensive human energy use.\n\nHowever, if the mass is critical only when the delayed neutrons are included, then the reaction can be controlled, for example by the introduction or removal of neutron absorbers. This is what allows nuclear reactors to be built. Fast neutrons are not easily captured by nuclei; they must be slowed (slow neutrons), generally by collision with the nuclei of a neutron moderator, before they can be easily captured. Today, this type of fission is commonly used to generate electricity.\n\nIf nuclei are forced to collide, they can undergo nuclear fusion. This process may release or absorb energy. When the resulting nucleus is lighter than that of iron, energy is normally released; when the nucleus is heavier than that of iron, energy is generally absorbed. This process of fusion occurs in stars, which derive their energy from hydrogen and helium. They form, through stellar nucleosynthesis, the light elements (lithium to calcium) as well as some of the heavy elements (beyond iron and nickel, via the S-process). The remaining abundance of heavy elements, from nickel to uranium and beyond, is due to supernova nucleosynthesis, the R-process.\n\nOf course, these natural processes of astrophysics are not examples of nuclear \"technology\". Because of the very strong repulsion of nuclei, fusion is difficult to achieve in a controlled fashion. Hydrogen bombs obtain their enormous destructive power from fusion, but their energy cannot be controlled. Controlled fusion is achieved in particle accelerators; this is how many synthetic elements are produced. A fusor can also produce controlled fusion and is a useful neutron source. However, both of these devices operate at a net energy loss. Controlled, viable fusion power has proven elusive, despite the occasional hoax. Technical and theoretical difficulties have hindered the development of working civilian fusion technology, though research continues to this day around the world.\n\nNuclear fusion was initially pursued only in theoretical stages during World War II, when scientists on the Manhattan Project (led by Edward Teller) investigated it as a method to build a bomb. The project abandoned fusion after concluding that it would require a fission reaction to detonate. It took until 1952 for the first full hydrogen bomb to be detonated, so-called because it used reactions between deuterium and tritium. Fusion reactions are much more energetic per unit mass of fuel than fission reactions, but starting the fusion chain reaction is much more difficult.\n\nA nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either fission or a combination of fission and fusion. Both reactions release vast quantities of energy from relatively small amounts of matter. Even small nuclear devices can devastate a city by blast, fire and radiation. Nuclear weapons are considered weapons of mass destruction, and their use and control has been a major aspect of international policy since their debut.\n\nThe design of a nuclear weapon is more complicated than it might seem. Such a weapon must hold one or more subcritical fissile masses stable for deployment, then induce criticality (create a critical mass) for detonation. It also is quite difficult to ensure that such a chain reaction consumes a significant fraction of the fuel before the device flies apart. The procurement of a nuclear fuel is also more difficult than it might seem, since sufficiently unstable substances for this process do not currently occur naturally on Earth in suitable amounts.\n\nOne isotope of uranium, namely uranium-235, is naturally occurring and sufficiently unstable, but it is always found mixed with the more stable isotope uranium-238. The latter accounts for more than 99% of the weight of natural uranium. Therefore, some method of isotope separation based on the weight of three neutrons must be performed to enrich (isolate) uranium-235.\n\nAlternatively, the element plutonium possesses an isotope that is sufficiently unstable for this process to be usable. Terrestrial plutonium does not currently occur naturally in sufficient quantities for such use, so it must be manufactured in a nuclear reactor.\n\nUltimately, the Manhattan Project manufactured nuclear weapons based on each of these elements. They detonated the first nuclear weapon in a test code-named \"Trinity\", near Alamogordo, New Mexico, on July 16, 1945. The test was conducted to ensure that the implosion method of detonation would work, which it did. A uranium bomb, Little Boy, was dropped on the Japanese city Hiroshima on August 6, 1945, followed three days later by the plutonium-based Fat Man on Nagasaki. In the wake of unprecedented devastation and casualties from a single weapon, the Japanese government soon surrendered, ending World War II.\n\nSince these bombings, no nuclear weapons have been deployed offensively. Nevertheless, they prompted an arms race to develop increasingly destructive bombs to provide a nuclear deterrent. Just over four years later, on August 29, 1949, the Soviet Union detonated its first fission weapon. The United Kingdom followed on October 2, 1952; France, on February 13, 1960; and China component to a nuclear weapon. Approximately half of the deaths from Hiroshima and Nagasaki died two to five years afterward from radiation exposure. A radiological weapons is a type of nuclear weapon designed to distribute hazardous nuclear material in enemy areas. Such a weapon would not have the explosive capability of a fission or fusion bomb, but would kill many people and contaminate a large area. A radiological weapon has never been deployed. While considered useless by a conventional military, such a weapon raises concerns over nuclear terrorism.\n\nThere have been over 2,000 nuclear tests conducted since 1945. In 1963, all nuclear and many non-nuclear states signed the Limited Test Ban Treaty, pledging to refrain from testing nuclear weapons in the atmosphere, underwater, or in outer space. The treaty permitted underground nuclear testing. France continued atmospheric testing until 1974, while China continued up until 1980. The last underground test by the United States was in 1992, the Soviet Union in 1990, the United Kingdom in 1991, and both France and China continued testing until 1996. After signing the Comprehensive Test Ban Treaty in 1996 (which had as of 2011 not entered into force), all of these states have pledged to discontinue all nuclear testing. Non-signatories India and Pakistan last tested nuclear weapons in 1998.\n\nNuclear weapons are the most destructive weapons known - the archetypal weapons of mass destruction. Throughout the Cold War, the opposing powers had huge nuclear arsenals, sufficient to kill hundreds of millions of people. Generations of people grew up under the shadow of nuclear devastation, portrayed in films such as \"Dr. Strangelove\" and \"The Atomic Cafe\".\n\nHowever, the tremendous energy release in the detonation of a nuclear weapon also suggested the possibility of a new energy source.\n\nNuclear power is a type of nuclear technology involving the controlled use of nuclear fission to release energy for work including propulsion, heat, and the generation of electricity. Nuclear energy is produced by a controlled nuclear chain reaction which creates heat—and which is used to boil water, produce steam, and drive a steam turbine. The turbine is used to generate electricity and/or to do mechanical work.\n\nCurrently nuclear power provides approximately 15.7% of the world's electricity (in 2004) and is used to propel aircraft carriers, icebreakers and submarines (so far economics and fears in some ports have prevented the use of nuclear power in transport ships). All nuclear power plants use fission. No man-made fusion reaction has resulted in a viable source of electricity.\n\nThe medical applications of nuclear technology are divided into diagnostics and radiation treatment.\n\nImaging - The largest use of ionizing radiation in medicine is in medical radiography to make images of the inside of the human body using x-rays. This is the largest artificial source of radiation exposure for humans. Medical and dental x-ray imagers use of cobalt-60 or other x-ray sources. A number of radiopharmaceuticals are used, sometimes attached to organic molecules, to act as radioactive tracers or contrast agents in the human body. Positron emitting nucleotides are used for high resolution, short time span imaging in applications known as Positron emission tomography.\n\nRadiation is also used to treat diseases in radiation therapy.\n\nSince some ionizing radiation can penetrate matter, they are used for a variety of measuring methods. X-rays and gamma rays are used in industrial radiography to make images of the inside of solid products, as a means of nondestructive testing and inspection. The piece to be radiographed is placed between the source and a photographic film in a cassette. After a certain exposure time, the film is developed and it shows any internal defects of the material.\n\nGauges - Gauges use the exponential absorption law of gamma rays\n\nElectrostatic control - To avoid the build-up of static electricity in production of paper, plastics, synthetic textiles, etc., a ribbon-shaped source of the alpha emitter Am can be placed close to the material at the end of the production line. The source ionizes the air to remove electric charges on the material.\n\nRadioactive tracers - Since radioactive isotopes behave, chemically, mostly like the inactive element, the behavior of a certain chemical substance can be followed by \"tracing\" the radioactivity. Examples:\n\nOil and Gas Exploration- Nuclear well logging is used to help predict the commercial viability of new or existing wells. The technology involves the use of a neutron or gamma-ray source and a radiation detector which are lowered into boreholes to determine the properties of the surrounding rock such as porosity and lithography.\n\nRoad Construction - Nuclear moisture/density gauges are used to determine the density of soils, asphalt, and concrete. Typically a cesium-137 source is used.\n\n\nIn biology and agriculture, radiation is used to induce mutations to produce new or improved species. Another use in insect control is the sterile insect technique, where male insects are sterilized by radiation and released, so they have no offspring, to reduce the population.\n\nIn industrial and food applications, radiation is used for sterilization of tools and equipment. An advantage is that the object may be sealed in plastic before sterilization. An emerging use in food production is the sterilization of food using food irradiation.\nFood irradiation is the process of exposing food to ionizing radiation in order to destroy microorganisms, bacteria, viruses, or insects that might be present in the food. The radiation sources used include radioisotope gamma ray sources, X-ray generators and electron accelerators. Further applications include sprout inhibition, delay of ripening, increase of juice yield, and improvement of re-hydration. Irradiation is a more general term of deliberate exposure of materials to radiation to achieve a technical goal (in this context 'ionizing radiation' is implied). As such it is also used on non-food items, such as medical hardware, plastics, tubes for gas-pipelines, hoses for floor-heating, shrink-foils for food packaging, automobile parts, wires and cables (isolation), tires, and even gemstones. Compared to the amount of food irradiated, the volume of those every-day applications is huge but not noticed by the consumer.\n\nThe genuine effect of processing food by ionizing radiation relates to damages to the DNA, the basic genetic information for life. Microorganisms can no longer proliferate and continue their malignant or pathogenic activities. Spoilage causing micro-organisms cannot continue their activities. Insects do not survive or become incapable of procreation. Plants cannot continue the natural ripening or aging process. All these effects are beneficial to the consumer and the food industry, likewise.\n\nThe amount of energy imparted for effective food irradiation is low compared to cooking the same; even at a typical dose of 10 kGy most food, which is (with regard to warming) physically equivalent to water, would warm by only about 2.5 °C (4.5 °F).\n\nThe specialty of processing food by ionizing radiation is the fact, that the energy density per atomic transition is very high, it can cleave molecules and induce ionization (hence the name) which cannot be achieved by mere heating. This is the reason for new beneficial effects, however at the same time, for new concerns. The treatment of solid food by ionizing radiation can provide an effect similar to heat pasteurization of liquids, such as milk. However, the use of the term, cold pasteurization, to describe irradiated foods is controversial, because pasteurization and irradiation are fundamentally different processes, although the intended end results can in some cases be similar.\n\nDetractors of food irradiation have concerns about the health hazards of induced radioactivity. Also, a report for the American Council on Science and Health entitled \"Irradiated Foods\" states: \"The types of radiation sources approved for the treatment of foods have specific energy levels well below that which would cause any element in food to become radioactive. Food undergoing irradiation does not become any more radioactive than luggage passing through an airport X-ray scanner or teeth that have been X-rayed.\" \n\nFood irradiation is currently permitted by over 40 countries and volumes are estimated to exceed annually worldwide.\n\nFood irradiation is essentially a non-nuclear technology; it relies on the use of ionizing radiation which may be generated by accelerators for electrons and conversion into bremsstrahlung, but which may use also gamma-rays from nuclear decay. There is a worldwide industry for processing by ionizing radiation, the majority by number and by processing power using accelerators. Food irradiation is only a niche application compared to medical supplies, plastic materials, raw materials, gemstones, cables and wires, etc.\n\nNuclear accidents, because of the powerful forces involved, are often very dangerous. Historically, the first incidents involved fatal radiation exposure. Marie Curie died from aplastic anemia which resulted from her high levels of exposure. Two scientists, an American and Canadian respectively, Harry Daghlian and Louis Slotin, died after mishandling the same plutonium mass. Unlike conventional weapons, the intense light, heat, and explosive force is not the only deadly component to a nuclear weapon. Approximately half of the deaths from Hiroshima and Nagasaki died two to five years afterward from radiation exposure.\n\nCivilian nuclear and radiological accidents primarily involve nuclear power plants. Most common are nuclear leaks that expose workers to hazardous material. A nuclear meltdown refers to the more serious hazard of releasing nuclear material into the surrounding environment. The most significant meltdowns occurred at Three Mile Island in Pennsylvania and Chernobyl in the Soviet Ukraine. The earthquake and tsunami on March 11, 2011 caused serious damage to three nuclear reactors and a spent fuel storage pond at the Fukushima Daiichi nuclear power plant in Japan. Military reactors that experienced similar accidents were Windscale in the United Kingdom and SL-1 in the United States.\n\nMilitary accidents usually involve the loss or unexpected detonation of nuclear weapons. The Castle Bravo test in 1954 produced a larger yield than expected, which contaminated nearby islands, a Japanese fishing boat (with one fatality), and raised concerns about contaminated fish in Japan. In the 1950s through 1970s, several nuclear bombs were lost from submarines and aircraft, some of which have never been recovered. The last twenty years have seen a marked decline in such accidents.\n\n\n"}
{"id": "26502128", "url": "https://en.wikipedia.org/wiki?curid=26502128", "title": "Pandit Deendayal Petroleum University", "text": "Pandit Deendayal Petroleum University\n\nPandit Deendayal Petroleum University (or PDPU) Gujarat, established through PDPU Act, a legislature passed by Gujarat state assembly in 2007. The university is located at Raisan village of Gandhinagar city in an area known as the Knowledge Corridor, and it is near the upcoming GIFT City. It is 8 km from Gujarat state capital Gandhinagar, and 23 km from Gujarat's largest city Ahmedabad.\n\nThe university is ranked 55th in India and 1st in Gujarat by National Institutional Ranking Framework (NIRF), Ministry of Human Resource Development, Government of India. PDPU has also been awarded Scientific & Industrial Research Organization (SIRO) recognition by Department of Scientific and Industrial Research, Ministry of Science & Technology, Government of India.\n\nThe university has four schools, located on the same campus. The schools include the School of Petroleum Technology, School of Technology, School of Petroleum Management, and School of Liberal Studies. The President of University Board of Governors is Dr. Mukesh Ambani and the Chairman of the Standing Committee is Shri D. Rajagopalan, IAS (Retd.), Government of Gujarat.\n\nThe university also has its own 1 megawatt solar power plant. The Government of Gujarat has decided to set up an international Centre of Excellence in Automobile at PDPU with investment of Rs. 150 Crores (US$ 25M) in joint venture with Maruti Suzuki India Ltd(MSIL).\n\nPandit Deendayal Petroleum University (PDPU) has been established by Gujarat energy Research and Management Institute (GERMI) as a Private University through the State Act enacted on 4 April 2007. \n\nThe University is recognised by UGC (University Grants Commission) and is a member of AIU (Association of Indian Universities).The university has been accredited with Grade A by the National Assessment and Accreditation Council In March 2018, Minister of Human Resource Development, Prakash Javadekar announced that the University Grants Commission (India) has granted autonomy to PDPU, making it the only of two private universities in India to have accorded this autonomy. Henceforth university will have the freedom to start new courses, off campus centers, skill development courses, research parks and any other new academic programs. It will also give it the freedom to hire foreign faculty, enroll foreign students, give incentive based emoluments to the faculty, enter into academic collaborations and run open distance learning programs. \n\nThe 4 schools of PDPU offer both niche programs as well as programs of general interest, covering UG, PG and Doctoral programs in Management, Humanities, and Engineering:\n\nThe campus is located at 14 km from the Ahmedabad Airport and 21 km from Ahmedabad's Kalupur Railway Station. It is 3 km off the main road connecting Gandhinagar to Ahmedabad Airport, Ahmedabad. The university is spread across a lush green pollution-free 100 acres campus with very good biodiversity. The entire space is dotted with lots of mango trees, and the campus houses 18 different varieties of birds.\n\nThe university started with a single building and then over the years, expanded infrastructure to four large administrative blocks, an auditorium block, hostel blocks, cafeteria block, sports grounds, and manicured gardens.\n\nThe entire area is walled with 4 access points, is CCTV monitored, and tobacco-free. On-campus facilities (apart from classrooms and offices) include SBI Branch, pharmacy, stationery store, resident daytime doctor, 24x7 doctor-on-call, ambulance, travel desk and campus store.\n\nThe Office of Research and Sponsored Programs provides support for the free and responsible conduct of investigative, scholarly and creative activities, including support from the initial stages of proposal development to grants management, publication, and the transfer of technology. It provides internally funded grants, pre-award and post-award support for externally funded grants and offer education and support information on grant compliance and the responsible conduct of research.\n\nThe university pursues active partnerships and collaborates with foreign universities through MOUs (Memorandum of Understanding). Partnership includes Summer student Exchange Program, and Co-operation in academics. Currently active partnerships include:\n\nWashington & Jefferson College, Memorial University, Edith Cowan University, University of Bahrain, University of Manitoba, University of Wollongong, University of Alberta, University of Tulsa, Oklahoma University, University of Houston, Georgia Institute of Technology, Texas A&M, University of Saskatchewan, University of Regina, Western University, University of Trinidad & Tobago\n\nPDPU campus has sporting facilities and boys and girls teams in Basketball, Volleyball, Cricket, Tennis, Football, Kabaddi, Table Tennis, Badminton, Athletics, Taekwondo, Ultimate frisbee, Kho-Kho, Chess and Carrom.\n\nPetrocup is the annual sporting event, an inter-university contest with almost 5000 participants competing for the top position from 100+ campuses in India and abroad.\n\nKaushik Jain\n\n14. http://spt.pdpu.ac.in/student-chapters.html\n\n"}
{"id": "20011641", "url": "https://en.wikipedia.org/wiki?curid=20011641", "title": "Plambeck Bulgarian Wind Farm", "text": "Plambeck Bulgarian Wind Farm\n\nThe German wind farm developer Plambeck Neue Energien founded a Bulgarian Joint Venture in 2008. This Joint Venture actually develops several wind farm projects. The medium term target is to find wind farm sites with a capacity for wind turbines with a nominal output of together up to 250 MW with a capital investment required of up to US$450 million.\n\n"}
{"id": "57305922", "url": "https://en.wikipedia.org/wiki?curid=57305922", "title": "Plastic China", "text": "Plastic China\n\nPlastic China () is a 2016 Chinese documentary film depicting the lives of two families who make their living recycling plastic waste imported from developed countries. The film premiered at the International Documentary Film Festival Amsterdam in November of 2016, and was shown at the 2017 Sundance Film Festival.\n"}
{"id": "23959939", "url": "https://en.wikipedia.org/wiki?curid=23959939", "title": "Plastilina", "text": "Plastilina\n\nRoma Plastilina is a brand of non-hardening modeling clay sold by Spanish company JOVI and its subsidiaries. JOVI Modeling Clay, Plastilina, is mainly composed of vegetable matter, making it lighter and giving 33% more volume. It is sold on the internet and in many arts and craft stores..\n\n\n"}
{"id": "23840623", "url": "https://en.wikipedia.org/wiki?curid=23840623", "title": "Plug-in electric vehicle", "text": "Plug-in electric vehicle\n\nA plug-in electric vehicle (PEV) is any motor vehicle that can be recharged from an external source of electricity, such as wall sockets, and the electricity stored in the rechargeable battery packs drives or contributes to drive the wheels. PEV is a subset of electric vehicles that includes all-electric or battery electric vehicles (BEVs), plug-in hybrid vehicles (PHEVs), and electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles. In China, plug-in electric vehicles are called new energy vehicles (NEVs).\n\nPlug-in cars have several benefits compared to conventional internal combustion engine vehicles. They have lower operating and maintenance costs, and produce little or no local air pollution. They reduce dependence on petroleum and may severally reduce greenhouse gas emissions, depending on the electricity source, as motors are typically much more efficient than their engine equivalents. Plug-in hybrids capture most of these benefits when they are operating in all-electric mode. Sales of first mass-production plug-in cars by major carmakers began in late December 2010, with the introduction of the all-electric Nissan Leaf and the plug-in hybrid Chevrolet Volt.\n\nCumulative global sales of highway legal plug-in electric passenger cars and light utility vehicles achieved the 1 million unit milestone in September 2015, 2 million in December 2016, the 3 million mark in November 2017, and 4 million in September 2018. Despite the rapid growth experienced, the stock of plug-in electric cars represented just about 1 out of every 300 vehicles on the world's roads by September 2018. The global plug-in stock is dominated by all-electrics (BEVs), with about 2.6 million (65%) units by November 2018. , the Nissan Leaf is the world's top selling highway-capable all-electric car in history, with global sales of 370,000 units. The Tesla Model S ranks second with 250,000 units sold worldwide .\n\n, China has the world's largest stock of highway legal light-duty plug-in electric vehicles with cumulative sales of almost 2 million plug-in electric passenger cars. Among country markets, the United States ranks second with 1 million plug-in electric cars sold through September 2018. More than 1 million light-duty plug-in electric passenger cars have been registered in Europe by June 2018, with sales led by Norway with almost 275,000 light-duty plug-in electric vehicles registered . In October 2018, Norway became the first country where 1 in every 10 passenger cars registered is a plug-in electric vehicle. China is the world's leader in the plug-in heavy-duty segment. Accounting for electric all-electric buses, and plug-in commercial and sanitation trucks, the Chinese stock of new energy vehicles rises to 2.21 million units by the end of September 2018. , China was the world's largest electric bus market with a stock of almost 385,000 vehicles, more than 99% of the global stock.\nA plug-in electric vehicle (PEV) is any motor vehicle with rechargeable battery packs that can be charged from the electric grid, and the electricity stored on board drives or contributes to drive the wheels for propulsion. Plug-in electric vehicles are also sometimes referred to as grid-enabled vehicles (GEV) and also as electrically chargeable vehicles.\n\nPEV is a subcategory of electric vehicles that includes battery electric vehicles (BEVs), plug-in hybrid vehicles, (PHEVs), and electric vehicle conversions of hybrid electric vehicles and conventional internal combustion engine vehicles. Even though conventional hybrid electric vehicles (HEVs) have a battery that is continually recharged with power from the internal combustion engine and regenerative braking, they can not be recharged from an off-vehicle electric energy source, and therefore, they do not belong to the category of plug-in electric vehicles.\n\n\"Plug-in electric drive vehicle\" is the legal term used in U.S. federal legislation to designate the category of motor vehicles eligible for federal tax credits depending on battery size and their all-electric range. In some European countries, particularly in France, \"electrically chargeable vehicle\" is the formal term used to designate the vehicles eligible for these incentives.\nWhile the term \"plug-in electric vehicle\" most often refers to automobiles or \"plug-in cars\", there are several other types of plug-in electric vehicle, including electric motorcycles and scooters, neighborhood electric vehicles or microcars, city cars, vans, buses, electric trucks or lorries, and military vehicles.\n\nA battery electric vehicle (BEV) uses chemical energy stored in rechargeable battery packs as its only source for propulsion. BEVs use electric motors and motor controllers instead of internal combustion engines (ICEs) for propulsion.\n\nA plug-in hybrid operates as an all-electric vehicle or BEV when operating in charge-depleting mode, but it switches to charge-sustaining mode after the battery has reached its minimum state of charge (SOC) threshold, exhausting the vehicle's all-electric range (AER).\n\nA plug-in hybrid electric vehicle (PHEV or PHV), also known as a plug-in hybrid, is a hybrid electric vehicle with rechargeable batteries that can be restored to full charge by connecting a plug to an external electric power source. A plug-in hybrid shares the characteristics of both a conventional hybrid electric vehicle and an all-electric vehicle: it uses a gasoline engine and an electric motor for propulsion, but a PHEV has a larger battery pack that can be recharged, allowing operation in all-electric mode until the battery is depleted.\n\nAn aftermarket electric vehicle conversion is the modification of a conventional internal combustion engine vehicle (ICEV) or hybrid electric vehicle (HEV) to electric propulsion, creating an all-electric or plug-in hybrid electric vehicle.\n\nThere are several companies in the U.S. offering conversions. The most common conversions have been from hybrid electric cars to plug-in hybrid, but due to the different technology used in hybrids by each carmaker, the easiest conversions are for 2004–2009 Toyota Prius and for the Ford Escape/Mercury Mariner Hybrid.\n\nIn China the term new energy vehicles (NEVs) refers to vehicles that are partially or fully powered by electricity, such as battery electric vehicles (BEVs) and plug-in hybrids (PHEVs). The Chinese government began implementation of its NEV program in 2009 to foster the development and introduction of new energy vehicles.\n\nPEVs have several advantages. These include reduced greenhouse gas emissions, improved air quality, noise reduction and national security benefits. According to the Center for American Progress, PEVs are an important part of the group of technologies that will help the U.S. meet its goal under the Paris Agreement, which is a 26-28 percent reduction in greenhouse gas emissions by the year 2025.\n\nInternal combustion engines are relatively inefficient at converting on-board fuel energy to propulsion as most of the energy is wasted as heat, and the rest while the engine is idling. Electric motors, on the other hand, are more efficient at converting stored energy into driving a vehicle. Electric drive vehicles do not consume energy while at rest or coasting, and modern plug-in cars can capture and reuse as much as one fifth of the energy normally lost during braking through regenerative braking. Typically, conventional gasoline engines effectively use only 15% of the fuel energy content to move the vehicle or to power accessories, and diesel engines can reach on-board efficiencies of 20%, while electric drive vehicles typically have on-board efficiencies of around 80%.\n\nIn the United States, as of early 2010 with a national average electricity rate of per kWh, the cost per mile for a plug-in electric vehicle operating in all-electric mode is estimated between $0.02 to $0.04, while the cost per mile of a standard automobile varies between $0.08 to $0.20, considering a gasoline price of $3.00 per gallon. As petroleum price is expected to increase in the future due to oil production decline and increases in global demand, the cost difference in favor of PEVs is expected to become even more advantageous.\n\nAccording to Consumer Reports, as of December 2011 the Nissan Leaf has a cost of 3.5 cents per mile and the Chevrolet Volt has a cost in electric mode of 3.8 cents per mile. The Volt cost per mile is higher because it is heavier than the Leaf. These estimates are based on the fuel economy and energy consumption measured on their tests and using a U.S. national average rate of 11 cents per kWh of electricity. When the Volt runs in range-extended mode using its premium gasoline-powered engine, the plug-in hybrid has a cost of 12.5 cents per mile. The out-of-pocket cost per mile of the three most fuel efficient gasoline-powered cars as tested by the magazine are the Toyota Prius, with a cost of 8.6 cents per miles, the Honda Civic Hybrid with 9.5 cents per mile, the Toyota Corolla with 11.9 cents per mile, and the Hyundai Elantra 13.1 cents per mile. The analysis also found that on trips up to , the Volt is cheaper to drive than the Prius and the other three cars due to the Volt's driving range on electricity. The previous operating costs do not include maintenance, depreciation or other costs.\n\nAll-electric and plug-in hybrid vehicles also have lower maintenance costs as compared to internal combustion vehicles, since electronic systems break down much less often than the mechanical systems in conventional vehicles, and the fewer mechanical systems on board last longer due to the better use of the electric engine. PEVs do not require oil changes and other routine maintenance checks.\n\nThe Edison Electric Institute (EEI) conducted an analysis that demonstrated that between January 1976 and February 2012 the real price for gasoline has been much more volatile than the real price of electricity in the United States. The analysis is based on a plug-in electric vehicle with an efficiency of 3.4 miles per kW-hr (like the Mitsubishi i MiEV) and a gasoline-powered vehicle with a fuel economy rated at (like the 2012 Fiat 500). The EEI estimated that operating a plug-in would have had an equivalent cost of around a gallon in the late 1970s and early 1980s, and around a gallon since the late 1990s. In contrast, the price to operate an internal combustion engine vehicle has had much ample variations, costing more than per gallon during the 1979 energy crisis, then had a couple of lows with prices at less than during 1999 and 2001, only to climb and reach a maximum of more than before the beginning of the 2007–2009 financial crisis, by early 2012 has fluctuated around . The analysis found that the cost of an equivalent electric-gallon of gasoline would have been not only cheaper to operate during the entire analysis period but also that equivalent electricity prices are more stable and have been declining in terms of equivalent dollars per gallon.\n\nElectric cars, as well as plug-in hybrids operating in all-electric mode, emit no harmful tailpipe pollutants from the onboard source of power, such as particulates (soot), volatile organic compounds, hydrocarbons, carbon monoxide, ozone, lead, and various oxides of nitrogen. The clean air benefit is usually local because, depending on the source of the electricity used to recharge the batteries, air pollutant emissions are shifted to the location of the generation plants. In a similar manner, plug-in electric vehicles operating in all-electric mode do not emit greenhouse gases from the onboard source of power, but from the point of view of a well-to-wheel assessment, the extent of the benefit also depends on the fuel and technology used for electricity generation. This fact has been referred to as the long tailpipe of plug-in electric vehicles. From the perspective of a full life cycle analysis, the electricity used to recharge the batteries must be generated from renewable or clean sources such as wind, solar, hydroelectric, or nuclear power for PEVs to have almost none or zero well-to-wheel emissions. On the other hand, when PEVs are recharged from coal-fired plants, they usually produce slightly more greenhouse gas emissions than internal combustion engine vehicles and higher than hybrid electric vehicles. In the case of plug-in hybrid electric vehicles operating in hybrid mode with assistance of the internal combustion engine, tailpipe and greenhouse emissions are lower in comparison to conventional cars because of their higher fuel economy.\n\nThe magnitude of the potential advantage depends on the mix of generation sources and therefore varies by country and by region. For example, France can obtain significant emission benefits from electric and plug-in hybrids because most of its electricity is generated by nuclear power plants; similarly, most regions of Canada are primarily powered with hydroelectricity, nuclear, or natural gas which have no or very low emissions at point of generation; and the state of California, where most energy comes from natural gas, hydroelectric and nuclear plants can also secure substantial emission benefits. The United Kingdom also has a significant potential to benefit from PEVs as natural gas plants dominate the generation mix. On the other hand, emission benefits in Germany, China, India, and the central regions of the United States are limited or non-existent because most electricity is generated from coal. However these countries and regions might still obtain some air quality benefits by reducing local air pollution in urban areas. Cities with chronic air pollution problems, such as Los Angeles, México City, Santiago, Chile, São Paulo, Beijing, Bangkok and Kathmandu may also gain local clean air benefits by shifting the harmful emission to electric generation plants located outside the cities. Nevertheless, the location of the plants is not relevant when considering greenhouse gas emission because their effect is global.\n\nA report published in June 2011, prepared by Ricardo in collaboration with experts from the UK's Low Carbon Vehicle Partnership, found that hybrid electric cars, plug-in hybrids and all-electric cars generate more carbon emissions during their production than current conventional vehicles, but still have a lower overall carbon footprint over the full life cycle. The higher carbon footprint during production of electric drive vehicles is due mainly to the production of batteries. As an example, 43 percent of production emissions for a mid-size electric car are generated from the battery production, while for standard mid-sized gasolineinternal combustion engine vehicle, around 75% of the embedded carbon emissions during production comes from the steel used in the vehicle glider. The following table summarizes key results of this study for four powertrain technologies:\nThe Ricardo study also found that the lifecycle carbon emissions for mid-sized gasoline and diesel vehicles are almost identical, and that the greater fuel efficiency of the diesel engine is offset by higher production emissions.\n\nIn 2014 Volkswagen published the results of life-cycle assessment of its electric vehicles certified by TÜV NORD, and independent inspection agency. The study found that emissions during the use phase of its all-electric VW e-Golf are 99% lower than those of the Golf 1.2 TSI when powers comes from exclusively hydroelectricity generated in Germany, Austria and Switzerland. Accounting for the full lifecycle, the e-Golf reduces emissions by 61%, offsetting higher production emissions. When the actual EU-27 electricity mix is considered, the e-Golf emissions are still 26% lower than those of the conventional Golf 1.2 TSI. Similar results were found when comparing the e-Golf with the Golf 1.6 TDI. The analysis considered recycling of the three vehicles at the end of their lifetime.\n\nUniti\n\nThe Swedish automotive startup Uniti Sweden AB has been working on an electric city car entirely designed to be sustainable. The CEO Lewis Horne said “\"When people sell electric cars, the motivation is sustainability. So, why are they so heavy, and why are they the same cars? Where you just take out petrol and you put in the huge battery and you call it sustainable. But it’s not.\"”. The company has been designing a car that uses sustainable and recycled material such as bio-composite to reduce the footprint of the car during production. In addition, the fully automated factory provided in partnership with Siemens will be able to work with the lights turned off during 22h per day to save energy.\n\nThe following table compares tailpipe and upstream emissions estimated by the U.S. Environmental Protection Agency for all series production model year 2014 plug-in electric vehicles available in the U.S. market. Total emissions include the emissions associated with the production and distribution of electricity used to charge the vehicle, and for plug-in hybrid electric vehicles, it also includes emissions associated with tailpipe emissions produced from the internal combustion engine. These figures were published by the EPA in October 2014 in its annual report \"\"Light-Duty Automotive Technology, Carbon Dioxide Emissions, and Fuel Economy Trends: 1975 Through 2014\".\" All emissions are estimated considering average real world city and highway operation based on the EPA 5-cycle label methodology, using a weighted 55% city and 45% highway driving. For the first time, the \"2014 Trends \"report presents an analysis of the impact of alternative fuel vehicles, with emphasis in plug-in electric vehicles because as their market share is approaching 1%, the EPA concluded that PEVs began to have a measurable impact on the U.S. overall new vehicle fuel economy and emissions.\n\nFor purposes of an accurate estimation of emissions, the analysis took into consideration the differences in operation between plug-in hybrids. Some, like the Chevrolet Volt, can operate in all-electric mode without using gasoline, and others operate in a blended mode like the Toyota Prius PHV, which uses both energy stored in the battery and energy from the gasoline tank to propel the vehicle, but that can deliver substantial all-electric driving in blended mode. In addition, since the all-electric range of plug-in hybrids depends on the size of the battery pack, the analysis introduced a utility factor as a projection of the share of miles that will be driven using electricity by an average driver, for both, electric only and blended EV modes. Since all-electric cars do not produce tailpipe emissions, the utility factor applies only to plug-in hybrids. The following table shows the overall fuel economy expressed in terms of miles per gallon gasoline equivalent (mpg-e) and the utility factor for the ten MY2014 plug-in hybrids available in the U.S. market, and EPA's best estimate of the tailpipe emissions produced by these PHEVs.\n\nIn order to account for the upstream emissions associated with the production and distribution of electricity, and since electricity production in the United States varies significantly from region to region, the EPA considered three scenarios/ranges with the low end scenario corresponding to the California powerplant emissions factor, the middle of the range represented by the national average powerplant emissions factor, and the upper end of the range corresponding to the powerplant emissions factor for the Rocky Mountains. The EPA estimates that the electricity GHG emission factors for various regions of the country vary from 346 g /kWh in California to 986 g /kWh in the Rockies, with a national average of 648 g /kWh.\n\nThe Union of Concerned Scientists (UCS) published a study in 2012 that assessed average greenhouse gas emissions in the U.S. resulting from charging plug-in car batteries from the perspective of the full life-cycle (well-to-wheel analysis) and according to fuel and technology used to generate electric power by region. The study used the model year 2011 Nissan Leaf all-electric car to establish the analysis baseline, and electric-utility emissions are based on EPA's 2009 estimates. The UCS study expressed the results in terms of miles per gallon instead of the conventional unit of grams of greenhouse gases or carbon dioxide equivalent emissions per year in order to make the results more friendly for consumers. The study found that in areas where electricity is generated from natural gas, nuclear, hydroelectric or renewable sources, the potential of plug-in electric cars to reduce greenhouse emissions is significant. On the other hand, in regions where a high proportion of power is generated from coal, hybrid electric cars produce less equivalent emissions than plug-in electric cars, and the best fuel efficient gasoline-powered subcompact car produces slightly less emissions than a PEV. In the worst-case scenario, the study estimated that for a region where all energy is generated from coal, a plug-in electric car would emit greenhouse gas emissions equivalent to a gasoline car rated at a combined city/highway driving fuel economy of . In contrast, in a region that is completely reliant on natural gas, the PEV would be equivalent to a gasoline-powered car rated at .\n\nThe study concluded that for 45% of the U.S. population, a plug-in electric car will generate lower equivalent emissions than a gasoline-powered car capable of combined , such as the Toyota Prius and the Prius c. The UCS also found that for 37% of the population, the electric car emissions will fall in the range of a gasoline-powered car rated at a combined fuel economy of , such as the Honda Civic Hybrid and the Lexus CT200h. Only 18% of the population lives in areas where the power-supply is more dependent on burning carbon, and the greenhouse gas emissions will be equivalent to a car rated at a combined fuel economy of , such as the Chevrolet Cruze and Ford Focus. The study found that there are no regions in the U.S. where plug-in electric cars will have higher greenhouse gas emissions than the average new compact gasoline engine automobile, and the area with the dirtiest power supply produces emissions equivalent to a gasoline-powered car rated at .\n\nIn September 2014 the UCS published an updated analysis of its 2012 report. The 2014 analysis found that 60% of Americans, up from 45% in 2009, live in regions where an all-electric car produce fewer equivalent emissions per mile than the most efficient hybrid. The UCS study found several reasons for the improvement. First, electric utilities have adopted cleaner sources of electricity to their mix between the two analysis. The 2014 study used electric-utility emissions based on EPA's 2010 estimates, but since coal use nationwide is down by about 5% from 2010 to 2014, actual efficiency in 2014 is better than estimated in the UCS study. Second, electric vehicles have become more efficient, as the average 2013 all-electric vehicle used 0.33 kWh per mile, representing a 5% improvement over 2011 models. Also, some new models are cleaner than the average, such as the BMW i3, which is rated at 0.27 kWh by the EPA. An i3 charged with power from the Midwest grid would be as clean as a gasoline-powered car with about , up from for the average electric car in the 2012 study. In states with a cleaner mix generation, the gains were larger. The average all-electric car in California went up to equivalent from in the 2012 study. States with dirtier generation that rely heavily on coal still lag, such as Colorado, where the average BEV only achieves the same emissions as a gasoline-powered car. The author of the 2014 analysis noted that the benefits are not distributed evenly across the U.S. because electric car adoptions is concentrated in the states with cleaner power.\n\nIn November 2015 the Union of Concerned Scientists published a new report comparing two battery electric vehicles (BEVs) with similar gasoline vehicles by examining their global warming emissions over their full life-cycle, cradle-to-grave analysis. The two BEVs modeled, midsize and full-size, are based on the two most popular BEV models sold in the United States in 2015, the Nissan Leaf and the Tesla Model S. The study found that all-electric cars representative of those sold today, on average produce less than half the global warming emissions of comparable gasoline-powered vehicles, despite taken into account the higher emissions associated with BEV manufacturing. Considering the regions where the two most popular electric cars are being sold, excess manufacturing emissions are offset within 6 to 16 months of average driving. The study also concluded that driving an average EV results in lower global warming emissions than driving a gasoline car that gets in regions covering two-thirds of the U.S. population, up from 45% in 2009. Based on where EVs are being sold in the United States in 2015, the average EV produces global warming emissions equal to a gasoline vehicle with a fuel economy rating. The authors identified two main reason for the fact that EV-related emissions have become even lower in many parts of the country since the first study was conducted in 2012. Electricity generation has been getting cleaner, as coal-fired generation has declined while lower-carbon alternatives have increased. In addition, electric cars are becoming more efficient. For example, the Nissan Leaf and the Chevrolet Volt, have undergone improvements to increase their efficiencies compared to the original models launched in 2010, and other even more efficient BEV models, such as the most lightweight and efficient BMW i3, have entered the market.\n\nOne criticism to the UCS study is that the analysis was made using average emissions rates across regions instead of marginal generation at different times of the day. The former approach does not take into account the generation mix within interconnected electricity markets and shifting load profiles throughout the day. An analysis by three economist affiliated with the National Bureau of Economic Research (NBER), published in November 2014, developed a methodology to estimate marginal emissions of electricity demand that vary by location and time of day across the United States. The marginal analysis, applied to plug-in electric vehicles, found that the emissions of charging PEVs vary by region and hours of the day. In some regions, such as the Western U.S. and Texas, emissions per mile from driving PEVs are less than those from driving a hybrid car. However, in other regions, such as the Upper Midwest, charging during the recommended hours of midnight to 4 a.m. implies that PEVs generate more emissions per mile than the average car currently on the road. The results show a fundamental tension between electricity load management and environmental goals as the hours when electricity is the least expensive to produce tend to be the hours with the greatest emissions. This occurs because coal-fired units, which have higher emission rates, are most commonly used to meet base-level and off-peak electricity demand; while natural gas units, which have relatively low emissions rates, are often brought online to meet peak demand.\n\nA study published in the UK in April 2013 assessed the carbon footprint of plug-in electric vehicles in 20 countries. As a baseline the analysis established that manufacturing emissions account for 70 g CO/km for an electric car and 40 g CO/km for a petrol car. The study found that in countries with coal-intensive generation, PEVs are no different from conventional petrol-powered vehicles. Among these countries are China, Indonesia, Australia, South Africa and India. A pure electric car in India generates emissions comparable to a petrol car.\n\nThe country ranking was led by Paraguay, where all electricity is produced from hydropower, and Iceland, where electricity production relies on renewable power, mainly hydro and geothermal power. Resulting carbon emissions from an electric car in both countries are 70 g CO/km, which is equivalent to a petrol car, and correspond to manufacturing emissions. Next in the ranking are other countries with low carbon electricity generation, including Sweden (mostly hydro and nuclear power ), Brazil (mainly hydropower) and France (predominantly nuclear power). Countries ranking in the middle include Japan, Germany, the UK and the United States.\n\nThe following table shows the emissions intensity estimated in the study for those countries where electric vehicle are available, and the corresponding emissions equivalent in miles per US gallon of a petrol-powered car:\n\nFor many net oil importing countries the 2000s energy crisis brought back concerns first raised during the 1973 oil crisis. For the United States, the other developed countries and emerging countries their dependence on foreign oil has revived concerns about their vulnerability to price shocks and supply disruption. Also, there have been concerns about the uncertainty surrounding peak oil production and the higher cost of extracting unconventional oil. A third issue that has been raised is the threat to national security because most proven oil reserves are concentrated in relatively few geographic locations, including some countries with strong resource nationalism, unstable governments or hostile to U.S. interests. In addition, for many developing countries, and particularly for the poorest African countries, high oil prices have an adverse impact on the government budget and deteriorate their terms of trade thus jeopardizing their balance of payments, all leading to lower economic growth.\n\nThrough the gradual replacement of internal combustion engine vehicles for electric cars and plug-in hybrids, electric drive vehicles can contribute significantly to lessen the dependence of the transport sector on imported oil as well as contributing to the development of a more resilient energy supply.\n\nPlug-in electric vehicles offer users the opportunity to sell electricity stored in their batteries back to the power grid, thereby helping utilities to operate more efficiently in the management of their demand peaks. A vehicle-to-grid (V2G) system would take advantage of the fact that most vehicles are parked an average of 95 percent of the time. During such idle times the electricity stored in the batteries could be transferred from the PEV to the power lines and back to the grid. In the U.S. this transfer back to the grid have an estimated value to the utilities of up to $4,000 per year per car. In a V2G system it would also be expected that battery electric (BEVs) and plug-in hybrids (PHEVs) would have the capability to communicate automatically with the power grid to sell demand response services by either delivering electricity into the grid or by throttling their charging rate.\n\n, plug-in electric vehicles are significantly more expensive as compared to conventional internal combustion engine vehicles and hybrid electric vehicles due to the additional cost of their lithium-ion battery pack. According to a 2010 study by the National Research Council, the cost of a lithium-ion battery pack was about /kWh of usable energy, and considering that a PHEV-10 requires about 2.0 kWh and a PHEV-40 about 8 kWh, the manufacturer cost of the battery pack for a PHEV-10 is around and it goes up to for a PHEV-40. , and based on the three battery size options offered for the Tesla Model S, the New York Times estimated the cost of automotive battery packs between to per kilowatt-hour. A 2013 study by the American Council for an Energy-Efficient Economy reported that battery costs came down from per kWh in 2007 to per kWh in 2012. The U.S. Department of Energy has set cost targets for its sponsored battery research of per kWh in 2015 and per kWh by 2022. Cost reductions through advances in battery technology and higher production volumes will allow plug-in electric vehicles to be more competitive with conventional internal combustion engine vehicles.\n\nAccording to a study published in February 2016 by Bloomberg New Energy Finance (BNEF), battery prices fell 65% since 2010, and 35% just in 2015, reaching per kWh. The study concludes that battery costs are on a trajectory to make electric vehicles without government subsidies as affordable as internal combustion engine cars in most countries by 2022. BNEF projects that by 2040, long-range electric cars will cost less than expressed in 2016 dollars. BNEF expects electric car battery costs to be well below per kWh by 2030, and to fall further thereafter as new chemistries become available.\n\nA study published in 2011 by the Belfer Center, Harvard University, found that the gasoline costs savings of plug-in electric cars do not offset their higher purchase prices when comparing their lifetime net present value of purchase and operating costs for the U.S. market at 2010 prices, and assuming no government subidies. According to the study estimates, a PHEV-40 is more expensive than a conventional internal combustion engine, while a battery electric vehicles is more expensive. These findings assumed a battery cost of per kWh, which means that the Chevrolet Volt battery pack cost around and the Nissan Leaf pack costs . The study also assumed a gasoline price of per gallon (as of mid June 2011), that vehicles are driven per year, an average price of electricity of per kWh, that the plug-in hybrid is driven in all-electric mode 85% of the time, and that the owner of PEVs pay to install a Level II 220/240 volt charger at home.\n\nThe study also include hybrid electric vehicles in the comparison, and analyzed several scenarios to determine how the comparative net savings will change over the next 10 to 20 years, assuming that battery costs will decrease while gasoline prices increase, and also assuming higher fuel efficiency of conventional cars, among other scenarios. Under the future scenarios considered, the study found that BEVs will be significantly less expensive than conventional cars ( to cheaper), while PHEVs, will be more expensive than BEVs in almost all comparison scenarios, and only less expensive than conventional cars in a scenario with very low battery costs and high gasoline prices. The reason for the different savings among PEVs is because BEVs are simpler to build and do not use liquid fuel, while PHEVs have more complicated powertrains and still have gasoline-powered engines. The following table summarizes the results of four of the seven scenarios analyzed by the study.\n\nAccording to a study by the Electric Power Research Institute published in June 2013, the total cost of ownership of the 2013 Nissan Leaf SV is substantially lower than that of comparable conventional and hybrid vehicles. For comparison, the study constructed average hybrid and conventional vehicles and assumed an average US distance per trip distribution. The study took into account the manufacturer's suggested retail price, taxes, credits, destination charge, electric charging station, fuel cost, maintenance cost, and additional cost due to the use of a gasoline vehicle for trips beyond the range of the Leaf.\n\nDespite the widespread assumption that plug-in recharging will take place overnight at home, residents of cities, apartments, dormitories, and townhouses do not have garages or driveways with available power outlets, and they might be less likely to buy plug-in electric vehicles unless recharging infrastructure is developed. Electrical outlets or charging stations near their places of residence, in commercial or public parking lots, streets and workplaces are required for these potential users to gain the full advantage of PHEVs, and in the case of EVs, to avoid the fear of the batteries running out energy before reaching their destination, commonly called range anxiety. Even house dwellers might need to charge at the office or to take advantage of opportunity charging at shopping centers. However, this infrastructure is not in place and it will require investments by both the private and public sectors.\n\nSeveral cities in California and Oregon, and particularly San Francisco and other cities in the San Francisco Bay Area and Silicon Valley, already have deployed public charging stations and have expansion plans to attend both plug-ins and all-electric cars. Some local private firms such as Google and Adobe Systems have also deployed charging infrastructure. In Google's case, its Mountain View campus has 100 available charging stations for its share-use fleet of converted plug-ins available to its employees. Solar panels are used to generate the electricity, and this pilot program is being monitored on a daily basis and performance results are published on the RechargeIT website. , Estonia is the first and only country that had deployed an EV charging network with nationwide coverage, with 165 fast chargers available along highways at a minimum distance of between , and a higher density in urban areas.\n\nThe importance to build the infrastructure necessary to support electric vehicles is illustrated by the decision of Car2Go in San Diego, California, that due to insufficient charging infrastructure decided to replace all of its all-electric car fleet with gasoline-powered cars starting on 1 May 2016. When the carsharing service started in 2011, Car2Go expected 1,000 charging stations to be deployed around the city, but only 400 were in place by early 2016. As a result, an average of 20% of the carsharing fleet is unavailable at any given time because the cars are either being charged or because they don’t have enough electricity in them to be driven. Also, many of the company’s 40,000 San Diego members say they often worry their Car2Go will run out of charge before they finish their trip.\n\nA different approach to resolve the problems of range anxiety and lack of recharging infrastructure for electric vehicles was developed by Better Place. Its business model considers that electric cars are built and sold separately from the battery pack. As customers are not allowed to purchase battery packs, they must lease them from Better Place which will deploy a network of battery swapping stations thus expanding EVs range and allowing long distance trips. Subscribed users pay a per-distance fee to cover battery pack leasing, charging and swap infrastructure, the cost of sustainable electricity, and other costs. Better Place signed agreement for deployment in Australia, Denmark, Israel, Canada, California, and Hawaii. The Renault Fluence Z.E. was the electric car built with switchable battery technology sold for the Better Place network. The robotic battery-switching operation was completed in about five minutes.\n\nAfter implementing the first modern commercial deployment of the battery swapping model in Israel and Denmark, Better Place filed for bankruptcy in Israel in May 2013. The company's financial difficulties were caused by the high investment required to develop the charging and swapping infrastructure, about million in private capital, and a market penetration significantly lower than originally predicted by Shai Agassi. Less than 1,000 Fluence Z.E. cars were deployed in Israel and around 400 units in Denmark.\n\nTesla Motors designed its Model S to allow fast battery swapping. In June 2013, Tesla announced their goal to deploy a battery swapping station in each of its supercharging stations. At a demonstration event Tesla showed that a battery swap operation with the Model S takes just over 90 seconds, about half the time it takes to refill a gasoline-powered car used for comparison purposes during the event. The first stations are planned to be deployed along Interstate 5 in California where, according to Tesla, a large number of Model S sedans make the San Francisco-Los Angeles trip regularly. These will be followed by the Washington, DC to Boston corridor.\n\nThe REVA NXR exhibited in the 2009 Frankfurt Motor Show and the Nissan Leaf SV trim both have roof-mounted solar panels. These solar panels are designed to trickle charge the batteries when the car is moving or parked. Another proposed technology is REVive, by Reva. When the REVA NXR's batteries are running low or are fully depleted, the driver is able to send an SMS to REVive and unlock a hidden reserve in the battery pack. Reva has not provided details on how the system will work. The Fisker Karma uses solar panel in the roof to recharge the 12-volt lead-acid accessory battery. The Nissan Leaf SL trim also has a small solar panel at the rear of the roof/spoiler that can trickle charge the auxiliary 12-volt lead-acid battery.\n\nThe existing electrical grid, and local transformers in particular, may not have enough capacity to handle the additional power load that might be required in certain areas with high plug-in electric car concentrations. As recharging a single electric-drive car could consume three times as much electricity as a typical home, overloading problems may arise when several vehicles in the same neighborhood recharge at the same time, or during the normal summer peak loads. To avoid such problems, utility executives recommend owners to charge their vehicles overnight when the grid load is lower or to use smarter electric meters that help control demand. When market penetration of plug-in electric vehicles begins to reach significant levels, utilities will have to invest in improvements for local electrical grids in order to handle the additional loads related to recharging to avoid blackouts due to grid overload. Also, some experts have suggested that by implementing variable time-of-day rates, utilities can provide an incentive for plug-in owners to recharge mostly overnight, when rates are lower. \n\nGeneral Motors is sponsoring the Pecan Street demonstration project in Austin, Texas. The project objective is to learn the charging patterns of plug-in electric car owners, and to study how a residential fleet of electric vehicles might strain the electric grid if all owners try to charge them at the same, which is what the preliminary monitoring found when the plug-in cars return home in the evening. The Mueller neighborhood is the test ground, and , the community has nearly 60 Chevrolet Volt owners alone. This cluster of Volts was achieved thanks to GM's commitment to match the federal government's $7,500 rebate incentive, which effectively halves the purchase price of the plug-hybrid electric cars.\n\nElectric cars and plug-in hybrids when operating in all-electric mode at low speeds produce less roadway noise as compared to vehicles propelled by an internal combustion engine, thereby reducing harmful noise health effects. However, blind people or the visually impaired consider the noise of combustion engines a helpful aid while crossing streets, hence plug-in electric cars and conventional hybrids could pose an unexpected hazard when operating at low speeds. Several tests conducted in the U.S. have shown that this is a valid concern, as vehicles operating in electric mode can be particularly hard to hear below for all types of road users and not only the visually impaired. At higher speeds the sound created by tire friction and the air displaced by the vehicle start to make sufficient audible noise.\n\nSome carmakers announced they have decided to address this safety issue, and as a result, the Nissan Leaf electric car and Chevrolet Volt plug-in hybrid, both launched in December 2010, as well as the Fisker Karma plug-in hybrid launched in 2011 launched in 2012, include electric warning sounds to alert pedestrians, the blind and others to their presence. , most of the hybrids and plug-in electric and hybrids available in the United States, Japan and Europe make warning noises using a speaker system.\n\nThe Japanese Ministry of Land, Infrastructure, Transport and Tourism issued guidelines for hybrid and other near-silent vehicles in January 2010. In the United States the Pedestrian Safety Enhancement Act of 2010 was approved by the U.S. Senate and the House of Representatives in December 2010. After several delays, the National Highway Traffic Safety Administration (NHTSA) issued its ruling in February 2018. It requires hybrids and electric vehicles traveling at less than to emit warning sounds that pedestrians must be able to hear over background noises. The regulation requires full compliance in September 2020, but 50% of \"quiet\" vehicles must have the warning sounds by September 2019.\n\nIn April 2014 the European Parliament approved legislation that requires the mandatory use of the AVAS for all new electric and hybrid electric vehicles and car manufacturers have to comply within 5 years.\nLithium-ion batteries may suffer thermal runaway and cell rupture if overheated or overcharged, and in extreme cases this can lead to combustion. To reduce these risks, lithium-ion battery packs contain fail-safe circuitry that shuts down the battery when its voltage is outside the safe range. When handled improperly, or if manufactured defectively, some rechargeable batteries can experience thermal runaway resulting in overheating. Especially prone to thermal runaway are lithium-ion batteries. Reports of exploding cellphones have been reported in newspapers. In 2006, batteries from Apple, HP, Toshiba, Lenovo, Dell and other notebook manufacturers were recalled because of fire and explosions. Also, during the Boeing 787 Dreamliner's first year of service, at least four aircraft suffered from electrical system problems stemming from its lithium-ion batteries, resulting in the whole Dreamliner fleet being voluntarily grounded in January 2013.\n\nSeveral plug-in electric vehicle fire incidents have taken place since the introduction of mass-production plug-in electric vehicles in 2008. Most of them have been thermal runaway incidents related to the lithium-ion batteries and have involved the Zotye M300 EV, Chevrolet Volt, Fisker Karma, BYD e6, Dodge Ram 1500 Plug-in Hybrid, Toyota Prius Plug-in Hybrid, Mitsubishi i-MiEV and Outlander P-HEV. , four fires after a crash have been reported associated with the batteries of all-electric cars involving a BYD e6 and three Tesla Model S cars.\n\nThe first modern crash-related fire was reported in China in May 2012, after a high-speed car crashed into a BYD e6 taxi in Shenzhen. The second reported incident occurred in the United States on October 1, 2013, when a Tesla Model S caught fire after the electric car hit metal debris on a highway in Kent, Washington state, and the debris punctured one of 16 modules within the battery pack. A second reported fire occurred on October 18, 2013, in Merida, Mexico. In this case the vehicle was being driven at high speed through a roundabout and crashed through a wall and into a tree. On November 6, 2013, a Tesla Model S being driven on Interstate 24 near Murfreesboro, Tennessee caught fire after it struck a tow hitch on the roadway, causing damage beneath the vehicle.\n\nThe U.S. National Highway Traffic Safety Administration (NHTSA) is conducting a study due in 2014 to establish whether lithium-ion batteries in plug-electric vehicles pose a potential fire hazard. The research is looking at whether the high-voltage batteries can cause fires when they are being charged and when the vehicles are involved in an accident. Both General Motors and Nissan have published a guide for firefighters and first responders to properly handle a crashed plug-in electric-drive vehicle and safely disable its battery and other high voltage systems.\n\nCommon technology for plug-ins and electric cars is based on the lithium-ion battery and an electric motor which uses rare-earth elements. The demand for lithium, heavy metals, and other specific elements (such as neodymium, boron and cobalt) required for the batteries and powertrain is expected to grow significantly due to the future sales increase of plug-in electric vehicles in the mid and long term. , the Toyota Prius battery contains more than of the rare-earth element lanthanum, and its motor magnets use neodymium and dysprosium. While only of lithium carbonate equivalent (LCE) are required in a smartphone and in a tablet computer, electric vehicles and stationary energy storage systems for homes, businesses or industry use much more lithium in their batteries. a hybrid electric passenger car might use of LCE, while one of Tesla's high performance electric cars could use as much as .\n\nSome of the largest world reserves of lithium and other rare metals are located in countries with strong resource nationalism, unstable governments or hostility to U.S. interests, raising concerns about the risk of replacing dependence on foreign oil with a new dependence on hostile countries to supply strategic materials.\n\n\nThe main deposits of lithium are found in China and throughout the Andes mountain chain in South America. In 2008 Chile was the leading lithium metal producer with almost 30%, followed by China, Argentina, and Australia. In the United States lithium is recovered from brine pools in Nevada.\n\nNearly half the world's known reserves are located in Bolivia, and according to the US Geological Survey, Bolivia's Salar de Uyuni desert has 5.4 million tons of lithium. Other important reserves are located in Chile, China, and Brazil. Since 2006 the Bolivian government have nationalized oil and gas projects and is keeping a tight control over mining its lithium reserves. Already the Japanese and South Korean governments, as well as companies from these two countries and France, have offered technical assistance to develop Bolivia's lithium reserves and are seeking to gain access to the lithium resources through a mining and industrialization model suitable to Bolivian interests.\n\nAccording to a 2011 study conducted at Lawrence Berkeley National Laboratory and the University of California Berkeley, the currently estimated reserve base of lithium should not be a limiting factor for large-scale battery production for electric vehicles, as the study estimated that on the order of 1 billion 40 kWh Li-based batteries (about 10 kg of lithium per car) could be built with current reserves, as estimated by the U.S. Geological Survey. Another 2011 study by researchers from the University of Michigan and Ford Motor Company found that there are sufficient lithium resources to support global demand until 2100, including the lithium required for the potential widespread use of hybrid electric, plug-in hybrid electric and battery electric vehicles. The study estimated global lithium reserves at 39 million tons, and total demand for lithium during the 90-year period analyzed at 12–20 million tons, depending on the scenarios regarding economic growth and recycling rates.\n\nA 2016 study by Bloomberg New Energy Finance (BNEF) found that availability of lithium and other finite materials used in the battery packs will not be a limiting factor for the adoption of electric vehicles. BNEF estimated that battery packs will require less than 1% of the known reserves of lithium, nickel, manganese, and copper through 2030, and 4% of the world’s cobalt. After 2030, the study states that new battery chemistries will probably shift to other source materials, making packs lighter, smaller, and cheaper.\n\nChina has 48% of the world's reserves of rare-earth elements, the United States has 13%, and Russia, Australia, and Canada have significant deposits. Until the 1980s, the U.S. led the world in rare-earth production, but since the mid-1990s China has controlled the world market for these elements. The mines in Bayan Obo near Baotou, Inner Mongolia, are currently the largest source of rare-earth metals and are 80% of China's production. In 2010 China accounted for 97% of the global production of 17 rare-earth elements. Since 2006 the Chinese government has been imposing export quotas reducing supply at a rate of 5% to 10% a year.\n\nPrices of several rare-earth elements increased sharply by mid-2010 as China imposed a 40% export reduction, citing environmental concerns as the reason for the export restrictions. These quotas have been interpreted as an attempt to control the supply of rare earths. However, the high prices have provided an incentive to begin or reactivate several rare-earth mining projects around the world, including the United States, Australia, Vietnam, and Kazakhstan.\n\nIn September 2010, China temporarily blocked all exports of rare earths to Japan in the midst of a diplomatic dispute between the two countries. These minerals are used in hybrid cars and other products such wind turbines and guided missiles, thereby augmenting the worries about the dependence on Chinese rare-earth elements and the need for geographic diversity of supply. A December 2010 report published by the US DoE found that the American economy vulnerable to rare-earth shortages and estimates that it could take 15 years to overcome dependence on Chinese supplies. China raised export taxes for some rare earths from 15 to 25%, and also extended taxes to exports of some rare-earth alloys that were not taxed before. The Chinese government also announced further reductions on its export quotas for the first months of 2011, which represent a 35% reduction in tonnage as compared to exports during the first half of 2010.\n\nOn September 29, 2010, the U.S. House of Representatives approved the Rare Earths and Critical Materials Revitalization Act of 2010 (H.R.6160). The approved legislation is aimed at restoring the U.S. as a leading producer of rare-earth elements, and would support activities in the U.S. Department of Energy (US DoE) to discover and develop rare-earth sites inside of the U.S. in an effort to reduce the auto industry's near-complete dependence on China for the minerals. A similar bill, the Rare Earths Supply Technology and Resources Transformation Act of 2010 (S. 3521), is being discussed in the U.S. Senate.\n\nIn order to avoid its dependence on rare-earth minerals, Toyota Motor Corporation announced in January 2011 that it is developing an alternative motor for future hybrid and electric cars that does not need rare-earth materials. Toyota engineers in Japan and the U.S. are developing an induction motor that is lighter and more efficient than the magnet-type motor used in the Prius, which uses two rare earths in its motor magnets. Other popular hybrids and plug-in electric cars in the market that use these rare-earth elements are the Nissan Leaf, the Chevrolet Volt and Honda Insight. For its second generation RAV4 EV due in 2012, Toyota is using an induction motor supplied by Tesla Motors that does not require rare-earth materials. The Tesla Roadster and the Tesla Model S use a similar motor.\n\nWith the exception of Tesla Motors, almost all new cars in the United States are sold through dealerships, so they play a crucial role in the sales of electric vehicles, and negative attitudes can hinder early adoption of plug-in electric vehicles. Dealers decide which cars they want to stock, and a salesperson can have a big impact on how someone feels about a prospective purchase. Sales people have ample knowledge of internal combustion cars while they do not have time to learn about a technology that represents a fraction of overall sales. As with any new technology, and in the particular case of advanced technology vehicles, retailers are central to ensuring that buyers, especially those switching to a new technology, have the information and support they need to gain the full benefits of adopting this new technology.\n\nThere are several reasons for the reluctance of some dealers to sell plug-in electric vehicles. PEVs do not offer car dealers the same profits as gasoline-powered car. Plug-in electric vehicles take more time to sell because of the explaining required, which hurts overall sales and sales people commissions. Electric vehicles also may require less maintenance, resulting in loss of service revenue, and thus undermining the biggest source of dealer profits, their service departments. According to the National Automobile Dealers Association (NADS), dealers on average make three times as much profit from service as they do from new car sales. However, a NADS spokesman said there was not sufficient data to prove that electric cars would require less maintenance. According to the New York Times, BMW and Nissan are among the companies whose dealers tend to be more enthusiastic and informed, but only about 10% of dealers are knowledgeable on the new technology.\n\nA study conducted at the Institute of Transportation Studies (ITS), at the University of California, Davis (UC Davis) published in 2014 found that many car dealers are less than enthusiastic about plug-in vehicles. ITS conducted 43 interviews with six automakers and 20 new car dealers selling plug-in vehicles in California’s major metro markets. The study also analyzed national and state-level J.D. Power 2013 Sales Satisfaction Index (SSI) study data on customer satisfaction with new car dealerships and Tesla retail stores. The researchers found that buyers of plug-in electric vehicles were significantly less satisfied and rated the dealer purchase experience much lower than buyers of non-premium conventional cars, while Tesla Motors earned industry-high scores. According to the findings, plug-in buyers expect more from dealers than conventional buyers, including product knowledge and support that extends beyond traditional offerings.\n\nIn 2014 Consumer Reports published results from a survey conducted with 19 secret shoppers that went to 85 dealerships in four states, making anonymous visits between December 2013 and March 2014. The secret shoppers asked a number of specific questions about cars to test the salespeople’s knowledge about electric cars. The consumer magazine decided to conduct the survey after several consumers who wanted to buy a plug-in car reported to the organization that some dealerships were steering them toward gasoline-powered models. The survey found that not all sales people seemed enthusiastic about making PEV sales; a few outright discouraged it, and even one dealer was reluctant to even show a plug-in model despite having one in stock. And many sales people seemed not to have a good understanding of electric-car tax breaks and other incentives or of charging needs and costs. Consumer Reports also found that when it came to answering basic questions, sales people at Chevrolet, Ford, and Nissan dealerships tended to be better informed than those at Honda and Toyota. The survey found that most of the Toyota dealerships visited recommended against buying a Prius Plug-in and suggested buying a standard Prius hybrid instead. Overall, the secret shoppers reported that only 13 dealers “discouraged sale of EV,” with seven of them being in New York. However, at 35 of the 85 dealerships visited, the secret shoppers said sales people recommended buying a gasoline-powered car instead.\n\nThe ITS-Davis study also found that a small but influential minority of dealers have introduced new approaches to better meet the needs of plug-in customers. Examples include marketing carpool lane stickers, enrolling buyers in charging networks, and preparing incentive paperwork for customers. Some dealers assign seasoned sales people as plug-in experts, many of whom drive plug-ins themselves to learn and be familiar with the technology and relate the car’s benefits to potential buyers. The study concluded also that carmakers could do much more to support dealers selling PEVs.\n\nSeveral national and local governments around the world have established tax credits, grants and other financial and non-financial incentives for consumers to purchase a plug-in electric vehicle as a policy to promote the introduction and mass market adoption of this type of vehicles.\n\n, 17 of the 27 European Union member states provide tax incentives for electrically chargeable vehicles. The incentives consist of tax reductions and exemptions, as well as of bonus payments for buyers of plug-in and hybrid vehicles. Other countries offering subsidies and tax incentives include China, the United States, Japan, South Korea, India, some provinces in Canada, and Costa Rica.\n\nDuring the 1990s several highway-capable plug-in electric cars were produced in limited quantities, all were battery electric vehicles. PSA Peugeot Citroën launched several electric \"Électrique\" versions of its models starting in 1991, notably the Citroën C15, C25 and Berlingo and Peugeot J5 and Partner panel vans and the Citroën AX and Saxo and Peugeot 106 superminis. Other models were available through leasing mainly in California. Popular models included the General Motors EV1 and the Toyota RAV4 EV. Some of the latter were sold to the public and were still in use by the early 2010s. \n\nIn the late 2000s began a new wave of mass production plug-in electric cars, motorcycles and light trucks. However, , most electric vehicles in the world roads were low-speed, low-range neighborhood electric vehicles (NEVs) or electric quadricycles. Pike Research estimated there were almost 479,000 NEVs on the world roads in 2011. , the GEM neighborhood electric vehicle ranked as the market leader in North America, with global sales of more than 50,000 units since 1998. Sales of low-speed electric vehicles experienced considerable growth in China between 2012 and 2016. , the Chinese stock of NEVs was estimated to be between 3 million and 4 million units, with most powered by lead-acid batteries.\n\n, there were over 60 models of highway-capable plug-in electric passenger cars and light-utility vans available in the world. , there were 45 different plug-in electric passenger car models offered in Europe, 20 available in North America, 19 in China, 14 in Japan, and 7 in Australia. There are also available several commercial models of plug-in motorcycles, all-electric buses, and heavy-duty trucks.\n\nThe Renault-Nissan-Mitsubishi Alliance is the world's leading electric vehicle manufacturer. By mid-2018 the Alliance's all-electric vehicle sales totaled about 600,000 units worldwide, including the models manufactured by Mitsubishi Motors, now part of the Alliance.\n\nTesla is the world's second largest plug-in electric car manufacturer with global sales of almost 500,000 units. Its Model S was the world's top selling plug-in car for two years running, 2015 and 2016, Since inception, the Model S has sold 250,000 worldwide up until September 2018.\n\nRenking next is BYD Auto with about 435,000 units delivered in China through September 2018. Its Qin plug-in hybrid is the company's top selling model with over 124,000 units sold in China through September 2018, making it the all-time best-selling plug-in electric car in the country.\n\n, the BMW Group had sold 313,000 plug-in cars, accounting for global sales its BMW i cars, BMW iPerformance plug-in hybrid models, and MINI brand plug-ins.\n\nBYD Auto ended 2015 as the world's best selling manufacturer of highway legal light-duty plug-in electric vehicles, with 61,722 units sold, mostly plug-in hybrids, followed by Tesla, with 50,580 units sold in 2015. BYD was the world's top selling plug-in car manufacturer for a second year running, with 101,183 units sold in 2016, one more time followed again by Tesla with 76,243 units delivered. In 2017 BYD ranked for the third year in a row as the global top plug-in car manufacturer with 113,669 units delivered. The Renault-Nissan-Mitsubishi Alliance was the top selling automotive group with 119,995 units sold worldwide in 2017.\n\nThe global stock of plug-in electric vehicles between 2005 and 2009 consisted exclusively of all-electric cars, totaling about 1,700 units in 2005, and almost 6,000 in 2009. The plug-in stock rose to about 12,500 units in 2010, of which, only 350 vehicles were plug-in hybrids. By comparison, during the Golden Age of the electric car at the beginning of the 20th century, the EV stock peaked at approximately 30,000 vehicles. After the introduction of the Nissan Leaf and the Chevrolet Volt in late December 2010, the first mass-production plug-in cars by major carmakers, plug-in car sales grew to about 50,000 units in 2011, jumped to 125,000 in 2012, and rose to almost 213,000 plug-in electric cars and utility vans in 2013. Sales totaled over 315,000 units in 2014, up 48% from 2013, By mid-September 2015, the global stock of highway legal plug-in electric passenger cars and utility vans reached the one million sales milestone. Sales of plug-in electric vehicles achieved the one million milestone almost twice as fast as hybrid electric vehicles (HEV). While it took four years and 10 months to reach one-million PEV sales, it took more than around nine years and a few months for HEVs to reach its first million sales. A 2016 analysis by the Consumer Federation of America (CFA) found that five years after its introduction, sales of plug-in electric cars in the American market outsold conventional hybrids during the same period.\n\nThe global ratio between all-electrics (BEVs) and plug-in hybrids (PHEVs) was 60:40 between 2014 and the first half of 2016, mainly due to the large all-electric market in China. In the U.S. and Europe, the ratio approached a 50:50 split. All-electric cars oversold plug-in hybrids in 2016, with pure electrics representing about 61% of the global stock, up from 58.9% at the end of 2015. The global ratio between battery BEVs and PHEVs was 66:34 in 2017. Cumulative global sales of highway-capable light-duty pure electric vehicles since 2010 achieved the one million unit milestone in September 2016.\nIn five years, global sales of highway legal light-duty plug-in electric vehicles have increased more than ten-fold, totaling more than 565,000 units in 2015. Plug-in sales in 2015 increased about 80% from 2014, driven mainly by China and Europe. Both markets passed in 2015 the U.S. as the largest plug-in electric car markets in terms of total annual sales, with China ranking as the world's best-selling plug-in electric passenger car country market in 2015. About 775,000 plug-in cars and vans were sold in 2016, and cumulative global sales passed the 2 million milestone by the end of 2016. The global market share of the light-duty plug-in vehicle segment achieved a record 0.86% of total new car sales in 2016, up from 0.62% in 2015 and 0.38% in 2014. Global light-duty plug-in vehicle sales passed the 3 million milestone in November 2017. About 1.2 million plug-ins cars and vans were sold worldwide in 2017, with China accounting for about half of global sales in 2017. The global stock reached 4 million in September 2018, but despite the rapid growth experienced, the plug-in electric car segment represented just about 1 out of every 300 vehicles on the world's roads. According to Navigant Research, there were about 2.6 million all-electric cars on the world's roads by November 2018.\n\n, with cumulative sales of more than 645,000 plug-in electric passenger cars, China has the world's largest fleet of light-duty plug-in electric vehicles, after having overtook during 2016 both the U.S. and Europe in terms of cumulative sales. This figure accounts for both, domestically produced new energy passenger cars and imports. The fleet of Chinese plug-in cars represents 43.0% of the global stock of light-duty plug-in vehicles at the end of 2016. Among country markets, the U.S. ranks second, with over 570,000 plug-in passenger cars sold through December 2016, representing 28.1% of the global stock of plug-ins. Japan has the world's third largest plug-in stock, with about 147,500 highway legal plug-in electric vehicles sold in the country between July 2009 and December 2016. , about 637,500 plug-in electric passenger cars and vans have been registered in Europe, representing 31.4% of the global stock, the second largest after China. The region was the world's largest light-duty plug-in market until October 2016.\n\n, 25 cities accounted for 44% of the world’s stock of plug-in electric cars, while representing just 12% of world passenger vehicle sales. Shanghai led the world with cumulative sales of over 162,000 electric vehicles since 2011, followed by Beijing with 147,000 and Los Angeles with 143,000. Ranking next, with a stock of more than 50,000 electric vehicles are Shenzhen, Oslo, Hangzhou, San Francisco, Tianjin, Tokyo, San Jose, California, and Qingdao. These figures refer to the city proper and the broader surrounding metropolitan areas. Among these cities, Bergen has the highest market share of the plug-in segment, with about 50% of new car sales in 2017, followed by Oslo with 40%, Shanghai, Beijing, Shenzhen, Hangzhou, Tianjin, and San Jose with market shares ranging from 9% to 13%, Stockholm with 8%, and San Francisco with 7%.\n\n, more than 951,000 new energy vehicles have been sold in China since 2011, making the country the world's leader when all plug-in automotive segments are considered, including passenger cars, electric buses, and commercial plug-in heavy-duty trucks. The stock of new energy vehicles in China achieved the 500,000 unit milestone in March 2016. , China had the world's largest electric bus stock with about 385,000 units, representing more than 99% of the global stock. The European market account for 1,273 plug-in electric buses and the U.S. for only 200.\n\nThe stock of new energy vehicles in China totaled 2.21 million units by the end of September 2018, of which, 81% are all-electric vehicles. These figures include heavy-duty commercial vehicles such buses and sanitation trucks, which represent about 11% of the total stock. China is the world's largest electric bus market, and its electric bus stock grew nearly sixfold between 2014 and 2015, reaching about 385,000 units by the end of 2017.\n\n, cumulative sales of domestically produced highway legal plug-in electric passenger cars totaled over 1.2 million units since 2005, of which, a total of 579,000 were sold in 2017, representing about half of global plug-in car sales in 2017. Only vehicles manufactured in the country are accounted for because imports are not subject to government subsidies. Domestically produced cars account for about 96% of new energy car sales in China. A particular feature of the Chinese passenger plug-in market is the dominance of small entry level vehicles.\n\nIn September 2016, the Chinese stock of plug-in passenger cars reached the same level of the American stock, and by November 2016, China’s cumulative total plug-in passenger vehicles sales had surpassed those of Europe, allowing China to become the market with the world's largest stock of light-duty plug-in electric vehicles, with almost 600,000 plug-in passenger cars. China also surpassed the U.S. and Europe in terms of annual sales of light-duty plug-in electric vehicles, both in calendar years 2015 and current-year-to-date through November.\n\nThe stock of new energy vehicles sold in China since 2011 achieved the 500,000 unit milestone in March 2016, making the country the world's leader in the plug-in heavy-duty segment, including electric buses and plug-in trucks. Cumulative sales of new energy passenger cars achieved the 500,000 unit milestone in September 2016, excluding imports. , the BYD Qin plug-in hybrid, with 89,393 units sold since its inception, continued to rank as the all-time top selling plug-in electric car in the country. The BAIC EC-Series all-electric city car was Chinese the top selling plug-in car in 2017 with 78,079 units sold in China, making also the city car the world's top selling plug-in car in 2017. BYD Auto was again the top selling Chinese car manufacturer in 2017.\n\nSince the market launch of the Tesla Roadster in 2008, cumulative sales of highway legal plug-in electric cars in the U.S. achieved the one million unit milestone in September 2018. , the American stock represented 28.1% of the global light-duty plug-in stock, down from about 40% in 2014. , the U.S. has the world's third largest stock of plug-in passenger cars, after having being overtook by Europe in 2015 and China during 2016. California is the largest U.S. car market, and accounts for approximately 48% percent of cumulative plug-in sales in the American market from 2011 to June 2016, and also accounts for about 50% of nationwide all-electric car sales and 47% of total plug-in hybrid sales. The other nine states that follow California's Zero Emission Vehicle (ZEV) regulations have accounted for another 10% of cumulative plug-in car sales in the U.S. during the same period. California's plug-in car stock totaled 491,000 units at the end of October 2018.\n\nNationwide sales climbed from 17,800 units delivered in 2011 to 53,200 during 2012, and reached 97,100 in 2013. In 2014 plug-in electric car sales totaled 123,248 units, and fell to 114,248 in 2015. A total of 157,181 plug-in cars were sold in 2016, up 37.6% from 2015, and rose to 194,479 in 2017, up 23.7% from 2016. The market share of plug-in electric passenger cars increased from 0.14% of new car sales in 2011 to 0.37% in 2012, 0.62% in 2013, and reached 0.75% of new car sales in 2014. As plug-in car sales slowed down during 2015, the segment's market share fell to 0.66% of new car sales, but increased to 0.90% in 2016. The plug-in segment passed the 1% market share for the first time in 2017, with 1.13% of the country's total annual new car sales. The highest-ever monthly market share for plug-in electric vehicles was achieved in December 2017 with 1.58% of new car sales. The previous record was achieved in September 2017. Monthly sales of plug-in cars in the American market passed the 1% mark for the first time in September 2016 (1.12%). December 2017 is also the best monthly plug-in sales volume on record ever, with 25,149 units delivered.\n\n, Japan had a stock of light-duty plug-in vehicles of about 207,200. , the Japanese stock consisted of 86,390 all-electric cars (57.1%) and 64,860 plug-in hybrids (42.9%). Plug-in segment sales climbed from 1,080 units in 2009 to 12,630 in 2011, and reached 24,440 in 2012. Global sales of pure electric cars in 2012 were led by Japan with a 28% market share of the segment sales. Japan ranked second after the U.S. in terms of its share of plug-in hybrid sales in 2012, and until 2016, the country ranked third.\n\nThe plug-in segment sales remained flat in 2014 with 30,390 units sold, and a market share of 1.06% of total new car sales in the country (kei cars not included). Sales totaled 24,660 units in 2015 and 24,851 units in 2016. The rate of growth of the Japanese plug-in segment slowed down from 2013, with annual sales falling behind Europe, the U.S. and China during 2014 and 2015. The segment market share fell from 0.68% in 2014 to 0.59% in 2016. The decline in plug-in car sales reflects the Japanese government and the major domestic carmakers decision to adopt and promote hydrogen fuel cell vehicles instead of plug-in electric vehicles. Sales recovered in 2017, with almost 56,000 plug-in cars sold, and the segment's market share reached 1.1%. Since December 2010, Nissan has sold 72,494 Leafs through December 2016, making the Leaf the all-time best-selling plug-in car in the country. \n\n, about 943,600 plug-in electric passenger cars and vans have been registered in Europe, making the region the world's second largest after China. , European sales of plug-in cars and vans are led by Norway with over 200,000 units registered, followed by France with almost 158,000, and the UK with over 137,000 units. Norway was the top selling plug-in country market in Europe in 2016 and 2017. The other top selling European markets in terms of cumulative registrations are Germany, the Netherlands and Sweden.\n\nCumulative sales of light-duty plug-in electric vehicles in Europe passed the 500,000 unit milestone in May 2016. Norway passed the 100,000th registered plug-in unit milestone in April 2016, France passed the same milestone in September 2016, and the Netherlands in November 2016. The UK achieved the 100,000 unit mark in March 2017.\n\n, France ranked as the largest European market for light-duty electric commercial vehicles or utility vans, accounting for nearly half of all vans sold in the European Union. The French market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015. Denmark is the second largest European market, with over 2,600 plug-in electric vans sold in 2015, with an 8.5% market share of all vans sold in the country. Most of the van sold in the Danish market are plug-in hybrids, accounting for almost all of the plug-in hybrid van sales across the EU. European sales of plug-in electric cars passed 200,000 units for the first time in 2016, and the plug-in segment achieved a market share of 1.3%. Sales in 2017 exceeded 300,000 units with a market share of 2%.\n\n, the stock of light-duty plug-in electric vehicles registered in Norway totaled more than 200,000 units, making Norway the European country with the largest stock of plug-in cars and vans, and the fourth largest in the world. , and accounting for both new and used imports registrations, the Norwegian light-duty plug-in electric fleet consisted of 141,951 all-electric passenger cars and vans, and 67,171 plug-in hybrids. The registered plug-in stock includes almost 2,700 all-electric vans and about 24,500 used imported electric cars from neighboring countries . \n\nThe government's target of 50,000 all-electric cars on Norwegian roads was reached in April 2015, more than two years earlier than expected, thanks to the successful policies implemented to promote electric vehicle adoption that include fiscal and non-monetary incentives. The milestone of 100,000 light-duty plug-in electric vehicles registered was achieved in April 2016, and 100,000 all-electric vehicles in December 2016, representing about 10% of all pure electric cars that have been sold worldwide. , there were over 200,000 plug-in electric cars and vans on Norwegian roads.\n\nThe Norwegian fleet of plug-in electric cars is one of the cleanest in the world because 98% of the electricity generated in the country comes from hydropower. Norway, with about 5.2 million people, is the country with the largest EV ownership per capita in the world. In March 2014, Norway became the first country where over one in every 100 registered passenger cars is plug-in electric. The plug-in car market penetration reached 2% in March 2015, passed 5% in December 2016, and achieved 10% in October 2018.\n\n, the stock of light-duty plug-in electric vehicles registered in France totaled 149,797 plug-in cars and electric utility vans delivered since 2010, , the country ranked as the third largest plug-in market in Europe after Norway and the Netherlands, and the world's sixth. , and accounting for registrations since 2010, the plug-in electric stock consisted consisted of 92,256 all-electric passenger cars, 25,269 all-electric utility vans, and 32,272 plug-in hybrids. The stock of light-duty plug-in electric vehicles registered in France passed the 100,000 unit milestone in October 2016., France is the country with the world's largest market for light-duty electric commercial vehicles or utility vans. Nearly half of the vans sold in the European Union are sold in France as a result of a national purchase incentive scheme, which French companies have embraced. The market share of all-electric utility vans reached a market share of 1.22% of new vans registered in 2014, and 1.30% in 2015.\n\nMore than 137,000 light-duty plug-in electric vehicles have been registered in the UK up until December 2017, including about 5,100 plug-in commercial vans. Since the launch of the Plug-In Car Grant in January 2011, a total of 127,509 eligible cars have been registered through December 2017, and, , the number of claims made through the Plug-in Van Grant scheme totaled 2,938 units since the launch of the scheme in 2012. Before the introduction of series production plug-in vehicles, a total of 1,096 all-electric vehicles were registered in the UK between 2006 and December 2010. Before 2011, the G-Wiz, a heavy quadricycle, listed as the top-selling EV for several years. , the UK had the fourth largest European stock of light-duty plug-in vehicles, and with 36,907 plug-in passenger cars registered in 2016, ranked as the second best selling European market that year after Norway.\n\n, a total of 129,246 plug-in electric cars have been registered in Germany since 2010. The country is the largest passenger car market in Europe, however, , ranked as the fifth largest plug-in market in Europe. About 80% of the plug-in cars registered in the country through September 2016 were registered since January 2014. The official German definition of electric vehicles changed at the beginning of 2013, before that, official statistics only registered all-electric vehicles because plug-in hybrids were accounted together with conventional hybrids. As a result, the registrations figures for 2012 and older do not account for total new plug-in electric car registrations.\n\nA record of 54,492 plug-in cars were registered in 2017, up 217% the previous year, and consisting of 29,436 plug-in hybrids and 25,056 all-electric cars. Registrations in 2017 achieved a market share of 1.58%. The top selling models in 2017 were the Audi A3 e-tron (4,454), Renault Zoe (4,322), and BMW i3 (4,319). \n\n, there were 121,542 highway legal light-duty plug-in electric vehicles registered in the Netherlands, consisting of 98,217 range-extended and plug-in hybrids, 21,115 pure electric cars, and 2,210 all-electric light utility vans. When buses, trucks, motorcycles, quadricycles and tricycles are accounted for, the Dutch plug-in electric-drive fleet climbs to 123,499 units. The country's electric vehicle stock reaches 165,886 units when fuel cell electric vehicles (43), mopeds (4,376), electric bicycles (37,652), and microcars (316) are accounted for. A distinct feature of the Dutch plug-in market is dominance of plug-in hybrids, which represented 81% of the country's stock of passenger plug-in electric cars and vans registered at the end of December 2017.\n\nThe Netherlands listed as the world's third best-selling country market for light-duty plug-in vehicles in 2015, with 43,971 units registered that year. Until December 2015, the Netherlands had Europe's largest fleet light-duty plug-in vehicles. Plug-in sales fell sharply during 2016, and as a result, by early October 2016, the Netherlands listed as the third largest European plug-in market, after being surpassed during the year by both Norway and France. , the Netherlands had the second largest plug-in market concentration per capita in the world after Norway. The stock of light-duty plug-in electric vehicles registered in the Netherlands achieved the 100,000 unit milestone in November 2016. However, due to changes in tax rules, the plug-in market share declined from 9.9% in 2015, to 6.7% in 2016, and fell to 2.6% in 2017.\n\nThe world's top selling highway-capable all-electric car ever is the Nissan Leaf with global sales of 370,000 by October 2018. The United States is the world's largest Leaf market with 126,747 units sold through October 2018. The other two top markets are Japan with 100,000 units delivered by April 2018, and Europe with 100,000 units by June 2018. The European market is led by Norway with 48,235 units registered up until October 2018, including used imports, and the UK with 25,000 by June 2018.\n\nRanking second is the all-electric Tesla Model S with global deliveries of 250,000 units , with the United States as its leading market with about 138,000 units delivered through October 2018. The Tesla Model S was the world's top selling plug-in car for two years in a row, 2015 and 2016. The BAIC EC-Series all-electric city car ranked as the world's top selling plug-in car in 2017 with 78,079 units sold in China. The world's all-time top selling all-electric light utility vehicle is the Renault Kangoo Z.E., with global sales of 35,310 units through September 2018.\n\nThe following table presents global sales of the top selling highway-capable electric cars and light utility vehicles produced since the introduction of the first modern production all-electric car, the Tesla Roadster, in 2008 and October 2018. The table includes all-electric passenger cars and utility vans with cumulative sales of about or over 100,000 units.\n\n, the Volt/Ampera family is the world's best selling plug-in hybrid and the third best selling plug-in electric car after the Nissan Leaf and the Model S. Chevrolet Volt and Opel/Vauxhall Ampera combined sales totaled about 177,000 units worldwide through October 2018, including just over 10,000 Opel/Vauxhall Amperas sold in Europe through June 2016. Volt sales are led by the United States with 148,556 units delivered through October 2018, followed by Canada with 16,653 units through September 2018. The Netherlands ranked as the leading European market with about 6,000 Amperas and Volts registered through December 2015.\n\nRanking next is the Mitsubishi Outlander P-HEV with 156,329 units sold worldwide . Europe is the Outlander P-HEV leading market with 105,813 units sold, followed by Japan with 42,451 units, both through March 2018. European sales are led by the UK with 36,237 units sold, followed by the Netherlands with 25,489 units and Norway with 14,196.\n\nRanking third is the Toyota Prius Plug-in Hybrid with 128,900 units sold worldwide through December 2017. The United States is the market leader with 65,703 units delivered through December 2017. Japan ranks next with about 48,800 units, followed by Europe with 13,100 units, both, through December 2017. Production of the first generation Prius Plug-in ended in June 2015. The second generation Prius plug-in hybrid, the Toyota Prius Prime, was released in the United States in November 2016.\n\nThe following table presents plug-in hybrid models with cumulative global sales of around or more than 75,000 units since the introduction of the first modern production plug-in hybrid car, the BYD F3DM, in 2008 up until September 2018:\n\n\n\n"}
{"id": "2421241", "url": "https://en.wikipedia.org/wiki?curid=2421241", "title": "Potassium titanyl phosphate", "text": "Potassium titanyl phosphate\n\nPotassium titanyl phosphate (KTP) is an inorganic compound with the formula KTiOPO. It is a white solid. KTP is an important nonlinear optical material that is commonly used for frequency doubling diode pumped solid-state lasers such as Nd:YAG and other neodymium-doped lasers.\n\nThe compound is prepared by the reaction of titanium dioxide with a mixture of KHPO and KHPO near 1300 K. The potassium salts serve both as reagents and flux.\n\nThe material has been characterized by X-ray crystallography. KTP has an orthorhombic crystal structure. It features octahedral Ti(IV) and tetrahedral phosphate sites. Potassium has a high coordination number. All heavy atoms (Ti, P, K) are linked exclusively by oxides, which interconnect these atoms.\n\nCrystals of KTP are highly transparent for wavelengths between 350–2700 nm with a reduced transmission out to 4500 nm where the crystal is effectively opaque. Its second harmonic generation (SHG) coefficient is about three times higher than KDP. It has a Mohs hardness of about 5.\n\nKTP is also used as an optical parametric oscillator for near IR generation up to 4 µm. It is particularly suited to high power operation as an optical parametric oscillator due to its high damage threshold and large crystal aperture. The high degree of birefringent walk-off between the pump signal and idler beams present in this material limit its use as an optical parametric oscillator for very low power applications.\n\nThe material has a relatively high threshold to optical damage (~15 J/cm²), a excellent optical nonlinearity and excellent thermal stability in theory. In practice, KTP crystals need to have stable temperature to operate if they are pumped with 1064 nm (infrared, to output 532 nm green). However, it is prone to photochromic damage (called grey tracking) during high-power 1064 nm second-harmonic generation which tends to limit its use to low- and mid-power systems.\n\nOther such materials include potassium titanyl arsenate (KTiOAsO).\nIt is used to produce \"greenlight\" to perform some laser prostate surgery. KTP crystals coupled with or Nd:YVO crystals are commonly found in green laser pointers.\n\nKTP is also used as an electro-optic modulator, optical waveguide material, and in directional couplers.\n\nPeriodically poled potassium titanyl phosphate (PPKTP) consists of KTP with switched domain regions within the crystal for various nonlinear optic applications and frequency conversion. It can be wavelength tailored for efficient second harmonic generation, sum frequency generation, and difference frequency generation. The interactions in PPKTP are based upon quasi-phase-matching, achieved by periodic poling of the crystal, whereby a structure of regularly spaced ferroelectric domains with alternating orientations are created in the material.\n\nPPKTP is commonly used for Type 1 & 2 frequency conversions for pump wavelengths of 730-3500 nm.\n\nOther materials used for periodic poling are wide band gap inorganic crystals like lithium niobate (resulting in periodically poled lithium niobate, PPLN), lithium tantalate, and some organic materials.\n\nOther materials used for laser frequency doubling are\n\n"}
{"id": "43602880", "url": "https://en.wikipedia.org/wiki?curid=43602880", "title": "Purchase and Sale Agreement", "text": "Purchase and Sale Agreement\n\nA Purchase and Sale Agreement (typically referred to in the shorthand \"PSA\"), is an agreement between a buyer and a seller of real estate property, company stock, or other assets.\n\nThe person or company acquiring, receiving and purchasing the property, stock or assets is referred to as the \"Buyer\" and the person or company disposing, conveying and selling the stock or assets is referred to as \"Seller\". The PSA will set out the various rights and obligations of both the Buyer and Seller, and may also require other documents be executed and recorded in the public records, such as an Assignment, Deed of Trust, or Farmout Agreement.\n\nIn the oil and natural gas industries, a PSA is the primary legal contract by which companies exchange oil and gas assets or stock in an oil and gas business entity, for cash, debt, stock, or other assets.\n"}
{"id": "33552071", "url": "https://en.wikipedia.org/wiki?curid=33552071", "title": "Reinventing Fire", "text": "Reinventing Fire\n\nReinventing Fire: Bold Business Solutions for the New Energy Era is a 2011 book, by Amory B. Lovins and the Rocky Mountain Institute, that explores converting the United States to almost total reliance on renewable energy sources, such as solar energy and wind power. Lovins says that renewable energy is already cheaper than fossil fuels and his analysis predicts further reductions in renewable energy prices.\n\n\"Reinventing Fire\" was launched at the Washington National Geographic Society, in October 2011. Bill Clinton says the book is a “wise, detailed and comprehensive blueprint.” The book has forewords by Marvin Odum, from Shell Oil, and John W. Rowe, CEO of Exelon. The first paragraph of the preface says: \n\nImagine fuel without fear. No climate change. No oil spills, dead coal miners, dirty air, devastated lands, lost wildlife. No energy poverty. No oil-fed wars, tyrannies, or terrorists. Nothing to run out. Nothing to cut off. Nothing to worry about. Just energy abundance, benign and affordable, for all, for ever.\nFen Montaigne in \"The Guardian\" has said that the book is impressive in both its scope and detail:\n\nLovins discusses everything from how to redesign heavy trucks to make them more fuel efficient to ways to change factory pipes to conserve energy — the book lays out a plan for the U.S. to achieve the following by 2050: cars completely powered by hydrogen fuel cells, electricity, and biofuels; 84 percent of trucks and airplanes running on biomass fuels; 80 percent of the nation's electricity produced by renewable power; $5 trillion in savings; and an economy that has grown by 158 percent.\nBy combining reduced energy use with energy efficiency gains, Lovins says that there will be a $5 trillion saving over the next 40 years and a faster-growing economy. This can all be done, the book jacket says, without \"new federal taxes, subsidies, mandates, or laws. The policy innovations needed to unlock and speed it need no act of Congress.\" The profitable commercialization of existing energy-saving technologies, through market forces, can be led by business.\n\n\n"}
{"id": "41471157", "url": "https://en.wikipedia.org/wiki?curid=41471157", "title": "Renewable energy in Algeria", "text": "Renewable energy in Algeria\n\nRenewable energy in Algeria ranges from hydropower, solar and wind energy in Algeria.\n\n"}
{"id": "27127", "url": "https://en.wikipedia.org/wiki?curid=27127", "title": "Sulfur", "text": "Sulfur\n\nSulfur or sulphur is a chemical element with symbol S and atomic number 16. It is abundant, multivalent, and nonmetallic. Under normal conditions, sulfur atoms form cyclic octatomic molecules with a chemical formula S. Elemental sulfur is a bright yellow crystalline solid at room temperature. Chemically, sulfur reacts with all elements except for gold, platinum, iridium, tellurium, and the noble gases.\n\nSulfur is the tenth most common element by mass in the universe, and the fifth most common on Earth. Though sometimes found in pure, native form, sulfur on Earth usually occurs as sulfide and sulfate minerals. Being abundant in native form, sulfur was known in ancient times, being mentioned for its uses in ancient India, ancient Greece, China, and Egypt. In the Bible, sulfur is called brimstone. Today, almost all elemental sulfur is produced as a byproduct of removing sulfur-containing contaminants from natural gas and petroleum. The greatest commercial use of the element is the production of sulfuric acid for sulfate and phosphate fertilizers, and other chemical processes. The element sulfur is used in matches, insecticides, and fungicides. Many sulfur compounds are odoriferous, and the smells of odorized natural gas, skunk scent, grapefruit, and garlic are due to organosulfur compounds. Hydrogen sulfide gives the characteristic odor to rotting eggs and other biological processes.\n\nSulfur is an essential element for all life, but almost always in the form of organosulfur compounds or metal sulfides. Three amino acids (cysteine, cystine, and methionine) and two vitamins (biotin and thiamine) are organosulfur compounds. Many cofactors also contain sulfur including glutathione and thioredoxin and iron–sulfur proteins. Disulfides, S–S bonds, confer mechanical strength and insolubility of the protein keratin, found in outer skin, hair, and feathers. Sulfur is one of the core chemical elements needed for biochemical functioning and is an elemental macronutrient for all living organisms.\n\nSulfur forms polyatomic molecules with different chemical formulas, the best-known allotrope being octasulfur, cyclo-S. The point group of cyclo-S is D and its dipole moment is 0 D. Octasulfur is a soft, bright-yellow solid that is odorless, but impure samples have an odor similar to that of matches. It melts at , boils at and sublimes easily. At , below its melting temperature, cyclo-octasulfur changes from α-octasulfur to the β-polymorph. The structure of the S ring is virtually unchanged by this phase change, which affects the intermolecular interactions. Between its melting and boiling temperatures, octasulfur changes its allotrope again, turning from β-octasulfur to γ-sulfur, again accompanied by a lower density but increased viscosity due to the formation of polymers. At higher temperatures, the viscosity decreases as depolymerization occurs. Molten sulfur assumes a dark red color above . The density of sulfur is about 2 g/cm, depending on the allotrope; all of the stable allotropes are excellent electrical insulators.\n\nSulfur burns with a blue flame with formation of sulfur dioxide, which has a suffocating and irritating odor. Sulfur is insoluble in water but soluble in carbon disulfide and, to a lesser extent, in other nonpolar organic solvents, such as benzene and toluene. The first and second ionization energies of sulfur are 999.6 and 2252 kJ/mol, respectively. Despite such figures, the +2 oxidation state is rare, with +4 and +6 being more common. The fourth and sixth ionization energies are 4556 and 8495.8 kJ/mol, the magnitude of the figures caused by electron transfer between orbitals; these states are only stable with strong oxidants such as fluorine, oxygen, and chlorine. \nSulfur reacts with nearly all other elements with the exception of gold, platinum, iridium, nitrogen, tellurium, iodine and the noble gases. Some of those reactions need elevated temperatures.\n\nSulfur forms over 30 solid allotropes, more than any other element. Besides S, several other rings are known. Removing one atom from the crown gives S, which is more of a deep yellow than the S. HPLC analysis of \"elemental sulfur\" reveals an equilibrium mixture of mainly S, but with S and small amounts of S. Larger rings have been prepared, including S and S.\n\nAmorphous or \"plastic\" sulfur is produced by rapid cooling of molten sulfur—for example, by pouring it into cold water. X-ray crystallography studies show that the amorphous form may have a helical structure with eight atoms per turn. The long coiled polymeric molecules make the brownish substance elastic, and in bulk this form has the feel of crude rubber. This form is metastable at room temperature and gradually reverts to crystalline molecular allotrope, which is no longer elastic. This process happens within a matter of hours to days, but can be rapidly catalyzed.\n\nSulfur has 25 known isotopes, four of which are stable: S (), S (), S (), and S (). Other than S, with a half-life of 87 days and formed in cosmic ray spallation of Ar, the radioactive isotopes of sulfur have half-lives less than 3 hours.\n\nWhen sulfide minerals are precipitated, isotopic equilibration among solids and liquid may cause small differences in the δS-34 values of co-genetic minerals. The differences between minerals can be used to estimate the temperature of equilibration. The δC-13 and δS-34 of coexisting carbonate minerals and sulfides can be used to determine the pH and oxygen fugacity of the ore-bearing fluid during ore formation.\n\nIn most forest ecosystems, sulfate is derived mostly from the atmosphere; weathering of ore minerals and evaporites contribute some sulfur. Sulfur with a distinctive isotopic composition has been used to identify pollution sources, and enriched sulfur has been added as a tracer in hydrologic studies. Differences in the natural abundances can be used in systems where there is sufficient variation in the S of ecosystem components. Rocky Mountain lakes thought to be dominated by atmospheric sources of sulfate have been found to have different δS values from lakes believed to be dominated by watershed sources of sulfate.\n\nS is created inside massive stars, at a depth where the temperature exceeds 2.5×10 K, by the fusion of one nucleus of silicon plus one nucleus of helium. As this is part of the alpha process that produces elements in abundance, sulfur is the 10th most common element in the universe.\n\nSulfur, usually as sulfide, is present in many types of meteorites. Ordinary chondrites contain on average 2.1% sulfur, and carbonaceous chondrites may contain as much as 6.6%. It is normally present as troilite (FeS), but there are exceptions, with carbonaceous chondrites containing free sulfur, sulfates and other sulfur compounds. The distinctive colors of Jupiter's volcanic moon Io are attributed to various forms of molten, solid and gaseous sulfur.\n\nIt is the fifth most common element by mass in the Earth. Elemental sulfur can be found near hot springs and volcanic regions in many parts of the world, especially along the Pacific Ring of Fire; such volcanic deposits are currently mined in Indonesia, Chile, and Japan. These deposits are polycrystalline, with the largest documented single crystal measuring 22×16×11 cm. Historically, Sicily was a major source of sulfur in the Industrial Revolution.\n\nNative sulfur is synthesised by anaerobic bacteria acting on sulfate minerals such as gypsum in salt domes. Significant deposits in salt domes occur along the coast of the Gulf of Mexico, and in evaporites in eastern Europe and western Asia. Native sulfur may be produced by geological processes alone. Fossil-based sulfur deposits from salt domes were until recently the basis for commercial production in the United States, Russia, Turkmenistan, and Ukraine. Currently, commercial production is still carried out in the Osiek mine in Poland. Such sources are now of secondary commercial importance, and most are no longer worked.\n\nCommon naturally occurring sulfur compounds include the sulfide minerals, such as pyrite (iron sulfide), cinnabar (mercury sulfide), galena (lead sulfide), sphalerite (zinc sulfide) and stibnite (antimony sulfide); and the sulfates, such as gypsum (calcium sulfate), alunite (potassium aluminium sulfate), and barite (barium sulfate). On Earth, just as upon Jupiter's moon Io, elemental sulfur occurs naturally in volcanic emissions, including emissions from hydrothermal vents.\n\nCommon oxidation states of sulfur range from −2 to +6. Sulfur forms stable compounds with all elements except the noble gases.\n\nSulfur polycations, S, S and S are produced when sulfur is reacted with mild oxidising agents in a strongly acidic solution. The colored solutions produced by dissolving sulfur in oleum were first reported as early as 1804 by C.F. Bucholz, but the cause of the color and the structure of the polycations involved was only determined in the late 1960s. S is deep blue, S is yellow and S is red.\n\nTreatment of sulfur with hydrogen gives hydrogen sulfide. When dissolved in water, hydrogen sulfide is mildly acidic:\n\nHydrogen sulfide gas and the hydrosulfide anion are extremely toxic to mammals, due to their inhibition of the oxygen-carrying capacity of hemoglobin and certain cytochromes in a manner analogous to cyanide and azide (see below, under \"precautions\").\n\nReduction of elemental sulfur gives polysulfides, which consist of chains of sulfur atoms terminated with S centers:\nThis reaction highlights a distinctive property of sulfur: its ability to catenate (bind to itself by formation of chains). Protonation of these polysulfide anions produces the polysulfanes, HS where x = 2, 3, and 4. Ultimately, reduction of sulfur produces sulfide salts:\nThe interconversion of these species is exploited in the sodium-sulfur battery.\n\nThe radical anion S gives the blue color of the mineral lapis lazuli.\n\nThe principal sulfur oxides are obtained by burning sulfur:\nMultiple sulfur oxides are known; the sulfur-rich oxides include sulfur monoxide, disulfur monoxide, disulfur dioxides, and higher oxides containing peroxo groups.\n\nSulfur forms sulfur oxoacids, some of which cannot be isolated and are only known through the salts. Sulfur dioxide and sulfites () are related to the unstable sulfurous acid (HSO). Sulfur trioxide and sulfates () are related to sulfuric acid (HSO). Sulfuric acid and SO combine to give oleum, a solution of pyrosulfuric acid (HSO) in sulfuric acid.\nThiosulfate salts (), sometimes referred as \"hyposulfites\", used in photographic fixing (hypo) and as reducing agents, feature sulfur in two oxidation states. Sodium dithionite (), contains the more highly reducing dithionite anion (). \n\nSeveral sulfur halides are important to modern industry. Sulfur hexafluoride is a dense gas used as an insulator gas in high voltage transformers; it is also a nonreactive and nontoxic propellant for pressurized containers. Sulfur tetrafluoride is a rarely used organic reagent that is highly toxic. Sulfur dichloride and disulfur dichloride are important industrial chemicals. Sulfuryl chloride and chlorosulfuric acid are derivatives of sulfuric acid; thionyl chloride (SOCl) is a common reagent in organic synthesis.\n\nAn important S–N compound is the cage tetrasulfur tetranitride (SN). Heating this compound gives polymeric sulfur nitride ((SN)), which has metallic properties even though it does not contain any metal atoms. Thiocyanates contain the SCN group. Oxidation of thiocyanate gives thiocyanogen, (SCN) with the connectivity NCS-SCN. Phosphorus sulfides are numerous, the most important commercially being the cages PS and PS.\n\nThe principal ores of copper, zinc, nickel, cobalt, molybdenum, and other metals are sulfides. These materials tend to be dark-colored semiconductors that are not readily attacked by water or even many acids. They are formed, both geochemically and in the laboratory, by the reaction of hydrogen sulfide with metal salts. The mineral galena (PbS) was the first demonstrated semiconductor and was used as a signal rectifier in the cat's whiskers of early crystal radios. The iron sulfide called pyrite, the so-called \"fool's gold\", has the formula FeS. Processing these ores, usually by roasting, is costly and environmentally hazardous. Sulfur corrodes many metals through tarnishing.\n\nSome of the main classes of sulfur-containing organic compounds include the following:\n\nCompounds with carbon-sulfur multiple bonds are uncommon, an exception being carbon disulfide, a volatile colorless liquid that is structurally similar to carbon dioxide. It is used as a reagent to make the polymer rayon and many organosulfur compounds. Unlike carbon monoxide, carbon monosulfide is stable only as an extremely dilute gas, found between solar systems.\n\nOrganosulfur compounds are responsible for some of the unpleasant odors of decaying organic matter. They are widely known as the odorant in domestic natural gas, garlic odor, and skunk spray. Not all organic sulfur compounds smell unpleasant at all concentrations: the sulfur-containing monoterpenoid (grapefruit mercaptan) in small concentrations is the characteristic scent of grapefruit, but has a generic thiol odor at larger concentrations. Sulfur mustard, a potent vesicant, was used in World War I as a disabling agent.\n\nSulfur-sulfur bonds are a structural component used to stiffen rubber, similar to the disulfide bridges that rigidify proteins (see biological below). In the most common type of industrial \"curing\" or hardening and strengthening of natural rubber, elemental sulfur is heated with the rubber to the point that chemical reactions form disulfide bridges between isoprene units of the polymer. This process, patented in 1843, made rubber a major industrial product, especially in automobile tires. Because of the heat and sulfur, the process was named vulcanization, after the Roman god of the forge and volcanism.\n\nBeing abundantly available in native form, sulfur was known in ancient times and is referred to in the Torah (Genesis). English translations of the Bible commonly referred to burning sulfur as \"brimstone\", giving rise to the term \"fire-and-brimstone\" sermons, in which listeners are reminded of the fate of eternal damnation that await the unbelieving and unrepentant. It is from this part of the Bible that Hell is implied to \"smell of sulfur\" (likely due to its association with volcanic activity). According to the Ebers Papyrus, a sulfur ointment was used in ancient Egypt to treat granular eyelids. Sulfur was used for fumigation in preclassical Greece; this is mentioned in the \"Odyssey\". Pliny the Elder discusses sulfur in book 35 of his \"Natural History\", saying that its best-known source is the island of Melos. He mentions its use for fumigation, medicine, and bleaching cloth.\n\nA natural form of sulfur known as \"shiliuhuang\" (石硫黄) was known in China since the 6th century BC and found in Hanzhong. By the 3rd century, the Chinese discovered that sulfur could be extracted from pyrite. Chinese Daoists were interested in sulfur's flammability and its reactivity with certain metals, yet its earliest practical uses were found in traditional Chinese medicine. A Song dynasty military treatise of 1044 AD described different formulas for Chinese black powder, which is a mixture of potassium nitrate (), charcoal, and sulfur. It remains an ingredient of black gunpowder.\n\nIndian alchemists, practitioners of \"the science of mercury\" (sanskrit rasaśāstra, रसशास्त्र), wrote extensively about the use of sulfur in alchemical operations with mercury, from the eighth century AD onwards. In the rasaśāstra tradition, sulfur is called \"the smelly\" (sanskrit gandhaka, गन्धक).\n\nEarly European alchemists gave sulfur a unique alchemical symbol, a triangle at the top of a cross. In traditional skin treatment, elemental sulfur was used (mainly in creams) to alleviate such conditions as scabies, ringworm, psoriasis, eczema, and acne. The mechanism of action is unknown—though elemental sulfur does oxidize slowly to sulfurous acid, which is (through the action of sulfite) a mild reducing and antibacterial agent.\n\nIn 1777, Antoine Lavoisier helped convince the scientific community that sulfur was an element, not a compound.\n\nSulfur deposits in Sicily were the dominant source for more than a century. By the late 18th century, about 2,000 tonnes per year of sulfur were imported into Marseilles, France, for the production of sulfuric acid for use in the Leblanc process. In industrializing Britain, with the repeal of tariffs on salt in 1824, demand for sulfur from Sicily surged upward. The increasing British control and exploitation of the mining, refining, and transportation of the sulfur, coupled with the failure of this lucrative export to transform Sicily's backward and impoverished economy, led to the 'Sulfur Crisis' of 1840, when King Ferdinand II gave a monopoly of the sulfur industry to a French firm, violating an earlier 1816 trade agreement with Britain. A peaceful solution was eventually negotiated by France.\n\nIn 1867, elemental sulfur was discovered in underground deposits in Louisiana and Texas. The highly successful Frasch process was developed to extract this resource.\n\nIn the late 18th century, furniture makers used molten sulfur to produce decorative inlays in their craft. Because of the sulfur dioxide produced during the process of melting sulfur, the craft of sulfur inlays was soon abandoned. Molten sulfur is sometimes still used for setting steel bolts into drilled concrete holes where high shock resistance is desired for floor-mounted equipment attachment points. Pure powdered sulfur was used as a medicinal tonic and laxative. With the advent of the contact process, the majority of sulfur today is used to make sulfuric acid for a wide range of uses, particularly fertilizer.\n\n\"Sulfur\" is derived from the Latin word ', which was Hellenized to '. The spelling ' appears toward the end of the Classical period. (The true Greek word for sulfur, , is the source of the international chemical prefix \"thio-\".) In 12th-century Anglo-French, it was '; in the 14th century the Latin ' was restored, for '; and by the 15th century the full Latin spelling was restored, for \"sulfur, sulphur\". The parallel \"f~ph\" spellings continued in Britain until the 19th century, when the word was standardized as \"sulphur\". \"Sulfur\" was the form chosen in the United States, whereas Canada uses both. The IUPAC adopted the spelling \"sulfur\" in 1990, as did the Nomenclature Committee of the Royal Society of Chemistry in 1992, restoring the spelling \"sulfur\" to Britain. Oxford Dictionaries note that \"in chemistry and other technical uses ... the \"-f-\" spelling is now the standard form for this and related words in British as well as US contexts, and is increasingly used in general contexts as well.\"\n\nSulfur may be found by itself and historically was usually obtained in this form; pyrite has also been a source of sulfur. In volcanic regions in Sicily, in ancient times, it was found on the surface of the Earth, and the \"Sicilian process\" was used: sulfur deposits were piled and stacked in brick kilns built on sloping hillsides, with airspaces between them. Then, some sulfur was pulverized, spread over the stacked ore and ignited, causing the free sulfur to melt down the hills. Eventually the surface-borne deposits played out, and miners excavated veins that ultimately dotted the Sicilian landscape with labyrinthine mines. Mining was unmechanized and labor-intensive, with pickmen freeing the ore from the rock, and mine-boys or \"carusi\" carrying baskets of ore to the surface, often through a mile or more of tunnels. Once the ore was at the surface, it was reduced and extracted in smelting ovens. The conditions in Sicilian sulfur mines were horrific, prompting Booker T. Washington to write \"I am not prepared just now to say to what extent I believe in a physical hell in the next world, but a sulphur mine in Sicily is about the nearest thing to hell that I expect to see in this life.\"\n\nElemental sulfur was extracted from salt domes (in which it sometimes occurs in nearly pure form) until the late 20th century. Sulfur is now produced as a side product of other industrial processes such as in oil refining, in which sulfur is undesired. As a mineral, native sulfur under salt domes is thought to be a fossil mineral resource, produced by the action of ancient bacteria on sulfate deposits. It was removed from such salt-dome mines mainly by the Frasch process. In this method, superheated water was pumped into a native sulfur deposit to melt the sulfur, and then compressed air returned the 99.5% pure melted product to the surface. Throughout the 20th century this procedure produced elemental sulfur that required no further purification. Due to a limited number of such sulfur deposits and the high cost of working them, this process for mining sulfur has not been employed in a major way anywhere in the world since 2002.\nToday, sulfur is produced from petroleum, natural gas, and related fossil resources, from which it is obtained mainly as hydrogen sulfide. Organosulfur compounds, undesirable impurities in petroleum, may be upgraded by subjecting them to hydrodesulfurization, which cleaves the C–S bonds:\nThe resulting hydrogen sulfide from this process, and also as it occurs in natural gas, is converted into elemental sulfur by the Claus process. This process entails oxidation of some hydrogen sulfide to sulfur dioxide and then the comproportionation of the two:\n\nOwing to the high sulfur content of the Athabasca Oil Sands, stockpiles of elemental sulfur from this process now exist throughout Alberta, Canada. Another way of storing sulfur is as a binder for concrete, the resulting product having many desirable properties (see sulfur concrete). Sulfur is still mined from surface deposits in poorer nations with volcanoes, such as Indonesia, and worker conditions have not improved much since Booker T. Washington's days.\n\nThe world production of sulfur in 2011 amounted to 69 million tonnes (Mt), with more than 15 countries contributing more than 1 Mt each. Countries producing more than 5 Mt are China (9.6), US (8.8), Canada (7.1) and Russia (7.1). Production has been slowly increasing from 1900 to 2010; the price was unstable in the 1980s and around 2010.\n\nElemental sulfur is used mainly as a precursor to other chemicals. Approximately 85% (1989) is converted to sulfuric acid (HSO):\nIn 2010, the United States produced more sulfuric acid than any other inorganic industrial chemical. The principal use for the acid is the extraction of phosphate ores for the production of fertilizer manufacturing. Other applications of sulfuric acid include oil refining, wastewater processing, and mineral extraction.\n\nSulfur reacts directly with methane to give carbon disulfide, used to manufacture cellophane and rayon. One of the uses of elemental sulfur is in vulcanization of rubber, where polysulfide chains crosslink organic polymers. Large quantities of sulfites are used to bleach paper and to preserve dried fruit. Many surfactants and detergents (e.g. sodium lauryl sulfate) are sulfate derivatives. Calcium sulfate, gypsum, (CaSO·2HO) is mined on the scale of 100 million tonnes each year for use in Portland cement and fertilizers.\n\nWhen silver-based photography was widespread, sodium and ammonium thiosulfate were widely used as \"fixing agents.\" Sulfur is a component of gunpowder (\"black powder\").\n\nSulfur is increasingly used as a component of fertilizers. The most important form of sulfur for fertilizer is the mineral calcium sulfate. Elemental sulfur is hydrophobic (not soluble in water) and cannot be used directly by plants. Over time, soil bacteria can convert it to soluble derivatives, which can then be used by plants. Sulfur improves the efficiency of other essential plant nutrients, particularly nitrogen and phosphorus. Biologically produced sulfur particles are naturally hydrophilic due to a biopolymer coating and are easier to disperse over the land in a spray of diluted slurry, resulting in a faster uptake.\n\nThe botanical requirement for sulfur equals or exceeds the requirement for phosphorus. It is an essential nutrient for plant growth, root nodule formation of legumes, and immunity and defense systems. Sulfur deficiency has become widespread in many countries in Europe. Because atmospheric inputs of sulfur continue to decrease, the deficit in the sulfur input/output is likely to increase unless sulfur fertilizers are used.\n\nOrganosulfur compounds are used in pharmaceuticals, dyestuffs, and agrochemicals. Many drugs contain sulfur, early examples being antibacterial sulfonamides, known as \"sulfa drugs\". Sulfur is a part of many bacterial defense molecules. Most β-lactam antibiotics, including the penicillins, cephalosporins and monolactams contain sulfur.\n\nMagnesium sulfate, known as Epsom salts when in hydrated crystal form, can be used as a laxative, a bath additive, an exfoliant, magnesium supplement for plants, or (when in dehydrated form) as a desiccant.\n\nElemental sulfur is one of the oldest fungicides and pesticides. \"Dusting sulfur\", elemental sulfur in powdered form, is a common fungicide for grapes, strawberry, many vegetables and several other crops. It has a good efficacy against a wide range of powdery mildew diseases as well as black spot. In organic production, sulfur is the most important fungicide. It is the only fungicide used in organically farmed apple production against the main disease apple scab under colder conditions. Biosulfur (biologically produced elemental sulfur with hydrophilic characteristics) can also be used for these applications.\n\nStandard-formulation dusting sulfur is applied to crops with a sulfur duster or from a dusting plane. Wettable sulfur is the commercial name for dusting sulfur formulated with additional ingredients to make it water miscible. It has similar applications and is used as a fungicide against mildew and other mold-related problems with plants and soil.\n\nElemental sulfur powder is used as an \"organic\" (i.e., \"green\") insecticide (actually an acaricide) against ticks and mites. A common method of application is dusting the clothing or limbs with sulfur powder.\n\nA diluted solution of lime sulfur (made by combining calcium hydroxide with elemental sulfur in water) is used as a dip for pets to destroy ringworm (fungus), mange, and other dermatoses and parasites.\n\nSulfur candles of almost pure sulfur were burned to fumigate structures and wine barrels, but are now considered too toxic for residences.\n\nSmall amounts of sulfur dioxide gas addition (or equivalent potassium metabisulfite addition) to fermented wine to produce traces of sulfurous acid (produced when SO reacts with water) and its sulfite salts in the mixture, has been called \"the most powerful tool in winemaking.\" After the yeast-fermentation stage in winemaking, sulfites absorb oxygen and inhibit aerobic bacterial growth that otherwise would turn ethanol into acetic acid, souring the wine. Without this preservative step, indefinite refrigeration of the product before consumption is usually required. Similar methods go back into antiquity but modern historical mentions of the practice go to the fifteenth century. The practice is used by large industrial wine producers and small organic wine producers alike.\n\nSulfur dioxide and various sulfites have been used for their antioxidant antibacterial preservative properties in many other parts of the food industry. The practice has declined since reports of an allergy-like reaction of some persons to sulfites in foods.\n\nSulfur (specifically octasulfur, S) is used in pharmaceutical skin preparations for the treatment of acne and other conditions. It acts as a keratolytic agent and also kills bacteria, fungi, scabies mites and other parasites. Precipitated sulfur and colloidal sulfur are used, in form of lotions, creams, powders, soaps, and bath additives, for the treatment of acne vulgaris, acne rosacea, and seborrhoeic dermatitis.\n\nCommon adverse effects include irritation of the skin at the application site, such as dryness, stinging, itching and peeling.\n\nSulfur is converted to hydrogen sulfide (HS) through reduction, partly by bacteria. HS kills bacteria (possibly including \"Propionibacterium acnes\" which plays a role in acne,) fungi, and parasites such as scabies mites.\n\nSulfur can be used to create decorative inlays in wooden furniture. After a design has been cut into the wood, molten sulfur is poured in and then scraped away so it is flush. Sulfur inlays were particularly popular in the late 18th and early 19th centuries, notably amongst Pennsylvania German cabinetmakers. The practice soon died out, as less toxic and flammable substances were substituted. However, some modern craftsmen have occasionally revived the technique in the creation of replica pieces.\n\nSulfur is an essential component of all living cells. It is either the seventh or eighth most abundant element in the human body by weight, about equal in abundance to potassium, and slightly greater than sodium and chlorine. A human body contains about 140 grams of sulfur.\n\nIn plants and animals, the amino acids cysteine and methionine contain most of the sulfur, and the element is present in all polypeptides, proteins, and enzymes that contain these amino acids. In humans, methionine is an essential amino acid that must be ingested. However, save for the vitamins biotin and thiamine, cysteine and all sulfur-containing compounds in the human body can be synthesized from methionine. The enzyme sulfite oxidase is needed for the metabolism of methionine and cysteine in humans and animals.\n\nDisulfide bonds (S-S bonds) between cysteine residues in peptide chains are very important in protein assembly and structure. These covalent bonds between peptide chains confer extra toughness and rigidity. For example, the high strength of feathers and hair is due in part to the high content of S-S bonds with cysteine and sulfur. Eggs are high in sulfur to nourish feather formation in chicks, and the characteristic odor of rotting eggs is due to hydrogen sulfide. The high disulfide bond content of hair and feathers contributes to their indigestibility and to their characteristic disagreeable odor when burned.\n\nHomocysteine and taurine are other sulfur-containing acids that are similar in structure, but not coded by DNA, and are not part of the primary structure of proteins. Many important cellular enzymes use prosthetic groups ending with -SH moieties to handle reactions involving acyl-containing biochemicals: two common examples from basic metabolism are coenzyme A and alpha-lipoic acid. Two of the 13 classical vitamins, biotin and thiamine, contain sulfur, with the latter being named for its sulfur content.\n\nIn intracellular chemistry, sulfur operates as a carrier of reducing hydrogen and its electrons for cellular repair of oxidation. Reduced glutathione, a sulfur-containing tripeptide, is a reducing agent through its sulfhydryl (-SH) moiety derived from cysteine. The thioredoxins, a class of small proteins essential to all known life, use neighboring pairs of reduced cysteines to work as general protein reducing agents, with similar effect.\n\nMethanogenesis, the route to most of the world's methane, is a multistep biochemical transformation of carbon dioxide. This conversion requires several organosulfur cofactors. These include coenzyme M, CHSCHCHSO, the immediate precursor to methane.\n\nInorganic sulfur forms a part of iron–sulfur clusters as well as many copper, nickel, and iron proteins. Most pervasive are the ferrodoxins, which serve as electron shuttles in cells. In bacteria, the important nitrogenase enzymes contains an Fe–Mo–S cluster and is a catalyst that performs the important function of nitrogen fixation, converting atmospheric nitrogen to ammonia that can be used by microorganisms and plants to make proteins, DNA, RNA, alkaloids, and the other organic nitrogen compounds necessary for life.\n\nThe sulfur cycle was the first of the biogeochemical cycles to be discovered. In the 1880s, while studying Beggiatoa (a bacterium living in a sulfur rich environment), Sergei Winogradsky found that it oxidized hydrogen sulfide (HS) as an energy source, forming intracellular sulfur droplets. Winogradsky referred to this form of metabolism as inorgoxidation (oxidation of inorganic compounds). He continued to study it together with Selman Waksman until the 1950s.\n\nSulfur oxidizers can use as energy sources reduced sulfur compounds, including hydrogen sulfide, elemental sulfur, sulfite, thiosulfate, and various polythionates (e.g., tetrathionate). They depend on enzymes such as sulfur oxygenase and sulfite oxidase to oxidize sulfur to sulfate. Some lithotrophs can even use the energy contained in sulfur compounds to produce sugars, a process known as chemosynthesis. Some bacteria and archaea use hydrogen sulfide in place of water as the electron donor in chemosynthesis, a process similar to photosynthesis that produces sugars and utilizes oxygen as the electron acceptor. The photosynthetic green sulfur bacteria and purple sulfur bacteria and some lithotrophs use elemental oxygen to carry out such oxidization of hydrogen sulfide to produce elemental sulfur (S), oxidation state = 0. Primitive bacteria that live around deep ocean volcanic vents oxidize hydrogen sulfide in this way with oxygen; the giant tube worm is an example of a large organism that uses hydrogen sulfide (via bacteria) as food to be oxidized.\n\nThe so-called sulfate-reducing bacteria, by contrast, \"breathe sulfate\" instead of oxygen. They use organic compounds or molecular hydrogen as the energy source. They use sulfur as the electron acceptor, and reduce various oxidized sulfur compounds back into sulfide, often into hydrogen sulfide. They can grow on other partially oxidized sulfur compounds (e.g. thiosulfates, thionates, polysulfides, sulfites). The hydrogen sulfide produced by these bacteria is responsible for some of the smell of intestinal gases (flatus) and decomposition products.\n\nSulfur is absorbed by plants roots from soil as sulfate and transported as a phosphate ester. Sulfate is reduced to sulfide via sulfite before it is incorporated into cysteine and other organosulfur compounds.\n\nElemental sulfur is non-toxic, as are most of the soluble sulfate salts, such as Epsom salts. Soluble sulfate salts are poorly absorbed and laxative. When injected parenterally, they are freely filtered by the kidneys and eliminated with very little toxicity in multi-gram amounts.\n\nWhen sulfur burns in air, it produces sulfur dioxide. In water, this gas produces sulfurous acid and sulfites; sulfites are antioxidants that inhibit growth of aerobic bacteria and a useful food additive in small amounts. At high concentrations these acids harm the lungs, eyes or other tissues. In organisms without lungs such as insects or plants, sulfite in high concentration prevents respiration.\n\nSulfur trioxide (made by catalysis from sulfur dioxide) and sulfuric acid are similarly highly acidic and corrosive in the presence of water. Sulfuric acid is a strong dehydrating agent that can strip available water molecules and water components from sugar and organic tissue.\n\nThe burning of coal and/or petroleum by industry and power plants generates sulfur dioxide (SO) that reacts with atmospheric water and oxygen to produce sulfuric acid (HSO) and sulfurous acid (HSO). These acids are components of acid rain, lowering the pH of soil and freshwater bodies, sometimes resulting in substantial damage to the environment and chemical weathering of statues and structures. Fuel standards increasingly require that fuel producers extract sulfur from fossil fuels to prevent acid rain formation. This extracted and refined sulfur represents a large portion of sulfur production. In coal-fired power plants, flue gases are sometimes purified. More modern power plants that use synthesis gas extract the sulfur before they burn the gas.\n\nHydrogen sulfide is as toxic as hydrogen cyanide, and kills by the same mechanism (inhibition of the respiratory enzyme cytochrome oxidase), though hydrogen sulfide is less likely to cause surprise poisonings from small inhaled amounts because of its disagreeable odor. Hydrogen sulfide quickly deadens the sense of smell and a victim may breathe increasing quantities without noticing the increase until severe symptoms cause death. Dissolved sulfide and hydrosulfide salts are toxic by the same mechanism.\n\n\n"}
{"id": "5347767", "url": "https://en.wikipedia.org/wiki?curid=5347767", "title": "Tanga (carriage)", "text": "Tanga (carriage)\n\nA tonga or tanga (IAST ṭā̃gā, Hindi: टाँगा, Urdu: تانگا , Bengali: টাঙ্গা) is a light carriage or curricle drawn by ONE horse (compare ekka) used for transportation in India, Pakistan, and Bangladesh. They have a canopy over the carriage with a single pair of large wheels. The passengers reach the seats from the rear while the driver sits in front of the carriage. Some space is available for baggage below the carriage, between the wheels. This space is often used to carry hay for the horses.\n\nTangas were popular before the advent of automobiles and are still in use in some parts of South Asia. They are a popular mode of transportation because they are fun to ride in, and are usually cheaper to hire than a taxi or rickshaw. However, in many cities, tangas are not allowed to use highways because of their slow pace. In Pakistan, tangas are mainly found in the older parts of cities and towns, and are becoming less popular for utilitarian travel and more popular for pleasure. Tangas have become a traditional feature of weddings and other social functions in Pakistan, as well as in other nations.\n\nIn India, tangas also prevail in rural areas of North India like Uttar Pradesh, Rajasthan, Madhya Pradesh, and Punjab. Apart from the modern modes of transport, tangas are still standing in line at bus stops, railway stations to transport luggage and passengers to their destinations in small towns of North India. The culture of the tanga is disappearing due to the speed of modern transportation and the earnings people make. However, there are still some that continue to support themselves and keep the tradition alive. Tourists who come to India still take rides in tangas to experience their Indian charm. They are still among the most appreciated experiences of North India\n\n"}
{"id": "57445960", "url": "https://en.wikipedia.org/wiki?curid=57445960", "title": "Titanium foam", "text": "Titanium foam\n\nTitanium foams exhibit high specific strength, high energy absorption, excellent corrosion resistance and biocompatibility. These materials are ideally suited for applications within the aerospace industry. An inherent resistance to corrosion allows the foam to be a desirable candidate for various filtering applications. Further, titanium's physiological inertness makes its porous form a promising candidate for biomedical implantation devices. The largest advantage in fabricating titanium foams is that the mechanical and functional properties can be adjusted through manufacturing manipulations that vary porosity and cell morphology. The high appeal of titanium foams is directly correlated to a multi-industry demand for advancement in this technology.\n\nBanhart describes two dominating perspectives in which cellular metals are characterized, referring to them as atomistic and macroscopic. The atomistic (or molecular) perspective holds that a cellular material is a construction of struts, membranes, and other elements which possess mechanical properties of their bulk metal counterpart. Indeed, the physical, mechanical, and thermal properties of titanium foams are commonly measured using the same methods as that of their solid counterparts. However, special precautions must be taken due to the cellular structure of metal foams. From a macroscopic perspective, the cellular structure is perceived as a homogenous structure and characterized by considering the effective (or averaged) material parameters.\n\nAs with other metal foams, the properties of titanium foams depend mostly on the properties of the starting material and the relative density of the resultant foam.\nThermal properties in foams – such as melting point, specific heat and expansion coefficient – remain constant for both the foams and the metals from which they are composed. However, the mechanical properties of foams are greatly influenced by microstructure, which include the aforementioned properties as well as anisotropy and defects within the foam's structure.\n\nThe mechanical properties of titanium foams are sensitive to the presence of interstitial solutes, which present limitations to processing routes and utilization. Titanium has a high affinity for atmospheric gases. In foams, this is evidenced by the metal's tendency to trap oxides within cell edges.\nMicro-hardness of cell walls, elastic modulus, and yield strength increase as a result of interstitial solutes; ductility, which is a function of the quantity of interstitial impurities, is consequently reduced. Of the atmospheric gases, nitrogen has the most significant impact, followed by oxygen and carbon.\nThese impurities are often present in the precursor mixture and also introduced during processing.\n\nGibson & Ashby micromechanical models for porous materials provide mathematical equations for the prediction of mechanical parameters based on experimentally determined geometric constants. The constants of proportionality are determined by fitting experimental data to various mathematical models for structures consisting of cubes and solid struts and are dependent upon cell geometry. A limitation of the Gibson & Ashby model is that it is most accurate for foams exhibiting porosities higher than 70%, although experimental comparisons for lower porosity foams have shown agreement with this model. \nYe & Dunand found reasonable agreement to the Gibson & Ashby model for titanium foams exhibiting 42% porosity. Ultrasonic measurements provided an average Young's modulus value of 39 GPa, which is in relatively good agreement with the Gibson & Ashby prediction of 35 GPa.\n\nThe Gibson & Ashby models assume ideal structures; microstructural irregularities (e.g. inhomogeneous pore distribution; defects) are not considered. Additionally, experimental results from which the predetermined proportionality constants were based on experimental values that were obtained from simple compression tests. Consequently, they may not be applicable for multiaxial loads.\nOverall, the models are an excellent tool for general estimations of mechanical parameters; yet, precise predictive values cannot be relied upon\n\nMinimum solid area models assume that the load bearing area (cross-sectional area normal to the stress) is the logical basis for modeling mechanical behavior. MSA models assume pore interaction results in reduction of stress. Therefore, the minimum solid areas are the carriers of stress. As a result, predicted mechanical properties fluctuate based on the quantification of the solid area of the foam.\nFor titanium foams consisting of partially sintered powders, the minimum solid area consists of the neck area between powders through the cross-section of cell walls between macropores.\nThe mathematical relationships in MSA models\nare relatively consistent with the Gibson & Ashby model.\nHowever, the MSA models are designed to predict mechanical property parameters over a broader range of porosity levels. Like the Gibson & Ashby models, MSA models were derived assuming idealized (defect-free) structures containing uniform pore shapes, size and distribution.\n\nThe most frequently reported mechanical property of titanium foams is compressive strength. It was generally accepted that the compressive properties of metal foams depended on the properties of the cell wall rather than on pore size. However, more recent research has indicated that smaller pore sizes equate to higher compressive strength. As pore sizes reach nano-dimensions, the relationship is even more clear due to changes in deformation mechanism.\nTuncer & Arslan fabricated titanium foams via the space-holder method using various shaped space-holders to elucidate the effect of cell morphology on mechanical properties. They found that foams created with needle-like, carbamide space-holders exhibited a decrease in elastic modulus and yield strength when compared to spherical pores.\n\nHere all the credit goes to the Guruji of my Eklavya.\n\nMany metal foam manufacturing techniques are accomplished by the introduction of a gaseous phase into a precursor matrix, which can occur in either molten metal or a powdered metal form. Due to titanium's high melting point (1670 °C) and high chemical affinity with oxygen, nitrogen, carbon and hydrogen (which dissolve rapidly either in liquid or solid titanium at a temperature above 400 °C), solid-state processes based on powder densification are the preferred method of fabrication.\nProcessing methods must also be designed to avoid exposure to air or moisture; vacuum or inert gas sintering processes are usually sufficient for preventing contamination.\n\nUtilizing powder metallurgy routes for titanium foam fabrication allows for production at lower temperatures than those required through a melt process and reduces overall risks for contamination. In loose-powder sintering (also known as gravity sintering), pores are created through diffusion bonding arising from the voids existing between packed powder particles. Axial compaction followed by sintering follows the same procedure as above, but pressures is applied for compaction of the precursor material. For both compaction methods, the resulting pore morphology is dependent upon the morphology of the metallic powder, making it difficult to control the size, shape, and distribution of the pores. Another disadvantage includes the relatively high probability of pore collapse and limited achievable porosity levels.\n\nTo produce titanium foams via expansion of pressurized gas, the titanium precursor mixture is placed within a gas-tight metal can, which is evacuated after filling. The metal can is pressurized with inert gas—most commonly argon – and is pressed isostatically. The gas-filled pores are contained within the compacted matrix, and upon exposure to elevated temperatures, these bubbles expand through creep of the surrounding metal matrix. Since processing titanium foams using hot isostatic pressing (HIP) eliminates the need for separate compaction and sintering processes, a wider variety of custom shapes and sizes are possible than via loose powder sintering techniques. Disadvantages of this process include reduced pore connectivity, limited achievable porosity, and a complicated experimental set-up. However, a unique aspect of the HIP process with respect to titanium (and other polymorphic materials) is that transformation superplasticity can be enhanced through the HIP process by way of thermal cycling, or by cycling around the alpha/beta allotropic temperature boundaries of the metal.\n\nTitanium undergoes allotropic transformation from its α-phase (hexagonal close-packed (HCP) structure at temperatures less than 882.5 °C) to its β-phase (body centered cubic (BCC) structure at temperatures above 882.3 °C). Alpha-phase titanium products typically exhibit medium to high strength with excellent creep strength, whereas beta-phase titanium products typically exhibit very high strength and low ductility. Foams created under thermal cycling conditions have been shown to exhibit increased porosity due to the density difference between allotropic phases. Davis, Teisen, Schuh & Dunand produced titanium foams with 41% porosity (as compared to 27% porosity through the normal HIP creep mechanism). Increases in overall ductility were also observed in foams created through thermal cycling. In a similar experiment, porosity of 44% was achieved and determined as the maximum achievable porosity under thermal cycling conditions. A later study also utilized exploitation of transformation superplasticity conditions through HIP, but in this case, the titanium powder in the precursor matrix was replaced with titanium wires to create anisotropic pores. The resulting anisotropic pores showed closer correlation with natural bone in that the foams exhibited higher elastic moduli, yield strength and deformation when subjected to longitudinally loaded forces than when loads were applied transversely.\n\nThe space-holder technique is the most commonly employed method for producing titanium foams.\n\nThe space-holder technique allows for the fabrication of higher porosity foams (35-80% ) than other techniques, while also giving the engineer more control over pore fraction, shape and connectivity. Mechanical properties can be adjusted through the size, shape and quantity of space-holders employed. The space-holder technique was first demonstrated by Zhao and Sun for the fabrication of aluminum foams in a powder metallurgical method, which consisted of the incorporation of NaCl as a space-holder. The space-holder was mixed into the powder mixture and dissolved prior to sintering. The same method was used to create titanium foams for the first time when Wen et al. utilized ammonium hydrogen carbonate spacers.\n\nThe size and shape of the metal powder has a direct impact on the stability of the precursor as well as the resulting foam. For this purpose, powders that increase packing efficiency are most advantageous. The use of spherical particles may result in less contact of particles which consequently leads to larger secondary pores and a higher probability of pore collapse prior to complete sintering. This factor can be limited through different compaction techniques that decrease the degree of interstitial sites around the titanium particles. However, this method also has limitations; for example, the powders cannot be compacted to such a degree that would promote deformation of the spacer (unless anisotropic pore shape is desired).\n\nThe selection of the space-holder is one of the most crucial steps because it defines many of the properties of the resulting foam, including cell shape, cell size and macroporosity. The space-holder should be inert and represent the size and shape of the desired pores. The porosity may be adjusted anywhere between 50 and 85% without the filler material becoming a part of the resultant foam. It is also important to select a spacer that has limited or no solubility in titanium, as this incorporation will affect the mechanical properties of the resulting foam. It is also possible to produce gradient structures in which the porosities differ within the foam based on the differences in spacer size in the preform.\n\nThe degree of homogeneity in pore distribution of the final product is primarily dependent on the adequacy of mixing of the precursor. The difference in particle size between the titanium powders and the spacers directly impacts the ability to adequately mix the preform. The greater the size difference, the more difficult it is to control this process. Nonhomogeneous mixing resulting from the use of spacers considerably larger than the titanium particles employed and has shown adverse effects in the stability of the precursor after removal of spacer and in the distribution of porosity.\nSpacer size has been investigated by Tuncer & Sharma in three different studies. It was shown that the use of a coarse spacer results in thicker pore walls while the use of finer spacers results in enhanced compaction, leading to increased densification. Increased densification is evidenced by a monomodal pore distribution with the employment of fine spacers and a bimodal distribution using coarse spacers. Further, finer spacers result in a more homogenous pore distribution. Sharma et al. utilized acicular spacers and achieved porosities up to 60% where pores were undistorted. In samples employing fine particles, porosities up to 70% were achievable before noting distortion in the pores. However, the bimodal pore distribution observed in coarse-spacer samples showed to be beneficial in terms of mechanical properties in that higher compressive strengths were observed, beyond those that might exist due to the inverse relationship of porosity and compressive strength alone.\n\nThe precursor mixture of powders and space-holders are compacted into a mold under a specified pressure. This can be achieved through uniaxial or isostatic processes. The pores resulting from this method are open and interconnected via windows between neighboring pores with the size of the pores partially dependent upon the coordination number and contact area of the resulting compact. Compaction pressure must be high enough to ensure sufficient mechanical strength for retention of pore geometry specified by the space-holder, but not too high enough as to cause deformation of the space-holder.\n\nSpacers can be categorized by their mode of removal: those which are removed thermally when temperature is raised during sintering and those which are dissolved with a solvent, either prior to or after foam formation. For spacers removed via thermal mechanisms, the sintering temperature should be high enough to decompose the spacer, but not high enough to induce diffusional bonding between the metal particles. This results in a very fragile compact prior to higher temperature sintering and increases risk of collapse. When employing dissolvable spacers, it is possible to remove the spacer after sintering, which reduces the risk of pore collapse. \nIn most cases, foams created using space-holders contain bimodal pore distributions with macro-sized pores resulting from the space-holder particles and micro-sized pores located on the pore walls and resulting from incomplete sintering of the powder matrix. As a result, the macropores typically exhibit rough internal surfaces. In some applications, such as for the use of bio-medical implants, this is an advantageous property. Inner porosity (or micro-porosity) has been shown to reduce stiffness; thus, reduce the risk of stress-shielding effects, while also offering improved osseointegration.\n\nSodium chloride is the most commonly chosen space-holder for titanium foams\n\nBansiddhi & Dunand first demonstrated NaCl as a permanent space-holder for the fabrication of NiTi foams. The resulting foams consisted of 32-36% porosity with more complete densification than they observed when producing NiTi foams using a sodium fluoride (NaF) space-holder. However, processing parameters resulted in molten NaCl and a metal/salt blend in the cavities of the foam. Certain risks are associated with using a molten space-holder including reaction with the metal, dissolving of the space-holder in the metal and prevention of densification through the creation of a thin layer of liquid between the metal and particles. Near complete densification was achieved when NaCl was used as a permanent space-holder in pure titanium foam. In this case, a temperature below the melting point of NaCl was used; titanium is less creep resistant than NiTi, which allows for densification at lower temperatures. The resulting foams achieved porosity of 50-67% with minimal observable microporosity. Anisotropic pore shape in some areas alluded to NaCl's deformation during HIP, which is desirable for some applications. Additionally, an observed, rough inner surface of the pores holds advantages for biomedical implant applications. \nJha et al. achieved 65-80% porosity through the use of NaCl as a space-holder and a cold compaction process at various pressures with two-stage sintering. In this case, NaCl was removed through dissolution after the second stage of sintering. Resulting Young's moduli (8-15 GPa) were considerably lower than the Young's modulus of 29 GPa achieved by Ye & Dunand for 50% porosity foams. This illustrates the known relationship between porosity and Young's modulus wherein Young's modulus decreases linearly with increasing porosity. Achievable porosity through the space-holder method is directly related to the type and amount of space-holder utilized (up to a threshold maximum achievable porosity level).\n\nMagnesium can be removed either thermally or by reactive measures through the dissolution in acid.\n\nAnother commonly employed space-holder for titanium foams is carbamide; demonstrated porosities range from 20-75%. Wen et al. produced foams exhibiting a bimodal pore distribution with porosities ranging from 55-75%, Young's moduli between 3-6.4 GPa, and a plateau stress of 10-35 MPa. An inverse relationship between plateau stress and porosity was observed with increased porosity resulting in decreased plateau stress. Tuncer et al. utilized carbamide in combination with irregular shaped titanium powders in an effort to increase green strength through increased packing efficiency (of particles). This also eliminated the need for the incorporation of a binder.\n\nTapioca starch can be burnt off easily through the sintering process and is insoluble in titanium. Titanium foams consisting of a bimodal pore distribution (macropores ranging from 100-300 μm) and 64-79% porosity, exhibited yield strengths of 23-41 MPa and Young's moduli of 1.6-3.7 GPa.\n\nAlthough ammonium bicarbonate has been utilized in the manufacturing of titanium foams, it is not an ideal spacer in that it has a low melting/dissociation point and some solubility in titanium. This results in considerable shrinkage which makes control of pore shape difficult. Furthermore, the decomposition releases environmentally harmful gases.\n\nFreeze-casting is a directional solidification technique that is utilized to fabricate materials exhibiting anisotropic, elongated pore structures. Pore morphology is defined, in large part, by the morphology of the solidified fluid.\nTitanium foams exhibiting dendritic\nand lamellar\npore structures have been produced, through the use of non-aqueous and aqueous processing respectively. These materials exhibit anisotropic mechanical properties as a result of their anisotropic pore structures. Compressive strength for loads applied parallel to the wall direction of titanium foams are found to be, on average, 2.5 times greater than for those applied perpendicular to the wall direction.\n\nPotential structural applications for titanium foams include their general incorporation into light-weight structures and as components for mechanical energy absorption. The most important considerations for the use of titanium foams in structural applications includes their porosity, specific strength, ductility in compression and cost. Because of low manufacturing costs, most metal foams marketed for structural applications are of a close-celled aluminum variety. In comparison, titanium foam manufacturing incurs a higher cost, but this cost is defensible in space applications where the material offers an otherwise incomparable reduction in overall weight. The lower thermal conductivity of titanium may also be appreciated in rocket construction. The specific strength, overall energy absorbing capability and high melting point all reinforce titanium's superiority to aluminum in aerospace and military applications. When used for aerospace applications, levels of porosity close to 90% are desired.\nTitanium foams are capable of retaining their high tensile strength at temperatures up to 400 °C; a limit imposed by the metal's low resistance to oxidation.\n\nThe driving force for titanium foam's replacement of existing materials in the aerospace sector results from the following five factors:\n\n• Weight reduction: as a substitute for steels and nickel-based superalloys;\n\n• Application temperature: as a substitute for aluminum and nickel-based alloys and steels\n\n• Corrosion resistance: as a substitute for aluminum alloys and low-alloyed steels\n\n• Galvanic compatibility: with polymer matrix composites as substitutes for aluminum alloys\n\n• Space constraints: as substitutes for aluminum alloys and steels\n\nThe most urgent problem of engineering and its advanced branch of aerospace engineering is\nthe efficient use of materials as well as increased service life.\n\nSandwich panel cores are used throughout the aerospace industry; they are integrated within aircraft bodies, floors and internal panels. Sandwich constructions consist of two faces separated by a thick, light-weight core and are most commonly composed of balsa-wood, foamed polymers, glue-bonded aluminum or Nomex (paper) honeycombs. Typically, the cores are combined with reinforcing fibers to increase their shear modulus. Indeed, carbon fiber-reinforced polymers exhibit the highest specific stiffness and strength of these materials. However, polymers decompose at low temperatures; thus employment of the aforementioned materials pose inherent challenges due to the limited range of temperature they may be utilized within as well as their moisture-dependent properties.\nThe largest and most inadequately predicted failure within the core results from strain localization. Strain localization refers to the development of bands exhibiting intensive straining as a result of the localization of deformations in the solid.\n\nTitanium foams exhibiting auxetic pore structures are of interest for incorporation in sandwich panel cores due to their enhanced shear performance.\n\nTitanium alloys are the choice material for a diverse range of biomedical implants.\nCurrently employed titanium alloy implants include: hip joints,\nbone screws,\nknee joints, spinal fusions, shoulder joints, and bone plates. These alloys range from high ductility, commercially-pure titanium foams with high formability, to heat-treatable alloys with high strength. Titanium is well-suited for use in Magnetic Resonance Imaging (MRI) and Computed Tomography (CT),\n\nBiomedical implants should have low density for patient comfort and high porosity and surface area to facilitate vascularization and the ingrowth of new bone. Ideally, the implant will allow sufficiently easy fluid flow for cell nutrition and osteoblast multiplication as well as migration for cellular colonization of the implant to become uniform. The pores contained within the foam's cellular matrix mimic the extracellular matrix of bone, allowing the body to fixate with the implant. The porosity of the implant also promotes apposition and facilitates vascularization−as cells are able to attach, reproduce and form basic functions. It has been shown that a macropore size of 200-500 µm is preferred for ingrowths of new bone tissues and transportation of body fluids. The lower bound is controlled by the size of cells (~20 µm), and the upper bound is related to the specific surface area through the availability of binding sites. Finer pores further help in tissue growth and biofluid movement. Anisotropic, elongated pores (such as those attainable via the freeze-casting technique) may be beneficial in bone implants in that they can further mimic the structure of bone.\n\nThe porous surface geometry of the foam promotes bone in-growth, provides anchorage for fixation, and ensures stresses are transferred from the implant to the bone. Surface roughness in the pore can enhance bone in-growth, and coarser cell size facilitates faster tissue growth.\nTo optimize the implant's functionality and ability to successfully fuse with bone, it may be necessary to manipulate the material's manufacturing methods in order to modify the foam's pore structure. Changes in pore structure can directly influence implant strength as well as other key properties.\n\nHuman cancellous bone possesses a stiffness ranging from 12-23 GPa; careful control and modification of manufacturing parameters to achieve similar strengths is imperative for practicality of integration. Correctly predicting the Young's modulus for foams is imperative for actual biomedical integration; a mismatch of Young's moduli between the implant and the bone can result in stress-shielding effects from a disproportional handling of stress. The implant which typically exhibits a higher Young's modulus than the bone will absorb most of the load. As a result of this imbalance, the starting bone density will be reduced, there will be tissue death and, eventually, implant failure.\n\nNatural bone exhibits the ability to adjust local fiber away from the low-stress regions toward high stress regions through the distribution of porosity, thus maximizing overall comfort. Using finite element analysis, researchers examined the effect of filling pores with bone on mechanical properties. They concluded that bone ingrowth significantly improved the mechanical properties, evidenced by decreased localized plasticity and stress concentrations. In effect, the titanium foam in the study allowed the bone to exhibit its natural ability to adjust local fiber away from the low-stress regions toward high stress regions.\n\nPrevious experimentation and analysis has demonstrated that random combinations of pore size and shape result in lower Young's moduli. Theoretical models for the quantification of Young's moduli do not account for random pore size and shape distribution, so experimental measurements must be conducted in the presence of heterogeneous pore size and distribution. This is a limitation of the micro-mechanical models discussed above.\n\nCurrently utilized implants take a great deal of time to integrate with the body after the initial surgical procedure occurs. True adhesion between the implant and the bone has been difficult to achieve and, unfortunately, success rates of implant fixation are low due to the implant's failure to achieve long-term osseointegration into the bone.\n\nThe problem of osseointegration is best understood by examining the process of natural bone growth. In the body, bone and tissues experience self-regeneration, and structural modifications occur normally in response to environmental stimuli. Successful osseointegration occurs in three main stages that follow a natural biologically determined procedure: (1) incorporation of the implant into the bone's formation, (2) adaption of the new bone mass to carry weight and (3) remodeling of the new bone structure. The first stage in this process is the most crucial for overall success; the implant and the bone must form a rapid connection, and this bond must be strong and enduring. Due to its porous structure, a titanium metal foam implant may be able to achieve close fixation with the bone and will decrease patient recovery time considerably. Essentially, the foam becomes an extracellular matrix in the body as tissue is integrated into it. Today, the implants most commonly used for bone replacement lack the ability to promote these characteristics, which are found in natural bone and, as a result, the implants have limited lifetimes. This phenomenon of osseointegration works similarly to direct fracture healing. However, instead of a bone fragment-end reconnecting to bone, the fragment-end connects to an implant surface. In a study on fibroblastic interactions with high porosity Ti6Al4V, the metal foam was supportive of cell attachment and proliferation, migration through the porous network, and proved capable of sustaining a large cell population.\n\nTitanium's propensity to form an oxide layer on its surface prevents corrosion of surfaces that are in contact with human tissues because the surface oxides minimize diffusion of metal ions from the bulk material to the surface. When titanium gains a coating to make it more bioactive, it can turn the already biocompatible titanium surface into an interface able to enhance osteoblast adhesion and able to promote osseointegration. Today, research is heavily focused on improving the success rate of integration and uses an understanding of the natural process of bone growth and repair to create coatings that will enhance the surface finish and surface properties of the implant. These adjustments allow the artificial structure to mimic biological materials and to gain acceptance into the body with fewer negative side effects.\n\nA 3-year clinical and radiographic study found implants in humans coated by nanocrystalline HA to support osseointegration. The nanocrystalline HA was developed with a large rough surface of interconnecting pores between 10 and 20 nm of the silica matrix gel, resulting in a porous bone structure. Mean rates of marginal bone loss were insignificant and the periotest values were indicative of a solid osseointegration.\nIn effect, the pores are structured in such a way that they are able hold onto the proteins on the biomaterial's surface. Ideally, this allows the body to engage in self-repair in that the synthetic HA is recognized as a like-nanomaterial in which live tissues may develop\nTitanium foams can be coated with HA through various methods including plasma spraying, sol-gel and electrophoretic deposition. It has been shown that HA-coated titanium exhibits increased interfacial strength in comparison to titanium foams without the coating. In an effort to enhance bone in-growth, Spoerke et al. developed a method for growing organoapatites on titanium implants. Organoapatites may assist in-bone in-growth at the implant interface. The foams were manufactured using a modified HIP process, which exploits the allotropic nature of titanium to create higher porosity foams. Previous in vitro experimentation with the organoapatite-titanium foam held promising results including the possibility that ingrown tissue within these coated pores will improve the lifetime use of the foam through reduction of stress-shielding effects.\n\nIn the lab, synthetic nanocrystalline bone grafting material in mice has shown in-growth of vascularized fibrous tissue which resulted in improved healing. Furthermore, new blood vessels were observed at day 5 after implantation, and the implant showed a high functional vessel density. In a study examining the femoral epiphyses of rabbits in two to eight weeks of healing, bone-to-implant contact was compared to bone growth inside the chambers for four different implant surfaces. The researchers found that bone substitute materials may improve the bone apposition onto titanium.\n\n\n"}
{"id": "43589512", "url": "https://en.wikipedia.org/wiki?curid=43589512", "title": "Two-dimensional materials", "text": "Two-dimensional materials\n\n2D Materials, sometimes referred to as single layer materials, are crystalline materials consisting of a single layer of atoms. These materials have found use in applications such as photovoltaics, semiconductors, electrodes and water purification.\n\n2D materials can generally be categorised as either 2D allotropes of various elements or compounds (consisting of two or more covalently bonding elements). The elemental 2D materials generally carry the -ene suffix in their names while the compounds have -ane or -ide suffixes. Layered combinations of different 2D materials are generally called van der Waals heterostructures.\n\nThe efficient integration of 2D functional layers with three-dimensional (3D) systems remains a significant challenge, limiting device performance and circuit design.\n\nSome 700 2D materials have been predicted to be stable, although many remain to be synthesized. The global market for 2D materials is expected to reach US$390 million by 2025, mostly for graphene in the semiconductor, electronics, battery energy and composite materials markets.\nThe first 2D material, graphene, a single layer of graphite, was isolated in 2004. Thereafter many other 2D materials were identified.\n\nThe first MXene was discovered in 2011 at Drexel University.\n\nGraphene is a crystalline allotrope of carbon in the form of a nearly transparent (to visible light) one atom thick sheet. It is hundreds of times stronger than most steels by weight. It has the highest known thermal and electrical conductivity, displaying current densities 1,000,000 times that of copper. It was first produced in 2004.\n\nAndre Geim and Konstantin Novoselov won the 2010 Nobel Prize in Physics \"for groundbreaking experiments regarding the two-dimensional material graphene\". They first produced it by lifting graphene flakes from bulk graphite with adhesive tape and then transferring them onto a silicon wafer.\n\nGraphyne is another 2-dimensional carbon allotrope whose structure is similar to graphene's. It can be seen as a lattice of benzene rings connected by acetylene bonds. Depending on the content of the acetylene groups, graphyne can be considered a mixed hybridization, sp, where 1 < n < 2, and versus graphene's (pure sp) and diamond (pure sp).\n\nFirst-principle calculations using phonon dispersion curves and ab-initio finite temperature, quantum mechanical molecular dynamics simulations showed graphyne and its boron nitride analogues to be stable.\n\nThe existence of graphyne was conjectured before 1960. It has not yet been synthesized. However, graphdiyne (graphyne with diacetylene groups) was synthesized on copper substrates. Recently it has been claimed to be a competitor for graphene, due to the potential of direction-dependent Dirac cones.\n\nBorophene is a crystalline atomic monolayer of boron and also known as \"boron sheet\". \nFirst predicted by theory in the mid-1990s, \ndifferent borophene structures were experimentally confirmed in 2015.\n\nGermanene is a two-dimensional allotrope of germanium, with a buckled honeycomb structure. \nExperimentally synthesized germanene exhibits a honeycomb structure. \nThis honeycomb structure consists of two hexagonal sub-lattices that are vertically displaced by 0.2 A from each other.\n\nSilicene is a two-dimensional allotrope of silicon, with a hexagonal honeycomb structure similar to that of graphene.\n\nIn 2016 researchers predicted a 2d hexagonal, metallic allotrope of SiBN with only sp bonds.\n\nStanene is a predicted topological insulator that may display dissipationless currents at its edges near room temperature. It is composed of tin atoms arranged in a single layer, in a manner similar to graphene. Its buckled structure leads to high reactivity against common air pollutions such as NOx and COx and is able to trap and dissociate them at low temperature.\n\nPhosphorene is a 2-dimensional, crystalline allotrope of phosphorus. Its mono-atomic hexagonal structure makes it conceptually similar to graphene. However, phosphorene has substantially different electronic properties; in particular it possesses a nonzero band gap while displaying high electron mobility. This property potentially makes it a better semiconductor than graphene. \nThe synthesis of phosphorene mainly consists of micromechanical cleavage or liquid phase exfoliation methods. The former has a low yield while the latter produce free standing nanosheets in solvent and not on the solid support. The bottom up approaches like chemical vapor deposition are still blank because of its high reactivity. Therefore, in the current scenario, the most effective method for large area fabrication of thin films of phosphorene consists of wet assembly techniques like Langmuir-Blodgett involving the assembly followed by deposition of nanosheets on solid supports\n\nBismuthene, the two-dimensional allotrope of bismuth, was synthesized on silicon carbide in 2016 and is predicted to be a topological insulator. At first glance the system is similar to graphene, as the Bi atoms arrange in a honeycomb lattice. However the bandgap is as large as 800mV due to the large spin-orbit-coupling of the Bi atoms and their interaction with the substrate. Thus, room-temperature applications of the quantum spin Hall effect come into reach.\n\nSingle and double atom layers of platinum in a two-dimensional film geometry has been demonstrated. These atomically thin platinum films are epitaxially grown on graphene which imposes a compressive strain that modifies the surface chemistry of the platinum, while also allowing charge transfer through the graphene. Single atom layer of palladium with the thickness down to 2.6 Å, and rhodium with the thickness of less than 4 Å have also been synthesized and characterized with atomic force microscopy and transmission electron microscopy.\n\nTwo-dimensional alloys is single atomic layer of alloy that is incommensurate with underlying substrate. The 2D ordered alloy of Pb and Sn has been synthesized and characterized with scanning tunneling microscopy and low-energy electron diffraction in 2003. Moreover, the 2D all proportional solid solution alloy of Pb and Bi has been synthesized in 2011.\n\nThe supracrystals of 2D materials have been proposed and theoretically simulated. These monolayer crystals are built of supra atomic periodic structures where atoms in the nodes of the lattice are replaced by symmetric complexes. For example, in the hexagonal structure of graphene patterns of 4 or 6 carbon atoms would be arranged hexagonally instead of single atoms, as the repeating node in the unit cell.\n\nGraphane is a polymer of carbon and hydrogen with the formula unit where \"n\" is large. Graphane is a form of fully hydrogenated (on both sides) graphene. Partial hydrogenation is then hydrogenated graphene.\n\nGraphane's carbon bonds are in \"sp\" configuration, as opposed to graphene's \"sp\" bond configuration. Thus graphane is a two-dimensional analog of cubic diamond.\n\nThe first theoretical description of graphane was reported in 2003 and its preparation was reported in 2009.\n\nGraphane can be formed by electrolytic hydrogenation of graphene, few-layer graphene or high-oriented pyrolytic graphite. In the last case mechanical exfoliation of hydrogenated top layers can be used.\n\np-doped graphane is postulated to be a high-temperature BCS theory superconductor with a T above 90 K.\n\n2D boron nitride is an sp conjugated compound that forms a honeycomb structure of alternating boron and nitrogen atoms with a lattice spacing of 1.45Å. It adopts the hexagonal (h-BN) allotrope of the three possible crystalline forms of boron nitride because it is the most ubiquitous and stable structure. Boron nitride nanosheets contain two different edges. In the armchair edge structure, the edge consists of either boron or nitrogen atoms. In the zig-zag edge structure, the edge consists of alternating boron and nitrogen atoms. These 2D structures can stack on top of each other and are held by Van der Waal forces to form what is called few-layer boron nitride nanosheets. In these structures, the boron atoms of one sheet are positioned on top or below the nitrogen atoms due to electron deficient nature of boron and electron rich nature of nitrogen, respectively. Due to several similar structural similarities with graphene, boron nitride nanosheets are considered graphene analogs, often called “white graphene\".\n\nBoron nanosheets (BNNS) defined as single or few layers of boron nitride whose aspect ratio is small. However, there are a couple variations of 2D boron nitride structure. Boron nitride nanoribbons (BNNR) are boron nitride nanosheets with significant edge effects and have widths that are smaller than 50 nanometers. Boron nitride nanomeshes (BNNM) are boron nitride nanosheets that are placed upon specific metal substrates.\n\nBoron nitride nanosheets have a wide bandgap that ranges from 5 to 6 eV and can be changed by the presence of Stone-Wales defects within the structure, by doping or functionalization, or by changing the number of layers. Due to this large bandgap and tunability as well as its surface flatness, boron nitride nanosheets are considered to be an excellent electric insulators and are often used as dielectrics in electrical devices.\n\n2D boron nitride structures are excellent thermal conductors, with a thermal conductivity range of 100-270 W/mK. It has been suggested that single layer boron nitride nanosheets have a greater thermal conductivity than other forms of boron nitride nanosheets due to decreased phonon scattering from subsequent layers.\n\nThe thermal stability of boron nitride nanosheets is very high due to the high thermal stability properties of hexagonal boron nitride. As single layer and few-layer boron nitride nanosheets begin to oxidize and lose their electrical properties at 800 °C, they are excelle\n\nChemical vapor deposition is a popular synthesis method to produce boron nitride because it is a highly controllable process that produces high quality and defect free monolayer and few-layer boron nitride nanosheets. In the majority of CVD methods, boron and nitride precursors react with a metal substrate at high temperature. This allows for nanosheets of a large area as the layers grow uniformly on the substrate. There is a wide range of boron and nitride precursors such as borazine and selection of these precursors depend on factors such as toxicity, stability, reactivity, and the nature of the CVD method. However, despite the high quality of the nanosheets synthesized by chemical vapor deposition, it is not a good method for the large scale production of boron nitride nanosheets for applications.\n\nWhile there are several mechanical cleaving methods to produce boron nitride nanosheets, they employ the same principle: using shear forces to break the Van der Waals interactions between the layers of boron nitride. The advantage of mechanical cleavage is that the nanosheets isolated from these techniques have few defects and retain the lateral size of the original substrate.\n\nInspired by its use in the isolation of graphene, micromechanical cleavage, also known as the Scotch-tape method, has been used to consistently isolate few-layer and monolayer boron nitride nanosheets by subsequent exfoliation of the beginning material with adhesive tape. However, the disadvantage of this technique is that it is not scalable for large scale production.\n\nBall milling is another technique used to mechanically exfoliate boron nitride sheets from the parent substrate. In this process, shear forces are applied on the face of bulk boron nitride by rolling balls, which break the Van der Waal interactions between each layer. While the ball milling technique may allow for large quantities of boron nitride nanosheets, it does not allow for control the size or the number of layers of the resulting nanosheets. Furthermore, these nanosheets have more defects due to the aggressive nature of this technique. However, improvements such as the addition of a milling agent such as benzyl benzoate or the use of smaller balls has allowed for a greater yield of higher quality nanosheets.\n\nBoron nitride nanosheets have also been isolated by using a vortex fluidic device, which uses centripetal force to shear off layers of boron nitride.\n\nBoron nitride nanosheets may also be synthesized by the unzipping of boron nitride nanotubes (BNNT). These nanotubes can be made into sheets by breaking the bonds connecting the N and B atoms by potassium intercalation or by etching by plasma or an inert gas. The unzipping of boron nitride nanotubes by plasma can be used to control the size of the nanosheets, but it produces semiconducting boron nitride nanosheets. The potassium intercalation method produces a low yield of nanosheets as boron nitride is resistive to the effects of intercalants.\n\nSolvent exfoliation is often used in tandem with sonication to break the weak Van der Waals interactions present in bulk boron nitride to isolate large quantities of boron nitride nanosheets. Polar solvents such as isopropyl alcohol and DMF have been found to be more effective in exfoliating boron nitride layers than nonpolar solvents because these solvents possess a similar surface energy to the surface energy of boron nitride nanosheets. Combinations of different solvents also exfoliate boron nitride better than when the solvents were used individually. However, many solvents that can be used to exfoliate boron nitride are fairly toxic and expensive. Common solvents such as water and isopropyl alcohol have been determined to be comparable to these toxic polar solvents in exfoliating boron nitride sheets.\n\nChemical functionalization of boron nitride involves attaching molecules onto the outer and inner layers of bulk boron nitride. There are three types of functionalization that can be done to boron nitride: covalent functionalization, ionic functionalization, or non covalent functionalization. Layers are then exfoliated by placing the functionalized boron nitride into a solvent and allow the solvation force between the attached groups and the solvent to overcome the Van der Waal forces present in each layer. This method is slightly different than solvent exfoliation as solvent exfoliation relies on the similarities between the surface energies of the solvent and boron nitride layers to overcome the Van der Waals interactions.\n\nThe reaction of a mixture of boron and nitrogen precursors at high temperature can produce boron nitride nanosheets. In one method, boric acid and urea were reacted together at 900˚C. The numbers of layers of these nanosheets were controlled by the urea content as well as the temperature\n\n\"See also: graphene, boron nitride\"\n\nBorocarbonitrides are two-dimensional compounds that are synthesized such that they contain boron, nitrogen, and carbon atoms in a ratio BCN Borocarbonitrides are distinct from B,N co-doped graphene in that the former contains separate boron nitride\n\nand graphene domains as well as rings with B-C, B-N, C-N, and C-C bonds. These compounds generally have a high surface area, but borocarbonitrides synthesized from a high surface area carbon material, urea, and boric acid tend to have the highest surface areas. This high surface area coupled with the presence of Stone-Wales defects in the structure of borocarbonitrides also allows for high absorption of CO and CH which may make borocarbonitride compounds a useful material in sequestering these gases\n\nThe band gap of borocarbonitrides range from 1.0-3.9eV and is dependent on the content of the carbon and boron nitride domains as they have different electrical properties. Borocarbonitrides with a high carbon content have lower bandgaps whereas those with higher content of boron nitride domains have higher band gaps. Borocarbonitrides synthesized in gas or solid reactions also tend to have large bandgaps and are more insulating in character. The wide range of composition of boronitrides allows for the tuning of the bandgap, which when coupled with its high surface area and Stone-Wales defects may make boronitrides a promising material in electrical devices.\n\nA high surface area carbon material such as activated charcoal, boric acid, and urea are mixed together and then heated at high temperatures to synthesize borocarbonitride. The composition of the resulting compounds may be changed by varying the concentration of the reagents as well as the temperature.\n\nIn chemical vapor deposition, boron, nitrogen, and carbon precursors react at high heat and are deposited onto a metal substrate. Varying the concentration of precursors and the selection of certain precursors will give different ratios of boron, nitrogen, and carbon in the resulting borocarbonitride compound.\n\nBorocarbonitride can also be synthesized by random stacking of boronitride and graphene domains through covalent interactions or through liquid interactions. In the first method, graphene and boron nitride sheets are functionalized and then are reacted to form layers of borocarbonitride. In the second method, boron nitride and graphite powder are dissolved in isopropanol and dimethylformamide, respectively, and then sonicated. This is then exfoliated to isolate borocarbonitride layers.\n\nGermanane is a single-layer crystal composed of germanium with one hydrogen bonded in the z-direction for each atom. Germanane’s structure is similar to graphane, Bulk germanium does not adopt this structure. Germanane is produced in a two-step route starting with calcium germanide. From this material, the calcium (Ca) is removed by de-intercalation with HCl to give a layered solid with the empirical formula GeH. The Ca sites in Zintyl-phase interchange with the hydrogen atoms in the HCl solution, producing GeH and CaCl2.\n\nMolybdenum disulfide monolayers consist of a unit of one layer of molybdenum atoms covalently bonded to two layers of sulfur atoms. While bulk molybdenum sulfide exists as 1T, 2H, or 3R polymorphs, molybdenum disulfide monolayers are found only in the 1T or 2H form. The 2H form adopts a trigonal prismatic geometry while the 1T form adopts an octahedral or trigonal antiprismatic geometry. Molybdenum monolayers can also can be stacked due to Van der Waals interactions between each layer.\n\nThe electrical properties of molybdenum sulfide in electrical devices depends on factors such as the number of layers, the synthesis method, the nature of the substrate on which the monolayers are placed on, and mechanical strain.\n\nAs the number of layers decrease, the band gap begins to increase from 1.2eV in the bulk material up to a value of 1.9eV for a monolayer. Odd number of molybdenum sulfide layers also produce different electrical properties than even numbers of molybednum sulfide layers due to cyclic stretching and releasing present in the odd number of layers. Molybdenum sulfide is a p-type material, but it shows ambipolar behavior when molybdenum sulfide monolayers that were 15 nm thick were used in transistors. However, most electrical devices containing molybdenum sulfide monolayers tend to show n-type behavior.\n\nThe band gap of molybdenum disulfide monolayers can also be adjusted by applying mechanical strain or an electrical field. Increasing mechanical strain shifts the phonon modes of the molybdenum sulfide layers. This results in a decrease of the band gap and metal-to-insulator transition. Applying an electric field of 2-3Vnm also decreases the indirect bandgap of molybdenum sulfide bilayers to zero.\n\nSolution phase lithium intercalation and exfolation of bulk molybdenum sulfide produces molybdenum sulfide layers with metallic and semiconducting character due to the distribution of 1T and 2H geometries within the material. This is due to the two forms of molybdenum sulfide monolayers having different electrical properties. The 1T polymorph of molybednum sulfide is metallic in character while the 2H form is more semiconducting. However, molybdenum disulfide layers produced by electrochemical lithium intercalation are predominantly 1T and thus metallic in character as there is no conversion to the 2H form from the 1T form.\n\nThe thermal conductivity of molybdenum disulfide monolayers at room temperature is 34.5W/mK while the thermal conductivity of few-layer molybdenum disulfide is 52W/mK. The thermal conductivity of graphene, on the other hand, is 5300W/mK. Due to the rather low thermal conductivity of molybdenum disulfide nanomaterials, it is not as promising material for high thermal applications as some other 2D materials.\n\nExfoliation techniques for the isolating of molybdenum disulfide monolayers include mechanical exfoliation, solvent assisted exfoliation, and chemical exfolation.\n\nSolvent assisted exfoliation is done by sonicating bulk molybdenum disulfide in an organic solvent such as isopropanol and N-methyl-2-pyrrolidone, which disperses the bulk material into nanosheets as the Van der Waals interactions between the layers in the bulk materaial are broken. The amount of nanosheets produced is controlled by the sonication time, the solvent-molybdenum disulfide interactions, and the centrifuge speed. Compared to other exfoliation techniques, solvent assisted exfoliation is the simplest method for large scale production of molybdenum disulfide nanosheets.\n\nThe micromechanical exfoliation of molybdenum disulfide was inspired by the same technique used in the isolation of graphene nanosheets. Micromechanical exfoliation allows for low defect molybdenum disulfide nanosheets but is not suitable for large scale production due to low yield.\n\nChemical exfoliation involves functionalizing molybdenum difsulfide and then sonicating to disperse the nanosheets. The most notable chemical exfoliation technique is lithium intercalation in which lithium is intercalated into bulk molybdenum disulfide and then dispersed into nanosheets by the addition of water.\n\nChemical vapor deposition of molybdenum disulfide nanosheets involves reacting molybdenum and sulfur precursors on a substrate at high temperatures. This technique is often used in the preparing electrical devices with molybdenum disulfide components because the nanosheets are applied directly on the substrate; unfavorable interactions between the substrate and the nanosheets that would have occurred had they been separately synthesized are decreased. In addition, since the thickness and area of the molybdenum disulfide nanosheets can be controlled by the selection of specific precursors, the electrical properties of the nanosheets can be tuned.\n\nPulsed laser deposition involves the thinning of bulk molybdenum disulfide by laser to produce single or multi-layer molybdenum disulfide nanosheets. This allows for synthesis of molybdenum disulfide nanosheets with a defined shape and size. The quality of the nanosheets are determined by the energy of the laser and the irradation angle.\n\nLasers can also be used to form molybdenum disulfide nanosheets from molybdenum disulfide fullerene-like molecules.\n\nTungsten diselenide is an inorganic compound with the formula . The compound adopts a hexagonal crystalline structure similar to molybdenum disulfide. Every tungsten atom is covalently bonded to six selenium ligands in a trigonal prismatic coordination sphere, while each selenium is bonded to three tungsten atoms in a pyramidal geometry. The tungsten – selenium bond has a bond distance of 2.526 Å and the distance between selenium atoms is 3.34 Å. Layers stack together via van der Waals interactions. is a stable semiconductor in the group-VI transition metal dichalcogenides. The electronic bandgap of can be tuned by mechanical strain which can also allow for conversion of the band type from indirect-to-direct in a bilayer.\n\nHafnium Disulphide is a group IVB TMD with formula . Like other TMDs, it possess a layered structure with strong covalent bonding between the Hf and S atoms in a layer and weak Van der Wall forces between layers. The compound has type structure and is an indirect band gap semiconducting material. The interlayer spacing between the layers is 0.56 nm, which is small compared to group VIB TMDs like , making it difficult to cleave its atomic layers. However, recently its crystals with large interlayer spacing has grown using a chemical vapor transport route. These crystals exfoliate in solvents like N-Cyclohexyl-2-pyrrolidone (CHP) in a time of just some minutes resulting in a high-yield production of its few-layers resulting in increase of its indirect bandgap from 0.9 eV to 1.3 eV. As an application in electronics, its field-effect transistors has been realised using its few layers as a conducting channel material offering a high current modulation ratio larger than 10000 at room temperature. Therefore, group IVB TMDs also holds potential applications in the field of opto-electronics.\n\nMXenes are layered transition metal carbides and carbonitrides with general formula of MXT, where M stands for early transition metal, X stands for carbon and/or nitrogen and T stands for surface terminations (mostly =O, -OH or -F). MXenes have high electric conductivity (10000-1500 Scm) combined with hydrophilic surfaces that can be tuned with solvents. This materials show promise in energy storage applications and composites. They are synthesized from ceramic precursor MAX phases by removing the single atomic layer \"A\" where M stands for Ti, Mo, W, Nb, Zr, Hf, V, Cr, Ta, Sc, A stands for Al, Si, and X stands for C, N. Millions of predicted solid solutions of these materials have been identified and 30+ MXenes have been synthesized.\n\nNi(HITP) is an organic, crystalline, structurally tunable electrical conductor with a high surface area. HITP is an organic chemical (2,3,6,7,10,11-hexaaminotriphenylene). It shares graphene's hexagonal honeycomb structure. Multiple layers naturally form perfectly aligned stacks, with identical 2-nm openings at the centers of the hexagons. Room temperature electrical conductivity is ~40 S cm, comparable to that of bulk graphite and among the highest for any conducting metal-organic frameworks (MOFs). The temperature dependence of its conductivity is linear at temperatures between 100 K and 500 K, suggesting an unusual charge transport mechanism that has not been previously observed in organic semiconductors.\n\nThe material was claimed to be the first of a group formed by switching metals and/or organic compounds. The material can be isolated as a powder or a film with conductivity values of 2 and 40 S cm, respectively.\n\nA 2015 study stacked two different TMD layers onto graphene. This composite displayed negative differential resistance – applying more voltage to the device reduced the current flowing through it. A recent study combined graphene and hexagonal boron nitride to form a planar two-dimensional material. This combination displayed variety of structural formations that potentially varies the electrical, thermal and optical properties.\n\nMicroscopy techniques such as transmission electron microscopy, scanning probe microscopy, scanning tunneling microscopy, and atomic force microscopy are used to characterize the thickness and size of the 2D materials. Electrical properties and structural properties such as composition and defects are characterized by raman spectroscopy, x-ray diffraction, and x-ray photoelectron spectroscopy.\n\nAs of 2014, none of these materials has been used for large scale commercial applications (with the possible exception of graphene). Despite this, many are under close consideration for a number of industries, in areas including electronics and optoelectronics, sensors, biological engineering, filtration, lightweight/strong composite materials, photovoltaics, medicine, quantum dots, thermal management, ethanol distillation and energy storage, cryptography and have enormous potential.\n\nGraphene has been the most studied. In small quantities it is available as a powder and as a dispersion in a polymer matrix, or adhesive, elastomer, oil and aqueous and non-aqueous solutions. The dispersion is claimed to be suitable for advanced composites, paints and coatings, lubricants, oils and functional fluids, capacitors and batteries, thermal management applications, display materials and packaging, inks and 3D-printers’ materials, and barriers and films.\n\nResearch on 2D nanomaterials is still in its infancy, with the majority of research focusing on elucidating the unique material characteristics and few reports focusing on biomedical applications of 2D nanomaterials. Nevertheless, recent rapid advances in 2D nanomaterials have raised important yet exciting questions about their interactions with biological moieties. 2D nanoparticles such as carbon-based 2D materials, silicate clays, transition metal dichalcogenides (TMDs), and transition metal oxides (TMOs) provide enhanced physical, chemical, and biological functionality owing to their uniform shapes, high surface-to-volume ratios, and surface charge.\n\nTwo-dimensional (2D) nanomaterials are ultrathin nanomaterials with a high degree of anisotropy and chemical functionality. 2D nanomaterials are highly diverse in terms of their mechanical, chemical, and optical properties, as well as in size, shape, biocompatibility, and degradability. These diverse properties make 2D nanomaterials suitable for a wide range of applications, including drug delivery, imaging, tissue engineering, and biosensors, among others. However, their low-dimension nanostructure gives them some common characteristics. For example, 2D nanomaterials are the thinnest materials known, which means that they also possess the highest specific surface areas of all known materials. This characteristic makes these materials invaluable for applications requiring high levels of surface interactions on a small scale. As a result, 2D nanomaterials are being explored for use in drug delivery systems, where they can adsorb large numbers of drug molecules and enable superior control over release kinetics. Additionally, their exceptional surface area to volume ratios and typically high modulus values make them useful for improving the mechanical properties of biomedical nanocomposites and nanocomposite hydrogels, even at low concentrations. Their extreme thinness has been instrumental for breakthroughs in biosensing and gene sequencing. Moreover, the thinness of these molecules allows them to respond rapidly to external signals such as light, which has led to utility in optical therapies of all kinds, including imaging applications, photothermal therapy (PTT), and photodynamic therapy (PDT).\n\nDespite the rapid pace of development in the field of 2D nanomaterials, these materials must be carefully evaluated for biocompatibility in order to be relevant for biomedical applications. The newness of this class of materials means that even the relatively well-established 2D materials like graphene are poorly understood in terms of their physiological interactions with living tissues. Additionally, the complexities of variable particle size and shape, impurities from manufacturing, and protein and immune interactions have resulted in a patchwork of knowledge on the biocompatibility of these materials.\n"}
{"id": "13442173", "url": "https://en.wikipedia.org/wiki?curid=13442173", "title": "United Nations Environment Organization", "text": "United Nations Environment Organization\n\nProposals for the creation of a United Nations Environmental Organization (UNEO) have come as some question the efficacy of the current United Nations Environmental Programme (UNEP) at dealing with the scope of global environmental issues. Created to act as an anchor institution in the system of Global Environmental Governance (GEG), it has failed to meet those demands. The UNEP has been hindered by its title as a Programme as opposed to a Specialized agency like the WTO or WHO, in addition to a lack of voluntary funding, and a location removed from the centers of political power, in Nairobi, Kenya. These factors have led to widespread calls for UNEP reform, and following the publication of Fourth Assessment Report of the IPCC in February 2007, a \"Paris Call for Action\" read out by French President Chirac and supported by 46 countries, called for the UNEP to be replaced by a new and more powerful United Nations Environment Organization, to be modelled on the World Health Organization. The 52 countries included the European Union nations, but notably did not include the United States and BRIC (Brasil, Russia, India, and China), the top five emitters of greenhouse gases.\n\nA number of factors have limited the UNEP's ability to fulfill its mandate.\n\nThe decision to establish the UNEP as a program as opposed to a Specialized Agency like the WHO has had enormous implications for UNEP's performance. “In the UN hierarchy, Programmes have the least independence and authority,” and this has made it difficult for UNEP to establish authority over subsequent UN bodies dealing with the environment. Additionally, “born out of a General Assembly Resolution, its mandate was limited and deprived of any implementing powers”.\n\nThe UNEP has been handicapped by its location in Nairobi, Kenya. The first UN body headquartered in the Global South, the move was meant to ease tensions between the developed and developing world. However, its location in Kenya has led to an isolation from other international environmental power structures, as well as a “physical fragmentation of the governance structure” due to its removal from other UN bodies headquartered in Geneva and New York City. UNEP's headquarters in Nairobi also make it difficult to attract the top-notch staff which are needed to effectively run the Programme. This is due to an uncertain security situation in Kenya and quality of life issues.\n\nMany of UNEP's struggles can be traced back to a lack of funding, and that lack of funding can be traced back to the UNEP's unique funding system. Unlike other international organizations, the UNEP does not have mandatory assessments, but instead is reliant on the voluntary contributions of UN member states. “UNEP’s unreliable and highly discretionary financial arrangement compromises the financial stability of the organization, its ability to plan beyond the current budget cycle, and its autonomy” as well as making the UNEP too dependent on certain member states which then hold undue influence in UNEP agenda setting. Voluntary contributions to the Environment Fund have decreased by 36% over the past ten years and have been displaced by restricted and earmarked contributions which now make up 2/3 of the UNEP's budget.\n\nThe UNEP was created to perform the tasks of an anchor institution in the system of Global Environmental Governance (GEG). According to the Nairobi Declaration on the Role and Mandate of the United Nations Environmental program, “the role of UNEP was to be the leading global environmental authority that sets the global environmental agenda, promotes the coherent implementation of the Environmental dimension of sustainable development within the UN system, and serves as an authoritative advocate for the global environment”. Maria Ivanova, Director of the Global Environmental Governance Project at the Yale Center for Environmental Law and Policy, writes in a working paper entitled Assessing UNEP as Anchor Institution for the Global Environment: Lessons for the UNEO Debate that “anchor institutions are the primary, though not the only, international organizations in certain global issue areas and typically perform three core functions: 1) overseeing monitoring, assessment, and reporting on the state of the issue in their purview; 2) setting an agenda for action and advancing standards, policies, and guidelines; and 3) developing institutional capacity to address existing and emerging problems.” However, while the UNEP was chartered to perform these three primary tasks, it has failed in many ways:\n\nThe UNEP has had the most success in the areas of monitoring, assessment, and reporting; it is thought to do relatively well in the arena of global environmental assessment (GEA), and publishes the Global Environment Outlook (GEO) reports. However, it fails to analyze environmental issues at the state level, and responsibility for monitoring, assessment, and reporting is allocated to all eight divisions of UNEP, creating redundancies. Additionally, the UNEP system of reporting is disorganized and difficult to access. \"The public cannot use UNEP's publications and benefit from the organization's work to the fullest due to the lack of a single easily accessible, searchable, and sortable database or catalog of publications\".\n\n“Another important function critical to the effectiveness of an anchor institution is agenda setting and management of intergovernmental processes to gain agreement on standards, policies, and guidelines or even just serving as the central forum for deliberation and debate.” While UNEP has had considerable success in the creation of treaties and multilateral environmental agreements (the Basel Convention on the Transboundary Movement of Hazardous Wastes, the Convention on Biological Diversity, the Montreal Protocol on the Protection of the Ozone Layer, etc.), it has struggled to coordinate the efforts of the numerous international environmental regulatory bodies after their inception. “UNEP has not succeeded in becoming the central forum for debate and deliberation in the environmental field, like the WTO for trade or the WHO for health.” This lack of a central coordinating authority has led to an erosion in the efficacy of Global Environmental Governance. “UNEP has not been able to fulfill its coordination mandate effectively in its two key areas of responsibility (1) coordination of multilateral environmental agreements and (2) coordination of the environmental activities of other international organizations”.\n\nThe UNEP has struggled to establish its role in the International system and this has led to it also having difficulty in Capacity Development (defined by Ivanova as Education and Training, Financing, Technical Assistance, and Institution and Network Building). The UNEP mandate calls for the UNEP to play a primarily normative role in GEG, however “the organization now views implementation as its primary strategy.” This shift has been necessitated by a desire from states for less treaties and more concrete action and the need for the UNEP to provide tangible results for potential governmental and private sources of funding, upon which the UNEP is dependent. However, in moving away from a normative role, the UNEP has moved away from its strengths: “information provision, development of common norms and principles, and institutional capacity development”.\n\nAfter the 58th meeting of the UN General Assembly, a proposal was made by the member states of the European Union to transform the UNEP into the United Nations Environmental Organization. In 2007 at UNEP's Governing Council meeting, the EU repeated its call for a ““a significant strengthening of UNEP, along the lines sketched out in Cartagena as well as in the recent announcements of the Executive Director, which will help UNEP to become more effective in catalyzing action to address major environmental threats” and “that an upgrade of UNEP into a UNEO, with stable, adequate and predictable resources and with the appropriate international standing, would enable the organization to fully fulfill its mandate and to live up to the expectations of developed and developing countries”. Proponents of a UNEO argue that it would play a vital role in increasing the political importance of environmental issues in the UN, and could therefore play a vital role in solving the many environmental challenges the earth currently faces. Specifics of a UNEO would need to be worked out by the international community, but certain basic aspects of a UNEO would likely include\n\nWhile still working within the UN system and reporting to ECOSOC, designation as a specialized agency would allow UNEO to “enjoy budgetary autonomy” and “determine most details of their programming.” Additionally, creation of a UNEO would not require consensus among the members of the UN, and “membership in a UNEO and the UN could differ”.\n\nAs with other Specialized Agencies, the institutional structure of a UNEO would consist of a plenary body, an executive organ, and a secretariat. Proponents of a UNEO suggest that a UNEO plenary body would “elect the members of the executive organ, appoint the Director General and approve the budget and work programme.” The plenary body would also likely include a wide range of observers from other international organizations to non-governmental organizations and representatives of civil society. Specialized agencies have an executive organ which ensures that the agency achieves its operational goals and manages the budget. A UNEO executive organ is likely to “prepare the budget and work programme of the UNEO.” The Secretariat would “as the focal point for all the agency’s activities,” however there is considerable variety in the organization of the Secretariats of various Specialized Agencies.\n\nThe exact mandate and function of a UNEO would be determined by the states involved, but they are likely to be similar to those of the UNEP. “It has been argued that a UNEO should help to systematically pool the scientific knowledge on environmental issues and help to define global environmental strategic guidelines to promote coordination and synergies.” However, unlike other specialized agencies like the WTO, it would not have the power to adjudicate environmental disputes. Proponents suggest that a UNEO could serve as an umbrella organization, helping to coordinate the “over 500 multilateral environment agreements (MEA) and numerous international organizations and fora dealing in an uncoordinated way with the environment” and make them more efficient and less redundant.\n\nA UNEO would likely rely on assessed contributions, which would alleviate UNEP problems that stem from a reliance on voluntary contributions. As a specialized agency, a UNEO would be able to determine the assessment scales of members separately from the UN general budget. Voluntary contributions would likely be used for specific projects, but assessed contributions would pay for the operating budget of a UNEO.\n\nWhile the number of countries who favor the creation of a UNEO has grown, there remain fears among many states that the creation of a UNEO would “favor environmental policies to the detriment of other policies, notably economic development and, thus, poverty eradication.” Some states also feel that the UNEP should first be strengthened and given time to improve international environmental governance before a UNEO is created.\n\nThere is a consensus among policy-makers that UNEP needs to be strengthened, and “in November 2006, the Secretary-General’s High-level Panel on UN System-wide Coherence in the Areas of Development, Humanitarian Assistance, and the Environment recommended that 'UNEP should be upgraded and have real authority as the environmental policy pillar of the UN system'”. And while disagreement remains over whether the creation of a UNEO is necessary, according to a French embassy statement, support is growing. “Following the Paris Call for Action, the Group of Friends of the UNEO was set up and now has 52 States, bringing together, besides the European Union, countries from every geographical area\", and many hope that a new administration in the United States will make the creation of a UNEO more likely.\n\n\n\n"}
{"id": "20491903", "url": "https://en.wikipedia.org/wiki?curid=20491903", "title": "Velocity", "text": "Velocity\n\nThe velocity of an object is the rate of change of its position with respect to a frame of reference, and is a function of time. Velocity is equivalent to a specification of an object's speed and direction of motion (e.g. to the north). Velocity is a fundamental concept in kinematics, the branch of classical mechanics that describes the motion of bodies.\n\nVelocity is a physical vector quantity; both magnitude and direction are needed to define it. The scalar absolute value (magnitude) of velocity is called , being a coherent derived unit whose quantity is measured in the SI (metric system) as metres per second (m/s) or as the SI base unit of (m⋅s). For example, \"5 metres per second\" is a scalar, whereas \"5 metres per second east\" is a vector. \nIf there is a change in speed, direction or both, then the object has a changing velocity and is said to be undergoing an \"acceleration\".\n\nTo have a constant velocity, an object must have a constant speed in a constant direction. Constant direction constrains the object to motion in a straight path thus, a constant velocity means motion in a straight line at a constant speed.\n\nFor example, a car moving at a constant 20 kilometres per hour in a circular path has a constant speed, but does not have a constant velocity because its direction changes. Hence, the car is considered to be undergoing an acceleration.\n\nSpeed describes only how fast an object is moving, whereas velocity gives both how fast it is and in which direction the object is moving. If a car is said to travel at 60 km/h, its speed has been specified. However, if the car is said to move at 60 km/h to the north, its velocity has now been specified.\n\nThe big difference can be noticed when we consider movement around a circle. When something moves in a circular path (at a constant speed, see above) and returns to its starting point, its average velocity is zero but its average speed is found by dividing the circumference of the circle by the time taken to move around the circle. This is because the average velocity is calculated by only considering the displacement between the starting and the end points while the average speed considers only the total distance traveled.\n\nVelocity is defined as the rate of change of position with respect to time, which may also be referred to as the \"instantaneous velocity\" to emphasize the distinction from the average velocity. In some applications the \"average velocity\" of an object might be needed, that is to say, the constant velocity that would provide the same resultant displacement as a variable velocity in the same time interval, , over some time period . Average velocity can be calculated as:\n\nThe average velocity is always less than or equal to the average speed of an object. This can be seen by realizing that while distance is always strictly increasing, displacement can increase or decrease in magnitude as well as change direction.\n\nIn terms of a displacement-time (\"x\" vs. \"t\") graph, the instantaneous velocity (or, simply, velocity) can be thought of as the slope of the tangent line to the curve at any point, and the average velocity as the slope of the secant line between two points with \"t\" coordinates equal to the boundaries of the time period for the average velocity.\n\nThe average velocity is the same as the velocity averaged over time – that is to say, its time-weighted average, which may be calculated as the time integral of the velocity:\n\nwhere we may identify\n\nand\n\nIf we consider as velocity and as the displacement (change in position) vector, then we can express the (instantaneous) velocity of a particle or object, at any particular time , as the derivative of the position with respect to time:\n\nFrom this derivative equation, in the one-dimensional case it can be seen that the area under a velocity vs. time ( vs. graph) is the displacement, . In calculus terms, the integral of the velocity function is the displacement function . In the figure, this corresponds to the yellow area under the curve labeled ( being an alternative notation for displacement).\n\nSince the derivative of the position with respect to time gives the change in position (in metres) divided by the change in time (in seconds), velocity is measured in metres per second (m/s). Although the concept of an instantaneous velocity might at first seem counter-intuitive, it may be thought of as the velocity that the object would continue to travel at if it stopped accelerating at that moment.\n\nAlthough velocity is defined as the rate of change of position, it is often common to start with an expression for an object's acceleration. As seen by the three green tangent lines in the figure, an object's instantaneous acceleration at a point in time is the slope of the line tangent to the curve of a graph at that point. In other words, acceleration is defined as the derivative of velocity with respect to time:\n\nFrom there, we can obtain an expression for velocity as the area under an acceleration vs. time graph. As above, this is done using the concept of the integral:\n\nIn the special case of constant acceleration, velocity can be studied using the suvat equations. By considering a as being equal to some arbitrary constant vector, it is trivial to show that\nwith as the velocity at time and as the velocity at time . By combining this equation with the suvat equation , it is possible to relate the displacement and the average velocity by\nIt is also possible to derive an expression for the velocity independent of time, known as the Torricelli equation, as follows:\nwhere etc.\n\nThe above equations are valid for both Newtonian mechanics and special relativity. Where Newtonian mechanics and special relativity differ is in how different observers would describe the same situation. In particular, in Newtonian mechanics, all observers agree on the value of t and the transformation rules for position create a situation in which all non-accelerating observers would describe the acceleration of an object with the same values. Neither is true for special relativity. In other words, only relative velocity can be calculated.\n\nThe kinetic energy of a moving object is dependent on its velocity and is given by the equation\nignoring special relativity, where \"E\" is the kinetic energy and \"m\" is the mass. Kinetic energy is a scalar quantity as it depends on the square of the velocity, however a related quantity, momentum, is a vector and defined by \nIn special relativity, the dimensionless Lorentz factor appears frequently, and is given by \nwhere γ is the Lorentz factor and \"c\" is the speed of light.\n\nEscape velocity is the minimum speed a ballistic object needs to escape from a massive body such as Earth. It represents the kinetic energy that, when added to the object's gravitational potential energy, (which is always negative) is equal to zero. The general formula for the escape velocity of an object at a distance \"r\" from the center of a planet with mass \"M\" is\nwhere \"G\" is the Gravitational constant and \"g\" is the Gravitational acceleration. The escape velocity from Earth's surface is about 11 200 m/s, and is irrespective of the direction of the object. This makes \"escape velocity\" somewhat of a misnomer, as the more correct term would be \"escape speed\": any object attaining a velocity of that magnitude, irrespective of atmosphere, will leave the vicinity of the base body as long as it doesn't intersect with something in its path.\n\nRelative velocity is a measurement of velocity between two objects\nas determined in a single coordinate system. Relative velocity is fundamental in both classical and modern physics, since many systems in physics deal with the relative motion of two or more particles. In Newtonian mechanics, the relative velocity is independent of the chosen inertial reference frame. This is not the case anymore with special relativity in which velocities depend on the choice of reference frame.\n\nIf an object A is moving with velocity vector v and an object B with velocity vector w, then the velocity of object A \"relative to\" object B is defined as the difference of the two velocity vectors:\nSimilarly the relative velocity of object B moving with velocity w, relative to object A moving with velocity v is:\nUsually the inertial frame is chosen in which the latter of the two mentioned objects is in rest.\n\nIn the one-dimensional case, the velocities are scalars and the equation is either:\n\nIn polar coordinates, a two-dimensional velocity is described by a radial velocity, defined as the component of velocity away from or toward the origin (also known as \"velocity made good\"), and an angular velocity, which is the rate of rotation about the origin (with positive quantities representing counter-clockwise rotation and negative quantities representing clockwise rotation, in a right-handed coordinate system).\n\nThe radial and angular velocities can be derived from the Cartesian velocity and displacement vectors by decomposing the velocity vector into radial and transverse components. The transverse velocity is the component of velocity along a circle centered at the origin.\n\nwhere\nThe \"magnitude of the radial velocity\" is the dot product of the velocity vector and the unit vector in the direction of the displacement.\nwhere\nThe \"magnitude of the transverse velocity\" is that of the cross product of the unit vector in the direction of the displacement and the velocity vector. It is also the product of the angular speed formula_27 and the magnitude of the displacement.\nsuch that\n\nAngular momentum in scalar form is the mass times the distance to the origin times the transverse velocity, or equivalently, the mass times the distance squared times the angular speed. The sign convention for angular momentum is the same as that for angular velocity.\nwhere\n\nThe expression formula_33 is known as moment of inertia.\nIf forces are in the radial direction only with an inverse square dependence, as in the case of a gravitational orbit, angular momentum is constant, and transverse speed is inversely proportional to the distance, angular speed is inversely proportional to the distance squared, and the rate at which area is swept out is constant. These relations are known as Kepler's laws of planetary motion.\n\n\n"}
{"id": "32095050", "url": "https://en.wikipedia.org/wiki?curid=32095050", "title": "Wadebridge Renewable Energy Network", "text": "Wadebridge Renewable Energy Network\n\nWadebridge Renewable Energy Network (WREN) based in Wadebridge, Cornwall, is a grass roots social enterprise aiming to transform the area into the first solar powerered and renewable energy powered town in the UK.\nThe group plans to install 1 MW peak capacity of solar panels; with ten installations already in place and another ninety planned they hope to generate at least a third of its electricity from solar and wind power by 2015.\n\nThe WREN Steering Group consists of residents, councillors from Cornwall Council and Wadebridge Town Council, together with representatives of the Wadebridge Chamber of Commerce.\n\nThe scheme could also generate £450,000 a year for the town with money coming from a Feed-in tariff which offers a premium price for renewable energy.\nThe county council has granted planning permission for four new solar farms and sent plans for a further five out for consultation.\nIn February 2012 the WREN project was awarded £68,000 as part of the coalition Government's Local Energy Assessment Fund and in May 2012 won an award for Best Third Sector Business in the 2012 Cornwall Business Awards.\n\nIn 2013 Stephen Frankel (chairman of WREN) was named South West Sustainable Energy Champion at an award ceremony in Bath.\n\n"}
{"id": "19949953", "url": "https://en.wikipedia.org/wiki?curid=19949953", "title": "Water power engine", "text": "Water power engine\n\nA water power engine includes prime movers driven by water and which may be classified under three categories:\n\nHydro power is generated when the natural force from the water's current moves a device (fan, propeller, wheel) that is pushed by the force of the water. Ordinary water weighs 8.36 lbs per gallon (1 kg per liter). The force make the turbine mechanism spin, creating electricity. As long as there is flow, it is possible to produce electricity. The advantage of electricity generated in this way is that it is a renewable resource. A small-scale Micro Hydro Power can be a reliable and long lasting piece of technology. The disadvantage of the system is that technology has yet to be developed more than what it is today.\n\n"}
{"id": "52562072", "url": "https://en.wikipedia.org/wiki?curid=52562072", "title": "Workhorse Group", "text": "Workhorse Group\n\nWorkhorse Group Incorporated is an American manufacturing company based in Cincinnati, Ohio, currently focused on manufacturing electrically powered delivery and utility vehicles. \n\nThe company was founded in 1998 by investors who took over the production of General Motors' P30/P32 series stepvan and motorhome chassis. By 2005 they were taken over by Navistar International, which had been selling them diesel engines since before. Navistar then shuttered the plant in 2012, to cut costs after having suffered heavy losses. \n\nIn March 2015 AMP Electric Vehicles took over Workhorse Custom Chassis, changing the company name to Workhorse Group Incorporated, and began offering a range of electrically powered delivery vans.\n\nAs of 2016, the company offers the familiar W62 chassis and a newer, narrow-tracked version called the W88. Their first product was the P-series, based on the Chevrolet/GMC P30-series stepvan/mobile home chassis. An earlier version was the W42 chassis, and they were also managerially involved with the construction of Navistar's eStar electric van, until it too was cancelled in early 2013. Workhorse also briefly offered an integrated chassis/body model called the MetroStar, hearkening back to the long-lived International Harvester Metro Van line.\n\nThere were also the low-floor bus chassis (LF72), as well as a rear-engined recreational vehicle chassis called the UFO.\n\nIn November 2016, Workhorse announced that they were working on an electrically powered pickup truck, called the W-15. North Carolina's Duke Energy has stated that it will buy 500 of the vehicles, and the city of Orlando is also interested. It is scheduled to have 460 horsepower and a battery range of 80 miles. A gasoline range extender supplies further range.\n\n"}
