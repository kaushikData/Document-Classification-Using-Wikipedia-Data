{"id": "25146747", "url": "https://en.wikipedia.org/wiki?curid=25146747", "title": "1967 Nicosia Britannia disaster", "text": "1967 Nicosia Britannia disaster\n\nThe Nicosia Britannia disaster was the death of 126 passengers and crew on a Bristol Britannia of the Swiss airline Globe Air when it flew into the ground south of Nicosia Airport, in Cyprus.\n\nThe Britannia was operating a charter flight bringing tourists from Bangkok in Thailand to Basel in Switzerland with stopovers in Colombo, Bombay, and Cairo. The flight stopped at Colombo in Sri Lanka and then Bombay in India with the next stop due to be Cairo. The crew diverted the flight to Nicosia due to bad weather at Cairo. The aircraft was on the third attempt to land on Runway 32 in a violent thunderstorm when it flew into a hill near the village of Lakatamia and burst into flames.\n\nAt the time of the crash, both pilots had exceeded their authorized duty time by three hours. The flight's first officer had less than 50 hours flying time in Britannia aircraft.\n\nTwo German (Christa Blümel and Peter Femfert) and two Swiss (Veronika Gysin and Nicolas Pulver) passengers survived; three of them were seriously injured and were treated at a United Nations field hospital near Nicosia, the fourth, Nicolas Pulver, was reported to be unhurt.\n"}
{"id": "2473", "url": "https://en.wikipedia.org/wiki?curid=2473", "title": "Abacá", "text": "Abacá\n\nAbacá ( ; ), binomial name Musa textilis, is a species of banana native to the Philippines, grown as a commercial crop in the Philippines, Ecuador, and Costa Rica. The plant, also known as Manila hemp, has great economic importance, being harvested for its fiber, also called Manila hemp, extracted from the leaf-stems. \n\nThe plant grows to , and averages about . The fiber was originally used for making twines and ropes; now most is pulped and used in a variety of specialized paper products including tea bags, filter paper and banknotes. It is classified as a hard fiber, along with coir, henequin and sisal.\n\nThe abacá plant is stoloniferous, meaning that the plant produces runners or shoots along the ground that then root at each segment. Cutting and transplanting rooted runners is the primary technique for creating new plants, since seed growth is substantially slower. Abacá has a \"false trunk\" or pseudostem about in diameter. The leaf stalks (petioles) are expanded at the base to form sheaths that are tightly wrapped together to form the pseudostem. There are from 12 to 25 leaves, dark green on the top and pale green on the underside, sometimes with large brown patches. They are oblong in shape with a deltoid base. They grow in succession. The petioles grow to at least in length. \n\nWhen the plant is mature, the flower stalk grows up inside the pseudostem. The male flower has five petals, each about long. The leaf sheaths contain the valuable fiber. After harvesting, the coarse fibers range in length from long. They are composed primarily of cellulose, lignin, and pectin.\n\nThe fruit, which is inedible and is rarely seen as harvesting occurs before the plant fruits, grows to about in length and in diameter. It has black turbinate seeds that are in diameter.\n\nThe abacá plant belongs to the banana family, Musaceae; it resembles the closely related wild seeded bananas, \"Musa acuminata\" and \"Musa balbisiana\". Its scientific name is \"Musa textilis\". Within the genus \"Musa\", it is placed in section \"Callimusa\" (now including the former section \"Australimusa\"), members of which have a diploid chromosome number of 2n = 20.\n\nBefore synthetic textiles came into use, \"M. textilis\" was a major source of high quality fiber: soft, silky and fine. Ancestors of the modern abaca are thought to have originated from the Eastern Philippines where there are lot of rains (no pronounced dry season), in fact wild type of abaca can still be found in the interior forests of Catanduanes Island which is often not cultivated. \n\nToday, Catanduanes has many other modern kinds of abaca which are more competitive. For many years, breeders from various research institutions have made the cultivated varieties of Catanduanes Island even more competitive in local and international markets. This results in the optimum production of the island which had a consistent highest production throughout the archipelago.\n\nEuropeans first came into contact with Abaca fibre when Magellan made land in the Philippines in 1521, as the natives were cultivating it and utilizing it in bulk for textiles already. By 1897, the Philippines were exporting almost 100,000 tons of abacá, and it was one of the three biggest cash crops, along with tobacco and sugar. In fact, from 1850 through the end of the 19th century, sugar or abacá alternated with each other as the biggest export crop of the Philippines. This 19th-century trade was predominantly with the United States and the making of ropes was done mainly in New England, although in time the rope-making was moved back to the Philippines. \n\nExcluding the Philippines, abacá was first cultivated on a large scale in Sumatra in 1925 under the Dutch, who had observed its cultivation in the Philippines for cordage since the nineteenth century, followed up by plantings in Central America in 1929 sponsored by the U.S. Department of Agriculture. It also was transplanted into India and Guam. Commercial planting began in 1930 in British North Borneo; with the commencement of World War II, the supply from the Philippines was eliminated by the Japanese.\n\nIn the early 1900s, a train running from Danao to Argao would transport Philippine abaca from the plantations to Cebu city for export. The train and tracks were destroyed during the Second world war, however the Abaca plantations continue and are now transported to Cebu by road.\n\nAfter the war, the U.S. Department of Agriculture started production in Panama, Costa Rica, Honduras, and Guatemala. Today, abacá is produced primarily in the Philippines and Ecuador. The Philippines produces between 85% and 95% of the world's abacá, and the production employs 1.5 million people. Production has declined because of virus diseases.\nDue to its strength, it is a sought after product and is the strongest of the natural fibers. It is used by the paper industry for such specialty uses such as tea bags, banknotes and decorative papers. It can be used to make handcrafts such as bags, carpets, clothing and furniture. \n\nAbacá rope is very durable, flexible and resistant to salt water damage, allowing its use in hawsers, ship's lines and fishing nets. A rope can require to break. Abacá fiber was once used primarily for rope, but this application is now of minor significance. Lupis is the finest quality of abacá. Sinamay is woven chiefly from abacá.\n\nThe inner fibers are used in the making of hats, including the \"Manila hats,\" hammocks, matting, cordage, ropes, coarse twines, and types of canvas. It is called Manila hemp in the market although it is unlike true hemp, and is also known as Cebu hemp and Davao hemp. Abacá cloth is found in museum collections around the world, like the Boston Museum of Fine Arts and the Textile Museum of Canada.\n\nPhilippine indigenous tribes still weave abaca-based textiles like \"t'nalak\", made by the Tiboli tribe of South Cotabato, and \"dagmay\", made by the Bagobo people.\n\nThe plant is normally grown in well-drained loamy soil, using rhizomes planted at the start of the rainy season. In addition, new plants can be started by seeds. Growers harvest abacá fields every three to eight months after an initial growth period of 12–25 months. Harvesting is done by removing the leaf-stems after flowering but before fruit appears. The plant loses productivity between 15 and 40 years. The slopes of volcanoes provide a preferred growing environment. Harvesting generally includes several operations involving the leaf sheaths:\n\nWhen the processing is complete, the bundles of fiber are pale and lustrous with a length of .\n\nIn Costa Rica, more modern harvest and drying techniques are being developed to accommodate the very high yields obtained there.\n\nAccording to the Philippine Fiber Industry Development Authority, the Philippines provided 87.4% of the world's abaca in 2014, earning the Philippines US$111.33 million. The demand is still greater than the supply. The remainder came from Ecuador (12.5%) and Costa Rica (0.1%). The Bicol region in the Philippines produced 27,885 metric tons of abaca in 2014, the largest of any Philippine region. \n\nThe Philippine Rural Development Program (PRDP) and the Department of Agriculture reported that in 2009-2013, Bicol Region had 39% share of Philippine abaca production while overwhelming 92% comes from Catanduanes Island. Eastern Visayas, the second largest producer had 24% and the Davao Region, the third largest producer had 11% of the total production. Around 42 percent of the total abaca fiber shipments from the Philippines went to the United Kingdom in 2014, making it the top importer. Germany imported 37.1 percent abaca pulp from the Philippines, importing around 7,755 metric tons (MT). Sales of abaca cordage surged 20 percent in 2014 to a total of 5,093 MT from 4,240 MT, with the United States holding around 68 percent of the market.\n\nAbacá is vulnerable to a number of pathogens, notably abaca bunchy top virus and abaca bract mosaic virus.\n\n\n\n"}
{"id": "20363554", "url": "https://en.wikipedia.org/wiki?curid=20363554", "title": "Aleuritic acid", "text": "Aleuritic acid\n\nAleuritic acid, or α-aleuritic acid, is a major ingredient in shellac, constituting about 35% of it. It is used as a starting material in the perfume industry for the preparation of musk aroma.\n"}
{"id": "52885198", "url": "https://en.wikipedia.org/wiki?curid=52885198", "title": "Ballintaggart Ogham Stones", "text": "Ballintaggart Ogham Stones\n\nBallintaggart Ogham Stones is a collection of ogham stones forming a National Monument located in County Kerry, Ireland.\n\nBallintaggart Ogham Stones are located inside a round enclosure (diameter 30 m / 100 ft), immediately east of Dingle racecourse and southeast of the town.\n\nThe stones were carved in the 5th and 6th centuries AD and served as burial markers.\n\nThis was anciently the site of a church and old burial ground (\"An Cheallúnach\" or\" An Lisín\").\n\nThe ogham stones are rounded, made of water-rolled sandstone. Eight of them form a circle, each one lying down pointing outwards. The ninth lies at the centre. Several have been inscribed with crosses.\n\n"}
{"id": "3840994", "url": "https://en.wikipedia.org/wiki?curid=3840994", "title": "Bent's rule", "text": "Bent's rule\n\nIn chemistry, Bent's rule describes and explains the relationship between the orbital hybridization of central atoms in molecules and the electronegativities of substituents. The rule was stated by Henry Bent as follows: \nThe chemical structure of a molecule is intimately related to its properties and reactivity. Valence bond theory proposes that molecular structures are due to covalent bonds between the atoms and that each bond consists of two overlapping and typically hybridised atomic orbitals. Traditionally, p-block elements in molecules are assumed to hybridise strictly as sp, where \"n\" is either 1, 2, or 3. In addition, the hybrid orbitals are all assumed to be equivalent (i.e. the sp orbitals have the same p character). Results from this approach are usually good, but they can be improved upon by allowing isovalent hybridization, in which the hybridised orbitals may have noninteger and unequal p character. Bent's rule provides a qualitative estimate as to how these hybridised orbitals should be constructed. Bent's rule is that in a molecule, a central atom bonded to multiple groups will hybridise so that orbitals with more s character are directed towards electropositive groups, while orbitals with more p character will be directed towards groups that are more electronegative. By removing the assumption that all hybrid orbitals are equivalent sp orbitals, better predictions and explanations of properties such as molecular geometry and bond strength can be obtained. Bent's rule has been proposed as an alternative to VSEPR theory as an elementary explanation for observed molecular geometries of simple molecules with the advantages of being more easily reconcilable with modern theories of bonding and having stronger experimental support.\n\nBent's rule can be generalized to d-block elements as well. The hybridisation of a metal center is arranged so that orbitals with more s character are directed towards ligands that form bonds with more covalent character. Equivalently, orbitals with more d character are directed towards groups that form bonds of greater ionic character. The validity of Bent's rule for 75 bond types between the main group elements was examined recently. For bonds with the larger atoms from the lower periods, trends in orbital hybridization depend strongly on both electronegativity and orbital size.\n\nIn the early 1930s, shortly after much of the initial development of quantum mechanics, those theories began to be applied towards molecular structure by Pauling, Slater, Coulson, and others. In particular, Pauling introduced the concept of hybridisation, where atomic s and p orbitals are combined to give hybrid sp, sp, and sp orbitals. Hybrid orbitals proved powerful in explaining the molecular geometries of simple molecules like methane (tetrahedral with an sp carbon). However, slight deviations from these ideal geometries became apparent in the 1940s. A particularly well known example is water, where the angle between hydrogens is 104.5°, far less than the expected 109.5°. To explain such discrepancies, it was proposed that hybridisation can result in orbitals with unequal s and p character. A. D. Walsh described in 1947 a relationship between the electronegativity of groups bonded to carbon and the hybridisation of said carbon. Finally, in 1961, Henry A. Bent published a major review of the literature that related molecular structure, central atom hybridisation, and substituent electronegativities and it is for this work that Bent's rule takes its name.\n\nAn informal justification of Bent's rule relies on s orbitals being lower in energy than p orbitals. Bonds between elements of different electronegativities will be polar and the electron density in such bonds will be shifted towards the more electronegative element. Applying this to the molecule fluoromethane provides a demonstration of Bent's rule. Because carbon is more electronegative than hydrogen, the electron density in the C-H bonds will be closer to carbon. The energy of those electrons will depend heavily on the hybrid orbitals that carbon contributes to these bonds because of the increased electron density near the carbon. By increasing the amount of s character in those hybrid orbitals, the energy of those electrons can be reduced because s orbitals are lower in energy than p orbitals.\n\nBy the same logic and the fact that fluorine is more electronegative than carbon, the electron density in the C-F bond will be closer to fluorine. The hybrid orbital that carbon contributes to the C-F bond will have relatively less electron density in it than in the C-H case and so the energy of that bond will be less dependent on the carbon's hybridisation. By directing hybrid orbitals of more p character towards the fluorine, the energy of that bond is not increased very much.\n\nInstead of directing equivalent sp orbitals towards all four substituents, shifting s character towards the C-H bonds will stabilize those bonds greatly because of the increased electron density near the carbon, while shifting s character away from the C-F bond will increase its energy by a lesser amount because that bond's electron density is further from the carbon. The atomic s character on the carbon atom has been directed toward the more electropositive hydrogen substituents and away from the electronegative fluorine, which is exactly what Bent's rule suggests.\n\nAlthough fluoromethane is a special case, the above argument can be applied to any structure with a central atom and 2 or more substituents. The key is that concentrating atomic s character in orbitals directed towards electropositive substituents by depleting it in orbitals directed towards electronegative substituents results in an overall lowering of the energy of the system. This stabilizing trade off is responsible for Bent's rule. \n\nBent's rule can be extended to rationalize the hybridization of nonbonding orbitals as well. On the one hand, a lone pair (an occupied nonbonding orbital) can be thought of as the limiting case of an electropositive substituent, with electron density completely polarized towards the central atom. Bent's rule predicts that, in order to stabilize the unshared, closely held nonbonding electrons, \"lone pair orbitals should take on high s character\". On the other hand, an unoccupied nonbonding orbital can be thought of as the limiting case of an electronegative substituent, with electron density completely polarized towards the ligand. Bent's rule predicts that, in order to leave as much s character as possible for the remaining occupied orbitals, \"unoccupied nonbonding orbitals should maximize p character\". \n\nExperimentally, the first conclusion is in line with the reduced bond angles of molecules with lone pairs like water or ammonia compared to methane, while the second conclusion accords with the planar structure of molecules with unoccupied nonbonding orbitals, like monomeric borane and carbenium ions.\n\nBent’s rule can be used to explain trends in both molecular structure and reactivity. After determining how the hybridisation of the central atom should affect a particular property, the electronegativity of substituents can be examined to see if Bent’s rule holds.\n\nKnowing the angles between bonds is a crucial component in determining a molecular structure. In valence bond theory, covalent bonds are assumed to consist of two electrons lying in overlapping, usually hybridised, atomic orbitals from bonding atoms. Orbital hybridisation explains why methane is tetrahedral and ethylene is planar for instance. However, there are deviations from the ideal geometries of sp hybridisation such as in water and ammonia. The bond angles in those molecules are 104.5° and 107° respectively, which are below the expected tetrahedral angle of 109.5°. The traditional approach to explain those differences is VSEPR theory. In that framework, valence electrons are assumed to lie in localized regions and lone pairs are assumed to repel each other to a greater extent than bonding pairs.\n\nBent’s rule provides an alternative explanation as to why some bond angles differ from the ideal geometry. First, a trend between central atom hybridisation and bond angle can be determined by using the model compounds methane, ethylene, and acetylene. In order, the carbon atoms are directing sp, sp, and sp orbitals towards the hydrogen substituents. The bond angles between substituents are 109.5°, ~120°, and 180°. This simple system demonstrates that hybridised atomic orbitals with higher p character will have a smaller angle between them. This result can be made rigorous and quantitative as Coulson's theorem (see Formal theory section below).\n\nNow that the connection between hybridisation and bond angles has been made, Bent’s rule can be applied to specific examples. The following were used in Bent’s original paper, which considers the group electronegativity of the methyl group to be less than that of the hydrogen atom because methyl substitution reduces the acid dissociation constants of formic acid and of acetic acid.\n\nAs one moves down the table, the substituents become more electronegative and the bond angle between them decreases. According to Bent's rule, as the substituent electronegativies increase, orbitals of greater p character will be directed towards those groups. By the above discussion, this will decrease the bond angle. This agrees with the experimental results. Comparing this explanation with VSEPR theory, VSEPR cannot explain why the angle in dimethyl ether is greater than 109.5°\n\nIn predicting the bond angle of water, Bent’s rule suggests that hybrid orbitals with more s character should be directed towards the lone pairs, while that leaves orbitals with more p character directed towards the hydrogens, resulting in deviation from idealized O(sp) hybrid orbitals with 25% s character and 75% p character. In the case of water, with its 104.5° HOH angle, the OH bonding orbitals are constructed from O(~sp) orbitals (~20% s, ~80% p), while the lone pairs consist of O(~sp) orbitals (~30% s, ~70% p). As discussed in the justification above, the lone pairs behave as very electropositive substituents and have excess s character. As a result, the bonding electrons have increased p character. This increased p character in those orbitals decreases the bond angle between them to less than the tetrahedral 109.5°. The same logic can be applied to ammonia (107.0° HNH bond angle, with three N(~sp or 23% s) bonding orbitals and one N(~sp or 32% s) lone pair), the other canonical example of this phenomenon. \n\nThe same trend holds for nitrogen containing compounds. Against the expectations of VSEPR theory but consistent with Bent's rule, the bond angles of ammonia (NH) and nitrogen trifluoride (NF) are 107° and 102°, respectively.\n\nUnlike VSEPR theory, whose theoretical foundations now appear shaky, Bent's rule is still considered to be an important principle in modern treatments of bonding. For instance, a modification of this analysis is still viable, even if the lone pairs of HO are considered to be inequivalent by virtue of their symmetry (i.e., only s, and in-plane p and p oxygen AOs are hybridized to form the two O-H bonding orbitals σ and lone pair \"n\", while p becomes an inequivalent pure p-character lone pair \"n\"), as in the case of lone pairs emerging from natural bond orbital methods.\n\nSimilarly to bond angles, the hybridisation of an atom can be related to the lengths of the bonds it forms. As bonding orbitals increase in s character, the σ bond length decreases.\n\nBy adding electronegative substituents and changing the hybridisation of the central atoms, bond lengths can be manipulated. If a molecule contains a structure X-A--Y, replacement of the substituent X by a more electronegative atom changes the hybridization of central atom A and shortens the adjacent A--Y bond.\nBecause fluorine is so much more electronegative than hydrogen, in fluoromethane the carbon will direct hybrid orbitals higher in s character towards the three hydrogens than towards the fluorine. In difluoromethane, there are only two hydrogens so less s character in total is directed towards them and more is directed towards the two fluorines, which shortens the C—F bond lengths relative to fluoromethane. This trend holds all the way to tetrafluoromethane whose C-F bonds have the highest s character (25%) and the shortest bond lengths in the series.\n\nThe same trend also holds for the chlorinated analogs of methane, although the effect is less dramatic because chlorine is less electronegative than fluorine.\n\nThe above cases seem to demonstrate that the size of the chlorine is less important than its electronegativity. A prediction based on sterics alone would lead to the opposite trend, as the large chlorine substituents would be more favorable far apart. As the steric explanation contradicts the experimental result, Bent’s rule is likely playing a primary role in structure determination.\n\nPerhaps the most direct measurement of s character in a bonding orbital between hydrogen and carbon is via the H−C coupling constants determined from NMR spectra. Theory predicts that \"J\" values will be much higher in bonds with more s character. In particular, the one bond C-H coupling constant \"J\" is related to the fractional s character of the carbon hybrid orbital used to form the bond through the empirical relationship formula_1. (For instance the pure sp hybrid atomic orbital found in the C-H bond of methane would have 25% s character resulting in a expected coupling constant of 500 Hz × 0.25 = 125 Hz, in excellent agreement with the experimentally determined value.)\n\nAs the electronegativity of the substituent increases, the amount of p character directed towards the substituent increases as well. This leaves more s character in the bonds to the methyl protons, which leads to increased \"J\" coupling constants.\n\nThe inductive effect can be explained with Bent’s rule. The inductive effect is the transmission of charge through covalent bonds and Bent’s rule provides a mechanism for such results via differences in hybridisation. In the table below, as the groups bonded to the central carbon become more electronegative, the central carbon becomes more electron-withdrawing as measured by the polar substituent constant. The polar substituent constants are similar in principle to σ values from the Hammett equation, as an increasing value corresponds to a greater electron-withdrawing ability. Bent's rule suggests that as the electronegativity of the groups increase, more p character is diverted towards those groups, which leaves more s character in the bond between the central carbon and the R group. As s orbitals have greater electron density closer to the nucleus than p orbitals, the electron density in the C−R bond will more shift towards the carbon as the s character increases. This will make the central carbon more electron-withdrawing to the R group. Thus, the electron-withdrawing ability of the substituents has been transferred to the adjacent carbon, exactly what the inductive effect predicts.\n\nBent's rule provides an additional level of accuracy to valence bond theory. Valence bond theory proposes that covalent bonds consist of two electrons lying in overlapping, usually hybridised, atomic orbitals from two bonding atoms. The assumption that a covalent bond is a linear combination of atomic orbitals of just the two bonding atoms is an approximation (see molecular orbital theory), but valence bond theory is accurate enough that it has had and continues to have a major impact on how bonding is understood.\n\nIn valence bond theory, two atoms each contribute an atomic orbital and the electrons in the orbital overlap form a covalent bond. Atoms do not usually contribute a pure hydrogen-like orbital to bonds. If atoms could only contribute hydrogen-like orbitals, then the experimentally confirmed tetrahedral structure of methane would not be possible as the 2s and 2p orbitals of carbon do not have that geometry. That and other contradictions led to the proposing of orbital hybridisation. In that framework, atomic orbitals are allowed to mix to produce an equivalent number of orbitals of differing shapes and energies. In the aforementioned case of methane, the 2s and three 2p orbitals of carbon are hybridized to yield four equivalent sp orbitals, which resolves the structure discrepancy. Orbital hybridisation allowed valence bond theory to successfully explain the geometry and properties of a vast number of molecules.\n\nIn traditional hybridisation theory, the hybrid orbitals are all equivalent. Namely the atomic s and p orbital(s) are combined to give four orbitals, three orbitals, or two orbitals. These combinations are chosen to satisfy two conditions. First, the total amount of s and p orbital contributions must be equivalent before and after hybridisation. Second, the hybrid orbitals must be orthogonal to each other. If two hybrid orbitals were not orthogonal, by definition they would have nonzero orbital overlap. Electrons in those orbitals would interact and if one of those orbitals were involved in a covalent bond, the other orbital would also have a nonzero interaction with that bond, violating the two electron per bond tenet of valence bond theory.\n\nTo construct hybrid s and p orbitals, let the first hybrid orbital be given by , where p is directed towards a bonding group and \"λ\" determines the amount of p character this hybrid orbital has. This is a weighted sum of the wavefunctions. Now choose a second hybrid orbital , where \"p\" is directed in some way and \"λ\" is the amount of \"p\" character in this second orbital. The value of \"λ\" and direction of \"p\" must be determined so that the resulting orbital can be normalized and so that it is orthogonal to the first hybrid orbital. The hybrid can certainly be normalized, as it is the sum of two normalized wavefunctions. Orthogonality must be established so that the two hybrid orbitals can be involved in separate covalent bonds. The inner product of orthogonal orbitals must be zero and computing the inner product of the constructed hybrids gives the following calculation.\n\nNote that the s orbital is normalized and so the inner product formula_3. Also, the \"s\" orbital is orthogonal to the \"p\" and \"p\" orbitals, which leads to two terms in the above equaling zero. Finally, the last term is the inner product of two normalized functions that are at an angle of \"ω\" to each other, which gives cos \"ω\" by definition. However, the orthogonality of bonding orbitals demands that formula_4, so we get Coulson's theorem as a result:\n\nThis means that the four s and p atomic orbitals can be hybridised in arbitrary directions provided that all of the coefficients \"λ\" satisfy the above condition pairwise to guarantee the resulting orbitals are orthogonal.\n\nBent's rule, that centrals atoms direct orbitals of greater p character towards more electronegative substituents, is easily applicable to the above by noting that an increase in the \"λ\" coefficient increases the p character of the hybrid orbital. Thus, if a central atom A is bonded to two groups X and Y and Y is more electronegative than X, then A will hybridise so that . More sophisticated theoretical and computation techniques beyond Bent’s rule are needed to accurately predict molecular geometries from first principles, but Bent’s rule provides an excellent heuristic in explaining molecular structures.\n\n"}
{"id": "20632694", "url": "https://en.wikipedia.org/wiki?curid=20632694", "title": "Bernhard von Wüllerstorf-Urbair", "text": "Bernhard von Wüllerstorf-Urbair\n\nBaron Bernhard von Wüllerstorf-Urbair, also: von Wüllersdorf-Urbair or von Wüllerstorf und Urbair, (29 January 1816 – 10 August 1883) was an Austrian vice admiral and, from 1865 to 1867, (k.k.) Austrian Imperial Minister of Trade. He was captain of the frigate SMS \"Novara\", including during the world voyage (Novara-Expedition) in 1857-1859.\n\nHe was born on 29 January 1816 in Triest (Küstenland, today in Italy).\n\nAfter attending the grammar schools (Gymnasium) in Padua and in Ofen, in 1828 he joined the pioneer-cadet school in Tulln.\nAs a cadet of the 40th Infantry Regiment, he accepted the 1833 invitation to move to the Austrian Imperial War Navy.\nImmediately commanding a ship, he had to acquire his further education largely through self-study.\nLine ship transported in 1839, he seized the opportunity to be assigned to the observatory in Vienna,\nwhere he was taught by astronomers Littrow and Schaub.\nAfter the end of these studies,\nhe was with the management of the Naval Observatory in Venice and the lessons in astronomy and nautical at the Naval Academy operation.\nThrough his elegant appearance and his winning nature, he also quickly connected to the Venetian society. Immediately after his marriage to Anna O'Connor of Connaught on 12 April 1848, Venice broke from the revolution. He fled the city with his young wife and, in the course of the escape, her death took place. He traveled immediately to Trieste, where Field Marshal Lieutenant Gyulay had gathered the true-believer remnants of the Navy.\nHe was an important employee of the reorganization concerned with the Vice-Admiral . In 1849, he became Lieutenant. The next few years passed, in alternating between staff work and ship commands. In 1855, he was adviser to the new navy commander Archduke Ferdinand Max. Soon he was able to advance his enthusiastic lord's plan for a voyage around the world: he assisted with its planning in 1856. As line-captain and Commodore of the frigate , already at the time, he commanded the expedition, which lasted from 30 April 1857 to 26 August 1859. Numerous research findings, rich collections for the Vienna museums, and high reputation for the Imperial Navy were the main results of the voyage around the world. Due to his scientific knowledge, he assisted all oceanographic, hydrographic and meteorological observations.\nFor his scientific achievements, he was added in 1863 as an honorary member of the Bavarian Academy of Sciences.\nHis 3-volume report was published, in 1861, as \"Journey of the Austrian Frigate Novara around the Earth in 1857, 1858, 1859 under the command of Commodore B. von Wüllersdorf-Urbair\".\nAfter a deployment in the waters around which the free throngs of Garibaldi threatened Sicily, he became Rear Admiral and representative of the navy commander at Reichsrat in Vienna. In 1864, he led the German-Danish War, a squadron into the North Sea according to where Wilhelm von Tegetthoff, even before his arrival, the Battle of Heligoland (1864) had beaten. This suspended a little justified criticism of him.\nIn autumn 1865, he was asked by Prime Minister Count Richard Belcredi to head the Department of Commerce. As Minister in a politically controversial government (in the contemporary press also as a \"Three Count Ministry\", referring to him as next to Prime Minister Count Belcredi still the Counts Larisch-Mönich and Mensdorff-Pouilly), he tried concluding trade treaties, plus dealt with the communications and postal services. He also led a program designed to supplement the rail network, which largely came to be implemented (see Memorandum 1866, \"A railway network for the Austrian monarchy\"). Under his leadership office, the expansion of the port of Trieste was also addressed. The Austro-Hungarian Compromise of 1867 led him to resign.\nBernhard von Wüllerstorf-Urbair died in Bolzano, which was then part of the old county of Tyrol. He is buried in the cemetery of the Old Parish Church of Gries in the district.\n\nThe following are selected works by Bernhard von Wüllerstorf-Urbair:\n\n\n\n\n(\"German\").\n"}
{"id": "75110", "url": "https://en.wikipedia.org/wiki?curid=75110", "title": "Biot–Savart law", "text": "Biot–Savart law\n\nIn physics, specifically electromagnetism, the Biot–Savart law ( or ) is an equation describing the magnetic field generated by a constant electric current. It relates the magnetic field to the magnitude, direction, length, and proximity of the electric current. The Biot–Savart law is fundamental to magnetostatics, playing a role similar to that of Coulomb's law in electrostatics. When magnetostatics does not apply, the Biot–Savart law should be replaced by Jefimenko's equations. The law is valid in the magnetostatic approximation, and is consistent with both Ampère's circuital law and Gauss's law for magnetism. It is named after Jean-Baptiste Biot and Félix Savart, who discovered this relationship in 1820.\n\nThe Biot–Savart law is used for computing the resultant magnetic field B at position r in 3D-space generated by a \"steady\" current \"I\" (for example due to a wire). A steady (or stationary) current is a continual flow of charges which does not change with time and the charge neither accumulates nor depletes at any point. The law is a physical example of a line integral, being evaluated over the path \"C\" in which the electric currents flow (e.g. the wire). The equation in SI units is\n\nIn a \"non\"-magnetostatic situation, the Biot–Savart law ceases to be true (it is superseded by Jefimenko's equations), while Gauss's law for magnetism and the Maxwell–Ampère law are still true.\n\n\n\n\n\n"}
{"id": "6587493", "url": "https://en.wikipedia.org/wiki?curid=6587493", "title": "Candareen", "text": "Candareen\n\nA candareen (; ; Singapore English usage: hoon) is a traditional measurement of weight in East Asia. It is equal to 10 cash and is of a mace. It is approximately 378 milligrams. A troy candareen is approximately .\n\nIn Hong Kong, one candareen is 0.3779936375 grams and, in the Weights and Measures Ordinance, it is ounces avoirdupois. In Singapore, one candareen is 0.377994 grams.\n\nThe word \"candareen\" comes from the Malay \"kandūri\". An earlier English form of the name was \"condrin\". The candareen was also formerly used to describe a unit of currency in imperial China equal to 10 li () and is of a mace. The Mandarin Chinese word \"fēn\" is used to denote of a Chinese renminbi yuan but the term \"candareen\" for that currency is now obsolete.\n\nOn 1 May 1878 the Imperial Maritime Customs was opened to the public and China's first postage stamps, the \"Large Dragons\" (), were issued to handle payment. The stamps were inscribed \"CHINA\" in both Latin and Chinese characters, and denominated in candareens.\n\n"}
{"id": "14304275", "url": "https://en.wikipedia.org/wiki?curid=14304275", "title": "Cork grease", "text": "Cork grease\n\nCork grease is a lip balm-like grease for woodwind and reed instruments such as saxophones, clarinets, bassoons, and oboes. It is used on the corks to make putting the instrument together possible without damaging the cork and therefore possibly damaging the instrument itself. Cork grease also acts as a preservative, keeping the wooden cork moist and thick, in turn ensuring a good seal between parts of the instrument so that no air may leak through joints upon playing. Cork grease can help woodwind players adjust their instruments' tuning pieces (e.g. barrels, necks, bocals, staples) in respect to their pitch.\n\nCork grease is made with ingredients such as elm extract, organic sunflower oil,coconut oil and hemp seed oil. In past it was made from animal fat. Overall that is natural organic balm widely used for woodwind instruments.\n"}
{"id": "7404442", "url": "https://en.wikipedia.org/wiki?curid=7404442", "title": "Delta Electricity", "text": "Delta Electricity\n\nDelta Electricity is an electricity generation company in Australia. The company was formed by the Government of New South Wales in 1996 as part of its reform of the electricity sector in the State, which saw the breakup of the Electricity Commission of New South Wales. Delta Electricity, which at the time owned only the Vales Point Power Station, was sold to Sunset Power International for $1 million in November 2015. It has a portfolio of generating sites mainly using thermal coal power.\n\nDelta Electricity, as a State-owned corporation has owned and operated the following power stations to generate electricity for sale under contract. Since December 2015, Delta Electricity only operates the Vales Point Power Station.\n\nDelta Electricity was formed by the Government of New South Wales in 1996 as part of its reform of the electricity sector in the State, which saw the breakup of the Electricity Commission of New South Wales.\n\nFollowing a report by the Health Rivers Commission, in 1998 the Minister for Urban Affairs and Planning, Craig Knowles, announced that a small hydro-electric power station would be installed in the Chichester Dam to generate electricity, reduce greenhouse emissions and allow surplus power to be sold back to the State grid. The mini-power station was completed in 2001 and operated by Delta Electricity, and generates up to of electricity at times of peak flow; with an average annual generation of .\n\nAs the Keneally Labor government moved to privatise components of the electricity industry in New South Wales including the electricity trading rights of Delta Electricity, on 14 December 2010 four of the five directors of Delta (including the chairman) suddenly stood down in protest over the proposed sale. On 28 February 2011, at the direction of the New South Wales Government, the newly constituted Board of Delta entered into contracts with energy retailer, TRUenergy, for the supply of electricity under Generation Trading Agreements from the Wallerawang and Mount Piper Power Stations. A subsequent NSW Parliamentary Inquiry was held, but the directors of Delta who resigned refused to give evidence before the Inquiry unless guarantees of parliamentary privilege would be given by the Government. Keneally refused to provides guarantees and, according to the Inquiry chairman, the Government stymied the Inquiry's ability to uncover the facts as to the resignation of the directors.\n\nIn May 2012, the New South Wales Parliament passed legislation to sell the State-owned generators. In July 2013, EnergyAustralia acquired from Delta Electricity Wallerawang and Mount Piper Power Stations, near Lithgow, New South Wales, for A$160 million. In November 2014, EnergyAustralia announced that it would permanently close Wallerawang due to ongoing reduced energy demand, lack of access to competitively priced coal and the power station’s high operating costs. EnergyAustralia began the process of removing useful equipment from the station in 2015 and began demolition of the site when this process has been completed.\n\nIn early 2015, the Colongra Power Station at Lake Munmorah was sold to Snowy Hydro. In November 2015, Delta Electricity, which at the time owned only the Vales Point Power Station, was sold to Sunset Power International for $1 million. Delta Electricity was dissolved in October 2016. The NSW Government retained ownership of the decommissioned Munmorah Power Station (Generation Property Management) which is being demolished.\n\n"}
{"id": "1789624", "url": "https://en.wikipedia.org/wiki?curid=1789624", "title": "Dewetting", "text": "Dewetting\n\nIn fluid mechanics, dewetting is one of the processes that can occur at a solid–liquid or liquid–liquid interface. Generally, dewetting describes the process of retraction of a fluid from a non-wettable surface it was forced to cover. The opposite process—spreading of a liquid on a substrate—is called wetting. The factor determining the spontaneous spreading and dewetting for a drop of liquid placed on a solid substrate with ambient gas, is the so-called spreading coefficient S: \n\nformula_1\n\nwhere formula_2 is the solid-gas surface tension, formula_3 is the solid-liquid surface tension and formula_4 is the liquid-gas surface tension (measured for the mediums before they are brought in contact with each other).\n\nWhen formula_5, the spontaneous spreading occurs, and if formula_6, partial wetting is observed, meaning the liquid will only cover the substrate to some extent . \n\nThe equilibrium contact angle formula_7is determined from the Young-Laplace equation. \n\nSpreading and dewetting are important processes for many applications, including adhesion, lubrication, painting, printing, and protective coating. For most applications, dewetting is an unwanted process, because it destroys the applied liquid film.\n\nDewetting can be inhibited or prevented by photocrosslinking the thin film prior to annealing, or by incorporating nanoparticle additives into the film. \n\nSurfactants can have a significant effect on the spreading coefficient. When a surfactant is added, it's amphiphilic properties cause it to be more energetically favorable to migrate to the surface, decreasing the interfacial tension and thus increasing the spreading coefficient (i.e. making S more positive). As more surfactant molecules are absorbed into the interface, the free energy of the system decreases in tandem to the surface tension decreasing, eventually causing the system to become completely wetting.\n\nIn biology, by analogy with the physics of liquid dewetting, the process of tunnel formation through endothelial cells has been referred to as cellular dewetting.\n\nIn most dewetting studies a thin polymer film is spin-cast onto a substrate. Even in the case of formula_6 the film does not dewet immediately if it is in a metastable state, e.g. if the temperature is below the glass transition temperature of the polymer. Annealing such a metastable film above its glass transition temperature increases the mobility of the polymer-chain molecules and dewetting takes place.\n\nThe process of dewetting occurs by the nucleation and growth of randomly formed holes, which coalesce to form a network of filaments, before breaking into droplets. When starting from a continuous film, an irregular pattern of droplets is formed. The droplet size and droplet spacing may vary over several orders of magnitude, since the dewetting starts from randomly formed holes in the film. There is no spatial correlation between the dry patches that develop. These dry patches grow and the material is accumulated in the rim surrounding the growing hole. In the case where the initially homogeneous film is thin (in the range of ), a polygon network of connected strings of material is formed, like a Voronoi pattern of polygons. These strings then can break up into droplets, a process which is known as the Plateau-Rayleigh instability. At other film thicknesses, other complicated patterns of droplets on the substrate can be observed, which stem from a fingering instability of the growing rim around the dry patch.\n"}
{"id": "12220033", "url": "https://en.wikipedia.org/wiki?curid=12220033", "title": "Dolphin Energy", "text": "Dolphin Energy\n\nDolphin Energy is a gas company of Abu Dhabi, United Arab Emirates. It was established in March 1999 by the Government of Abu Dhabi. As of today, Dolphin Energy is owned by Mubadala Development Company, on behalf of the Government of Abu Dhabi, (51% of shares), Total S.A. (24.5%) and Occidental Petroleum (24.5%). It as also a subsidiary based in Doha, Qatar.\n\nThe major project of Dolphin Energy is the Dolphin Gas Project, which involves the production and processing of natural gas from Qatar's North Field, and transportation of the gas by offshore pipeline to the United Arab Emirates and Oman. In July 2007, the company announced it began production of gas from Qatar's North Field. The gas is processed at Gas Processing Plant in Qatar's Ras Laffan Industrial City and then transported to the refined methane by sub-sea export pipeline from Qatar to Dolphin's Gas Receiving Facilities at Taweelah in Abu Dhabi.\n"}
{"id": "4022308", "url": "https://en.wikipedia.org/wiki?curid=4022308", "title": "El Centro Integrado de Technologia Appropriada", "text": "El Centro Integrado de Technologia Appropriada\n\nEl Centro Integrado de Tecnología Apropiada (CITA) is a Cuban appropriate technology organization. It is the equivalent of the UK's Centre for Alternative Technology. It is stationed at Camagüey (Circunvalación Norte). \n\nIn 2005 a representative of CITA participated in the \"Public Seminar Series\" of the International Institute for Resource Industries and Sustainability Studies (IRIS), on the topic \"Renewable Energy in Cuba: Sustainable Solutions for the Future (April, 2005)\". \n\nCentro Integrado de Tecnología Apropiada (CITA)\n\nInternational Institute for Resource Industries & Sustainability Studies (IRIS)\n"}
{"id": "7313417", "url": "https://en.wikipedia.org/wiki?curid=7313417", "title": "Electricity Authority of Cyprus", "text": "Electricity Authority of Cyprus\n\nThe Electricity Authority of Cyprus (EAC) (Greek: \"Αρχή Ηλεκτρισμού Κύπρου (ΑΗΚ)\") was founded in 1952 by the British colonial government. The 28 private electricity companies of the time were nationalized and absorbed into the EAC. The Authority never received any subsidies from the government as these have always been prohibited by law. Its head office is located in Strovolos. The EAC currently holds a near monopoly on electricity generation in Cyprus. It operates through three power stations with a total capacity of 1460 MW:\n\n\nThe company also distributes electricity produced by five privately held windfarms:\n\n\nAdditionally, individuals, private companies, and the Government own almost 54 MW of solar panels and almost 10 MW of biofuel installations and the EAC distributes the electricity these produce too.\n\nIn 2015, the EAC generated a total of 4,128 GWh of electricity consuming 947,226 tonnes of fuel costing €288,632,000. Maximum demand in the areas controlled by the Republic of Cyprus reached 939 MW. A total of 2.0 GWh of the produced electricity in 2015 valued €240,000 ended up in the area occupied by Turkey and no money could be collected for it.\n\nThe Authority served 559,700 customers in 2015, that is 280 per employee, up from 260 in 2014. The electricity sales per employee reached 2.02 GWh, up from 1.83 GWh in 2014.\n\nCompany investments in 2015 on its assets reached €17,721,000.\n\nOn 11 July 2011, a total of 98 containers of munitions stored at Evangelos Florakis Naval Base adjacent to Vasilikos Power Station exploded causing extensive damage to the station. To cope with the extended loss of its largest power station, the Authority had to impose rolling blackouts. In 2010, the maximum demand had reached 1,144 MW, the highest ever, and an even higher one was expected in 2011.\n\nAs a precondition to the accession of Cyprus to the European Union, the local market for electricity generation has been opened to private companies, but so far no private power plants have been built, although four licenses have been granted by Cyprus Energy Regulatory Authority. In the meanwhile, the EAC diversified into communication and cable television services in cooperation with a private company.\nIn 2005, the company was involved in a high profile scandal involving the alleged theft of millions of Cyprus pounds from the employees' pension scheme.\n\n"}
{"id": "19079887", "url": "https://en.wikipedia.org/wiki?curid=19079887", "title": "Energy in Finland", "text": "Energy in Finland\n\nEnergy in Finland describes energy and electricity production, consumption and import in Finland. Energy policy of Finland describes the politics of Finland related to energy. Electricity sector in Finland is the main article of electricity in Finland.\n\nFinland lacks domestic sources of fossil energy and must import substantial amounts of petroleum, natural gas, and other energy resources, including uranium for nuclear power.\n\nEnergy consumption in Finland per capita is the highest in European Union. Reasons for this include industries with high energy consumption (half of energy is consumed by industry), high standards of living, cold climate (25% of consumption is used in heating) and long distances (16% of consumption is used in transport).\n\nThere was no sustainable decline in CO emission in Finland during 1990–2007. The energy use decline 2008–2009 is based on recession and at least some paper industry factories relocation abroad. The annual changes of CO emissions of Finland were in some years 7–20% during 1990–2007. Increase of emissions was 18% in 1996 and 20% in 2006. The peat energy use and CO emissions per capita had correlation in 1990–2007.\n\nThe share of electricity generated from renewable energy in Finland has been stable from 1998 to 2005: 11 to 12 percent plus yearly changing hydropower, together around 24 to 27 percent. The RE of total energy has been 24 percent (1998 to 2005). The forest industry black liquor and forest industry wood burning were 57 percent (1990) and 67 percent (2005) of the RE of total energy. The rest is mainly water power. The most of available hydropower for energy is already in use.\n\nFinal consumption of energy - i.e. after losses through transformation and transmission - was 1 102 petajoules, which equals 202 gigajoules per capita in 2013. \nOf this, 46% is consumed by industry, 16% in transportation and 25% in heating.\n\nEnergy consumption per capita in Finland is the highest in EU. Reasons for this include energy-intensive industry, a high standard of living, a cold climate and long distances. Rise of energy consumption stopped in the 21st century, mainly due to changes of industry. There is now less heavy industry and the energy efficiency has improved. New energy consuming business is the data centres of international enterprises.\n\nEnergy consumption increased 44 percent in electricity and 30 percent in the total energy use from 1990 to 2006. The increase in electricity consumption 15,000 GWh from 1995 to 2005 was more than the total hydropower capacity. The electricity consumption increased almost equally in all sectors (industry, homes, and services).\n\nAmong all industries, the heaviest users are paper and pulp industry, metal industry, oil refining and chemical industry.\n\nThe forest industry uses 30 percent of all electricity in Finland (1990–2005). Its process wastes, wood residues and black liquor, gave 7000–8000 GWh RE electricity in 2005. In the year 2005 this and electricity consumption fell 10% compared to 2004 based on the long forest industry strike.\n\nEnergy consumption for heating has increased, when population and average size of homes has grown.\n\n80% of the energy use of households was spent on heating in 2008–2011.\n\nTransport uses 30% of all energy, but 40% of the energy produced with oil. Consumption per kilometre has decreased, but amount of kilometres has grown.\n\nThe Kyoto agreement had obligation to restrict the traffic emissions in Finland between 2008–12 in the year 1990 level. According to Ministry report in 2004 the share of public transport in Finland is lower that in the most European countries.\n\nIn 2014, the energy products import was worth 10 billion euros.\n\nIn January to September 2016 the import of energy products was worth of 5 billion euros, 15% less than the year before. The biggest source was Russia, 64% of all imports. Export was worth of 2.8 billion euros, one percent more than the previous year. 78% of export went to OECD countries. Decrease of price of petroleum products has affected the decrease of value of imported energy.\n\nIn 2009 the consumption of energy sources in electricity generation by mode of production was: 28% nuclear power, 16% hydro power, 13% coal, 11% natural gas, 5% peat and 10% wood fuels and other renewables. Net imports of electricity in 2009 were 15%. In 2011, 16% of electricity consumption was derived from imported electricity.\n\nEuropean Commission (EC) demanded for Finland a €32,000-day penalty from the Court of Justice of the European Union in March 2013 for not implementing the electricity directive in time.\n\nThe share of electricity generation from renewables in Finland was 40% 2012 and target 33% by 2020. In comparison, unlike Finland most countries have target to increase the share of electricity generation from renewables from 2012 to 2020 in Europe as:\n\nDuring first half of 2015 the most important sources were wood (26% of total consumption), oil (23%), nuclear (18%), coal (9%), gas (7%), hydropower (5%) and peat (5%). Wind power covered one per cent of consumption, other sources in total four per cent.\n\nThere are no fossil fuels in Finland.\n\nAbout one quarter of energy production in Finland comes from burning wood. There are no forests grown for fuel. Instead, most firewood is byproduct of other uses of wood. The black liquor (byproduct of pulp production) and peel and branches (byrpduct of sawmill industry) are used by the forest industry itself in creating its own energy by wood burning.\n\nThe Finnish Association for Nature Conservation (FANC) demands Finland not to burn stumps and sturdy wood that are 15% of wood chips burned according to government energy policy. According to Chatham House such a bioenergy increase the climate change. According to Otto Bruun Finnish government bioenergy aims will annually reduce Finland’s carbon sink more than the combined air and road travel climate change emissions.\n\nFinland does not have any of its own petroleum resources, so it relies 100% on petroleum imports. In 2007 oil imports were almost 11 million tonnes in Finland. In 2006, Finnish oil imports came from Russia (64 percent), Norway (11 percent), Denmark (11 percent), and the rest from United Kingdom, Kazakhstan, and Algeria. Petroleum comprises 24 percent of Finnish energy consumption. Most of petroleum is used in vehicles, but about 260,000 homes are heated by heating oil.\n\nNeste Oil is the sole oil refiner in Finland, exporting petroleum products such as gasoline and fuel oil to Baltic countries and North America. Oil imports were valued at 6.5 billion euros and exports 3 billion euros in 2006.\n\nAs of 2008, Finland's nuclear power program has four nuclear reactors in two power plants. The first of these came into operation in 1977. In 2000–2014 the four units have produced annually 21.4–22.7 TWh electricity, which has been 27–35% of energy production and 24–28 % of energy consumption in Finland. They are among the world's most efficient, with average capacity factors of 94% in the 1990s. A fifth nuclear reactor is under construction in Olkiluoto Nuclear Power Plant. The unit is expected to start up by 2019.\n\nIf all planned projects are completed, the share of electricity produced by nuclear could double by 2025, reaching around 60%.\n\nRussian Rosatom is the major owner of future nuclear plant Fennovoima with a share of 34%. Finnish municipals and cities are also major owners. According to Greenpeace in February 2017 there were initiatives to phase out from this nuclear plant in Oulu, Helsinki, Vantaa, Hamina and Nurmijärvi.\n\nCoal is imported from Russia and Poland. 5.6 million tonnes were used in 2007.\n\nAccording to Finnwatch (27 September 2010) there are 13 coal power plants in Finland. The companies Pohjolan Voima, Fortum, Helsingin Energia and Rautaruukki consume coal most. According to the statistics of the Customs 18.3 million tonnes of coal was imported in Finland between 2007–2009 from: 72.5% Russia; 7.3% USA; 6.6% Canada; 5.9% Australia; 3.0% Poland, 1.4% South Africa; 1.3% Colombia and 1.1% Indonesia. The majority of Finnish coal is mined in the Kuznetsk Basin of the Kemerovo Oblast, Russia.\n\nThe Finnish companies know the country of origin of coal. The specific mine of origin is not always known, especially for the coal blends. According to the Finnwatch inquiry in 2010 none of the Finnish companies have yet made a commitment to give up coal consumption. Based on new investments, companies reported the following reductions in their future coal use: Helsingin Energia −40 % by 2020, Lahti Energia several tens of % by 2012 and Vantaan Energia −30 % by 2014.\n\nThe ILO Agreement 176 (1995) addresses health and safety risks in mines. Finland ratified the agreement in 1997. However, as of 2017 the agreement was not ratified in the following countries that export coal to Finland: Canada, Australia, Colombia, Kazakhstan, Indonesia and China. At least two companies in Finland reported (2010) using the UN Global Compact initiative criteria in their supplier relationships. No Finnish company reported signing the UN Global Compact Initiative. According to the DanWatch report ”The Curse of Coal” Danish DONG Energy and Swedish Vattenfall have underlined UN Global Compact Initiative.\n\nIn 2010 the share of gas in Total Primary Energy Supply (TPES) was about 10%. Finland was 100% dependent on a single supplier in gas, namely Russia, and there is no gas storage capacity. However, in Finland, gas is essentially never used in direct heating of homes, which are heated by direct electric heating, oil or district heating. 75% of gas is used for production of electricity or combined heat and power and in industry, with domestic use being rare. In total, 93% of the gas is sold to large installations directly rather than by retail. In Helsinki, however, there are 30,000 network-connected domestic gas users and 300 restaurants. There is an alternative fuel obligation, so that in the event of a gas supply disturbance, other fuels can be immediately substituted. The gas distribution network reaches only the southeastern coast, with the northernmost point at Ikaalinen.\n\nThe neighbouring country Sweden was 100% dependent on Danish gas in 2010. The share of gas in Sweden was lower than in Finland, 3.5% in 2009 (13 Twh gas /376 Twh total final use). The gas dependency in Finland and Sweden was less than in average in OECD countries in 2010. 16 out of 28 IEA member countries are dependent on gas over 20% in TPES.\n\nNatural gas has been used since 1974 after the first oil crisis. Gasum is the Finnish importer and seller of natural gas, which owns and operates Finnish natural gas transmission system. Natural gas vehicles aren't popular in Finland, but natural gas powered busses exist.\n\nPeat and hard coal are the most harmful energy sources for global warming in Finland. According to VTT studies peat is often the most harmful one.\n\nPeat was the most popular energy source in Finland for new energy investments 2005–2015. The new energy plants in Finland starting 2005–2015 have as energy source: peat 36% and hard coal 11%: combined: 47%. The major carbon dioxide emitting peat plants during 2005–15 are/will be (CO2 kt): PVO 2700 kt, Jyväskylän Energia 561 kt, Etelä-Pohjanmaan Voima Oy (EPV Energia) 374 kt, Kuopion Energia 186 kt, UPM Kymmene 135 kt and Vapo 69 kt. EPV Energy is partner in TVO nuclear plants and Jyväskylän and Kuopion Energia partners in Fennovoima nuclear plants in Finland.\n\nAccording to IEA country report the Finnish subsidies for peat undermine the goal to reduce CO emissions and counteracts other environmental policies and The European Union emissions trading scheme. IEA recommends to adhere to the timetable to phase out the peat subsidies in 2010. “To encourage sustained production of peat in the face of negative incentives from the European Union's emissions trading scheme for greenhouse gases, Finland has put in place a premium tariff scheme to subsidise peat. The premium tariff is designed to directly counter the effect of the European Union's emissions trading scheme.”\n\nFinland has more than 330 hydropower plants, with total production of 3100 megawatts.\n\nEnergy companies have no renewable energy obligations in Finland.\n\nThe share of renewable energy in per cent in Finland was 28% in 2012 and 25% in 2000. The share of renewable energy 5 years average 2006–2010 was 24.7 % and 10 years average 2001–2010 was 26.0 %. The EU set target for Finland (38%) by 2020 was reached in 2014, and in 2015 39.3 % of consumed energy was renewable (or 35% based on national calculation: i.e. 454,6 PJ (renewable energy)/ 1306,3 PJ (total energy consumption)). The new target value for renewable energy set by Finland is 50% by 2030 (based on end consumption values). The share of renewable energy in Finland:\n\n\nThe total renewable energy generating capacity has increased in Finland during the 2010s (in 2010: 5,170 MW; 2016: 7,067 MW). In 2016 the estimated renewable energy production was over 130 terawatt-hours in Finland. \n\nRenewable energy sources (Statistics Finland, 2015)\n\nThe renewable energy objectives set by the European Union are 22 percent renewable source electricity and 12 percent renewable of primary energy by 2010 under the European Union directive 2003/30/EC (Directive on the Promotion of the use of biofuels and other renewable fuels for transport) and white paper. This includes the objectives of 40 GW wind power, 3 GW photovoltaics and 5.75 percent biofuels by 2010. \n\nIn 2008, Finland's greenhouse gas emissions totalled 70.1 million tons of carbon dioxide (CO2e). A little over three quarters of them were based on energy or released from the energy sector.\n\nThe carbon dioxide emissions by fossil fuels in 2008 originated from 45% oil, 39% coal and 15% natural gas. In the year 2000 the shares were nearly equal: 48% oil and 37% coal. The fossil traffic fuels: motor petrol, diesel and aviation petrol are oil products. The biomass included 47% of black liquer and 52% of wood in 2008. These shares were practically same during 1990–2006. All biomass and agricultural warming gas emissions are free of charge in the EU emissions trading in 2008–2012. According to the official statistics the annual fossil fuel and coal emissions in Finland have large annual variation. E.g. the fossil fuel CO emissions dropped 18% in the year 2005 and 13% in 2008, but the annual coal emissions increased 22% in 1996, 22% in 2001 and 58% in 2006.\n\nAccording to the energy statistics the major changing factors for the annual emission changes were the consumption of coal and peat. In 2006 the hard coal increase was 92% subject to industry (including energy producing industry) separate electricity generation from hard coal. At the same time the controversial peat consumption was increased. The district heating used 42% of hard coal in average 1990–2006, but its annual variation was small compared to the industry separate electricity generation.\n\nParticulate, the size of which is from a few nanometers to visible dust particles, are considered the most important environmental factor affecting human life. About half of particulates are of anthropogenic origin: traffic, industry and energy production. In Finland, the most important source is burning wood as fuel. Also the NO2 and SO2 gases become particulates in the atmosphere.\n\nThe objective of RE (2005) of electricity was 35% (1997–2010). However, (2006) the Finnish objective was dropped to 31.5% (1997–2010). According to 'Renewables Global Status Report' Finland aims to increase RE only 2% in 13 years. This objective to add the RE use with 2% in 13 years is among the modest of all the EU countries.\n\nThe public energy subsidies in Finland in 2013 were €700 million for fossil energy and €60 million for renewable energy (mainly wood and wind). An increased feed-in tariff was used for new wind power industry in 2011 to 2015.\n\n\n"}
{"id": "1985145", "url": "https://en.wikipedia.org/wiki?curid=1985145", "title": "Expedition (book)", "text": "Expedition (book)\n\nExpedition is a science fiction and speculative fiction book by artist-author Wayne Douglas Barlowe. Subtitled \"Being an Account in Words and Artwork of the 2358 A.D. Voyage to Darwin IV\", it is written as though published in the year 2366, five years after Barlowe's participation in a voyage to an alien planet, dubbed Darwin IV in honor of Charles Darwin.\n\nIn the 24th century the exploitation of the Earth's ecosystem has created an environment so toxic that mass extinctions have wiped out nearly half of its animal population. Most of the remaining fauna, save humans, have suffered horrible mutation. Aided by the benevolent and technologically superior alien race, the Yma, humanity begins to repair their ravaged world while simultaneously learning more about the universe around them. When an unmanned Yma probe discovers evidence of alien life on another planet, the titular \"expedition\" is sent to investigate.\n\nBarlowe writes as a sort of 24th century Audubon, presenting his findings in a collection of paintings, sketches, field notes, and diary entries from his explorations of Darwin IV. He details a bewildering variety of alien lifeforms such as Gyrosprinters, Arrowtongues, Grovebacks, Daggerwrists, Skewers, Emperor Sea Striders, and Eosapians. Unlike the aliens presented in much of popular science fiction, which often seem to be variations of terrestrial lifeforms, Barlowe's creatures are truly alien: none of them possess eyes or true jaws; their body structures are often unlike any found on Earth; and they have unique modes of locomotion, sensing, and eating. Very late in the expedition, the explorer encounters lifeforms which use tools (the Eosapiens), giving a very strong indication they are intelligent.\n\nA conservationist theme is present throughout the book. The expedition is designed to have as minimal an impact as possible on Darwin IV's environment. When two of the expedition's members suffer a fatal accident, Yma technology is used to remove all traces of the accident from Darwin IV's environment. At the conclusion of the expedition, Darwin IV is left in the same pristine state it was in prior to the expedition, with the exception of a metal obelisk placed in a remote area by the expedition.\n\nA portion of the book, \"Sea Strider Skull and Littoralope\" was reprinted by the American Littoral Society.\n\nThe Discovery Channel produced a television special adapted from Barlowe's \"Expedition\", entitled \"Alien Planet\", which first aired on May 14, 2005. This program was faithful to the book in its presentation of the lifeforms found on Darwin IV. However, instead of being presented as the artist's own experiences, the program is presented as the findings of two autonomous robotic probes.\n\n"}
{"id": "17118362", "url": "https://en.wikipedia.org/wiki?curid=17118362", "title": "Federal University of Petroleum Resources Effurun", "text": "Federal University of Petroleum Resources Effurun\n\nThe Federal University of Petroleum Resources Effurun (FUPRE) in Delta State, Nigeria was established and approved by the Federal Executive Council meeting of 14 March 2007 and admitted its first set of undergraduates in 2008.\n\nThe university was established under the Federal Government of Nigeria initiative to build a specialized university in the Niger Delta to produce manpower and expertise for the oil and gas sector.\n\nThe National Universities Commission(NUC) approved sharing of facilities between the Petroleum Training Institute Effurun and FUPRE until it moved to its permanent site on development of its main campus at Ugbomro, Uvwie Local Government Area in 2010.\n\nThe administrative structure of the university consists of:\n\n\n\nThe institution presently have two colleges with ten departments. The colleges are:\n\nThe College of Science and College of Technology has started running courses for its academic session.\n\nThe main focus is petroleum engineering and technology related courses such as:\n\nThe institution presently has three educational programmes:\n\n"}
{"id": "49752646", "url": "https://en.wikipedia.org/wiki?curid=49752646", "title": "Fluorcarmoite-(BaNa)", "text": "Fluorcarmoite-(BaNa)\n\nFluorcarmoite-(BaNa) is a rare phosphate mineral, belonging to arrojadite group, with the formula Ba[]NaNa[]CaMgAl(PO)(POOH)F. It is a barium-rich member of the group, as is arrojadite-(BaNa), arrojadite-(BaFe), fluorarrojadite-(BaFe) and an unapproved species ferri-arrojadite-(BaNa). The \"-(BaNa)\" suffix informs about the dominance of the particular elements (here barium and sodium) at the corresponding structural sites.\n\nThe arrojadite group is defined in form of the complex, general formula ABCaNaMAl(PO)(POOH)W, where:\nThe two suffixes in the name correspond to A1 and B1 sites. Third suffix may be present in special cases.\n"}
{"id": "18708307", "url": "https://en.wikipedia.org/wiki?curid=18708307", "title": "Ford Taurus (fourth generation)", "text": "Ford Taurus (fourth generation)\n\nThe fourth-generation Ford Taurus is an automobile that was produced by Ford for the 2000 to 2007 model years. While mechanically similar to its 1996-1999 predecessor, major revisions to the bodyshell of the sedan were done to alter its controversial styling as well as add interior room; it was available in four-door sedan and five-door station wagon models.\n\nThe fourth-generation Taurus would be the final derived from the original 1986 model line. Slotting between the mid-size and full-size sedan classes, the Taurus was closer in exterior and interior dimensions, as well as engine options, to the Chevrolet Impala and Dodge Intrepid (than the Malibu and Stratus). In 2004 and 2005, as part of its effort to increase the use of globally sourced platforms, Ford introduced the Volvo-developed Five Hundred and Mazda-developed Fusion to fill the slot of the Taurus in the Ford line (for non-fleet buyers, the Five Hundred also served as a replacement for the Crown Victoria).\n\nThe Taurus nameplate returned in 2008, as Ford renamed the Five Hundred to increase its sales.\n\nWhen the third generation Taurus debuted, it was hurt by criticism of its design, which was formed from oval derived design elements. The design was very controversial, and it strongly limited the appeal of the car. As a result, for the fourth generation Taurus, Ford designed it with a more subdued, angular design, as part of Ford's New Edge styling, in hopes of increasing the car's appeal. Instead of sloping back, this car's trunk stood upright in a more traditional shape, which greatly increased trunk space. The roof was also raised into a more upright stance to increase headroom, which can be seen by the thicker C-pillar and larger area between the tops of the doors and the top of the roof.\n\nThe front and rear clips were also redesigned on the Taurus and Sable sedans; all body panels were brand-new except the doors. Station wagons received the new front clips but from the firewall back they were essentially the same as the 1996-1999 wagons. The Taurus now had the turn signals integrated into the headlamps, similar to that of the previous generation Sable. The front bumper was also redesigned to include a larger front grille which, like the previous generation, contained a chrome bar running through the middle containing the Ford logo. The rear clip was redesigned with a larger trunk and trunklid, as mentioned above, as well as giving the Taurus two large taillights as opposed to the rear lightbar used in the previous generation cars. Mounted on the trunklid was a large chrome bar containing the Ford logo, like in the front. In 2003 for the 2004 model year, the front clip was slightly redesigned, and the Taurus got a new front bumper and grille. The grille was made smaller, with the chrome bar removed, replaced by just a large Ford logo in the center. The taillights were slightly redesigned, originally to include amber turn signals, but this was cancelled at the eleventh hour. Instead the rear was given larger reverse lights, and the chrome bar above the license plate bracket was deleted.\n\nAs with the exterior, the interior was completely redesigned with a more conservative style, although some features from the previous cars were carried over. The dashboard had a more linear appearance, instead of curving around the driver. The \"Integrated Control Panel\" was carried over but enlarged, reshaped, and placed in the center of the dash instead of being tilted toward the driver. The Flip-Fold center console was also carried over but it was revamped as well. When folded out, it now rested against the floor instead of the dashboard, and had different cupholders and storage areas. Unlike previous Tauruses, this one offered rear cupholders that either slid or folded out of the front console, depending on which console the car was equipped with.\n\nThis Taurus' interior was available in two configurations; a front bench seat with a column mounted shifter and the Flip-Fold center console, or bucket seats with a traditional console and a floor mounted shifter. The configuration for a steering column mounted shifter and a center console, which made a brief return for 1999, had been dropped. The interior also contained many new safety features; side airbags, tether straps, and a glow in the dark trunk release mounted inside of the trunk. This interior also contained a new system which Ford called the \" Advanced Personal Safety System\". This system, at the time of a collision, would detect the driver and passenger's positions as well as seatbelt usage, and would inflate the airbags to match, possibly preventing airbag-related injuries. For 2004, the interior got a minor revision. This included a new steering wheel with a center airbag pad that was shaped like an upside-down taco and new gauges with a diagnostic center that would tell if there were any problems with the car, as well as average fuel economy. It also was able to perform a \"system check\" at the driver's request to make sure that the engine was functioning properly.\n\nThe two 1999 models were carried over, and two more were added. The most basic model was the LX, with the SE as the mid priced model. Two new trim levels were offered for 2000: the SE Special Value Group and the top-end model SE Comfort. These new trim levels were renamed the following year to SES and SEL respectively. The SES and LX models were dropped in 2005, leaving the SE and SEL.\n\nThe SEL line received a few upgrades for 2003, to give it a more upscale image. Among the changes were a new instrument cluster, wheels, as well as a slight redesign of the dash, with woodgrain replacing the black trim. It also got woodgrain on the steering wheel rim and around the power window switches on the front doors. Also in 2003, Ford created a Centennial Edition Taurus to celebrate Ford's 100th anniversary. This special Taurus included many extras, such as lighter wood trim, special leather seats, headlights with black accents, special wheels, a special leather case for the owner's manual, a leather jacket that said \"Ford: 100 Years\", a similar watch, and a letter from William Clay Ford, Jr. Production was limited to 4,000 units.\n\nFor 2002, 2003 and 2004, the SES model received a \"Sport\" package, which consisted of five-spoked rims known as \"slicers\", and the Duratec engine standard. In addition, the exterior of the vehicle received Sport badging on the front quarter-panels, the chrome bar on the grille was changed to body color, the interior received two-toned cloth seats, a two-toned dash applique, special \"Sport\" floor mats, and a leather-wrapped steering wheel. This model was only offered in four colors.\n\nThe engines were carried over from the previous generation, with the Vulcan being the only available engine on the LX and SE, producing and of torque, and as the base engine on the SES. The Duratec engine was optional on the SES and standard on the SEL, producing . For 2005, with the LX and SES models being dropped, the Duratec was only available on the SEL, and in 2006 the Duratec was dropped altogether, with the Vulcan becoming the standard and only engine available on the SEL. Some pre-2004 Vulcans were mated to the four speed AX4S automatic transmission; all other Tauruses of this generation received the AX4N transmission.\n\nThe Mercury Sable, the sister model of the Taurus aimed at a more upscale audience, was also redesigned for 2000. Like with previous generations, the Sable shared all mechanical components with the Taurus with a unique body. The new Sable was also largely carried over from the previous model, with changes limited a new front fascia, rear fascia, wheels, a taller roof and trunk, and a new interior, though the interior was the same design of the Taurus with added woodgrain trim. Like with previous generations, the Sable offered the same powertrains and equipment as the Taurus. The Sable was again offered in GS and LS models in wagon and sedan body styles, with a new top-of-the-line LS Premium trim. An LS Platinum edition was also briefly offered. The Sable was updated in 2004 with a new grille, front bumper, steering wheel, instrument cluster, wheels and taillamps. The Sable was discontinued in 2005 and replaced by the Mercury Montego (which would be renamed as the Sable in 2008) and Mercury Milan. This generation of Sable was not sold in Canada, as the Mercury nameplate had been discontinued there by the 2000 model year.\n\nTaurus sales had slumped significantly in the years prior to its demise, losing significant market share to counterparts from Japan. The Taurus, which had at one time been the best-selling midsize sedan, had already lost its position when it was surpassed in sales by the Honda Accord and Toyota Camry, and in 2005 fell to #4 behind the Nissan Altima.\nDue to waning popularity and customer demand, Ford decided to slowly discontinue the Taurus. Production of the Taurus wagon was discontinued on December 8, 2004; sedan retail sales halted after a short 2006 model year, and the Taurus became sold exclusively to fleets in the United States, while still being sold to retail customers in Canada. Production ended on October 27, 2006, as Ford idled the Atlanta plant, as part of its \"The Way Forward\" restructuring plan.\n\nThe last Ford Taurus rolled off the assembly line around 7:00am, destined for delivery to S. Truett Cathy, owner of Chick-fil-A. Mr. Cathy's original restaurant was located across from the Ford Atlanta plant. There was no official event or function of any kind to mark the end of production. Rather than investing in an older nameplate, Ford had decided to replace the Taurus with the full-size Five Hundred and midsize Fusion sedans, as well as replacing the Taurus wagon with the Freestyle crossover SUV.\n\nThe discontinuation of the Taurus was controversial, especially in the context of Ford's well-publicized financial problems at the time. While many believed that the Taurus was discontinued because it could no longer compete in the growing sedan market, others believed that if Ford wanted to save the car, they could have easily done so. Autoblog went as far as calling the Taurus the biggest fall from grace in history, and even blamed Ford's current financial problems on their failure to keep the Taurus competitive, as well as how they focused nearly all of their development resources and marketing on trucks and SUVs. The Truth About Cars published a review/editorial also showing their disappointment at how Ford neglected the Taurus to the point where it became a \"rental car\".\n\nMSNBC interviewed many Ford workers who felt that Ford unjustly abandoned the car that had done so much to revitalize Ford and the US industry. In an October 25, 2006 USA Today editorial, \"How Ford starved its Taurus\" it was noted that the Japanese stick with their winners and make them better (such as the Toyota Corolla, which has been in continuous production since the 1960s), while Detroit automakers retires cars or entire division nameplates in search of \"the next big thing\".\nHowever, after Alan Mulally took position as Ford's CEO, rumors were rampant that he was interested in reviving the Taurus. These were fueled by the fact that he said in an interview with the Associated Press that he was baffled to find out that the Taurus had been discontinued when taking position as CEO at Ford, as well as stating that he believed that discontinuing the Taurus was a mistake, and that the Five Hundred should have been named \"Taurus\" from the beginning. The rumors of a possible Taurus revival were confirmed in mid-2007, when the revamped versions of the Five Hundred and Freestyle were unveiled as \"Taurus\" and \"Taurus X\" at the 2007 Chicago Auto Show, a decision that was influenced strongly by Mulally. In a later interview, Mulally explained that the fact that the Taurus was well known and had a positive brand equity associated with it strongly influenced his decision to revive the name.\n\n"}
{"id": "13772320", "url": "https://en.wikipedia.org/wiki?curid=13772320", "title": "Freeman Tilden", "text": "Freeman Tilden\n\nFreeman Tilden (August 22, 1883 – May 13, 1980) was one of the first people to set down the principles and theories of Heritage Interpretation in his 1957 book, \"Interpreting Our Heritage\".\n\nHis work with the United States National Park Service inspired generations of interpreters across the world and continues to be a definitive text for the discipline. According to thematic interpretation expert, Sam H. Ham, Tilden's quotation on page 38 of \"Interpreting Our Heritage\" (which was taken from a US National Park Service administrative manual) has become one of the most cited phrases in the interpretation literature worldwide:\n\n"}
{"id": "3474165", "url": "https://en.wikipedia.org/wiki?curid=3474165", "title": "Galaxite", "text": "Galaxite\n\nGalaxite, also known as 'mangan-spinel' is an isometric mineral belonging to the spinel group of oxides with the ideal chemical formula MnAlO.\n\nGalaxite is the manganese (Mn) rich endmember of the aluminium (Al) series of the spinel group. Divalent iron (Fe) and magnesium (Mg) readily substitute for the manganese in the crystal structure. Trivalent iron may also substitute for the aluminium. Thus, reflecting most natural samples, the formula may be better represented as (Mn,Fe,Mg)(Al,Fe)O.\n\nGalaxite generally occurs as small granular aggregates with a red-brownish tone. It has a vitreous luster and leaves a brownish-red streak. It is rated 7.5 on the Mohs Scale.\n\nIt was first described in 1932 for an occurrence at Bald Knob, Alleghany County, North Carolina near its namesakes, the town of Galax, Virginia, named after the plant galax or wandflower which grows in the area.\n\nIt occurs in carbonate-rich metamorphosed manganese ore deposits. It occurs associated with alleghanyite, rhodonite, sonolite, spessartine, tephroite, kutnohorite, manganhumite, jacobsite, kellyite and alabandite in the Bald Knob area. Associated minerals include katoptrite, magnetite, manganostibite, magnussonite, tephroite, manganhumite and manganosite in the Brattfors mine area of Nordmark, Värmland, Sweden.\n\nIt is sometimes used as a gemstone.\n\n"}
{"id": "13412633", "url": "https://en.wikipedia.org/wiki?curid=13412633", "title": "Graminivore", "text": "Graminivore\n\nIn zoology, a graminivore (not to be confused with a granivore) is an herbivorous animal that feeds primarily on grass (specifically \"true\" grasses, plants of the family Poaceae). The word is derived from Latin \"graminis\", meaning \"grass\", and \"vorare\", meaning \"to eat.\" Graminivory is a form of grazing. These herbivorous animals have digestive systems that are adapted to digest large amounts of cellulose, which is abundant in fibrous plant matter and more difficult to break down for many other animals. As such, they have specialized enzymes to aid in digestion and in some cases symbiotic bacteria that live in their digestive track and \"assist\" with the digestive process through fermentation as the matter travels through the intestines.\n\nHorses, cattle, capybara, hippopotamuses, geese, and giant pandas are examples of vertebrate graminivores. Some carnivorous vertebrates, such as dogs and cats, are known to eat grass occasionally. Grass consumption in dogs can be a way to rid their intestinal tract of parasites that may be threatening to the carnivore's health. Various invertebrates also have graminivorous diets. Many grasshoppers, such as individuals from the Acrididae family, have diets consisting primarily of plants from the Poaceae family.\n\nGraminivores generally exhibit a preference on which species of grass they choose to consume. For example, according to a study done on North American bison (\"Bison bison L.)\" feeding on shortgrass plains in north-eastern Colorado, the cattle consumed a total of thirty-six different species of plant. Of that thirty-six, five grass species were favoured and consumed the most pervasively. The average consumption of these five species comprised about 80% of their diet. A few of these species include \"Aristida longiseta Steud.,\" \"Muhlenbergia sp\"., and \"Bouteloua gracilis (H.B.K.) Lag\".\n\n \n"}
{"id": "11876198", "url": "https://en.wikipedia.org/wiki?curid=11876198", "title": "Grid-tie inverter", "text": "Grid-tie inverter\n\nA grid-tie inverter converts direct current (DC) into an alternating current (AC) suitable for injecting into an electrical power grid, normally 120 V RMS at 60 Hz or 240 V RMS at 50 Hz. Grid-tie inverters are used between local electrical power generators: solar panel, wind turbine, hydro-electric, and the grid.\n\nTo inject electrical power efficiently and safely into the grid, grid-tie inverters must accurately match the voltage and phase of the grid sine wave AC waveform.\n\nSome electricity companies pay for electrical power that is injected into the grid.\n\nElectricity companies, in some countries, pay for electrical power that is injected into the electricity utility grid. Payment is arranged in several ways.\n\nWith net metering the electricity company pays for the net power injected into the grid, as recorded by a meter in the customer's premises. For example, a customer may consume 400 kilowatt-hours over a month and may return 500 kilowatt-hours to the grid in the same month. In this case the electricity company would pay for the 100 kilowatt hours balance of power fed back into the grid. In the US, net metering policies vary by jurisdiction.\n\nFeed-in tariff, based on a contract with a distribution company or other power authority, is where the customer is paid for electrical power injected into the grid.\n\nIn the United States, grid-interactive power systems are specified in the National Electric Code, which also mandates requirements for grid-interactive inverters.\n\nGrid-tie inverters convert DC electrical power into AC power suitable for injecting into the electric utility company grid. The grid tie inverter (GTI) must match the phase of the grid and maintain the output voltage slightly higher than the grid voltage at any instant. A high-quality modern grid-tie inverter has a fixed unity power factor, which means its output voltage and current are perfectly lined up, and its phase angle is within 1 degree of the AC power grid. The inverter has an on-board computer that senses the current AC grid waveform, and outputs a voltage to correspond with the grid. However, supplying reactive power to the grid might be necessary to keep the voltage in the local grid inside allowed limitations. Otherwise, in a grid segment with considerable power from renewable sources, voltage levels might rise too much at times of high production, i.e. around noon with solar panels.\n\nGrid-tie inverters are also designed to quickly disconnect from the grid if the utility grid goes down. This is an NEC requirement that ensures that in the event of a blackout, the grid tie inverter shuts down to prevent the energy it transfers from harming any line workers who are sent to fix the power grid.\n\nProperly configured, a grid tie inverter enables a home owner to use an alternative power generation system like solar or wind power without extensive rewiring and without batteries. If the alternative power being produced is insufficient, the deficit is sourced from the electricity grid.\n\nGrid-tie inverters include conventional low-frequency types with transformer coupling, newer high-frequency types, also with transformer coupling, and transformerless types. Instead of converting direct current directly into AC suitable for the grid, high-frequency transformers types use a computer process to convert the power to a high-frequency and then back to DC and then to the final AC output voltage suitable for the grid.\n\nTransformerless inverters, which are popular in Europe, are lighter, smaller, and more efficient than inverters with transformers. But transformerless inverters have been slow to enter the US market because of concerns that transformerless inverters, which do not have galvanic isolation between the DC side and grid, could inject dangerous DC voltages and currents into the grid under fault conditions.\n\nHowever, since 2005, the NFPA's NEC allows transformerless, or non-galvanically isolated, inverters by removing the requirement that all solar electric systems be negative grounded and specifying new safety requirements. Amendments to VDE 0126-1-1 and IEC 6210 define the design and procedures needed for such systems: primarily, ground current measurement and DC to grid isolation tests.\n\nManufacturers datasheets for their inverters usually include the following data:\n\n\n"}
{"id": "2253553", "url": "https://en.wikipedia.org/wiki?curid=2253553", "title": "Haggertyite", "text": "Haggertyite\n\nHaggertyite is a rare barium, iron, magnesium, titanate mineral: Ba(FeTiMg)O first described in 1996 from the Crater of Diamonds State Park near Murfreesboro in Pike County, Arkansas. The microscopic metallic mineral crystallizes in the hexagonal system and forms tiny hexagonal plates associated with richterite and serpentinitized olivine of mafic xenoliths in the lamproite host rock. It is an iron(II) rich member of the magnetoplumbite group. It is a light grey opaque mineral with calculated Mohs hardness of 5. \n\nIt was named for geophysicist Stephen E. Haggerty (born 1938) of the Florida International University.\n\n"}
{"id": "1110875", "url": "https://en.wikipedia.org/wiki?curid=1110875", "title": "High-temperature electrolysis", "text": "High-temperature electrolysis\n\nHigh-temperature electrolysis (also HTE or steam electrolysis) is a technology for producing hydrogen from water at high temperatures.\n\nHigh temperature electrolysis is more efficient economically than traditional room-temperature electrolysis because some of the energy is supplied as heat, which is cheaper than electricity, and also because the electrolysis reaction is more efficient at higher temperatures. In fact, at 2500 °C, electrical input is unnecessary because water breaks down to hydrogen and oxygen through thermolysis. Such temperatures are impractical; proposed HTE systems operate between 100 °C and 850 °C.\n\nThe efficiency improvement of high-temperature electrolysis is best appreciated by assuming that the electricity used comes from a heat engine, and then considering the amount of heat energy necessary to produce one kg hydrogen (141.86 megajoules), both in the HTE process itself and also in producing the electricity used. At 100 °C, 350 megajoules of thermal energy are required (41% efficient). At 850 °C, 225 megajoules are required (64% efficient).\n\nThe selection of the materials for the electrodes and electrolyte in a solid oxide electrolyser cell is essential. One option being investigated for the process used yttria-stabilized zirconia (YSZ) electrolytes, nickel-cermet steam/hydrogen electrodes, and mixed oxide of lanthanum, strontium and cobalt oxygen electrodes.\n\nEven with HTE, electrolysis is a fairly inefficient way to store energy. Significant conversion losses of energy occur both in the electrolysis process, and in the conversion of the resulting hydrogen back into power.\n\nAt current hydrocarbon prices, HTE can not compete with pyrolysis of hydrocarbons as an economical source of hydrogen.\n\nHTE is of interest as a more efficient route to the production of hydrogen, to be used as a carbon neutral fuel and general energy storage. It may become economical if cheap non-fossil fuel sources of heat (concentrating solar, nuclear, geothermal) can be used in conjunction with non-fossil fuel sources of electricity (such as solar, wind, ocean, nuclear).\n\nPossible supplies of cheap high-temperature heat for HTE are all nonchemical, including nuclear reactors, concentrating solar thermal collectors, and geothermal sources. HTE has been demonstrated in a laboratory at 108 kilojoules (thermal) per gram of hydrogen produced, but not at a commercial scale. The first commercial generation IV reactors are expected around 2030.\n\nGiven a cheap, high-temperature heat source, other hydrogen production methods are possible. In particular, see the thermochemical sulfur-iodine cycle. Thermochemical production might reach higher efficiencies than HTE because no heat engine is required. However, large-scale thermochemical production will require significant advances in materials that can withstand high-temperature, high-pressure, highly corrosive environments.\n\nThe market for hydrogen is large (50 million metric tons/year in 2004, worth about $135 billion/year) and growing at about 10% per year (see hydrogen economy). This market is met by pyrolysis of hydrocarbons to produce the hydrogen, which results in CO2 emissions. The two major consumers are oil refineries and fertilizer plants (each consumes about half of all production). Should hydrogen-powered cars become widespread, their consumption would greatly increase the demand for hydrogen in a hydrogen economy.\n\nDuring electrolysis, the amount of electrical energy that must be added equals the change in Gibbs free energy of the reaction plus the losses in the system. The losses can (theoretically) be arbitrarily close to zero, so the maximum thermodynamic efficiency of any electrochemical process equals 100%. In practice, the efficiency is given by electrical work achieved divided by the Gibbs free energy change of the reaction.\n\nIn most cases, such as room temperature water electrolysis, the electric input is larger than the enthalpy change of the reaction, so some energy is released as waste heat. In the case of electrolysis of steam into hydrogen and oxygen at high temperature, the opposite is true. Heat is absorbed from the surroundings, and the heating value of the produced hydrogen is higher than the electric input. In this case the efficiency relative to electric energy input can be said to be greater than 100%. The maximum theoretical efficiency of a fuel cell is the inverse of that of electrolysis at the same temperature. It is thus impossible to create a perpetual motion machine by combining the two processes.\n\nHigh temperature electrolysis with solid oxide electrolyser cells has also been proposed to produce oxygen on Mars from atmospheric carbon dioxide, using zirconia electrolysis devices.\n\n"}
{"id": "13910416", "url": "https://en.wikipedia.org/wiki?curid=13910416", "title": "Higher sulfur oxides", "text": "Higher sulfur oxides\n\nHigher sulfur oxides are a group of chemical compounds with the formula SO where x lies between 0 and 1. They contain peroxo (O−O) groups and the oxidation state of sulfur is +6 as in SO.\nMonomeric SO can be isolated at low temperatures (below 78 K) following the reaction of SO and atomic oxygen or photolysis of SO–ozone mixtures. The favoured structure is:\n\nColourless polymeric condensates are formed in the reaction of gaseous SO or SO with O in a silent electric discharge. The structure of the polymers is based on β-SO (one of the three forms of solid SO) with oxide bridges (−O−) replaced randomly by peroxide bridges(−O−O−). As such these compounds are non-stoichiometric.\n"}
{"id": "28069094", "url": "https://en.wikipedia.org/wiki?curid=28069094", "title": "Holaniku at Keahole Point", "text": "Holaniku at Keahole Point\n\nHolaniku at Keahole Point is a 2MW micro-scaled concentrated solar power plant in the Kona District (west coast) of the island of Hawaii. It is located in the Natural Energy Laboratory of Hawaii at Keahole Point.\n\nHolaniku at Keahole Point is the first commercial solar thermal power plant to be built using solar collectors manufactured by Sopogy. The project was developed by Keahole Solar Power, LLC. an Engineering, Procurement, and Contracting (EPC) company. The plant contains over 1,000 Sopogy MicroCSP SopoNova parabolic trough solar collectors. The power plant uses the sun's heat to create steam. Most of the steam created is used onsite for other experimental uses, equating to only 0.5MW maximum that can be utilized to generate electricity. Little, if any, electrical power leaves the site and enters the state electrical grid.\n\n\n"}
{"id": "8892966", "url": "https://en.wikipedia.org/wiki?curid=8892966", "title": "Hydrocarbon dew point", "text": "Hydrocarbon dew point\n\nThe hydrocarbon dew point is the temperature (at a given pressure) at which the hydrocarbon components of any hydrocarbon-rich gas mixture, such as natural gas, will start to condense out of the gaseous phase. It is often also referred to as the HDP or the HCDP. The maximum temperature at which such condensation takes place is called the \"cricondentherm\". The hydrocarbon dew point is a function of the gas composition as well as the pressure.\n\nThe hydrocarbon dew point is universally used in the natural gas industry as an important quality parameter, stipulated in contractual specifications and enforced throughout the natural gas supply chain, from producers through processing, transmission and distribution companies to final end users.\n\nThe hydrocarbon dew point of a gas is a different concept from the water dew point, the latter being the temperature (at a given pressure) at which water vapor present in a gas mixture will condense out of the gas.\n\nIn the United States, the hydrocarbon dew point of processed, pipelined natural gas is related to and characterized by the term GPM which is the gallons of liquifiable hydrocarbons contained in of natural gas at a stated temperature and pressure. When the liquifiable hydrocarbons are characterized as being hexane or higher molecular weight components, they are reported as GPM (C6+).\n\nHowever, the quality of raw produced natural gas is also often characterized by the term GPM meaning the gallons of liquifiable hydrocarbons contained in of the raw natural gas. In such cases, when the liquifiable hydrocarbons in the raw natural gas are characterized as being ethane or higher molecular weight components, they are reported as GPM (C2+). Similarly, when characterized as being propane or higher molecular weight components, they are reported as GPM (C3+).\n\nCare must be taken not to confuse the two different definitions of the term GPM.\n\nAlthough GPM is an additional parameter of some value, most pipeline operators and others who process, transport, distribute or use natural gas are primarily interested in the actual HCDP, rather than GPM. Furthermore, GPM and HCDP are not interchangeable and one should be careful not to confuse what each one exactly means.\n\nThere are primarily two categories of HCDP determination. One category involves \"theoretical\" methods, and the other involves \"experimental\" methods.\n\nThe theoretical methods use the component analysis of the gas mixture (usually via gas chromatography, GC) and then use an equation of state (EOS) to calculate what the dew point of the mixture should be at a given pressure. The Peng–Robinson and Kwong–Redlich–Soave equations of state are the most commonly used for determining the HCDP in the natural gas industry.\n\nThe theoretical methods using GC analysis suffer from four sources of error:\n\n\nThe significant advantage of using the theoretical models is that the HCDP at several pressures (as well as the cricondentherm) can be determined from a single analysis. This provides for operational uses such as determining the phase of the stream flowing through the flow-meter, determining if the sample has been affected by ambient temperature in the sample system, and avoiding amine foaming from liquid hydrocarbons in the amine contactor. However, recent developments in combining experimental methods and software enhancements have eliminated this shortcoming (see Combined Experimental and Theoretical Approach below)\n\nGC vendors with a product targeting the HCDP analysis include Emerson, ABB, Thermo-fisher, as well as other companies.\n\nIn the \"experimental\" methods, one actually cools a surface on which gas condenses and then measures the temperature at which the condensation takes place. The experimental methods can be divided into manual and automated systems. Manual systems, such as the Bureau of Mines dewpoint tester, depend on an operator to manually cool the chilled mirror slowly and to visually detect the onset of condensation. The automated methods use automatic mirror chilling controls and sensors to detect the amount of light reflected by the mirror and detect when condensation occurs through changes in the reflected light. The chilled mirror technique is a first principle measurement. Depending on the specific method used to establish the dew point temperature, some correction calculations may be necessary. As condensation must necessarily have already occurred for it to be detected, the reported temperature is lower than when using theoretical methods.\nSimilar to GC analysis, the experimental method is subject to potential sources of error. The first error is in the detection of condensation. A key component in chilled mirror dew point measurements is the subtlety with which condensate can be detected — in other words, the thinner the film is when detected, the better. A manual chilled mirror device relies on the operator to determine when a mist has formed on the mirror, and, depending on the device, can be highly subjective. It is also not always clear what is condensing: water or hydrocarbons. Because of the low resolution that has traditionally been available, the operator has been prone to under report the dew point, in other words, to report the dew point temperature as being below what it actually is. This is due to the fact that by the time condensation had accumulated enough to be visible, the dew point had already been reached and passed. The most modern manual devices make possible greatly improved reporting accuracy. There are two manufacturers of manual devices, and each of their devices meet the requirements for dew point measurement apparatus as defined in the ASTM Manual for Hydrocarbon Analysis. However, there are significant differences between the devices – including the optical resolution of the mirror and the method of mirror cooling – depending on the manufacturer. \n\nAutomated chilled mirror devices provide significantly more repeatable results, but these measurements can be affected by contaminants that may compromise the mirror's surface. In many instances it is important to incorporate an effective filtration system that prepares the gas for analysis. On the other hand, filtration may alter the gas composition slightly and filter elements are subject to clogging and saturation. Advances in technology have led to analyzers that are less affected by contaminants and certain devices can also measure the dew point of water that may be present in the gas. One recent innovation is the use of spectroscopy to determine the nature of the condensate at dewpoint. Another device user laser interferometry to register extremely tenuous amounts of condensation. It is asserted that these technologies are less affected by interference from contaminants. Another source of error is the speed of the cooling of the mirror and the measurement of the temperature of the mirror when the condensation is detected. This error can be minimized by controlling the cooling speed, or having a fast condensation detection system.\n\nExperimental methods only provide a HCDP at the pressure at which the measurement is taken, and cannot provide the cricondentherm or the HCDP at other pressures. As the cricondentherm of natural gas is typically around 27 bar, there are gas preparation systems currently available which adjust input pressure to this value. Although, as pipeline operators often wish to know the HCDP at their current line pressure, the input pressure of many experimental systems can be adjusted by a regulator.\n\nThere are instruments that can be operated in either manual or automatic mode from the Vympel company.\n\nCompanies who offer an automated chilled mirror system include: Vympel, Ametek, Michell Instruments, and ZEGAZ Instruments\n\nA recent innovation is to combine the experimental method with theoretical. If the composition of the gas is analyzed by a C6+ GC, AND a dewpoint is experimentally measured at any pressure, then the experimental dewpoint can be used in combination with the GC analysis to provide a more exact phase diagram. This approach overcome the main shortcoming of the experimental method which is not knowing the whole phase diagram. An example of this software is provided by Starling Associates.\n\n\n"}
{"id": "2711919", "url": "https://en.wikipedia.org/wiki?curid=2711919", "title": "Jacob's ladder (toy)", "text": "Jacob's ladder (toy)\n\nA Jacob's ladder (also magic tablets, Chinese blocks, and klick-klack toy) is a folk toy consisting of blocks of wood held together by strings or ribbons. It is \"a toy that does some very weird things.\" When the ladder is held at one end, blocks appear to cascade down the strings. This effect is a visual illusion which is the result of one block after another flipping over. It may be considered a kinetic illusion, where the blocks appear to change position when they do not. Its name \"Jacob's Ladder\" comes from the biblical ladder to heaven, mentioned in Genesis 28:12.\n\nOf unknown origin, the earliest known review of the Jacob's Ladder is an 1889 \"Scientific American\" article which tells how it is built and works:\nAn arrangement of interlaced ribbons allows each block to act as if hinged to the next one at either of its two ends. The same mechanism is used in the 1980s toy Rubik's Magic, but with plastic filaments run diagonally across squares, with the result that the squares can hinge along either of two \"adjacent\" sides.\n\nThe toy has been described as originating in China, as being found in King Tut's tomb, though unmentioned in its inventory, and as one of the few toys allowed on Sunday by Puritans in colonial America, \"but the true origin of the toy remains a mystery.\"\n\nMany slight variants have been patented in the United States, one from the 1940s having in one block an indentation to hold a penny, which then appeared to dis- and re-appear. The Japanese polymath Hiraga Gennai (1729–1779) constructed a Jacob's ladder which later came to be called \"Gennai's Wondrous Click-clack\" (\"Gennai no fushigina katakata\", 源内の不思議なカタカタ).\n\n\n"}
{"id": "20011601", "url": "https://en.wikipedia.org/wiki?curid=20011601", "title": "Kavarna Wind Farm", "text": "Kavarna Wind Farm\n\nThe Kavarna Wind Farm () is a proposed wind power project in Kavarna, Bulgaria. It will have 100 individual wind turbines with a nominal output of around 2 MW which will deliver up to 200 MW of power, enough to power over 79,800 homes, with a capital investment required of approximately US$450 million.\n\n"}
{"id": "7899440", "url": "https://en.wikipedia.org/wiki?curid=7899440", "title": "Knee (construction)", "text": "Knee (construction)\n\nIn woodworking, a knee is a natural or cut, curved piece of wood. Knees, sometimes called ships knees, are a common form of bracing in boat building and occasionally in timber framing. A knee rafter in carpentry is a bent rafter used to gain head room in an attic.\n\nWood is a highly anisotropic material (its strength varies considerably with the direction of applied force, i.e. parallel, radial, or tangential to the grain). Because wood is strongest when loaded in tension or compression along the grain, the best knees are those in which the wood grain follows the bend. For a knee with relatively little bend, it may be possible to cut the knee out of a single straight-grained board and still achieve sufficient strength. However, with increasing bend this method becomes problematic since more and more of the knee is aligned across the grain and is therefore considerably weaker. A knee laid out this way might easily snap in two under hand pressure alone, even if it is generously sized. In boat joinery constantly subject to shock and fatigue loading this method is unsuitable.\n\nTo avoid this issue knees requiring sharper curves are made using methods which ensure that the wood grain and direction of load are closely aligned. This can be achieved by steam bending, laminating, or selecting a natural crook with matching grain - a \"grown knee\". Grown knees are generally considered as the \"best\" method among boat builders and have a strong traditions associated with their use, but they may not achieve the same strength as a good laminated knee.\n\nBent - Bent knees are formed by plasticizing the wood to make it flexible via boiling, steaming, or microwaving (for small components). While still hot, the wood can be bent into a shape suitable for the location - either on a form or by forcing and securing it directly into the final service location. Steam-bending is a time-honored method for shaping boat frames, but it does weaken the wood slightly, it can leave residual stresses which may cause breakage or spring-back over time, and it is limited in the degree of bend which it can achieve, particularly for thick members. Also, not all species of wood steam bend well.\n\nLaminated - Laminated knees are formed by coating thin, flexible strips of wood with adhesive, layering them to achieve the required thickness, then forcing the desired bend into the layup and securing it until the adhesive sets. Laminated knees are very strong and can be made in shapes which would be difficult to achieve using other methods, but they require time for the adhesive to cure, they are messier to construct, and they must use a jig or fixture to secure them until the adhesive cures. \nGrown - The term \"grown knees\" refers to any knee which is made from a natural crook or bend in a tree. Grown knees can be taken from several locations within a tree, with the most common being the intersection of the trunk and a large branch, crotches, and the roots. The roots are a particularly useful source as the root structure of many species of trees naturally spreads out laterally just beneath the ground in order to help anchor the tree. This provides a fairly reliable source of approximately 90 degree crooks which may be impossible to find in other portions of the tree. In order to obtain this raw material for knees builders may dig up a stump in its entirety, as unlike other portions of the tree, it is impossible to judge the quality and quantity of available material in the roots as they are underground. Once the stump has been dug up the knees can be sawn or split from suitable natural crooks. However, knees sawn from a stump can quickly dull tools used to shape and finish them - as the roots grow they envelop small particles of soil and rock, which acts as an embedded abrasive and accelerates the wear of edged tools. For species of wood with appropriate splitting characteristics, such as oak, the stump can be split into wedges, with one large root on each wedge; each wedge is then carved into a rib for a small boat.\nIn principle tree shaping could be used to grow a tree into the desired shape.\n\nA sharp bend in a piece of wood is also called \"cranked\". Commonly used in shipbuilding known as ship’s knee for their advantage of reducing the encroachment into the usable space of the structure since there is no spandrel. Also knee rafter increases the usable space in an attic by creating a kneewall-like space.\n\nA ship’s knee has two parts called the arm (shorter) and a body (longer). The outside surfaces come to a corner, (typically 90 degrees in buildings) called the heel. The inside surface retains its natural shape and the curve is called the bosom. The thickness between the heel and bosom is the throat. The ends of the arm and body are the toes.\n\nThe names of ship’s knees are based on their position:\n\nKnees can be blind pegged with foxtail wedges on both ends or bolted in place.\n\nDue to tradition, ease of workability, strength and durability characteristics, some species of wood are particularly prized for making knees. Tamarack (also known as hackmatack) stumps are among the preferred softwood species for grown knees, while white oak, live oak, and elm are preferred for hardwoods for bent knees due to their ease of steam bending.\n\n"}
{"id": "39763592", "url": "https://en.wikipedia.org/wiki?curid=39763592", "title": "Langbeinites", "text": "Langbeinites\n\nLangbeinites are a family of crystalline substances based on the structure of langbeinite with general formula MM(SO), where M is a large univalent cation such as potassium, rubidium, caesium, or ammonium), and M is a small divalent cation for example (magnesium, calcium, manganese, iron, cobalt, nickel, copper, zinc or cadmium). The sulfate group, SO, can be substituted by other tetrahedral anions with a double negative charge such as tetrafluoroberyllate BeF, selenate (SeO), chromate (CrO), molybdate (MO), or tungstates. Although monofluorophosphates are predicted, they have not been described. By redistributing charges other anions with the same shape such as phosphate also form langbeinite structures. In these the M atom must have a greater charge to balance the extra three negative charges.\n\nAt higher temperatures the crystal structure is cubic P23. However, the crystal structure may change to lower symmetries at lower temperatures, for example, P2, P1, or P222. Usually this temperature is well below room temperature, but in a few cases the substance must be heated to acquire the cubic structure.\n\nThe crystal structures of langbeinites consist of a network of oxygen vertex-connected tetrahedral polyanions (such as sulfate) and distorted metal ion-oxygen octahedra. The unit cell contains four formula units. In the cubic form the tetrahedral anions are slightly rotated from the main crystal axes. When cooled, this rotation disappears and the tetrahedra align, resulting in lower energy as well as lower crystal symmetry.\n\nSulfates include dithallium dicadmium sulfate, Dirubidium dicadmium sulfate dipotassium dicadmium sulfate, dithallium manganese sulfate. dirubidium dicalcium trisulfate.\n\nSelenates include diammonium dimanganese selenate. A diammonium dicadmium selenate langbeinite could not be crystallised from water, but a trihydrate exists.\n\nChromate based langbeinites include dicaesium dimanganese chromate.\n\nMolybdates include RbCo(MoO). Potassium members are absent, as are zinc and copper containing solids, which all crystallize in different forms. Manganese, magenesium, cadmium and some nickel double molydates exist as langbeinites.\n\nDouble tungstates of the form AB(WO) are predicted to exist in the langbeinite form.\n\nExamples with tetrafluroberyllate include dipotassium dimanganese tetrafluoroberyllate KMn(BeF),\n\nOther tetrafluoroberyllates may include RbMg(BeF) TlMg(BeF) TlMn(BeF) RbNi(BeF) TlNi(BeF) RbZn(BeF) TlZn(BeF) CsCa(BeF) RbCa(BeF) RbCsMnCd(BeF) CsMnCd(BeF) RbCsCd(BeF) CsCd(BeF). TlCd(BeF) (NH)Cd(BeF) KRbMnCd(BeF) KMnCd(BeF) RbMnCd(BeF) RbCd(BeF) RbCsCo(BeF) (NH)Co(BeF) KCo(BeF) RbCo(BeF) TlCo(BeF) RbCsMn(BeF) CsMn(BeF) RbCsZn(BeF) (NH)Mg(BeF) (NH)Mn(BeF) (NH)Ni(BeF) (NH)Zn(BeF) KRbMg(BeF) KMg(BeF). KRbMn(BeF) KMn(BeF) KNi(BeF) KZn(BeF)\n\nThe phosphate containing langbeinites were found in 1972 with the discovery of KTi(PO), and since then a few more phosphates that also contain titanium have been found such as NaFeTi(PO), NaCrTi(PO). By substituting metals in AMTi(PO), A from K, Rb, Cs, and M from Cr, Fe or V other langbeinites are made. The NASICON-type structure competes for these kinds of phosphates, so not all possibilities are langbeinites.\nOther phosphate based substances include KYTi(PO) KErTi(PO) KYbTi(PO), KCrTi(PO) KAlSn(PO) RbYbTi(PO). Sodium barium diiron tris-(phosphate) NaBaFe(PO) is yet another variation with the same structure but differently charged ions. Most phosphates of this kind of formula do not form langbeinites, instead crystallise in the NASICON structure with archetype NaZr(PO)(SiO).\n\nA langbeinite with arsenate is known to exist by way of KScSn(AsO).\n\nLanbeinite crystals can show ferroelectric or ferroelastic properties. Diammonium dicadmium sulfate identified by Jona and Pepinsky with a unit cell size of 10.35 Å becomes ferroelectric when the temperature drops below 95K. The phase transition temperature is not fixed, and can vary depending on the crystal or history of temperature change. So for example the phase transition in diammonium dicadmium sulfate can occur between 89 and 95 K. Under pressure the highest phase transition temperature increases. ∂T/∂P = 0.0035 degrees/bar. At 824 bars there is a triple point with yet another transition diverging at a slope of ∂T/∂P = 0.103 degrees/bar. For dipotassium dimanganese sulfate pressure causes the transition to rise at the rate of 6.86 °C/kbar. The latent heat of the transition is 456 cal/mol.\n\nDithallium dicadmium sulfate was shown to be ferroelectric in 1972.\n\nDipotassium dicadmium sulfate is thermoluminescent with stronger outputs of light at 350 and 475 K. This light output can be boosted forty times with a trace amount of samarium. Dipotassium dimagnesium sulfate doped with dysprosium develops thermoluminescence and mechanoluminescence after being irradiated with gamma rays. Since gamma rays occur naturally, this radiation induced thermoluminescence can be used to date evaporites in which langbeinite can be a constituent.\n\nAt higher temperatures the crystals take on cubic form, whereas at the lowest temperatures they can transform to an orthorhombic crystal group. For some types there are two more phases, and as the crystal is cooled it goes from cubic, to monoclinic, to triclinic to orthorhombic. This change to higher symmetry on cooling is very unusual in solids. For some langbeinites only the cubic form is known, but that may be because it has not been studied at low enough temperatures yet. Those that have three phase transitions go through these crystallographic point groups: P23 – P2 – P1 – P222, whereas the single phase change crystals only have P23 – P222.\n\nKCd(SO) has a transition temperature above room temperature, so that it is ferroelectric in standard conditions. The orthorhombic cell size is a=10.2082 Å, b=10.2837 Å, c=10.1661 Å.\n\nWhere the crystals change phase there is a discontinuity in the heat capacity. The transitions may show thermal hysteresis.\n\nDifferent cations can be substituted so that for example KCd(SO) and TlCd(SO) can form solid solutions for all ratios of thallium and potassium. Properties such as the phase transition temperature and unit cell sizes vary smoothly with the composition.\n\nLangeinites containing transition metals can be coloured. For example, cobalt langbeinite shows a broad absorption around 555 nm due to the cobalt T(F)→T(P) electronic transition.\n\nThe enthalpy of formation (ΔfHm) for solid (NH)Cd(SO) at 298.2K is −3031.74±0.08 kJ/mol, and for KCd(SO) it is －3305.52±0.17 kJ/mol.\n\nThe orthovanadates have four formula per cell, with a slightly distorted cell that has orthorhombic symmetry.\nLangbeinite structured double selenates are difficult to make, perhaps because selenate ions arranged around the dication leave space for water, so hydrates crystallise from double selenate solutions. For example, when ammonia selenate and cadmium selenate solution is crystallized it forms diammonium dicadmium selenate trihydrate: (NH)Cd(SeO)•3HO and when heated it loses both water and ammonia to form a pyroselenate rather than a langeinite.\nDiammonium dicadmium sulfate can be made by evaporating a solution of ammonium sulfate and cadmium sulfate. Dithallium dicadmium sulfate can be made by evaporating a water solution at 85 °C. Other substances may be formed during crystallisation from water such as Tutton's salts or competing compounds like RbCd(SO)·5HO.\n\nPotassium and ammonium nickel langbeinite can be made from nickel sulfate and the other sulfates by evaporating a water solution at 85 °C.\n\nDipotassium dizinc sulfate can be formed into large crystals by melting zinc sulfate and potassium sulfate together at 753K. A crystal can be slowly drawn out of the melt from a rotating crucible at about 1.2 mm every hour.\n\nLi(HO)Hf(PO) can be made by heating HfCl, LiBO, HPO, water and hydrochloric acid to 180 °C for eight days under pressure.\nLi(HO)Hf(PO) converts to LiHf(PO) on heating to 200 °C.\n\nThe sol-gel method produces a gel from a solution mixture, which is then heated. RbFeZr(PO) can be made by mixing solutions of FeCl, RbCl, ZrOCl, and dripping in HPO. The gel produced was dried out at 95 °C and then baked at various temperatures from 400 to 1100 °C.\n\nLangbeinites crystals can be made by the Bridgman technique, Czochralski process or flux technique.\n\nA Tutton's salt may be heat treated and dehydrate, e.g. (NH)Mn(SeO) can be made from (NH)Mn(SeO)·6(HO) heated to 100 °C, forming (NH)(SeO) as a side product. Similarly the ammonium vandadium Tutton's salt, (NH)V(SO), heated to 160 °C in a closed tube produces (NH)V(SO). At lower temperatures a hydroxy compound is formed.\n\nFew uses have been made of these substances. Lanbeinite itself can be used as an \"organic\" fertiliser with potassium, magnesium and sulfur, all needed for plant growth. Electrooptic devices could be made from some of these crystals, particularly those that have cubic transition temperatures as temperatures above room temperature. Research continues into this. Ferroelectric crystals could store information in the location of domain walls.\n\nThe phosphate langbeinites are insoluble, stable against heat, and can accommodate a large number of different ions, and have been considered for immobilizing unwanted radioactive waste.\n\nRare earth containing zirconium phosphate langeinites have been investigated for use in white LEDs and plasma displays. Langbeinites that contain bismuth are photoluminescent.\nIn case of iron-containing ones complex magnetic behavior may be found.\n"}
{"id": "16049255", "url": "https://en.wikipedia.org/wiki?curid=16049255", "title": "Larson–Miller relation", "text": "Larson–Miller relation\n\nThe Larson–Miller relation, also widely known as the Larson–Miller parameter and often abbreviated LMP, is a parametric relation used to extrapolate experimental data on creep and rupture life of engineering materials.\n\nF.R. Larson and J. Miller proposed that creep rate could adequately be described by the Arrhenius type equation:\n\nWhere r is the creep process rate, A is a constant, R is the universal gas constant, T is the absolute temperature, and formula_2 is the activation energy for the creep process. Taking the natural log of both sides:\n\nWith some rearrangement:\n\nUsing the fact that creep rate is inversely proportional to time, the equation can be written as:\n\nTaking the natural log:\n\nAfter some rearrangement the relation finally becomes:\n\nThis equation is of the same form as the Larson–Miller relation.\n\nwhere the quantity LMP is known as the Larson–Miller parameter. Using the assumption that activation energy is independent of applied stress, the equation can be used to relate the difference in rupture life to differences in temperature for a given stress. The material constant C is typically found to be in the range of 20 to 22 for metals when time is expressed in hours and temperature in degrees Rankine.\n\nThe Larson–Miller model is used for experimental tests so that results at certain temperatures and stresses can predict rupture lives of time spans that would be impractical to reproduce in the laboratory.\n\nExpanding the equation as a Taylor series makes the relationship easier to understand. Only the first terms are kept.\nChanging the time, by a factor of 10, changes the logarithm by 1 and the LMP changes by an amount equal to the temperature. \nTo get an equal change in LMP by changing the temperature, the temperature needs to be raised or lowered by about 5% of its absolute value.\nTypically a 5% increase in absolute temperature will increase the rate of creep by a factor of ten.\n\nThe equation was developed during the 1950s while Miller and Larson were employed by GE performing research on turbine blade life.\n\n\n"}
{"id": "10278", "url": "https://en.wikipedia.org/wiki?curid=10278", "title": "List of explosives used during World War II", "text": "List of explosives used during World War II\n\nAlmost all the common explosives listed here were mixtures of several common components:\n\n\nThis is only a partial list; there were many others. Many of these compositions are now obsolete and only encountered in legacy munitions and unexploded ordnance.\n\nTwo nuclear explosives, containing mixtures of uranium and plutonium, respectively, were also used at the bombings of Hiroshima and Nagasaki\n\n"}
{"id": "1120348", "url": "https://en.wikipedia.org/wiki?curid=1120348", "title": "MKM steel", "text": "MKM steel\n\nMKM steel, an alloy containing nickel and aluminum, was developed in 1931 by the Japanese metallurgist Tokuhichi Mishima. While conducting research into the properties of nickel, Mishima discovered that a strongly magnetic steel could be created by adding aluminum to non-magnetic nickel steel. \n\nThe developers claim MKM steel is tough and durable, inexpensive to produce, maintains strong magnetism when miniaturized and can produce a stable magnetic force in spite of temperature changes or vibration. MKM steel is similar to Alnico.\n\nMKM is an acronym for Mitsujima ka magnetic, 'Mitsujima ka' being the name of the inventor's childhood home.\n"}
{"id": "1667554", "url": "https://en.wikipedia.org/wiki?curid=1667554", "title": "Magnetic semiconductor", "text": "Magnetic semiconductor\n\nMagnetic semiconductors are semiconductor materials that exhibit both ferromagnetism (or a similar response) and useful semiconductor properties. If implemented in devices, these materials could provide a new type of control of conduction. Whereas traditional electronics are based on control of charge carriers (n- or p-type), practical magnetic semiconductors would also allow control of quantum spin state (up or down). This would theoretically provide near-total spin polarization (as opposed to iron and other metals, which provide only ~50% polarization), which is an important property for spintronics applications, e.g. spin transistors. \n\nWhile many traditional magnetic materials, such as magnetite, are also semiconductors (magnetite is a semimetal semiconductor with bandgap 0.14 eV), materials scientists generally predict that magnetic semiconductors will only find widespread use if they are similar to well-developed semiconductor materials. To that end, dilute magnetic semiconductors (DMS) have recently been a major focus of magnetic semiconductor research. These are based on traditional semiconductors, but are doped with transition metals instead of, or in addition to, electronically active elements. They are of interest because of their unique spintronics properties with possible technological applications. Doped Wide band-gap metal oxides such as zinc oxide (ZnO) and titanium oxide (TiO) are among the best candidates for industrial DMS due to their multifunctionality in opticomagnetic applications. In particular, ZnO-based DMS with properties such as transparency in visual region and piezoelectricity have generated huge interest among the scientific community as a strong candidate for the fabrication of spin transistors and spin-polarized light-emitting diodes, while copper doped TiO in the anatase phase of this material has further been predicted to exhibit favorable dilute magnetism. \n\nHideo Ohno and his group at the Tohoku University were the first to measure ferromagnetism in transition metal doped compound semiconductors such as indium arsenide and gallium arsenide doped with manganese referred to as GaMnAs. These materials exhibited reasonably high Curie temperatures (yet below room temperature) that scales with the concentration of p-type charge carriers. Ever since, ferromagnetic signals have been measured from various semiconductor hosts doped with different transition atoms.\n\nThe pioneering work of Dietl \"et al.\" showed that a modified Zener model for magnetism \nwell describes the carrier dependence, as well as anisotropic properties of GaMnAs.\nThe same theory also\npredicted that room-temperature ferromagnetism should exist in heavily p-type doped ZnO and GaN doped by Co and Mn, respectively.\nThese predictions were followed of a flurry of theoretical and experimental studies of various oxide and nitride semiconductors, \nwhich apparently seemed to confirm room temperature ferromagnetism in nearly any semiconductor or insulator material \nheavily doped by transition metal impurities.\nHowever, early Density functional theory (DFT) studies were clouded by band gap errors and overly delocalized defect levels,\nand more advanced DFT studies refute most of the previous predictions of ferromagnetism.\nLikewise, it has been shown that for most of the oxide based materials studies for magnetic semiconductors\ndo not exhibit an intrinsic \"carrier-mediated\" ferromagnetism as postulated by Dietl \"et al.\"\nTo date, GaMnAs remains the only semiconductor material with robust coexistence of ferromagnetism persisting up to rather high Curie temperatures around 100–200 K.\n\nThe manufacturability of the materials depend on the thermal equilibrium solubility of the dopant in the base material. E.g., solubility of many dopants in zinc oxide is high enough to prepare the materials in bulk, while some other materials have so low solubility of dopants that to prepare them with high enough dopant concentration thermal nonequilibrium preparation mechanisms have to be employed, e.g. growth of thin films.\n\nPermanent magnetization has been observed in a wide range of semiconductor based materials.\nSome of them exhibit a clear correlation between carrier concentration and magnetization,\nincluding the work of\nT. Story and co-workers where they demonstrated that the ferromagnetic Curie temperature of Mn-doped PbSnTe can be controlled by the carrier concentration. \nThe theory proposed by Dietl required charge carriers in the case of holes to mediate the magnetic coupling of manganese dopants in the prototypical magnetic semiconductor, Mn-doped GaAs. If there is an insufficient hole concentration in the magnetic semiconductor, then the Curie temperature would be very low or would exhibit only paramagnetism. However, if the hole concentration is high (<nowiki»</nowiki>~10 cm), then the Curie temperature would be higher, between 100–200 K.\nHowever, many of the semiconductor materials studied exhibit a permanent magnetization \"extrinsic\" \nto the semiconductor host material.\nA lot of the elusive extrinsic ferromagnetism (or \"phantom ferromagnetism\") \nis observed in thin films or nanostructured materials.\n\nSeveral examples of ferromagnetic semiconductor materials are e.g.:\n\n\n"}
{"id": "12860504", "url": "https://en.wikipedia.org/wiki?curid=12860504", "title": "Marshall 1959", "text": "Marshall 1959\n\nThe Marshall Super Lead Model 1959 is a guitar amplifier head made by Marshall. One of the famous Marshall Plexis, it was introduced in 1965 and with its associated 4×12\" cabinets gave rise to the \"Marshall stack\".\n\nThe 1959 (Marshall's identifying numbers are not years of manufacture) was produced from 1965 to 1981 (when it was replaced by the JCM800), is an amplifier in Marshall's \"Standard\" series. It was designed by Ken Bran and Dudley Craven after The Who's guitarist Pete Townshend asked Marshall for a 100 watt amplifier. Its output was first channeled into an 8×12\" cabinet, but that single, unwieldy cabinet was quickly changed to a pair of 4x12\" cabinets, 1960a \"angled\" on top and 1960b \"box\" on bottom. creating the famous \"Marshall full stack\". The amplifier also came as a PA and a bass version.\n\nThe Plexiglas panel led to the name \"Plexi\", and while 50-watt models of the time are also called Plexis, the 1959 100 watt model is generally thought of as the \"definitive\" Plexi.\n\nIn 1969, Marshall replaced the Plexiglas panel with one of gold aluminum. There were other modifications: In 1966, the KT66 tubes of the JTM-models were replaced with EL34. After 1976, the plate voltages were lowered slightly for improved reliability. But during the 1970s, Marshall's increasing exports overseas led to a problem: Often the EL34 tubes would break during transportation, to the point where amps began being shipped from the factory with more rugged Tung-Sol 6550 tubes, which are \"stiffer and not as harmonically rich\" as the EL34 tubes.\n\nThe amplifier was reissued for the first time in 1988 (the 1959S), and again from 1991 to 1993 (the 1959X) and from 1993 to 1995 (the 1959 SLP). In 2005, Marshall introduced the 1959 HW (for \"hand-wired\"), based on the 1967–1969 models, with negative feedback added corresponding to the 1969 model. This amplifier was called \"expensive but good\". \"Guitar Player\" magazine called the 1959 \"monumentally huge, frightfully loud, and painfully expensive\", and its review of the 1959HW said it was \"quick, percussive, articulate\", and required a \"total commitment to volume\".\n\nThe 1959 had 100 watts of power, two channels, and four inputs. They were equipped with 4x KT66 tubes, but models made after 1967 had four EL34 tubes instead; it had three ECC83 tubes in the pre-amplification stage. A model with tremolo, the 1959T, was available until 1973.\n\nThe amplifier had four inputs into two channels. The lead channel has a boosted bright tone, and the rhythm channel has a flat response. Each channel has a high and a low gain input; the low gain input is attenuated by 6 dB. The channels can be linked with a jumper cable.\n\nBesides Pete Townshend of The Who, early users include Eric Clapton, who in 1966, when he founded Cream, traded in his famous Bluesbreaker combo for a 1959 Plexi, and Jimi Hendrix, who used a 1959 with four 4×12\" cabinets (his \"couple of great refrigerators\") at the 1969 Woodstock Festival and established the Marshall as the \"definitive rock amp\".\n\nGabriel Almeda\n"}
{"id": "27700601", "url": "https://en.wikipedia.org/wiki?curid=27700601", "title": "Minister of Energy (Belgium)", "text": "Minister of Energy (Belgium)\n\nThis is the list of Belgian ministers of energy.\n"}
{"id": "2680184", "url": "https://en.wikipedia.org/wiki?curid=2680184", "title": "Oil analysis", "text": "Oil analysis\n\nOil analysis (OA) is the laboratory analysis of a lubricant's properties, suspended contaminants, and wear debris. \"OA\" is performed during routine predictive maintenance to provide meaningful and accurate information on lubricant and machine condition. By tracking oil analysis sample results over the life of a particular machine, trends can be established which can help eliminate costly repairs. The study of wear in machinery is called tribology. Tribologists often perform or interpret oil analysis data.\n\nOA can be divided into three categories:\n\nOil sampling is a procedure for collecting a volume of fluid from lubricated or hydraulic machinery for the purpose of oil analysis. Much like collecting forensic evidence at a crime scene, when collecting an oil sample, it is important to ensure that procedures are used to minimize disturbance of the sample during and after the sampling process. Oil samples are typically drawn into a small, clean bottle which is sealed and sent to a laboratory for analysis.\n\nOA was first used after World War II by the US railroad industry to monitor the health of locomotives. In 1946 the Denver and Rio Grande Railroad's research laboratory successfully detected diesel engine problems through wear metal analysis of used oils. A key factor in their success was the development of the spectrograph, an instrument which replaced several wet chemical methods for detecting and measuring individual chemical element such as iron or copper. This practice was soon accepted and used extensively throughout the railroad industry.\n\nBy 1955 OA had matured to the point that the United States Bureau of Naval Weapons began a major research program to adopt wear metal analysis for use in aircraft component failure prediction. These studies formed the basis for a Joint Oil Analysis Program (JOAP) involving all branches of the U.S. Armed Forces. The JOAP results proved conclusively that increases in component wear could be confirmed by detecting corresponding increases in the wear metal content of the lubricating oil. In 1958 Pacific Intermountain Express (P.I.E.) was the first trucking company to set up an in-house used oil analysis laboratory to control vehicle maintenance costs which was managed by Bob Herguth. In 1960 the first independent commercial oil analysis laboratory was started by Edward Forgeron in Oakland, CA.\n\nIn addition to monitoring oil contamination and wear metals, modern usage of OA includes the analysis of the additives in oils to determine if an extended drain interval may be used. Maintenance costs can be reduced using OA to determine the remaining useful life of additives in the oil. By comparing the OA results of new and used oil, a tribologist can determine when an oil must be replaced. Careful analysis might even allow the oil to be \"sweetened\" to its original additive levels by either adding fresh oil or replenishing additives that were depleted.\n\nOil analysis professionals and analysts can get certified in compliance with ISO standards by passing exams administered by the International Council for Machinery Lubrication (ICML).\n\nFor purposes of Oil Analysis Program (OAP) trend analysis, replacement, replenishment or drain and flush of lubricating fluids in excess of half an engine’s oil capacity (2.5 gallons or more) will be considered an Oil Change and the engine will be placed in code Charlie (C) for three flights to establish a new working trend. Oil-Wetted Maintenance (OWM) is any replacement of engine components within an oil-lubricated system (bearings, gearbox, pumps, etc.). OWM actions shall be documented on DD Form 2026 and submitted to OAP lab for update of Oil Analysis database.\n(a) Special Samples can be requested by the laboratory whenever they feel its necessary.\n(b) Whenever directed by the unit maintenance activity to investigate suspected deficiencies.\n\nThe NDI/JOAP laboratory will set the standards and intervals of oil analysis. The AMU will have to deal with it.\n\nA typical predictive maintenance technique is ferrography, which analyses iron in oil.\n\n\n\n\n\n\n"}
{"id": "46262052", "url": "https://en.wikipedia.org/wiki?curid=46262052", "title": "Operation Cleaver", "text": "Operation Cleaver\n\nOperation Cleaver, as labelled in a report by American firm Cylance Inc. in late 2014, was a cyberwarfare covert operation targeting critical infrastructure organizations worldwide, allegedly planned and executed by Iran. \n\nCylance's report was later tacitly acknowledged in a confidential report by Federal Bureau of Investigation (FBI), though Iranian officials denied involvement in the operation. \n\nIn December 2014, California-based cyber security firm Cylance Inc. published results of a 2-year investigation, an 86-page technical report, indicating that an operation, called \"Operation Cleaver\", has targeted the military, oil and gas, energy and utilities, transportation, airlines, airports, hospitals and aerospace industries organizations worldwide. \n\nThe title \"Operation Cleaver\" alludes to frequent uses of the word \"cleaver\" in the malware's coding.\n\nAccording to the report, over 50 entities in 16 countries have been hit by the campaign, based in the United States, Israel, China, Saudi Arabia, India, Germany, France and England among others.\nCylance's research does not name individual companies, but \"Reuters\" reports citing \"a person familiar with the research\" Navy Marine Corps Intranet, Calpine, Saudi Aramco, Pemex, Qatar Airlines and Korean Air were among the specific targets.\n\nStuart McClure, Cylance founder and CEO believes that the hackers are sponsored by Iran and have ties to Islamic Revolutionary Guard Corps.\n\nAccording to \"Reuters\", the Federal Bureau of Investigation has filed a confidential \"Flash\" report, providing technical details about malicious software and techniques used in the attacks. The technical document said the hackers typically launch their attacks from two IP addresses that are in Iran, but does not attribute the attacks to the Iranian government. FBI warned businesses to stay vigilant and to report any suspicious activity spotted on the companies' computer systems.\n\nIran has officially denied involvement in the hacking campaign. \"This is a baseless and unfounded allegation fabricated to tarnish the Iranian government image, particularly aimed at hampering current nuclear talks\", said Hamid Babaei, spokesman for Permanent mission of Islamic Republic of Iran to the United Nations. \n\n"}
{"id": "2195504", "url": "https://en.wikipedia.org/wiki?curid=2195504", "title": "Oxygenate", "text": "Oxygenate\n\nOxygenated chemical compounds contain oxygen as a part of their chemical structure. The term usually refers to oxygenated fuels. Oxygenates are usually employed as gasoline additives to reduce carbon monoxide and soot that is created during the burning of the fuel. Compounds related to soot, like polyaromatic hydrocarbons (PAHs) and nitrated PAHs, are reduced also.\n\nThe oxygenates commonly used are either alcohols or ethers:\n\nIn the United States, the Environmental Protection Agency had authority to mandate that minimum proportions of oxygenates be added to automotive gasoline on regional and seasonal basis from 1992 until 2006 in an attempt to reduce air pollution, in particular ground-level ozone and smog. In addition to this North American automakers have in 2006 and 2007 promoted a blend of 85% ethanol and 15% gasoline, marketed as E85, and their flex-fuel vehicles, \"e.g.\" GM's \"Live Green, Go Yellow\" campaign. U.S. Corporate Average Fuel Economy (CAFE) standards give an artificial 54% fuel efficiency bonus to vehicles capable of running on 85% alcohol blends over vehicles not adapted to run on 85% alcohol blends. There is also alcohols' intrinsically cleaner combustion, however due to its lower energy density it is not capable of producing as much energy per gallon as gasoline. Much gasoline sold in the United States is blended with up to 10% of an oxygenating agent. This is known as oxygenated fuel and often (but not entirely correctly, as there are reformulated gasolines without oxygenate) as reformulated gasoline. Methyl tert(iary)-butyl ether (MTBE) was the most popular fuel additive in the US, prior to government mandated use of ethanol.\n\n\n"}
{"id": "27310823", "url": "https://en.wikipedia.org/wiki?curid=27310823", "title": "Pachner moves", "text": "Pachner moves\n\nIn topology, a branch of mathematics, Pachner moves, named after Udo Pachner, are ways of replacing a triangulation of a piecewise linear manifold by a different triangulation of a homeomorphic manifold. Pachner moves are also called bistellar flips. Any two triangulations of a piecewise linear manifold are related by a finite sequence of Pachner moves.\n\nLet formula_1 be the formula_2-simplex. formula_3 is a combinatorial \"n\"-sphere with its triangulation as the boundary of the \"n+1\"-simplex. \n\nGiven a triangulated piecewise linear \"n\"-manifold formula_4, and a co-dimension \"0\" subcomplex formula_5 together with a simplicial isomorphism formula_6, the Pachner move on \"N\" associated to \"C\" is the triangulated manifold formula_7. By design, this manifold is PL-isomorphic to formula_4 but the isomorphism does not preserve the triangulation.\n"}
{"id": "52719800", "url": "https://en.wikipedia.org/wiki?curid=52719800", "title": "Pentafluorosulfur hypofluorite", "text": "Pentafluorosulfur hypofluorite\n\nPentafluorosulfur hypofluorite is an oxyfluoride of sulfur in the +6 oxidation state, with a fluorine atom attached to oxygen. The formula is SOF. In standard conditions it is a gas.\n\nSOF can be made by reacting thionyl fluoride with fluorine at 200 °C with a silver difluoride catalyst.\n\nThe molecular shape has five fluorine and one oxygen atom arranged around a sulfur atom in an octahedral arrangement. Another fluorine atom is attached to the oxygen in almost a straight line with the S-O connection. So the molecular formula can also be written as SFOF. The average S-F distance is 1.53 Å. The angles ∠FSF and ∠FSO are 90°.\n\nThe F nuclear magnetic resonance spectrum of SOF compared to SF has a -131.5 ppm shift for the hypofluorite fluorine, and 1.75 ppm for the opposite F. The other four fluorine atoms have a shift of 3.64 ppm. Spin coupling of o-F to SF is 17.4 Hz, between SF and opposite (apex) SF 155 Hz, and between apex and hypofluorite it is 0.0.\n\nAlkalis such as potassium hydroxide react\n\nAlkenes react to add to a double bond, with -OSF on one carbon, and -F on the other.\n\nThermal decomposition produces thionyl tetrafluoride and oxygen.\n\nSome reactions of SOF result in fluorination of other molecules\n"}
{"id": "5389553", "url": "https://en.wikipedia.org/wiki?curid=5389553", "title": "Photophoresis", "text": "Photophoresis\n\nPhotophoresis denotes the phenomenon that small particles suspended in gas (aerosols) or liquids (hydrocolloids) start to migrate when illuminated by a sufficiently intense beam of light. The existence of this phenomenon is owed to a non-uniform distribution of temperature of an illuminated particle in a fluid medium. Separately from photophoresis, in a fluid mixture of different kinds of particles, the migration of some kinds of particles may be due to differences in their absorptions of thermal radiation and other thermal effects collectively known as thermophoresis. In laser photophoresis, particles migrate once they have a refractive index different from their surrounding medium. The migration of particles is usually possible when the laser is slightly or not focused. A particle with a higher refractive index compared to its surrounding molecule moves away from the light source due to momentum transfer from absorbed and scattered light photons. This is referred to as a radiation pressure force. This force depends on light intensity and particle size but has nothing to do with the surrounding medium. Just like in Crookes radiometer, light can heat up one side and gas molecules bounce from that surface with greater velocity, hence push the particle to the other side. Under certain conditions, with particles of diameter comparable to the wavelength of light, the phenomenon of a negative indirect photophoresis occurs, due to the unequal heat generation on the laser irradiation between the back and front sides of particles, this produces a temperature gradient in the medium around the particle such that molecules at the far side of the particle from the light source may get to heat up more, causing the particle to move towards the light source.\n\nIf the suspended particle is rotating, it will also experience the Yarkovsky effect.\n\nDiscovery of photophoresis is usually attributed to Felix Ehrenhaft in the 1920s, though earlier observations were made by others including Augustin-Jean Fresnel.\n\nThe applications of photophoresis expand into the various divisions of science, thus physics, chemistry as well as in biology. Photophoresis is applied in particle trapping and levitation, in the field flow fractionation of particles, in the determination of thermal conductivity and temperature of microscopic grains and also in the transport of soot particles in the atmosphere. The use of light in the separation of particles aerosols based on their optical properties, makes possible the separation of organic and inorganic particles of the same aerodynamic size.\n\nRecently, photophoresis has been suggested as a chiral sorting mechanism for single walled carbon nanotubes. The proposed method would utilise differences in the absorption spectra of semiconducting carbon nanotubes arising from optically excited transitions in electronic structure. If developed the technique would be orders of magnitudes faster than currently established ultracentrifugation techniques.\n\nDirect photophoresis is caused by the transfer of photon momentum to a particle by refraction and reflection. Movement of particles in the forward direction occurs when the particle is transparent and has an index of refraction larger compared to its surrounding medium. Indirect photophoresis occurs as a result of an increase in the kinetic energy of molecules when particles absorb incident light only on the irradiated side, thus creating a temperature gradient within the particle. In this situation the surrounding gas layer reaches temperature equilibrium with the surface of the particle. Molecules with higher kinetic energy in the region of higher gas temperature impinge on the particle with greater momenta than molecules in the cold region; this causes a migration of particles in a direction opposite to the surface temperature gradient. The component of the photophoretic force responsible for this phenomenon is called the radiometric force. This comes as a result of uneven distribution of radiant energy (source function within a particle).\nIndirect photophoretic force depends on the physical properties of the particle and the surrounding medium.\n\nFor pressures formula_1, where the free mean path of the gas is much larger than the characteristic size formula_2 of the suspended particle (direct photophoresis), the longitudinal force is \nwhere the mean temperature of the scattered gas is (thermal accommodation coefficient formula_4, momentum accommodation coefficient formula_5)\nand the black body temperature of the particle (net light flux formula_7, Stefan Boltzmann constant formula_8, temperature of the radiation field formula_9)\nformula_11 is the thermal conductivity of the particle.\nThe asymmetry factor for spheres formula_12 is usually formula_13 (positive longitudinal photophoresis).\nFor non-spherical particles, the average force exerted on the particle is given by the same equation where the radius formula_2 is now the radius of the respective volume-equivalent sphere.\n\n"}
{"id": "36702245", "url": "https://en.wikipedia.org/wiki?curid=36702245", "title": "Posiva", "text": "Posiva\n\nPosiva Oy is a Finnish company with headquarters in the municipality of Eurajoki, Finland. It was founded in 1995 by Teollisuuden Voima (60% of stock) and Fortum (40% of stock), two Finnish nuclear plant operators, for researching and creating a method of final disposal of spent nuclear fuel from their plants.\n\nFor this purpose, Posiva is currently constructing the Onkalo spent nuclear fuel repository, the world's first deep geological repository, at the Olkiluoto Nuclear Power Plant site.\n"}
{"id": "1086503", "url": "https://en.wikipedia.org/wiki?curid=1086503", "title": "Ronan Point", "text": "Ronan Point\n\nRonan Point was a 22-storey tower block in Canning Town in Newham, East London, which partly collapsed on 16 May 1968, only two months after it had opened. A gas explosion blew out some load-bearing walls, causing the collapse of one entire corner of the building, which killed four people and injured 17. Although there were few casualties the spectacular nature of the failure, caused by both poor design and poor construction, led to a loss of public confidence in high-rise residential buildings, and major changes in UK building regulations resulted.\n\nRonan Point, named after Deputy Mayor Harry Ronan (a former Chairman of the Housing Committee of the London Borough of Newham), was part of the wave of tower blocks built in the 1960s as cheap, affordable prefabricated housing for inhabitants of West Ham and other areas of London. The tower was built by Taylor Woodrow Anglian using a technique known as Large Panel System building (LPS), which involves casting large concrete prefabricated sections off-site and bolting them together to construct the building. The precast system used was the Danish Larsen & Nielsen system.\n\nConstruction started in 1966 and was completed on 11 March 1968.\n\nAt approximately 5:45 am on 16 May 1968, resident Ivy Hodge went into her kitchen in flat 90, a corner flat on the 18th floor of the building, and lit a match to light the gas stove for a cup of tea. The match sparked a gas explosion that blew out the load-bearing flank walls, which had been supporting the four flats above. It is believed that the weaknesses were in the joints connecting the vertical walls to the floor slabs. The flank walls fell away, leaving the floors above unsupported and causing the progressive collapse of the south-east corner of the building.\n\nThe building had just opened, and three of the four flats immediately above Hodge's were unoccupied. Four of the 260 residents were killed immediately and seventeen were injured, including a young mother who was stranded on a narrow ledge when the rest of her living room disappeared. Hodge survived, despite being blown across the room by the explosion—as did her gas stove, which she took to her new address.\n\nIn the immediate aftermath of the collapse, the government commissioned an inquiry, led by Hugh Griffiths, QC. It reported on dangers caused by pressure on the walls from explosion, wind, or fire, finding that although the design had complied with the current regulations,\n\nRonan Point was partly rebuilt after the explosion, using strengthened joints designed to deal with those issues, and the Building Regulations were altered to ensure that similar designs would not be permitted in the future. However, public confidence in the safety of residential tower blocks was irreparably shaken, and the public scepticism was later found to be appropriate.\n\nSome people, including Sam Webb, an architect who had given evidence to the Griffiths inquiry, were not satisfied that every issue had been properly investigated. It was later shown that:\n\nThe concern, most particularly about the fire separation issue, eventually led the council to evacuate the building, and then to demolish it in 1986 in a forensic manner (rather than, for example, using explosives). When this was done, the extent of the defects found shocked even some of the activists, such as the architect Sam Webb, who had been lobbying for years that the building was unsafe. On the lower floors, cracks were found in the concrete where it had been point loaded, and it was alleged that the extra pressure on those points during a high wind (such as during the Great Storm of 1987, barely a year after the demolition) would soon have led to building collapse.\n\nThe partial collapse of Ronan Point led to major changes in building regulations. The first of these came with the 5th Amendment to the Building Regulations in 1970. These are now embodied in Part A of the Building Regulations and cover “Disproportionate Collapse”. They require that “the building shall be constructed so that in the event of an accident the building will not suffer collapse to an extent disproportionate to the cause”. They specifically cover pressures which may be caused for example by wind forces, explosions (either internal or external), or vehicle incursions, and note that seismic design may occasionally be required.\n\nImmediately after the publication of the report the Government brought out interim measures to ensure the safety and integrity of buildings in the event of an explosion. All new buildings of over five storeys constructed after November 1968 were required to be able to resist an explosive force of —the value still used as of 2014. Existing buildings were allowed to resist an explosive force of , provided that the gas supply was removed and flats were refitted for electric cooking and heating. The gas supply was removed from Ronan Point and the other eight blocks on the estate.\n\nIt has been said that the location of the explosion, on the fifth floor from the top of the building, was critical to the collapse. If it had been much higher, the lesser momentum of the debris falling onto the floor of the explosion flat would not have caused its collapse, even though it might have sustained some damage, and the progressive collapse of the floors beneath would therefore not have occurred. If it had been much lower, the friction of the joints, under the great weight of the building above, would have prevented the wall panels from blowing out, and there would have been no structural damage at all. \n\nThis may be why no other such collapse had occurred worldwide at the time. Nonetheless, it demonstrated an area of design which had not previously been considered. Many other jurisdictions, including for example the US, have since amended their building codes to require that buildings subject to explosions or other accidents will not collapse to an extent disproportionate to the cause.\n\nTwo days after the Grenfell Tower fire in 2017, John Knapton, emeritus professor of structural engineering at Newcastle University, claimed that regulations which came into force in 1971, following lessons learned from Ronan Point, had improved building structural strength in such a way as to prevent the collapse of the Grenfell Tower, which was built in 1974.\n\nNewham Council voted in 1984 to demolish Ronan Point. All nine blocks on the Freemason Estate, comprising 990 flats, were demolished in 1986 and the area was redeveloped with two-storey houses with gardens. Many other similar LPS buildings have since been demolished.\n\nThe Building Research Establishment published a series of reports in the 1980s to advise local councils and building owners on checking the structural stability of their LPS blocks. The contents of two of the reports relied on local authorities sending returns in to the Ministry of Housing and Local Government during 1968 and 1969. This was not exhaustive, with many authorities failing to do so and thus not having their blocks assessed after the issue of interim structural methods by the Ministry in 1968–69. Among these authorities were Lambeth and Southwark in London, and Birmingham. Birmingham owned over 300 LPS blocks, and when these were assessed in 1998 it was found that a number which did not meet 5 psi (34 kN/m²) still had a piped gas supply. A number of those blocks were demolished. The London Borough of Southwark owns the largest LPS estate in the UK, the Aylesbury Estate, which has a piped gas supply; it has been questioned whether the structure is strong enough to resist a 5 psi explosion. Southwark Council admitted in 2017 that strengthening work ordered after Ronan Point may not have been carried out on the Ledbury Estate, after structural weaknesses were found that led to the evacuation of four tower blocks.\n\nWithin a couple of decades of the collapse of Ronan Point, the public's lack of confidence in the LPS construction technique, together with the social problems within such developments, led to the demolition of many tower blocks. Despite this many buildings like Ronan Point are still standing, there are at least 1,585 of them and 200 towers with 20 storeys or higher. There are concerns that problems might emerge as the buildings weaken with age. Wind, heat and rain affect buildings. Metal bolts expand when they rust and crack the concrete round them. In Tottenham on the Broadwater Farm estate two tower blocks, Tangmere House and Northholt House are structurally unsound and could collapse catastrophically if there is a gas explosion or if a vehicle collides with the base. Both are to be evacuated urgently. Other buildings in Broadwater Farm also have less serious problems.\n\nIn May 2018, 50 years after the partial collapse, Ronan Point was the subject of an experimental documentary film, ‘And then we heard shouts and cries’ by artist Ricky Chambers. Chambers' grandparents and mother had lived in flat 87 on the 18th floor of the tower block at the time of the gas explosion.\n\nA number of books have covered the collapse of Ronan Point, including \"Collapse: Why Buildings Fall Down\" by Phil Wearne . This was written to accompany the television series of the same name shown on Channel 4 in early 2000.\n\nBuilding Research Establishment reports:\n\n"}
{"id": "14331070", "url": "https://en.wikipedia.org/wiki?curid=14331070", "title": "Senvion", "text": "Senvion\n\nSenvion S.A. (formerly REpower Systems SE) is a wind turbine manufacturer founded in 2001 in Germany, majority owned by the private equity firm, Centerbridge Partners since April 2015. \n"}
{"id": "6715583", "url": "https://en.wikipedia.org/wiki?curid=6715583", "title": "Society for Organizational Learning", "text": "Society for Organizational Learning\n\nThe Society for Organizational Learning (SoL) is an American organization founded in 1997 by Peter Senge. It replaced the Center for Organizational Learning at MIT. Since 1999, SoL publishes its own journal, \"Reflections\". Its European Counterpart is the European Consortium for the Learning Organisation (ECLO), established in Brussels in 1990. It was founded by several European multinationals as a response to the US orientated approach of Peter Senge with a special emphasis to the European complexity of regions, nations, history, tradition, etc. Its Journal \"The Learner\" and its electronic version \"eLearner\" are among the oldest publications in this field.\n\nSoL has a \"Sustainability Consortium\" which helps large corporations including Unilever, Coca-Cola, Seventh Generation, and Schlumberger share ideas and tackle common issues related to sustainability.\nThe ECLO is regularly cooperating with EU projects to promote the idea of sustainable learning, conceptualizing projects, and disseminating its results.\n\n"}
{"id": "392489", "url": "https://en.wikipedia.org/wiki?curid=392489", "title": "Stephen Gray (scientist)", "text": "Stephen Gray (scientist)\n\nStephen Gray (December 1666 – 7 February 1736) was an English dyer and astronomer who was the first to systematically experiment with electrical conduction. Until his work in 1729 the emphasis had been on the simple generation of static charges and investigations of the static phenomena (electric shocks, plasma glows, etc.). He also first made the distinction between conduction and insulation, and discovered the action-at-a-distance phenomenon of electrostatic induction.\n\nGray was born in Canterbury, Kent and after some basic schooling, he was apprenticed to his father (and later his elder brother) in the cloth-dyeing trade. His interests lay with natural science and particularly with astronomy, and he managed to educate himself in these developing disciplines, mainly through wealthy friends in the district who gave him access to their libraries and scientific instruments. Science was very much a rich-man's hobby at this time.\n\nHe ground his own lenses and constructed his own telescope, and with this instrument he made a number of minor discoveries (mainly in the area of sunspots), gaining a reputation for accuracy in his observations. Some of his reports were published by the Royal Society through the agency of a friend Henry Hunt who was a member of the Society's secretarial staff.\n\nSome of this material came to the notice of John Flamsteed (who was related to some Kent friends of Gray) the first English Astronomer Royal, who was building the new Royal Greenwich Observatory. Flamsteed was attempting to construct a detailed and accurate star-map of the heavens, in the hope that this would eventually solve the problem of longitude determination for ocean navigators. Gray helped him with many of the observations and calculations (possibly without being paid).\n\nGray and Flamsteed became constant correspondents and friends, and this seems to have created problems for Gray in being accepted formally into the world of science. Flamsteed was involved in a prolonged dispute (more like a 'heated battle') with Sir Isaac Newton over access to preliminary star-chart data. This boiled over and became a factional war in the Royal Society, which Newton dominated (virtually excluding Flamsteed and his associates) for decades.\n\nGray worked for a while on the second English observatory being built at Cambridge, but it was badly managed by Newton's friend and associate Roger Cotes, and finally the project collapsed leaving Gray with little option but to return to his dyeing trade in Canterbury. His health was a problem, and before long he was in London assisting Dr John Theophilus Desaguliers, an acolyte of Isaac Newton and occasionally one of the Royal Society's demonstrators. Desaguliers acted as a scientific consultant and gave lectures around the country (and on the Continent) about new scientific discoveries: he also ran a boarding house for visiting gentlemen with scientific interests. Gray was not paid by Desaguliers, but provided with accommodation in exchange for his ability to discuss scientific subjects with the boarders.\n\nDesaguliers' boarding house was demolished to make way for Westminster Bridge, and poverty intervened for Gray. In 1720, through the efforts of John Flamsteed and Sir Hans Sloane (later President of the Royal Society) he managed to obtain a pensioned position at the Charterhouse in London (a home for destitute gentlemen who had served their country), also linked to a boys' school. During this time he began experimenting again with static electricity, using a glass tube as a friction generator.\n\nOne night, in his Charterhouse rooms, he noticed that the cork at the end of his tube (needed to keep moisture and dust out) generated an attractive force on small pieces of paper and chaff when the tube was rubbed. Normally the cork would not have carried an electrical charge, but climatic conditions and variations in the materials meant that the cork was accumulating charge. When he extended the cork with a small fir stick plugged into the middle, the charge manifested itself at the end of the stick, and then on an ivory ball (perforated with a hole) he had stuck on the end. So he tried longer sticks, and finally he added a length of an oily hemp pack-thread connected to the ivory ball. In the process he had discovered that the \"electric virtue\" was not just a 'static' phenomenon (like a local pin-prick), but rather a fluid-like substance that would carry over distance. The terminating ivory ball would still act to attract light objects in the same way as the electrified glass tube.\n\nOver the next few days he extended the reach of his thread-wire (he only had a short piece of wire, and did not understand the significance of metal as a conductor) and found that it would carry from his balcony down into the courtyard below. He discovered that electricity would travel around bends in the thread and that it appeared unaffected by gravity. He was also able to transmit charges to metal objects (poker, tongs, kettle, etc.) which were generally regarded in those days as 'non-electrics' because they couldn't generate or hold static charge. He also discovered that silk would not carry the 'virtue', while the thicker pack-thread and wire could.\n\nThen between 30 June and 2 July 1729 while in Kent he extended this first electrical network and made many new discoveries. On a visit to the Reverend Granville Wheler, (a wealthy friend, member of the Royal Society and Famsteed's relative), the two men extended the conduction experiments through pack-thread laced up and down the length of a large gallery in Wheler's manor house, Otterden Place in Kent. In the process, Gray and Wheler discovered the importance of insulating their thread 'conductor' from earth contact (the wall of the house) by using silk for suspension. They noticed that if wire was used to support the pack-thread, all the 'electrical virtue' leaked away. Initially they thought the difference was due to the relative thicknesses of the silk, thread and wire, but later they realised that silk itself was much less conducting than the wire—so they used only silk to support (and thereby insulate) the hemp pack-thread used as their main conductor.\n\nThe next day they dropped the thread from the house tower to the garden and then extended it out across a paddock to a distance of 800 feet using paired garden-stakes with short spans of silk to keep the pack-thread from touching the ground. Wheler reported this to many of his Royal Society friends, and Gray wrote the full details in a letter to John Desaguliers.\n\nFrom these experiments came an understanding of the role played by conductors and insulators (names applied by John Desaguliers). The French scientists, the Abbe Nollet and C.F. du Fay, visited Gray and Wheler in 1732, saw the experiment, and returned to France where du Fay formulated the first comprehensive theory of electricity called the \"two-fluid\" theory. This theory was championed by the Abbé Nollet and accepted by most experimenters in Europe for a time, but later it was refined and then superseded by the ideas of the English experimenters John Bevis and William Watson who were in correspondence with Benjamin Franklin's group in Philadelphia. They jointly devised a theory of a single-fluid/two-states—virtually the super-abundance or absence of one fluid—which Watson later termed positive and negative. These ideas fitted the facts slightly better than the two-fluid concept, especially after the invention of the Leyden Jar, and so this single-fluid theory eventually prevailed. We now know that both were almost equally incorrect.\n\nGray went on to make more electrical discoveries, the most noticeable being electrical induction (creating an electrical charge in a suspended object without contact). This experiment was widely celebrated around Europe as the famous \"Flying Boy\" demonstration: a boy was suspended on silk cords, and then charged by Gray bringing his rubbed tube (static electric generator) close to the boy's feet, but without touching. Gray showed that the boy's face and hands still attracted chaff, paper, etc. Gray certainly realised that the phenomenon of 'electric virtue' was the same as lightning (as did most experimenters), many years before Franklin (supposedly) flew his kite and the French experimenters Dilibard and Delor captured a charge from lightning in a Leyden Jar.\n\nWhen Sloane took over the Royal Society on Newton's death, Gray belatedly received the recognition denied him previously. Gray was too poor to pay the dues so he was not a member of the Royal Society, and many of his experiments had been taken up and became part of the demonstration repertoire of John Desaguliers. There is also a story that he was denied recognition by the Newton faction within the Royal Society because of his links to Flamsteed (who was constantly in dispute with Isaac Newton), but this can be dismissed as highly unlikely: Newton died in March 1727 nearly two years before Gray began his conduction experiments, and Hans Sloan, who ran the Royal Society after Newton's death was a friend and financial supporter of Gray. The fact is that electricity was not considered that important at the time, and the Society's magazine was not published for a couple of years due to financial constraints.\n\nSloan took an active part in promoting Gray who received the Royal Society's first Copley Medal in 1731 for his work on conduction and insulation, and also its second in 1732 for the induction experiments. In 1732 the Royal Society also admitted him as an honorary member, and he died destitute a few years later in 1736.\n\nDespite the importance of his discoveries (it can be argued that he was the inventor of electrical communications) he received little credit, supposedly because of the factional dispute in the Royal Society, and the dominance of Newtonianism (which became the Free Masonic 'ideology'). John Desaguliers was far more famous than Gray, and many of the discoveries became attached to the name Desaguliers by virtue of his flamboyant demonstrations. By the time Gray's priority was publicly recognised, experiments in electricity had moved on and people were interested more in the spectacular feats by Franklin and others in capturing lightning in their Leyden Jars. So Gray's discoveries tended to look trivial, and for this reason, some historians tend to overlook his work.\n\nThere is no monument to Gray, and little recognition of what he achieved in his scientific discoveries. He is believed to be buried in a common grave in an old London cemetery, in an area reserved for pauper pensioners from the Charterhouse. In 2017 the School of Physical Sciences at the University of Kent, in Canterbury, initiated the Stephen Gray Lectures in his memory. The first two Lecturers were Dr David H. Clark (2017) and Prof Sir Michael Berry (2018).\n\n\n"}
{"id": "36435766", "url": "https://en.wikipedia.org/wiki?curid=36435766", "title": "Time crystal", "text": "Time crystal\n\nA time crystal or space-time crystal is a structure that repeats in time, as well as in space. Normal three-dimensional crystals have a repeating pattern in space, but remain unchanged as time passes. Time crystals repeat themselves in time as well, leading the crystal to change from moment to moment. A time crystal never reaches thermal equilibrium, as it is a type of non-equilibrium matter, a form of matter proposed in 2012, and first observed in 2017. This state of matter cannot be isolated from its environment—it is an open system in non-equilibrium.\n\nThe idea of a time crystal was first described by Nobel laureate and MIT professor Frank Wilczek in 2012. Later work developed a more precise definition for time crystals. It was proven that they cannot exist in equilibrium. Then, in 2014 Krzysztof Sacha at Jagiellonian University in Krakow predicted the behaviour of discrete time crystals in a periodically-driven many-body system. In 2016, Norman Yao et al. at the University of California, Berkeley proposed a different way to create time crystals in spin systems. From there, Christopher Monroe and Mikhail Lukin independently confirmed this in their labs. Both experiments were published in \"Nature\" in 2017.\n\nThe idea of a space-time crystal was first put forward by Frank Wilczek, a professor at MIT and Nobel laureate, in 2012.\n\nIn 2013, Xiang Zhang, a nanoengineer at University of California, Berkeley, and his team proposed creating a time crystal in the form of a constantly rotating ring of charged ions.\n\nIn response to Wilczek and Zhang, Patrick Bruno, a theorist at the European Synchrotron Radiation Facility in Grenoble, France, published several papers in 2013 claiming to show that space-time crystals were impossible. Also later Masaki Oshikawa from the University of Tokyo showed that time crystals would be impossible at their ground state; moreover, he implied that any matter cannot exist in non-equilibrium in its ground state.\n\nSubsequent work developed more precise definitions of time translation symmetry-breaking which ultimately led to a 'no-go' proof that quantum time crystals in equilibrium are not possible.\n\nSeveral realizations of time crystals, which avoid the equilibrium no-go arguments, were later proposed. Krzysztof Sacha at Jagiellonian University in Krakow predicted the behaviour of discrete time crystals in a periodically driven system of ultracold atoms. Later works suggested periodically driven quantum spin systems could show similar behaviour.\n\nNorman Yao at Berkeley studied a different model of time crystals. His blueprint was successfully used by two teams: a group led by Harvard's Mikhail Lukin and a group led by Christopher Monroe at University of Maryland.\n\nSymmetries in nature lead directly to conservation laws, something which is precisely formulated by the Noether theorem.\n\nThe basic idea of \"time-translation symmetry\" is that a translation in time has no effect on physical laws, i.e. that the laws of nature that apply today were the same in the past and will be the same in the future. This symmetry implies the conservation of energy.\n\n Normal crystals exhibit \"broken translation symmetry\": they have repeated patterns in space, and are not invariant under arbitrary translations or rotations. The laws of physics are unchanged by arbitrary translations and rotations. However, if we hold fixed the atoms of a crystal, the dynamics of electrons or other particles in the crystal depends on how it moves relative to the crystal, and particles' momentum can change by interacting with the atoms of a crystal — for example in Umklapp processes. Quasimomentum, however, is conserved in a perfect crystal.\n\nTime crystals seem to break \"time-translation symmetry,\" and have repeated patterns in time. Fields or particles may change their energy by interacting with a time crystal, just as they can change their momentum by interacting with a spatial crystal.\n\nTime crystals do not violate the laws of thermodynamics: energy in the overall system is conserved, such a crystal does not spontaneously convert thermal energy into mechanical work, and it cannot serve as a perpetual store of work. But it may change perpetually in a fixed pattern in time for as long as the system can be maintained. They possess \"motion without energy\"—their apparent motion does not represent conventional kinetic energy.\n\nIt has been proven that a time crystal cannot exist in thermal equilibrium. Recent years have seen more studies of non-equilibrium quantum fluctuations.\n\nIn October 2016, Christopher Monroe at the University of Maryland claimed to have created the world's first discrete time crystal. Using the idea from Yao's proposal, his team trapped a chain of Yb (ytterbium) ions in a Paul trap, confined by radio frequency electromagnetic fields. One of the two spin states was selected by a pair of laser beams. The lasers were pulsed, with the shape of the pulse controlled by an acousto-optic modulator, using the Tukey window to avoid too much energy at the wrong optical frequency. The hyperfine electron states in that setup, S |F=0, m = 0⟩ and |F = 1, m = 0⟩, have very close energy levels, separated by 12.642831 GHz. Ten Doppler-cooled ions were placed in a line 0.025 mm long and coupled together.\n\nThe researchers observed a subharmonic oscillation of the drive. The experiment showed \"rigidity\" of the time crystal, where the oscillation frequency remained unchanged even when the time crystal was perturbed, and that it gained a frequency of its own and vibrated according to it (rather than only the frequency of the drive). However, once the perturbation or frequency of vibration grew too strong, the time crystal \"melted\" and lost this subharmonic oscillation, and it returned to the same state as before where it moved only with the induced frequency.\n\nLater in 2016, Mikhail Lukin at Harvard also reported the creation of a driven time crystal. His group used a diamond crystal doped with a high concentration of nitrogen-vacancy centers, which have strong dipole-dipole coupling and relatively long-lived spin coherence. This strongly-interacting dipolar spin system was driven with microwave fields, and the ensemble spin state with an optical (laser) field was determine. It was observed that the spin polarization evolved at half the frequency of the microwave drive. The oscillations persisted for over 100 cycles. This subharmonic response to the drive frequency is seen as a signature of time-crystalline order.\nA similar idea called a choreographic crystal has been proposed.\n\n"}
