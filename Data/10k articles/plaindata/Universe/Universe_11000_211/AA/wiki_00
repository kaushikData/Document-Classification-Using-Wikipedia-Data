{"id": "21152701", "url": "https://en.wikipedia.org/wiki?curid=21152701", "title": "Applications of capacitors", "text": "Applications of capacitors\n\nCapacitors have many uses in electronic and electrical systems. They are so ubiquitous that it is rare that an electrical product does not include at least one for some purpose.\n\nA capacitor can store electric energy when it is connected to its charging circuit. And when it is disconnected from its charging circuit, it can dissipate that stored energy, so it can be used like a temporary battery. Capacitors are commonly used in electronic devices to maintain power supply while batteries are being changed. (This prevents loss of information in volatile memory.)\n\nConventional electrostatic capacitors provide less than 360 joules per kilogram of energy density, while capacitors using developing technology can provide more than 2.52 kilojoules per kilogram.\n\nIn car audio systems, large capacitors store energy for the amplifier to use on demand. \n\nAn uninterruptible power supply (UPS) can be equipped with maintenance-free capacitors to extend service life.\n\nGroups of large, specially constructed, low-inductance high-voltage capacitors (\"capacitor banks\") are used to supply huge pulses of current for many pulsed power applications. These include electromagnetic forming, Marx generators, pulsed lasers (especially TEA lasers), pulse forming networks, fusion research, and particle accelerators.\n\nLarge capacitor banks (reservoirs) are used as energy sources for the exploding-bridgewire detonators or slapper detonators in nuclear weapons and other specialty weapons. Experimental work is under way using banks of capacitors as power sources for electromagnetic armour and electromagnetic railguns or coilguns.\n\nReservoir capacitors are used in power supplies where they smooth the output of a full or half wave rectifier. They can also be used in charge pump circuits as the energy storage element in the generation of higher voltages than the input voltage.\n\nCapacitors are connected in parallel with the DC power circuits of most electronic devices to smooth current fluctuations for signal or control circuits. Audio equipment, for example, uses several capacitors in this way, to shunt away power line hum before it gets into the signal circuitry. The capacitors act as a local reserve for the DC power source, and bypass AC currents from the power supply. This is used in car audio applications, when a stiffening capacitor compensates for the inductance and resistance of the leads to the lead-acid car battery.\n\nIn electric power distribution, capacitors are used for power factor correction. Such capacitors often come as three capacitors connected as a three phase Electrical load.Usually, the values of these capacitors are given not in farads but rather as a reactive power in volt-amperes reactive (VAr). The purpose is to counteract inductive loading from devices like Induction motor,electric motors and transmission lines to make the load appear to be mostly resistive. Individual motor or lamp loads may have capacitors for power factor correction, or larger sets of capacitors (usually with automatic switching devices) may be installed at a load center within a building or in a large utility electrical substation. In high-voltage direct current transmission systems, power factor correction capacitors may have tuning inductors to suppress harmonic currents that would otherwise be injected into the AC power system.\n\nBecause capacitors pass AC but block DC signals (when charged up to the applied DC voltage), they are often used to separate the AC and DC components of a signal. This method is known as \"AC coupling\" or \"capacitive coupling\". Here, a large value of capacitance, whose value need not be accurately controlled, but whose reactance is small at the signal frequency, is employed.\n\nA decoupling capacitor is a capacitor used to decouple one part of a circuit from another. Noise caused by other circuit elements is shunted through the capacitor, reducing the effect they have on the rest of the circuit. It is most commonly used between the power supply and ground.\nAn alternative name is \"bypass capacitor\" as it is used to bypass the power supply or other high impedance component of a circuit.\n\nWhen an inductive circuit is opened, the current through the inductance collapses quickly, creating a large voltage across the open circuit of the switch or relay. If the inductance is large enough, the energy will generate an electric spark, causing the contact points to oxidize, deteriorate, or sometimes weld together, or destroying a solid-state switch. A snubber capacitor across the newly opened circuit creates a path for this impulse to bypass the contact points, thereby preserving their life; these were commonly found in contact breaker ignition systems, for instance. Similarly, in smaller scale circuits, the spark may not be enough to damage the switch but will still radiate undesirable radio frequency interference (RFI), which a filter capacitor absorbs. Snubber capacitors are usually employed with a low-value resistor in series, to dissipate energy and minimize RFI. Such resistor-capacitor combinations are available in a single package.\n\nCapacitors are also used in parallel to interrupt units of a high-voltage circuit breaker in order to equally distribute the voltage between these units. In this case they are called grading capacitors.\n\nIn schematic diagrams, a capacitor used primarily for DC charge storage is often drawn vertically in circuit diagrams with the lower, more negative, plate drawn as an arc. The straight plate indicates the positive terminal of the device, if it is polarized (see electrolytic capacitor).\n\nIn single phase squirrel cage motors, the primary winding within the motor housing is not capable of starting a rotational motion on the rotor, but is capable of sustaining one. To start the motor, a secondary winding is used in series with a non-polarized \"starting capacitor\" to introduce a lag in the sinusoidal current through the starting winding. When the secondary winding is placed at an angle with respect to the primary winding, a rotating electric field is created. The force of the rotational field is not constant, but is sufficient to start the rotor spinning. When the rotor comes close to operating speed, a centrifugal switch (or current-sensitive relay in series with the main winding) disconnects the capacitor. The start capacitor is typically mounted to the side of the motor housing. These are called capacitor-start motors, and have relatively high starting torque.\n\nThere are also capacitor-run induction motors which have a permanently connected phase-shifting capacitor in series with a second winding. The motor is much like a two-phase induction motor.\n\nMotor-starting capacitors are typically non-polarized electrolytic types, while running capacitors are conventional paper or plastic film dielectric types.\n\n5.Signal processing.\nThe energy stored in capacitor can be used to represent information, either in binary form, as in DRAMs, or in analogue form, as in analog sampled filters and Charge-coupled device CCDs. Capacitors can be used in analog circuits as components of integrators or more complex filters and in negative feedback loop stabilization. Signal processing circuits also use capacitors to integrate a current signal.\n\nCapacitors and inductors are applied together in tuned circuits to select information in particular frequency bands. For example, radio receivers rely on variable capacitors to tune the station frequency. Speakers use passive analog crossovers, and analog equalizers use capacitors to select different audio bands.\n\nMost capacitors are designed to maintain a fixed physical structure. However, various factors can change the structure of the capacitor; the resulting change in capacitance can be used to sense those factors.\n\nThe effects of varying the characteristics of the dielectric can also be used for sensing and measurement. Capacitors with an exposed and porous dielectric can be used to measure humidity in air. Capacitors are used to accurately measure the fuel level in airplanes; as the fuel covers more of a pair of plates, the circuit capacitance increases.\n\nA capacitor with a flexible plate can be used to measure strain or pressure or weight.\n\nIndustrial pressure transmitters used for process control use pressure-sensing diaphragms, which form a capacitor plate of an oscillator circuit. Capacitors are used as the sensor in condenser microphones, where one plate is moved by air pressure, relative to the fixed position of the other plate. Some accelerometers use microelectromechanical systems (MEMS) capacitors etched on a chip to measure the magnitude and direction of the acceleration vector. They are used to detect changes in acceleration, e.g. as tilt sensors or to detect free fall, as sensors triggering airbag deployment, and in many other applications. Some fingerprint sensors use capacitors.\n\nCapacitive touch switches are now used on many consumer electronic products.\n\nA capacitor can possess spring-like qualities in an oscillator circuit. In the image example, a capacitor acts to influence the biasing voltage at the npn transistor's base. The resistance values of the voltage-divider resistors and the capacitance value of the capacitor together control the oscillatory frequency.\n\nCapacitors may retain a charge long after power is removed from a circuit; this charge can cause dangerous or even potentially fatal shocks or damage connected equipment. For example, even a seemingly innocuous device such as a disposable camera flash unit powered by a 1.5 volt AA battery contains a capacitor which may be charged to over 300 volts. This is easily capable of delivering a shock. Service procedures for electronic devices usually include instructions to discharge large or high-voltage capacitors. Capacitors may also have built-in discharge resistors to dissipate stored energy to a safe level within a few seconds after power is removed. High-voltage capacitors are stored with the terminals shorted, as protection from potentially dangerous voltages due to dielectric absorption.\n\nSome old, large oil-filled capacitors contain polychlorinated biphenyls (PCBs). It is known that waste PCBs can leak into groundwater under landfills. Capacitors containing PCB were labelled as containing \"Askarel\" and several other trade names. PCB-filled capacitors are found in very old (pre 1975) fluorescent lamp ballasts, and other applications.\n\nHigh-voltage capacitors may catastrophically fail when subjected to voltages or currents beyond their rating, or as they reach their normal end of life. Dielectric or metal interconnection failures may create arcing that vaporizes dielectric fluid, resulting in case bulging, rupture, or even an explosion. Capacitors used in RF or sustained high-current applications can overheat, especially in the center of the capacitor rolls. Capacitors used within high-energy capacitor banks can violently explode when a short in one capacitor causes sudden dumping of energy stored in the rest of the bank into the failing unit. High voltage vacuum capacitors can generate soft X-rays even during normal operation. Proper containment, fusing, and preventive maintenance can help to minimize these hazards.\n\nHigh-voltage capacitors can benefit from a pre-charge to limit in-rush currents at power-up of high voltage direct current (HVDC) circuits. This will extend the life off the component and may mitigate high-voltage hazards.\n"}
{"id": "7737602", "url": "https://en.wikipedia.org/wiki?curid=7737602", "title": "Aquamid", "text": "Aquamid\n\nAquamid is a non-absorbable soft volume filler for aesthetic and reconstructive purposes. Aquamid is the trade name for a specific formulation of 97.5% water for injection and 2.5% cross-linked polyacrylamide. It is injected subcutaneously to correct wrinkles and folds or to add volume. Common aesthetic indications are nasolabial folds, lip augmentation, cheek contouring, nose enhancement. Aquamid is also used to correct signs of facial lipoatrophy or fat wasting in HIV+ patients.\n\nAquamid has been evaluated in several clinical trials involving more than 5,000 patients including a comparative trial in the United States. Data from this trial has been used to support a PMA application to the FDA. Aquamid is not yet approved for sale in the United States. \n\nAquamid is developed, produced and commercialized by Contura International. It was approved in Europe in 2001 for facial augmentation and minor body contouring and is available in several countries in Europe, Asia, the Middle East and Latin America.\n\n"}
{"id": "40132974", "url": "https://en.wikipedia.org/wiki?curid=40132974", "title": "Balli Dam", "text": "Balli Dam\n\nThe Balli Dam is a gravity dam under construction on the Ortasu River (a tributary of the Hezil River) in Uludere district of Şırnak Province, southeast Turkey. Under contract from Turkey's State Hydraulic Works, construction on the dam began in 2008 and a completion date has not been announced.\n\nThe reported purpose of the dam is water storage and it can also support an 8.36 MW hydroelectric power station in the future. Another purpose of the dam which has been widely reported in the Turkish press is to reduce the freedom of movement of Kurdistan Workers' Party (PKK) militants. Blocking and flooding valleys in close proximity to the Iraq–Turkey border is expected to help curb cross-border PKK smuggling and deny caves in which ammunition can be stored. A total of 11 dams along the border; seven in Şırnak Province and four in Hakkâri Province were implemented for this purpose. In Şırnak they are the Silopi, Şırnak and Uludere Dams downstream of the Balli Dam and the Kavşaktepe, Musatepe and Çetintepe Dams upstream on the Ortasu River. In Hakkari are the Gölgeliyamaç (since cancelled) and Çocuktepe Dams on the Güzeldere River and the Aslandağ and Beyyurdu Dams on the Bembo River.\n\n"}
{"id": "51511497", "url": "https://en.wikipedia.org/wiki?curid=51511497", "title": "Ballyboodan Ogham Stone", "text": "Ballyboodan Ogham Stone\n\nBallyboodan Ogham Stone (CIIC 038) is a ogham stone and National Monument located in County Kilkenny, Ireland.\n\nBallyboodan Ogham Stone lies in an enclosure on the roadside, south of Knocktopher.\n\nBallyboodan Ogham Stone was carved c. AD 700–900. It was rediscovered before 1841, and was knocked down by treasure-seekers. In 1850 the tenant of the land wanted to destroy it as an obstacle to the plough, but luckily it was saved by the landlord, Sir Hercules Richard Langrishe, 3rd Baronet.\n\nBallyboodan Ogham Stone is a block of slate measuring 231 × 175 × 23 cm and has Ogham carvings incised on one edge. (, \"Here is Corb, son of Labraid\").\n"}
{"id": "5133659", "url": "https://en.wikipedia.org/wiki?curid=5133659", "title": "Bandsaw box", "text": "Bandsaw box\n\nBand saw boxes are boxes made out of wood using only a bandsaw for cutting them out. The wood may be a solid block, a laminated block or a log from the woodpile. Whereas most boxes have straight sides and square corners, band saw boxes have virtually no restrictions as to shape. They can be oval, heart-shaped, lizard-shaped, or any shape the maker can think of. Other tools such as belt sanders and drum sanders can be used to shape and sand the box smooth. Relief cuts are always needed if you are going to make a bandsaw box. If you don't, the box always seems to fall apart. If the bandsaw has a little hitch in it the box usually snaps in two.\n\nThere are multiple techniques for constructing band saw boxes.\n\nThe primary technique starts by cutting the main shape of the box. Then a 1/8\" to 1/4\" piece of wood is cut off what is to become the back. The drawer shape is cut within the main shape, which involves cutting through the main body, and the body must be glued back together. Once the drawer shape is cut, the usual technique is to remove 1/8\" to 1/4\" of material from both the front and rear of the drawer shape to be used as faces. The remaining stock is then reduced or hollowed out to produce a drawer or cavity. The front and rear drawer faces are glued back to the remaining hollowed drawer stock, and the back that was cut off is glued to the main body. A handle can be added to the front of the drawer if desired.\n\n\n"}
{"id": "264752", "url": "https://en.wikipedia.org/wiki?curid=264752", "title": "Baryonic dark matter", "text": "Baryonic dark matter\n\nIn astronomy and cosmology, baryonic dark matter is dark matter composed of baryons. Only a small proportion of the dark matter in the universe is likely to be baryonic.\n\nAs \"dark matter\", baryonic dark matter is undetectable by its emitted radiation, but its presence can be inferred from gravitational effects on visible matter. This form of dark matter is composed of \"baryons\", heavy subatomic particles such as protons and neutrons and combinations of these, including non-emitting ordinary atoms.\n\nBaryonic dark matter may occur in non-luminous gas or in Massive Astrophysical Compact Halo Objects (MACHOs) - condensed objects such as black holes, neutron stars, white dwarfs, very faint stars, or non-luminous objects like planets and brown dwarfs.\n\nThe total amount of baryonic dark matter can be inferred from models of Big Bang nucleosynthesis, and observations of the cosmic microwave background. Both indicate that the amount of baryonic dark matter is much smaller than the total amount of dark matter.\n\nFrom the perspective of Big Bang nucleosynthesis, a larger amount of ordinary (baryonic) matter implies a denser early universe, more efficient conversion of matter to helium-4, and less unburned deuterium remaining. If all of the dark matter in the universe were baryonic, then there would be much less deuterium in the universe than is observed. This could be resolved if more deuterium were somehow generated, but large efforts in the 1970s failed to identify plausible mechanisms for this to occur. For instance, MACHOs, which include, for example, brown dwarfs (bodies of hydrogen and helium with masses less than ), never begin nuclear fusion of hydrogen, but they do burn deuterium. Other possibilities that were examined include \"Jupiters\", which are similar to brown dwarfs but have masses formula_1 and do not burn anything, and white dwarfs.\n\n"}
{"id": "6119750", "url": "https://en.wikipedia.org/wiki?curid=6119750", "title": "CHAPS detergent", "text": "CHAPS detergent\n\nCHAPS is a zwitterionic surfactant used in the laboratory to solubilize biological macromolecules such as proteins. It may be synthesized from cholic acid and is zwitterionic due to its quaternary ammonium and sulfonate groups; it is structurally similar to certain bile acids, such as taurodeoxycholic acid and taurochenodeoxycholic acid. It is used as a non-denaturing detergent in the process of protein purification and is especially useful in purifying membrane proteins, which are often sparingly soluble or insoluble in aqueous solution due to their native hydrophobicity.\n\nCHAPS is an abbreviation for 3-[(3-cholamidopropyl)dimethylammonio]-1-propanesulfonate. A related detergent, called CHAPSO, has the same basic chemical structure with an additional hydroxyl functional group; its full chemical name is 3-[(3-cholamidopropyl)dimethylammonio]-2-hydroxy-1-propanesulfonate. Both detergents have low light absorbance in the ultraviolet region of the electromagnetic spectrum, which is useful for monitoring ongoing chemical reactions or protein-protein binding with UV/Vis spectroscopy.\n\n"}
{"id": "19071829", "url": "https://en.wikipedia.org/wiki?curid=19071829", "title": "Carbon Power Plant", "text": "Carbon Power Plant\n\nCarbon Power Plant, also known as Castle Gate Power Plant was a small, 190-MWe coal-fired power station in Utah, USA operated by PacifiCorp. Its units 1 and 2, rated at 75 and 113.6 MWe, were launched into service in 1954 and 1957. The plant is located at , about north of Helper, Utah, on the east bank of Price River. \n\nThe plant was shut down on April 16, 2015. \n"}
{"id": "39443843", "url": "https://en.wikipedia.org/wiki?curid=39443843", "title": "Chienshan Power Plant", "text": "Chienshan Power Plant\n\nThe Chienshan Power Plant () is a fuel-fired power plant in Huxi Township, Penghu County, Taiwan. With the total capacity of 140 MW, the power plant is the largest diesel fuel-fired power plant in Taiwan. Chienshan Power Plant is the only major power plant in Penghu Island.\n\n"}
{"id": "5974", "url": "https://en.wikipedia.org/wiki?curid=5974", "title": "Corundum", "text": "Corundum\n\nCorundum is a crystalline form of aluminium oxide () typically containing traces of iron, titanium, vanadium and chromium. It is a rock-forming mineral. It is also a naturally transparent material, but can have different colors depending on the presence of transition metal impurities in its crystalline structure. Corundum has two primary gem varieties, ruby and sapphire. Rubies are red due to the presence of chromium, and sapphires exhibit a range of colors depending on what transition metal is present. A rare type of sapphire, padparadscha sapphire, is pink-orange. \n\nThe name \"corundum\" is derived from the Tamil word \"Kurundam\", which in turn derives from the Sanskrit \"Kuruvinda\".\n\nBecause of corundum's hardness (pure corundum is defined to have 9.0 on the Mohs scale), it can scratch almost every other mineral. It is commonly used as an abrasive on everything from sandpaper to large tools used in machining metals, plastics, and wood. Some emery is a mix of corundum and other substances, and the mix is less abrasive, with an average Mohs hardness of 8.0.\n\nIn addition to its hardness, corundum has a density of 4.02 g/cm, which is unusually high for a transparent mineral composed of the low-atomic mass elements aluminium and oxygen.\n\nCorundum occurs as a mineral in mica schist, gneiss, and some marbles in metamorphic terranes. It also occurs in low silica igneous syenite and nepheline syenite intrusives. Other occurrences are as masses adjacent to ultramafic intrusives, associated with lamprophyre dikes and as large crystals in pegmatites. It commonly occurs as a detrital mineral in stream and beach sands because of its hardness and resistance to weathering. The largest documented single crystal of corundum measured about , and weighed . The record has since been surpassed by certain synthetic boules.\n\nCorundum for abrasives is mined in Zimbabwe, Pakistan, Afghanistan, Russia, Sri Lanka, and India. Historically it was mined from deposits associated with dunites in North Carolina, US and from a nepheline syenite in Craigmont, Ontario. Emery-grade corundum is found on the Greek island of Naxos and near Peekskill, New York, US. Abrasive corundum is synthetically manufactured from bauxite. Four corundum axes dating back to 2500 BCE from the Liangzhou culture have been discovered in China.\n\nIn 1837, Marc Antoine Gaudin made the first synthetic rubies by fusing alumina at a high temperature with a small amount of chromium as a pigment. In 1847, Ebelmen made white synthetic sapphires by fusing alumina in boric acid. In 1877 Frenic and Freil made crystal corundum from which small stones could be cut. Frimy and Auguste Verneuil manufactured artificial ruby by fusing and with a little chromium at temperatures above . In 1903, Verneuil announced he could produce synthetic rubies on a commercial scale using this flame fusion process.\n\nThe Verneuil process allows the production of flawless single-crystal sapphires, rubies and other corundum gems of much larger size than normally found in nature. It is also possible to grow gem-quality synthetic corundum by flux-growth and hydrothermal synthesis. Because of the simplicity of the methods involved in corundum synthesis, large quantities of these crystals have become available on the market causing a significant reduction of price in recent years. Apart from ornamental uses, synthetic corundum is also used to produce mechanical parts (tubes, rods, bearings, and other machined parts), scratch-resistant optics, scratch-resistant watch crystals, instrument windows for satellites and spacecraft (because of its transparency in the ultraviolet to infrared range), and laser components.\n\nCorundum crystallizes with trigonal symmetry in the space group \"R\"\"c\" and has the lattice parameters a = 4.75 Å and c = 12.982 Å at standard conditions. The unit cell contains six formula units.\n\nThe toughness of corundum is sensitive to surface roughness and crystallographic orientation. It may be 6-7 MPa·m for synthetic crystals, and ~4 for natural\n\nIn the lattice of corundum, the oxygen atoms form a slightly distorted hexagonal close packing, in which two-thirds of the gaps between the octahedra are occupied by aluminum ions.\n"}
{"id": "1419800", "url": "https://en.wikipedia.org/wiki?curid=1419800", "title": "Cryogenic Dark Matter Search", "text": "Cryogenic Dark Matter Search\n\nThe Cryogenic Dark Matter Search (CDMS) is a series of experiments designed to directly detect particle dark matter in the form of WIMPs. Using an array of semiconductor detectors at millikelvin temperatures, CDMS has set the most sensitive limits to date on the interactions of WIMP dark matter with terrestrial materials. The first experiment, CDMS I, was run in a tunnel under the Stanford University campus. The current experiment, SuperCDMS, is located deep underground in the Soudan Mine in northern Minnesota.\n\nObservations of the large-scale structure of the universe show that matter is aggregated into very large structures that have not had time to form under the force of their own self-gravitation. It is generally believed that some form of missing mass is responsible for increasing the gravitational force at these scales, although this mass has not been directly observed. This is a problem; normal matter in space will heat up until it gives off light, so if this missing mass exists, it is generally assumed to be in a form that is not commonly observed on earth.\n\nA number of proposed candidates for the missing mass have been put forward over time. Early candidates included heavy baryons that would have had to be created in the big bang, but more recent work on nucleosynthesis seems to have ruled most of these out. Another candidate are new types of particles known as weakly interacting massive particles, or \"WIMP\"s. As the name implies, WIMPs interact weakly with normal matter, which explains why they are not easily visible.\n\nDetecting WIMPs thus presents a problem; if the WIMPs are very weakly interacting, detecting them will be extremely difficult. Detectors like CDMS and similar experiments measure huge numbers of interactions within their detector volume in order to find the extremely rare WIMP events.\n\nThe CDMS detectors measure the ionization and phonons produced by every particle interaction in their germanium and silicon crystal substrates. These two measurements determine the energy deposited in the crystal in each interaction, but also give information about what kind of particle caused the event. The ratio of ionization signal to phonon signal differs for particle interactions with atomic electrons (\"electron recoils\") and atomic nuclei (\"nuclear recoils\"). The vast majority of background particle interactions are electron recoils, while WIMPs (and neutrons) are expected to produce nuclear recoils. This allows WIMP-scattering events to be identified even though they are rare compared to the vast majority of unwanted background interactions.\n\nFrom Supersymmetry, the probability of a spin-independent interaction between a WIMP and a nucleus would be related to the amount of nucleons in the nucleus. Thus, a WIMP would be more likely to interact with a germanium detector than a silicon detector, since germanium is a much heavier element. Neutrons would be able to interact with both silicon and germanium detectors with similar probability. By comparing rates of interactions between silicon and germanium detectors, CDMS is able to determine the probability of interactions being caused by neutrons.\n\nCDMS detectors are disks of germanium or silicon, cooled to millikelvin temperatures by a dilution refrigerator. The extremely low temperatures are needed to limit thermal noise which would otherwise obscure the phonon signals of particle interactions. Phonon detection is accomplished with superconduction transition edge sensors (TESs) read out by SQUID amplifiers, while ionization signals are read out using a FET amplifier. CDMS detectors also provide data on the phonon pulse shape which is crucial in rejecting near-surface background events.\n\nSimultaneous detection of ionization and heat with semiconductors at low temperature was first proposed by Blas Cabrera, Lawrence M. Krauss, and Frank Wilczek.\n\nCDMS collected WIMP search data in a shallow underground site at Stanford University through 2002, and has operated (with collaboration from the University of Minnesota) in the Soudan Mine since 2003. A new detector, SuperCDMS, with interleaved electrodes, more mass, and even better background rejection is currently taking data at Soudan.\n\nOn December 17, 2009, the collaboration announced the possible detection of two candidate WIMPs, one on August 8, 2007 and the other on October 27, 2007. Due to the low number of events, the team could exclude false positives from background noise such as neutron collisions. It is estimated that such noise would produce two or more events 25% of the time. Polythene absorbers were fitted to reduce any neutron background.\n\nA 2011 analysis with lower energy thresholds, looked for evidence for low-mass WIMPs (M < 9 GeV). Their limits rule out hints claimed by a new germanium experiment called CoGeNT and the long-standing DAMA/NaI, DAMA/LIBRA annual modulation result.\n\nA further analysis of data in Physical Review Letters May 2013, revealed 3 WIMP detections with expected background of 0.7, with masses expected from WIMPs, including neutralinos. There is a 0.19% chance that these are anomalous background noise, giving the result a 99.8% (3 sigma) confidence level. Whilst not conclusive evidence for WIMPs this provides strong weight to the theories. This signal was observed by the CDMS II-experiment and it is called the CDMS Si-signal (sometimes the experiment is also called CDMS Si) because it was observed by the silicon detectors.\n\nSuperCDMS search results from October 2012 to June 2013 were published in June 2014, finding 11 events in the signal region for WIMP mass less than 30 GeV, and set an upper limit for spin-independent cross section disfavoring a recent CoGeNT low mass signal.\n\nA second generation of SuperCDMS is planned for SNOLAB. This is expanded from SuperCDMS Soudan in every way:\n\nThe increase in detector mass is not quite as large, because about 25% of the detectors will be made of silicon, which only weights 44% as much. Filling all 31 towers at this ratio would result in about 222 kg \n\nAlthough the project has suffered repeated delays (earlier plans hoped for construction to begin in 2014 and 2016), it remains active, with space allocated in SNOLAB and a scheduled construction start in early 2018.\n\nA third generation of SuperCDMS is envisioned, although still in the early planning phase. GEODM (Germanium Observatory for Dark Matter), with roughly 1500 kg of detector mass, has expressed interest in the SNOLAB \"Cryopit\" location.\n\nIncreasing the detector mass only makes the detector more sensitive if the unwanted background detections do not increase as well, thus each generation must be cleaner and better shielded than the one before. The purpose of building in ten-fold stages like this is to develop the necessary shielding techniques before finalizing the GEODM design.\n\n"}
{"id": "20688933", "url": "https://en.wikipedia.org/wiki?curid=20688933", "title": "DAMA/LIBRA", "text": "DAMA/LIBRA\n\nThe DAMA/LIBRA experiment \nis a particle detector experiment designed to detect dark matter using the direct detection approach, by using a matrix of NaI(Tl) scintillation detectors to detect dark matter particles in the galactic halo. The experiment aims to find an annual modulation of the number of detection events, caused by the variation of the velocity of the detector relative to the dark matter halo as the Earth orbits the Sun. It is located underground at the Laboratori Nazionali del Gran Sasso in Italy.\n\nIt is a follow-on to the DAMA/NaI experiment which observed an annual modulation signature over 7 annual cycles (1995-2002).\n\nThe detector is made of 25 highly radiopure scintillating thallium-doped sodium iodide (NaI(Tl)) crystals placed in a 5 by 5 matrix. Each crystal is coupled to two low background photomultipliers. The detectors are placed inside a sealed copper box flushed with highly pure nitrogen; to reduce the natural environmental background the copper box is surrounded by a low background multi-ton shield. In addition, 1 m of concrete, made from the Gran Sasso rock material, almost fully surrounds this passive shield. The installation has a 3-level sealing system which prevents environmental air reaching the detectors. The whole installation is air-conditioned and several operative parameters are continuously monitored and recorded.\n\nDAMA/LIBRA was upgraded in 2008 and in 2010. In particular, after the upgrade in 2010 the experiment entered in its phase 2, with an increase of the set-up’s sensitivity\nthanks to the lowering of the energy threshold. The DAMA/LIBRA-phase 2 is in data taking.\n\nDAMA/LIBRA phase 1 data collection started in September 2003. The DAMA/LIBRA released data correspond to 7 annual cycles. Considering these data together with those by DAMA/NaI, a total exposure (1.33 ton × yr) has been collected over 14 annual cycles. This experiment has further confirmed the presence of a model-independent annual modulation effect in the data in 2-6 keV range that satisfy all the features expected for a dark matter signal with high statistical significance.\n\nAs previously done for DAMA/NaI, careful investigations on absence of any significant systematics or side reaction in DAMA/LIBRA have been quantitatively carried out.\n\nThe obtained model independent evidence is compatible with a wide set of scenarios regarding the nature of the dark matter candidate and related astrophysical and particle physics.\n\nThe results can be compared with the CoGeNT signal and other experiment limits to evaluate interpretations as WIMPs, neutralino, and other models.\n\nThe obvious criticism of the seasonal variation of events recorded in the DAMA/LIBRA experiment is that it is in fact due to some purely seasonal effect unconnected with WIMPs. Although the deep underground location minimizes temperature swings and other direct sunlight effects, there are annual humidity fluctuations and other non-obvious effects. At moment, all these criticisms are taken in account by DAMA collaboration in analysis of the experimental data and they have been excluded, as discussed in published results. A repetition of this experiment in the Southern Hemisphere with the variation in phase with DAMA/LIBRA would discount this objection; if on the other hand variation was detected in the Southern Hemisphere that was 6 months out of phase with DAMA/LIBRA, then the seasonal variation objection would be upheld.\n\nImproved versions of DAMA/LIBRA, named SABRE (Sodium-iodide with Active Background REjection) are under construction in two places. One is at LNGS, and the other is in Australia at the Stawell Underground Physics Laboratory, a laboratory being constructed 1025 m below the surface in a gold mine in Stawell, Victoria. First results are expected in 2017.\n\n"}
{"id": "542575", "url": "https://en.wikipedia.org/wiki?curid=542575", "title": "Drude model", "text": "Drude model\n\nThe Drude model of electrical conduction was proposed in 1900 by Paul Drude to explain the transport properties of electrons in materials (especially metals). The model, which is an application of kinetic theory, assumes that the microscopic behavior of electrons in a solid may be treated classically and looks much like a pinball machine, with a sea of constantly jittering electrons bouncing and re-bouncing off heavier, relatively immobile positive ions.\n\nThe two most significant results of the Drude model are an electronic equation of motion,\n\nand a linear relationship between current density and electric field ,\n\nHere is the time, is the average momentum per electron and , and are respectively the electron charge, number density, mass, and mean free time between ionic collisions (that is, the mean time an electron has traveled since the last collision). The latter expression is particularly important because it explains in semi-quantitative terms why Ohm's law, one of the most ubiquitous relationships in all of electromagnetism, should be true.\n\nThe model was extended in 1905 by Hendrik Antoon Lorentz (and hence is also known as the Drude–Lorentz model) and is a classical model. Later it was supplemented with the results of quantum theory in 1933 by Arnold Sommerfeld and Hans Bethe, leading to the Drude–Sommerfeld model.\n\nThe Drude model considers the metal to be formed of a mass of positively charged ions from which a number of \"free electrons\" were detached. These may be thought to have become delocalized when the valence levels of the atom came in contact with the potential of the other atoms.\n\nThe Drude model neglects any long-range interaction between the electron and the ions or between the electrons. The only possible interaction of a free electron with its environment is via instantaneous collisions. The average time between subsequent collisions of such an electron is , and the nature of the collision partner of the electron does not matter for the calculations and conclusions of the Drude model.\n\nAfter a collision event, the velocity (and direction) of the electron only depends on the local temperature distribution and is completely independent of the velocity of the electron before the collision event.\n\nThe simplest analysis of the Drude model assumes that electric field is both uniform and constant, and that the thermal velocity of electrons is sufficiently high such that they accumulate only an infinitesimal amount of momentum between collisions, which occur on average every seconds.\n\nThen an electron isolated at time will on average have been travelling for time since its last collision, and consequently will have accumulated momentum\n\nDuring its last collision, this electron will have been just as likely to have bounced forward as backward, so all prior contributions to the electron's momentum may be ignored, resulting in the expression\n\nSubstituting the relations\n\nresults in the formulation of Ohm's law mentioned above:\n\nThe dynamics may also be described by introducing an effective drag force. At time the average electron's momentum will be\n\nbecause, on average, a fraction of of the electrons will not have experienced another collision, and the ones that have will contribute to the total momentum to only a negligible order.\n\nWith a bit of algebra and dropping terms of order , this results in the differential equation\n\nwhere denotes average momentum and the charge of the electrons. This, which is an inhomogeneous differential equation, may be solved to obtain the general solution of\n\nfor . The steady state solution, , is then\n\nAs above, average momentum may be related to average velocity and this in turn may be related to current density,\n\nand the material can be shown to satisfy Ohm's law formula_14 with a DC-conductivity :\n\nThe Drude model can also predict the current as a response to a time-dependent electric field with an angular frequency . The complex conductivity turns out to be\n\nHere it is assumed that\n\nPlease note, that in other conventions, used by engineers, is replaced by (or ) in all equations, which reflects the phase difference with respect to origin, rather than delay at the observation point traveling in time. The imaginary part indicates that the current lags behind the electrical field, which happens because the electrons need roughly a time to accelerate in response to a change in the electrical field. Here the Drude model is applied to electrons; it can be applied both to electrons and holes; i.e., positive charge carriers in semiconductors. The curves for are shown in the graph.\n\nIf an oscillating sinusoidal electric field with frequency formula_19 is applied to the solid, the negatively charged electrons behave as a plasma that tends to move a distance \"x\" apart from the positively charged background. As a result, the sample is polarized and there will be an excess charge at the opposite surfaces of the sample.\n\nThe dielectric constant of the sample is expressed as\nwhere formula_21 is the electric displacement and formula_22 is the polarization density.\n\nThe polarization density is written as\nand the polarization density with \"n\" electron density is\nAfter a little algebra the relation between polarization density and electric field can be expressed as\nThe frequency dependent dielectric function of the solid is\nAt a resonance frequency formula_27, called the plasma frequency, the dielectric function changes sign from negative to positive and real part of the dielectric function drops to zero.\nThe plasma frequency represents a plasma oscillation resonance or plasmon. The plasma frequency can be employed as a direct measure of the square root of the density of valence electrons in a solid. Observed values are in reasonable agreement with this theoretical prediction for a large number of materials. Below the plasma frequency, the dielectric function is negative and the field cannot penetrate the sample. Light with angular frequency below the plasma frequency will be totally reflected. Above the plasma frequency the light waves can penetrate the sample.\n\nThe characteristic behavior of a Drude metal in the time or frequency domain, i.e. exponential relaxation with time constant or the frequency dependence for stated above, is called Drude response. In a conventional, simple, real metal (e.g. sodium, silver, or gold at room temperature) such behavior is not found experimentally, because the characteristic frequency is in the infrared frequency range, where other features that are not considered in the Drude model (such as band structure) play an important role. But for certain other materials with metallic properties, frequency-dependent conductivity was found that closely follows the simple Drude prediction for . These are materials where the relaxation rate is at much lower frequencies. This is the case for certain doped semiconductor single crystals, high-mobility two-dimensional electron gases, and heavy-fermion metals.\n\nHistorically, the Drude formula was first derived in an incorrect way, namely by assuming that the charge carriers form a classical ideal gas. Arnold Sommerfeld considered quantum theory and extended the theory to the free electron model, where the carriers follow Fermi–Dirac distribution. Amazingly, the conductivity predicted turns out to be the same as in the Drude model, as it does not depend on the form of the electronic speed distribution.\n\nDrude model provides a very good explanation of DC and AC conductivity in metals, the Hall effect, and the magnetoresistance in metals near room temperature. The model also explains partly the Wiedemann–Franz law of 1853. However, it greatly overestimates the electronic heat capacities of metals. In reality, metals and insulators have roughly the same heat capacity at room temperature. The model can also be applied to positive (hole) charge carriers, as demonstrated by the Hall effect.\n\nOne note of trivia surrounding the theory is that in his original paper, Drude made a conceptual error, estimating the Lorenz number of Wiedemann-Franz law to be in fact be twice of what it classically should have been, thus making it seem in agreement with the experimental measurement. Another surprise is that the experimental value for the specific heat is about 100 smaller than the classical prediction but this factor cancels out with the mean electronic speed that is actually about 100 times bigger than Drude's calculation.\n\n\n"}
{"id": "30955378", "url": "https://en.wikipedia.org/wiki?curid=30955378", "title": "Duna Kör", "text": "Duna Kör\n\nDuna Kör () is a Hungarian environmental organization founded in 1984 as a protest body to prevent the construction of the Gabčíkovo – Nagymaros Dams. Opponents of the dam argued that it would create an environmental disaster that would displace thousands of Hungarians from villages and towns where their families had lived for centuries. Opponents of the regime soon joined this burgeoning environmental protest and by the fall of 1988 the Danube Circle had about 10,000 core followers who actively demonstrated against the dam in the streets of Budapest. These actions mirrored protests earlier in the summer of 1988, in which more than 30,000 people marched in Budapest to express their anger over the Romanian government’s plan to bulldoze entire Hungarian villages in Transylvania. Hungary had not seen public protests on this scale since 1956.\n\nIt is significant in that it is seen to be the start of the erosion of Communist power in Hungary, which ended in May 1990 with the first free democratic elections.\n\nThe founder was biologist János Vargha. The organization was awarded the Right Livelihood Award in 1985.\n"}
{"id": "26388078", "url": "https://en.wikipedia.org/wiki?curid=26388078", "title": "Electric Vehicle Grand Prix", "text": "Electric Vehicle Grand Prix\n\nThe Electric Vehicle Grand Prix (stylized as evGrand Prix) is an electric go-kart race held at Purdue University and the Indianapolis Motor Speedway.\n\nPurdue University, in conjunction with University of Notre Dame, University of Indianapolis, Ivy Tech Community College, Purdue University Calumet, and Indiana University Northwest, was awarded a $6.1 million grant by the United States Department of Energy. This grant was awarded to create The Indiana Advanced Electric Vehicle Training and Education Consortium (I-AEVtec). The goal of this consortium is to educate and train the workforce needed to design, manufacture, and maintain advanced electric vehicles and the associated infrastructure. This goal includes creating online courses related to batteries, fuel cells, motors, controls, electric vehicles, and environmental impact. As part of this grant, Purdue created an event called the Electric Vehicle Grand Prix. The grant was part of the grants announced by President Obama at a speech in Elkhart, Indiana in August 2009.\n\nThe Electric Vehicle Grand Prix is an event at Purdue University that allows students to get real experience with electric vehicles. The first Electric Vehicle Grand Prix was held on April 18, 2010. Students join a team either through a build class or a student-organized team. Each team builds a battery-powered electric go-kart and races it in the event. The 2010 race was an endurance race consisting of 80 laps and a battery change. In 2011, a second event took place at the Indianapolis Motor Speedway where teams would compete in the International evGrand Prix. For the 2013 International evGrand Prix, the event was split up into two races. The first race featured electric karts with standard motors and batteries while the second race featured karts with upgraded motors and batteries.\nIn 2015 for the first time the race was sanctioned by USAC. In 2016, the evGrand Prix became part of the Student Karting World Finals, along with the High School evGrand Prix and the new National Gas Grand Prix, all of which were sanctioned by USAC.\n\nCurrently at Purdue there are four classes being offered that relate directly to electric vehicles across multiple disciplines. They include \"Communication and Emerging Technologies\" and \"Electric Vehicle Systems\".\n\n\n\nOn October 25, 2014, five EV and five gas karts competed on the same track for the first time in Purdue history. The race was scheduled for October 18, but was delayed due to rain. Cary Racing swept the front row with a gas kart on pole, and an EV starting second. The gas karts dominated the race, as eventual winner Eli Salamie leading all 40 laps for Cary Racing. Christian Jones in the #34 PEF kart was the highest finishing EV kart in 4th.\n\n\n\n\n"}
{"id": "45634972", "url": "https://en.wikipedia.org/wiki?curid=45634972", "title": "Electricity Authority (Israel)", "text": "Electricity Authority (Israel)\n\nThe Israel Public Utility Authority for Electricity (in short: The Electricity Authority) is a government authority charged with providing utility services, setting tariffs, regulation and oversight of the electricity market in Israel. The Authority was established in 1996, concurrent with the expiration of the 70-year-old concession of the Israel Electric Corporation (IEC), and pursuant to the Electricity Market Law of 1996. Prior to the establishment of the Electricity Authority, the Electric Corporation itself operated vis-a-vis the government of Israel in various fields related to regulation of the electricity system in Israel, such as the initiation of the construction of new power plants.\nThe powers and tasks of the Authority, as well as the composition of the Authority's members and its manner of operation are defined in and based on the Electricity Market Law. The Electricity Authority serves as an independent professional regulator in matters concerning the electricity market in Israel, and is responsible for the regulation and oversight of the provision of public utilities in the field of electricity in Israel. The Authority is obligated to maintain a balance of interests between consumers, the Electric Corporation, electricity manufacturers, and the state.\nPart of the role of the Electricity Authority is the setting of the electricity tariffs in Israel, and the implementation of Israeli government decisions which have set a required level of power production by private producers and by renewable energy, through the encouragement and regulation of private power producers and producers of electricity from renewable energies, alongside the Electric Corporation.\n\n\nThe Chairperson of the Authority is appointed by the government for a term of five years, with the possibility of an additional four-year extension. The Authority is currently headed by Dr. Assaf Eilat.\n\nThis department is charged with creating the standards and tariffs for the various service providers in the electricity market. The activity of the department focuses on promoting renewable energy facilities on one hand, and on examining the effects and indirect financial costs of the emission of pollutants from power plants that use fuels to produce power, to subsidize renewable-energy-based producers through setting premiums for non-pollution in conjunction with the Ministry of Environmental Protection on the other\n\nThis department works to guarantee the financial resilience of electricity producers in Israel through consolidating models and parameters for the setting and updating of financing costs and the proper return on equity capital of electricity producers, which is expressed through the various electricity rates, based on financial models and future scenarios. The department examines the effect of extreme scenarios on the profitability and financial resilience of the vital utility provider and the private electricity producers and monitors their financing costs and risks. The department also acts to consolidate financing-supporting regulations and standards and to deal with existing barriers to financing, and accompanies financial closing processes on behalf of the Authority.\n\nThe department is charged with analyzing the financial reports of the power producers through matching the records to the rate structure set by the Authority, including the implementation of streamlining coefficients. This department initiates demand for special reports to the Authority to be used for cost oversight.\n\nThe department is charged with issuing licenses for the operations of electricity producers, stands in continuous contact with license-seekers in the fields of production, conduction and distribution, advises the Authority assembly regarding the granting of licenses and oversees the fulfillment of their terms and conditions under law.\n\nThe department's activities focus on monitoring and inspecting the financial activities of electricity providers. The department is charged with setting the structure and level of the rate and its means of update. The department performs pricing of other services offered by vital utility providers in accordance with the decisions of the Authority, and performs monitoring and oversight of the investment and development plans of the vital utility providers and their impact on the rate. In concert with other departments in the authority, the Economics Department inspects normative costs of the services that are set under various regulation and determined by the departments in the course of the ongoing work.\n\nThe department is charged with the relaiability and quality of the electric power supply, and gathers the amalgam of service-oriented aspect having to do with reliable supply and quality of electricity, including the area of connections, shortage policy, development policy at the execution level, self-initiated works, the issue of historical distributors, smart counting and smart grid. The department is also in charge of studying feasibility surveys and surveys of connecting private producers to the distribution and conduction grid.\n\nThe department is charged with setting standards in the field of consumer affairs, and conducts monitoring and oversight upon the fulfillment of the standards through reports and field monitoring.The department is in charge of implementing the directives of the Freedom of Information Law. The Public Inquiries division within the department operates vis-a-vis the consumers and various consumer groups, as well as with other intra-governmental and external agencies regarding various consumer issues. The department examines consumer complaints against electricity providers and rules on them. Its rulings on cases are binding upon the electricity providers.\n\nThe Strategy and Policy Monitoring Department is charged with representing the Authority and coordinating its operations with senior outside government figures and figures in the business sector and in academia, developing and deepening the Authority's professional cooperation with comparable regulators in Israel and abroad, developing and consolidating the public policy, positioning and brand promotion for the Electricity Authority and setting the Authority's public relations policy. In addition the department is charged with the Authority's spokesman's apparatus.\n\nThe department's activity focuses on environmental consequences to operations in the electricity market, promotion of renewable energy policies, analyzing economic costs and the ways to integrate them. In the course of its work the department is in charge of contact with various environmentally-relevant entities in the market and accompanies the professional departments on these issues.\n\n\n"}
{"id": "14703301", "url": "https://en.wikipedia.org/wiki?curid=14703301", "title": "Enron Wind", "text": "Enron Wind\n\nEnron Wind Systems, part of the Enron group, were manufacturers of wind turbines. The group was formed after Enron acquired Zond Corporation of California in January 1997, which was then the largest US developer of wind-powered electricity.\n\nOn May 10, 2002, following the Enron scandal, General Electric acquired the assets of Enron Wind Systems; it has continued as GE Wind Energy. The empty shell of Enron Wind ceased to exist.\n\n"}
{"id": "9908", "url": "https://en.wikipedia.org/wiki?curid=9908", "title": "Equation of state", "text": "Equation of state\n\nIn physics and thermodynamics, an equation of state is a thermodynamic equation relating state variables which describe the state of matter under a given set of physical conditions, such as pressure, volume, temperature (PVT), or internal energy. Equations of state are useful in describing the properties of fluids, mixtures of fluids, solids, and the interior of stars.\nThe most prominent use of an equation of state is to correlate densities of gases and liquids to temperatures and pressures. One of the simplest equations of state for this purpose is the ideal gas law, which is roughly accurate for weakly polar gases at low pressures and moderate temperatures. However, this equation becomes increasingly inaccurate at higher pressures and lower temperatures, and fails to predict condensation from a gas to a liquid. Therefore, a number of more accurate equations of state have been developed for gases and liquids. At present, there is no single equation of state that accurately predicts the properties of all substances under all conditions.\n\nAnother common use is in modeling the interior of stars, including neutron stars, dense matter (quark–gluon plasmas) and radiation fields. A related concept is the perfect fluid equation of state used in cosmology.\n\nEquations of state can also describe solids, including the transition of solids from one crystalline state to another.\n\nIn a practical context, equations of state are instrumental for PVT calculations in process engineering problems, such as petroleum gas/liquid equilibrium calculations. A successful PVT model based on a fitted equation of state can be helpful to determine the state of the flow regime, the parameters for handling the reservoir fluids, and pipe sizing.\n\nMeasurements of equation-of-state parameters, especially at high pressures, can be made using lasers.\n\nBoyle's Law was perhaps the first expression of an equation of state. In 1662, the Irish physicist and chemist Robert Boyle performed a series of experiments employing a J-shaped glass tube, which was sealed on one end. Mercury was added to the tube, trapping a fixed quantity of air in the short, sealed end of the tube. Then the volume of gas was measured as additional mercury was added to the tube. The pressure of the gas could be determined by the difference between the mercury level in the short end of the tube and that in the long, open end. Through these experiments, Boyle noted that the gas volume varied inversely with the pressure. In mathematical form, this can be stated as:\n\nThe above relationship has also been attributed to Edme Mariotte and is sometimes referred to as Mariotte's law. However, Mariotte's work was not published until 1676.\n\nIn 1787 the French physicist Jacques Charles found that oxygen, nitrogen, hydrogen, carbon dioxide, and air expand to roughly the same extent over the same 80-kelvin interval. Later, in 1802, Joseph Louis Gay-Lussac published results of similar experiments, indicating a linear relationship between volume and temperature:\n\nDalton's Law of partial pressure states that the pressure of a mixture of gases is equal to the sum of the pressures of all of the constituent gases alone.\n\nMathematically, this can be represented for \"n\" species as:\n\nIn 1834, Émile Clapeyron combined Boyle's Law and Charles' law into the first statement of the \"ideal gas law\". Initially, the law was formulated as \"pV\" = \"R\"(\"T\" + 267) (with temperature expressed in degrees Celsius), where \"R\" is the gas constant. However, later work revealed that the number should actually be closer to 273.2, and then the Celsius scale was defined with 0°C = 273.15K, giving:\n\nIn 1873, J. D. van der Waals introduced the first equation of state derived by the assumption of a finite volume occupied by the constituent molecules. His new formula revolutionized the study of equations of state, and was most famously continued via the Redlich–Kwong equation of state and the Soave modification of Redlich-Kwong.\n\nFor a given amount of substance contained in a system, the temperature, volume, and pressure are not independent quantities; they are connected by a relationship of the general form\n\nAn equation used to model this relationship is called an equation of state. In the following sections major equations of state are described, and the variables used here are defined as follows. Any consistent set of units may be used, although SI units are preferred. Absolute temperature refers to use of the Kelvin (K) or Rankine (°R) temperature scales, with zero being absolute zero.\n\nThe classical ideal gas law may be written\n\nIn the form shown above, the equation of state is thus\n\nIf the calorically perfect gas approximation is used, then the ideal gas law may also be expressed as follows\n\nwhere formula_19 is the density, formula_20 is the adiabatic index (ratio of specific heats), formula_21 is the internal energy per unit mass (the \"specific internal energy\"), formula_22 is the specific heat at constant volume, and formula_23 is the specific heat at constant pressure.\n\nCubic equations of state are called such because they can be rewritten as a cubic function of V.\n\nThe Van der Waals equation of state may be written:\n\nwhere formula_25 is molar volume. The substance-specific constants formula_26 and formula_27 can be calculated from the critical properties formula_28 and formula_29 (noting that formula_29 is the molar volume at the critical point) as:\n\nAlso written as\n\nProposed in 1873, the van der Waals equation of state was one of the first to perform markedly better than the ideal gas law. In this landmark equation formula_26 is called the attraction parameter and formula_27 the repulsion parameter or the effective molecular volume. While the equation is definitely superior to the ideal gas law and does predict the formation of a liquid phase, the agreement with experimental data is limited for conditions where the liquid forms. While the van der Waals equation is commonly referenced in text-books and papers for historical reasons, it is now obsolete. Other modern equations of only slightly greater complexity are much more accurate.\n\nThe van der Waals equation may be considered as the ideal gas law, \"improved\" due to two independent reasons:\n\nWith the reduced state variables, i.e. V=V/V, P=P/P and T=T/T, the reduced form of the Van der Waals equation can be formulated:\n\nThe benefit of this form is that for given T and P, the reduced volume of the liquid and gas can be calculated directly using Cardano's method for the reduced cubic form:\n\nFor P<1 and T<1, the system is in a state of vapor–liquid equilibrium. The reduced cubic equation of state yields in that case 3 solutions. The largest and the lowest solution are the gas and liquid reduced volume.\n\nIntroduced in 1949, the Redlich-Kwong equation of state was a considerable improvement over other equations of the time. It is still of interest primarily due to its relatively simple form. While superior to the van der Waals equation of state, it performs poorly with respect to the liquid phase and thus cannot be used for accurately calculating vapor–liquid equilibria. However, it can be used in conjunction with separate liquid-phase correlations for this purpose.\n\nThe Redlich-Kwong equation is adequate for calculation of gas phase properties when the ratio of the pressure to the critical pressure (reduced pressure) is less than about one-half of the ratio of the temperature to the critical temperature (reduced temperature):\n\nWhere ω is the acentric factor for the species.\n\nThis formulation for formula_54 is due to Graboski and Daubert. The original formulation from Soave is:\n\nfor hydrogen:\n\nWe can also write it in the polynomial form, with:\n\nthen we have:\nwhere formula_60 is the universal gas constant and Z=PV/(RT) is the compressibility factor.\n\nIn 1972 G. Soave replaced the 1/ term of the Redlich-Kwong equation with a function α(T,ω) involving the temperature and the acentric factor (the resulting equation is also known as the Soave-Redlich-Kwong equation of state; SRK EOS). The α function was devised to fit the vapor pressure data of hydrocarbons and the equation does fairly well for these materials.\n\nNote especially that this replacement changes the definition of \"a\" slightly, as the formula_61 is now to the second power.\n\nThe SRK EOS may be written as\n\nwhere\n\nwhere formula_54 and other parts of the SRK EOS is defined in the SRK EOS section.\n\nA downside of the SRK EOS, and other cubic EOS, is that the liquid molar volume is significantly less accurate than the gas molar volume. Peneloux et alios (1982) proposed a simple correction for this by introducing a volume translation\n\nwhere formula_66 is an additional fluid component parameter that translate the molar volume slightly. On the liquid branch of the EOS, a small change in molar volume corresponds to a large change in pressure. On the gas branch of the EOS, a small change in molar volume corresponds to a much smaller change in pressure than for the liquid branch. Thus, the perturbation of the molar gas volume is small. Unfortunately, there are two versions that occur in science and industry.\n\nIn the first version only formula_67 is translated,\n\nIn the second version both formula_67 and formula_70 are translated, or the translation of formula_67 is followed by a renaming of the composite parameter . This gives\n\nThe c-parameter of a fluid mixture is calculated by\n\nThe c-parameter of the individual fluid components in a petroleum gas and oil can be estimated by the correlation\n\nwhere the Rackett compressibility factor formula_75 can be estimated by\n\nA nice feature with the volume translation method of Peneloux et alios (1982) is that it does not affect the vapor-liquid equilibrium calculations. It should be noted that this method of volume translation can also be applied to other cubic EOSs if the c-parameter correlation is adjusted to match the selected EOS.\n\nIn polynomial form:\n\nwhere formula_81 is the acentric factor of the species, formula_60 is the universal gas constant and formula_83 is compressibility factor.\n\nThe Peng–Robinson equation of state (PR EOS) was developed in 1976 at The University of Alberta by Ding-Yu Peng and Donald Robinson in order to satisfy the following goals:\n\nFor the most part the Peng–Robinson equation exhibits performance similar to the Soave equation, although it is generally superior in predicting the liquid densities of many materials, especially nonpolar ones. The departure functions of the Peng–Robinson equation are given on a separate article.\n\nThe analytic values of its characteristic constants are:\n\nA modification to the attraction term in the Peng–Robinson equation of state published by Stryjek and Vera in 1986 (PRSV) significantly improved the model's accuracy by introducing an adjustable pure component parameter and by modifying the polynomial fit of the acentric factor.\n\nThe modification is:\n\nwhere formula_88 is an adjustable pure component parameter. Stryjek and Vera published pure component parameters for many compounds of industrial interest in their original journal article. At reduced temperatures above 0.7, they recommend to set formula_89 and simply use formula_90. For alcohols and water the value of formula_91 may be used up to the critical temperature and set to zero at higher temperatures.\n\nA subsequent modification published in 1986 (PRSV2) further improved the model's accuracy by introducing two additional pure component parameters to the previous attraction term modification.\n\nThe modification is:\n\nwhere formula_88, formula_94, and formula_95 are adjustable pure component parameters.\n\nPRSV2 is particularly advantageous for VLE calculations. While PRSV1 does offer an advantage over the Peng–Robinson model for describing thermodynamic behavior, it is still not accurate enough, in general, for phase equilibrium calculations. The highly non-linear behavior of phase-equilibrium calculation methods tends to amplify what would otherwise be acceptably small errors. It is therefore recommended that PRSV2 be used for equilibrium calculations when applying these models to a design. However, once the equilibrium state has been determined, the phase specific thermodynamic values at equilibrium may be determined by one of several simpler models with a reasonable degree of accuracy.\n\nOne thing to note is that in the PRSV equation, the parameter fit is done in a particular temperature range which is usually below the critical temperature. Above the critical temperature, the PRSV alpha function tends to diverge and become arbitrarily large instead of tending towards 0. Because of this, alternate equations for alpha should be employed above the critical point. This is especially important for systems containing hydrogen which is often found at temperatures far above its critical point. Several alternate formulations have been proposed. Some well known ones are by Twu et all or by Mathias and Copeman.\n\nThe Elliott, Suresh, and Donohue (ESD) equation of state was proposed in 1990. The equation seeks to correct a shortcoming in the Peng–Robinson EOS in that there was an inaccuracy in the van der Waals repulsive term. The EOS accounts for the effect of the shape of a non-polar molecule and can be extended to polymers with the addition of an extra term (not shown). The EOS itself was developed through modeling computer simulations and should capture the essential physics of the size, shape, and hydrogen bonding.\n\nwhere:\n\nand\n\nwhere\nThe characteristic size parameter is related to the shape parameter formula_66 through\nwhere\n\nNoting the relationships between Boltzmann's constant and the Universal gas constant, and observing that the number of molecules can be expressed in terms of Avogadro's number and the molar mass, the reduced number density formula_103 can be expressed in terms of the molar volume as\n\nThe shape parameter formula_114 appearing in the Attraction term and the term formula_115 are given by\n\nwhere formula_118 is the depth of the square-well potential and is given by\n\nThe model can be extended to associating components and mixtures of nonassociating components. Details are in the paper by J.R. Elliott, Jr. \"et al.\" (1990).\n\nThe Cubic-Plus-Association (CPA) equation of state combines the Soave-Redlich-Kwong equation with an association term from Wertheim theory. The development of the equation began in 1995 as a research project that was funded by Shell, and in 1996 an article was published which presented the CPA equation of state.\n\nwhere \"a\" is associated with the interaction between molecules and \"b\" takes into account the finite size of the molecules, similar to the Van der Waals equation.\n\nThe reduced coordinates are:\n\nAlthough usually not the most convenient equation of state, the virial equation is important because it can be derived directly from statistical mechanics. This equation is also called the Kamerlingh Onnes equation. If appropriate assumptions are made about the mathematical form of intermolecular forces, theoretical expressions can be developed for each of the coefficients. \"A\" is the first virial coefficient, which has a constant value of 1 and makes the statement that when volume is large, all fluids behave like ideal gases. The second virial coefficient \"B\" corresponds to interactions between pairs of molecules, \"C\" to triplets, and so on. Accuracy can be increased indefinitely by considering higher order terms. The coefficients \"B\", \"C\", \"D\", etc. are functions of temperature only.\n\nOne of the most accurate equations of state is that from Benedict-Webb-Rubin-Starling shown next. It was very close to a virial equation of state. If the exponential term in it is expanded to two Taylor terms, a virial equation can be derived:\n\nNote that in this virial equation, the fourth and fifth virial terms are zero. The second virial coefficient is monotonically decreasing as temperature is lowered. The third virial coefficient is monotonically increasing as temperature is lowered.\n\nwhere\n\nValues of the various parameters for 15 substances can be found in \n\nStatistical associating fluid theory (SAFT) equations of state use statistical mechanical methods (in particular perturbation theory) to describe the interactions between molecules in a system. The idea of a SAFT equation of state was first proposed by Chapman et al. in 1989, but since then many different SAFT equations of state have been proposed. Often SAFT equations represent molecules as collections of spherical particles that interact with one another. One popular SAFT equation represents molecules as chains composed of spherical segments (PC-SAFT). In general, SAFT equations give more accurate results than traditional cubic equations of state, especially for systems containing liquids or solids.\n\nMultiparameter equations of state (MEOS) can be used to represent pure fluids with high accuracy, in both the liquid and gaseous states. MEOS's represent the Helmholtz function of the fluid as the sum of ideal gas and residual terms. Both terms are explicit in reduced temperature and reduced density - thus:\n\nwhere:\n\nThe reduced density and temperature are typically, though not always, the critical values for the pure fluid.\n\nOther thermodynamic functions can be derived from the MEOS by using appropriate derivatives of the Helmholtz function; hence, because integration of the MEOS is not required, there are few restrictions as to the functional form of the ideal or residual terms. Typical MEOS use upwards of 50 fluid specific parameters, but are able to represent the fluid's properties with high accuracy. MEOS are available currently for about 50 of the most common industrial fluids including refrigerants. Mixture models also exist.\n\nWhen considering water under very high pressures, in situations such as underwater nuclear explosions, sonic shock lithotripsy, and sonoluminescence, the stiffened equation of state is often used:\n\nwhere formula_136 is the internal energy per unit mass, formula_137 is an empirically determined constant typically taken to be about 6.1, and formula_138 is another constant, representing the molecular attraction between water molecules. The magnitude of the correction is about 2 gigapascals (20,000 atmospheres).\n\nThe equation is stated in this form because the speed of sound in water is given by formula_139.\n\nThus water behaves as though it is an ideal gas that is \"already\" under about 20,000 atmospheres (2 GPa) pressure, and explains why water is commonly assumed to be incompressible: when the external pressure changes from 1 atmosphere to 2 atmospheres (100 kPa to 200 kPa), the water behaves as an ideal gas would when changing from 20,001 to 20,002 atmospheres (2000.1 MPa to 2000.2 MPa).\n\nThis equation mispredicts the specific heat capacity of water but few simple alternatives are available for severely nonisentropic processes such as strong shocks.\n\nAn ultrarelativistic fluid has equation of state\n\nwhere formula_41 is the pressure, formula_142 is the mass density, and formula_143 is the speed of sound.\n\nThe equation of state for an ideal Bose gas is\n\nwhere α is an exponent specific to the system (e.g. in the absence of a potential field, α = 3/2), \"z\" is exp(\"μ\"/\"kT\") where \"μ\" is the chemical potential, Li is the polylogarithm, ζ is the Riemann zeta function, and \"T\" is the critical temperature at which a Bose–Einstein condensate begins to form.\n\nThe equation of state from Jones–Wilkins–Lee is used to describe the detonation products of explosives.\n\nThe ratio formula_146 is defined by using formula_147 = density of the explosive (solid part) and formula_148 = density of the detonation products. The parameters formula_149, formula_150, formula_151, formula_152 and formula_153 are given by several references. In addition, the initial density (solid part) formula_154, speed of detonation formula_155, Chapman–Jouguet pressure formula_156 and the chemical energy of the explosive formula_157 are given in such references. These parameters are obtained by fitting the JWL-EOS to experimental results. Typical parameters for some explosives are listed in the table below.\nCommon abbreviations: formula_158\n\n\n\n"}
{"id": "3190268", "url": "https://en.wikipedia.org/wiki?curid=3190268", "title": "Estovers", "text": "Estovers\n\nIn English law, an estover is an allowance made to a person out of an estate, or other thing, for his or her support. Estovers are wood, that a tenant is allowed to take, for life or a period of years, from the land he holds for the repair of his house, the implements of husbandry, hedges and fences, and for firewood.\n\nThe word derives from the French \"estover\", \"estovoir\", a verb used as a substantive meaning \"that which is necessary\". This word is of disputed origin; it has been referred to the Latin \"stare\", to stand, or \"studere\", to desire.\n\nThe Old English word for estover was \"bote\" or \"boot\", also spelled \"bot\" or \"bót\", (literally meaning 'good' or 'profit' and cognate with the word \"better\"). The various kinds of estovers were known as house-bote, cart or plough-bote, hedge or hay-bote, and fire-bote. Anglo-Saxon law also imposed \"bot\" fines in the modern sense of compensation. These rights might be restricted by express covenants. Copyholders had similar rights over the land they occupied and over the waste of the manor, in which case the rights are known as Commons of estovers.\nBurrill in his dated \"A law dictionary and glossary\" published in New York (1871) states:\n\n"}
{"id": "14944327", "url": "https://en.wikipedia.org/wiki?curid=14944327", "title": "F. A. Davis", "text": "F. A. Davis\n\nFrank Allston Davis (September 8, 1850 – January 2, 1917) was a publishing executive who founded the F. A. Davis Company in Philadelphia, Pennsylvania. After moving to the Tampa Bay Area, he introduced electricity to St. Petersburg, Florida and founded the city of Pinellas Park.\n\nDavis was born in Duxbury, Vermont and, as a child, worked as a groundskeeper and attended school. His mother died in 1861, after which he lived with Samuel Cook Turner. He worked as a lawnmower salesman in Asbury Park, New Jersey during the summer of 1870. By 1872, he earned $30 per month as a teacher.\n\nDavis relocated to Philadelphia and worked as an agent for several publishing firms. He married Lizzie Fritz and their son, Alonzo, was born in 1873. In 1879, he was working as an agent for the largest medical publisher in the world, William Wood and Company when he formed his own publishing company, F. A. Davis Company. Similar to William Wood, Davis's company specialized in publishing medical material including medical studies. Davis's first wife died in the 1880s.\n\nOn April 29, 1885, Davis attended an American Medical Association meeting which included a study about the benefits of the Pinellas peninsula. Davis eventually published the study and, in 1889, he travelled to Tarpon Springs, Florida. He felt improvement in his muscular rheumatism and advertised for Florida in one of his medical journals. In Tarpon Springs he met Jacob Disston, the brother of Florida land baron Hamilton Disston. Davis and Disston combined to bring electricity to Tarpon Springs in 1895. In the same year, Davis married Elizabeth Irene Craven.\n\nElectricity had little impact on Tarpon Springs so Davis received an electricity franchise from St. Petersburg on February 2, 1897 and moved his plant there. St. Petersburg's first wood-powered electrical service was initiated on August 5, 1897. On February 4, 1902, Davis was granted a trolley franchise by St. Petersburg. He paid for trolley construction with funding from Jacob Disston and others, and by purchasing and re-selling the city's phone system. Trolley construction began on May 30, 1904 and operation began four months later. In 1905, he expanded the trolley system to Disston City. Through his publishing company, Davis promoted the area with \"Florida\" magazine in 1905 and two books in 1906. Continuing to expand his influence in St. Petersburg, Davis tore down the Brantley Pier and built the Electric Pier in 1906. He paid $80,000 for a 500-passenger steamboat named \"Favorite\" which first arrived on October 17, 1906 and was a popular attraction at the Electric Pier.\n\nDavis's progress - and St. Petersburg's in general - was hampered when Tampa tycoon, Henry B. Plant, purchased the city's lifeline, the Orange Belt Railroad, in 1906. Davis was dealt another blow by the banking panic of 1907. Two years later, he shifted control of much of his St. Petersburg holdings to his occasional rival, H. Walter Fuller. After a conflict over St. Petersburg waterfront area, Davis purchased of Hamilton Disston's land around 1911 and established the city of Pinellas Park. Davis, his son, and P. J. McDevitt advertised the new city drawing people from Pennsylvania and Ohio. McDevitt became the city's first mayor. Davis sold a free lot in Pinellas Park for every ten-acre farm purchased, resulting in 111 farm sales from 1910 to 1912. Lack of drainage in Pinellas Park caused problems in August 1915 when of rain fell.\n\nDavis died in Philadelphia from heart failure, reportedly due to World War I and concern over his estate. Jacob Disston closed out Davis's remaining holdings in Florida while his wife, Elizabeth, remained involved with F. A. Davis Company until her death in 1964.\n"}
{"id": "31380083", "url": "https://en.wikipedia.org/wiki?curid=31380083", "title": "Flood pulse concept", "text": "Flood pulse concept\n\nThe flood pulse concept explains how the periodic inundation and drought (flood pulse) control the lateral exchange of water, nutrients ad organisms between the main river channel (or lake) and the connected floodplain (Junk et al., 1989). The annual flood pulse is the most important aspect and the most biologically productive feature of a river's ecosystem., describing the movement, distribution and quality of water in river ecosystems and the dynamic interaction in the transition zone between water and land (aquatic/terrestrial transition zones - ATTZ). It contrasts with previous ecological theories which considered floods to be catastrophic events.\nRiver floodplain systems consist of an area surrounding a river that is periodically flooded by the overflow of the river as well as by precipitation, called the aquatic/terrestrial transition zone (ATTZ). The ATTZ is the area covered by water only during the flooding. This flooding in turn creates unique habitat that is essential to the survival of many different species. The flood pulse concept is unique because it incorporates the outlying rivers and streams which add a lateral aspect to previous concepts, e.g. the River Continuum Concept (RCC) that failed in explain processes that happen in big rives and their floodplains. From this lateral perspective, rivers can be seen as a collection of width-based water systems.\n\nFlooding consists of multiple stages. First, at the start of the flooding, nutrients rush in from the area where the flood begins. During flood periods, the most important element is called the moving littoral. As flooding begins and water levels increase nutrients that have been mineralized in the dry phase are suspended with sediments in the flood waters and main river. The moving littoral consists of the water from the shoreline to a few meters deep in the river. This pulse of water is the primary driver of high productivity and decomposition rates as it moves nutrients in and out of the system and is good breeding ground for many species of estuarial organisms. At this point in time production rates exceed decomposition rates. As water levels stabilize, decomposition rates outpace production rates, frequently contributing to dissolved oxygen deficiency. When the water starts receding, the moving littoral reverses, concentrating nutrients and contributing to phytoplankton growth.\n\nThe flood pulse helps maintain genetic and species diversity in the floodplain ecosystem, and it brings in oxygen to help fauna and decomposition. The flood pulse also increases yields by increasing the surface area of water and showers the land with river biota. Flood plain systems also serve as migration routes, hibernation spots, and spawning locations for many species. For the red-bellied piranha, their two annual reproductive seasons are dependent on the flooding pulse. However, the flood pulse has the potential to overpower some species; when flood pulses occur at unusual times or last for too long, terrestrial vegetation can be overwhelmed. Furthermore, the receding of the flood at the end of the flood pulse can lead to oxygen deficiency.\n\nRiver floodplain systems can be both natural and man-made; the latter occur when dams and levees create a flood plain. Humans have had several effects on the flood pulse. Through ecosystem alterations such as dams, debris removal, channelization, levees, navigation, irrigation, contamination, logging, fire suppression, species introduction, and agricultural runoff, humans have contributed to the destruction of wetlands and the extinction of species. Biota relies on the flood plain for food supply, spawning and shelter, and flood pulses that are too quick or slow interrupt this. This can have devastating effects on riparian ecosystems.\n\nThe flood pulse concept is one of three primary models describing large river ecosystems. The others include the river continuum concept (RCC) and the serial discontinuity concept. Related theories include the nutrient spiraling concept. Many theorists have criticized the flood pulse concept and believe that other concepts could help explain the phenomena that occur in large rivers. Some say that the flood pulse concept is inadequate because it only applies to temperate and tropical systems. The flood pulse concept involves many assumptions; many theorists object to the concept on the basis of these assumptions. The flood pulse concept assumes that all systems are either hierarchical or linear, that physical features control biological structures, and that there is dynamic equilibrium between the biological and the physical rhythms. Because of their criticisms of the flood pulse concept, some theorists prefer the river continuum concept. However, Junk et al. argue that the river continuum concept is not sufficient because it is based on research done on small temperate streams and has mistakenly been applied to all water systems; furthermore, the river continuum concept does not explain habitats that fluctuate between lotic and lentic states, whereas the flood pulse concept adequately covers these systems.\n"}
{"id": "10851298", "url": "https://en.wikipedia.org/wiki?curid=10851298", "title": "Global Commons Institute", "text": "Global Commons Institute\n\nThe Global Commons Institute was founded in the United Kingdom in 1990 by Aubrey Meyer and others to campaign for a fair way to tackle climate change. \n\nIt has in particular promoted the model of Contraction and Convergence of emissions as a means to tackle climate change. Many of the founders and signatories to the first statement in favour of contraction and convergence were members of the Green Party.\n\n\n"}
{"id": "23364086", "url": "https://en.wikipedia.org/wiki?curid=23364086", "title": "Hertzsprung–Russell diagram", "text": "Hertzsprung–Russell diagram\n\nThe Hertzsprung–Russell diagram, abbreviated as H–R diagram, HR diagram or HRD, is a scatter plot of stars showing the relationship between the stars' absolute magnitudes or luminosities versus their stellar classifications or effective temperatures. More simply, it plots each star on a graph plotting the star's brightness against its temperature (color).\n\nThe diagram was created circa 1910 by Ejnar Hertzsprung and Henry Norris Russell and represents a major step towards an understanding of stellar evolution.\n\nThe related colour–magnitude diagram (CMD) plots the apparent magnitudes of stars against their colour, usually for a cluster so that the stars are all at the same distance.\n\nIn the nineteenth-century large-scale photographic spectroscopic surveys of stars were performed at Harvard College Observatory, producing spectral classifications for tens of thousands of stars, culminating ultimately in the Henry Draper Catalogue. In one segment of this work Antonia Maury included divisions of the stars by the width of their spectral lines. Hertzsprung noted that stars described with narrow lines tended to have smaller proper motions than the others of the same spectral classification. He took this as an indication of greater luminosity for the narrow-line stars, and computed secular parallaxes for several groups of these, allowing him to estimate their absolute magnitude.\n\nIn 1910 Hans Rosenberg published a diagram plotting the apparent magnitude of stars in the Pleiades cluster against the strengths of the calcium K line and two hydrogen Balmer lines. These spectral lines serve as a proxy for the temperature of the star, an early form of spectral classification. The apparent magnitude of stars in the same cluster is equivalent to their absolute magnitude and so this early diagram was effectively a plot of luminosity against temperature. The same type of diagram is still used today as a means of showing the stars in clusters without having to initially know their distance and luminosity. Hertzsprung had already been working with this type of diagram, but his first publications showing it were not until 1911. This was also the form of the diagram using apparent magnitudes of a cluster of stars all at the same distance.\n\nRussell's early (1913) versions of the diagram included Maury's giant stars identified by Hertzsprung, those nearby stars with parallaxes measured at the time, stars from the Hyades (a nearby open cluster), and several moving groups, for which the moving cluster method could be used to derive distances and thereby obtain absolute magnitudes for those stars.\n\nThere are several forms of the Hertzsprung–Russell diagram, and the nomenclature is not very well defined. All forms share the same general layout: stars of greater luminosity are toward the top of the diagram, and stars with higher surface temperature are toward the left side of the diagram.\n\nThe original diagram displayed the spectral type of stars on the horizontal axis and the absolute visual magnitude on the vertical axis. The spectral type is not a numerical quantity, but the sequence of spectral types is a monotonic series that reflects the stellar surface temperature. Modern observational versions of the chart replace spectral type by a color index (in diagrams made in the middle of the 20th Century, most often the B-V color) of the stars. This type of diagram is what is often called an observational Hertzsprung–Russell diagram, or specifically a color–magnitude diagram (CMD), and it is often used by observers. In cases where the stars are known to be at identical distances such as within a star cluster, a color–magnitude diagram is often used to describe the stars of the cluster with a plot in which the vertical axis is the apparent magnitude of the stars. For cluster members, by assumption there is a single additive constant difference between their apparent and absolute magnitudes, called the distance modulus, for all of that cluster of stars. Early studies of nearby open clusters (like the Hyades and Pleiades) by Hertzsprung and Rosenberg produced the first CMDs, antedating by a few years Russell's influential synthesis of the diagram collecting data for all stars for which absolute magnitudes could be determined.\n\nAnother form of the diagram plots the effective surface temperature of the star on one axis and the luminosity of the star on the other, almost invariably in a log-log plot. Theoretical calculations of stellar structure and the evolution of stars produce plots that match those from observations. This type of diagram could be called \"temperature-luminosity diagram\", but this term is hardly ever used; when the distinction is made, this form is called the \"theoretical Hertzsprung–Russell diagram\" instead. A peculiar characteristic of this form of the H–R diagram is that the temperatures are plotted from high temperature to low temperature, which aids in comparing this form of the H–R diagram with the observational form.\n\nAlthough the two types of diagrams are similar, astronomers make a sharp distinction between the two. The reason for this distinction is that the exact transformation from one to the other is not trivial. To go between effective temperature and color requires a color–temperature relation, and constructing that is difficult; it is known to be a function of stellar composition and can be affected by other factors like stellar rotation. When converting luminosity or absolute bolometric magnitude to apparent or absolute visual magnitude, one requires a bolometric correction, which may or may not come from the same source as the color–temperature relation. One also needs to know the distance to the observed objects (\"i.e.\", the distance modulus) and the effects of interstellar obscuration, both in the color (reddening) and in the apparent magnitude (where the effect is called \"extinction\"). Color distortion (including reddening) and extinction (obscuration) are also apparent in stars having significant circumstellar dust. The ideal of direct comparison of theoretical predictions of stellar evolution to observations thus has additional uncertainties incurred in the conversions between theoretical quantities and observations.\n\nMost of the stars occupy the region in the diagram along the line called the main sequence. During the stage of their lives in which stars are found on the main sequence line, they are fusing hydrogen in their cores. The next concentration of stars is on the horizontal branch (helium fusion in the core and hydrogen burning in a shell surrounding the core). Another prominent feature is the Hertzsprung gap located in the region between A5 and G0 spectral type and between +1 and −3 absolute magnitudes (\"i.e.\" between the top of the main sequence and the giants in the horizontal branch). RR Lyrae variable stars can be found in the left of this gap on a section of the diagram called the instability strip. Cepheid variables also fall on the instability strip, at higher luminosities.\n\nThe H-R diagram can be used by scientists to roughly measure how far away a star cluster or galaxy is from Earth. This can be done by comparing the apparent magnitudes of the stars in the cluster to the absolute magnitudes of stars with known distances (or of model stars). The observed group is then shifted in the vertical direction, until the two main sequences overlap. The difference in magnitude that was bridged in order to match the two groups is called the distance modulus and is a direct measure for the distance (ignoring extinction). This technique is known as main sequence fitting and is a type of spectroscopic parallax. Not only the turn-off in the main sequence can be used, but also the tip of the red giant branch stars.\n\nContemplation of the diagram led astronomers to speculate that it might demonstrate stellar evolution, the main suggestion being that stars collapsed from red giants to dwarf stars, then moving down along the line of the main sequence in the course of their lifetimes. Stars were thought therefore to radiate energy by converting gravitational energy into radiation through the Kelvin–Helmholtz mechanism. This mechanism resulted in an age for the Sun of only tens of millions of years, creating a conflict over the age of the Solar System between astronomers, and biologists and geologists who had evidence that the Earth was far older than that. This conflict was only resolved in the 1930s when nuclear fusion was identified as the source of stellar energy.\n\nFollowing Russell's presentation of the diagram to a meeting of the Royal Astronomical Society in 1912, Arthur Eddington was inspired to use it as a basis for developing ideas on stellar physics. In 1926, in his book \"The Internal Constitution of the Stars\" he explained the physics of how stars fit on the diagram. The paper anticipated the later discovery of nuclear fusion and correctly proposed that the star's source of power was the combination of hydrogen into helium, liberating enormous energy. This was a particularly remarkable jump of insight, since at that time the source of a star's energy was still unsolved, thermonuclear energy had not been proven to exist, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. Eddington managed to sidestep this problem by concentrating on the thermodynamics of radiative transport of energy in stellar interiors. Eddington predicted that dwarf stars remain in an essentially static position on the main sequence for most of their lives. In the 1930s and 1940s, with an understanding of hydrogen fusion, came an evidence-backed theory of evolution to red giants following which were speculated cases of explosion and implosion of the remnants to white dwarfs. The term supernova nucleosynthesis is used to describe the creation of elements during the evolution and explosion of a pre-supernova star, a concept put forth by Fred Hoyle in 1954. The pure mathematical quantum mechanics and classical mechanical models of stellar processes enable the Hertzsprung–Russell diagram to be annotated with known conventional paths known as stellar sequences — there continue to be added rarer and more anomalous examples as more stars are analysed and mathematical models considered.\n\n\n\n"}
{"id": "2377120", "url": "https://en.wikipedia.org/wiki?curid=2377120", "title": "Hexamine fuel tablet", "text": "Hexamine fuel tablet\n\nA hexamine fuel tablet is a form of solid fuel in tablet form. The tablets burn smokelessly, have a high energy density, do not liquefy while burning and leave no ashes. Invented in Murrhardt, Germany, in 1936, the main component is hexamine, which was discovered by Aleksandr Butlerov in 1859. Some fuel tablets use 1,3,5-trioxane as another ingredient.\n\nA number of alternative names are in use, including heat tablet and Esbit. Esbit is a genericized trademark as it is used to refer to similar products made by other companies. In most countries from the former Soviet bloc this is known as \"dry alcohol\".\n\nThe tablets are used for cooking by campers, the military and relief organizations. They are often used with disposable metal stoves that are included with field ration packs. Backpackers concerned with ultra light gear tend to buy or make their own much lighter stove. An Esbit beverage-can stove can be made by cutting off the bottom of an aluminium soft drink can, and turning it upside down to support the fuel tablet. A pot can be supported above this with a circle of chicken wire or metal tent pegs. The burning tablets are sensitive to wind, so a simple windscreen should be used, such as a strip of aluminium foil curved in a circle around the pot and stove. If necessary, the fuel tablet can be placed on a rock or on the dirt, with a pot supported above it by rocks, but this is less than ideal.\n\nAnother common use is to provide a relatively safe (see disadvantages below) heat source for model steam engines, such as those manufactured by Wilesco and Mamod, and other external combustion engines such as Stirling engines and pop pop boats.\n\nHexamine is prepared by the reaction of formaldehyde and ammonia. In an acid environment hexamine is converted to toxic formaldehyde, which the main hazard for toxicity is by ingestion.\n\nFuel tablets are simple, ultra-lightweight compared to other stove options, and compact; the entire stove system and fuel can be stored inside a 0.5 litre cooking pot. As with trioxane, hexamine has an almost infinite shelf life if stored properly, in a sealed dry container. However, the heat given off cannot be easily adjusted, so water can be boiled, but cooking requiring simmering is more difficult. Tablets are a powerful stove fuel (30.0 MJ/kg), and are sensitive to wind and dampness. Fuel tablets are expensive and are not as widely available compared to alternatives such as alcohol or petrol, but can use very cheap stoves. \n\nEsbit's Material Safety Data Sheet states combustion can create formaldehyde, ammonia, nitrogen oxide, hydrogen cyanide and ingestion may cause nausea, vomiting, gastrointestinal disturbances, and kidney damage. When burned, the chemical oxidation of the fuel yields noxious fumes, requiring foods being cooked to be contained in a receptacle such as a pot or pan with a tight fitting lid. Burned tablets leave a sticky dark residue on the bottom of pots. If tablets are stored or used under damp conditions then they can break up while burning and shed burning fragments, although this claim is hard to verify or reproduce.\n\nHexamine is a precursor for the simplest synthesis of the chemical explosive RDX; in many areas its availability is tightly regulated due to this.\n\n\n"}
{"id": "13404590", "url": "https://en.wikipedia.org/wiki?curid=13404590", "title": "Homefueler", "text": "Homefueler\n\nHomefueler is a home hydrogen station. It uses single phase AC power and water for the pressurized alkaline electrolyzer to generate hydrogen, a diaphragm compressor handles a filling pressure of 5,000 psig (350 bar). Storage is 13 kg, daily production is 2 kg H. The hydrogen dispensing system is aimed at providing enough energy for 1 - 2 cars.\n\nThe system is tested by NFCRC University of California, Irvine and is based on the HyStat-A Energy Station.\n\n\n"}
{"id": "53842112", "url": "https://en.wikipedia.org/wiki?curid=53842112", "title": "Hydrogen-bridged cations", "text": "Hydrogen-bridged cations\n\nHydrogen-bridged cations are a type of charged species in which a hydrogen atom is simultaneously bonded to two atoms through partial sigma bonds. While best observable in the presence of superacids at room temperature, spectroscopic evidence has suggested that hydrogen-bridged cations exist in ordinary solvents. These ions have been the subject of debate as they constitute a type of charged species of uncertain electronic structure.\n\nTwo models provide an explanation of their structure, the classical and the non-classical view. The classical view (Figure 1.a) involves a fast-equilibrating system in which a hydrogen atom rapidly shifts between two adjacent carbon atoms. In this model, fast equilibrium results from a low energy barrier between the two conformations of the molecule, and each conformer has a localized positive charge. The potential energy diagram of this model is characterized by a double-well with two energy minima.\n\nThe non-classical view (Figure 1.b) involves the delocalization of two electrons over three atoms (1 hydrogen and 2 carbon atoms). The model is characterized by a 3-membered ring where a hydrogen atom is located between two other atoms with partial sigma bonds. The potential energy diagram of this model is characterized by a single energy minimum, where the structure of the cation corresponds to the transition state between the two conformers of the classical view.\n\nFigure 1 - Potential Energy Surface (PES) of (a) classical and (b) non-classical view of a hydrogen-bridged cation.\n\nFor several decades after they were first proposed around 1950, the existence and importance of non-classical ions in organic chemistry was bitterly controversial. As hydrogen bridges discussed here are 3 center (3 atom) - 2 electron bonds, the investigations over the possibilities of such systems laid an important framework from which to understand this bonding. Many of these studies centered around the 2-norbornyl cation. Observations made by Saul Winstein and others suggested that highly delocalized and symmetric intermediates were present in the reactions of various substituted norbornyl cations, evidence for non-classical ions. H. C. Brown, the most outspoken opponent of non-classical ions, believed that such non-classical bonding was invoked far too widely and saw no reason to deviate from the classical idea of rapidly equilibrating, discrete carbocations. In 1973, G. Olah was able to directly observe the 2-norbornyl cation by low-temperature NMR and confirm the presence of a non-classical 2-norbornyl cation, allowing the field to reach some conclusions about the possibilities of 3 center - 2 electron bonds involving carbon. This verification of delocalized sigma bonding in the 2-norbornyl system was a critical foundation for understanding hydrogen bridges between carbon atoms, another form of delocalized sigma bonding.\n\nScheme 1 - The classical and non-classical models for bonding in the 2-norbornyl cation are shown above. The non-classical model (a) shows one three-center two-electron bond with a delocalized positive charge. The classical model, (b) describes a rapid equilibration between three distinct carbocations rather than delocalization.\n\nTwo different types of C-H-C bonding are recognized. The first is an \"open\" type, entailing linear geometry and negligible bonding between the terminal carbon atoms, while the second is the \"closed\" type, with triangular geometry allowing bonding interaction between terminal carbons. The relationship between these two types of 3-center 2-electron bonding can be shown through a molecular orbital diagram. Because of the additional overlap between the orbitals, the bonding orbital for the \"closed\" type is pushed lower in energy relative to the \"open\" type. The presence of two electrons in this system implies that the closed geometry will be energetically favorable, which has been seen in studies of metal-H-metal systems.\n\nFigure 2 - A molecular orbital diagram for open and closed hydrogen bridged cations with carbon is shown above. The open and closed structures show different orbital overlap which leads to different bonding energy.\n\nIn closed 3 center 2 electron bonds, the atoms are arranged in a triangular shape to increase orbital overlap as shown above. Because there are only two electrons in the system, this overlap causes a net reduction in energy relative to the open, linear bond. While the closed C-H-C bond has not been isolated or studied, it is well established that hydrogen-bridged \"metals\" prefer the closed triangular bonding pattern. One example of closed C-H-C bonding is seen in the detection of \"protonated ethene\" through mass spectrometry, , with the bridging hydrogen sitting atop the π-bond of ethene. These closed bridges are likely short-lived intermediates, as there is no steric hindrance to prevent further reaction.\n\nWhile supposedly less favored energetically, the steric properties of some molecules promote the formation of open C-H-C bridges, as shown below in Scheme 2. In 1978 T.S. Sorenson obtained NMR evidence for hydrido-bridged carbocations with 3-center 2-electron bonds through using the 1,6-dimethyl-1-cyclodecyl cation. The steric restriction of the ten-membered ring allowed the formation of a bridging hydrogen species. Expanding upon this approach, in 1992 McMurry developed the \"in\"-bicyclo[4,4,4]-1-tetradecyl cation, where the additional ring vastly improved the stability of the molecule by maintain a more rigid structure around the hydrogen bridge.\n\nWhile they are reactive intermediates, hydrogen bridged cations can be stabilized sterically. Hydrogen bridged cations are generally formed by producing a carbocation through the addition of a proton to an alcohol or alkene. Instead of continuing the reaction through the nucleophile addition of the conjugate base to the carbocation, a neighboring C-H bond can interact with the carbocation to form the hydrogen bridge. In Sorensen's 1978 synthesis and observation of the 1,6-dimethyl-1-cyclododecyl anion, the hydroxyl group of 1,6-dimethyl-1-cyclodecanol was removed using fluorosulfonic acid. This allowed the formation of the 1,6 hydrogen bridge. With McMurry's \"in\"-bicyclo[4.4.4]-tetradecyl cation, the alkene across from the methyne hydrogen was protonated by trifluoroacetic acid (TFA) to allow a bridge to the tertiary carbocation bridgehead.\n\nScheme 2 - Final steps in the synthesis of (a) 1,6-dimethyl-1-cyclodecyl cation, first characterized by T.S. Sorensen in 1978 and (b) the in-bicyclo[4.4.4]-1-tetradecyl cation which J. E. McMurry synthesized in 1992. Both involve the use of a strong acid to form a linear 3 center-2 electron bond.\n\nIn both cases, the final step used an extremely strong acid to form a carbocation across the molecule from the hydrogen to be bridged. In Sorensen's use of fluorosulfonic acid, a hydroxyl group leaves to form a tertiary carbocation. With TFA, an alkene is protonated, again causing the creation of the carbocation bridgehead. However, the stability of McMurry's \"in\"-bycyclo[4.4.4]-tetradecyl cation allows the alkene to be protonated with much weaker acids as well. Even in pure acetic acid, the alkene is protonated to about 50% as determined with NMR.\n\nHydrogen-bridged cations have been implicated in the transannular migration of hydrogens. It has been noticed that the transannular rearrangement in the reaction between an acid and a cycloalkene always results in \"cis\" addition of the nucleophile. Presumably, the stereoselectivity of this type of reaction comes from a nucleophilic attack on a hydrogen-bridged cation (Scheme 1). In this model, the electron cloud of the hydrogen bridge prevents the nucleophile from having accessibility to that side of the molecule, thus forcing it to attack from a \"cis\" orientation with respect to the initial proton.\n\nScheme 3 -Transannular migration of a hydrogen and \"cis\" nucleophilic attack.\n\nThe most significant trait of bridging hydrogens in NMR spectra is their upfield shift. Due to the increased number of bonds that the hydrogen atom is part of, the higher electron density around hydrogen shields the H nucleus, causing its chemical shift to appear at negative ppm. Characterization of the dimethylcyclodecyl ion by Sorensen in 1978 relied on low-temperature NMR, as the ion slowly decomposes at above -70 °C, and showed the bridging hydrogen at δ -3.9. McMurry's characterization of the more stable \"in\"-bicyclo[4,4,4]-tetradecyl cation showed two broad 12 proton peaks at δ 2.5 and 1.9, and a broad proton singlet at δ -3.46. Depending on the length of bridging chains in a similar bridged bicyclo[3,3,1]-nonyl cations the upfield shift of the bridging hydrogen can vary from δ -6.50 to 0.07. This is actually due to a change in C-H-C bond angle, where change in bond angle affects the bonding nature of the 3 center-2 electron bond.\n\nScheme 4 - The classical and non-classical bonding of the in-bicyclo[4.4.4]-1-tetradecyl cation are shown above. The non-classical model (a) shows one hydrogen bridged molecule while the classical model (b) would imply rapid interconversion between two distinct carbocations and lack the hydrogen bridge. McMurry confirmed the presence of the 3-center 2-electron bond and the hydrogen bridged species by NMR.\n\nAn asymmetric C-H-C stretching band at 2213 cm was observed in McMurry's synthesis. This was assigned through observation of a 1558 cm replacing the 2113 cm band upon substitution of the \"in\"-hydrogen with \"in\"-deuterium. Because the isotropic shift was observed while all other peaks remained constant, the two peaks were able to be assigned to the bridging hydrogen or deuterium. The energy of 2133 cm for the C-H-C stretching is lower than typical for C-H bonds as a result of adding a third body to the oscillating system.\n\nHydrogen-Bridged cations have been implicated in the catalysis of isomerization of radical species. This mechanism involves the transformation of a radical ion into its lower-energy hydrogen-shifted isomer using a neutral molecule as the catalyst. The need for a catalyst is the result of a large energy barrier that prevents spontaneous interconversion of the protonated species. While 1,2-alkyl shifts. of radical species are fully allowed processes and do not involve the formation of a hydrogen-bridged cation intermediate, these intermediates have been proposed to provide a feasible low-energy pathway for forbidden 1,3-alkyl shifts.\n\nThe term Proton-Transfer Catalysis (PTC) was first introduced by Diethard K. Bhome in 1992. In this process, a neutral molecule transports a proton from a high- to a low-energy site of a protonated radical to catalyze its isomerization. The first confirmed example of PTC was the isomerization of the methylene oxonium ion.\n\nThe two species involved in this mechanism are the more stable distonic methylene oxonium ion (CHOH) and the less stable ionized methanol (CHOH) species. These two species are separated by a large (108.4 kJ/mol) energy barrier that prevents their spontaneous interconversion. However, it was observed that when ionized methanol interacts with water in the chemical ionization source of a mass spectrometer, it readily transforms into the more stable distonic ion.\n\nA hydrogen-bridged cation was proposed to be responsible for this isomerization (Scheme 5). In 1996, Gauld modeled the potential energy surfaces of this reaction using water as a spectator molecule and as a mode of proton transport. This study determined that a transition state in the mechanism of water as a spectator ion (no hydrogen bridge) posed an energy barrier of 115 kJ/mol (higher than that for spontaneous isomerization). Water-mediated isomerization was modeled with a 3- and a 5-membered transition state hydrogen-bridged cation. These models only posed energy barriers of 12.3 and 8.5 kJ/mol, respectively (Scheme 6). Since a spectator-water model resulted in a higher energy barrier, it was established that the PTC mechanism proposed by Audier and Mourgues with hydrogen-bridged cations is energetically plausible, thus supporting the existence of a hydrogen bridge.\n\nScheme 5 -Proposed reaction mechanism for the isomerization of ionized methanol to the methylene oxonium ion.\n\nScheme 6 - 3-membered and 5-membered transition states for the isomerization of ionized methanol to the methylene oxonium ion.\n\nThe requirements for the neutral molecule catalyst is that it must be basic enough to deprotonate one site of the ionic species, but sufficiently weak to donate the proton to a different atom on the substrate ion. This is best exemplified by considering the proton affinity of the neutral molecule. For example, the PA value of water is 690 kJ/mol, which is found between the PA of the carbon (670 kJ/mol) and that of the oxygen (699 kJ/mol) in CHOH. In this example, the water molecule is basic enough to deprotonate the carbon atom, and acidic enough to donate the proton back but to the oxygen atom, thus facilitating the isomerization of the original species.\n"}
{"id": "20905958", "url": "https://en.wikipedia.org/wiki?curid=20905958", "title": "Indian Mesa Wind Farm", "text": "Indian Mesa Wind Farm\n\nIndian Mesa Wind Farm is a wind farm located in Pecos County, Texas. The farm consists of one hundred twenty-five Vestas V-47 660 kilowatt wind turbines that produce up to 82.5 megawatts of electricity. Electricity produced by the project is purchased by the Lower Colorado River Authority (51 megawatts) and TXU (31.5 megawatts). The project is built on land owned by local ranches and the University of Texas. The project was completed in 2001 by Orion Energy LLC and development partner National Wind Power. \n\nNextEra_Energy_Resources acquired the farm in June, 2002.\n"}
{"id": "51251625", "url": "https://en.wikipedia.org/wiki?curid=51251625", "title": "Infinitum AS", "text": "Infinitum AS\n\nInfinitum AS, former Norsk Resirk AS, is a corporation that operates the national paid recycling scheme for bottles and cans marked with the official \"recyclable\" or \"deposit\" (Pant in Norwegian) logo in Norway. The beverages containers included in the program are the ones made of aluminum, steel and plastic (PET) produced in or imported to the country. The deposit scheme for certain one-way containers is mandatory in Norway by law.\n\nThe company was established in 1999 and is owned by companies and organizations in beverage industry and food trading (giant chain stores). The company aims to ensure the highest possible return in deposit recycling packaging for beverages at the lowest cost and environmental impact, for the time being, Infinitum managed to reach 95% return rate for one-way containers in Norway.\n"}
{"id": "54666820", "url": "https://en.wikipedia.org/wiki?curid=54666820", "title": "Magnus Maclean", "text": "Magnus Maclean\n\nProf Magnus Maclean FRSE MIEE MICE LLD(1857-1937) was an electrical engineer who assisted Lord Kelvin in his electrical experiments and later became Professor of Electrical Engineering in Glasgow (one of the first to hold such a title). He also lectured in Celtic Studies at Glasgow University. The Magnus Maclean Memorial Prize given to students of electrical engineering is named in his honour.\n\nHe was born on the isle of Skye on 1 November 1857. He was educated at Colbost on the island then sent to Glasgow for secondary education. He then began training as a Free Church minister at the Free Church Training College in Glasgow and also studied at Glasgow University. However, her abandoned this after two years and became a teacher in Sutherland.\n\nRe-entering Glasgow University in 1881 with a Lorimer bursary for Mathematics, and a London Highland Society scholarship. He studied Natural Philosophy (Physics) and Mathematics at Glasgow University graduating MA around 1883.\n\nFrom at least 1880 he was the personal assistant to William Thomson, Lord Kelvin in his electrical experiments. In 1888 he was elected a Fellow of the Royal Society of Edinburgh. His proposers were William Thomson, Lord Kelvin, William Jack, Thomas Muir, and Thomas Gray. \n\nIn 1899 he became Professor of Electrical Engineering at the Royal Technical College in Glasgow.\nGlasgow University gave him an honorary doctorate (LLD) in 1919.\n\nIn later life he lived at 51 Kersland Terrace in Glasgow.\n\nHe retired in 1924 and died on 2 September 1937.\n\nHis painting by James Raeburn Middleton is held by Strathclyde University.\n\n\n \n"}
{"id": "454305", "url": "https://en.wikipedia.org/wiki?curid=454305", "title": "Mitsubishi Heavy Industries", "text": "Mitsubishi Heavy Industries\n\nMHI's products include aerospace components, air conditioners, aircraft, automotive components, forklift trucks, hydraulic equipment, machine tools, missiles, power generation equipment, printing machines, ships and space launch vehicles. Through its defense-related activities it is the world's 23rd-largest defense contractor measured by 2011 defense revenues and the largest based in Japan.\n\nMHI is one of the core companies of the Mitsubishi Group.\n\nOn November 28, 2018, the company was ordered by the South Korea Supreme Court to pay compensation for forced labor which the company oversaw during the Japanese occupation of Korea.\n\nIn 1857, at the request of the Tokugawa Shogunate, a group of Dutch engineers began work on the \"Nagasaki Yotetsusho\", a modern, Western-style foundry and shipyard near the Dutch settlement of Dejima, at Nagasaki. This was renamed \"Nagasaki Seitetsusho\" in 1860, and construction was completed in 1861. Following the Meiji Restoration of 1868, the shipyard was placed under control of the new Government of Meiji Japan. The first dry dock was completed in 1879.\n\nIn 1884, Yataro Iwasaki, the founder of Mitsubishi, leased the \"Nagasaki Seitetsusho\" from the Japanese government, renamed it the \"Nagasaki Shipyard & Machinery Works\" and entered the shipbuilding business on a large scale. Iwasaki purchased the shipyards outright in 1887. In 1891, Mitsubishi Heavy Industries - Yokohama Machinery Works was started as \"Yokohama Dock Company, Ltd\". Its main business was ship repairs, to which it added ship servicing by 1897. The works was renamed \"Mitsubishi Shipyard of Mitsubishi Goshi Kaisha\" in 1893 and additional dry docks were completed in 1896 and 1905.\n\nThe Mitsubishi Heavy Industries - Shimonoseki Shipyard & Machinery Works was established in 1914. It produced industrial machinery and merchant ships.\n\nThe Nagasaki company was renamed \"Mitsubishi Shipbuilding & Engineering Company, Ltd.\" in 1917 and again renamed as \"Mitsubishi Heavy Industries\" in 1934. It became the largest private firm in Japan, active in the manufacture of ships, heavy machinery, airplanes and railway cars. Mitsubishi Heavy Industries merged with the Yokohama Dock Company in 1935. From its inception, the Mitsubishi Nagasaki shipyards were heavily involved in contracts for the Imperial Japanese Navy. The largest battleship \"Musashi\" was completed at Nagasaki in 1942.\n\nThe \"Kobe Shipyard of Mitsubishi Goshi Kaisha\" was established in 1905. The Kobe Shipyard merged with Mitsubishi Heavy Industries in 1934. The Kobe Shipyard constructed the ocean liner \"Argentina Maru\" (later repurposed as the aircraft carrier \"Kaiyo\"), and the submarines the \"I-19\" and \"I-25\".\n\nFollowing the dissolution of the \"zaibatsu\" after the surrender of Japan at the end of World War II, Mitsubishi divided into three companies. Mitsubishi Nagasaki became \"West Japan Heavy Industries, Ltd.\" The Nagasaki Shipyard was renamed \"Mitsubishi Shipbuilding & Engineering Co., Ltd.\" in 1952. The Mitsubishi Kobe Shipyard became \"Central Japan Heavy Industries, Ltd.\" in 1950.\n\nIn 1964, the three independent companies from the 1950 break-up were merged again into one company under the name of Mitsubishi Heavy Industries, Ltd. The Nagasaki works was renamed the \"Nagasaki Shipyard & Engine Works\". The Kobe works was renamed the Mitsubishi Heavy Industries - Kobe Shipyard & Machinery Works.\n\nIn 1970, MHI's automobile parts department became an independent company as Mitsubishi Motors.\n\nIn 1974, its Tokyo headquarters was targeted in a bombing that killed eight people.\n\nMHI participated in a ¥540 billion emergency rescue of Mitsubishi Motors in January 2005, in partnership with Mitsubishi Corporation and Mitsubishi Tokyo Financial Group. As part of the rescue, MHI acquired ¥50 billion of Mitsubishi Motors stock, increasing its ownership stake to 15 percent and making the automaker an affiliate again.\n\nIn October 2009, MHI announced an order for up to 100 regional jets from the United States-based airline Trans States Holdings.\n\nMHI entered talks with Hitachi in August 2011 about a potential merger of the two companies, in what would have been the largest merger between two Japanese companies in history. The talks subsequently broke down and were suspended.\n\nIn November 2012, Mitsubishi Heavy Industries and Hitachi agreed to merge their thermal power generation businesses into a joint venture to be owned 65% by Mitsubishi Heavy Industries and 35% by Hitachi. The joint venture began operations in February 2014.\n\nIn June 2014 Siemens and Mitsubishi Heavy Industries announced their formation of joint ventures to bid for Alstom's troubled energy and transportation businesses (in locomotives, steam turbines, and aircraft engines). A rival bid by General Electric (GE) has been criticized by French government sources, who consider Alstom's operations as a \"vital national interest\" at a moment when the French unemployment level stands above 10% and some voters are turning towards the far-right.\n\nMHI has aerospace facilities in Nagoya, Aichi, Komaki, Aichi and Mississauga, Ontario, Canada.\n\nIn the 1950s the company began to re-enter the aerospace industry in earnest. Along with other major Japanese companies it was involved in design and production of the NAMC YS-11, the first Japanese airliner to enter production after World War II. In 1956 work started on the design of the Mitsubishi MU-2, which became the company's first postwar aircraft design. \n\nIn the defense sector, MHI has produced jet fighters for the Japan Air Self-Defense Force and anti-submarine helicopters for the Japan Maritime Self-Defense Force, as well as aero-engines, missiles and torpedoes. It produced North American F-86 Sabre, Lockheed F-104 Starfighter and McDonnell Douglas F-4 Phantom II fighters. It manufactured 139 Mitsubishi F-15J fighter aircraft from 1981 and produced 200 Sikorsky S-70 family Mitsubishi H-60 helicopters from 1989, in both cases under license production. The company also plays an important role in the Japanese Ballistic Missile Defense System program. \n\nIn the space systems sector, MHI is the producer of the H-IIA and H-IIB launch vehicles, Japan's main rockets, and provides launch services to JAXA related to the launch vehicles. The company is also involved in the International Space Station program.\n\nOn 1 April 2008, MHI established Mitsubishi Aircraft Corporation as a subsidiary to develop and produce the MRJ or Mitsubishi Regional Jet, a 70 to 90 passenger regional airliner. MHI is the majority shareholder of the new company, with Toyota Motor Corporation owning 10%.\n\nOn December 12, 2012, Mitsubishi Heavy Industries has bought Goodrich turbine business, previously owned by United Technologies.\n\nIn the civil aircraft sector, MHI develops and manufactures major airframe components, including fuselage panels for the Boeing 777 and composite-material wing boxes for the 787.\nIn June 2014, the company joined four other major Japanese companies in signing an agreement to build parts for Boeing's 777X aircraft.\n\nIn 2010 MHI commenced production of the Type 10 advanced main battle tank, at a cost of $11.3 million per unit, to replace the Type 90 tank.\n\n\nThe nuclear business of MHI operates facilities in the cities of Kobe and Takasago in Hyogo Prefecture and in Yokohama, Kanagawa Prefecture. It also operates a nuclear fuel manufacturing plant in Tōkai, Ibaraki which processes 440 Metric tons of Uranium per year.\n\nMHI has also developed the Mitsubishi APWR design. MHI has also signed a memorandum of understanding with Areva for the establishment of a joint venture, Atmea, for their next reactor design ATMEA1.\n\nMHI has also been selected as the core company to develop a new generation of Fast Breeder Reactors (FBR) by the Japanese government. After that announcement was made, MHI established a new company, Mitsubishi FBR Systems, Inc. (MFBR) specifically for the development and realization of FBR technology, starting what is likely to be the most aggressive corporate venture into FBR and Generation IV reactor technology.\n\nMHI is currently developing a $15.8 billion nuclear power plant in Sinop, Turkey in partnership with Itochu and Engie, which would be its first overseas nuclear project.\n\nMHI attempted to acquire the energy business of Alstom in 2014 in order to develop its service network in Southeast Asia. MHI remains interested in acquisitions in the crude oil and gas sectors as of 2015. Following financial difficulties at Areva, MHI announced in 2015 it will make a proposal to take a minority ownership stake in Areva.\n\nMHI has shipbuilding facilities in Nagasaki, Kobe and Shimonoseki, Japan. is the primary shipbuilding division of MHI. It primarily produces specialized commercial vessels, including LNG carriers, and passenger cruise ships.\n\nOn 1 December, 2017, Mitsubishi Heavy Industries, Ltd. (MHI) has announced that it will launch two new wholly owned companies on 1 January, 2018 in conjunction with reorganization of its shipbuilding business. Mitsubishi Shipbuilding Co., Ltd. will primarily undertake construction of ships that require intensive outfitting, and Mitsubishi Heavy Industries Marine Structure Co., Ltd. will mainly engage in the manufacture of large ships and marine structures.\n\nMHI has installed more 3,282 MW worldwide until December 2009, mainly turbines with 1 and 2.4 MW. The company is developing 7-MW-turbines for offshore wind power. Tests are planned for 2013 in Europe.\n\nOn September 27, 2013, MHI and Vestas Wind Systems announced a joint-venture to develop offshore wind energy based on Vestas' V164 8.0MW turbine.\n\nMHI's products include:\n\n\nOn November 28, 2018, Mitsubishi Heavy Industries was ordered by the South Korea Supreme Court to pay 150m won ($133,000; £104,000) in compensation to 10 surviving Koreans who were victims of forced labor which the company oversaw during the Japanese occupation of Korea. 18 family members of other victims of the forced labour which Mitsubishi Heavy Industries oversaw and who previously sued sometime before 2008 will also be awarded compensation as well. All 28 plaintiffs had previously filed a lawsuit in Japan, but had their lawsuit dismissed the Supreme Court of Japan in 2008.\n\n"}
{"id": "4248537", "url": "https://en.wikipedia.org/wiki?curid=4248537", "title": "Mumbai Refinery (HPCL)", "text": "Mumbai Refinery (HPCL)\n\nThe HPCL Mumbai refinery is one of the most complex refineries in the country, is constructed on an area of 321 acres. This versatile refinery which is the first of India’s modern refineries, symbolizes the country’s industrial strength and progress in the oil industry. Mumbai Refinery has grown over the years as the main hub of petroleum products. The refinery has reached to present level through several upgradation and restructuring processes.\n\nThe Mumbai Refinery was commissioned by Esso Standard in 1954, with an installed capacity of 1.25 MMTPA.\nLube refinery, Lube India Ltd, was commissioned in 1969 with a capacity of 165 TMTPA of Lube Oil Base Stock (LOBS) production\nCrude processing capacity increased to 3.5 MMTPA during 1969. In 1974, the Government of India took over Esso and Lube India by the Esso (Acquisition of Undertakings in India) Act 1974 and formed HPCL. \nExpansion of fuels block was carried out by installation of new 2 MMTPA crude units in 1985.\nSecond expansion of Lube Refinery took place to increase the capacity of the refinery to 335 MMTPA, so far the largest in India. The installed capacity of the refinery was later enhanced to 6.5 MMTPA. The current installed capacity of the refinery is 7.5 MMTPA\n"}
{"id": "53932606", "url": "https://en.wikipedia.org/wiki?curid=53932606", "title": "Nanoscale vacuum-channel transistor", "text": "Nanoscale vacuum-channel transistor\n\nA nanoscale vacuum-channel transistor (NVCT) is a transistor in which the electron transport medium is a vacuum. In a traditional solid-state transistor, a semiconductor channel exists between the source and the drain, and the current flows through the semiconductor. However, in a nanoscale vacuum-channel transistor, no material exists between the source and the drain, and therefore, the current flows through the vacuum.\n\nTheoretically, a vacuum-channel transistor is expected to operate faster than a traditional solid-state transistor, and have higher power output. Moreover, vacuum-channel transistors are expected to operate at higher temperature and radiation level than a traditional transistor making them suitable for space application.\n\nAlthough vacuum-channel transistors are not currently commercially available, several implementations and prototypes of vacuum-channel transistors are reported in the literature such as vertical field-emitter vacuum-channel transistor, gate-insulated planar electrodes vacuum-channel transistor, vertical vacuum-channel transistor, and all-around gate vacuum-channel transistor.\n\nThe concept of using field-emitted electron beam in a diode was first mentioned in a 1961 article by Kenneth Shoulders. However, due to the technological difficulty of fabricating a field-emitter electron source, such a diode was not implemented.\n\nAs the field of microfabrication advanced, it became possible to fabricate field-emitted electron sources, thereby paving the way for vacuum-channel transistors. The first successful implementation was reported by Gary et. al. in 1986. However, early vacuum-channel transistors suffered from high gate threshold voltage and couldn't compete with solid-state transistors. \n\nMore recent advances in microfabrication have allowed the vacuum-channel length between the source and the drain to be shrunk, thereby reducing the gate threshold voltage. In 2012, Srisonphan et. al. reported a new design of vacuum-channel transistor with a gate threshold value of 0.5V, which is comparable to the gate threshold voltage of current solid-state transistors.\n\nAs the shrinking of solid-state transistors is reaching its theoretical limit, vacuum-channel transistors may offer an alternative.\n\nA nanoscale vacuum-channel transistor is essentially a miniaturized version of a vacuum tube. It consists of a field-emitter electron source, a collector electrode, and a gate electrode. The electron source and the collector electrodes are separated by a small distance, usually of the order of several nanometers. When a voltage is applied across the source and the collector electrode, due to field-emission, electrons are emitted from the source electrode, travel through the gap and are collected by the collector electrode. A gate electrode is used to control the current flow through the vacuum-channel.\n\nDespite the name, vacuum-channel transistors do not need to be evacuated. The gap traversed by the electrons is so small that collisions with molecules gas at atmospheric pressure are infrequent enough not to matter.\n\nThe nanoscale vacuum-channel transistors have several benefits over traditional solid-state transistors such as high speed, high output power, and operation at high temperature and immunity to strong radiations. The advantages of a vacuum-channel transistor over a solid-state transistor are discussed in detail below:\n\nIn a solid-state transistor, the electrons collide with the semiconductor lattice and suffer from scattering which slows down the speed of the electrons. In fact, in silicon, the velocity of electrons is limited to 1.4×10 cm/s. However, in vacuum electrons do not suffer from scattering and can reach a velocity of up to 3×10 cm/s. Therefore, a vacuum-channel transistor can operate at a faster speed than a silicon solid-state transistor.\n\nThe band-gap of silicon is 1.11eV, and the thermal energy of electrons should remain lower than this value for silicon to retain its semiconductor properties. This places a limit on the operating temperature of silicon transistors. However, no such limitation exists in vacuum. Therefore, a vacuum-channel transistor can operate at a much higher temperature, only limited by the melting temperature of the materials used for its fabrication. The vacuum-transistor can be used in applications where a tolerance to high temperature is required.\n\nThe radiation can ionize the atoms in a solid-state transistors. These ionized atoms and corresponding electrons can interfere with the electron transport between the source and collector. However, no ionization occur in the vacuum-channel transistors. Therefore, a vacuum-channel transistor can be used in a high radiation environment such as outer space or inside a nuclear reactor.\n\nThe performance of a vacuum-channel transistor depends upon the field emission of electrons from the source electrode. However, due to the high electric field, the source electrodes degrades over time, thereby decreasing the emission current. Due to the degradation of electrons source electrode, vacuum-channel transistors suffer from poor reliability.\n\n"}
{"id": "44166389", "url": "https://en.wikipedia.org/wiki?curid=44166389", "title": "Oil depletion allowance", "text": "Oil depletion allowance\n\nThe oil depletion allowance in American (US) tax law is an allowance claimable by anyone with an economic interest in a mineral deposit or standing timber. The principle is that the asset is a capital investment that is a wasting asset, and therefore depreciation can reasonably be offset (effectively as a capital loss) against income.\n\nThe oil depletion allowance has been subject of interest, because of the relationship of big oil with the US government, and because one method (percentage depletion) of claiming the allowance makes it possible to write off more than the whole capital cost of the asset.\n\nTwo methods of depletion calculations are available, detailed regulations determine which can be used, but in some circumstances the asset owner can choose.\n\nWith this method the original investment is effectively amortized over the productive life of the asset, starting with the original capital investment, the annual percentage being the percentage of the reserves at the beginning of the year that are sold in the course of the year. The amortized amount is deducted from the net income before calculating taxes. The total amount deducted by this method cannot exceed the original value of the capital invested.\n\nWith this method, a fixed percentage of the gross income is treated as deductible. The percentage is dependent on the nature of the resource being extracted. It is possible under this scheme for the total deductibles (or indeed the annual deductible) to exceed the original capital investment.\n\nThe following percentages are prescribed by the Internal Revenue Code, section 613(b).\nFor geothermal assets the rate is 15%\n\nFor independent producers or royalty owners of oil and gas, the deduction for percentage depletion is limited to the smaller of:\n\n\nAmounts not deductible due to the 65% limit can be carried forward.\n\nThis article uses text from \"Internal Revenue Code, Section 613(b)\", and \"Publication 535 (2013), Business Expenses \" which are in the Public Domain as works of the US Federal Government\n\n"}
{"id": "7836920", "url": "https://en.wikipedia.org/wiki?curid=7836920", "title": "Polymethylpentene", "text": "Polymethylpentene\n\nPolymethylpentene (PMP), also known as poly(4-methyl-1-pentene), is a thermoplastic polymer of 4-methyl-1-pentene. It is used for gas permeable packaging, autoclavable medical and laboratory equipment, microwave components, and cookware. It is commonly called TPX, which is a trademark of Mitsui Chemicals.\n\nPolymethylpentene is a 4-methyl-1-pentene based linear isotactic polyolefin and is made by Ziegler-Natta type catalysis. The commercially available grades are usually copolymers. It can be extruded and molded (by injection molding or blow molding).\n\nPolymethylpentene melts at ≈ 235 °C. It has a very low density (0.84 g/cm) and is transparent. It has low moisture absorption, and exceptional acoustical and electrical properties. Its properties are reasonably similar to those of other polyolefins, although it is more brittle and more gas permeable. The polymer also has a high thermal stability, excellent dielectric characteristics and a high chemical resistance. The crystalline phase has a lower density than the amorphous phase.\n\nIn comparison to other materials being used for operating in THz range, TPX shows excellent optical properties with a wavelength independent refractive index of 1.460±0.005 between visible light and 100~GHz.\n\n\n\n"}
{"id": "5137675", "url": "https://en.wikipedia.org/wiki?curid=5137675", "title": "Price of oil", "text": "Price of oil\n\nThe price of oil, or the oil price, (generally) refers to the spot price of a barrel of benchmark crude oil—a reference price for buyers and sellers of crude oil such as West Texas Intermediate (WTI), Brent ICE, Dubai Crude, OPEC Reference Basket, Tapis Crude, Bonny Light, Urals oil, Isthmus and Western Canadian Select (WCS). There is a differential in the price of a barrel of oil based on its grade—determined by factors such as its specific gravity or API and its sulphur content—and its location—for example, its proximity to tidewater and/or refineries. Heavier, sour crude oils lacking in tidewater access—such as Western Canadian Select— are less expensive than lighter, sweeter oil—such as WTI.\n\nIn 1960 the Organization of the Petroleum Exporting Countries (OPEC) was founded in Baghdad, Iraq by its first five members —Iran, Iraq, Kuwait, Saudi Arabia and Venezuela—, with Qatar and Libya joining immediately, followed by United Arab Emirates, Algeria, Nigeria, Ecuador and Gabon after a decade. The goal of these countries was to increase its influence in the world oil market, then dominated by a cartel known as the \"Seven Sisters\", five of which were headquartered in the United States. These companies had been controlling posted prices since the so-called 1927 Red Line Agreement and 1928 Achnacarry Agreement, and had achieved a high level of price stability until 1972. Angola joined the OPEC in 2007 and Equatorial Guinea in 2017.\n\nFrom 1999 til mid 2008, the price of oil rose significantly. It was explained by the rising oil demand in countries like China and India.\nIn the middle of the financial crisis of 2007–2008, the price of oil underwent a significant decrease after the record peak of US$147.27 it reached on July 11, 2008. On December 23, 2008, WTI crude oil spot price fell to US$30.28 a barrel, the lowest since the financial crisis of 2007–2010 began. The price sharply rebounded after the crisis and rose to US$82 a barrel in 2009. In July 2008 oil reached a record peak of US$147.27 but by February 2009 it sank beneath $40 a barrel. On 31 January 2011, the Brent price hit $100 a barrel for the first time since October 2008, on concerns about the political unrest in Egypt. For about three and half years the price largely remained in the $90–$120 range. In the middle of 2014, price started declining due to a significant increase in oil production in USA, and declining demand in the emerging countries. The oil glut—caused by multiple factors—spurred a sharp downward spiral in the price of oil that continued through February 2016. By February 3, 2016 oil was below $30— a drop of \"almost 75 percent since mid-2014 as competing producers pumped 1–2 million barrels of crude daily exceeding demand, just as China's economy hit lowest growth in a generation.\" Some analysts speculate that it may continue to drop further, perhaps as low as $18\n\nAccording to a report released on February 15, 2016 by Deloitte LLP—the audit and consulting firm—with global crude oil at near ten-year low prices, 35% of listed E&P oil and gas companies are at a high risk of bankruptcy worldwide. Indeed, bankruptcies \"in the oil and gas industry could surpass levels seen in the Great Recession.\"\n\nThere are two views dominating the oil market discourse. There are those who strongly believe that the market has undergone structural changes and that low oil prices are here to stay for a prolonged period. At the other end of the spectrum, there are those who think that this is yet another cycle and oil prices will recover sooner rather than later.\n\nA 2016 survey of the academic literature finds that \"most major oil price fluctuations dating back to 1973 are largely explained by shifts in the demand for crude oil\". As the global economy expands, so does demand for crude oil. The authors note that the price of oil has also increased at times due to greater \"demand for stocks (or inventories) of crude oil... to guard against future shortages in the oil market. Historically, inventory demand has been high in times of geopolitical tension in the Middle East, low spare capacity in oil production, and strong expected global economic growth.\" In particular, political events can have a strong influence on the oil price. Historical examples include OPEC’s 1973 embargo in reaction to the Yom Kippur War and the 1979 Iranian Revolution. Financial analysts and academics have had very few tools to study such political events compared to what is available on economic aspects of oil price formation. The PRIX index was developed in attempt to fill this gap with a metric on political developments and corresponding export trends from world’s 20 largest oil exporters.\n\nThe supply of oil is dependent on geological discovery, the legal and tax framework for oil extraction, the cost of extraction, the availability and cost of technology for extraction, and the political situation in oil-producing countries. Both domestic political instability in oil producing countries and conflicts with other countries can destabilise the oil price. In 2008 the \"New York Times\" reported, for example, in the 1940s the price of oil was about $17 rising to just over $20 during the Korean War (1951–1953). During the Vietnam War (1950s1970s) the price of oil slowly declined to under $20. During the Arab oil embargo of 1973—the first oil shock—the price of oil rapidly rose to double in price. During the 1979 Iranian Revolution the price of oil rose. During the second oil shock the price of oil peaked in April 1980 at $103.76. During the 1980s there was a period of \"conservation and insulation efforts\" and the price of oil dropped slowly to . It again reached a peak of c. $65 during the 1990 Persian Gulf crisis and war. Following that, there was a period of global recessions and the price of oil hit a low of before it peaked at a high of $45 on September 11, 2001 only to drop again to a low of $26 on May 8, 2003. The price rose to $80 with the U.S.-led invasion of Iraq. By March 3, 2008 the price of oil reached $103.95 a barrel on the New York Mercantile Exchange.\n\nAlthough the oil price is largely determined by the balance between supply and demand—as with all commodities—some commentators including \"Business Week,\" the \"Financial Times\" and the \"Washington Post\", argued that the rise in oil prices prior to the financial crisis of 2007–2008 was due to speculation in futures markets.\n\nFor a dissenting view of oil prices being determined by demand, see Forbes, \"Global Oil Demand is Always Rising – and Not Related to Price\".\n\nIn North America this generally refers to the WTI Cushing Crude Oil Spot Price West Texas Intermediate (WTI), also known as Texas Light Sweet, a type of crude oil used as a benchmark in oil pricing and the underlying commodity of New York Mercantile Exchange's oil futures contracts. WTI is a light crude oil, lighter than Brent Crude oil. It contains about 0.24% sulfur, rating it a sweet crude, sweeter than Brent. Its properties and production site make it ideal for being refined in the United States, mostly in the Midwest and Gulf Coast regions. WTI has an API gravity of around 39.6 (specific gravity approx. 0.827) per barrel (159 liters) of either WTI/light crude as traded on the New York Mercantile Exchange (NYMEX) for delivery at Cushing, Oklahoma. Cushing, Oklahoma, a major oil supply hub connecting oil suppliers to the Gulf Coast, has become the most significant trading hub for crude oil in North America.\n\nIn Europe and some other parts of the world, the oil price benchmark is Brent as traded on the Intercontinental Exchange (ICE, into which the International Petroleum Exchange has been incorporated) for delivery at Sullom Voe.\n\nOther important benchmarks include Dubai, Tapis, and the OPEC basket. The Energy Information Administration (EIA) uses the imported refiner acquisition cost, the weighted average cost of all oil imported into the US, as its \"world oil price\".\n\nIn Robert Mabro's 2006 book on challenges and opportunities in oil in the 21st century, after the collapse of the OPEC-administered pricing system in 1985, and a short lived experiment with netback pricing, oil-exporting countries adopted a market-linked pricing mechanism. First adopted by PEMEX in 1986, market-linked pricing received wide acceptance and by 1988 became and still is the main method for pricing crude oil in international trade. The current reference, or pricing markers, are Brent, WTI, and Dubai/Oman.\n\nOil is marketed among other products in commodity markets. By 2008 widely traded oil futures, and related natural gas futures, included with most of these oil futures having delivery dates every month:\n\nIn June 2008 \"Business Week\" reported that the surge in oil prices prior to 2008 had led some commentators to argue that at least some of the rise was due to speculation in the futures markets. However, although speculation can greatly raise the oil price in the short run, in the long run fundamental market conditions will determine the oil price. Storing oil is expensive, and all speculators must ultimately, and generally within a few months, sell the oil they purchase.\n\nAccording to a U.S. Commodity Futures Trading Commission (CFTC) May 29, 2008 report the \"Multiple Energy Market Initiatives\" was launched in partnership with the United Kingdom Financial Services Authority and ICE Futures Europe in order to expand surveillance and information sharing of various futures contracts. Part 1 is \"Expanded International Surveillance Information for Crude Oil Trading.\" This announcement has received wide coverage in the financial press, with speculation about oil futures price manipulation.\n\nThe interim report by the Interagency Task Force, released in July, found that speculation had not caused significant changes in oil prices and that fundamental supply and demand factors provide the best explanation for the crude oil price increases. The report found that the primary reason for the price increases was that the world economy had expanded at its fastest pace in decades, resulting in substantial increases in the demand for oil, while the oil production grew sluggishly, compounded by production shortfalls in oil-exporting countries.\n\nThe report stated that as a result of the imbalance and low price elasticity, very large price increases occurred as the market attempted to balance scarce supply against growing demand, particularly in the last three years. The report forecast that this imbalance would persist in the future, leading to continued upward pressure on oil prices, and that large or rapid movements in oil prices are likely to occur even in the absence of activity by speculators. The task force continues to analyze commodity markets and intends to issue further findings later in the year.\n\nThe oil-storage trade, also referred to as contango, a market strategy in which large, often vertically-integrated oil companies purchase oil for immediate delivery and storage—when the price of oil is low— and hold it in storage until the price of oil increases. Investors bet on the future of oil prices through a financial instrument, oil futures in which they agree on a contract basis, to buy or sell oil at a set date in the future. Crude oil is stored in salt mines, tanks and oil tankers.\n\nInvestors can choose to take profits or losses prior to the oil-delivery date arrives. Or they can leave the contract in place and physical oil is \"delivered on the set date\" to an \"officially designated delivery point\", in the United States, that is usually Cushing, Oklahoma. When delivery dates approach, they close out existing contracts and sell new ones for future delivery of the same oil. The oil never moves out of storage. If the forward market is in \"contango\"—the forward price is higher than the current spot price—the strategy is very successful.\n\nScandinavian Tank Storage AB and its founder Lars Jacobsson introduced the concept on the market in early 1990. But it was in 2007 through 2009 the oil storage trade expanded, with many participants—including Wall Street giants, such as Morgan Stanley, Goldman Sachs, and Citicorp—turning sizeable profits simply by sitting on tanks of oil. By May, 2007 Cushing's inventory fell by nearly 35% as the oil-storage trade heated up.\n\nBy the end of October 2009 one in twelve of the largest oil tankers was being used more for temporary storage of oil, rather than transportation.\n\nFrom June 2014 to January 2015, as the price of oil dropped 60 percent and the supply of oil remained high, the world's largest traders in crude oil purchased at least 25 million barrels to store in supertankers to make a profit in the future when prices rise. Trafigura, Vitol, Gunvor, Koch, Shell and other major energy companies began to book booking oil storage supertankers for up to 12 months. By 13 January 2015 At least 11 Very Large Crude Carriers (VLCC) and Ultra Large Crude Carriers (ULCC)\" have been reported as booked with storage options, rising from around five vessels at the end of last week. Each VLCC can hold 2 million barrels.\"\n\nIn 2015 as global capacity for oil storage was out-paced by global oil production, and an oil glut occurred. Crude oil storage space became a tradable commodity with CME Group— which owns NYMEX— offering oil-storage futures contracts in March 2015. Traders and producers can buy and sell the right to store certain types of oil.\n\nBy 5 March 2015, as oil production outpaces oil demand by 1.5 million barrels a day, storage capacity globally is dwindling. In the United States alone, according to data from the Energy Information Administration, U.S. crude-oil supplies are at almost 70% of the U. S. storage capacity, the highest to capacity ratio since 1935.\n\nPeak oil is the period when the maximum rate of global petroleum extraction is reached, after which the rate of production enters terminal decline. It relates to a long-term decline in the available supply of petroleum. This, combined with increasing demand, will significantly increase the worldwide prices of petroleum derived products. Most significant will be the availability and price of liquid fuel for transportation.\n\nThe US Department of Energy in the Hirsch report indicates that \"The problems associated with world oil production peaking will not be temporary, and past \"energy crisis\" experience will provide relatively little guidance.\"\n\nAccording to the United Nations, world oil demand is projected to reach over 99 million barrels per day in 2018.\n\nA major rise or decline in oil price can have both economic and political impacts. The decline on oil price during 1985–1986 is considered to have contributed to the fall of the Soviet Union. Low oil prices could alleviate some of the negative effects associated with the resource curse, such as authoritarian rule and gender inequality. Lower oil prices could however also lead to domestic turmoil and diversionary war. The reduction in food prices that follows lower oil prices could have positive impacts on violence globally.\n\nResearch shows that declining oil prices make oil-rich states less bellicose. Low oil prices could also make oil-rich states engage more in international cooperation, as they become more dependent on foreign investments. The influence of the United States reportedly increases as oil prices decline, at least judging by the fact that \"both oil importers and exporters vote more often with the United States in the United Nations General Assembly\" during oil slumps.\n\nThe macroeconomics impact on lower oil prices is lower inflation. A lower inflation rate is good for the consumers. This means that the general price of a basket of goods would increase at a bare minimum on a year to year basis. Consumer can benefit as they would have a better purchasing power, which may improve real gdp . However, in recent countries like Japan, the decrease in oil prices may cause deflation and it shows that consumers are not willing to spend even though the prices of goods are decreasing yearly, which indirectly increases the real debt burden.\nDeclining oil prices may boost consumer oriented stocks but may hurt oil-based stocks. It is estimated that 17–18% of S&P would decline with declining oil prices.\n\nThe oil importing economies like EU, Japan, China or India would benefit, however the oil producing countries would lose. A Bloomberg article presents results of an analysis by Oxford Economics on the GDP growth of countries as a result of a drop from $84 to $40. It shows the GDP increase between 0.5% to 1.0% for India, USA and China, and a decline of greater than 3.5% from Saudi Arabia and Russia. A stable price of $60 would add 0.5 percentage point to global gross domestic product.\n\nKatina Stefanova has argued that falling oil prices do not imply a recession and a decline in stock prices. Liz Ann Sonders, Chief Investment Strategist at Charles Schwab, had earlier written that that positive impact on consumers and businesses outside of the energy sector, which is a larger portion of the US economy will outweigh the negatives. Taking cues from a legendary oil investor, Harold Hamm, ranked as one of the richest men in the world by Forbes, Shawn Baldwin, Chairman of alternative investment firm The AIA Group, speculates that oil prices will rise by year-end 2016 from current levels.\n\nEconomists have observed that the 2015–2016 oil glut also known as 2010s oil glut started with a considerable time-lag, more than six years after the beginning of the Great Recession: \"\"the price of oil\" [had] \"stabilized at a relatively high level (around $100 a barrel) unlike all previous recessionary cycles since 1980 (start of First Persian Gulf War). But nothing guarantee[d] such price levels in perpetuity\"\".\n\nDuring 2014–2015, OPEC members consistently exceeded their production ceiling, and China experienced a marked slowdown in economic growth. At the same time, U.S. oil production nearly doubled from 2008 levels, due to substantial improvements in shale \"fracking\" technology in response to record oil prices. A combination of factors led a plunge in U.S. oil import requirements and a record high volume of worldwide oil inventories in storage, and a collapse in oil prices that continues into 2016.\n\nIt has also been argued that the collapse in oil prices in 2015 should be very beneficial for developed western economies, who are generally oil importers and aren't over exposed to declining demand from China. In the Asia-Pacific region, exports and economic growth were at significant risk across economies reliant on commodity exports as an engine of growth. The most vulnerable economies were those with a high dependence on fuel and mineral exports to China, such as: Korea DPR, Mongolia and Turkmenistan – where primary commodity exports account for 59–99% of total exports and more than 50% of total exports are destined to China. The decline in China’s demand for commodities also adversely affected the growth of exports and GDP of large commodity-exporting economies such as Australia (minerals) and the Russian Federation (fuel). On the other hand, lower commodity prices led to an improvement in the trade balance – through lower the cost of raw materials and fuels – across commodity importing economies, particularly Cambodia, Kyrgyzstan, Nepal and other remote island nations (Kiribati, Maldives, Micronesia (F.S), Samoa, Tonga, and Tuvalu) which are highly dependent on fuel and agricultural imports \n\nThe North Sea oil and gas industry was financially stressed by the reduced oil prices, and called for government support in May 2016.\n\nThe use of hedging using commodity derivatives as a risk management tool on price exposure to liquidity and earnings, has been long established in North America. Chief Financial Officers (CFOS) use derivatives to dampen, remove or mitigate price uncertainty. Bankers also use hedge funds to more \"safely increase leverage to smaller oil and gas companies.\" However, when not properly used, \"derivatives can multiply losses\" particularly in North America where investors are more comfortable with higher levels of risk than in other countries.\n\nWith the large number of bankruptcies as reported by Deloitte \"funding [for upstream oil industry] is shrinking and hedges are unwinding.\" \"Some oil producers are also choosing to liquidate hedges for a quick infusion of cash, a risky bet.\"\n\nTo finance exploration and production of the unconventional shale oil industry in the United States, \"hundreds of billions of dollars of capital came from non-bank participants [non-bank buyers of bank energy credits] in leveraged loans] that were thought at the time to be low risk. However, with the oil glut that continued into 2016, about a third of oil companies are facing bankruptcy. While investors were aware that there was a risk that the operator might declare bankruptcy, they felt protected because \"they had come in at the 'bank' level, where there was a senior claim on the assets [and] they could get their capital returned.\"\n\nBy 2012,\n\nA classic example of taking on too much risk through hedging is the 1982 collapse of Penn Square Bank caused by plummeting of the price of oil in 1981. Penn Square Bank had lent too much to Exploration and Production E&P operators. Penn Square Bank caused the failure of Seafirst in 1982 and then Continental Illinois. When they failed and were liquidated by the Federal Deposit Insurance Corporation (FDIC) the non-bank buyers or participants of bank energy credits of these leveraged loans participants were considered to be 'unsecured claims,' not 'true sales' and they were not able to collect any capital.\n\nAt the 5th annual World Pensions Forum in 2015, Jeffrey Sachs advised institutional investors to divest from carbon-reliant oil industry firms in their pension fund's portfolio.\n\nBecause of oversupply and lack of agreements between oil-producing countries members of the OPEC (Saudi Arabia in particular, which pumped at world's records) and also because of lack of coordinated efforts between OPEC and Non-OPEC countries (Russian being a big player, refusing to reduce production) the price of oil fell rapidly in 2015 and continued to slide in 2016 causing the cost of WTI crude to fall to a 10-year low of $26.55 on January 20. The average price of oil in January 2016 was well below $35. Oil did not recover until April 2016, when oil went above the $45 mark.\n\nBy 20 January 2016, the OPEC Reference Basket was down to US$22.48/bbl—less than one-fourth of its high from June 2014 ($110.48), less than one-sixth of its record from July 2008 ($147.27), and back below the April 2003 starting point ($23.27) of its historic run-up. According to the United Nations, growth prospects of the oil exporters are predicted to remain subdued past 2018.\n\n"}
{"id": "25233", "url": "https://en.wikipedia.org/wiki?curid=25233", "title": "Quartz", "text": "Quartz\n\nQuartz is a mineral composed of silicon and oxygen atoms in a continuous framework of SiO silicon–oxygen tetrahedra, with each oxygen being shared between two tetrahedra, giving an overall chemical formula of SiO. Quartz is the second most abundant mineral in Earth's continental crust, behind feldspar.\n\nQuartz crystals are chiral, and exist in two forms, the normal α-quartz and the high-temperature β-quartz. The transformation from α-quartz to β-quartz takes place abruptly at 573 °C (846 K). Since the transformation is accompanied by a significant change in volume, it can easily induce fracturing of ceramics or rocks passing through this temperature limit.\n\nThere are many different varieties of quartz, several of which are semi-precious gemstones. Since antiquity, varieties of quartz have been the most commonly used minerals in the making of jewelry and hardstone carvings, especially in Eurasia.\n\nThe word \"quartz\" is derived from the German word \"Quarz\", which had the same form in the first half of the 14th century in Middle High German in East Central German and which came from the Polish dialect term \"kwardy\", which corresponds to the Czech term \"tvrdý\" (\"hard\").\n\nThe Ancient Greeks referred to quartz as κρύσταλλος (\"krustallos\") derived from the Ancient Greek \"κρύος\" (\"kruos\") meaning \"icy cold\", because some philosophers (including Theophrastus) apparently believed the mineral to be a form of supercooled ice. Today, the term \"rock crystal\" is sometimes used as an alternative name for the purest form of quartz.\n\nQuartz belongs to the trigonal crystal system. The ideal crystal shape is a six-sided prism terminating with six-sided pyramids at each end. In nature quartz crystals are often twinned (with twin right-handed and left-handed quartz crystals), distorted, or so intergrown with adjacent crystals of quartz or other minerals as to only show part of this shape, or to lack obvious crystal faces altogether and appear massive. Well-formed crystals typically form in a 'bed' that has unconstrained growth into a void; usually the crystals are attached at the other end to a matrix and only one termination pyramid is present. However, doubly terminated crystals do occur where they develop freely without attachment, for instance within gypsum. A quartz geode is such a situation where the void is approximately spherical in shape, lined with a bed of crystals pointing inward.\n\nα-quartz crystallizes in the trigonal crystal system, space group \"P\"321 or \"P\"321 depending on the chirality. β-quartz belongs to the hexagonal system, space group \"P\"622 and \"P\"622, respectively. These space groups are truly chiral (they each belong to the 11 enantiomorphous pairs). Both α-quartz and β-quartz are examples of chiral crystal structures composed of achiral building blocks (SiO tetrahedra in the present case). The transformation between α- and β-quartz only involves a comparatively minor rotation of the tetrahedra with respect to one another, without change in the way they are linked.\n\nAlthough many of the varietal names historically arose from the color of the mineral, current scientific naming schemes refer primarily to the microstructure of the mineral. Color is a secondary identifier for the cryptocrystalline minerals, although it is a primary identifier for the macrocrystalline varieties.\n\nPure quartz, traditionally called rock crystal or clear quartz, is colorless and transparent or translucent, and has often been used for hardstone carvings, such as the Lothair Crystal. Common colored varieties include citrine, rose quartz, amethyst, smoky quartz, milky quartz, and others.\n\nThe most important distinction between types of quartz is that of \"macrocrystalline\" (individual crystals visible to the unaided eye) and the microcrystalline or cryptocrystalline varieties (aggregates of crystals visible only under high magnification). The cryptocrystalline varieties are either translucent or mostly opaque, while the transparent varieties tend to be macrocrystalline. Chalcedony is a cryptocrystalline form of silica consisting of fine intergrowths of both quartz, and its monoclinic polymorph moganite. Other opaque gemstone varieties of quartz, or mixed rocks including quartz, often including contrasting bands or patterns of color, are agate, carnelian or sard, onyx, heliotrope, and jasper.\n\nAmethyst is a form of quartz that ranges from a bright to dark or dull purple color. The world's largest deposits of amethysts can be found in Brazil, Mexico, Uruguay, Russia, France, Namibia and Morocco. Sometimes amethyst and citrine are found growing in the same crystal. It is then referred to as ametrine. An amethyst is formed when there is iron in the area where it was formed.\n\nBlue quartz contains inclusions of fibrous magnesio-riebeckite or crocidolite.\n\nInclusions of the mineral dumortierite within quartz pieces often result in silky-appearing splotches with a blue hue, shades giving off purple and/or grey colors additionally being found. \"Dumortierite quartz\" (sometimes called \"blue quartz\") will sometimes feature contrasting light and dark color zones across the material. Interest in the certain quality forms of blue quartz as a collectible gemstone particularly arises in India and in the United States.\n\nCitrine is a variety of quartz whose color ranges from a pale yellow to brown due to ferric impurities. Natural citrines are rare; most commercial citrines are heat-treated amethysts or smoky quartzes. However, a heat-treated amethyst will have small lines in the crystal, as opposed to a natural citrine's cloudy or smokey appearance. It is nearly impossible to differentiate between cut citrine and yellow topaz visually, but they differ in hardness. Brazil is the leading producer of citrine, with much of its production coming from the state of Rio Grande do Sul. The name is derived from the Latin word \"citrina\" which means \"yellow\" and is also the origin of the word \"citron\". Sometimes citrine and amethyst can be found together in the same crystal, which is then referred to as ametrine. Citrine has been referred to as the \"merchant's stone\" or \"money stone\", due to a superstition that it would bring prosperity.\n\nCitrine was first appreciated as a golden-yellow gemstone in Greece between 300 and 150 BC, during the Hellenistic Age. The yellow quartz was used prior to that to decorate jewelry and tools but it was not highly sought-after.\n\nMilk quartz or milky quartz is the most common variety of crystalline quartz. The white color is caused by minute fluid inclusions of gas, liquid, or both, trapped during crystal formation, making it of little value for optical and quality gemstone applications.\n\nRose quartz is a type of quartz which exhibits a pale pink to rose red hue. The color is usually considered as due to trace amounts of titanium, iron, or manganese, in the material. Some rose quartz contains microscopic rutile needles which produces an asterism in transmitted light. Recent X-ray diffraction studies suggest that the color is due to thin microscopic fibers of possibly dumortierite within the quartz.\n\nAdditionally, there is a rare type of pink quartz (also frequently called crystalline rose quartz) with color that is thought to be caused by trace amounts of phosphate or aluminium. The color in crystals is apparently photosensitive and subject to fading. The first crystals were found in a pegmatite found near Rumford, Maine, USA and in Minas Gerais, Brazil.\n\nSmoky quartz is a gray, translucent version of quartz. It ranges in clarity from almost complete transparency to a brownish-gray crystal that is almost opaque. Some can also be black. The translucency results from natural irradiation creating free silicon within the crystal.\n\nPrasiolite, also known as \"vermarine\", is a variety of quartz that is green in color. Since 1950, almost all natural prasiolite has come from a small Brazilian mine, but it is also seen in Lower Silesia in Poland. Naturally occurring prasiolite is also found in the Thunder Bay area of Canada. It is a rare mineral in nature; most green quartz is heat-treated amethyst.\n\nNot all varieties of quartz are naturally occurring. Some clear quartz crystals can be treated using heat or gamma-irradiation to induce color where it would not otherwise have occurred naturally. Susceptibility to such treatments depends on the location from which the quartz was mined.\n\nPrasiolite, an olive colored material, is produced by heat treatment; natural prasiolite has also been observed in Lower Silesia in Poland. Although citrine occurs naturally, the majority is the result of heat-treated amethyst. Carnelian is widely heat-treated to deepen its color.\n\nBecause natural quartz is often twinned, synthetic quartz is produced for use in industry. Large, flawless, single crystals are synthesized in an autoclave via the hydrothermal process; emeralds are also synthesized in this fashion.\n\nLike other crystals, quartz may be coated with metal vapors to give it an attractive sheen.\n\nQuartz is a defining constituent of granite and other felsic igneous rocks. It is very common in sedimentary rocks such as sandstone and shale. It is a common constituent of schist, gneiss, quartzite and other metamorphic rocks. Quartz has the lowest potential for weathering in the Goldich dissolution series and consequently it is very common as a residual mineral in stream sediments and residual soils.\n\nWhile the majority of quartz crystallizes from molten magma, much quartz also chemically precipitates from hot hydrothermal veins as gangue, sometimes with ore minerals like gold, silver and copper. Large crystals of quartz are found in magmatic pegmatites. Well-formed crystals may reach several meters in length and weigh hundreds of kilograms.\n\nNaturally occurring quartz crystals of extremely high purity, necessary for the crucibles and other equipment used for growing silicon wafers in the semiconductor industry, are expensive and rare. A major mining location for high purity quartz is the Spruce Pine Gem Mine in Spruce Pine, North Carolina, United States. Quartz may also be found in Caldoveiro Peak, in Asturias, Spain.\n\nThe largest documented single crystal of quartz was found near Itapore, Goiaz, Brazil; it measured approximately 6.1×1.5×1.5 m and weighed more than 44 tonnes.\n\nTridymite and cristobalite are high-temperature polymorphs of SiO that occur in high-silica volcanic rocks. Coesite is a denser polymorph of SiO found in some meteorite impact sites and in metamorphic rocks formed at pressures greater than those typical of the Earth's crust. Stishovite is a yet denser and higher-pressure polymorph of SiO found in some meteorite impact sites. Lechatelierite is an amorphous silica glass SiO which is formed by lightning strikes in quartz sand.\n\nThe word \"quartz\" comes from the German , which is of Slavic origin (Czech miners called it \"křemen\"). Other sources attribute the word's origin to the Saxon word \"Querkluftertz\", meaning \"cross-vein ore\".\n\nQuartz is the most common material identified as the mystical substance maban in Australian Aboriginal mythology. It is found regularly in passage tomb cemeteries in Europe in a burial context, such as Newgrange or Carrowmore in Ireland. The Irish word for quartz is \"grianchloch\", which means 'sunstone'. Quartz was also used in Prehistoric Ireland, as well as many other countries, for stone tools; both vein quartz and rock crystal were knapped as part of the lithic technology of the prehistoric peoples.\n\nWhile jade has been since earliest times the most prized semi-precious stone for carving in East Asia and Pre-Columbian America, in Europe and the Middle East the different varieties of quartz were the most commonly used for the various types of jewelry and hardstone carving, including engraved gems and cameo gems, rock crystal vases, and extravagant vessels. The tradition continued to produce objects that were very highly valued until the mid-19th century, when it largely fell from fashion except in jewelry. Cameo technique exploits the bands of color in onyx and other varieties.\n\nRoman naturalist Pliny the Elder believed quartz to be water ice, permanently frozen after great lengths of time. (The word \"crystal\" comes from the Greek word \"κρύσταλλος\", \"ice\".) He supported this idea by saying that quartz is found near glaciers in the Alps, but not on volcanic mountains, and that large quartz crystals were fashioned into spheres to cool the hands. This idea persisted until at least the 17th century. He also knew of the ability of quartz to split light into a spectrum.\n\nIn the 17th century, Nicolas Steno's study of quartz paved the way for modern crystallography. He discovered that regardless of a quartz crystal's size or shape, its long prism faces always joined at a perfect 60° angle.\n\nQuartz's piezoelectric properties were discovered by Jacques and Pierre Curie in 1880. The quartz oscillator or resonator was first developed by Walter Guyton Cady in 1921. George Washington Pierce designed and patented quartz crystal oscillators in 1923. Warren Marrison created the first quartz oscillator clock based on the work of Cady and Pierce in 1927.\n\nEfforts to synthesize quartz began in the mid nineteenth century as scientists attempted to create minerals under laboratory conditions that mimicked the conditions in which the minerals formed in nature: German geologist Karl Emil von Schafhäutl (1803–1890) was the first person to synthesize quartz when in 1845 he created microscopic quartz crystals in a pressure cooker. However, the quality and size of the crystals that were produced by these early efforts were poor.\nBy the 1930s, the electronics industry had become dependent on quartz crystals. The only source of suitable crystals was Brazil; however, World War II disrupted the supplies from Brazil, so nations attempted to synthesize quartz on a commercial scale. German mineralogist Richard Nacken (1884–1971) achieved some success during the 1930s and 1940s. After the war, many laboratories attempted to grow large quartz crystals. In the United States, the U.S. Army Signal Corps contracted with Bell Laboratories and with the Brush Development Company of Cleveland, Ohio to synthesize crystals following Nacken's lead. (Prior to World War II, Brush Development produced piezoelectric crystals for record players.) By 1948, Brush Development had grown crystals that were 1.5 inches (3.8 cm) in diameter, the largest to date. By the 1950s, hydrothermal synthesis techniques were producing synthetic quartz crystals on an industrial scale, and today virtually all the quartz crystal used in the modern electronics industry is synthetic.\n\nSome types of quartz crystals have piezoelectric properties; they develop an electric potential upon the application of mechanical stress. An early use of this property of quartz crystals was in phonograph pickups. One of the most common piezoelectric uses of quartz today is as a crystal oscillator. The quartz clock is a familiar device using the mineral. The resonant frequency of a quartz crystal oscillator is changed by mechanically loading it, and this principle is used for very accurate measurements of very small mass changes in the quartz crystal microbalance and in thin-film thickness monitors.\n\n"}
{"id": "49905070", "url": "https://en.wikipedia.org/wiki?curid=49905070", "title": "Radioactive source", "text": "Radioactive source\n\nA radioactive source is a known quantity of a radionuclide which emits ionizing radiation; typically one or more of the radiation types gamma rays, alpha particles, beta particles, and neutron radiation.\n\nSources can be used for irradiation, where the radiation performs a significant ionising function on a target material, or as a radiation metrology source, which is used for the calibration of radiometric process and radiation protection instrumentation. They are also used for industrial process measurements, such as thickness gauging in the paper and steel industries. Sources can be sealed in a container (highly penetrating radiation) or deposited on a surface (weakly penetrating radiation), or they can be in a fluid. \nAs an irradiation source they are used in medicine for radiation therapy and in industry for such as industrial radiography, food irradiation, sterilization, vermin disinfestation, and irradiation crosslinking of PVC. \n\nRadionuclides are chosen according to the type and character of the radiation they emit, intensity of emission, and the half-life of their decay. Common source radionuclides include cobalt-60, iridium-192, and strontium-90. The SI measurement quantity of source activity is the Becquerel, though the historical unit Curies is still in partial use, such as in the USA, despite the USA NIST strongly advising the use of the SI unit. The SI unit for health purposes is mandatory in the EU.\n\nAn irradiation source typically lasts for between 5 and 15 years before its activity drops below useful levels. However sources with long half-life radionuclides when utilised as calibration sources can be used for much longer. \nMany radioactive sources are sealed, meaning they are permanently either completely contained in a capsule or firmly bonded solid to a surface. Capsules are usually made of stainless steel, titanium, platinum or another inert metal. The use of sealed sources removes \"almost\" all risk of dispersion of radioactive material into the environment due to mishandling, but the container is not intended to attenuate radiation, so further shielding is required for radiation protection. Sealed sources are used in almost all applications where the source does not need to be chemically or physically included in a liquid or gas.\n\nSealed sources are categorised by the IAEA according to their activity in relation to a minimum dangerous source (where a dangerous source is one that could cause significant injury to humans). The ratio used is A/D, where A is the activity of the source and D is the minimum dangerous activity.\n\nNote that sources with sufficiently low radioactive output (such as those used in Smoke detectors) as to not cause harm to humans are not categorised.\n\nCalibration sources are used primarily for the calibration of radiometric instrumentation, which is used on process monitoring or in radiological protection. \n\nCapsule sources, where the radiation effectively emits from a point, are used for beta, gamma and X-ray instrument calibration. High level sources are normally used in a calibration cell: a room with thick walls to protect the operator and the provision of remote operation of the source exposure. \n\nThe plate source is in common use for the calibration of radioactive contamination instruments. This has a known amount of radioactive material fixed to its surface, such as an alpha and/or beta emitter, to allow the calibration of large area radiation detectors used for contamination surveys and personnel monitoring. Such measurements are typically counts per unit time received by the detector, such as counts per minute or counts per second. \n\nUnlike the capsule source, the plate source emitting material must be on the surface to prevent attenuation by a container or self-shielding due to the material itself. This is particularly important with alpha particles which are easily stopped by a small mass. The Bragg curve shows the attenuation effect in free air.\n\nUnsealed sources are sources that are not in a permanently sealed container, and are used extensively for medical purposes. They are used when the source needs to be dissolved in a liquid for injection into a patient or ingestion by the patient. Unsealed sources are also used in industry in a similar manner for leak detection as a Radioactive tracer.\n\nDisposal of expired radioactive sources presents similar challenges to the disposal of other nuclear waste, although to a lesser degree. Spent low level sources will sometimes be sufficiently inactive that they are suitable for disposal via normal waste disposal methods — usually landfill. Other disposal methods are similar to those for higher-level radioactive waste, using various depths of borehole depending on the activity of the waste.\n\nA notorious incident of neglect in disposing of a high level source was the Goiânia accident, which resulted in several fatalities.\n\n"}
{"id": "5047118", "url": "https://en.wikipedia.org/wiki?curid=5047118", "title": "Range (particle radiation)", "text": "Range (particle radiation)\n\nIn passing through matter, charged particles ionize and thus lose energy in many steps, until their energy is (almost) zero. The distance to this point is called the range of the particle. The range depends on the type of particle, on its initial energy and on the material through which it passes.\n\nFor example, if the ionising particle passing through the material is a positive ion like an alpha particle or proton, it will collide with atomic electrons in the material via Coulombic interaction. Since the mass of the proton or alpha particle is much greater than that of the electron, there will be no significant deviation from the radiation's incident path and very little kinetic energy will be lost in each collision. As such, it will take many successive collisions for such heavy ionising radiation to come to a halt within the stopping medium or material. Maximum energy loss will take place in a head-on collision with an electron.\n\nSince large angle scattering is rare for positive ions, a range may be well defined for that radiation, depending on its energy and charge, as well as the ionisation energy of the stopping medium. Since the nature of such interactions is statistical, the number of collisions required to bring a radiation particle to rest within the medium will vary slightly with each particle (i.e., some may travel further and undergo fewer collisions than others). Hence, there will be a small variation in the range, known as straggling.\n\nThe energy loss per unit distance (and hence, the density of ionization), or stopping power also depends on the type and energy of the particle and on the material. Usually, the energy loss per unit distance increases while the particle slows down. The curve describing this fact is called the Bragg curve. Shortly before the end, the energy loss passes through a maximum, the Bragg Peak, and then drops to zero (see the figures in Bragg Peak and in stopping power). This fact is of great practical importance for radiation therapy.\n\nThe range of alpha particles in ambient air amounts to only several centimeters; this type of radiation can therefore be stopped by a sheet of paper. Although beta particles scatter much more than alpha particles, a range can still be defined; it frequently amounts to several hundred centimeters of air.\n\nThe mean range can be calculated by integrating the inverse stopping power over energy.\n\nThe range of a heavy charged particle is approximately proportional to the mass of the particle and the inverse of the density of the medium, and is a function of the initial velocity of the particle.\n\n\n"}
{"id": "4423452", "url": "https://en.wikipedia.org/wiki?curid=4423452", "title": "S10 NBC Respirator", "text": "S10 NBC Respirator\n\nThe S10 NBC Respirator is a military gas mask that was formerly used within all branches of the British Armed Forces. Following the mask's replacement by the General Service Respirator in 2011, the S10 is now widely available to the public on the army surplus market.\n\nThe S10 was introduced in 1986 as a replacement for the S6 NBC Respirator in service from the 1960s, and is manufactured by Avon Rubber. S10s were issued to British nationals in Saudi Arabia during the 1991 Gulf War and were marked by a blue coloured PSM (Primary Speech Module) fitting or a blue painted spot on the forehead; although meant to be returned to the authorities after the Iraqi threat (from Scud missile attacks, potentially carrying chemical warheads) had expired, large numbers were unaccounted for in the wake of the conflict.\n\nThe S10 respirator was originally due to be replaced by the new General Service Respirator (GSR) in 2007, although replacement efforts officially began in 2011 and all S10 Respirators have since been replaced with the GSR. All issued filters for the S10 have expired, the last of which expired in 2014. No new S10 Respirators will be bought. \n\nTwo filter canisters are issued for the S10, as with the S6; a light pressed metal type for riot control situations or training (marked with a painted red stripe or red tape), and a heavier plastic encased type for protection against NBC agents. The latter have a maximum shelf-life of 10 years, and the mask itself (the facepiece) of 20 years. The mask has many features including a fail-safe drinking device, a Secondary Speech Transmitter (SST) on the side which can accept special microphones, and corrective lenses can be fitted to the eyepieces. The mask also contains an inner mask to decrease fogging on the lenses.\n\nVariants include:\n\n"}
{"id": "26829036", "url": "https://en.wikipedia.org/wiki?curid=26829036", "title": "Sanitation harvest", "text": "Sanitation harvest\n\nIn forestry and silviculture, a sanitation harvest or sanitation cutting is a harvest of trees for the purpose of removing insects or diseases from a stand of trees. Sanitation harvesting is used to prevent the diseases or pests from spreading to other nearby trees. It is a form of intermediate management and is used in order to improve an already existing stand of trees.\n\n"}
{"id": "1029711", "url": "https://en.wikipedia.org/wiki?curid=1029711", "title": "Solar zenith angle", "text": "Solar zenith angle\n\nThe solar zenith angle is the angle between the zenith and the centre of the Sun's disc. The solar elevation angle is the altitude of the Sun, the angle between the horizon and the centre of the Sun's disc. Since these two angles are complementary, the cosine of either one of them equals the sine of the other. They can both be calculated with the same formula, using results from spherical trigonometry.\n\nwhere\n\nThe calculated values are approximations due to the distinction between common/geodetic latitude and geocentric latitude. However, the two values differ by less than 12 minutes of arc, which is less than the apparent angular radius of the sun.\n\nThe formula also neglects the effect of atmospheric refraction.\n\nSunset and sunrise occur (approximately) when the zenith angle is 90°, where the hour angle \"h\" satisfies\n\nPrecise times of sunset and sunrise occur when the upper limb of the Sun appears, as refracted by the atmosphere, to be on the horizon.\n\nA weighted daily average zenith angle, used in computing the local albedo of the Earth, is given by\nwhere \"Q\" is the instantaneous insolation.\n\nFor example, the solar elevation angle is :\n\nAn exact calculation is given in position of the Sun. Other approximations exist elsewhere.\n\n"}
{"id": "502209", "url": "https://en.wikipedia.org/wiki?curid=502209", "title": "Soviet submarine K-27", "text": "Soviet submarine K-27\n\nK-27 was the only submarine of Project 645 in the Soviet Navy. Project 645 was not assigned a NATO reporting name. That project produced one test model nuclear submarine, which incorporated a pair of experimental VT-1 nuclear reactors that used a liquid-metal coolant (Lead-bismuth eutectic), placed into the modified hull of a November class submarine (Project 627A).\n\nThe keel of \"K-27\" was laid down on 15 June 1958 at Severodvinsk Shipyard No. 402. It was launched on 1 April 1962, and went into service as an experimental \"attack submarine\" on 30 October 1963. \"K-27\" was officially commissioned into the Soviet Northern Fleet on 7 September 1965. \"K-27\" was assigned to the 17th submarine division, headquartered at Gremikha).\n\nThe nuclear reactors of \"K-27\" were troublesome from their first criticality, but the \"K-27\" was able to engage in test operations for about five years. On 24 May 1968, the power output of one of her reactors suddenly dropped sharply; radioactive gases were released into her engine room; and the radiation levels throughout \"K-27\" increased dangerously – by 1.5 grays per hour. This radiation consisted mostly of gamma rays and thermal neutrons, with some alpha radiation and beta radiation in addition – generated by the released radioactive gasses such as xenon and krypton in her reactor compartment.\n\nThe training of the crew by the Soviet Navy had been inadequate, and these sailors did not recognize that their nuclear reactor had suffered from extensive fuel element failures. By the time they gave up their attempts to repair the reactor at sea, nine of the crewmen had accumulated fatal radioactive exposures.\n\nAbout one-fifth of the reactor core had experienced inadequate cooling caused by uneven coolant flows. Hot spots in the reactor had ruptured, releasing nuclear fuel and nuclear fission products into the liquid-metal coolant, which circulated them throughout her reactor compartment.\n\n\"K-27\" was laid up in Gremikha Bay starting on 20 June 1968. The cooling-off of the reactors and various experimental projects were carried out aboard the submarine through 1973. These included the successful restarting of the starboard reactor up to 40% of maximal power production. Plans were considered to slice off the reactor compartment and replace it with a new one containing standard VM-A water-cooled reactors. The rebuilding or replacement of the nuclear reactor was considered to be too expensive, and also to be inappropriate because more modern nuclear submarines had already entered service in the Soviet Navy.\n\n\"K-27\" was officially decommissioned on 1 February 1979 and her reactor compartment was filled with a special solidifying mixture of furfuryl alcohol and bitumen during the summer of 1981 to seal the compartment to avoid pollution of the ocean with radioactive products. This work was performed by the Severodvinsk shipyard No. 893 \"Zvezdochka\".\n\nThen \"K-27\" was towed to a special training area in the eastern Kara Sea, and it was scuttled there on 6 September 1982 near the location 72°31'28\"N., 55°30'09\"E. off the northeastern coast of Novaya Zemlya (at Stepovoy Bay), in a fjord at a depth of just 33 meters (108 feet). It was necessary for a naval salvage tug to ram the stern of \"K-27\" to pierce her aft ballast tanks and sink it, because \"K-27\"s bow had impacted the seafloor while her stern was still afloat. This scuttling was performed contrary to the International Atomic Energy Agency's requirement that nuclear-powered submarines and surface ships must be scuttled at depths not less than 3,000 meters.\n\nThe last scientific expedition of the \"Russian Ministry of Emergencies\" to the Kara Sea examined the site of the scuttling in September 2006. Numerous samples of the seawater, the seafloor, and the sealife were gathered and then analyzed. The final report stated that the radiation levels of the area were stable.\n\nLessons in nuclear submarine construction and safety learned from Projekt 645 were applied in Projects 705 and 705K – that produced the Soviet Alfa class submarines. These were equipped with similar liquid-metal-cooled reactors.\n\nIn September 2012 it was reported that the submarine needed to be lifted from her shallow bed in the Kara Sea. The vessel was said to be a \"nuclear time bomb\", and that the rusting and decaying vessel may be reaching a critical level leading to an \"uncontrolled chain reaction\". Although a joint Russian and Norwegian mission in 2012 did not find alarming levels of radioactivity in the water and soil surrounding the submarine, an urgent consideration pertains to the dismantling of the nuclear reactors should the submarine be raised. Because the reactors were cooled by liquid metals, the nuclear rods became fused with the coolant when the reactors were stopped and conventional methods cannot be used for disassembling the reactors. However, France's Commissariat à l'énergie atomique et aux énergies alternatives designed and donated special equipment for a dedicated dry-dock (SD-10) in Gremikha, which was used to dismantle the Alfa-class submarines that shared this design feature. However, since the last Alfa reactor was dismantled in 2011, this equipment is at risk.\n\nIn 2017, plans were again mooted to raise the submarine, by 2022. The Krylov State Research Center of St Petersburg announced that it was working on plans for a catamaran floating dock, capable of such heavy lifts from the seabed.\n\n\n"}
{"id": "2473210", "url": "https://en.wikipedia.org/wiki?curid=2473210", "title": "Sterile neutrino", "text": "Sterile neutrino\n\nSterile neutrinos (or inert neutrinos) are hypothetical particles (neutral leptons – neutrinos) that interact only via gravity and do not interact via any of the fundamental interactions of the Standard Model. The term \"sterile neutrino\" is used to distinguish them from the known \"active neutrinos\" in the Standard Model, which are charged under the weak interaction.\n\nThis term usually refers to neutrinos with right-handed chirality (see right-handed neutrino), which may be added to the Standard Model. Occasionally it is used in a general sense for any neutral fermion, instead of the more cautiously vague name neutral heavy leptons (NHLs) or heavy neutral leptons (HNLs).\n\nThe existence of right-handed neutrinos is theoretically well-motivated, as all other known fermions have been observed with both left and right chirality, and they can explain the observed active neutrino masses in a natural way. The mass of the right-handed neutrinos themselves is unknown and could have any value between 10 GeV and less than 1 eV.\n\nThe number of sterile neutrino types (should they exist) is not yet theoretically established. This is in contrast to the number of active neutrino types, which has to equal that of charged leptons and quark generations to ensure the anomaly freedom of the electroweak interaction.\n\nThe search for sterile neutrinos is an active area of particle physics. If they exist and their mass is smaller than the energies of particles in the experiment, they can be produced in the laboratory, either by mixing between active and sterile neutrinos or in high energy particle collisions. If they are heavier, the only directly observable consequence of their existence would be the observed active neutrino masses. They may, however, be responsible for a number of unexplained phenomena in physical cosmology and astrophysics, including dark matter, baryogenesis or dark radiation. In May 2018, physicists of the MiniBooNE experiment reported a stronger neutrino oscillation signal than expected, a possible hint of sterile neutrinos.\n\nExperimental results show that all produced and observed neutrinos have left-handed helicities (spin antiparallel to momentum), and all antineutrinos have right-handed helicities, within the margin of error. In the massless limit, it means that only one of two possible chiralities is observed for either particle. These are the only helicities (and chiralities) included in the Standard Model of particle interactions.\n\nRecent experiments such as neutrino oscillation, however, have shown that neutrinos have a non-zero mass, which is not predicted by the Standard Model and suggests new, unknown physics. This unexpected mass explains neutrinos with right-handed helicity and antineutrinos with left-handed helicity: Since they do not move at the speed of light, their helicity is not relativistic invariant (it is possible to move faster than them and observe the opposite helicity). Yet all neutrinos have been observed with left-handed \"chirality\", and all antineutrinos right-handed. Chirality is a fundamental property of particles and \"is\" relativistic invariant: It is the same regardless of the particle's speed and mass in every inertial reference frame. Although note that a particle with mass that starts out left-handed can develop a right-handed component as it travels – chirality is \"not\" conserved in the propagation of a free particle.\n\nThe question, thus, remains: Do neutrinos and antineutrinos differ only in their chirality? Or do exotic right-handed neutrinos and left-handed antineutrinos exist as separate particles from the common left-handed neutrinos and right-handed antineutrinos?\n\nSuch particles would belong to a singlet representation with respect to the strong interaction and the weak interaction, having zero electric charge, zero weak hypercharge, zero weak isospin, and, as with the other leptons, no color, although they do have a B-L of −1. If the standard model is embedded in a hypothetical SO(10) grand unified theory, they can be assigned an X charge of −5. The left-handed anti-neutrino has a B-L of +1 and an X charge of +5.\n\nDue to the lack of electric charge, hypercharge, and color, sterile neutrinos would not interact electromagnetically, weakly, or strongly, making them extremely difficult to detect. They have Yukawa interactions with ordinary leptons and Higgs bosons, which via the Higgs mechanism lead to mixing with ordinary neutrinos. \nIn experiments involving energies larger than their mass they would participate in all processes in which ordinary neutrinos take part, but with a quantum mechanical probability that is suppressed by the small mixing angle. That makes it possible to produce them in experiments if they are light enough. \nThey would also interact gravitationally due to their mass, however, and if they are heavy enough, they could explain cold dark matter or warm dark matter. In some grand unification theories, such as SO(10), they also interact via gauge interactions which are extremely suppressed at ordinary energies because their gauge boson is extremely massive. They do not appear at all in some other GUTs, such as the Georgi–Glashow model (i.e. all its SU(5) charges or quantum numbers are zero).\n\nAll particles are initially massless under the Standard Model, since there are no Dirac mass terms in the Standard Model's Lagrangian. The only mass terms are generated by the Higgs mechanism, which produces non-zero Yukawa couplings between the left-handed components of fermions, the Higgs field, and their right-handed components. This occurs when the SU(2) doublet Higgs field formula_1 acquires its non-zero vacuum expectation value, formula_2, spontaneously breaking its SU(2) × U(1) symmetry, and thus yielding non-zero Yukawa couplings:\n\nSuch is the case for charged leptons, like the electron; but within the standard model, the right-handed neutrino does not exist, so even with a Yukawa coupling neutrinos remain massless. In other words, there are no mass terms for neutrinos under the Standard Model: the model only contains a left-handed neutrino and its antiparticle, a right-handed antineutrino, for each generation, produced in weak eigenstates during weak interactions. (See neutrino masses in the Standard Model for a detailed explanation.)\n\nIn the seesaw mechanism, one eigenvector of the neutrino mass matrix, which includes sterile neutrinos, is predicted to be significantly heavier than the other.\n\nA sterile neutrino would have the same weak hypercharge, weak isospin, and mass as its antiparticle. For any charged particle, for example the electron, this is not the case: its antiparticle, the positron, has opposite electric charge, among other opposite charges. Similarly, an up quark has a charge of + and (for example) a color charge of red, while its antiparticle has an electric charge of − and a color charge of anti-red.\n\nSterile neutrinos allow the introduction of a Dirac mass term as usual. This can yield the observed neutrino mass, but it requires that the strength of the Yukawa coupling be much weaker for the electron neutrino than the electron, without explanation. Similar problems (although less severe) are observed in the quark sector, where the top and bottom masses differ by a factor of 40.\n\nUnlike for the left-handed neutrino, a Majorana mass term can be added for a sterile neutrino without violating local symmetries (weak isospin and weak hypercharge) since it has no weak charge. However, this would still violate total lepton number.\n\nIt is possible to include both Dirac and Majorana terms: this is done in the seesaw mechanism (below). In addition to satisfying the Majorana equation, if the neutrino were also its own antiparticle, then it would be the first Majorana fermion. In that case, it could annihilate with another neutrino, allowing neutrinoless double beta decay. The other case is that it is a Dirac fermion, which is not its own antiparticle.\n\nTo put this in mathematical terms, we have to make use of the transformation properties of particles. For free fields, a Majorana field is defined as an eigenstate of charge conjugation. However, neutrinos interact only via the weak interactions, which are not invariant under charge conjugation (C), so an interacting Majorana neutrino cannot be an eigenstate of C. The generalized definition is: \"a Majorana neutrino field is an eigenstate of the CP transformation\". Consequently, Majorana and Dirac neutrinos would behave differently under CP transformations (actually Lorentz and CPT transformations). Also, a massive Dirac neutrino would have nonzero magnetic and electric dipole moments, whereas a Majorana neutrino would not. However, the Majorana and Dirac neutrinos are different only if their rest mass is not zero. For Dirac neutrinos, the dipole moments are proportional to mass and would vanish for a massless particle. Both Majorana and Dirac mass terms however can appear in the mass Lagrangian.\n\nIn addition to the left-handed neutrino, which couples to its family charged lepton in weak charged currents, if there is also a right-handed sterile neutrino partner (a weak isosinglet with zero charge) then it is possible to add a Majorana mass term without violating electroweak symmetry. Both neutrinos have mass and handedness is no longer preserved (thus \"left or right-handed neutrino\" means that the state is mostly left or right-handed). To get the neutrino mass eigenstates, we have to diagonalize the general mass matrix formula_4:\n\nwhere formula_6 is big and formula_7 is of intermediate size terms.\n\nApart from empirical evidence, there is also a theoretical justification for the seesaw mechanism in various extensions to the Standard Model. Both Grand Unification Theories (GUTs) and left-right symmetrical models predict the following relation:\n\nAccording to GUTs and left-right models, the right-handed neutrino is extremely heavy: , while the smaller eigenvalue is approximately equal to\n\nThis is the seesaw mechanism: as the sterile right-handed neutrino gets heavier, the normal left-handed neutrino gets lighter. The left-handed neutrino is a mixture of two Majorana neutrinos, and this mixing process is how sterile neutrino mass is generated.\n\nThe production and decay of sterile neutrinos could happen through the mixing with virtual (\"off mass shell\") neutrinos. There were several experiments set up to discover or observe NHLs, for example the NuTeV (E815) experiment at Fermilab or LEP-l3 at CERN. They all led to establishing limits to observation, rather than actual observation of those particles. If they are indeed a constituent of dark matter, sensitive X-ray detectors would be needed to observe the radiation emitted by their decays.\n\nSterile neutrinos may mix with ordinary neutrinos via a Dirac mass after electroweak symmetry breaking, in analogy to quarks and charged leptons.\nSterile neutrinos and (in more-complicated models) ordinary neutrinos may also have Majorana masses. In the type 1 seesaw mechanism both Dirac and Majorana masses are used to drive ordinary neutrino masses down and make the sterile neutrinos much heavier than the Standard Model's interacting neutrinos. In some models the heavy neutrinos can be as heavy as the GUT scale (). In other models they could be lighter than the weak gauge bosons W and Z as in the so-called νMSM model where their masses are between GeV and keV. A light (with the mass ) sterile neutrino was suggested as a possible explanation of the results of the Liquid Scintillator Neutrino Detector experiment.\nOn 11 April 2007, researchers at the MiniBooNE experiment at Fermilab announced that they had not found any evidence supporting the existence of such a sterile neutrino. More-recent results and analysis have provided some support for the existence of the sterile neutrino.\n\nTwo separate detectors near a nuclear reactor in France found 3% of anti-neutrinos missing. They suggested the existence of a fourth neutrino with a mass of 0.7 keV. Sterile neutrinos are also candidates for dark radiation. Daya Bay has also searched for a light sterile neutrino and excluded some mass regions. Daya Bay Collaboration measured the anti-neutrino energy spectrum, and found that anti-neutrinos at an energy of around 5 MeV are in excess relative to theoretical expectations. It also recorded 6% missing anti-neutrinos. This could suggest that sterile neutrinos exist or that our understanding of neutrinos is not complete.\n\nThe number of neutrinos and the masses of the particles can have large-scale effects that shape the appearance of the cosmic microwave background. The total number of neutrino species, for instance, affects the rate at which the cosmos expanded in its earliest epochs: more neutrinos means a faster expansion. The Planck Satellite 2013 data release is compatible with the existence of a sterile neutrino. The implied mass range is from 0–3 eV. In 2016, scientists at the IceCube Neutrino Observatory did not find any evidence for the sterile neutrino. However, in May 2018, physicists of the MiniBooNE experiment reported a stronger neutrino oscillation signal than expected, a possible hint of sterile neutrinos.\n\n\n"}
{"id": "17694960", "url": "https://en.wikipedia.org/wiki?curid=17694960", "title": "TACA Flight 390", "text": "TACA Flight 390\n\nTACA Flight 390 was a scheduled flight on May 30, 2008, by TACA Airlines from San Salvador, El Salvador, to Miami, Florida, United States, with intermediate stops at Tegucigalpa and San Pedro Sula in Honduras. In this hull loss/fatalities accident, the Airbus A320-233 (registration EI-TAF, c/n 1374) overran the runway after landing at Tegucigalpa's Toncontín International Airport and rolled out into a street, crashing into an embankment and smashing several cars in the process.\n\nThe flight crew included Salvadorans Captain Cesare Edoardo D'Antonio Mena and First Officer Juan Rodolfo Artero Arevalo. All cabin crew members operating on the flight were Hondurans. The passengers consisted of:\n\nA list of passengers was provided in the fifth press release on the crash from TACA Airlines. This list was in the Spanish and English sections.\n\nFive people died as a result of the accident, including Captain D'Antonio. The deceased passengers were later confirmed as Jeanne Chantal Neele, wife of Brian Michael Fraser Neele (Brazil's ambassador to Honduras), and Nicaraguan businessman Harry Brautigam, president of the Central American Bank for Economic Integration; Brautigam died from a heart attack. Ambassador Fraser Neele sustained injuries in the crash. The former head of the Honduran armed forces, General Daniel López Carballo, was also injured. There were two fatalities on the ground, one a taxi driver, in one of three vehicles crushed on the street by the aircraft. Mario Castillo, a survivor, said that the business class passengers sustained the most serious injuries.\n\nHonduran authorities delegated the investigation of the accident to the Civil Aviation Authority of El Salvador as per the Convention on International Civil Aviation. The accident report stated that the airplane had landed with a 12-knot tailwind, 400 meters from the displaced approach end of the runway. Since this was the first intermediate stop on a long transcontinental flight, the aircraft was near its upper landing-weight limit (63.5t vs. 64.5t maximum allowable). In addition, the runway was wet, due to the passage of Tropical Storm Alma.\n\n\n"}
{"id": "44271020", "url": "https://en.wikipedia.org/wiki?curid=44271020", "title": "Togo Triangle", "text": "Togo Triangle\n\nThe Togo Triangle is an offshore market for stolen oil off the coast of Nigeria and Togo near the Niger Delta. The Triangle has been compared to an \"open-air drug market\" for trade in illegal crude oil, noted for the presence of pirates.\n"}
{"id": "49312984", "url": "https://en.wikipedia.org/wiki?curid=49312984", "title": "Twelve-angled stone", "text": "Twelve-angled stone\n\nThe twelve-angled stone is an archeological artefact in Cuzco, Peru. It was part of a stone wall of an Inca palace, and is considered to be a national heritage object. The stone is currently part of a wall of the palace of the Archbishop of Cuzco.\n\nThe twelve-angled stone is composed of a formation of diorite rocks and is recognized by its fine finishing and twelve-angled border, an example of perfectionist Incan architecture. The block is categorized as Cultural Heritage of the Nation of Peru and is located in the city of Cuzco, 1105 km from Lima. The stone is a great example of Inca knowledge in the evolution of construction. There are other stones with the same vertices but the twelve-angled stone is the most famous.\n\nAs an example of the Inca’s advanced stonework, the stone is a popular tourist attraction in Cuzco and a site of pride for many locals. The perfectly cut stone is part of a wall known as the Hatun Rumiyoc, which makes up the outside of the Archbishop’s palace.\n\n"}
{"id": "17054403", "url": "https://en.wikipedia.org/wiki?curid=17054403", "title": "Vestas V90-3MW", "text": "Vestas V90-3MW\n\nThe Vestas V90-3MW is a three bladed upwind wind turbine generator that uses pitch control and a doubly fed induction generator (50 Hz version). Its manufacturer claims to have installed over 500 units of this type globally since launch.\n\nVestas claims the turbine provides 50% more power for roughly the same weight as the V80. The V90-3MW should not be confused with the V90-2MW, which is essentially a V80-2MW with longer blades. It is produced in both an on and offshore version.\n\nThe first V90-3MW was erected in northern Germany in May 2002. 15 test turbines were deployed around the world in different climatic conditions, so that when it went into production, the V90-3MW had been tested in more sites than the V80-2MW. Following a number of gearbox problems, the V90-3MW was withdrawn for offshore sales in early 2007 before being reissued for offshore use in May 2008. Presently, the nacelles are exclusively made at the Vestas Nacelles works in Taranto, Italy. Towers and blades may come from a number of locations. Vestas produces a 3.65 MW low-wind version called the V136.\n\nTowers may weigh 160 t if 80 m tall, or 285 t at 105 m (235 t in Germany).\n\nDifferences from its predecessor model the V80 include use of an oil cooled generator, and a compact gearbox, which now butts up directly to the hub rather than through a low speed shaft. The V90-3MW can be differentiated from the V80 by the shape of the nacelle, which has a cut-out profile at the back. Other differences include:\n\nThe V90-3MW can be specified with one of five different 'noise modes'. Each mode is set in the turbine software as part of the installation, although may be changed to another mode later. Each different noise mode implies a different power curve, so that for quieter operation, some energy yield is sacrificed. Noise mitigation is managed by adjustments to the blade pitch angle.\n\nFor licensing reasons, the 60 Hz model sold in the US and Canada uses a slightly different converter system, allowing only one-way power flow through the rotor converter, rather than 2-way power flow used in the standard version. This restriction does not apply to 60 Hz models sold in other regions (i.e. Japan).\n\nThe V90-3MW is used offshore in the following wind farms:\n\n"}
{"id": "24443097", "url": "https://en.wikipedia.org/wiki?curid=24443097", "title": "Văcăreni Wind Farm", "text": "Văcăreni Wind Farm\n\nThe Văcăreni Wind Farm is a proposed wind power project in Văcăreni, Tulcea County, Romania. It will have 48 individual wind turbines with a nominal output of around 5 MW each which will deliver up to 240 MW of power, enough to power over 144,000 homes, with a capital investment required of approximately US$480 million.\n"}
{"id": "8038396", "url": "https://en.wikipedia.org/wiki?curid=8038396", "title": "West number", "text": "West number\n\nThe West number is an empirical parameter used to characterize the performance of Stirling engines and other Stirling systems. It is very similar to the Beale number where a larger number indicates higher performance; however, the West number includes temperature compensation. The West number is often used to approximate of the power output of a Stirling engine. The average value is (0.25) for a wide variety of engines, although it may range up to (0.35) , particularly for engines operating with a high temperature differential.\n\nThe West number may be defined as:\n\nwhere:\n\nWhen the Beale number is known, but the West number is not known, it is possible to calculate it. First calculate the West number at the temperatures \"T\" and \"T\" for which the Beale number is known, and then use the resulting West number to calculate output power for other temperatures.\n\nTo estimate the power output of a new engine design, nominal values are assumed for the West number, pressure, swept volume and frequency, and the power is calculated as follows:\n\nFor example, with an absolute temperature ratio of 2, the portion of the equation representing temperature correction equals 1/3. With a temperature ratio of 3, the temperature term is 1/2. This factor accounts for the difference between the West equation, and the Beale equation in which this temperature term is taken as a constant. Thus, the Beale number is typically in the range of 0.10 to 0.15, which is about 1/3 to 1/2 the value of the West number.\n\n"}
