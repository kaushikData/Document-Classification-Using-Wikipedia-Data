{"id": "44612185", "url": "https://en.wikipedia.org/wiki?curid=44612185", "title": "ABT Summer", "text": "ABT Summer\n\nMV ABT Summer was an oil tanker which was built at the South Korean shipbuilding yard of Ulsan and launched in 1974. The vessel was 344 metres in length and almost 82 metres in breadth. While under a Liberian flag, fully laden with Iranian crude and en route to Rotterdam, she sank 700 nautical miles off the Angolan coast. An unexplained explosion occurred on May 28, 1991, and the ship and its cargo began to burn. Five of the crew of thirty-two were killed in the incident, four of whom were initially reported as missing. The following day, a slick 32 km long and 7 km wide began to form. The ship continued to burn for three days before sinking on June 1. The vessel's 260,000 tonne cargo of oil was lost, leaving a visible slick on the ocean surface of approximately eighty square miles. Attempts to locate the wreck following the incident proved unsuccessful.\n\n"}
{"id": "6791136", "url": "https://en.wikipedia.org/wiki?curid=6791136", "title": "Aerodynamic levitation", "text": "Aerodynamic levitation\n\nAerodynamic levitation is the use of gas pressure to levitate materials so that they are no longer in physical contact with any container. In scientific experiments this removes contamination and nucleation issues associated with physical contact with a container.\n\nThe term aerodynamic levitation could be applied to many objects that use gas pressure to counter the force of gravity, and allow stable levitation. Helicopters and air hockey pucks are two good examples of objects that are aerodynamically levitated. However, more recently this term has also been associated with a scientific technique which uses a cone-shaped nozzle allowing stable levitation of 1-3mm diameter spherical samples without the need for active control mechanisms.\n\nThese systems allow spherical samples to be levitated by passing gas up through a diverging conical nozzle. Combining this with >200W continuous CO laser heating allows sample temperatures in excess of 3000 degrees Celsius to be achieved.\n\nWhen heating materials to these extremely high temperatures levitation in general provides two key advantages over traditional furnaces. First, contamination that would otherwise occur from a solid container is eliminated. Second, the sample can be undercooled, i.e. cooled below its normal freezing temperature without actually freezing.\n\nUndercooling, or supercooling, is the cooling of a liquid below its equilibrium freezing temperature while it remains a liquid. This can occur wherever crystal nucleation is suppressed. In levitated samples, heterogeneous nucleation is suppressed due to lack of contact with a solid surface. Levitation techniques typically allow samples to be cooled several hundred degrees Celsius below their equilibrium freezing temperatures.\n\nSince crystal nucleation is suppressed by levitation, and since it is not limited by sample conductivity (unlike electromagnetic levitation), aerodynamic levitation can be used to make glassy materials, from high temperature melts that cannot be made by standard methods. Several silica-free, aluminium oxide based glasses have been made.\n\nIn the last few years a range of in situ measurement techniques have also been developed. The following measurements can be made with varying precision:\n\nelectrical conductivity,\nviscosity,\ndensity,\nsurface tension,\nspecific heat capacity,\n\nIn situ aerodynamic levitation has also been combined with:\n\nX-ray synchrotron radiation,\nneutron scattering,\nNMR spectroscopy\n\n"}
{"id": "144242", "url": "https://en.wikipedia.org/wiki?curid=144242", "title": "Alexis Thérèse Petit", "text": "Alexis Thérèse Petit\n\nAlexis Thérèse Petit (; 2 October 1791, Vesoul, Haute-Saône – 21 June 1820, Paris) was a French physicist.\n\nPetit is known for his work on the efficiencies of air- and steam-engines, published in 1818 (\"Mémoire sur l’emploi du principe des forces vives dans le calcul des machines\"). His well-known discussions with the French physicist Sadi Carnot, founder of thermodynamics, may have stimulated Carnot in his reflexions on heat engines and thermodynamic efficiency.\n\nPetit was born in Vesoul, Haute-Saône. At the age of 10, he proved that he was already capable of taking the difficult entrance exam to France's most prestigious scientific school of the time, the École Polytechnique of Paris. He was then placed in a preparatory school where he actually served as a \"répétiteur\" to help his own classmates digest the course material. He duly entered Polytechnique at the lowest permissible age, in 1807, and graduated \"hors-rang\" in 1809 (which is to say that he clearly outranked all of his classmates).\n\nAfter graduation, Petit stayed at Polytechnique as a faculty member, first as \"répétiteur\" in Analysis and Mechanics (1809) then in Physics (1810). He taught for some time at Lycée Bonaparte. At Polytechnique, he served as a substitute (1814) for Hassenfratz whom he would replace in 1815. He thus became the second professor of physics at Polytechnique and the youngest person ever to hold that position, at the age of 23.\n\nPetit and François Arago were brothers-in-law because they married two sisters. In 1814, the two men collaborated on a paper entitled \"Mémoire sur les variations que le pouvoir réfringent d’une même substance éprouve par l’effet gradué de la chaleur\".\n\nPetit first collaborated with Pierre Louis Dulong for the competition of the Académie des sciences about refrigeration (1815). Petit is now probably best known for the surprising Dulong–Petit law concerning the specific heat capacity of metals, which both men formulated together in 1819. Petit also designed a special thermometer (using weights) to determine the thermal dilatation coefficients of several metals.\n\nPetit died from tuberculosis at the age of 28, shortly after the passing of his wife. He was succeeded by Dulong as professor of physics at the Polytechnique (1820).\n\nRoberto Piazza's 2016 paper on the Dulong–Petit law provides biographical and temperament details by contemporary and fellow physicist, Jules Jamin. “Petit had a lively intelligence, an elegant and easy speech, he seduced with an amiable look, got easily attached, and surrendered himself to his tendencies rather than governing them. He was credited with an instinctive scientific intuition, a power of premature invention, certain presages of an assured future that everyone foresaw and even desired, so great was the benevolence which he inspired. Dulong was the opposite: His language was thoughtful, his attitude serious and his appearance cold[. . . ] He worked slowly but with certainty, with a continuity and a power of will that nothing stopped, I should say with a courage that no danger could push back. In the absence of that vivacity of the mind which invents easily, but likes to rest, he had the sense of scientific exactness, the gusto for precision experiments, the talent of combining them, the patience of completing them, and the art, unknown before him, to carry them to the limits of accuracy[. . . ] Petit had more mathematical tendency, Dulong was more experimental; the first carried in the work more brilliant easiness, the second more continuity; One represented imagination, the other reason, which moderates and contains it.”\n\n"}
{"id": "45700784", "url": "https://en.wikipedia.org/wiki?curid=45700784", "title": "Appliance recycling", "text": "Appliance recycling\n\nAppliance recycling consists of dismantling waste home appliances and scrapping their parts for reuse. Recycling appliances for their original or other purposes, involves disassembly, removal of hazardous components and destruction of the end-of-life equipment to recover materials, generally by shredding, sorting and grading. The rate at which appliances are discarded has increased with technological advancement. This correlation directly leads to the question of appropriate disposal. The main types of appliances that are recycled are televisions, refrigerators, air conditioners, washing machines, and computers. When appliances are recycled, they can be looked upon as valuable resources. If disposed of improperly, appliances can become environmentally harmful and poison ecosystems.\n\nThe strength of appliance recycling legislation varies around the world.\n\nFor example recycling one refrigerator can save of foam insulation, and of energy.\n\nA key part of appliance recycling is the manual dismantling of each product. The disassembly removes hazardous components, while sorting out reusable parts. Procedures vary from one appliance to the other. The amount of hazardous components able to be removed also depends on the type of appliance. Low removal rates of hazardous components reduce the recyclability of valuable materials. Each type of appliance has its own set of characteristics and components. This makes characterization of appliances essential to sorting and separating parts. Research on appliance dismantling has become an active area, intending to help recycling reach maximum efficiency.\n\nThere is a certain process used to recover materials from appliances. Parts are generally removed in order from largest to smallest. Metals are extracted first and then plastics. Materials are sorted by either size, shape, or density. Sizing is a good means of sorting to quicken future processing. It also classifies fractions that show composition. Materials report to larger or finer fractions based on original dimension, toughness, or brittleness. Shape classification contributes to the dynamics of the material. Classification by density is important when it comes to determining the use of a material.\n\nBatteries and copper are sorted out first for quality control purposes. The materials are then compacted. Next, iron and steel (ferrous metals) are extracted using electromagnets. They are collected and made ready for sale. Then metals are separated from non-metals using eddy currents. Eddy currents are created by rapidly alternating magnetic fields, which induce metals to jump away from non-metals. Then water separation is used to sort plastics and glass from circuit boards and copper wires. Circuit boards and copper content is then sold. Plastics and glass are further compacted for reuse.\n\nAlthough appliance recycling is still quite new, countries have been making the effort to enact laws and regulations regarding the electric waste. Early addressing of waste home appliance recycling started with Japan, Switzerland, Sweden, the Netherlands, and Germany.\n\nIn 2003 Waste Electrical and Electronic Equipment Directive (WEEE) passed into European Law. It sets collection, recycling and recovery targets for all types of electrical goods.\n\nBy the 1950s and 60s Japan had already become a major producer of electric appliances. The first initiatives to recycle were launched in the 70s. Due to costs, disassembly was hardly achievable. The Home Appliance Recycling Law was enacted in 1998 and came into force in 2001, and recycling of waste electrics became a legal requirement under the Specific Household Appliance Recycling Law and the Law for Promotion of Effective Utilisation Resources. Appliance manufacturers are now required to finance the recycling of their products. The Association for Electric Home Appliances is a trade group that is responsible for orphaned products.\n\nChina produces a significant share of the world’s appliances. This country also has a high influx of appliance waste. There has not been much progress in appliance recycling efficiency. China’s undeveloped dismantling and processing has led to elevated levels of toxins in waste appliance site vicinities. Their appliance recycling methods require severe improvement.\n\nThe United States is the largest waste appliance producer in the world, however there is still no federal law requiring appliance recycling and its legislation varies between states. On a state level, many mandatory electronic recovery programs have been implemented. There are also several commercial appliance recyclers, for example, Appliance Recycling Centers of America (ARCA). ARCA is a company based in Minneapolis, with a chain of recycling depots nationwide.\n\nIn 2003, the California Electronic Waste Recycling Act was signed. It established a new program for consumers to return, recycle, and ensure the safe and environmentally sound disposal of video display devices, such as televisions and computer monitors, that are hazardous wastes when discarded. In 2005, consumers began paying a 6-10 dollar fee when buying an electronic device. These fees are used to pay e-waste collectors and recyclers to cover their cost of managing e-waste. \nThe EWRA classifies e-waste by dividing the products into two categories: electronic devices and covered electronic devices. Only covered electronic devices (CEDs) are included in the EWRA, however all electronic devices needed recycling measures to be taken. The CEDs include televisions and computers that have LCD displays or contain cathode ray tubes.\n\nAustralia has the same approach as the U.S. at this moment. There are several commercial appliance recyclers in Australia, as well as some organisations that remove waste appliances and offer rebates, sponsored by the government. Some retailers like Appliances Online also remove and recycle customers' old appliances using services like Sims Metal Management.\nExtended producer responsibility (EPR) is defined as an environmental protection strategy that makes the manufacturer of the appliance responsible for its entire life cycle and especially for the “take-back”, recycling and final disposal of the product. Essentially, manufacturers must now finance product treatment and recycling. Countries where this strategy has been adopted for waste appliances are: Switzerland (1998), Denmark (1999), Netherlands (1999), Norway (1999), Belgium (2001), Japan (2001), Sweden (2001) and Germany (2005), but it has also been expanded through legislation among certain South American countries such as Argentina, Brazil, Colombia and Peru. Countries in which EPR has long been established, demonstrate that the combination of government legislation and sound company practices can produce a higher take-back and recycling rate. An example of this is the Sony Corporation in Japan, achieving a 53% recycling rate. Other ways countries approach the issue of waste appliances is either by offering recycling facilities or banning importation. Almost all countries, at least offer facilities that aid in appliance recycling. Many implement extended producer responsibility, in addition to recycling facilities.\n\n"}
{"id": "8777614", "url": "https://en.wikipedia.org/wiki?curid=8777614", "title": "Atomic Industrial Forum", "text": "Atomic Industrial Forum\n\nThe Atomic Industrial Forum (AIF) was an industrial policy organization for the commercial development of nuclear power nuclear energy.\n\nThe Atomic Industrial Forum history dates to Autumn 1952, when it was being first organized:\n\nIn response, some 30 industrialists, engineers, and educators met in January 1953 to establish the forum. The AIF was formally incorporated on April 10, 1953 in New York City, and marked the beginning of the commercial nuclear power industry in the United StatesThe first Executive Director of AIF was Charles Robbins.\n\nAs a profit trade association the AIF advocated the peaceful uses of atomic energy and increasing the role of the private sector in its development. Its first order of business was to advocate revising the Atomic Energy Act of 1946 to allow and foster the commercial ownership of non weapons nuclear facilities, such as production of radioactive isotopes and nuclear power plants. AIF established strong working relationships with the U.S. Atomic Energy Commission and the Congressional Joint Committee on Atomic Energy. AIF's efforts helped to achieve the passage of the Atomic Energy Act of 1954 which resulted in the growth of a commercial nuclear industry. AIF was organized on the basis of an executive committee, the annual election of officers and a permanent operations staff, headed by an Executive Director, Mr. Charles Robbins.\n\nIn 1963 AIF established an international public information program. Working with other forums around the world, the program sought, through publications, workshops, exhibitions, speeches and outreach, to foster and achieve better understanding of the peaceful uses of atomic energy. Its first program director was Charles B.Yulish.\n\nBoth the government and private sectors involvement in atomic energy grew steadily; eventually, more that 125 commercial nuclear power plants provided 20 percent of America's electricity.\nAt the same time there were increasing debates on safeguards and regulation. The Atomic Energy Commission, which both promoted, developed and regulated nuclear development, was split into two agencies—the Energy Research and Development Agency, now the Department of Energy, and the independent U.S. Nuclear Regulatory Administration. \nAs new challenges and opportunities evolved, new industry efforts and resources were required to address these matters.\n\nIn 1987 the AIF was reconfigured into the Nuclear Utility Management and Resources Council (NUMARC), which addressed generic regulatory and technical issues, and the U.S. Council for Energy Awareness (USCEA), founded in 1979. In 1994 these two organizations were again reorganized and re-purposed. The Nuclear Energy Institute and the American Nuclear Energy Council (ANEC conducted public affairs, and the nuclear division of the Edison Electric Institute (EEI), was responsible for issues involving nuclear fuel supply and management, and the economics of nuclear energy.\n\nIn 2011, the Nuclear Energy Institute became the leading organization representing the nuclear industry. NEI headquarters is in Washington, DC.\n"}
{"id": "39294118", "url": "https://en.wikipedia.org/wiki?curid=39294118", "title": "British wildwood", "text": "British wildwood\n\nBritish wildwood, or simply 'the wildwood', is the wholly natural landscape which developed across major parts of England after the last ice age. This woodland was not yet affected by human intervention, and was home to many species which are not now found in England, such as elk and brown bears. Over centuries, starting in the Neolithic period, this wildwood gradually gave way to open plains and fields as human populations increased and began to exploit and develop the land to their advantage. Much of the areas of woodland that remain in England descend from the original wildwood but are now in a semi-natural state due to being managed and controlled, for example as a source of timber. These are known as ancient woodland. True wildwood is thought to be no longer extant in the UK.\n\n"}
{"id": "23924014", "url": "https://en.wikipedia.org/wiki?curid=23924014", "title": "Coral reef fish", "text": "Coral reef fish\n\nCoral reef fish are fish which live amongst or in close relation to coral reefs. Coral reefs form complex ecosystems with tremendous biodiversity. Among the myriad inhabitants, the fish stand out as colourful and interesting to watch. Hundreds of species can exist in a small area of a healthy reef, many of them hidden or well camouflaged. Reef fish have developed many ingenious specialisations adapted to survival on the reefs.\n\nCoral reefs occupy less than one percent of the surface area of the world oceans, but still they provide a home for 25 percent of all marine fish species. Reef habitats are a sharp contrast to the open water habitats that make up the other 99% of the world oceans.\n\nHowever, loss and degradation of coral reef habitat, increasing pollution, and overfishing including the use of destructive fishing practices, are threatening the survival of the coral reefs and the associated reef fish.\n\nCoral reefs are the result of thousands of years of coevolution among algae, invertebrates and fish. They have become crowded and complex environments, and the fish have evolved many ingenious ways of surviving. Most fishes found on coral reefs are ray-finned fishes, known for the characteristic sharp, bony rays and spines in their fins. These spines provide formidable defences, and when erected they can usually be locked in place or are venomous. Many reef fish have also evolved cryptic coloration to confuse predators.\n\nReef fish have also evolved complex adaptive behaviours. Small reef fish get protection from predators by hiding in reef crevices or by shoaling and schooling. Many reef fish confine themselves to one small neighbourhood where every hiding place is known and can be immediately accessed. Others cruise the reefs for food in shoals, but return to a known area to hide when they are inactive. Resting small fish are still vulnerable to attack by crevice predators, so many fish, such as triggerfish, squeeze into a small hiding place and wedge themselves by erecting their spines.\n\nAs an example of the adaptations made by reef fish, the yellow tang is a herbivore which feeds on benthic turf algae. They also provide cleaner services to marine turtles, by removing algal growth from their shells. They do not tolerate other fish with the same colour or shape. When alarmed, the usually placid yellow tang can erect spines in its tail and slash at its opponent with rapid sideways movements.\n\nCoral reefs contain the most diverse fish assemblages to be found anywhere on earth, with perhaps as many as 6,000–8,000 species that can be found dwelling within coral reef ecosystems of the world's oceans.\n\nThe mechanisms that first led to, and continue to maintain, such concentrations of fish species on coral reefs has been widely debated over the last 50 years. While many reasons have been proposed, there is no general scientific consensus on which of these is the most influential, but it seems likely that a number of factors contribute. These include the rich habitat complexity and diversity inherent in coral reef ecosystems, the wide variety and temporal availability of food resources available to coral reef fishes, a host of pre and post larval settlement processes, and as yet unresolved interactions between all these factors.\n\nThere are two major regions of coral reef development recognized; the Indo-Pacific (which includes the Pacific and Indian Oceans as well as the Red Sea), and the tropical western Atlantic (also known as the \"wider\" or \"greater\" Caribbean). Each of these two regions contains its own unique coral reef fish fauna with no natural overlap in species. Of the two regions, the richest by far in terms of reef fish diversity is the Indo-Pacific where there are an estimated 4,000–5,000 species of fishes associated with coral reef habitats. Another 500–700 species can be found in the greater Caribbean region.\n\nMost reef fishes have body shapes that are different from open water fishes. Open water fish are usually built for speed in the open sea, streamlined like torpedoes to minimise friction as they move through the water. Reef fish are operating in the relatively confined spaces and complex underwater landscapes of coral reefs. For this manoeuvrability is more important than straight line speed, so coral reef fish have developed bodies which optimize their ability to dart and change direction. They outwit predators by dodging into fissures in the reef or playing hide and seek around coral heads.\n\nMany reef fish, such as butterflyfish and angelfishes, have evolved bodies which are deep and laterally compressed like a pancake. Their pelvic and pectoral fins are designed differently, so they act together with the flattened body to optimise manoeuvrability.\n\nCoral reef fishes exhibit a huge variety of dazzling and sometimes bizarre colours and patterns. This is in marked contrasts to open water fishes which are usually countershaded with silvery colours.\n\nThe patterns have different functions. Sometimes they camouflage the fish when the fish rests in places with the right background. Colouration can also be used to help species recognition during mating. Some unmistakable contrasting patterns are used to warn predators that the fish has venomous spines or poisonous flesh.\n\nThe foureye butterflyfish gets its name from a large dark spot on the rear portion of each side of the body. This spot is surrounded by a brilliant white ring, resembling an eyespot. A black vertical bar on the head runs through the true eye, making it hard to see. This can result in a predator thinking the fish is bigger than it is, and confusing the back end with the front end. The butterflyfish's first instinct when threatened is to flee, putting the false eyespot closer to the predator than the head. Most predators aim for the eyes, and this false eyespot tricks the predator into believing that the fish will flee tail first. When escape is not possible, the butterflyfish will sometimes turn to face its aggressor, head lowered and spines fully erect, like a bull about to charge. This may serve to intimidate the other animal or may remind the predator that the butterflyfish is too spiny to make a comfortable meal.\n\nThe psychedelic \"Synchiropus splendidus\" (right) is not easily seen due to its bottom-feeding habit and its small size, reaching only about 6 cm. It feeds primarily on small crustaceans and other invertebrates, and is popular in the aquarium trade.\n\nJust as some prey species evolved cryptic colouration and patterns to help avoid predators, some ambush predators evolved camouflage that lets them ambush their prey. The tassled scorpionfish is an ambush predator that looks like part of a sea floor encrusted with coral and algae. It lies in wait on the sea floor for crustaceans and small fish, such as gobies, to pass by. Another ambush predator is the striated frogfish (right). They lie on the bottom and wave a conspicuous worm-like lure strategically attached above their mouth. Normally about 10 cm (4 in) long, they can also inflate themselves like puffers.\n\nGobies avoid predators by tucking themselves into coral crevices or partly burying themselves in sand. They continually scan for predators with eyes that swivel independently. The camouflage of the tassled scorpionfish can prevent gobies from seeing them until it's too late.\n\nThe clown triggerfish has strong jaws for crushing and eating sea urchins, crustaceans and hard-shell molluscs. Its ventral (lower) surface has large, white spots on a dark background, and its dorsal (upper) surface has black spots on yellow. This is a form of countershading: from below, the white spots look like the lighted surface of the water above; and from above, the fish blends more with the coral reef below. The brightly painted yellow mouth may deter potential predators.\n\nMany reef fish species have evolved different feeding strategies accompanied by specialized mouths, jaws and teeth particularly suited to deal with their primary food sources found in coral reef ecosystems. Some species even shift their dietary habits and distributions as they mature. This is not surprising, given the huge variety in the types of prey on offer around coral reefs.\n\nFor example, the primary food source of butterflyfishes are the coral polyps themselves or the appendages of polychaetes and other small invertebrate animals. Their mouths protrude like forceps, and are equipped with fine teeth that allow them to nip off such exposed body parts of their prey. Parrotfishes eat algae growing on reef surfaces, utilizing mouths like beaks well adapted to scrape off their food. Other fish, like snapper, are generalized feeders with more standard jaw and mouth structures that allow them to forage on a wide range of animal prey types, including small fishes and invertebrates.\n\nCarnivores are the most diverse of feeding types among coral reef fishes. There are many more carnivore species on the reefs than herbivores. Competition among carnivores is intense, resulting in a treacherous environment for their prey. Hungry predators lurk in ambush or patrol every part of the reef, night and day.\n\nSome fishes associated with reefs are generalized carnivores that feed on a variety of animal prey. These typically have large mouths that can be rapidly expanded, thereby drawing in nearby water and any unfortunate animals contained within the inhaled water mass. The water is then expelled through the gills with the mouth closed, thereby trapping the helpless prey For example, the bluestripe snapper has a varied diet, feeding on fishes, shrimps, crabs, stomatopods, cephalopods and planktonic crustaceans, as well as plant and algae material. Diet varies with age, location and the prevalent prey items locally.\n\nGoatfish are tireless benthic feeders, using a pair of long chemosensory barbels (whiskers) protruding from their chins to rifle through the sediments in search of a meal. Like goats, they seek anything edible: worms, crustaceans, molluscs and other small invertebrates are staples. The yellowfin goatfish (\"Mulloidichthys vanicolensis\") often schools with the blue-striped snapper. The yellowfins change their colouration to match that of the snapper. Presumably this is for predator protection, since goatfish are a more preferred prey than bluestripe snapper. By night the schools disperse and individual goatfish head their separate ways to loot the sands. Other nocturnal feeders shadow the active goatfish, waiting patiently for overlooked morsels.\n\nMoray eels and coral groupers (\"Plectropomus pessuliferus\") are known to cooperate with each other when hunting. Grouper are protogynous hermaphrodites, who school in \"harems\" of three to fifteen females. When no male is available, in each school the largest female shifts sex to male. If the final male disappears, changes to the largest female occur, with male behavior occurring within several hours and sperm production occurring within ten days. It is probably as a result of behavioural triggers.\n\nLarge schools of forage fish, such as surgeonfish and cardinalfish, move around the reef feeding on tiny zooplankton. The forage fish are, in turn, eaten by larger fish, such as the bigeye trevally. Fish receive many benefits from schooling behaviour, including defence against predators through better predator detection, since each fish is on the lookout. Schooling fish have developed remarkable displays of precise choreography which confuse and evade predators. For this they have evolved special pressure sensors along their sides, called lateral lines, that let them feel each other's movements and stay synchronized.\n\nBigeye trevally also form schools. They are swift predators who patrol the reef in hunting packs. When they find a school of forage fish, such as cardinalfish, they surround them and herd them close to the reef. This panics the prey fish, and their schooling becomes chaotic, leaving them open to attack by the trevally.\n\nThe titan triggerfish can move relatively large rocks when feeding and is often followed by smaller fishes that feed on leftovers. They also use a jet of water to uncover sand dollars buried in sand.\n\nBarracuda are ferocious predators on other fishes, with razor-sharp conical teeth which make it easy for them to rip their prey to shreds. Barracuda patrol the outer reef in large schools, and are extremely fast swimmers with streamlined, torpedo-shaped bodies.\n\nPorcupinefish are medium to large sized, and are usually found swimming among or near coral reefs. They inflate their body by swallowing water, reducing potential predators to those with much bigger mouths.\nFish can not groom themselves. Some fish specialise as cleaner fish, and establish cleaning stations where other fish can come to have their parasites nibbled away. The \"resident fish doctor and dentist on the reef is the bluestreak cleaner wrasse\". The bluestreak is marked with a conspicuous bright blue stripe and behaves in a stereotypical way which attracts larger fish to its cleaning station. As the bluestreak snacks on the parasites it gently tickles its client. This seems to bring the larger fish back again for regular servicing.\n\nThe reef lizardfish secretes a mucus coating which reduces drag when they swim and also protects it from some parasites. But other parasites find the mucus itself good to eat. So lizardfish visit the cleaner wrasse, which clean the parasites from the skin, gills and mouth.\n\nHerbivores feed on plants. The four largest groups of coral reef fishes that feed on plants are the parrotfishes, damselfishes, rabbitfishes, and surgeonfishes. All feed primarily on microscopic and macroscopic algae growing on or near coral reefs.\n\nAlgae can drape reefs in kaleidoscopes of colours and shapes. Algae are primary producers, which means they are plants synthesising food directly from solar energy and carbon dioxide and other simple nutrient molecules. Without algae, everything on the reef would die. One important algal group, the bottom dwelling (benthic) algae, grows over dead coral and other inert surfaces, and provides grazing fields for herbivores such as parrotfish.\n\nParrotfish are named for their parrot-like beaks and bright colours. They are large herbivores that graze on the algae that grows on hard dead corals. Equipped with two pairs of crushing jaws and their beaks, they pulverize chunks of algae-coated coral, digesting the algae and excreting the coral as fine sand.\n\nSmaller parrotfish are relatively defenceless herbivores, poorly defended against predators like barracuda. They have evolved to find protection by schooling, sometimes with other species like shoaling rabbitfish. Spinefoot rabbitfish are named for their defensive venomous spines, and they are seldom attacked by predators. Spines are a last-ditch defence. It is better to avoid predator detection in the first place, and avoid being thrust into risky spine-to-fang battles. So rabbitfish have also evolved skilful colour changing abilities.\n\nDamselfish are a group of species that feed on zooplankton and algae, and are an important reef forage fish for larger predators. They are small, typically five centimetres (two inches) long. Many species are aggressive towards other fishes which also graze on algae, such as surgeonfish. Surgeonfish sometimes use schooling as a countermeasure to defensive attacks by solitary damselfish.\n\nSymbiosis refers to two species that have a close relationship with each other. The relationship can be mutualistic, when both species benefit from the relationship, commensalistic, when one species benefits and the other is unaffected, and parasitistic, when one species benefits, and the other is harmed.\n\nAn example of commensalism occurs between the hawkfish and fire coral. Thanks to their large, skinless pectoral fins, hawkfish can perch on fire corals without harm. Fire corals are not true corals, but are hydrozoans possessing stinging cells called nematocysts which would normally prevent close contact. The protection fire corals offer hawkfish means the hawkfish has the high ground of the reef, and can safely survey its surroundings like a hawk. Hawkfish usually stay motionless, but dart out and grab crustaceans and other small invertebrates as they pass by. They are mostly solitary, although some species form pairs and share a head of coral.\n\nA more bizarre example of commensalism occurs between the slim, eel-shaped pinhead pearlfish and a particular species of sea cucumber. The pearlfish enters the sea cucumber through its anus, and spends the day safely protected inside the seacucumbers alimentary tract. At night it emerges the same way and feeds on small crustaceans.\n\nSea anemones are common on reefs. The tentacles of sea anemones bristle with tiny harpoons (nematocysts) primed with toxins, and are an effective deterrent against most predators. However, saddle butterflyfish, which are up to 30 cm (12 in) long, have developed a resistance to these toxins. Saddle butterflyfish usually flutter gently rather than swim. But in the presence of their preferred food, sea anemones, this gentleness disappears, and the butterflyfish dash in and out, ripping off the anemone tentacles.\n\nThere is a mutualistic relationship between sea anemones and clownfish. This gives the sea anemones a second line of defence. They are guarded by fiercely territorial clownfish, who are also immune to the anemone toxins. To get their meal, butterflyfish must get past these protective clownfish who, although smaller, are not intimidated. An anemone without its clownfish will quickly be eaten by butterflyfish. In return, the anemones provide the clownfish protection from their predators, who are not immune to anemone stings. As a further benefit to the anemone, waste ammonia from the clownfish feed symbiotic algae found in the anemone's tentacles.\n\nAs with all fish, coral reef fish harbour parasites. Since coral reef fish are characterized by high biodiversity, parasites of coral reef fish show tremendous variety. Parasites of coral reef fish include nematodes, Platyhelminthes (cestodes, digeneans, and monogeneans), leeches, parasitic crustaceans such as isopods and copepods, and various microorganisms such as myxosporidia and microsporidia. Some of these fish parasites have heteroxenous life cycles (i.e. they have several hosts) among which sharks (certain cestodes) or molluscs (digeneans). The high biodiversity of coral reefs increases the complexity of the interactions between parasites and their various and numerous hosts. Numerical estimates of parasite biodiversity have shown that certain coral fish species have up to 30 species of parasites.\n\nMany reef fish are toxic. Toxic fish are fish which contain strong toxins in their bodies. There is a distinction between poisonous fish and venomous fish. Both types of fish contain strong toxins, but the difference is in the way the toxin is delivered. Venomous fish deliver their toxins (called venom) by biting, stinging, or stabbing, causing an envenomation. Venomous fish don't necessarily cause poisoning if they are eaten, since the venom is often destroyed in the digestive system. By contrast, poisonous fish contain strong toxins which are not destroyed by the digestive system. This makes them poisonous to eat.\n\nVenomous fish carry their venom in venom glands and use various delivery systems, such as spines or sharp fins, barbs or spikes, and fangs. Venomous fish tend to be either very visible, using flamboyant colours to warn enemies, or skilfully camouflaged and maybe buried in the sand. Apart from the defence or hunting value, venom might have value for bottom dwelling fish by killing the bacteria that try to invade their skin. Few of these venoms have been studied. They are a yet to be tapped resource for bioprospecting to find drugs with medical uses.\n\nThe most venomous known fish is the reef stonefish. It has a remarkable ability to camouflage itself amongst rocks. It is an ambush predator that sits on the bottom waiting for prey to come close. It does not swim away if disturbed, but erects 13 venomous spines along its back. For defence, it can shoot venom from each or all of these spines. Each spine is like a hypodermic needle, delivering the venom from two sacs attached to the spine. The stonefish has control over whether to shoot its venom, and does so when provoked or frightened. The venom results in severe pain, paralysis and tissue death, and can be fatal if not treated. Despite its formidable defence, the stonefish does have predators. Some bottom feeding rays and sharks with crushing teeth feed on them, as does the Stokes' seasnake\n\nUnlike the stonefish which can shoot venom, the lionfish can only release venom when something strikes its spines. Although not native to the US coast, lionfish have appeared around Florida and have spread up the coast to New York. They are attractive aquarium fish, sometimes used to stock ponds, and may have been washed into the sea during a hurricane. Lionfish can aggressively dart at scuba divers and attempt to puncture the facemask with their venomous spines.\n\nThe spotted trunkfish is a reef fish which secretes a colourless ciguatera toxin from glands on its skin when touched. The toxin is only dangerous when ingested, so there's no immediate harm to divers. However, predators as large as nurse sharks can die as a result of eating a trunkfish. Ciguatera toxins appear to accumulate in top predators of coral reefs. Many of the Caribbean groupers and the barracuda for example may contain enough of this toxin to cause severe symptoms in humans who eat them. What makes the situation particularly dangerous is that such species may be toxic only at certain sizes or locations, making it difficult to know whether or when they are or are not safe to eat. In some locations this leads to many cases of ciguatera poisoning among tropical islanders.\n\nThe stargazer buries itself in sand and can deliver electric shocks as well as venom. It is a delicacy in some cultures (the venom is destroyed when it is cooked), and can be found for sale in some fish markets with the electric organ removed. They have been called \"the meanest things in creation\".\n\nThe giant moray is a reef fish at the top of the food chain. Like many other apex reef fish, it is likely to cause ciguatera poisoning if eaten. Outbreaks of ciguatera poisoning in the 11th to 15th centuries from large, carnivorous reef fish, caused by harmful algal blooms, could be a reason why Polynesians migrated to Easter Island, New Zealand, and possibly Hawaii.\n\nCoral reefs in the Indo-Pacific are dominated by whitetip, blacktip and grey reef shark. Coral reefs in the western Atlantic Ocean are dominated by the Caribbean reef shark. These sharks are all species of requiem shark, and all have the robust, streamlined bodies that are typical of the requiem shark. They are fast-swimming, agile predators that feed primarily on free-swimming bony fishes and cephalopods. Other species of reef sharks include the Galapagos shark, the tawny nurse shark and hammerheads.\n\nThe whitetip reef shark is a small shark usually less than in length. It is associated almost exclusively with coral reefs where it can be encountered around coral heads and ledges with high vertical relief, or over sandy flats, in lagoons, or near drop-offs to deeper water. They prefer very clear water and rarely swim far from the bottom. Whitetip reef sharks spend most of the daytime time resting inside caves. Unlike other requiem sharks, which usually rely on ram ventilation and must constantly swim to breathe, this shark can pump water over its gills and lie still on the bottom. They have slender, lithe body, which allows them to wriggle into crevices and holes and extract prey inaccessible to other reef sharks. On the other hand, they are rather clumsy when attempting to take food suspended in open water.\n\nThe whitetip reef shark does not frequent very shallow water like the blacktip reef shark, nor the outer reef like the grey reef shark. They generally remain within a highly localized area. An individual shark may use the same cave for months to years. The daytime home range of a whitetip reef shark is limited to about , and at night this range increases to .\n\nThe whitetip reef shark is highly responsive to olfactory, acoustic, and electrical cues given off by potential prey. Its visual system is attuned more to movement and/or contrast than to object details. It is especially sensitive to natural and artificial low-frequency sounds in the 25–100 Hz range, which evoke struggling fish. They hunt primarily at night, when many fishes are asleep and easily taken. After dusk, a group of sharks may target the same prey item, covering every exit route from a particular coral head. Each shark hunts for itself and in competition with the others in its group. They feed mainly on bony fishes, including eels, squirrelfishes, snappers, damselfishes, parrotfishes, surgeonfishes, triggerfishes, and goatfishes, as well as octopus, spiny lobsters, and crabs. Important predators of the whitetip reef shark include tiger sharks and Galapagos sharks.\n\nThe blacktip reef shark is typically about long. It is usually found over reef ledges and sandy flats, though it can also enter brackish and freshwater environments. This species likes shallow water, while the whitetip and the grey reef shark are prefer deeper water. Younger sharks favour shallow sandy flats, and older sharks spend more time around reef ledges and near reef drop-offs. Blacktip reef sharks are strongly attached to their own area, where they may remain for up to several years. A tracking study off Palmyra Atoll in the central Pacific has found that the blacktip reef shark had a home range of about , among the smallest of any shark species. The size and location of the range does not change with time of day. The blacktip reef shark may be encountered alone or in small groups. Large social aggregations have also been observed. They are active predators of small bony fishes, cephalopods, and crustaceans, and also feed on sea snakes and seabirds. Blacktip reef sharks are preyed on by groupers, grey reef sharks, tiger sharks, and members of their own species. At Palmyra Atoll, adult blacktip reef sharks avoid patrolling tiger sharks by staying out of the central, deeper lagoon.\n\nGrey reef sharks are usually less than 1.9 metres (6.2 ft) long. Despite their moderate size, grey reef sharks actively expel most other shark species from favored habitats. In areas where this species co-exists with the blacktip reef shark, the latter species occupy the shallow flats while the grey reef sharks stay in deeper water. Many grey reef sharks have a home range on a specific area of the reef, to which they continually return. However, they are social rather than territorial. During the day, these sharks often form groups of 5–20 individuals near coral reef drop-offs, splitting up in the evening as the sharks begin to hunt. They are found over continental and insular shelves, preferring the leeward (away from the direction of the current) sides of coral reefs with clear water and rugged topography. They are frequently found near the drop-offs at the outer edges of the reef, and less commonly within lagoons. On occasion, this shark may venture several kilometers out into the open ocean.\n\nShark researcher Leonard Compagno comments on the relationship between the three species.\n\n\"[The grey reef shark] ...shows microhabitat separation from the blacktip reef sharks; around islands where both species occur, the blacktip occupies shallow flats, while the grey reef shark is usually found in deeper areas, but where the blacktip is absent, the grey reef shark is commonly found on the flats... [The grey reef shark] complements the whitetip shark as it is far more adapt at catching off-bottom fish than the whitetip, but the later is far more competent in extracting prey from crevices and holes in reefs.\"\n\nThe Caribbean reef shark is up to 3 metres (10 ft) long, one of the largest apex predators in the reef ecosystem. Like the whitetip reef shark, they have been documented resting motionless on the sea bottom or inside caves, unusual behaviour for requiem sharks. Caribbean reef shark play a major role in shaping Caribbean reef communities. They are more active at night, with no evidence of seasonal changes in activity or migration. Juveniles tend to remain in a localized area throughout the year, while adults range over a wider area. The Caribbean reef shark feeds on a wide variety of reef-dwelling bony fishes and cephalopods, as well as some elasmobranchs such as eagle rays and yellow stingrays . Young sharks feed on small fishes, shrimps, and crabs. In turn, young sharks are preyed on by larger sharks such as the tiger shark and the bull shark.\n\n\n"}
{"id": "908654", "url": "https://en.wikipedia.org/wiki?curid=908654", "title": "Dynamometer", "text": "Dynamometer\n\nA dynamometer or \"dyno\" for short, is a device for measuring force, torque, or power. For example, the power produced by an engine, motor or other rotating can be calculated by simultaneously measuring torque and rotational speed (RPM).\n\nIn addition to being used to determine the torque or power characteristics of a machine under test, dynamometers are employed in a number of other roles. In standard emissions testing cycles such as those defined by the United States Environmental Protection Agency, dynamometers are used to provide simulated road loading of either the engine (using an engine dynamometer) or full powertrain (using a chassis dynamometer). In fact, beyond simple power and torque measurements, dynamometers can be used as part of a testbed for a variety of engine development activities, such as the calibration of engine management controllers, detailed investigations into combustion behavior, and tribology.\n\nIn the medical terminology, hand-held dynamometers are used for routine screening of grip and hand strength, and the initial and ongoing evaluation of patients with hand trauma or dysfunction. They are also used to measure grip strength in patients where compromise of the cervical nerve roots or peripheral nerves is suspected.\n\nIn the rehabilitation, kinesiology, and ergonomics realms, force dynamometers are used for measuring the back, grip, arm, and/or leg strength of athletes, patients, and workers to evaluate physical status, performance, and task demands. Typically the force applied to a lever or through a cable is measured and then converted to a moment of force by multiplying by the perpendicular distance from the force to the axis of the level.\n\nAn absorbing dynamometer acts as a load that is driven by the prime mover that is under test (e.g. Pelton wheel). The dynamometer must be able to operate at any speed and load to any level of torque that the test requires.\n\nAbsorbing dynamometers are not to be confused with \"inertia\" dynamometers, which calculate power solely by measuring power required to accelerate a known mass drive roller and provide no variable load to the prime mover.\n\nAn absorption dynamometer is usually equipped with some means of measuring the operating torque and speed.\n\nThe power absorption unit (PAU) of a dynamometer absorbs the power developed by the prime mover. This power absorbed by the dynamometer is then converted into heat, which generally dissipates into the ambient air or transfers to cooling water that dissipates into the air. Regenerative dynamometers, in which the prime mover drives a DC motor as a generator to create load, make excess DC power and potentially - using a DC/AC inverter - can feed AC power back into the commercial electrical power grid.\n\nAbsorption dynamometers can be equipped with two types of control systems to provide different main test types.\n\nThe dynamometer has a \"braking\" torque regulator - the power absorption unit is configured to provide a set braking force torque load, while the prime mover is configured to operate at whatever throttle opening, fuel delivery rate, or any other variable it is desired to test. The prime mover is then allowed to accelerate the engine through the desired speed or RPM range. Constant force test routines require the PAU to be set slightly torque deficient as referenced to prime mover output to allow some rate of acceleration. Power is calculated based on rotational speed x torque x constant. The constant varies depending on the units used.\n\nIf the dynamometer has a speed regulator (human or computer), the PAU provides a variable amount of braking force (torque) that is necessary to cause the prime mover to operate at the desired single test speed or RPM. The PAU braking load applied to the prime mover can be manually controlled or determined by a computer. Most systems employ eddy current, oil hydraulic, or DC motor produced loads because of their linear and quick load change abilities.\n\nPower is calculated based on rotational speed x torque x constant, with the constant varying with the output unit desired and the input units used.\n\nA \"motoring dynamometer\" acts as a motor that drives the equipment under test. It must be able to drive the equipment at any speed and develop any level of torque that the test requires. In common usage, AC or DC motors are used to drive the equipment or \"load\" device.\n\nIn most dynamometers power (\"P\") is not measured directly, but must be calculated from torque (\"τ\") and angular velocity (\"ω\") values or force (\"F\") and linear velocity (\"v\"):\n\nDivision by a conversion constant may be required, depending on the units of measure used.\n\nFor imperial units,\n\nFor metric units,\n\nA dynamometer consists of an absorption (or absorber/driver) unit, and usually includes a means for measuring torque and rotational speed. An absorption unit consists of some type of rotor in a housing. The rotor is coupled to the engine or other equipment under test and is free to rotate at whatever speed is required for the test. Some means is provided to develop a braking torque between the rotor and housing of the dynamometer. The means for developing torque can be frictional, hydraulic, electromagnetic, or otherwise, according to the type of absorption/driver unit.\n\nOne means for measuring torque is to mount the dynamometer housing so that it is free to turn except as restrained by a torque arm. The housing can be made free to rotate by using trunnions connected to each end of the housing to support it in pedestal-mounted trunnion bearings. The torque arm is connected to the dyno housing and a weighing scale is positioned so that it measures the force exerted by the dyno housing in attempting to rotate. The torque is the force indicated by the scales multiplied by the length of the torque arm measured from the center of the dynamometer. A load cell transducer can be substituted for the scales in order to provide an electrical signal that is proportional to torque.\n\nAnother means to measure torque is to connect the engine to the dynamo through a torque sensing coupling or torque transducer. A torque transducer provides an electrical signal that is proportional to the torque.\n\nWith electrical absorption units, it is possible to determine torque by measuring the current drawn (or generated) by the absorber/driver. This is generally a less accurate method and not much practiced in modern times, but it may be adequate for some purposes.\n\nWhen torque and speed signals are available, test data can be transmitted to a data acquisition system rather than being recorded manually. Speed and torque signals can also be recorded by a chart recorder or plotter.\n\nIn addition to classification as absorption, motoring, or universal, as described above, dynamometers can also be classified in other ways.\n\nA dyno that is coupled directly to an engine is known as an \"engine dyno\".\n\nA dyno that can measure torque and power delivered by the power train of a vehicle directly from the drive wheel or wheels (without removing the engine from the frame of the vehicle), is known as a \"chassis dyno\".\n\nDynamometers can also be classified by the type of absorption unit or absorber/driver that they use. Some units that are capable of absorption only can be combined with a motor to construct an absorber/driver or \"universal\" dynamometer.\n\n\nEddy current (EC) dynamometers are currently the most common absorbers used in modern chassis dynos. The EC absorbers provide a quick load change rate for rapid load settling. Most are air cooled, but some are designed to require external water cooling systems.\n\nEddy current dynamometers require an electrically conductive core, shaft, or disc moving across a magnetic field to produce resistance to movement. Iron is a common material, but copper, aluminum, and other conductive materials are also usable.\n\nIn current (2009) applications, most EC brakes use cast iron discs similar to vehicle disc brake rotors, and use variable electromagnets to change the magnetic field strength to control the amount of braking.\n\nThe electromagnet voltage is usually controlled by a computer, using changes in the magnetic field to match the power output being applied.\n\nSophisticated EC systems allow steady state and controlled acceleration rate operation.\n\nA powder dynamometer is similar to an eddy current dynamometer, but a fine magnetic powder is placed in the air gap between the rotor and the coil. The resulting flux lines create \"chains\" of metal particulate that are constantly built and broken apart during rotation, creating great torque. Powder dynamometers are typically limited to lower RPM due to heat dissipation problems.\n\nHysteresis dynamometers use a magnetic rotor, sometimes of AlNiCo alloy, that is moved through flux lines generated between magnetic pole pieces. The magnetisation of the rotor is thus cycled around its B-H characteristic, dissipating energy proportional to the area between the lines of that graph as it does so.\n\nUnlike eddy current brakes, which develop no torque at standstill, the hysteresis brake develops largely constant torque, proportional to its magnetising current (or magnet strength in the case of permanent magnet units) over its entire speed range. Units often incorporate ventilation slots, though some have provision for forced air cooling from an external supply.\n\nHysteresis and Eddy Current dynamometers are two of the most useful technologies in small ( and less) dynamometers.\n\nElectric motor/generator dynamometers are a specialized type of adjustable-speed drive. The absorption/driver unit can be either an alternating current (AC) motor or a direct current (DC) motor. Either an AC motor or a DC motor can operate as a generator that is driven by the unit under test or a motor that drives the unit under test. When equipped with appropriate control units, electric motor/generator dynamometers can be configured as universal dynamometers. The control unit for an AC motor is a variable-frequency drive, while the control unit for a DC motor is a DC drive. In both cases, regenerative control units can transfer power from the unit under test to the electric utility. Where permitted, the operator of the dynamometer can receive payment (or credit) from the utility for the returned power via net metering.\n\nIn engine testing, universal dynamometers can not only absorb the power of the engine, but can also drive the engine for measuring friction, pumping losses, and other factors.\n\nElectric motor/generator dynamometers are generally more costly and complex than other types of dynamometers.\n\nA fan is used to blow air to provide engine load. The torque absorbed by a fan brake may be adjusted by changing the gearing or the fan itself, or by restricting the airflow through the fan. It should be noted that, due to the low viscosity of air, this variety of dynamometer is inherently limited in the amount of torque that it can absorb.\n\nAn oil shear brake has a series of friction discs and steel plates similar to the clutches in an automobile automatic transmission. The shaft carrying the friction discs is attached to the load through a coupling. A piston pushes the stack of friction discs and steel plates together creating shear in the oil between the discs and plates applying a torque. Torque control can be pneumatic or hydraulic. Force lubrication maintains a film of oil between the surfaces to eliminate wear. Reaction is smooth to zero RPM without stick-slip. Loads up to hundreds of thermal horsepower can be absorbed through the required force lubrication and cooling unit. Most often, the brake is kinetically grounded through a torque arm anchored by a strain gauge which produces a current under load fed to the dynamometer control. Proportional or servo control valves are generally used to allow the dynamometer control to apply pressure to provide the program torque load with feedback from the strain gauge closing the loop. As torque requirements go up there are speed limitations.\n\nThe hydraulic brake system consists of a hydraulic pump (usually a gear-type pump), a fluid reservoir, and piping between the two parts. Inserted in the piping is an adjustable valve, and between the pump and the valve is a gauge or other means of measuring hydraulic pressure. In simplest terms, the engine is brought up to the desired RPM and the valve is incrementally closed. As the pumps outlet is restricted, the load increases and the throttle is simply opened until at the desired throttle opening. Unlike most other systems, power is calculated by factoring flow volume (calculated from pump design specifications), hydraulic pressure, and RPM. Brake HP, whether figured with pressure, volume, and RPM, or with a different load cell-type brake dyno, should produce essentially identical power figures. Hydraulic dynos are renowned for having the quickest load change ability, just slightly surpassing eddy current absorbers. The downside is that they require large quantities of hot oil under high pressure and an oil reservoir.\n\nThe water brake absorber is sometimes mistakenly called a \"hydraulic dynamometer\". Invented by British engineer William Froude in 1877 in response to a request by the Admiralty to produce a machine capable of absorbing and measuring the power of large naval engines, water brake absorbers are relatively common today. They are noted for their high power capability, small size, light weight, and relatively low manufacturing costs as compared to other, quicker reacting, \"power absorber\" types.\n\nTheir drawbacks are that they can take a relatively long period of time to \"stabilize\" their load amount, and that they require a constant supply of water to the \"water brake housing\" for cooling. In many parts of the country, environmental regulations now prohibit \"flow through\" water, and so large water tanks must be installed to prevent contaminated water from entering the environment.\n\nThe schematic shows the most common type of water brake, known as the \"variable level\" type. Water is added until the engine is held at a steady RPM against the load, with the water then kept at that level and replaced by constant draining and refilling (which is needed to carry away the heat created by absorbing the horsepower). The housing attempts to rotate in response to the torque produced, but is restrained by the scale or torque metering cell that measures the torque.\nIn most cases, motoring dynamometers are symmetrical; a 300 kW AC dynamometer can absorb 300 kW as well as motor at 300 kW. This is an uncommon requirement in engine testing and development. Sometimes, a more cost-effective solution is to attach a larger absorption dynamometer with a smaller motoring dynamometer. Alternatively, a larger absorption dynamometer and a simple AC or DC motor may be used in a similar manner, with the electric motor only providing motoring power when required (and no absorption). The (cheaper) absorption dynamometer is sized for the maximum required absorption, whereas the motoring dynamometer is sized for motoring. A typical size ratio for common emission test cycles and most engine development is approximately 3:1. Torque measurement is somewhat complicated since there are two machines in tandem - an inline torque transducer is the preferred method of torque measurement in this case. An eddy-current or waterbrake dynamometer, with electronic control combined with a variable frequency drive and AC induction motor, is a commonly used configuration of this type. Disadvantages include requiring a second set of test cell services (electrical power and cooling), and a slightly more complicated control system. Attention must be paid to the transition between motoring and braking in terms of control stability.\n\nDynamometers are useful in the development and refinement of modern engine technology. The concept is to use a dyno to measure and compare power transfer at different points on a vehicle, thus allowing the engine or drivetrain to be modified to get more efficient power transfer. For example, if an engine dyno shows that a particular engine achieves of torque, and a chassis dynamo shows only , one would know that the drivetrain losses are nominal. Dynamometers are typically very expensive pieces of equipment, and so are normally used only in certain fields that rely on them for a particular purpose.\n\nA 'brake' dynamometer applies variable load on the prime mover (PM) and measures the PM's ability to move or hold the RPM as related to the \"braking force\" applied. It is usually connected to a computer that records applied braking torque and calculates engine power output based on information from a \"load cell\" or \"strain gauge\" and a speed sensor.\n\nAn 'inertia' dynamometer provides a fixed inertial mass load, calculates the power required to accelerate that fixed and known mass, and uses a computer to record RPM and acceleration rate to calculate torque. The engine is generally tested from somewhat above idle to its maximum RPM and the output is measured and plotted on a graph.\n\nA 'motoring' dynamometer provides the features of a brake dyno system, but in addition, can \"power\" (usually with an AC or DC motor) the PM and allow testing of very small power outputs (for example, duplicating speeds and loads that are experienced when operating a vehicle traveling downhill or during on/off throttle operations).\n\nThere are essentially 3 types of dynamometer test procedures:\n\n\n\nIn every type of sweep test, there remains the issue of potential power reading error due to the variable engine/dyno/vehicle total rotating mass. Many modern computer-controlled brake dyno systems are capable of deriving that \"inertial mass\" value, so as to eliminate this error.\n\nA \"sweep test\" will almost always be suspect, as many \"sweep\" users ignore the rotating mass factor, preferring to use a blanket \"factor\" on every test on every engine or vehicle. Simple inertia dyno systems aren't capable of deriving \"inertial mass\", and thus are forced to use the same (assumed) inertial mass on every vehicle tested.\n\nUsing steady state testing eliminates the rotating inertial mass error of a sweep test, as there is no acceleration during this type of test.\n\nAggressive throttle movements, engine speed changes, and engine motoring are characteristics of most transient engine tests. The usual purpose of these tests are vehicle emissions development and homologation. In some cases, the lower-cost eddy-current dynamometer is used to test one of the transient test cycles for early development and calibration. An eddy current dyno system offers fast load response, which allows rapid tracking of speed and load, but does not allow motoring. Since most required transient tests contain a significant amount of motoring operation, a transient test cycle with an eddy-current dyno will generate different emissions test results. Final adjustments are required to be done on a motoring-capable dyno.\n\nAn engine dynamometer measures power and torque directly from the engine's crankshaft (or flywheel), when the engine is removed from the vehicle. These dynos do not account for power losses in the drivetrain, such as the gearbox, transmission, and differential.\n\nA chassis dynamometer, sometimes referred to as a rolling road, measures power delivered to the surface of the \"drive roller\" by the drive wheels. The vehicle is often parked on the roller or rollers, which the car then turns, and the output measured thereby.\n\nModern roller-type chassis dyno systems use the \"Salvisberg roller\", which improves traction and repeatability, as compared to the use of smooth or knurled drive rollers. Chassis dynamometers can be fixed or portable, and can do much more than display RPM, horsepower, and torque. With modern electronics and quick reacting, low inertia dyno systems, it is now possible to tune to best power and the smoothest runs in real time.\n\nOther types of chassis dynamometers are available that eliminate the potential for wheel slippage on old style drive rollers, attaching directly to the vehicle hubs for direct torque measurement from the axle.\n\nMotor vehicle emissions development and homologation dynamometer test systems often integrate emissions sampling, measurement, engine speed and load control, data acquisition, and safety monitoring into a complete test cell system. These test systems usually include complex emissions sampling equipment (such as constant volume samplers and raw exhaust gas sample preparation systems) and analyzers. These analyzers are much more sensitive and much faster than a typical portable exhaust gas analyzer. Response times of well under one second are common, and are required by many transient test cycles. In retail settings it is also common to tune the air-fuel ratio using a wideband oxygen sensor that is graphed along with the RPM.\n\nIntegration of the dynamometer control system with automatic calibration tools for engine system calibration is often found in development test cell systems. In these systems, the dynamometer load and engine speed are varied to many engine operating points, while selected engine management parameters are varied and the results recorded automatically. Later analysis of this data may then be used to generate engine calibration data used by the engine management software.\n\nBecause of frictional and mechanical losses in the various drivetrain components, the measured rear wheel brake horsepower is generally 15-20 percent less than the brake horsepower measured at the crankshaft or flywheel on an engine dynamometer.\n\nThe Graham-Desaguliers Dynamometer was invented by George Graham and mentioned in the writings of John Desagulier in 1719. Desaguliers modified the first dynamometers, and so the instrument became known as the Graham-Desaguliers dynamometer.\n\nThe Regnier dynamometer was invented and made public in 1798 by Edmé Régnier, a French rifle maker and engineer.\n\nA patent was issued (dated June 1817) to Siebe and Marriot of Fleet Street, London for an improved weighing machine.\n\nGaspard de Prony invented the de Prony brake in 1821.\n\nMacneill's road indicator was invented by John Macneill in the late 1820s, further developing Marriot's patented weighing machine.\n\nFroude Hofmann, of Worcester, UK, manufactures engine and vehicle dynamometers. They credit William Froude with the invention of the hydraulic dynamometer in 1877, and say that the first commercial dynamometers were produced in 1881 by their predecessor company, Heenan & Froude.\n\nIn 1928, the German company \"Carl Schenck Eisengießerei & Waagenfabrik\" built the first vehicle dynamometers for brake tests that have the basic design of modern vehicle test stands.\n\nThe eddy current dynamometer was invented by Martin and Anthony Winther around 1931, but at that time, DC Motor/generator dynamometers had been in use for many years. A company founded by the Winthers brothers, Dynamatic Corporation, manufactured dynamometers in Kenosha, Wisconsin until 2002. Dynamatic was part of Eaton Corporation from 1946 to 1995. In 2002, Dyne Systems of Jackson, Wisconsin acquired the Dynamatic dynamometer product line. Starting in 1938, Heenan & Froude manufactured eddy current dynamometers for many years under license from Dynamatic and Eaton.\n\n\n"}
{"id": "6168349", "url": "https://en.wikipedia.org/wiki?curid=6168349", "title": "Electric energy consumption", "text": "Electric energy consumption\n\nElectric energy consumption is the form of energy consumption that uses electric energy. Electric energy consumption is the actual energy demand made on existing electricity supply.\n\nThe total electricity consumption in 2012 was 20,900 TWh.\n\nElectric energy is most often measured either in joules (J), or in watt hours (W·h) representing a constant power over a period of time.\n\nElectric and electronic devices consume electric energy to generate desired output (i.e., light, heat, motion, etc.). During operation, some part of the energy—depending on the electrical efficiency—is consumed in unintended output, such as waste heat.\n\nElectricity has been generated in power stations since 1882. The invention of the steam turbine in 1883 to drive the electric generator started a strong increase of world electricity consumption.\n\nIn 2008, the world total of electricity production was 20.279 petawatt-hours (PWh). This number corresponds to an average power of 2.31 TW continuously during the year. The total energy needed to produce this power is roughly a factor 2 to 3 higher because a power plant's efficiency of generating electricity is roughly 30–50%. The generated power is thus in the order of 5 TW. This is approximately a third of the total energy consumption of 15 TW (\"see world energy consumption\").\n\nIn 2005, the primary energy used to generate electricity was 41.60 Quadrillion BTU [12, 192 TWh] (Coal 21.01 quads [6,157 TWh], Natural Gas 6.69 quads [1,960 TWh], Petroleum 1.32 quads [387 TWh], Nuclear electric power 8.13 quads [2,383 TWh], Renewable energy 4.23 quads [1,240 TWh] respectively). The gross generation of electricity in that year was 14.50 Quads [4,250 TWh]; the difference, 27.10 Quads [7,942 TWh], was conversion losses. Among all electricity, 4.84 Quads [1,418 TWh] was used in residential area, 4.32 Quads [1,266 TWh] used in commercial, 3.47 Quads [1,017 TWh] used in industrial and 0.03 Quads [8.79 TWh] used in transportation.\n\n1 Quad = 1 Quadrillion BTU = 1 x 10 BTU = 293 TWh\n\n16,816 TWh (83%) of electric energy was consumed by final users. The difference of 3,464 TWh (17%) was consumed in the process of generating power and lost in transmission to end users.\n\nA sensitivity analysis on an adaptive neuro-fuzzy network model for electric demand estimation shows that employment is the most critical factor influencing electrical consumption. The study used six parameters as input data, employment, GDP, dwelling, population, HDD and CDD, with electricity demand as output variable.\n\nAt the world level, energy consumption was cut down by 1.5% during 2009, for the first time since World War II.\nExcept in Asia and Middle East, consumptions were reduced in all the world regions. In OECD countries, accounting for 53% of the total,\nelectricity demand scaled down by more than 4.5% in both Europe and North America while it shrank by above 7% in Japan. Electricity demand also dropped by more than 4.5% in CIS countries, driven by a large cut in Russian consumption. Conversely, in China and India (22% of the world's consumption), electricity consumption continued to rise at a strong pace (+6-7%) to meet energy demand related to high economic growth. In Middle East, growth rate was softened but remained high, just below 4%.\n\nThe table lists the top 37 electricity consuming countries, which use 19,000 TWh/a. i.e. 90% of the consumption of all more than 190 countries. The total consumption (including the amount consumed by the power plants) and the energy sources to generate this electricity is given per country. The data are of 2012. The last column contains the number of millions of inhabitants.\n\nTotal consumption (2nd column) divided by number of inhabitants (last column) gives a country's consumption per head. In W-Europe this is between 5 and 8 MWh/a. (1 MWh equals 1000 kWh.) In Scandinavia, USA, Canada, Taiwan and South Korea it is much more, in developing countries much less. The worlds average is 3 MWh/a. A very low consumption per head, as in Indonesia, means that many inhabitants are not connected to the electricity grid, and this is the reason that the world's 7th and 8th most populous countries—Nigeria (177M) and Bangladesh (156M)—do not appear in the table.\n\nFrom 2012 to 2014 worldwide electricity consumption increased 5%. Nuclear and fossil generated electricity rose 3%, renewable electricity 12%.\n\nA small part of the renewables, solar and wind electricity, increased much more, 46% in line with the strong growth since 1990.\n\nIn Brazil windpower inceased 140%, in China not only solar and wind increased fast, 81%, but also nuclear, 36%.\n\nListed countries are top 20 populous countries and/or top 20 GDP (PPP) countries and Saudi Arabia as of CIA World Factbook 2009. <br>\n30 countries (exclude EU/IEA) in this table represent 77% of world population, 84% of world GDP, 83% of world electricity generation.<br>\nProductivity per Electricity generation (concept similar to Energy intensity) can be measured by dividing GDP amount by the electricity generated. World average was $3.5 production/kWh.<br>\nElectricity generation include Final consumption, in process consumption, and losses.\n\nAbout 17% of total electricity production is consumed by in-processes, such as self-consumption of power plants, grid losses and storage losses. In 2008, total electricity generation accounted for 20,261 TWh (20.26 PWh), while 3,464 TWh (3.46 PWh) were self-consumption and losses and 16,816 TWh (16.82 PWh) went to final consumption.\n\nIn the consumption rate in Industry, China is highest with 67.8%, South Korea is 51.0% (7th), Germany 46.1% (11th), Japan 31.5% (26th), USA 24.0% (28th) In the Commercial and Public Service, Japan is highest with 36.4%, USA 35.6% (3rd), China 5.4% (29th). For Domestic usage, Saudi Arabia is highest with 56.9%, USA 36.2% (8th), Japan 29.8% (16th), China 15.5% (29th), Korea 13.8% (30th).\n\nDefinition\n\n\nElectric energy consumption per inhabitant by primary energy source in some countries and areas in 2008 is in the table. \n\nFor the OECD with 8 991 kWh/yr/person: 1.026 watt/person.\n\nIn all scenarios, increasing efficiency will result in less electricity needed for a given demand of power and light. But demand will increase strongly on account of\n\nAs transport and heating become more climate-friendly, the environmental effect of energy consumption will be more determined by electricity. This is mainly supplied by burning fossil fuel which disturbs the natural carbon cycle. The scenarios arrive at very different results for the environment.\n\nThe International Energy Agency expects revision of subsidy for fossil fuel which amounted to 550 billion dollar in 2013, more than four times renewable energy subsidy. In this scenario almost half of the increase in 2040 of electricity consumption is covered by more than 80% growth of renewable energy. Many new nuclear plants will be constructed, mainly to replace old ones. The nuclear part of electricity generation will increase from 11 to 12%. The renewable part goes up much, from 21 to 33%. The IEA warns that in order to restrict global warming to 2 °C, the carbon dioxide emission must not exceed 1000 gigaton (Gt) from 2014. This limit is reached in 2040 and emissions will not drop to zero ever.\n\nThe World Energy Council sees world electricity consumption increasing to more than 40,000 TWh/a in 2040. The fossil part of generation depends on energy policy. It can stay around 70% in the so-called Jazz scenario where countries rather independently \"improvise\" but it can also decrease to around 40% in the Symphony scenario if countries work \"orchestrated\" for more climate friendly policy. Carbon dioxide emission, 32 Gt/a in 2012, will increase to 46 Gt/a in Jazz but decrease to 26 Gt/a in Symphony. Accordingly, until 2040 the renewable part of generation will stay at about 20% in Jazz but increase to about 45% in Symphony.\n\n"}
{"id": "9048527", "url": "https://en.wikipedia.org/wiki?curid=9048527", "title": "Elliott Company", "text": "Elliott Company\n\nElliott Company designs, manufactures, installs, and services turbo-machinery for prime movers and rotating machinery. Headquartered in Jeannette, Pennsylvania, Elliott Company is a wholly owned subsidiary of the Japan-based company, Ebara Corporation, and is a unit of Elliott Group, Ebara Corporation's worldwide turbomachinery business. Elliott Group employs more than 2000 employees worldwide at 32 locations, with approximately 900 in Jeannette.\n\nFounded in 1901 as the Liberty Manufacturing Company to produce boiler cleaning equipment based on the patents of William Swan Elliott, the company incorporated as the Elliott Company in 1910. Elliott Company moved to the former Clifford-Capell Fan Works in Jeannette, Pennsylvania, in 1914 and has maintained a factory and offices there since.\n\nElliott purchased the Kerr Turbine Company in 1924 and Ridgway Dynamo & Engine Co. in 1926. These acquisitions allowed Elliott to begin manufacturing turbines and compressors and enter the rotating machinery market.\n\nIn the 1930s and during World War II, Elliott supplied the United States Navy with some of the electric motors and generators used in fleet submarines under the name Elliott Motor Company. Elliott's entry in the \"Dictionary of American Naval Fighting Ships\" shows its name abbreviated as Ell, and may be the source of a common misspelling of the company's name as \"Elliot\". The Elliott company's broader war contribution included turbines, generators, blowers, ejectors, heaters, and other warship and engine parts.\n\nIn 1941, Elliott became the first American manufacturer to build a turbocharger for diesel engines. Elliott would go on to produce more than 40,000 diesel turbochargers.\n\nIn 1957, the Elliott family sold the company to Carrier Corporation, which, in turn, was sold to United Technologies Corporation in 1979.\n\nIn 1962, Elliott developed their Plant-Air-Package (PAP) line of products which was sold to Fusheng Group of Taiwan in 2003, and became the FS-Elliott company (Not affiliated with Elliott Company). FS-Elliott is headquartered in Export Pa.\n\nIn 1984, Elliott launched the first of 32 Service Centers located throughout the world. In 1987, the management of the Elliott Company returned the firm to independent operation through a management buyout.\n\nIn 2000, Ebara Corporation of Tokyo, Japan, an Elliott licensee, purchased Elliott. Ebara Corporation is a large manufacturing and environmental services corporation whose shares are traded on the Nikkei Stock Exchange.\n\nIn 2010, Elliott had its Centennial Celebration, 100 years in the name of Elliott Company, in Jeannette Pennsylvania. The celebration featured free food, games, and tents featuring Elliott history and memorabilia from personal collectors.\n\nElliott currently designs and builds heavy-duty steam turbines, air and gas compressors, power recovery turbines and power generating equipment. Elliott products are used in oil and gas fields, refineries, chemical processing plants, steel mills, electric generating stations, sugar and paper mills, and various mining operations.\n\nCompressors: Elliott designs and manufactures multi-stage centrifugal air and gas compressors using EDGE technology. They currently offer six different compressor types: M-line, MB-line, MBP-line, P-line, PH-line, and A-line. Each compressor line is engineered to meet varying frame sizes, flow rates, and pressure capacities.\n\nSteam Turbines/Expanders: Elliott offers single-stage, multi-stage, single-valve, and multi-valve turbines ranging from 10 to and at varying operational speeds. Elliott also develops power recovery expander turbines used to drive compressors and generators in refinery fluid catalytic-cracking services.\n\nLube Oil Consoles and Dry Gas Seal Packages: To complement its compressors and turbines, Elliott offers a range of lubrication, seal and piping, and other auxiliary systems. Many consoles are packaged to meet individual customer requirements.\n\nControl Systems: Elliott Digital Control Systems allow for the remote operation of turbo-machinery. Some systems use graphical interfaces to make complex monitoring and control easier.\n\nJeannette, Pennsylvania is home to Elliott’s engineering and administration facilities, as well as the company's U.S. manufacturing center. This complex comprises 40 buildings on . The production capabilities of this facility were increased by 50% through the company's “Factory 2000” initiative.\n\nOutside of the United States:\nElliott operates sales and service offices in Canada, Europe, Latin America and South America, the Middle East, and Asia. Its Asia-Pacific manufacturing facility, owned and operated by the Ebara Corporation, is located in Sodegaura, Chiba, Japan and serves customers in Asia, Australia, and the Pacific region.\n\nThere is also a Houston, Texas location. (Renowned foreman James Leon Rodgers was based here.)\n\n"}
{"id": "35764902", "url": "https://en.wikipedia.org/wiki?curid=35764902", "title": "Energy-related products", "text": "Energy-related products\n\nErP, energy-related products, are products that use energy, or that do not use energy but have an indirect impact on energy consumption, such as water using devices, building insulation products, windows, etc. Compared to ErP, energy-using products (EuP) are products which are dependent of energy input (electricity etc.).\nAll ErP and EuP are subject to energy efficiency requirements.\n\nIn November 2009, the Eco-Design Directive EuP was replaced with the new energy-related products directive (ErP) 2009/125/EC. The old directive for energy using products only covered products that used energy, such as washing machines or computers. The new ErP-Directive covers products under the old EuP Directive as well as products that are energy-related and do not directly use energy such as water-saving taps and showerheads. With this directive, the European Union regulator laid the groundwork for specific implementing measures affecting a broad range of EuP and ErP. The goal is the reduction of energy along the supply chain: from the design stage throughout production, transport, packaging and so on. Products that comply with this directive are easily recognized by carrying the CE marking. In this case, the CE mark covers product safety and energy efficiency requirements.\n\n\n"}
{"id": "23391694", "url": "https://en.wikipedia.org/wiki?curid=23391694", "title": "Energy in Cape Verde", "text": "Energy in Cape Verde\n\nCape Verde is a net importer of energy, with no significant fossil energy resources. , 176,743 metric tonnes of fuel (about 3,550 barrels per day) were sold on the internal market. Electricity production was 443 GWh in 2016, of which 81% from thermal power, 17% from wind power and 1.4% from solar power. The main electricity producing company of Cape Verde is Electra. Electra serves all islands of Cape Verde except Boa Vista, where electricity and water are produced and distributed by the public-private company \"Águas e Energia de Boavista\". Other smaller electricity producers are \"Cabeólica\", which operates 4 wind parks, \"Águas de Ponta Preta\" on the island of Sal, and \"Electric Wind\" on Santo Antão.\n"}
{"id": "33711800", "url": "https://en.wikipedia.org/wiki?curid=33711800", "title": "Energy in Latvia", "text": "Energy in Latvia\n\nLatvia is net energy importer. \nPrimary energy use in Latvia was 49 TWh, or 22 TWh per million persons in 2009.\n\nAlmost half of the electricity used in the country is provided by renewable energy sources. The main renewable resource is hydroelectric power. Latvia has laws that regulate the building of power plants and plans to sell electricity at higher prices. This is a stimulus for investment, especially taking into consideration the fact that Latvia cannot offer big subsidies in order to attract investment. A production quota is approved for each renewable energy source every year.\n\nNatural gas companies include Latvijas Gāze.\n"}
{"id": "21589312", "url": "https://en.wikipedia.org/wiki?curid=21589312", "title": "Eurosolar", "text": "Eurosolar\n\nEurosolar is the non-profit European Association for Renewable Energy () that conducts its work independently of political parties, institutions, commercial enterprises, and interest groups. Eurosolar develops and encourages political and economic action plans and concepts for the introduction of renewable energy. The association has offices in 14 countries that include Austria, Italy, Turkey, and Ukraine. Eurosolar has approximately 2,500 members, close to 400 legal groups, and owns the \"Solar Age\" magazine, released on a quarterly basis. A history of the association is available.\n\nFormed on 2 August 1988 in Bonn, West Germany, Eurosolar runs an annual event called the Solar Prize awards, rewarding progress in renewable energy.\n\n"}
{"id": "1471037", "url": "https://en.wikipedia.org/wiki?curid=1471037", "title": "Graphical timeline of the Big Bang", "text": "Graphical timeline of the Big Bang\n\nThis timeline of the Big Bang shows a sequence of events as currently theorized by scientists. \n\nIt is a logarithmic scale that shows formula_1 \"second\" instead of \"second\". For example, one microsecond is formula_2. To convert −30 read on the scale to second calculate formula_3 second = one millisecond. On a logarithmic time scale a step lasts ten times longer than the previous step.\n\n"}
{"id": "4132781", "url": "https://en.wikipedia.org/wiki?curid=4132781", "title": "Greg Fahy", "text": "Greg Fahy\n\nGregory M. Fahy is a cryobiologist and biogerontologist, and is also Vice President and Chief Scientific Officer at Twenty-First Century Medicine, Inc. Fahy is the world's foremost expert in organ cryopreservation by vitrification. Fahy introduced the modern successful approach to vitrification for cryopreservation in cryobiology and he is widely credited, along with William F. Rall, for introducing vitrification into the field of reproductive biology.\n\nIn the summer of 2005, where he was a keynote speaker at the annual Society for Cryobiology meeting, Fahy announced that Twenty-First Century Medicine had successfully cryopreserved a rabbit kidney at -130 °C by vitrification and transplanted it into a rabbit after rewarming, with subsequent long-term life support by the vitrified-rewarmed kidney as the sole kidney. This research breakthrough was later published in the peer-reviewed journal \"Organogenesis\".\n\nFahy is also a well-known biogerontologist and is the originator and Editor-in-Chief of \"The Future of Aging: Pathways to Human Life Extension\", a multi-authored book on the future of biogerontology. He currently serves on the editorial boards of \"Rejuvenation Research\" and the \"Open Geriatric Medicine Journal\" and served for 16 years as a Director of the American Aging Association and for 6 years as the editor of AGE News, the organization's newsletter.\n\nFahy has over thirty years of experience in the field of cryopreservation. As a scientist with the American Red Cross, he was the originator of the first practical method of cryopreservation by vitrification and the inventor of computer-based systems to apply this technology to whole organs. Before joining Twenty-First Century Medicine, he was the chief scientist for Organ, Inc and of LRT, Inc. He was also Head of the Tissue Cryopreservation Section of the Transfusion and Cryopreservation Research Program of the U.S. Naval Medical Research Institute in Bethesda, Maryland where he spearheaded the original concept of ice blocking agents. In 2014, he was named a Fellow of the Society for Cryobiology in recognition of the impact of his work in low temperature biology.\n\nA native of California, Fahy holds a Bachelor of Science degree in Biology from the University of California, Irvine and a Ph.D. in pharmacology from the Medical College of Georgia in Augusta.\n\nHe currently serves on the board of directors of two organizations and as a referee for numerous scientific journals and funding agencies, and holds 35 patents on cryopreservation methods, aging interventions, transplantation, and other topics.\n\nGreg Fahy was named as a Fellow of the Society for Cryobiology in 2014, and in 2010 he received the Distinguished Scientist Award for Reproductive Biology from the Reproductive Biology Professional Group of the American Society of Reproductive Medicine. More recently, he received the Cryopreservation Award from the International Longevity and Cryopreservation Summit held in Madrid, Spain in 2017 in recognition of his career in and dedication to the field of cryobiology. Dr. Fahy also received the Grand Prize for Medicine from INPEX in 1995 for his invention of computerized organ cryoprotectant perfusion technology. In 2005, he was recognized as a Fellow of the American Aging Association.\n\n\n"}
{"id": "17385453", "url": "https://en.wikipedia.org/wiki?curid=17385453", "title": "Gérard Magnin", "text": "Gérard Magnin\n\nGérard Magnin (born 1951) was Executive Director of Energy Cities between 1994 and 2014, after having been the original founder of this European network that brings together about 1000 local authorities from 30 European countries. He has been Regional Director for the French Agency for the Environment and Energy Management (ADEME) in Franche-Comté region (FR) for ten years. Initially trained in electrical engineering, followed by economics and politics, he taught economic and social sciences for eight years.\n\nA supporter of social innovation, he strives to integrate the energy and climate issue into the concerns of all of society’s stakeholders, especially those of local authorities. He regularly highlights the fact that the major innovations came from the field and that the networking of experiments and those responsible for them is a powerful factor for change. He continues to persuade national and European levels to acknowledge the fact that none of the EU objectives can be achieved without heavy involvement on behalf of the local authorities.\n\nHe was placed as a director on the board of EDF under request of the French government, but resigned in 2016 prior to a vote that supported Hinkley Point C nuclear reactor which he called 'very risky'.\n\n"}
{"id": "293270", "url": "https://en.wikipedia.org/wiki?curid=293270", "title": "Heat wave", "text": "Heat wave\n\nA heat wave is a period of excessively hot weather, which may be accompanied by high humidity, especially in oceanic climate countries. While definitions vary, a heat wave is usually measured relative to the usual weather in the area and relative to normal temperatures for the season. Temperatures that people from a hotter climate consider normal can be termed a heat wave in a cooler area if they are outside the normal climate pattern for that area.\n\nThe term is applied both to hot weather variations and to extraordinary spells of hot which may occur only once a century. Severe heat waves have caused catastrophic crop failures, thousands of deaths from hyperthermia, and widespread power outages due to increased use of air conditioning. A heat wave is considered extreme weather, and a danger because heat and sunlight may overheat the human body. Heat waves can usually be detected using forecasting instruments so that a warning call can be issued.\n\nA definition based on Frich et al.'s Heat Wave Duration Index is that a heat wave occurs when the daily maximum temperature of more than five consecutive days exceeds the average maximum temperature by , the normal period being 1961–1990.\n\nA formal, peer-reviewed definition from the \"Glossary of Meteorology\" is:\nIn the Netherlands, a heat wave is defined as a period of at least 5 consecutive days in which the maximum temperature in De Bilt exceeds , provided that on at least 3 days in this period the maximum temperature in De Bilt exceeds . This definition of a heat wave is also used in Belgium and Luxembourg.\n\nIn Denmark, a national heat wave (\"hedebølge\") is defined as a period of at least 3 consecutive days of which period the average maximum temperature across more than fifty percent of the country exceeds – the Danish Meteorological Institute further defines a \"warmth wave\" (\"varmebølge\") when the same criteria are met for a temperature, while in Sweden, a heat wave is defined as at least 5 days in a row with a daily high exceeding .\n\nIn the United States, definitions also vary by region; however, a heat wave is usually defined as a period of at least two or more days of excessively hot weather. In the Northeast, a heat wave is typically defined as three consecutive days where the temperature reaches or exceeds , but not always as this ties in with humidity levels to determine a heat index threshold. The same does not apply to drier climates. A heat storm is a Californian term for an extended heat wave. Heat storms occur when the temperature reaches for three or more consecutive days over a wide area (tens of thousands of square miles). The National Weather Service issues heat advisories and excessive heat warnings when unusual periods of hot weather are expected.\n\nIn Adelaide, South Australia, a heat wave is defined as five consecutive days at or above , or three consecutive days at or over . The Australian Bureau of Meteorology defines a heat wave as \"three days or more of maximum and minimum temperatures that are unusual for the location\". Until the introduction of this new Pilot Heatwave Forecast there was no national definition that described heatwave or measures of heatwave severity.\n\nIn the United Kingdom, the Met Office operates a Heat Health Watch system which places each Local Authority region into one of four levels. Heatwave conditions are defined by the maximum daytime temperature and minimum nighttime temperature rising above the threshold for a particular region. The length of time spent above that threshold determines the particular level. Level 1 is normal summer conditions. Level 2 is reached when there is a 60% or higher risk that the temperature will be above the threshold levels for two days and the intervening night. Level 3 is triggered when the temperature has been above the threshold for the preceding day and night, and there is a 90% or higher chance that it will stay above the threshold in the following day. Level 4 is triggered if conditions are more severe than those of the preceding three levels. Each of the first three levels is associated with a particular state of readiness and response by the social and health services, and Level 4 is associated with more widespread response.\n\nA more general indicator that allows comparing heat waves in different regions of the World, characterized by different climates, has been recently developed. This was used to estimate heat waves occurrence at the global scale from 1901 to 2010, finding a substantial and sharp increase in the amount of affected areas in the last two decades.\n\nHeat waves form when high pressure aloft (from ) strengthens and remains over a region for several days up to several weeks. This is common in summer (in both Northern and Southern Hemispheres) as the jet stream 'follows the sun'. On the equator side of the jet stream, in the upper layers of the atmosphere, is the high pressure area.\n\nSummertime weather patterns are generally slower to change than in winter. As a result, this upper level high pressure also moves slowly. Under high pressure, the air subsides (sinks) toward the surface, warming and drying adiabatically. This warmer sinking air creates a high level inversion that acts as a dome capping the atmosphere, inhibiting convection, thereby trapping high humidity warm air below it. Typically, convection is present along the periphery of the cap where the pressure becomes less. This peripheral convection, however, can add to the high pressure dome by ventilating the upper level outflow of the thunderstorms into it. The end result is a continual build-up of heat at the surface that people experience as a heat wave.\n\nIn the Eastern United States a heat wave can occur when a high pressure system originating in the Gulf of Mexico becomes stationary just off the Atlantic Seaboard (typically known as a Bermuda High). Hot humid air masses form over the Gulf of Mexico and the Caribbean Sea while hot dry air masses form over the desert Southwest and northern Mexico. The SW winds on the back side of the High continue to pump hot, humid Gulf air northeastward resulting in a spell of hot and humid weather for much of the Eastern States.\n\nIn the Western Cape Province of South Africa, a heat wave can occur when a low pressure offshore and high pressure inland combine to form a Bergwind. The air warms as it descends from the Karoo interior, and the temperature will rise about 10 °C from the interior to the coast. Humidities are usually very low, and the temperatures can be over 40 °C in summer. The highest official temperatures recorded in South Africa (51.5 °C) was recorded one summer during a bergwind occurring along the Eastern Cape coastline.\n\nGlobal warming boosts the probability of extreme weather events, like heat waves, far more than it boosts more moderate events.\n\nThe \"heat index\" (as shown in the table above) is a measure of how hot it feels when relative humidity is factored with the actual air temperature.\nHyperthermia, also known as heat stroke, becomes commonplace during periods of sustained high temperature and humidity. Sweating is absent from 84–100% of those affected. Older adults, very young children, and those who are sick or overweight are at a higher risk for heat-related illness. The chronically ill and elderly are often taking prescription medications (e.g., diuretics, anticholinergics, antipsychotics, and antihypertensives) that interfere with the body's ability to dissipate heat.\n\nHeat edema presents as a transient swelling of the hands, feet, and ankles and is generally secondary to increased aldosterone secretion, which enhances water retention. When combined with peripheral vasodilation and venous stasis, the excess fluid accumulates in the dependent areas of the extremities. The heat edema usually resolves within several days after the patient becomes acclimated to the warmer environment. No treatment is required, although wearing support stockings and elevating the affected legs will help minimize the edema.\n\nHeat rash, also known as prickly heat, is a maculopapular rash accompanied by acute inflammation and blocked sweat ducts. The sweat ducts may become dilated and may eventually rupture, producing small pruritic vesicles on an erythematous base. Heat rash affects areas of the body covered by tight clothing. If this continues for a duration of time it can lead to the development of chronic dermatitis or a secondary bacterial infection. Prevention is the best therapy. It is also advised to wear loose-fitting clothing in the heat. However, once heat rash has developed, the initial treatment involves the application of chlorhexidine lotion to remove any desquamated skin. The associated itching may be treated with topical or systemic antihistamines. If infection occurs a regimen of antibiotics is required.\n\nHeat syncope is related to heat exposure that produces orthostatic hypotension. This hypotension can precipitate a near-syncopal episode. Heat syncope is believed to result from intense sweating, which leads to dehydration, followed by peripheral vasodilation and reduced venous blood return in the face of decreased vasomotor control. Management of heat syncope consists of cooling and rehydration of the patient using oral rehydration therapy (sport drinks) or isotonic IV fluids. People who experience heat syncope should avoid standing in the heat for long periods of time. They should move to a cooler environment and lie down if they recognize the initial symptoms. Wearing support stockings and engaging in deep knee-bending movements can help promote venous blood return.\n\nHeat exhaustion is considered by experts to be the forerunner of heat stroke (hyperthermia). It may even resemble heat stroke, with the difference being that the neurologic function remains intact. Heat exhaustion is marked by excessive dehydration and electrolyte depletion. Symptoms may include diarrhea, headache, nausea and vomiting, dizziness, tachycardia, malaise, and myalgia. Definitive therapy includes removing patients from the heat and replenishing their fluids. Most patients will require fluid replacement with IV isotonic fluids at first. The salt content is adjusted as necessary once the electrolyte levels are known. After discharge from the hospital, patients are instructed to rest, drink plenty of fluids for 2–3 hours, and avoid the heat for several days. If this advice is not followed it may then lead to heat stroke.\n\nOne public health measure taken during heat waves is the setting-up of air-conditioned public cooling centers.\n\nHeat waves are the most lethal type of weather phenomenon in the United States. Between 1992 and 2001, deaths from excessive heat in the United States numbered 2,190, compared with 880 deaths from floods and 150 from hurricanes. The average annual number of fatalities directly attributed to heat in the United States is about 400. The 1995 Chicago heat wave, one of the worst in US history, led to approximately 739 heat-related deaths over a period of five days. Eric Klinenberg has noted that in the United States, the loss of human life in hot spells in summer exceeds that caused by all other weather events combined, including lightning, rain, floods, hurricanes, and tornadoes. Despite the dangers, Scott Sheridan, professor of geography at Kent State University, found that less than half of people 65 and older abide by heat-emergency recommendations like drinking lots of water. In his study of heat-wave behavior, focusing particularly on seniors in Philadelphia, Phoenix, Toronto, and Dayton, Ohio, he found that people over 65 \"don't consider themselves seniors.\" One of his older respondents said: \"Heat doesn't bother me much, but I worry about my neighbors.\"\n\nAccording to the Agency for Health care Research and Quality, about 6,200 Americans are hospitalized each summer due to excessive heat, and those at highest risk are poor, uninsured or elderly. More than 70,000 Europeans died as a result of the 2003 European heat wave.\n\nConcern is now focusing on predicting the future likelihood of heat waves and their severity. In addition, because in most of the world most of those suffering the impacts of a heat wave will be inside a building, and this will modify the temperatures they are exposed to, there is the need to link climate models to building models. This means producing example time series of future weather. Other work has shown that future mortality due to heat waves could be reduced if buildings were better designed to modify the internal climate, or the occupants better educated about the issues, so they took action in time.\n\nThe number of heat fatalities is likely highly underreported due to lack of reports and misreports. Part of the mortality observed during a heat wave, however, can be attributed to a so-called \"harvesting effect\", a term for a \"short-term forward mortality displacement\". It has been observed that for some heat waves, there is a compensatory decrease in overall mortality during the subsequent weeks after a heat wave. Such compensatory reduction in mortality suggests that heat affects especially those so ill that they \"would have died in the short term anyway\".\n\nAnother explanation for underreporting is the social attenuation in most contexts of heat waves as a health risk. As shown by the deadly French heat wave in 2003, heat wave dangers result from the intricate association of natural and social factors.\n\nIn addition to physical stress, excessive heat causes psychological stress, to a degree which affects performance, and is also associated with an increase in violent crime. High temperatures are associated with increased conflict both at the interpersonal level and at the societal level. In every society, crime rates go up when temperatures go up, particularly violent crimes such as assault, murder, and rape. Furthermore, in politically unstable countries, high temperatures are an aggravating factor that lead toward civil wars.\n\nAdditionally, high temperatures have a significant effect on income. A study of counties in the United States found that economic productivity of individual days declines by about 1.7% for each degree Celsius above .\n\nAbnormally hot temperatures cause electricity demand to increase during the peak summertime hours of 4 to 7 p.m. when air conditioners are straining to overcome the heat. If a hot spell extends to three days or more, however, nighttime temperatures do not cool down, and the thermal mass in homes and buildings retains the heat from previous days. This heat build-up causes air conditioners to turn on earlier and to stay on later in the day. As a result, available electricity supplies are challenged during a higher, wider, peak electricity consumption period.\n\nHeat waves often lead to electricity spikes due to increased air conditioning use, which can create power outages, exacerbating the problem. During the 2006 North American heat wave, thousands of homes and businesses went without power, especially in California. In Los Angeles, electrical transformers failed, leaving thousands without power for as long as five days.\nThe 2009 South Eastern Australia Heat Wave caused the city of Melbourne, Australia to experience some major power disruptions which left over half a million people without power as the heat wave blew transformers and overloaded a power grid.\n\nIf a heat wave occurs during a drought, which dries out vegetation, it can contribute to bushfires and wildfires. During the disastrous heat wave that struck Europe in 2003, fires raged through Portugal, destroying over or of forest and or of agricultural land and causing an estimated €1 billion worth of damage. High end farmlands have irrigation systems to back up crops with.\n\nHeat waves can and do cause roads and highways to buckle and melt, water lines to burst, and power transformers to detonate, causing fires. See the 2006 North American heat wave article about heat waves causing physical damage.\n\nHeat waves can also damage rail roads, such as buckling and kinking rails, which can lead to slower traffic, delays, and even cancellations of service when rails are too dangerous to traverse by trains. Sun kinking is caused when certain types of rail design like short section rails welded together or fish plate rails expand and push on other sections of rail causing them to warp and kink. Sun kinking can be a serious problem in hotter climates like Southern USA, parts of Canada, the Middle East, etc.\n\nIn the 2013 heatwave in England, gritters (normally only seen in snow) were sent out to grit melting tarmac roads.\n\n\n\n"}
{"id": "29853326", "url": "https://en.wikipedia.org/wiki?curid=29853326", "title": "Henderson Station One", "text": "Henderson Station One\n\nHenderson Station One (HMP&L One) was a coal-fired power station owned and operated by the city of Henderson, Kentucky.\n\nCiting rising costs and mounting environmental regulations, Henderson Municipal Power and Light closed its 58-year-old Station One power plant on Water Street by December 31, 2008.\n\nIt has since been replaced by Henderson Station Two (HMP&L Two) as a part of Sebree Station.\n\n\n"}
{"id": "6168723", "url": "https://en.wikipedia.org/wiki?curid=6168723", "title": "Heterodiamond", "text": "Heterodiamond\n\nHeterodiamond is a superhard material containing boron, carbon, and nitrogen (BCN). It is formed at high temperatures and high pressures, e.g., by application of an explosive shock wave to a mixture of diamond and cubic boron nitride (c-BN). The heterodiamond is a polycrystalline material coagulated with nano-crystallites and the fine powder is tinged with deep bluish black. The heterodiamond has both the high hardness of diamond and the excellent heat resistance of cubic BN. These characteristic properties are due to the diamond structure combined with the sp σ-bonds among carbon and the hetero atoms.\n\nCubic BCN can be synthesized from graphite-like BCN at pressures above 18 GPa and temperatures higher than 2200 K. The bulk modulus of c-BCN is 282 GPa which is one of the highest bulk moduli known for any solid, and is exceeded only by the bulk moduli of diamond and c-BN. The hardness of c-BCN is higher than that of c-BN single crystals.\n\n"}
{"id": "29565745", "url": "https://en.wikipedia.org/wiki?curid=29565745", "title": "Humberto Ríos Labrada", "text": "Humberto Ríos Labrada\n\nHumberto Ríos Labrada is a Cuban folk musician, agricultural scientist and environmentalist. He was awarded the Goldman Environmental Prize in 2010, for his work for biodiversity and sustainable development of Cuban agriculture.\n"}
{"id": "2167518", "url": "https://en.wikipedia.org/wiki?curid=2167518", "title": "Kamptulicon", "text": "Kamptulicon\n\nKamptulicon, whose name was derived from the Greek \"kamptos\" (“flexible”) + \"oulos\" (“thick”), was a floor covering made from powdered cork and natural rubber.\n\nFirst patented by Elijah Galloway in 1843, kamptulicon was first launched in public at the 1862 International Exhibition in London, where it caused a sensation. Its promoters compared it to thick, soft leather, and lauded its ease of cleaning, water resistance, warmth, and sound-deadening qualities. Critics, however, pointed out that its grey-brown colour was unattractive. Attempts were made to brighten it up by stencilling patterns on it with oil paint, but these suffered from a lack of durability.\n\nKamptulicon was manufactured by sprinkling powdered cork on to thin bands of rubber, which was then rolled and rerolled until thoroughly mixed. It was then coated on one or both sides with linseed oil varnish or oil paint. Powdered sulphur was also sometimes mixed in, and the material then heated to produce a form of vulcanized kamptulicon.\n\nAs well as a floor covering, kamptulicon was also used as cushions in stamping presses, and as polishing wheels for metals.\n\nWithin a few years, faced by stiff competition from the manufacturers of oilcloth coupled with huge increases in the price of natural rubber, kamptulicon production ceased.\n\n"}
{"id": "51276804", "url": "https://en.wikipedia.org/wiki?curid=51276804", "title": "Khalid El-Aabidi", "text": "Khalid El-Aabidi\n\nKhalid El-Aabidi (born 14 September 1995) is a Moroccan Olympic weightlifter. He represented his country at the 2016 Summer Olympics.\n"}
{"id": "21055946", "url": "https://en.wikipedia.org/wiki?curid=21055946", "title": "Kraftwerke Oberhasli", "text": "Kraftwerke Oberhasli\n\nKraftwerke Oberhasli AG (KWO) is a Swiss energy supply company, based in Innertkirchen and operating several hydroelectric plants in the Oberhasli area of the Canton of Berne. It also operates a number of tourist attractions in the same area, mostly with some relationship to its energy supply business. It uses the brands Grimselstrom and Grimselwelt, the latter specifically for its tourism ventures. Both brands are named after the Grimsel Pass that forms the upper end of its operating area.\n\nFounded in 1925, the company had its first power plant, \"Handeck 1\", online by 1932. Currently, KWO is operating nine plants, fed by the reservoirs of Grimselsee, Oberaarsee, Räterichsbodensee, Gelmersee and Totensee, with a total of 26 turbines giving a total maximum power output of 1.125 Gigawatts. The company produces around 2,350 Gigawatt hours of electricity annually, which represents about 7% of the country's total hydroelectric energy production.\n\nThe company also owns the Meiringen–Innertkirchen railway, the Gelmerbahn funicular, and the Sidelhornbahn, Triftbahn and Tällibahn aerial lifts. All of these were originally built to assist the construction and maintenance of its hydroelectric plants, but now provide transport for local residents and/or tourists. Additionally the Kraftwerke Oberhasli operates and maintains the Reichenbachfall Funicular, although this is owned by the neighbouring EWR Energie company.\n\nKWO employs 530 people, or a total of 364 full-time jobs.\n\n"}
{"id": "31062930", "url": "https://en.wikipedia.org/wiki?curid=31062930", "title": "Methods in Ecology and Evolution", "text": "Methods in Ecology and Evolution\n\nMethods in Ecology and Evolution is a peer-reviewed scientific journal focused on advancing new methodologies in ecology and evolution. It began publication in 2010, and is the second youngest journal of the British Ecological Society. \"Methods in Ecology and Evolution\" is only available online. According to the 2014 \"Journal Citation Reports\", the journal was ranked 9th out of 149 journals in the category \"Ecology\".\n\n\"Methods in Ecology and Evolution\" publishes methodological papers from all areas of ecology and evolution, including statistical and theoretical methods, methods in phylogenetics, and fieldwork methodologies.\n\nAs well as publishing research papers, the journal encourages the publication of supplementary material such as computer code, methodological tutorials, and explanatory videos. A regular podcast is produced by the editorial office, in which selected authors are asked to discuss their papers in greater depth.\n\n"}
{"id": "40766961", "url": "https://en.wikipedia.org/wiki?curid=40766961", "title": "Minimum control speeds", "text": "Minimum control speeds\n\nThe minimum control speed (V) of an aircraft (specifically an airplane) is a V-speed that specifies the calibrated airspeed below which directional or lateral control of the aircraft can no longer be maintained, after the failure of one or more engines. The V only applies if at least one engine is still operative, and will depend on the stage of flight. Indeed, multiple Vs have to be calculated for landing, air travel, and ground travel, and there are more still for aircraft with four or more engines. These are all included in the aircraft flight manual of all multi-engine aircraft. When design engineers are sizing an airplane's vertical tail and flight control surfaces, they have to take into account the effect this will have on the airplane's minimum control speeds.\n\nMinimum control speeds are typically established by flight tests as part of an aircraft certification process. They provide a guide to the pilot in the safe operation of the aircraft.\n\nWhen an engine on a multi-engine aircraft fails, the thrust distribution on the aircraft becomes asymmetrical, resulting in a yawing moment in the direction of the failed engine. A sideslip develops, causing the total drag of the aircraft to increase considerably, resulting in a drop in the aircraft's rate of climb. The rudder, and to a certain extent the ailerons via the use of bank angle, are the only aerodynamic controls available to the pilot to counteract the asymmetrical thrust yawing moment.\n\nThe higher the speed of the aircraft, the easier it is to counteract the yawing moment using the aircraft's controls. The minimum control speed is the airspeed below which the force the rudder or ailerons can apply to the aircraft is not large enough to counteract the asymmetrical thrust at a maximum power setting. Above this speed it should be possible to maintain control of the aircraft and maintain straight flight with asymmetrical thrust.\n\nLoss of engine power of wing mount propellor aircraft and blown lift aircraft affects the lift distribution over the wing, causing a roll toward the inoperative engine. In some aircraft roll authority is more limiting than rudder authority in determining Vs.\n\nAviation regulations (such as FAR and EASA) define several different Vs and require design engineers to size the vertical tail and the aerodynamic flight controls of the aircraft to comply with these regulations. The minimum control speed in the air (V) is the most important minimum control speed of a multi-engine aircraft, which is why V is simply listed as V in many aviation regulations and aircraft flight manuals. On the airspeed indicator of a twin-engine aircraft of less than 2722 kg, the V is indicated by a red radial line, as standardised by FAR 23. \nMost test pilot schools use multiple, more specific minimum control speeds, as V will change depending on the stage of flight. Other defined Vs include minimum control speed on the ground (V) and minimum control speed during approach and landing (V). In addition, with aircraft with four or more engines, Vs exist for cases with either one or two engines inoperative on the same wing. Figure 1 illustrates the Vs that are defined in the relevant civil aviation regulations and in military specifications.\n\nThe vertical tail or vertical stabilizer of a multi-engine aircraft plays a crucial role in maintaining directional control while an engine fails or is inoperative. The larger the tail, the more capable it will be of providing the required force to counteract the asymmetrical thrust yawing moment. This means that the smaller the tail is, the higher the V will be. However, a larger tail is more costly and harder to accommodate, and comes with other aerodynamic issues such as increased prevalence of slipstreams. Engineers designing the vertical tail must make a decision based on, amongst other factors, their budget, the weight of the aircraft, and the maximum bank angle of 5° (away from the inoperative engine), as stated by FAR.\n\nV is also used to calculate the minimum takeoff safety speed. A high V therefore results in higher takeoff speeds, and so longer runways are required, which is undesirable for airport operators.\n\nAny factor that has influence on the balance of forces and on the yawing and rolling moments after engine failure might also affect Vs. When the vertical tail is designed and the V is measured, the worst-case scenario for all factors is taken into account. This ensures that the Vs published in the AFMs guaranteed to be safe.\n\nHeavier aircraft are more stable and more resistant to yawing moments, and therefore have lower Vs. The longitudinal centre of gravity affects the V as well: the further from the tail it is, the lower the minimum control speed, because the rudder will be able to provide a larger yawing moment, and so it is easier to counteract the imbalance in thrust. The lateral centre of gravity also has an effect: the nearer the inoperative engine it is, the larger the moment of the working engine, and so the more force the rudder has to apply. This means that if the lateral centre of gravity shifts towards the inoperative engine, the aircraft's V will increase. The thrust of most engines depends on altitude and temperature; increasing altitude and higher temperatures decrease thrust. This means that if the air temperature is higher and the aircraft has a higher altitude, the force of the operative engine will be lower, the rudder will have to provide less counteractive force, and so the V will be lower. The bank angle also influences the minimum control speed. A small bank angle away from the inoperative engine is required for smallest possible sideslip and therefore lower V. Finally, if the P-factor of the working engine increases, then its yawing moment increases, and the aircraft's V increases as a result.\n\nAircraft with four or more engines have not only a V (often called V under these circumstances), where the critical engine alone is inoperative, but also a V that applies when the engine inboard of the critical engine, on the same wing, is also inoperative. Civil aviation regulations (FAR, CS and equivalent) no longer require a V to be determined, although it is still required for military aircraft with four or more engines. On turbojet and turbofan aircraft, the outboard engines are usually equally critical. Three-engine aircraft such as the MD-11 and BN-2 Trislander do not have a V; a failed centerline engine has no effect on V.\n\nWhen two opposing engines of aircraft with four or more engines are inoperative, there is no thrust asymmetry, hence there is no rudder requirement for maintaining steady straight flight; Vs play no role. There may be less power available to maintain flight overall, but the minimum safe control speeds remain the same as they would be for an aircraft being flown at 50% thrust on all four engines.\n\nFailure of a single inboard engine, from a set of four, has a much smaller effect on controllability. This is because an inboard engine is closer to the aircraft's centre of gravity, so the lack of yawing moment is decreased. In this situation, if speed is maintained at or above the published V, as determined for the critical engine, safe control can be maintained.\n\nIf an engine fails during taxiing or takeoff, the thrust yawing moment will force the aircraft to one side on the runway. If the airspeed is not high enough and hence, the rudder-generated side force is not powerful enough, the aircraft will deviate from the runway centerline and may even veer off the runway. The airspeed at which the aircraft, after engine failure, deviates 9.1 m from the runway centerline, despite using maximum rudder but without the use of nose wheel steering, is the minimum control speed on the ground (V).\n\nThe minimum control speed during approach and landing (V) is similar to V, but the aircraft configuration is the landing configuration. V is defined for part 25 aircraft only in civil aviation regulations. However, when maximum thrust is selected for a go-around, the flaps will be selected up from the landing position, and V no longer applies, but V does.\n\nDue to the inherent risks of operating at or close to VMC with asymmetric thrust, and the desire to simulate and practise these manoeuvres in pilot training and certification V may be defined. V safe single-engine speed is the minimum speed to intentionally render the critical engine inoperative, established and designated by the manufacturer as the safe, intentional, one engine inoperative speed. This speed is selected to reduce the accident potential from loss of control due to simulated engine failures at inordinately slow airspeed.\n"}
{"id": "25925571", "url": "https://en.wikipedia.org/wiki?curid=25925571", "title": "Montreal Refinery", "text": "Montreal Refinery\n\nThe Montreal Refinery is an oil refinery located in the city of Montreal inside the Rivière-des-Prairies–Pointe-aux-Trembles borough. The refinery is not far from the Montreal East Refinery. This refinery is the largest Suncor Energy refinery.\n\n"}
{"id": "48634391", "url": "https://en.wikipedia.org/wiki?curid=48634391", "title": "Non ideal compressible fluid dynamics", "text": "Non ideal compressible fluid dynamics\n\nNon ideal compressible fluid dynamics, abbreviated as NICFD, is a branch of fluid mechanics studying the actual characteristics of dense vapors, supercritical flows and compressible two-phase flows, namely whereby the thermodynamic behavior of the fluid differs considerably from that of a perfect gas. At high reduced pressure and temperature, close to the saturation curve the speed of sound is largely sensitive to density variations along isentropes. Consequently, the fluid flow departs from the ideality assumption and under particular conditions may even exhibit non classical gas dynamic phenomena, whose nature is governed by the value of the fundamental derivative of gas-dynamics Γ. A non-monotonic Mach number trend along an expansion is typical for 0 < Γ < 1, while for Γ < 0 values admit the occurrence of inverse gas-dynamics phenomena such as rarefaction shock waves , splitting waves or even composite waves. Inverse gas-dynamics behavior has been theoretically predicted for heavy complex molecules in the vapor region,and a recent study discovered that two-phase rarefaction shock waves are physically realizable close to the critical point.\n\nApplication NICFD flows can be found in numerous fields. Basically, they are encountered in Organic Rankine Cycle (ORC) turbogenerators, refrigerator, supercritical carbon dioxide power system, pharmaceutical processes, transportation of fuels at high-speed, and in transonic and supersonic wind tunnels.\n"}
{"id": "41807186", "url": "https://en.wikipedia.org/wiki?curid=41807186", "title": "Perchloratoborate", "text": "Perchloratoborate\n\nPerchloratoborate is an anion of the form [B(ClO)]. It can form partly stable solid salts with heavy alkali metals. They are more stable than nitratoborate salts. K[B(ClO)] decomposes at 35 °C, Rb[B(ClO)] is stable to 50 °C, and Cs[B(ClO)] can exist up to 80 °C.\n\nPerchloratoborates are analogous to perchloratoaluminates ([Al(ClO)]).\n\nAnother related anion is the chloroperchloratoborate, ClB(ClO).\n\nBoron perchlorate itself is unstable above −5 °C.\n\nOn thermal decomposition the alkali perchloratoborate salts form an alkali perchlorate, and boron trioxide as a solid residue, and gas containing dichlorine heptoxide, chlorine dioxide, chlorine, and oxygen.\n\nWhen the alkali perchloratoborates first start to decompose at the lower temperatures, the reaction is endothermic, and dichlorine heptoxide is formed. However, if caesium perchloratoborate is heated the decomposition becomes exothermic above 90 °C, and at 100 °C it explodes exothermically forming chlorine and oxygen.\n\nWhen rubidium perchloratoborate is reacted with extra perchloric acid, it forms RbBO(ClO).\n\nIn water, alkali perchloratoborates decompose exothermically to form boric acid, perchloric acid, and the perchlorate.\n\nNitronium perchloratoborate (NOB(ClO))can be formed by reacting nitronium perchlorate with boron trichloride in solution. Similarly ammonium perchlorate reacts with BCl forming ammonium perchloratoborate.\n\nThe metal perchloratoborates can also be formed from the metal perchlorate dissolved in anhydrous perchloric acid reacting with boron trichloride. Another way is to react a metal chloridoborate (MBCl) with perchloric acid. Chloridoborates can be made from the metal chloride and boron trichloride dissolved in nitrosyl chloride.\nExtra ClO drives the reaction forward.\n\nAlso formed is BClClO and BCl(ClO) which disproportionates above −78 °C to the boron perchlorate and dichloroboron perchlorate.\n\nCaesium perchloratoborate is hydroscopic. It has a density of 2.5 g/cm. It has no colour.\n\nInfrared absorption bands are observed in caesium perchloratoborate at 640 and 1,087 cm.\n\nPotassium perchloratoborate has density 2.18 g/cm, and rubidium perchloratoborate has density 2.32 g/cm.\n\nThe three alkali perchloratoborates fume in moist air, are all crystalline and colourless.\n"}
{"id": "19650570", "url": "https://en.wikipedia.org/wiki?curid=19650570", "title": "Pneumobilia", "text": "Pneumobilia\n\nPneumobilia is the presence of gas in the biliary system. It is typically detected on a radiographic imaging exam, such as ultrasound, CT, or MRI. It is a common finding in patients that have recently undergone biliary surgery or endoscopic biliary procedure. While the presence of air within biliary system is not harmful, this finding may alternatively suggest a pathological process, such as a biliary-enteric anastomosis, an infection of the biliary system, an incompetent sphincter of Oddi, or spontaneous biliary-enteric fistula.\n\nIn a healthy individual with normal anatomy, there is no air within the biliary tree. When this finding is present, it may be secondary to:\n\nOther rare causes that have been reported include duodenal diverticulum, paraduodenal abscess, operative trauma, and carcinoma of the duodenum, stomach and bile duct.\n"}
{"id": "4771910", "url": "https://en.wikipedia.org/wiki?curid=4771910", "title": "Pour point", "text": "Pour point\n\nThe pour point of a liquid is the temperature below which the liquid loses its flow characteristics. In crude oil a high pour point is generally associated with a high paraffin content, typically found in crude deriving from a larger proportion of plant material. That type of crude oil is mainly derived from a kerogen Type III.\n\nASTM D97, Standard Test Method for Pour Point of Crude Oils. The specimen is cooled inside a cooling bath to allow the formation of paraffin wax crystals. At about 9 °C above the expected pour point, and for every subsequent 3 °C, the test jar is removed and tilted to check for surface movement. When the specimen does not flow when tilted, the jar is held horizontally for 5 sec. If it does not flow, 3 °C is added to the corresponding temperature and the result is the pour point temperature.\n\nIt is also useful to note that failure to flow at the pour point may also be due to the effect of viscosity or the previous thermal history of the specimen. Therefore, the pour point may give a misleading view of the handling properties of the oil. Additional fluidity tests may also be undertaken. An approximate range of pour point can be observed from the specimen's upper and lower pour point.\n\nASTM D5949, Standard Test Method for Pour Point of Petroleum Products (Automatic Pressure Pulsing Method) is an alternative to the manual test procedure. It uses automatic apparatus and yields pour point results in a format similar to the manual method (ASTM D97) when reporting at a 3 °C.\n\nThe D5949 test method determines the pour point in a shorter period of time than manual method D97. Less operator time is required to run the test using this automatic method. Additionally, no external chiller bath or refrigeration unit is needed. D5949 is capable of determining pour point within a temperature range of −57 °C to +51 °C. Results can be reported at 1 °C or 3 °C testing intervals. This test method has better repeatability and reproducibility than manual method D97.\n\nUnder ASTM D5949, the test sample is heated and then cooled by a Peltier device at a rate of 1.5±0.1 °C/min. At either 1 °C or 3 °C intervals, a pressurized pulse of compressed gas is imparted onto the surface of the sample. Multiple optical detectors continuously monitor the sample for movement. The lowest temperature at which movement is detected on the sample surface is determined to be the pour point.\n\nTwo pour points can be derived which can give an approximate temperature window depending on its thermal history. Within this temperature range, the sample may appear liquid or solid. This peculiarity happens because wax crystals form less readily when it has been heated within the past 24 hrs and contributes to the lower pour point.\n\nThe upper pour point is measured by pouring the test sample directly into a test jar. The sample is then cooled and inspected for pour point as per the usual pour point method. The method usually gives higher pour point because the thermal history has not been cancelled by a prolonged thermal treatment.\n\nThe lower pour point is measured by first pouring the sample into a stainless steel pressure vessel. The vessel is then screwed tight and heated to above 102 °C in an oil bath. After a specified time, the vessel is removed and cooled for a short while. The sample is then poured into a test jar and immediately closed with a cork carrying the thermometer. The sample is then cooled and then inspected for pour point as per the usual pour point method.\n\n\n"}
{"id": "6486771", "url": "https://en.wikipedia.org/wiki?curid=6486771", "title": "Q-pit", "text": "Q-pit\n\nQ-pits are kiln sites which were dug for the production of white coal prior to the Industrial Revolution when white coal was largely superseded by the use of coke.\n\nThe white coal produced in Q-pits was largely used in the smelting of lead from about 1550 to 1750, when a process was discovered that used coal. The large sections of white coal had previously been mixed with charcoal to give the right temperature, as charcoal alone was too hot and would have volatilised the lead. Some evidence exists to suggest that some had a secondary use in the charking of coal into coke.\n\nMany Q-pits were located in deciduous woodlands and as such they are an important landscape feature indicating both previous industrial activity and the presence of a woodland at the site or nearby. The pits are often found in association with saw pits.\n\nThe pits were created by removing soil to create a depression about 12-13 feet (4m) in diameter, breached by a 'spout' and thus forming a 'Q' shape. The pits were dug from the end of the Middle Ages up to around 1760, the start of the Industrial Revolution. Potash pits were of a similar shape and size, however they were used to make potash for use in coursing or degreasing wool and were once found in sheep rearing areas.\n\nThe pits are a common feature in lead mining districts such as the Leadhills in Scotland and East Derbyshire. Due to their small size, they are not likely to be confused with quarries, although bomb craters from World War II can occasionally lead the landscape historian astray.\n\nIn 2007 an isolated example was identified and excavated in Westbury-sub-Mendip near the lead smelting areas on the Mendip Hills in Somerset. Over 200 such pits have been surveyed in Ecclesall Woods, Sheffield.\n"}
{"id": "2109352", "url": "https://en.wikipedia.org/wiki?curid=2109352", "title": "Quinonoid zwitterion", "text": "Quinonoid zwitterion\n\nA quinonoid zwitterion is a special type of zwitterion (or more precisely Mesoionic) based on quinone related chemical compounds. The benzene derivate 1,3-dihydroxy-4,6-diaminobenzene is easily oxidized by air in water or methanol to the quinonoid. This compound was first prepared in 1883 and the quinonoid structure first proposed in 1956. In 2002 the compound was found to be more stable and to exist as the zwitterion after a proton transfer. Evidence for this structure is based on NMR spectroscopy and x-ray crystallography. The positive charge is delocalized between the amino groups over 4 bonds involving 6 pi electrons. The negative charge is spread likewise between the oxygen atoms.\n\n\n"}
{"id": "18359053", "url": "https://en.wikipedia.org/wiki?curid=18359053", "title": "Race Rocks Tidal Power Demonstration Project", "text": "Race Rocks Tidal Power Demonstration Project\n\nThe Race Rocks Tidal Power Demonstration Project (official name: Pearson College - EnCana - Clean Current Tidal Power Demonstration Project at Race Rocks) was a joint project of the Lester B. Pearson College, EnCana Corporation and Clean Current Power Systems Incorporated to use tidal power at Race Rocks near Victoria, British Columbia in Canada. The Race Rocks Tidal Current Generator was installed from July to September 2006 and it was planned to replace two diesel generators at Race Rocks Ecological Reserve. It was the first in-stream tidal current generator in North America.\n\nThe water lubricated bearing system did not perform as expected, and the prototype was decommissioned in May 2007, so that the bearing system could be redesigned.\nAfter changes to the bearings, shroud and turbine, Clean Current Power Systems reinstalled the unit on October 17, 2008. On September 17, 2011 the project ended when the turbine/generator was removed permanently.\n\nThe 65 kW direct drive variable speed permanent magnet generator with bi-directional ducted horizontal axis turbine is placed at the depth of to .\n\n"}
{"id": "31797223", "url": "https://en.wikipedia.org/wiki?curid=31797223", "title": "Renewable Energy Association", "text": "Renewable Energy Association\n\nThe Renewable Energy Association (REA) is a trade association for the (overall) renewables industry in the UK. The REA covers renewable power, heat, and transport. The REA is a not-for-profit company limited by guarantee. The REA is primarily funded from member subscriptions.\n\nREA was established in 2001 as a not-for-profit trade association, representing British renewable energy producers and promoting the use of renewable energy in the UK.\n"}
{"id": "27248938", "url": "https://en.wikipedia.org/wiki?curid=27248938", "title": "Renewable energy debate", "text": "Renewable energy debate\n\nThere is a renewable energy debate about the constraints and opportunities associated with the use of renewable energy.\n\nRenewable electricity production, from sources such as wind power and solar power, is sometimes criticized for being variable or intermittent. However, the International Energy Agency has stated that its significance depends on a range of factors, such as the penetration of the renewables concerned.\n\nThere have been \"not in my back yard\" (NIMBY) concerns relating to the visual and other impacts of some wind farms, with local residents sometimes fighting or blocking construction. In the USA, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive and there are many examples of community wind farm developments. According to a town councillor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.\n\nThe market for renewable energy technologies has continued to grow. Climate change concerns, coupled with high oil prices, peak oil, and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization. New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.\n\nRenewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:\nRenewable energy resources exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries.\n\nVariability inherently affects solar energy, as the production of electricity from solar sources depends on the amount of light energy in a given location. Solar output varies throughout the day, the seasons, with cloud cover and by latitude on the globe. Windblown sand erodes glass in dry climates, protective layers add expenses. These factors are fairly predictable, and some solar thermal systems make use of molten salt heat storage to produce power when the sun is not shining.\n\nWind-generated power is a variable resource, and the amount of electricity produced at any given point in time by a given plant will depend on wind speeds, air density, and turbine characteristics (among other factors). If wind speed is too low (less than about 2.5 m/s) then the wind turbines will not be able to make electricity, and if it is too high (more than about 25 m/s) the turbines will have to be shut down to avoid damage. While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable.\n\nCapacity factors for PV solar are rather poor varying between 10-20% of the rated nameplate capacity. Onshore wind is better at 20-35% and offshore wind is best at 45%. This means that more total capacity needs to be installed in order to achieve an average output for the year. The capacity factor relates to statements about capacity increases, generation may have increased by a much smaller figure.\n\nThe International Energy Agency says that there has been too much attention on issue of the variability of renewable electricity production. This issue only applies to certain renewable technologies, mainly wind power and solar photovoltaics, and to a lesser extent run-of-the-river hydroelectricity. The significance of this \"predictable variability depends on a range of factors which include the market penetration of the renewables concerned, the nature of the energy sources used to balance the intermittentcy, as well as demand side flexibility. Variability will rarely be a barrier to increased renewable energy deployment. But at high levels of market penetration it requires careful analysis and management, and additional costs may be required for dispatchable back-up or system modification. Renewable electricity supply in the 20-50+% penetration range has already been implemented in several European systems, albeit in the context of an integrated European grid system:\n\nIn 2011, the Intergovernmental Panel on Climate Change, the world's leading climate researchers selected by the United Nations, said \"as infrastructure and energy systems develop, in spite of the complexities, there are few, if any, fundamental technological limits to integrating a portfolio of renewable energy technologies to meet a majority share of total energy demand in locations where suitable renewable resources exist or can be supplied\". IPCC scenarios \"generally indicate that growth in renewable energy will be widespread around the world\". The IPCC said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\n\nMark Z. Jacobson says that there is no shortage of renewable energy and a \"smart mix\" of renewable energy sources can be used to reliably meet electricity demand:\n\nBecause the wind blows during stormy conditions when the sun does not shine and the sun often shines on calm days with little wind, combining wind and solar can go a long way toward meeting demand, especially when geothermal provides a steady base and hydroelectric can be called on to fill in the gaps.\nAs physicist Amory Lovins has said:\n\nThe variability of sun, wind and so on, turns out to be a non-problem if you do several sensible things. One is to diversify your renewables by technology, so that weather conditions bad for one kind are good for another. Second, you diversify by site so they're not all subject to the same weather pattern at the same time because they're in the same place. Third, you use standard weather forecasting techniques to forecast wind, sun and rain, and of course hydro operators do this right now. Fourth, you integrate all your resources — supply side and demand side...\"\n\nThe combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with despatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet our needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world: \nIn 2009, eight American and three European authorities, writing in the leading electrical engineers' professional journal, didn't find \"a credible and firm technical limit to the amount of wind energy that can be accommodated by electricity grids\". In Fact, not one of more than 200 international studies, nor official studies for the eastern and western U.S. regions, nor the International Energy Agency, has found major costs or technical barriers to reliably integrating up to 30% variable renewable supplies into the grid, and in some studies much more.\n\nRenewable electricity supply in the 20-50+% range has already been implemented in several European systems, albeit in the context of an integrated European grid system: \n\nIn 2010, four German states, totalling 10 million people, relied on wind power for 43–52% of their annual electricity needs. Denmark isn't far behind, supplying 22% of its power from wind in 2010 (26% in an average wind year). The Extremadura region of Spain is getting up to 25% of its electricity from solar, while the whole country meets 16% of its demand from wind. Just during 2005–2010, Portugal vaulted from 17% to 45% renewable electricity. \nIntegration of renewable energy has caused some grid stability problems in Germany. Voltage fluctuations have caused problems with sensitive equipment. In one case, Hydro Aluminium plant in Hamburg was forced to shut down when the rolling mill's highly sensitive monitor stopped production so abruptly that the aluminum belts snagged. They hit the machines and destroyed a piece of the mill. The malfunction was caused when voltage off the electricity grid weakened for a millisecond. A survey of members of the Association of German Industrial Energy Companies (VIK) revealed that the number of short interruptions to the German electricity grid has grown by 29 percent in the years 2009–2012. Over the same time period, the number of service failures has grown 31 percent, and almost half of those failures have led to production stoppages. Damages have ranged between €10,000 and hundreds of thousands of euros, according to company information.\n\nMinnkota Power Cooperative, the leading U.S. wind utility in 2009, supplied 38% of its retail sales from the wind.\n\nMark A. Delucchi and Mark Z. Jacobson report that there are at least seven ways to design and operate variable renewable energy systems so that they will reliably satisfy electricity demand: \n\n\nJacobson and Delucchi argue that wind, water and solar power can be scaled up in cost-effective ways to meet our energy demands, freeing us from dependence on both fossil fuels and nuclear power. In 2009 they published \"A Plan to Power 100 Percent of the Planet With Renewables\" in \"Scientific American\". The article addressed a number of issues, such as the worldwide spatial footprint of wind turbines, the availability of scarce materials needed for manufacture of new systems, the ability to produce reliable energy on demand and the average cost per kilowatt hour. A more detailed and updated technical analysis has been published as a two-part article in the journal \"Energy Policy\".\n\nRenewable energy is naturally replenished and renewable power technologies increase energy security for the energy poor locales because they reduce dependence on foreign sources of fuel. Unlike power stations relying on uranium and recycled plutonium for fuel, they are not subject to the volatility of global fuel markets. Renewable power decentralises electricity supply and so minimises the need to produce, transport and store hazardous fuels; reliability of power generation is improved by producing power close to the energy consumer. An accidental or intentional outage affects a smaller amount of capacity than an outage at a larger power station.\n\nThe Fukushima I nuclear accidents in Japan have brought new attention to how national energy systems are vulnerable to natural disasters, with climate change is already bringing more weather and climate extremes. These threats to our old energy systems provide a rationale for investing in renewable energy. Shifting to renewable energy \"can help us to meet the dual goals of reducing greenhouse gas emissions, thereby limiting future extreme weather and climate impacts, and ensuring reliable, timely, and cost-efficient delivery of energy\". Investing in renewable energy can have significant dividends for our energy security.\n\nRenewable energy technologies are getting cheaper, through technological change and through the benefits of mass production and market competition. A 2011 IEA report said: \"A portfolio of renewable energy technologies is becoming cost-competitive in an increasingly broad range of circumstances, in some cases providing investment opportunities without the need for specific economic support,\" and added that \"cost reductions in critical technologies, such as wind and solar, are set to continue.\" , there have been substantial reductions in the cost of solar and wind technologies:\n\nThe price of PV modules per MW has fallen by 60 percent since the summer of 2008, according to Bloomberg New Energy Finance estimates, putting solar power for the first time on a competitive footing with the retail price of electricity in a number of sunny countries. Wind turbine prices have also fallen - by 18 percent per MW in the last two years - reflecting, as with solar, fierce competition in the supply chain. Further improvements in the levelised cost of energy for solar, wind and other technologies lie ahead, posing a growing threat to the dominance of fossil fuel generation sources in the next few years.\nHydro-electricity and geothermal electricity produced at favourable sites are now the cheapest way to generate electricity. Renewable energy costs continue to drop, and the levelised cost of electricity (LCOE) is declining for wind power, solar photovoltaic (PV), concentrated solar power (CSP) and some biomass technologies. Wind and Solar are able to produce electricity for 20-40% of the year. \nRenewable energy is also the most economic solution for new grid-connected capacity in areas without cheap fossil fuels. As the cost of renewable power falls, the scope of economically viable applications increases. Renewable technologies are now often the most economic solution for new generating capacity. Where “oil-fired generation is the predominant power generation source (e.g. on islands, off-grid and in some countries) a lower-cost renewable solution almost always exists today”. Indicative, levelised, economic costs for renewable power (exclusive of subsidies or policy incentives) are shown in the Table below.\n\nAs of 2012, renewable power generation technologies accounted for around half of all new power generation capacity additions globally. In 2011, additions included 41 gigawatt (GW) of new wind power capacity, 30 GW of PV, 25 GW of hydro-electricity, 6 GW of biomass, 0.5 GW of CSP, and 0.1 GW of geothermal power. Hydropower provides 16.3% of the world's electricity. When combined with the other renewables wind, geothermal, solar, biomass and waste: together they make up 21.7% of electricity generation worldwide in 2013.\n\nThe \"base load\" is the minimum level of demand on an electrical grid over a span of time, some variation in demand may be compensated by varying production or electricity trading. The criteria for baseload power generation are low price, availability and reliability. Over the years as technology and available resources evolved, a variety of power sources have been used. Hydroelectricity was the first method and this is still the case in a few wet climates like Brazil, Canada, Norway and Iceland. Coal became the most popular baseload supply with the development of the steam turbine and bulk transport, and this is standard in much of the world. Nuclear power is also used and is in competition with coal, France is predominantly nuclear and uses less than 10% fossil fuel. In the US, the increasing popularity of natural gas is likely to replace coal as the base. There are no countries where the majority of baseload power is supplied by wind, solar, biofuels or geothermal, as each of these sources fails one or more of the criteria of low price, availability and reliability.\n\nRenewable power technologies can have significant environmental benefits. Unlike coal and natural gas, they can generate electricity and fuels without releasing significant quantities of CO and other greenhouse gases that contribute to climate change, however the greenhouse gas savings from a number of biofuels have been found to be much less than originally anticipated, as discussed in the article Indirect land use change impacts of biofuels.\n\nBoth solar and wind have been criticized from an aesthetic point of view. However, methods and opportunities exist to deploy these renewable technologies efficiently and unobtrusively: fixed solar collectors can double as noise barriers along highways, and extensive roadway, parking lot, and roof-top area is currently available; amorphous photovoltaic cells can also be used to tint windows and produce energy. Advocates of renewable energy also argue that current infrastructure is less aesthetically pleasing than alternatives, but sited further from the view of most critics.\n\nIn 2015 hydropower generated 16.6% of the worlds total electricity and 70% of all renewable electricity. The major advantage of conventional hydroelectric systems with reservoirs is their ability to store potential power for later production on demand. When used in conjunction with intermittent sources like wind and solar, a constant supply of electricity is achieved. Other advantages include longer life than fuel-fired generation, low operating costs, and other uses of the reservoir. In areas without natural water flow, pumped-storage plants provide a constant supply of electricity. Overall, hydroelectric power can be far less expensive than electricity generated from fossil fuels or nuclear energy, and areas with abundant hydroelectric power attract industry. In Canada it's estimated there are 160,000 megawatts of undeveloped hydro potential.\n\nHowever, there are several disadvantages associated with conventional dam and reservoir hydroelectricity. These include: dislocation if there are people living where the reservoirs are planned, release of significant amounts of carbon dioxide at construction and flooding of the reservoir, disruption of aquatic ecosystems and birdlife, adverse impacts on the river environment, potential risks of sabotage and terrorism, and in rare cases catastrophic failure of the dam wall.\n\nHydro is a flexible source of electricity since plants can be ramped up and down very quickly to adapt to changing electrical demands. The cost of operating a hydroelectric plant is nearly immune to changes in the cost or availability of fossil fuels such as oil, natural gas or coal, and no imports are needed. The average cost of electricity from a hydro plant larger than 10 megawatts is 3 to 5 U.S. cents per kilowatt-hour. Hydroelectric plants have long economic lives, with some plants still in service after 50–100 years. Operating labor cost is also usually low, as plants are automated and have few personnel on site during normal operation.\n\nWhile many hydroelectric projects supply public electricity networks, some are created to serve specific industrial enterprises. Dedicated hydroelectric projects are often built to provide the substantial amounts of electricity needed for aluminium electrolytic plants, for example. The Grand Coulee Dam switched to support Alcoa aluminium in Bellingham, Washington, United States for American World War II airplanes before it was allowed to provide irrigation and power to citizens (in addition to aluminium power) after the war. In Suriname, the Brokopondo Reservoir was constructed to provide electricity for the Alcoa aluminium industry. New Zealand's Manapouri Power Station was constructed to supply electricity to the aluminium smelter at Tiwai Point.\n\nSince hydroelectric dams do not burn fossil fuels, they do not directly produce carbon dioxide or pollutants. While some carbon dioxide is produced during cement manufacture and construction of the project, this is a tiny fraction of the operating emissions of equivalent fossil-fuel electricity generation. One measurement of greenhouse gas and other external comparison between energy sources can be found in the ExternE project by the Paul Scherrer Institut and the University of Stuttgart which was funded by the European Commission. According to that study, hydroelectricity produces the least amount of greenhouse gases and externality of any energy source. Coming in second place was wind, third was nuclear energy, and fourth was solar photovoltaic. The low greenhouse gas impact of hydroelectricity is found especially in temperate climates. The above study was for local energy in Europe; presumably similar conditions prevail in North America and Northern Asia, which all see a regular, natural freeze/thaw cycle (with associated seasonal plant decay and regrowth). Greater greenhouse gas emissions of methane are found in the tropical regions.\n\nThe cost of large dams and reservoirs is justified by some of the added benefits. Reservoirs often provide facilities for water sports, and become tourist attractions themselves. In some countries, aquaculture in reservoirs is common. Multi-use dams installed for irrigation support agriculture with a relatively constant water supply. Large reservoirs can control flooding and alleviate droughts, which would otherwise harm people living downstream. The Columbia River Treaty between The US and Canada required that in the 1960s and 1970s, very large reservoirs were constructed for flood control. In order to offset the cost of dam construction some locations included large hydroelectric plants.\n\n\nLarge reservoirs required for the operation of conventional hydroelectric dams result in submersion of extensive areas upstream of the dams, changing biologically rich and productive lowland and riverine valley forests, marshland and grasslands into artificial lakes. Ideally a reservoir would be large enough to average the annual flow of water or in its smallest form provide sufficient water for irrigation. The loss of land is often exacerbated by habitat fragmentation of surrounding areas caused by the reservoir. In Europe and North America environmental concerns around land flooded by large reservoirs ended 30 years of dam construction in the 1990s, since then only run of the river projects have been approved. Large dams and reservoirs continue to be built in countries like China, Brazil and India.\n\nA consequence is the need to relocate the people living where the reservoirs are planned. In 2000, the World Commission on Dams estimated that dams had physically displaced 40-80 million people worldwide. An example is the contentious Three Gorges Dam which displaced 1.24 million residents. In 1954 the river flooded , killing 33,000 people and forcing 18 million people to move to higher ground. The dam now provides a flood storage capacity for 22 cubic kilometres of water.\n\nWhen water flows it has the ability to transport particles heavier than itself downstream. This may negatively affect the reservoir capacity and subsequently their power stations, particularly those on rivers or within catchment areas with high siltation. Siltation can fill a reservoir and reduce its capacity to control floods along with causing additional horizontal pressure on the upstream portion of the dam. Eventually, some reservoirs can become full of sediment and useless or over-top during a flood and fail.\n\nSome reservoirs in tropical regions produce substantial amounts of methane. This is due to plant material in flooded areas decaying in an anaerobic environment, and forming methane, a greenhouse gas. According to the World Commission on Dams report, where the reservoir is large compared to the generating capacity (less than 100 watts per square metre of surface area) and no clearing of the forests in the area was undertaken prior to impoundment of the reservoir, greenhouse gas emissions from the reservoir may be higher than those of a conventional oil-fired thermal generation plant. There is a lack of knowledge in the scientific community regarding reservoir GHG emissions, producing many diverging positions. To resolve this situation, the International Energy Agency is coordinating an analysis of actual emissions. In boreal reservoirs of Canada and Northern Europe, greenhouse gas emissions are typically only 2% to 8% of any kind of conventional fossil-fuel thermal generation. A new class of underwater logging operation that targets drowned forests can mitigate the effect of forest decay.\n\nBecause large conventional dammed-hydro facilities hold back large volumes of water, a failure due to poor construction, natural disasters or sabotage can be catastrophic to downriver settlements and infrastructure. During Typhoon Nina in 1975 Banqiao Dam failed in Southern China when more than a year's worth of rain fell within 24 hours. The resulting flood resulted in the deaths of 26,000 people, and another 145,000 from epidemics. Millions were left homeless. Also, the creation of a dam in a geologically inappropriate location may cause disasters such as 1963 disaster at Vajont Dam in Italy, where almost 2000 people died. Smaller dams and micro hydro facilities create less risk, but can form continuing hazards even after being decommissioned. For example, the small 1939 Kelly Barnes Dam failed in 1967, causing 39 deaths with the Toccoa Flood, ten years after its power plant was decommissioned.\n\nHydroelectric projects can be disruptive to surrounding aquatic ecosystems downstream of the plant site. Changes in the amount of river flow will correlate with the amount of energy produced by a dam. Water exiting a reservoir usually contains very little suspended sediment, which can lead to scouring of river beds and loss of riverbanks. For fish migration a fish ladder may be required. For fish going through a high head turbine is usually fatal. Reservoir water passing through a turbine alters the downstream river environment. Downstream changes to the water temperature and dissolved gases have adverse effects on some species of fish. In addition to this, alteration to the amount of water let through the dam can also change the composition of gasses in the water downstream. Changes in the amount of discharged water also have the ability to interrupt mating season for various species of fish by dewatering their spawning grounds and forcing them to retreat. Even if mating season has passed, any newly hatched fry can be killed off by low water levels in their spawning areas.\n\nUnlike fossil fuel based technologies, solar power does not lead to any harmful emissions during operation, but the production of the panels leads to some amount of pollution.\n\nThe energy payback time of a power generating system is the time required to generate as much energy as was consumed during production of the system. In 2000 the energy payback time of PV systems was estimated as 8 to 11 years and in 2006 this was estimated to be 1.5 to 3.5 years for crystalline silicon PV systems and 1-1.5 years for thin film technologies (S. Europe).\n\nAnother economic measure, closely related to the energy payback time, is the energy returned on energy invested (EROEI) or energy return on investment (EROI), which is the ratio of electricity generated divided by the energy required to build \"and maintain\" the equipment. (This is not the same as the economic return on investment (ROI), which varies according to local energy prices, subsidies available and metering techniques.) With lifetimes of at least 30 years, the EROEI of PV systems are in the range of 10 to 30, thus generating enough energy over their lifetimes to reproduce themselves many times (6-31 reproductions) depending on what type of material, balance of system (BOS), and the geographic location of the system.\n\nOne issue that has often raised concerns is the use of cadmium in cadmium telluride solar cells (CdTe is only used in a few types of PV panels). Cadmium in its metallic form is a toxic substance that has the tendency to accumulate in ecological food chains. The amount of cadmium used in thin-film PV modules is relatively small (5-10 g/m²) and with proper emission control techniques in place the cadmium emissions from module production can be almost zero. Current PV technologies lead to cadmium emissions of 0.3-0.9 microgram/kWh over the whole life-cycle. Most of these emissions actually arise through the use of coal power for the manufacturing of the modules, and coal and lignite combustion leads to much higher emissions of cadmium. Life-cycle cadmium emissions from coal is 3.1 microgram/kWh, lignite 6.2, and natural gas 0.2 microgram/kWh. Note that if electricity produced by photovoltaic panels were used to manufacture the modules instead of electricity from burning coal, cadmium emissions from coal power usage in the manufacturing process could be entirely eliminated.\n\nSolar power plants require large amounts of land. According to the Bureau of Land Management, there are twenty proposals to use in total about 180 square miles of public land in California. If all twenty proposed projects were built, they would total 7,387 megawatts. The requirement for so much land has spurred efforts to encourage solar facilities to be built on already-disturbed lands, and the Department of Interior identified Solar Energy Zones that it judges to contain lower value habitat where solar development would have less of an impact on ecosystems. Sensitive wildlife impacted by large solar facility plans include the desert tortoise, Mohave Ground Squirrel, Mojave fringe-toed lizard, and desert bighorn sheep.\n\nIn the United States, some of the land in the eastern portion of the Mojave Desert is to be preserved, but the solar industry has mainly expressed interest in areas of the western desert, \"where the sun burns hotter and there is easier access to transmission lines\", said Kenn J. Arnecke of \"FPL Energy\", a sentiment shared by many executives in the industry.\n\nBiofuel production has increased in recent years. Some commodities like maize (corn), sugar cane or vegetable oil can be used either as food, feed, or to make biofuels. The Food vs. fuel debate is the dilemma regarding the risk of diverting farmland or crops for biofuels production to the detriment of the food supply. The biofuel and food price debate involves wide-ranging views, and is a long-standing, controversial one in the literature. There is disagreement about the significance of the issue, what is causing it, and what can or should be done to remedy the situation. This complexity and uncertainty is due to the large number of impacts and feedback loops that can positively or negatively affect the price system. Moreover, the relative strengths of these positive and negative impacts vary in the short and long terms, and involve delayed effects. The academic side of the debate is also blurred by the use of different economic models and competing forms of statistical analysis.\n\nAccording to the International Energy Agency, new biofuels technologies being developed today, notably cellulosic ethanol, could allow biofuels to play a much bigger role in the future than previously thought. Cellulosic ethanol can be made from plant matter composed primarily of inedible cellulose fibers that form the stems and branches of most plants. Crop residues (such as corn stalks, wheat straw and rice straw),\nwood waste, and municipal solid waste are potential sources of cellulosic biomass. Dedicated energy crops, such as switchgrass, are also promising cellulose sources that can be sustainably produced in many\nregions of the United States.\n\nThe ethanol and biodiesel production industries also create jobs in plant construction, operations, and maintenance, mostly in rural communities. According to the Renewable Fuels Association, the ethanol industry created almost 154,000 U.S. jobs in 2005 alone, boosting household income by $5.7 billion. It also contributed about $3.5 billion in tax revenues at the local, state, and federal levels.\n\nBiofuels are different from fossil fuels in regard to carbon emissions being short term, but are similar to fossil fuels in that biofuels contribute to air pollution. Burning produces airborne carbon particulates, carbon monoxide and nitrous oxides. The WHO estimates 3.7 million premature deaths worldwide in 2012 due to air pollution.\n\nMark Diesendorf, formerly Professor of Environmental Science at the University of Technology, Sydney and a principal research scientist with CSIRO has summarised some of the benefits of onshore wind farms as follows.\nStudies of birds and offshore wind farms in Europe have found that there are very few bird collisions. Several offshore wind sites in Europe have been in areas heavily used by seabirds. Improvements in wind turbine design, including a much slower rate of rotation of the blades and a smooth tower base instead of perchable lattice towers, have helped reduce bird mortality at wind farms around the world. However older smaller wind turbines may be hazardous to flying birds. Birds are severely impacted by fossil fuel energy; examples include birds dying from exposure to oil spills, habitat loss from acid rain and mountaintop removal coal mining, and mercury poisoning.\n\nThere have been \"not in my back yard\" (NIMBY) concerns relating to the visual and other impacts of some wind farms, with local residents sometimes fighting or blocking construction.\n\nIn the USA, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. Elsewhere, there are concerns that some installations can negatively affect TV and radio reception and Doppler weather radar, as well as produce excessive sound and vibration levels leading to a decrease in property values. Potential broadcast-reception solutions include predictive interference modeling as a component of site selection.\n\nHowever, residents in other areas have been more positive and there are many examples of community wind farm developments. According to a town councillor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.\n\nA starting point for better understanding community concerns about wind farms is often through public outreach initiatives (e.g., surveys, town hall meetings) to clarify the nature of concerns. Community concerns regarding wind power projects have been shown to be based more on people’s perception rather than actual fact. In tourist areas, for example, there is a misperception that the siting of wind farms will adversely affect tourism. Yet surveys conducted in tourist areas in Germany, Belgium, and Scotland show that this is simply not the case. Similarly, according to Valentine, concerns over wind turbine noise, shadow flicker, and bird life threats are not supported by actual data. The difficulty is that the general public often does not have ready access to information necessary to assess the pros and cons of wind power developments. However, even where a general public supports wind power in principle and is well informed, there are often important 'qualifications' around the delivery of infrastructure (i.e. providing mitigation of development impacts on local ecology and assets).\n\nMedia reports tend to emphasize storylines that have popular appeal (i.e. famous figures who are opposed to a particular development). Consequently, media coverage often fails to provide the full project information that the public needs to effectively evaluate the merits of a wind project. Moreover, misinformation about wind power may be propagated by fossil fuel and nuclear power special interest groups. Often there is an ideological right wing interest which tends to dominate, supporting anti-green and anti-climate-science positions. The Australian anti-wind site \"Stop These Things\" best illustrates this approach, describing environmentalists as ‘Greentards’.\n\nThe lesson for planners and policymakers is that some forms of public opposition can be mitigated by providing community members with comprehensive information on a given project. In fact, not only will a more proactive media strategy help reduce opposition but it may also actually lead to enhanced support.\n\nPublic perceptions generally improve after wind projects become operational. Surveys conducted with communities that host wind energy developments in the United Kingdom, Scotland, France, the United States, and Finland have demonstrated that wind farms which are properly planned and sited can engender project support. Wind energy projects, which have been well-planned to reduce social and environmental problems, have been shown to positively influence wind power perceptions once completed. Support is enhanced when community members are offered investment opportunities and involvement in the wind power development. Many wind power companies work with local communities to reduce environmental and other concerns associated with particular wind farms. \nAppropriate government consultation, planning and approval procedures also help to minimize environmental risks. Some people may still object to wind farms but, according to The Australia Institute, their concerns should be weighed against the need to address the threats posed by climate change and the opinions of the broader community.\n\nIn other cases there is direct community ownership of wind farm projects. In Germany, hundreds of thousands of people have invested in citizens' wind farms across the country and thousands of small and medium-sized enterprises are running successful businesses in a new sector that in 2008 employed 90,000 people and generated 8 percent of Germany's electricity. Wind power has gained very high social acceptance in Germany. Surveys of public attitudes across Europe and in many other countries show strong public support for wind power.\n\nIn America, wind projects are reported to boost local tax bases, helping to pay for schools, roads and hospitals. Wind projects also revitalize the economy of rural communities by providing steady income to farmers and other landowners.\n\nThe Intrepid Wind Farm, in Iowa, is an example of one wind farm where the environmental impact of the project has been minimized through consultation and co-operation:\n\"Making sure the wind farm made as gentle an environmental impact as possible was an important consideration. Therefore, when MidAmerican first began planning the Intrepid site, they worked closely with a number of state and national environmental groups. Using input from such diverse groups as the Iowa Department of Natural Resources, the Nature Conservancy, Iowa State University, the U.S. Fish and Wildlife Service, the Iowa Natural Heritage Foundation, and the Iowa Chapter of the Sierra Club, MidAmerican created a statewide map of areas in the proposed region that contained specific bird populations or habitats. Those areas were then avoided as site planning got underway in earnest. In order to minimize the wind farm's environmental impact even further, MidAmerican also worked in conjunction with the Army Corp of Engineers, to secure all necessary permits related to any potential risk to wetlands in the area. Regular inspections are also conducted to make certain that the wind farm is causing no adverse environmental impact to the region.\"\nOther examples include:\n\nEven though a source of renewable energy may last for billions of years, renewable energy infrastructure, like hydroelectric dams, will not last forever, and must be removed and replaced at some point. Events like the shifting of riverbeds, or changing weather patterns could potentially alter or even halt the function of hydroelectric dams, lowering the amount of time they are available to generate electricity. A reservoirs capacity may also be affected by silting which may not be cost-effective to remove.\n\nWind turbines suffer from wear and fatigue and are scheduled to last 25 years before being replaced, often by much taller units.\n\nSome have claimed that geothermal being a renewable energy source depends on the rate of extraction being slow enough such that depletion does not occur. If depletion does occur, the temperature can regenerate if given a long period of non-use.\n\nThe government of Iceland states: \"It should be stressed that the geothermal resource is not strictly renewable in the same sense as the hydro resource.\" It estimates that Iceland's geothermal energy could provide 1700 MW for over 100 years, compared to the current production of 140 MW. Radioactive elements in the Earth's crust continuously decay, replenishing the heat. The International Energy Agency classifies geothermal power as renewable. Geothermal power in Iceland is developed in a stepwise development method to ensure that it is sustainable instead of excessive, which would deplete the resource.\n\nThe U.S. electric power industry now relies on large, central power stations, including coal, natural gas, nuclear, and hydropower plants that together generate more than 95% of the nation’s electricity. Over the next few decades uses of renewable energy could help to diversify the nation’s bulk power supply. In 2016 renewable hydro, solar, wind, geothermal and biomass produced 39% of California’s electricity.\n\nAlthough most of today’s electricity comes from large, central-station power plants, new technologies offer a range of options for generating electricity nearer to where it is needed, saving on the cost of transmitting and distributing power and improving the overall efficiency and reliability of the system.\n\nImproving energy efficiency represents the most immediate and often the most cost-effective way to reduce oil dependence, improve energy security, and reduce the health and environmental impact of the energy system. By reducing the total energy requirements of the economy, improved energy efficiency could make increased reliance on renewable energy sources more practical and affordable.\n\nExisting organizations and conservative political groups are disposed to keep renewable energy proposals out of the agenda at many levels. Most Republicans do not support renewable energy investment because their framework is built on staying with current energy sources while promoting national drilling to reduce dependence on imports. In contrast, progressives and libertarians tend to support renewable energy by encouraging job growth, national investment and tax incentives. Thus, polarized organizational frameworks that shape industrial and governmental policies for renewable energy tend to create barriers for implementing renewable energy.\n\nAccording to an article by Henrik Lund, the theory of Choice Awareness seeks to understand and explain why the descriptions of the best alternatives do not develop independently and what can be done about it. The theory argues that public participation, and hence the awareness of choices, has been an important factor in successful decision-making processes Choice Awareness theory emphasizes the fact that different organizations see things differently and that current organizational interests hinder passing renewable energy policies. Given these conditions leaves the public with a situation of no choice. Consequently, this leaves the general public in a state to abide by conventional energy sources such as coal and oil.\n\nIn a broad sense most individuals, especially those that do not engage in public discourse of current economic policies, have little to no awareness of renewable energy. Enlightening communities on the socioeconomic implications of fossil fuel use is a potent mode of rhetoric that can promote the implementation of renewable energy sources. Transparent local planning also proves useful in public discourse when used to determine the location of wind farms in communities supporting renewable energy. According to an article by John Barry et al., a crucial factor communities need to engage discourse on is the principle of \"assumption of and imperative towards consensus.\" This principle claims that a community cannot neglect its energy or climate change responsibilities, and that it must do its part in helping to decrease carbon emissions through renewable energy reformation. Hence, communities that continually engage in mutual learning and discourse by conflict resolution will help progress renewable energy.\n\nLegislative definitions of renewable energy, used when determining energy projects eligible for subsidies or tax breaks, usually exclude conventional nuclear reactor designs. Physicist Bernard Cohen elucidated in 1983 that uranium dissolved in seawater, when used in Breeder reactors (which are reactors that \"breed\" more fissile nuclear fuel than they consume from base fertile material) is effectively inexhaustible, with the seawater bearing uranium constantly replenished by river erosion carrying more uranium into the sea, and could therefore be considered a renewable source of energy.\n\nIn 1987, the World Commission on Environment and Development(WCED), an organization independent from, but created by, the United Nations, published Our Common Future, in which breeder reactors, and, when it is developed, fusion power are both classified within the same category as conventional renewable energy sources, such as solar and falling water.\n\n\n"}
{"id": "1947471", "url": "https://en.wikipedia.org/wiki?curid=1947471", "title": "Rishon model", "text": "Rishon model\n\nThe Harari–Shupe preon model (also known as rishon model, RM) is the earliest effort to develop a preon model to explain the phenomena appearing in the Standard Model (SM) of particle physics. It was first developed independently by Haim Harari and by Michael A. Shupe and later expanded by Harari and his then-student Nathan Seiberg.\n\nThe model has two kinds of fundamental particles called rishons (which means \"primary\" in Hebrew). They are T (\"Third\" since it has an electric charge of +⅓ \"e\", or Tohu which means \"unformed\" in Hebrew Genesis) and V (\"Vanishes\", since it is electrically neutral, or Vohu which means \"void\" in Hebrew Genesis). All leptons and all flavours of quarks are three-rishon ordered triplets. These groups of three rishons have spin-½. They are as follows:\n\nEach rishon has a corresponding antiparticle. Hence:\n\nThe W boson = TTTVVV;\nThe W boson = .\n\nBaryon number (\"B\") and lepton number (\"L\") are not conserved, but the quantity \"B\"−\"L\" is conserved. A baryon number violating process (such as proton decay) in the model would be <br>\n <br>\n/|\\   /|\\   /|\\   /|\\<br>\n\nIn the expanded Harari–Seiberg version the rishons possess color and hypercolor, explaining why the only composites are the observed quarks and leptons. Under certain assumptions, it is possible to show that the model allows exactly for three generations of quarks and leptons.\n\nCurrently, there is no scientific evidence for the existence of substructure within quarks and leptons, but there is no profound reason why such a substructure may not be revealed at shorter distances. In 2008, Piotr Zenczykowski has derived the RM by starting from a non-relativistic O(6) phase space. Such model is based on fundamental principles and the structure of Clifford algebras, and fully recovers the RM by naturally explaining several obscure and otherwise artificial features of the original model.\n\n"}
{"id": "12270902", "url": "https://en.wikipedia.org/wiki?curid=12270902", "title": "Slash rating", "text": "Slash rating\n\nThe slash rating, under the United States National Electrical Code, is given to circuit interrupt hardware and specifies a maximum line-to-ground voltage rating in combination with a maximum line-to-line voltage rating. One common application would be for a three-phase electrical loads. For example a 120/240 V rating can disqualify a circuit breaker for use with a delta system load that would otherwise work with a wye system load.\n\n"}
{"id": "1358074", "url": "https://en.wikipedia.org/wiki?curid=1358074", "title": "Solar mirror", "text": "Solar mirror\n\nA solar mirror contains a substrate with a reflective layer for reflecting the solar energy, and in most cases an interference layer. This may be a planar mirror or parabolic arrays of solar mirrors used to achieve a substantially concentrated reflection factor for solar energy systems.\n\nSee article \"Heliostat\" for more information on solar mirrors used for terrestrial energy.\n\nThe substrate is the mechanical layer which holds the mirror in shape.\n\nGlass may also be used as a protective layer to protect the other layers from abrasion and corrosion. Although glass is brittle, it is a good material for this purpose, because it is highly transparent (low optical losses), resistant to ultraviolet light (UV), fairly hard (abrasion resistant), chemically inert, and fairly easy to clean. It is composed of a float glass with high optical transmission characteristics in the visible and infrared ranges, and is configured to transmit visible light and infrared radiation. The top surface, known as the \"first surface\", will reflect some of the incident solar energy, due to the reflection coefficient caused by its index of refraction being higher than air. Most of the solar energy is transmitted through the glass substrate to the lower layers of the mirror, possibly with some refraction, depending on the angle of incidence as light enters the mirror.\n\nMetal substrates (\"Metal Mirror Reflectors\") may also be used in solar reflectors. NASA Glenn Research Center, for example, used a mirror comprising a reflective aluminum surface on a metallic honeycomb as a prototype reflector unit for a proposed power system for the International Space Station. One technology uses aluminum composite reflector panels, achieving over 93% reflectivity and coated with a speciality coating for surface protection. Metal reflectors offer some advantages over glass reflectors, as they are lightweight and stronger than glass and relatively inexpensive. The ability to retain parabolic shape in reflectors is another advantage, and normally the subframe requirements are reduced by more than 300%. The top surface reflection coating allows for better efficiency.\n\nThe reflective layer is designed to reflect the maximum amount of solar energy incident upon it, back through the glass substrate. The layer comprises a highly reflective thin metal film, usually either silver or aluminum, but occasionally other metals. Because of sensitivity to abrasion and corrosion, the metal layer is usually protected by the (glass) substrate on top, and the bottom may be covered with a protective coating, such as a copper layer and varnish.\n\nDespite the use of aluminum in generic mirrors, aluminum is not always used as the reflective layer for a solar mirror. The use of silver as the reflective layer is claimed to lead to higher efficiency levels, because it is the most reflective metal. This is because of aluminum's reflection factor in the UV region of the spectrum. Locating the aluminum layer on the first surface exposes it to weathering, which reduces the mirror's resistance to corrosion and makes it more susceptible to abrasion. Adding a protective layer to the aluminum would reduce its reflectivity.\n\nAn interference layer may be located on the first surface of the glass substrate. It can be used to tailor the reflectance. It may also be designed for diffuse reflectance of near-ultraviolet radiation, in order to prevent it from passing through the glass substrate. This substantially enhances the overall reflection of near-ultraviolet radiation from the mirror. The interference layer may be made of several materials, depending on the desired refractive index, such as titanium dioxide.\n\nThe intensity of solar thermal energy from solar radiation at the surface of the earth is about , of area normal to the direction of the sun, under clear-sky conditions. When solar energy is unconcentrated, the maximum collector temperature is about . This is useful for space heating and heating water. For higher temperature applications, such as cooking, or supplying a heat engine or turbine-electrical generator, this energy must be concentrated.\n\nSolar thermal systems have been constructed to produce concentrated solar power (CSP), for generating electricity. The large Sandia Lab solar power tower uses a Stirling engine heated by a solar mirror concentrator. Another configuration is the trough system.\n\n\"Solar dynamic\" energy systems have been proposed for various spacecraft applications, including solar power satellites, where a reflector focuses sunlight on to a heat engine such as the Brayton cycle type.\n\nPhotovoltaic cells (PV) which can convert solar radiation directly into electricity are quite expensive per unit area. Some types of PV cell, e.g. gallium arsenide, if cooled, are capable of converting efficiently up to 1,000 times as much radiation as is normally provided by simple exposure to direct sunlight.\n\nIn tests done by Sewang Yoon and Vahan Garboushian, for Amonix Corp. silicon solar cell conversion efficiency is shown to increase at higher levels of concentration, proportional to the logarithm of the concentration, provided external cooling is available to the photocells. Similarly, higher efficiency multijunction cells also improve in performance with high concentration.\n\nTo date no large scale testing has been performed on this concept. Presumably this is because the increased cost of the reflectors and cooling generally is not economically justified.\n\nTheoretically, for space-based solar power satellite designs, solar mirrors could reduce PV cell costs and launch costs since they are expected to be both lighter and cheaper than equivalent large areas of PV cells. Several options were studied by Boeing corporation. In their Fig. 4. captioned \"Architecture 4. GEO Harris Wheel\", the authors describe a system of solar mirrors used to augment the power of some nearby solar collectors, from which the power is then transmitted to receiver stations on earth.\n\nAnother advanced space concept proposal is the notion of Space Reflectors which reflect sunlight on to small spots on the night side of the Earth to provide night time illumination. An early proponent of this concept was Dr. Krafft Arnold Ehricke, who wrote about systems called \"Lunetta\", \"Soletta\", \"Biosoletta\" and \"Powersoletta\".\n\nA preliminary series of experiments called Znamya (\"Banner\") was performed by Russia, using solar sail prototypes that had been repurposed as mirrors. Znamya-1 was a ground test. Znamya-2 was launched aboard the Progress M-15 resupply mission to the Mir space station on 27 October 1992. After undocked from Mir, the Progress deployed the reflector. This mission was successful in that the mirror deployed, although it did not illuminate the Earth. The next flight Znamya-2.5 failed. Znamya-3 never flew.\n\n"}
{"id": "51654", "url": "https://en.wikipedia.org/wiki?curid=51654", "title": "Soliton", "text": "Soliton\n\nIn mathematics and physics, a soliton is a self-reinforcing solitary wave packet that maintains its shape while it propagates at a constant velocity. Solitons are caused by a cancellation of nonlinear and dispersive effects in the medium. (The term \"dispersive effects\" refers to a property of certain systems where the speed of the waves varies according to frequency.) Solitons are the solutions of a widespread class of weakly nonlinear dispersive partial differential equations describing physical systems.\n\nThe soliton phenomenon was first described in 1834 by John Scott Russell (1808–1882) who observed a solitary wave in the Union Canal in Scotland. He reproduced the phenomenon in a wave tank and named it the \"Wave of Translation\".\n\nA single, consensus definition of a soliton is difficult to find. ascribe three properties to solitons:\n\nMore formal definitions exist, but they require substantial mathematics. Moreover, some scientists use the term \"soliton\" for phenomena that do not quite have these three properties (for instance, the 'light bullets' of nonlinear optics are often called solitons despite losing energy during interaction).\n\nDispersion and nonlinearity can interact to produce permanent and localized wave forms. Consider a pulse of light traveling in glass. This pulse can be thought of as consisting of light of several different frequencies. Since glass shows dispersion, these different frequencies travel at different speeds and the shape of the pulse therefore changes over time. However, also the nonlinear Kerr effect occurs; the refractive index of a material at a given frequency depends on the light's amplitude or strength. If the pulse has just the right shape, the Kerr effect exactly cancels the dispersion effect, and the pulse's shape does not change over time, thus is a soliton. See soliton (optics) for a more detailed description.\n\nMany exactly solvable models have soliton solutions, including the Korteweg–de Vries equation, the nonlinear Schrödinger equation, the coupled nonlinear Schrödinger equation, and the sine-Gordon equation. The soliton solutions are typically obtained by means of the inverse scattering transform, and owe their stability to the integrability of the field equations. The mathematical theory of these equations is a broad and very active field of mathematical research.\n\nSome types of tidal bore, a wave phenomenon of a few rivers including the River Severn, are 'undular': a wavefront followed by a train of solitons. Other solitons occur as the undersea internal waves, initiated by seabed topography, that propagate on the oceanic pycnocline. Atmospheric solitons also exist, such as the morning glory cloud of the Gulf of Carpentaria, where pressure solitons traveling in a temperature inversion layer produce vast linear roll clouds. The recent and not widely accepted soliton model in neuroscience proposes to explain the signal conduction within neurons as pressure solitons.\n\nA topological soliton, also called a topological defect, is any solution of a set of partial differential equations that is stable against decay to the \"trivial solution\". Soliton stability is due to topological constraints, rather than integrability of the field equations. The constraints arise almost always because the differential equations must obey a set of boundary conditions, and the boundary has a nontrivial homotopy group, preserved by the differential equations. Thus, the differential equation solutions can be classified into homotopy classes.\n\nNo continuous transformation maps a solution in one homotopy class to another. The solutions are truly distinct, and maintain their integrity, even in the face of extremely powerful forces. Examples of topological solitons include the screw dislocation in a crystalline lattice, the Dirac string and the magnetic monopole in electromagnetism, the Skyrmion and the Wess–Zumino–Witten model in quantum field theory, the magnetic skyrmion in condensed matter physics, and cosmic strings and domain walls in cosmology.\n\nIn 1834, John Scott Russell describes his \"wave of translation\". The discovery is described here in Scott Russell's own words:\n\nScott Russell spent some time making practical and theoretical investigations of these waves. He built wave tanks at his home and noticed some key properties:\n\nScott Russell's experimental work seemed at odds with Isaac Newton's and Daniel Bernoulli's theories of hydrodynamics. George Biddell Airy and George Gabriel Stokes had difficulty accepting Scott Russell's experimental observations because they could not be explained by the existing water wave theories. Their contemporaries spent some time attempting to extend the theory but it would take until the 1870s before Joseph Boussinesq and Lord Rayleigh published a theoretical treatment and solutions. In 1895 Diederik Korteweg and Gustav de Vries provided what is now known as the Korteweg–de Vries equation, including solitary wave and periodic cnoidal wave solutions.\nIn 1965 Norman Zabusky of Bell Labs and Martin Kruskal of Princeton University first demonstrated soliton behavior in media subject to the Korteweg–de Vries equation (KdV equation) in a computational investigation using a finite difference approach. They also showed how this behavior explained the puzzling earlier work of Fermi, Pasta, Ulam, and Tsingou.\n\nIn 1967, Gardner, Greene, Kruskal and Miura discovered an inverse scattering transform enabling analytical solution of the KdV equation. The work of Peter Lax on Lax pairs and the Lax equation has since extended this to solution of many related soliton-generating systems.\n\nNote that solitons are, by definition, unaltered in shape and speed by a collision with other solitons. So solitary waves on a water surface are \"near\"-solitons, but not exactly – after the interaction of two (colliding or overtaking) solitary waves, they have changed a bit in amplitude and an oscillatory residual is left behind.\n\nSolitons are also studied in quantum mechanics, thanks to the fact that they could provide a new foundation of it through de Broglie's unfinished program, known as \"Double solution theory\" or \"Nonlinear wave mechanics\". This theory, developed by de Broglie in 1927 and revived in the 1950s, is the natural continuation of his ideas developed between 1923 and 1926, which extended the wave-particle duality introduced by Einstein for the light quanta, to all the particles of matter.\n\nMuch experimentation has been done using solitons in fiber optics applications. Solitons in a fiber optic system are described by the Manakov equations.\nSolitons' inherent stability make long-distance transmission possible without the use of repeaters, and could potentially double transmission capacity as well.\n\nSolitons may occur in proteins and DNA. Solitons are related to the low-frequency collective motion in proteins and DNA.\n\nA recently developed model in neuroscience proposes that signals, in the form of density waves, are conducted within neurons in the form of solitons.\n\nIn magnets, there also exist different types of solitons and other nonlinear waves. These magnetic solitons are an exact solution of classical nonlinear differential equations — magnetic equations, e.g. the Landau–Lifshitz equation, continuum Heisenberg model, Ishimori equation, nonlinear Schrödinger equation and others.\n\nThe bound state of two solitons is known as a \"bion\", or in systems where the bound state periodically oscillates, a \"breather\".\n\nIn field theory \"bion\" usually refers to the solution of the Born–Infeld model. The name appears to have been coined by G. W. Gibbons in order to distinguish this solution from the conventional soliton, understood as a \"regular\", finite-energy (and usually stable) solution of a differential equation describing some physical system. The word \"regular\" means a smooth solution carrying no sources at all. However, the solution of the Born–Infeld model still carries a source in the form of a Dirac-delta function at the origin. As a consequence it displays a singularity in this point (although the electric field is everywhere regular). In some physical contexts (for instance string theory) this feature can be important, which motivated the introduction of a special name for this class of solitons.\n\nOn the other hand, when gravity is added (i.e. when considering the coupling of the Born–Infeld model to general relativity) the corresponding solution is called \"EBIon\", where \"E\" stands for Einstein.\n\n\n\n\n"}
{"id": "30307082", "url": "https://en.wikipedia.org/wiki?curid=30307082", "title": "Som Energia", "text": "Som Energia\n\nSom Energia is a Spanish renewable energies cooperative that was officially founded in December 2010 in Girona, Catalonia, making it the first of its kind in Spain. The project to create the cooperative was begun in November 2009 by a group of former students and lecturers at University of Girona and other contributors, with the objective of setting up something similar to initiatives such as Ecopower (Belgium) or Enercoop (France). Som Energia intends to offer its members the possibility of consuming energy from sources that are 100% renewable at a price similar to conventional energy, as well as developing its own renewable energy projects.\nOn November 2017, the cooperative had over 38.700 members, had invested over 12.5 million euro in renewable energy production projects had produced over 9.922.296 kWh and employed 45 people.\n\n"}
{"id": "50958", "url": "https://en.wikipedia.org/wiki?curid=50958", "title": "Sulfur dioxide", "text": "Sulfur dioxide\n\nSulfur dioxide (also sulphur dioxide in British English) is the chemical compound with the formula . It is a toxic gas with a burnt match smell. It is released naturally by volcanic activity and is produced as a by-product of the burning of fossil fuels contaminated with sulfur compounds and copper extraction.\n\nSO is a bent molecule with \"C\" symmetry point group.\nA valence bond theory approach considering just \"s\" and \"p\" orbitals would describe the bonding in terms of resonance between two resonance structures.\n\nThe sulfur–oxygen bond has a bond order of 1.5. There is support for this simple approach that does not invoke \"d\" orbital participation.\nIn terms of electron-counting formalism, the sulfur atom has an oxidation state of +4 and a formal charge of +1.\n\nIt is found on Earth and exists in very small concentrations and in the atmosphere at about 1 ppm.\n\nOn other planets, it can be found in various concentrations, the most significant being the atmosphere of Venus, where it is the third-most significant atmospheric gas at 150 ppm. There, it condenses to form clouds, and is a key component of chemical reactions in the planet's atmosphere and contributes to global warming. It has been implicated as a key agent in the warming of early Mars, with estimates of concentrations in the lower atmosphere as high as 100 ppm, though it only exists in trace amounts. On both Venus and Mars, as on Earth, its primary source is thought to be volcanic. The atmosphere of Io is 90% sulfur dioxide and trace amounts are thought to also exist in the atmosphere of Jupiter.\n\nAs an ice, it is thought to exist in abundance on the Galilean moons—as sublimating ice or frost on the trailing hemisphere of Io, a natural satellite of Jupiter and in the crust and mantle of Europa, Ganymede, and Callisto, possibly also in liquid form and readily reacting with water.\n\nSulfur dioxide is primarily produced for sulfuric acid manufacture (see contact process). In the United States in 1979, 23.6 million tonnes (26,014,547 US short tons) of sulfur dioxide were used in this way, compared with 150 thousand tonnes (165,347 US short tons) used for other purposes. Most sulfur dioxide is produced by the combustion of elemental sulfur. Some sulfur dioxide is also produced by roasting pyrite and other sulfide ores in air.\nSulfur dioxide is the product of the burning of sulfur or of burning materials that contain sulfur:\n\nTo aid combustion, liquefied sulfur (140–150 °C, 284-302 °F) is sprayed through an atomizing nozzle to generate fine drops of sulfur with a large surface area. The reaction is exothermic, and the combustion produces temperatures of 1000–1600 °C, 1832-2912 °F). The significant amount of heat produced is recovered by steam generation that can subsequently be converted to electricity.\n\nThe combustion of hydrogen sulfide and organosulfur compounds proceeds similarly. For example:\n\nThe roasting of sulfide ores such as pyrite, sphalerite, and cinnabar (mercury sulfide) also releases SO:\n\nA combination of these reactions is responsible for the largest source of sulfur dioxide, volcanic eruptions. These events can release millions of tonnes of SO.\n\nSulfur dioxide can also be a byproduct in the manufacture of calcium silicate cement; CaSO is heated with coke and sand in this process:\n\nUntil the 1970s, commercial quantities of sulfuric acid and cement were produced by this process in Whitehaven, England. Upon being mixed with shale or marl, and roasted, the sulfate liberated sulfur dioxide gas, used in sulfuric acid production, the reaction also produced calcium silicate, a precursor in cement production.\n\nOn a laboratory scale, the action of hot concentrated sulfuric acid on copper turnings produces sulfur dioxide.\n\nSulfite results from the reaction of aqueous base and sulfur dioxide. The reverse reaction involves acidification of sodium metabisulfite:\n\nTreatment of basic solutions with sulfur dioxide affords sulfite salts (e.g. sodium sulfite):\n\nFeaturing sulfur in the +4 oxidation state, sulfur dioxide is a reducing agent. It is oxidized by halogens to give the sulfuryl halides, such as sulfuryl chloride:\n\nSulfur dioxide is the oxidising agent in the Claus process, which is conducted on a large scale in oil refineries. Here, sulfur dioxide is reduced by hydrogen sulfide to give elemental sulfur:\n\nThe sequential oxidation of sulfur dioxide followed by its hydration is used in the production of sulfuric acid.\n\nSulfur dioxide is one of the few common acidic yet reducing gases. It turns moist litmus pink (being acidic), then white (due to its bleaching effect). It may be identified by bubbling it through a dichromate solution, turning the solution from orange to green (Cr (aq)). It can also reduce ferric ions to ferrous. \n\nSulfur dioxide can react with certain 1,3-dienes in a cheletropic reaction to form cyclic sulfones. This reaction is exploited on an industrial scale for the synthesis of sulfolane, which is an important solvent in the petrochemical industry.\n\nSulfur dioxide can bind to metal ions as a ligand to form metal sulfur dioxide complexes, typically where the transition metal is in oxidation state 0 or +1. Many different bonding modes (geometries) are recognized, but in most cases, the ligand is monodentate, attached to the metal through sulfur, which can be either planar and pyramidal η.\n\nSulfur dioxide is an intermediate in the production of sulfuric acid, being converted to sulfur trioxide, and then to oleum, which is made into sulfuric acid. Sulfur dioxide for this purpose is made when sulfur combines with oxygen. The method of converting sulfur dioxide to sulfuric acid is called the contact process. Several billion kilograms are produced annually for this purpose.\n\nSulfur dioxide is sometimes used as a preservative for dried apricots, dried figs, and other dried fruits, owing to its antimicrobial properties, and is called E220 when used in this way in Europe. As a preservative, it maintains the colorful appearance of the fruit and prevents rotting. It is also added to sulfured molasses.\n\nSulfur dioxide was first used in winemaking by the Romans, when they discovered that burning sulfur candles inside empty wine vessels keeps them fresh and free from vinegar smell.\n\nIt is still an important compound in winemaking, and is measured in parts per million (\"ppm\") in wine. It is present even in so-called unsulfurated wine at concentrations of up to 10 mg/L. It serves as an antibiotic and antioxidant, protecting wine from spoilage by bacteria and oxidation - a phenomenon that leads to the browning of the wine and a loss of cultivar specific flavors. Its antimicrobial action also helps minimize volatile acidity. Wines containing sulfur dioxide are typically labeled with \"containing sulfites\".\n\nSulfur dioxide exists in wine in free and bound forms, and the combinations are referred to as total SO. Binding, for instance to the carbonyl group of acetaldehyde, varies with the wine in question. The free form exists in equilibrium between molecular SO (as a dissolved gas) and bisulfite ion, which is in turn in equilibrium with sulfite ion. These equilibria depend on the pH of the wine. Lower pH shifts the equilibrium towards molecular (gaseous) SO, which is the active form, while at higher pH more SO is found in the inactive sulfite and bisulfite forms. The molecular SO is active as an antimicrobial and antioxidant, and this is also the form which may be perceived as a pungent odor at high levels. Wines with total SO concentrations below 10 ppm do not require \"contains sulfites\" on the label by US and EU laws. The upper limit of total SO allowed in wine in the US is 350 ppm; in the EU it is 160 ppm for red wines and 210 ppm for white and rosé wines. In low concentrations, SO is mostly undetectable in wine, but at free SO concentrations over 50 ppm, SO becomes evident in the smell and taste of wine.\n\nSO is also a very important compound in winery sanitation. Wineries and equipment must be kept clean, and because bleach cannot be used in a winery due the risk of cork taint, a mixture of SO, water, and citric acid is commonly used to clean and sanitize equipment. Ozone (O) is now used extensively for sanitizing in wineries due to its efficacy, and because it does not affect the wine or most equipment.\n\nSulfur dioxide is also a good reductant. In the presence of water, sulfur dioxide is able to decolorize substances. Specifically, it is a useful reducing bleach for papers and delicate materials such as clothes. This bleaching effect normally does not last very long. Oxygen in the atmosphere reoxidizes the reduced dyes, restoring the color. In municipal wastewater treatment, sulfur dioxide is used to treat chlorinated wastewater prior to release. Sulfur dioxide reduces free and combined chlorine to chloride.\n\nSulfur dioxide is fairly soluble in water, and by both IR and Raman spectroscopy; the hypothetical sulfurous acid, HSO, is not present to any extent. However, such solutions do show spectra of the hydrogen sulfite ion, HSO, by reaction with water, and it is in fact the actual reducing agent present:\n\nSulfur dioxide is toxic in large amounts. It or its conjugate base bisulfite is produced biologically as an intermediate in both sulfate-reducing organisms and in sulfur-oxidizing bacteria, as well. The role of sulfur dioxide in mammalian biology is not yet well understood. Sulfur dioxide blocks nerve signals from the pulmonary stretch receptors and abolishes the Hering–Breuer inflation reflex.\n\nIt was shown that endogenous sulfur dioxide plays a role in diminishing an experimental lung damage caused by oleic acid. Endogenous sulfur dioxide lowered lipid peroxidation, free radical formation, oxidative stress and inflammation during an experimental lung damage. Conversely, a successful lung damage caused a significant lowering of endogenous sulfur dioxide production, and an increase in lipid peroxidation, free radical formation, oxidative stress and inflammation. Moreover, blockade of an enzyme that produces endogenous SO significantly increased the amount of lung tissue damage in the experiment. Conversely, adding acetylcysteine or glutathione to the rat diet increased the amount of endogenous SO produced and decreased the lung damage, the free radical formation, oxidative stress, inflammation and apoptosis.\n\nIt is considered that endogenous sulfur dioxide plays a significant physiological role in regulating cardiac and blood vessel function, and aberrant or deficient sulfur dioxide metabolism can contribute to several different cardiovascular diseases, such as arterial hypertension, atherosclerosis, pulmonary arterial hypertension, stenocardia.\n\nIt was shown that in children with pulmonary arterial hypertension due to congenital heart diseases the level of homocysteine is higher and the level of endogenous sulfur dioxide is lower than in normal control children. Moreover, these biochemical parameters strongly correlated to the severity of pulmonary arterial hypertension. Authors considered homocysteine to be one of useful biochemical markers of disease severity and sulfur dioxide metabolism to be one of potential therapeutic targets in those patients.\n\nEndogenous sulfur dioxide also has been shown to lower the proliferation rate of endothelial smooth muscle cells in blood vessels, via lowering the MAPK activity and activating adenylyl cyclase and protein kinase A. Smooth muscle cell proliferation is one of important mechanisms of hypertensive remodeling of blood vessels and their stenosis, so it is an important pathogenetic mechanism in arterial hypertension and atherosclerosis.\n\nEndogenous sulfur dioxide in low concentrations causes endothelium-dependent vasodilation. In higher concentrations it causes endothelium-independent vasodilation and has a negative inotropic effect on cardiac output function, thus effectively lowering blood pressure and myocardial oxygen consumption. The vasodilating and bronchodilating effects of sulfur dioxide are mediated via ATP-dependent calcium channels and L-type (\"dihydropyridine\") calcium channels. Endogenous sulfur dioxide is also a potent antiinflammatory, antioxidant and cytoprotective agent. It lowers blood pressure and slows hypertensive remodeling of blood vessels, especially thickening of their intima. It also regulates lipid metabolism.\n\nEndogenous sulfur dioxide also diminishes myocardial damage, caused by isoproterenol adrenergic hyperstimulation, and strengthens the myocardial antioxidant defense reserve.\n\nBeing easily condensed and possessing a high heat of evaporation, sulfur dioxide is a candidate material for refrigerants. Prior to the development of chlorofluorocarbons, sulfur dioxide was used as a refrigerant in home refrigerators.\n\nSulfur dioxide is a versatile inert solvent widely used for dissolving highly oxidizing salts. It is also used occasionally as a source of the sulfonyl group in organic synthesis. Treatment of aryl diazonium salts with sulfur dioxide and cuprous chloride yields the corresponding aryl sulfonyl chloride, for example:\n\nInjections of sulfur dioxide in the stratosphere has been proposed in climate engineering. The cooling effect would be similar to what has been observed after the large explosive volcano eruption of Mount Pinatubo in 1991. However this form of geoengineering would have uncertain regional consequences on rainfall patterns, for example in monsoon regions.\n\nSulfur dioxide is a noticeable component in the atmosphere, especially following volcanic eruptions. According to the United States Environmental Protection Agency, the amount of sulfur dioxide released in the U.S. per year was:\n\nSulfur dioxide is a major air pollutant and has significant impacts upon human health. In addition, the concentration of sulfur dioxide in the atmosphere can influence the habitat suitability for plant communities, as well as animal life. Sulfur dioxide emissions are a precursor to acid rain and atmospheric particulates. Due largely to the US EPA’s Acid Rain Program, the U.S. has had a 33% decrease in emissions between 1983 and 2002. This improvement resulted in part from flue-gas desulfurization, a technology that enables SO to be chemically bound in power plants burning sulfur-containing coal or oil. In particular, calcium oxide (lime) reacts with sulfur dioxide to form calcium sulfite:\nAerobic oxidation of the CaSO gives CaSO, anhydrite. Most gypsum sold in Europe comes from flue-gas desulfurization.\n\nSulfur can be removed from coal during burning by using limestone as a bed material in fluidized bed combustion.\n\nSulfur can also be removed from fuels before burning, preventing formation of SO when the fuel is burnt. The Claus process is used in refineries to produce sulfur as a byproduct. The Stretford process has also been used to remove sulfur from fuel. Redox processes using iron oxides can also be used, for example, Lo-Cat or Sulferox.\n\nFuel additives such as calcium additives and magnesium carboxylate may be used in marine engines to lower the emission of sulfur dioxide gases into the atmosphere.\n\nAs of 2006, China was the world's largest sulfur dioxide polluter, with 2005 emissions estimated to be . This amount represents a 27% increase since 2000, and is roughly comparable with U.S. emissions in 1980.\n\nInhaling sulfur dioxide is associated with increased respiratory symptoms and disease, difficulty in breathing, and premature death. In 2008, the American Conference of Governmental Industrial Hygienists reduced the short-term exposure limit to 0.25 parts per million (ppm). The OSHA PEL is currently set at 5 ppm (13  mg/m) time-weighted average. NIOSH has set the IDLH at 100 ppm. In 2010, the EPA \"revised the primary SO NAAQS by establishing a new one-hour standard at a level of 75 parts per billion (ppb). EPA revoked the two existing primary standards because they would not provide additional public health protection given a one-hour standard at 75 ppb.\"\n\nA 2011 systematic review concluded that exposure to sulfur dioxide is associated with preterm birth.\n\nIn the United States, the Center for Science in the Public Interest lists the two food preservatives, sulfur dioxide and sodium bisulfite, as being safe for human consumption except for certain asthmatic individuals who may be sensitive to them, especially in large amounts. Symptoms of sensitivity to sulfiting agents, including sulfur dioxide, manifest as potentially life-threatening trouble breathing within minutes of ingestion.\n\n\n"}
{"id": "46579163", "url": "https://en.wikipedia.org/wiki?curid=46579163", "title": "Tesla Powerwall", "text": "Tesla Powerwall\n\nThe Powerwall and Powerpack are rechargeable lithium-ion battery stationary energy storage products manufactured by Tesla, Inc. The Powerwall is intended to be used for home energy storage and stores electricity for solar self-consumption, time of use load shifting, backup power, and off-the-grid use. The larger Powerpack is intended for commercial or electric utility grid use and can be used for peak shaving, load shifting, backup power, demand response, microgrids, renewable power integration, frequency regulation, and voltage control.\n\nAnnounced in 2015, with a pilot demonstration of 500 units built and installed during 2015, production of the product was initially at the Tesla Fremont factory before being moved to the under-construction Gigafactory 1 in Nevada. The second generation of both products was announced in October 2016.\n\nTesla started development in 2012, installing prototypes at selected industrial customers. In some cases, PowerPacks have saved 20% of the electrical bill. Tesla originally announced the Powerwall at the April 30, 2015 product launch with power output of 2 kW steady and 3.3 kW peak, but Musk said at the June 2015 Tesla shareholders meeting that this would be more than doubled to 5 kW steady with 7 kW peak, with no increase in price. He also announced that Powerwall deliveries would be prioritized to partners who minimize the cost to the end user, with a Powerwall installation price of .\n\nWhen originally announced in 2015, two models of Powerwall were planned: 10 kWh capacity for backup applications and 7 kWh capacity for daily cycle applications. By March 2016, however, Tesla had \"quietly removed all references to its 10-kilowatt-hour residential battery from the Powerwall website, as well as the company's press kit. The company's smaller battery designed for daily cycling is all that remains.\" The 10 kWh battery as originally announced has a nickel-cobalt-aluminum cathode, like the Tesla Model S, which was projected to function as a backup/uninterruptible power supply, and had a projected cycle life of 1000–1500 cycles.\n\nIn October 2016, Tesla announced that nearly 300 MWh of Tesla batteries had been deployed in 18 countries. The Powerwall 2 was unveiled in October 2016 at Universal Studios' Colonial Street, Los Angeles, backlot street set and is designed to work with the solar panel roof tiles to be produced by SolarCity.\n\nSince March 2016, there was only a single model: the 6.4 kWh version for daily cycle applications, before the Powerwall 2 was introduced:\n\nTesla installed a grid storage facility for Southern California Edison with a capacity of 80 MWh at a power of 20 MW between September 2016 and December 2016. the storage unit was one of the largest accumulator batteries on the market. Tesla installed 400 Powerpack-2 modules at the Mira Loma transformer station in California. The battery storage serves to store energy at a low network load and then to feed this energy back into the grid at peak load. The principal way of adding peak generation capacity prior to this was the use of gas-fired power stations.\n\nThe first generation Powerwall has a 6.4 kWh capacity for daily cycle applications. Users with larger energy needs can connect multiple Powerwalls to expand the capacity even higher. In March 2016 Tesla quietly discontinued a previously announced 10 kWh capacity model designed to produce backup power as the 6.4 kWh version can also be configured to act as backup power.\n\nThe Powerpack is a bigger unit with 100 kWh (first generation) and 210 kWh (2nd generation) of storage for commercial and utility grid use. To meet the variety of energy needs in industry, \"Powerpack is infinitely scalable\", said Elon Musk. Tesla's objective is to \"fundamentally change the way the world uses energy\" by \"fostering a clean energy ecosystem and helping wean the world off fossil fuels\" using backup energy storage for renewable energy. The Powerpack 2 has 200 kWh of storage, probably using the 2170 cell by the end of 2016.\n\nThe Powerwall is optimized for daily cycling, such as for load shifting. Tesla uses proprietary technology for packaging and cooling the cells in packs with liquid coolant. Elon Musk, the chairman, CEO and product architect of the Tesla company, promised not to start patent infringement lawsuits against anyone who, in good faith, used Tesla's technology for Powerwalls as he had promised with Tesla cars.\n\nThe daily cycle 7 kWh PW1 battery uses nickel-manganese-cobalt chemistry and can be cycled 5,000 times before warranty expiration. The Tesla Powerwall has a 92.5% round-trip efficiency when charged or discharged by a 400–450 V system at 2 kW with a temperature of when the product is brand new. Age of the product, temperatures above or below , and charge rates or discharge rates above 2 kW would lower this efficiency number, decreasing the system performance.\n\nFirst-generation Powerwalls include a DC-to-DC converter to sit between a home's existing solar panels and the home's existing DC to AC inverter. If the existing inverter is not storage-ready, one must be purchased. The second generation Powerwall incorporates a DC-to-AC inverter of Tesla's own design. Production of the 2170 cell for the PW2 and PP2 began at Gigafactory in January 2017.\n\nThe National Fire Protection Association conducted two worst-case scenario tests in 2016, igniting Powerpacks to initiate thermal run-off. The design contained damage to the structures.\n\nThe Powerwall was unveiled on April 30, 2015, with a 7 kWh Powerwall model that would retail for and a 10 kWh model at . Shipments of 500 pilot units were planned to begin in the late summer of 2015. Musk indicated that he believed the low Tesla price would cause other storage producers to follow. Before the April 30, 2015, unveiling, some existing solar-panel users participated in a demonstration program and paid up to for a 10 or 15 kWh Tesla battery.\n\n, Powerwalls were sold to companies including SolarCity and OUXO Energy for installation. SolarCity was running a pilot project in 500 California houses, using 10 kWh battery packs. A market overview calculates Powerwall 2 at 0.23 Australian dollars per warranted kWh.\n\nAs of May 2015, Tesla Powerwall had already sold out through to the middle of 2016. Reservations within the first few weeks were over 50,000 units for the Powerwall (), and 25,000 units for the Powerpack (), therefore combined orders of .\n\nDuring the first quarter of 2016, Tesla delivered over 25 MWh of energy storage to customers on four continents. Over 2,500 Powerwalls and nearly 100 Powerpacks were delivered in North America, Asia, Europe, and Africa. The first Powerwall in Portugal has been sold by OUXO Energy. , nearly 300 MWh of Tesla batteries had been deployed worldwide.\n\nAt the announcement, a larger battery called Powerpack—storing 100 kWh of electrical energy—was projected to be available for industrial consumers, reaching a price point of $250/kWh. The Powerpack was projected to comprise the majority of stationary storage production at Gigafactory 1 while Powerwall would play a smaller part, giving Tesla a profit margin of 20 percent.\n\nIn September 2016, Tesla priced the Powerpack at $445/kWh, and a system with 200 kWh of energy and 100 kW of peak power was the cheapest available priced at $145,100. A bi-directional 250 kW inverter costs $52,500. By October 2016, a limited system of Powerpack 2 cost $398/kWh.\n\nMusk predicted in 2016 that the utility power will need to increase to supply more electric vehicles, eventually reaching an equilibrium with about 1/3 of power coming from distributed energy and 2/3 from utilities. Battery storage is one of the ways to mitigate the increasing duck curve, particularly in California.\n\nA May 2015 article in \"Forbes\" magazine calculated that using a Tesla Powerwall 1 model combined with solar panels in a home would cost 30 cents/kWh for electricity if a home remains connected to the grid (the article acknowledges that the Tesla battery could make economic sense in applications that are entirely off-grid). US consumers got electricity from the power grid for 12.5 cents/kWh on average. The article concluded the \"...Tesla's Powerwall Is Just Another Toy For Rich Green People.\" Bloomberg and Catalytic Engineering also agreed that the Tesla system was most useful in places where electricity prices are high.\n\nThere are however a number of such locations, including Hawaii and other remote islands that generate electricity with shipped-in or flown-in fuels. Residential California PG&E customers pay as much as 40 cents/kWh if they reach Tier 3 in electrical usage.\nArctic and sub-Arctic locations with high energy prices cannot generate sufficient solar energy in the winter due to little or no sunlight.\n\nThe Swiss bank UBS said that the Powerwall makes sense in Australia and Germany where electricity is very costly but solar panels are well distributed.\n\nAs of November 2016, the cost to install one Powerwall 2 starts at AU$8,750 in Australia or US$1,600 in the US. As of February 2017, the cost to install one Powerwall 1 is US$3,000 (sized for residential use), while the cost to install one Powerwall 2 jumped to US$5,500 (larger capacity but still residential-size, will run many homes for 2 or 3 days without outside power if fully charged).\n\nEnergy technology company Enphase Energy, based in California, has announced it will release its lithium iron phosphate battery as part of a complete alternating current Enphase Home Energy Solution starting in winter 2016 in Australia and New Zealand with Genesis Energy conducting trials. The system, which includes monitoring and control of solar generation, home energy consumption, and battery storage, will be sold at wholesale through solar distributors, who sell to solar installers. Enphase's modular 'building block' batteries are more efficient than the Tesla Powerwall (96% compared to Tesla's 92% round-trip efficiency). The Enphase AC Battery also includes an inverter inside the casing and works with all existing solar systems or alternatively, in homes without solar. Lithium iron phosphate batteries are known to be the most stable and safe of the various lithium batteries.\n\nLG Chem announced RESU10H(Type-R)_Expansion system as part of their Energy Storage System product portfolio and are compatible with SolarEdge inverters.\n\nBYD's energy storage system is another competitor of Tesla's Powerpack. UC San Diego installed this system, which has 5 megawatt-hour (MWh) capacity—enough to power 2,500 homes—in September 2014. BYD is a large supplier of rechargeable batteries, and is also known for its leading position in electric buses.\n\nSonnen and AutoGrid collaborated on combining house batteries into a large-scale utility-level grid storage system. Eos claimed a battery price of $160/kWh in 2017, before the cost of integration by Siemens.\n\n\n"}
{"id": "24971974", "url": "https://en.wikipedia.org/wiki?curid=24971974", "title": "Trimdon Grange Wind Farm", "text": "Trimdon Grange Wind Farm\n\nTrimdon Grange Wind Farm is an onshore wind farm near Trimdon Grange, County Durham, England. Commissioned in October 2008 it consists of four 1.3 MW Nordex N60 turbines giving a total capacity of 5.2 MW. It was developed by EDF and is operated by Cumbria Wind Farms.\n\nThe proposed location of the wind farm caused controversy in 2004 when an agent of Tony Blair, then Prime Minister, wrote to a local action group, Trimdon Area Group Against Wind Farms, claiming that the site was unsuitable. The location is about one mile from Tony Blair's old constituency house.\n"}
{"id": "40483531", "url": "https://en.wikipedia.org/wiki?curid=40483531", "title": "Tubular pinch effect", "text": "Tubular pinch effect\n\nThe tubular pinch effect is a phenomenon in fluid mechanics, which has importance in membrane technology.\n\nMark C. Porter first suspected that the pinch effect was responsible for the return of separated particles into the core flow by the membrane. This effect was first demonstrated in 1956 by G. Sergé and A. Silberberg. They had been working with dilute suspensions of spherical particles in pipelines. While the particle was flowing through the pipeline, it appeared to migrate away from the pipe axis and pipe wall and reach equilibrium in a radial eccentric position. \nIf:\nthen the pinch effect follows the relation:\n\nThis effect is of importance in cross-flow filtration and especially in dialysis. It is significant especially for particles with a diameter of 5 µm and for particles which follow laminar flow conditions and slows down the process of filter cake formation, which prolongs the service life and the filtering stays permanently high.\n\n"}
{"id": "21281445", "url": "https://en.wikipedia.org/wiki?curid=21281445", "title": "Walbrook Wharf", "text": "Walbrook Wharf\n\nWalbrook Wharf is an operating freight wharf in the Port of London located in the City of London adjacent to Cannon Street station. It has safeguarded wharf status by the Mayor of London and the Port of London Authority (PLA).\n\nIt is used as a waste transfer station owned by the City of London Corporation and operated by Cory Environmental. Refuse from central London is transferred onto barges for transport to the Belvedere Incinerator in the London Borough of Bexley.\n\nWalbrook Wharf was formerly arranged as a dock, but modern containerised loading has resulted in the infilling of the dock. The wharf is the point where the ancient stream, the Walbrook fed into the Thames, a location also known as Dowgate.\n\nThe riverside walk runs along Walbrook Wharf and is only closed to pedestrians at this point when waste is being transferred onto barges.\n"}
{"id": "1715797", "url": "https://en.wikipedia.org/wiki?curid=1715797", "title": "Water splitting", "text": "Water splitting\n\nWater splitting is the chemical reaction in which water is broken down into oxygen and hydrogen:\n\nEfficient and economical photochemical water splitting would be a technological breakthrough that could underpin a hydrogen economy. A version of water splitting occurs in photosynthesis, but hydrogen is not produced. No practical version of water splitting has been demonstrated, but the two component reactions (H production and O production) are well known. The reverse of water splitting is the basis of the hydrogen fuel cell.\n\nElectrolysis of water is the decomposition of water (HO) into oxygen (O) and hydrogen (H) due to an electric current being passed through the water. \n\nIn power to gas production schemes, the excess power or off peak power created by wind generators or solar arrays is used for load balancing of the energy grid by storing and later injecting the hydrogen into the natural gas grid.\nProduction of hydrogen from water is energy intensive. Potential electrical energy supplies include hydropower, wind turbines, or photovoltaic cells. Usually, the electricity consumed is more valuable than the hydrogen produced so this method has not been widely used. In contrast with low-temperature electrolysis, high-temperature electrolysis (HTE) of water converts more of the initial heat energy into chemical energy (hydrogen), potentially doubling efficiency to about 50%. Because some of the energy in HTE is supplied in the form of heat, less of the energy must be converted twice (from heat to electricity, and then to chemical form), and so the process is more efficient.\n\nA version of water splitting occurs in photosynthesis, but the electrons are shunted, not to protons, but to the electron transport chain in photosystem II. The electrons are used to convert carbon dioxide into sugars.\n\nWhen photosystem I gets photo-excited, electron transfer reactions gets initiated, which results in reduction of a series of electron acceptors, eventually reducing NADP to NADPH and PS I is oxidized. The oxidized photosystem I captures electrons from photosystem II through a series of steps involving agents like plastoquinone, cytochromes and plastocyanine. The photosystem II then brings about water oxidation resulting in evolution of oxygen, the reaction being catalyzed by CaMnO clusters embedded in complex protein environment; the complex is known as oxygen evolving complex (OEC).\nIn biological hydrogen production, the electrons produced by the photosystem are shunted not to a chemical synthesis apparatus but to hydrogenases, resulting in formation of H. This biohydrogen is produced in an bioreactor.\n\nUsing electricity produced by photovoltaic systems potentially offers the cleanest way to produce hydrogen, other than nuclear, wind, geothermal, and hydroelectric. Again, water is broken down into hydrogen and oxygen by electrolysis, but the electrical energy is obtained by a photoelectrochemical cell (PEC) process. The system is also named artificial photosynthesis.\n\nThe conversion of solar energy to hydrogen by means of water splitting process is one of the most interesting ways to achieve clean and renewable energy. However, if this process is assisted by photocatalysts suspended directly in water rather than a photovoltaic or an electrolytic system, the reaction takes place in one step, it therefore can be more efficient.\n\nNuclear radiation routinely breaks water bonds, in the Mponeng gold mine, South Africa, researchers found in a naturally high radiation zone, a community dominated by a new phylotype of \"Desulfotomaculum\", feeding on primarily radiolytically produced H. Spent nuclear fuel/\"nuclear waste\" is also being looked at as a potential source of hydrogen.\n\nIn thermolysis, water molecules split into their atomic components hydrogen and oxygen. For example, at 2200 °C about three percent of all HO are dissociated into various combinations of hydrogen and oxygen atoms, mostly H, H, O, O, and OH. Other reaction products like HO or HO remain minor. At the very high temperature of 3000 °C more than half of the water molecules are decomposed, but at ambient temperatures only one molecule in 100 trillion dissociates by the effect of heat. The high temperatures and material constraints have limited the applications of this approach.\n\nSome prototype Generation IV reactors, such as the High-temperature engineering test reactor, operate at 850 to 1000 degrees Celsius, considerably hotter than existing commercial nuclear power plants. General Atomics predicts that hydrogen produced in a High Temperature Gas Cooled Reactor (HTGR) would cost $1.53/kg. In 2003, steam reforming of natural gas yielded hydrogen at $1.40/kg. At gas prices, hydrogen cost $2.70/kg. Hence, just within the United States, a savings of tens of billions of dollars per year is possible with a nuclear-powered supply. Much of this savings would translate into reduced oil and natural gas imports.\n\nOne side benefit of a nuclear reactor that produces both electricity and hydrogen is that it can shift production between the two. For instance, the plant might produce electricity during the day and hydrogen at night, matching its electrical generation profile to the daily variation in demand. If the hydrogen can be produced economically, this scheme would compete favorably with existing grid energy storage schemes. What is more, there is sufficient hydrogen demand in the United States that all daily peak generation could be handled by such plants.\n\nThe hybrid thermoelectric Copper-chlorine cycle is a cogeneration system using the waste heat from nuclear reactors, specifically the CANDU supercritical water reactor.\n\nThe high temperatures necessary to split water can be achieved through the use of concentrating solar power. Hydrosol-2 is a 100-kilowatt pilot plant at the Plataforma Solar de Almería in Spain which uses sunlight to obtain the required 800 to 1,200 °C to split water. Hydrosol II has been in operation since 2008. The design of this 100-kilowatt pilot plant is based on a modular concept. As a result, it may be possible that this technology could be readily scaled up to megawatt range by multiplying the available reactor units and by connecting the plant to heliostat fields (fields of sun-tracking mirrors) of a suitable size.\n\nMaterial constraints due to the required high temperatures are reduced by the design of a membrane reactor with simultaneous extraction of hydrogen and oxygen that exploits a defined thermal gradient and the fast diffusion of hydrogen. With concentrated sunlight as heat source and only water in the reaction chamber, the produced gases are very clean with the only possible contaminant being water. A \"Solar Water Cracker\" with a concentrator of about 100 m² can produce almost one kilogram of hydrogen per sunshine hour.\n\nResearch is being conducted over photocatalysis, the acceleration of a photoreaction in the presence of a catalyst. Its comprehension has been made possible ever since the discovery of water electrolysis by means of the titanium dioxide. Artificial photosynthesis is a research field that attempts to replicate the natural process of photosynthesis, converting sunlight, water and carbon dioxide into carbohydrates and oxygen. Recently, this has been successful in splitting water into hydrogen and oxygen using an artificial compound called Nafion.\n\nHigh-temperature electrolysis (also HTE or steam electrolysis) is a method currently being investigated for the production of hydrogen from water with oxygen as a by-product. Other research includes thermolysis on defective carbon substrates, thus making hydrogen production possible at temperatures just under 1000 °C.\n\nThe iron oxide cycle is a series of thermochemical processes used to produce hydrogen. The iron oxide cycle consists of two chemical reactions whose net reactant is water and whose net products are hydrogen and oxygen. All other chemicals are recycled. The iron oxide process requires an efficient source of heat.\n\nThe sulfur-iodine cycle (S-I cycle) is a series of thermochemical processes used to produce hydrogen. The S-I cycle consists of three chemical reactions whose net reactant is water and whose net products are hydrogen and oxygen. All other chemicals are recycled. The S-I process requires an efficient source of heat.\n\nMore than 352 thermochemical cycles have been described for water splitting or thermolysis., These cycles promise to produce hydrogen oxygen from water and heat without using electricity. Since all the input energy for such processes is heat, they can be more efficient than high-temperature electrolysis. This is because the efficiency of electricity production is inherently limited. Thermochemical production of hydrogen using chemical energy from coal or natural gas is generally not considered, because the direct chemical path is more efficient.\n\nFor all the thermochemical processes, the summary reaction is that of the decomposition of water:\nAll other reagents are recycled. None of the thermochemical hydrogen production processes have been demonstrated at production levels, although several have been demonstrated in laboratories.\n\nThere is also research into the viability of nanoparticles and catalysts to lower the temperature at which water splits.\n\nRecently Metal-Organic Framework (MOF)-based materials have been shown to be a highly promising candidate for water splitting with cheap, first row transition metals.;\n\nResearch is concentrated on the following cycles:\n\n\n"}
{"id": "1403906", "url": "https://en.wikipedia.org/wiki?curid=1403906", "title": "Windscale fire", "text": "Windscale fire\n\nThe Windscale fire of 10 October 1957 was the worst nuclear accident in Great Britain's history, ranked in severity at level 5 out of a possible 7 on the International Nuclear Event Scale. The fire took place in Unit 1 of the two-pile Windscale facility on the northwest coast of England in Cumberland (now Sellafield, Cumbria). The two graphite-moderated reactors, referred to at the time as \"piles\", had been built as part of the British post-war atomic bomb project. Windscale Pile No. 1 was operational in October 1950 followed by Pile No. 2 in June 1951.\n\nThe fire burned for three days and there was a release of radioactive contamination that spread across the UK and Europe. Of particular concern at the time was the radioactive isotope iodine-131, which may lead to cancer of the thyroid, and it has been estimated that the incident caused 240 additional cancer cases. No one was evacuated from the surrounding area, but there was a worry that milk might be dangerously contaminated. Milk from about of nearby countryside was diluted and destroyed for about a month. \n\nHowever, A 2010 study of workers directly involved in the cleanup found no significant long term health effects from their involvement.\n\nThe December 1938 discovery of nuclear fission by Otto Hahn and Fritz Strassmann—and its explanation and naming by Lise Meitner and Otto Frisch—raised the possibility that an extremely powerful atomic bomb could be created. During the Second World War, Frisch and Rudolf Peierls at the University of Birmingham calculated the critical mass of a metallic sphere of pure uranium-235, and found that as little as might explode with the power of thousands of tons of dynamite. In response, the British government initiated an atomic bomb project, codenamed Tube Alloys. The August 1943 Quebec Agreement merged Tube Alloys with the American Manhattan Project. As overall head of the British contribution to the Manhattan Project, James Chadwick forged a close and successful partnership with the Americans, and ensured that British participation was complete and wholehearted.\n\nAfter the war ended the Special Relationship between Britain and the United States \"became very much less special\". The British government had trusted that America would continue to share nuclear technology, which it considered a joint discovery, but little information was exchanged immediately after the war, and the Atomic Energy Act of 1946 (McMahon Act) officially ended technical cooperation. Its control of \"restricted data\" prevented the United States' allies from receiving any information. The British government saw this as a resurgence of United States isolationism akin to that which had occurred after the First World War. This raised the possibility that Britain might have to fight an aggressor alone. It also feared that Britain might lose its great power status, and therefore its influence in world affairs, The Prime Minister of the United Kingdom, Clement Attlee, set up a cabinet sub-committee, the Gen 75 Committee (known informally as the \"Atomic Bomb Committee\"), on 10 August 1945 to examine the feasibility of a renewed nuclear weapons programme.\n\nThe Tube Alloys Directorate was transferred from the Department of Scientific and Industrial Research to the Ministry of Supply on 1 November 1945, and Lord Portal was appointed Controller of Production, Atomic Energy (CPAE), with direct access to the Prime Minister. An Atomic Energy Research Establishment (AERE) was established at RAF Harwell, south of Oxford, under the directorship of John Cockcroft. Christopher Hinton agreed to oversee the design, construction and operation of the new nuclear weapons facilities, which included a uranium metal plant at Springfields in Lancashire, and nuclear reactors and plutonium processing facilities at Windscale in Cumbria. He established his headquarters in a former Royal Ordnance Factory at Risley in Lancashire on 4 February 1946.\n\nIn July 1946, the Chiefs of Staff Committee recommended that Britain acquire nuclear weapons. They estimated that 200 bombs would be required by 1957. The 8 January 1947 meeting of the Gen 163 Committee, a subcommittee of the Gen 75 Committee, agreed to proceed with the development of atomic bombs, and endorsed Portal's proposal to place Penney, now the Chief Superintendent Armament Research (CSAR) at Fort Halstead in Kent, in charge of the development effort, which was codenamed High Explosive Research. Penney contended that \"the discriminative test for a first-class power is whether it has made an atomic bomb and we have either got to pass the test or suffer a serious loss of prestige both inside this country and internationally.\"\n\nThrough their participation in the wartime Tube Alloys and Manhattan Project, British scientists had considerable knowledge of the production of fissile materials. The Americans had created two kinds: uranium-235 and plutonium, and had pursued three different methods of uranium enrichment. An early decision had to be made as to whether High Explosive Research should concentrate on uranium-235 or plutonium. While everyone would have liked to pursue every avenue, like the Americans had, it was doubtful whether the cash-strapped post-war British economy could afford the money or the skilled manpower that this would require. The scientists who had remained in Britain favoured uranium-235, but those who had been working in America were strongly in favour of plutonium. They estimated that a uranium-235 bomb would require ten times the fissile material as one using plutonium to produce half the TNT equivalent. Estimates of the cost of nuclear reactors varied, but it was reckoned that a uranium enrichment plant would cost ten times as much to produce the same number of atomic bombs as a reactor. The decision was therefore taken in favour of plutonium.\n\nThe reactors were built in a short time near the village of Seascale, Cumberland. They were known as Windscale Pile 1 and Pile 2, housed in large concrete buildings a few hundred feet apart. The core of the reactors consisted of a large block of graphite with horizontal channels drilled through it for the fuel cartridges. Each cartridge consisted of a uranium rod about long encased in an aluminium canister to protect it from the air, as uranium becomes highly reactive when hot and can catch fire. The cartridge was finned, allowing heat exchange with the environment to cool the fuel rods while they were in the reactor. Rods were pushed in the front of the core, the \"charge face\", with new rods being added at a calculated rate. This pushed the other cartridges in the channel towards the rear of the reactor, eventually causing them to fall out the back, the \"discharge face\", into a water filled channel where they cooled and could be collected. The chain reaction in the core converted the uranium into a variety of isotopes, including some plutonium, which was separated from the other materials using chemical processing. As this plutonium was intended for weapons purposes, the burnup of the fuel would have been kept low to reduce production of the heavier plutonium isotopes like plutonium-240 and plutonium-241.\n\nThe design initially called for the core to be cooled like the B Reactor, which used a constant supply of water that poured through the channels in the graphite. There was considerable concern that such a system was subject to catastrophic failure in the event of a loss-of-coolant accident. This would cause the reactor to run out of control in seconds, potentially exploding. At Hanford, this possibility was dealt with by constructing a 30-mile (48-km) escape road to evacuate the staff were this to occur, abandoning the site. Lacking any location where a 30-mile area could be abandoned if a similar event were to occur in the UK, the designers desired a passively safe cooling system. In place of water, they used air cooling driven by convection through a 400-foot (120-m) tall chimney, which could create enough airflow to cool the reactor under normal operating conditions. The chimney was arranged so it pulled air through the channels in the core, cooling the fuel via fins on the cartridges. For additional cooling, huge fans were positioned in front of the core, which could greatly increase the airflow rate.\n\nDuring construction, Terence Price, one of the many physicists working on the project, began to consider what would happen if one of the fuel cartridges being pushed out the back of the core were to break open. This could happen, for example, if a new cartridge being inserted was pushed too hard, causing the one at the back of the channel to fall past the relatively narrow water channel and strike the floor behind it. In that event, the hot uranium could catch fire, with the fine uranium oxide dust being blown up the chimney to escape. When he raised the issue at a meeting and suggested that filters be added to the chimneys, the concern was dismissed as being too difficult to deal with and was not even recorded in the minutes. Sir John Cockcroft, leading the project team, was alarmed enough to order that filters be installed, which required them to be constructed on the ground while the chimneys were still being built, and then winched into position at the top once the chimney's concrete had set. These became known as \"Cockcroft's Folly\" by workers and engineers.\n\nIn the end, Price's concerns came to pass. So many cartridges missed the water channel that it became routine for staff to walk through the chimney ductwork with shovels and scoop the cartridges back into the water. On other occasions, fuel cartridges became stuck in the channels and burst open while still in the core. In spite of these precautions and the stack filters, scientist Frank Leslie discovered radioactivity around the site and the village, but this information was kept secret, even from the staff at the station.\n\nOnce commissioned and settled into operations, Pile 2 experienced a mysterious rise in core temperature. Unlike the Americans and the Soviets, the British had little experience with the behaviour of graphite when exposed to neutrons. Hungarian-American physicist Eugene Wigner had discovered that graphite, when bombarded by neutrons, suffers dislocations in its crystalline structure, causing a build-up of potential energy. This energy, if allowed to accumulate, could escape spontaneously in a powerful rush of heat.\n\nThe sudden bursts of energy worried the operators, who turned to the only viable solution, heating the reactor core in a process known as annealing. When graphite is heated beyond 250 °C it becomes plastic, and the Wigner dislocations can relax into their natural state. This process was gradual and caused a uniform release which spread throughout the core.\n\nWinston Churchill publicly committed the UK to building a hydrogen bomb, and gave the scientists a tight schedule in which to do so. This was then hastened after the US and USSR began working on a test ban and possible disarmament agreements which would begin to take effect in 1958. To meet this deadline there was no chance of building a new reactor to produce the required tritium, so the Windscale Pile 1 fuel loads were modified by adding enriched uranium and lithium-magnesium, the latter of which would produce tritium during neutron bombardment. All of these materials were highly flammable, and a number of the Windscale staff raised the issue of the inherent dangers of the new fuel loads. These concerns were brushed aside.\n\nWhen their first H-bomb test failed, the decision was made to build a large fusion-boosted-fission weapon instead. This required huge quantities of tritium, five times as much, and it had to be produced as rapidly as possible as the test deadlines approached. To boost the production rates, they used a trick that had been successful in increasing plutonium production in the past; by reducing the size of the cooling fins on the fuel cartridges, the temperature of the fuel loads increased, which caused a small but useful increase in neutron enrichment rates. This time they also took advantage of the smaller fins by building larger interiors in the cartridges, allowing more fuel in each one. These changes triggered further warnings from the technical staff, which were again brushed aside. Christopher Hinton, Windscale's director, left in frustration.\n\nAfter a first successful production run of tritium in Pile 1, the heat problem was presumed to be negligible and full-scale production began. But by raising the temperature of the reactor beyond the design specifications, the scientists had altered the normal distribution of heat in the core, causing hot spots to develop in Pile 1. These were not detected because the thermocouples used to measure the core temperatures were positioned based on the original heat distribution design, and were not measuring the parts of the reactor which became hottest.\n\nOn 7 October 1957 operators of Pile 1 noticed that the reactor was heating up more than normal, and a Wigner release was ordered. This had been carried out eight times in the past, and it was known that the cycle would cause the entire reactor core to heat up evenly. During this attempt the temperatures anomalously began falling across the reactor core, except in channel 2053, whose temperature was rising. Concluding that 2053 was releasing energy but none of the others were, on the morning of 8 October the decision was made to try a second Wigner release. This attempt caused the temperature of the entire reactor to rise, indicating a successful release.\n\nEarly in the morning of 10 October it was suspected that something unusual was going on. The temperature in the core was supposed to gradually fall as Wigner energy release ended, but the monitoring equipment showed something more ambiguous, and one thermocouple indicated that core temperature was instead rising. As this process continued, the temperature continued to rise and eventually reached 400 °C. In an effort to cool the pile, the cooling fans were sped up and airflow was increased. Radiation detectors in the chimney then indicated a release, and it was assumed that a cartridge had burst. This was not a fatal problem, and had happened in the past. Unknown to the operators, the cartridge had not just burst, but caught fire, and this was the source of the anomalous heating in channel 2053, not a Wigner release.\n\nSpeeding up the fans increased the airflow in the channel, fanning the flames. The fire spread to surrounding fuel channels, and soon the radioactivity in the chimney was rapidly increasing. A foreman, arriving for work, noticed smoke coming out of the chimney. The core temperature continued to rise, and the operators began to suspect the core was on fire.\n\nOperators tried to examine the pile with a remote scanner but it had jammed. Tom Hughes, second in command to the Reactor Manager, suggested examining the reactor personally and so he and another operator went to the charge face of the reactor, clad in protective gear. A fuel channel inspection plug was taken out close to a thermocouple registering high temperatures and it was then that the operators saw that the fuel was red hot.\n\n\"An inspection plug was taken out,\" said Tom Hughes in a later interview, \"and we saw, to our complete horror, four channels of fuel glowing bright cherry red.\"\n\nThere was now no doubt that the reactor was on fire, and had been for almost 48 hours. Reactor Manager Tom Tuohy donned full protective equipment and breathing apparatus and scaled the 80-foot (24-m) ladder to the top of the reactor building, where he stood atop the reactor lid to examine the rear of the reactor, the discharge face. Here he reported a dull red luminescence visible, lighting up the void between the back of the reactor and the rear containment. Red hot fuel cartridges were glowing in the fuel channels on the discharge face. He returned to the reactor upper containment several times throughout the incident, at the height of which a fierce conflagration was raging from the discharge face and playing on the back of the reinforced concrete containment — concrete whose specifications required that it be kept below a certain temperature to prevent its collapse.\n\nOperators were unsure what to do about the fire. First they tried to blow the flames out by running the fans at maximum speed, but this fed the flames. Tom Hughes and his colleague had already created a fire break by ejecting some undamaged fuel cartridges from around the blaze, and Tom Tuohy suggested trying to eject some from the heart of the fire by bludgeoning the melted cartridges through the reactor and into the cooling pond behind it with scaffolding poles. This proved impossible and the fuel rods refused to budge, no matter how much force was applied. The poles were withdrawn with their ends red hot; one returned dripping molten metal. Hughes knew this had to be molten irradiated uranium, causing serious radiation problems on the charge hoist itself.\n\n\"It [the exposed fuel channel] was white hot,\" said Hughes' colleague on the charge hoist with him, \"it was just white hot. Nobody, I mean, nobody, can believe how hot it could possibly be.\"\n\nNext, the operators tried to extinguish the fire using carbon dioxide. The new gas-cooled Calder Hall reactors on the site had just received a delivery of 25 tonnes of liquid carbon dioxide and this was rigged up to the charge face of Windscale Pile 1, but there were problems getting it to the fire in useful quantities.\n\n\"So we got this rigged up,\" Hughes recounted, \"and we had this poor little tube of carbon dioxide and I had absolutely no hope it was going to work.\"\n\nOn the morning of Friday 11 October, when the fire was at its worst, eleven tons of uranium were ablaze. Temperatures were becoming extreme (one thermocouple registered 1,300 °C) and the biological shield around the stricken reactor was now in severe danger of collapse. Faced with this crisis, Tuohy suggested using water. This was risky, as molten metal oxidises in contact with water, stripping oxygen from the water molecules and leaving free hydrogen, which could mix with incoming air and explode, tearing open the weakened containment. Faced with a lack of other options, the operators decided to go ahead with the plan.\n\nAbout a dozen fire hoses were hauled to the charge face of the reactor; their nozzles were cut off and the lines themselves connected to scaffolding poles and fed into fuel channels about a metre (roughly 3 feet) above the heart of the fire. Tuohy once again hauled himself onto the reactor shielding and ordered the water to be turned on, listening carefully at the inspection holes for any sign of a hydrogen reaction as the pressure was increased. The water was unsuccessful in extinguishing the fire, requiring further measures to be taken.\n\nTuohy then ordered everyone out of the reactor building except himself and the Fire Chief in order to shut off all cooling and ventilating air entering the reactor. Tuohy then climbed up several times and reported watching the flames leaping from the discharge face slowly dying away. During one of the inspections, he found that the inspection plates—which were removed with a metal hook to facilitate viewing of the discharge face of the core—were stuck fast. This, he reported, was due to the fire trying to suck air in from wherever it could.\n\n\"I have no doubt it was even sucking air in through the chimney at this point to try and maintain itself,\" he remarked in an interview.\n\nFinally he managed to pull the inspection plate away and was greeted with the sight of the fire dying away.\n\n\"First the flames went, then the flames reduced and the glow began to die down,\" he described, \"I went up to check several times until I was satisfied that the fire was out. I did stand to one side, sort of hopefully,\" he went on to say, \"but if you're staring straight at the core of a shut down reactor you're going to get quite a bit of radiation.\" (Tuohy lived to the age of 90, despite his exposure.)\n\nWater was kept flowing through the pile for a further 24 hours until it was completely cold.\n\nThe reactor tank itself has remained sealed since the accident and still contains about 15 tons of uranium fuel. It was thought that the remaining fuel could still reignite if disturbed, due to the presence of pyrophoric uranium hydride formed in the original water dousing. Subsequent research, conducted as part of the decommissioning process, has ruled out this possibility. The pile is not scheduled for final decommissioning until 2037.\n\nThere was a release to atmosphere of radioactive material that spread across the UK and Europe. The fire released an estimated 740 terabecquerels (20,000 curies) of iodine-131, as well as 22 TBq (594 curies) of caesium-137 and 12,000 TBq (324,000 curies) of xenon-133, among other radionuclides. Later reworking of contamination data has shown national and international contamination may have been higher than previously estimated. For comparison, the 1986 Chernobyl explosion released approximately 1,760,000 TBq of iodine-131; 79,500 TBq caesium-137; 6,500,000 TBq xenon-133; 80,000 TBq strontium-90; and 6100 TBq plutonium, along with about a dozen other radionuclides in large amounts. The Three Mile Island accident in 1979 released 25 times more xenon-135 than Windscale, but much less iodine, caesium and strontium. Estimates by the Norwegian Institute of Air Research indicate that atmospheric releases of xenon-133 by the Fukushima Daiichi nuclear disaster were broadly similar to those released at Chernobyl, and thus well above the Windscale fire releases.\n\nThe presence of the chimney scrubbers at Windscale was credited with maintaining partial containment and thus minimizing the radioactive content of the smoke that poured from the chimney during the fire. These scrubbers were installed at great expense on the insistence of John Cockcroft and were known as Cockcroft's Folly until the 1957 fire.\n\nOf particular concern at the time was the radioactive isotope iodine-131, which has a half-life of only 8 days but is taken up by the human body and stored in the thyroid. As a result, consumption of iodine-131 often leads to cancer of the thyroid. Estimates of additional cancer cases and mortality resulting from the radiological release have varied.\n\nNo one was evacuated from the surrounding area, but there was concern that milk might be dangerously contaminated. Milk from about 500 km of nearby countryside was destroyed (diluted a thousandfold and dumped in the Irish Sea) for about a month. A 2010 study of workers directly involved in the cleanup—and thus expected to have seen the highest exposure rates—found no significant long term health effects from their involvement.\n\nThe reactor was unsalvageable; where possible, the fuel rods were removed, and the reactor bioshield was sealed and left intact. Approximately 6,700 fire-damaged fuel elements and 1,700 fire-damaged isotope cartridges remain in the pile. The damaged reactor core was still slightly warm as a result of continuing nuclear reactions. In 2000 it was estimated that the core still contained\nas well as smaller activities of other radionuclides. Windscale Pile 2, though undamaged by the fire, was considered too unsafe for continued use. It was shut down shortly afterwards. No air-cooled reactors have been built since. The final removal of fuel from the damaged reactor was scheduled to begin in 2008 and to continue for a further four years.\n\nInspections showed that there had not been a graphite fire, and the damage to the graphite was localised, caused by severely overheated uranium fuel assemblies nearby.\n\nA board of inquiry met under the chairmanship of Sir William Penney from 17 to 25 October 1957. Its report (the \"Penney Report\") was submitted to the Chairman of the United Kingdom Atomic Energy Authority and formed the basis of the Government White Paper submitted to Parliament in November 1957. The report itself was released at the Public Record Office in January 1988. In 1989 a revised transcript was released, following work to improve the transcription of the original recordings.\n\nPenney reported on 26 October 1957, 16 days after the fire was extinguished and reached four conclusions:\n\n\nThose who had been directly involved in the events were heartened by Penney's conclusion that the steps taken had been \"prompt and efficient\" and had \"displayed considerable devotion to duty\". Some considered that the determination and courage shown by Thomas Tuohy, and the critical role he played in the aversion of complete disaster, had not been properly recognised. Tuohy died on 12 March 2008; he had never received any kind of public recognition for his decisive actions. The Board of Inquiry's report concluded officially that the fire had been caused by \"an error of judgment\" by the same people who then risked their lives to contain the blaze. It was later suggested by the grandson of Harold Macmillan, Prime Minister at the time of the fire, that the US Congress might have vetoed plans of Macmillan and US president Dwight Eisenhower for joint nuclear weapons development if they had known that it was due to reckless decisions by the UK government, and that Macmillan had covered up what really happened. Tuohy said of the officials who told the US that his staff had caused the fire that \"they were a shower of bastards\".\n\nThe Windscale site was decontaminated and is still in use. Part of the site was later renamed Sellafield after being transferred to BNFL; the whole site is now owned by the Nuclear Decommissioning Authority.\n\nThe release of radiation by the Windscale fire was greatly exceeded by the Chernobyl disaster in 1986, but the fire has been described as the worst reactor accident until Three Mile Island in 1979. Epidemiological estimates put the number of additional cancers caused by the Three Mile Island accident at not more than one; only Chernobyl produced immediate casualties.\n\nThree Mile Island was a civilian reactor, and Chernobyl primarily so, both being used for electrical power production. In contrast Windscale was for purely military purposes.\n\nThe reactors at Three Mile Island, unlike those at Windscale and Chernobyl, were in buildings designed to contain radioactive materials released by a reactor accident.\n\nOther military reactors have produced immediate, known casualties such as the 1961 incident at the SL-1 plant in Idaho which killed three operators.\n\nThe accident at Windscale was also contemporary to the Kyshtym disaster, a far more serious accident which happened on 29 September 1957 at the Mayak plant in the Soviet Union, when the failure of the cooling system for a tank storing tens of thousands of tons of dissolved nuclear waste resulted in a non-nuclear explosion.\n\nThe Windscale fire was retrospectively graded as level 5, an accident with wider consequences, on the International Nuclear Event Scale.\n\nIn 1968 a paper was submitted to the journal \"Nature\", on a study of radioisotopes found in oysters from the Irish Sea, using gamma spectroscopy. The oysters were found to contain Ce, Ce, Ru, Ru, Cs, Zr and Nb. In addition a zinc activation product (Zn) was found; this is thought to be due to the corrosion of magnox fuel cladding in cooling ponds. A number of harder-to-detect pure alpha and beta decaying radionuclides were also present, such as Sr and Pu, but these do not appear in gamma spectroscopy as they do not generate any appreciable gamma rays as they decay.\n\nIn 1999, the BBC produced an educational documentary film about the fire as a 30-minute episode of \"Disaster\" (Series 3) entitled \"The Windscale Fire\". It subsequently was released on DVD.\n\nIn 2007, the BBC produced another documentary about the accident entitled \"Windscale: Britain’s Biggest Nuclear Disaster\", which investigates the history of the first British nuclear facility and its role in the development of nuclear weapons. The documentary features interviews with key scientists and plant operators, such as Tom Tuohy, who was the deputy general manager of Windscale. The documentary suggests that the fire — the first fire in any nuclear facility — was caused by the relaxation of safety measures, as a result of pressure from the British government to quickly produce fissile materials for nuclear weapons.\n\nThe following substances were placed inside metal cartridges and subjected to neutron irradiation to create radioisotopes. Both the target material and some of the product isotopes are listed below. Of these, the polonium-210 release made the most significant contribution to the collective dose on the general population.\n\n\n\n"}
{"id": "3460433", "url": "https://en.wikipedia.org/wiki?curid=3460433", "title": "Wove paper", "text": "Wove paper\n\nWove paper is a writing paper with a uniform surface, not ribbed or watermarked. \nThe papermaking mould's wires run parallel to each other to produce laid paper, but they are woven together into a fine wire mesh for wove paper. The originator of this new papermaking technique was James Whatman (1702–59) from Kent, England.\n\nFor 500 years European paper makers could only produce what came to be called laid paper. In 1757 John Baskerville printed his famous edition of Virgil on a new kind of paper, called Wove (known in Europe as Vélin). This paper is now known to have been made by the elder James Whatman. Twenty-five years later (1780s) the manufacture of wove paper spread quickly to other paper mills in England, and was also being developed in France and America. All this took place over a decade before a machine to replace making paper by hand was conceived. With the establishment of the paper machine (1807), the manufacture of paper on a wove wire base would become the predominant standard in the world, with laid paper being relegated to certain specialist uses, such as being used as a support for charcoal drawings. Today more than 99% of the world's paper is made in this way.\n\nWhatman paper is a type of wove paper named after James Whatman. It is notable for its exceptional quality.\nWhatman paper is grained, strong and rigid, without laid lines. It is used in publishing, filtering, and chromatography.\n\n"}
