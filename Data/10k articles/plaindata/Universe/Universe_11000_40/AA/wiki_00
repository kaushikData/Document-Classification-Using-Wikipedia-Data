{"id": "28443079", "url": "https://en.wikipedia.org/wiki?curid=28443079", "title": "1913 Great Meteor Procession", "text": "1913 Great Meteor Procession\n\nThe 1913 Great Meteor Procession occurred on February 9, 1913. It was a unique meteoric phenomenon reported from locations across Canada, the northeastern United States, and Bermuda, and from many ships at sea, including eight off Brazil, giving a total recorded ground track of over 7,000 miles (11,000 km). The meteors were particularly unusual in that there was no apparent radiant, that is to say, no point in the sky from which the meteors appeared to originate. The observations were analysed in detail, later the same year, by the astronomer Clarence Chant, leading him to conclude that as all accounts were positioned along a great circle arc, the source had been a small, short-lived natural satellite of the Earth.\n\nJohn A. O'Keefe, who conducted several studies of the event, proposed that the meteors should be referred to as the \"Cyrillids\", in reference to the feast day of Cyril of Alexandria (February 9 in the Roman Catholic calendar from 1882–1969).\n\nThe evening of February 9 was cloudy across much of the densely populated northeast United States, meaning that some 30 million potential observers were for the most part unaware of the phenomenon. Nevertheless, more than a hundred individual reports – largely from more remote areas of Canada – were later collected by Clarence Chant, with additional observations unearthed by later researchers. At around 21hr EST, witnesses were surprised to see a procession of between 40 and 60 bright, slow-moving fireballs moving from horizon to horizon in a practically identical path. Individual fireballs were visible for at least 30 to 40 seconds, and the entire procession took some 5 minutes to cross the sky. An observer at Appin, Ontario, described its appearance at one of the most easterly parts of its track across Canada:\n\nA huge meteor appeared travelling from northwest by west to southeast, which, as it approached, was seen to be in two parts and looked like two bars of flaming material, one following the other. They were throwing out a constant stream of sparks and after they had passed they shot out balls of fire straight ahead that travelled more rapidly than the main bodies. They seemed to pass over slowly and were in sight about five minutes. Immediately after their disappearance in the southeast a ball of clear fire, that looked like a big star, passed across the sky in their wake. This ball did not have a tail or show sparks of any kind. Instead of being yellow like the meteors, it was clear like a star.\n\nSubsequent observers also noted a large, white, tail-less body bringing up the rear, but the various bodies making up the meteor procession continued to disintegrate and to travel at different rates throughout their course, so that by the time observations were made in Bermuda, the leading bodies were described as \"like large arc lights in appearance, slightly violet in colour\", followed closely by yellow and red fragments.\n\nResearch carried out in the 1950s by Alexander D. Mebane uncovered a handful of reports from newspaper archives in the northern United States. At Escanaba, Michigan, the \"Press\" stated the \"end of the world was apprehended by many\" as numerous meteors travelled across the northern horizon. In Batavia, New York, a few observers saw the meteors and many people heard a thundering noise, while other reports were made in Nunda-Dansville, New York (where several residents again thought the world was ending) and Osceola, Pennsylvania.\n\nOne curious feature of the reports, highlighted by Mebane, was that several appeared to indicate a second meteor procession on the same course around 5 hours later, although the Earth's rotation meant that there was no obvious mechanism to explain this. One observer, an A. W. Brown from Thamesville, Ontario, reported seeing both the initial meteor procession and a second one on the same course at 02:20 the next morning. Chant's original report also referred to a series of three groups of \"dark objects\" which passed, on the same course as the previous meteors, from west to east over Toronto on the afternoon of February 10, which he suggested were \"something of a meteoric nature\".\n\nWilliam Henry Pickering noted that at eight stations in Canada a trembling of the house or ground was felt. In many other places loud, thunder-like sounds were heard, occasionally by people who had not seen the meteors themselves. Pickering used the sound reports to perform a check on the height of the meteors, which he calculated at 35 miles (56 km).\n\nThe first detailed study of the reports was produced by the Canadian astronomer Clarence Chant, who wrote about the meteors in vol. 7 of the \"Journal of the Royal Astronomical Society of Canada\". The orbit was later discussed by Pickering and G. J. Burns, who concluded that it was essentially satellitic. Although this explanation was later attacked by Charles Wylie, who attempted to prove that the shower had a radiant, further studies by Lincoln LaPaz (who criticised Wylie's methods as \"unscientific\") and John O'Keefe showed that the meteors had most likely represented a body, or group of bodies, which had been temporarily captured into orbit about the Earth before disintegrating.\n\nO'Keefe later suggested that the meteors, which he referred to as the \"Cyrillids\", could have in fact represented the last remnant of a circumterrestrial ring, formed from the ejecta of a postulated lunar volcano. This theory was a development of O'Keefe's unusual hypothesis on the origin of tektites.\n\n"}
{"id": "1464555", "url": "https://en.wikipedia.org/wiki?curid=1464555", "title": "Air shower (physics)", "text": "Air shower (physics)\n\nAn air shower is an extensive (many kilometres wide) cascade of ionized particles and electromagnetic radiation produced in the atmosphere when a \"primary\" cosmic ray (i.e. one of extraterrestrial origin) enters the atmosphere. When a particle, which could be a proton, a nucleus, an electron, a photon, or (rarely) a positron, strikes an atom's nucleus in the air it produces many energetic hadrons. The unstable hadrons decay in the air speedily into other particles and electromagnetic radiation, which are part of the shower components. The secondary radiation rains down, including x-rays, muons, protons, antiprotons, alpha particles, pions, electrons, positrons, and neutrons.\n\nThe dose from cosmic radiation is largely from muons, neutrons, and electrons, with a dose rate that varies in different parts of the world and based largely on the geomagnetic field, altitude, and solar cycle. Airline crews receive more cosmic rays if they routinely work flight routes that take them close to the North or South pole at high altitudes, where this type of radiation is maximal.\n\nThe air shower was discovered by Bruno Rossi in 1934. By observing the cosmic ray with the detectors placed apart from each other, Rossi recognized that many particles arrive simultaneously at the detectors. This phenomenon is now called an air shower.\n\nAfter the primary cosmic particle has collided with the air molecule, the main part of the first interactions are pions. Also kaons and baryons may be created. Pions and kaons are not stable, thus they may decay into other particles.\n\nThe neutral pions formula_1 decay into photons formula_2 in a process formula_3. The photons produced form an electromagnetic cascade by creating more photons, protons, antiprotons, electrons and positrons.\n\nThe charged pions formula_4 preferentially decay into muons and neutrinos in the processes formula_5 and formula_6. This is how the muons and neutrinos are produced in the air shower.\n\nAlso, kaon may be an origin of muons, which means the decay process is formula_7. In the other hand kaons can produce also pions via the decay mode formula_8.\n\nThe original particle arrives with high energy and hence a velocity near the speed of light, so the products of the collisions tend also to move generally in the same direction as the primary, while to some extent spreading sidewise. In addition, the secondary particles produce a widespread flash of light in forward direction due to the Cherenkov effect, as well as fluorescence light that is emitted isotropically from the excitation of nitrogen molecules. The particle cascade and the light produced in the atmosphere can be detected with surface detector arrays and optical telescopes. Surface detectors typically use Cherenkov detectors or Scintillation counters to detect the charged secondary particles at ground level. The telescopes used to measure the fluorescence and Cherenkov light use large mirrors to focus the light on PMT clusters. Finally, air showers emit radio waves due to the deflection of electrons and positrons by the geomagnetic field. As advantage over the optical techniques, radio detection is possible around the clock and not only during dark and clear nights. Thus, several modern experiments, e.g., TAIGA, LOFAR, or the Pierre Auger Observatory use radio antennas in addition to particle detectors and optical techniques.\n\nThe longitudinal profile of the number of charged particles can be parameterized by the Gaisser–Hillas function.\n\n\n"}
{"id": "11042777", "url": "https://en.wikipedia.org/wiki?curid=11042777", "title": "Airgap (microelectronics)", "text": "Airgap (microelectronics)\n\nAirgap is an invention in microelectronic fabrication by IBM. \n\nBy insulating copper wires within a chip with vacuum holes, capacitance can be minimized enabling chips to work faster or draw less power. A vacuum is believed to be the ultimate insulator for wiring capacitance, which occurs when two adjacent wires on a chip draw electrical energy from one another, generating undesirable heat and slowing the speed at which data can move through a chip. IBM estimates that this technology alone can lead to 35% higher speeds in current flow or 15% lower power consumption.\n\nIBM researchers have figured out a way to manufacture these \"airgaps\" on a massive scale, using the self-assembly properties of certain polymers, and then combine this with regular CMOS manufacturing techniques, saving enormous resources since they don't have to retool the entire process. When making the chips the entire wafer is prepared with a polymer material that when removed at a later stage leaves trillions of holes, just 20 nanometers in diameter, evenly spaced. Even though the name suggests that the holes are filled with air, they are in fact filled with nothing, vacuum. IBM has already proven this technique in their labs, and is already deployed in their manufacturing plant in East Fishkill, New York where they have made prototype POWER6 processors using this technology. Full scale deployment is scheduled for IBM's 45 nm node in 2009 after which this technology will also be available to IBM's customers. \n\nAirgap was developed in a collaborative effort between IBM's Almaden Research Center and T.J. Watson Research Center, and the University of Albany, New York.\n\n"}
{"id": "25187276", "url": "https://en.wikipedia.org/wiki?curid=25187276", "title": "American Threnody", "text": "American Threnody\n\nAmerican Threnody is a 2007 American documentary film, directed by Robert Rex Jackson. It concerns the Maxey Flat Low Level Radioactive Waste facility in eastern Kentucky. The facility was built on the former site of the farm where the filmmaker's grandfather was born. The film examines the impact of the facility on the community and examines the persistent containment problems that have been the subject of media coverage. Current methods of storing and disposing of transuranic isotopes and how they differ from the techniques used at Maxey Flat are also explored.\n\n"}
{"id": "25421323", "url": "https://en.wikipedia.org/wiki?curid=25421323", "title": "Antigen retrieval", "text": "Antigen retrieval\n\nTissues that have been preserved with formaldehyde, a highly reactive compound, contain a variety of chemical modifications that can reduce the detectability of proteins in biomedical procedures such as immunohistochemistry. Antigen retrieval is an approach to reducing or eliminating these chemical modifications. The two primary methods of antigen retrieval are heat-mediated epitope retrieval (HIER) and proteolytic induced epitope retrieval (PIER).\n\nIt's the cleavage of peptides, exposing the antigen; uses enzymes including Proteinase K, Trypsin, and Pepsin.\n\nIt has low success rate for restoring immunoreactivity and potentially destroys both tissue morphology and the antigen of interest.\n\nIt's the reversal of cross-links and restoration of protein structures by heat; method efficiency dependent on time, temperature, buffer and pH. In general HIER allows better antigen retrieval than PIER.\n\nIt's the reversal of cross-links using acids such as hydrochloric acid (pH 1) or formic acid (pH 2).\n"}
{"id": "7324672", "url": "https://en.wikipedia.org/wiki?curid=7324672", "title": "Baltic Gas Interconnector", "text": "Baltic Gas Interconnector\n\nThe Baltic Gas Interconnector was a project of the natural gas submarine pipeline between Germany, Denmark and Sweden. The pipeline would connect the existing pipeline networks of southern Scandinavian and Continental European countries in order to secure uninterrupted supply of natural gas. \n\nIn Germany, landfall of the pipeline was to be in Rostock area. The German onshore section was to include a compressor station and a connection to the existing gas network. The length of planned offshore section was around . The Danish landing point was to be in Avedøre, and the pipeline was planned to connect with the Avedøre power plant. In Sweden, the landing point was to be in Trelleborg and the Swedish onshore section was to continue approximately to the existing gas grid.\n\nThe pipeline was designed for a pressure of with a diameter of . The planned annual capacity was with option for later increase up to .\n\nThe consortium to build the Baltic Gas Interconnector consisted of DONG Energy (originally Energi E2), Hovedstadsregionens Naturgas (HNG), VNG - Verbundnetz Gas AG, E.ON Sverige AB, Göteborgs Energi, Lunds Energi and Öresundskraft.\n\nThe feasibility study which was completed in 2001 included market assessment, seabed survey, offshore and onshore installations estimated total cost to be €225 million ($202.3 million). The pipeline was scheduled to become operational circa 2004–2005. Environmental impact assessment started in 2002. Authorization from Swedish government was given in 2004, by Denmark – in 2005. The last phase of authorization was to come from Germany, in 2006. The project has not been implemented yet.\n\nDuring the initial stages of the project, gas was planned to transported from the North Sea which is now in depletion. Consequently, Russian gas has been considered as an alternative source for the pipeline. However, due to considerations for linking Nord Stream pipeline to Swedish pipeline network, implementation of BGI is being revisited.\n\n"}
{"id": "2122657", "url": "https://en.wikipedia.org/wiki?curid=2122657", "title": "Bhabha Atomic Research Centre", "text": "Bhabha Atomic Research Centre\n\nThe Bhabha Atomic Research Centre (BARC) is India's premier nuclear research facility headquartered in Trombay, Mumbai, Maharashtra. BARC is a multi-disciplinary research centre with extensive infrastructure for advanced research and development covering the entire spectrum of nuclear science, engineering and related areas.\n\nBARC's core mandate is to sustain peaceful applications of nuclear energy, primarily for power generation. It manages all facts of nuclear power generation, from theoretical design of reactors to, computerised modelling and simulation, risk analysis, development and testing of new reactor fuel materials, etc. It also conducts research in spent fuel processing, and safe disposal of nuclear waste. Its other research focus areas are applications for isotopes in industries, medicine, agriculture, etc. BARC operates a number of research reactors across the country.\n\nThe Government of India created the Atomic Energy Establishment, Trombay (AEET) on 3 January 1954. It was established to consolidate all the research and development activity for nuclear reactors and technology under the Atomic Energy Commission. All scientists and engineers engaged in the fields of reactor design and development, instrumentation, metallurgy and material science etc. were transferred with their respective programmes from the Tata Institute of Fundamental Research (TIFR) to AEET, with TIFR retaining its original focus for fundamental research in the sciences. After Homi J. Bhabha's death in 1966, the centre was renamed as the Bhabha Atomic Research Centre on 22 January 1967. All the directors of the BARC were highly qualified doctorates in their discipline and were internationally recognised for their contribution in academia, who were the crown of this prestigious research organisation.\n\nThe first reactors at BARC and its affiliated power generation centres were imported from the west. India's first power reactors, installed at the Tarapur Atomic Power Station were from the United States.\n\nThe primary importance of BARC is as a research centre. The BARC and the Indian government has consistently maintained that the reactors are used for this purpose only: Apsara (1956; named by the then Prime Minister of India, Jawaharlal Nehru when he likened the blue Cerenkov radiation to the beauty of the Apsaras), CIRUS (1960; the \"Canada-India Reactor\" with assistance from the US), the now-defunct ZERLINA (1961; Zero Energy Reactor for Lattice Investigations and Neutron Assay), Purnima I (1972), Purnima II (1984), Dhruva (1985), Purnima III (1990), and KAMINI.\n\nThe plutonium used in India's 1974 Smiling Buddha nuclear test came from CIRUS. The 1974 test (and the 1998 tests that followed) gave Indian scientists the technological know-how and confidence not only to develop nuclear fuel for future reactors to be used in power generation and research, but also the capacity to refine the same fuel into weapons-grade fuel to be used in the development of nuclear weapons. It is one of the world's most important ATOMIC RESEARCH CENTRE. \n\nBARC also designed and built India's first Pressurised water reactor at Kalpakkam, a 80MW land based prototype of INS Arihant's nuclear power unit, as well as the Arihant's propulsion reactor.\n\nIndia is not a part of the Nuclear Non-Proliferation Treaty (NPT), citing concerns that it unfairly favours the established nuclear powers, and provides no provision for complete nuclear disarmament. Indian officials argued that India's refusal to sign the treaty stemmed from its fundamentally discriminatory character; the treaty places restrictions on the non-nuclear weapons states but does little to curb the modernisation and expansion of the nuclear arsenals of the nuclear weapons states.\n\nMore recently, India and the United States signed an agreement to enhance nuclear cooperation between the two countries, and for India to participate in an international consortium on fusion research, ITER (International Thermonuclear Experimental Reactor) so there are signs that the west wants to bring India in the Nuclear mainstream.\n\nThe BARC also conducts research in biotechnology at the Gamma Gardens, and has developed numerous disease resistant and high-yielding crop varieties, particularly groundnuts. It also conducts research in Liquid Metal Magnetohydrodynamics for power generation.\n\nOn 4 June 2005, with the goal of encouraging research in basic sciences, BARC started the Homi Bhabha National Institute. Research institutions affiliated to BARC(Bhabha Atomic Research Centre) include IGCAR (Indira Gandhi Centre for Atomic Research), RRCAT (Raja Ramanna Centre for Advanced Technology), and VECC (Variable Energy Cyclotron Centre).\n\nPower projects that have benefited from BARC expertise but which fall under the NPCIL (Nuclear Power Corporation of India Limited) are KAPP (Kakrapar Atomic Power Project), RAPP (Rajasthan Atomic Power Project), and TAPP (Tarapur Atomic Power Project).\n\nThe Bhabha Atomic Research Centre in addition to its nuclear research mandate, also conducts research in other high technology areas like accelerators, micro electron beams, materials design, supercomputers, and computer vision among the few. The BARC has dedicated departments for these specialised fields. BARC has designed and developed, for its own use an infrastructure of supercomputers, Anupam using state of the art technology.\n\n"}
{"id": "52516401", "url": "https://en.wikipedia.org/wiki?curid=52516401", "title": "Biomes in Brazil", "text": "Biomes in Brazil\n\nAccording to IBGE (2004), Brazil has its territory occupied by six terrestrial biomes and one marine biome.\n\nThe term \"biome\" has several meanings. In a narrow sense (e.g., Whittaker, 1975; Coutinho, 2006), used in literature, it names physio-functionally defined small-scale areas, habitat types or ecosystem types. Although it includes both the plants and the animals and microorganisms of a community, in practice, it is defined by the climate and physiognomy or general appearance of the plants of the community.\n\nIn the broad sense, adopted by Joly et al. (1978) and the IBGE (2016), biome can be understood as a synonym of \"biogeographic province\" (e.g., Rizzini, 1963, Eiten 1977, Cabrera and Willink 1980, the term \"floristic province\" or \"phytogeographic\" is used when considering plant species only), or as an approximate synonym of \"morphoclimatic and phytogeographical domain\" (Ab'Sáber, 1967, 2003).\n\nIn this broad sense, the \"Projeto Radam\" (Veloso et al., 1973) applies the term \"phytoecological region\", and IBGE (2012) adopts the term \"floristic region\". However, the term \"region\" must be understood, in this case, in the generalist sense of \"area\". The terms \"region\" and \"province\" have specific traditional meanings in phytogeography: regions are areas characterized by endemic families, and provinces are areas characterized by endemic genera and species.\n\nIn the case of the 'domains' of Ab'Sáber (1967, 2003), the defined area is characterized by the predominance of certain geomorphological and climatic characteristics, and also by a certain predominant floristic province (vegetative type). However, there is no uniformity: enclaves from other provinces, characteristics of other domains, may occur within this area.\n\nThe Amazon Forest is the largest forest formation on the planet, conditioned by the humid equatorial climate. It is equivalent to 35% of the forest areas of the planet. It has a wide variety of plant formations.\n\nThe Cerrado presents diverse regions, ranging from clean fields devoid of woody vegetation to cerradão, a dense tree formation. Its climate is particularly striking, presenting two well-defined seasons.\n\nThe Atlantic Forest is composed of a series of ecosystems with very different structures and composition of flowers, as well as the climatic characteristics of the region where it occurs, having as a common element the exposure to the humid winds that blow from the ocean.\n\nThe Caatinga has dry soils and its vegetation is formed by palm trees, such as buriti, oiticica, babassu and carnauba. Much of its northeastern part suffers a high risk of desertification due to the degradation of vegetation cover and soil.\n\nThe Pampa is characterized by the amount of herbaceous species and several typologies of the country, composing in some regions, environments integrated with the Araucária forest. The flat plains of the Gaucho plains and plateaus and the soft-wavy reliefs are colonized by pioneering species that form an open savanna vegetation.\n\nThe Pantanal is an alluvial plain influenced by rivers that drain the basin of the Upper Paraguay, where it develops a fauna and flora of rare beauty and abundance. This ecosystem is formed by largely sandy terrains, covered by different physiognomies due to the variety of microregions and flood regimes.\n\nThe Brazilian marine biome is located on the \"Marine Zone of Brazil\", the continental shelf biotope, and presents several ecosystems.\n\nThe Brazilian Coastal Zone has as distinctive aspects in its long extension through different biomes that arrive until the coast, the biome of the Amazônia, the biome of the Caatinga and bioma of the Atlantic Forest. These biomes with wide variety of species and ecosystems, cover more than 8,500 km of coastline.\n\n\n"}
{"id": "3297280", "url": "https://en.wikipedia.org/wiki?curid=3297280", "title": "Bob Hasan", "text": "Bob Hasan\n\nMohamad \"Bob\" Hasan (born 1931) is an Indonesian businessman, former Minister of Trade and Industry, and friend of former president of Indonesia, Suharto.\n\nHasan, was born in Semarang, Central Java, Indonesia in February 1931. He is a legally adopted son of Gatot Subroto, a general in the Indonesian Army, who commanded then-Colonel Suharto in the 1950s, and through whom Hasan met Suharto.\n\nAfter Suharto replaced Subroto as commander of the Army's Diponegoro Division, Hasan worked with Suharto to develop a wide range of side businesses, controlled by the military, that provided much of the funding for the Division as well as extra income for its officers.\n\nAfter Suharto took the presidency in 1966, he initiated a massive expansion of Indonesian commercial logging, especially in the islands outside of Java. In the 1970s Hasan served as the required Indonesian \"partner\" for foreign companies wanting to harvest timber in Indonesia, working most notably with the United States corporation Georgia Pacific, and also established a number of joint ventures between his and government-owned companies. In 1981 the government banned the export of unmilled timber, leading to many foreign companies selling their Indonesian operations to domestic owners interested in establishing processing operations; Hasan, already a major shareholder in Georgia Pacific's Indonesia operation, became its sole owner when the company left Indonesia in 1983. Starting from timber, he expanded his business interests into financial, insurance, automotive, and other industries, primarily through his Kalimanis holding company. Hasan's Kalimanis group was reported to control over 2 million hectares (7,700 square miles) of prime concessions in Kalimantan.\n\nHasan was also Chairman of the Indonesian Wood Panel Association (Apkindo) Under Hasan, Apkindo was given complete control of plywood pricing, marketing, and exports. Apkindo helped Indonesia gain about three-quarters of the worldwide plywood export market by the early 1990s, sometimes using techniques described by observers as \"predatory pricing\". Hasan personally profited from his chairmanship both by supporting business he owned, and through control of the fees paid to the organization by other members.\n\nHasan ran PT Nusantara Ampera Bakti (Nusamba) which is 80%-owned by foundations controlled by Suharto.\n\nHasan was said to become the mediator in disputes between Suharto's six children, after the death of Suharto's wife. He was reported to orchestrate the deal of settling shareholder dispute of the Busang gold deposit found by Bre-X Minerals in Kalimantan.\n\nSuharto appointed Hasan Minister of Trade and Industry in 1998, making him the only Indonesian of Chinese descent ever to join Suharto's cabinet. His appointment was criticised by some foreign financial analysts as evidence that Suharto was not interested in substantial fiscal changes after the Asian financial crisis began in 1997. As a result of International Monetary Fund (IMF) requirements during the crisis, Apkindo was closed down in 1998.\n\nHasan was frequently the subject of allegations of corruption as a result of his business dealings and control of much of Indonesian industry; after Suharto stepped down in 1998, a series of court judgements found evidence of crimes. He was fined 50 billion rupiah (US$7 million) as a result of a lawsuit filed by several youth organisations, alleging that Hasan had ordered the burning of forests in Sumatra. In February 2001, a court unanimously convicted him of causing a US$244 million loss to the Indonesian government through a fraudulent forest-mapping project in Java in the early 1990s, leading to imprisonment. He was imprisoned at Cipinang prison and then at the more secure Nusa Kambangan, an island off the coast of south-central Java, until his release on parole in February 2004. Hasan was the first and among the most prominent of former Suharto associates convicted of fraud and corruption after Suharto resigned in May 1998.\n\nHasan was a member of the International Olympic Committee from 1994 to 2004, when the IOC expelled him due to his corruption conviction. The IOC was criticised by the Indonesian government in 2000 after the IOC argued that Hasan should be allowed to attend the 2000 Olympic Games in Sydney, Australia despite his being under arrest at the time.\n"}
{"id": "849227", "url": "https://en.wikipedia.org/wiki?curid=849227", "title": "Bronze wool", "text": "Bronze wool\n\nBronze wool is a bundle of very fine bronze filaments, used in finishing and repair work to polish wood or metal objects. Bronze wool is similar to steel wool, but is used in its place to avoid some problems associated with broken filaments: steel rusts quickly, especially in a marine environment. Furthermore, steel is magnetic and can affect the operation of marine equipment, such as a compass. Steel can also discolor some materials, such as oak. This discoloration results from a reaction between the tannates in the oak and the iron in the steel, forming iron tannate, a black compound.\n\nBronze wool also has uses for filter elements, again when rusting would be a problem.\n\nThe main US retail supplier of bronze wool is Homax Group, under their Rhodes American brand.\n\nBronze wool has largely been replaced for cost reasons, by plastic mesh abrasives from makers such as Webrax and 3M Scotch-Brite. These use grains of aluminium oxide or silicon carbide, bonded to a non-woven web of nylon fibres. Like bronze wool, they avoid rust problems.\n"}
{"id": "14177702", "url": "https://en.wikipedia.org/wiki?curid=14177702", "title": "Bruno Van Peteghem", "text": "Bruno Van Peteghem\n\nBruno Van Peteghem (born in New Caledonia) was awarded the Goldman Environmental Prize in 2001, for his campaign to place the island's coral reef (among the world's largest and most unusual) on UNESCO's World Heritage List in order to protect the reef against destruction from nickel mining industries.\n"}
{"id": "7997950", "url": "https://en.wikipedia.org/wiki?curid=7997950", "title": "Bulk moulding compound", "text": "Bulk moulding compound\n\nBulk molding compound (BMC) or bulk molding composite is a ready to mold, glass-fiber reinforced thermoset polyester material primarily used in injection moulding and compression moulding. The material is provided in bulk or logs.\n\nBMC is manufactured by mixing strands (>1”) of chopped glass fibers in a mixer with a thermoset resin (commonly polyester resin, vinyl ester resin or epoxy resin). The glass fibers in BMC result in better strength properties than standard thermoplastic products. Typical applications include demanding electrical applications, corrosion resistant needs, appliance, automotive, and transit.\n\nfabric or fibre's are pre-impregneted under pre-catalysed (https://en.wiktionary.org/wiki/precatalyst) resin.The catalyst is largely content at ambient temperature giving the material useful like when difrested.They are nearly solid at ambient temperature so appeared to be adhesive. The prepregs are laid on to the mold by hand.Then vacuum forged and heated to 120-180°c. this allow the resin to flow followed by carrying addition pressure can be applied by autoclave.\n"}
{"id": "4978101", "url": "https://en.wikipedia.org/wiki?curid=4978101", "title": "CANFLEX", "text": "CANFLEX\n\nCANFLEX; the name is derived from its function: CANDU FLEXible fuelling, is an advanced fuel bundle design developed by Atomic Energy of Canada Ltd. (AECL), along with the Korean Atomic Energy Research Institute (KAERI) for use in CANDU design nuclear reactors. The designers claim that it will deliver many benefits to current and future CANDU reactors-using natural uranium or other advanced nuclear fuel cycles. These include greater operating and safety margins, extended plant life, better economics and increased power.\n\nThe CANFLEX bundle has 43 fuel elements, with two element sizes. It is about 10 cm (four inches) in diameter, 0.5 m (20 inches) long and weighs about 20 kg (44 lbs) and replaces 37-pin standard bundle. It has been designed specifically to increase fuel performance by utilizing two different pin diameters. This reduces the power rating of the hottest pins in the bundles, for the same total bundle power output. Also, the design incorporates special geometry modifications that enhance the heat transfer between the fuel and surrounding coolant. Twenty-four of these fuel bundles have been tested in the Point Lepreau CANDU 6 reactor in New Brunswick, Canada, and results indicate CANFLEX meets all expectations and regulatory requirements.\n\nThe Bruce Nuclear Generating Station has recently announced a conversion to CANFLEX fuel for their reactors, in 2006.\n\n\n"}
{"id": "23706537", "url": "https://en.wikipedia.org/wiki?curid=23706537", "title": "Campus Center for Appropriate Technology", "text": "Campus Center for Appropriate Technology\n\nThe Campus Center for Appropriate Technology (CCAT) is a student-run sustainability organization located at Humboldt State University in Arcata, California. CCAT has partnered with Appropedia to bring over 30 years of student-based open-source appropriate technology solutions to the world.\n\nThe organization is housed in a 70-year-old house and contains scores of resource-saving devices such as solar roof panels, a gravity-based rainwater collection system to water the gardens, and an electrical generator which runs on biodiesel made from cooking oil from the school's cafeteria.\n\n"}
{"id": "2263904", "url": "https://en.wikipedia.org/wiki?curid=2263904", "title": "Carbon footprint", "text": "Carbon footprint\n\nA carbon footprint is historically defined as the total emissions caused by an individual, event, organization, or product, expressed as carbon dioxide equivalent.\n\nIn most cases, the total carbon footprint cannot be exactly calculated because of inadequate knowledge of and data about the complex interactions between contributing processes, including the influence of natural processes that store or release carbon dioxide. For this reason, Wright, Kemp, and Williams, have suggested to define the carbon footprint as:\n\nGreenhouse gases (GHGs) can be emitted through land clearance and the production and consumption of food, fuels, manufactured goods, materials, wood, roads, buildings, transportation and other services. For simplicity of reporting, it is often expressed in terms of the amount of carbon dioxide, or its equivalent of other GHGs, emitted.\n\nMost of the carbon footprint emissions for the average U.S. household come from \"indirect\" sources, e.g. fuel burned to produce goods far away from the final consumer. These are distinguished from emissions which come from burning fuel directly in one's car or stove, commonly referred to as \"direct\" sources of the consumer's carbon footprint. \nThe concept name of the carbon footprint originates from ecological footprint, discussion, which was developed by William E. Rees and Mathis Wackernagel in the 1990s. This accounting approach compares how much people demand compared to what the planet can renew. This allows to assess the number of \"earths\" that would be required if everyone on the planet consumed resources at the same level as the person calculating their ecological footprint. The carbon Footprint is one part of the ecological footprint. The carbon part was popularized by a large campaign of BP in 2005. In 2007, carbon footprint was used as a measure of carbon emissions to develop the energy plan for City of Lynnwood, Washington. Carbon footprints are more focused than ecological footprints since they measure merely emissions of gases that cause climate change into the atmosphere.\n\nCarbon footprint is one of a family of footprint indicators, which also includes water footprint and land footprint.\n\nAn individual's, nation's, or organization's carbon footprint can be measured by undertaking a GHG emissions assessment or other calculative activities denoted as carbon accounting. Once the size of a carbon footprint is known, a strategy can be devised to reduce it, e.g. by technological developments, better process and product management, changed Green Public or Private Procurement (GPP), carbon capture, consumption strategies, carbon offsetting and others.\n\nSeveral free online carbon footprint calculators exist, including a few supported by publicly available peer-reviewed data and calculations including the University of California, Berkeley's CoolClimate Network research consortium and CarbonStory. These websites ask you to answer more or less detailed questions about your diet, transportation choices, home size, shopping and recreational activities, usage of electricity, heating, and heavy appliances such as dryers and refrigerators, and so on. The website then estimates your carbon footprint based on your answers to these questions. A systematic literature review was conducted to objectively determine the best way to calculate individual/household carbon footprints. This review identified 13 calculation principles and subsequently used the same principles to evaluate the 15 most popular online carbon footprint calculators. A recent study’s results by Carnegie Mellon's Christopher Weber found that the calculation of carbon footprints for products is often filled with large uncertainties. The variables of owning electronic goods such as the production, shipment, and previous technology used to make that product, can make it difficult to create an accurate carbon footprint. It is important to question, and address the accuracy of Carbon Footprint techniques, especially due to its overwhelming popularity.\n\nCarbon Footprints can be reduced through the development of alternative projects, such as solar and wind energy, which are environment friendly, renewable resources, or reforestation, the restocking of existing forests or woodlands that have previously been depleted. These examples are known as Carbon Offsetting, the counteracting of carbon dioxide emissions with an equivalent reduction of carbon dioxide in the atmosphere.\nThe main influences on carbon footprints include population, economic output, and energy and carbon intensity of the economy. These factors are the main targets of individuals and businesses in order to decrease carbon footprints. Production creates a large carbon footprint, scholars suggest that decreasing the amount of energy needed for production would be one of the most effective ways to decrease a carbon footprint. This is due to the fact that Electricity is responsible for roughly 37% of Carbon Dioxide emissions. Coal production has been refined to greatly reduce carbon emissions; since the 1980s, the amount of energy used to produce a ton of steel has decreased by 50%.\n\nThe global average carbon footprint in 2007 was around 5.7 tons COe/cap. The EU average for this time was about 13.8 tons COe/cap, whereas for the U.S., Luxembourg and Australia it was over 25 tons COe/cap. The footprints per capita of countries in Africa and India were well below average. To set this numbers into context, assuming a global population around 9-10 billion by 2050 a carbon footprint of about 2 - 2.5 tons COe per capita is needed to stay within a 2 °C target. The carbon footprint calculations are based on a consumption based approach using a Multi-Regional Input-Output database, which accounts for all Greenhouse Gas (GHG) emissions in the global supply chain and allocates them to the final consumer of the purchased commodities. GHG emissions related to land use cover change are not included.\n\nMobility (driving, flying & small amount from public transit), shelter (electricity, heating, construction) and food are the most important consumption categories determining the carbon footprint of a person. In the EU, the carbon footprint of mobility is evenly split between direct emissions (e.g. from driving private cars) and emissions embodied in purchased products related to mobility (air transport service, emissions occurring during the production of cars and during the extraction of fuel).\n\nThe carbon footprint of U.S. households is about 5 times greater than the global average. For most U.S. households the single most important action to reduce their carbon footprint is driving less or switching to a more efficient vehicle.\n\nThe following table compares, from peer-reviewed studies of full life cycle emissions and from various other studies, the carbon footprint of various forms of energy generation: nuclear, hydro, coal, gas, solar cell, peat and wind generation technology.\n\nNote: 3.6 MJ = megajoule(s) == 1 kW·h = kilowatt-hour(s), thus 1 g/MJ = 3.6 g/kW·h.<br>Legend: B = Black coal (supercritical)–(new subcritical), Br = Brown coal (new subcritical), cc = combined cycle, oc = open cycle, T = low-temperature/closed-circuit (geothermal doublet), T = high-temperature/open-circuit, W = Light Water Reactors, W = Heavy Water Reactors, #Educated estimate.\n\nThese three studies thus concluded that hydroelectric, wind, and nuclear power produced the least CO per kilowatt-hour of any other electricity sources.\nThese figures do not allow for emissions due to accidents or terrorism. Wind power and solar power, emit no carbon from the operation, but do leave a footprint during construction phase and maintenance during operation. Hydropower from reservoirs also has large footprints from initial removal of vegetation and ongoing methane (stream detritus decays anaerobically to methane in bottom of reservoir, rather than aerobically to CO if it had stayed in an unrestricted stream).\n\nThe table above gives the carbon footprint per kilowatt-hour of electricity generated, which is about half the world's man-made CO output. The CO footprint for heat is equally significant and research shows that using waste heat from power generation in combined heat and power district heating, chp/dh has the lowest carbon footprint, much lower than micro-power or heat pumps.\n\nThis section gives representative figures for the carbon footprint of the fuel burned by different transport types (not including the carbon footprints of the vehicles or related infrastructure themselves). The precise figures vary according to a wide range of factors.\n\nSome representative figures for CO emissions are provided by LIPASTO's survey of average direct emissions (not accounting for high-altitude radiative effects) of airliners expressed as CO and CO equivalent per passenger kilometre:\n\n\nHowever, emissions per unit distance traveled is not necessarily the best indicator for the carbon footprint of air travel, because the distances covered are commonly longer than by other modes of travel. It is the total emissions for a trip that matters for a carbon footprint, not the merely rate of emissions. For example, a greatly more distant holiday destination may be chosen than if another mode of travel were used, because air travel makes the longer distance feasible in the limited time available.\n\nCO emissions per passenger kilometre (pkm) for all road travel for 2011 in Europe as provided by the European Environment Agency:\n\n\nFor vehicles, average figures for CO emissions per kilometer for road travel for 2013 in Europe, normalized to the NEDC test cycle, are provided by the International Council on Clean Transportation:\n\n\nAverage figures for the United States are provided by the US Environmental Protection Agency, based on the EPA Federal Test Procedure, for the following categories:\n\n\nIn 2005, the US company Amtrak's carbon dioxide equivalent emissions per passenger kilometre were 0.116 kg, about twice as high as the UK rail average (where much more of the system is electrified), and about eight times a Finnish electric intercity train.\n\nAverage carbon dioxide emissions by ferries per passenger-kilometre seem to be . However, 18-knot ferries between Finland and Sweden produce of CO, with total emissions equalling a CO equivalent of , while 24–27-knot ferries between Finland and Estonia produce of CO with total emissions equalling a CO equivalent of .\n\nSeveral organizations offer footprint calculators for public and corporate use, and several organizations have calculated carbon footprints of products. The US Environmental Protection Agency has addressed paper, plastic (candy wrappers), glass, cans, computers, carpet and tires. Australia has addressed lumber and other building materials. Academics in Australia, Korea and the US have addressed paved roads. Companies, nonprofits and academics have addressed mailing letters and packages. Carnegie Mellon University has estimated the CO footprints of 46 large sectors of the economy in each of eight countries. Carnegie Mellon, Sweden and the Carbon Trust have addressed foods at home and in restaurants.\nThe Carbon Trust has worked with UK manufacturers on foods, shirts and detergents, introducing a CO label in March 2007. The label is intended to comply with a new British Publicly Available Specification (i.e. not a standard), PAS 2050, and is being actively piloted by The Carbon Trust and various industrial partners. As of August 2012 The Carbon Trust state they have measured 27,000 certifiable product carbon footprints.\nEvaluating the package of some products is key to figuring out the carbon footprint. The key way to determine a carbon footprint is to look at the materials used to make the item. For example, a juice carton is made of an aseptic carton, a beer can is made of aluminum, and some water bottles either made of glass or plastic. The larger the size, the larger the footprint will be.\n\nIn a 2014 study by Scarborough et al., the real-life diets of British people were surveyed and their dietary greenhouse gas footprints estimated. Average dietary greenhouse-gas emissions per day (in kilograms of carbon dioxide equivalent) were:\n\n\nThe precise carbon footprint of different textiles varies considerably according to a wide range of factors. However, studies of textile production in Europe suggest the following carbon dioxide equivalent emissions footprints per kilo of textile at the point of purchase by a consumer:\n\n\nAccounting for durability and energy required to wash and dry textile products, synthetic fabrics generally have a substantially lower carbon footprint than natural ones.\n\nThe carbon footprint of materials (also known as embodied carbon) varies widely. The carbon footprint of many common materials can be found in the Inventory of Carbon & Energy database, the GREET databases and models, and LCA databases via openLCA Nexus\n\nCement production and carbon footprint resulting from soil sealing was 8.0 Mg person of total per capita CO emissions (Italy, year 2003); the balance between C loss due to soil sealing and C stocked in man-made infrastructures resulted in a net loss to the atmosphere, -0.6 Mg C ha y.\n\nCarbon dioxide emissions into the atmosphere, and the emissions of other GHGs, are often associated with the burning of fossil fuels, like natural gas, crude oil and coal. While this is harmful to the environment, carbon offsets can be purchased in an attempt to make up for these harmful effects.\n\nThe Kyoto Protocol defines legally binding targets and timetables for cutting the GHG emissions of industrialized countries that ratified the Kyoto Protocol. Accordingly, from an economic or market perspective, one has to distinguish between a \"mandatory market\" and a \"voluntary market\". Typical for both markets is the trade with emission certificates:\n\n\nTo reach the goals defined in the Kyoto Protocol, with the least economical costs, the following flexible mechanisms were introduced for the mandatory market:\n\n\nThe CDM and JI mechanisms requirements for projects which create a supply of emission reduction instruments, while Emissions Trading allows those instruments to be sold on international markets.\n\n- Projects which are compliant with the requirements of the CDM mechanism generate Certified Emissions Reductions (CERs). - Projects which are compliant with the requirements of the JI mechanism generate Emission Reduction Units (ERUs).\n\nThe CERs and ERUs can then be sold through Emissions Trading. The demand for the CERs and ERUs being traded is driven by:\n\n- Shortfalls in national emission reduction obligations under the Kyoto Protocol.\n- Shortfalls amongst entities obligated under local emissions reduction schemes.\n\nNations which have failed to deliver their Kyoto emissions reductions obligations can enter Emissions Trading to purchase CERs and ERUs to cover their treaty shortfalls. Nations and groups of nations can also create local emission reduction schemes which place mandatory carbon dioxide emission targets on entities within their national boundaries. If the rules of a scheme allow, the obligated entities may be able to cover all or some of any reduction shortfalls by purchasing CERs and ERUs through Emissions Trading. While local emissions reduction schemes have no status under the Kyoto Protocol itself, they play a prominent role in creating the demand for CERs and ERUs, stimulating Emissions Trading and setting a market price for emissions.\n\nA well-known mandatory local emissions trading scheme is the EU Emissions Trading Scheme (EU ETS).\n\nNew changes are being made to the trading schemes. The EU Emissions Trading Scheme is set to make some new changes within the next year. The new changes will target the emissions produced by flight travel in and out of the European Union.\n\nOther nations are scheduled to start participating in Emissions Trading Schemes within the next few year. These nations include China, India and the United States.\n\nIn contrast to the strict rules set out for the mandatory market, the voluntary market provides companies with different options to acquire emissions reductions. A solution, comparable with those developed for the mandatory market, has been developed for the voluntary market, the Verified Emission Reductions (VER). This measure has the great advantage that the projects/activities are managed according to the quality standards set out for CDM/JI projects but the certificates provided are not registered by the governments of the host countries or the Executive Board of the UNO. As such, high quality VERs can be acquired at lower costs for the same project quality. However, at present VERs can not be used in the mandatory market.\n\nThe voluntary market in North America is divided between members of the Chicago Climate Exchange and the Over The Counter (OTC) market. The Chicago Climate Exchange is a voluntary yet legally binding cap-and-trade emission scheme whereby members commit to the capped emission reductions and must purchase allowances from other members or offset excess emissions. The OTC market does not involve a legally binding scheme and a wide array of buyers from the public and private spheres, as well as special events that want to go carbon neutral. Being carbon neutral refers to achieving net zero carbon emissions by balancing a measured amount of carbon released with an equivalent amount sequestered or offset, or buying enough carbon credits to make up the difference.\n\nThere are project developers, wholesalers, brokers, and retailers, as well as carbon funds, in the voluntary market. Some businesses and nonprofits in the voluntary market encompass more than just one of the activities listed above. A report by Ecosystem Marketplace shows that carbon offset prices increase as it moves along the supply chain—from project developer to retailer.\n\nWhile some mandatory emission reduction schemes exclude forest projects, these projects flourish in the voluntary markets. A major criticism concerns the imprecise nature of GHG sequestration quantification methodologies for forestry projects. However, others note the community co-benefits that forestry projects foster.\nProject types in the voluntary market range from avoided deforestation, afforestation/reforestation, industrial gas sequestration, increased energy efficiency, fuel switching, methane capture from coal plants and livestock, and even renewable energy. Renewable Energy Certificates (RECs) sold on the voluntary market are quite controversial due to additionality concerns. Industrial Gas projects receive criticism because such projects only apply to large industrial plants that already have high fixed costs. Siphoning off industrial gas for sequestration is considered picking the low hanging fruit; which is why credits generated from industrial gas projects are the cheapest in the voluntary market.\n\nThe size and activity of the voluntary carbon market is difficult to measure. The most comprehensive report on the voluntary carbon markets to date was released by Ecosystem Marketplace and New Carbon Finance in July 2007.\n\nÆON of Japan is firstly approved by Japanese authority to indicate carbon footprint on three private brand goods in October 2009.\n\nThe most common way to reduce the carbon footprint of humans is to Reduce, Reuse, Recycle, Refuse. In manufacturing this can be done by recycling the packing materials, by selling the obsolete inventory of one industry to the industry who is looking to buy unused items at lesser price to become competitive. Nothing should be disposed off into the soil, all the ferrous materials which are prone to degrade or oxidize with time should be sold as early as possible at reduced price.\n\nThis can also be done by using reusable items such as thermoses for daily coffee or plastic containers for water and other cold beverages rather than disposable ones. If that option isn't available, it is best to properly recycle the disposable items after use. When one household recycles at least half of their household waste, they can save 1.2 tons of carbon dioxide annually.\n\nAnother easy option is to drive less. By walking or biking to the destination rather than driving, not only is a person going to save money on gas, but they will be burning less fuel and releasing fewer emissions into the atmosphere. However, if walking is not an option, one can look into carpooling or mass transportation options in their area.\n\nYet another option for reducing the carbon footprint of humans is to use less air conditioning and heating in the home. By adding insulation to the walls and attic of one's home, and installing weather stripping or caulking around doors and windows one can lower their heating costs more than 25 percent. Similarly, one can very inexpensively upgrade the \"insulation\" (clothing) worn by residents of the home. For example, it's estimated that wearing a base layer of long underwear (top and bottom) made from a lightweight, super insulating fabric like microfleece (\"aka\" Polartec®, Capilene®) can conserve as much body heat as a full set of clothing, allowing a person to remain warm with the thermostat lowered by over 5 °C. These measures all help because they reduce the amount of energy needed to heat and cool the house. One can also turn down the heat while sleeping at night or away during the day, and keep temperatures moderate at all times. Setting the thermostat just 2 degrees lower in winter and higher in summer could save about 1 ton of carbon dioxide each year.\n\nChoice of diet is a major influence on a person's carbon footprint. Animal sources of protein (especially red meat), rice (typically produced in high methane-emitting paddies), foods transported long distance and/or via fuel-inefficient transport (e.g., highly perishable produce flown long distance) and heavily processed and packaged foods are among the major contributors to a high carbon diet. Scientists at the University of Chicago have estimated \"that the average American diet – which derives 28% of its calories from animal foods – is responsible for approximately one and a half more tonnes of greenhouse gasses – as equivalents – per person, per year than a fully plant-based, or vegan, diet.\" Their calculations suggest that even replacing one third of the animal protein in the average American's diet with plant protein (e.g., beans, grains) can reduce the diet's carbon footprint by half a tonne. Exchanging two thirds of the animal protein with plant protein is roughly equivalent to switching from a Toyota Camry to a Prius. Finally, throwing food out not only adds its associated carbon emissions to a person or household's footprint, it adds the emissions of transporting the wasted food to the garbage dump and the emissions of food decomposition, mostly in the form of the highly potent greenhouse gas, methane.\n\nThe carbon handprint movement emphasizes individual forms of carbon offsetting, like using more public transportation or planting trees in deforested regions, to reduce one's carbon footprint and increase their \"handprint.\"\n\nFurthermore, the carbon footprint in the food industry can be reduced by optimizing the supply chain. A life cycle or supply chain carbon footprint study can provide useful data which will help the business to identify critical areas for improvement and provides a focus. Such studies also demonstrate a company’s commitment to reducing carbon footprint now ahead of other competitors as well as preparing companies for potential regulation. In addition to increased market advantage and differentiation eco-efficiency can also help to reduce costs where alternative energy systems are implemented.\n\nA July 2017 study published in \"Environmental Research Letters\" argued that the most significant way individuals could mitigate their own carbon footprint is to have fewer children, followed by living without a vehicle, forgoing air travel and adopting a plant-based diet.\n\n"}
{"id": "6948685", "url": "https://en.wikipedia.org/wiki?curid=6948685", "title": "Carpenter (theatre)", "text": "Carpenter (theatre)\n\nIn theatre, a carpenter is a stagehand who builds sets and stage elements. They usually are hired by the production manager, crew chief or technical director and in some less common cases they may be hired by director or producer. They are usually paid by the hour.\n\nCarpenters receive drafting from the technical director who uses the designers' renderings, models, and/or drafting of the set to create the technical drawings for the production. Working mainly with woods and metals, they use techniques that include woodworking and welding. They build set pieces, including some standard elements—flats, platforms and columns—as well as pieces of the stage. For example, a carpenter may be responsible for building stairs and ramps on and off of the performance area and for leveling the stage floor itself.\n\nOnly carpenters trained as riggers are trusted to do rigging (see fly crew). Often union houses and some larger theatres make distinctions between carpenters and riggers, but most smaller theatres do not, due to staffing limitations. \n\nProfessional carpenters do not work on anything with an electrical component (see electrician). They also do not paint the set, as this is the job of a scenic artist.\n\nOften, stage carpentry for a large production is organized with one \"master carpenter\" or \"shop foreman\" and many subordinate carpenters.\n\n"}
{"id": "9530273", "url": "https://en.wikipedia.org/wiki?curid=9530273", "title": "Carpenter pencil", "text": "Carpenter pencil\n\nA carpenter pencil (carpentry pencil, carpenter's pencil) is a pencil that has a body with a rectangular or elliptical cross-section to prevent it from rolling away. Carpenter pencils are easier to grip than standard pencils, because they have a larger surface area. The non-round core allows thick or thin lines to be drawn by rotating the pencil. Thin lines are required for high precision markings and are easy to erase, but thick markings are needed to mark on rough surfaces. The lead is strong to withstand the stress of marking on such surfaces. The pencil is robust to survive in a construction environment, for example when placed in a bag together with heavy tools. The core is often stronger than in other pencils. Carpenter pencils are also used by builders, because they are suitable for marking on rough surfaces, such as concrete or stone. This shape and lead density aid in marking legible lines with a straight edge that are clear and easy to follow with a saw blade.\n\nCarpenter pencils are typically manually sharpened with a knife, since sharpeners for round pencils do not work. \n\nNotching the middle of the lead with the corner of a file makes it possible to draw two parallel lines at once.\n\nThe flat pencil is one of the oldest pencil types. The first versions were made by hollowing out sticks of juniper wood. A superior technique was discovered: two wooden halves were carved with a groove running down them, a plumbago stick placed in one of the grooves, and the two halves then glued together—essentially the same method in use to this day.\n\nSimilar pencils (called 'jumbo pencils') are sometimes used by children. A pencil that is designed for a child rather than a carpenter would have a softer core, enabling the user to draw with less physical effort. Carpenter pencils are sometimes used by artists and designers to draw a thick line easily when needed. For instance, Old English letters are easier to draw with a carpenter pencil than with an ordinary pen.\n\n"}
{"id": "31293272", "url": "https://en.wikipedia.org/wiki?curid=31293272", "title": "Charged-device model", "text": "Charged-device model\n\nThe charged-device model (CDM) is a model for characterizing the susceptibility of an electronic device to damage from electrostatic discharge (ESD). The model is an alternative to the human-body model (HBM). \n\nDevices that are classified according to CDM are exposed to a charge at a standardized voltage level, and then tested for survival. If it withstands this voltage level, it is tested at the next level and so on, until the device fails. \n\nCDM is standardized as ANSI/ESDA/JEDEC joint standard JS-002.\n\n\n"}
{"id": "11394844", "url": "https://en.wikipedia.org/wiki?curid=11394844", "title": "Diapalma", "text": "Diapalma\n\nIn pharmacology, diapalma (from Lat \"dia\", \"made of\" + \"palma\", \"palm\") is a desiccating or drying plaster, named for the wood of the palm tree, from which the spatula is made that is used to stir the mixture while boiling. It was formerly composed of common oil, hogs-fat, and litharge of gold; or also of palm oil, litharge, and zinc sulfate. Now, it is made of white wax, emplastrum simplex, and sulfate of zinc.\n"}
{"id": "30509934", "url": "https://en.wikipedia.org/wiki?curid=30509934", "title": "Ecological fitting", "text": "Ecological fitting\n\nEcological fitting is \"the process whereby organisms colonize and persist in novel environments, use novel resources or form novel associations with other species as a result of the suites of traits that they carry at the time they encounter the novel condition.\" \nIt can be understood as a situation in which a species' interactions with its biotic and abiotic environment seem to indicate a history of coevolution, when in actuality the relevant traits evolved in response to a different set of biotic and abiotic conditions. The simplest form of ecological fitting is resource tracking, in which an organism continues to exploit the same resources, but in a new host or environment. In this framework, the organism occupies a multidimensional operative environment defined by the conditions in which it can persist, similar to the idea of the Hutchinsonian niche. In this case, a species can colonize new environments (e.g. an area with the same temperature and water regime) and/or form new species interactions (e.g. a parasite infecting a new host) which can lead to the misinterpretation of the relationship as coevolution, although the organism has not evolved and is continuing to exploit the same resources it always has. The more strict definition of ecological fitting requires that a species encounter an environment or host outside of its original operative environment and obtain realized fitness based on traits developed in previous environments that are now co-opted for a new purpose. This strict form of ecological fitting can also be expressed either as colonization of new habitat or the formation of new species interactions.\n\nThe evolutionary ecologist Dr. Daniel Janzen began to explicate the idea of ecological fitting with a 1980 paper that observed that many instances of ecological interactions were inferred to be the result of coevolution when this was not necessarily the case, and encouraged ecologists to use the term coevolution more strictly. He observed that the existing defense traits of plants were likely produced by co-evolution with herbivores or parasites that no longer co-occurred with the plants, but that these traits were continuing to protect the plants against new attacks.\nHe expanded this idea in a 1985 paper written while visiting Santa Rosa National Park in Costa Rica. While there, he observed that almost all of the species in the park occupied large geographic ranges, and despite the heterogeneity of habitats across these ranges, individuals were mostly identical across locations, indicating that little local adaptation had taken place. He described the cyclical life history pattern he believed responsible for this pattern: a species begins as a small population occupying a small area with little genetic variation, but then over the course of a few generations grows to occupy a large area, either because of the emergence of a genotype successful over a wider range, or because of the removal of a geographic barrier. This large interconnected population is now subject to many contradictory selection pressures and thus remains evolutionarily static until a disturbance separates populations, restarting the cycle. This cyclic life history pattern is dependent on three premises: that the ancestral range of most species is smaller than the ones now occupied, that biological communities have porous borders and are thus subject to invasion, and that species possess robust genotypes that allow them to colonize new habitats without evolution. Thus, many biological communities may be made up of organisms that despite their complex biological interactions have very little evolutionary history with each other.\n\nEcological fitting represents a contrasting view to, and null hypothesis for, the hypothesis that current species interactions are evidence of coevolution. Coevolution occurs when each species in a relationship imposes evolutionary selection on the other(s). Examples could include mutualisms or predator-prey systems. The traditional view of plant-insect, host-parasite, and other tightly associated species, explained by Ehrlich & Raven (1964) defines coevolution as the primary mechanism for these associations In his 1980 paper, Janzen gives a response to these adaptationist explanations of why a phenotype or species might exist in a particular environment, and expressed his concern with what he perceived as an overuse of coevolutionary explanations for current species associations. He stated that it would be difficult to distinguish between coevolution and ecological fitting, leading ecologists to potentially spurious explanations of current species associations. It is difficult to determine whether a close relationship is the result of coevolution or of ecological fitting because ecological fitting is a sorting process in which only associations that 'fit', or increase fitness (biology), will be maintained. When trying to determine which process is at work in a particular interaction, it is important to remember that species can only come into contact through biotic expansion and ecological fitting, followed by adaptation or coevolution. Thus, both processes are important in shaping interactions and communities.\n\nEcological fitting can occur by a variety of mechanisms, and can help to explain some ecological phenomena. Resource tracking can help to explain the parasite paradox: that parasites are specialists with narrow environmental ranges, which would encourage host fidelity, yet scientists commonly observe parasite shifts onto novel hosts, both in the phylogenetic record and in ecological time. Ecological fitting can explain the frequency of this phenomenon: similar to the expansion phase of the cyclic life cycle described by Janzen, a species undergoes taxon pulses, usually in a time of ecological disturbance, and expands its range, disperses, and colonizes new areas. For parasite-host, insect-plant, or plant-pollinator associations, this colonization is facilitated by the organism tracking an ancestral resource, and not tracking a particular species. The probability of this is increased when the tracked resource is widespread, or when specialization on a certain resource is a shared trait among distantly related species. This resource tracking has been demonstrated for both insect-plant and parasite-host systems in which sister species are capable of surviving on each other's hosts, even if they were never associated in nature.\n\nWhen operating under the more strict definition of ecological fitting, in which traits must be exapted for a new purpose, several mechanisms could be operating. Phenotypic plasticity, in which an organism changes phenotype in response to environmental variables, allows for individuals with existing genotypes to obtain fitness in novel conditions without adaptation occurring. Correlated trait evolution can encourage ecological fitting when direct selection on one trait causes a correlated change in another, potentially creating a phenotype that is pre-adapted to possible future conditions. Phylogenetic conservatism is the latent retention of genetic changes from past conditions: for instance, historical exposure to a certain host may predispose it to colonization in the future. Finally, fixed traits such as body size may lead to entirely different biotic interactions in different environments, e.g. pollinators visiting different sets of flowers.\n\nStudies of introduced species can provide some of the best evidence for ecological fitting, because species invasions represent natural experiments testing how a new species fits into a community. Invasion ecology teaches us that changes in geographic range can occur quickly, as is required by the Janzen model for ecological fitting, and ecological fitting provides an important mechanism whereby new species can fit into an existing community without adaptation. These natural experiments have often shown that communities dominated by invasive species, such as those on Ascension Island, can be as diverse and complex as native communities. Additionally, phylogenetic studies show evidence for ecological fitting when lineages of the associated species do not correlate over evolutionary time; that is, if host-parasite or other interactions are as tightly coevolved as was previously believed, parasites should not be switching to unrelated hosts. This kind of host switching has been shown many times: in insect-plant relationships where oligophagy in locusts manifests itself on distantly related plants, plant-disperser relationships among Mediterranean birds, plant-pollinator relationships between hummingbirds and Heliconia flowers, and for parasite-host associations ranging from flatworms in frogs to parasitic worms in primates or in trout. Another study examined the time required for sugarcane, \"Saccharum officinarum\", to accumulate diverse arthropod pest communities. It determined that time did not influence pest species richness, indicating that host-parasite associations were forming in ecological, not evolutionary, time.\n\nThe human-made cloud forest on Green Mountain, Ascension Island represents an example of how unrelated and unassociated plant species can form a functioning ecosystem without a shared evolutionary history. 19th-century accounts of the island, including that of Charles Darwin on his expedition aboard the Beagle, described the rocky island as destitute and bare. Plants were brought to the island by colonists, but the most important change occurred in 1843 with the terraforming of Green Mountain by botanist Joseph Dalton Hooker, who recommended planting trees on Green Mountain and vegetation on the slopes to encourage deeper soils. Plants were regularly sent from England until, in the 1920s, the mountain was green and verdant, and could be described as a functioning cloud forest. Although some of the species likely were introduced together because of their coevolutionary relationships, the overwhelming mechanism governing relationships is clearly ecological fitting. The system has changed dramatically and even provides ecosystem services such as carbon sequestration, all as a result of ecological fitting. This is important in the light of climate change for two reasons: species ranges may be shifting dramatically, and ecological fitting is an important mechanism for the construction of communities over ecological time, and it shows that human-made systems could be integral in the mitigation of climate change.\n\nEcological fitting can influence species diversity either by promoting diversification through genetic drift, or by maintaining evolutionary stasis through gene flow. Research has shown that ecological fitting can result in parasite assemblages that are just as diverse as those created over evolutionary time, indicating the importance of ecological factors for biodiversity. Ecological fitting can contribute to 3 types of evolutionary transition. The first is simple ecological fitting, in which organisms track resources to form novel species interactions and increase individual fitness. The second is a shift from an organism's ancestral ecology to a derived ecology, or a more true form of ecological fitting: traits are exapted from their original purpose to increase fitness. Finally, a more dramatic form involves the creation of new evolutionary arenas, requiring morphological or ecological changes to gain fitness under new conditions. Any of these processes can promote speciation or diversification under the right circumstances. Each form of ecological fitting can encourage speciation only if the population is sufficiently isolated from other populations to prevent gene flow from swamping local adaptation to newly formed species associations. Host-plant or other specialized relationships have been previously regarded as an evolutionary 'dead-end' because they seem to limit diversity, but they can actually promote it according to coevolutionary theory. Insects that feed on plants induce them to develop new defense mechanisms, which frees them from herbivory. In this new adaptive zone, or ecospace, plant clades can undergo evolutionary radiation, in which diversification of the clade occurs quickly due to adaptive change. The herbivorous insects may eventually succeed in adapting to the plants' defenses, and would also be capable of diversifying, in the absence of competition by other herbivorous insects. Thus, species associations can lead to rapid diversification of both lineages and contribute to overall community diversity.\n\nEcological fitting can also maintain populations in stasis, influencing diversity by limiting it. If populations are well connected through gene flow, local adaptation may not be able to occur (known as antagonistic gene flow), or the well-connected population could evolve as a whole without speciation occurring. The Geographic Mosaic of Coevolution theory can help to explain this: it suggests that coevolution or speciation of a species occurs across a wide geographic scale, rather than at the level of populations, so that populations experiencing selection for a particular trait affect gene frequencies across the geographic region due to gene flow. Populations of a species interact with different species in different parts of its range, so populations may be experiencing a small sub-set of the interactions to which the species as a whole is adapted. This is based on three premises: there is an environmental and biotic interaction mosaic affecting fitness in different areas, there are certain areas where species are more coevolved than others, and that there is mixing of allele frequencies and traits between the regions to create more homogeneous populations. Thus, depending on connectivity of populations and strength of selection pressure in different arenas, a widespread population can coevolve with another species, or individual populations can specialize, potentially resulting in diversification.\n\nEcological fitting can explain aspects of species associations and community assembly, as well as invasion ecology. It is another mechanism, in addition to coevolution and in situ evolution (in which new phenotypes evolve and travel sympatrically), that can explain the creation and maintenance of species associations within a community. The phenomenon of ecological fitting helps to weigh in on some of the great debates in community ecology. The Clementisian school of community ecology, based on the work of Frederic Clements, a plant ecologist who studied ecological succession, holds that communities are constructed by deterministic processes that assemble a 'superorganism' from the individual species present. With the removal or exchange of a species, the community would be unstable. On the other hand, the Gleasonian view, promoted by Henry Gleason, who was also a plant ecologist studying successional communities, is more individualistic and emphasizes the role of random processes such as dispersal in community assembly. The Clementsian view would emphasize coevolution and strict niche fidelity as a major factor structuring communities, also known as the niche-assembly perspective, whereas the Gleasonian, or dispersal assembly view emphasizes neutral and historical processes, including ecological fitting. These views of community assembly raise the question: do species continue stable relationships over time, or do all individuals represent \"asymmetrical pegs in square holes\"? Some of these question can be answered through phylogenetic studies, which can determine when certain traits arose, and thus whether species interactions and community assembly occurs primarily through coevolution or through dispersal and ecological fitting. Support exists for each, indicating that each has a varied role to play, depending on the community and on historical factors.\n\nA field of recent importance for the application of ecological fitting is that of emerging infectious disease: infectious diseases that have emerged or increased incidence in the last 20 years, as a result of evolution, range expansion, or ecological changes. Climate change represents an ecological perturbation that induces range and phenological shifts in many species, which can encourage parasite transmission and host switching without any evolutionary change occurring. When species begin to infect host species with which they were not previously associated, it may be the result of ecological fitting. Even organisms with complex life histories can switch hosts as long as the resource required by each life stage is phylogenetically conserved and geographically widespread, meaning that it is difficult to predict based on life history complexity or other external factors. This has been used to explain the mysterious appearance of the bullfrog lung trematode \"Haematoloechus floedae\" in Costa Rican leopard frogs, even though bullfrogs do not and have never occurred in this area. When emerging infectious disease is the result of ecological fitting and host specificity is loose, then recurrent host shifts are likely to occur and the difficult task of building a predictive framework for management is necessary.\n\n\n"}
{"id": "3254125", "url": "https://en.wikipedia.org/wiki?curid=3254125", "title": "Energy quality", "text": "Energy quality\n\nEnergy quality is the contrast between different forms of energy, the different trophic levels in ecological systems and the propensity of energy to convert from one form to another. The concept refers to the empirical experience of the characteristics, or qualia, of different energy forms as they flow and transform. It appeals to our common perception of the heat value, versatility, and environmental performance of different energy forms and the way a small increment in energy flow can sometimes produce a large transformation effect on both energy physical state and energy. For example the transition from a solid state to liquid may only involve a very small addition of energy. Methods of evaluating energy quality are sometimes concerned with developing a system of ranking energy qualities in hierarchical order.\n\nSince before antiquity there has been deep philosophical, aesthetic and scientific interest in the contrast of quality with quantity. In some respects the history of modern and postmodern thought can be characterized by the phenomenological approach to these two concepts. A central question has been whether the many different qualitative aspects of the world can be understood in terms of rational quantities, or whether the qualitative and quantitative are irreconcilable: that is, there is no \"rational quality\", or quale ratio. Many scientists and analytic philosophers say they are not, and therefore consider some qualitative phenomena like, for instance, spirituality, and astrology to be unquantifiable, unanalysable by scientific methods, and therefore ungrounded in physical reality. The notion of energy quality therefore has a tendency to be linked with phenomena many scientists consider unquantifiable, or at least incommunicable, and are consequently dismissed out of hand.\n\nAt the same time many people have also recognised qualitative differences in the way things can be done by different entities (both physical and biological). Humans, for example have qualitatively different capacities than many other mammals, due, in part, to their opposable thumb. In the attempt to formalise some of the qualitative differences, entities were grouped according to distinguishing features or capacities. Different schools of thought used different methods to make distinctions. Some people chose taxonomic and genome structure, while others chose energetic function as the basis of classifications. The former are often associated with biology, while the latter with the trophic food chain analysis of ecology. These can be considered attempts to formalise quantitative, scientific studies of the qualitative differences between entities. The efforts were not isolated to biology and ecology, since engineers were also interested in quantifying the amount of work that qualitatively different sources of energy could provide.\n\nAccording to Ohta (1994, pp. 90–91) the ranking and scientific analysis of energy quality was first proposed in 1851 by William Thomson under the concept of \"availability\". This concept was continued in Germany by Z. Rant, who developed it under the title, \"die Exergie\" (the exergy). It was later continued and standardised in Japan. Exergy analysis now forms a common part of many industrial and ecological energy analyses. For example, I.Dincer and Y.A. Cengel (2001, p. 132) state that energy forms of different qualities are now commonly dealt with in steam power engineering industry. Here the \"quality index\" is the relation of exergy to the energy content (Ibid.). However energy engineers were aware that the notion of heat quality involved the notion of value – for example A. Thumann wrote, \"The essential quality of heat is not the amount but rather its 'value'\" (1984, p. 113) – which brings into play the question of teleology and wider, or ecological-scale goal functions. In an ecological context S.E. Jorgensen and G.Bendoricchio say that exergy is used as a goal function in ecological models, and expresses energy \"with a built-in measure of quality like energy\" (2001, p. 392).\n\nThere appear to be two main kinds of methodology used for the calculation of energy quality. These can be classed as either receiver or donor methods. One of the main differences that distinguishes these classes is the assumption of whether energy quality can be upgraded in an energy transformation process.\n\nReceiver methods: view energy quality as a measure and indicator of the relative ease with which energy converts from one form to another. That is, how much energy is received from a transformation or transfer process. For example, A. Grubler used two types of indicators of energetic quality \"pars pro toto\": the hydrogen/carbon (H/C) ratio, and its inverse, the carbon intensity of energy. Grubler used the latter as an indicator of relative environmental quality. However Ohta says that in multistage industrial conversion systems, such as a hydrogen production system using solar energy, the energy quality is not upgraded (1994, p. 125).\n\nDonor methods: view energy quality as a measure of the amount of energy used in an energy transformation, and that goes into sustaining a product or service (H.T.Odum 1975, p. 3). That is how much energy is donated to an energy transformation process. These methods are used in ecological physical chemistry, and ecosystem evaluation. From this view, in contrast with that outlined by Ohta, energy quality \"is\" upgraded in the multistage trophic conversions of ecological systems. Here, upgraded energy quality has a greater capacity to feedback and control lower grades of energy quality. Donor methods attempt to understand the \"usefulness\" of an energetic process by quantifying the extent to which higher quality energy controls lower quality energy.\n\nT.Ohta suggested that the concept of energy quality may be more intuitive if one considers examples where the form of energy remains constant but the amount of energy flowing, or transferred is varied. For instance if we consider only the inertial form of energy, then the energy quality of a moving body is higher when it moves with a greater velocity. If we consider only the heat form of energy, then a higher temperature has higher quality. And if we consider only the light form of energy then light with higher frequency has greater quality (Ohta 1994, p. 90). All these differences in energy quality are therefore easily measured with the appropriate scientific instrument.\n\nThe situation becomes more complex when the form of energy does not remain constant. In this context Ohta formulated the question of energy quality in terms of the conversion of energy of one form into another, that is the transformation of energy. Here, energy quality is defined by the relative ease with which the energy transforms, from form to form.\nIf energy A is relatively easier to convert to energy B but energy B is relatively harder to convert to energy A, then the quality of energy A is defined as being higher than that of B. The ranking of energy quality is also defined in a similar way. (T.Ohta 1994, p. 90).\nNomenclature: Prior to Ohta's definition above, A.W.Culp produced an energy conversion table describing the different conversions from one energy to another. Culp's treatment made use of a subscript to indicate which energy form is being talked about. Therefore, instead of writing \"energy A\", like Ohta above, Culp referred to \"J\", to specify electrical form of energy, where\" J\" refers to \"energy\", and the \"e\"subscript refers to electrical form of energy. Culps notation anticipated Scienceman's (1997) later maxim that all energy should be specified as form energy with the appropriate subscript.\n\nThe notion of energy quality was also recognised in the economic sciences. In the context of biophysical economics energy quality was measured by the amount of economic output generated per unit of energy input (C.J. Cleveland et al. 2000). The estimation of energy quality in an economic context is also associated with embodied energy methodologies. Another example of the economic relevance of the energy quality concept is given by Brian Fleay. Fleay says that the \"Energy Profit Ratio (EPR) is one measure of energy quality and a pivotal index for assessing the economic performance of fuels. Both the direct and indirect energy inputs embodied in goods and services must be included in the denominator.\" (2006; p. 10) Fley calculates the EPR as the energy output/energy input.\n\nOhta sought to order energy form conversions according to their quality and introduced a hierarchical scale for ranking energy quality based on the relative ease of energy conversion (see table to right after Ohta, p. 90). It is evident that Ohta did not analyse all forms of energy. For example, water is left out of his evaluation. It is important to note that the ranking of energy quality is not determined solely with reference to the efficiency of the energy conversion. This is to say that the evaluation of \"relative ease\" of an energy conversion is only partly dependent on transformation efficiency. As Ohta wrote, \"the turbine generator and the electric motor have nearly the same efficiency, therefore we cannot say which has the higher quality\" (1994, p. 90). Ohta therefore also included, 'abundance in nature' as another criterion for the determination energy quality rank. For example, Ohta said that, \"the only electrical energy which exists in natural circumstances is lightning, while many mechanical energies exist.\" (Ibid.). (See also table 1. in Wall's article for another example ranking of energy quality).\n\nLike Ohta, H.T.Odum also sought to order energy form conversions according to their quality, however his hierarchical scale for ranking was based on extending ecological system food chain concepts to thermodyanmics rather than simply relative ease of transformation . For H.T.Odum energy quality rank is based on the amount of energy of one form required to generate a unit of another energy form. The ratio of one energy form input to a different energy form output was what H.T.Odum and colleagues called transformity: \"the EMERGY per unit energy in units of emjoules per joule\" (H.T.Odum 1988, p. 1135).\n\n"}
{"id": "39820557", "url": "https://en.wikipedia.org/wiki?curid=39820557", "title": "Fire in the Night", "text": "Fire in the Night\n\nFire in the Night is a 2013 British documentary film about the Piper Alpha disaster directed by Anthony Wonke. It won the Audience Award at the 2013 Edinburgh International Film Festival. It was first shown on television on 9 July 2013 on BBC Two, three days after the 25th anniversary of the disaster.\n\n"}
{"id": "8839947", "url": "https://en.wikipedia.org/wiki?curid=8839947", "title": "Fixed Bed Nuclear Reactor", "text": "Fixed Bed Nuclear Reactor\n\nThe Fixed Bed Nuclear Reactor (FBNR) is a simple, small, proliferation resistant, inherently safe and passively cooled nuclear reactor with reduced environmental impact. The reactor is being developed under the auspice of the International Atomic Energy Agency. Its science and technology is in the public domain.\n\nThe main developer is Federal University of Rio Grande do Sul in cooperation with international institutions. The institutions that so far have shown interest in participating in this project include Imperial College of the University of London, Institute of Theoretical and Experimental Physics (ITEP) and the Institute of Physics and Power Engineering (IPPE) in the Russian Federation and some individual scientists in Uruguay, Vietnam, Turkey, Finland, Switzerland, and the USA.\n\n"}
{"id": "2239614", "url": "https://en.wikipedia.org/wiki?curid=2239614", "title": "Foil (fluid mechanics)", "text": "Foil (fluid mechanics)\n\nA foil is a solid object with a shape such that when placed in a moving fluid at a suitable angle of attack the lift (force generated perpendicular to the fluid flow) is substantially larger than the drag (force generated parallel to the fluid flow). If the fluid is a gas, the foil is called an airfoil or aerofoil, and if the fluid is water the foil is called a hydrofoil.\n\nA foil generates lift primarily as a result of its shape and angle of attack. When oriented at a suitable angle, the foil deflects the oncoming fluid, resulting in a force on the foil in the direction opposite to the deflection. This force can be resolved into two components: lift and drag. This \"turning\" of the fluid in the vicinity of the foil creates curved streamlines which results in lower pressure on one side and higher pressure on the other. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flowfield about the foil has a higher average velocity on the upper surface than on the lower surface.\n\nA more detailed description of the flowfield is given by the simplified Navier-Stokes equations, applicable when the fluid is incompressible. However, since the effects of the compressibility of air at low speeds is negligible, these simplified equations can be used for both airfoils and hydrofoils as long as the fluid flow is substantially less than the speed of sound (up to about Mach 0.3).\n\nThe degenerate case of a foil is a simple flat plate. When set at an angle (the angle of attack) to the flow the plate will deflect the fluid passing over and under it, and this deflection will result in a lift force on the plate. However, while it does generate lift, it also generates a large amount of drag.\n\nSince even a simple flat plate can generate lift, a significant factor in foil design is the minimization of drag. An example of this is the rudder of a boat or aircraft. When designing a rudder a key design factor is the minimization of drag in its neutral position, which is balanced with the need to produce sufficient lift with which to turn the craft at a reasonable rate.\n\nOther types of foils, both natural and man-made, seen both in air and water, have features that delay or control the onset of lift-induced drag, \"flow separation\", and stall (see Bird flight, Fin, Airfoil, Placoid scale, Tubercle, Vortex generator, Canard (close-coupled), Blown flap, Leading edge slot, Leading edge slats), as well as Wingtip vortices (see Winglet).\n\nLifted weight is proportional to lift coefficient, density of fluid, wing area and true speed by square. A comparison of lifted weight as a function of altitude and depth reveals big differences by a factor of about 3’000 in total from 11 km above sea level to 10 km below sea level, divided into factors of:\n~ 4 between summit and sea level,\n~ 400 between flying close to the ground\nand planing on water,\n~ 2 between planing on water\nand in a fully submerged state.\nThe most dramatic changes are due to different fluids and levels of altitude. The most interesting sector to discuss lift is close to sea level: aircraft approaching the ground, plates planing on water and hydrofoils only barely submerged in water. There is one basic similarity across of these: Almost any shape, as long as it is not too thick, will work as an (air)foil and produce lift when the angle of attack is in the right range.\n\n"}
{"id": "802817", "url": "https://en.wikipedia.org/wiki?curid=802817", "title": "Ford Escape", "text": "Ford Escape\n\nThe Ford Escape is a compact crossover vehicle sold by Ford since 2000 over three generations. Ford released the original model in 2000 for the 2001 model year—a model jointly developed and released with Mazda of Japan—who took a lead in the engineering of the two models and sold their version as the Mazda Tribute. Although the Escape and Tribute share the same underpinnings constructed from the Ford CD2 platform (based on Mazda GF underpinnings), the only panels common to the two vehicles are the roof and floor pressings. Powertrains were supplied by Mazda with respect to the base inline-four engine, with Ford providing the optional V6. At first, the twinned models were assembled by Ford in the US for North American consumption, with Mazda in Japan supplying cars for other markets. This followed a long history of Mazda-derived Fords, starting with the Ford Courier in the 1970s. Ford also sold the first generation Escape in Europe and China as the Ford Maverick, replacing the previous Nissan-sourced model. Then in 2004, for the 2005 model year, Ford's luxury Mercury division released a rebadged version called the Mercury Mariner, sold mainly in North America. The first iteration Escape remains notable as the first SUV to offer a hybrid drivetrain option, released in 2004 for the 2005 model year to North American markets only.\n\nMainstream production of the first generation Escape/Tribute ended in late 2006. For Asia-Pacific markets, both received respective facelifts in 2006 and had production fully transferred to Ford Lio Ho in Taiwan. Extended production of the Mazda lasted until 2010, with the Ford lingering on until 2012.\n\nSecond generations of the Ford Escape, Mercury Mariner, and Mazda Tribute were released in 2007 for the 2008 model year, but mostly restricted to North America. In other markets, the first generation models were either replaced by updated first series versions built in Taiwan, and/or by the unrelated Mazda CX-7 (2006) and Ford Kuga (2008). The North American second generations were merely reskins of the first, with carry-over mechanicals, but with restyled hanging panels and a redesigned interior. Unlike the collaborative approach taken with the previous model, this time the design and engineering was carried out by Ford. A hybrid option was again available. The Mercury version lasted until late 2010, withdrawn from the market as part of the closure of the Mercury brand, with Mazda's Tribute ending production in late 2011. Ford ended manufacture of the second series Escape in 2012.\n\nFord released a third generation in 2012 for the 2013 model year, again, limited to North America. This time, rather than issuing an indigenous, albeit Mazda-derived model, Ford rebadged the Europe-designed Ford Kuga. Although still manufactured in the US, and fitted with slightly different powertrains, the third generation Escape is fully aligned with the Kuga as per the \"One Ford\" plan of having only one vehicle per segment internationally.\n\nThe first generation of Ford Escape was released in 2000 for the 2001 model year. It was jointly developed with Mazda, in which Ford owned a controlling interest, and was released simultaneously with the Mazda Tribute. Both are built on the Ford CD2 platform, in turn based on the Mazda GF platform.\n\nAt the time, larger sport-utility vehicles tended to use pickup truck-based, body-on-frame designs. Other car makers, Jeep, Toyota and Honda had been offering smaller unibody designs, the Jeep Cherokee (XJ), RAV4 and CR-V respectively. Solid rear axles were commonly used on the full-sized truck-based SUVs and Jeep Cherokee due to their ability to carry heavy loads at the expense of a comfortable ride and good handling. Ford and Mazda decided to offer a car-like, unibody design with a fully independent suspension and rack and pinion steering similar to the RAV4 and CR-V, the Escape. Although not meant for serious off-roading, a full-time all-wheel-drive (AWD) system supplied by Dana was optional, which included a locking center differential activated by a switch on the dashboard. The AWD system normally sends most of the power from the engine to the front wheels. If slipping is detected at the front, more power will be sent to the rear wheels in a fraction of a second. The four wheel drive system was a newer version of Ford's \"Control Trac\" 4x4 system, dubbed the Control Trac II 4WD in the Escape. This system allowed the front wheels to receive 100% of the torque until a slip was detected. Using a Rotary Blade Coupling, the rear wheels could be sent up to 100% of the power in fractions of a second. When switching the system from \"Auto\" to \"On,\" the front and rear axles are locked at a 50/50 split; the reaction time necessary to engage the rear wheels is reduced via an integrated bypass clutch. The Control Trac II system allows for a four-wheel drive vehicle without the use of a center differential. The entire braking system was built by Continental Teves, including the ABS and various related suspension components. CKD production began in 2002 at Ford Lio Ho Motor Co. in Taiwan for various Asian markets.\n\nOne main difference between the Tribute and the Ford Escape is that the Tribute's suspension is tuned for a firmer ride than the Escape, in order to correspond with Mazda's sporty image.\n\nIn North America, it slotted below the larger, truck-based Explorer in Ford's lineup, but was larger than the small SUV offerings from Honda and Toyota. Although it is technically a crossover vehicle, it is marketed by Ford as part of its traditional SUV lineup (Escape, Explorer, Expedition) rather than its separate crossover lineup (Edge, Flex).\n\nFrom 2001 to 2004, the Ford Escape was sold in Europe under the Maverick name, and replaced a rebadged version of the Nissan Mistral/Terrano II. Only two versions were made, the 2.0 L Zetec inline 4 engine with manual transmission and 3.0 L Duratec V6 with automatic transmission, both using gasoline as fuel. The absence of a diesel version did not help sales and the vehicle was temporarily discontinued in late 2003. However, the Maverick, in the UK for example, was only available in XLT trim. Plus, the dashboard was not the same as the US Escape; it was instead taken from the Mazda Tribute. The Maverick was reintroduced in 2005 in certain European markets with the Duratec V6 engine. It was announced that the Maverick would be assembled in Russia for the Russian market. As of 2006, the Maverick was no longer sold in Europe, leaving Ford without a compact SUV until the 2008 Ford Kuga was introduced. The Maverick was primarily designed for on-road use – sold with normal road tires, and to be used with front-wheel drive most of the time.\n\nCrash-test results for the Escape have been mixed. In the New Car Assessment Program administered by the USA-based National Highway Traffic Safety Administration, the car received five out of five stars for driver protection and four out of five stars for passenger protection in a frontal impact. The SUV received five stars for both driver and rear passenger in the side impact test. In the Insurance Institute for Highway Safety's frontal offset test, 2001–2004 Escapes received a score of \"Marginal\". In the side impact crash test, vehicles equipped with the optional side air bags received a score of \"Good\" in the , while those without the optional air bags received a score of \"Poor\".\n\nAll Escapes are equipped with a passive 'Immobiliser' called SecuriLock. This feature includes an 'RFID' chip embedded in the key which is read by the car each time the vehicle is started. If the vehicle fails to receive a valid confirmation signal from the key, the vehicle will not run, even if the key is perfectly cut to match the original. Theft, injury, and collision losses reported to insurance companies for the Escape are among the lowest in its class.\n\nIn the United States, all Escapes included standard equipment such as power windows, power door locks, anti-lock braking system (ABS), keyless entry, a folding rear-bench seat, 16-inch wheels, and air conditioning. In addition, an Escape buyer could choose from one of several different trim levels that were available, which included:\n\nXLS (2001–2007): As the most basic trim level of the Escape, the XLS included: the 2.0-liter \"Zetec\" (2001–2004) and the 2.3-liter \"DuraTec\" (2005–2007) engines, a five-speed manual transmission, 15-inch steel wheels, an AM/FM stereo with cassette and CD players (later, a six-disc, in-dash CD changer) and four speakers, high-back front bucket seats, and cloth-and-vinyl seating surfaces. Options include 15- or 16-inch alloy wheels, the 3.0-liter V6 engine (2001–2004), and a four-speed automatic transmission (some of which was available as the XLS Popular Group).\n\nXLT (2001–2007): As the top-of-the-line trim level of the Escape in 2001, and the most popular trim level of the Escape throughout its entire run (2001–2007), the XLT added the following equipment to the base XLS trim level: 16-inch alloy wheels, cloth seating surfaces, and an enhanced interior. Options included an AM/FM stereo with a six-disc, in-dash CD changer (which later became standard equipment on all Escapes), the 3.0-liter V6 engine, a four-speed automatic transmission, a power sunroof, leather-and-vinyl-trimmed seating surfaces, and the seven-speaker premium audio system with amplifier and rear-mounted subwoofer.\n\nXLT Sport (2002–2007): The XLT Sport was one of the more popular trim levels of the Escape from 2002 to 2007. It added equipment to the standard XLT equipment: the V6 engine, four-speed automatic transmission, sport interior trim, and 16-inch machined alloy wheels. Options were the same as the standard XLT trim level.\n\nLimited (2003–2007): As the top-of-the-line trim level of the Escape from 2002–2007, the Limited trim level added the following equipment to the XLT Sport trim level: an AM/FM stereo with six-disc in-dash CD/MP3 changer, the seven-speaker premium audio system with amplifier and rear-mounted subwoofer, low-back front bucket seats, leather-trimmed seating surfaces, dual power heated front bucket seats, a security system, color-keyed exterior trim, luxury interior trim, and a unique front grille. Options were limited, but included a power sunroof.\n\nHybrid (2005–2007): Based on the midrange XLT trim level, the Hybrid included: the 2.3-liter \"DuraTec\" inline-four engine with an electric motor, power front bucket seats, low-back front bucket seats, enhanced partially-recycled cloth seating surfaces, and unique 16-inch alloy wheels. Options included a powered sunroof, a unique integrated GPS navigational system with hybrid information system, Sirius Satellite Radio, the seven-speaker premium audio system with amplifier and rear-mounted subwoofer, leather-trimmed seating surfaces, and a \"two-tone\" exterior paint scheme, with silver-painted lower exterior trim and front and rear bumpers.\n\nThe Escape and Tribute were updated in February 2004 for the 2005 model year with a new base engine (the 2.3 L Duratec 23), which replaced the Zetec 2.0 L 4-cylinder. The most powerful engine remained the Duratec 3.0 L V6, with new engine mounts. Ford also added advanced airbag and seatbelt safety systems, an intelligent AWD system, and exterior changes, which included a redesigned front bumper. The 2005 model year was the first with an automatic transmission available on the base four-cylinder models. The automatic shifter was moved from the column to the console on all models equipped with automatic transmissions. Ford also deleted the recline feature on the rear seats to improve the safety of occupants in the rear seats in the case of a rear crash.\n\nA revamped ZC Escape designed in Taiwan went on sale in the second half of 2006 for the Asian and Pacific markets (except South Korea, where the North American-market Escape is sold). Major external changes included a redesigned front bumper, grille, headlights and hood, and rear bumper, as well as LED taillights.\n\nOn the inside, changes included a floor-mounted automatic-transmission shifter, in place of the column shifter, as well as a redesigned center stack containing audio and climate controls. Climate control is automatic on all models except the XLS. The Limited model also featured full color-coded bumpers, wheel arches and side moldings, as well as side mirrors with integrated LED indicators. Rear drum brakes have been replaced by disc brakes all round.\n\nThe 3.0 L V6 has been modified to reduce fuel consumption by over 10%, while the 2.3 L 4-cylinder has improved midrange torque and an electronic throttle, as well as a slight increase in power to . Both engines had been certified to meet Euro III emission regulations. A four-speed automatic carried over and was the sole transmission choice. Two different four-speed automatic transmissions were used, CD4E for 3.0 L V6 and GF4AX-EL for 2.3 L 4-cylinder.\n\nThe ZD Escape went on sale in mid-2008, bringing numerous changes. In Australia, the V6 engine was dropped, leaving only the 2.3-liter four-cylinder.\n\nThe model range was also simplified, with only a single specification available. Changes to the body included an all-new front bumper, grille, headlights and bonnet, featuring an enlarged Ford emblem set upon a three-bar chrome grille. At the rear, new, slimmer tail lights were featured, which were arranged horizontally, rather than vertically. In addition, the B-pillar was now painted black, rather than body-colour. Compared with the previous model, all external bumpers, mirrors, and cladding were painted the same colour as the body (previously, this was only available on the upscale Limited model). Equipment levels have also improved. Compared to the base model ZC Escape, the ZD included standard side airbags, automatic climate control, 16\" alloy wheels, and mirrors with integrated indicators. Unlike most other competitors in its class, curtain airbags and electronic stability control were not available.\n\nIn 2009, another facelift of the ZD Escape brought about a new grille and front bumper. Chrome trim was completely removed from the grille, replaced with a smaller, black honeycomb grille as the last ever Ford Escape. In its final years, it was sold side by side with its successor, Ford Kuga until the Kuga replaced it in 2013.\n\nThe Tribute made its debut at the 2000 Los Angeles Auto Show as a compact crossover SUV, a segment pioneered by the Toyota RAV4 in 1994. It was the first SUV offered by Mazda since the Mazda Navajo, a rebadged two-door Ford Explorer which was retired after the 1994 model year. In Japan, Mazda had an SUV called the Mazda Proceed Levante, a rebadged Suzuki Escudo, but the Tribute was Mazda's first original SUV. The Ford plant in Claycomo, Missouri assembled Tribute for the North American market, alongside Ford Escape. The Mazda plant in Hofu, Japan and the Ford Lio Ho plant in Taiwan assembled Tribute for their respective markets.\n\nThe 2001–2006 Mazda Tribute was available as front- or four-wheel drive and featured a plain-looking but comfortable roomy interior, decent handling and car-like ride. The Tribute and Escape debuted in 2000, offering front or all wheel drive and a choice of a transversely mounted 2.0 L Ford Zetec 4-cylinder engine with 129 PS (95 kW)/183 N·m (135 lb·ft), or 3.0 L Ford Duratec V6 with 203 PS (150 kW)/265 N·m (195 lb·ft). The 2.0 L 4-cylinder engine had Timing belt driven Camshafts, while the 3.0 L Duratec V6 featured a maintenance-free timing chain.\n\nBoth the Escape and Tribute were refreshed in 2004 for the 2005 model year. The base engine became the Mazda 2.3 L MZR 4 with 153 PS (114 kW)/206 N·m (152 lb·ft), and the top remained the 3.0 L Duratec V6, now with torque down to 261 N·m (193 lb·ft). In North American-built models, a floor-mounted automatic transmission shifter replaced the column shifter. However, Japanese-built models continued with a column shifter. Mazda decided to halt production after the 2006 model year\n\nMazda Japan released a limited version, the Mazdaspeed Tribute, in the year 2004 with a 2.3-liter MZR engine that has 220 PS (164.05 kW)/ 254 N·m (195 lb·ft) that is utilized by a column shifter. However, Mazda Japan decided to end its production in 2005.\n\nA significantly face-lifted version of the Tribute was released in the second half of 2006 for Asia Pacific markets. The updated Tribute featured a larger, bolder grille, with an enlarged Mazda emblem, as well as restyled front bumper and headlights. Side mirrors featured integrated indicators.\n\nOn the inside, changes included a new floor-mounted automatic-transmission shifter, in place of the old column shifter (Asia-Pacific model only; US-built Tribute gained the floor shifter in 2005). The dash was updated with a brand-new radio and automatic climate control with digital read-out, on certain models. Mechanically, the rear drum brakes were replaced by disc brakes. Engines remain the same, but the V6 has been modified to reduce fuel consumption by over 10%, while the 4-cylinder has improved mid-range torque and an electronic throttle. Both engines had been certified to meet Euro III emission regulations.\n\nStarting from 2006, Mazda stopped selling and producing Tribute in Japan, and replaced it with the similar-sized CX-7. In 2008, Mazda Australia also discontinued the Tribute, the absence of the Tribute being filled by the Mazda CX-7 introduced in the previous year. For other Asia-Pacific markets, the production of Tribute was shifted to the Ford Lio Ho plant in Zhongli, Taiwan, which also produces Tribute's twin, Ford Escape, for Asia-Pacific markets. This arrangement continued until early 2010, when the Tribute for the Asia-Pacific markets ceased production, being fully replaced by the CX-7 imported from Japan.\n\nIntroduced in 2004 for the 2005-model-year refresh of the Ford Escape in the US, Ford's Mercury division released a luxury version called the Mercury Mariner. The Mariner sits above the Escape in the Ford-Mercury-Lincoln hierarchy. The Mariner is Mercury's first car-based SUV, and is slotted below the Mountaineer in the lineup. The Mariner was officially offered in the US, Mexico, Saudi Arabia, Kuwait, and the UAE. The Mercury includes stylistic differences, such as a two-tone interior, turn signal repeaters borrowed from the European-market Ford Maverick (the Escape's name in Europe), monotone cladding, and the signature Mercury \"waterfall\" front grille. Unlike its counterparts, a manual transmission was not part of the powertrain lineup. The Mariner was the first Mercury with a four-cylinder since the Mercury Cougar was dropped in 2002. For 2006, the lineup was expanded with the introduction of the Mariner Hybrid. Sales ended after the 2007 model, replaced by a second generation, again a rebadged Ford Escape.\n\nOn September 7, 2006, Ford delivered a special \"Presidential Edition\" Mercury Mariner Hybrid to former President Bill Clinton. Its custom features include: LED lighting, 110-volt outlet, rear bucket seats, center console & rear seat fold-out writing desks, personal DVD players for each seat, refrigerator, increased rear seat legroom. There have also been several undisclosed security modifications made to the vehicle.\n\nThe Mariner Hybrid powertrain was identical to its sibling, the Ford Escape Hybrid. It was launched to the U.S. market in 2006 and was discontinued in 2010 (in the second generation) with the rest of the brand. The Mariner hybrid sold a total of 12,300 units.\n\nLike the Ford Escape Hybrid, the Mariner Hybrid is a \"full\" hybrid electric system, meaning the system can switch automatically between pure electric power, pure gasoline engine power, or a combination of electric battery and gasoline engine operating together, for maximum performance and efficiency at all speeds and loads. When braking or decelerating, the Mariner's hybrid system uses regenerative braking, where the electric drive motor becomes a generator, converting the vehicle's momentum back to electricity for storage in the batteries. With , the Mariner Hybrid has nearly the same acceleration performance as the conventional V6 Mariner. Again, just like the Escape Hybrid, it gets a respectable average of and is sometimes said to be the most fuel efficient sport utility vehicle on the road.\n\nThe Ford Escape Hybrid and Mercury Mariner Hybrid are the gasoline-electric hybrid powered versions that launched in the U.S. in 2004 for the 2005 model year. Built in Kansas City, Missouri, it was the first hybrid SUV to hit the market. The Ford Escape Hybrid was the first American-built hybrid and the first hybrid vehicle from an American automaker, joined by the Chevrolet Silverado/GMC Sierra Hybrids during the same model year. According to the Environmental Protection Agency, the first generation Ford Escape Hybrid is 70% more efficient than the regular Escape. The Mercury Mariner Hybrid is a rebadged version of the Escape Hybrid. It features revised front-end styling and a more luxurious interior.\n\nEscape hybrid versions can be identified by the \"Hybrid\" badges located on the front driver's and passenger's doors as well as near the right tailgate. In addition, the driver's side window in the cargo area is smaller in size in order to accommodate a ventilation slot for the high voltage battery. There was also a \"Special Appearance Package\" available as an option on the 2005–2007 Hybrid models. This package replaced the traditional lower cladding of the Escape with a silver finish. Standard equipment on the Escape Hybrid includes: an eight-way power adjustable driver's seat, dual-zone automatic air conditioning, cruise control, a six-CD stereo, 16-inch alloy wheels, power door locks with remote keyless entry, and power windows.\n\nFord built 17,000 Escape Hybrids in the second half of 2004, four times as many as it had originally planned. Starting in 2005 New York City and other cities in the world began using the Ford Escape Hybrid as taxicabs. The Ford Escape Hybrid won the \"North American Truck of the Year\" award in 2005.\n\nThe Escape Hybrid uses technology similar to that used in the Toyota Prius. Ford engineers realized their technology may conflict with patents held by Toyota, which led to a 2004 patent-sharing accord between the companies, licensing Ford's use of some of Toyota's hybrid technology in exchange for Toyota's use of some of Ford's diesel and direct-injection engine technology. Ford maintains that Ford received no technical assistance from Toyota in developing the hybrid powertrain, but that some hybrid engine technologies developed by Ford independently were found to be similar to technologies previously patented by Toyota, so licenses were obtained. Aisin Seiki Co. Ltd., a Japanese automotive components supplier belonging to the Toyota Group, supplies the HD-10 hybrid continuously variable transmission for the Escape Hybrid. While Toyota produces its third-generation Prius transmission in-house, Aisin is the only supplier of hybrid transmissions to other manufacturers. Friction has arisen concerning Aisin's allocation of limited production capacity and engineering resources to Ford. Sanyo Electric Co. built the , 330 volt 5.5 Ah (would make it 1.8kWh storage), 250-cell nickel metal hydride (NiMH) battery pack for the 2005 Escape Hybrid.\n\nThe Escape Hybrid is a full hybrid, meaning the system can switch automatically between pure electric power, pure gasoline engine power, or a combination of electric battery and gasoline engine operating together, for maximum performance and efficiency at all speeds and loads. When braking or decelerating, the Escape's hybrid system uses regenerative braking, where the electric drive motor becomes a generator, converting the vehicle's momentum back to electricity for storage in the batteries. The Escape Hybrid's 133 horsepower (99 kW) Atkinson cycle gasoline I4 engine and electric motor combine to give , which gives the Hybrid Escape nearly the same acceleration performance as the conventional V6 Escape due to the electric motor's torque being available from zero rpm.\n\nThe hybrid is said to give approximately 75% greater efficiency, the FWD version has EPA ratings of 30 mpg and 28 highway, with combined 29 mpg. The AWD version EPA ratings 28 city and 26 highway, combined 27 mpg in city traffic, and has demonstrated it can travel 400–500 miles (644–805 km) on a single mpg (7.6L-8.1 L/100 km; 35–37 mpg) on the highway. To obtain these mileage figures, the owners manual states that pure gasoline, not ethanol blends, must be used. Unlike conventional vehicles, hybrids often achieve better figures in the city because they do not waste power idling and can recover some power when stopping (by using regenerative braking) that would be wasted in a conventional vehicle.\n\nIn 2006, Ford showed an Escape that could run on E85 fuel.\n\nThe second generation Ford Escape debuted at the 2006 Los Angeles International Auto Show. The North American-market Escape and its Mazda Tribute and Mercury Mariner siblings were redesigned in order to stay competitive with other new compact SUVs, however most of the internals have been carried over for the 2008 model year. The Escape still uses the CD2 platform. Ford also included an electronic stability control system standard on the 2008 Escape.\n\nThe updated Escape received some styling cues from the Explorer, Edge, and Expedition. Changes include a new grille with larger headlamps in the front fascia, while the sides were revised with cleaner lines and rounder wheel arches. The interior is also completely redesigned, including the newest standard Ford family navigation system.\n\nThe 2008 Escape and its Mercury Mariner sibling were the first vehicles to feature Ford's pull-drift steering mode, an enhancement made possible by applying software control to the Electric Power Steering (EPS) system.\n\nOn June 23, 2010, it was announced that Ford would end production on the second generation Escape in 2011 and move production to its Louisville Assembly Plant in Louisville, Kentucky, where it was slated to be succeeded by a Taiwanese version of its European CUV counterpart, the Ford Kuga.\n\nA new concept version for the Asian market, called the \"Ford Escape Adventure Concept\", was revealed at the 2007 Tokyo Motor Show. It features a revised front and rear fascia, incorporating Ford's three-bar grille styling theme and restyled LED tail lamps.\n\nThe 2.3 L Duratec 23 was replaced by a new 2.5 L Duratec 25, which boosted standard power to and of torque, while increasing fuel economy by 1 mpg (~2L/100 km) on both urban and extra-urban cycles. The optional 3.0 L Duratec 30 V6 was thoroughly updated, resulting in a increase, bringing power output to and of torque. The Duratec 30 also sees a fuel economy improvement. The \"Escape\" badge is entirely removed from the front doors.\n\nThe Hybrid is also upgraded to use the 2.5 L (albeit still using the Atkinson cycle for better fuel economy). Efficiency improved to city and highway according to the USEPA. The 2.5 L engine brings the Hybrid's power output up by when the electric motor is added in.\n\nOther mechanical changes include a new rear stabilizer bar, revised suspension tuning, upgrades to the 3.0 L V6 that brought power to 240 hp, and a new exhaust system on all Escapes. The braking system for the Hybrid versions has been revised with a vacuum assist unit that reviewers have said give the brakes a consistent feel over the entire travel of the brake pedal. Previous versions of the Hybrid were reported to have a slightly mushy brake feel, primarily due to the fact that for the first part of the brake travel and braking the system is regenerating power instead of engaging the brake pads against the rotors. However, the brakes, when tested, gave the Escape extremely long stops.\n\nAlso in 2009, Ford's SYNC system is standard on the Hybrid, Hybrid Limited, and conventional Limited models, and optional on the XLS and XLT.\n\nThe Escape underwent some minor aerodynamic changes for 2009, including a revised front chin spoiler and rear tire spoilers. Along with the addition of rear tire spoilers is an optional 17\" chrome-clad wheel equipped with a new Michelin \"Low-Rolling-Resistance-Tire\", slightly increasing ground clearance and improving traction over the standard 16\" wheels and tires. Another new feature is Ford's Easy-Fuel capless fuel filling system.\n\nAs of August 2009 the Escape was among the four Ford vehicles, along with the Focus, F-Series and Ranger, to benefit the most from the Cash for Clunkers program, where it showed a 49 percent increase in sales.\n\nIn 2009 for the 2010 model year, the Escape will add three new features that will be standard on all trims: Ford's MyKey, trailer sway controls and Integrated Spotter Mirror for better blind spot viewing. Everything else added in the 2008 and 2009 model years will be carried over, but the optional orders have been renamed to Rapid Specification Codes (100s for XLS, 200s for XLT, and 300s for Limited).\n\nNew optional features for 2010: active park assist (APA) is a new feature available since mid-2009 as an option on the 2010 Ford Escape Limited (currently only optional on the Ford Flex, Lincoln MKT, and Lincoln MKS). Active Park Assist will detect an available parallel parking space and automatically steer the vehicle into the space (hands free) while the driver controls the accelerator, gearshift and brakes. The system will visually and audibly instruct the driver to park the vehicle. Active Park Assist system uses sensors on the front and rear of the vehicle to guide the vehicle into a parking space. Rear-view camera system – uses an exterior camera embedded in the rear of the vehicle that sends images to a video display in the rearview mirror or the navigation system screen to help enhance visibility directly behind the vehicle when it is in reverse.\n\nNo cosmetic changes were made for the 2011 model year. The only minor change is the standardization of SYNC Traffic & Directions on any models coming equipped with Sync.\n\nThere are no cosmetic or equipment changes for the 2012 model year. The third-generation Escape was unveiled at the 2012 North American International Auto Show in Detroit.\n\nPreviously unavailable Electronic Stability Control system became standard on the second generation. In Insurance Institute for Highway Safety crash tests the Escape along with its cousins, the Mercury Mariner and the Mazda Tribute, are rated \"Good\" in both frontal and side impact crash tests. They are rated \"Good\" for rear crash protection as well and were given the \"Top Safety Pick\" award until 2010. In roof strength tests the Escape receives a \"Marginal\" rating while hybrid models are rated \"Poor\".\n\nIn 2007 for the 2008 model year, the Tribute was significantly revamped, like its Ford Escape and Mercury Mariner siblings. Originally set to be renamed the Mazda CX-5, the vehicle kept the Tribute name. The changes were significant, but fell short of a \"clean sheet\" redesign, as the vehicles remained on the CD2 platform, and kept the old 2.3 L MZR I4, and 3.0 L AJ V6 engines. Visible changes include all new sheet metal and interior. The interior was significantly upgraded using all new components and higher quality materials, and was generally praised by automotive journalists. However, unlike the first generation of the Tribute, which had unique exterior and interior from its siblings, the new model only differs from its siblings in the \"nose\" (front fenders, hood, and front fascia), tail lights and detailing. Notable changes to the exterior include higher belt line, and more pronounced wheel arches. Overall the car was to look larger and more substantial than the previous model. As a cost-saving measure, the rear brake was reverted to drum brake, with predictable criticisms.\n\nThe 2008 Mazda Tribute (non-hybrid) was first unveiled at the 2007 Montreal International Auto Show, and the 2008 Mazda Tribute went on sale in March 2007.\n\nA new addition was the Hybrid model which was previously only available on the Ford Escape and Mercury Mariner.\n\nThe Tribute received additional major changes to improve performance for the 2009 model year, mostly by way of mechanical upgrades. Most significantly, all new engines replaced the increasingly outmatched 2.3L I4 and 3.0L V6. Mazda's new MZR 2.5L I4 replaced the 2.3L, boosting horsepower to and of torque at 4000 rpm. Despite increased horsepower, fuel economy also increased by on both urban and extra-urban cycles. The optional 3.0 L (AJ) V6 was thoroughly updated, resulting in a increase, bringing power output to and of torque. It also sees a 1 mpg improvement. The Tribute Hybrid was dropped after the 2009 model year.\n\nAnother significant change was the switch to Ford's new 6F 6-speed automatic, which became standard on all V6 equipped models and optional on the I4. As well, new front and rear stabilizer bars were added for 2009 to improve ride handling after complaints about diminished performance following the 2008 changes. Others changes included redesigned seats, daytime running lamps, optional steering mounted audio controls, and other additional features. The Tribute was discontinued in November 2011 at the end of the 2011 model year, replaced by the Mazda CX-5 for 2012.\n\nFor the 2008 model year the Mariner was significantly updated with a new look although it remained on the Ford CD2 platform used by the previous generation. The first 2008 Mercury Mariner was unveiled at the South Florida International Auto Show on October 6, 2006. The changes included a new seats, headlights, taillights, a new liftgate, a higher beltline and new doors and wheels. The interior was also significantly updated with higher quality materials and more refined features. The engines remained the same but the 3.0 L Duratec V6 has been modified to reduce fuel consumption by 10%.\n\nThe 2009 Ford Escape and Mercury Mariner were unveiled at the 2008 Washington Auto Show. Sporting a 2.5-liter engine and 6-speed automatic transmission that replaced the four-speed automatic transmission, the new powertrain improved the EPA fuel economy by 1 mile per gallon and increased power by 11% to . Also, the existing 3.0-liter Duratec V6 was bumped from to . The new engine was also the new basis for Ford's hybrid models, including the Ford Escape Hybrid and Mercury Mariner Hybrid. \"For every eight Escape and Mariner vehicles we sell, one of them is a hybrid, and the appeal is growing,\" says Sue Ciscke, Ford senior vice president, Sustainability, Environment and Safety Engineering.\n\nFor the 2010 model year, the Mariner added Ford's MyKey and trailer sway controls as standard on all trim levels. The Mariner engine has a Flex-Fuel option on all of them which features being able to use E85 fuel and/or regular unleaded only on the optional V6 engine. Ford has also done away with the Euro-style turn signal repeaters for this model year. For the 2011 model year, the Mariner featured HD Radio as a standard, but continued with the same features as the 2010 models. This version of the Mariner was its last, as Ford discontinued the Mercury brand due to declining sales. Ford ended production of the Mariner in October 2010. The last one rolled off the assembly line on October 5, 2010.\n\nThe second generation Ford Escape Hybrid received some styling tweaks inside and out. The major cosmetic changes included new bumpers, grille, headlights and taillights to match Ford's new edge style. But the drivetrain was essentially the same mechanically but has had extensive software modifications. For 2009, a larger, more powerful engine was introduced, together with a revised suspension, addition of stability control, the debut of \"Sync\" voice-control system and a capless fuel filler system. The batteries and other hybrid components in a 2009 Escape hybrid added about 136 kg (300 lbs) to the vehicle. However, the added weight was blamed for an adverse effect in handling. Furthermore, from 2009 onward, rear disc brakes of previous years were swapped for drum brakes, which was criticized as a \"strange step backward\".\n\nThe second generation Escape Hybrid was offered in two levels of specification, an undesignated \"base\" model and the more expensive \"Limited\" trim. The \"base\" included: a 60/40 split-fold rear bench seat, \"AdvanceTrac\" with roll stability control, and a single-disc four-speaker CD/MP3 stereo with Sirius Satellite Radio compatibility. The \"Limited\" adds: a chrome front grille, heated front seats, a six-way power driver's seat and full leather upholstery, rear park assist, ambient lighting, and 16-inch six-spoke alloy wheels. For the 2009 model year, Ford SYNC became standard on both Hybrid trims and the Ford Escape Hybrid replaced the \"ESCAPE\" badges on doors with \"\"HYBRID\",\" while relocating the logo for Ford hybrid models from near the driver's side doors next to the \"HYBRID\" text. 2010 models saw the addition of MyKey, trailer sway controls, and integrated spotter mirror for better blind spot viewing. New optional extras are active park assist and a rear-view camera.\n\nFord announced the development of a prototype E85 Hybrid Escape, the first flexible fuel hybrid electric vehicle capable of running on 85% ethanol. In 2007 Ford produced 20 demonstration Escape Hybrid E85s for real-world testing in fleets around the U.S.\n\nFrom 2009, the gas engine was 2.5L Atkinson-cycle four cylinder engine with 155 hp at 6,000 rpm with an electric motor that produces 94 hp at 5,000 rpm. The maximum combined output of both was 177 hp.\n\nThe U.S. Environmental Protection Agency (EPA) rated the fuel economy for the 2010 Escape Hybrid (FWD) at city, and highway. The following table compares fuel economy, carbon footprint, and petroleum consumption between the hybrid version and other drivetrains of the Escape family as estimated by the EPA and the U.S. Department of Energy. The Escape Hybrid met both California's SULEV and PZEV standards, with tailpipe emissions better than 90% less than the average 2003 new car and zero evaporative emissions.\n\nBy early 2012 Ford discontinued the production of the Escape Hybrid due to the introduction of the third generation Escape. Two of the new 2013 model year Escapes have direct-injected and turbocharged EcoBoost units (of 1.6 and 2.0 liters) that deliver a higher fuel economy than the 2012 model. A total of 122,850 Escape Hybrids were built since 2005, along with 12,300 units of its sibling the Mercury Mariner Hybrid, discontinued in 2010.\n\n\nThree companies have converted Ford Escape Hybrids to plug-in under a contract with the NYSERDA and delivered them in 2007: Electrovaya of Toronto Canada, Hymotion also of Toronto Canada, Hybrids Plus of Boulder Colorado, United States.\n\nFord developed a research Escape Plug-in Hybrid and delivered the first of a fleet of 20 to Southern California Edison (SCE) in December 2007 to begin road testing. This project is a collaboration aimed to explore the future of plug-in hybrids and evaluate how the vehicles might interact with the home and the utility's electrical grid. Some of the vehicles will be evaluated \"in typical customer settings\", according to Ford. Ford also developed the first ever flexible-fuel plug-in hybrid SUV, which was delivered to the United States Department of Energy in June 2008. This plug-in version of the Escape Hybrid runs on gasoline or E85 and is also part of the demonstration fleet Ford developed in a partnership with Southern California Edison and the Electric Power Research Institute.\n\nBoth the E85 version and the conventional gasoline engine version use a 10 kwh lithium-ion battery, which allows for a range at or less. When the battery's charge drops to 30%, the vehicle switches to its four-cylinder engine, assisted by the batteries, operating as a regular hybrid electric vehicle. The vehicle has a display system which shows the driver how efficient the vehicle is at any given time. If the vehicle uses its engine and is running in traditional hybrid mode, fuel economy is rated at in the city and on the highway.\n\nThis fleet of 20 Ford Escape Plug-ins has been running in field testing with utility company fleets in California, New York, Ohio, North Carolina, Alabama, Georgia, Massachusetts, Michigan, and Quebec, Canada. About 130 more will be produced for testing with financing from a $30 million grant from the U.S. Department of Energy. Sales are scheduled for 2012.\n\nOn August 2009 Ford delivered the first Escape Plug-in equipped with intelligent vehicle-to-grid (V2G) communications and control system technology to American Electric Power of Columbus, Ohio. This technology allows the vehicle operator to program when to recharge the vehicle, for how long and at what utility rate. The battery systems communicate directly with the electrical grid via smart meters provided by utility companies through wireless networking. During the two years since the demonstration program began, the fleet of Escape Plug-ins has logged more than , and Ford plans to equip all 21 plug-in hybrid Escapes with the vehicle-to-grid communications technology.\n\nThe Ford demonstration vehicles and Hybrids Plus conversions are similar. The conversion involves the replacement of the original NiMH battery, located on the floor of the trunk, with a larger capacity Li-ion battery, in the same location and substantially the same volume as the original battery. The Electrovaya and Hymotion conversions retain the original battery, and augment its capacity with a Li-ion battery that occupies a significant portion of the trunk. In all cases, the conversion also involves the addition of a charger and of a power plug.\n\nThe third generation was designed and rebadged by Ford of Europe alongside the largely identical European-market Ford Kuga. It was released to North American markets in 2012 for the 2013 model year. Other markets that previously used the \"Escape\" nameplate have switched to \"Kuga\" under the One Ford program.\n\n"}
{"id": "1037854", "url": "https://en.wikipedia.org/wiki?curid=1037854", "title": "Free electron model", "text": "Free electron model\n\nIn solid-state physics, the free electron model is a simple model for the behaviour of charge carriers in a metallic solid. It was developed in 1927, principally by Arnold Sommerfeld who combined the classical Drude model with quantum mechanical Fermi–Dirac statistics and hence it is also known as the Drude–Sommerfeld model.\n\nGiven its simplicity, it is surprisingly successful in explaining many experimental phenomena, especially\n\nThe free electron model solved many of the inconsistencies related to the Drude model and gave insight into several other properties of metals. The model considers that metals are composed of a quantum electron gas where ions play almost no role. The model can be very predictive when applied to alkali and noble metals.\n\nIn the free electron model four main assumptions are taken into account:\n\nThe name of the model comes from the first two assumptions, as each electron can be treated as free particle with a respective quadratic relation between energy and momentum.\n\nThe crystal lattice is not explicitly taken into account in the free electron model, but a quantum-mechanical justification was given a year later (1928) by Bloch's theorem: an unbound electron moves in a periodic potential as a free electron in vacuum, except for the electron mass \"m\" becoming an effective mass \"m*\" which may deviate considerably from \"m\" (one can even use negative effective mass to describe conduction by electron holes). Effective masses can be derived from band structure computations that were not originally taken into account in the free electron model.\n\nMany physical properties follow directly from the Drude model as some equations do not depend on the statistical distribution of the particles. Taking the classical velocity distribution of an ideal gas or the velocity distribution of a Fermi gas only changes the results related to the speed of the electrons.\n\nMainly, the free electron model and the Drude model predict the same DC electrical conductivity \"σ\" for Ohm's law, that is\n\nwhere formula_4 is the current density, formula_5 is the external electric field, formula_6 is the electronic density (number of electrons/volume) and formula_7 is the electron electric charge.\n\nOther quantities that remain the same under the free electron model as under Drude's are the AC susceptibility, the plasma frequency, the magnetoresistance, and the Hall coefficient related to the Hall effect.\n\nMany properties of the free electron model follow directly from equations related to the Fermi gas, as the independent electron approximation leads to an ensemble of non-interacting electrons. For a three-dimensional electron gas we can define the Fermi energy as\n\nwhere formula_9 is the reduced Planck constant. The Fermi energy defines the Fermi level, \"i.e.\" the maximal energy an electron in the metal can have at zero temperature. For metals the Fermi energy is in the order of units of electronvolts.\n\nThe 3D density of states (number of energy states, per energy per volume) of a non-interacting electron gas is given by:\n\nwhere formula_11 is the energy of a given electron. This formula takes into account the spin degeneracy but does not consider a possible energy shift due to the bottom of the conduction band. For 2D the density of states is constant and for 1D is inversely proportional to the square root of the electron energy.\n\nAdditionally the Fermi energy is used to define chemical potential formula_12. Sommerfeld expansion is a technique used to calculate the chemical potential for higher energies, that is\n\nwhere formula_14 is the temperature and we define formula_15 as the Fermi temperature (formula_16 is Boltzmann constant). The perturbative approach is justified as the Fermi temperature is usually of about 10 K for a metal, hence at room temperature or lower the Fermi energy and the chemical potential are practically equivalent.\n\nThe total energy per unit volume (at formula_17) can also be calculated by integrating over the phase space of the system, we obtain\n\nwhich does not depend on temperature. Compare with the energy per electron of an ideal gas: formula_19, which is null at zero temperature. For an ideal gas to have the same energy as the electron gas, the temperatures would need to be of the order of the Fermi temperature. Thermodynamically, this energy of the electron gas corresponds to a zero-temperature pressure given by\n\nwhere formula_21 is the volume and formula_22 is the total energy, the derivative performed at temperature and chemical potential constant. This pressure is called the electron degeneracy pressure and does not come from repulsion or motion of the electrons but from the restriction that no more than two electrons (due to the two values of spin) can occupy the same energy level. This pressure defines the compressibility or bulk modulus of the metal\n\nThis expression gives the right order of magnitude for the bulk modulus for alkali metals and noble metals, which show that this pressure is as important as other effects inside the metal. For other metals the crystalline structure has to be taken into account.\n\nOne open problem in solid-state physics before the arrival of the free electron model was related to the low heat capacity of metals. Even when the Drude model was a good approximation for the Lorenz number of the Wiedemann-Franz law, the classical argument is based on the idea that the volumetric heat capacity of an ideal gas is\n\nIf this was the case, the heat capacity of a metal could be much higher due to this electronic contribution. Nevertheless, such a large heat capacity was never measured, rising suspicions about the argument. By using Sommerfeld's expansion one can obtain corrections of the energy density at finite temperature and obtain the volumetric heat capacity of an electron gas, given by:\n\nwhere the prefactor to formula_26is considerably smaller than the 3/2 found in formula_27, about 100 times smaller at room temperature and much smaller at lower formula_28. The good estimation of the Lorenz number in the Drude model was a result of the classical mean velocity of electron being about 100 larger than the quantum version, compensating the large value of the classical heat capacity. The free electron model calculation of the Lorenz factor is about twice the value of Drude's and its closer to the experimental value. With this heat capacity the free electron model is also able to predict the right order of magnitude and temperature dependence at low \"T\" for the Seebeck coefficient of the thermoelectric effect.\n\nEvidently, the electronic contribution alone does not predict the Dulong–Petit law, i.e. the observation that the heat capacity of a metal is constant at high temperatures. The free electron model can be improved in this sense by adding the lattice vibrations contribution. Two famous schemes to include the lattice into the problem are the Einstein solid model and Debye model. With the addition of the later, the volumetric heat capacity of a metal at low temperatures can be more precisely written in the form, \nwhere formula_30 and formula_31 are constants of related to the material. The linear term comes from the electronic contribution while the cubic term comes from Debye model. At high temperature this expression is no longer correct, the electronic heat capacity can be neglected, and the total heat capacity of the metal tends to a constant.\n\nNotice that without the relaxation time approximation, there is no reason for the electrons to deflect their motion, as there are no interactions, thus the mean free path should be infinite. Drude model considered the mean free path of electrons to be close to the distance between ions in the material, implying the earlier conclusion that the diffusive motion of the electrons was due to collisions with the ions. The mean free paths in the free electron model are instead given by formula_32 (where formula_33 is the Fermi speed) and are in the order of hundreds of ångströms, at least one order of magnitude larger than any possible classical calculation. The mean free path is then not a result electron-ion collisions but instead is related to imperfections in the material, either due to defects and impurities in the metal, or due to thermal fluctuations.\n\nThe free electron model presents several inadequacies that are contradicted by experimental observation. We list some inaccuracies below:\n\nOther inadequacies are present in the Wiedemann-Franz law at intermediate temperatures and the frequency-dependence of metals in the optical spectrum.\n\nMore exact values for the electrical conductivity and Wiedemann-Franz law can be obtained by softening the relaxation-time approximation by appealing to Boltzmann transport equations or Kubo formula.\n\nThe spin is mostly neglected in the free electron model and its consequences can lead to emergent magnetic phenomena like Pauli paramagnetism and ferromagnetism.\n\nAn immediate continuation to the free electron model can be obtained by assuming the empty lattice approximation, which forms the basis of the band structure model known as the nearly free electron model.\n\nAdding repulsive interactions between electrons does not change very much the picture presented here. Lev Landau showed that a Fermi gas under repulsive interactions, can be seen as a gas of equivalent quasiparticles that slightly modify the properties of the metal. Landau's model is now known as the Fermi liquid theory. More exotic phenomena like superconductivity, where interactions can be attractive, require a more refined theory.\n\n\n"}
{"id": "4413754", "url": "https://en.wikipedia.org/wiki?curid=4413754", "title": "Gravitational energy", "text": "Gravitational energy\n\nGravitational energy is the potential energy a body with mass has in relation to another massive object due to gravity. It is potential energy associated with the gravitational field. Gravitational energy is dependent on the masses of two bodies, their distance apart and the gravitational constant ().\n\nIn everyday cases only one body is accelerating measurably, and its acceleration is constant (for example, dropping a ball on Earth). For such scenarios the Newtonian formula can – for the potential energy of the accelerating body with respect to the stationary – be reduced to:\n\nwhere formula_2 is the gravitational potential energy, formula_3 is the mass of the object accelerating, formula_4 is the acceleration of the object, and formula_5 is the distance between the bodies. Note that this formula treats the potential energy as a positive quantity.\n\nIn classical mechanics, two or more masses always have a gravitational potential. Conservation of energy requires that this gravitational field energy is always negative. The gravitational potential energy is the potential energy an object has because it is within a gravitational field.\n\nThe force one point mass formula_6 exerts onto another point mass formula_3 is given by Newton's law of gravitation: formula_8\n\nTo get the total work done by an external force to bring point mass formula_3 from infinity to the final distance formula_10 (for example the radius of Earth) of the two mass points, the force is integrated with respect to displacement:\n\nBecause formula_13, the total work done on the object can be written as:\n\nIn general relativity gravitational energy is extremely complex, and there is no single agreed upon definition of the concept. It is sometimes modeled via the Landau–Lifshitz pseudotensor that allows retention for the energy-momentum conservation laws of classical mechanics. Addition of the matter stress–energy–momentum tensor to the Landau–Lifshitz pseudotensor results in a combined matter plus gravitational energy pseudotensor that has a vanishing 4-divergence in all frames - ensuring the conservation law. Some people object to this derivation on the grounds that pseudotensors are inappropriate in general relativity, but the divergence of the combined matter plus gravitational energy pseudotensor is a tensor.\n\n"}
{"id": "13764", "url": "https://en.wikipedia.org/wiki?curid=13764", "title": "Hassium", "text": "Hassium\n\nHassium is a synthetic chemical element with symbol Hs and atomic number 108. It is named after the German state of Hesse. It is a synthetic element and radioactive; the most stable known isotope, Hs, has a half-life of approximately 10 seconds.\n\nIn the periodic table of the elements, it is a d-block transactinide element. Hassium is a member of the 7th period and belongs to the group 8 elements: it is thus the sixth member of the 6d series of transition metals. Chemistry experiments have confirmed that hassium behaves as the heavier homologue to osmium in group 8. The chemical properties of hassium are characterized only partly, but they compare well with the chemistry of the other group 8 elements. In bulk quantities, hassium is expected to be a silvery metal that reacts readily with oxygen in the air, forming a volatile tetroxide.\n\nThe synthesis of element 108 was first attempted in 1978 by a research team led by Yuri Oganessian at the Joint Institute for Nuclear Research (JINR) in Dubna, Moscow Oblast, Russian SFSR, Soviet Union. The team used a reaction that would generate 108 from radium and calcium. The researchers were uncertain in interpreting result data; the paper did not unambiguously claim discovery. That same year, another team at JINR investigated the possibility of synthesis of element 108 in reactions between lead and iron; they were uncertain in interpreting result data as well, openly suggesting a possibility that element 108 had not been created.\n\nNew experiments were performed in 1983. In each experiment, a thin layer of a target material was installed on a rotating wheel and bombarded at a shallow angle. This was made so that fission fragments from spontaneously fissioning nuclides formed could escape the target and be detected in a number of fission track detectors surrounding the wheel. The experiments probably resulted in synthesis of element 108: bismuth was bombarded with manganese to obtain 108, lead was bombarded with iron to obtain 108, and californium was bombarded with neon to obtain 108. These experiments were not claimed as a discovery and were only announced by Oganessian in a conference rather than in a written report.\n\nIn 1984, researchers in Dubna published a written report. The researchers performed a number of experiments set up as the previous ones with, bombarding target materials (bismuth and lead) with ions of lighter element (manganese and iron, correspondingly).\n\nAlso in 1984, a research team led by Peter Armbruster and Gottfried Münzenberg at the Gesellschaft für Schwerionenforschung (GSI; \"Institute for Heavy Ion Research\") in Darmstadt, Hesse, West Germany, attempted to create element 108. The team bombarded a target of lead with accelerated nuclei of iron. They reported synthesis of 3 atoms of 108.\n\nIn 1985, the International Union of Pure and Applied Chemistry (IUPAC) and the International Union of Pure and Applied Physics (IUPAP) formed a Joint Working Party (JWP) to assess discoveries and establish final names for the elements with atomic number greater than 100. The party held meetings with delegates from the three competing institutes; in 1990, they established criteria on recognition of an element, and in 1991, they finished the work on assessing discoveries, and disbanded. These results were published in 1993.\n\nAccording to the report, the 1984 works from JINR and GSI simultaneously independently established synthesis of element 108. Of the two 1984 works, the one from GSI was said to be sufficient as a discovery on its own, while the JINR work, which preceded the GSI one, \"very probably\" displayed synthesis of element 108, but that is determined in retrospect given the work from Darmstadt. The report concluded that the major credit should be awarded to GSI.\n\nAccording to Mendeleev's nomenclature for unnamed and undiscovered elements, hassium should be known as \"eka-osmium\". In 1979, IUPAC published recommendations according to which the element was to be called \"unniloctium\" (with the corresponding symbol of \"Uno\"), a systematic element name as a placeholder, until the element was discovered (and the discovery then confirmed) and a permanent name was decided on. Although widely used in the chemical community on all levels, from chemistry classrooms to advanced textbooks, the recommendations were mostly ignored among scientists in the field, who either called it \"element 108\", with the symbol of \"E108\", \"(108)\", or even simply \"108\", or used the proposed name \"hassium\".\n\nThe name \"hassium\" was proposed by Peter Armbruster and his colleagues, the officially recognised German discoverers, in September 1992. It is derived from the Latin name \"Hassia\" for the German state of Hesse where the institute is located. In 1994, IUPAC Commission on Nomenclature of Inorganic Chemistry recommended that element 108 be named \"hahnium\" (Hn) after the German physicist Otto Hahn so that elements named after Hahn and Lise Meitner (meitnerium) would be next to each other, honoring their joint discovery of nuclear fission. This was because they felt that Hesse did not merit an element being named after it. GSI protested saying that this contradicted the long-standing convention to give the discoverer the right to suggest a name; the American Chemical Society supported the GSI. IUPAC relented and the name \"hassium\" (Hs) was adopted internationally in 1997.\n\nHassium is not known to occur naturally on Earth; the half-lives of all its known isotopes are short enough that no primordial hassium would have survived to the present day. This does not rule out the possibility of unknown longer-lived isotopes or nuclear isomers existing, some of which could still exist in trace quantities today if they are long-lived enough. In the early 1960s, it was predicted that long-lived deformed isomers of hassium might occur naturally on Earth in trace quantities. This was theorized in order to explain the extreme radiation damage in some minerals that could not have been caused by any known natural radioisotopes, but could have been caused by superheavy elements.\n\nIn 1963, Soviet scientist Viktor Cherdyntsev, who had previously claimed the existence of primordial curium-247, claimed to have discovered element 108 (specifically, the Hs isotope, which supposedly had a half-life of 400 to 500 million years) in natural molybdenite and suggested the name \"sergenium\" (symbol Sg; at the time, this symbol had not yet been taken by seaborgium) for it; this name takes its origin in the name for the Silk Road and was explained as \"coming from Kazakhstan\". His rationale for claiming that sergenium was the heavier homologue to osmium was that minerals supposedly containing sergenium formed volatile oxides when boiled in nitric acid, similarly to osmium. His findings were criticized by Soviet physicist Vladimir Kulakov on the grounds that some of the properties Cherdyntsev claimed sergenium had were inconsistent with the then-current nuclear physics.\n\nThe chief questions raised by Kulakov were that the claimed alpha decay energy of sergenium was many orders of magnitude lower than expected and the half-life given was eight orders of magnitude shorter than what would be predicted for a nuclide alpha decaying with the claimed decay energy, but at the same time a corrected half-life in the region of 10 years would be impossible as it would imply that the samples contained about 100 milligrams of sergenium. In 2003 it was suggested that the observed alpha decay with energy 4.5 MeV could be due to a low-energy and strongly enhanced transition between different hyperdeformed states of a hassium isotope around Hs, thus suggesting that the existence of superheavy elements in nature was at least possible, although unlikely.\n\nIn 2004, the Joint Institute for Nuclear Research conducted a search for natural hassium. This was done underground to avoid interference and false positives from cosmic rays, but no results have been released, strongly implying that no natural hassium was found. The possible extent of primordial hassium on Earth is uncertain; it might now only exist in traces, or could even have completely decayed by now after having caused the radiation damage long ago.\n\nIn 2006, it was hypothesized that an isomer of Hs might have a half-life of around years, which would explain the observation of alpha particles with energies of around 4.4 MeV in some samples of molybdenite and osmiridium. This isomer of Hs could be produced from the beta decay of Bh and Sg, which, being homologous to rhenium and molybdenum respectively, should occur in molybdenite along with rhenium and molybdenum if they occurred in nature. Since hassium is homologous to osmium, it should also occur along with osmium in osmiridium if it occurred in nature. The decay chains of Bh and Sg are very hypothetical and the predicted half-life of this hypothetical hassium isomer is not long enough for any sufficient quantity to remain on Earth. It is possible that more Hs may be deposited on the Earth as the Solar System travels through the spiral arms of the Milky Way, which would also explain excesses of plutonium-239 found on the floors of the Pacific Ocean and the Gulf of Finland, but minerals enriched with Hs are predicted to also have excesses of uranium-235 and lead-207, and would have different proportions of elements that are formed during spontaneous fission, such as krypton, zirconium, and xenon. Thus, the occurrence of hassium in nature in minerals such as molybdenite and osmiride is theoretically possible, but highly unlikely.\n\nHassium has no stable or naturally occurring isotopes. Several radioactive isotopes have been synthesized in the laboratory, either by fusing two atoms or by observing the decay of heavier elements. Twelve different isotopes have been reported with atomic masses from 263 to 277 (with the exceptions of 272, 274, and 276), three of which, hassium-265, hassium-267, hassium-269, have known metastable states. Most of these decay predominantly through alpha decay, but some also undergo spontaneous fission.\n\nThe lightest isotopes, which usually have shorter half-lives were synthesized by direct fusion between two lighter nuclei and as decay products. The heaviest isotope produced by direct fusion is Hs; heavier isotopes have only been observed as decay products of elements with larger atomic numbers. In 1999, scientists at University of California in Berkeley, California, United States, announced that they had succeeded in synthesizing three atoms of Og. These parent nuclei were reported to have successively emitted three alpha particles to form hassium-273 nuclei, which were claimed to have undergone an alpha decay, emitting alpha particles with decay energies of 9.78 and 9.47 MeV and half-life 1.2 s, but their claim was retracted in 2001 as it came out the data was fabricated. The isotope was successfully produced in 2010 by the same team. The new data matched the previous (fabricated) data.\n\nAccording to calculations, 108 is a proton magic number for deformed nuclei (nuclei that are far from spherical), and 162 is a neutron magic number for deformed nuclei. This means that such nuclei are permanently deformed in their ground state but have high, narrow fission barriers to further deformation and hence relatively long life-times to spontaneous fission. The spontaneous fission half-lives in this region are typically reduced by a factor of 10 in comparison with those in the vicinity of the spherical doubly magic nucleus Fl, caused by the narrower fission barrier for such deformed nuclei. Hence, the nucleus Hs has promise as a deformed doubly magic nucleus. Experimental data from the decay of the darmstadtium (Z=110) isotopes Ds and Ds provides strong evidence for the magic nature of the N=162 sub-shell. The syntheses of Hs, Hs, and Hs also fully support the assignment of N=162 as a magic number. In particular, the low decay energy for Hs is in complete agreement with calculations.\n\nEvidence for the magicity of the Z=108 proton shell can be obtained from two sources: the variation in the partial spontaneous fission half-lives for isotones and the large gap in the alpha Q value for isotonic nuclei of hassium and darmstadtium. For spontaneous fission, it is necessary to measure the half-lives for the isotonic nuclei Sg, Hs and Ds. Since the isotopes Sg and Ds are not currently known, and fission of Hs has not been measured, this method cannot yet be used to confirm the stabilizing nature of the Z=108 shell. Good evidence for the magicity of the Z=108 shell can nevertheless be found from the large differences in the alpha decay energies measured for Hs, Ds and Ds. More conclusive evidence would come from the determination of the decay energy for the unknown nucleus Ds.\nVarious calculations show that hassium should be the heaviest known group 8 element, consistent with the periodic law. Its properties should generally match those expected for a heavier homologue of osmium, with a few deviations arising from relativistic effects.\n\nThe previous members of group 8 have relatively high melting points (Fe, 1538 °C; Ru, 2334 °C; Os, 3033 °C). Much like them, hassium is predicted to be a solid at room temperature, although the melting point of hassium has not been precisely calculated. Hassium should crystallize in the hexagonal close-packed structure (/ = 1.59), similarly to its lighter congener osmium. Pure metallic hassium is calculated to have a bulk modulus (resistance to uniform compression) comparable to that of diamond (442 GPa). Hassium is expected to have a bulk density of 40.7 g/cm, the highest of any of the 118 known elements and nearly twice the density of osmium, the most dense measured element, at 22.61 g/cm. This results from hassium's high atomic weight, the lanthanide and actinide contractions, and relativistic effects, although production of enough hassium to measure this quantity would be impractical, and the sample would quickly decay. Osmium is the densest element of the first 6 periods, and its heavier congener hassium is expected to be the densest element of the first 7 periods.\n\nThe atomic radius of hassium is expected to be around 126 pm. Due to the relativistic stabilization of the 7s orbital and destabilization of the 6d orbital, the Hs ion is predicted to have an electron configuration of [Rn] 5f 6d 7s, giving up a 6d electron instead of a 7s electron, which is the opposite of the behavior of its lighter homologues. On the other hand, the Hs ion is expected to have an electron configuration of [Rn] 5f 6d 7s, analogous to that calculated for the Os ion.\n\nHassium is the sixth member of the 6d series of transition metals and is expected to be much like the platinum group metals. Calculations on its ionization potentials, atomic radius, as well as radii, orbital energies, and ground levels of its ionized states are similar to that of osmium, implying that hassium's properties would resemble those of the other group 8 elements, iron, ruthenium, and osmium. Some of these properties were confirmed by gas-phase chemistry experiments. The group 8 elements portray a wide variety of oxidation states, but ruthenium and osmium readily portray their group oxidation state of +8 (the second-highest known oxidation state for any element, which is very rare for other elements) and this state becomes more stable as the group is descended. Thus hassium is expected to form a stable +8 state. Analogously to its lighter congeners, hassium is expected to also show other stable lower oxidation states, such as +6, +5, +4, +3, and +2. Indeed, hassium(IV) is expected to be more stable than hassium(VIII) in aqueous solution.\n\nThe group 8 elements show a very distinctive oxide chemistry which allows extrapolations to be made easily for hassium. All the lighter members have known or hypothetical tetroxides, MO. Their oxidising power decreases as one descends the group. FeO is not known due to its extraordinarily large electron affinity (the amount of energy released when an electron is added to a neutral atom or molecule to form a negative ion) which results in the formation of the well-known oxoanion ferrate(VI), . Ruthenium tetroxide, RuO, formed by oxidation of ruthenium(VI) in acid, readily undergoes reduction to ruthenate(VI), . Oxidation of ruthenium metal in air forms the dioxide, RuO. In contrast, osmium burns to form the stable tetroxide, OsO, which complexes with the hydroxide ion to form an osmium(VIII) -\"ate\" complex, [OsO(OH)]. Therefore, eka-osmium properties for hassium should be demonstrated by the formation of a stable, very volatile tetroxide HsO, which undergoes complexation with hydroxide to form a hassate(VIII), [HsO(OH)]. Ruthenium tetroxide and osmium tetroxide are both volatile, due to their symmetrical tetrahedral molecular geometry and their being charge-neutral; hassium tetroxide should similarly be a very volatile solid. The trend of the volatilities of the group 8 tetroxides is known to be RuO < OsO > HsO, which completely confirms the calculated results. In particular, the calculated enthalpies of adsorption (the energy required for the adhesion of atoms, molecules, or ions from a gas, liquid, or dissolved solid to a surface) of HsO, −(45.4 ± 1) kJ/mol on quartz, agrees very well with the experimental value of .\n\nDespite the fact that the selection of a volatile hassium compound (hassium tetroxide) for gas-phase chemical studies was clear from the beginning, the chemical characterization of hassium was considered a difficult task for a long time. Although hassium isotopes were first synthesized in 1984, it was not until 1996 that a hassium isotope long-lived enough to allow chemical studies to be performed was synthesized. Unfortunately, this hassium isotope, Hs, was then synthesized indirectly from the decay of Cn; not only are indirect synthesis methods not favourable for chemical studies, but also the reaction that produced the isotope Cn had a low yield (its cross-section was only 1 pb), and thus did not provide enough hassium atoms for a chemical investigation. The direct synthesis of Hs and Hs in the reaction Cm(Mg,xn)Hs (x = 4 or 5) appeared more promising, as the cross-section for this reaction was somewhat larger, at 7 pb. This yield was still around ten times lower than that for the reaction used for the chemical characterization of bohrium. New techniques for irradiation, separation, and detection had to be introduced before hassium could be successfully characterized chemically as a typical member of group 8 in early 2001.\n\nRuthenium and osmium have very similar chemistry due to the lanthanide contraction, but iron shows some differences from them: for example, although ruthenium and osmium form stable tetroxides in which the metal is in the +8 oxidation state, iron does not. Consequently, in preparation for the chemical characterization of hassium, researches focused on ruthenium and osmium rather than iron, as hassium was expected to also be similar to ruthenium and osmium due to the actinide contraction. Nevertheless, in the planned experiment to study hassocene (Hs(CH)), ferrocene may also be used for comparison along with ruthenocene and osmocene.\nThe first chemistry experiments were performed using gas thermochromatography in 2001, using Os and Os as a reference. During the experiment, 5 hassium atoms were synthesized using the reaction Cm(Mg,5n)Hs. They were then thermalized and oxidized in a mixture of helium and oxygen gas to form the tetroxide.\n\nThe measured deposition temperature indicated that hassium(VIII) oxide is less volatile than osmium tetroxide, OsO, and places hassium firmly in group 8. However, the enthalpy of adsorption for HsO measured, , was significantly lower than what was predicted, , indicating that OsO was more volatile than HsO, contradicting earlier calculations, which implied that they should have very similar volatilities. For comparison, the value for OsO is . It is possible that hassium tetroxide interacts differently with the different chemicals (silicon nitride and silicon dioxide) used for the detector; further research is required, including more accurate measurements of the nuclear properties of Hs and comparisons with RuO in addition to OsO.\n\nIn 2004 scientists reacted hassium tetroxide and sodium hydroxide to form sodium hassate(VIII), a reaction well known with osmium. This was the first acid-base reaction with a hassium compound, forming sodium hassate(VIII):\n\nThe team from the University of Mainz were planning to study the electrodeposition of hassium atoms using the new TASCA facility at the GSI. Their aim was to use the reaction Ra(Ca,4n)Hs. In addition, scientists at the GSI were hoping to utilize TASCA to study the synthesis and properties of the hassium(II) compound hassocene, Hs(CH), using the reaction Ra(Ca,xn). This compound is analogous to the lighter ferrocene, ruthenocene, and osmocene, and is expected to have the two cyclopentadienyl rings in an eclipsed conformation like ruthenocene and osmocene and not in a staggered conformation like ferrocene. Hassocene was chosen because it has hassium in the low formal oxidation state of +2 (although the bonding between the metal and the rings is mostly covalent in metallocenes) rather than the high +8 state which had previously been investigated, and relativistic effects were expected to be stronger in the lower oxidation state. Many metals in the periodic table form metallocenes, so that trends could be more easily determined, and the highly symmetric structure of hassocene and its low number of atoms also make relativistic calculations easier. Hassocene should be a stable and highly volatile compound.\n"}
{"id": "1782102", "url": "https://en.wikipedia.org/wiki?curid=1782102", "title": "Hoop rolling", "text": "Hoop rolling\n\nHoop rolling, also called hoop trundling, is both a sport and a child's game in which a large hoop is rolled along the ground, generally by means of an object wielded by the player. The aim of the game is to keep the hoop upright for long periods of time, or to do various tricks.\n\nHoop rolling has been documented since antiquity in Africa, Asia and Europe. Played as a target game, it is an ancient tradition widely dispersed among different societies. In Asia, the earliest records date from Ancient China, and in Europe from Ancient Greece.\n\nIn the West, the most common materials for the equipment have been wood and metal. Wooden hoops, driven with a stick about one foot long, are struck with the centre or the 2/3 point of the stick in order to ensure good progress. Metal hoops, instead of being struck, are often guided by a metal hook.\n\nA version of hoop rolling played as a target game is encountered as an ancient tradition among aboriginal peoples in many parts of the world. The game, known as hoop-and-pole, is ubiquitous throughout most of Africa. It is also found on other continents. In America, where it has been played by a great number of unrelated tribes and is known in English as hoop-and-stick or hoop-and-dart, the game has exhibited many variations of materials and size of implements and rules of play. It is postulated that its wide distribution is a factor of the rich symbolical possibilities of the game, rather than indicating radial diffusion from a single center of invention.\n\nThe Greeks referred to the hoop as the \"trochus\". Hoop rolling was practised in the gymnasium, and the prop was also used for tumbling and dance with different techniques. Although a popular form of recreation, hoop rolling was not featured in competition at the major sports festivals.\n\nHoops, also called \"krikoi,\" were probably made of bronze, iron, or copper, and were driven with a stick called the \"elater\". The hoop was sized according to the player, as it had to come up to the level of the chest. Greek vases generally show the elater as a short, straight stick. The sport was regarded as healthful, and was recommended by Hippocrates for strengthening weak constitutions. Even very young children would play with hoops.\n\nThe hoop thus held symbolic meanings in Greek myth and culture. A bronze hoop was one of the toys of the infant Dionysus, and hoop driving is an attribute of Ganymede, often depicted on Greek vase paintings from the 5th century BCE. Images of the hoop are sometimes presented in the context of ancient Greek pederastic tradition.\n\nThe Romans learned hoop driving from the Greeks and generally held the sport in high regard. The Latin term for hoop is also \"trochus,\" at times referred to as the \"Greek hoop.\" The stick was known as a \"clavis\" or \"radius,\" had the shape of a key, and was made of metal with a wooden handle. Roman hoops were fitted with metal rings that slid freely along the rim. According to Martial, this was done so that the tinkling of the rings would warn passers by of the hoop's approach: \"Why do these jingling rings move about upon the rolling wheel? In order that the passers-by may get out of the way of the hoop.\"(14. CLXIX) He also indicates that the metal tires of wooden cart wheels could be used as hoops: \"A wheel must be protected. You make me a useful present. It will be a hoop to children, but to me a tyre for my wheel.\"(14. CLXVIII) Martial also mentions the sport was practised by Sarmatian boys, who rolled their hoops on the frozen Danube river. According to Strabo, one of the popular Roman venues for practising the sport was the Campus Martius, which was large enough to accommodate a wide variety of activities.\n\nThe Roman game was to roll the hoop while throwing a spear or stick through it. For Romans, this was more an entertainment and military development, not a philosophical activity. Several ancient sources praise the sport. According to Horace, hoop driving was one of the manly sports. Ovid in his Tristia is more specific, putting the sport in the same category with horsemanship, javelin throwing and weapon practice: \"Usus equi nunc est, levibus nunc luditur armis, Nunc pila, nunc celeri volvitur orbe trochus.\" It was also presented as a virtue in the Distichs of Cato, which enjoin youth to \"Trocho lude; aleam fuge\" – \"Play with the hoop, flee the dice.\" A 2nd-century CE medical text by Antyllus, preserved in an anthology of Oribasius, Emperor Julian's physician, describes hoop rolling as a form of physical and mental therapy. Antyllus indicates that at first the player should roll the hoop maintaining an upright posture, but after warming up he can begin to jump and run through the hoop. Such exercises, he holds, are best done before a meal or a bath, as with any physical exercise.\n\nEarly 19th-century travellers saw children playing with hoops over much of Europe and beyond.\n\nThe game was also a common pastime of African village children on the Tanganyika plateau, and not long after it is recorded in the Freetown settler community. In China, the game may well go back to 1000 BCE or further. Christian missionaries encountered it there in the 19th century. Children in late Edo period Japan also were known to play the game.\n\nIn English the sport is known by several names, \"hoop and stick,\" \"bowling hoops,\" or \"gird and cleek\" in Scotland, where the gird is the hoop and the cleek, the stick.\n\nIn the west, around the end of the 19th century, the game was played by boys up to about twelve years of age. Hoops would at times have pairs of tin squares nailed to the inside of the circle, to jingle as the hoop was rolled. Up to a dozen such pairs of rattles might be placed around the rim of the hoop. Some preferred the ashen hoops, round on the outside and flat on the inside, to the ones made of iron, as the latter could break windows and hurt the legs of the passers by and horses.\n\nAmong the games played with the hoops—besides simply trundling them, which is a matter of driving them forward while keeping them upright—are hoop races, as well as games of dexterity. Among these are \"toll,\" in which the player has to drive his hoop between two stones placed two to three inches apart without touching either one. Another such game is \"turnpike\", in which one player drives the hoop between pairs of objects, such as bricks, at first placed so that the opening is about a foot wide, with each gate kept by a different player. After running all the gates, the openings are made smaller by one inch, and the player trundling the hoop runs the course again. The process repeats until he strikes the side of a gate, then he and the turnpike keeper switch places.\n\nConflict games such as \"hoop battle\" or \"tournament\" can also be played. For this game, boys organise into opposing teams that drive their hoops against each other with the aim of knocking down as many of the opponents' hoops as possible. Only those hoops which fall as a result of a strike by another hoop are counted out. In some parts of England, boys played a similar game called \"encounters,\" where two boys would drive their hoops against each other, with the one whose hoop was left standing being declared the winner.\n\nThe \"hoop hunt\" is yet another game, in which one or more hoops are allowed to roll down a hill, with the double aim of rolling as far as possible and then of locating the hoop wherever it may have ended up.\n\nIn England, children are known to have played the game as early as the 15th century. By the late 18th century, boys driving hoops in the London streets had become a nuisance, according to Joseph Strutt. Throughout the 1840s, a barrage of denunciations appeared in the papers against \"The Hoop Nuisance,\" in which their iron hoops were blamed for inflicting severe injuries to pedestrians' shins The London police attempted to eradicate the practice, confiscating the iron hoops of boys and girls trundling them through the streets and parks. That campaign, however, seems to have failed, as it was accompanied by renewed complaints about the increase of the nuisance.\n\nOther writers mocked the complainers as grumblers depriving the \"juvenile community\" of a healthy and harmless pastime that had been practised for hundreds of years \"without any apparent inconvenience to the public at large.\" The passion for passing laws was ridiculed: \"Enact, say our modern philosophers, enact. Pass statute after statute. Regulate with exquisite minuteness the cries of the baby in the cradle, the laughter of the hoop-trundling boy, the murmurrings of the toothless old man.\" In the 1860s, the anti-trundling campaign was taken up by Charles Babbage, who blamed the boys for driving iron hoops under horses' legs, with the result that the rider is thrown and very often the horse breaks a leg. Babbage achieved a certain notoriety in this matter, being denounced in debate in Commons in 1864 for \"commencing a crusade against the popular game of tip-cat and the trundling of hoops.\"\n\nThe fuss over boys playing with hoops reached halfway around the globe. In the Colony of Tasmania, boys trundling hoops were blamed for endangering horsemen and rending ladies' dresses, and the Hobart paper called for their banishment to the suburbs, bye-laws, and police attention.\n\nNot only schoolboys, but even graduate students at Cambridge enjoyed trundling hoops after their lectures. The practice, however, was brought to an end sometime before 1816, by means of a statute that forbade Masters of Arts to roll hoops or play marbles.\n\nBy the early 19th century, the game was already part of the standard physical education of girls, together with jumping rope and dumbbells. Girls from four to fourteen could be seen by the hundreds, trundling their hoops across the grass in the London parks. Though held to be common in the early years of the 19th century, the simplicity and innocence of those years was alleged to have been replaced by the 1850s with a precocious maturity, where \"Instead of trundling hoops, urchins smoke cigars.\"\n\nIn the mid-19th century, bent ash was favoured as material for making wooden hoops. In early 20th-century England, girls played with a wooden hoop driven with a wooden stick, while boys' hoops were made of metal and the sticks were key-shaped and also made of metal. In some locations, hoops with spokes and bells were available in stores, but they were often disdained by boys.\n\nA great number of widely separated Native American peoples play or played an ancient target-shooting version of hoop rolling currently known as chunkey. Though the forms of the game exhibited great variation, generally certain elements were present, namely a prepared terrain over which a disc or hoop was rolled at high speed, at which implements similar to spears were thrown. The game, when played by adults, was often associated with gambling; and quite often, very valuable prizes, such as horses, exchanged hands. The game has been played by tribes such as the Arapaho, the Omaha, the Pawnee and many others.\n\nSince hoop and stick involves spear throwing, it is thought to predate the introduction of the bow and arrow that took place around 500 CE. In the California region in the 18th century, it was widespread and known as \"takersia.\" Canadian Inuit players divide into two groups. While the first group rolls the hoops—a large and a small one—the players in the other group attempt to throw spears through the hoops. The Cheyenne named two months of the year after the game: January is known as \"Ok sey' e shi his,\" \"Hoop-and-stick game moon,\" and February as \"Mak ok sey' i shi,\" \"Big hoop-and-stick game moon.\" Among the Blackfeet, children would play the game by throwing a feathered stick through the rolling hoop. Salish and Pend d'Oreilles youth played hoop and arrow games \"to become skillful at bringing down small game for the village\" in early spring, when the men were gone in search of large game.\n\nAmong the European settlers, hoop-rolling was a seasonal sport, seeing the greatest activity in the winter. Children, besides rolling the hoops, also tossed them back and forth, catching them on their sticks. In the 1830s, hoop trundling was seen as an activity so characteristic of the young that it was adopted by a fanatic sect in Kentucky whose members mimicked children's activities in order to gain access to heaven. Hoop driving was also seen as a remedy for the sedentary and overprotected lives led by many American girls of the mid-19th century. The game was popular with both girls and boys: in an 1898 survey of 1000 boys and 1000 girls in Massachusetts, both the girls and the boys named hoop and stick their favorite toy. In Ohio, the wood of the American elm (\"Ulmus americana\") was particularly valued for making hoop-poles.\n\nAt Bryn Mawr College, Wellesley College, and Wheaton College, the Hoop Rolling Contest is an annual spring tradition that dates back to 1895, and is only open to graduating seniors on that college's May Day celebration.\n\n"}
{"id": "41150204", "url": "https://en.wikipedia.org/wiki?curid=41150204", "title": "Karnataka Forest Service", "text": "Karnataka Forest Service\n\nKarnataka Forest Service (abbreviated as K.F.S) or known as State Forest Service (abbreviated as S.F.S) is awarded to a person who is selected in the K.F.S exam conducted by Karnataka Public Service Commission (KPSC). Selected officers will be appointed by the Government of Karnataka as The Assistant Conservator of Forests after completing the training period of 2 years at Central Academy for State Forest Service Dehradun or Central Academy for State Forest Service Coimbatore and Completing the probationary period of 2 years as The Range Forest Officer. The post of Assistant Conservator of Forests is equivalent to the post of Assistant commissioner and also equivalent to the post of Assistant Commissioner of Police. The officer who is recruited as Assistant Conservator of Forests is a person entrusted with responsibility to manage the forests, environment, and wildlife of the concerned Sub-Division and he will be assisted by the officers belonging to Karnataka Forest Subordinate Service.\n\n"}
{"id": "58665770", "url": "https://en.wikipedia.org/wiki?curid=58665770", "title": "List of pipeline accidents in the United States in 2015", "text": "List of pipeline accidents in the United States in 2015\n\nThe following is a list of pipeline accidents in the United States in 2015. It is one of several lists of U.S. pipeline accidents. See also list of natural gas and oil production accidents in the United States.\n\nThis is not a complete list of all pipeline accidents. For natural gas alone, the Pipeline and Hazardous Materials Safety Administration (PHMSA), a United States Department of Transportation agency, has collected data on more than 3,200 accidents deemed serious or significant since 1987.\n\nA \"significant incident\" results in any of the following consequences:\n\nPHMSA and the National Transportation Safety Board (NTSB) post incident data and results of investigations into accidents involving pipelines that carry a variety of products, including natural gas, oil, diesel fuel, gasoline, kerosene, jet fuel, carbon dioxide, and other substances. Occasionally pipelines are repurposed to carry different products.\n\n"}
{"id": "74908", "url": "https://en.wikipedia.org/wiki?curid=74908", "title": "Louisiana Story", "text": "Louisiana Story\n\nLouisiana Story (1948) is a 78-minute black-and-white American film. Although the events and characters depicted are fictional and the film was commissioned by the Standard Oil Company to promote its drilling ventures in the Louisiana bayoux, it is often misidentified as a documentary film, when in fact, it is a docufiction. The script was written by Frances H. Flaherty and Robert J. Flaherty, directed by Robert J. Flaherty. \n\nThe film deals with the adventures of a young Cajun boy and his pet raccoon, who live a somewhat idyllic existence playing in the bayous of Louisiana. A sub-plot involves his elderly father's allowing an oil company to drill for oil in the inlet that runs behind their house. A completely assembled miniature oil rig on a slender barge is towed into the inlet from connecting narrow waterways. Although there is a moment of crisis when the rig strikes a gas pocket, most of this is dealt with swiftly and off-camera, and the barge, rig, and friendly drillers depart expeditiously, leaving behind a phenomenally clean environment and a wealthy Cajun family.\n\nConflict and action for the plot is provided by the presence of a giant alligator in the area, which is believed to have eaten the pet raccoon and which is hunted in revenge. There is no individual or organized resistance to the incursion of the oil seekers, even after the (brief, offscreen) disaster, who are unequivocally portrayed as friendly, progressive humanitarians.\n\nThe boy, named in the film as Alexander Napoleon Ulysses Le Tour but in the credits just identified as \"the boy\", was played by Joseph Boudreaux. The film was photographed by Richard Leacock and edited by Helen van Dongen, who were also the associate producers. Original release was through independent film distributor Lopert Films.\n\nThe film was shot on location in the Louisiana bayou country, using local residents for actors. However, none of the members of the Cajun family (boy, father and mother) were actually related, and the film does not deal with Cajun culture, the reality of the hard lives of the Cajun people, or with the mechanics of drilling for oil. The story itself is completely fictional.\n\nIn 1952, it was reissued by an exploitation film outfit with a new title, \"Cajun\", on the bottom-half of a double-bill with another film called \"Watusi\".\n\nThe film was nominated for an Academy Award for Best Writing, Motion Picture Story in 1948. In 1949, Virgil Thomson won the Pulitzer Prize for Music for his score to the film (which is based on a famous field tape of indigenous Cajun musicians and was performed by the Philadelphia Symphony). Through 2016, this has remained the only Pulitzer Prize awarded for a film score. In 1994, \"Louisiana Story\" was selected for preservation in the United States National Film Registry by the Library of Congress as being \"culturally, historically, or aesthetically significant\". The movie was also in the top 10 of the first British Film Institute's Sight and Sound poll in 1952.\nThe film is recognized by American Film Institute in these lists:\n\n\n"}
{"id": "23475491", "url": "https://en.wikipedia.org/wiki?curid=23475491", "title": "Luttinger–Kohn model", "text": "Luttinger–Kohn model\n\nA flavor of the k·p perturbation theory used for calculating the structure of multiple, degenerate electronic bands in bulk and quantum well semiconductors. The method is a generalization of the single band k·p theory.\n\nIn this model the influence of all other bands is taken into account by using Löwdin's perturbation method.\n\nAll bands can be subdivided into two classes:\n\n\nThe method concentrates on the bands in \"Class A\", and takes into account \"Class B\" bands perturbatively.\n\nWe can write the perturbed solution formula_1 as a linear combination of the unperturbed eigenstates formula_2:\n\nAssuming the unperturbed eigenstates are orthonormalized, the eigenequation are:\n\nwhere\n\nFrom this expression we can write:\n\nwhere the first sum on the right-hand side is over the states in class A only, while the second sum is over the states on class B. Since we are interested in the coefficients formula_7 for \"m\" in class A, we may eliminate those in class B by an iteration procedure to obtain:\n\nEquivalently, for formula_10 (formula_11):\n\nand\n\nWhen the coefficients formula_10 belonging to Class A are determined so are formula_15.\n\nThe Hamiltonian including the spin-orbit interaction can be written as:\n\nwhere formula_17 is the Pauli spin matrix vector. Substituting into the Schrödinger equation we obtain\n\nwhere\n\nand the perturbation Hamiltonian can be defined as\n\nThe unperturbed Hamiltonian refers to the band-edge spin-orbit system (for \"k\"=0). At the band edge, conduction band Bloch waves exhibit s-like symmetry, while valence band states are p-like (3-fold degenerate without spin). Let us denote these states as formula_21, and formula_22, formula_23 and formula_24 respectively. These Bloch functions can be pictured as periodic repetition of atomic orbitals, repeated at intervals corresponding to the lattice spacing. The Bloch function can be expanded in the following manner:\n\nwhere \"j' \" is in Class A and formula_26 is in Class B. The basis functions can be chosen to be\n\nUsing Löwdin's method, only the following eigenvalue problem needs to be solved\n\nwhere\n\nThe second term of formula_38 can be neglected compared to the similar term with p instead of k. Similarly to the single band case, we can write for formula_39\n\nWe now define the following parameters\n\nand the band structure parameters (or the Luttinger parameters) can be defined to be\n\nThese parameters are very closely related to the effective masses of the holes in various valence bands. formula_48 and formula_49 describe the coupling of the formula_22, formula_23 and formula_24 states to the other states. The third parameter formula_53 relates to the anisotropy of the energy band structure around the formula_54 point when formula_55.\n\nThe Luttinger-Kohn Hamiltonian formula_56 can be written explicitly as a 8X8 matrix (taking into account 8 bands - 2 conduction, 2 heavy-holes, 2 light-holes and 2 split-off)\n"}
{"id": "9506405", "url": "https://en.wikipedia.org/wiki?curid=9506405", "title": "Magnesioferrite", "text": "Magnesioferrite\n\nMagnesioferrite is a magnesium iron oxide mineral, a member of the magnetite series of spinels. \nMagnesioferrite crystallizes as black metallic octahedral crystals. It is named after its chemical composition of magnesium and ferric iron.\nThe density is 4.6 - 4.7 (average = 4.65), and the diaphaniety is opaque. Occurs as well-formed fine sized crystals or massive and granular. \nIts hardness is 6-6.5. It has a metallic luster and a dark red streak.\n\nIt occurs in fumaroles, as a result of combustion metamorphism and coal seam fires, in glass spherules related to meteorite impacts, and as accessory phase in kimberlites and carbonatites.\n\nIt has been reported from Vesuvius and Stromboli, Italy.\n"}
{"id": "43401773", "url": "https://en.wikipedia.org/wiki?curid=43401773", "title": "March 1969 nor'easter", "text": "March 1969 nor'easter\n\nThe March 1969 nor'easter was an extratropical cyclone that moved into the Gulf of Mexico on March 5, moving through southern Georgia, then deepened as it moved along the lower Eastern Seaboard, before swinging wide of New England and Atlantic Canada. Heavy snows fell across eastern Maryland, southern Delaware, and Martha's Vineyard in Massachusetts. It was a strong system, with maximum sustained winds of a central pressure close to while south of Atlantic Canada. The system then moved into the far northern Atlantic Ocean while splitting into two low pressure areas on March 10.\n\nLate on March 5, a frontal wave moved offshore Galveston, Texas. The system moved along the Gulf coast on March 6. Gale-force winds were confined to behind its prefrontal squall line on March 6. The cyclone moved along the Florida-Georgia border with a central pressure down to . The low tracked up the Eastern Seaboard from the late on the 6th through the morning of the 7th while strengthening quickly, with its central pressure falling to , then with storm-force winds offshore Cape Charles, Virginia. The cyclone tracked offshore New England. By the morning of the 8th, the cyclone's center pressure had fallen to while the system developed maximum sustained winds of south of Newfoundland. During the day, the cyclone absorbed a previously strong cyclone to its north near Anticosti Island. The system slowly weakened while moving slowly northeast offshore eastern Newfoundland on the 9th and 10th, losing hurricane-force winds on the 9th. The system split into two distinct low pressure centers on 10th as it approached southern Greenland.\n\nThe cyclone's pre-frontal squall line moved into western Florida between 4:30 and 6:00 p.m. on March 6. Winds gusted to between and across Sarasota and Levy counties. Tides rose to between and above normal. Damage caused was between $50,000 and $500,000. A heavy wet snow fell across the Eastern Shore of Maryland and the southern half of Delaware from late on March 6 into March 7, up to a depth of in Maryland and in Lewes, Delaware. At Martha's Vineyard, heavy snow and winds up to created snow drifts exceeding in height.\n"}
{"id": "33674894", "url": "https://en.wikipedia.org/wiki?curid=33674894", "title": "Mt Stuart Wind Farm", "text": "Mt Stuart Wind Farm\n\nThe Mt Stuart Wind Farm is a wind farm in New Zealand constructed by Pioneer Generation. It is located 15 kilometres west of Milton in the Otago region of the South Island.\n\nResource consents were granted in February 2010. Construction began in April 2011\nand the nine turbine wind farm was commissioned in December 2012.\n\n\n"}
{"id": "32344175", "url": "https://en.wikipedia.org/wiki?curid=32344175", "title": "Mutriku Breakwater Wave Plant", "text": "Mutriku Breakwater Wave Plant\n\nThe Mutriku Breakwater Wave Plant is a wave power plant commissed by\nthe Basque Energy Agency in the bay of Mutriku in the Bay of Biscay and is the world's first breakwater wave power plant with a multiple turbine arrangement. The plant has a capacity of 300 kW from 16 turbo generator sets. It was inaugurated on July 8, 2011.\n"}
{"id": "12323291", "url": "https://en.wikipedia.org/wiki?curid=12323291", "title": "New York City steam system", "text": "New York City steam system\n\nThe New York City steam systems include Con Edison’s Steam Operations, and other smaller steam systems that provide steam to New York University and Columbia University. Many individual buildings in New York have their own steam systems.\n\nCon Edison’s Steam Operations is a district heating system which takes steam produced by steam generating stations and carries it under the streets of Manhattan to heat and cool high rise buildings and businesses. Some New York businesses and facilities also use the steam for cleaning and disinfection.\n\nThe New York Steam Company began providing service in lower Manhattan on March 3, 1882. Today, Consolidated Edison operates the largest commercial steam system in the world. It's bigger than the next nine put together. The organization within Con Edison that is responsible for the system's operation is known as Steam Operations, providing steam service to over 1,700 customers and serving commercial and residential establishments in Manhattan from Battery Park to 96th Street uptown on the West side and 89th Street on the East side of Manhattan. Roughly of steam flow through the system every year.\n\nSteam provides heat and cooling to many buildings in New York. The steam system provides humidity to art museums, steam cleaning for restaurants to clean dishes, and other uses.\n\nApproximately 30% of the ConEd steam system’s installed capacity and 50% of the annual steam generated comes from cogeneration. Cogeneration and Heat Recovery Steam Generation (HRSG) significantly increases the fuel efficiency of cogenerated electricity and thereby reduces the emission of pollutants, such as NOx, sulfur dioxide, carbon dioxide, and particulate matter, and reduces the city's carbon footprint. Con Edison is promoting the use of steam for cooling in the summer months, something that can be accomplished with the installation of absorption chillers. Such trigeneration systems reduce peak electrical loads and save construction costs associated with expanding electrical infrastructure.\n\nClouds of condensation can sometimes be seen rising from manholes in Manhattan through orange and white \"chimneys\". This can be caused by external water being boiled by contact with the steam pipes or by leaks in the steam system itself.\n\nAt least 12 steam pipe explosions have occurred in New York City since 1987. The most recent incident was the 2018 Steam Pipe explosion which occurred in the Flatiron District and forced the evacuation of 49 buildings. The explosion released concrete, asphalt, \"asbestos-containing material\" and mud into the air. The asbestos cleared out of the air to safe-level. A previous incident was the 2007 New York City steam explosion, and another on June 28, 1996, at a plant on East 75th Street.\n\n\nIn New York City, many individual buildings use either a hot water or a steam system for heating, and will have a boiler in the basement. A boiler is an enclosed vessel or tank that heats water using oil or gas.\n\nA steam boiler will usually keep the heat at 180 degrees F, then when the thermostat indicates that heat is needed it will increase the temperature to above the boiling point, 212 degrees F. This will send steam, which is lighter than air, to rise through pipes into the building’s radiators. Many thermostats measures outdoor air temperature instead of room temperature to determine — this is less efficient\n\nThere are differences between hot water heat and steam heat. Steam heat is noisier, but it delivers heat quicker. Steam systems can produce more uneven heat, and the radiators are generally larger.\n\nThe noise known as a “steam-hammer” sounds like someone hammering on a pipe. It is caused when water condenses and is trapped in a horizontal section of pipe so it cannot drain back to the boiler. Then when the system is next turned on, this water is hurled by the steam into the pipe fittings, creating the loud bang.\n\n\n"}
{"id": "1049624", "url": "https://en.wikipedia.org/wiki?curid=1049624", "title": "Phosphoric acid fuel cell", "text": "Phosphoric acid fuel cell\n\nPhosphoric acid fuel cells (PAFC) are a type of fuel cell that uses liquid phosphoric acid as an electrolyte. They were the first fuel cells to be commercialized. Developed in the mid-1960s and field-tested since the 1970s, they have improved significantly in stability, performance, and cost. Such characteristics have made the PAFC a good candidate for early stationary applications.\n\nElectrolyte is highly concentrated or pure liquid phosphoric acid (HPO) saturated in a silicon carbide matrix (SiC). Operating range is about 150 to 210 °C. The electrodes are made of carbon paper coated with a finely dispersed platinum catalyst.\n\nAnode reaction: 2H(g) → 4H + 4e‾\n\nCathode reaction: O(g) + 4H + 4e‾ → 2HO\n\nOverall cell reaction: 2 H + O → 2HO\n\nAt an operating range of 150 to 200 °C, the expelled water can be converted to steam for air and water heating (combined heat and power). This potentially allows efficiency increases of up to 70%.\nPAFCs are CO-tolerant and even can tolerate a CO concentration of about 1.5%, which broadens the choice of fuels they can use. If gasoline is used, the sulfur must be removed. At lower temperatures phosphoric acid is a poor ionic conductor, and CO poisoning of the platinum electro-catalyst in the anode becomes severe. However, they are much less sensitive to CO than PEFCs and AFCs.\n\nDisadvantages include rather low power density and aggressive electrolyte.\n\nPAFC have been used for stationary power generators with output in the 100 kW to 400 kW range and are also finding application in large vehicles such as buses.\n\nMajor manufacturers of PAFC technology include Doosan Fuel Cell America Inc. (formerly ClearEdge Power & UTC Power) and Fuji Electric. \n\nIndia's DRDO has developed PAFC for air-independent propulsion for integration into their Kalvari-class submarines.\n\n\n"}
{"id": "57684", "url": "https://en.wikipedia.org/wiki?curid=57684", "title": "Project Pluto", "text": "Project Pluto\n\nProject Pluto was a United States government program to develop nuclear-powered ramjet engines for use in cruise missiles. Two experimental engines were tested at the United States Department of Energy Nevada Test Site (NTS) in 1961 and 1964.\n\nOn January 1, 1957, the U.S. Air Force and the U.S. Atomic Energy Commission selected the Lawrence Radiation Laboratory (the predecessor of the Lawrence Livermore National Laboratory's, LLNL) to study the feasibility of applying heat from nuclear reactors to ramjet engines. This research became known as \"Project Pluto\". The work was directed by Dr. Ted Merkle, leader of the laboratory's R-Division.\nOriginally carried out at Livermore, California, the work was moved to new facilities constructed for $1.2 million on of Jackass Flats at the NTS, known as Site 401. The complex consisted of of roads, critical-assembly building, control building, assembly and shop buildings, and utilities. Also required for the construction was of oil well casing, which was necessary to store the approximately of pressurized air used to simulate ramjet flight conditions for Pluto.\n\nThe principle behind the nuclear ramjet was relatively simple: motion of the vehicle pushed air in through the front of the vehicle (ram effect), a nuclear reactor heated the air, and then the hot air expanded at high speed out through a nozzle at the back, providing thrust.\n\nThe notion of using a nuclear reactor to heat the air was fundamentally new. Unlike commercial reactors, which are surrounded by concrete, the Pluto reactor had to be small and compact enough to fly, but durable enough to survive a trip to a potential target. The nuclear engine could, in principle, operate for months, so a Pluto cruise missile could be left airborne for a prolonged time before being directed to carry out its attack.\n\nThe success of this project would depend upon a series of technological advances in metallurgy and materials science. Pneumatic motors necessary to control the reactor in flight had to operate while red-hot and in the presence of intense radiation. The need to maintain supersonic speed at low altitude and in all kinds of weather meant that the reactor, code-named \"Tory\", had to survive high temperatures and conditions that would melt the metals used in most jet and rocket engines. Ceramic fuel elements would have to be used; the contract to manufacture the 500,000 pencil-sized elements was given to the Coors Porcelain Company.\n\nThe proposed use for nuclear-powered ramjets would be to power a cruise missile, called SLAM, for Supersonic Low Altitude Missile. In order to reach ramjet speed, it would be launched from the ground by a cluster of conventional rocket boosters. Once it reached cruising altitude and was far away from populated areas, the nuclear reactor would be made critical. Since nuclear power gave it almost unlimited range, the missile could cruise in circles over the ocean until ordered \"down to the deck\" for its supersonic dash to targets in the Soviet Union. The SLAM, as proposed, would carry a payload of many nuclear weapons to be dropped on multiple targets, making the cruise missile into an unmanned bomber. After delivering all its warheads, the missile could then spend weeks flying over populated areas at low altitudes, causing tremendous ground damage with its shock wave and fallout. When it finally lost enough power to fly, and crash-landed, the engine would have a good chance of spewing deadly radiation for months to come.\n\nOn May 14, 1961, the world's first nuclear ramjet engine, \"Tory-IIA\", mounted on a railroad car, roared to life for a few seconds. Three years later, \"Tory-IIC\" was run for five minutes at full power. Despite these and other successful tests, the Pentagon, sponsor of the \"Pluto project\", had second thoughts. The weapon was considered \"too provocative\", and it was believed that it would compel the Soviets to construct a similar device, against which there was no known defense. Intercontinental ballistic missile technology had proven to be more easily developed than previously thought, reducing the need for such highly capable cruise missiles. On July 1, 1964, seven years and six months after it was started, \"Project Pluto\" was canceled.\n\n\n\n"}
{"id": "1956416", "url": "https://en.wikipedia.org/wiki?curid=1956416", "title": "Renewable Energy Certificate (United States)", "text": "Renewable Energy Certificate (United States)\n\nRenewable Energy Certificates (RECs), also known as Green tags, Renewable Energy Credits, Renewable Electricity Certificates, or Tradable Renewable Certificates (TRCs), are tradable, non-tangible energy commodities in the United States that represent proof that 1 megawatt-hour (MWh) of electricity was generated from an eligible renewable energy resource (renewable electricity) and was fed into the shared system of power lines which transport energy. Solar renewable energy certificates (SRECs) are RECs that are specifically generated by solar energy. Renewable Energy Certificates provide a mechanism for the purchase of renewable energy that is added to and pulled from the electrical grid. The updated Greenhouse Gas Protocol Scope 2 Guidance guarantees of origin, RECs and I-RECs as mainstream instruments for documenting and tracking electricity consumed from renewable sources. \n\nThese certificates can be sold and traded or bartered, and the owner of the REC can claim to have purchased renewable energy. According to the U.S. Department of Energy's Green Power Network, RECs represent the environmental attributes of the power produced from renewable energy projects and are sold separately from commodity electricity. While traditional carbon emissions trading programs use penalties and incentives to achieve established emissions targets, RECs simply incentivize carbon-neutral renewable energy by providing a production subsidy to electricity generated from renewable sources. \n\nA green energy provider (such as a wind farm) is credited with one REC for every 1,000 kWh or 1 MWh of electricity it produces (for reference, an average residential customer consumes about 800 kWh in a month). A certifying agency gives each REC a unique identification number to make sure it doesn't get double-counted. The green energy is then fed into the electrical grid (by mandate), and the accompanying REC can then be sold on the open market. \"Retirement occurs when a Renewable Energy Certificate (REC) is used by the owner of the REC. Use of the REC may include, but is not limited to, (1) use of the REC by an end-use customer, marketer, generator, or utility to comply with a statutory or regulatory requirement, (2) a public claim associated with a purchase of RECs by an end-use customer, or (3) the sale of any component attributes of a REC for any purpose. Once a REC is retired, it may not be sold, donated, or transferred to any other party. No party other than the owner may make claims associated with retired RECs.\"\n\nEnergy from any grid-tied source is bought and sold with contracts specifying the generator and purchaser. In the trade of renewable energy, RECs specify that a unit of renewable energy was generated. Because once electricity is placed on the electrical grid it mixes with electricity from multiple sources and becomes indistinguishable, RECs are used to track the ownership of environmental and social benefits of the renewable energy. The majority of RECs are sold separately from the electricity itself. In these cases, the electricity is sold as \"null\" energy without its environmental and social benefits, as if it were generated by non-renewable resources such as coal or natural gas. When RECs are purchased in combination with non-renewable electricity this constitutes the legal purchase of renewable energy. This is how electrical grid connected renewable energy is traded in the U.S. Grid-connected renewable energy is used by electric utility companies in meeting their regulatory requirements and by individuals and businesses wishing to lessen their environmental impact. RECs allow for purchasers to support renewable energy generation and allows the economic forces of supply and demand to spur the further development of renewable energy generation.\n\nThere are two main markets for renewable energy certificates in the United States – compliance markets and voluntary markets. \n\nCompliance markets are created by a policy that exists in 29 U.S. states, plus the District of Columbia and Puerto Rico, called Renewable Portfolio Standard. In these states, the electric companies are required to supply a certain percent of their electricity from renewable generators by a specified year. For example, in California the law is 33% renewable by 2020, whereas New York has a 24% requirement by 2013. Electric utilities in these states demonstrate compliance with their requirements by purchasing RECs; in the California example, the electric companies would need to hold RECs equivalent to 33% of their electricity sales.\n\nVoluntary markets are ones in which customers choose to buy renewable power out of a desire to use renewable energy. Most corporate and household purchases of renewable energy are voluntary purchases. Renewable energy generators located in states that do not have a Renewable Portfolio Standard can sell their RECs to voluntary buyers, usually at a cheaper price than compliance market RECs.\n\nRECs can be traded directly from buyer to seller, but third party marketers, brokers, or asset managers are commonly found in the marketplace. Renewable generation facilities will often sell their credits to these entities, who then resell them on the market at a later date.\n\nTexas developed the first comprehensive RECs system in the U.S., a web-based platform that provides for the issuance, registration, trade, and retirement of RECs. The Texas REC Program, which only tracks renewable energy certificates, started operating in July 2001. \n\nPrices depend on many factors, such as the vintage year the RECs were generated, location of the facility, whether there is a tight supply/demand situation, whether the REC is used for RPS compliance, even the type of power created. Solar renewable energy certificates or SRECs, for example, tend to be more valuable in the 16 states that have set aside a portion of the RPS specifically for solar energy. This differentiation is intended to promote diversity in the renewable energy mix which in an undifferentiated, competitive REC market, favors the economics and scale achieved by wind farms. \n\nIn the United States, spot prices for SRECs generally decreased from 2010 to 2014. In New Jersey, the spot price for a 2010 SREC was $665.04 in July 2010 and about $160 in May 2014 for SRECs generated in different years. In Delaware, the spot price for a 2010 SREC was $255 in July 2010 and about $50 in May 2014 for SRECs generated in different years. Rates for 2015 to 2017 RECS purchased have averaged between $0.15—$0.045 per kWh produced.\n\nIn Canada, 2008–09 BCHydro offers $3 /MWh for \"green attributes\", for long-term contracts, 20 plus years. Many Independent Power Producers (IPPs) believe that this is much less than \"fair market value\", but have no alternative.\n\nWhile the value of RECs fluctuate, most sellers are legally obligated to \"deliver\" RECs to their customers within a few months of their generation date. Other organizations will sell as many RECs as possible and then use the funds to guarantee a specific fixed price per MWh generated by a future wind farm, for example, making the building of the wind farm a financially viable prospect. The income provided by RECs, and a long-term stabilized market for tags can generate the additional incentive needed to build renewable energy plants.\n\nRECs are known under functionally equivalent names, such as Green Tags or Tradable Renewable Certificates (TRCs), depending on the market. The U.S. currently does not have a national registry of RECs issued. The Center for Resource Solutions administers a voluntary program which ensures that RECs are properly accounted for and that no double counting takes place. Under the Green-e Energy program, participants are required to submit to an annual Verification Process Audit of all eligible transactions to ensure the RECs meet the requirements for certification. The certification process requires 3rd party verification to be performed by an independent certified public accountant or a certified internal auditor. CRS maintains a list of auditors who meet the criteria to be listed on the program website. Increasingly RECs are being assigned unique ID numbers and tracked through regional tracking systems/registries such as WREGIS, NEPOOL, GATS, ERCOT, NARR, MIRECS, NRTEC, NC-RETS and M-RETS.\n\nThe following generation technologies qualify as producers of RECs:\n\n\n\"Additionality\" in the context of greenhouse gas (GHG) regulations means that a purchased renewable energy certificate introduces new renewable energy onto the electricity grid beyond what would have happened without the project or \"business as usual\". The U.S. Environmental Protection Agency (EPA) favors performance based measures of additionality, such as the megawatt hour (MWh) equivalent per REC.\n\nWhereas air and water pollution travels across state and national boundaries irrespective of its origin, the value of RECs and the emergence of RECs markets depend very much on the markets created state by state through legislative action to mandate a Renewable Portfolio Standard. Such a balkanized approach to establishing RECs markets and incentives state by state creates issues of equity as some states could legitimately claim that their neighboring states (and their electricity consumers) with voluntary RPS are operating as free riders of pollution prevention, paid for by states (and their electricity consumers) with mandatory RPS. We can learn from EPA's SOx and NOx cap and trade program regarding how the principle of additionality with a national standard provided a benchmark for measuring and validating the commodification of pollution prevention credits that lead to market-driven initiatives with proven results in improving regional and national air quality.\n\nIn states with a Renewable Portfolio Standard, a RECs purchase enables the utility company to meet its minimum renewable electricity percentage without having to install that renewable generating capacity itself, regardless of the source of generating renewable energy. By analogy, in the EPA cap and trade program, a \"clean\" utility in one state can sell its NOx credits to a \"dirty\" utility in another state that would otherwise have to install additional smokestack scrubbers.\n\nThe United States Environmental Protection Agency claims to have the highest percentage use of green power of any federal agency. In 2007, it offset the electricity use of 100% of its offices. The Air Force is the largest purchaser in the US government in absolute terms, purchasing 899,142 MWh worth of RECs. Among colleges and universities, the University of Pennsylvania in Philadelphia is the largest purchaser of RECs, buying 192,727 MWh of RECs from wind power. The corporate leader is Intel, with 1,302,040 MWh purchased in 2007, and the largest purchaser among retailers is Whole Foods, which purchased 509,104 MWH, or enough RECs to offset 100% of its electricity needs.\n\nNote that research shows that RECs purchased and retired voluntarily in the United States (i.e., not for compliance with a Renewable Portfolio Standard) do not lead to any significant additional renewable energy investment or generation.\n\n"}
{"id": "3823804", "url": "https://en.wikipedia.org/wiki?curid=3823804", "title": "Ring flip", "text": "Ring flip\n\nRing flipping (also known as ring inversion or ring reversal) is a phenomenon involving the interconversion (by rotation) about single bonds of cyclic conformers having equivalent ring shapes but not necessarily equivalent spatial positions of substituent atoms.\n\nThe six-membered ring of the alkane, cyclohexane, has the 'chair' as its preferred conformation and through proton NMR it would be expected that the produced spectrum would show the different sorts of protons (axial and equatorial) resonating at different frequencies, and so two would be seen, due to the protons being in slightly different chemical environments. However, only one signal can be seen as the two isomers are rapidly interconverting. This also occurs with monosubstituted cyclohexanes. This changes with the use of low temperature, as the two isomers visible interconvert much more slowly than at room temperature.\n\nWhen cyclohexane undergoes a ring inversion, all axial bonds become equatorial, or vice versa. The half-chair conformation is the energy maximum when proceeding from the 'chair' to the 'twisted boat' conformation. The 'true boat' conformation is the energy maximum for the interchanging of the two mirror image twist boat conformers, the second of which is converted to the other chair confirmation through another half-chair.\n\nHowever, the chemical reactivity of cyclohexane, is inconsistent with two types of hydrogens in a stable form of the molecule (for example, there is only one monochlorocyclohexane, not two, as would be predicted if axial and equatorial hydrogens could be replaced independently), and this is due to the presence of rapid ring inversion as explained above. In substituted cyclohexanes, steric repulsion can be minimised by the substituents occupying positions along the equatorial plane rather than axial positions, and this would also lead to greater stability in the molecule.\n\n"}
{"id": "29268997", "url": "https://en.wikipedia.org/wiki?curid=29268997", "title": "Solar cell research", "text": "Solar cell research\n\nThere are currently many research groups active in the field of photovoltaics in universities and research institutions around the world. This research can be categorized into three areas: making current technology solar cells cheaper and/or more efficient to effectively compete with other energy sources; developing new technologies based on new solar cell architectural designs; and developing new materials to serve as more efficient energy converters from light energy into electric current or light absorbers and charge carriers.\n\nOne way of reducing the cost is to develop cheaper methods of obtaining silicon that is sufficiently pure. Silicon is a very common element, but is normally bound in silica, or silica sand. Processing silica (SiO) to produce silicon is a very high energy process - at current efficiencies, it takes one to two years for a conventional solar cell to generate as much energy as was used to make the silicon it contains. More energy efficient methods of synthesis are not only beneficial to the solar industry, but also to industries surrounding silicon technology as a whole.\n\nThe current industrial production of silicon is via the reaction between carbon (charcoal) and silica at a temperature around 1700 °C. In this process, known as carbothermic reduction, each tonne of silicon (metallurgical grade, about 98% pure) is produced with the emission of about 1.5 tonnes of carbon dioxide.\n\nSolid silica can be directly converted (reduced) to pure silicon by electrolysis in a molten salt bath at a fairly mild temperature (800 to 900 °C). While this new process is in principle the same as the FFC Cambridge Process which was first discovered in late 1996, the interesting laboratory finding is that such electrolytic silicon is in the form of porous silicon which turns readily into a fine powder, with a particle size of a few micrometers, and may therefore offer new opportunities for development of solar cell technologies.\n\nAnother approach is also to reduce the amount of silicon used and thus cost, is by micromachining wafers into very thin, virtually transparent layers that could be used as transparent architectural coverings. The technique involves taking a silicon wafer, typically 1 to 2 mm thick, and making a multitude of parallel, transverse slices across the wafer, creating a large number of slivers that have a thickness of 50 micrometres and a width equal to the thickness of the original wafer. These slices are rotated 90 degrees, so that the surfaces corresponding to the faces of the original wafer become the edges of the slivers. The result is to convert, for example, a 150 mm diameter, 2 mm-thick wafer having an exposed silicon surface area of about 175 cm per side into about 1000 slivers having dimensions of 100 mm × 2 mm × 0.1 mm, yielding a total exposed silicon surface area of about 2000 cm per side. As a result of this rotation, the electrical doping and contacts that were on the face of the wafer are located at the edges of the sliver, rather than at the front and rear as in the case of conventional wafer cells. This has the interesting effect of making the cell sensitive from both the front and rear of the cell (a property known as bifaciality). Using this technique, one silicon wafer is enough to build a 140 watt panel, compared to about 60 wafers needed for conventional modules of same power output.\n\nThese structures make use of some of the same thin-film light absorbing materials but are overlain as an extremely thin absorber on a supporting matrix of conductive polymer or mesoporous metal oxide having a very high surface area to increase internal reflections (and hence increase the probability of light absorption). Using nanocrystals allows one to design architectures on the length scale of nanometers, the typical exciton diffusion length. In particular, single-nanocrystal ('channel') devices, an array of single p-n junctions between the electrodes and separated by a period of about a diffusion length, represent a new architecture for solar cells and potentially high efficiency.\n\nThin-film photovoltaic cells can use less than 1% of the expensive raw material (silicon or other light absorbers) compared to wafer-based solar cells, leading to a significant price drop per Watt peak capacity. There are many research groups around the world actively researching different thin-film approaches and/or materials.\n\nOne particularly promising technology is crystalline silicon thin films on glass substrates. This technology combines the advantages of crystalline silicon as a solar cell material (abundance, non-toxicity, high efficiency, long-term stability) with the cost savings of using a thin-film approach.\n\nAnother interesting aspect of thin-film solar cells is the possibility to deposit the cells on all kind of materials, including flexible substrates (PET for example), which opens a new dimension for new applications.\n\nAs of December 2014, the world record for solar cell efficiency at 46% was achieved by using multi-junction concentrator solar cells, developed from collaboration efforts of Soitec, , France together with Fraunhofer ISE, Germany.\n\nThe National Renewable Energy Laboratory (NREL) won one of \"R&D Magazine\"<nowiki>'</nowiki>s R&D 100 Awards for its Metamorphic Multijunction photovoltaic cell, an ultra-light and flexible cell that converts solar energy with record efficiency.\n\nThe ultra-light, highly efficient solar cell was developed at NREL and is being commercialized by Emcore Corp. of Albuquerque, N.M., in partnership with the Air Force Research Laboratories Space Vehicles Directorate at Kirtland Air Force Base in Albuquerque.\n\nIt represents a new class of solar cells with clear advantages in performance, engineering design, operation and cost. For decades, conventional cells have featured wafers of semiconducting materials with similar crystalline structure. Their performance and cost effectiveness is constrained by growing the cells in an upright configuration. Meanwhile, the cells are rigid, heavy and thick with a bottom layer made of germanium.\n\nIn the new method, the cell is grown upside down. These layers use high-energy materials with extremely high quality crystals, especially in the upper layers of the cell where most of the power is produced. Not all of the layers follow the lattice pattern of even atomic spacing. Instead, the cell includes a full range of atomic spacing, which allows for greater absorption and use of sunlight. The thick, rigid germanium layer is removed, reducing the cell's cost and 94% of its weight. By turning the conventional approach to cells on its head, the result is an ultra-light and flexible cell that also converts solar energy with record efficiency (40.8% under 326 suns concentration).\n\nThe invention of conductive polymers (for which Alan Heeger, Alan G. MacDiarmid and Hideki Shirakawa were awarded a Nobel prize) may lead to the development of much cheaper cells that are based on inexpensive plastics. However, organic solar cells generally suffer from degradation upon exposure to UV light, and hence have lifetimes which are far too short to be viable. The bonds in the polymers, are always susceptible to breaking up when radiated with shorter wavelengths. Additionally, the conjugated double bond systems in the polymers which carry the charge, react more readily with light and oxygen. So most conductive polymers, being highly unsaturated and reactive, are highly sensitive to atmospheric moisture and oxidation, making commercial applications difficult.\n\nExperimental non-silicon solar panels can be made of quantum heterostructures, e.g. carbon nanotubes or quantum dots, embedded in conductive polymers or mesoporous metal oxides. In addition, thin films of many of these materials on conventional silicon solar cells can increase the optical coupling efficiency into the silicon cell, thus boosting the overall efficiency. By varying the size of the quantum dots, the cells can be tuned to absorb different wavelengths. Although the research is still in its infancy, quantum dot modified photovoltaics may be able to achieve up to 42% energy conversion efficiency due to multiple exciton generation (MEG).\n\nMIT researchers have found a way of using a virus to improve solar cell efficiency by a third.\n\nMany new solar cells use transparent thin films that are also conductors of electrical charge. The dominant conductive thin films used in research now are transparent conductive oxides (abbreviated \"TCO\"), and include fluorine-doped tin oxide (SnO:F, or \"FTO\"), doped zinc oxide (e.g.: ZnO:Al), and indium tin oxide (abbreviated \"ITO\"). These conductive films are also used in the LCD industry for flat panel displays. The dual function of a TCO allows light to pass through a substrate window to the active light-absorbing material beneath, and also serves as an ohmic contact to transport photogenerated charge carriers away from that light-absorbing material. The present TCO materials are effective for research, but perhaps are not yet optimized for large-scale photovoltaic production. They require very special deposition conditions at high vacuum, they can sometimes suffer from poor mechanical strength, and most have poor transmittance in the infrared portion of the spectrum (e.g.: ITO thin films can also be used as infrared filters in airplane windows). These factors make large-scale manufacturing more costly.\n\nA relatively new area has emerged using carbon nanotube networks as a transparent conductor for organic solar cells. Nanotube networks are flexible and can be deposited on surfaces a variety of ways. With some treatment, nanotube films can be highly transparent in the infrared, possibly enabling efficient low-bandgap solar cells. Nanotube networks are p-type conductors, whereas traditional transparent conductors are exclusively n-type. The availability of a p-type transparent conductor could lead to new cell designs that simplify manufacturing and improve efficiency.\n\nDespite the numerous attempts at making better solar cells by using new and exotic materials, the reality is that the photovoltaics market is still dominated by silicon wafer-based solar cells (first-generation solar cells). This means that most solar cell manufacturers are currently equipped to produce this type of solar cells. Consequently, a large body of research is being done all over the world to manufacture silicon wafer-based solar cells at lower cost and to increase the conversion efficiencies without an exorbitant increase in production cost. The ultimate goal for both wafer-based and alternative photovoltaic concepts is to produce solar electricity at a cost comparable to currently market-dominant coal, natural gas, and nuclear power in order to make it the leading primary energy source. To achieve this it may be necessary to reduce the cost of installed solar systems from currently about US$1.80 (for bulk Si technologies) to about US$0.50 per Watt peak power. Since a major part of the final cost of a traditional bulk silicon module is related to the high cost of solar grade polysilicon feedstock (about US$0.4/Watt peak) there exists substantial drive to make Si solar cells thinner (material savings) or to make solar cells from cheaper upgraded metallurgical silicon (so called \"dirty Si\").\n\nIBM has a semiconductor wafer reclamation process that uses a specialized pattern removal technique to repurpose scrap semiconductor wafers to a form used to manufacture silicon-based solar panels. The new process was recently awarded the “2007 Most Valuable Pollution Prevention Award” from The National Pollution Prevention Roundtable (NPPR).\n\nResearchers at Idaho National Laboratory, along with partners at Lightwave Power Inc. in Cambridge, MA and Patrick Pinhero of the University of Missouri, have devised an inexpensive way to produce plastic sheets containing billions of nanoantennas that collect heat energy generated by the sun and other sources, which garnered two 2007 Nano50 awards. The company ceased operations in 2010. While methods to convert the energy into usable electricity still need to be developed, the sheets could one day be manufactured as lightweight \"skins\" that power everything from hybrid cars to computers and iPods with higher efficiency than traditional solar cells. The nanoantennas target mid-infrared rays, which the Earth continuously radiates as heat after absorbing energy from the sun during the day; also double-sided nanoantenna sheets can harvest energy from different parts of the Sun's spectrum. In contrast, traditional solar cells can only use visible light, rendering them idle after dark.\n\nJapan's National Institute of Advanced Industrial Science and Technology (AIST) has succeeded in developing a transparent solar cell that uses ultraviolet (UV) light to generate electricity but allows visible light to pass through it. Most conventional solar cells use visible and infrared light to generate electricity. Used to replace conventional window glass, the installation surface area could be large, leading to potential uses that take advantage of the combined functions of power generation, lighting and temperature control.\n\nThis transparent, UV-absorbing system was achieved by using an organic-inorganic heterostructure made of the p-type semiconducting polymer film deposited on a Nb-doped strontium titanate substrate. PEDOT:PSS is easily fabricated into thin films due to its stability in air and its solubility in water. These solar cells are only activated in the UV region and result in a relatively high quantum yield of 16% electron/photon. Future work in this technology involves replacing the strontium titanate substrate with a strontium titanate film deposited on a glass substrate in order to achieve a low-cost, large-area manufacture.\n\nSince then, other methods have been discovered to include the UV wavelengths in solar cell power generation. Some companies report using nano-phosphors as a transparent coating to turn UV light into visible light. Others have reported extending the absorption range of single-junction photovoltaic cells by doping a wide band gap transparent semiconductor such as GaN with a transition metal such as manganese.\n\nFlexible solar cell research is a research-level technology, an example of which was created at the Massachusetts Institute of Technology in which solar cells are manufactured by depositing photovoltaic material on flexible substrates, such as ordinary paper, using chemical vapor deposition technology. The technology for manufacturing solar cells on paper was developed by a group of researchers from the Massachusetts Institute of Technology with support from the National Science Foundation and the Eni-MIT Alliance Solar Frontiers Program.\n\nThree-dimensional solar cells that capture nearly all of the light that strikes them and could boost the efficiency of photovoltaic systems while reducing their size, weight and mechanical complexity. The new 3D solar cells, created at the Georgia Tech Research Institute, capture photons from sunlight using an array of miniature “tower” structures that resemble high-rise buildings in a city street grid. Solar3D, Inc. plans to commercialize such 3D cells, but its technology is currently patent-pending.\n\nLuminescent solar concentrators convert sunlight or other sources of light into preferred frequencies; they concentrate the output for conversion into desirable forms of power, such as electricity. They rely on luminescence, typically fluorescence, in media such as liquids, glasses, or plastics treated with a suitable coating or dopant. The structures are configured to direct the output from a large input area onto a small converter, where the concentrated energy generates photoelectricity. The objective is to collect light over a large area at low cost; luminescent concentrator panels can be made cheaply from materials such as glasses or plastics, while photovoltaic cells are high-precision, high-technology devices, and accordingly expensive to construct in large sizes.\n\nResearch is in progress at universities such as Radboud University Nijmegen and Delft University of Technology. For example, at Massachusetts Institute of Technology researchers have developed approaches for conversion of windows into sunlight concentrators for generation of electricity. They paint a mixture of dyes onto a pane of glass or plastic. The dyes absorb sunlight and re-emit it as fluorescence within the glass, where it is confined by internal reflection, emerging at the edges of the glass, where it encounters solar cells optimized for conversion of such concentrated sunlight. The concentration factor is about 40, and the optical design yields a solar concentrator that unlike lens-based concentrators, need not be directed accurately at the sun, and can produce output even from diffuse light. Covalent Solar is working on commercialization of the process.\n\nMetamaterials are heterogeneous materials employing the juxtaposition of many microscopic elements, giving rise to properties not seen in ordinary solids. Using these, it \"may\" become possible to fashion solar cells that are excellent absorbers over a narrow range of wavelengths. High absorption in the microwave regime has been demonstrated, but not yet in the 300-1100-nm wavelength regime.\n\nSome systems combine photovoltaic with thermal solar, with the advantage that the thermal solar part carries heat away and cools the photovoltaic cells. Keeping temperature down lowers the resistance and improves the cell efficiency.\n\nPentacene-based photovoltaics are claimed to improve the energy-efficiency ratio to up to 95%, effectively doubling the efficience of today's most efficient techniques.\n\nIntermediate band photovoltaics in solar cell research provides methods for exceeding the Shockley–Queisser limit on the efficiency of a cell. It introduces an intermediate band (IB) energy level in between the valence and conduction bands. Theoretically, introducing an IB allows two photons with energy less than the bandgap to excite an electron from the valence band to the conduction band. This increases the induced photocurrent and thereby efficiency. \n\nLuque and Marti first derived a theoretical limit for an IB device with one midgap energy level using detailed balance. They assumed no carriers were collected at the IB and that the device was under full concentration. They found the maximum efficiency to be 63.2%, for a bandgap of 1.95eV with the IB 0.71eV from either the valence or conduction band.\nUnder one sun illumination the limiting efficiency is 47%. \n\n"}
{"id": "580936", "url": "https://en.wikipedia.org/wiki?curid=580936", "title": "Sulfur trioxide", "text": "Sulfur trioxide\n\nSulfur trioxide (alternative spelling sulphur trioxide) is the chemical compound with the formula SO, with a relatively narrow liquid range. In the gaseous form, this species is a significant pollutant, being the primary agent in acid rain.\n\nIt is prepared on an industrial scale as a precursor to sulfuric acid.\n\nIn perfectly dry apparatus, sulfur trioxide vapor is invisible, and the liquid is transparent. However, it fumes profusely even in a relatively dry atmosphere (it has been used as a smoke agent) due to formation of a sulfuric acid mist. This vapor has no odor but is extremely corrosive.\n\nGaseous SO is a trigonal planar molecule of D symmetry, as predicted by VSEPR theory. SO belongs to the D point group.\n\nIn terms of electron-counting formalism, the sulfur atom has an oxidation state of +6 and a formal charge of +2. The Lewis structure consists of an S=O double bond and two S–O dative bonds without utilizing d-orbitals.\n\nThe electrical dipole moment of gaseous sulfur trioxide is zero. This is a consequence of the 120° angle between the S-O bonds.\n\nThe nature of solid SO is complex because structural changes are caused by traces of water.\n\nUpon condensation of the gas, absolutely pure SO condenses into a trimer, which is often called \"γ\"-SO. This molecular form is a colorless solid with a melting point of 16.8 °C. It adopts a cyclic structure described as [S(=O)(\"μ\"-O)].\n\nIf SO is condensed above 27 °C, then \"α\"-SO forms, which has a melting point of 62.3 °C. \"α\"-SO is fibrous in appearance. Structurally, it is the polymer [S(=O)(\"μ\"-O)]. Each end of the polymer is terminated with OH groups. \"β\"-SO, like the alpha form, is fibrous but of different molecular weight, consisting of an hydroxyl-capped polymer, but melts at 32.5 °C. Both the gamma and the beta forms are metastable, eventually converting to the stable alpha form if left standing for sufficient time. This conversion is caused by traces of water.\n\nRelative vapor pressures of solid SO are alpha < beta < gamma at identical temperatures, indicative of their relative molecular weights. Liquid sulfur trioxide has a vapor pressure consistent with the gamma form. Thus heating a crystal of \"α\"-SO to its melting point results in a sudden increase in vapor pressure, which can be forceful enough to shatter a glass vessel in which it is heated. This effect is known as the \"alpha explosion\".\n\nSO is aggressively hygroscopic. The heat of hydration is sufficient that mixtures of SO and wood or cotton can ignite. In such cases, SO dehydrates these carbohydrates.\n\nSO is the anhydride of HSO. Thus, the following reaction occurs:\n\nThe reaction occurs both rapidly and exothermically, too violently to be used in large-scale manufacturing. At or above 340 °C, sulfuric acid, sulfur trioxide, and water coexist in significant equilibrium concentrations.\n\nSulfur trioxide also reacts with sulfur dichloride to yield the useful reagent, thionyl chloride.\n\nSO is a strong Lewis acid readily forming crystalline complexes with pyridine, dioxane, and trimethylamine. These adducts can be used as sulfonating agents.\n\nSulfur trioxide can be prepared in the laboratory by the two-stage pyrolysis of sodium bisulfate. Sodium pyrosulfate is an intermediate product:\n\n\nIn contrast, KHSO does not undergo the same reaction.\n\nIndustrially SO is made by the contact process. Sulfur dioxide, which in turn is produced by the burning of sulfur or iron pyrite (a sulfide ore of iron). After being purified by electrostatic precipitation, the SO is then oxidised by atmospheric oxygen at between 400 and 600 °C over a catalyst. A typical catalyst consists of vanadium pentoxide (VO) activated with potassium oxide KO on kieselguhr or silica support. Platinum also works very well but is too expensive and is poisoned (rendered ineffective) much more easily by impurities.\n\nThe majority of sulfur trioxide made in this way is converted into sulfuric acid not by the direct addition of water, with which it forms a fine mist, but by absorption in concentrated sulfuric acid and dilution with water of the produced oleum.\nIt was once produced industrially by heating calcium sulfate with silica.\n\nSulfur trioxide is an essential reagent in sulfonation reactions. These processes afford detergents, dyes, and pharmaceuticals. Sulfur trioxide is generated in situ from sulfuric acid or is used as a solution in the acid.\n\nAlong with being a strong oxidizing agent, sulfur trioxide will cause serious burns on both inhalation and ingestion because it is highly corrosive and hygroscopic in nature. SO should be handled with extreme care as it reacts violently with water and produces highly corrosive sulfuric acid. It should also be kept away from organic material due to the strong dehydrating nature of sulfur trioxide and its ability to react violently with such materials.\n\n\n"}
{"id": "5414499", "url": "https://en.wikipedia.org/wiki?curid=5414499", "title": "Swedish Electricians' Union", "text": "Swedish Electricians' Union\n\nThe Swedish Electricians' Union (\"Svenska Elektrikerförbundet\": SEF) is a trade union in Sweden. It has a membership of 24,775 (in 2011) and represents electricians in light and heavy instillations, as well as radio, TV and electronics technicians and power station staff.\n\nSEF is affiliated with the Swedish Trade Union Confederation.\n\n"}
{"id": "22157108", "url": "https://en.wikipedia.org/wiki?curid=22157108", "title": "Tantalum capacitor", "text": "Tantalum capacitor\n\nA tantalum electrolytic capacitor is an electrolytic capacitor, a passive component of electronic circuits. It consists of a pellet of tantalum metal as an anode, covered by an insulating oxide layer that forms the dielectric, surrounded by liquid or solid electrolyte as a cathode. Because of its very thin and relatively high permittivity dielectric layer, the tantalum capacitor distinguishes itself from other conventional and electrolytic capacitors in having high capacitance per volume (high volumetric efficiency) and lower weight.\n\nTantalum is a conflict mineral. Tantalum electrolytic capacitors are considerably more expensive than comparable aluminum electrolytic capacitors.\n\nTantalum capacitors are inherently polarized components. Reverse voltage can destroy the capacitor. Non-polar or bipolar tantalum capacitors are made by effectively connecting two polarized capacitors in series, with the anodes oriented in opposite directions.\n\nElectrolytic capacitors use a chemical feature of some special metals, historically called \"valve metals\", which can form an insulating oxide layer. Applying a positive voltage to the tantalum anode material in an electrolytic bath forms an oxide barrier layer with a thickness proportional to the applied voltage. This oxide layer serves as the dielectric in an electrolytic capacitor. The properties of this oxide layer compared with tantalum oxide layer are given in the following table:\n\nAfter forming a dielectric oxide on the rough anode structures, a cathode is needed. An electrolyte acts as the cathode of electrolytic capacitors. There are many different electrolytes in use. Generally, the electrolytes will be distinguished into two species, \"non-solid\" and \"solid\" electrolytes. Non-solid electrolytes are a liquid medium whose conductivity is ionic. Solid electrolytes have electron conductivity and thus solid electrolytic capacitors are more sensitive against voltages spikes or current surges.\nThe oxide layer may be destroyed if the polarity of the applied voltage is reversed.\n\nEvery electrolytic capacitor in principle forms a \"plate capacitor\" whose capacitance is greater the larger the electrode area, A, and the permittivity, ε, are and the thinner the thickness, d, of the dielectric is.\n\nThe dielectric thickness of electrolytic capacitors is very thin, in the range of nanometers per volt. Despite this, the dielectric strengths of these oxide layers are quite high. Thus, tantalum capacitors can achieve a high volumetric capacitance compared to other capacitor types.\n\nAll etched or sintered anodes have a much larger total surface area compared to a smooth surface of the same overall dimensions. This surface area increase boosts the capacitance value by a factor of up to 200 (depending on the rated voltage) for solid tantalum electrolytic capacitors.\n\nThe volume of an electrolytic capacitor is defined by the product of capacitance and voltage, the so-called \"CV-volume\". However, in comparing the permittivities of different oxide materials, it is seen that tantalum pentoxide has an approximately 3 times higher permittivity than aluminum oxide. Tantalum electrolytic capacitors of a given CV value can therefore be smaller than aluminum electrolytic capacitors.\n\nA typical tantalum capacitor is a chip capacitor and consists of tantalum powder pressed and sintered into a pellet as the anode of the capacitor, with the oxide layer of tantalum pentoxide as a dielectric, and a solid manganese dioxide electrolyte as the cathode.\n\nTantalum capacitors are manufactured from a powder of relatively pure elemental tantalum metal. A common figure of merit for comparing volumetric efficiency of powders is expressed in capacitance (C, usually in μF) times volts (V) per gram (g). Since the mid-1980s, manufactured tantalum powders have exhibited around a ten-fold improvement in CV/g values (from approximately 20k to 200k). The typical particle size is between 2 and 10 μm. Figure 1 shows powders of successively finer grain, resulting in greater surface area per unit volume. Note the very great difference in particle size between the powders.\nThe powder is compressed around a tantalum wire (known as the riser wire) to form a \"pellet\". The riser wire ultimately becomes the anode connection to the capacitor. This pellet/wire combination is subsequently vacuum sintered at high temperature (typically 1200 to 1800 °C) which produces a mechanically strong pellet and drives off many impurities within the powder. During sintering, the powder takes on a sponge-like structure, with all the particles interconnected into a monolithic spatial lattice. This structure is of predictable mechanical strength and density, but is also highly porous, producing a large internal surface area (see Figure 2).\n\nLarger surface areas produce higher capacitance; thus high \"CV\"/g powders, which have lower average particle sizes, are used for low voltage, high capacitance parts. By choosing the correct powder type and sintering temperature, a specific capacitance or voltage rating can be achieved. For example, a 220 μF 6 V capacitor will have a surface area close to 346 cm, or 80% of the size of a sheet of paper (US Letter, 8.5×11 inch paper has area ~413 cm), although the total volume of the pellet is only about 0.0016 cm.\n\nThe dielectric is then formed over all the tantalum particle surfaces by the electrochemical process of anodization. To achieve this, the “pellet” is submerged into a very weak solution of acid and DC voltage is applied. The total dielectric thickness is determined by the final voltage applied during the forming process. Initially the power supply is kept in a constant current mode until the correct voltage (i.e. dielectric thickness) has been reached; it then holds this voltage and the current decays to close to zero to provide a uniform thickness throughout the device and production lot.\nThe chemical equations describing the dielectric formation process at the anode are as follows:\n\nThe oxide forms on the surface of the tantalum, but it also grows into the material. For each unit thickness of oxide growth, one third grows out and two thirds grows in. Due to the limits of oxide growth, there is a limit on the maximum voltage rating of tantalum oxide for each of the presently available tantalum powders (see Figure 3).\n\nThe dielectric layer thickness generated by the forming voltage is directly proportional to the voltage proof of electrolytic capacitors. Electrolytic capacitors are manufactured with a safety margin in oxide layer thickness, which is the ratio between voltage used for electrolytical creation of dielectric and rated voltage of the capacitor, to ensure reliable functionality.\n\nThe safety margin for solid tantalum capacitors with manganese dioxide electrolyte is typically between 2 and 4. That means that for a 25 V tantalum capacitor with a safety margin of 4 the dielectric voltage proof can withstand 100 V to provide a more robust dielectric. This very high safety factor is substantiated by the failure mechanism of solid tantalum capacitors, “field crystallization”.\n\nFor tantalum capacitors with solid polymer electrolyte the safety margin is much lower, typically around 2.\n\nThe next stage for solid tantalum capacitors is the application of the cathode plate (wet tantalum capacitors use a liquid electrolyte as a cathode in conjunction with their casing). This is achieved by pyrolysis of manganese nitrate into manganese dioxide. The “pellet” is dipped into an aqueous solution of nitrate and then baked in an oven at approximately 250 °C to produce the dioxide coat. The chemical equation is:\n\nThis process is repeated several times through varying specific gravities of nitrate solution, to build up a thick coat over all internal and external surfaces of the “pellet”, as shown in Figure 4.\n\nIn traditional construction, the “pellet” is successively dipped into graphite and then silver to provide a good connection from the manganese dioxide cathode plate to the external cathode termination(see Figure 5).\nThe picture below shows the production flow of tantalum electrolytic chip capacitors with sintered anode and solid manganese dioxide electrolyte.\n\nTantalum electrolytic capacitors are made in three different styles:\n\n\nMore than 90% of all tantalum electrolytic capacitors are manufactured in SMD style as tantalum chip capacitors. It has contact surfaces on the end faces of the case and is manufactured in different sizes, typically following the EIA-535-BAAC standard. The different sizes can also be identified by case code letters. For some case sizes (A to E), which have been manufactured for many decades, the dimensions and case coding over all manufactures are still largely the same. However, new developments in tantalum electrolytic capacitors such as the multi-anode technique to reduce the ESR or the \"face down\" technique to reduce the inductance have led to a much wider range of chip sizes and their case codes. These departures from EIA standards mean devices from different manufacturers are no longer always uniform.\n\nAn overview of the dimensions of conventional tantalum rectangular chip capacitors and their coding is shown in the following table:\n\n\nThe main feature of modern non-solid (wet) tantalum electrolytic capacitors are their energy density compared with that of solid tantalum and wet aluminum electrolytic capacitors within the same temperature range. Due to their self-healing properties (the non-solid electrolyte can deliver oxygen to form new oxide layer in weak areas of the dielectric), the dielectric thickness can be formed with much lower safety margins and consequently with much thinner dielectric than for solid types, resulting in a higher CV value per volume unit. Additionally, wet tantalum capacitors are able to operate at voltages in excess of 100 V up to 630 V, have a relatively low ESR, and have the lowest leakage current of all electrolytic capacitors.\n\nThe original wet tantalum capacitors developed in the 1930s were axial capacitors, having a wound cell consisting of a tantalum anode and foil cathode separated by a paper stripe soaked with an electrolyte, mounted in a silver case and non-hermetic elastomer sealed. Because of the inertness and stability of the tantalum dielectric oxide layer against strong acids, the wet tantalum capacitors could use sulfuric acid as an electrolyte, thus providing them with a relatively low ESR.\n\nBecause in the past, silver casings had problems with silver migration and whiskers which led to increasing leakage currents and short circuits, new styles of wet tantalum capacitors use a sintered tantalum pellet cell and a gelled sulfuric acid electrolyte mounted in a pure tantalum case.\n\nDue to their relatively high price, wet tantalum electrolytic capacitors have few consumer applications. They are used in ruggedized industrial applications, such as in probes for oil exploration. Types with military approvals can provide the extended capacitance and voltage ratings, along with the high quality levels required for avionics, military, and space applications.\n\nThe group of \"valve metals\" capable of forming an insulating oxide film was discovered in 1875. In 1896 Karol Pollak patented a capacitor using aluminum electrodes and a liquid electrolyte. Aluminum electrolytic capacitors were commercially manufactured in the 1930s.\n\nThe first tantalum electrolytic capacitors with wound tantalum foils and non-solid electrolyte were developed in 1930 by Tansitor Electronic Inc. (US), and were used for military purposes.\n\nSolid electrolyte tantalum capacitors were invented by Bell Laboratories in the early 1950s as a miniaturized and more reliable low-voltage support capacitor to complement their newly invented transistor. The solution R. L. Taylor and H. E. Haring from the Bell labs found for the new miniaturized capacitor found in early 1950 was based on experience with ceramics. They ground metallic tantalum to a powder, pressed this powder into a cylindrical form and then sintered the powder particles at high temperature between under vacuum conditions, into a pellet (“slug”).\n\nThese first sintered tantalum capacitors used a liquid electrolyte. In 1952 Bell Labs researchers discovered the use manganese dioxide as a solid electrolyte for a sintered tantalum capacitor.\n\nAlthough the fundamental inventions came from the Bell Labs, the innovations for manufacturing commercially viable tantalum electrolytic capacitors were done by the researchers of the Sprague Electric Company. Preston Robinson, Sprague's Director of Research, is considered to be the actual inventor of tantalum capacitors in 1954. His invention was supported by R. J. Millard, who introduced the “reform” step in 1955, a significant improvement in which the dielectric of the capacitor was repaired after each dip-and-convert cycle of MnO deposition. This dramatically reduced the leakage current of the finished capacitors.\n\nThis first solid electrolyte manganese dioxide had 10 times better conductivity than all other types of non-solid electrolyte capacitors. In the style of tantalum pearls, they soon found wide use in radio and new television devices.\n\nIn 1971, Intel launched its first microcomputer (the MCS 4) and 1972 Hewlett Packard launched one of the first pocket calculators (the HP 35). The requirements for capacitors increased, especially the demand for lower losses. The equivalent series resistance (ESR) for bypass and decoupling capacitors of standard electrolytic capacitors needed to be decreased.\n\nAlthough solid tantalum capacitors offered lower ESR and leakage current values than the aluminum electrolytics, in 1980 a price shock for tantalum in the industry dramatically reduced the usability of tantalum capacitors, especially in the entertainment industry.\n\nThe development of conducting polymers by Alan J. Heeger, Alan MacDiarmid and Hideki Shirakawa in 1975 was a break-through in point of lower ESR. The conductivity of conductive polymers such as polypyrrole (PPy) or PEDOT are better by a factor of 1000 than that of manganese dioxide, and are close to the conductivity of metals.\nIn 1993 NEC introduced their SMD polymer tantalum electrolytic capacitors, called \"NeoCap\". In 1997 Sanyo followed with their \"POSCAP\" polymer tantalum chips.\n\nA new conductive polymer for tantalum polymer capacitors was presented by Kemet at the \"1999 Carts\" conference. This capacitor used the newly developed organic conductive polymer PEDT Poly(3,4-ethylenedioxythiophene), also known as PEDOT (trade name Baytron).\n\nThis development to low ESR capacitors with high CV-volumes in chip style for the rapid growing SMD technology in the 1990s increased the demand on tantalum chips dramatically. However, another price explosion for tantalum in 2000/2001 forced the development of niobium electrolytic capacitors with manganese dioxide electrolyte, which have been available since 2002. The materials and processes used to produce niobium-dielectric capacitors are essentially the same as for existing tantalum-dielectric capacitors. The characteristics of niobium electrolytic capacitors and tantalum electrolytic capacitors are roughly comparable.\n\nTantalum electrolytic capacitors as discrete components are not ideal capacitors, as they have losses and parasitic inductive parts. All properties can be defined and specified by a series equivalent circuit composed of an idealized capacitance and additional electrical components which model all losses and inductive parameters of a capacitor. In this series-equivalent circuit the electrical characteristics are defined by:\n\n\nUsing a series equivalent circuit rather than a parallel equivalent circuit is specified by IEC/EN 60384-1.\n\nThe electrical characteristics of tantalum electrolytic capacitors depend on the structure of the anode and the electrolyte used. This influences the capacitance value of tantalum capacitors, which depend on operating frequency and temperature. The basic unit of electrolytic capacitors capacitance is microfarad (μF).\n\nThe capacitance value specified in the data sheets of the manufacturers is called rated capacitance C or nominal capacitance C and is the value for which the capacitor has been designed. Standardized measuring condition for electrolytic capacitors is an AC measuring method with a frequency of 100 to 120 Hz. Electrolytic capacitors differ from other capacitor types, whose capacitances are typically measured at 1 kHz or higher. For tantalum capacitors a DC bias voltage of 1.1 to 1.5 V for types with a rated voltage of ≤2.5 V or 2.1 to 2.5 V for types with a rated voltage of >2.5 V may be applied during the measurement to avoid reverse voltage.\n\nThe percentage of allowed deviation of the measured capacitance from the rated value is called capacitance tolerance. Electrolytic capacitors are available in different tolerance series classifications, whose values are specified in the E series specified in IEC 60063. For abbreviated marking in tight spaces, a letter code for each tolerance is specified in IEC 60062.\n\nThe required capacitance tolerance is determined by the particular application. Electrolytic capacitors, which are often used for filtering and bypassing capacitors don’t have the need for narrow tolerances because they are mostly not used for accurate frequency applications like oscillators.\n\nReferring to IEC/EN 60384-1 standard the allowed operating voltage for tantalum capacitors is called \"rated voltage U \" or \"nominal voltage U\". The rated voltage U is the maximum DC voltage or peak pulse voltage that may be applied continuously at any temperature within the rated temperature range T (IEC/EN 60384-1).\n\nThe voltage rating of electrolytic capacitors decreases with increasing temperature. For some applications it is important to use a higher temperature range. Lowering the voltage applied at a higher temperature maintains safety margins. For some capacitor types therefore the IEC standard specify a \"temperature derated voltage\" for a higher temperature, the \"category voltage U\". The category voltage is the maximum DC voltage or peak pulse voltage that may be applied continuously to a capacitor at any temperature within the category temperature range T. The relation between both voltages and temperatures is given in the picture right.\n\nLower voltage applied may have positive influences for tantalum electrolytic capacitors. Lowering the voltage applied increases the reliability and reduces the expected failure rate.\n\nApplying a higher voltage than specified may destroy tantalum electrolytic capacitors.\n\nThe surge voltage indicates the maximum peak voltage value that may be applied to electrolytic capacitors during their application for a limited number of cycles. The surge voltage is standardized in IEC/EN 60384-1. For tantalum electrolytic capacitors the surge voltage shall be 1.3 times of the rated voltage, rounded off to the nearest volt.\nThe surge voltage applied to tantalum capacitors may influence the capacitors failure rate.\n\nTransient voltage or a current spike applied to tantalum electrolytic capacitors with solid manganese dioxide electrolyte can cause some tantalum capacitors to fail and may directly lead to a short.\n\nTantalum electrolytic are polarized and generally require anode electrode voltage to be positive relative to the cathode voltage.\n\nWith a reverse voltage applied, a reverse leakage current flows in very small areas of microcracks or other defects across the dielectric layer to the anode of the electrolytic capacitor. Although the current may only be a few microamps, it represents a very high localized current density which can cause a tiny hot-spot. This can cause some conversion of amorphous tantalum pentoxide to the more conductive crystalline form. When a high current is available, this effect can avalanche and the capacitor may become a total short.\n\nNevertheless, tantalum electrolytic capacitors can withstand for short instants a reverse voltage for a limited number of cycles. The most common guidelines for tantalum reverse voltage are:\n\nThese guidelines apply for short excursion and should never be used to determine the maximum reverse voltage under which a capacitor can be used permanently.\n\nTantalum electrolytic capacitors, as well as other conventional capacitors, have two electrical functions. For timers or similar applications, capacitors are seen as a storage component to store electrical energy. But for smoothing, bypassing, or decoupling applications like in power supplies, the capacitors work additionally as AC resistors to filter undesired AC components from voltage rails. For this (biased) AC function the frequency dependent AC resistance (impedance \"Z\") is as important as the capacitance value.\n\nThe impedance is the complex ratio of the voltage to the current with both magnitude and phase at a particular frequency in an AC circuit. In this sense impedance is a measure of the ability of the capacitor to attenuate alternating currents and can be used like Ohms law\n\nThe impedance is a frequency dependent AC resistance and possesses both magnitude and phase at a particular frequency. In data sheets of electrolytic capacitors, only the impedance magnitude \"|Z|\" is specified, and simply written as \"Z\". Regarding to the IEC/EN 60384-1 standard, the impedance values of tantalum electrolytic capacitors are measured and specified at 10 kHz or 100 kHz depending on the capacitance and voltage of the capacitor.\n\nBesides measuring, the impedance can also be calculated using the idealized components out of a capacitor's series-equivalent circuit, including an ideal capacitor \"C\", a resistor \"ESR\", and an inductance \"ESL\". In this case the impedance at the angular frequency \"ω\" therefore is given by the geometric (complex) addition of \"ESR\", by a capacitive reactance \"X\"\n\nand by an inductive reactance \"X\" (Inductance)\n\nformula_4.\n\nThen \"Z\" is given by\n"}
{"id": "9668094", "url": "https://en.wikipedia.org/wiki?curid=9668094", "title": "United Nations Scientific Committee on the Effects of Atomic Radiation", "text": "United Nations Scientific Committee on the Effects of Atomic Radiation\n\nThe United Nations Scientific Committee on the Effects of Atomic Radiation (UNSCEAR) was set up by resolution of the United Nations General Assembly in 1955. 21 states are designated to provide scientists to serve as members of the committee which holds formal meetings (sessions) annually and submits a report to the General Assembly. The organisation has no power to set radiation standards nor to make recommendations in regard to nuclear testing. It was established solely to \"define precisely the present exposure of the population of the world to ionizing radiation.\" A small secretariat, located in Vienna and functionally linked to the UN Environment Program, organizes the annual sessions and manages the preparation of documents for the Committee's scrutiny.\n\nUNSCEAR issues major public reports on \"Sources and Effects of Ionizing Radiation\" from time to time. As of 2017, there have been 28 major publications from 1958 to 2017. The reports are all available from the UNSCEAR website. These works are very highly regarded as sources of authoritative information and are used throughout the world as a scientific basis for evaluation of radiation risk. The publications review studies undertaken separately from a range of sources. Reports from UN member states and other international organisations on data from survivors of the atomic bombings of Hiroshima and Nagasaki, the Chernobyl disaster, accidental, occupational, and medical exposure to ionizing radiation.\n\nOriginally, in 1955, India and the Soviet Union wanted to add several neutralist and communist states, such as mainland China. Eventually a compromise with the US was made and Argentina, Belgium, Egypt and Mexico were permitted to join. The organisation was charged with collecting all available data on the effects of \"ionising radiation upon man and his environment.\" (James J. Wadsworth - American representative to the General Assembly).\n\nThe Committee was originally based in the Secretariat Building in New York City, but moved to Vienna in 1974. \n\nThe Secretaries of the Committee have been:\n\n\nUNSCEAR has published 20 major reports, latest is the summary 2010 (14 pages), last full report is 2008 report Vol.I and Vol.II with scientific annexes (A to E).\n\n\"UNSCEAR 2008 REPORT Vol.I\" main report and 2 scientific annexes\n\n\"UNSCEAR 2008 REPORT Vol.II\" 3 scientific annexes\n\n\n"}
{"id": "30330189", "url": "https://en.wikipedia.org/wiki?curid=30330189", "title": "What Is the Electric Car?", "text": "What Is the Electric Car?\n\nWhat Is the Electric Car? is a 2010 documentary film that explains the benefits of electric cars. One reviewer stated that the movie \"teeters on the brink of tedious but repeatedly saves itself with moments of cleverness or insight.\" The film features several actors, scientists, engineers and activists, all of whom contribute their thoughts and explanations regarding electric cars and electric vehicle technology.\n\nThe film premiered on December 14, 2010 at the Egyptian Theatre in Hollywood, California.\n\n"}
{"id": "3059064", "url": "https://en.wikipedia.org/wiki?curid=3059064", "title": "Wood warping", "text": "Wood warping\n\nWood warping is a deviation from flatness in timber as a result of stresses and uneven shrinkage. Warping can also occur in wood considered \"dry\" (wood can take up and release moisture indefinitely), when it takes up moisture unevenly, or – especially – is allowed to return to its \"dry\" equilibrium state unevenly, too slowly, or too quickly. Many factors can contribute to wood warp; wood species, grain orientation, air flow, sunlight, uneven finishing, temperature – even cutting season and the moon's gravitational pull are taken into account in some traditions (e.g., violin making).\n\nThe types of wood warping include:\n\n\nWood warping costs the wood industry in the U.S. millions of dollars per year. Straight wood boards that leave a cutting facility sometimes arrive at the store yard warped. This little understood process is finally being looked at in a serious way. Although wood warping has been studied for years, the warping control model for manufacturing composite wood hasn't been updated for about 40 years.\n\nA researcher at Texas A&M University, Zhiyong Cai, has researched wood warping, and was working on a computer software program in 2003 to help manufacturers make changes in the manufacturing process so that wood doesn't arrive at its destination warped after it leaves the mill or factory.\n\n\n"}
