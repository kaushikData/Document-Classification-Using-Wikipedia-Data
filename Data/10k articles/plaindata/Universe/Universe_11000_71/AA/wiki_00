{"id": "45575125", "url": "https://en.wikipedia.org/wiki?curid=45575125", "title": "2015 Zasyadko mine disaster", "text": "2015 Zasyadko mine disaster\n\nOn 4 March 2015, at around 05:20 local time, there was a mining accident at the Zasyadko coal mine in rebel-held Eastern Ukraine. It is suspected to have been caused by a gas explosion.\n\nTwenty-three people were confirmed dead. Local rebels claimed a death toll of 30. There were 230 people in the mine at the time of the explosion. The Speaker of the Ukrainian parliament, Volodymyr Groysman, called for a minute's silence for 32 fatalities, but later retracted that figure to say that one had died and 30 others' status was unknown.\n\nUkrainian President Petro Poroshenko called for police and rescue services to have access to the mine.\n\n"}
{"id": "51641984", "url": "https://en.wikipedia.org/wiki?curid=51641984", "title": "2016 Southeastern United States gasoline shortage", "text": "2016 Southeastern United States gasoline shortage\n\nThe 2016 Southeastern United States gasoline shortage was an phenomenon caused by the 2016 Colonial Pipeline Leak and the resulting panic buying in which many gas stations across six states have entirely run out of gasoline, causing price hikes, halts of services, and several declarations of states of emergency.\n\nOn Monday, September 12, 2016, a leak occurred in Shelby County, Alabama, spilling an estimated 350,000 US gallons of summer-grade gasoline, requiring a partial shutdown of the pipeline, and causing gas shortages in much of the Southeastern United States. Six states are affected (Alabama, Georgia, Tennessee, North Carolina, South Carolina, and Virginia), with Alabama, Tennessee, North Carolina and Georgia declaring states of emergency. These declarations have eliminated certain size and weight restrictions on vehicles carrying gasoline, and the hours which they are allowed to deliver.\n\nMany gas stations in the affected regions entirely ran out of gas. Panic buying greatly contributed to this.\n\nThe Colonial Pipeline leak in Shelby County, Alabama was first detected on September 9. By September 17, the affected regions began experiencing gas shortages due to the leak and panic buying. Colonial has announced the construction of a bypass pipeline.\n\nOn September 21, Colonial Pipeline Company announced the completion of an emergency bypass pipeline. Whilst flow is now at optimal rates, Colonial claims it will be \"several days\" before processed fuel reaches and replenishes affected areas.\n\nOn September 13, Georgia governor Nathan Deal declared a state of emergency. On September 15, Alabama governor Robert J. Bentley declared a state of emergency. The pipeline was shut down on September 16, and federal regulators began investigating the cause of the leak. North Carolina governor Pat McCrory, Tennessee governor Bill Haslam and South Carolina governor Nikki Haley all declared states of emergency, allowing fuel tankers to work longer hours to maintain the availability of gasoline.\n\nOn September 21, the states of North Carolina and Virginia declared their states of emergency over upon the news of the Colonial Pipeline's completion of the bypass.\n"}
{"id": "19058591", "url": "https://en.wikipedia.org/wiki?curid=19058591", "title": "88888 Lights Out", "text": "88888 Lights Out\n\n88888 Lights Out was a campaign with the stated goal of increasing awareness of global warming and promoting actions to reduce energy consumption. By encouraging India's residents to turn out the lights for eight minutes and to become more aware of environmental concerns, the organisers sought to limit greenhouse gas emissions and reduce pollution of the globe.\n\nThe campaign was started by Exnora International, a non-governmental organisation in India. It called for people to switch off their lights for eight minutes at 8 p.m. on August 8, 2008 (8-8-8-8-8) to spread awareness and take action to reduce energy consumption and the resulting environmental damage.\n\nIn the city of Chennai, Governor Surjit Singh Barnala ordered the lights off in Raj Bhavan at 8 p.m. for 8 minutes and said that an awakening should be created among the people of all walks of life on the root cause of global warming, it occurred at the very beginning of the 2008 Olympics.\n\nThe effort has been part of a wider effort by the groups involved to bring attention to environmental issues.\n\n\n"}
{"id": "33711547", "url": "https://en.wikipedia.org/wiki?curid=33711547", "title": "Acoustic plaster", "text": "Acoustic plaster\n\nAcoustic plaster is plaster which contains fibres or aggregate so that it absorbs sound.\n\nSuch plaster is applied in thicknesses of up to 1.5 inches. As compared with other sound insulation, it is easy to apply and is fireproof but it can be more fragile, being affected by physical stress and humidity. Acoustic plaster is used in construction of rooms which require good acoustic qualities such as auditoria and libraries.\n\nProprietary types of acoustic plaster developed in the 1920s included Macoustic Plaster, Sabinite, Kalite, Wyodak, Old Newark and Sprayo-Flake produced by companies such as US Gypsum. These superseded felts and quilts as a common preference of architects but were difficult to apply and so were superseded in turn by acoustic tiles.\n"}
{"id": "3804700", "url": "https://en.wikipedia.org/wiki?curid=3804700", "title": "Acrylate polymer", "text": "Acrylate polymer\n\nAcrylate polymers belong to a group of polymers which could be referred to generally as plastics. They are noted for their transparency, resistance to breakage, and elasticity. They are also commonly known as acrylics or polyacrylates. Acrylate polymer is commonly used in cosmetics such as nail polish as an adhesive.\n\nAcrylate monomers, used to form acrylate polymers, are based on the structure of acrylic acid, which consists of a vinyl group and a carboxylic acid ester terminus or a nitrile. Other typical acrylate monomers are derivatives of acrylic acid, such as methyl methacrylate in which one vinyl hydrogen and the carboxylic acid hydrogen are both replaced by methyl groups, and acrylonitrile in which the carboxylic acid group is replaced by the related nitrile group.\n\nOther examples of acrylate monomers are:\n\nAcrylic elastomer is a general term for a type of synthetic rubber whose main component is acrylic acid alkylester (ethyl or butyl ester). Acrylic elastomer has characteristics of heat and oil resistance. \n\nIt is divided into old type and new type: Old types include ACM (copolymer of acrylic acid ester and 2-chloroethyl vinyl ether) containing chlorine and ANM (copolymer of acrylic acid ester and acrylonitrile) without chloride. \nOther than the slightly better water resistance of ANM, there are no physical differences; even processability is poor for both types. Since prices are also high, demand is not so high vis-à-vis the characteristics. On the other hand, the new type of acrylic rubber does not contain any chlorine despite its unclear chemical composition. Processability has been improved; most of the tackiness to rolls, as well as staining problems related to mold have been solved. \n\nMajor characteristics of acrylic rubber include heat resistance and oil resistance; it can endure a temperature of 170–180 ℃ under dry heat or in oil. Since it does not have a double bond, acrylic rubber also boasts of good weatherability and ozone resistance. \n\nIts cold resistance is not that good, however. The saturation point is −15 ℃ for the old type and −28...−30 ℃ for the new type. In terms of vulcanization, the standard method for the old type is amine vulcanization. To minimize permanent deformation, the old type requires curing for 24 hours at a temperature of 150 ℃. On the other hand, for the new type, the press curing time and follow-up vulcanization time are significantly reduced by combining metal soap and sulfur. It has no special characteristics. The rebound resilience and abrasion resistance of the new type are poor, and even its electrical characteristics are considerably poor compared with acrylonitrile-butadiene rubber and butyl rubber. \n\nThe materials are used mainly for oil seals and packagings related to automobiles.\n\n\n"}
{"id": "8585320", "url": "https://en.wikipedia.org/wiki?curid=8585320", "title": "Ashmole Bestiary", "text": "Ashmole Bestiary\n\nThe Ashmole Bestiary (Bodleian Library MS. Ashmole 1511) is a late 12th or early 13th century English illuminated manuscript Bestiary containing a creation story and detailed allegorical descriptions of over 100 animals. Rich colour miniatures of the animals are also included.\n\nThe \"Aberdeen Bestiary\" (Aberdeen University Library MS 24) may have been created by the same artist.\n\nHugh of Fouilloy's moral treatise on birds, \"De avibus\", is incorporated into the text with 29 full colour illustrations. \n\n"}
{"id": "28322793", "url": "https://en.wikipedia.org/wiki?curid=28322793", "title": "Association of Energy Engineers", "text": "Association of Energy Engineers\n\nThe Association of Energy Engineers (AEE), founded in 1977, is a nonprofit professional society whose stated mission is “to promote the scientific and educational interests of those engaged in the energy industry and to foster action for sustainable development.”\n\nSince 1981 the Association of Energy Engineers has certified more than 26,000 professionals, whose credentials are recognized by the U.S. Department of Energy and the U.S. Agency for International Development.\n\nAEE offers the following certifications:\n\n\nEach year the Association of Energy Engineers (AEE) presents three conference and trade show events for energy and facility professionals. These events, held throughout the continental United States, provide opportunities to find out more about the issues and marketplace developments that impact your decisions, as well as to see the latest technologies first hand.\n\nAEE's three annual trade show events are:\n\nThe Association of Energy Engineers publishes three journals:\n\n\nAEE also publishes books targeted for energy- and facility-involved professionals. AEE members receive newsletters on the energy industry. In addition, AEE regularly releases reports on the energy industry.\n\n\nRe Certified GeoExchange Designer (CGD) - International Ground Source Heat Pump Association\n"}
{"id": "51498564", "url": "https://en.wikipedia.org/wiki?curid=51498564", "title": "Charles E. Wyman", "text": "Charles E. Wyman\n\nCharles E. Wyman, also known as Charles Wyman, is a Ford Motor Company, Chair and a Distinguished Professor & Endowed Chair in Chemical & Environmental Engineering at University of California, Riverside, USA. Wyman was also the Paul E. and Joan H. Queneau Distinguished Professor in Environmental Engineering Design at the Thayer School of Engineering at Dartmouth College, Prior to joining UC Riverside. He is the co-founder and former Chief Development Officer and Chair of the Scientific Advisory Board of Mascoma Corporation. He is one of the researchers responsible for the development of the Bioenergy, Biomass & Biofuel. He is an authoritative figure in the broad area of ethanol. He has made significant seminal contributions in the specific areas of pretreatment, enzymatic hydrolysis, and dehydration of cellulosic biomass to reactive intermediates for biological or catalytic conversion into fuels and chemicals. Wyman has over 200 journal papers, 25 books chapters (with several second and third editions), and has given numerous (over 300) national and international talks, 30 technical reports and about more than 20 patents.\n\nCharles got his bachelor's degree in Chemical Engineering from University of Massachusetts, USA in 1967. Wyman graduated with both master's and PhD in Chemical Engineering from Princeton University in 1971. Focusing towards start up he even later did his MBA from University of Denver in 1988.\n\nPrior to joining UC Riverside as a professor in 2005, Wyman was a faculty member of Dartmouth College. He has addressed several important problems in cellulosic biomass to ethanol and other products with particular focus on Coordinated development of leading biomass pretreatment technologies. Being a fellow he has handled several projects of American Association for Advancement of Science(AAAS). He has been coordinator to train National Renewable Energy Laboratory (NREL) since 17 years as role and leadership focused or on vast area of Biomass. He is also cofounder and a team member Vertimass LLC a sustainable transportation fuels that reduce greenhouse gas emissions and improve energy security and domestic economies.\n\nWymann has developed several biological models. He has first time introduced direct iterative based how biotech can transform biofuels problems, which is still challenging tasks in research community. He has put forth his knowledge and ideas in some of highly rated journals. He has distinction of getting his paper listed in most cited paper since 23 years and also of leading researcher in the field of Bioenergy. Wyman is an eminent fellow of American Association for Advancement of Science since 2006 and Has been honoured C.D. Scott Award in Biotechnology in 1999, NREL Hubbard Leadership Award in 1992 and NREL Staff Leadership Award, 1991 and a reviewers of reputed journals. He has been listed among top most cited papers in Journal of Bioresource Technology.\n\n"}
{"id": "52537212", "url": "https://en.wikipedia.org/wiki?curid=52537212", "title": "Composition (objects)", "text": "Composition (objects)\n\nCompositional objects are wholes instantiated by collections of parts. If an ontology wishes to permit the inclusion of compositional objects it must define which collections of objects are to be considered parts composing a whole. Mereology, the study of relationships between parts and their wholes, provides specifications on how parts must relate to one another in order to compose a whole.\n\nOntological disputes do not revolve around what particular matter is present; rather, the center of disputation is what objects can be said to be instantiated by a given collection of matter. The token objects posited by a given ontology may be classified as instances of one or more distinct object types. As the types of objects accepted proliferate, so do the possible tokens that a given collection of matter can be said to instantiate. This creates variations in size between ontologies, which serve as an arena for disputes among philosophers. The ontologies of present concern are those that include compositional objects among posited types. Compositional objects are objects made of a collection of one or more parts . These objects seem to be included in any intuitively constructed ontology as objects ordinarily encountered are doubtless composed of parts. For example, any ontology that affirms the existence of tables, rabbits, or rocks necessarily commits to the inclusion of some compositional objects. The specification of ‘some’ compositional objects foretells the point of attack suffered by these theories. Clarification demands that these theories provide a means to account for which compositional objects are included and which are excluded. One may include tables and, presumably, chairs, but what about the composition of the table and surrounding chairs? What characteristics of a collection of parts determine that they form a whole?\n\nMereological nihilism is an extreme eliminative position. Mereological nihilism denies that any objects actually instantiate the parthood relation appealed to in theoretical descriptions of mereology. If there are no relationships that count as parthood relationships, then there are no composite objects. One may initially seek to reject such a position by pointing to its counterintuitive conclusions. However, there are other mereological positions that prove equally counterintuitive and so a more substantial rebuttal is required. A principled rejection of mereological nihilism is put forward those committed to atomless gunk. A mereology is gunky if every part is itself a whole composed of further parts. There is no end to the decomposition of objects, no fundamental part or mereological atom. There is no place for the atoms posited by mereological nihilism in gunky ontologies. This causes a problem because if all that exists are atoms, but there is nothing like an atom that exists within an ontology, then nothing can be said to exist (Van Cleve, 2008). Noting the appeal of accepting that things do exist, one must reject mereological nihilism in order to maintain a gunky ontology. Not everyone will strive to maintain a gunky ontology and so mereological nihilism is still potentially a viable position.\n\nThere are various attempts to conserve the existence of parthood relationships. These theories all attempt to specify characteristics that a collection of objects must possess in order to compose a whole. Characteristics may derive from some principle or be proposed as brute fact.\n\nA principled account of the composition relationship will appeal to a general characteristic which is sufficient to instantiate the relationship. Many of these accounts appeal to characteristics derived from intuitive notions about what does or does not allow objects to function as parts in a whole. Two such proposed restricting characteristics are connection and cohesion (Van Cleve, 2008). First, connection is the stipulation that objects must be spatially continuous to some degree in order to be considered parts composing a whole. Objects like tables are made of legs connected to tops. Tables and legs are in direct contact with one another, the parts are spatially contiguous. Yet, the chairs are only in proximity to the table and so do not compose a table set. In order to maintain the standard of absolute contiguity one would have to recruit the air molecules bridging span between the table and chairs. This is unsatisfactory though because it fails to exclude extraordinary objects such as the table, the air molecules, and the dog’s nose as he begs for food. It seems that it is necessary to redefine connection as some degree of proximity between parts within a whole .\n(1) On a continuum of discrete points, if there are both instances of both composition and not, then the series of points instantiating composition (e.g. (1, 2, 3, 4)) is continuous with any series not (e.g. (5, 6, 7)).\n(2) There is no principled way determine a cutoff for composition along such continuums (no non-arbitrary way to determine between (1, 2, 3) and (1, 2, 3, 4)).\n(3) Since the nature of existence does not allow for indeterminacy a cutoff must be specified (a failure to determine between (1, 2, 3) and (1, 2, 3, 4) leaves (4) in a position between existence and non-existence that does not exist).\nConclusion: If composition is to be non-arbitrary then it must either always occur or never.\nSider’s rejection of any bounding of degree is not particular to spatial proximity. Degree of cohesion can also be represented as a continuum. Much like absolute spatial contiguity was determined too strict, absolute cohesion is also rejected. To illustrate Van Cleve (2008) describes how a rod and line compose a fishing rod. The line must move with the rod to some degree. In order to accomplish this knots of line are tied around the rod. As the knots are tightened the line becomes more and more fixed to the rod. There is a cutoff where the line could be tighter, yet is tight enough to compose the fishing rod. Any variable represented on a continuum will fail to provide a principled determination of this cutoff.\n\nAccording to Van Inwagen a collection of objects are considered parts composing a whole when that whole demonstrates life (Van Cleeve, 2008). This approach guarantees the existence of you and me, while ruling out extraordinary objects consistent with other conservative theories. Detractors of the 'life' criterion point out the difficulty of defining when life is present. It is not clear if a virion, a virus particle composed of nucleic acid and surrounding capsid, is a compositional object or not. Additionally, in some formerly paradigmatic cases of life it can be difficult to identify when it is no longer present, and thus the compositional object is no longer extant (e.g. brain death).\n\nMereological universalism is an extreme permissive position. Essentially, mereological universalism contends that any collection of objects constitutes a whole. This secures the existence of any compositional objects intuitively thought to exist. However, by the same light that ordinary objects exist, so do much stranger ones. For example, there exists both the object composed of my key ring and keys and the object composed of the moon and six pennies located on James Van Cleve’s desk (Van Cleve, 2008). Motivation for such a counterintuitive position is not immediately apparent, but arises from the ability to reject all alternatives. Despite little intuitive appeal, mereological universalism seems less susceptible to principled rejection than any of its alternatives.\n\nEdition), Edward N. Zalta (ed.), URL = <https://plato.stanford.edu/archives/spr2016/entries/ordinary-objects/>.\nVan Cleve, J. (2008). The moon and sixpence: a defense of mereological universalism.\nVarzi, Achille, \"Mereology\", The Stanford Encyclopedia of Philosophy (Winter 2016 Edition), \nEdward N. Zalta (ed.), forthcoming URL = <https://plato.stanford.edu/archives/win2016/entries/mereology/>.\n"}
{"id": "20782290", "url": "https://en.wikipedia.org/wiki?curid=20782290", "title": "Daniel Dingel", "text": "Daniel Dingel\n\nDaniel Dingel was a Filipino engineer who claimed to have invented a “\"hydrogen reactor\"” able to power a water-fuelled car.\n\nDingel says he began working on his hydrogen reactor in 1969, and claims to have used the device to power his 1996 Toyota Corolla. Dingel explains that his invention splits producing hydrogen from water in an onboard water tank and does not produce any carbon emissions. However, he has never revealed the secret to his invention. In an interview with the \"Philippine Daily Inquirer\", Dingel said that he would willing to reveal the secret of his invention if the buyer would hire 200 Filipinos and their families.\n\nDingel is known as a vocal critic of Filipino government officials and scientists who have refused to support his invention. The Philippines' Department of Science and Technology, in turn, has since declared his invention \"a hoax.\" \n\nIn November 2000, John Ding Young of Formosa Plastics Group (FPG) sought Dingel out and, convinced that the invention was genuine, signed a “preliminary understanding” with him for several projects. He aims to have business partners to get an \"international patent\" and to commercialize his technology.\n\nIn December 2008, Dingel became even more controversial when he was found guilty and sentenced to a maximum of 20 years imprisonment in an Estafa (swindling) case filed against him by Young and FPG. In a decision dated Dec. 9, 2008 Judge Rolando How of the Parañaque City Regional Trial Court’s Branch 257 found him guilty of taking $410,000 from FPG, saying that Dingel \"defrauded Young when the inventor failed to fulfill his obligation of developing his “hydrogen reactor” and creating experimental cars in 2000.\" \n\nYoung claimed that Dingel signed a joint venture agreement with FPG, and initially received $30,000 in goodwill money and $20,000 for research and development. Young said that Dingel then visited the FPG headquarters in Taipei and asked for $300,000 so he could purchase three cars which he would use as prototypes when he returned to the Philippines. Young adds that in September 2001 he sent another $60,000 in additional funds, as agreed upon in the joint venture agreement.\n\nFormer Philippine Solicitor General Frank Chavez, whom Dingel asked to serve as his counsel, said that he would appeal the court decision before it became final on the Christmas Eve of 2008.\n\nDaniel Dingel died on October 18, 2010 in Las Pinas City, Metro Manila in the Philippines.\n"}
{"id": "34132621", "url": "https://en.wikipedia.org/wiki?curid=34132621", "title": "Dewatering screw press", "text": "Dewatering screw press\n\nA dewatering screw press is a screw press that separates liquids from solids. A screw press can be used in place of a belt press, centrifuge, or filter paper. It is a simple, slow moving device that accomplishes dewatering by continuous gravitational drainage. Screw presses are often used for materials that are difficult to press, for example those that tend to pack together. The screw press squeezes the material against a screen or filter and the liquid is collected through the screen for collection and use. \n\nAn example of a dewatering press is a wine press. Dating back to Roman times, these machines worked similarly to the modern screw press but possessed some disadvantages which have been corrected and improved within modern presses. The ancient wine press only allowed for grapes to be juiced in batches and often a thick cake would form against the screen, making it difficult for the juice to flow through the screen and be collected for wine. Most modern screw presses allow for a continuous flow of material by surrounding the screw with a screen, which also helps to avoid the build up of a layer of solid material on the screen. One modern approach even removes the screen in favor of a system of fixed and moving rings, which often eliminates solids buildup entirely.\n\nThe most commonly known screw press of this design is said to have been invented by famous Greek mathematician Archimedes and is known as the screw conveyor. The screw conveyor consists of a shaft, which is surrounded by a spiral steel plate, similar in design and appearance to a corkscrew. This design is used in a multitude of screw presses. There are some machines of this and also of similar design that are not screw presses at all - they do not separate solids from liquids but are instead used to fuse them together. An example of this is a mold-filling machine. Plastic pellets are inserted at one end and heat is applied, melting the pellets and discharging them into a mold. Another example is known as a cooker-extruder and is used in the production of snack foods such as pretzels and more.\n\nMost screw presses can have dilute materials pumped directly into the screw press, although pre-thickening sometimes improves the performance of the press. This is typically done with a static or sidehill screen, a rotating drum screen, belt press, or a gravity table.\n\nPatented in 1900, Valerius Anderson’s interrupted flight design is most commonly used as opposed to the continuous flight design. Anderson, upon studying the continuous flight design, noticed that it led to co-rotation and a less efficient job being done dewatering, especially with softer materials. He solved this by putting interruptions on the flights of the screw. The interruptions allowed for the materials to stop moving forward between interruptions along the shaft and also allows for an adequate buildup of the material before it is pushed through the screw press to container that catches the material. This allowed for a better job at the dewatering and a consistent cake material being released. \n\nThe interrupted flight design screw presses uses were broadened from just soft or mushy materials to include most materials screw presses were used for because unlike the continuous design screw presses the interrupted flight design did not require constant feed or consistency of material. If either were diminished in the continuous design so would production of the dewatered product, in order to avoid this while maintaining the continuous flight design a larger and heavier press with variable speed settings was a necessity; the press also entailed the need of an operator. \n\nThe interrupted flight design eliminated the need for consistency as the compression of the screw did not change as the material did not progress through the screw until a sufficient amount of the material had formed, as described above. This also eliminates the need for changing speed and an operator. The design allows for self-correction and efficiency that is unavailable with the continuous design. It allowed for a more economically effective screw press that has been used for more than just slimy or slippery materials.\n\nAfter a period of time and its initial patent, resistor teeth were added to the presses where there was no flighting in order to increase the agitation of the materials adding to the limitation of the tendencies of co-rotation within the press\n\nThe buildup of press cake moisture is controlled by a discharge door or cone. Screw presses possess different options that include perforated/slotted screens, a rotating cone, hard surfacing on the screw, and supplemental screen surface in the inlet hopper on the face of the cone. The standard construction for screw presses is of stainless steel with a carbon steel frame on the larger presses. \n\nThe specific details of the design of a screw press depend on the material however. The configurations, screw speeds, screens for maximum outlet consistency, including an excellent capture rate vary per material. Most screw presses are designed to feed material that has a 40-60% water make up. The length and diameter ratio of the screw press also depends on the material. The range of the capacity of a screw press\n\nLarger presses use a foot-mounted gearbox while smaller presses use a hollow-shaft gearbox. Currently, nearly all presses are driven by electric motors due to their reliable and low cost frequency drives. The electric motors replaced the previously popular hydraulic motor drives. A vertical design was popular in the 1800s through the 1950s but they are no longer made. Most screw presses are currently built with the screws in a horizontal configuration. One newer version uses an angled screw design to reduce floor footprint and press cake moisture. \n\nCompression is created within the screw press by increasing the inner shaft diameter of the screw. For example, if a 16\" screw press has a 6\" shaft at the start, the flights on the screw will be 5\" tall. If this 6\" shaft diameter is then increased to 12\" at the discharge, the fights will be only 2\" tall at this point. Thus compression is applied as the material is being pressed from a 5\" opening through a 2\" space. \n\nThis compression can also be achieved tightening the separation of the flights of the screw. If at the inlet, the pitch is 16\", the material thus will move 16\" with each revolution. If it is then decreased to 8\" at the point of discharge, the material will move 8\" per revolution. This results in there being more material forced into the press than there is being forced out of the press at a time. This creates the desired compression and pushes the liquid through the screen.\n\nAnother way to achieve compression is to place a cone at the point of discharge. This can also be called a choke, stopper, or door. In many designs it is bolted into a fixed position, making a fixed, smaller opening which the material must pass through. More commonly found however, the screw press has the cone pushed into the point of discharge via a hydraulic or air cylinder. \n\nSome other types of presses are vapor-tight presses, and twin-screw presses. Vapor-tight presses are used during the production of soybean protein concentrate (SPC), citrus and apple pectin, bioresin, and Xanthan gum. Twin-screw presses contain two overlapping compression screws. This is more complicated on a mechanical level because the screws must remain synchronized in order for them to work properly. These are often used for slippery materials and feature an internal shredding action.\n\nThere are two major kinds of screw presses of this design. One type, known as Expellers ®, removes water from fibrous material, while the other removes free liquid from a material. \n\nOil expellers are used to squeeze the fat out of soybeans, peanuts, sunflower seeds, canola (rape seeds), and other oil seeds. The expeller works by exerting extremely high pressures which convert the fat in seeds into a liquid oil. Once the oil is liquefied the oil flows through the screen and is collected.\n\nScrew presses that are used to free liquid from material are commonly used in the pulp and paper industries, municipal biosolids, septage and grease trap sludge, food production, food waste, manure, and also within the chemical industry.\n\nPulp and paper industries remove water within cellulose fiber.\n\nBiosolids are dewatered and heated through a specific process which includes raising the pH to a level of 12. Septage and grease trap sludge is dewatered with a simple screw press of the above stated design. Nutrient management programs dewater hog and cow manure for sale and commercial use.\n\nAlcohol solutions are squeezed from foods with screw presses (such as soybeans, protein, pectin, and xanthan gum.) Food processing factories use screw presses to separate water from waste streams and convert the solid into animal feeds. For example, sugar beet pulp, orange peel, and spent grain.\n\nFish and orange peel dewatering often provide maximum yield when dewatered within a press of the interrupted flight design and with the addition of steam begin injected into the material. Commonly steam injection holes are drilled into the resistor teeth of the press close to the screw's shaft.\n\nWithin the chemical industry screw presses are used for “ABS, sodium alginate and carrageenan, synthetic rubber, synthetic resin, hydrated polymer, naphthalene, elastomeric adhesive, color film emulsion, CmC, pharmaceuticals” and more.\n\n"}
{"id": "6925818", "url": "https://en.wikipedia.org/wiki?curid=6925818", "title": "Egyptian Atomic Energy Authority", "text": "Egyptian Atomic Energy Authority\n\nThe Egyptian Atomic Energy Authority (EAEA) has been established in 1955. It leads the national research and development in the basic and applied peaceful nuclear research. \n\nEgypt was the second in the African Continent, after South Africa, to build a nuclear reactor. The first research reactor (ET-RR-1), commissioned in 1961, is a Van de Graf type 4 MW reactor engineered and built by Russia. Another research reactor (ET-RR-2) is 22 MW open pool MultiPurpose Reactor (MPR) located at Inshas, 60 km from Cairo, engineered and built by INVAP from Argentina.\n\nThe EAEA has scientists educated in the topmost universities and research institutes. It is organized into four research centers: \n\n\nThese centres are further subdivided into major research divisions. \n\nThe EAEA is a member of the International Atomic Energy Agency and other regional and international organizations.\n\n\n"}
{"id": "9975293", "url": "https://en.wikipedia.org/wiki?curid=9975293", "title": "Empresa Nacional de Electricidade de Angola", "text": "Empresa Nacional de Electricidade de Angola\n\nEmpresa Nacional de Electricidade de Angola (E.N.E.) is an electricity company of Angola.\n\n"}
{"id": "10264", "url": "https://en.wikipedia.org/wiki?curid=10264", "title": "Enrico Fermi", "text": "Enrico Fermi\n\nEnrico Fermi (; ; 29 September 1901 – 28 November 1954) was an Italian and naturalized-American physicist and the creator of the world's first nuclear reactor, the Chicago Pile-1. He has been called the \"architect of the nuclear age\" and the \"architect of the atomic bomb\". He was one of the very few physicists in history to excel both theoretically and experimentally. Fermi held several patents related to the use of nuclear power, and was awarded the 1938 Nobel Prize in Physics for his work on induced radioactivity by neutron bombardment and the discovery of transuranic elements. He made significant contributions to the development of quantum theory, nuclear and particle physics, and statistical mechanics.\n\nFermi's first major contribution was to statistical mechanics. After Wolfgang Pauli announced his exclusion principle in 1925, Fermi followed with a paper in which he applied the principle to an ideal gas, employing a statistical formulation now known as Fermi–Dirac statistics. Today, particles that obey the exclusion principle are called \"fermions\". Later Pauli postulated the existence of an uncharged invisible particle emitted along with an electron during beta decay, to satisfy the law of conservation of energy. Fermi took up this idea, developing a model that incorporated the postulated particle, which he named the \"neutrino\". His theory, later referred to as Fermi's interaction and still later as weak interaction, described one of the four fundamental forces of nature. Through experiments inducing radioactivity with recently discovered neutrons, Fermi discovered that slow neutrons were more easily captured than fast ones, and developed the Fermi age equation to describe this. After bombarding thorium and uranium with slow neutrons, he concluded that he had created new elements; although he was awarded the Nobel Prize for this discovery, the new elements were subsequently revealed to be fission products.\n\nFermi left Italy in 1938 to escape new Italian Racial Laws that affected his Jewish wife Laura Capon. He emigrated to the United States where he worked on the Manhattan Project during World War II. Fermi led the team that designed and built Chicago Pile-1, which went critical on 2 December 1942, demonstrating the first artificial self-sustaining nuclear chain reaction. He was on hand when the X-10 Graphite Reactor at Oak Ridge, Tennessee, went critical in 1943, and when the B Reactor at the Hanford Site did so the next year. At Los Alamos he headed F Division, part of which worked on Edward Teller's thermonuclear \"Super\" bomb. He was present at the Trinity test on 16 July 1945, where he used his Fermi method to estimate the bomb's yield.\n\nAfter the war, Fermi served under J. Robert Oppenheimer on the General Advisory Committee, which advised the Atomic Energy Commission on nuclear matters and policy. Following the detonation of the first Soviet fission bomb in August 1949, he strongly opposed the development of a hydrogen bomb on both moral and technical grounds. He was among the scientists who testified on Oppenheimer's behalf at the 1954 hearing that resulted in the denial of the latter's security clearance. Fermi did important work in particle physics, especially related to pions and muons, and he speculated that cosmic rays arose through material being accelerated by magnetic fields in interstellar space. Many awards, concepts, and institutions are named after Fermi, including the Enrico Fermi Award, the Enrico Fermi Institute, the Fermi National Accelerator Laboratory, the Fermi Gamma-ray Space Telescope, the Enrico Fermi Nuclear Generating Station, and the synthetic element fermium, making him one of 16 scientists who have elements named after them.\n\nEnrico Fermi was born in Rome, Italy, on 29 September 1901. He was the third child of Alberto Fermi, a division head (\"\") in the Ministry of Railways, and Ida de Gattis, an elementary school teacher. His sister, Maria, was two years older than him, and his brother Giulio was a year older. After the two boys were sent to a rural community to be wet nursed, Enrico rejoined his family in Rome when he was two and a half. Although he was baptised a Roman Catholic in accordance with his grandparents' wishes, his family was not particularly religious; Enrico was an agnostic throughout his adult life. As a young boy he shared the same interests as his brother Giulio, building electric motors and playing with electrical and mechanical toys. Giulio died during the administration of an anesthetic for an operation on a throat abscess in 1915. Maria died in an airplane crash near Milano in 1959.\n\nOne of Fermi's first sources for his study of physics was a book he found at the local market at \"Campo de' Fiori\" in Rome. Published in 1840, the 900-page \"Elementorum physicae mathematicae\", was written in Latin by Jesuit Father Andrea Caraffa, a professor at the Collegio Romano. It covered mathematics, classical mechanics, astronomy, optics, and acoustics, insofar as these disciplines were understood when the book was written. Fermi befriended another scientifically inclined student, Enrico Persico, and together the two worked on scientific projects such as building gyroscopes and trying to accurately measure the acceleration of Earth's gravity. Fermi's interest in physics was further encouraged by his father's colleague Adolfo Amidei, who gave him several books on physics and mathematics, which he read and assimilated quickly.\n\nFermi graduated from high school in July 1918 and, at Amidei's urging, applied to the \"Scuola Normale Superiore\" in Pisa. Having lost one son, his parents were reluctant to let him move away from home for four years while attending it, but in the end they acquiesced. The school provided free lodging for students, but candidates had to take a difficult entrance exam that included an essay. The given theme was \"Specific characteristics of Sounds\". The 17-year-old Fermi chose to derive and solve the partial differential equation for a vibrating rod, applying Fourier analysis in the solution. The examiner, Professor Giulio Pittarelli from the Sapienza University of Rome, interviewed Fermi and praised him, saying that he would become an outstanding physicist in the future. Fermi achieved first place in the classification of the entrance exam.\n\nDuring his years at the \"Scuola Normale Superiore\", Fermi teamed up with a fellow student named Franco Rasetti with whom he would indulge in light-hearted pranks and who would later become Fermi's close friend and collaborator. In Pisa, Fermi was advised by the director of the physics laboratory, Luigi Puccianti, who acknowledged that there was little that he could teach Fermi, and frequently asked Fermi to teach him something instead. Fermi's knowledge of quantum physics reached such a high level that Puccianti asked him to organize seminars on the topic. During this time Fermi learned tensor calculus, a mathematical technique invented by Gregorio Ricci and Tullio Levi-Civita that was needed to demonstrate the principles of general relativity. Fermi initially chose mathematics as his major, but soon switched to physics. He remained largely self-taught, studying general relativity, quantum mechanics, and atomic physics.\n\nIn September 1920, Fermi was admitted to the Physics department. Since there were only three students in the department—Fermi, Rasetti, and Nello Carrara—Puccianti let them freely use the laboratory for whatever purposes they chose. Fermi decided that they should research X-ray crystallography, and the three worked to produce a Laue photograph—an X-ray photograph of a crystal. During 1921, his third year at the university, Fermi published his first scientific works in the Italian journal \"Nuovo Cimento\". The first was entitled \"On the dynamics of a rigid system of electrical charges in translational motion\" ('). A sign of things to come was that the mass was expressed as a tensor—a mathematical construct commonly used to describe something moving and changing in three-dimensional space. In classical mechanics, mass is a scalar quantity, but in relativity it changes with velocity. The second paper was \"On the electrostatics of a uniform gravitational field of electromagnetic charges and on the weight of electromagnetic charges\" ('). Using general relativity, Fermi showed that a charge has a weight equal to U/c, where U was the electrostatic energy of the system, and c is the speed of light.\n\nThe first paper seemed to point out a contradiction between the electrodynamic theory and the relativistic one concerning the calculation of the electromagnetic masses, as the former predicted a value of 4/3 U/c. Fermi addressed this the next year in a paper \"Concerning a contradiction between electrodynamic and the relativistic theory of electromagnetic mass\" in which he showed that the apparent contradiction was a consequence of relativity. This paper was sufficiently well-regarded that it was translated into German and published in the German scientific journal \"Physikalische Zeitschrift\" in 1922. That year, Fermi submitted his article \"On the phenomena occurring near a world line\" (\"\") to the Italian journal \"I Rendiconti dell'Accademia dei Lincei\". In this article he examined the Principle of Equivalence, and introduced the so-called \"Fermi coordinates\". He proved that on a world line close to the time line, space behaves as if it were a Euclidean space.\nFermi submitted his thesis, \"A theorem on probability and some of its applications\" (\"\"), to the \"Scuola Normale Superiore\" in July 1922, and received his laurea at the unusually young age of 20. The thesis was on X-ray diffraction images. Theoretical physics was not yet considered a discipline in Italy, and the only thesis that would have been accepted was one on experimental physics. For this reason, Italian physicists were slow in embracing the new ideas like relativity coming from Germany. Since Fermi was quite at home in the lab doing experimental work, this did not pose insurmountable problems for him.\n\nWhile writing the appendix for the Italian edition of the book \"Fundamentals of Einstein Relativity\" by August Kopff in 1923, Fermi was the first to point out that hidden inside the famous Einstein equation () was an enormous amount of nuclear potential energy to be exploited. \"It does not seem possible, at least in the near future\", he wrote, \"to find a way to release these dreadful amounts of energy—which is all to the good because the first effect of an explosion of such a dreadful amount of energy would be to smash into smithereens the physicist who had the misfortune to find a way to do it.\"\n\nIn 1924 Fermi was initiated into Freemasonry in the Masonic Lodge \"Adriano Lemmi\" of the Grand Orient of Italy.\n\nFermi decided to travel abroad, and spent a semester studying under Max Born at the University of Göttingen, where he met Werner Heisenberg and Pascual Jordan. Fermi then studied in Leiden with Paul Ehrenfest from September to December 1924 on a fellowship from the Rockefeller Foundation obtained through the intercession of the mathematician Vito Volterra. Here Fermi met Hendrik Lorentz and Albert Einstein, and became good friends with Samuel Goudsmit and Jan Tinbergen. From January 1925 to late 1926, Fermi taught mathematical physics and theoretical mechanics at the University of Florence, where he teamed up with Rasetti to conduct a series of experiments on the effects of magnetic fields on mercury vapour. He also participated in seminars at the Sapienza University of Rome, giving lectures on quantum mechanics and solid state physics. While giving lectures on the new quantum mechanics based on the remarkable accuracy of predictions of the Schrödinger equation, the Italian physicist would often say, \"It has no business to fit so well!\"\n\nAfter Wolfgang Pauli announced his exclusion principle in 1925, Fermi responded with a paper \"On the quantisation of the perfect monoatomic gas\" (\"\"), in which he applied the exclusion principle to an ideal gas. The paper was especially notable for Fermi's statistical formulation, which describes the distribution of particles in systems of many identical particles that obey the exclusion principle. This was independently developed soon after by the British physicist Paul Dirac, who also showed how it was related to the Bose–Einstein statistics. Accordingly, it is now known as Fermi–Dirac statistics. Following Dirac, particles that obey the exclusion principle are today called \"fermions\", while those that do not are called \"bosons\".\n\nProfessorships in Italy were granted by competition (\"\") for a vacant chair, the applicants being rated on their publications by a committee of professors. Fermi applied for a chair of mathematical physics at the University of Cagliari on Sardinia, but was narrowly passed over in favour of Giovanni Giorgi. In 1926, at the age of 24, he applied for a professorship at the Sapienza University of Rome. This was a new chair, one of the first three in theoretical physics in Italy, that had been created by the Minister of Education at the urging of Professor Orso Mario Corbino, who was the University's professor of experimental physics, the Director of the Institute of Physics, and a member of Benito Mussolini's cabinet. Corbino, who also chaired the selection committee, hoped that the new chair would raise the standard and reputation of physics in Italy. The committee chose Fermi ahead of Enrico Persico and Aldo Pontremoli, and Corbino helped Fermi recruit his team, which was soon joined by notable students such as Edoardo Amaldi, Bruno Pontecorvo, Ettore Majorana and Emilio Segrè, and by Franco Rasetti, whom Fermi had appointed as his assistant. They were soon nicknamed the \"Via Panisperna boys\" after the street where the Institute of Physics was located.\n\nFermi married Laura Capon, a science student at the University, on 19 July 1928. They had two children: Nella, born in January 1931, and Giulio, born in February 1936. On 18 March 1929, Fermi was appointed a member of the Royal Academy of Italy by Mussolini, and on 27 April he joined the Fascist Party. He later opposed Fascism when the 1938 racial laws were promulgated by Mussolini in order to bring Italian Fascism ideologically closer to German National Socialism. These laws threatened Laura, who was Jewish, and put many of Fermi's research assistants out of work.\n\nDuring their time in Rome, Fermi and his group made important contributions to many practical and theoretical aspects of physics. In 1928, he published his \"Introduction to Atomic Physics\" (\"\"), which provided Italian university students with an up-to-date and accessible text. Fermi also conducted public lectures and wrote popular articles for scientists and teachers in order to spread knowledge of the new physics as widely as possible. Part of his teaching method was to gather his colleagues and graduate students together at the end of the day and go over a problem, often from his own research. A sign of success was that foreign students now began to come to Italy. The most notable of these was the German physicist Hans Bethe, who came to Rome as a Rockefeller Foundation fellow, and collaborated with Fermi on a 1932 paper \"On the Interaction between Two Electrons\" ().\n\nAt this time, physicists were puzzled by beta decay, in which an electron was emitted from the atomic nucleus. To satisfy the law of conservation of energy, Pauli postulated the existence of an invisible particle with no charge and little or no mass that was also emitted at the same time. Fermi took up this idea, which he developed in a tentative paper in 1933, and then a longer paper the next year that incorporated the postulated particle, which Fermi called a \"neutrino\". His theory, later referred to as Fermi's interaction, and still later as the theory of the weak interaction, described one of the four fundamental forces of nature. The neutrino was detected after his death, and his interaction theory showed why it was so difficult to detect. When he submitted his paper to the British journal \"Nature\", that journal's editor turned it down because it contained speculations which were \"too remote from physical reality to be of interest to readers\". Thus Fermi saw the theory published in Italian and German before it was published in English.\n\nIn the introduction to the 1968 English translation, physicist Fred L. Wilson noted that:\n\nIn January 1934, Irène Joliot-Curie and Frédéric Joliot announced that they had bombarded elements with alpha particles and induced radioactivity in them. By March, Fermi's assistant Gian-Carlo Wick had provided a theoretical explanation using Fermi's theory of beta decay. Fermi decided to switch to experimental physics, using the neutron, which James Chadwick had discovered in 1932. In March 1934, Fermi wanted to see if he could induce radioactivity with Rasetti's polonium-beryllium neutron source. Neutrons had no electric charge, and so would not be deflected by the positively charged nucleus. This meant that they needed much less energy to penetrate the nucleus than charged particles, and so would not require a particle accelerator, which the Via Panisperna boys did not have.\nFermi had the idea to resort to replacing the polonium-beryllium neutron source with a radon-beryllium one, which he created by filling a glass bulb with beryllium powder, evacuating the air, and then adding 50 mCi of radon gas, supplied by Giulio Cesare Trabacchi. This created a much stronger neutron source, the effectiveness of which declined with the 3.8-day half-life of radon. He knew that this source would also emit gamma rays, but, on the basis of his theory, he believed that this would not affect the results of the experiment. He started by bombarding platinum, an element with a high atomic number that was readily available, without success. He turned to aluminium, which emitted an alpha particle and produced sodium, which then decayed into magnesium by beta particle emission. He tried lead, without success, and then fluorine in the form of calcium fluoride, which emitted an alpha particle and produced nitrogen, decaying into oxygen by beta particle emission. In all, he induced radioactivity in 22 different elements. Fermi rapidly reported the discovery of neutron-induced radioactivity in the Italian journal \"La Ricerca Scientifica\" on 25 March 1934.\n\nThe natural radioactivity of thorium and uranium made it hard to determine what was happening when these elements were bombarded with neutrons but, after correctly eliminating the presence of elements lighter than uranium but heavier than lead, Fermi concluded that they had created new elements, which he called hesperium and ausonium. The chemist Ida Noddack criticised this work, suggesting that some of the experiments could have produced lighter elements than lead rather than new, heavier elements. Her suggestion was not taken seriously at the time because her team had not carried out any experiments with uranium, and its claim to have discovered masurium (technetium) was disputed. At that time, fission was thought to be improbable if not impossible on theoretical grounds. While physicists expected elements with higher atomic numbers to form from neutron bombardment of lighter elements, nobody expected neutrons to have enough energy to split a heavier atom into two light element fragments in the manner that Noddack suggested.\nThe Via Panisperna boys also noticed some unexplained effects. The experiment seemed to work better on a wooden table than a marble table top. Fermi remembered that Joliot-Curie and Chadwick had noted that paraffin wax was effective at slowing neutrons, so he decided to try that. When neutrons were passed through paraffin wax, they induced a hundred times as much radioactivity in silver compared with when it was bombarded without the paraffin. Fermi guessed that this was due to the hydrogen atoms in the paraffin. Those in wood similarly explained the difference between the wooden and the marble table tops. This was confirmed by repeating the effect with water. He concluded that collisions with hydrogen atoms slowed the neutrons. The lower the atomic number of the nucleus it collides with, the more energy a neutron loses per collision, and therefore the fewer collisions that are required to slow a neutron down by a given amount. Fermi realised that this induced more radioactivity because slow neutrons were more easily captured than fast ones. He developed a diffusion equation to describe this, which became known as the Fermi age equation.\n\nIn 1938 Fermi received the Nobel Prize in Physics at the age of 37 for his \"demonstrations of the existence of new radioactive elements produced by neutron irradiation, and for his related discovery of nuclear reactions brought about by slow neutrons\". After Fermi received the prize in Stockholm, he did not return home to Italy, but rather continued to New York City with his family in December 1938, where they applied for permanent residency. The decision to move to America and become U.S. citizens was due primarily to the racial laws in Italy.\n\nFermi arrived in New York City on 2 January 1939. He was immediately offered posts at five universities, and accepted a post at Columbia University, where he had already given summer lectures in 1936. He received the news that in December 1938, the German chemists Otto Hahn and Fritz Strassmann had detected the element barium after bombarding uranium with neutrons, which Lise Meitner and her nephew Otto Frisch correctly interpreted as the result of nuclear fission. Frisch confirmed this experimentally on 13 January 1939. The news of Meitner and Frisch's interpretation of Hahn and Strassmann's discovery crossed the Atlantic with Niels Bohr, who was to lecture at Princeton University. Isidor Isaac Rabi and Willis Lamb, two Columbia University physicists working at Princeton, found out about it and carried it back to Columbia. Rabi said he told Enrico Fermi, but Fermi later gave the credit to Lamb:\nNoddack was proven right after all. Fermi had dismissed the possibility of fission on the basis of his calculations, but he had not taken into account the binding energy that would appear when a nuclide with an odd number of neutrons absorbed an extra neutron. For Fermi, the news came as a profound embarrassment, as the transuranic elements that he had partly been awarded the Nobel Prize for discovering had not been transuranic elements at all, but fission products. He added a footnote to this effect to his Nobel Prize acceptance speech.\nThe scientists at Columbia decided that they should try to detect the energy released in the nuclear fission of uranium when bombarded by neutrons. On 25 January 1939, in the basement of Pupin Hall at Columbia, an experimental team including Fermi conducted the first nuclear fission experiment in the United States. The other members of the team were Herbert L. Anderson, Eugene T. Booth, John R. Dunning, G. Norris Glasoe, and Francis G. Slack. The next day, the Fifth Washington Conference on Theoretical Physics began in Washington, D.C. under the joint auspices of George Washington University and the Carnegie Institution of Washington. There, the news on nuclear fission was spread even further, fostering many more experimental demonstrations.\n\nFrench scientists Hans von Halban, Lew Kowarski, and Frédéric Joliot-Curie had demonstrated that uranium bombarded by neutrons emitted more neutrons than it absorbed, suggesting the possibility of a chain reaction. Fermi and Anderson did so too a few weeks later. Leó Szilárd obtained of uranium oxide from Canadian radium producer Eldorado Gold Mines Limited, allowing Fermi and Anderson to conduct experiments with fission on a much larger scale. Fermi and Szilárd collaborated on a design of a device to achieve a self-sustaining nuclear reaction—a nuclear reactor. Owing to the rate of absorption of neutrons by the hydrogen in water, it was unlikely that a self-sustaining reaction could be achieved with natural uranium and water as a neutron moderator. Fermi suggested, based on his work with neutrons, that the reaction could be achieved with uranium oxide blocks and graphite as a moderator instead of water. This would reduce the neutron capture rate, and in theory make a self-sustaining chain reaction possible. Szilárd came up with a workable design: a pile of uranium oxide blocks interspersed with graphite bricks. Szilárd, Anderson, and Fermi published a paper on \"Neutron Production in Uranium\". But their work habits and personalities were different, and Fermi had trouble working with Szilárd.\n\nFermi was among the first to warn military leaders about the potential impact of nuclear energy, giving a lecture on the subject at the Navy Department on 18 March 1939. The response fell short of what he had hoped for, although the Navy agreed to provide $1,500 towards further research at Columbia. Later that year, Szilárd, Eugene Wigner, and Edward Teller sent the famous letter signed by Einstein to U.S. President Roosevelt, warning that Nazi Germany was likely to build an atomic bomb. In response, Roosevelt formed the Advisory Committee on Uranium to investigate the matter.\nThe Advisory Committee on Uranium provided money for Fermi to buy graphite, and he built a pile of graphite bricks on the seventh floor of the Pupin Hall laboratory. By August 1941, he had six tons of uranium oxide and thirty tons of graphite, which he used to build a still larger pile in Schermerhorn Hall at Columbia.\n\nThe S-1 Section of the Office of Scientific Research and Development, as the Advisory Committee on Uranium was now known, met on 18 December 1941, with the U.S. now engaged in World War II, making its work urgent. Most of the effort sponsored by the Committee had been directed at producing enriched uranium, but Committee member Arthur Compton determined that a feasible alternative was plutonium, which could be mass-produced in nuclear reactors by the end of 1944. He decided to concentrate the plutonium work at the University of Chicago. Fermi reluctantly moved, and his team became part of the new Metallurgical Laboratory there.\n\nThe possible results of a self-sustaining nuclear reaction were unknown, so it seemed inadvisable to build the first nuclear reactor on the University of Chicago campus in the middle of the city. Compton found a location in the Argonne Woods Forest Preserve, about from Chicago. Stone & Webster was contracted to develop the site, but the work was halted by an industrial dispute. Fermi then persuaded Compton that he could build the reactor in the squash court under the stands of the University of Chicago's Stagg Field. Construction of the pile began on 6 November 1942, and Chicago Pile-1 went critical on 2 December. The shape of the pile was intended to be roughly spherical, but as work proceeded Fermi calculated that criticality could be achieved without finishing the entire pile as planned.\n\nThis experiment was a landmark in the quest for energy, and it was typical of Fermi's approach. Every step was carefully planned, every calculation meticulously done. When the first self-sustained nuclear chain reaction was achieved, Compton made a coded phone call to James B. Conant, the chairman of the National Defense Research Committee.\nTo continue the research where it would not pose a public health hazard, the reactor was disassembled and moved to the Argonne Woods site. There Fermi directed experiments on nuclear reactions, revelling in the opportunities provided by the reactor's abundant production of free neutrons. The laboratory soon branched out from physics and engineering into using the reactor for biological and medical research. Initially, Argonne was run by Fermi as part of the University of Chicago, but it became a separate entity with Fermi as its director in May 1944.\n\nWhen the air-cooled X-10 Graphite Reactor at Oak Ridge went critical on 4 November 1943, Fermi was on hand just in case something went wrong. The technicians woke him early so that he could see it happen. Getting X-10 operational was another milestone in the plutonium project. It provided data on reactor design, training for DuPont staff in reactor operation, and produced the first small quantities of reactor-bred plutonium. Fermi became an American citizen in July 1944, the earliest date the law allowed.\n\nIn September 1944, Fermi inserted the first uranium fuel slug into the B Reactor at the Hanford Site, the production reactor designed to breed plutonium in large quantities. Like X-10, it had been designed by Fermi's team at the Metallurgical Laboratory, and built by DuPont, but it was much larger, and was water-cooled. Over the next few days, 838 tubes were loaded, and the reactor went critical. Shortly after midnight on 27 September, the operators began to withdraw the control rods to initiate production. At first all appeared to be well, but around 03:00, the power level started to drop and by 06:30 the reactor had shut down completely. The Army and DuPont turned to Fermi's team for answers. The cooling water was investigated to see if there was a leak or contamination. The next day the reactor suddenly started up again, only to shut down once more a few hours later. The problem was traced to neutron poisoning from xenon-135, a fission product with a half-life of 9.2 hours. DuPont had deviated from the Metallurgical Laboratory's original design in which the reactor had 1,500 tubes arranged in a circle, and had added 504 tubes to fill in the corners. The scientists had originally considered this over-engineering a waste of time and money, but Fermi realized that if all 2,004 tubes were loaded, the reactor could reach the required power level and efficiently produce plutonium.\nIn mid-1944, Robert Oppenheimer persuaded Fermi to join his Project Y at Los Alamos, New Mexico. Arriving in September, Fermi was appointed an associate director of the laboratory, with broad responsibility for nuclear and theoretical physics, and was placed in charge of F Division, which was named after him. F Division had four branches: F-1 Super and General Theory under Teller, which investigated the \"Super\" (thermonuclear) bomb; F-2 Water Boiler under L. D. P. King, which looked after the \"water boiler\" aqueous homogeneous research reactor; F-3 Super Experimentation under Egon Bretscher; and F-4 Fission Studies under Anderson. Fermi observed the Trinity test on 16 July 1945, and conducted an experiment to estimate the bomb's yield by dropping strips of paper into the blast wave. He paced off the distance they were blown by the explosion, and calculated the yield as ten kilotons of TNT; the actual yield was about 18.6 kilotons.\n\nAlong with Oppenheimer, Compton, and Ernest Lawrence, Fermi was part of the scientific panel that advised the Interim Committee on target selection. The panel agreed with the committee that atomic bombs would be used without warning against an industrial target. Like others at the Los Alamos Laboratory, Fermi found out about the atomic bombings of Hiroshima and Nagasaki from the public address system in the technical area. Fermi did not believe that atomic bombs would deter nations from starting wars, nor did he think that the time was ripe for world government. He therefore did not join the Association of Los Alamos Scientists.\n\nFermi became the Charles H. Swift Distinguished Professor of Physics at the University of Chicago on 1 July 1945, although he did not depart the Los Alamos Laboratory with his family until 31 December 1945. He was elected a member of the U.S. National Academy of Sciences in 1945. The Metallurgical Laboratory became the Argonne National Laboratory on 1 July 1946, the first of the national laboratories established by the Manhattan Project. The short distance between Chicago and Argonne allowed Fermi to work at both places. At Argonne he continued experimental physics, investigating neutron scattering with Leona Marshall. He also discussed theoretical physics with Maria Mayer, helping her develop insights into spin–orbit coupling that would lead to her receiving the Nobel Prize.\n\nThe Manhattan Project was replaced by the Atomic Energy Commission (AEC) on 1 January 1947. Fermi served on the AEC General Advisory Committee, an influential scientific committee chaired by Robert Oppenheimer. He also liked to spend a few weeks of each year at the Los Alamos National Laboratory, where he collaborated with Nicholas Metropolis, and with John von Neumann on Rayleigh–Taylor instability, the science of what occurs at the border between two fluids of different densities.\nFollowing the detonation of the first Soviet fission bomb in August 1949, Fermi, along with Isidor Rabi, wrote a strongly worded report for the committee, opposing the development of a hydrogen bomb on moral and technical grounds. Nonetheless, Fermi continued to participate in work on the hydrogen bomb at Los Alamos as a consultant. Along with Stanislaw Ulam, he calculated that not only would the amount of tritium needed for Teller's model of a thermonuclear weapon be prohibitive, but a fusion reaction could still not be assured to propagate even with this large quantity of tritium. Fermi was among the scientists who testified on Oppenheimer's behalf at the Oppenheimer security hearing in 1954 that resulted in denial of Oppenheimer's security clearance.\n\nIn his later years, Fermi continued teaching at the University of Chicago. His PhD students in the post-war period included Owen Chamberlain, Geoffrey Chew, Jerome Friedman, Marvin Goldberger, Tsung-Dao Lee, Arthur Rosenfeld and Sam Treiman. Jack Steinberger was a graduate student, and Mildred Dresselhaus was highly influenced by Fermi during the year she overlapped with him as a PhD student. Fermi conducted important research in particle physics, especially related to pions and muons. He made the first predictions of pion-nucleon resonance, relying on statistical methods, since he reasoned that exact answers were not required when the theory was wrong anyway. In a paper co-authored with Chen Ning Yang, he speculated that pions might actually be composite particles. The idea was elaborated by Shoichi Sakata. It has since been supplanted by the quark model, in which the pion is made up of quarks, which completed Fermi's model, and vindicated his approach.\n\nFermi wrote a paper \"On the Origin of Cosmic Radiation\" in which he proposed that cosmic rays arose through material being accelerated by magnetic fields in interstellar space, which led to a difference of opinion with Teller. Fermi examined the issues surrounding magnetic fields in the arms of a spiral galaxy. He mused about what is now referred to as the \"Fermi paradox\": the contradiction between the presumed probability of the existence of extraterrestrial life and the fact that contact has not been made.\nToward the end of his life, Fermi questioned his faith in society at large to make wise choices about nuclear technology. He said:\n\nFermi underwent an exploratory operation in Billings Memorial Hospital on 9 October 1954, after which he returned home. 50 days later, Fermi died at age 53 of stomach cancer in his home in Chicago, and was interred at Oak Woods Cemetery.\n\nFermi received numerous awards in recognition of his achievements, including the Matteucci Medal in 1926, the Nobel Prize for Physics in 1938, the Hughes Medal in 1942, the Franklin Medal in 1947, and the Rumford Prize in 1953. He was awarded the Medal for Merit in 1946 for his contribution to the Manhattan Project. Fermi was elected a Foreign Member of the Royal Society (FRS) in 1950. The Basilica of Santa Croce, Florence, known as the \"Temple of Italian Glories\" for its many graves of artists, scientists and prominent figures in Italian history, has a plaque commemorating Fermi. In 1999, \"Time\" named Fermi on its list of the top 100 persons of the twentieth century. Fermi was widely regarded as an unusual case of a 20th-century physicist who excelled both theoretically and experimentally. The historian of physics, C. P. Snow, wrote that \"if Fermi had been born a few years earlier, one could well imagine him discovering Rutherford's atomic nucleus, and then developing Bohr's theory of the hydrogen atom. If this sounds like hyperbole, anything about Fermi is likely to sound like hyperbole\".\n\nFermi was known as an inspiring teacher, and was noted for his attention to detail, simplicity, and careful preparation of his lectures. Later, his lecture notes were transcribed into books. His papers and notebooks are today in the University of Chicago. Victor Weisskopf noted how Fermi \"always managed to find the simplest and most direct approach, with the minimum of complication and sophistication.\" Fermi's ability and success stemmed as much from his appraisal of the art of the possible, as from his innate skill and intelligence. He disliked complicated theories, and while he had great mathematical ability, he would never use it when the job could be done much more simply. He was famous for getting quick and accurate answers to problems that would stump other people. Later on, his method of getting approximate and quick answers through back-of-the-envelope calculations became informally known as the \"Fermi method\", and is widely taught.\n\nFermi was fond of pointing out that Alessandro Volta, working in his laboratory, could have had no idea where the study of electricity would lead. Fermi is generally remembered for his work on nuclear power and nuclear weapons, especially the creation of the first nuclear reactor, and the development of the first atomic and hydrogen bombs. His scientific work has stood the test of time. This includes his theory of beta decay, his work with non-linear systems, his discovery of the effects of slow neutrons, his study of pion-nucleon collisions, and his Fermi–Dirac statistics. His speculation that a pion was not a fundamental particle pointed the way towards the study of quarks and leptons.\n\nMany things bear Fermi's name. These include the Fermilab particle accelerator and physics lab in Batavia, Illinois, which was renamed in his honor in 1974, and the Fermi Gamma-ray Space Telescope, which was named after him in 2008, in recognition of his work on cosmic rays. Three nuclear reactor installations have been named after him: the Fermi 1 and Fermi 2 nuclear power plants in Newport, Michigan, the Enrico Fermi Nuclear Power Plant at Trino Vercellese in Italy, and the RA-1 Enrico Fermi research reactor in Argentina. A synthetic element isolated from the debris of the 1952 Ivy Mike nuclear test was named fermium, in honor of Fermi's contributions to the scientific community. This makes him one of 16 scientists who have elements named after them.\n\nSince 1956, the United States Atomic Energy Commission has named its highest honor, the Fermi Award, after him. Recipients of the award include well-known scientists like Otto Hahn, Robert Oppenheimer, Edward Teller and Hans Bethe.\n\nFor a full list of his papers, see pages 75–78 in ref.\n\n\n"}
{"id": "43434834", "url": "https://en.wikipedia.org/wiki?curid=43434834", "title": "For the Best and for the Onion", "text": "For the Best and for the Onion\n\nFor the Best and for the Onion () is a 2008 Nigerien documentary film about onion farmers in Galmi, Niger, written and directed by Sani Elhadj Magori.\n\nFilmed in the director's hometown over the course of a single growing season, the film follows how the price of onions affects the lives of two young villagers who wish to wed, while the father of the would-be bride, Yaro, struggles to make enough from his crop to be able to offer his daughter a fitting marriage.\n\n\n"}
{"id": "25778920", "url": "https://en.wikipedia.org/wiki?curid=25778920", "title": "GE Wind (offshore)", "text": "GE Wind (offshore)\n\nGE Power's offshore wind business is a joint venture with Alstom, created in 2015 when most of that company's other electrical power and generation assets were acquired. GE's stake in the joint venture is 50 % plus 1 share.\n\nFormerly known as Alstom Wind, originally Alstom Ecotècnia, the company was the wind power company of energy infrastructure company Alstom, within its 'Power' operating division, from 2010 to 2015. The company originated as Ecotècnia S.c.c.l., a Spanish wind power equipment manufacturing and installation company established in 1981, acquired by Alstom 2007 for €350 million.\n\nThe subsidiary's main product is the 6MW 'Haliade' offshore wind turbine, one of the most powerful turbines on Earth]]\n\nEcotècnia was a manufacturer and installer of wind turbines established in 1981, headquartered in Barcelona, Spain. In 1999 it became part of the Basque-based cooperative Mondragon Corporation.\n\nThe company's first wind generator was a 30 kW machine, developed by 1984 with funding assistance from the Spanish Science Ministry. In 1991 the company developed a 150 kW machine, and in 1992 won its first commercial project - for fifty 150 kW turbines at Tarifa, Spain. The particular demands of installing wind turbines in mountainous regions in Spain which included poor road access and blustery (high turbulence) conditions led to specific design features of Ecotècnia's turbines - including a modular construction of the turbine (three components: rotor and shaft; mainframe and yaw system; and the drive train - each less than 30t), as well as isolation of the gearbox from the main drive, reducing non-torque gearbox loads.\n\nIn 2005 the company's estimate world market share (by installed capacity) was 2.1%. By 2007 the company had installed over wind farms with over 1GW total of rate power; the company had increased the power output of its wind turbine offering from 30 kW in 1984 to 1.67MW by 2003.\n\nIn the first half of the 1990s the company installed wind farms using its \"ECO20\" 150 kW model, from 1995 to 2000 the company's primary models were the \"ECO44\" (640 kW) and \"ECO48\" (750 kW) models. After 2003 most of the company's installations used its 1.67MW IEC-1400-1 class II \"ECO74\", and class IIIA \"ECO80\", three blade turbine models; both typically used a mechanically isolated Winergy AG PEAB 4390.2 planetary gearbox driving a doubly fed induction generator (typically ABB, Siemens, Winergy sourced) with IGBT inverter control, driven by individually pitch controlled LM manufactured blades.\n\nThe majority of its installations were in Spain, with approximately 10% in other countries including Portugal, France, Italy as well as installations in Japan and India.\n\nIn mid 2007 Alstom acquired Ecotècnia for 350 million euro. By late 2007 the company employed over 800 people, with sales of approximately €400 million pa., and operated wind turbine assembly plants in Somozas and Buñuel, other sites at Rio del Pozo (control panels) and Coreses (tower manufacturing). The company also manufactured small scale autonomous energy systems for isolated locations (tradename 'CICLOPS') comprising windgenerator (10 kW), photovoltaic solar source (2 to 10 kW), generator, battery and inverter, and was active in small scale photovoltaic cell installation (factory Pla de Santa Maria.).\n\nThe first \"ECO100\" 3MW wind turbine was formally inaugurated at the Vieux Moulin wind farm (Pithiviers, Paris, France) 1 October 2009.\n\nThe company was renamed \"Alstom Wind S.L.\" in April 2010.\n\nIn 2010 Alstom began construction of a turbine nacelle factory in Amarillo, USA, completed in mid 2011. In November 2011 a 300MW per year capacity manufacturing plant in Camaçari near Salvador, Bahia, Brazil was formally opened.\n\nIn March 2012 a prototype of Alstom \"Haliade 150\" 6MW offshore turbine was formally inaugurated; the turbine was developed for large scale offshore wind projects off the French coast; in January 2011 Alstom joined a consortium led by EDF Energies Nouvelles as turbine supplier (also including Dong Energy, Nass & Wind, WPD Offshore) to bid for proposed offshore wind farms in France of 6GW capacity. The \"Haliade 150\" turbine was a development of the previous mechanically isolated transmission designs developed by Ecotècnia; the gearbox driven electrically generator was omitted and replaced with a gearless direct drive permanent magnet synchronous generator designed for higher efficiency and greater reliability with fewer moving parts. The turbine uses 73.5m turbine blades from LM Wind Power.\n\nIn April 2012 the French state awarded an EDF/Dong Energy/Alstom consortium three contracts for offshore wind farms (Saint-Nazaire, Courseulles-sur-Mer, Fécamp.) off the northwestern coast of France of total power 1.4GW. As a result, Alstom confirmed that it would be constructing wind turbine factories at Cherbourg-en-Cotentin (Turbine blades in association with LM Power, wind turbine towers), and at Saint-Nazaire (Nacelles and generators) in the Montoir-de-Bretagne port area.\n\nA 6MW 'Haliade' began generating electricity in July 2012 during certification testing. The turbines electrical generator is supplied by partner company, General Electric subsidiary GE Power Conversion (Converteam), who are also expected to establish a production plant in Saint Nazaire.\n\nIn late 2012 Alstom announced the intention to construct a wind turbine tower manufacturing facility in Canoas, Rio Grande do Sul, Brazil, on a site adjacent to an established Alstom power transmission factory. The plant was officially inaugurated in August 2013.\n\nIn Feb. 2013 the company announced the cut of 35% of its workforce in Spain, due to a collapse in the Spanish wind market due to a change to governmental support for wind developments. The facilities at Somozas, Galicia (electrical equipment), and Zamora, Castille and Leon region (towers) were to close with electrical equipment production moved to the plant at Buñuel, Navarra. Total job losses were later reduced from 373 to 265.\n\nA \"Haliade\" 150 turbine was installed at the Belwind wind farm in November 2013 for operational tests, at the time being one of the largest operational wind turbines in the world.\n\nIn December 2014 the St. Nazairre nacelle and generator assembly factory was officially opened.\n\nIn early 2015 a wind turbine tower factory was opened in Jacobina, Bahia, Brazil; constructed as a joint venture (51%/49%) between Andrade Gutierrez and Alstom.\n\nInitial orders for the company's Haliade wind turbine included 5 units for the Block Island Wind Farm (USA, March 2015); and 66 units for DEME for the Merkur offshore wind farm (Germany, July 2015).\n\nIn Nov 2015 most of Alstom's energy generation and transmission assets were acquired by General Electric, the combined business under GE was to be named GE Power.. The two parties also created three joint ventures, among them a hydro and offshore\nrenewable business unit.\n\nIn February 2016 the first of a series production of 300 permanent magnet wind generators was completed at the St. Nazaire factory. A long planned test period for the 6MW Haliade is expected to start in Spring 2016 at Østerild Wind Turbine Test Field.\n\n\n"}
{"id": "4291021", "url": "https://en.wikipedia.org/wiki?curid=4291021", "title": "Great White Spot", "text": "Great White Spot\n\nThe Great White Spot, also known as Great White Oval, on Saturn, named by analogy to Jupiter's Great Red Spot, are periodic storms that are large enough to be visible by telescope from Earth by their characteristic white appearance. The spots can be several thousands of kilometers wide.\n\nThe \"Cassini\" orbiter was able to track the 2010-2011 instance of the storm, also known as the Northern Electrostatic Disturbance because of an increase in radio and plasma interference, or the Great Springtime Storm.\n\nCassini data has revealed a loss of acetylene in the white clouds, an increase of phosphine, and an unusual temperature drop in the center of the storm. After the visible aspects of the storm subsided, in 2012 a \"belch\" of heat and ethylene was emitted from two hotspots that merged.\n\nThe phenomenon is somewhat periodic at 28.5-year intervals, when Saturn's northern hemisphere tilts most toward the Sun. The following is a list of recorded sightings.\n\nThat none were recorded before 1876 is a mystery, in some ways akin to the long observational gap of the Great Red Spot in the 18th and early 19th centuries; the 1876 Great White Spot (GWS) was extremely prominent, being visible in apertures as small as 60 mm. It is not known if the earlier record was simply poor, or if the 1876 GWS was truly a first for the telescopic era. Some believe that neither scenario is likely.\n\nIn 1992, Mark Kidger described three significant GWS patterns:\n\nBased on these apparent regularities, in 1992 Kidger forecasted (incorrectly, given the 2010-11 storm) that the next GWS would occur in the North Temperate Zone in 2016, and would probably be less spectacular than the 1990 GWS.\n\nA \"classic\" Great White Spot is a spectacular event, in which a brilliant white storm enlivens Saturn's usually bland atmosphere; all the major ones have occurred in the planet's northern hemisphere. They typically begin as discrete, literal \"spots\", but then rapidly expand in longitude, as the 1933 and 1990 GWSs did; in fact, the latter eventually lengthened enough to encircle the planet.\n\nThough computer modeling had by the early 1990s suggested these massive atmospheric upwellings were caused by thermal instability, in 2015 two Caltech planetary scientists proposed a more detailed mechanism. The theory is that as Saturn's upper atmosphere undergoes seasonal cooling, it first gets less dense as the heavier water rains out, passes a density minimum, and then gets more dense as the remaining hydrogen and helium continue to cool. Low-density upper-layer gases tend to suppress convection, but high-density upper layers are unstable and cause a thunderstorm when they break into lower layers. The theory is that the storms are significantly delayed from the winter solstice due to the time it takes for the very large atmosphere to cool. The team proposes that similar storms are not seen on Jupiter because that planet has less water vapor in its upper atmosphere.\n\nSaturn's rings block the view of the northern hemisphere from Earth during the winter solstice, so historical data on the GWS is unavailable during this season, but the Cassini space probe has been able to observe the whole planet since it arrived shortly after the winter solstice in 2004.\n\n\n\n"}
{"id": "25416383", "url": "https://en.wikipedia.org/wiki?curid=25416383", "title": "Grid Trade Master Agreement", "text": "Grid Trade Master Agreement\n\nThe Grid Trade Master Agreement (GTMA) is an agreement for trading electricity within the United Kingdom.\n\nIt is normally used either when a power station has excess electricity, which it wishes to \"sell back\" to the power grid, or when the electricity company wishes to buy surplus power to meet a surge in demand, or reduction in supply.\n\nAlthough electricity is a non-tangible asset, these agreements are used in order to trade guarantees to provide power for a certain length of time.\n"}
{"id": "37838", "url": "https://en.wikipedia.org/wiki?curid=37838", "title": "Hall-effect thruster", "text": "Hall-effect thruster\n\nIn spacecraft propulsion, a Hall-effect thruster (HET) is a type of ion thruster in which the propellant is accelerated by an electric field. Hall-effect thrusters trap electrons in a magnetic field and then use the electrons to ionize propellant, efficiently accelerate the ions to produce thrust, and neutralize the ions in the plume. Hall-effect thrusters (based on the discovery by Edwin Hall) are sometimes referred to as Hall thrusters or Hall-current thrusters. Hall thrusters are often regarded as a moderate specific impulse (1,600s) space propulsion technology. The Hall-effect thruster has benefited from considerable theoretical and experimental research since the 1960s.\n\nHall thrusters operate on a variety of propellants, the most common being xenon. Other propellants of interest include krypton, argon, bismuth, iodine, magnesium, and zinc.\n\nHall thrusters are able to accelerate their exhaust to speeds between 10 and 80 km/s (1,000–8,000 s specific impulse), with most models operating between 15 and 30 km/s (1,500–3,000 s specific impulse).\n\nThe thrust produced by a Hall thruster varies depending on the power level. Devices operating at 1.35 kW produce about 83 mN of thrust. High-power models have demonstrated up to 5.4 N in the laboratory. Power levels up to 100 kW have been demonstrated by xenon Hall thrusters.\n\n, Hall-effect thrusters ranged in input power levels from 1.35 to 10 kilowatts and had exhaust velocities of 10–50 kilometers per second, with thrust of 40–600 millinewtons and efficiency in the range of 45–60 percent.\n\nThe applications of Hall-effect thrusters include control of the orientation and position of orbiting satellites and use as a main propulsion engine for medium-size robotic space vehicles.\n\nHall thrusters were studied independently in the United States and the Soviet Union. They were first described publicly in the US in the early 1960s. However, the Hall thruster was first developed into an efficient propulsion device in the Soviet Union. In the US, scientists focused instead on developing gridded ion thrusters.\n\nTwo types of Hall thrusters were developed in the Soviet Union:\nThe SPT design was largely the work of A. I. Morozov. The first SPT to operate in space, an SPT-50 aboard a Soviet Meteor spacecraft, was launched December 1971. They were mainly used for satellite stabilization in North-South and in East-West directions. Since then until the late 1990s 118 SPT engines completed their mission and some 50 continued to be operated. Thrust of the first generation of SPT engines, SPT-50 and SPT-60 was 20 and 30 mN respectively. In 1982, SPT-70 and SPT-100 were introduced, their thrusts being 40 and 83 mN, respectively. In the post-Soviet Russia high-power (a few kilowatts) SPT-140, SPT-160, SPT-200, T-160 and low-power (less than 500 W) SPT-35 were introduced.\n\nSoviet and Russian TAL-type thrusters include the D-38, D-55, D-80, and D-100.\n\nSoviet-built thrusters were introduced to the West in 1992 after a team of electric propulsion specialists from NASA's Jet Propulsion Laboratory, Glenn Research Center, and the Air Force Research Laboratory, under the support of the Ballistic Missile Defense Organization, visited Russian laboratories and experimentally evaluated the SPT-100 (i.e., a 100 mm diameter SPT thruster). Over 200 Hall thrusters have been flown on Soviet/Russian satellites in the past thirty years. No failures have ever occurred on orbit. Hall thrusters continue to be used on Russian spacecraft and have also flown on European and American spacecraft. Space Systems/Loral, an American commercial satellite manufacturer, now flies Fakel SPT-100's on their GEO communications spacecraft.\n\nSince their introduction to the west in the early 1990s, Hall thrusters have been the subject of a large number of research efforts throughout the United States, France, Italy, Japan, and Russia (with many smaller efforts scattered in various countries across the globe). Hall thruster research in the US is conducted at several government laboratories, universities and private companies. Government and government funded centers include NASA's Jet Propulsion Laboratory, NASA's Glenn Research Center, the Air Force Research Laboratory (Edwards AFB, CA), and The Aerospace Corporation. Universities include the US Air Force Institute of Technology, University of Michigan, Stanford University, The Massachusetts Institute of Technology, Princeton University, Michigan Technological University, and Georgia Tech. A considerable amount of development is being conducted in industry, such as IHI in Japan, Aerojet and Busek in the USA, SNECMA in France, LAJP in Ukraine, and SITAEL in Italy.\n\nThe first use of Hall thrusters on lunar orbit was the European Space Agency (ESA) lunar mission SMART-1 in 2003.\n\nOn a western satellite Hall thrusters were first demonstrated on the Naval Research Laboratory (NRL) STEX spacecraft, which flew the Russian D-55. The first American Hall thruster to fly in space was the Busek BHT-200 on TacSat-2 technology demonstration spacecraft. The first flight of an American Hall thruster on an operational mission, was the Aerojet BPT-4000, which launched August 2010 on the military Advanced Extremely High Frequency GEO communications satellite. At 4.5 kW, the BPT-4000 is also the highest power Hall thruster ever flown in space. Besides the usual stationkeeping tasks, the BPT-4000 is also providing orbit raising capability to the spacecraft. Several countries worldwide continue efforts to qualify Hall thruster technology for commercial uses.\n\nThe essential working principle of the Hall thruster is that it uses an electrostatic potential to accelerate ions up to high speeds. In a Hall thruster, the attractive negative charge is provided by an electron plasma at the open end of the thruster instead of a grid. A radial magnetic field of about 100–300 G (0.01–0.03 T) is used to confine the electrons, where the combination of the radial magnetic field and axial electric field cause the electrons to drift in azimuth thus forming the Hall current from which the device gets its name.\nA schematic of a Hall thruster is shown in the adjacent image. An electric potential between 150 and 800 volts is applied between the anode and cathode.\n\nThe central spike forms one pole of an electromagnet and is surrounded by an annular space, and around that is the other pole of the electromagnet, with a radial magnetic field in between.\n\nThe propellant, such as xenon gas, is fed through the anode, which has numerous small holes in it to act as a gas distributor. Xenon propellant is used because of its high atomic weight and low ionization potential. As the neutral xenon atoms diffuse into the channel of the thruster, they are ionized by collisions with circulating high-energy electrons (typically 10–40 eV, or about 10% of the discharge voltage). Once ionized, the xenon ions typically have a charge of +1, though a small fraction (~20%) have +2.\n\nThe xenon ions are then accelerated by the electric field between the anode and the cathode. For discharge voltages of 300 V, the ions reach speeds of around 15 km/s (9.3 mps) for a specific impulse of 1,500 seconds (15 kN·s/kg). Upon exiting, however, the ions pull an equal number of electrons with them, creating a plasma plume with no net charge.\n\nThe radial magnetic field is designed to be strong enough to substantially deflect the low-mass electrons, but not the high-mass ions, which have a much larger gyroradius and are hardly impeded. The majority of electrons are thus stuck orbiting in the region of high radial magnetic field near the thruster exit plane, trapped in E×B (axial electric field and radial magnetic field). This orbital rotation of the electrons is a circulating Hall current, and it is from this that the Hall thruster gets its name. Collisions with other particles and walls, as well as plasma instabilities, allow some of the electrons to be freed from the magnetic field, and they drift towards the anode.\n\nAbout 20–30% of the discharge current is an electron current, which does not produce thrust, thus limiting the energetic efficiency of the thruster; the other 70–80% of the current is in the ions. Because the majority of electrons are trapped in the Hall current, they have a long residence time inside the thruster and are able to ionize almost all of the xenon propellant, allowing mass use of 90–99%. The mass use efficiency of the thruster is thus around 90%, while the discharge current efficiency is around 70%, for a combined thruster efficiency of around 63% (= 90% × 70%). Modern Hall thrusters have achieved efficiencies as high as 75% through advanced designs.\n\nCompared to chemical rockets, the thrust is very small, on the order of 83 mN for a typical thruster operating at 300 V, 1.5 kW. For comparison, the weight of a coin like the U.S. quarter or a 20-cent Euro coin is approximately 60 mN. As with all forms of electrically powered spacecraft propulsion, thrust is limited by available power, efficiency, and specific impulse.\n\nHowever, Hall thrusters operate at the high specific impulses that is typical for electric propulsion. One particular advantage of Hall thrusters, as compared to a gridded ion thruster, is that the generation and acceleration of the ions takes place in a quasi-neutral plasma, so there is no Child-Langmuir charge (space charge) saturated current limitation on the thrust density. This allows much smaller thrusters compared to gridded ion thrusters.\n\nAnother advantage is that these thrusters can use a wider variety of propellants supplied to the anode, even oxygen, although something easily ionized is needed at the cathode.\n\nAlthough conventional (annular) Hall thrusters are efficient in the kilowatt power regime, they become inefficient when scaled to small sizes. This is due to the difficulties associated with holding the performance scaling parameters constant while decreasing the channel size and increasing the applied magnetic field strength. This led to the design of the cylindrical Hall thruster. The cylindrical Hall thruster can be more readily scaled to smaller sizes due to its nonconventional discharge-chamber geometry and associated magnetic field profile. The cylindrical Hall thruster more readily lends itself to miniaturization and low-power operation than a conventional (annular) Hall thruster. The primary reason for cylindrical Hall thrusters is that it is difficult to achieve a regular Hall thruster that operates over a broad envelope from ~1 kW down to ~100 W while maintaining an efficiency of 45-55%.\n\nSputtering erosion of discharge channel walls and pole pieces that protect the magnetic circuit causes failure of thruster operation. Therefore, annular and cylindrical Hall thrusters have limited lifetime. Although magnetic shielding has been shown to dramatically reduce discharge channel wall erosion, pole piece erosion is still a concern. As an alternative, an unconventional Hall thruster design called external discharge Hall thruster or external discharge plasma thruster (XPT) has been introduced. External discharge Hall thruster does not possess any discharge channel walls or pole pieces. Plasma discharge is produced and sustained completely in open space outside the thruster structure, and thus erosion free operation is achieved.\n\nHall thrusters have been flying in space since December 1971 when the Soviets launched an SPT-50 on a Meteor satellite. Over 240 thrusters have flown in space since that time with a 100% success rate. Hall thrusters are now routinely flown on commercial GEO communications satellites where they are used for orbital insertion and stationkeeping.\n\nThe first Hall thruster to fly on a western satellite was a Russian D-55 built by TsNIIMASH, on the NRO's STEX spacecraft, launched on October 3, 1998.\n\nThe solar electric propulsion system of the European Space Agency's SMART-1 spacecraft used a Snecma PPS-1350-G Hall thruster. SMART-1 was a technology demonstration mission that orbited the Moon. This use of the PPS-1350-G, starting on September 28, 2003, was the first use of a Hall thruster outside geosynchronous earth orbit (GEO). Unlike most Hall thruster propulsion systems used in commercial applications, the Hall thruster on SMART-1 could be throttled over a range of power, specific impulse, and thrust. It has a discharge power range of 0.46–1.19 kW, a specific impulse of 1,100–1,600 s and thrust of 30–70 mN.\n\nThe largest planned Hall-effect thruster is NASA's 40 kW Advanced Electric Propulsion System (AEPS), meant to propel large-scale science missions and cargo transportation in deep space.\n\n"}
{"id": "19985412", "url": "https://en.wikipedia.org/wiki?curid=19985412", "title": "Jan Hamrin", "text": "Jan Hamrin\n\nJan Hamrin has spent 30 years promoting renewable energy through research, policy formulation, and the development of consumer programs. She founded the non-profit Center for Resource Solutions in 1997 and retired as president of CRS at the end of 2007.\n\nHamrin has served as advisor to the G-8 and numerous legislatures and regulatory commissions as well as co-authoring two books: \"Affected with the Public Interest: Electric Industry Restructuring in an Era of Competition\" (1994) and \"Investing in the Future: A Regulator’s Guide to Renewables\" (1993). In 1981 Hamrin founded and served for nine years as Executive Director of the Independent Energy Producers’ Association (IEP) in California and played a key role in the implementation of the Public Utilities Regulatory Policies Act (PURPA) in California and elsewhere.\n\nJan Hamrin received her Ph.D. in Ecology, with emphasis on public policy evaluation of environmental and energy programs, from the University of California, Davis. She also holds master's degrees in Public Administration from U.C. Davis as well as a B.S. from the University of New Mexico.\n\nIn October 2008, Hamrin received a Green Power Leadership \"Pioneer\" award from the U.S. Environmental Protection Agency (EPA)'s Green Power Partnership, and the U.S. Department of Energy, for a career spent \"building the market for green power\".\n"}
{"id": "26998589", "url": "https://en.wikipedia.org/wiki?curid=26998589", "title": "Landsnet", "text": "Landsnet\n\nLandsnet is a transmission system operator of the Icelandic high-voltage power grid. It is a public company owned by Landsvirkjun (64.73%), Rafmagnsveitur rikisins (22.51%), Orkuveita Reykjavíkur (6.78%), and Orkubu Vestfjarða (5.98%).\n\nLandsnet was established in 2005 by separating from Landsvirkjun. It is a member of the European Network of Transmission System Operators for Electricity.\n"}
{"id": "9130142", "url": "https://en.wikipedia.org/wiki?curid=9130142", "title": "Lars Haltbrekken", "text": "Lars Haltbrekken\n\nLars Haltbrekken (born 9 March 1971) is a Norwegian environmentalist and was elected to the Parliament of Norway in 2017 for the Socialist Left Party (Norway). He was chairman of Friends of the Earth Norway from 2005 to 2016 after having been deputy chairman from 2003-2005. Haltbrekken was also chairman of Natur og Ungdom in 1995 and 1996. In between the two leaderships he worked primarily trying to prevent natural gas power plants in Norway, and was chairman of Fellesaksjonen mot gasskraftverk. He grew up in Trondheim.\n"}
{"id": "35938255", "url": "https://en.wikipedia.org/wiki?curid=35938255", "title": "Linear topology", "text": "Linear topology\n\nIn algebra, a linear topology on a left \"A\"-module \"M\" is a topology on \"M\" that is invariant under translations and admits a fundamental system of neighborhood of 0 that consists of submodules of \"M\". If there is such a topology, \"M\" is said to be linearly topologized. If \"A\" is given a discrete topology, then \"M\" becomes a topological \"A\"-module with respect to a linear topology.\n\n"}
{"id": "16885900", "url": "https://en.wikipedia.org/wiki?curid=16885900", "title": "Louis-Élisabeth de La Vergne de Tressan", "text": "Louis-Élisabeth de La Vergne de Tressan\n\nLouis-Élisabeth de la Vergne, comte de Tressan (4 November 1705, Le Mans - 31 October 1783, from a fall from a carriage en route to Saint-Leu-la-Forêt) was a French soldier, physician, scientist, medievalist and writer, best known for his adaptations of \"romans chevaleresques\" of the Middle Ages, which contributed to the rise of the Troubadour style in the French arts.\n\nAged 13, he was the companion of the young Louis XV, then lieutenant-général and king's aide de camp at the battle of Fontenoy. He was made governor of Toul and was summoned by king Stanislas to his court at Lunéville, where he received the title of grand marshal.\n\nThe first director of the Société Royale des Sciences et Belles-Lettres of Nancy in 1751 and a member of several other academies in France and abroad, he was elected a member of the Académie des sciences in 1749 and of the Académie française in 1780.\n\nA friend of Voltaire and Buffon, he frequented the salon of Mme de Tencin and composed several odes as well as adaptations of chivalric romances, which he translated and adapted from Spanish and Old French, into editions which would be re-issued several times. He was also the author of one of the first treatises on electricity in French, and collaborated on volumes VI and VII of the \"Encyclopédie\" of Diderot and D’Alembert.\n\n\n"}
{"id": "4031918", "url": "https://en.wikipedia.org/wiki?curid=4031918", "title": "Lungmen Nuclear Power Plant", "text": "Lungmen Nuclear Power Plant\n\nThe Lungmen Nuclear Power Plant () (formerly Gongliao Nuclear Power Plant and Fourth Nuclear Power Plant, often abbreviated as: ), located nearby Fulong Beach, Gongliao District, New Taipei City, is Taiwan's fourth nuclear power plant, consisting of two ABWRs each of 1,300 MWe net. It is owned by Taiwan Power Company (Taipower). It was intended to be the first of these advanced Generation III reactors built outside Japan. In 2014 construction of the plant was deferred.\n\nThe preceding four reactors in Japan were completed in four to five years. Taipower, however, did not award the contract as a turnkey plant, but hired General Electric to build the reactors, Mitsubishi Heavy Industries to supply the turbines and the generators, and other contractors for the rest, making the project very difficult to manage. In addition, the project was canceled in 2000 by the government when it was 10-30% complete, only to restart the following year. All these have resulted in significant cost overruns, while in 2011 the Taiwan Atomic Energy Council (AEC) criticized Taipower's management of the project. The construction has been delayed by legal, regulatory and political delays. The proposed 2014 national referendum to decide the fate of the power plant was rejected from the ballot for contradictory and confusing language, despite gathering more than 120,000 signatures.\nIn 2018, Taipower started shipping unused fuel rods back to the US. Taipower aims to remove all fuel rods from the plant by 2020. The ruling Democratic Progressive Party plans to shut down all nuclear plants in Taiwan by 2025. The removal of the fuel rods may mean that the plant's construction will never be restarted. \n\nThe Fourth Nuclear Power Plant was first proposed in 1978, and Taipower selected the ABWR in 1996 after a competitive bidding process. The reactor is designed by General Electric, but is supported by Hitachi, Shimizu Corporation, Toshiba, and other American, Taiwanese, and other Chinese and international companies.\n\nConstruction of the plant began in 1999 and was expected to be completed in 2004. Delays in construction (some for political reasons) have increased the cost of the project. Costs have also increased from increases in the costs of raw materials from 2003.\n\nThe 921 earthquake in 1999 prompted three legislators to inspect construction progress; they cited rusty rebar and potential seawater seepage into the plant's foundation as potential issues. President Chen Shui-bian was elected along with other Democratic Progressive Party (DPP) legislators in March 2000 on an anti-nuclear platform which included stopping plant construction. DPP legislators called for a halt to the project in April 2000, which led to the suspension of construction in October 2000 by Premier Chang Chun-hsiung. Contractors were idled for 111 days from 2000 October 27 to 14 February 2001. This delay caused a 576-day delay towards commercial operation.\n\nTaipower constructed a wharf in 2003 to accommodate delivery of heavy components, such as the reactor vessel. The wharf was beset with construction delays and may have contributed to local beach erosion, prompting further protests. Each reactor vessel delivery was greeted with activist protests and construction delays resulted in the vessels being stored longer than expected, with Unit 1's reactor vessel not installed until 2005.\n\nDesign of the plant was carried out by Stone & Webster (S&W), but Taipower canceled its contract with S&W in 2007, leading to protracted litigation from 2007 to 2011.\n\nIn 2010, aboriginal artifacts were found at the construction site, prompting calls to halt construction again.\n\nTaipower was fined by the AEC twice, in 2008 and 2011, for completing design changes without first obtaining approval from the plant's designer, General Electric. Taipower later stated the changes were executed to stay on schedule, and approval was later obtained from GE for 97% of the changes.\n\nThe temporary cancellation by the Government and other project management difficulties caused significant delays, pushing the price tag of the plant to more than US$7.5 billion in 2009. , the total price tag was close to US$10 billion, or NT$300 billion.\n\nIn August 2014, the plant completed its safety tests required before operation can begin. 126 systems were tested including cooling, shutdown, containment, control and power generation. Minister of Economic Affairs Chang Chia-juch said, \"Passing the rigorous review illustrates the high standards of care invested in the design and construction of the facility.\" The reactor will be closed until the status is decided by referendum. In the meantime, the reactor costs 1.3 billion NT Dollars a year to maintain in a mothballed state.\nEnvironmental groups called for a national referendum in April 2011, following the Fukushima Daiichi nuclear disaster. By obtaining the signatures of a certain percentage of registered voters, or by legislative action, a referendum may be referred to the national ballot for voter action. However, due to the politically sensitive nature of the Fourth Nuclear Plant issue, legislatively-referred and voter-referred referendums have failed to enter the national ballot.\n\nBecause of the controversy over the plant, in February 2013 the Kuomintang (KMT)-led government proposed that a referendum should be issued, which would allow the people of the country to decide the fate of the plant. The proposed referendum was sponsored by 32 KMT lawmakers led by Lee Ching-hua and asked \"Do you agree that the construction of the Fourth Nuclear Power Plant should be halted and that it not become operational?\" Referendums must have good voter turnout (greater than 50% of registered voters must participate in the referendum vote), and a majority of participating voters must vote yes in order for the referendum to become binding, meaning that poor voter turnout would ensure referendum defeat (and in the case of the Lee referendum, continued construction). The opposition DPP claimed that getting better than 50% voter turnout in a non-Presidential election year was too difficult, and barricaded the legislative chamber to prevent a vote to place the Lee referendum on the national ballot. This led to a scuffle when KMT legislators attempted to break through the barricade, and the Lee referendum was not voted on. In the following legislative session, protesters called on the government to withdraw the Lee referendum, and it was subsequently withdrawn.\n\nAnnette Lu used the local referendum process to debate the plant's fate as early as 2012. Her petition for a local referendum gathered 50,000 signatures by March 2013, but the proposed referendum was rejected by the Executive Yuan Referendum Review Committee in May 2013, ruling the issue was of national importance, and could not be decided by local referendum. She launched another local referendum in June 2013 in opposition to the KMT-authored Lee referendum. Meanwhile, Lu filed suit in 2014 in the Taipei High Administrative Court over the Executive Yuan's rejection of the earlier referendum. The court's decision is expected in August 2014.\n\nA competing referendum was launched in 2012 by National Taiwan University professor Kao Cheng-yan, initially as a local referendum and then a national one in July 2014, once sufficient signatures had been gathered. The language of the Kao referendum was formulated to require positive action: \"Do you agree to allow Taiwan Power Co to insert fuel rods into the Fourth Nuclear Power Plant in New Taipei City for a test run\" so poor voter turnout would result in referendum defeat, and thus create binding opposition to starting the plant.\n\nIn April 2014, the government decided to halt construction. The first reactor was sealed after the completion of safety checks, and construction of the second reactor was halted. A final decision on whether to proceed with construction would be subject to another referendum. The ruling KMT stated the purpose of a new referendum was to have the people decide the plant's fate while weighing the consequences of not bringing a major power source on-line. Electricity prices are estimated to rise by 14–40%, and electricity rationing could be imposed as early as 2021. Completely cancelling the plant would force Taipower to book the construction budget as a total loss and force it into insolvency.\n\nMeanwhile, also in April 2014, the opposition DPP planned to introduce special legislation to bypass the provisions of the Referendum Act, allowing a simple majority vote (with no threshold of participation) to decide the plant's fate. Premier Jiang Yi-huah cited the prior 2000 precedent when construction was interrupted by executive order which was later ruled unconstitutional as the reason why he rejected the proposed special legislation. The Executive Yuan stated the plant could start operations if the referendum on halting construction was not held, assuming safety tests were passed first. Former DPP chairman Lin Yi-hsiung went on a hunger strike for two weeks to oppose continued construction, reflecting a popular poll, which favored lowering the threshold of participation and opposed continued construction.\n\nThe proposed Kao referendum was rejected by the Executive Yuan Referendum Review Committee in August 2014, citing the language as confusing and contradictory, since the reason given for the referendum is in opposition to nuclear power, while the referendum itself asks to start a nuclear reactor. Undaunted, supporters of the Kao referendum vowed in October 2014 to initiate another referendum.\n\nPast polls in Taiwan have showed more support for nuclear power among educated than uneducated people. The China Times poll taken in 2000 showed 60% support among those with higher degrees, and only 40% among those with only primary education.\n\nThe Ministry of Economic Affairs proposed in August 2014 that construction be halted for three years until a national referendum could be held, with Taipower estimating the three-year cost of sealing Unit 1 as less than 2 billion.\n\nIn September 2014 Taiwan Power Co. submitted its plan to the Atomic Energy Council (AEC) to mothball Unit 1 and halt construction on Unit 2 of the No. 4 nuclear power plant for three years, beginning in 2015. The plan, written in accordance with the April 2014 government directive to halt construction, underwent several revisions until its final submission in January 2015; the AEC approved Taipower's final plan in February, and it will take effect in July 2015 and run through 2017. Of the 126 systems which passed safety testing, 80 will remain in operation, 14 require periodic testing to ensure they can support their safety functions and 32 will be kept in low-humidity storage.\n\nIn September 2015 GE started International Court of Arbitration proceedings over withheld payments.\n\nA Generation III nuclear reactor has a 72-hour capability of passive cooling to prevent damage to its core should the plant face a total blackout after an emergency shut down. If core overheating and meltdown became unavoidable, these reactors have core catchers that will trap the molten fuel and stop the nuclear reaction (although Lungmen, as other ABWR does not have a core catcher but rely on passive cooling of the corium). Finally, a tight containment ensures that no evacuation zone is required around a Generation III nuclear power plant.\n\nThe ABWR was designed to a 0.3G earthquake acceleration standard with the Lungmen units seismic hardening increased to 0.4G.\n\nSome custom made power plants have lower reliability in the first few years of operation. Some parts may prove unsuitable and need replacement or modifications may be necessary. ABWR uses standardized parts to avoid this, but the 3rd and 4th units displayed low reliability (45% - 70%). It is believed this experience provided knowledge that will give better results in future units. The first two units built, Kashiwazaki Kariwa 6 & 7 fared much better than the following two, Hamaoka 5 and Shika 2.\n\n\n"}
{"id": "4490708", "url": "https://en.wikipedia.org/wiki?curid=4490708", "title": "Magnolia (oil platform)", "text": "Magnolia (oil platform)\n\nMagnolia is an offshore oil drilling and production Extended Tension Leg Platform in the Gulf of Mexico. It was the world's deepest ETLP, reaching , beating the Marco Polo TLP by . In March 2018, Big Foot took over this claim in .\n\nThe hull consists of four circular columns connected at the bottom by rectangular pontoons. At the base of each column, a pontoon extends outward to support two tethers, which are connected to pile foundations on the seabed. The design capacity is an estimated daily production of of oil and of natural gas.\n\nThe Magnolia field is located approximately south of Cameron, Louisiana, in Garden Banks blocks 783 and 784 in the Gulf of Mexico. It is located along the southern edge of the Titan Mini-Basin where multiple deep-water reservoir sands encounter a series of down-to-the-basin and antithetic faults adjacent to salt.\n\n"}
{"id": "1435143", "url": "https://en.wikipedia.org/wiki?curid=1435143", "title": "Meme hack", "text": "Meme hack\n\nA meme hack is changing a meme to express a point of view not intended or inherent in the original image, or even opposite to the original. The meme can be thoughts, concepts, ideas, theories, opinions, beliefs, practices, habits, songs, or icons. Distortions of corporate logos are also referred to as subvertising. Another definition is: \"Intentionally altering a concept or phrase, or using it in a different context, so as to subvert the meaning.\"\n"}
{"id": "860668", "url": "https://en.wikipedia.org/wiki?curid=860668", "title": "Microcar (brand)", "text": "Microcar (brand)\n\nMicrocar is a French microcar manufacturer. The company was founded in 1984 as a division of Bénéteau group, a major sailboat manufacturer. Production moved to a new custom-built factory in September 2000. In September 2008, Microcar was acquired by Ligier Automobiles in a deal backed by the Italian private equity firm 21 Investimenti Partners. The merger created Europe's second-biggest manufacturer of microcars, and largest maker of quadricycles, or \"sans permis\" (license-exempt) vehicles. The Microcar and Ligier brands are to retain their separate identities and production facilities. Phillipe Ligier, son of company founder Guy Ligier, serves as CEO of the expanded Ligier Automobiles.\n\nThe current model range consists solely of the M.Go model introduced in 2009. The M.Go is available in conventional, diesel-engined S, S PACK, MICA, SXI, and Sport trim levels, as well as full electric.\n\nPrior to the M.Go, Microcar was known for their long-running MC Series models, sold as the MC1 and MC2. Both where available as a 2-seat, or 4-seat long wheel base version. The long wheel base being 40mm longer than the short Virgo models. The company also sold a small commercial vehicle called the Sherpa, which was a badge-engineered Ligier X-Pro. The Sherpa was discontinued after the Ligier merger, and the MC has been dropped in favor of the new M.Go. The M.Go was produced with a petrol engine for the UK market.\n\n\nFrom 2006 to 2010, the long-wheelbase version of Microcar's MC Series was used as the basis for the ZENN EV assembled in Canada by ZENN Motor Company with an electric drivetrain. It was marketed in the U.S. and Canada under the ZENN brand as a \"Neighborhood Electric Vehicle\"(NEV), with its top speed governed to . Microcar began distributing the ZENN in Europe during 2007 under its own brand, as the Microcar ZENN. ZENN Motor purchased engineless, rolling chassis from Microcar and installed their own electric motor and drivetrain. The ZENNs retailed for $16,900 while actual cost was $65,000. Microcar brought electric vehicle production fully in-house with the M.Go Electric in 2009, and ZENN ceased production of its MC-based vehicle in 2010.\n\n"}
{"id": "20863293", "url": "https://en.wikipedia.org/wiki?curid=20863293", "title": "Morindone", "text": "Morindone\n\nMorindone is an anthraquinone compound obtained from various \"Morinda\" species, especially \"M. tinctoria\", but also \"M. citrifolia\". Its principal use is as a dye, but it has also been investigated for anticancer and microbial uses.\n\nMorindone is obtained from the root bark of \"M. tinctoria\" or related species in two stages. In the first step, small roots of immature plants are boiled in alcohol to obtain morindin, a yellowish substance which can also be used in dyeing. Further heating brings about hydrolysis of two glucose monomers through sublimation, leaving intensely red crystals.\n\n\"M. tinctoria\" is extensively grown in India for commercial production. Moridin content in the roots peaks in two to three years and drops off considerably after that; some attempts have been made to speed up production using tissue cultures.\n\nMorindone requires a mordant, and the color obtained varies depending on the substance used. Aluminum mordants give a red color, while iron and chromium produce duskier shades. The traditional mordant used in Java, jirak bark (\"Symplocos fasciculata\"), is rich in aluminum salts.\n\nCompared to modern dyestuffs, morindone is not as fast or as stable. Since it can be readily cultivated, however, interest in it remains high. Recent research has examined cell culture as a means of increasing yields.\n\n"}
{"id": "40361162", "url": "https://en.wikipedia.org/wiki?curid=40361162", "title": "Northern Electricity Distribution Company", "text": "Northern Electricity Distribution Company\n\nNorthern Electricity Distribution Company (NEDCO) is an electricity distribution utility company in Ghana. The company is a subsidiary of the Volta River Authority, the main electricity generation company in the country. The company is the sole supplier of electricity to the three Northern Regions of Ghana: Northern Region, Upper East Region and Upper West Region and part of the Asante and Volta Regions. The Electricity Company of Ghana supplies the other regions.\n\nNEDCO was established in 2005 as part of the government of Ghana's policy to reform the country's energy sector. The policy also gave birth to Ghana Grid Company in 2008.\n\nThe operations of NEDCO cover over sixty percent of the total land area of Ghana. Its operations extend into the northern parts of Volta Region and Ashanti Region. It also supplies power to Dapaong in Togo, as well as the border towns of Po, Leo and Yuoga in Burkina Faso.\n\n"}
{"id": "14656037", "url": "https://en.wikipedia.org/wiki?curid=14656037", "title": "Oil megaprojects (2005)", "text": "Oil megaprojects (2005)\n\nThis page summarizes projects that brought more than of new liquid fuel capacity to market with the first production of fuel beginning in 2005. This is part of the Wikipedia summary of Oil Megaprojects—see that page for further details. 2005 saw 23 projects come on stream with an aggregate capacity of when full production was reached (which may not have been in 2005).\n\nThis table is available in csv format here (updated daily).\n"}
{"id": "761110", "url": "https://en.wikipedia.org/wiki?curid=761110", "title": "Olivier van Noort", "text": "Olivier van Noort\n\nOlivier van Noort (1558 – 22 February 1627) was a Dutch merchant captain and the first Dutchman to circumnavigate the world.\n\nOlivier van Noort was born in 1558 in Utrecht. He left Rotterdam on 2 July 1598 with four ships and a plan to attack Spanish possessions in the Pacific and to trade with China and the Spice Islands during the Eighty Years' War between the Netherlands and Spain. His ships were poorly equipped, especially in the way of armament and the crews were unruly.\n\nNonetheless, Van Noort sailed through the Magellan Strait, and captured a number of ships (Spanish and otherwise) along the Pacific coast of South America. He lost two ships on the way due to a storm, including his largest ship, the \"Hendrick Frederick\", which was wrecked on Ternate in the Maluku Islands. In November and December 1600, he established a berth for his two remaining sailboats, \"Mauritius\" and \"Eendracht\", in the surroundings of Corregidor Island at Manila Bay in the Philippines. From there he engaged in what were perceived by the Spanish as pirate activities, targeting the sailing route to and from Manila. This situation was ended after the naval combat of Fortune Island on December 14, 1600. The Spanish lost their flagship, the galleon \"San Antonio\" (its wreck would be found in 1992 and yield a treasure in porcelain and gold pieces) but the Spanish captured the Dutch \"Eendracht\", making van Noort's position untenable and forcing him to retire from the Philippines.\n\nVan Noort returned to Rotterdam via what would become the Dutch East Indies and the Cape of Good Hope on 26 August 1601 with his last ship, the \"Mauritius\", and 45 of originally 248 men. The venture barely broke even, but was the inspiration for more such expeditions. The united Dutch East India Company was formed a few months later.\n\nVan Noort's voyage is also told in the book, The Golden Keys (Doubleday 1956, 1970) by Hans Koning, a fictionalized retelling of the voyage of van Noort, and a previous well known voyage of Gerrit de Veer.\n\n"}
{"id": "24336495", "url": "https://en.wikipedia.org/wiki?curid=24336495", "title": "Olmedilla Photovoltaic Park", "text": "Olmedilla Photovoltaic Park\n\nThe Olmedilla Photovoltaic Park is a 60-megawatt (MW) photovoltaic power plant, located in Olmedilla de Alarcón, Spain. When completed in July 2008, it was the world's largest power plant using photovoltaic technology.\n\nThe plant employs more than 270,000 conventional solar panels, using solar cells made of conventional crystalline silicon. Olmedilla generates about 87,500 megawatt-hours per year, enough to power 40,000 homes. Construction of the plant cost €384 million (US$530 million).\n\n"}
{"id": "11690640", "url": "https://en.wikipedia.org/wiki?curid=11690640", "title": "Operation Looking Glass", "text": "Operation Looking Glass\n\nLooking Glass (or Operation Looking Glass) is the code name for an airborne command and control center operated by the United States. In more recent years it has been more officially referred to as the ABNCP (Airborne Command Post). It provides command and control of U.S. nuclear forces in the event that ground-based command centers have been destroyed or otherwise rendered inoperable. In such an event, the general officer aboard the Looking Glass serves as the Airborne Emergency Action Officer (AEAO) and by law assumes the authority of the National Command Authority and could command execution of nuclear attacks. The AEAO is supported by a battle staff of approximately 20 people, with another dozen responsible for the operation of the aircraft systems. The name Looking Glass, which is another name for a mirror, was chosen for the Airborne Command Post because the mission operates in parallel with the underground command post at Offutt Air Force Base.\n\nThe code name \"Looking Glass\" came from the aircraft's ability to \"mirror\" the command and control functions of the underground command post at Strategic Air Command headquarters.\n\nOperation Looking Glass was initiated by the U.S. Air Force's Strategic Air Command in 1961 and operated by the 34th Air Refueling Squadron, Offutt AFB, Nebraska. In August 1966 the mission transferred to the 38th Strategic Reconnaissance Squadron, the 2nd Airborne Command and Control Squadron in April 1970, to the 7th Airborne Command and Control Squadron in July 1994, and to the USSTRATCOM's Strategic Communications Wing One in October 1998.\n\nThe Strategic Air Command began the Looking Glass mission on February 3, 1961, using EC-135C airplanes from the Airborne Command Post (ABNCP) based at its headquarters at Offutt AFB, Nebraska backed up by aircraft flying with the Second Air Force / 913th Air Refueling Squadron at Barksdale AFB Louisiana, Eighth Air Force / 99th Air Refueling Squadron at Westover AFB, Massachusetts, and Fifteenth Air Force / 22d Air Refueling Squadron, March AFB, California.\n\nEC-135 Looking Glass aircraft were airborne 24 hours a day for over 29 years, until July 24, 1990, when \"The Glass\" ceased continuous airborne alert, but remained on ground or airborne alert 24 hours a day.\n\nLooking Glass mirrors ground-based command, control, and communications located at the USSTRATCOM Global Operations Center (GOC) at Offutt Air Force Base. The EC-135 Looking Glass aircraft were equipped with the Airborne Launch Control System, capable of transmitting launch commands to U.S. ground-based intercontinental ballistic missiles (ICBMs) in the event that the ground launch control centers were rendered inoperable.\n\nThe Looking Glass was also designed to help ensure continuity and reconstitution of the US government in the event of a nuclear attack on North America. Although the two types of aircraft are distinct, the \"Doomsday Plane\" nickname is also frequently associated with the E-4 \"Nightwatch\" Advanced Airborne Command Post mission and aircraft.\n\nThe Looking Glass was the anchor in what was known as the World Wide Airborne Command Post (WWABNCP) network. This network of specially equipped EC-135 aircraft would launch from ground alert status and establish air-to-air wireless network connections in the event of a U.S. national emergency. Members of the WWABNCP network included: \n\nThe Eastern Auxiliary (EAST Aux) and Western Auxiliary (West Aux) Command Posts were also part of the WWABNCP (\"wah-bin-cop\") network and were capable of assuming responsibility for Looking Glass as the anchor. The West Aux 906th Air Refueling Squadron was based at Minot AFB, North Dakota, and moved to the 4th Airborne Command and Control Squadron at Ellsworth AFB, South Dakota, in April 1970 and the East Aux mission 301st Air Refueling Squadron was based at Lockbourne AFB, Ohio, and moved to the 3rd Airborne Command & Control Squadron at Grissom AFB, Indiana, in April 1970. After 1975, East Aux was assumed from the Looking Glass backup ground alert aircraft launched from Offutt AFB. In June 1992, United States Strategic Command took over the Looking Glass mission from the Strategic Air Command, as SAC was disbanded and Strategic Command assumed the nuclear deterrence mission. \n\nOn October 1, 1998 the United States Navy fleet of E-6Bs replaced the EC-135C in performing the \"Looking Glass\" mission, previously carried out for 37 years by the U.S. Air Force. Unlike the original Looking Glass aircraft, the E-6Bs are modified Boeing 707 aircraft, not the military-only KC-135. The E-6B provides the National Command Authority with the same capability as the EC-135 fleet to control the nation's intercontinental ballistic missile (ICBM) force, nuclear-capable bombers and submarine-launched ballistic missiles (SLBM). With the assumption of this mission, a USSTRATCOM battle staff now flies with the TACAMO crew.\n\nIf the USSTRATCOM Global Operations Center (GOC) is unable to function in its role, the E-6B Looking Glass can assume command of all U.S. nuclear-capable forces. Flying aboard each ABNCP is a crew of 22, which includes an air crew, a Communications Systems Officer and team, an Airborne Emergency Action Officer (an Admiral or General officer), a Mission Commander, a Strike Advisor, an Airborne Launch Control System/Intelligence Officer, a Meteorological Effects Officer, a Logistics Officer, a Force Status Controller, and an Emergency Actions NCO. In addition to being able to direct the launch of ICBMs using the Airborne Launch Control System, the E-6B can communicate Emergency Action Messages (EAM) to nuclear submarines running at depth by extending a 2½-mile-long trailing wire antenna (TWA) for use with the Survivable Low Frequency Communications System (SLFCS), as the EC-135C could.\n\nThere was some speculation that the \"mystery plane\" seen flying over the White House on September 11, 2001, was some newer incarnation of Looking Glass. However, as indicated by retired Major General Donald Shepperd, speaking on CNN on September 12, 2007, the plane circling the White House on 9/11 resembled an E-4B which was likely launched from Nightwatch ground alert at Andrews Air Force Base.\n\n\n"}
{"id": "22303", "url": "https://en.wikipedia.org/wiki?curid=22303", "title": "Oxygen", "text": "Oxygen\n\nOxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table, a highly reactive nonmetal, and an oxidizing agent that readily forms oxides with most elements as well as with other compounds. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula . Diatomic oxygen gas constitutes 20.8% of the Earth's atmosphere. As compounds including oxides, the element makes up almost half of the Earth's crust.\n\nDioxygen is used in cellular respiration and many major classes of organic molecules in living organisms contain oxygen, such as proteins, nucleic acids, carbohydrates, and fats, as do the major constituent inorganic compounds of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as a component of water, the major constituent of lifeforms. Oxygen is continuously replenished in Earth's atmosphere by photosynthesis, which uses the energy of sunlight to produce oxygen from water and carbon dioxide. Oxygen is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (), strongly absorbs ultraviolet UVB radiation and the high-altitude ozone layer helps protect the biosphere from ultraviolet radiation. However, ozone present at the surface is a byproduct of smog and thus a pollutant.\n\nOxygen was isolated by Michael Sendivogius before 1604, but it is commonly believed that the element was discovered independently by Carl Wilhelm Scheele, in Uppsala, in 1773 or earlier, and Joseph Priestley in Wiltshire, in 1774. Priority is often given for Priestley because his work was published first. Priestley, however, called oxygen \"dephlogisticated air\", and did not recognize it as a chemical element. The name \"oxygen\" was coined in 1777 by Antoine Lavoisier, who first recognized oxygen as a chemical element and correctly characterized the role it plays in combustion.\n\nCommon uses of oxygen include production of steel, plastics and textiles, brazing, welding and cutting of steels and other metals, rocket propellant, oxygen therapy, and life support systems in aircraft, submarines, spaceflight and diving.\n\nThe name \"oxygen\" was coined in 1777 by Antoine Lavoisier, whose experiments with oxygen helped to discredit the then-popular phlogiston theory of combustion and corrosion. Its name derives from the Greek roots ὀξύς \"oxys\", \"acid\", literally \"sharp\", referring to the sour taste of acids and -γενής \"-genes\", \"producer\", literally \"begetter\", because at the time of naming, it was mistakenly thought that all acids required oxygen in their composition.\n\nOne of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on mechanics, Philo of Byzantium. In his work \"Pneumatica\", Philo observed that inverting a vessel over a burning candle and surrounding the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration.\n\nIn the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John Mayow (1641–1679) refined this work by showing that fire requires only a part of air that he called \"spiritus nitroaereus\". In one experiment, he found that placing either a mouse or a lit candle in a closed container over water caused the water to rise and replace one-fourteenth of the air's volume before extinguishing the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion.\n\nMayow observed that antimony increased in weight when heated, and inferred that the nitroaereus must have combined with it. He also thought that the lungs separate nitroaereus from air and pass it into the blood and that animal heat and muscle movement result from the reaction of nitroaereus with certain substances in the body. Accounts of these and other experiments and ideas were published in 1668 in his work \"Tractatus duo\" in the tract \"De respiratione\".\n\nRobert Hooke, Ole Borch, Mikhail Lomonosov, and Pierre Bayen all produced oxygen in experiments in the 17th and the 18th century but none of them recognized it as a chemical element. This may have been in part due to the prevalence of the philosophy of combustion and corrosion called the \"phlogiston theory\", which was then the favored explanation of those processes.\n\nEstablished in 1667 by the German alchemist J. J. Becher, and modified by the chemist Georg Ernst Stahl by 1731, phlogiston theory stated that all combustible materials were made of two parts. One part, called phlogiston, was given off when the substance containing it was burned, while the dephlogisticated part was thought to be its true form, or calx.\n\nHighly combustible materials that leave little residue, such as wood or coal, were thought to be made mostly of phlogiston; non-combustible substances that corrode, such as iron, contained very little. Air did not play a role in phlogiston theory, nor were any initial quantitative experiments conducted to test the idea; instead, it was based on observations of what happens when something burns, that most common objects appear to become lighter and seem to lose something in the process.\n\nPolish alchemist, philosopher, and physician Michael Sendivogius in his work \"De Lapide Philosophorum Tractatus duodecim e naturae fonte et manuali experientia depromti\" (1604) described a substance contained in air, referring to it as 'cibus vitae' (food of life), and this substance is identical with oxygen. Sendivogius, during his experiments performed between 1598 and 1604, properly recognized that the substance is equivalent to the gaseous byproduct released by the thermal decomposition of potassium nitrate. In Bugaj’s view, the isolation of oxygen and the proper association of the substance to that part of air which is required for life, lends sufficient weight to the discovery of oxygen by Sendivogius. This discovery of Sendivogius was however frequently denied by the generations of scientists and chemists which succeeded him.\n\nIt is also commonly claimed that oxygen was first discovered by Swedish pharmacist Carl Wilhelm Scheele. He had produced oxygen gas by heating mercuric oxide and various nitrates in 1771–2. Scheele called the gas \"fire air\" because it was then the only known agent to support combustion. He wrote an account of this discovery in a manuscript titled \"Treatise on Air and Fire\", which he sent to his publisher in 1775. That document was published in 1777.\n\nIn the meantime, on August 1, 1774, an experiment conducted by the British clergyman Joseph Priestley focused sunlight on mercuric oxide (HgO) contained in a glass tube, which liberated a gas he named \"dephlogisticated air\". He noted that candles burned brighter in the gas and that a mouse was more active and lived longer while breathing it. After breathing the gas himself, Priestley wrote: \"The feeling of it to my lungs was not sensibly different from that of common air, but I fancied that my breast felt peculiarly light and easy for some time afterwards.\" Priestley published his findings in 1775 in a paper titled \"An Account of Further Discoveries in Air,\" which was included in the second volume of his book titled \"Experiments and Observations on Different Kinds of Air\". Because he published his findings first, Priestley is usually given priority in the discovery.\n\nThe French chemist Antoine Laurent Lavoisier later claimed to have discovered the new substance independently. Priestley visited Lavoisier in October 1774 and told him about his experiment and how he liberated the new gas. Scheele also dispatched a letter to Lavoisier on September 30, 1774, that described his discovery of the previously unknown substance, but Lavoisier never acknowledged receiving it (a copy of the letter was found in Scheele's belongings after his death).\nLavoisier conducted the first adequate quantitative experiments on oxidation and gave the first correct explanation of how combustion works. He used these and similar experiments, all started in 1774, to discredit the phlogiston theory and to prove that the substance discovered by Priestley and Scheele was a chemical element.\n\nIn one experiment, Lavoisier observed that there was no overall increase in weight when tin and air were heated in a closed container. He noted that air rushed in when he opened the container, which indicated that part of the trapped air had been consumed. He also noted that the tin had increased in weight and that increase was the same as the weight of the air that rushed back in. This and other experiments on combustion were documented in his book \"Sur la combustion en général\", which was published in 1777. In that work, he proved that air is a mixture of two gases; 'vital air', which is essential to combustion and respiration, and \"azote\" (Gk. \"\" \"lifeless\"), which did not support either. \"Azote\" later became \"nitrogen\" in English, although it has kept the earlier name in French and several other European languages.\n\nLavoisier renamed 'vital air' to \"oxygène\" in 1777 from the Greek roots \" (oxys)\" (acid, literally \"sharp\", from the taste of acids) and \"-γενής (-genēs)\" (producer, literally begetter), because he mistakenly believed that oxygen was a constituent of all acids. Chemists (such as Sir Humphry Davy in 1812) eventually determined that Lavoisier was wrong in this regard (hydrogen forms the basis for acid chemistry), but by then the name was too well established.\n\n\"Oxygen\" entered the English language despite opposition by English scientists and the fact that the Englishman Priestley had first isolated the gas and written about it. This is partly due to a poem praising the gas titled \"Oxygen\" in the popular book \"The Botanic Garden\" (1791) by Erasmus Darwin, grandfather of Charles Darwin.\nJohn Dalton's original atomic hypothesis presumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed that water's formula was HO, giving the atomic mass of oxygen was 8 times that of hydrogen, instead of the modern value of about 16. In 1805, Joseph Louis Gay-Lussac and Alexander von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen; and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the diatomic elemental molecules in those gases.\n\nBy the late 19th century scientists realized that air could be liquefied and its components isolated by compressing and cooling it. Using a cascade method, Swiss chemist and physicist Raoul Pierre Pictet evaporated liquid sulfur dioxide in order to liquefy carbon dioxide, which in turn was evaporated to cool oxygen gas enough to liquefy it. He sent a telegram on December 22, 1877 to the French Academy of Sciences in Paris announcing his discovery of liquid oxygen. Just two days later, French physicist Louis Paul Cailletet announced his own method of liquefying molecular oxygen. Only a few drops of the liquid were produced in each case and no meaningful analysis could be conducted. Oxygen was liquified in a stable state for the first time on March 29, 1883 by Polish scientists from Jagiellonian University, Zygmunt Wróblewski and Karol Olszewski.\n\nIn 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen for study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them separately. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed . This method of welding and cutting metal later became common.\n\nIn 1923, the American scientist Robert H. Goddard became the first person to develop a rocket engine that burned liquid fuel; the engine used gasoline for fuel and liquid oxygen as the oxidizer. Goddard successfully flew a small liquid-fueled rocket 56 m at 97 km/h on March 16, 1926 in Auburn, Massachusetts, US.\n\nOxygen levels in the atmosphere are trending slightly downward globally, possibly because of fossil-fuel burning.\n\nAt standard temperature and pressure, oxygen is a colorless, odorless, and tasteless gas with the molecular formula , referred to as dioxygen.\n\nAs \"dioxygen\", two oxygen atoms are chemically bound to each other. The bond can be variously described based on level of theory, but is reasonably and simply described as a covalent double bond that results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms, the filling of which results in a bond order of two. More specifically, the double bond is the result of sequential, low-to-high energy, or Aufbau, filling of orbitals, and the resulting cancellation of contributions from the 2s electrons, after sequential filling of the low σ and σ orbitals; σ overlap of the two atomic 2p orbitals that lie along the O-O molecular axis and overlap of two pairs of atomic 2p orbitals perpendicular to the O-O molecular axis, and then cancellation of contributions from the remaining two of the six 2p electrons after their partial filling of the lowest and orbitals.\n\nThis combination of cancellations and σ and overlaps results in dioxygen's double bond character and reactivity, and a triplet electronic ground state. An electron configuration with two unpaired electrons, as is found in dioxygen orbitals (see the filled * orbitals in the diagram) that are of equal energy—i.e., degenerate—is a configuration termed a spin triplet state. Hence, the ground state of the molecule is referred to as triplet oxygen. The highest energy, partially filled orbitals are antibonding, and so their filling weakens the bond order from three to two. Because of its unpaired electrons, triplet oxygen reacts only slowly with most organic molecules, which have paired electron spins; this prevents spontaneous combustion.\nIn the triplet form, molecules are paramagnetic. That is, they impart magnetic character to oxygen when it is in the presence of a magnetic field, because of the spin magnetic moments of the unpaired electrons in the molecule, and the negative exchange energy between neighboring molecules. Liquid oxygen is so magnetic that, in laboratory demonstrations, a bridge of liquid oxygen may be supported against its own weight between the poles of a powerful magnet.\n\nSinglet oxygen is a name given to several higher-energy species of molecular in which all the electron spins are paired. It is much more reactive with common organic molecules than is molecular oxygen per se. In nature, singlet oxygen is commonly formed from water during photosynthesis, using the energy of sunlight. It is also produced in the troposphere by the photolysis of ozone by light of short wavelength, and by the immune system as a source of active oxygen. Carotenoids in photosynthetic organisms (and possibly animals) play a major role in absorbing energy from singlet oxygen and converting it to the unexcited ground state before it can cause harm to tissues.\n\nThe common allotrope of elemental oxygen on Earth is called dioxygen, , the major part of the Earth's atmospheric oxygen (see Occurrence). O has a bond length of 121 pm and a bond energy of 498 kJ/mol, which is smaller than the energy of other double bonds or pairs of single bonds in the biosphere and responsible for the exothermic reaction of O with any organic molecule. Due to its energy content, O is used by complex forms of life, such as animals, in cellular respiration. Other aspects of are covered in the remainder of this article.\n\nTrioxygen () is usually known as ozone and is a very reactive allotrope of oxygen that is damaging to lung tissue. Ozone is produced in the upper atmosphere when combines with atomic oxygen made by the splitting of by ultraviolet (UV) radiation. Since ozone absorbs strongly in the UV region of the spectrum, the ozone layer of the upper atmosphere functions as a protective radiation shield for the planet. Near the Earth's surface, it is a pollutant formed as a by-product of automobile exhaust. At low earth orbit altitudes, sufficient atomic oxygen is present to cause corrosion of spacecraft.\n\nThe metastable molecule tetraoxygen () was discovered in 2001, and was assumed to exist in one of the six phases of solid oxygen. It was proven in 2006 that this phase, created by pressurizing to 20 GPa, is in fact a rhombohedral cluster. This cluster has the potential to be a much more powerful oxidizer than either or and may therefore be used in rocket fuel. A metallic phase was discovered in 1990 when solid oxygen is subjected to a pressure of above 96 GPa and it was shown in 1998 that at very low temperatures, this phase becomes superconducting.\n\nOxygen dissolves more readily in water than nitrogen, and in freshwater more readily than seawater. Water in equilibrium with air contains approximately 1 molecule of dissolved for every 2 molecules of (1:2), compared with an atmospheric ratio of approximately 1:4. The solubility of oxygen in water is temperature-dependent, and about twice as much (14.6 mg·L) dissolves at 0 °C than at 20 °C (7.6 mg·L). At 25 °C and of air, freshwater contains about 6.04 milliliters (mL) of oxygen per liter, and seawater contains about 4.95 mL per liter. At 5 °C the solubility increases to 9.0 mL (50% more than at 25 °C) per liter for water and 7.2 mL (45% more) per liter for sea water.\n\nOxygen condenses at 90.20 K (−182.95 °C, −297.31 °F), and freezes at 54.36 K (−218.79 °C, −361.82 °F). Both liquid and solid are clear substances with a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light). High-purity liquid is usually obtained by the fractional distillation of liquefied air. Liquid oxygen may also be condensed from air using liquid nitrogen as a coolant.\n\nOxygen is a highly reactive substance and must be segregated from combustible materials.\n\nThe spectroscopy of molecular oxygen is associated with the atmospheric processes of aurora and airglow. The absorption in the Herzberg continuum and Schumann–Runge bands in the ultraviolet produces atomic oxygen that is important in the chemistry of the middle atmosphere. Excited state singlet molecular oxygen is responsible for red chemiluminescence in solution.\n\nNaturally occurring oxygen is composed of three stable isotopes, O, O, and O, with O being the most abundant (99.762% natural abundance).\n\nMost O is synthesized at the end of the helium fusion process in massive stars but some is made in the neon burning process. O is primarily made by the burning of hydrogen into helium during the CNO cycle, making it a common isotope in the hydrogen burning zones of stars. Most O is produced when N (made abundant from CNO burning) captures a He nucleus, making O common in the helium-rich zones of evolved, massive stars.\n\nFourteen radioisotopes have been characterized. The most stable are O with a half-life of 122.24 seconds and O with a half-life of 70.606 seconds. All of the remaining radioactive isotopes have half-lives that are less than 27 s and the majority of these have half-lives that are less than 83 milliseconds. The most common decay mode of the isotopes lighter than O is β decay to yield nitrogen, and the most common mode for the isotopes heavier than O is beta decay to yield fluorine.\n\nOxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass as part of oxide compounds such as silicon dioxide and is the most abundant element by mass in the Earth's crust. It is also the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 10 tonnes). Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% by volume) and Venus have much less. The surrounding those planets is produced solely by the action of ultraviolet radiation on oxygen-containing molecules such as carbon dioxide.\n\nThe unusually high concentration of oxygen gas on Earth is the result of the oxygen cycle. This biogeochemical cycle describes the movement of oxygen within and between its three main reservoirs on Earth: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for modern Earth's atmosphere. Photosynthesis releases oxygen into the atmosphere, while respiration, decay, and combustion remove it from the atmosphere. In the present equilibrium, production and consumption occur at the same rate.\n\nFree oxygen also occurs in solution in the world's water bodies. The increased solubility of at lower temperatures (see Physical properties) has important implications for ocean life, as polar oceans support a much higher density of life due to their higher oxygen content. Water polluted with plant nutrients such as nitrates or phosphates may stimulate growth of algae by a process called eutrophication and the decay of these organisms and other biomaterials may reduce the content in eutrophic water bodies. Scientists assess this aspect of water quality by measuring the water's biochemical oxygen demand, or the amount of needed to restore it to a normal concentration.\n\nPaleoclimatologists measure the ratio of oxygen-18 and oxygen-16 in the shells and skeletons of marine organisms to determine the climate millions of years ago (see oxygen isotope ratio cycle). Seawater molecules that contain the lighter isotope, oxygen-16, evaporate at a slightly faster rate than water molecules containing the 12% heavier oxygen-18, and this disparity increases at lower temperatures. During periods of lower global temperatures, snow and rain from that evaporated water tends to be higher in oxygen-16, and the seawater left behind tends to be higher in oxygen-18. Marine organisms then incorporate more oxygen-18 into their skeletons and shells than they would in a warmer climate. Paleoclimatologists also directly measure this ratio in the water molecules of ice core samples as old as hundreds of thousands of years.\n\nPlanetary geologists have measured the relative quantities of oxygen isotopes in samples from the Earth, the Moon, Mars, and meteorites, but were long unable to obtain reference values for the isotope ratios in the Sun, believed to be the same as those of the primordial solar nebula. Analysis of a silicon wafer exposed to the solar wind in space and returned by the crashed Genesis spacecraft has shown that the Sun has a higher proportion of oxygen-16 than does the Earth. The measurement implies that an unknown process depleted oxygen-16 from the Sun's disk of protoplanetary material prior to the coalescence of dust grains that formed the Earth.\n\nOxygen presents two spectrophotometric absorption bands peaking at the wavelengths 687 and 760 nm. Some remote sensing scientists have proposed using the measurement of the radiance coming from vegetation canopies in those bands to characterize plant health status from a satellite platform. This approach exploits the fact that in those bands it is possible to discriminate the vegetation's reflectance from its fluorescence, which is much weaker. The measurement is technically difficult owing to the low signal-to-noise ratio and the physical structure of vegetation; but it has been proposed as a possible method of monitoring the carbon cycle from satellites on a global scale.\n\nIn nature, free oxygen is produced by the light-driven splitting of water during oxygenic photosynthesis. According to some estimates, green algae and cyanobacteria in marine environments provide about 70% of the free oxygen produced on Earth, and the rest is produced by terrestrial plants. Other estimates of the oceanic contribution to atmospheric oxygen are higher, while some estimates are lower, suggesting oceans produce ~45% of Earth's atmospheric oxygen each year.\n\nA simplified overall formula for photosynthesis is:\n\nor simply\n\nPhotolytic oxygen evolution occurs in the thylakoid membranes of photosynthetic organisms and requires the energy of four photons. Many steps are involved, but the result is the formation of a proton gradient across the thylakoid membrane, which is used to synthesize adenosine triphosphate (ATP) via photophosphorylation. The remaining (after production of the water molecule) is released into the atmosphere.\n\nOxygen is used in mitochondria to generate ATP during oxidative phosphorylation. The reaction for aerobic respiration is essentially the reverse of photosynthesis and is simplified as:\n\nIn vertebrates, diffuses through membranes in the lungs and into red blood cells. Hemoglobin binds , changing color from bluish red to bright red ( is released from another part of hemoglobin through the Bohr effect). Other animals use hemocyanin (molluscs and some arthropods) or hemerythrin (spiders and lobsters). A liter of blood can dissolve 200 cm of .\n\nUntil the discovery of anaerobic metazoa, oxygen was thought to be a requirement for all complex life.\n\nReactive oxygen species, such as superoxide ion () and hydrogen peroxide (), are reactive by-products of oxygen use in organisms. Parts of the immune system of higher organisms create peroxide, superoxide, and singlet oxygen to destroy invading microbes. Reactive oxygen species also play an important role in the hypersensitive response of plants against pathogen attack. Oxygen is damaging to obligately anaerobic organisms, which were the dominant form of early life on Earth until began to accumulate in the atmosphere about 2.5 billion years ago during the Great Oxygenation Event, about a billion years after the first appearance of these organisms.\n\nAn adult human at rest inhales 1.8 to 2.4 grams of oxygen per minute. This amounts to more than 6 billion tonnes of oxygen inhaled by humanity per year.\n\nThe free oxygen partial pressure in the body of a living vertebrate organism is highest in the respiratory system, and decreases along any arterial system, peripheral tissues, and venous system, respectively. Partial pressure is the pressure that oxygen would have if it alone occupied the volume.\n\nFree oxygen gas was almost nonexistent in Earth's atmosphere before photosynthetic archaea and bacteria evolved, probably about 3.5 billion years ago. Free oxygen first appeared in significant quantities during the Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first billion years, any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations. When such oxygen sinks became saturated, free oxygen began to outgas from the oceans 3–2.7 billion years ago, reaching 10% of its present level around 1.7 billion years ago.\n\nThe presence of large amounts of dissolved and free oxygen in the oceans and atmosphere may have driven most of the extant anaerobic organisms to extinction during the Great Oxygenation Event (\"oxygen catastrophe\") about 2.4 billion years ago. Cellular respiration using enables aerobic organisms to produce much more ATP than anaerobic organisms. Cellular respiration of occurs in all eukaryotes, including all complex multicellular organisms such as plants and animals.\n\nSince the beginning of the Cambrian period 540 million years ago, atmospheric levels have fluctuated between 15% and 30% by volume. Towards the end of the Carboniferous period (about 300 million years ago) atmospheric levels reached a maximum of 35% by volume, which may have contributed to the large size of insects and amphibians at this time.\n\nVariations in atmospheric oxygen concentration have shaped past climates. When oxygen declined, atmospheric density dropped, which in turn increased surface evaporation, causing precipitation increases and warmer temperatures.\n\nAt the current rate of photosynthesis it would take about 2,000 years to regenerate the entire in the present atmosphere.\n\nOne hundred million tonnes of are extracted from air for industrial uses annually by two primary methods. The most common method is fractional distillation of liquefied air, with distilling as a vapor while is left as a liquid.\n\nThe other primary method of producing is passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves, which absorbs the nitrogen and delivers a gas stream that is 90% to 93% . Simultaneously, nitrogen gas is released from the other nitrogen-saturated zeolite bed, by reducing the chamber operating pressure and diverting part of the oxygen gas from the producer bed through it, in the reverse direction of flow. After a set cycle time the operation of the two beds is interchanged, thereby allowing for a continuous supply of gaseous oxygen to be pumped through a pipeline. This is known as pressure swing adsorption. Oxygen gas is increasingly obtained by these non-cryogenic technologies (see also the related vacuum swing adsorption).\n\nOxygen gas can also be produced through electrolysis of water into molecular oxygen and hydrogen. DC electricity must be used: if AC is used, the gases in each limb consist of hydrogen and oxygen in the explosive ratio 2:1. A similar method is the electrocatalytic evolution from oxides and oxoacids. Chemical catalysts can be used as well, such as in chemical oxygen generators or oxygen candles that are used as part of the life-support equipment on submarines, and are still part of standard equipment on commercial airliners in case of depressurization emergencies. Another air separation method is forcing air to dissolve through ceramic membranes based on zirconium dioxide by either high pressure or an electric current, to produce nearly pure gas.\n\nOxygen storage methods include high pressure oxygen tanks, cryogenics and chemical compounds. For reasons of economy, oxygen is often transported in bulk as a liquid in specially insulated tankers, since one liter of liquefied oxygen is equivalent to 840 liters of gaseous oxygen at atmospheric pressure and . Such tankers are used to refill bulk liquid oxygen storage containers, which stand outside hospitals and other institutions that need large volumes of pure oxygen gas. Liquid oxygen is passed through heat exchangers, which convert the cryogenic liquid into gas before it enters the building. Oxygen is also stored and shipped in smaller cylinders containing the compressed gas; a form that is useful in certain portable medical applications and oxy-fuel welding and cutting.\n\nUptake of from the air is the essential purpose of respiration, so oxygen supplementation is used in medicine. Treatment not only increases oxygen levels in the patient's blood, but has the secondary effect of decreasing resistance to blood flow in many types of diseased lungs, easing work load on the heart. Oxygen therapy is used to treat emphysema, pneumonia, some heart disorders (congestive heart failure), some disorders that cause increased pulmonary artery pressure, and any disease that impairs the body's ability to take up and use gaseous oxygen.\n\nTreatments are flexible enough to be used in hospitals, the patient's home, or increasingly by portable devices. Oxygen tents were once commonly used in oxygen supplementation, but have since been replaced mostly by the use of oxygen masks or nasal cannulas.\n\nHyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes addressed with this therapy. Increased concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in the blood. Increasing the pressure of as soon as possible helps to redissolve the bubbles back into the blood so that these excess gasses can be exhaled naturally through the lungs.\nNormobaric oxygen administration at the highest available concentration is frequently used as first aid for any diving injury that may involve inert gas bubble formation in the tissues. There is epidemiological support for its use from a statistical study of cases recorded in a long term database.\nAn application of as a low-pressure breathing gas is in modern space suits, which surround their occupant's body with the breathing gas. These devices use nearly pure oxygen at about one-third normal pressure, resulting in a normal blood partial pressure of . This trade-off of higher oxygen concentration for lower pressure is needed to maintain suit flexibility.\n\nScuba and surface-supplied underwater divers and submariners also rely on artificially delivered . Submarines, submersibles and atmospheric diving suits usually operate at normal atmospheric pressure. Breathing air is scrubbed of carbon dioxide by chemical extraction and oxygen is replaced to maintain a constant partial pressure. Ambient pressure divers breathe air or gas mixtures with an oxygen fraction suited to the operating depth. Pure or nearly pure use in diving at pressures higher than atmospheric is usually limited to rebreathers, or decompression at relatively shallow depths (~6 meters depth, or less), or medical treatment in recompression chambers at pressures up to 2.8 bar, where acute oxygen toxicity can be managed without the risk of drowning. Deeper diving requires significant dilution of with other gases, such as nitrogen or helium, to prevent oxygen toxicity.\n\nPeople who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental supplies. Pressurized commercial airplanes have an emergency supply of automatically supplied to the passengers in case of cabin depressurization. Sudden cabin pressure loss activates chemical oxygen generators above each seat, causing oxygen masks to drop. Pulling on the masks \"to start the flow of oxygen\" as cabin safety instructions dictate, forces iron filings into the sodium chlorate inside the canister. A steady stream of oxygen gas is then produced by the exothermic reaction.\n\nOxygen, as a mild euphoric, has a history of recreational use in oxygen bars and in sports. Oxygen bars are establishments found in the United States since the late 1990s that offer higher than normal exposure for a minimal fee. Professional athletes, especially in American football, sometimes go off-field between plays to don oxygen masks to boost performance. The pharmacological effect is doubted; a placebo effect is a more likely explanation. Available studies support a performance boost from oxygen enriched mixtures only if it is breathed \"during\" aerobic exercise.\n\nOther recreational uses that do not involve breathing include pyrotechnic applications, such as George Goble's five-second ignition of barbecue grills.\n\nSmelting of iron ore into steel consumes 55% of commercially produced oxygen. In this process, is injected through a high-pressure lance into molten iron, which removes sulfur impurities and excess carbon as the respective oxides, and . The reactions are exothermic, so the temperature increases to 1,700 °C.\n\nAnother 25% of commercially produced oxygen is used by the chemical industry. Ethylene is reacted with to create ethylene oxide, which, in turn, is converted into ethylene glycol; the primary feeder material used to manufacture a host of products, including antifreeze and polyester polymers (the precursors of many plastics and fabrics).\n\nMost of the remaining 20% of commercially produced oxygen is used in medical applications, metal cutting and welding, as an oxidizer in rocket fuel, and in water treatment. Oxygen is used in oxyacetylene welding burning acetylene with to produce a very hot flame. In this process, metal up to thick is first heated with a small oxy-acetylene flame and then quickly cut by a large stream of .\n\nThe oxidation state of oxygen is −2 in almost all known compounds of oxygen. The oxidation state −1 is found in a few compounds such as peroxides. Compounds containing oxygen in other oxidation states are very uncommon: −1/2 (superoxides), −1/3 (ozonides), 0 (elemental, hypofluorous acid), +1/2 (dioxygenyl), +1 (dioxygen difluoride), and +2 (oxygen difluoride).\n\nWater () is an oxide of hydrogen and the most familiar oxygen compound. Hydrogen atoms are covalently bonded to oxygen in a water molecule but also have an additional attraction (about 23.3 kJ/mol per hydrogen atom) to an adjacent oxygen atom in a separate molecule. These hydrogen bonds between water molecules hold them approximately 15% closer than what would be expected in a simple liquid with just van der Waals forces.\nDue to its electronegativity, oxygen forms chemical bonds with almost all other elements to give corresponding oxides. The surface of most metals, such as aluminium and titanium, are oxidized in the presence of air and become coated with a thin film of oxide that passivates the metal and slows further corrosion. Many oxides of the transition metals are non-stoichiometric compounds, with slightly less metal than the chemical formula would show. For example, the mineral FeO (wüstite) is written as , where \"x\" is usually around 0.05.\n\nOxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (). The Earth's crustal rock is composed in large part of oxides of silicon (silica , as found in granite and quartz), aluminium (aluminium oxide , in bauxite and corundum), iron (iron(III) oxide , in hematite and rust), and calcium carbonate (in limestone). The rest of the Earth's crust is also made of oxygen compounds, in particular various complex silicates (in silicate minerals). The Earth's mantle, of much larger mass than the crust, is largely composed of silicates of magnesium and iron.\n\nWater-soluble silicates in the form of , , and are used as detergents and adhesives.\n\nOxygen also acts as a ligand for transition metals, forming transition metal dioxygen complexes, which feature metal–. This class of compounds includes the heme proteins hemoglobin and myoglobin. An exotic and unusual reaction occurs with , which oxidizes oxygen to give OPtF, dioxygenyl hexafluoroplatinate.\n\nAmong the most important classes of organic compounds that contain oxygen are (where \"R\" is an organic group): alcohols (R-OH); ethers (R-O-R); ketones (R-CO-R); aldehydes (R-CO-H); carboxylic acids (R-COOH); esters (R-COO-R); acid anhydrides (R-CO-O-CO-R); and amides (). There are many important organic solvents that contain oxygen, including: acetone, methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid, and formic acid. Acetone () and phenol () are used as feeder materials in the synthesis of many different substances. Other important organic compounds that contain oxygen are: glycerol, formaldehyde, glutaraldehyde, citric acid, acetic anhydride, and acetamide. Epoxides are ethers in which the oxygen atom is part of a ring of three atoms. The element is similarly found in almost all biomolecules that are important to (or generated by) life.\n\nOxygen reacts spontaneously with many organic compounds at or below room temperature in a process called autoxidation. Most of the organic compounds that contain oxygen are not made by direct action of . Organic compounds important in industry and commerce that are made by direct oxidation of a precursor include ethylene oxide and peracetic acid.\n\nThe NFPA 704 standard rates compressed oxygen gas as nonhazardous to health, nonflammable and nonreactive, but an oxidizer. Refrigerated liquid oxygen (LOX) is given a health hazard rating of 3 (for increased risk of hyperoxia from condensed vapors, and for hazards common to cryogenic liquids such as frostbite), and all other ratings are the same as the compressed gas form.\n\nOxygen gas () can be toxic at elevated partial pressures, leading to convulsions and other health problems. Oxygen toxicity usually begins to occur at partial pressures more than 50 kilopascals (kPa), equal to about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level partial pressure of about 21 kPa. This is not a problem except for patients on mechanical ventilators, since gas supplied through oxygen masks in medical applications is typically composed of only 30%–50% by volume (about 30 kPa at standard pressure).\n\nAt one time, premature babies were placed in incubators containing -rich air, but this practice was discontinued after some babies were blinded by the oxygen content being too high.\n\nBreathing pure in space applications, such as in some modern space suits, or in early spacecraft such as Apollo, causes no damage due to the low total pressures used. In the case of spacesuits, the partial pressure in the breathing gas is, in general, about 30 kPa (1.4 times normal), and the resulting partial pressure in the astronaut's arterial blood is only marginally more than normal sea-level partial pressure.\n\nOxygen toxicity to the lungs and central nervous system can also occur in deep scuba diving and surface supplied diving. Prolonged breathing of an air mixture with an partial pressure more than 60 kPa can eventually lead to permanent pulmonary fibrosis. Exposure to a partial pressures greater than 160 kPa (about 1.6 atm) may lead to convulsions (normally fatal for divers). Acute oxygen toxicity (causing seizures, its most feared effect for divers) can occur by breathing an air mixture with 21% at or more of depth; the same thing can occur by breathing 100% at only .\n\nHighly concentrated sources of oxygen promote rapid combustion. Fire and explosion hazards exist when concentrated oxidants and fuels are brought into close proximity; an ignition event, such as heat or a spark, is needed to trigger combustion. Oxygen is the oxidant, not the fuel, but nevertheless the source of most of the chemical energy released in combustion.\n\nConcentrated will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure but at slightly more than atmospheric pressure, instead of the normal pressure that would be used in a mission.\n\nLiquid oxygen spills, if allowed to soak into organic matter, such as wood, petrochemicals, and asphalt can cause these materials to detonate unpredictably on subsequent mechanical impact.\n\n\n\n"}
{"id": "47776867", "url": "https://en.wikipedia.org/wiki?curid=47776867", "title": "Ozarka", "text": "Ozarka\n\nOzarka is a brand of spring water which is bottled and sold in the South Central United States, including Arkansas, Texas, Oklahoma, Louisiana, Mississippi, and portions of Tennessee, Missouri, and Kansas. The Ozarka Spring Water Company was founded in Eureka Springs, Arkansas in 1905, but is now a division of Nestlé Waters North America, Inc. Ozarka's slogan is Born Better. Eureka Springs is situated in the Ozark Mountains, from which the company's name comes. The Ozarka water is selected from natural springs sources in Texas. It is no longer sourced in Eureka Springs, Arkansas. \n\n"}
{"id": "26448345", "url": "https://en.wikipedia.org/wiki?curid=26448345", "title": "Plasma actuator", "text": "Plasma actuator\n\nPlasma actuators are a type of actuator currently being developed for aerodynamic flow control. Plasma actuators impart force in a similar way to ionocraft.\n\nThe working of these actuators is based on the formation of a low-temperature plasma between a pair of asymmetric electrodes by application of a high-voltage AC signal across the electrodes. Consequently, air molecules from the air surrounding the electrodes are ionized, and are accelerated through the electric field.\n\nPlasma actuators operating at the atmospheric conditions are promising for flow control, mainly for their physical properties, such as the induced body force by a strong electric field and the generation of heat during an electric arc, and the simplicity of their constructions and placements. In particular, the recent invention of glow discharge plasma actuators by Roth (2003) that can produce sufficient quantities of glow discharge plasma in the atmosphere pressure air helps to yield an increase in flow control performance.\n\nEither a direct current (DC) or an alternating current (AC) power supply or a microwave microdischarge can be used for different configurations of plasma actuators. One schematic of an AC power supply design for a dielectric barrier discharge plasma actuator is given here as an example. The performance of plasma actuators is determined by dielectric materials and power inputs, later is limited by the qualities of MOSFET or IGBT.\nThe driving waveforms can be optimized to achieve a better actuation (induced flow speed). However, a sinusoidal waveform may be more preferable for the simplicity in power supply construction. The additional benefit is the relatively less electromagnetic interference. Pulse width modulation can be adopted to instantaneously adjust the strength of actuation.\n\nManipulation of the encapsulated electrode and distributing the encapsulated electrode throughout the dielectric layer has been shown to alter the performance of the dielectric barrier discharge (DBD) plasma actuator. Locating the initial encapsulated electrode closer to the dielectric surface results in induced velocities higher than the baseline case for a given voltage. In addition, Actuators with a shallow initial electrode are able to more efficiently impart momentum and mechanical power into the flow.\n\nNo matter how much funding has been invested and the number of various private claims of a high induced speed, the maximum, average speed induced by plasma actuators on an atmospheric pressure conviction, without any assistant of mechanical amplifier (chamber, cavity etc.), is still less than 10 m/s.\n\nWhen dealing with real life aircraft equipped with plasma actuators, it is important to consider the effect of temperature. The temperature variations encountered during a flight envelope may have adverse effects in actuator performance. It is found that for a constant peak-to-peak voltage the maximum velocity produced by the actuator depends directly on the dielectric surface temperature. The findings suggest that by changing the actuator temperature the performance can be maintained or even altered at different environmental conditions. Increasing dielectric surface temperature can increase the plasma actuator performance by increasing the momentum flux whilst consuming slightly higher energy.\n\nSome recent applications of plasma actuation include high-speed flow control using localized arc filament plasma actuators, and low-speed flow control using dielectric barrier discharges and sliding discharges. The present research of plasma actuators is mainly focused on three directions: (1) various designs of plasma actuators; (2) flow control applications; and (3) control-oriented modeling of flow applications under plasma actuation. In addition, new experimental and numerical methods are being developed to provide physical insights.\n\nA plasma actuator induces a local flow speed perturbation, which will be developed downstream to a vortex sheet. As a result, plasma actuators can behave as vortex generators. The difference between this and traditional vortex generation is that there are no mechanical moving parts or any drilling holes on aerodynamic surfaces, demonstrating an important benefit of plasma actuators. Three dimensional actuators such as Serpentine geometry plasma actuator generate streamwise oriented vortices, which are useful to control the flow.\n\nActive noise control normally denotes noise cancellation, that is, a noise-cancellation speaker emits a sound wave with the same amplitude but with inverted phase (also known as antiphase) to the original sound. However, active noise control with plasma adopts different strategies. The first one uses the discovery that sound pressure could be attenuated when it passes through a plasma sheet.\nThe second one, and being more widely used, is to actively suppress the flow-field that is responsible to flow-induced noise (also known as aeroacoustics), using plasma actuators. It has been demonstrated that both tonal noise and broadband noise (difference can refer to tonal versus broadband) can be actively attenuated by a carefully designed plasma actuator.\n\nPlasma has been introduced to hypersonic flow control. Firstly, plasma could be much easier generated for hypersonic vehicle at high altitude with quite low atmospheric pressure and high surface temperature. Secondly, the classical aerodynamic surface has little actuation for the case.\n\nInterest in plasma actuators as active flow control devices is growing rapidly due to their lack of mechanical parts, light weight and high response frequency. The characteristics of a dielectric barrier discharge (DBD) plasma actuator when exposed to an unsteady flow generated by a shock tube is examined. A Study shows that not only is the shear layer outside of the shock tube affected by the plasma but the passage of the shock front and high-speed flow behind it also greatly influences the properties of the plasma\n\nPlasma actuators could be mounted on the airfoil to control flight attitude and thereafter flight trajectory. The cumbersome design and maintenance efforts of mechanical and hydraulic transmission systems in a classical rudder can thus be saved. The price to pay is that one should design a suitable high voltage/power electric system satisfying EMC rule. Hence, in addition to flow control, plasma actuators hold potential in top-level flight control, in particular for UAV and extraterrestrial planet (with suitable atmospheric conditions) investigations.\n\nOn the other hand, the whole flight control strategy should be reconsidered taking account of characteristics of plasma actuators. One preliminary roll control system with DBD plasma actuators is shown in the figure.\n\nIt can be seen that plasma actuators deployed on the both sides of an airfoil. The roll control can be controlled by activating plasma actuators according to the roll angle feedback.\nAfter studying various feedback control methodologies, the bang–bang control method was chosen to design the roll control system based on plasma actuators. The reason is that bang-bang control is time optimal and insensitive to plasma actuations, which quickly vary in difference atmospheric and electric conditions.\n\nVarious numerical models have been proposed to simulate plasma actuations in flow control. They are listed below according to the computational cost, from the most expensive to the cheapest.\n\nThe most important potential of plasma actuators is its ability to bridge fluids and electricity. A modern closed-loop control system and the following information theoretical methods can be applied to the relatively classical aerodynamic sciences. A control-oriented model for plasma actuation in flow control has been proposed for a cavity flow control case.\n\n"}
{"id": "7122953", "url": "https://en.wikipedia.org/wiki?curid=7122953", "title": "Plasma contactor", "text": "Plasma contactor\n\nPlasma contactors are devices used on spacecraft in order to prevent accumulation of electrostatic charge through the expulsion of plasma (often Xenon).\n\nAn electrical contactor is an electrically controlled switch which closes a power or high voltage electrical circuit. A plasma contactor changes the electrically insulating vacuum into a conductor by providing movable electrons and positive gas ions. This conductive path closes a phantom loop circuit to discharge or neutralize the static electricity that can build up on a spacecraft.\n\nSpace contains regions with varying concentrations of charged particles such as the plasma sheet, and a static charge builds up as the spacecraft moves between these regions, or as the electrical potential varies within such a region.\n\nStatic electricity may also build up on a spacecraft as a result of space radiation, including sunlight, depending on the materials used on the surfaces of the spacecraft.\n\nA plasma contactor is mounted on the Z1 segment of the International Space Station Integrated Truss Structure.\n\n\n"}
{"id": "36979", "url": "https://en.wikipedia.org/wiki?curid=36979", "title": "Rice", "text": "Rice\n\nRice is the seed of the grass species \"Oryza sativa\" (Asian rice) or \"Oryza glaberrima\" (African rice). As a cereal grain, it is the most widely consumed staple food for a large part of the world's human population, especially in Asia. It is the agricultural commodity with the third-highest worldwide production (rice, 741.5 million tonnes in 2014), after sugarcane (1.9 billion tonnes) and maize (1.0 billion tonnes).\nSince sizable portions of sugarcane and maize crops are used for purposes other than human consumption, rice is the most important grain with regard to human nutrition and caloric intake, providing more than one-fifth of the calories consumed worldwide by humans. There are many varieties of rice and culinary preferences tend to vary regionally.\n\nRice, a monocot, is normally grown as an annual plant, although in tropical areas it can survive as a perennial and can produce a ratoon crop for up to 30 years. Rice cultivation is well-suited to countries and regions with low labor costs and high rainfall, as it is labor-intensive to cultivate and requires ample water. However, rice can be grown practically anywhere, even on a steep hill or mountain area with the use of water-controlling terrace systems. Although its parent species are native to Asia and certain parts of Africa, centuries of trade and exportation have made it commonplace in many cultures worldwide.\nThe traditional method for cultivating rice is flooding the fields while, or after, setting the young seedlings. This simple method requires sound planning and servicing of the water damming and channeling, but reduces the growth of less robust weed and pest plants that have no submerged growth state, and deters vermin. While flooding is not mandatory for the cultivation of rice, all other methods of irrigation require higher effort in weed and pest control during growth periods and a different approach for fertilizing the soil.\n\nThe name wild rice is usually used for species of the genera \"Zizania\" and \"Porteresia\", both wild and domesticated, although the term may also be used for primitive or uncultivated varieties of \"Oryza\".\n\nFirst used in English in the middle of the 13th century, the word \"rice\" derives from the Old French \"ris\", which comes from the Italian \"riso\", in turn from the Latin \"oriza\", which derives from the Greek ὄρυζα (\"oruza\"). The Greek word is the source of all European words (cf. Welsh \"reis\", German \"Reis\", Lithuanian \"ryžiai\", Serbo-Croatian \"riža\", Polish \"ryż\", Dutch \"rijst\", Hungarian \"rizs\", Romanian \"orez\", Spanish \"arroz\").\n\nThe origin of the Greek word is unclear. It is sometimes held to be from the Tamil word (\"arisi\"), or rather Old Tamil \"arici\". However, Krishnamurti disagrees with the notion that Old Tamil \"arici\" is the source of the Greek term, and proposes that it was borrowed from descendants of Proto-Dravidian *\"wariñci\" instead. Mayrhofer suggests that the immediate source of the Greek word is to be sought in Old Iranian words of the types *\"vrīz-\" or *\"vrinj-\" (Source of the modern Persian word \"Berenj\"), but these are ultimately traced back to Indo-Aryan (as in Sanskrit \"vrīhí-\"). P. T. Srinivasa Iyengar assumed that the Sanskrit \"vrīhí-\" is derived from the Tamil \"arici\", while Ferdinand Kittel derived it from the Dravidian root \"variki\". However, R. Swaminatha Aiyar believes that the Sanskrit \"vrīhí-\" is derived from a Proto-Indo-Iranian root, and the Old Tamil \"arici\" is also of Indo-European origin.\n\nThe rice plant can grow to tall, occasionally more depending on the variety and soil fertility. It has long, slender leaves long and broad. The small wind-pollinated flowers are produced in a branched arching to pendulous inflorescence long. The edible seed is a grain (caryopsis) long and thick.\n\nThe varieties of rice are typically classified as long-, medium-, and short-grained. The grains of long-grain rice (high in amylose) tend to remain intact after cooking; medium-grain rice (high in amylopectin) becomes more sticky. Medium-grain rice is used for sweet dishes, for \"risotto\" in Italy, and many rice dishes, such as \"arròs negre\", in Spain. Some varieties of long-grain rice that are high in amylopectin, known as Thai Sticky rice, are usually steamed. A stickier medium-grain rice is used for \"sushi\"; the stickiness allows rice to hold its shape when molded. Medium-grain rice is used extensively in Japan, including to accompany savoury dishes, where it is usually served plain in a separate dish. Short-grain rice is often used for rice pudding.\n\nInstant rice differs from parboiled rice in that it is fully cooked and then dried, though there is a significant degradation in taste and texture. Rice flour and starch often are used in batters and breadings to increase crispiness.\n\nRice is typically rinsed before cooking to remove excess starch. Rice produced in the US is usually fortified with vitamins and minerals, and rinsing will result in a loss of nutrients. Rice may be rinsed repeatedly until the rinse water is clear to improve the texture and taste.\n\nRice may be soaked to decrease cooking time, conserve fuel, minimize exposure to high temperature, and reduce stickiness. For some varieties, soaking improves the texture of the cooked rice by increasing expansion of the grains. Rice may be soaked for 30 minutes up to several hours.\n\nBrown rice may be soaked in warm water for 20 hours to stimulate germination. This process, called germinated brown rice (GBR), activates enzymes and enhances amino acids including gamma-aminobutyric acid to improve the nutritional value of brown rice. This method is a result of research carried out for the United Nations International Year of Rice.\n\nRice is cooked by boiling or steaming, and absorbs water during cooking. With the absorption method, rice may be cooked in a volume of water similar to the volume of rice. With the rapid-boil method, rice may be cooked in a large quantity of water which is drained before serving. Rapid-boil preparation is not desirable with enriched rice, as much of the enrichment additives are lost when the water is discarded. Electric rice cookers, popular in Asia and Latin America, simplify the process of cooking rice. Rice (or any other grain) is sometimes quickly fried in oil or fat before boiling (for example saffron rice or risotto); this makes the cooked rice less sticky, and is a cooking style commonly called pilaf in Iran and Afghanistan or biryani in India and Pakistan .\n\nIn Arab cuisine, rice is an ingredient of many soups and dishes with fish, poultry, and other types of meat. It is also used to stuff vegetables or is wrapped in grape leaves (dolma). When combined with milk, sugar, and honey, it is used to make desserts. In some regions, such as Tabaristan, bread is made using rice flour. Medieval Islamic texts spoke of medical uses for the plant. Rice may also be made into congee (also called rice porridge or rice gruel) by adding more water than usual, so that the cooked rice is saturated with water, usually to the point that it disintegrates. Rice porridge is commonly eaten as a breakfast food, and is also a traditional food for the sick.\n\nRice is the staple food of over half the world's population. It is the predominant dietary energy source for 17 countries in Asia and the Pacific, 9 countries in North and South America and 8 countries in Africa. Rice provides 20% of the world’s dietary energy supply, while wheat supplies 19% and maize (corn) 5%.\n\nCooked, unenriched, white, long-grained rice is composed of 68% water, 28% carbohydrates, 3% protein, and negligible fat (table). In a 100 gram serving, it provides 130 calories and contains no micronutrients in significant amounts, with all less than 10% of the Daily Value (DV) (table). Cooked, white, short-grained rice also provides 130 calories and contains moderate amounts of B vitamins, iron, and manganese (10–17% DV) per 100 gram amount (table).\n\nA detailed analysis of nutrient content of rice suggests that the nutrition value of rice varies based on a number of factors. It depends on the strain of rice, such as white, brown, red, and black (or purple) varieties having different prevalence across world regions. It also depends on nutrient quality of the soil rice is grown in, whether and how the rice is polished or processed, the manner it is enriched, and how it is prepared before consumption.\n\nA 2018 World Health Organization (WHO) guideline showed that fortification of rice to reduce malnutrition may involve different micronutrient strategies, including iron only, iron with zinc, vitamin A, and folic acid, or iron with other B-complex vitamins, such as thiamin, niacin, vitamin B6, and pantothenic acid. A systematic review of clinical research on the efficacy of rice fortification showed the strategy had the main effect of reducing the risk of iron deficiency by 35% and increasing blood levels of hemoglobin. The guideline established a major recommendation: \"Fortification of rice with iron is recommended as a public health strategy to improve the iron status of populations, in settings where rice is a staple food.\"\n\nRice grown experimentally under elevated carbon dioxide levels, similar to those predicted for the year 2100 as a result of human activity, had less iron, zinc, and protein, as well as lower levels of thiamin, riboflavin, folic acid, and pantothenic acid.\n\nAs arsenic is a natural element in soil, water, and air, the United States Food and Drug Administration (FDA) monitors the levels of arsenic in foods, particularly in rice products used commonly for infant food. While growing, rice plants tend to absorb arsenic more readily than other food crops, requiring expanded testing by the FDA for possible arsenic-related risks associated with rice consumption in the United States. In April 2016, the FDA proposed a limit of 100 parts per billion (ppb) for inorganic arsenic in infant rice cereal and other foods to minimize exposure of infants to arsenic. For water contamination by arsenic, the United States Environmental Protection Agency has set a lower standard of 10 ppb.\n\nArsenic is a Group 1 carcinogen. The amount of arsenic in rice varies widely with the greatest concentration in brown rice and rice grown on land formerly used to grow cotton, such as in Arkansas, Louisiana, Missouri, and Texas. White rice grown in Arkansas, Louisiana, Missouri, and Texas, which account collectively for 76 percent of American-produced rice, had higher levels of arsenic than other regions of the world studied, possibly because of past use of arsenic-based pesticides to control cotton weevils. Jasmine rice from Thailand and Basmati rice from Pakistan and India contain the least arsenic among rice varieties in one study. China has set a limit of 150 ppb for arsenic in rice.\n\nCooked rice can contain \"Bacillus cereus\" spores, which produce an emetic toxin when left at . When storing cooked rice for use the next day, rapid cooling is advised to reduce the risk of toxin production. One of the enterotoxins produced by \"Bacillus cereus\" is heat-resistant; reheating contaminated rice kills the bacteria, but does not destroy the toxin already present.\n\nRice can be grown in different environments, depending upon water availability. Generally, rice does not thrive in a waterlogged area, yet it can survive and grow herein and it can also survive flooding.\n\n\nWild rice, from which the crop was developed, may have its native range in Australia. Chinese legends attribute the domestication of rice to Shennong, the legendary emperor of China and inventor of Chinese agriculture. Genetic evidence has shown that rice originates from a single domestication 8,200–13,500 years ago in the Pearl River valley region of Ancient China. Previously, archaeological evidence had suggested that rice was domesticated in the Yangtze River valley region in China.\n\nFrom East Asia (particularly from China), rice was spread to Southeast and South Asia across the caravan routes of the central Asian steppes. Because Buddhist monks were vegetarian, they carried rice with them across the steppes. Thus, a Buddhist text called the Aggañña Sutta stated that \"rice grows as long as Buddhism spreads.\" Rice was introduced to Europe through Western Asia, and to the Americas through European colonization.\n\nThere have been many debates on the origins of the domesticated rice. Genetic evidence published in the \"Proceedings of the National Academy of Sciences of the United States of America\" (PNAS) shows that all forms of Asian rice, both \"indica\" and \"japonica\", spring from a single domestication that occurred 8,200–13,500 years ago in China of the wild rice \"Oryza rufipogon\". A 2012 study published in \"Nature\", through a map of rice genome variation, indicated that the domestication of rice occurred in the Pearl River valley region of China based on the genetic evidence. From East Asia, rice was spread to South and Southeast Asia.\nBefore this research, the commonly accepted view, based on archaeological evidence, is that rice was first domesticated in the region of the Yangtze River valley in China. Some scholars have also suggested that it was first cultivated in the southern slopes of the Himalayas.\n\nMorphological studies of rice phytoliths from the Diaotonghuan archaeological site clearly show the transition from the collection of wild rice to the cultivation of domesticated rice. The large number of wild rice phytoliths at the Diaotonghuan level dating from 12,000–11,000 BP indicates that wild rice collection was part of the local means of subsistence. Changes in the morphology of Diaotonghuan phytoliths dating from 10,000–8,000 BP show that rice had by this time been domesticated. Soon afterwards the two major varieties of indica and japonica rice were being grown in Central China. In the late 3rd millennium BC, there was a rapid expansion of rice cultivation into mainland Southeast Asia and westwards across India and Nepal.\n\nIn 2003, Korean archaeologists claimed to have discovered the world's oldest domesticated rice. Their 15,000-year-old age challenges the accepted view that rice cultivation originated in China about 12,000 years ago. These findings were received by academia with strong skepticism, and the results and their publicizing has been cited as being driven by a combination of nationalist and regional interests. In 2011, a combined effort by the Stanford University, New York University, Washington University in St. Louis, and Purdue University has provided the strongest evidence yet that there is only one single origin of domesticated rice, in the Yangtze Valley of China.\n\nRice spread to the Middle East where, according to Zohary and Hopf (2000, p. 91), \"O. sativa\" was recovered from a grave at Susa in Iran (dated to the 1st century AD).\n\nAfrican rice has been cultivated for 3500 years. Between 1500 and 800 BC, \"Oryza glaberrima\" propagated from its original centre, the Niger River delta, and extended to Senegal. However, it never developed far from its original region. Its cultivation even declined in favour of the Asian species, which was introduced to East Africa early in the common era and spread westward. African rice helped Africa conquer its famine of 1203.\n\nToday, the majority of all rice produced comes from China, India, Indonesia, Bangladesh, Vietnam, Thailand, Myanmar, Pakistan, Philippines, Korea and Japan. Asian farmers still account for 87% of the world's total rice production.\n\nRice is the major food amongst all the ethnic groups in Nepal. In the Terai, most rice varieties are cultivated during the rainy season. The principal rice growing season, known as \"Berna-Bue Charne\", is from June to July when water is sufficient for only a part of the fields; the subsidiary season, known as \"Ropai, is from April to September, when there is usually enough water to sustain the cultivation of all rice fields. Farmers use irrigation channels throughout the cultivation seasons.\n\nThe Banaue Rice Terraces () are 2,000-year-old terraces that were carved into the mountains of Ifugao in the Philippines by ancestors of the indigenous people. The Rice Terraces are commonly referred to as the \"Eighth Wonder of the World\". It is commonly thought that the terraces were built with minimal equipment, largely by hand. The terraces are located approximately 1500 meters (5000 ft) above sea level. They are fed by an ancient irrigation system from the rainforests above the terraces. It is said that if the steps were put end to end, it would encircle half the globe.\nThe terraces are found in the province of Ifugao and the Ifugao people have been its caretakers. Ifugao culture revolves around rice and the culture displays an elaborate array of celebrations linked with agricultural rites from rice cultivation to rice consumption. The harvest season generally calls for thanksgiving feasts, while the concluding harvest rites called \"tango\" or \"tungul\" (a day of rest) entails a strict taboo on any agricultural work. Partaking of the \"bayah\" (rice beer), rice cakes, and betel nut constitutes an indelible practise during the festivities.\n\nThe Ifugao people practice traditional farming spending most of their labor at their terraces and forest lands while occasionally tending to root crop cultivation. The Ifugaos have also been known to culture edible shells, fruit trees, and other vegetables which have been exhibited among Ifugaos for generations. The building of the rice terraces consists of blanketing walls with stones and earth which are designed to draw water from a main irrigation canal above the terrace clusters. Indigenous rice terracing technologies have been identified with the Ifugao’s rice terraces such as their knowledge of water irrigation, stonework, earthwork and terrace maintenance. As their source of life and art, the rice terraces have sustained and shaped the lives of the community members.\n\nRice is the staple food amongst all the ethnic groups in Sri Lanka. Agriculture in Sri Lanka mainly depends on the rice cultivation. Rice production is acutely dependent on rainfall and government supply necessity of water through irrigation channels throughout the cultivation seasons. The principal cultivation season, known as \"Maha\", is from October to March and the subsidiary cultivation season, known as \"Yala\", is from April to September. During Maha season, there is usually enough water to sustain the cultivation of all rice fields, nevertheless in Yala season there is only enough water for cultivation of half of the land extent.\n\nTraditional rice varieties are now making a comeback with the recent interest in green foods.\n\nRice is the main export of Thailand, especially white jasmine rice 105 (Dok Mali 105). Thailand has a large number of rice varieties, 3,500 kinds with different characters, and five kinds of wild rice cultivates. In each region of the country there are different rice seed types. Their use depends on weather, atmosphere, and topography.\n\nThe northern region has both low lands and high lands. The farmers' usual crop is non-glutinous rice such as Niew Sun Pah Tong rice. This rice is naturally protected from leaf disease, and its paddy (unmilled rice) () has a brown color. The northeastern region is a large area where farmers can cultivate about 36 million square meters of rice. Although most of it is plains and dry areas, white jasmine rice 105—the most famous Thai rice—can be grown there. White jasmine rice was developed in Chonburi Province first and after that grown in many areas in the country, but the rice from this region has a high quality, because it's softer, whiter, and more fragrant. This rice can resist drought, acidic soil, and alkaline soil.\n\nThe central region is mostly composed of plains. Most farmers grow Jao rice. For example, Pathum Thani 1 rice which has qualities similar to white jasmine 105 rice. Its paddy has the color of thatch and the cooked rice has fragrant grains also.\n\nIn the southern region, most farmers transplant around boundaries to the flood plains or on the plains between mountains. Farming in the region is slower than other regions because the rainy season comes later. The popular rice varieties in this area are the Leb Nok Pattani seeds, a type of Jao rice. Its paddy has the color of thatch and it can be processed to make noodles.\n\nOne of the earliest known examples of companion planting is the growing of rice with Azolla, the mosquito fern, which covers the top of a fresh rice paddy's water, blocking out any competing plants, as well as fixing nitrogen from the atmosphere for the rice to use. The rice is planted when it is tall enough to poke out above the azolla. This method has been used for at least a thousand years.\n\nRice was grown in some areas of Mesopotamia (southern Iraq). With the rise of Islam it moved north to Nisibin, the southern shores of the Caspian Sea (in Gilan and Mazanderan provinces of Iran) and then beyond the Muslim world into the valley of the Volga. In Egypt, rice is mainly grown in the Nile Delta. In Palestine, rice came to be grown in the Jordan Valley. Rice is also grown in Saudi Arabia at Al-Hasa Oasis and in Yemen.\n\nRice was known to the Classical world, being imported from Egypt, and perhaps west Asia. It was known to Greece (where it is still cultivated in Macedonia and Thrace) by returning soldiers from Alexander the Great's military expedition to Asia. Large deposits of rice from the first century AD have been found in Roman camps in Germany.\n\nThe Moors brought Asiatic rice to the Iberian Peninsula in the 10th century. Records indicate it was grown in Valencia and Majorca. In Majorca, rice cultivation seems to have stopped after the Christian conquest, although historians are not certain.\n\nMuslims also brought rice to Sicily with cultivation starting in the 9th century, where it was an important crop long before it is noted in the plain of Pisa (1468) or in the Lombard plain (1475), where its cultivation was promoted by Ludovico Sforza, Duke of Milan, and demonstrated in his model farms.\n\nAfter the 15th century, rice spread throughout Italy and then France, later propagating to all the continents during the age of European exploration.\n\nIn European Russia, a short-grain, starchy rice similar to the Italian varieties, has been grown in the Krasnodar Krai, and known in Russia as \"Kuban Rice\" or \"Krasnodar Rice\". In the Russian Far East several \"japonica\" cultivars are grown in Primorye around the Khanka lake. Increasing scale of rice production in the region has recently brought criticism towards growers' alleged bad practices in regards to the environment.\n\nMost of the rice used today in the cuisine of the Americas is not native, but was introduced to Latin America and the Caribbean by European colonizers at an early date. However, there are at least two native (endemic) species of rice present in the Amazon region of South America, and one or both were used by the indigenous inhabitants of the region to create the domesticated form Oryza sp., some 4000 years ago.\n\nSpanish colonizers introduced Asian rice to Mexico in the 1520s at Veracruz, and the Portuguese and their African slaves introduced it at about the same time to colonial Brazil. Recent scholarship suggests that enslaved Africans played an active role in the establishment of rice in the New World and that African rice was an important crop from an early period. Varieties of rice and bean dishes that were a staple dish along the peoples of West Africa remained a staple among their descendants subjected to slavery in the Spanish New World colonies, Brazil and elsewhere in the Americas.\n\nIn 1694, rice arrived in South Carolina, probably originating from Madagascar. Tradition (possibly apocryphal) has it that pirate John Thurber was returning from a slave-trading voyage to Madagascar when he was blown off course and put into Charleston for repairs. While there he gave a bag of seed rice to explorer Dr. Henry Woodward, who planted the rice and experimented with it until finding that it grew exceptionally well in the wet Carolina soil.\n\nIn the United States, colonial South Carolina and Georgia grew and amassed great wealth from the slave labor obtained from the Senegambia area of West Africa and from coastal Sierra Leone. At the port of Charleston, through which 40% of all American slave imports passed, slaves from this region of Africa brought the highest prices due to their prior knowledge of rice culture, which was put to use on the many rice plantations around Georgetown, Charleston, and Savannah.\n\nFrom the enslaved Africans, plantation owners learned how to dyke the marshes and periodically flood the fields. At first the rice was laboriously milled by hand using large mortars and pestles made of wood, then winnowed in sweetgrass baskets (the making of which was another skill brought by slaves from Africa). The invention of the rice mill increased profitability of the crop, and the addition of water power for the mills in 1787 by millwright Jonathan Lucas was another step forward.\n\nRice culture in the southeastern U.S. became less profitable with the loss of slave labor after the American Civil War, and it finally died out just after the turn of the 20th century. Today, people can visit the only remaining rice plantation in South Carolina that still has the original winnowing barn and rice mill from the mid-19th century at the historic Mansfield Plantation in Georgetown, South Carolina. The predominant strain of rice in the Carolinas was from Africa and was known as 'Carolina Gold'. The cultivar has been preserved and there are current attempts to reintroduce it as a commercially grown crop.\n\nIn the southern United States, rice has been grown in southern Arkansas, Louisiana, and east Texas since the mid-19th century. Many Cajun farmers grew rice in wet marshes and low-lying prairies where they could also farm crayfish when the fields were flooded. In recent years rice production has risen in North America, especially in the Mississippi embayment in the states of Arkansas and Mississippi (see also Arkansas Delta and Mississippi Delta).\n\nRice cultivation began in California during the California Gold Rush, when an estimated 40,000 Chinese laborers immigrated to the state and grew small amounts of the grain for their own consumption. However, commercial production began only in 1912 in the town of Richvale in Butte County. By 2006, California produced the second-largest rice crop in the United States, after Arkansas, with production concentrated in six counties north of Sacramento. Unlike the Arkansas–Mississippi Delta region, California's production is dominated by short- and medium-grain \"japonica\" varieties, including cultivars developed for the local climate such as Calrose, which makes up as much as 85% of the state's crop.\n\nReferences to \"wild rice\" native to North America are to the unrelated \"Zizania palustris\".\n\nMore than 100 varieties of rice are commercially produced primarily in six states (Arkansas, Texas, Louisiana, Mississippi, Missouri, and California) in the U.S. According to estimates for the 2006 crop year, rice production in the U.S. is valued at $1.88 billion, approximately half of which is expected to be exported. The U.S. provides about 12% of world rice trade. The majority of domestic utilization of U.S. rice is direct food use (58%), while 16% is used in each of processed foods and beer. 10% is found in pet food.\n\nRice was one of the earliest crops planted in Australia by British settlers, who had experience with rice plantations in the Americas and India.\n\nAlthough attempts to grow rice in the well-watered north of Australia have been made for many years, they have consistently failed because of inherent iron and manganese toxicities in the soils and destruction by pests.\n\nIn the 1920s, it was seen as a possible irrigation crop on soils within the Murray-Darling Basin that were too heavy for the cultivation of fruit and too infertile for wheat.\n\nBecause irrigation water, despite the extremely low runoff of temperate Australia, was (and remains) very cheap, the growing of rice was taken up by agricultural groups over the following decades. Californian varieties of rice were found suitable for the climate in the Riverina, and the first mill opened at Leeton in 1951.\nEven before this Australia's rice production greatly exceeded local needs, and rice exports to Japan have become a major source of foreign currency. Above-average rainfall from the 1950s to the middle 1990s encouraged the expansion of the Riverina rice industry, but its prodigious water use in a practically waterless region began to attract the attention of environmental scientists. These became severely concerned with declining flow in the Snowy River and the lower Murray River.\n\nAlthough rice growing in Australia is highly profitable due to the cheapness of land, several recent years of severe drought have led many to call for its elimination because of its effects on extremely fragile aquatic ecosystems. The Australian rice industry is somewhat opportunistic, with the area planted varying significantly from season to season depending on water allocations in the Murray and Murrumbidgee irrigation regions.\n\nAustralian Aboriginal people have harvested native rice varieties for thousands of years, and there are ongoing efforts to grow commercial quantities of these species.\n\nIn 2016, world production of paddy rice was 741 million tonnes, led by China and India with a combined 50% of this total (table). Other major producers were Indonesia, Bangladesh and Vietnam (table).\n\nRice is a major food staple and a mainstay for the rural population and their food security. It is mainly cultivated by small farmers in holdings of less than 1 hectare. Rice is also a wage commodity for workers in the cash crop or non-agricultural sectors. Rice is vital for the nutrition of much of the population in Asia, as well as in Latin America and the Caribbean and in Africa; it is central to the food security of over half the world population. Developing countries account for 95% of the total production, with China and India alone responsible for nearly half of the world output.\n\nMany rice grain producing countries have significant losses post-harvest at the farm and because of poor roads, inadequate storage technologies, inefficient supply chains and farmer's inability to bring the produce into retail markets dominated by small shopkeepers. A World Bank – FAO study claims 8% to 26% of rice is lost in developing nations, on average, every year, because of post-harvest problems and poor infrastructure. Some sources claim the post-harvest losses to exceed 40%. Not only do these losses reduce food security in the world, the study claims that farmers in developing countries such as China, India and others lose approximately US$89 billion of income in preventable post-harvest farm losses, poor transport, the lack of proper storage and retail. One study claims that if these post-harvest grain losses could be eliminated with better infrastructure and retail network, in India alone enough food would be saved every year to feed 70 to 100 million people over a year.\n\nThe seeds of the rice plant are first milled using a rice huller to remove the chaff (the outer husks of the grain). At this point in the process, the product is called brown rice. The milling may be continued, removing the bran, \"i.e.\", the rest of the husk and the germ, thereby creating white rice. White rice, which keeps longer, lacks some important nutrients; moreover, in a limited diet which does not supplement the rice, brown rice helps to prevent the disease beriberi.\n\nEither by hand or in a rice polisher, white rice may be buffed with glucose or talc powder (often called polished rice, though this term may also refer to white rice in general), parboiled, or processed into flour. White rice may also be enriched by adding nutrients, especially those lost during the milling process. While the cheapest method of enriching involves adding a powdered blend of nutrients that will easily wash off (in the United States, rice which has been so treated requires a label warning against rinsing), more sophisticated methods apply nutrients directly to the grain, coating the grain with a water-insoluble substance which is resistant to washing.\n\nIn some countries, a popular form, parboiled rice (also known as converted rice and easy-cook rice) is subjected to a steaming or parboiling process while still a brown rice grain. The parboil process causes a gelatinisation of the starch in the grains. The grains become less brittle, and the color of the milled grain changes from white to yellow. The rice is then dried, and can then be milled as usual or used as brown rice. Milled parboiled rice is nutritionally superior to standard milled rice, because the process causes nutrients from the outer husk (especially thiamine) to move into the endosperm, so that less is subsequently lost when the husk is polished off during milling. Parboiled rice has an additional benefit in that it does not stick to the pan during cooking, as happens when cooking regular white rice. This type of rice is eaten in parts of India and countries of West Africa are also accustomed to consuming parboiled rice.\n\nRice bran, called \"nuka\" in Japan, is a valuable commodity in Asia and is used for many daily needs. It is a moist, oily inner layer which is heated to produce oil. It is also used as a pickling bed in making rice bran pickles and \"takuan\".\n\nRaw rice may be ground into flour for many uses, including making many kinds of beverages, such as \"amazake, horchata\", rice milk, and rice wine. Rice does not contain gluten, so is suitable for people on a gluten-free diet. Rice may also be made into various types of noodles. Raw, wild, or brown rice may also be consumed by raw-foodist or fruitarians if soaked and sprouted (usually a week to 30 days – gaba rice).\n\nProcessed rice seeds must be boiled or steamed before eating. Boiled rice may be further fried in cooking oil or butter (known as fried rice), or beaten in a tub to make \"mochi\".\n\nRice is a good source of protein and a staple food in many parts of the world, but it is not a complete protein: it does not contain all of the essential amino acids in sufficient amounts for good health, and should be combined with other sources of protein, such as nuts, seeds, beans, fish, or meat.\n\nRice, like other cereal grains, can be puffed (or popped). This process takes advantage of the grains' water content and typically involves heating grains in a special chamber. Further puffing is sometimes accomplished by processing puffed pellets in a low-pressure chamber. The ideal gas law means either lowering the local pressure or raising the water temperature results in an increase in volume prior to water evaporation, resulting in a puffy texture. Bulk raw rice density is about 0.9 g/cm³. It decreases to less than one-tenth that when puffed.\n\nUnmilled rice, known as \"paddy\" (Indonesia and Malaysia: padi; Philippines, palay), is usually harvested when the grains have a moisture content of around 25%. In most Asian countries, where rice is almost entirely the product of smallholder agriculture, harvesting is carried out manually, although there is a growing interest in mechanical harvesting. Harvesting can be carried out by the farmers themselves, but is also frequently done by seasonal labor groups. Harvesting is followed by threshing, either immediately or within a day or two. Again, much threshing is still carried out by hand but there is an increasing use of mechanical threshers. Subsequently, paddy needs to be dried to bring down the moisture content to no more than 20% for milling.\n\nA familiar sight in several Asian countries is paddy laid out to dry along roads. However, in most countries the bulk of drying of marketed paddy takes place in mills, with village-level drying being used for paddy to be consumed by farm families. Mills either sun dry or use mechanical driers or both. Drying has to be carried out quickly to avoid the formation of molds. Mills range from simple hullers, with a throughput of a couple of tonnes a day, that simply remove the outer husk, to enormous operations that can process 4,000 tonnes a day and produce highly polished rice. A good mill can achieve a paddy-to-rice conversion rate of up to 72% but smaller, inefficient mills often struggle to achieve 60%. These smaller mills often do not buy paddy and sell rice but only service farmers who want to mill their paddy for their own consumption.\n\nBecause of the importance of rice to human nutrition and food security in Asia, the domestic rice markets tend to be subject to considerable state involvement. While the private sector plays a leading role in most countries, agencies such as BULOG in Indonesia, the NFA in the Philippines, VINAFOOD in Vietnam and the Food Corporation of India are all heavily involved in purchasing of paddy from farmers or rice from mills and in distributing rice to poorer people. BULOG and NFA monopolise rice imports into their countries while VINAFOOD controls all exports from Vietnam.\n\nWorld trade figures are very different from those for production, as less than 8% of rice produced is traded internationally. In economic terms, the global rice trade was a small fraction of 1% of world mercantile trade. Many countries consider rice as a strategic food staple, and various governments subject its trade to a wide range of controls and interventions.\n\nDeveloping countries are the main players in the world rice trade, accounting for 83% of exports and 85% of imports. While there are numerous importers of rice, the exporters of rice are limited. Just five countries – Thailand, Vietnam, China, the United States and India – in decreasing order of exported quantities, accounted for about three-quarters of world rice exports in 2002. However, this ranking has been rapidly changing in recent years. In 2010, the three largest exporters of rice, in decreasing order of quantity exported were Thailand, Vietnam and India. By 2012, India became the largest exporter of rice with a 100% increase in its exports on year-to-year basis, and Thailand slipped to third position. Together, Thailand, Vietnam and India accounted for nearly 70% of the world rice exports.\n\nThe primary variety exported by Thailand and Vietnam were Jasmine rice, while exports from India included aromatic Basmati variety. China, an exporter of rice in early 2000s, was a net importer of rice in 2010 and will become the largest net importer, surpassing Nigeria, in 2013. According to a USDA report, the world's largest exporters of rice in 2012 were India (9.75 million tonnes), Vietnam (7 million tonnes), Thailand (6.5 million tonnes), Pakistan (3.75 million tonnes) and the United States (3.5 million tonnes).\n\nMajor importers usually include Nigeria, Indonesia, Bangladesh, Saudi Arabia, Iran, Iraq, Malaysia, the Philippines, Brazil and some African and Persian Gulf countries. In common with other West African countries, Nigeria is actively promoting domestic production. However, its very heavy import duties (110%) open it to smuggling from neighboring countries. Parboiled rice is particularly popular in Nigeria. Although China and India are the two largest producers of rice in the world, both countries consume the majority of the rice produced domestically, leaving little to be traded internationally.\n\nThe average world yield for rice was 4.3 tonnes per hectare, in 2010.\n\nAustralian rice farms were the most productive in 2010, with a nationwide average of 10.8 tonnes per hectare.\n\nYuan Longping of China National Hybrid Rice Research and Development Center, China, set a world record for rice yield in 2010 at 19 tonnes per hectare on a demonstration plot. In 2011, this record was surpassed by an Indian farmer, Sumant Kumar, with 22.4 tonnes per hectare in Bihar. Both these farmers claim to have employed newly developed rice breeds and System of Rice Intensification (SRI), a recent innovation in rice farming. SRI is claimed to have set new national records in rice yields, within the last 10 years, in many countries. The claimed Chinese and Indian yields have yet to be demonstrated on seven-hectare lots and to be reproducible over two consecutive years on the same farm.\n\nIn late 2007 to May 2008, the price of grains rose greatly due to droughts in major producing countries (particularly Australia), increased use of grains for animal feed and US subsidies for bio-fuel production. Although there was no shortage of rice on world markets this general upward trend in grain prices led to panic buying by consumers, government rice export bans (in particular, by Vietnam and India) and inflated import orders by the Philippines marketing board, the National Food Authority. This caused significant rises in rice prices. In late April 2008, prices hit 24 US cents a pound, twice the price of seven months earlier. Over the period of 2007 to 2013, the Chinese government has substantially increased the price it pays domestic farmers for their rice, rising to per metric ton by 2013. The 2013 price of rice originating from other southeast Asian countries was a comparably low per metric ton.\n\nOn April 30, 2008, Thailand announced plans for the creation of the Organisation of Rice Exporting Countries (OREC) with the intention that this should develop into a price-fixing cartel for rice. However, little progress had been made by mid-2011 to achieve this.\n\nAs of 2009 world food consumption of rice was 531.6 million metric tons of paddy equivalent (354,603 of milled equivalent), while the far largest consumers were China consuming 156.3 million metric tons of paddy equivalent (29.4% of the world consumption) and India consuming 123.5 million metric tons of paddy equivalent (23.3% of the world consumption). Between 1961 and 2002, per capita consumption of rice increased by 40%.\n\nRice is the most important crop in Asia. In Cambodia, for example, 90% of the total agricultural area is used for rice production.\n\nU.S. rice consumption has risen sharply over the past 25 years, fueled in part by commercial applications such as beer production. Almost one in five adult Americans now report eating at least half a serving of white or brown rice per day.\n\nRice cultivation on wetland rice fields is thought to be responsible for 11% of the anthropogenic methane emissions. Rice requires slightly more water to produce than other grains. Rice production uses almost a third of Earth’s fresh water.\n\nLong-term flooding of rice fields cuts the soil off from atmospheric oxygen and causes anaerobic fermentation of organic matter in the soil. Methane production from rice cultivation contributes ~1.5% of anthropogenic greenhouse gases. Methane is twenty times more potent a greenhouse gas than carbon dioxide.\n\nA 2010 study found that, as a result of rising temperatures and decreasing solar radiation during the later years of the 20th century, the rice yield growth rate has decreased in many parts of Asia, compared to what would have been observed had the temperature and solar radiation trends not occurred. The yield growth rate had fallen 10–20% at some locations. The study was based on records from 227 farms in Thailand, Vietnam, Nepal, India, China, Bangladesh, and Pakistan. The mechanism of this falling yield was not clear, but might involve increased respiration during warm nights, which expends energy without being able to photosynthesize.\n\nRice requires high temperature above but not more than . Optimum temperature is around 30 °C (T) and 20 °C (T).\n\nThe amount of solar radiation received during the 45 days leading up to harvest determines final crop output.\n\nHigh water vapor content (in humid tropics) subjects unusual stress which favors the spread of fungal and bacterial diseases.\n\nLight wind transports CO to the leaf canopy but strong wind causes severe damage and may lead to sterility (due to pollen dehydration, spikelet sterility, and abortive endosperms).\n\nRice pests are any organisms or microbes with the potential to reduce the yield or value of the rice crop (or of rice seeds). Rice pests include weeds, pathogens, insects, nematode, rodents, and birds. A variety of factors can contribute to pest outbreaks, including climatic factors, improper irrigation, the overuse of insecticides and high rates of nitrogen fertilizer application. Weather conditions also contribute to pest outbreaks. For example, rice gall midge and army worm outbreaks tend to follow periods of high rainfall early in the wet season, while thrips outbreaks are associated with drought.\n\nMajor rice insect pests include: the brown planthopper (BPH), several spp. of stemborers – including those in the genera \"Scirpophaga\" and \"Chilo\", the rice gall midge, several spp. of rice bugs – notably in the genus \"Leptocorisa\", the rice leafroller, rice weevils and the Chinese rice grasshopper. The fall army worm, a species of Lepidoptera, also targets and causes damage to rice crops.\n\nRice blast, caused by the fungus \"Magnaporthe grisea\", is the most significant disease affecting rice cultivation. Other major rice diseases include: sheath blight, rice ragged stunt (vector: BPH), and tungro (vector: \"Nephotettix\" spp). There is also an ascomycete fungus, \"Cochliobolus miyabeanus\", that causes brown spot disease in rice.\n\nSeveral nematode species infect rice crops, causing diseases such as Ufra (Ditylenchus dipsaci), White tip disease (Aphelenchoide bessei), and root knot disease (Meloidogyne graminicola). Some nematode species such as \"Pratylenchus\" spp. are most dangerous in upland rice of all parts of the world. Rice root nematode (\"Hirschmanniella oryzae\") is a migratory endoparasite which on higher inoculum levels will lead to complete destruction of a rice crop. Beyond being obligate parasites, they also decrease the vigor of plants and increase the plants' susceptibility to other pests and diseases.\n\nThese include the apple snail \"Pomacea canaliculata\", panicle rice mite, rats, and the weed \"Echinochloa crusgali\".\n\nCrop protection scientists are trying to develop rice pest management techniques which are sustainable. In other words, to manage crop pests in such a manner that future crop production is not threatened. Sustainable pest management is based on four principles: biodiversity, host plant resistance (HPR), landscape ecology, and hierarchies in a landscape – from biological to social. At present, rice pest management includes cultural techniques, pest-resistant rice varieties, and pesticides (which include insecticide). Increasingly, there is evidence that farmers' pesticide applications are often unnecessary, and even facilitate pest outbreaks. By reducing the populations of natural enemies of rice pests, misuse of insecticides can actually lead to pest outbreaks. The International Rice Research Institute (IRRI) demonstrated in 1993 that an 87.5% reduction in pesticide use can lead to an overall drop in pest numbers. IRRI also conducted two campaigns in 1994 and 2003, respectively, which discouraged insecticide misuse and smarter pest management in Vietnam.\n\nRice plants produce their own chemical defenses to protect themselves from pest attacks. Some synthetic chemicals, such as the herbicide 2,4-D, cause the plant to increase the production of certain defensive chemicals and thereby increase the plant’s resistance to some types of pests. Conversely, other chemicals, such as the insecticide imidacloprid, can induce changes in the gene expression of the rice that cause the plant to become more susceptible to attacks by certain types of pests. 5-Alkylresorcinols are chemicals that can also be found in rice.\n\nBotanicals, so-called \"natural pesticides\", are used by some farmers in an attempt to control rice pests. Botanicals include extracts of leaves, or a mulch of the leaves themselves. Some upland rice farmers in Cambodia spread chopped leaves of the bitter bush (\"Chromolaena odorata\") over the surface of fields after planting. This practice probably helps the soil retain moisture and thereby facilitates seed germination. Farmers also claim the leaves are a natural fertilizer and helps suppress weed and insect infestations. \n\nAmong rice cultivars, there are differences in the responses to, and recovery from, pest damage. Many rice varieties have been selected for resistance to insect pests. Therefore, particular cultivars are recommended for areas prone to certain pest problems. The genetically based ability of a rice variety to withstand pest attacks is called resistance. Three main types of plant resistance to pests are recognized as nonpreference, antibiosis, and tolerance. Nonpreference (or antixenosis) describes host plants which insects prefer to avoid; antibiosis is where insect survival is reduced after the ingestion of host tissue; and tolerance is the capacity of a plant to produce high yield or retain high quality despite insect infestation.\n\nOver time, the use of pest-resistant rice varieties selects for pests that are able to overcome these mechanisms of resistance. When a rice variety is no longer able to resist pest infestations, resistance is said to have broken down. Rice varieties that can be widely grown for many years in the presence of pests and retain their ability to withstand the pests are said to have durable resistance. Mutants of popular rice varieties are regularly screened by plant breeders to discover new sources of durable resistance.\n\nRice is parasitized by the weed eudicot \"Striga hermonthica\", which is of local importance for this crop.\n\nWhile most rice is bred for crop quality and productivity, there are varieties selected for characteristics such as texture, smell, and firmness. There are four major categories of rice worldwide: indica, japonica, aromatic and glutinous. The different varieties of rice are not considered interchangeable, either in food preparation or agriculture, so as a result, each major variety is a completely separate market from other varieties. It is common for one variety of rice to rise in price while another one drops in price.\n\nRice cultivars also fall into groups according to environmental conditions, season of planting, and season of harvest, called ecotypes. Some major groups are the Japan-type (grown in Japan), \"buly\" and \"tjereh\" types (Indonesia); \"aman\" (main winter crop), \"aus\" (\"aush\", summer), and \"boro\" (spring) (Bengal and Assam). Cultivars exist that are adapted to deep flooding, and these are generally called \"floating rice\".\n\nThe largest collection of rice cultivars is at the International Rice Research Institute in the Philippines, with over 100,000 rice accessions held in the International Rice Genebank. Rice cultivars are often classified by their grain shapes and texture. For example, Thai Jasmine rice is long-grain and relatively less sticky, as some long-grain rice contains less amylopectin than short-grain cultivars. Chinese restaurants often serve long-grain as plain unseasoned steamed rice though short-grain rice is common as well. Japanese mochi rice and Chinese sticky rice are short-grain. Chinese people use sticky rice which is properly known as \"glutinous rice\" (note: glutinous refer to the glue-like characteristic of rice; does not refer to \"gluten\") to make zongzi. The Japanese table rice is a sticky, short-grain rice. Japanese sake rice is another kind as well.\n\nIndian rice cultivars include long-grained and aromatic Basmati (ਬਾਸਮਤੀ) (grown in the North), long and medium-grained Patna rice, and in South India (Andhra Pradesh and Karnataka) short-grained Sona Masuri (also called as Bangaru theegalu). In the state of Tamil Nadu, the most prized cultivar is \"ponni\" which is primarily grown in the delta regions of the Kaveri River. Kaveri is also referred to as ponni in the South and the name reflects the geographic region where it is grown. In the Western Indian state of Maharashtra, a short grain variety called Ambemohar is very popular. This rice has a characteristic fragrance of Mango blossom.\n\nAromatic rices have definite aromas and flavors; the most noted cultivars are Thai fragrant rice, Basmati, Patna rice, Vietnamese fragrant rice, and a hybrid cultivar from America, sold under the trade name Texmati. Both Basmati and Texmati have a mild popcorn-like aroma and flavor. In Indonesia, there are also \"red\" and \"black\" cultivars.\n\nHigh-yield cultivars of rice suitable for cultivation in Africa and other dry ecosystems, called the new rice for Africa (NERICA) cultivars, have been developed. It is hoped that their cultivation will improve food security in West Africa.\n\nDraft genomes for the two most common rice cultivars, \"indica\" and \"japonica\", were published in April 2002. Rice was chosen as a model organism for the biology of grasses because of its relatively small genome (~430 megabase pairs). Rice was the first crop with a complete genome sequence.\n\nOn December 16, 2002, the UN General Assembly declared the year 2004 the International Year of Rice. The declaration was sponsored by more than 40 countries.\n\nThe high-yielding varieties are a group of crops created intentionally during the Green Revolution to increase global food production. This project enabled labor markets in Asia to shift away from agriculture, and into industrial sectors. The first \"Rice Car\", IR8 was produced in 1966 at the International Rice Research Institute which is based in the Philippines at the University of the Philippines' Los Baños site. IR8 was created through a cross between an Indonesian variety named \"Peta\" and a Chinese variety named \"Dee Geo Woo Gen.\"\n\nScientists have identified and cloned many genes involved in the gibberellin signaling pathway, including GAI1 (Gibberellin Insensitive) and SLR1 (Slender Rice). Disruption of gibberellin signaling can lead to significantly reduced stem growth leading to a dwarf phenotype. Photosynthetic investment in the stem is reduced dramatically as the shorter plants are inherently more stable mechanically. Assimilates become redirected to grain production, amplifying in particular the effect of chemical fertilizers on commercial yield. In the presence of nitrogen fertilizers, and intensive crop management, these varieties increase their yield two to three times.\n\nAs the UN Millennium Development project seeks to spread global economic development to Africa, the \"Green Revolution\" is cited as the model for economic development. With the intent of replicating the successful Asian boom in agronomic productivity, groups like the Earth Institute are doing research on African agricultural systems, hoping to increase productivity. An important way this can happen is the production of \"New Rices for Africa\" (NERICA). These rices, selected to tolerate the low input and harsh growing conditions of African agriculture, are produced by the African Rice Center, and billed as technology \"from Africa, for Africa\". The NERICA have appeared in \"The New York Times\" (October 10, 2007) and \"International Herald Tribune\" (October 9, 2007), trumpeted as miracle crops that will dramatically increase rice yield in Africa and enable an economic resurgence. Ongoing research in China to develop perennial rice could result in enhanced sustainability and food security.\n\nRice kernels do not contain vitamin A, so people who obtain most of their calories from rice are at risk of vitamin A deficiency. German and Swiss researchers have genetically engineered rice to produce beta-carotene, the precursor to vitamin A, in the rice kernel. The beta-carotene turns the processed (white) rice a \"gold\" color, hence the name \"golden rice.\" The beta-carotene is converted to vitamin A in humans who consume the rice. Although some rice strains produce beta-carotene in the hull, no non-genetically engineered strains have been found that produce beta-carotene in the kernel, despite the testing of thousands of strains. Additional efforts are being made to improve the quantity and quality of other nutrients in golden rice.\n\nThe International Rice Research Institute is currently further developing and evaluating Golden Rice as a potential new way to help address vitamin A deficiency.\n\nVentria Bioscience has genetically modified rice to express lactoferrin, lysozyme which are proteins usually found in breast milk, and human serum albumin, These proteins have antiviral, antibacterial, and antifungal effects.\n\nRice containing these added proteins can be used as a component in oral rehydration solutions which are used to treat diarrheal diseases, thereby shortening their duration and reducing recurrence. Such supplements may also help reverse anemia.\n\nDue to the varying levels that water can reach in regions of cultivation, flood tolerant varieties have long been developed and used. Flooding is an issue that many rice growers face, especially in South and South East Asia where flooding annually affects 20 million hectares. Standard rice varieties cannot withstand stagnant flooding of more than about a week, mainly as it disallows the plant access to necessary requirements such as sunlight and essential gas exchanges, inevitably leading to plants being unable to recover.\nIn the past, this has led to massive losses in yields, such as in the Philippines, where in 2006, rice crops worth $65 million were lost to flooding. Recently developed cultivars seek to improve flood tolerance.\n\nDrought represents a significant environmental stress for rice production, with 19–23 million hectares of rainfed rice production in South and South East Asia often at risk. Under drought conditions, without sufficient water to afford them the ability to obtain the required levels of nutrients from the soil, conventional commercial rice varieties can be severely affected – for example, yield losses as high as 40% have affected some parts of India, with resulting losses of around US$800 million annually.\n\nThe International Rice Research Institute conducts research into developing drought-tolerant rice varieties, including the varieties 5411 and Sookha dhan, currently being employed by farmers in the Philippines and Nepal respectively. In addition, in 2013 the Japanese National Institute for Agrobiological Sciences led a team which successfully inserted the DEEPER ROOTING 1 (DRO1) gene, from the Philippine upland rice variety Kinandang Patong, into the popular commercial rice variety IR64, giving rise to a far deeper root system in the resulting plants. This facilitates an improved ability for the rice plant to derive its required nutrients in times of drought via accessing deeper layers of soil, a feature demonstrated by trials which saw the IR64 + DRO1 rice yields drop by 10% under moderate drought conditions, compared to 60% for the unmodified IR64 variety.\n\nSoil salinity poses a major threat to rice crop productivity, particularly along low-lying coastal areas during the dry season. For example, roughly 1 million hectares of the coastal areas of Bangladesh are affected by saline soils. These high concentrations of salt can severely affect rice plants’ normal physiology, especially during early stages of growth, and as such farmers are often forced to abandon these otherwise potentially usable areas.\n\nProgress has been made, however, in developing rice varieties capable of tolerating such conditions; the hybrid created from the cross between the commercial rice variety IR56 and the wild rice species \"Oryza coarctata\" is one example. \"O. coarctata\" is capable of successful growth in soils with double the limit of salinity of normal varieties, but lacks the ability to produce edible rice. Developed by the International Rice Research Institute, the hybrid variety can utilise specialised leaf glands that allow for the removal of salt into the atmosphere. It was initially produced from one successful embryo out of 34,000 crosses between the two species; this was then backcrossed to IR56 with the aim of preserving the genes responsible for salt tolerance that were inherited from \"O. coarctata\". Extensive trials are planned prior to the new variety being available to farmers by approximately 2017–18.\nWhen the problem of soil salinity arises it will be opportune to select salt tolerant varieties (IRRI or to resort to soil salinity control.\n\nSoil salinity is often measured as the electric conductivity (EC) of the extract of a saturated soil paste (ECe). The EC units are usually expressed in millimho/cm or dS/m. The critical ECe value of 5.5 dS/m in the figure, obtained from measurements in farmers' fields, indicates that the rice crop is slightly salt sensitive.\n\nProducing rice in paddies is harmful for the environment due to the release of methane by methanogenic bacteria. These bacteria live in the anaerobic waterlogged soil, and live off nutrients released by rice roots. Researchers have recently reported in \"Nature\" that putting the barley gene SUSIBA2 into rice creates a shift in biomass production from root to shoot (above ground tissue becomes larger, while below ground tissue is reduced), decreasing the methanogen population, and resulting in a reduction of methane emissions of up to 97%. Apart from this environmental benefit, the modification also increases the amount of rice grains by 43%, which makes it a useful tool in feeding a growing world population.\n\nRice is used as a model organism for investigating the molecular mechanisms of meiosis and DNA repair in higher plants. Meiosis is a key stage of the sexual cycle in which diploid cells in the ovule (female structure) and the anther (male structure) produce haploid cells that develop further into gametophytes and gametes. So far, 28 meiotic genes of rice have been characterized. Studies of rice gene OsRAD51C showed that this gene is necessary for homologous recombinational repair of DNA, particularly the accurate repair of DNA double-strand breaks during meiosis. Rice gene OsDMC1 was found to be essential for pairing of homologous chromosomes during meiosis, and rice gene OsMRE11 was found to be required for both synapsis of homologous chromosomes and repair of double-strand breaks during meiosis.\n\nRice plays an important role in certain religions and popular beliefs. In many cultures relatives will scatter rice during or towards the end of a wedding ceremony in front of the bride and groom.\n\nThe pounded rice ritual is conducted during weddings in Nepal. The bride gives a leafplate full of pounded rice to the groom after he requests it politely from her.\n\nIn the Philippines rice wine, popularly known as \"tapuy\", is used for important occasions such as weddings, rice harvesting ceremonies and other celebrations.\n\nDewi Sri is the traditional rice goddess of the Javanese, Sundanese, and Balinese people in Indonesia. Most rituals involving Dewi Sri are associated with the mythical origin attributed to the rice plant, the staple food of the region. In Thailand a similar rice deity is known as \"Phosop\"; she is a deity more related to ancient local folklore than a goddess of a structured, mainstream religion. The same female rice deity is known as \"Po Ino Nogar\" in Cambodia and as \"Nang Khosop\" in Laos. Ritual offerings are made during the different stages of rice production to propitiate the Rice Goddess in the corresponding cultures.\n\nA 2014 study of Han Chinese communities found that a history of farming rice makes cultures more psychologically interdependent, whereas a history of farming wheat makes cultures more independent.\n\nA Royal Ploughing Ceremony is held in certain Asian countries to mark the beginning of the rice planting season. It is still honored in the kingdoms of Cambodia and Thailand.\n\n\n"}
{"id": "34841856", "url": "https://en.wikipedia.org/wiki?curid=34841856", "title": "Single-atom transistor", "text": "Single-atom transistor\n\nA single-atom transistor is a device that can open and close an electrical circuit by the controlled and reversible repositioning of one single atom. The single-atom transistor was invented and first demonstrated in 2004 by Prof. Thomas Schimmel and his team of scientists at the Karlsruhe Institute of Technology (former University of Karlsruhe). By means of a small electrical voltage applied to a control electrode, the so-called \"gate electrode\", a single silver atom is reversibly moved in and out of a tiny junction, in this way closing and opening an electrical contact.\n\nTherefore, the single-atom transistor works as an atomic switch or atomic relay, where the switchable atom opens and closes the gap between two tiny electrodes called \"source\" and \"drain\". The single-atom transistor opens perspectives for the development of future atomic-scale logics and quantum electronics.\n\nAt the same time, the device of the Karlsruhe team of researchers marks the lower limit of miniaturization, as feature sizes smaller than one atom cannot be produced lithographically. The device represents a quantum transistor, the conductance of the source-drain channel being defined by the rules of quantum mechanics. It can be operated at room temperature and at ambient conditions, i.e. neither cooling nor vacuum are required.\n\nFew atom transistors have been developed at Waseda University and at Italian CNR by Takahiro Shinada and Enrico Prati, who observed the Anderson-Mott transition in miniature by employing arrays of only two, four and six individually implanted As or P atoms.\n\n"}
{"id": "11242767", "url": "https://en.wikipedia.org/wiki?curid=11242767", "title": "Sludge bulking", "text": "Sludge bulking\n\nIn treatment of sewage one process used is the activated sludge process in which air is passed through a mixture of sewage and old sludge to allow the micro-organisms to break down the organic components of the sewage. Sludge is continually drawn off as new sewage enters the tank and this sludge must then be settled so that the supernatant (the remaining liquid) can be separated to pass on to further stages of treatment.\n\nSludge bulking occurs when the sludge fails to separate out in the sedimentation tanks. The main cause of sludge bulking is the growth of filamentous bacteria.\n\nFilamentous microorganisms grow in long strands that have much greater volume and surface area than conventional floc and are very slow to settle. Under certain growing conditions, filamentous organisms predominate. There is little robust scientific evidence that can be used to avoid sludge bulking but what there is indicates that over-loading works, having a carbohydrate rich input and having too low a recycle rate may all contribute.\n\nTo avoid sludge bulking some of the flow that enters the reactor can be bypassed, recycle ratio can be increased, lime or soda can be added to the reactor or the re-aeration rate increased.\n\n\n"}
{"id": "43557875", "url": "https://en.wikipedia.org/wiki?curid=43557875", "title": "Snapback (electrical)", "text": "Snapback (electrical)\n\nSnapback is a mechanism in a bipolar transistor in which avalanche breakdown or impact ionization provides a sufficient base current to turn on the transistor. It is used intentionally in the design of certain ESD protection devices integrated onto semiconductor chips. It can also be a parasitic failure mechanism when activated inadvertently, outwardly appearing much like latchup in that the chip seems to suddenly blow up when a high voltage is applied.\n\nSnapback is initiated by a small current from collector to base. In the case of ESD protection devices, this current is caused by avalanche breakdown due to a sufficiently large voltage applied across the collector-base junction. In the case of parasitic failures, the initiating current may result from inadvertently turning on the bipolar transistor and a sufficiently large voltage across the collector and base causing impact ionization, with some of the generated carriers then acting as the initiating current as they flow into the base. Once this initiating current flows into the base, the transistor turns on and the collector voltage decreases to the snapback holding voltage. This voltage happens at the point where the processes of base current generation and the bipolar transistor turning on are in balance: the collector-emitter current of the bipolar transistor decreases the collector voltage, which results in a lower electric field, which results in a smaller impact ionization or avalanche current and thus smaller base current, which weakens the bipolar action.\n\n"}
{"id": "21330822", "url": "https://en.wikipedia.org/wiki?curid=21330822", "title": "South Malawi montane forest-grassland mosaic", "text": "South Malawi montane forest-grassland mosaic\n\nThe South Malawi montane forest-grassland mosaic is an ecoregion of Malawi. \n\nThe ecoregion covers a region of highlands and plateaus that includes Mount Mulanje in the southeast and the lower Shire Highlands to the northeast and east, with the Tuchila Plain between them. The Shire Highlands includes the Zomba Plateau in the north and the Thyolo Mountains (1,462 m) to the south. \n\nThe low valley of the Shire River, part of the African Rift Valley system, bounds the Shire Highlands to the northwest, west, and southwest. The valley of the Ruo River, a tributary of the Shire, bounds the ecoregion on the south and southeast. The Phalombe Plain and the valley of Lake Chilwa lie to the northeast.\n\nThe ecoregion enjoyed lush forests, fertile soils, ample water, and mild climate, and is one of the most densely populated regions of Malawi. Blantyre, Malawi's second-largest city, is in the Shire Highlands. Tea is grown commercially in the highlands, especially around Thyolo and Mulanje, and is one of Malawi's chief exports.\n\nProtected areas in the ecoregion include:\n"}
{"id": "36408315", "url": "https://en.wikipedia.org/wiki?curid=36408315", "title": "Tondidarou", "text": "Tondidarou\n\nTondidarou is a small town and megalithic archaeological site in Niafunké Cercle, Timbuktu Region, Mali, northwest of Niafunké, about 150 kilometres south-west of Timbuktu. The site, located on the eastern bank of Lac Tagadji, was discovered by Jules Brévié in 1904 and is said to be \"defined by three groups of stone megaliths\", monoliths which are a \"remarkable collection of phalliform stone monuments.\" \"Ancient Egypt in Africa\" refers to the site as \"Diop's 'Egypt-influenced' phalliform stone circle of Tondidarou\". Eugene Maes was the first to seriously document the stones at Tondidarou in 1924. It was extensively excavated in around 1980. The site is dated to 670 - 790 AD.\n\n"}
{"id": "30651486", "url": "https://en.wikipedia.org/wiki?curid=30651486", "title": "Uranium Mill Tailings Remedial Action", "text": "Uranium Mill Tailings Remedial Action\n\nThe Uranium Mill Tailings Remedial Action (UMTRA) Project was created by the United States Department of Energy (DOE) to monitor the cleanup of uranium mill tailings. In 1978 the US Congress passed the Uranium Mill Tailings Radiation Control Act (UMTRCA) which tasked the DOE with the responsibility of stabilizing, disposing, and controlling uranium mill tailings and other contaminated material at uranium mill processing spread across 10 states and at approximately 5,200 associated properties. Under UMTRCA, the DOE created UMTRA to decommission 24 uranium mills and dispose of their residual mill tailings.\n"}
{"id": "26098970", "url": "https://en.wikipedia.org/wiki?curid=26098970", "title": "Uranium zirconium hydride", "text": "Uranium zirconium hydride\n\nThis compound, quite different from uranium hydride, is used as the fuel in the TRIGA reactor. Uranium zirconium hydride is used in most research reactors at universities.\n\nTRIGA\n"}
{"id": "44578263", "url": "https://en.wikipedia.org/wiki?curid=44578263", "title": "Void (composites)", "text": "Void (composites)\n\nA void is a pore that remains unoccupied in a composite material. A void is typically the result of an imperfection from the processing of the material and is generally deemed undesirable. Because a void is a non-uniformity in a composite material, it can affect the mechanical properties and lifespan of the composite. Voids can act as a crack nucleation site as well as allow moisture to penetrate the composite and contribute to anisotropy of the composite . This is an issue because crack formation and propagation can create unpredictable behavior in the laminate. For aerospace applications, a void content of approximately 1% is appropriate for performance while other grades of composites can have between 3%-5% void content. A small change in the percentage of void content may not seem like a large problem however for a loaded carbon fiber laminate composite, a 1%-3% increase in void content can reduce the mechanical properties of the composite by up to 20% Void content in composites is represented as a ratio, also called void ratio, where the volume of voids, solid material, and bulk volume are taken into account. Void ratio can be calculated by the formula below where e is the void ratio of the composite, V is the volume of the voids, and V is the volume of the bulk material.\n\nVoids are considered defects in composite structures and there are several types of voids that can form in composites depending on the fabrication route and matrix type. Among other factors that can influence the quantity and location of voids are pre-preg impregnation, surface morphology, curing parameters, compaction pressure, fiber bridging, excessive resin bleed, and the thickness of layup .\n\nA resin with a high viscosity will likely produce voids in a composite. It is difficult for a resin or matrix with a high viscosity to penetrate the original void spaces between adjacent fibers. This will cause voids to form close the fiber surface. Preventing these voids becomes a more daunting task when the fibers are packed tightly together in a composite \n\nA high void proportion can be obtained in a composite due to errors in processing as well. If the temperature used for curing is too low for the particular matrix used, complete degassing might not occur. However, if the temperature used for curing is too high for a particular matrix, gelation might occur too rapidly and voids may still be present . For example, if a laminate composite is cured at a temperature that is too low for the particular matrix used, the resin viscosity could remain high and hinder removing the void spaces between individual plies Some resins can cure at room temperature while other resins require temperatures up to 200 °C, but curing above or below the required temperature for a particular matrix can increase the amount of voids present in a composite. If the injection pressure in a resin injection pultrusion process is not high enough, the resin or matrix might not be able to penetrate the fiber bed to completely wet out the fibers without voids. Entrapped air or bubbles can be formed in the resin during resin mixing. If these bubbles are not removed before the wetting of the fibers or curing of the composite, the bubbles could become voids that can be found throughout the final composite structure.\n\nBecause voids are viewed as defects in composite materials, many methods are applied for reducing voids in composites. Traditionally, using vacuum bagging system and autoclave under pressure and heat will minimize or prevent voids from forming. \nThe vacuum bagging system combined with autoclave is a common method used in industrial processes to achieve a low void content for thermoset composites. Vacuum evacuation is the way reducing exciting amount of voids by physically transporting the voids out of the resin and fiber network through vacuum lines, and it is influenced by the viscosity of resin. Autoclave pressure is used to assist vacuum in removing trapped air and excess resin while at the same time preventing volatiles from coming out of the resin at high temperatures \nOptimization of injection flow rate is often calculated to minimize voids in Resin Transfer Molded (RTM) composites. During the injection phase, a liquid resin impregnates the fibers before curing and solidification, often creating voids in the part during the injection. Through an algorithm between fluid flow velocity (v) and the percentages of macro-voids (V) and micro-voids (V)\n\nan optimized rate can be obtained and the voids in RTM composites can be reduced, thus improving properties of the composite\n"}
{"id": "1624490", "url": "https://en.wikipedia.org/wiki?curid=1624490", "title": "Voltage doubler", "text": "Voltage doubler\n\nA voltage doubler is an electronic circuit which charges capacitors from the input voltage and switches these charges in such a way that, in the ideal case, exactly twice the voltage is produced at the output as at its input.\n\nThe simplest of these circuits are a form of rectifier which take an AC voltage as input and outputs a doubled DC voltage. The switching elements are simple diodes and they are driven to switch state merely by the alternating voltage of the input. DC-to-DC voltage doublers cannot switch in this way and require a driving circuit to control the switching. They frequently also require a switching element that can be controlled directly, such as a transistor, rather than relying on the voltage across the switch as in the simple AC-to-DC case.\n\nVoltage doublers are a variety of voltage multiplier circuit. Many, but not all, voltage doubler circuits can be viewed as a single stage of a higher order multiplier: cascading identical stages together achieves a greater voltage multiplication.\n\nThe Villard circuit, due to Paul Ulrich Villard, consists simply of a capacitor and a diode. While it has the great benefit of simplicity, its output has very poor ripple characteristics. Essentially, the circuit is a diode clamp circuit. The capacitor is charged on the negative half cycles to the peak AC voltage (\"V\"). The output is the superposition of the input AC waveform and the steady DC of the capacitor. The effect of the circuit is to shift the DC value of the waveform. The negative peaks of the AC waveform are \"clamped\" to 0 V (actually −\"V\", the small forward bias voltage of the diode) by the diode, therefore the positive peaks of the output waveform are 2\"V\". The peak-to-peak ripple is an enormous 2\"V\" and cannot be smoothed unless the circuit is effectively turned into one of the more sophisticated forms. This is the circuit (with diode reversed) used to\nsupply the negative high voltage for the magnetron in a microwave oven.\n\nThe Greinacher voltage doubler is a significant improvement over the Villard circuit for a small cost in additional components. The ripple is much reduced, nominally zero under open-circuit load conditions, but when current is being drawn depends on the resistance of the load and the value of the capacitors used. The circuit works by following a Villard cell stage with what is in essence a peak detector or envelope detector stage. The peak detector cell has the effect of removing most of the ripple while preserving the peak voltage at the output. The Greinacher circuit is also commonly known as the half-wave voltage doubler.\n\nThis circuit was first invented by Heinrich Greinacher in 1913 (published 1914) to provide the 200–300 V he needed for his newly invented ionometer, the 110 V AC supplied by the Zurich power stations of the time being insufficient. He later extended this idea into a cascade of multipliers in 1920. This cascade of Greinacher cells is often inaccurately referred to as a Villard cascade. It is also called a Cockcroft–Walton multiplier after the particle accelerator machine built by John Cockcroft and Ernest Walton, who independently discovered the circuit in 1932. \nThe concept in this topology can be extended to a voltage quadrupler circuit by using two Greinacher cells of opposite polarities driven from the same AC source. The output is taken across the two individual outputs. As with a bridge circuit, it is impossible to simultaneously ground the input and output of this circuit.\n\nThe Delon circuit uses a bridge topology for voltage doubling; consequently it is also called a full-wave voltage doubler. This form of circuit was, at one time, commonly found in cathode ray tube television sets where it was used to provide an Extra high tension (EHT) supply. Generating voltages in excess of 5 kV with a transformer has safety issues in terms of domestic equipment and in any case is uneconomical. However, black and white television sets required an e.h.t. of 10 kV and colour sets even more. Voltage doublers were used to either double the voltage on an e.h.t winding on the mains transformer or were applied to the waveform on the line flyback coils.\n\nThe circuit consists of two half-wave peak detectors, functioning in exactly the same way as the peak detector cell in the Greinacher circuit. Each of the two peak detector cells operates on opposite half-cycles of the incoming waveform. Since their outputs are in series, the output is twice the peak input voltage.\n\nIt is possible to use the simple diode-capacitor circuits described above to double the voltage of a DC source by preceding the voltage doubler with a chopper circuit. In effect, this converts the DC to AC before application to the voltage doubler. More efficient circuits can be built by driving the switching devices from an external clock so that both functions, the chopping and multiplying, are achieved simultaneously. Such circuits are known as switched capacitor circuits. This approach is especially useful in low-voltage battery-powered applications where integrated circuits require a voltage supply greater than the battery can deliver. Frequently, a clock signal is readily available on board the integrated circuit and little or no additional circuitry is needed to generate it.\n\nConceptually, perhaps the simplest switched capacitor configuration is that shown schematically in figure 5. Here two capacitors are simultaneously charged to the same voltage in parallel. The supply is then switched off and the capacitors are switched into series. The output is taken from across the two capacitors in series resulting in an output double the supply voltage. There are many different switching devices that could be used in such a circuit, but in integrated circuits MOSFET devices are frequently employed.\n\nAnother basic concept is the charge pump, a version of which is shown schematically in figure 6. The charge pump capacitor, C, is first charged to the input voltage. It is then switched to charging the output capacitor, C, in series with the input voltage resulting in C eventually being charged to twice the input voltage. It may take several cycles before the charge pump succeeds in fully charging C but after steady state has been reached it is only necessary for C to pump a small amount of charge equivalent to that being supplied to the load from C. While C is disconnected from the charge pump it partially discharges into the load resulting in ripple on the output voltage. This ripple is smaller for higher clock frequencies since the discharge time is shorter, and is also easier to filter. Alternatively, the capacitors can be made smaller for a given ripple specification. The practical maximum clock frequency in integrated circuits is typically in the hundreds of kilohertz.\n\nThe Dickson charge pump, or Dickson multiplier, consists of a cascade of diode/capacitor cells with the bottom plate of each capacitor driven by a clock pulse train. The circuit is a modification of the Cockcroft-Walton multiplier but takes a DC input with the clock trains providing the switching signal instead of the AC input. The Dickson multiplier normally requires that alternate cells are driven from clock pulses of opposite phase. However, since a voltage doubler, shown in figure 7, requires only one stage of multiplication only one clock signal is required.\n\nThe Dickson multiplier is frequently employed in integrated circuits where the supply voltage (from a battery for instance) is lower than that required by the circuitry. It is advantageous in integrated circuit manufacture that all the semiconductor components are of basically the same type. MOSFETs are commonly the standard logic block in many integrated circuits. For this reason the diodes are often replaced by this type of transistor, but wired to function as a diode - an arrangement called a diode-wired MOSFET. Figure 8 shows a Dickson voltage doubler using diode-wired n-channel enhancement type MOSFETs.\n\nThere are many variations and improvements to the basic Dickson charge pump. Many of these are concerned with reducing the effect of the transistor drain-source voltage. This can be very significant if the input voltage is small, such as a low-voltage battery. With ideal switching elements the output is an integral multiple of the input (two for a doubler) but with a single-cell battery as the input source and MOSFET switches the output will be far less than this value since much of the voltage will be dropped across the transistors. For a circuit using discrete components the Schottky diode would be a better choice of switching element for its extremely low voltage drop in the on state. However, integrated circuit designers prefer to use the easily available MOSFET and compensate for its inadequacies with increased circuit complexity.\n\nAs an example, an alkaline battery cell has a nominal voltage of . A voltage doubler using ideal switching elements with zero voltage drop will output double this, namely . However, the drain-source voltage drop of a diode-wired MOSFET when it is in the on state must be at least the gate threshold voltage which might typically be . This voltage \"doubler\" will only succeed in raising the output voltage by about to . If the drop across the final smoothing transistor is also taken into account the circuit may not be able to increase the voltage at all without using multiple stages. A typical Schottky diode, on the other hand, might have an on state voltage of . A doubler using this Schottky diode will result in a voltage of , or at the output after the smoothing diode, .\n\nCross-coupled switched capacitor circuits come into their own for very low input voltages. Wireless battery driven equipment such as pagers, bluetooth devices and the like may require a single-cell battery to continue to supply power when it has discharged to under a volt.\n\nWhen clock formula_1 is low transistor Q is turned off. At the same time clock formula_2 is high turning on transistor Q resulting in capacitor C being charged to \"V\". When formula_1 goes high the top plate of C is pushed up to twice \"V\". At the same time switch S closes so this voltage appears at the output. At the same time Q is turned on allowing C to charge. On the next half cycle the roles will be reversed: formula_1 will be low, formula_2 will be high, S will open and S will close. Thus, the output is supplied with 2\"V\" alternately from each side of the circuit.\n\nThe loss is low in this circuit because there are no diode-wired MOSFETs and their associated threshold voltage problems. The circuit also has the advantage that the ripple frequency is doubled because there are effectively two voltage doublers both supplying the output from out of phase clocks. The primary disadvantage of this circuit is that stray capacitances are much more significant than with the Dickson multiplier and account for the larger part of the losses in this circuit.\n\n\n"}
