{"id": "26587554", "url": "https://en.wikipedia.org/wiki?curid=26587554", "title": "Activator (phosphor)", "text": "Activator (phosphor)\n\nIn phosphors and scintillators, the activator is the element added as dopant to the crystal of the material to create desired type of nonhomogeneities.\n\nIn luminescence, only a small fraction of atoms, called emission centers or luminescence centers, emit light. In inorganic phosphors, these inhomogeneities in the crystal structure are created usually by addition of a trace amount of dopants, impurities called activators. (In rare cases dislocations or other crystal defects can play the role of the impurity.) The wavelength emitted by the emission center is dependent on the atom itself, its electronic configuration, and on the surrounding crystal structure.\n\nThe activators prolong the emission time (afterglow). In turn, other materials (such as nickel) can be used to quench the afterglow and shorten the decay part of the phosphor emission characteristics.\n\nThe electronic configuration of the activator depends on its oxidation state and is crucial for the light emission. Oxidation of the activator is one of the common mechanisms of phosphor degradation. The distribution of the activator in the crystal is also of high importance. Diffusion of the ions can cause depletion of the crystal from the activators with resulting loss of efficiency. This is another mechanism of phosphor degradation.\n\nThe scintillation process in inorganic materials is due to the electronic band structure found in the crystals. An incoming particle can excite an electron from the valence band to either the conduction band or the exciton band (located just below the conduction band and separated from the valence band by an energy gap). This leaves an associated hole behind, in the valence band. Impurities create electronic levels in the forbidden gap. The excitons are loosely bound electron-hole pairs which wander through the crystal lattice until they are captured as a whole by impurity centers. The latter then rapidly de-excite by emitting scintillation light (fast component). In case of inorganic scintillators, the activator impurities are typically chosen so that the emitted light is in the visible range or near-UV where photomultipliers are effective. The holes associated with electrons in the conduction band are independent from the latter. Those holes and electrons are captured successively by impurity centers exciting certain metastable states not accessible to the excitons. The delayed de-excitation of those metastable impurity states, slowed down by reliance on the low-probability forbidden mechanism, again results in light emission (slow component).\n\nThe activator is the main factor determining the phosphor emission wavelength. The nature of the host crystal can however to some degree influence the wavelength as well.\n\nMore activators can be used simultaneously.\n\nCommon examples of activators are:\n\nA newly discovered activator is Samarium(II), added to calcium fluoride. Sm(II) is one of the few materials reported which offers efficient scintillation in the red region of the spectrum, particularly when cooled by dry ice.\n"}
{"id": "27898078", "url": "https://en.wikipedia.org/wiki?curid=27898078", "title": "Administrația Națională de Meteorologie", "text": "Administrația Națională de Meteorologie\n\nAdministraţia Naţională de Meteorologie or ANM is the Romania government facility of weather prediction. The organisation's headquarters are in Bucharest. The organisation was found in late 18th century. In 1948 Romania ratifies the 1947 Washington Convention, turning from founding member as a full member of the World Meteorological Organization (18 August 1948) . Since 2003 is a member of EUMETSAT with a stake of 0.4456%. Today in Romania, ANM holds the monopoly for meteorological prediction because the state doesn't allow a private carrier in this field. Even if ANM holds monopoly on Romanian market, international media holdings and websites are usually using satellite prediction and the predictions of other meteorological institutions of Romania's neighborough. It is operated by Romanian Ministry of Environment.\n\nhttp://www.meteoromania.ro/index.php?id=35\nhttps://web.archive.org/web/20111010091557/http://www.wmo.int/pages/members/membership/index_en.html\n"}
{"id": "69079", "url": "https://en.wikipedia.org/wiki?curid=69079", "title": "Ammonium", "text": "Ammonium\n\nThe ammonium cation is a positively charged polyatomic ion with the chemical formula . It is formed by the protonation of ammonia (NH). Ammonium is also a general name for positively charged or protonated substituted amines and quaternary ammonium cations (), where one or more hydrogen atoms are replaced by organic groups (indicated by R).\n\nThe ammonium ion is generated when ammonia, a weak base, reacts with Brønsted acids (proton donors):\n\nThe ammonium ion is mildly acidic, reacting with Brønsted bases to return to the uncharged ammonia molecule:\nThus, treatment of concentrated solutions of ammonium salts with strong base gives ammonia. When ammonia is dissolved in water, a tiny amount of it converts to ammonium ions:\n\nThe degree to which ammonia forms the ammonium ion depends on the pH of the solution. If the pH is low, the equilibrium shifts to the right: more ammonia molecules are converted into ammonium ions. If the pH is high (the concentration of hydrogen ions is low), the equilibrium shifts to the left: the hydroxide ion abstracts a proton from the ammonium ion, generating ammonia.\n\nFormation of ammonium compounds can also occur in the vapor phase; for example, when ammonia vapor comes in contact with hydrogen chloride vapor, a white cloud of ammonium chloride forms, which eventually settles out as a solid in a thin white layer on surfaces.\n\nAmmonium cation is found in a variety of salts such as ammonium carbonate, ammonium chloride and ammonium nitrate. Most simple ammonium salts are very soluble in water. An exception is ammonium hexachloroplatinate, the formation of which was once used as a test for ammonium. The ammonium salts of nitrate and especially perchlorate are highly explosive, in these cases ammonium is the reducing agent.\n\nIn an unusual process, ammonium ions form an amalgam. Such species are prepared by the electrolysis of an ammonium solution using a mercury cathode. This amalgam eventually decomposes to release ammonia and hydrogen.\n\nThe lone electron pair on the nitrogen atom (N) in ammonia, represented as a line above the N, forms the bond with a proton (H). Thereafter, all four N–H bonds are equivalent, being polar covalent bonds. The ion has a tetrahedral structure and is isoelectronic with methane and borohydride. In terms of size, the ammonium cation (\"r\" = 175 pm) resembles the cesium cation (\"r\" = 183 pm).\n\nThe hydrogen atoms in the ammonium ion can be substituted with an alkyl group or some other organic group to form a substituted ammonium ion (IUPAC nomenclature: aminium ion). Depending on the number of organic groups, the ammonium cation is called a \"primary\", \"secondary\", \"tertiary\", or \"quaternary\". With the exception of the quaternary ammonium cations, the organic ammonium cations are weak acids.\n\nAn example of a reaction forming an ammonium ion is that between dimethylamine, (CH)NH, and an acid to give the dimethylaminium cation, (CH)NH:\n\nQuaternary ammonium cations have four organic groups attached to the nitrogen atom, they lack a hydrogen atom bonded to the nitrogen atom. These cations, such as the tetra-\"n\"-butylammonium cation, are sometimes used to replace sodium or potassium ions to increase the solubility of the associated anion in organic solvents. Primary, secondary, and tertiary ammonium salts serve the same function, but are less lipophilic. They are also used as phase-transfer catalysts and surfactants.\n\nAn unusual class of organic ammonium salts are derivatives of amine radical cations, RN such as tris(4-bromophenyl)ammonium hexachloroantimonate.\n\nAmmonium ions are a waste product of the metabolism of animals. In fish and aquatic invertebrates, it is excreted directly into the water. In mammals, sharks, and amphibians, it is converted in the urea cycle to urea, because urea is less toxic and can be stored more efficiently. In birds, reptiles, and terrestrial snails, metabolic ammonium is converted into uric acid, which is solid and can therefore be excreted with minimal water loss.\n\nAmmonium is an important source of nitrogen for many plant species, especially those growing on hypoxic soils. However, it is also toxic to most crop species and is rarely applied as a sole nitrogen source.\n\nThe ammonium ion has very similar properties to the heavier alkali metals and is often considered a close relative. Ammonium is expected to behave as a metal (NH ions in a sea of electrons) at very high pressures, such as inside gas giant planets such as Uranus and Neptune.\n\nUnder normal conditions, ammonium does not exist as a pure metal, but does as an amalgam (alloy with mercury).\n"}
{"id": "40983263", "url": "https://en.wikipedia.org/wiki?curid=40983263", "title": "Andre Finkelstein", "text": "Andre Finkelstein\n\nAndré Finkelstein (born 1923) was deputy director general of the IAEA from 1969 to 1973 and a high-ranking official in the Commissariat à l’Énergie Atomique (CEA), the French government's oversight body civilian and military nuclear activities.\n\nAs a graduate student, Finkelstein studied physical chemistry, spending two years at the University of Rochester and joined the CEA in 1953 after earning his Ph.D. He originally studied tritium, a radioactive isotope of hydrogen, in a CEA lab. When the CEA expanded in 1958, Finkelstein became a senior officer in the department of external relations and programs. While serving in this role, Finkelstein participated in conferences in Geneva on the peaceful use of nuclear energy and served as advisor and alternate to the French representative to the IAEA. In 1967, he was promoted to deputy director in the cabinet of the high commissioner. He also worked in collaboration with the European Nuclear Energy Agency (ENEA).\nIn 1969, Finkelstein became deputy director general in charge of research and isotopes at the IAEA, where he held until 1973. In 1974, Finkelstein returned to the CEA to become deputy commissioner of the mission for nuclear safety and protection. The CEA promoted him to secretary of the central commission for nuclear installation safety (commission centrale de sûreté des installations atomiques, CSSIA) within the same year. Finklestein later served as coordinator of archives and history at the CEA from 1983 until his retirement on July 1, 1988.\n\nCohen, Avner. \"The Avner Cohen Collection.\" André Finkelstein. NPIHP, The Woodrow Wilson Center for International Scholars, 03 Oct. 2013. Web. 05 Nov. 2013. <http://www.wilsoncenter.org/andré-finkelstein>.\n\n"}
{"id": "28417715", "url": "https://en.wikipedia.org/wiki?curid=28417715", "title": "Arnedo Solar Plant", "text": "Arnedo Solar Plant\n\nThe Arnedo Solar Plant is a solar photovoltaic power plant located in Arnedo, La Rioja, Spain. The power system installations are supplied and the solar plant is operated by T-Solar. The plant was built by Isolux Corsán in 2008. It cost €181 million.\n\nThe Arnedo Solar Plant has a capacity of 34 MW which is provided by 172,000 modules 200 W each. The plant is located on . It produces approximately annually, equivalent to the power consumption of 11,451 households.\n\n"}
{"id": "51952557", "url": "https://en.wikipedia.org/wiki?curid=51952557", "title": "Attract-kill pattern", "text": "Attract-kill pattern\n\nAn attract-kill pattern is an interaction between plant roots and pathogen, which plays an important role in suppression of \"Phytophthora\" disease in intercropping systems. The recent research from Key Laboratory of Agro-Biodiversity and Pest Management of Education Ministry of China indicated that maize roots attracted the zoospores of \"Phytophthora capsici\" and inhibited their motility. Then a large amount of cystospores closed to maize roots were lysed. The phenomenon has been widely found in various interactions between roots of non-host plant and \"Phytophthora\".\n\n"}
{"id": "35487742", "url": "https://en.wikipedia.org/wiki?curid=35487742", "title": "Bali–Java Powerline", "text": "Bali–Java Powerline\n\nThe Bali–Java Powerline is a planned electric power transmission line in Indonesia. It will be used for the crossing of the Bali Strait between Java and Bali. If built, the towers will be the world's tallest electricity pylons with a height of . As of January 2018, the project is underway with planning for land acquisition. However, as of April 2018, there are also indications that the project may never come to fruition as there is substantial resistance to the project for religious and cultural reasons.\n\nThe line on the Java side will begin in at the Paiton power plant, then through Watu Dodol in Banyuwangi Regency, and on to Bali. On the Bali side, it would be in parts of western Buleleng, Jembrana, Tabanan and Badung regencies.\n\nThe two towers that will carry the line over the Bali Strait (Selatbali) have planned positions as follows: for the on Java and for the on the Bali side.\n"}
{"id": "14600777", "url": "https://en.wikipedia.org/wiki?curid=14600777", "title": "Beneixama photovoltaic power plant", "text": "Beneixama photovoltaic power plant\n\nBeneixama photovoltaic power plant is a 20 MW photovoltaic power plant located in Beneixama, Spain. The plant consists of approximately 100,000 solar panels, encompassing an area of approximately 500,000 m. The panels are City Solar PQ 200 modules made of polycrystalline silicon solar cells.\nIn addition, 200 units of Siemens photovoltaic inverters \"Sinvert Solar 100 Master\" were installed.\n\nThe plant was built by City Solar, and completed in September, 2007.\n\n\n"}
{"id": "21415321", "url": "https://en.wikipedia.org/wiki?curid=21415321", "title": "Braes of Doune Wind Farm", "text": "Braes of Doune Wind Farm\n\nBraes of Doune Wind Farm is a wind farm located close to Stirling, Scotland and opened in 2007.\n\nThe farm was built by Alfred McAlpine in 2007 and handed over to Airtricity to operate. An agreement was reached with Centrica, the owners of Scottish Gas, to purchase energy output from the farm. The Special Purpose Company became owned jointly by Greencoat UK Wind and by a fund managed by Hermes GPE LLP after Greencoat UK Wind acquired its 50 per cent interest from SSE.\n\nThe farm has 36 Vestas V80 2.0 megawatt wind turbines with a total capacity of 72MW.\n\n\n"}
{"id": "11127840", "url": "https://en.wikipedia.org/wiki?curid=11127840", "title": "Clonostachys rosea f. rosea", "text": "Clonostachys rosea f. rosea\n\nClonostachys rosea f. rosea, also known as Gliocladium roseum, is a species of fungus in the family Bionectriaceae. It colonizes living plants as an endophyte, digests material in soil as a saprophyte and is also known as a parasite of other fungi and of nematodes. It produces a wide range of volatile organic compounds which are toxic to organisms including other fungi, bacteria, and insects, and is of interest as a biological pest control agent.\n\n\"Clonostachys rosea\" protects plants against \"Botrytis cinerea\" (\"grey mold\") by suppressing spore production. Its hyphae have been found to coil around, penetrate, and grow inside the hyphae and conidia of \"B. cinerea\".\n\nNematodes are infected by \"C. rosea\" when the fungus' conidia attach to their cuticle and germinate, going on to produce germ tubes which penetrate the host's body and kill it.\n\nIn 2008 an isolate of \"Clonostachys rosea\" (NRRL 50072) was identified as producing a series of volatile compounds that are similar to some existing fuels. However, the taxonomy of this isolate was later revised to \"Ascocoryne sarcoides\".\n\n\n"}
{"id": "216794", "url": "https://en.wikipedia.org/wiki?curid=216794", "title": "Commutator (electric)", "text": "Commutator (electric)\n\nA commutator is a rotary electrical switch in certain types of electric motors and electrical generators that periodically reverses the current direction between the rotor and the external circuit. It consists of a cylinder composed of multiple metal contact segments on the rotating armature of the machine. Two or more electrical contacts called \"brushes\" made of a soft conductive material like carbon press against the commutator, making sliding contact with successive segments of the commutator as it rotates. The windings (coils of wire) on the armature are connected to the commutator segments.\n\nCommutators are used in direct current (DC) machines: dynamos (DC generators) and many DC motors as well as universal motors. In a motor the commutator applies electric current to the windings. By reversing the current direction in the rotating windings each half turn, a steady rotating force (torque) is produced. In a generator the commutator picks off the current generated in the windings, reversing the direction of the current with each half turn, serving as a mechanical rectifier to convert the alternating current from the windings to unidirectional direct current in the external load circuit. The first direct current commutator-type machine, the dynamo, was built by Hippolyte Pixii in 1832, based on a suggestion by André-Marie Ampère.\n\nCommutators are relatively inefficient, and also require periodic maintenance such as brush replacement. Therefore, commutated machines are declining in use, being replaced by alternating current (AC) machines, and in recent years by brushless DC motors which use semiconductor switches. \n\nA commutator consists of a set of contact bars fixed to the rotating shaft of a machine, and connected to the armature windings. As the shaft rotates, the commutator reverses the flow of current in a winding. For a single armature winding, when the shaft has made one-half complete turn, the winding is now connected so that current flows through it in the opposite of the initial direction. In a motor, the armature current causes the fixed magnetic field to exert a rotational force, or a torque, on the winding to make it turn. In a generator, the mechanical torque applied to the shaft maintains the motion of the armature winding through the stationary magnetic field, inducing a current in the winding. In both the motor and generator case, the commutator periodically reverses the direction of current flow through the winding so that current flow in the circuit external to the machine continues in only one direction.\n\nPractical commutators have at least three contact segments, to prevent a \"dead\" spot where two brushes simultaneously bridge only two commutator segments. Brushes are made wider than the insulated gap, to ensure that brushes are always in contact with an armature coil. For commutators with at least three segments, although the rotor can potentially stop in a position where two commutator segments touch one brush, this only de-energizes one of the rotor arms while the others will still function correctly. With the remaining rotor arms, a motor can produce sufficient torque to begin spinning the rotor, and a generator can provide useful power to an external circuit.\n\nA commutator consists of a set of copper segments, fixed around the part of the circumference of the rotating machine, or the rotor, and a set of spring-loaded brushes fixed to the stationary frame of the machine. Two or more fixed brushes connect to the external circuit, either a source of current for a motor or a load for a generator.\n\nCommutator segments are connected to the coils of the armature, with the number of coils (and commutator segments) depending on the speed and voltage of the machine. Large motors may have hundreds of segments.\nEach conducting segment of the commutator is insulated from adjacent segments. Mica was used on early machines and is still used on large machines. Many other insulating materials are used to insulate smaller machines; plastics allow quick manufacture of an insulator, for example. The segments are held onto the shaft using a dovetail shape on the edges or underside of each segment. Insulating wedges around the perimeter of each segment are pressed so that the commutator maintains its mechanical stability throughout its normal operating range.\n\nIn small appliance and tool motors the segments are typically crimped permanently in place and cannot be removed. When the motor fails it is discarded and replaced. On large industrial machines (say, from several kilowatts to thousands of kilowatts in rating) it is economical to replace individual damaged segments, and so the end-wedge can be unscrewed and individual segments removed and replaced. Replacing the copper and mica segments is commonly referred to as \"refilling\". Refillable dovetailed commutators are the most common construction of larger industrial type commutators, but refillable commutators may also be constructed using external bands made of fiberglass (glass banded construction) or forged steel rings (external steel shrink ring type construction and internal steel shrink ring type construction). Disposable, molded type commutators commonly found in smaller DC motors are becoming increasingly more common in larger electric motors. Molded type commutators are not repairable and must be replaced if damaged. In addition to the commonly used heat, torque, and tonnage methods of seasoning commutators, some high performance commutator applications require a more expensive, specific \"spin seasoning\" process or over-speed spin-testing to guarantee stability of the individual segments and prevent premature wear of the carbon brushes. Such requirements are common with traction, military, aerospace, nuclear, mining, and high speed applications where premature failure can lead to serious negative consequences.\n\nFriction between the segments and the brushes eventually causes wear to both surfaces. Carbon brushes, being made of a softer material, wear faster and may be designed to be replaced easily without dismantling the machine. Older copper brushes caused more wear to the commutator, causing deep grooving and notching of the surface over time. The commutator on small motors (say, less than a kilowatt rating) is not designed to be repaired through the life of the device. On large industrial equipment, the commutator may be re-surfaced with abrasives, or the rotor may be removed from the frame, mounted in a large metal lathe, and the commutator resurfaced by cutting it down to a smaller diameter. The largest of equipment can include a lathe turning attachment directly over the commutator.\n\nEarly machines used brushes made from strands of copper wire to contact the surface of the commutator. However, these hard metal brushes tended to scratch and groove the smooth commutator segments, eventually requiring resurfacing of the commutator. As the copper brushes wore away, the dust and pieces of the brush could wedge between commutator segments, shorting them and reducing the efficiency of the device. Fine copper wire mesh or gauze provided better surface contact with less segment wear, but gauze brushes were more expensive than strip or wire copper brushes.\n\nModern rotating machines with commutators almost exclusively use carbon brushes, which may have copper powder mixed in to improve conductivity. Metallic copper brushes can be found in toy or very small motors, such as the one illustrated above, and some motors which only operate very intermittently, such as automotive starter motors.\n\nMotors and generators suffer from a phenomenon known as 'armature reaction', one of the effects of which is to change the position at which the current reversal through the windings should ideally take place as the loading varies. Early machines had the brushes mounted on a ring that was provided with a handle. During operation, it was necessary to adjust the position of the brush ring to adjust the commutation to minimise the sparking at the brushes. This process was known as 'rocking the brushes'.\n\nVarious developments took place to automate the process of adjusting the commutation and minimizing the sparking at the brushes. One of these was the development of 'high resistance brushes', or brushes made from a mixture of copper powder and carbon. Although described as high resistance brushes, the resistance of such a brush was of the order of milliohms, the exact value dependent on the size and function of the machine. Also, the high resistance brush was not constructed like a brush but in the form of a carbon block with a curved face to match the shape of the commutator.\n\nThe high resistance or carbon brush is made large enough that it is significantly wider than the insulating segment that it spans (and on large machines may often span two insulating segments). The result of this is that as the commutator segment passes from under the brush, the current passing to it ramps down more smoothly than had been the case with pure copper brushes where the contact broke suddenly. Similarly the segment coming into contact with the brush has a similar ramping up of the current. Thus, although the current passing through the brush was more or less constant, the instantaneous current passing to the two commutator segments was proportional to the relative area in contact with the brush.\n\nThe introduction of the carbon brush had convenient side effects. Carbon brushes tend to wear more evenly than copper brushes, and the soft carbon causes far less damage to the commutator segments. There is less sparking with carbon as compared to copper, and as the carbon wears away, the higher resistance of carbon results in fewer problems from the dust collecting on the commutator segments.\n\nThe ratio of copper to carbon can be changed for a particular purpose. Brushes with higher copper content perform better with very low voltages and high current, while brushes with a higher carbon content are better for high voltage and low current. High copper content brushes typically carry 150 to 200 amperes per square inch of contact surface, while higher carbon content only carries 40 to 70 amperes per square inch. The higher resistance of carbon also results in a greater voltage drop of 0.8 to 1.0 volts per contact, or 1.6 to 2.0 volts across the commutator.\n\nA spring is typically used with the brush, to maintain constant contact with the commutator. As the brush and commutator wear down, the spring steadily pushes the brush downwards towards the commutator. Eventually the brush wears small and thin enough that steady contact is no longer possible or it is no longer securely held in the brush holder, and so the brush must be replaced.\n\nIt is common for a flexible power cable to be directly attached to the brush, because current flowing through the support spring would cause heating, which may lead to a loss of metal temper and a loss of the spring tension.\n\nWhen a commutated motor or generator uses more power than a single brush is capable of conducting, an assembly of several brush holders is mounted in parallel across the surface of the very large commutator. This parallel holder distributes current evenly across all the brushes, and permits a careful operator to remove a bad brush and replace it with a new one, even as the machine continues to spin fully powered and under load.\n\nHigh power, high current commutated equipment is now uncommon, due to the less complex design of alternating current generators that permits a low current, high voltage spinning field coil to energize high current fixed-position stator coils. This permits the use of very small singular brushes in the alternator design. In this instance, the rotating contacts are continuous rings, called slip rings, and no switching happens.\n\nModern devices using carbon brushes usually have a maintenance-free design that requires no adjustment throughout the life of the device, using a fixed-position brush holder slot and a combined brush-spring-cable assembly that fits into the slot. The worn brush is pulled out and a new brush inserted.\n\nThe different brush types make contact with the commutator in different ways. Because copper brushes have the same hardness as the commutator segments, the rotor cannot be spun backwards against the ends of copper brushes without the copper digging into the segments and causing severe damage. Consequently, strip/laminate copper brushes only make tangential contact with the commutator, while copper mesh and wire brushes use an inclined contact angle touching their edge across the segments of a commutator that can spin in only one direction.\n\nThe softness of carbon brushes permits direct radial end-contact with the commutator without damage to the segments, permitting easy reversal of rotor direction, without the need to reorient the brush holders for operation in the opposite direction. Although never reversed, common appliance motors that use wound rotors, commutators and brushes have radial-contact brushes. In the case of a reaction-type carbon brush holder, carbon brushes may be reversely inclined with the commutator so that the commutator tends to push against the carbon for firm contact.\nThe contact point where a brush touches the commutator is referred to as the \"commutating plane\". To conduct sufficient current to or from the commutator, the brush contact area is not a thin line but instead a rectangular patch across the segments. Typically the brush is wide enough to span 2.5 commutator segments. This means that two adjacent segments are electrically connected by the brush when it contacts both.\n\nMost introductions to motor and generator design start with a simple two-pole device with the brushes arranged at a perfect 90-degree angle from the field. This ideal is useful as a starting point for understanding how the fields interact but it is not how a motor or generator functions in actual practice.\n\nIn a real motor or generator, the field around the rotor is never perfectly uniform. Instead, the rotation of the rotor induces field effects which drag and distort the magnetic lines of the outer non-rotating stator.\nThe faster the rotor spins, the further this degree of field distortion. Because a motor or generator operates most efficiently with the rotor field at right angles to the stator field, it is necessary to either retard or advance the brush position to put the rotor's field into the correct position to be at a right angle to the distorted field.\n\nThese field effects are reversed when the direction of spin is reversed. It is therefore difficult to build an efficient reversible commutated dynamo, since for highest field strength it is necessary to move the brushes to the opposite side of the normal neutral plane. These effects can be mitigated by a Compensation winding in the face of the field pole that carries armature current.\n\nThe effect can be considered to be analogous to timing advance in an internal combustion engine. Generally a dynamo that has been designed to run at a certain fixed speed will have its brushes permanently fixed to align the field for highest efficiency at that speed.\nSelf-induction – The magnetic fields in each coil of wire join and compound together to create a magnetic field that resists changes in the current, which can be likened to the current having inertia.\n\nIn the coils of the rotor, even after the brush has been reached, currents tend to continue to flow for a brief moment, resulting in a wasted energy as heat due to the brush spanning across several commutator segments and the current short-circuiting across the segments.\n\n\"Spurious resistance\" is an apparent increase in the resistance in the armature winding, which is proportional to the speed of the armature, and is due to the lagging of the current.\n\nTo minimize sparking at the brushes due to this short-circuiting, the brushes are advanced a few degrees further yet, beyond the advance for field distortions. This moves the rotor winding undergoing commutation slightly forward into the stator field which has magnetic lines in the opposite direction and which oppose the field in the stator. This opposing field helps to reverse the lagging self-inducting current in the stator.\n\nSo even for a rotor which is at rest and initially requires no compensation for spinning field distortions, the brushes should still be advanced beyond the perfect 90-degree angle as taught in so many beginners textbooks, to compensate for self-induction.\nAlthough direct current motors and dynamos once dominated industry, the disadvantages of the commutator have caused a decline in the use of commutated machines in the last century. These disadvantages are: \n\nWith the wide availability of alternating current, DC motors have been replaced by more efficient AC synchronous or induction motors. In recent years, with the widespread availability of power semiconductors, in many remaining applications commutated DC motors have been replaced with \"brushless direct current motors\". These don't have a commutator; instead the direction of the current is switched electronically. A sensor keeps track of the rotor position and semiconductor switches such as transistors reverse the current. Operating life of these machines is much longer, limited mainly by bearing wear.\n\nThese are single-phase AC-only motors with higher starting torque than could be obtained with split-phase starting windings, before high-capacitance (non-polar, relatively high-current electrolytic) starting capacitors became practical. They have a conventional wound stator as with any induction motor, but the wire-wound rotor is much like that with a conventional commutator. Brushes opposite each other are connected to each other (not to an external circuit), and transformer action induces currents into the rotor that develop torque by repulsion.\n\nOne variety, notable for having an adjustable speed, runs continuously with brushes in contact, while another uses repulsion only for high starting torque and in some cases lifts the brushes once the motor is running fast enough. In the latter case, all commutator segments are connected together as well, before the motor attains running speed.\n\nOnce at speed, the rotor windings become functionally equivalent to the squirrel-cage structure of a conventional induction motor, and the motor runs as such.\n\nCommutators were used as simple forward-off-reverse switches for electrical experiments in physics laboratories. There are two well-known historical types:\n\nThis is similar in design to the commutators used in motors and dynamos. It was usually constructed of brass and ivory (later ebonite).\n\nThis consisted of a block of wood or ebonite with four wells, containing mercury, which were cross-connected by copper wires. The output was taken from a pair of curved copper wires which were moved to dip into one or other pair of mercury wells.\nInstead of mercury, ionic liquids or other liquid metals could be used.\n\n\n\n"}
{"id": "44490578", "url": "https://en.wikipedia.org/wiki?curid=44490578", "title": "Culbone Stone", "text": "Culbone Stone\n\nThe Culbone Stone, an early mediaeval standing stone, is close to Culbone in the English county of Somerset. The stone is made from Hangman grit, a local sandstone, and has a wheeled ring cross carved into it. The stone has been scheduled as an ancient monument.\n\nThe stone lies in woodland close to the boundary between Oare and Porlock on a permissive path through private land. It is approximately in height and wide with a maximum depth of .\n\nIt is made of Hangman grit a local sandstone which represents the Middle Devonian sequence of North Devon and Somerset. The unusual freshwater deposits in the Hangman Grits were mainly formed in desert conditions.\n\nAt the top of the stone is an incised wheeled ring cross, with a diameter of which is a Christian symbol, the style of which suggests it dates from 7th to 9th century. One arm of the cross at the lower right extends out of the circle. A slightly earlier date of the 6th or 7th century has also been suggested.\n\nThe stone was discovered recumbent in 1939 or 1940 and placed upright at the location in which it was found. It has been suggested that the stone has been moved from its original site as part of the nearby Culbone Hill Stone Row. One of the stones in the row also has an inscribed cross.\n"}
{"id": "50078940", "url": "https://en.wikipedia.org/wiki?curid=50078940", "title": "Cyclorotor", "text": "Cyclorotor\n\nA cyclorotor, cycloidal rotor, cycloidal propeller or cyclogiro, is a fluid propulsion device that converts shaft power into the acceleration of a fluid using a rotating axis perpendicular to the direction of fluid motion. It uses several blades with a spanwise axis parallel to the axis of rotation and perpendicular to the direction of fluid motion. These blades are cyclically pitched twice per revolution to produce force (thrust or lift) in any direction normal to the axis of rotation. Cyclorotors are used for propulsion, lift, and control on air and water vehicles. An aircraft using cyclorotors as the primary source of lift, propulsion, and control is known as a cyclogyro. The patented application, used on the ships with particular actuation mechanisms both mechanics or hydraulics is named from the name of the German company that produces them: Voith–Schneider cycloidal propellers.\n\nCyclorotors produce thrust by combined action of a rotation of a fixed point of the blades around a center and the oscillation of the blades that changes their angle-of-attack over time. The joint action of the advancement produced by the orbital motion and pitch angle variation generates a higher thrust at low speed than any other propeller. In hover, the blades are actuated to a positive pitch (outward from the center of the rotor) on the upper half of their revolution and a negative pitch (inward towards the axis of rotation) over the lower half inducing a net upward aerodynamic force and opposite fluid downwash. By varying the phase of this pitch motion the force can be shifted to any perpendicular angle or even downward. Before blade stall, increasing the amplitude of the pitching kinematics will magnify thrust.\n\nThe origin of the rotocycloid propeller are Russian and relates to aeronautic domain. Sverchkov's Samoljot, St-Peterburg, 1909, or \"wheel orthopter\" has been the first vehicle expressly thought for using this propulsion. Its scheme came near to cyclogiro, but it's difficult to classify it precisely. It had three flat surfaces and a rudder; rear edge of one of surfaces could be bent, replacing the action of an elevator. Lift and thrust had to be created by paddle wheels consisting of 12 blades, established in pairs under a 120° angle. The blades of a concave shape were changing an angle of incidence by the means of eccentrics and springs. In a bottom of the craft 10 hp engine was arranged. Transmission was ensured by a belt. Empty weight was about 200 kg. \"Samoljot\" was constructed by the military engineer E.P.Sverchkov with the grants of the Main Engineering Agency in St. Petersburg in 1909, was demonstrated at the Newest Inventions Exhibition and won a medal. Otherwise, it could not pass the preliminary tests without flying. \n\nIn 1914, Russian inventor and scientist A.N. Lodygin addressed the Russian government with the project of the cyclogiro-like aircraft, which scheme wabs similar to Sverchkov's \"Samoljot\". The project was not carried out. \n\nIn 1933, experiments in Germany by Adolf Rohrbach resulted in a paddle-wheel wing arrangement. Oscillating winglets went from positive to negative angles of attack during each revolution to create lift, and their eccentric mounting would, in theory, produce nearly any combination of horizontal and vertical forces. The DVL evaluated Rohrbach’s design, but the foreign aviation journals of the time cast doubt on the soundness of the design which meant that funding for the project could not be raised, even with a latter proposal as a Luftwaffe transport aircraft.\nThere appears to be no evidence that this design was ever built, let alone flown.\nBased on Rohrbach’s paddle-wheel research, however, Platt in the US designed by 1933 his own independent Cyclogyro. His paddle-wheel wing arrangement was awarded a US patent (which was only one of many similar patents on file), and underwent extensive wind-tunnel testing at MIT in 1927. Despite this, there is no evidence Platt’s aircraft was ever built.\n\nThe first operative cycloid propulsion was developed at Voith. Its origins date to the decision of the Voith company to focus on the business of transmission gear assemblies for turbines. The famous Voight propeller was based on its fluid-dynamics know-how gained from previous turbine projects. It was invented by Ernst Schneider, and enhanced by Voith. It was launched with name of Voith-Schneider Propeller (VSP) for commercial vessels. This new marine drive could significantly improve the manoeuvrability of a ship as demonstrated in the successful sea trials on the test boat Torqueo, in 1937. The first Voith Schneider Propellers were put into operation in the narrow canals of Venice, Italy. During the 1937 World Fair in Paris, Voith was awarded the grand prize – three times – for its exhibition of Voith Schneider Propellers and Voith turbo-transmissions. A year later, two of Paris' fire-fighting boats started operating with the new VSP system.\n\nCyclorotors provide a high degree of control. Traditional propellers, rotors, and jet engines produce thrust only along their axis of rotation and require rotation of the entire device to alter the thrust direction. This rotation requires large forces and comparatively long time scales since the propeller inertia is considerable, and the rotor gyroscopic forces resist rotation. For many practical applications (helicopters, airplanes, ships) this requires rotating the entire vessel. In contrast, cyclorotors need only to vary the blade pitch motions. Since there is little inertia associated with blade pitch change, thrust vectoring in the plane perpendicular to the axis of rotation is rapid.\n\nCyclorotors can produce lift and thrust at high advance ratios, which, in theory, would enable a cyclogyro aircraft to fly at subsonic speeds well exceeding those of single rotor helicopters. Single rotor helicopters are limited in forward speed by a combination of retreating blade stall and sonic blade tip constraints. As helicopters fly forward, the tip of the advancing blade experiences a wind velocity that is the sum of the helicopter forward speed and rotor rotational speed. This value cannot exceed the speed of sound if the rotor is to be efficient and quiet. Slowing the rotor rotational speed avoids this problem, but presents another. In the traditional method of the composition of velocity it is easy to understand that the velocity experienced by the retreating blade has a value that is produced by the vector composition of the velocity of blade rotation and the freestream velocity. In this condition it is evident that in presence of a sufficiently high advance ratio the velocity of air on the retreating blade is low. The flapping movement of the blade changes the angle of attack. It is then possible for the blade to reach the stall condition. In this case it is necessary that the stalling blade increases the pitch angle to keep some lift capability. This risk puts constraints on the design of the system. An accurate choice of the wing profile is necessary and careful dimensioning of the radius of the rotor for the specified speed range. Slow speed cyclorotors bypass this problem through a horizontal axis of rotation and operating at a comparatively low blade tip speed. For higher speeds, which may become necessary for industrial applications, it seems necessary to adopt more sophisticated strategies and solutions. A solution is the independent actuation of the blades which have been recently patented and successfully tested for naval use by use on hydraulic actuation system. The horizontal axis of rotation always provides an advancement of the upper blades, that produce always a positive lift by the full rotor. These characteristics could help overcome two issues of helicopters: their low energy efficiency and the advance ratio limitation.\n\nThe advancement of the blades and oscillations are the two dynamic actions which are produced by a cyclorotor. It is evident that the wing-blades of a cyclorotor operates in different way than a traditional aircraft wing or a traditional helicopter wing. The blades of a cyclorotor oscillates by rotation around a point that rotating describes an ideal circumference. The combination of the advancement motion of the centre of rotation of the blade and the oscillation of the blade (it is a movement somehow similar to the pendulum), which continue to vary its pitch generate a complex set of aerodynamic phenomena: \n\nThe two effects are evidently correlated with a general increase of the thrust produced. \nIf compared to a helicopter or any other propeller, it is evident that the same blade section in a rotocycloid produces much more thrust at the same Reynolds number. This effect can be explained by considering the traditional behavior of a propeller.\n\nAt low Reynolds numbers turbulence and laminar flow conditions can ever been reached. Considering a traditional wing profile it is evident that those conditions minimizes the speed differences between upper and lower face of the wing. It is then evident that both lift and stall speed are reduced. A consequence is a reduction of angle of attach at which stall conditions are reached. \n\nIn this regime, conventional propellers and rotors must use larger blade area and rotate faster to achieve the same propulsive forces and lose more energy to blade drag. It is then evident that a cyclorotor is much more energy efficient than any other propeller. \n\nActual cyclorotors bypass this problem by quickly increasing and then decreasing blade angle of attack, which temporarily delays stall and achieves a high lift coefficient. This unsteady lift makes cyclorotors more efficient at small scales, low velocities, and high altitudes than traditional propellers. \nIt is otherwise evident that many living beings are still much more efficient, because they can change not only the pitch but also the shape of their wings, such as birds and some insects or they can change the property of the boundary layer such as sharkskin.\n\nSome research tries to acquire the same level of efficiency of the natural examples of wings or surfaces. One direction is to introduce morphing wing concepts. Another relates to the introduction of boundary layer control mechanisms, such as dielecric barrier discharge.\n\nDuring experimental evaluation, cyclorotors produced little aerodynamic noise. This likely due to the lower blade tip speeds, which produce lower intensity turbulence following the blades.\n\nIn small-scale tests, cyclorotors achieved a higher power loading than comparable scale traditional rotors at the same disk loading. This is attributed to utilizing unsteady lift and consistent blade aerodynamic conditions. The rotational component of velocity on propellers increases from root to tip and requires blade chord, twist, airfoil, etc., to be varied along the blade. Since the cyclorotor blade span is parallel to the axis of rotation, each spanwise blade section operates at similar velocities and the entire blade can be optimized.\n\nCyclorotor blades require support structure for their positioning parallel to the rotor axis of rotation. This structure, sometimes referred to as \"spokes,\" adds to the parasite drag and weight of the rotor. Cyclorotor blades are also centrifugally loaded in bending (as opposed to the axial loading on propellers), which requires blades with an extremely high strength to weight ratio or intermediate blade support spokes. Early 20th century cyclorotors featured short blade spans, or additional support structure to circumvent this problem.\n\nCyclorotors require continuously actuated blade pitch. The relative flow angle experienced by the blades as they rotate about the rotor varies substantially with advance ratio and rotor thrust. To operate most efficiently a blade pitch mechanism should adjust for these diverse flow angles. High rotational velocities makes it difficult to implement an actuator based mechanism, which calls for a fixed or variable shape track for pitch control, mounted parallel to blade trajectory, onto which are placed blade's followers such as rollers or airpads - the pitch control track shape reliably determines blade's pitch along the orbit regardless of the blade's RPM. While the pitching motions used in hover are not optimized for forward flight, in experimental evaluation they were found to provide efficient flight up to an advance ratio near one.\n\nWind turbines are a potential application of cyclorotors. They are named in this case variable-pitch vertical axis wind turbines, with large benefits with respect to traditional VAWTs. This kind of turbines is stated to overcome most of the traditional limitations of traditional Darrieus VAWTs.\n\nThe most widespread application of cyclorotors is for ship propulsion and control. In ships the cyclorotor is mounted with the axis of rotation vertical so that thrust can quickly be vectored any direction in plane with the water surface. In 1922, Kurt Kirstin fitted a pair of cyclorotors to a 32 ft boat in Washington, which eliminated the need for a rudder and provided extreme maneuverability. While the idea floundered in the United States after the Kirsten-Boeing Propeller Company lost a US Navy research grant, the Voith-Schneider propeller company successfully commercially employed the propeller. This Voith-Schneider propeller was fitted to more than 100 ships prior to the outbreak of the Second World War. Today, the same company sells the same propeller for highly maneuverable watercraft. It is applied on offshore drilling ships, tugboats, and ferries.\n\nA cyclogyro is a vertical takeoff and landing aircraft using a cyclorotor as a rotor wing for lift and often also for propulsion and control. Advances in cyclorotor aerodynamics made the first untethered model cyclogyro flight possible in 2011 at the Northwestern Polytechnic Institute in China. Since then, universities and companies have successfully flown small-scale cyclogyros in several configurations.\n\nThe performance of traditional rotors is severely deteriorated at low Reynolds Numbers by low angle-of-attack blade stall. Current hover-capable MAVs can stay aloft for only minutes. Cyclorotor MAVs (very small scale cyclogyros) could utilize unsteady lift to extend endurance. The smallest cyclogyro flown to date weighs only 29 grams and was developed by the advanced vertical flight laboratory at Texas A&M university.\n\nA large exposed area makes airships susceptible to gusts and difficult to takeoff, land, or moor in windy conditions. Propelling airships with cyclorotors could enable flight in more severe atmospheric conditions by compensating for gusts with rapid thrust vectoring. Following this idea, the US Navy seriously considered fitting of 6 primitive Kirsten-Boeing cyclorotors to the airship. The \"Shenandoah\" crashed while transiting a squall line on 3 September 1925 before any possible installation and testing. No large scale tests have been attempted since, but a 20m cyclorotor airship demonstrated improved performance over a traditional airship configuration in a test.\n\n"}
{"id": "175039", "url": "https://en.wikipedia.org/wiki?curid=175039", "title": "Czochralski process", "text": "Czochralski process\n\nThe Czochralski process is a method of crystal growth used to obtain single crystals of semiconductors (e.g. silicon, germanium and gallium arsenide), metals (e.g. palladium, platinum, silver, gold), salts and synthetic gemstones. The process is named after Polish scientist Jan Czochralski, who invented the method in 1915 while investigating the crystallization rates of metals. He made this discovery by accident, while studying the crystallization rate of metals when, instead of dipping his pen into the ink, he did so in molten tin and drew a tin filament, that later proved to be a single crystal.\n\nThe most important application may be the growth of large cylindrical ingots, or boules, of single crystal silicon used in the electronics industry to make semiconductor devices like integrated circuits. Other semiconductors, such as gallium arsenide, can also be grown by this method, although lower defect densities in this case can be obtained using variants of the Bridgman-Stockbarger technique.\n\nMonocrystalline silicon (mono-Si) grown by the \"Czochralski process\" is often referred to as \"monocrystalline Czochralski silicon\" (Cz-Si). It is the basic material in the production of integrated circuits used in computers, TVs, mobile phones and all types of electronic equipment and semiconductor devices. Monocrystalline silicon is also used in large quantities by the photovoltaic industry for the production of conventional mono-Si solar cells. The almost perfect crystal structure yields the highest light-to-electricity conversion efficiency for silicon.\n\nHigh-purity, semiconductor-grade silicon (only a few parts per million of impurities) is melted in a crucible at , usually made of quartz. Dopant impurity atoms such as boron or phosphorus can be added to the molten silicon in precise amounts to dope the silicon, thus changing it into p-type or n-type silicon, with different electronic properties. A precisely oriented rod-mounted seed crystal is dipped into the molten silicon. The seed crystal's rod is slowly pulled upwards and rotated simultaneously. By precisely controlling the temperature gradients, rate of pulling and speed of rotation, it is possible to extract a large, single-crystal, cylindrical ingot from the melt. Occurrence of unwanted instabilities in the melt can be avoided by investigating and visualizing the temperature and velocity fields during the crystal growth process. This process is normally performed in an inert atmosphere, such as argon, in an inert chamber, such as quartz.\n\nDue to the efficiencies of common wafer specifications, the semiconductor industry has used wafers with standardized dimensions. In the early days, the boules were smaller, only a few inches wide. With advanced technology, high-end device manufacturers use 200 mm and 300 mm diameter wafers. The width is controlled by precise control of the temperature, the speeds of rotation and the speed the seed holder is withdrawn. The crystal ingots from which these wafers are sliced can be up to 2 metres in length, weighing several hundred kilograms. Larger wafers allow improvements in manufacturing efficiency, as more chips can be fabricated on each wafer, so there has been a steady drive to increase silicon wafer sizes. The next step up, 450 mm, is currently scheduled for introduction in 2018. Silicon wafers are typically about 0.2–0.75 mm thick, and can be polished to great flatness for making integrated circuits or textured for making solar cells.\n\nThe process begins when the chamber is heated to approximately 1500 degrees Celsius, melting the silicon. When the silicon is fully melted, a small seed crystal mounted on the end of a rotating shaft is slowly lowered until it just dips below the surface of the molten silicon. The shaft rotates counterclockwise and the crucible rotates clockwise. The rotating rod is then drawn upwards very slowly—about 25 mm per hour when making a crystal of ruby—allowing a roughly cylindrical boule to be formed. The boule can be from one to two metres, depending on the amount of silicon in the crucible.\n\nThe electrical characteristics of the silicon are controlled by adding material like phosphorus or boron to the silicon before it is melted. The added material is called dopant and the process is called doping. This method is also used with semiconductor materials other than silicon, such as gallium arsenide.\n\nWhen silicon is grown by the Czochralski method, the melt is contained in a silica (quartz) crucible. During growth, the walls of the crucible dissolve into the melt and Czochralski silicon therefore contains oxygen at a typical concentration of 10 cm. Oxygen impurities can have beneficial or detrimental effects. Carefully chosen annealing conditions can give rise to the formation of oxygen precipitates. These have the effect of trapping unwanted transition metal impurities in a process known as gettering, improving the purity of surrounding silicon. However, formation of oxygen precipitates at unintended locations can also destroy electrical structures. Additionally, oxygen impurities can improve the mechanical strength of silicon wafers by immobilising any dislocations which may be introduced during device processing. It was experimentally shown in the 1990s that the high oxygen concentration is also beneficial for the radiation hardness of silicon particle detectors used in harsh radiation environment (such as CERN's LHC/HL-LHC projects). Therefore, radiation detectors made of Czochralski- and Magnetic Czochralski-silicon are considered to be promising candidates for many future high-energy physics experiments. It has also been shown that the presence of oxygen in silicon increases impurity trapping during post-implantation annealing processes.\n\nHowever, oxygen impurities can react with boron in an illuminated environment, such as that experienced by solar cells. This results in the formation of an electrically active boron–oxygen complex that detracts from cell performance. Module output drops by approximately 3% during the first few hours of light exposure.\n\nThe impurity concentration in the solid crystal that results from freezing an amount of volume can be obtained from consideration of the segregation coefficient.\n\nDuring the growth process, volume of melt formula_10 freezes, and there are impurities from the melt that are removed.\n\n\n"}
{"id": "14548330", "url": "https://en.wikipedia.org/wiki?curid=14548330", "title": "Dark star (dark matter)", "text": "Dark star (dark matter)\n\nA dark star is a type of star that may have existed early in the universe before conventional stars were able to form. The stars would be composed mostly of normal matter, like modern stars, but a high concentration of neutralino dark matter within them would generate heat via annihilation reactions between the dark-matter particles. This heat would prevent such stars from collapsing into the relatively compact sizes of modern stars and therefore prevent nuclear fusion among the normal matter atoms from being initiated.\n\nUnder this model, a dark star is predicted to be an enormous cloud of hydrogen and helium ranging between 4 and 2000 astronomical units in diameter and with a surface temperature low enough that the emitted radiation would be invisible to the naked eye.\n\nIn the unlikely event that dark stars have endured to the modern era, they could be detectable by their emissions of gamma rays, neutrinos, and antimatter and would be associated with clouds of cold molecular hydrogen gas that normally would not harbor such energetic particles.\n\n"}
{"id": "286016", "url": "https://en.wikipedia.org/wiki?curid=286016", "title": "Decantation", "text": "Decantation\n\nDecantation is a process for the separation of mixtures of immiscible liquids or of a liquid and a solid mixture such as a suspension. The layer closer to the top of the container—the less dense of the two liquids, or the liquid from which the precipitate or sediment has settled out—is poured off, leaving the other component or the more dense liquid of the mixture behind. An incomplete separation is witnessed during the separation of two immiscible liquids.\n\nDecantation can be used to separate immiscible liquids that have different densities. For example, when a mixture of water and oil are present in a beaker, a distinct layer between the two consistency is formed, with the oil layer floating on top of the water layer. This separation can be done by pouring oil out of the container, leaving water behind. Generally, this technique gives an incomplete separation as it is difficult to pour off all of the top layer without pouring out some parts of the bottom layer.\n\nA separatory funnel is an alternative apparatus for separating liquid layers. It has a valve at the bottom to allow draining off the bottom layer. It can give a better separation between the two liquids.\n\nDecantation can also separate solid and liquid mixtures by allowing gravity to pull the solid fragments to settle at the bottom of the container. In laboratory situations, decantation of mixtures containing solids and liquids occur in test tubes. To enhance productivity, test tubes should be placed at a 45° angle to allow sediments to settle at the bottom of the apparatus.\n\nA centrifuge may also be used in decantation as the natural process of settling down is time consuming and tedious. A centrifuge forces the precipitate to the bottom of the container; if the force is high enough, solids can aggregate to form pellets, making it easier to separate the mixtures. Then the liquid can be more easily poured away, as the precipitate will tend to remain in its compressed form. \n\nA decanter centrifuge may be used for a continuous solid liquid separation.\n\nDecantation is frequently used to purify a liquid by separating it from a suspension of insoluble particles (e.g. in red wine, where the wine is decanted from the potassium bitartrate crystals to avoid unsavory taste). This makes the wine more tonic and astringent.\n\nCream accelerates to the top of milk, allowing the separation of milk and cream. Fat is determined in butter by decantation.\n\nTo obtain a sample of clear water from muddy water, muddy water is left in a container until the mud settles, and then the clear water is poured into another container.\n\nIn sugar industry, processing of sugar beets into granular sugar many liquid - solid separations are encountered e.g. separation of syrups from crystals.\n\nDecantation is also present in nanotechnology. In the synthesis of high quality silver nanowire (AgNW) solutions and fabrication process of high performance electrodes, decantation is also being applied which greatly simplifies the purification process.\n\nAfter using a desiccant to absorb water from an organic liquid, the organic liquid can often be decanted away from the desiccant.\n\nThe process of deriving vinegar also requires decantation to remove fats from the raw substance. \nPlasma can be separated from blood through decantation by using a centrifuge.\n\nMercury is disposed off in water bodies during mining, turning the water unfit and toxic. The elimination of mercury from water can be done by decantation.\n\n"}
{"id": "63967", "url": "https://en.wikipedia.org/wiki?curid=63967", "title": "Double pendulum", "text": "Double pendulum\n\nIn physics and mathematics, in the area of dynamical systems, a double pendulum is a pendulum with another pendulum attached to its end, and is a simple physical system that exhibits rich dynamic behavior with a strong sensitivity to initial conditions. The motion of a double pendulum is governed by a set of coupled ordinary differential equations and is chaotic.\n\nSeveral variants of the double pendulum may be considered; the two limbs may be of equal or unequal lengths and masses, they may be simple pendula or compound pendula (also called complex pendula) and the motion may be in three dimensions or restricted to the vertical plane. In the following analysis, the limbs are taken to be identical compound pendula of length and mass , and the motion is restricted to two dimensions.\n\nIn a compound pendulum, the mass is distributed along its length. If the mass is evenly distributed, then the center of mass of each limb is at its midpoint, and the limb has a moment of inertia of about that point.\n\nIt is convenient to use the angles between each limb and the vertical as the generalized coordinates defining the configuration of the system. These angles are denoted and . The position of the center of mass of each rod may be written in terms of these two coordinates. If the origin of the Cartesian coordinate system is taken to be at the point of suspension of the first pendulum, then the center of mass of this pendulum is at:\nand the center of mass of the second pendulum is at\nThis is enough information to write out the Lagrangian.\n\nThe Lagrangian is\nThe first term is the \"linear\" kinetic energy of the center of mass of the bodies and the second term is the \"rotational\" kinetic energy around the center of mass of each rod. The last term is the potential energy of the bodies in a uniform gravitational field. The dot-notation indicates the time derivative of the variable in question.\n\nSubstituting the coordinates above and rearranging the equation gives\nThere is only one conserved quantity (the energy), and no conserved momenta. The two momenta may be written as\n\nThese expressions may be inverted to get\n\nThe remaining equations of motion are written as\n\nThese last four equations are explicit formulae for the time evolution of the system given its current state. It is not possible to go further and integrate these equations analytically, to get formulae for and as functions of time. It is, however, possible to perform this integration numerically using the Runge Kutta method or similar techniques.\n\nThe double pendulum undergoes chaotic motion, and shows a sensitive dependence on initial conditions. The image to the right shows the amount of elapsed time before the pendulum flips over, as a function of initial position when released at rest. Here, the initial value of ranges along the -direction from −3 to 3. The initial value ranges along the -direction, from −3 to 3. The colour of each pixel indicates whether either pendulum flips within:\nInitial conditions that do not lead to a flip within are plotted white.\n\nThe boundary of the central white region is defined in part by energy conservation with the following curve:\n\nWithin the region defined by this curve, that is if\n\nthen it is energetically impossible for either pendulum to flip. Outside this region, the pendulum can flip, but it is a complex question to determine when it will flip. Similar behavior is observed for a double pendulum composed of two point masses rather than two rods with distributed mass.\n\nThe lack of a natural excitation frequency has led to the use of double pendulum systems in seismic resistance designs in buildings, where the building itself is the primary inverted pendulum, and a secondary mass is connected to complete the double pendulum.\n\n\n\n\n"}
{"id": "24116630", "url": "https://en.wikipedia.org/wiki?curid=24116630", "title": "Enercon E-126", "text": "Enercon E-126\n\nThe Enercon E-126 is an onshore wind turbine model manufactured by the German company Enercon. With a hub height of , rotor diameter of and a total height of , the turbine can generate up to of power, making it the largest wind turbine in the world (by nameplate capacity) for several years, until it was overtook in 2014 by the Danish company Vestas with their V164-8.0 turbine. Their model number is a reference to their rotor diameter. \n\nThe power output of the generator was changed from to after technical revisions were performed in 2009. Since 2011 the E-126 is available as a 7.6 MW nameplate capacity. The E-126 incorporates power electronics and offers grid stabilising capabilities.\n\nThe weight of the foundation of the turbine tower is about 2,500 t, the tower itself 2,800 t, the machine housing 128 t, the generator 220 t, the rotor (including the blade) 364 t. The total weight is 6,012 tonnes exactly.\n\nThe first turbine of this model was installed in Emden, Germany in 2007.\nThe list price of one unit is $14 million plus install costs.\n\nIn June 2012, at least 147 Enercon E-126 windturbines were operating, in construction, or nearing final approval, 35 of them completed and operating.\nFurthermore, during 2010-2011, onshore wind farm projects still in their early design processes were considering wind turbines of the 2-3.5 MW class, or wind turbines of the 5-8 MW class. This approach is at least applied in the Netherlands. Examples for this trend are found for instance in the preliminary research for the \"Wind farm de Drentse Monden\" aiming at 300-450 MW with possibly 50-60 E-126/7.5 MW turbines, \"Wind farm N33\" aiming at >120 MW with possibly 15-40 E-126/7.5 MW turbines, \"Wind farm Krammer\" aiming at >100 MW with possibly 11-21 E-126/7.5 MW turbines, \"Wind farm Wieringermeer\" aiming 200-400 MW with possibly 60 or more 6+ MW turbines (in that case possibly Repower 6M/6.15MW).\n\nIn September 2010, the world's first wind park consisting entirely of Enercon E-126 turbines (11 turbines in total), was completed in Estinnes, Belgium. This wind park is capable, at max capacity, to supply 55,000 residences with pure wind energy. This wind park, though, has one 6 MW unit, so this reduces the wind park's peak capacity from 83.38 MW to 81.8MW.\n\nThe Markbygden Wind Farm is planned to have 1,101 turbines covering 500 km², to generate 4,000 MW and an annual yield up to 12 TWh, making it one of the world's largest wind farms. \nUnder construction in northern Sweden, it will contain a mix of Enercon E-126 7.58 MW wind turbines and Enercon E-101 3.05 MW wind turbines, the exact number of each type to be determined by further studies. \nThe pilot stage wind farm at Dragaliden was completed in 2010, generating 24 MW with 12 turbines.\n\nMeanwhile, the Netherlands government has given its final approval on 6 January 2011 for the 'Windpark Noordoostpolder, part of which consists of 38 Enercon E-126 7.58 MW wind turbines. Afterwards, an ultimate case for the highest Court of state by opponents has been closed on 8 February 2012, confirming the government decision. It’s now expected preparatory works will start before the summer of 2012.\n\nIn France, a pending approval for the wind farm 'Le Mont des 4 Faux', consisting of an initial number of 52 Enercon E-126 7.58 MW wind turbines but in April 2012 reduced to a new variant of 47 turbines (deleting one 5-unit row in order to meet some ornithological concerns), is considered to be confirmed as such 47 turbines wind farm in 2012. This E-126 wind farm is situated between Juniville and Machault, at the southern side of the French Ardennes, near Reims. Project developer is a company named Windvision.\n\n"}
{"id": "244391", "url": "https://en.wikipedia.org/wiki?curid=244391", "title": "Flash flood", "text": "Flash flood\n\nA flash flood is a rapid flooding of geomorphic low-lying areas: washes, rivers, dry lakes and basins. It may be caused by heavy rain associated with a severe thunderstorm, hurricane, tropical storm, or meltwater from ice or snow flowing over ice sheets or snowfields. Flash floods may occur after the collapse of a natural ice or debris dam, or a human structure such as a man-made dam, as occurred before the Johnstown Flood of 1889. Flash floods are distinguished from regular floods by having a timescale of less than six hours. The water that is temporarily available is often used by plants with rapid germination and short growth cycles and by specially adapted animal life.\n\nFlash floods can occur under several types of conditions. Flash flooding occurs when it rains rapidly on saturated soil or dry soil that has poor absorption ability. The runoff collects in gullies and streams and, as they join to form larger volumes, often forms a fast flowing front of water and debris.\n\nFlash floods most often occur in normally dry areas that have recently received precipitation, but they may be seen anywhere downstream from the source of the precipitation, even many miles from the source. In areas on or near volcanoes, flash floods have also occurred after eruptions, when glaciers have been melted by the intense heat. Flash floods are known to occur in the highest mountain ranges of the United States and are also common in the arid plains of the Southwestern United States. Flash flooding can also be caused by extensive rainfall released by hurricanes and other tropical storms, as well as the sudden thawing effect of ice dams. Human activities can also cause flash floods to occur. When dams fail, a large quantity of water can be released and destroy everything in its path.\n\nThe United States National Weather Service gives the advice \"Turn Around, Don't Drown\" for flash floods; that is, it recommends that people get out of the area of a flash flood, rather than trying to cross it. Many people tend to underestimate the dangers of flash floods. What makes flash floods most dangerous is their sudden nature and fast-moving water. A vehicle provides little to no protection against being swept away; it may make people overconfident and less likely to avoid the flash flood. More than half of the fatalities attributed to flash floods are people swept away in vehicles when trying to cross flooded intersections. As little as of water is enough to carry away most SUV-sized vehicles. The U.S. National Weather Service reported in 2005 that, using a national 30-year average, more people die yearly in floods, 127 on average, than by lightning (73), tornadoes (65), or hurricanes (16).\n\nIn deserts, flash floods can be particularly deadly for several reasons. First, storms in arid regions are infrequent, but they can deliver an enormous amount of water in a very short time. Second, these rains often fall on poorly absorbent and often clay-like soil, which greatly increases the amount of runoff that rivers and other water channels have to handle. These regions tend not to have the infrastructure that wetter regions have to divert water from structures and roads, such as storm drains, culverts, and retention basins, either because of sparse population or poverty, or because residents believe the risk of flash floods is not high enough to justify the expense. In fact, in some areas, desert roads frequently cross dry river and creek beds without bridges. From the driver's perspective, there may be clear weather, when a river unexpectedly forms ahead of or around the vehicle in a matter of seconds. Finally, the lack of regular rain to clear water channels may cause flash floods in deserts to be headed by large amounts of debris, such as rocks, branches, and logs.\n\nDeep slot canyons can be especially dangerous to hikers as they may be flooded by a storm that occurs on a mesa miles away. The flood sweeps through the canyon; the canyon makes it difficult to climb up and out of the way to avoid the flood.\n\n\n\n"}
{"id": "26270420", "url": "https://en.wikipedia.org/wiki?curid=26270420", "title": "Formazine", "text": "Formazine\n\nFormazine (formazin) is a heterocyclic polymer produced by reaction of hexamethylenetetramine with hydrazine sulfate.\n\nThe hexamethylenetetramine tetrahedral cage-like structure, similar to adamantane, serves as molecular building block to form a tridimensional polymeric network.\n\nFormazine is very poorly soluble in water and when directly synthesized in aqueous solution, by simply mixing its two highly soluble precursors, it forms small size colloidal particles. These organic colloids are responsible of the light scattering of the formazine suspensions in all the directions. Optical properties of colloidal suspensions depend on the suspended particles size and size distribution. Because formazine is a stable synthetic material with uniform particle size it is commonly used as a standard to calibrate turbidimeters and to control the reproducibility of their measurements. Formazin use was first proposed by Kingsbury \"et al.\" (1926) for the rapid standardization of turbidity measurements of albumin in urine. The unit is called Formazin Turbidity Unit (FTU). A suspension of 1.25 mg/L hydrazine sulfate and 12.5 mg/L hexamethylenetetramine in water has a turbidity of one FTU.\n\nIn the United States environmental monitoring the turbidity standard unit is called Nephelometric Turbidity Units (NTU), while the international standard unit is called Formazin Nephelometric Unit (FNU). The most generally applicable unit is Formazin Turbidity Unit (FTU), although different measurement methods can give quite different values as reported in FTU.\n\nFor turbidity measurement, a formazine suspension is prepared by mixing solutions of 10 g/L hydrazine sulfate and 100 g/L hexamethylenetetramine with ultrapure water. The resulting solution is left for 24 hours, at 25 °C ±3 °C, for the suspension to develop. This produces a suspension with a turbidity value of 4000 NTU/FAU/FTU/FNU. This is then diluted to a value to suit the instrument range. There is no straightforward relationship between FTU/FAU and NTU/FNU because it depends on the optical characteristics of the particular matter in the sample. A general difficulty encountered for preparing formazine standards is to obtain sufficiently reproducible and accurate results. The preparation temperature is essential because it affects the particle size of the formazine particles. Uncertainties related to temperature fluctuations are of the order of 1.2% per °C. \nThe purity of the water used in the preparation of the formazine dispersion is also important as it cannot initially contain colloidal particles. Experience shows that water filtered as required has a residual scatter of about 0.02 FTU = 20 mFTU (inherent brightening effect). This has to be taken into account during calibration and for the detection of very low turbidity levels. The commercially available aqueous dispersions of formazine standard are often traceable according to the EN ISO 7027 norm. The shelf life of formazine dispersions does not exceed a few months (a few weeks if the bottle has been opened) as their characteristics evolve with time due to the ageing of the colloidal particles (Ostwald ripening: change in their size distribution and in their number due to their coalescence/aggregation) and the possible development of micro-organisms (bacteria, microscopic fungi, yeast, ...).\n\n\"Formazine\" has been used as a name for a fictional stimulant in the \"Star Trek\" television series.\n\n\n"}
{"id": "25074314", "url": "https://en.wikipedia.org/wiki?curid=25074314", "title": "Fuqing Nuclear Power Plant", "text": "Fuqing Nuclear Power Plant\n\nThe Fuqing Nuclear Power Plant () is a nuclear power plant in Fuqing, Fujian Province, China. \nThe plant is located on the coast of Xinghua Bay, near Qianxue Village, Sanshan Town. \nThe station has four 1,089 megawatt (MW) CPR-1000 pressurized water reactors (PWRs). \nThe CPR-1000 is an advanced PWR design developed by China from the Areva-designed PWRs at the Daya Bay Nuclear Power Plant.\nThe plant was jointly constructed and is operated by China National Nuclear Corporation (51%), China Huadian Corp. (39%) and the Fujian Investment & Development Co Ltd. (10%).\n\nConstruction of the first unit began on 21 November 2008 and was completed in 2014.\nFirst concrete for Unit 2 was poured on 17 June 2009 and the unit was started in October 2015.\nFirst concrete for Unit 3 was poured on 31 December 2010. \nConstruction of Unit 4 was to begin in 2011, but was delayed until November 2012 by China's nuclear safety review after the Japanese nuclear accident.\n\nIn November 2014 it was announced that units 5 and 6 would be of the Hualong One (updated CPR-1000) design, with unit 5 scheduled to be in operation about 2019. \nThe first concrete was poured for Fuqing 5 on 7 May 2015.\n\n"}
{"id": "16393338", "url": "https://en.wikipedia.org/wiki?curid=16393338", "title": "Goodman relation", "text": "Goodman relation\n\nIn materials science and fatigue, the Goodman relation is an equation used to quantify the interaction of mean and alternating stresses on the fatigue life of a material.\n\nA Goodman diagram,\nsometimes called a Haigh diagram\nor a Haigh-Soderberg diagram,\nis a graph of (linear) mean stress vs. (linear) alternating stress, showing when the material fails at some given number of cycles. \n\nA scatterplot of experimental data shown on such a plot can often be approximated by a parabola known as the Gerber line,\nwhich can in turn be (conservatively) approximated by a straight line called the Goodman line.\n\nThe Goodman relation can be represented mathematically as:\n\nWhere formula_2 is the alternating stress, formula_3 is the mean stress, formula_4 is the fatigue limit for completely reversed loading, and formula_5 is the ultimate tensile strength of the material. So, the Goodman line connects formula_5 on abscissa and formula_4 on the ordinate. \n\nThe general trend given by the Goodman relation is one of decreasing fatigue life with increasing mean stress for a given level of alternating stress. The relation can be plotted to determine the safe cyclic loading of a part; if the coordinate given by the mean stress and the alternating stress lies under the curve given by the relation, then the part will survive. If the coordinate is above the curve, then the part will fail for the given stress parameters.\n\n"}
{"id": "4301763", "url": "https://en.wikipedia.org/wiki?curid=4301763", "title": "Grain growth", "text": "Grain growth\n\nIn materials science, grain growth is the increase in size of grains (crystallites) in a material at high temperature. This occurs when recovery and recrystallisation are complete and further reduction in the internal energy can only be achieved by reducing the total area of grain boundary. The term is commonly used in metallurgy but is also used in reference to ceramics and minerals. \n\nMost materials exhibit the Hall–Petch effect at room-temperature and so display a higher yield stress when the grain size is reduced. At high temperatures the opposite is true since the open, disordered nature of grain boundaries means that vacancies can diffuse more rapidly down boundaries leading to more rapid Coble creep. Since boundaries are regions of high energy they make excellent sites for the nucleation of precipitates and other second-phases e.g. Mg–Si–Cu phases in some aluminium alloys or martensite platlets in steel. Depending on the second phase in question this may have positive or negative effects.\n\nGrain growth has long been studied primarily by the examination of sectioned, polished and etched samples under the optical microscope. Although such methods enabled the collection of a great deal of empirical evidence, particularly with regard to factors such as temperature or composition, the lack of crystallographic information limited the development of an understanding of the fundamental physics. Nevertheless, the following became well-established features of grain growth:\n\nThe boundary between one grain and its neighbour (grain boundary) is a defect in the crystal structure and so it is associated with a certain amount of energy. As a result, there is a thermodynamic driving force for the total area of boundary to be reduced. If the grain size increases, accompanied by a reduction in the actual number of grains per volume, then the total area of grain boundary will be reduced. \n\nThe local velocity of a grain boundary at any point is proportional to the local curvature of the grain boundary, i.e.:\n\nformula_1,\nwhere formula_2 is the velocity of grain boundary, formula_3 is grain boundary mobility (generally depends on orientation of two grains), formula_4 is the grain boundary energy and formula_5 is the sum of the two principal surface curvatures. For example, shrinkage velocity of a spherical grain embedded inside another grain is \n\nformula_6, \nwhere formula_7 is radius of the sphere. This driving pressure is very similar in nature to the Laplace pressure that occurs in foams.\nIn comparison to phase transformations the energy available to drive grain growth is very low and so it tends to occur at much slower rates and is easily slowed by the presence of second phase particles or solute atoms in the structure.\n\nIdeal grain growth is a special case of normal grain growth where boundary motion is driven only by local curvature of the grain boundary. It results in the reduction of the total amount of grain boundary surface area i.e. total energy of the system. Additional contributions to the driving force by e.g. elastic strains or temperature gradients are neglected. If it holds that the rate of growth is proportional to the driving force and that the driving force is proportional to the total amount of grain boundary energy, then it can be shown that the time \"t\" required to reach a given grain size is approximated by the equation\n\nformula_8\n\nwhere \"d\" is the initial grain size, \"d\" is the final grain size and k is a temperature dependent constant given by an exponential law:\n\nformula_9 \n\nwhere \"k\" is a constant, \"T\" is the absolute temperature and \"Q\" is the activation energy for boundary mobility. Theoretically, the activation energy for boundary mobility should equal that for self-diffusion but this is often found not to be the case. \n\nIn general these equations are found to hold for ultra-high purity materials but rapidly fail when even tiny concentrations of solute are introduced.\n\nAn old-standing topic in grain growth is the evolution of the grains size distribution. Inspired by the work of Lifshitz and Slyozov on Ostwald ripening, Hillert has suggested that in a normal grain growth process the size distribution function must converge to a self-similar solution, i.e. it becomes invariant when the grain size is scaled with a characteristic length of the system formula_10that is proportional to the average grain size formula_11. \n\nSeveral simulation studies, however, have shown that the size distribution deviates from the Hillert's self-similar solution. Hence a search for a new possible self-similar solution was initiated that indeed led to a new class of self-similar distribution functions. Large-scale phase field simulations have shown that there is indeed a self-similar behavior possible within the new distribution functions. It was shown that the origin of the deviation from Hillert's distribution is indeed the geometry of grains specially when they are shrinking. \n\nIn common with recovery and recrystallisation, growth phenomena can be separated into continuous and discontinuous mechanisms. In the former the microstructure evolves from state A to B (in this case the grains get larger) in a uniform manner. In the latter, the changes occur heterogeneously and specific transformed and untransformed regions may be identified. Abnormal or discontinuous grain growth is characterised by a subset of grains growing at a high rate and at the expense of their neighbours and tends to result in a microstructure dominated by a few very large grains. In order for this to occur the subset of grains must possess some advantage over their competitors such as a high grain boundary energy, locally high grain boundary mobility, favourable texture or lower local second-phase particle density.\n\nIf there are additional factors preventing boundary movement, such as Zener pinning by particles, then the grain size may be restricted to a much lower value than might otherwise be expected. This is an important industrial mechanism in preventing the softening of materials at high temperature.\n\nCertain materials especially refractories which are processed at high temperatures end up with excessively large grain size and poor mechanical properties at room temperature. To mitigate this problem in a common sintering procedure, a variety of dopants are often used to inhibit grain growth.\n"}
{"id": "5530147", "url": "https://en.wikipedia.org/wiki?curid=5530147", "title": "Herbig Ae/Be star", "text": "Herbig Ae/Be star\n\nA Herbig Ae/Be star (HAeBe) is a pre-main-sequence star – a young (<10Myr) star of spectral types A or B. These stars are still embedded in gas-dust envelopes and are sometimes accompanied by circumstellar disks. Hydrogen and calcium emission lines are observed in their spectra. They are 2-8 Solar mass () objects, still existing in the star formation (gravitational contraction) stage and approaching the main sequence (i.e. they are not burning hydrogen in their center). In the Hertzsprung–Russell diagram these stars are located to the right of the main sequence. They are named after the American astronomer George Herbig, who first distinguished them from other stars in 1960.\nThe original Herbig criteria were:\n\nThere are now several known isolated Herbig Ae/Be stars (i.e. not connected with dark clouds or nebulae). Thus the most reliable criteria now can be:\n\nSometimes Herbig Ae/Be stars show significant brightness variability. They are believed to be due to clumps (protoplanets and planetesimals) in the circumstellar disk. In the lowest brightness stage the radiation from the star becomes bluer and linearly polarized (when the clump obscures direct star light, scattered from disk light relatively increases – it is the same effect as the blue color of our sky).\n\nAnalogs of Herbig Ae/Be stars in the smaller mass range (<2 ) – F, G, K, M spectral type pre-main-sequence stars – are called T Tauri stars. More massive (>8 ) stars in pre-main-sequence stage are not observed, because they evolve very quickly: when they become visible (i.e. disperses surrounding circumstellar gas and dust cloud), the hydrogen in the center is already burning and they are main-sequence objects.\n\nPlanets around Herbig Ae/Be stars include:\n\n"}
{"id": "41835732", "url": "https://en.wikipedia.org/wiki?curid=41835732", "title": "Horse and Rider (Leonardo da Vinci)", "text": "Horse and Rider (Leonardo da Vinci)\n\nHorse and Rider is a beeswax sculpture depicting a rider on a horse, attributed to Leonardo da Vinci 1508–1511. It was intended to be used as a model for a life-size sculpture, commissioned by Charles II d'Amboise, French Governor of Milan from 1503–1511. Charles II d'Amboise died in 1511, Leonardo died in 1519 and the monument to d'Amboise was never completed nor cast in bronze.\n\nThe approximately high, long, and wide beeswax sculpture is believed to be a maquette for a full size bronze sculpture. According to professor Ernesto Solari, curator of a 2016 exhibition of the sculpture, it is innovative, far removed from the classical models the young Leonardo had been familiar with during his time with Andrea del Verrocchio; particularly when Verrocchio was working on the Equestrian statue of Bartolomeo Colleoni. On the horse's chest is an embedded print of a right thumb, believed to be Leonardo's. Historian hypothesizes that \"this is a funeral monument. There are several clues that lead to this interpretation; the horse is portrayed disarranging the rider to indicate that the animal is frightened. It is going down to the underworld, while the knight Charles d’Amboise, on the other hand is portrayed in a serene mood and eyes closed, the hand on his heart; the Governor of Milan is parting from his loved ones. Finally, one can notice the thigh protector in the shape of a shell, a symbol of travel, in this case without return.\"\n\nIn 1506 Charles II d'Amboise summoned Leonardo to return to Milan from Florence. D'Amboise commissioned Leonardo to design the gardens for his suburban villa. At this time Leonardo began to develop the concept of an equestrian portrait of his patron, Charles d'Amboise.\n\nLeonardo is known to have used wax models to study the compositions of his paintings, as noted by Benvenuto Cellini in reference to the paintings in Milan and Florence.\n\nUpon Leonardo's death in 1519 his unfinished works, drawings and notebooks were inherited by Francesco Melzi, Leonardo's friend and protégé. The documents of the Melzi d'Eril family, who own Francesco's still existing Villa Melzi in Vaprio d'Adda, don't however have a record of this wax sculpture.\n\nAt the turn of the twentieth century, the beeswax sculpture was recorded to have been in the Melzi di Cusano family collections in Milan, and later in Giorgio Sangiorgi's (1886–1960) collection in Rome. An unnamed art collector moved the sculpture from Italy in the early 1920s, and by 1938 it was in Switzerland.\n\nItalian art historian Carlo Pedretti discovered the existence of the wax model in the late 1970s, and took black and white photos of it. When compiling a 1987 catalogue raisonné of Leonardo's drawings stored in the Royal Collection in Windsor Castle, Pedretti added some of his photos for comparison.\n\nDavid Nickerson, director of the Mallett at Bourdon House in London, acquired the wax model in 1985. Over the centuries the model had sustained damage, including the loss of one of the horse's legs, along with the rider's feet and hands. It was presented to a group of American businessmen in 1985. As it was continuing to degrade, they had a latex mold made to preserve its condition. Their intention was to market a limited edition of bronze castings, which didn't happen until 25 years later.\n\nThe beeswax sculpture was displayed around the world in the 1990s as part of a travelling exhibition named \"Leonardo da Vinci: Scientist, Inventor, Artist\" in Sweden in Stockholm, Malmö and Göteborg in 1995, Vienna in 1996, and in Boston and Singapore in 1997. Due to its fragility, it now remains in a temperature-controlled private collection in London.\n\nThe sculpture was attributed to Leonardo da Vinci by art historian Carlo Pedretti in 1985, mainly due to a note Leonardo had written for himself in another work. On a c.1503-1504 worksheet from the Codex Windsor set of Leonardo's drawings are sketches of horses, believed to be part of a study for the painting of \"The Battle of Anghiari\". In the middle of the sheet is a note to \"make one of wax about finger long\", and the bucking posture of one of the horses is similar to the sculpture. Leonardo may indeed have used wax models to prepare for Anghiari. Art historian Patricia Trutty-Coohill also noticed a resemblance between the rider and Charles II d'Amboise from Andrea Solari's painting from c.1507. Charles was one of Leonardo's patrons, and the subject matter would suit what's known of him. The beeswax statuette, including black and white photographs, was first published as a work by Leonardo in 1987 in \"The drawings and miscellaneous papers of Leonardo da Vinci in the collection of Her Majesty the Queen at Windsor Castle\". The book was part of a series that Pedretti had taken over from Kenneth Clark, cataloging Leonardo's drawings stored in the Royal Collection in Windsor Castle.\n\nAs with many artworks attributed to Leonardo, not all art scholars agree with Pedretti's attribution.\nWhen exhibited at the Boston Museum of Science in 1997, the museum agreed to change the credit on the label of the sculpture from \"by Leonardo\" to \"attributed to Leonardo\", but art historian Jack Wasserman still insisted that nothing has survived to support the attribution.\nArt historians Pietro Marani and Franco Cardini, and art critic Vittorio Sgarbi, likewise doubted the sculpture's provenance when the bronze cast was exhibited in Milan in 2016, commenting that there still isn't adequate hard evidence to support the attribution of the work to Leonardo. Following his studies of medieval cavalry, Cardini also criticised the historical accuracy of the sculpture.\n\nIn 1987 art collector Richard A. Lewis acquired the 1985 latex mold. Beginning in 2012, Lewis and a team of experts \"pulled\" a wax from the latex mold and, using the lost wax process, cast the \"Horse and Rider\" sculpture in bronze.\n\nBronze castings were sold with a certificate of authenticity stating each to be one of 996 castings, which were available in three different applied patinas and in silver. They were at sale for $25,000–35,000 each. The plan to cast up to 1000 statues however never reached fruition. Some of the replicas have since been for sale with the option of joint ownership to the 2012 bronze cast.\n\nThe 2012 bronze casting was unveiled to the public in August 2012 at Grey Stone Mansion in Beverly Hills, California. It has been exhibited in Las Vegas, Dallas, San Diego, and in 2016 in Milan. In 2015 the mold made of Leonardo's beeswax model, together with the 2012 bronze casting, were acquired by another private collector. Since then, further bronze castings have been exhibited in New York, London,Miami, and Oregon.\n"}
{"id": "52350670", "url": "https://en.wikipedia.org/wiki?curid=52350670", "title": "Hyning Scout Wood", "text": "Hyning Scout Wood\n\nHyning Scout Wood is a wood between Yealand Conyers and Warton in Lancashire.\n\nIts features include limestone pavement and coppicing for charcoal. The trees include beech, larch, sweet chestnut and Scots pine. Its woodland plants include bluebells, dog's mercury, hart's-tongue fern and Solomon's Seal. Roe deer and both grey and red squirrels are found there. \n\nIt is managed by the Woodland Trust; is part of the Arnside and Silverdale Area of Outstanding Natural Beauty and is recognised as a Biological Heritage Site by the county.\n"}
{"id": "1759897", "url": "https://en.wikipedia.org/wiki?curid=1759897", "title": "Ice-nine", "text": "Ice-nine\n\nIce-nine is a fictional material that appears in Kurt Vonnegut's novel \"Cat's Cradle\". Ice-nine is described as a polymorph of water which instead of melting at 0 °C (32 °F), melts at 45.8 °C (114.4 °F). When ice-nine comes into contact with liquid water below 45.8 °C, it acts as a seed crystal and causes the solidification of the entire body of water, which quickly crystallizes as more ice-nine. As people are mostly water, ice-nine kills nearly instantly when ingested or brought into contact with soft tissues exposed to the bloodstream, such as the eyes or tongue.\n\nIn the story, it is invented by Dr. Felix Hoenikker and developed by the Manhattan Project in order for the Marines to no longer need to deal with mud. The project is abandoned when it becomes clear that any quantity of it would have the power to destroy all life on earth. A global catastrophe involving freezing the world's oceans with ice-nine is used as a plot device in Vonnegut's novel.\n\nVonnegut encountered the idea of ice-nine while working at General Electric. He attributes the idea of ice-nine to his brother Bernard, who was researching the formation of ice crystals in the atmosphere . A later account of the events attributes the idea to the chemist Irving Langmuir, who devised the concept while helping H.G. Wells conceive ideas for stories. Vonnegut decided to adapt the idea into a story after Langmuir's death in 1957.\n\n\n"}
{"id": "1024947", "url": "https://en.wikipedia.org/wiki?curid=1024947", "title": "J/psi meson", "text": "J/psi meson\n\nThe (J/psi) meson or psion is a subatomic particle, a flavor-neutral meson consisting of a charm quark and a charm antiquark. Mesons formed by a bound state of a charm quark and a charm anti-quark are generally known as \"charmonium\". The is the most common form of charmonium, due to its low rest mass. The has a rest mass of , just above that of the (), and a mean lifetime of . This lifetime was about a thousand times longer than expected.\n\nIts discovery was made independently by two research groups, one at the Stanford Linear Accelerator Center, headed by Burton Richter, and one at the Brookhaven National Laboratory, headed by Samuel Ting of MIT. They discovered they had actually found the same particle, and both announced their discoveries on 11 November 1974. The importance of this discovery is highlighted by the fact that the subsequent, rapid changes in high-energy physics at the time have become collectively known as the \"November Revolution\". Richter and Ting were rewarded for their shared discovery with the 1976 Nobel Prize in Physics.\n\nThe background to the discovery of the was both theoretical and experimental. In the 1960s, the first quark models of elementary particle physics were proposed, which said that protons, neutrons and all other baryons, and also all mesons, are made from three kinds of fractionally-charged particles, the \"quarks\", that come in three different types or \"flavors\", called \"up\", \"down\", and \"strange\". Despite the capability of quark models to bring order to the \"elementary particle zoo\", their status was considered something like mathematical fiction at the time, a simple artifact of deeper physical reasons.\n\nStarting in 1969, deep inelastic scattering experiments at SLAC revealed surprising experimental evidence for particles inside of protons. Whether these were quarks or something else was not known at first. Many experiments were needed to fully identify the properties of the subprotonic components. To a first approximation, they were indeed the already-described quarks.\n\nOn the theoretical front, gauge theories with broken symmetry became the first fully viable contenders for explaining the weak interaction after Gerardus 't Hooft discovered in 1971 how to calculate with them beyond tree level. The first experimental evidence for these electroweak unification theories was the discovery of the weak neutral current in 1973. Gauge theories with quarks became a viable contender for the strong interaction in 1973 when the concept of asymptotic freedom was identified.\n\nHowever, a naive mixture of electroweak theory and the quark model led to calculations about known decay modes that contradicted observation: in particular, it predicted Z boson-mediated \"flavor-changing\" decays of a strange quark into a down quark, which were not observed. A 1970 idea of Sheldon Glashow, John Iliopoulos, and Luciano Maiani, known as the GIM mechanism, showed that the flavor-changing decays would be strongly suppressed if there were a fourth quark, \"charm\", that paired with the strange quark. This work led, by the summer of 1974, to theoretical predictions of what a charm/anticharm meson would be like. These predictions were ignored. The work of Richter and Ting was done for other reasons, mostly to explore new energy regimes. In the Brookhaven group, Glenn Everhart, Terry Rhoades, Min Chen, and Ulrich Becker were the first to discern a peak at 3.1 GeV in plots of production rates. This was the first recognition of the \"J\".\n\nBecause of the nearly simultaneous discovery, the is the only particle to have a two-letter name. Richter named it \"SP\", after the SPEAR accelerator used at SLAC; however, none of his coworkers liked that name. After consulting with Greek-born Leo Resvanis to see which Greek letters were still available, and rejecting \"iota\" because its name implies insignificance, Richter chose \"psi\"a name which, as Gerson Goldhaber pointed out, contains the original name \"SP\", but in reverse order. Coincidentally, later spark chamber pictures often resembled the psi shape. Ting assigned the name \"J\" to it, which is one letter away from \"K\", the name of the already-known strange meson; possibly by coincidence, \"J\" strongly resembles the Chinese character for Ting's name (丁). J is also the first letter of Ting's oldest daughter's name, Jeanne.\n\nSince the scientific community considered it unjust to give one of the two discoverers priority, most subsequent publications have referred to the particle as the \"\".\n\nThe first excited state of the was called the ψ′; it is now called the ψ(2S), indicating its quantum state. The next excited state was called the ψ″; it is now called ψ(3770), indicating mass in MeV. Other vector charm-anticharm states are denoted similarly with ψ and the quantum state (if known) or the mass. The \"J\" is not used, since Richter's group alone first found excited states.\n\nThe name \"charmonium\" is used for the and other charm-anticharm bound states. This is by analogy with positronium, which also consists of a particle and its antiparticle (an electron and positron in the case of positronium).\n\nIn a hot QCD medium, when the temperature is raised well beyond the Hagedorn temperature, the and its excitations are expected to melt. This is one of the predicted signals of the formation of the quark–gluon plasma. Heavy-ion experiments at CERN's Super Proton Synchrotron and at BNL's Relativistic Heavy Ion Collider have studied this phenomenon without a conclusive outcome as of 2009. This is due to the requirement that the disappearance of mesons is evaluated with respect to the baseline provided by the total production of all charm quark-containing subatomic particles, and because it is widely expected that some are produced and/or destroyed at time of QGP hadronization. Thus, there is uncertainty in the prevailing conditions at the initial collisions.\n\nIn fact, instead of suppression, enhanced production of is expected in heavy ion experiments at LHC where the quark-combinant production mechanism should be dominant given the large abundance of charm quarks in the QGP. Aside of , charmed B mesons (), offer a signature that indicates that quarks move freely and bind at-will when combining to form hadrons.\n\nHadronic decay modes of are strongly suppressed because of the OZI Rule. This effect strongly increases the lifetime of the particle and thereby gives it its very narrow decay width of just . Because of this strong suppression, electromagnetic decays begin to compete with hadronic decays. This is why the has a significant branching fraction to leptons.\n\n\n"}
{"id": "19434700", "url": "https://en.wikipedia.org/wiki?curid=19434700", "title": "Lipid therapy", "text": "Lipid therapy\n\nLipid therapy, fat therapy, or therapeutic lipovenous injections is a controversial medical technique that entails the injection and expulsion of fats and lipids, which proponents claim can improve cognitive and memory function. The technique recently gained notoriety, when it was revealed it had become a popular technique with French and Italian celebrities, and more recently a rumored handful of American celebrities.\n\nPatients who choose to undergo the controversial therapy are injected once a week for two months with a high-density animal fat.\n\nSince most toxins in the body are fat soluble (according to practitioners), this causes the patient to defecate a translucent slime, which is claimed to carry out the body's toxins, in turn boosting energy, increasing concentration, and improving memory. Many medical experts who oppose the technique have pointed out the slimy discharge how the body deals with any excess of intestinal and intravenous lipids, and it is a sign of high risk of artery damage and poor water absorption. \n\nsorting of any therapy as effective treatment for any condition is heavily restricted by law in many jurisdictions unless all such claims are scientifically validated. In the United States, for example, U.S. Food and Drug Administration regulations prohibit marketing any lipid therapy using medical claims, as such claims are unfounded.\n\nAdditionally, clinicians and doctors who perform the therapy have drawn scrutiny from the Department of Agriculture for questionable documentation on their acquisition of the animal fat.\n\nBlinding of patients and assessors to the therapy is difficult since lipid therapy can be easily discerned by the painful pressure in the thoracic cavity engendered by the sudden dip in plasma viscosity. Globules of lipid polymers also produce a significant shear strain on capillaries, causing the skin to pale and the tongue to swell.\n\nHowever, many patients claim to feel full of energy and stamina after undergoing therapy. Medical scientists are quick to point out this side effect is likely the result of rapid lipolysis, as free fatty acids are freed from glycerol, and forcibly diffused into blood and muscle fiber due to artificially elevated blood pressure.\n\n\n"}
{"id": "1659702", "url": "https://en.wikipedia.org/wiki?curid=1659702", "title": "Mammea americana", "text": "Mammea americana\n\nMammea americana, commonly known as mammee, mammee apple, mamey, mamey apple, Santo Domingo apricot, tropical apricot, or South American apricot, is an evergreen tree of the family Calophyllaceae, whose fruit is edible. It has also been classified as belonging to the family Guttiferae , which would make it a relative of the mangosteen.\n\nIn certain Latin American countries, \"Mammea americana\" is referred to as \"yellow mamey\" () in order to distinguish it from the unrelated but similar looking \"Pouteria sapota\", whose fruit is usually called \"red mamey\" ( or ).\n\nThe mammee tree is – high and is similar in appearance to the southern magnolia \"(Magnolia grandiflora)\". Its trunk is short and reaches - in diameter. The tree's upright branches form an oval head. Its dark-green foliage is quite dense, with opposite, leathery, elliptic leaves. The leaves can reach wide and twice as long.\n\nThe mammee flower is fragrant, has 4 or 6 white petals, and reaches – wide when fully blossomed. The flowers are borne either singly or in clusters of two or three, on short stalks. There can be, in a single flower, pistils, stamens or both, so there can be male, female or hermaphrodite flowers on one tree.\n\nThe mammee apple is a berry, though it is often misinterpreted to be a drupe. It is round or slightly irregular, with a brown or grey-brown thick rind. In fact, the rind consists of the exocarp and mesocarp of the fruit, while the pulp is formed from the endocarp. The stem is thick and short. The mammee apple has more or less visible floral remnant at the apex.\n\nMammee apples' diameter ranges from to . When unripe, the fruit is hard and heavy, but its flesh slightly softens when fully ripe. Beneath the skin, there is a white, dry membrane, whose taste is astringent, that adheres to the flesh. The flesh is orange or yellow, not fibrous, and can have various textures (crispy or juicy, firm or tender). Generally, the flesh smell is pleasant and appetizing.\n\nSmall fruits contain a single seed, while larger ones might have up to four. The seeds are brown, rough, oval and around long. The juice of the seed leaves an indelible stain.\n\nPropagation can be done by seed. Germination takes place from 60–260 days. Grafting is the preferred method of propagation.\n\nThe tree comes from tropical South America. In 1529, it was included by Oviedo in his \"Review of the Fruits of the New World\". It was then introduced to various regions in the Old World: West Africa, particularly Sierra Leone, Zanzibar, Southeast Asia and Hawaii. In the United States, the species is uniquely found in Hawaii and Florida. In the latter state, mammee apples were probably introduced from the Bahamas.\n\nThe mammea apple tree is confined to tropical or subtropical climates. In Central America, the species is found to grow up to an altitude of 1,000 m. It thrives best in rich, deep and well-drained soil, but is very adaptive; it also grows on limestone in Jamaica, in the oolithic limestone of the Bahamas, and on ancient coral bedrock in Barbados as well as coral cays off the coast of Florida.\n\nThe tree is very sensitive to low temperatures, but seems remarkably resistant to pests and diseases.\n\nIn Trinidad & Tobago, the grated seeds are mixed with rum or coconut oil to treat head lice and chiggers.\n\nUnderripe fruits are rich in pectin, and the tree bark is high in tannin.\n\nThough edible, this fruit has received little attention worldwide.\n\nThe raw flesh can be served in fruit salads, or with wine, sugar or cream, especially in Jamaica. In the Bahamas, the flesh is first put in salted water to remove its bitterness, before cooking it with much sugar to make a sort of jam. The flesh can also be consumed stewed.\n\nIn the French West Indies, an aromatic liqueur, ', or ', is distilled from the mammee flowers. This liqueur is believed to be tonic or digestive.\n\nVarious parts of the tree contain insecticidal substances, especially the seed kernel. In Puerto Rico, mammee leaves are wrapped around young tomato plants to keep mole crickets and cutworms away. In a similar way, the bark gum is melted with fat in Jamaica and Mexico, then applied to feet to repel chiggers or fleas on animals. The same effect is also obtained from infusions of half-ripe fruits.\n\nIn the Virgin Islands, the tannin from the bark is used to tan leather. The mammee timber is heavy and hard, yet easy to work; it has received, however, only limited commercial interest.\n\n"}
{"id": "38737042", "url": "https://en.wikipedia.org/wiki?curid=38737042", "title": "March 2013 nor'easter", "text": "March 2013 nor'easter\n\nThe March 2013 nor'easter was a powerful nor'easter that affected much of the United States, most notably New England. On March 6, the system moved into the Mid-Atlantic region of the east coast, and intensified into a nor'easter, dumping up to 3 feet of snow in some places. By late March 6, 2013, the nor'easter had knocked out power to about 250,000 homes and businesses.\n\nJust after the February winter storm, an extratropical cyclone developed on early March 1, 2013, in the Gulf of Alaska moved ashore in British Columbia. After moving ashore, the storm weakened as it moved into the Western United States. Over the next few days, the storm dumped large amounts of snow across the Western States and the Great Plains, affecting areas that were already impacted by a previous blizzard. The storm quickly moved eastwards while slowly restrengthening. On March 4, the storm developed a severe side over the southern states, after tapping into moisture coming from the Gulf of Mexico. On March 5, the storm developed into a blizzard, while it was over the midsection of the Eastern United States, adding more snow to areas that had already received record snowfall from previous major winter storms. The blizzard conditions resulted in multiple flight cancellations and traffic problems across the Eastern States. A maximum amount of 36 inches of snow was recorded near the Bear Paw Ski Bowl in Montana. On March 6, the system moved off the coast of Virginia, and intensified into a nor'easter. The storm curved towards the northeast, and began impacting New England. Light precipitation began as early as the morning of Wednesday, March 6. This precipitation started as drizzle but changed over to light snow showers as the day progressed. On the morning of Thursday, March 7, heavy snow began in parts of Connecticut, followed by a lull. This period of snow led to many 90 minute delays for Connecticut school districts. On Thursday, March 7, the nor'easter slowly began pulling away from the east coast as it moved northeastward. However, the storm continued to intensify, and brought heavy snow and powerful winds to parts of New England beginning Thursday night. From late March 7 to March 8, the nor'easter absorbed a smaller system coming in from the west, adding much more moisture to the storm, which resulted in heavy snowfall across parts of New England. Heavy snow in southern New England continued from Thursday night through much of the day on Friday. Over 2 days, the storm had dumped 10+ inches of snow across 11 states from Montana to Massachusetts. This added more snow to an already record-breaking snowfall season, which was brought on by multiple previous winter storms from February. Early on March 8, the nor'easter stalled off the coast of northern New England while maintaining its intensity, due to a blocking ridge of high pressure over Newfoundland. Later on March 8, the nor'easter began affecting southern Newfoundland, while continuing to intensify. At this time, the nor'easter reached its peak intensity of 986 millibars. On March 9, the ridge of high pressure over Newfoundland weakened enough for the nor'easter to begin moving out to sea. Up to 29.8 inches of snow were recorded in Milton, Massachusetts, by the end of the storm's snowfall on March 9. The nor'easter slowly weakened while moving eastwards, and its winds were still felt for several hours on March 9 after the snowfall had ended. By March 10, the nor'easter had completely left the east coast.\n\nOn March 11, the nor'easter reached the middle of the North Atlantic Ocean and continued weakening, as it slowly moved eastwards. On the same day, the nor'easter lost its frontal boundary and became an Upper-level low, and its eye began to shrink. On March 12, the nor'easter stalled, and began absorbing moisture coming from the tropics, and the storm lost its eye. On March 13, the system began losing its organization, and spawned a new frontal low to the north, which brought thunderstorms and strong winds to the Azores Islands. The storm slowly began moving eastward, as it continued weakening steadily. On March 14, The storm began to accelerate towards the northeast and became an extratropical storm again. Later on the same day, the system slightly reintensified and absorbed the new low, while spawning a few small circulations around the edge of the storm. The storm brought rain to Madeira, and brought cloudy weather to the Canary Islands. On March 15, the nor'easter began to rapidly weaken, and lost a lot of moisture, as it continued accelerating towards Western Europe. On March 16, the system began interacting with a much more powerful storm complex situated over the United Kingdom, and brought thunderstorms to parts of Western Europe. On March 17, the system rapidly became disorganized as it began being absorbed by the larger storm complex. During the next several days, the storm system continued to move eastward, while slowly degenerating. On March 21, the system was completely absorbed by the larger storm complex, while located over the northern Adriatic Sea.\n\nVirginia's Governor Bob McDonnell declared a state of emergency and about 100 National Guard soldiers for snow duty. The National Weather Service issued coastal flood warnings for parts of Massachusetts, New Jersey, New York, Delaware, Maryland and Virginia. About 300 National Guard troops will be used along the Massachusetts coast to help with flooding and possible evacuations.\n\nThe Chesapeake Bay Bridge was closed in both directions during the day Wednesday due to wind gusts of up to 60 mph. More than 1,900 flights were canceled March 6, raising to almost 4,100 the number of flights that have been canceled since the storm began according to flight tracker FlightStats. The storm dropped up to 10 inches of snow in the Chicago area on March 5. In the evening of March 6, Hatteras, North Carolina reported 3 feet of water on Highway 12 with the road impassable. Despite a forecast of 5-10 inches of snow, most of the Washington DC metro area was spared. On March 5, the federal government closed offices in Washington, D.C.\nThe White House canceled a planned celebration for the Alabama Crimson Tide and Congress called off several hearings. More than 954,000 students who attend major school districts in Washington, Virginia, Maryland and Ohio got the day off.\nAmtrak shut down some trains in Washington, Virginia, West Virginia and New York. Some snow totals from March 6 according to AccuWeather.com were: Frostburg, Maryland: 12.5\"; New Kensington, Pennsylvania: 12.0\"; Charlottesville, Virginia: 14.5\"; Worcester, Massachusetts 20.8 inches; Auburn, Massachusetts 19 inches; Randolph, Massachusetts:24.1 inches and Franklin, West Virginia: 24.0\". More than 440 flights were canceled on March 7, according to FlightAware.com. New York City was expected to get up to 3 inches of snow by the morning of March 8 with some higher total amounts possible on eastern Long Island. Boston was expected to see 2 to 4 inches, with higher totals where a winter storm warning was issued from Worcester to just north of Providence. Winter warnings were posted in seven Northeast states with most snowfall to occur on March 8. On March 7, the NWS in Boston recorded 10.2\" of snow, breaking the single-day mark set in 1941. The city ended with 13.1 inches of snow. The Blue Hill Meteorological Observatory in Massachusetts measured 29.8 inches of snow over a 36-hour period. Because the storm's wind area was so large, rough surf and rip currents were felt all the way southward towards the eastern coast of Florida over the weekend of March 9 and 10.\n\nThe snowfall totals in many areas of southern New England vastly exceeded what was expected from forecasts, which meant that many people were unprepared for a large storm. Most television meteorologists in Boston had predicted only a few inches of snow and mostly rain, but this forecast failed miserably. Many towns surrounding Boston received around two feet of snow, which greatly surprised residents. Boston schools were left in session as the height of the storm occurred since they did not know the gravity of the situation.\n\nMontana\n\nWyoming\n\nColorado\n\nNorth Dakota\n\nSouth Dakota\n\nMinnesota\n\nIowa\n\nWisconsin\n\nIllinois\n\nMichigan\n\nIndiana\n\nOhio\n\nKentucky\n\nPennsylvania\n\nMaryland\n\nWashington, D.C.\n\nVirginia\n\nWest Virginia\n\nNorth Carolina\n\nTennessee\n\nGeorgia\n\nConnecticut\n\nRhode Island\n\nMassachusetts\n\nNew Hampshire\n\nWeather-related traffic accidents account for at least 8 deaths, 5 in the Midwest and 3 in Virginia. The 67-foot fishing vessel, Seafarer, capsized on March 6, 2013 about 15 miles east of Assateague Island with 3 men on board. Two men remain missing after the Coast Guard called off the search on March 7, 2013.\n\n"}
{"id": "45461769", "url": "https://en.wikipedia.org/wiki?curid=45461769", "title": "Matched precipitation rate", "text": "Matched precipitation rate\n\nIn the irrigation industry, matched precipitation rate (MPR) is a term that is used to calculate the amount of precipitation in a given area is uniform. In order to be \"matched\" all sprinkler heads in a given zone must have the same rate of precipitation. This can be achieved by matching the gallonage of a standard rotor to its arc and reducing range accordingly(ie. 2 gallons at 90 degrees, 4 gallons at 180 degrees, or 8 gallons if the head does a full circle) or by using MPR nozzles or sprinklers. Virtually all major sprinkler manufactures offer some type of MPR nozzle or sprinkler. The primary benefit and main goal of MPR sprinklers is to prevent water waste from run-off while still ensuring that the area receives adequate moisture.\n\nThe mathematical formula for determining precipitation rate is (GPM x 96.3 ÷ area) = PR\n\nThe first MPR nozzles, the MP Rotator, were produced by Nelson Irrigation in 2003, and acquired by Hunter Industries in 2007. Since then the popularity of these highly uniform yet low application sprinklers has caused every major sprinkler manufacturer to produce their own version of an MPR sprinkler, including Rain Bird and Toro.\n"}
{"id": "21291", "url": "https://en.wikipedia.org/wiki?curid=21291", "title": "Nail (fastener)", "text": "Nail (fastener)\n\nIn woodworking and construction, a nail is a pin-shaped object of metal (or wood, called a tree nail or \"trunnel\") which is used as a fastener, as a peg to hang something, or sometimes as a decoration. Generally, nails have a sharp point on one end and a flattened head on the other, but headless nails are available. Nails are made in a great variety of forms for specialized purposes. The most common is a \"wire nail\". Other types of nails include \"pins\", \"tacks\", \"brads\", \"spikes\", and \"cleats.\"\n\nNails are typically driven into the workpiece by a hammer, a pneumatic nail gun, or a small explosive charge or primer. A nail holds materials together by friction in the axial direction and shear strength laterally. The point of the nail is also sometimes bent over or \"clinched\" after driving to prevent pulling out.\n\nThe history of the nail is divided roughly into three distinct periods: \n\nThe first nails were made of wrought-iron. Nails date back at least to Ancient Egypt — bronze nails found in Egypt have been dated 3400 BC. The Bible provides a number of references to nails, including the story in Judges of Jael the wife of Heber, who drives a nail (or tent-peg) into the temple of a sleeping Canaanite commander; the provision of iron for nails by King David for what would become Solomon's Temple; and in connection with the crucifixion of Christ.\n\nThe Romans made extensive use of nails. The Roman army, for example, left behind seven tons of nails when it evacuated the fortress of Inchtuthil in Perthshire in the United Kingdom in 86 to 87 CE.\n\nThe term \"penny\", as it refers to nails, probably originated in medieval England to describe the price of a hundred nails. Nails themselves were sufficiently valuable and standardized to be used as an informal medium of exchange.\nUntil around 1800 artisans known as \"nailers\" or \"nailors\" made nails by hand – note the surname Naylor.\n\nAt the time of the American Revolution, England was the largest manufacturer of nails in the world. Nails were expensive and difficult to obtain in the American colonies, so that abandoned houses were sometimes deliberately burned down to allow recovery of used nails from the ashes. This became such a problem in Virginia that a law was created to stop people from burning their houses when they moved. Families often had small nail-manufacturing setups in their homes; during bad weather and at night, the entire family might work at making nails for their own use and for barter. Thomas Jefferson wrote in a letter: \"In our private pursuits it is a great advantage that every honest employment is deemed honorable. I am myself a nail maker.\" The growth of the trade in the American colonies was theoretically held back by the prohibition of new slitting mills in America by the Iron Act of 1750, though there is no evidence that the Act was actually enforced.\n\nThe production of wrought-iron nails continued well into the 19th century, but ultimately was reduced to nails for purposes for which the softer cut nails were unsuitable, including horseshoe nails.\n\nThe slitting mill, introduced to England in 1590, simplified the production of nail rods, but the real first efforts to mechanise the nail-making process itself occurred between 1790 and 1820, initially in the United States and England, when various machines were invented to automate and speed up the process of making nails from bars of wrought iron. These nails were known as \"cut nails\" or \"square nails\" because of their roughly rectangular cross section. Cut nails were one of the important factors in the increase in balloon framing beginning in the 1830s and thus the decline of timber framing with wooden joints. Though still used for historical renovations, and for heavy-duty applications, such as attaching boards to masonry walls, \"cut nails\" are much less common today than \"wire nails\".\n\nThe cut-nail process was patented in America by Jacob Perkins in 1795 and in England by Joseph Dyer, who set up machinery in Birmingham. The process was designed to cut nails from sheets of iron, while making sure that the fibres of the iron ran down the nails. The Birmingham industry expanded in the following decades, and reached its greatest extent in the 1860s, after which it declined due to competition from wire nails, but continued until the outbreak of World War I.\n\nAs the name implies, wire nails are formed from wire. Usually coils of wire are drawn through a series of dies to reach a specific diameter, then cut into short rods that are then formed into nails. The nail tip is usually cut by a blade; the head is formed by reshaping the other end of the rod under high pressure. Other dies are used to cut grooves and ridges. Wire nails were also known as \"French nails\" for their country of origin. Belgian wire nails began to compete in England in 1863. Joseph Henry Nettlefold was making wire nails at Smethwick by 1875. Over the following decades, the nail-making process was almost completely automated. Eventually the industry had machines capable of quickly producing huge numbers of inexpensive nails with little or no human intervention.\n\nWith the introduction of cheap wire nails, the use of wrought iron for nail making quickly declined, as more slowly did the production of cut nails. In the United States, in 1892 more steel-wire nails were produced than cut nails. In 1913, 90% of manufactured nails were wire nails. Nails went from being rare and precious to being a cheap mass-produced commodity. Today almost all nails are manufactured from wire, but the term \"wire nail\" has come to refer to smaller nails, often available in a wider, more precise range of gauges than is typical for larger common and finish nails.\n\nFormerly made of bronze or wrought iron, today's nails are typically made of steel, often dipped or coated to prevent corrosion in harsh conditions or to improve adhesion. Ordinary nails for wood are usually of a soft, low-carbon or \"mild\" steel (about 0.1% carbon, the rest iron and perhaps a trace of silicon or manganese). Nails for concrete are harder, with 0.5–0.75% carbon.\n\nTypes of nail include:\n\nMost countries, except the United States, use a metric system for describing nail sizes. A \"50 × 3.0\" indicates a nail 50 mm long (not including the head) and 3 mm in diameter. Lengths are rounded to the nearest millimetre.\n\nFor example, finishing nail* sizes typically available from German suppliers are:\n\nIn the United States, the length of a nail is designated by its penny size.\n\n\nNails have been used in art, such as the Nail Men—a form of fundraising common in Germany and Austria during World War I.\n\nBefore the 1850s bocce and pétanque boules were wooden balls, sometimes partially reinforced with hand-forged nails. When cheap, plentiful machine-made nails became available, manufacturers began to produce the \"boule cloutée\"—a wooden core studded with nails to create an all-metal surface. Nails of different metals and colors (steel, brass, and copper) were used to create a wide variety of designs and patterns. Some of the old \"boules cloutées\" are genuine works of art and valued collector's items.\n\nOnce nails became cheap and widely available, they were often used in folk art and outsider art as a method of decorating a surface with metallic studs. Another common artistic use is the construction of sculpture from welded or brazed nails. string art\n\n\n"}
{"id": "2322476", "url": "https://en.wikipedia.org/wiki?curid=2322476", "title": "Portable Sanitation Association International", "text": "Portable Sanitation Association International\n\nThe Portable Sanitation Association International (PSAI) is an international trade association dedicated to expanding and improving portable sanitation services and facilities worldwide. It has its headquarters in Bloomington, Minnesota, United States. PSAI's first meeting was held in New Orleans in 1971.\n\nThrough their efforts, PSAI hopes to have the environment become a cleaner and safer place, and is devoted to the proper handling of human waste.\n"}
{"id": "12633123", "url": "https://en.wikipedia.org/wiki?curid=12633123", "title": "Prestige oil spill", "text": "Prestige oil spill\n\nThe Prestige oil spill occurred off the coast of Galicia, Spain, caused by the sinking of the 26 year old structurally deficient oil tanker in November 2002, carrying 77,000 tonnes of heavy fuel oil. During a storm, it burst a tank on November 13, and sank on November 19, 2002, about from the coast of Galicia. It is estimated that it spilled (or a mass of 60,000 metric tonnes) of heavy fuel oil.\nThe spill polluted thousands of kilometers of coastline and more than one thousand beaches on the Spanish, French and Portuguese coast, as well as causing great harm to the local fishing industry. The spill is the largest environmental disaster in the history of both Spain and Portugal. The amount of oil spilled was more than the \"Exxon Valdez\" incident and the toxicity considered higher, because of the higher water temperatures.\n\nIn 2007 the Southern District of New York dismissed a 2003 lawsuit by the Kingdom of Spain against the American Bureau of Shipping, the international classification society which had certified the \"Prestige\" as in compliance with rules and laws, because ABS was a \"person\" per the International Convention on Civil Liability for Oil Pollution Damage and exempt from direct liability for pollution damage. The 2012 trial of the Galicia regional High Court did not find the merchant shipping company, nor the insurer, the London P&I Club nor any Spanish government official, but only the Captain of the ship guilty and gave him a nine-month suspended sentence for disobedience. By November 2017, the London P&I Club was fined to pay $1 billion.\n\nThe \"Prestige\" was a 26-year-old Greek-operated, single-hulled oil tanker, officially registered in the Bahamas, but with a Liberian-registered single-purpose corporation as the owner.\n\nThe ship had a deadweight tonnage, or carrying capacity, of approximately 81,000 tons, a measurement that put it at the small end of the Aframax class of tankers, smaller than most carriers of crude oil but larger than most carriers of refined products. It was classed by the American Bureau of Shipping and insured by the London P&I Club, a shipowners' mutual known as the London Club.\n\nOn November 13, 2002, the \"Prestige\" was carrying 77,000 metric tons of two different grades of heavy fuel oil, crude #4. It encountered a winter storm off Costa de la Muerte, the Coast of Death, in Galicia northwestern Spain. The Greek captain, Apostolos Mangouras, reported a loud bang from the starboard side and as the ship began to take on water from high waves the engines shut down and he called for help from Spanish rescue workers. The Filipino crew was evacuated with rescue helicopters and the ship drifted within four miles of the Spanish coast already leaking oil. A veteran captain, Serafin Diaz, was lowered onto the ship per the Spanish governments Industry Ministry, to navigate the ship off the Spanish coast northwest, and saw the gaping 50-foot hole on the starboard side. Mangouras argued the ship should be brought into port where the leaking oil might be confined but under the threat of the Spanish navy Mangouras relented. After pressure from the French government, the vessel was also forced to change its course and head south into Portuguese waters in order to avoid endangering France's southern coast. Fearing for its own shore, the Portuguese authorities ordered its navy to intercept the ailing vessel and prevent it from approaching further.\n\nWith the French, Spanish, and Portuguese governments refusing to allow the ship to dock, and after several days of sailing and towing, it split in half on November 19, 2002. It sank only about 250 kilometers or 130 miles from the Spanish coast, releasing over of oil into the sea. An earlier oil slick had already reached the coast. The captain of the \"Prestige\" was taken into custody, accused of not cooperating with marine salvage crews and of harming the environment.\n\nThe \"Prestige\" oil spill is Spain's worst ecological disaster. After the sinking, the wreck continued to leak approximately 125 tons of oil a day, polluting the seabed and contaminating the coastline, especially along the territory of Galicia. The environmental damage was most severe on the coast of Galicia. The affected area is an important ecological region, supporting coral reefs and many species of sharks and birds, and the fishing industry. The heavy coastal pollution forced the region's government to suspend offshore fishing for six months.\n\nInitially, the government thought just 17,000 tons of the tanker's 77,000 tons of oil had been lost, and that the remaining 60,000 tons would freeze and not leak from the sunken tanker. In early 2003, it announced that half of the oil had been lost. As of August 2003, the figure had risen to about 63,000 tons, more than eighty percent of the tanker's 77,000 tons of fuel oil have been spilled off Spain's north-west coast.\n\nIn March 2006, new oil slicks were detected near the wreck of the \"Prestige\", slicks which investigators found to match the type of oil the \"Prestige\" carried. A study released in December 2006 led by José Luis De Pablos, a physicist at Madrid's Center for Energetic and Environmental Research, concluded that 16,000 to 23,000 tons of oil remained in the wreck, as opposed to the 700 to 1300 tons claimed by the Spanish government; that bioremediation of the remaining oil failed; and that bacteria corroding the hull could soon produce a rupture and quickly release much of the remaining oil and create another catastrophic spill. The report urged the government to take \"prompt\" action.\n\nExperts predicted marine life could suffer from the pollution for at least ten years due to the type of oil spill, which contained light fractions with polyaromatic hydrocarbons and could poison plankton, fish eggs and crustaceans with carcinogenic effects in fish and potentially humans as well.\n\nIn the subsequent months, thousands of volunteers joined the public company TRAGSA (the firm chosen by the regional government to deal with the cleanup) to help clean the affected coastline. The massive cleaning campaign was a success, recovering most portions of coastline from not only the effects of the oil spill but also the accumulated \"regular\" contamination. Galician activists founded the environmental movement \"Nunca Máis\" (Galician for Never Again), to denounce the passiveness of the conservative government regarding the disaster. A year after the spill, Galicia had more Blue Flags for its beaches (an award for those beaches with the highest standards in the European Union) than in the previous years.\n\nIn 2004, remotely operated vehicles (ROVs) like the one which originally explored the wreck of the RMS \"Titanic\" drilled small holes in the wreck and removed the remaining 13,000 m³ of cargo oil from the wreck, at 4000 meters below the sea surface. The ROVs also sealed cracks in the tanker's hull, and slowed the leakage to a trickle of 20 litres a day. In total, of oil were spilled. The oil was then pumped into large aluminium shuttles, specially manufactured for this salvage operation. The filled shuttles were then floated to the surface. The original plan to fill large bags with the oil proved to be too problematic and slow. After the oil removal was completed, a slurry rich in microbiologic agents was pumped in the hold to speed up the breakdown of any remaining oil. The total estimated cost of the operation was over €100 million.\n\nThe massive environmental and financial costs of the spill resulted in an inquiry into how a structurally deficient ship was able to travel out to sea. The \"Prestige\" had set sail from St. Petersburg, Russia, without being properly inspected. It traveled to the Atlantic via the shallow and vulnerable Baltic Sea. A previous captain in St. Petersburg, Esfraitos Kostazos, who complained to the owners about numerous structural deficiencies within the ship was rebuffed, later resigned in protest, and rather than repairing the defects, he was replaced with Mangouras.\n\nThe ownership of the \"Prestige\" was unclear, making it difficult to determine exactly who was responsible for the oil spill and exposing the difficulties in regulations posed by flags of convenience.\n\nSpanish investigators found that the failure in the hull of the \"Prestige\" had been predicted already: her two sister ships, \"Alexandros\" and \"Centaur\", had been submitted to extensive inspections under the \"Safe Hull\" program in 1996. The organization in charge of the inspections, the American Bureau of Shipping, found that both \"Alexandros\" and \"Centaur\" were in terminal decline. Due to metal fatigue in their hulls, modeling predicted that both ships would fail between frames 61 and 71 within five years. \"Alexandros\", \"Centaur\" and a third sister-ship, \"Apanemo\", were scrapped between 1999 and 2002. Little more than five years after the inspection, \"Prestige\"<nowiki>'</nowiki>s hull failed between frames 61 and 71.\n\nThe Spanish government was criticized for its decision to tow the ailing wreck out to sea — where it split in two — rather than in to a port. The refusal to allow the ship to take refuge in a sheltered port has been called a major contributing factor to the scale of the disaster. World Wildlife Fund's senior policy officer for shipping Simon Walmsley believed most of the blame lay with the classification society. \"It was reported as being substandard at one of the ports it visited before Spain. The whole inspection regime needs to be revamped and double-hulled tankers used instead,\" he said. The US and most other countries were planning to phase out single-hulled tankers by 2012.\n\nA report by the Galicia-based Barrie de la Maza economic institute in 2003 criticised the Spanish government's handling of the catastrophe. It estimated the cost of the clean-up to the Galician coast alone at €2.5 billion.\n\nThe 2013 court ruling put the cost of the disaster at 368 million euros ($494 million) to the Spanish state, 145 million euros to the Spanish region of Galicia and 68 million euros to France. The clean-up of the \"Exxon Valdez\" cost US$3 billion (almost €2.2 billion).\n\nSince the disaster, oil tankers similar to the \"Prestige\" have been directed away from the French and Spanish coastlines. The European Commissioner for Transport at the time, Spaniard Loyola de Palacio, pushed for the ban of single-hulled tankers.\n\nThe immediate legal consequence of the disaster was the arrest of the captain, Captain Mangouras. Captain Mangouras sought refuge for his seriously damaged vessel in a Spanish port. The acceptance of such a request has deep historic roots. Spain refused and launched the criminal charges against Mangouras that he refused to comply immediately with the Spanish demand to restart the engines and steam offshore. Bringing the ship into port and booming around her to contain the leaking oil would have been less harmful than sending her back to sea and almost inevitable sinking.\n\nIn May 2003, the Kingdom of Spain brought civil suit in the Southern District of New York against the American Bureau of Shipping (ABS), the Houston-based international classification society which had certified the \"Prestige\" as \"in class\" for its final voyage. The \"in class\" status states that the vessel is in compliance with all applicable rules and laws, not that it is or is not safe. For the world maritime industry, a key issue raised by the incident was whether classification societies could be held responsible for the consequences. International maritime trade publications including TradeWinds, Fairplay and Lloyd's List regularly presented the dispute as a possibly precedent-setting one which could prove fateful for international classification societies, whose assets are dwarfed by the scale of claims to which they could become subject. On 2 January 2007, the docket in that lawsuit (SDNY 03-cv-03573) was dismissed. The presiding judge ruled that ABS is a \"person\" as defined by the International Convention on Civil Liability for Oil Pollution Damage (CLC) and, as such, is exempt from direct liability for pollution damage. Additionally, the Judge ruled that, since the United States was not a signatory to the International CLC, the US Courts lack the necessary jurisdiction to adjudicate the case. Spain's original damage claim against ABS was some $700 million.\n\nThe Galicia regional High Court set the \"Prestige\" oil spill trial date for October 16, 2012, against officers, the insurer London Club, International Fund for Compensation for Oil Pollution Damage, the ship's owner, Liberia-based Mare Shipping Inc with its director general. The harbor master of A Coruña at the time, Ángel del Real, and a Galician government delegate, Arsenio Fernández de Mesa, had also been charged with \"aggravating the disaster by ignoring technical advice\". the hearing began on 16 June 2012 and expected to adjourn in November, the tenth anniversary of the disaster. The trial was held in a specially constructed courtroom in A Coruña’s exhibition complex. It considered evidence from 133 witnesses and 98 experts. Plaintiffs asked the Greek captain to be sentenced up 12 years, and demanded more than 4 billion euros ($5.0 billion) in damages overall.\n\nIn November 2013, the three Galicia High Court judges concluded, it was impossible to establish criminal responsibility, and Captain Apostolos Mangouras, Chief Engineer Nikolaos Argyropoulos and the former head of Spain's Merchant Navy, Jose Luis Lopez, were found not guilty of crimes against the environment. The captain was however accused of disobeying government authorities who wanted the tanker as far from the coast as possible. According to the court, that decision was correct and Mangouras, 78 at the time, was found guilty of disobedience and given a nine-month suspended sentence. The Spanish government decided to launch an appeal to the ruling against the exemption from civil liability of the captain.\n\nOn 26 January 2016, Spain’s Supreme Court convicted Captain Mangouras of recklessness resulting in catastrophic environmental damage, and sentenced him to two years in prison. On 15 November 2017, London Club was ordered to pay a $1 billion fine over the oil spill.\n\n"}
{"id": "2748153", "url": "https://en.wikipedia.org/wiki?curid=2748153", "title": "Rust and oxidation lubricant", "text": "Rust and oxidation lubricant\n\nRust and oxidation lubricant (R&O) is a mild lubricant treated with corrosion inhibitors. The oil is used mainly for bearing and lightly loaded gearbox applications.\n\nTurbine oil is classiflied as R&O but the quality is much higher.\n"}
{"id": "3329332", "url": "https://en.wikipedia.org/wiki?curid=3329332", "title": "Sculpture in the Environment", "text": "Sculpture in the Environment\n\nSculpture in the Environment (SITE) is an architecture and environmental design organisation, founded in 1970, and located in the Wall Street area of New York City. The firm works to unite building design with visual art, landscape, and green technology.\n\nIn the 1970s, Best Products contracted with James Wines’ Sculpture in the Environment (SITE) architecture firm to design seven highly unorthodox retail facilities, notably a tongue-in-cheek structure in Houston, Texas with a severely distressed facade. This building purportedly “appeared in more books on 20th century architecture than photographs of any other modern structure.”\n\n\n"}
{"id": "233234", "url": "https://en.wikipedia.org/wiki?curid=233234", "title": "Silica gel", "text": "Silica gel\n\nSilica gel is an amorphous and porous form of silicon dioxide (silica), consisting of an irregular tridimensional framework of alternating silicon and oxygen atoms with nanometer-scale voids and pores. The voids may contain water or some other liquids, or may be filled by gas or vacuum. In the latter case, the material is properly called silica xerogel.\n\nSilica xerogel with an average pore size of 2.4 nanometers has a strong affinity for water molecules and is widely used as a desiccant. It is hard and translucent, but considerably softer than massive silica glass or quartz; and remains hard when saturated with water. \n\nSilica xerogel is usually commercialized as coarse granules or beads, a few millimeters in diameter. Some grains may contain small amounts of a substance that changes color when they have absorbed some water. Small paper envelopes containing silica xerogel pellets, usually with a \"do not eat\" warning, are often included in dry food packages to absorb any humidity that might cause spoilage of the food.\n\n'Wet' silica gel, as may be freshly prepared from alkali silicate solutions, may vary in consistency from a soft transparent gel, similar to gelatin or agar, to a hard solid, namely a water-logged xerogel. It is sometimes used in laboratory processes, for example to suppress convection in liquids or prevent settling of suspended particles.\n\nSilica gel was in existence as early as the 1640s as a scientific curiosity. It was used in World War I for the adsorption of vapors and gases in gas mask canisters. The synthetic route for producing silica gel was patented by chemistry professor Walter A. Patrick at Johns Hopkins University in 1918.\n\nIn World War II, silica gel was indispensable in the war effort for keeping penicillin dry, protecting military equipment from moisture damage, as a fluid cracking catalyst for the production of high octane gasoline, and as a catalyst support for the manufacture of butadiene from ethanol, feedstock for the synthetic rubber program.\n\nType A - clear pellets, approximate pore diameter: 2.5 nm, drying and moistureproof properties, can be used as catalyst carriers, adsorbents, separators and variable-pressure adsorbent.\n\nType B - translucent white pellets, pore diameter: 4.5-7.0 nm, liquid adsorbents, drier and perfume carriers, also may be used as catalyst carriers, cat litter.\n\nType C - translucent, micro-pored structure, raw material for preparation of silica gel cat litter. Additionally dried and screened, it forms macro-pored silica gel which is used as drier, adsorbent and catalyst carrier.\n\nSilica alumina gel - light yellow, chemically stable, flame-resistant, insoluble except in alkali or hydrofluoric acid. Superficial polarity, thermal stability, performance greater than fine-pored silica gel.\n\nStabilizing silica gel - non-crystalline micro-porous solid powder, nontoxic, flame-resisting, used in brewery of grains for beer to improve taste, clearness, color and foam, removal of non-micro-organism impurities.\n\nSilica gel's high specific surface area (around 800 m/g) allows it to adsorb water readily, making it useful as a desiccant (drying agent). Silica gel is often described as \"absorbing\" moisture, which may be appropriate when the gel's microscopic structure is ignored, as in silica gel packs or other products. However, material silica gel removes moisture by \"adsorption\" onto the surface of its numerous pores rather than by \"absorption\" into the bulk of the gel.\n\nOnce saturated with water, the gel can be regenerated by heating it to 120 °C (250 °F) for 1–2 hours. Some types of silica gel will \"pop\" when exposed to enough water. This is caused by breakage of the silica spheres when contacting the water.\n\nAn aqueous solution of sodium silicate is acidified to produce a gelatinous precipitate that is washed, then dehydrated to produce colorless silica gel. When a visible indication of the moisture content of the silica gel is required, ammonium tetrachlorocobaltate(II) (NH)CoCl or cobalt chloride CoCl is added. This will cause the gel to be blue when dry and pink when hydrated. An alternative indicator is methyl violet which is orange when dry and green when hydrated. Due to the connection between cancer and cobalt chloride, it has been forbidden in Europe on silica gel.\n\nIn many items, moisture encourages the growth of mold and spoilage. Condensation may also damage other items like electronics and may speed the decomposition of chemicals, such as those in vitamin pills. Through the inclusion of silica gel packets, these items can be preserved longer.\n\nSilica gel may also be used to keep the relative humidity (RH) inside a high frequency radio or satellite transmission system waveguide as low as possible (see also Humidity buffering). Excessive moisture buildup within a waveguide can cause arcing inside the waveguide itself, damaging the power amplifier feeding it. Also, the beads of water that form and condense inside the waveguide change the characteristic impedance and frequency, degrading the signal. It is common for a small compressed air system (similar to a small home aquarium pump) to be employed to circulate the air inside the waveguide over a jar of silica gel.\n\nSilica gel is also used to dry the air in industrial compressed air systems. Air from the compressor discharge flows through a bed of silica gel beads. The silica gel adsorbs moisture from the air, preventing damage at the point of use of the compressed air due to condensation or moisture. The same system is used to dry the compressed air on railway locomotives, where condensation and ice in the brake air pipes can lead to brake failure.\n\nSilica gel is sometimes used as a preservation tool to control relative humidity in museum and library exhibitions and storage.\n\nOther applications include diagnostic test strips, inhalation devices, syringes, drug test kits and hospital sanitation kits.\n\nIn chemistry, silica gel is used in chromatography as a stationary phase. In column chromatography, the stationary phase is most often composed of silica gel particles of 40–63 μm. Different particle sizes are used for different kinds of column chromatography as the particle size is related to surface area. The differences in particle size dictate if the silica gel should be used for flash or gravity chromatography. In this application, due to silica gel's polarity, non-polar components tend to elute before more polar ones, hence the name normal phase chromatography. However, when hydrophobic groups (such as C groups) are attached to the silica gel then polar components elute first and the method is referred to as reverse phase chromatography. Silica gel is also applied to aluminium, glass, or plastic sheets for thin layer chromatography.\n\nThe hydroxy (OH) groups on the surface of silica can be functionalized to afford specialty silica gels that exhibit unique stationary phase parameters. These so-called functionalized silica gels are also used in organic synthesis and purification as insoluble reagents and scavengers.\n\nChelating groups have also been covalently bound to silica gel. These materials have the ability to remove metal ions selectively from aqueous media. Chelating groups can be covalently bound to polyamines that have been grafted onto a silica gel surface producing a material of greater mechanical integrity. Silica gel is also combined with alkali metals to form a M-SG reducing agent. (See SiGNa chemistry)\n\nSilica gel is not expected to biodegrade in either water or soil.\n\nSilica gel is also used as cat litter, by itself or in combination with more traditional materials, such as clays including bentonite. It is non-tracking and virtually odorless.\n\nSilica gel, also referred to as silica aerogel or hydrated silica, is listed by the FDA in the United States as generally recognized as safe (GRAS), meaning it can be added to food products without needing approval. Silica is allowed to be added to food in the US at up to 2% as permitted under 21 CFR 172.480. In the EU it can be in up to 5% concentrations.\n\nListed uses include: anticaking agent, defoaming agent, stabilizer, adsorbent, carrier, conditioning agent, chillproofing agent, filter aid, emulsifying agent, viscosity control agent, and anti-settling agent.\n\nGiven the water adsorption properties of silica gel, it is used in domestic water filters. The surface structure of silica gel allows the adsorption of some minerals which are dissolved in the water, or \"Ion-exchange\" as it is marketed. Due to the lack of regulations for domestic water filtration products, no studies validate the manufacturer claims regarding the effectiveness of the filtration system.\n\nSilica gel may be doped with a moisture indicator that gradually changes its color when it transitions from the anhydrous (dry) state, to the hydrated (wet) state. Common indicators are cobalt(II) chloride and methyl violet. Cobalt (II) chloride is deep blue when dry and pink when wet, but it is toxic and carcinogenic, and was reclassified by the European Union in July 2000 as a toxic material. Methyl violet can be formulated to change from orange to green, or orange to colorless. It is also toxic and potentially carcinogenic, but is safe enough to have medicinal uses.\n\nSilica gel is non-toxic, non-flammable, and non-reactive and stable with ordinary usage. It will react with hydrogen fluoride, fluorine, oxygen difluoride, chlorine trifluoride, strong acids, strong bases, and oxidizers. Silica gel is irritating to the respiratory tract and may cause irritation of the digestive tract, and dust from the beads may cause irritation to the skin and eyes, so precautions should be taken. Crystalline silica dust can cause silicosis, but synthetic amorphous silica gel is indurated, so does not cause silicosis. Additional hazards may occur when doped with a humidity indicator.\n\n"}
{"id": "28437", "url": "https://en.wikipedia.org/wiki?curid=28437", "title": "Simple harmonic motion", "text": "Simple harmonic motion\n\nIn mechanics and physics, simple harmonic motion is a special type of periodic motion or oscillation motion where the restoring force is directly proportional to the displacement and acts in the direction opposite to that of displacement.\n\nSimple harmonic motion can serve as a mathematical model for a variety of motions, such as the oscillation of a spring. In addition, other phenomena can be approximated by simple harmonic motion, including the motion of a simple pendulum as well as molecular vibration. Simple harmonic motion is typified by the motion of a mass on a spring when it is subject to the linear elastic restoring force given by Hooke's Law. The motion is sinusoidal in time and demonstrates a single resonant frequency. For simple harmonic motion to be an accurate model for a pendulum, the net force on the object at the end of the pendulum must be proportional to the displacement. This is a good approximation when the angle of the swing is small.\n\nSimple harmonic motion provides a basis for the characterization of more complicated motions through the techniques of Fourier analysis. Therefore it can be simply defined as the periodic motion of a body along a straight line, such that the acceleration is directed towards the center of the motion and also proportional to the displacement from that point.\n\nThe motion of a particle moving along a straight line with an acceleration whose direction is always towards a fixed point on the line and whose magnitude is proportional to the distance from the fixed point is called simple harmonic motion [SHM].\n\nIn the diagram, a simple harmonic oscillator, consisting of a weight attached to one end of a spring, is shown. The other end of the spring is connected to a rigid support such as a wall. If the system is left at rest at the equilibrium position then there is no net force acting on the mass. However, if the mass is displaced from the equilibrium position, the spring exerts a restoring elastic force that obeys Hooke's law.\n\nMathematically, the restoring force is given by\nwhere is the restoring elastic force exerted by the spring (in SI units: N), is the spring constant (N·m), and is the displacement from the equilibrium position (m).\n\nFor any simple mechanical harmonic oscillator:\n\nOnce the mass is displaced from its equilibrium position, it experiences a net restoring force. As a result, it accelerates and starts going back to the equilibrium position. When the mass moves closer to the equilibrium position, the restoring force decreases. At the equilibrium position, the net restoring force vanishes. However, at , the mass has momentum because of the acceleration that the restoring force has imparted. Therefore, the mass continues past the equilibrium position, compressing the spring. A net restoring force then slows it down until its velocity reaches zero, whereupon it is accelerated back to the equilibrium position again.\n\nAs long as the system has no energy loss, the mass continues to oscillate. Thus simple harmonic motion is a type of periodic motion.\nNote if the real space and phase space diagram are not co-linear, the phase space motion becomes elliptical. The area enclosed depends on the amplitude and the maximum momentum.\n\nIn Newtonian mechanics, for one-dimensional simple harmonic motion, the equation of motion, which is a second-order linear ordinary differential equation with constant coefficients, can be obtained by means of Newton's 2nd law and Hooke's law for a mass on a spring.\n\nwhere is the inertial mass of the oscillating body, is its displacement from the equilibrium (or mean) position, and is a constant (the spring constant for a mass on a spring).\n\nTherefore,\n\nSolving the differential equation above produces a solution that is a sinusoidal function.\nThis equation can be written in the form:\nwhere\n\nand, since where is the time period,\n\nThese equations demonstrate that the simple harmonic motion is isochronous (the period and frequency are independent of the amplitude and the initial phase of the motion).\n\nSubstituting with , the kinetic energy of the system at time is\nand the potential energy is\nIn the absence of friction and other energy loss, the total mechanical energy has a constant value\n\nThe following physical systems are some examples of simple harmonic oscillator.\n\nA mass attached to a spring of spring constant exhibits simple harmonic motion in closed space. The equation for describing the period\nshows the period of oscillation is independent of both the amplitude and gravitational acceleration. The above equation is also valid in the case when an additional constant force is being applied on the mass, i.e. the additional constant force cannot change the period of oscillation.\n\nSimple harmonic motion can be considered the one-dimensional projection of uniform circular motion. If an object moves with angular speed around a circle of radius centered at the origin of the -plane, then its motion along each coordinate is simple harmonic motion with amplitude and angular frequency .\n\nIn the small-angle approximation, the motion of a simple pendulum is approximated by simple harmonic motion. The period of a mass attached to a pendulum of length with gravitational acceleration formula_12 is given by\n\nThis shows that the period of oscillation is independent of the amplitude and mass of the pendulum but not of the acceleration due to gravity, formula_12, therefore a pendulum of the same length on the Moon would swing more slowly due to the Moon's lower gravitational field strength. Because the value of formula_12 varies slightly over the surface of the earth, the time period will vary slightly from place to place and will also vary with height above sea level.\n\nThis approximation is accurate only for small angles because of the expression for angular acceleration being proportional to the sine of the displacement angle:\nwhere is the moment of inertia. When is small, and therefore the expression becomes\nwhich makes angular acceleration directly proportional to , satisfying the definition of simple harmonic motion.\n\nA Scotch yoke mechanism can be used to convert between rotational motion and linear reciprocating motion. The linear motion can take various forms depending on the shape of the slot, but the basic yoke with a constant rotation speed produces a linear motion that is simple harmonic in form.\n\n\n"}
{"id": "323964", "url": "https://en.wikipedia.org/wiki?curid=323964", "title": "Subsistence agriculture", "text": "Subsistence agriculture\n\nSubsistence agriculture is a self-sufficiency farming system in which the farmers focus on growing enough food to feed themselves and their entire families. The output is mostly for local requirements with little or no surplus trade. The typical subsistence farm has a range of crops and animals needed by the family to feed and clothe themselves during the year. Planting decisions are made principally with an eye toward what the family will need during the coming year, and secondarily toward market prices. Tony Waters writes: \"Subsistence peasants are people who grow what they eat, build their own houses, and live without regularly making purchases in the marketplace.\" \n\nHowever, despite the primacy of self-sufficiency in subsistence farming, today most subsistence farmers also participate in trade to some degree, though usually it is for goods that are not necessary for survival, and may include sugar, iron roofing sheets, bicycles, used clothing, and so forth. Most subsistence farmers today reside in developing countries, although their amount of trade as measured in cash is less than that of consumers in countries with modern complex markets, many have important trade contacts and trade items that they can produce because of their special skills or special access to resources valued in the marketplace. \n\nSubsistence agriculture also emerged independently in Mexico where it was based on maize cultivation, and the Andes where it was based on the domestication of the potato. Subsistence agriculture was the dominant mode of production in the world until recently, when market-based capitalism became widespread. Subsistence horticulture may have developed independently in South East Asia and Papua New Guinea.\n\nSubsistence farming continues today in large parts of rural Africa, and parts of Asia and Latin America. In 2015, about 2 billion people (slightly more than 25% of the world's population) in 500 million households living in rural areas of developing nations survive as \"smallholder\" farmers, working less than 2 hectares (5 acres) of land. Subsistence agriculture had largely disappeared in Europe by the beginning of World War I, and in North America with the movement of sharecroppers and tenant farmers out of the American South and Midwest during the 1930s and 1940s. As recently as the 1950s, it was still common on family farms in North America and Europe to grow much of a family's own food and make much of its own clothing, although sales of some of the farm's production earned enough currency to buy certain staples, typically including sugar; coffee and tea; petroleum distillates (petrol, kerosene, fuel oil); textile products such as bolts of cloth, needles, and thread; medicines; hardware products such as nails, screws, and wire; and a few discretionary items such as candy or books. Many of the preceding items, as well as occasional services from physicians, veterinarians, blacksmiths, and others, were often bought with barter rather than currency. In Central and Eastern Europe subsistence and semi-subsistence agriculture reappeared within the transition economy since about 1990.\n\nIn this type of agriculture, a patch of forest land is cleared by a combination of felling and burning, and crops are grown. After 2-3 years the fertility of the soil begins to decline, the land is abandoned and the farmer moves to clear a fresh piece of land elsewhere in the forest as the process continues. While the land is left fallow the forest regrows in the cleared area and soil fertility and biomass is restored. After a decade or more, the farmer may return to the first piece of land. This form of agriculture is sustainable at low population densities, but higher population loads require more frequent clearing which prevents soil fertility from recovering, opens up more of the forest canopy, and encourages scrub at the expense of large trees, eventually resulting in deforestation and land erosion. Shifting cultivation is called Dredd in India, Ladang in Indonesia, Milpa in Central America and Mexico and Jhumming in North East India.\n\nWhile this 'slash-and-burn' technique may describe the method for opening new land, commonly the farmers in question have in existence at the same time smaller fields, sometimes merely gardens, near the homestead there they practice intensive 'non-shifting\" techniques until shortage of fields where they can employ \"slash and burn\" to clear land and (by the burning) provide fertilizer (ash). Such gardens nearer the homestead often regularly receive household refuse, the manure of any household chickens or goats, and compost piles where refuse is thrown initially just to get it out of the way. However, such farmers often recognize the value of such compost and apply it regularly to their smaller fields. They also may irrigate part of such fields if they are near a source of water.\n\nIn some areas of tropical Africa, at least, such smaller fields may be ones in which crops are grown on raised beds. Thus farmers practicing 'slash and burn' agriculture are often much more sophisticated agriculturalists than the term \"slash and burn\" subsistence farmers suggest.\n\nIn this type of farming people migrate along with their animals from one place to another in search of fodder for their animals. Generally they rear cattle, sheep, goats, camels and/or yaks for milk, skin, meat and wool. This way of life is common in parts of central and western Asia, India, east and south-west Africa and northern Eurasia. Examples are the nomadic Bhotiyas and Gujjars of the Himalayas. They carry their belongings, such as tents, etc.., on the backs of donkeys, horses, and camels. In mountainous regions, like Tibet and The Andes, Yak and Llama are reared. Reindeer are the livestock in arctic and sub-arctic areas. Sheep, goats, and camels are common animals, and cattle and horses are also important..\n\nIn intensive subsistence agriculture, the farmer cultivates a small plot of land using simple tools and more labor. Climate, with large number of days with sunshine and fertile soils permits growing of more than one crop annually on the same plot. Farmers use their small land holdings to produce enough, for their local consumption, while remaining produce is used for exchange against other goods. It results in much more food being produced per acre compared to other subsistence patterns. In the most intensive situation, farmers may even create terraces along steep hillsides to cultivate rice paddies. Such fields are found in densely populated parts of Asia, such as in The Philippines. They may also intensify by using manure, artificial irrigation and animal waste as fertilizer. Intensive subsistence farming is prevalent in the thickly populated areas of the monsoon regions of south, southwest, and east Asia.\n\nSubsistence agriculture can be used as a poverty alleviation strategy, specifically as a safety net for food-price shocks and for food security. Poor countries are limited in fiscal and institutional resources that would allow them to contain rises in domestic prices as well as to manage social assistance programs, which is often because they are using policy tools that are intended for middle- and high-income countries. Low-income countries tend to have populations in which 80% of poor are in rural areas and more than 90% of rural households have access to land, yet a majority of these rural poor have insufficient access to food. Subsistence agriculture can be used in low-income countries as a part of policy responses to a food crisis in the short and medium term, and provide a safety net for the poor in these countries.\n\n\n\nMarvin P Miracle, \"Subsistence Agriculture: Analytical Problems and Alternative Concepts,American Journal of Agricultural Economics, May, 1968, pp292-310.\nit is growing crops only for families\n"}
{"id": "34005717", "url": "https://en.wikipedia.org/wiki?curid=34005717", "title": "Subtropical Indian Ocean Dipole", "text": "Subtropical Indian Ocean Dipole\n\nThe Subtropical Indian Ocean Dipole (SIOD) is featured by the oscillation of sea surface temperatures (SST) in which the southwest Indian Ocean i.e. south of Madagascar is warmer and then colder than the eastern part i.e. off Australia. It was first identified in the studies of the relationship between the SST anomaly and the south-central Africa rainfall anomaly; the existence of such a dipole was identified from both observational studies and model simulations\n\nPositive phase of Subtropical Indian Ocean Dipole is characterized by warmer-than-normal sea surface temperature in the southwestern part, south of Madagascar, and colder-than-normal sea surface temperature off Australia, causing above-than-normal precipitation in many regions over south and central Africa. Stronger winds prevail along the eastern edge of the subtropical high, which become intensified and shifted slightly to the south during the positive events, leading to the enhanced evaporation in the eastern Indian Ocean, and therefore result in the cooling SST off Australia. On the other hand, reduced evaporation in the southwestern part causes reduced seasonal latent heat loss, and therefore results in increased temperature in the southwestern part, south of Madagascar. The negative phase of the SIOD is featured by the opposite conditions, with warmer SSTs in the eastern part, and cooler SSTs over the southwestern part. The physical condition favoring negative events is also just opposite. Also, Ekman transport accompanied with surface mixing process also plays a role in the formation of the SST dipole.\n\nGenerally speaking, the Subtropical Indian Ocean Dipole mode develops in December–January, peaks in February, then decays in the following two months, and finally dies down in May–June. The evolution and deformation process of the Subtropical Indian Ocean Dipole event is highly affected by the position of the subtropical high; atmospheric forcing plays a significant role in the evolution process of the Subtropical Indian Ocean Dipole event.\n\nSubtropical Indian Ocean Dipole related anomalies over the Southeastern Indian Ocean is also suggested to impact the position of Mascarene high and thus the Indian summer monsoon. Positive (negative) Subtropical Indian Ocean dipole events during boreal winter are always followed by weak (strong) Indian Summer Monsoons. During positive (negative) SIOD event, the Mascarene High shifting southeastward (northwestward) from austral to boreal summer causes a weakening (strengthening) of the monsoon circulation system by modulating the local Hadley cell during the Indian Summer Monsoon event.\n\nSouthwest Australia dry(wet) years are corresponding to anomalously cool(warm) waters in the tropical/subtropical Indian Ocean and anomalously warm(cool) waters in the subtropics off Australia, and these appear to be in phase with the large-scale winds over the tropical/subtropical Indian Ocean, which modify SST anomalies through anomalous Ekman transport in tropical Indian Ocean and through anomalous air–sea heat fluxes in the subtropics, which also alter the large-scale advection of moisture to the Southwestern Australia coast.\n\nThe spatial pattern of the dry(wet) composite SSTA shifted to the east of the spatial pattern of the positive(negative) Subtropical Indian Ocean Dipole event(previous definition of SIOD), and the calculation based on the Subtropical Indian Ocean Dipole Index may need re-consideration when the relationship between southwestern Australia rainfall and SIOD index is studied, which may require further work.\n\nPositive SIOD events also cause increased summer rains over large parts of southeastern Africa by bringing enhanced convergence of moisture. Higher temperature over the Southwestern Indian Ocean warm pole results in increased evaporation, and this moist air is advected to Mozambique and eastern South Africa, which is strengthened by the low pressure anomaly generated over this warm pole.\n\nThe Subtropical Indian Ocean Dipole event is suggested to be accompanied with similar dipole mode events in the Pacific and subtropical southern Atlantic, and linked with the Antarctic circumpolar wave.\n\nIt has also been suggested that the Subtropical Indian Ocean Dipole has impacts on the seasonal ocean-atmosphere gas exchanges in the southern Indian Ocean. Also, field experiments indicate that the warm anomalies related to southwestern warm pole are conductive to the reduction of the oceanic carbon dioxide uptake.\n\nThe Subtropical Indian Ocean Dipole Index is computed from SST anomaly difference between western (55E°-65°E,37S°-27°S) and eastern (90°E-100°E,28°S-18°S) Indian Ocean.\n\n\n"}
{"id": "13873779", "url": "https://en.wikipedia.org/wiki?curid=13873779", "title": "Sugarcane", "text": "Sugarcane\n\nSugarcane, or sugar cane, are several species of tall perennial true grasses of the genus \"Saccharum\", tribe Andropogoneae, native to the warm temperate to tropical regions of South and Southeast Asia, Polynesia and Melanesia, and used for sugar production. It has stout, jointed, fibrous stalks that are rich in the sugar sucrose, which accumulates in the stalk internodes. The plant is two to six metres (six to twenty feet) tall. All sugar cane species can interbreed and the major commercial cultivars are complex hybrids. Sugarcane belongs to the grass family Poaceae, an economically important seed plant family that includes maize, wheat, rice, and sorghum, and many forage crops.\n\nSucrose, extracted and purified in specialized mill factories, is used as raw material in the food industry or is fermented to produce ethanol. Sugarcane is the world's largest crop by production quantity, with 1.9 billion tonnes produced in 2016, and Brazil accounting for 41% of the world total (ethanol being produced on a large scale by the Brazilian sugarcane industry). In 2012, the Food and Agriculture Organization estimated it was cultivated on about , in more than 90 countries.\n\nThe global demand for sugar is the primary driver of sugarcane agriculture. Cane accounts for 80% of sugar produced; most of the rest is made from sugar beets. Sugarcane predominantly grows in the tropical and subtropical regions (sugar beets grow in colder temperate regions). Other than sugar, products derived from sugarcane include falernum, molasses, rum, \"cachaça\" (a traditional spirit from Brazil), bagasse, and ethanol. In some regions, people use sugarcane reeds to make pens, mats, screens, and thatch. The young, unexpanded inflorescence of Saccharum edule (\"duruka\" or \"tebu telor\") is eaten raw, steamed, or toasted, and prepared in various ways in Southeast Asia, including Fiji and certain island communities of Indonesia.\n\nThe Persians, followed by the Greeks, encountered the famous \"reeds that produce honey without bees\" in India between the 6th and 4th centuries BC. They adopted and then spread sugarcane agriculture. Merchants began to trade in sugar from India, which was considered a luxury and an expensive spice. In the 18th century AD, sugarcane plantations began in Caribbean, South American, Indian Ocean and Pacific island nations and the need for laborers became a major driver of large human migrations, including slave labor and indentured servants.\n\nSugarcane is a tropical, perennial grass that forms lateral shoots at the base to produce multiple stems, typically high and about in diameter. The stems grow into cane stalk, which when mature constitutes around 75% of the entire plant. A mature stalk is typically composed of 11–16% fiber, 12–16% soluble sugars, 2–3% nonsugars, and 63–73% water. A sugarcane crop is sensitive to the climate, soil type, irrigation, fertilizers, insects, disease control, varieties, and the harvest period. The average yield of cane stalk is per year. However, this figure can vary between 30 and 180 tonnes per hectare depending on knowledge and crop management approach used in sugarcane cultivation. Sugarcane is a cash crop, but it is also used as livestock fodder.\n\nSugarcane is indigenous to tropical parts of South and Southeast Asia. Different species likely originated in different locations, with \"Saccharum barberi\" originating in India and \"S. edule\" and \"S. officinarum\" in New Guinea. The earliest known production of crystalline sugar began in northern India. The exact date of the first cane sugar production is unclear. The earliest evidence of sugar production comes from ancient Sanskrit and Pali texts.\n\nAround the 8th century, Muslim and Arab traders introduced sugar from South Asia to the other parts of the Abbasid Caliphate in the Mediterranean, Mesopotamia, Egypt, North Africa, and Andalusia. By the 10th century, sources state that every village in Mesopotamia grew sugarcane. It was among the early crops brought to the Americas by the Spanish, mainly Andalusians, from their fields in the Canary Islands, and the Portuguese from their fields in the Madeira Islands.\n\nChristopher Columbus first brought sugarcane to the Caribbean during his second voyage to the Americas; initially to the island of Hispaniola (modern day Haiti and the Dominican Republic). In colonial times, sugar formed one side of the triangle trade of New World raw materials, along with European manufactured goods, and African slaves. Sugar (often in the form of molasses) was shipped from the Caribbean to Europe or New England, where it was used to make rum. The profits from the sale of sugar were then used to purchase manufactured goods, which were then shipped to West Africa, where they were bartered for slaves. The slaves were then brought back to the Caribbean to be sold to sugar planters. The profits from the sale of the slaves were then used to buy more sugar, which was shipped to Europe.\nFrance found its sugarcane islands so valuable that it effectively traded its portion of Canada, famously dubbed \"a few acres of snow\", to Britain for their return of Guadeloupe, Martinique and St. Lucia at the end of the Seven Years' War. The Dutch similarly kept Suriname, a sugar colony in South America, instead of seeking the return of the New Netherlands (New York).\n\nBoiling houses in the 17th through 19th centuries converted sugarcane juice into raw sugar. These houses were attached to sugar plantations in the Western colonies. Slaves often ran the boiling process under very poor conditions. Rectangular boxes of brick or stone served as furnaces, with an opening at the bottom to stoke the fire and remove ashes. At the top of each furnace were up to seven copper kettles or boilers, each one smaller and hotter than the previous one. The cane juice began in the largest kettle. The juice was then heated and lime added to remove impurities. The juice was skimmed and then channeled to successively smaller kettles. The last kettle, the \"teache\", was where the cane juice became syrup. The next step was a cooling trough, where the sugar crystals hardened around a sticky core of molasses. This raw sugar was then shoveled from the cooling trough into hogsheads (wooden barrels), and from there into the curing house.\n\nIn the British Empire, slaves were liberated after 1833 and many would no longer work on sugarcane plantations when they had a choice. British owners of sugarcane plantations therefore needed new workers, and they found cheap labour in China, Portugal and India. The people were subject to indenture, a long-established form of contract which bound them to forced labour for a fixed term; apart from the fixed term of servitude, this resembled slavery. The first ships carrying indentured labourers from India left in 1836. The migrations to serve sugarcane plantations led to a significant number of ethnic Indians, southeast Asians and Chinese settling in various parts of the world. In some islands and countries, the South Asian migrants now constitute between 10 and 50 percent of the population. Sugarcane plantations and Asian ethnic groups continue to thrive in countries such as Fiji, Natal, Burma, Sri Lanka, Malaysia, Indonesia, Philippines, British Guiana, Jamaica, Trinidad, Martinique, French Guiana, Guadeloupe, Grenada, St. Lucia, St. Vincent, St. Kitts, St. Croix, Suriname, Nevis, and Mauritius. \nBetween 1863 and 1900, traders and plantation owners from the British colony of Queensland (now a state of Australia) brought between 55,000 and 62,500 people from the South Pacific Islands to work on sugarcane plantations. It is estimated that one-third of these workers were coerced or kidnapped into slavery (known as blackbirding). Many others were paid very low wages. Between 1904 and 1908, most of the 10,000 remaining workers were deported in an effort to keep Australia racially pure and protect white workers from cheap foreign labour.\n\nCuban sugar derived from sugarcane was exported to the USSR, where it received price supports and was ensured a guaranteed market. The 1991 dissolution of the Soviet state forced the closure of most of Cuba's sugar industry.\n\nSugarcane remains an important part of the economy of Guyana, Belize, Barbados, and Haiti, along with the\nDominican Republic, Guadeloupe, Jamaica, and other islands.\n\nAbout 70% of the sugar produced globally comes from \"S. officinarum\" and hybrids using this species.\n\nSugarcane cultivation requires a tropical or subtropical climate, with a minimum of of annual moisture. It is one of the most efficient photosynthesizers in the plant kingdom. It is a C plant, able to convert up to 1% of incident solar energy into biomass. In prime growing regions, such as Mauritius, Dominican Republic, Puerto Rico, Peru, Brazil, Bolivia, Colombia, Guyana, Ecuador, Cuba, El Salvador, Jamaica, Bangladesh, India, Pakistan, Indonesia, Philippines, Malaysia, Australia and Hawaii, sugarcane crops can produce over 15 kg/m of cane. Once a major crop of the southeastern region of the United States, sugarcane cultivation has declined there in recent decades, and is now primarily confined to Florida, Louisiana, and South Texas.\n\nSugarcane is cultivated in the tropics and subtropics in areas with a plentiful supply of water for a continuous period of more than six to seven months each year, either from natural rainfall or through irrigation. The crop does not tolerate severe frosts. Therefore, most of the world's sugarcane is grown between 22°N and 22°S, and some up to 33°N and 33°S. When sugarcane crop is found outside this range, such as the Natal region of South Africa, it is normally due to anomalous climatic conditions in the region, such as warm ocean currents that sweep down the coast. In terms of altitude, sugarcane crop is found up to close to the equator in countries such as Colombia, Ecuador, and Peru.\n\nSugarcane can be grown on many soils ranging from highly fertile well-drained mollisols, through heavy cracking vertisols, infertile acid oxisols, peaty histosols, to rocky andisols. Both plentiful sunshine and water supplies increase cane production. This has made desert countries with good irrigation facilities such as Egypt some of the highest-yielding sugarcane-cultivating regions.\n\nAlthough some sugarcanes produce seeds, modern stem cutting has become the most common reproduction method. Each cutting must contain at least one bud, and the cuttings are sometimes hand-planted. In more technologically advanced countries like the United States and Australia, billet planting is common. Billets (stalks or stalk sections) harvested by a mechanical harvester are planted by a machine that opens and recloses the ground. Once planted, a stand can be harvested several times; after each harvest, the cane sends up new stalks, called ratoons. Successive harvests give decreasing yields, eventually justifying replanting. Two to 10 harvests are usually made depending on the type of culture. In a country with a mechanical agriculture looking for a high production of large fields, like in North America, sugar canes are replanted after two or three harvests to avoid a lowering in yields. In countries with a more traditional type of agriculture with smaller fields and hand harvesting, like in the French island la Réunion, sugar cane is often harvested up to 10 years before replanting.\n\nSugarcane is harvested by hand and mechanically. Hand harvesting accounts for more than half of production, and is dominant in the developing world. In hand harvesting, the field is first set on fire. The fire burns dry leaves, and chases away or kills any lurking venomous snakes, without harming the stalks and roots. Harvesters then cut the cane just above ground-level using cane knives or machetes. A skilled harvester can cut of sugarcane per hour.\n\nMechanical harvesting uses a combine, or sugarcane harvester. The Austoft 7000 series, the original modern harvester design, has now been copied by other companies, including Cameco / John Deere. The machine cuts the cane at the base of the stalk, strips the leaves, chops the cane into consistent lengths and deposits it into a transporter following alongside. The harvester then blows the trash back onto the field. Such machines can harvest each hour; however, harvested cane must be rapidly processed. Once cut, sugarcane begins to lose its sugar content, and damage to the cane during mechanical harvesting accelerates this decline. This decline is offset because a modern chopper harvester can complete the harvest faster and more efficiently than hand cutting and loading. Austoft also developed a series of hydraulic high-lift infield transporters to work alongside their harvesters to allow even more rapid transfer of cane to, for example, the nearest railway siding. This mechanical harvesting doesn't require the field to be set on fire; the remains left in the field by the machine consist of the top of the sugar cane and the dead leaves, which act as mulch for the next round of planting.\n\nThe cane beetle (also known as cane grub) can substantially reduce crop yield by eating roots; it can be controlled with imidacloprid (Confidor) or chlorpyrifos (Lorsban). Other important pests are the larvae of some butterfly/moth species, including the turnip moth, the sugarcane borer (\"Diatraea saccharalis\"), the African sugarcane borer (\"Eldana saccharina\"), the Mexican rice borer (\"Eoreuma loftini\"), the African armyworm (\"Spodoptera exempta\"), leaf-cutting ants, termites, spittlebugs (especially \"Mahanarva fimbriolata\" and \"Deois flavopicta\"), and the beetle \"Migdolus fryanus\". The planthopper insect \"Eumetopina flavipes\" acts as a virus vector, which causes the sugarcane disease ramu stunt.\n\nNumerous pathogens infect sugarcane, such as sugarcane grassy shoot disease caused by \"Phytoplasma\", whiptail disease or sugarcane smut, \"pokkah boeng\" caused by \"Fusarium moniliforme\", Xanthomonas axonopodis bacteria causes Gumming Disease, and red rot disease caused by \"Colletotrichum falcatum\". Viral diseases affecting sugarcane include sugarcane mosaic virus, maize streak virus, and sugarcane yellow leaf virus.\n\nSome sugarcane varieties are capable of fixing atmospheric nitrogen in association with the bacterium \"Gluconacetobacter diazotrophicus\". Unlike legumes and other nitrogen-fixing plants that form root nodules in the soil in association with bacteria, \"G. diazotrophicus\" lives within the intercellular spaces of the sugarcane's stem. Coating seeds with the bacteria is a newly developed technology that can enable every crop species to fix nitrogen for its own use.\n\nAt least 20,000 people are estimated to have died of chronic kidney disease (CKD) in Central America in the past two decades – most of them sugar cane workers along the Pacific coast. This may be due to working long hours in the heat without adequate fluid intake.\n\nTraditionally, sugarcane processing requires two stages. Mills extract raw sugar from freshly harvested cane and \"mill-white” sugar is sometimes produced immediately after the first stage at sugar-extraction mills, intended for local consumption. Sugar crystals appear naturally white in color during the crystallization process. Sulfur dioxide is added to inhibit the formation of color-inducing molecules as well as to stabilize the sugar juices during evaporation. Refineries, often located nearer to consumers in North America, Europe, and Japan, then produce refined white sugar, which is 99 percent sucrose. These two stages are slowly merging. Increasing affluence in the sugar-producing tropics increased demand for refined sugar products, driving a trend toward combined milling and refining.\n\nSugarcane processing produces cane sugar (sucrose) from sugarcane. Other products of the processing include bagasse, molasses, and filtercake.\n\nBagasse, the residual dry fiber of the cane after cane juice has been extracted, is used for several purposes:\nThe primary use of bagasse and bagasse residue is as a fuel source for the boilers in the generation of process steam in sugar plants. Dried filtercake is used as an animal feed supplement, fertilizer, and source of sugarcane wax.\n\nMolasses is produced in two forms: Blackstrap, which has a characteristic strong flavor, and a purer molasses syrup. Blackstrap molasses is sold as a food and dietary supplement. It is also a common ingredient in animal feed, is used to produce ethanol and rum, and in the manufacturing of citric acid. Purer molasses syrups are sold as molasses, and may also be blended with maple syrup, invert sugars, or corn syrup. Both forms of molasses are used in baking.\n\nSugar refining further purifies the raw sugar. It is first mixed with heavy syrup and then centrifuged in a process called \"affination\". Its purpose is to wash away the sugar crystals' outer coating, which is less pure than the crystal interior. The remaining sugar is then dissolved to make a syrup, about 60 percent solids by weight.\n\nThe sugar solution is clarified by the addition of phosphoric acid and calcium hydroxide, which combine to precipitate calcium phosphate. The calcium phosphate particles entrap some impurities and absorb others, and then float to the top of the tank, where they can be skimmed off. An alternative to this \"phosphatation\" technique is \"carbonatation\", which is similar, but uses carbon dioxide and calcium hydroxide to produce a calcium carbonate precipitate.\n\nAfter filtering any remaining solids, the clarified syrup is decolorized by filtration through activated carbon. Bone char or coal-based activated carbon is traditionally used in this role. Some remaining color-forming impurities are adsorbed by the carbon. The purified syrup is then concentrated to supersaturation and repeatedly crystallized in a vacuum, to produce white refined sugar. As in a sugar mill, the sugar crystals are separated from the molasses by centrifuging. Additional sugar is recovered by blending the remaining syrup with the washings from affination and again crystallizing to produce brown sugar. When no more sugar can be economically recovered, the final molasses still contains 20–30 percent sucrose and 15–25 percent glucose and fructose.\n\nTo produce granulated sugar, in which individual grains do not clump, sugar must be dried, first by heating in a rotary dryer, and then by blowing cool air through it for several days.\n\nRibbon cane is a subtropical type that was once widely grown in the southern United States, as far north as coastal North Carolina. The juice was extracted with horse or mule-powered crushers; the juice was boiled, like maple syrup, in a flat pan, and then used in the syrup form as a food sweetener. It is not currently a commercial crop, but a few growers find ready sales for their product.\n\nParticulate matter, combustion products, and volatile organic compounds are the primary pollutants emitted during the sugarcane processing. Combustion products include nitrogen oxides (NO), carbon monoxide (CO), CO, and sulfur oxides (SO). Potential emission sources include the sugar granulators, sugar conveying and packaging equipment, bulk loadout operations, boilers, granular carbon and char regeneration kilns, regenerated adsorbent transport systems, kilns and handling equipment (at some facilities), carbonation tanks, multi-effect evaporator stations, and vacuum boiling pans. Modern pollution prevention technologies are capable of addressing all of these potential pollutants. \n\nGlobal production of sugarcane in 2016 was 1.9 billion tonnes, with Brazil producing 41% of the world total and India 18% (table). The average worldwide yield of sugarcane crops in 2016 was 70.6 tonnes per hectare, led by Peru with 112 tonnes per hectare and Zambia with 103.\n\nThe theoretical possible yield for sugar cane is about 280 tonnes per hectare per year, and small experimental plots in Brazil have demonstrated yields of 236–280 tonnes of cane per hectare. Other promising regions for high-yield sugarcane production are in sun-drenched, irrigated farms of northern Africa, and other deserts with plentiful water from nearby rivers or irrigation canals.\n\nIn the United States, sugarcane is grown commercially in Florida, Hawaii, Louisiana and Texas.\n\nEthanol is generally available as a byproduct of sugar production. It can be used as a biofuel alternative to gasoline, and is widely used in cars in Brazil. It is an alternative to gasoline, and may become the primary product of sugarcane processing, rather than sugar.\n\nIn Brazil, gasoline is required to contain at least 22 percent bioethanol. This bioethanol is sourced from Brazil's large sugarcane crop.\n\nThe production of ethanol from sugar cane is more energy efficient than from corn or sugar beets or palm/vegetable oils, particularly if cane bagasse is used to produce heat and power for the process. Furthermore, if biofuels are used for crop production and transport, the fossil energy input needed for each ethanol energy unit can be very low. EIA estimates that with an integrated sugar cane to ethanol technology, the well-to-wheels CO emissions can be 90 percent lower than conventional gasoline.\n\nA textbook on renewable energy describes the energy transformation:\nPresently, 75 tons of raw sugar cane are produced annually per hectare in Brazil. The cane delivered to the processing plant is called burned and cropped (b&c), and represents 77% of the mass of the raw cane. The reason for this reduction is that the stalks are separated from the leaves (which are burned and whose ashes are left in the field as fertilizer), and from the roots that remain in the ground to sprout for the next crop. Average cane production is, therefore, 58 tons of b&c per hectare per year.\nEach ton of b&c yields 740 kg of juice (135 kg of sucrose and 605 kg of water) and 260 kg of moist bagasse (130 kg of dry bagasse). Since the lower heating value of sucrose is 16.5 MJ/kg, and that of the bagasse is 19.2 MJ/kg, the total heating value of a ton of b&c is 4.7 GJ of which 2.2 GJ come from the sucrose and 2.5 from the bagasse.\nPer hectare per year, the biomass produced corresponds to 0.27 TJ. This is equivalent to 0.86 W per square meter. Assuming an average insolation of 225 W per square meter, the photosynthetic efficiency of sugar cane is 0.38%.\nThe 135 kg of sucrose found in 1 ton of b&c are transformed into 70 litres of ethanol with a combustion energy of 1.7 GJ. The practical sucrose-ethanol conversion efficiency is, therefore, 76% (compare with the theoretical 97%).\nOne hectare of sugar cane yields 4,000 litres of ethanol per year (without any additional energy input, because the bagasse produced exceeds the amount needed to distill the final product). This, however, does not include the energy used in tilling, transportation, and so on. Thus, the solar energy-to-ethanol conversion efficiency is 0.13%.\nSugarcane is a major crop in many countries. It is one of the plants with the highest bioconversion efficiency. Sugarcane crop is able to efficiently fix solar energy, yielding some 55 tonnes of dry matter per hectare of land annually. After harvest, the crop produces sugar juice and bagasse, the fibrous dry matter. This dry matter is biomass with potential as fuel for energy production. Bagasse can also be used as an alternative source of pulp for paper production.\n\nSugarcane bagasse is a potentially abundant source of energy for large producers of sugarcane, such as Brazil, India and China. According to one report, with use of latest technologies, bagasse produced annually in Brazil has the potential of meeting 20 percent of Brazil’s energy consumption by 2020.\n\nA number of countries, in particular those devoid of any fossil fuel, have implemented energy conservation and efficiency measures to minimize energy used in cane processing and furthermore export any excess electricity to the grid. Bagasse is usually burned to produce steam, which in turn creates electricity. Current technologies, such as those in use in Mauritius, produce over 100 kWh of electricity per tonne of bagasse. With a total world harvest of over 1 billion tonnes of sugar cane per year, the global energy potential from bagasse is over 100,000 GWh. Using Mauritius as a reference, an annual potential of 10,000 GWh of additional electricity could be produced throughout Africa. Electrical generation from bagasse could become quite important, particularly to the rural populations of sugarcane producing nations.\n\nRecent cogeneration technology plants are being designed to produce from 200 to over 300 kWh of electricity per tonne of bagasse. As sugarcane is a seasonal crop, shortly after harvest the supply of bagasse would peak, requiring power generation plants to strategically manage the storage of bagasse.\n\nA greener alternative to burning bagasse for the production of electricity is to convert bagasse into biogas. Technologies are being developed to use enzymes to transform bagasse into advanced biofuel and biogas.\n\nIn most countries where sugarcane is cultivated, there are several foods and popular dishes derived directly from it, such as:\n\nMany parts of the sugarcane are commonly used as animal feeds where the plants are cultivated. The leaves make a good forage for ruminants.\n"}
{"id": "4244195", "url": "https://en.wikipedia.org/wiki?curid=4244195", "title": "Terra-3", "text": "Terra-3\n\nTerra-3 was a Soviet laser testing centre, located on the Sary Shagan anti-ballistic missile (ABM) testing range in the Karaganda Region of Kazakhstan. It was originally built to test missile defense concepts, but these attempts were dropped after the Anti-Ballistic Missile Treaty was signed. The site later hosted two modest devices used primarily for experiments in space tracking. Several other laser test sites were also active during this period. During the 1980s, officials within the United States Department of Defense (DoD) suggested it was the site of a prototypical anti-satellite weapon system. The site was abandoned and is now partially disassembled.\n\nDevelopment of laser weapons in the Soviet Union began in 1964-1965. Among many proposals for laser weapons was an explosively pumped gas dynamic laser. Construction—consisting of a large concrete bunker lined with steel plates—was begun at Sary Shagan, but the facility was far from complete when the Anti-Ballistic Missile Treaty was signed in 1972, and these efforts ended.\n\nThe buildings were then re-purposed for more modest laser systems. Vympel NPO led the construction and developed the tracking and aiming systems. The lasers were developed at Astrofizika, a company newly formed from the laser departments of several defense contractors. They installed two lasers at the site, a visible-light ruby laser that was installed in 1979, and an infrared carbon dioxide laser that was installed in 1982. Tracking systems were tested by fitting aircraft with laser detectors and then looking for signals when the lasers fired. There were also tests against satellites that passed over the site, in an effort to demonstrate the ability to blind optical sensors. Instead, these experiments demonstrated the inability of the tracking system to point the lasers with the required level of accuracy in order to be effective.\n\nWith the development of the Strategic Defense Initiative (SDI) in the early 1980s, the DoD began claiming that the Soviets were developing an anti-satellite laser weapons system at the Sary Shagan site. These statements were part of an argument suggesting that a sort of \"laser gap\" existed between the US and the USSR, harkening back to the mythical bomber gap and missile gap of previous decades. As it would turn out, this comparison was quite accurate, as the laser gap turned out to be equally mythical. Throughout, the Central Intelligence Agency (CIA) was returning reports on the site that were quite accurate, and at odds with the DoD's public statements. The DoD presented only the worst-case assessments found in the public portions of the CIA's reports.\n\nWith the ending of the Cold War, a delegation of US officials and experts were able to visit the site in July 1989. These observers noted a wide variety of evidence that the system—while intended to research the possibility of an anti-satellite laser capability—had never reached anywhere near the operational stage. The laser viewed by the US officials was extremely low-power, including the small size of the focusing optics and the uncooled director which would be incapable of handling a large laser. The lasers that they found were 1,000 times less powerful that the US's own MIRACL. The team dismissed the site as non-operational.\n\nWhen discussing the issue, Soviet officials were somewhat amused. They noted that the US public often had better information than their own military, and that excessive secrecy had led the Soviet citizenry to distrust the military's claims as to their own capabilities.\n\nTerra-3 is the topic of a widespread claim that the IR laser was used to target the \"Space Shuttle Challenger\" during its 6th orbital mission on 10 October 1984 (STS-41-G). According to reports by Steven Zaloga, the Shuttle was briefly illuminated and caused \"malfunctions on the space shuttle and distress to the crew,\" causing the United States to file a diplomatic protest about the incident. This claim appears to have started with former Soviet officials, notably Boris Kononenko. The crew members and \"knowledgeable members of the US intelligence community\" have denied that the shuttle was illuminated by the Terra-3.\n"}
{"id": "14334762", "url": "https://en.wikipedia.org/wiki?curid=14334762", "title": "The School for Field Studies", "text": "The School for Field Studies\n\nThe School for Field Studies (SFS) is an international non-profit academic institution and the United States' largest environmental study abroad program for college undergraduates. Since 1981, it has provided fully accredited, college level environmental education and has conducted research to propose sustainable solutions to critical environmental problems for community clients through its five-year research plans.\n\n"}
{"id": "4928954", "url": "https://en.wikipedia.org/wiki?curid=4928954", "title": "Tricine", "text": "Tricine\n\nTricine is an organic compound that is used in buffer solutions. The name tricine comes from tris and glycine, from which it was derived. It is a white crystalline powder that is moderately soluble in water. It is a zwitterionic amino acid that has a pKa1 value of 2.3 at 25 °C, while its pKa2 at 20 °C is 8.15. Its useful buffering range of pH is 7.4-8.8. Along with bicine, it is one of Good's buffering agents. Good first prepared tricine to buffer chloroplast reactions.\n\nTricine is a commonly used electrophoresis buffer and is also used in resuspension of cell pellets. It has a higher negative (more negative) charge than glycine allowing it to migrate faster. In addition its high ionic strength causes more ion movement and less protein movement. This allows for low molecular weight proteins to be separated in lower percent acrylamide gels. Tricine has been documented in the separation of proteins in the range of 1 to 100 kDa by electrophoresis. The tricine buffer at 25 mmol/L was found to be the most effective buffer among the ten tested for ATP assays using firefly luciferase. Tricine has also been found to be an effective scavenger of hydroxyl radicals in a study of radiation-induced membrane damage.\n\n"}
{"id": "47559813", "url": "https://en.wikipedia.org/wiki?curid=47559813", "title": "True muonium", "text": "True muonium\n\nTrue muonium or muononium is a theoretically predicted exotic atom made up of an antimuon and a muon. It is yet to be observed, but it may have been generated in the collision of electron and positron beams.\n\n\n"}
{"id": "53422802", "url": "https://en.wikipedia.org/wiki?curid=53422802", "title": "Tunø Knob Offshore Wind Farm", "text": "Tunø Knob Offshore Wind Farm\n\nTunø Knob Offshore Wind Farm is an offshore wind farm in the Bay of Aarhus, Denmark. It is located on the sandbar Tunø Knob, west of the Tunø island. \n\nThe wind farm was commissioned in 1995 with 10 turbines, each a 500 kW Vestas V39. The turbines are 45 metres high, based on concrete foundations at 3-6 meter deep waters, and together they produce enough electricity to supply c. 2,800 households. The Tunø Knob Offshore Wind Farm was the first offshore wind farm by Danish wind turbine company Vestas. \n\n"}
{"id": "34750362", "url": "https://en.wikipedia.org/wiki?curid=34750362", "title": "ZEPLIN-III", "text": "ZEPLIN-III\n\nThe ZEPLIN-III dark matter experiment attempted to detect galactic WIMPs using a 12 kg liquid xenon target. It operated at the Boulby Underground Laboratory (North-East England, UK) in the period 2006–2011. This was the last in a series of xenon-based experiments in the ZEPLIN programme pursued originally by the UK Dark Matter Collaboration (UKDMC). The ZEPLIN-III project was led by Imperial College London and also included the Rutherford Appleton Laboratory and the University of Edinburgh in the UK, as well as LIP-Coimbra in Portugal and ITEP-Moscow in Russia. It ruled out cross-sections for elastic scattering of WIMPs off nucleons above 3.9 × 10 pb (3.9 × 10 cm) from the two science runs conducted at Boulby (83 days in 2008 and 319 days in 2010/11).\n\nDirect dark matter search experiments look for extremely rare and very weak collisions expected to occur between the cold dark matter particles that are believed to permeate our galaxy and the nuclei of atoms in the active medium of a radiation detector. These hypothetical elementary particles could be Weakly Interacting Massive Particles, or WIMPs, weighing as little as a few protons or as much as several heavy nuclei. Their nature is not yet known, but no sensible candidates remain within the Standard Model of particle physics to explain the dark matter problem.\n\nCondensed noble gases, most notably liquid xenon and liquid argon, are excellent radiation detection media. They can produce two signatures for each particle interaction: a fast flash of light (scintillation) and the local release of charge (ionisation). In two-phase xenon – so called since it involves liquid and gas phases in equilibrium – the scintillation light produced by an interaction in the liquid is detected directly with photomultiplier tubes; the ionisation electrons released at the interaction site are drifted up to the liquid surface under an external electric field, and subsequently emitted into a thin layer of xenon vapour. Once in the gas, they generate a second, larger pulse of light (electroluminescence or proportional scintillation), which is detected by the same array of photomultipliers. These systems are also known as xenon 'emission detectors'.\n\nThis configuration is that of a time projection chamber (TPC); it allows three-dimensional reconstruction of the interaction site, since the depth coordinate (z) can be measured very accurately from the time separation between the two light pulses. The horizontal coordinates can be reconstructed from the hit pattern in the photomultiplier array(s). Critically for WIMP searches, the ratio between the two response channels (scintillation and ionisation) allows the rejection of the predominant backgrounds for WIMP searches: gamma and beta radiation from trace radioactivity in detector materials and the immediate surroundings. WIMP candidate events produce lower ionisation/scintillation ratios than the more prevalent background interactions.\n\nThe ZEPLIN programme pioneered the use of two-phase technology for WIMP searches. The technique itself, however, was first developed for radiation detection using argon in the early 1970s. Lebedenko, one of its pioneers at the Moscow Engineering Physics Institute, was involved in building ZEPLIN-III in the UK from 2001. Developed alongside it, but on a faster timescale, ZEPLIN-II was the first such WIMP detector to operate in the world (2005). This technology was also adopted very successfully by the XENON programme. Two-phase argon has also been used for dark matter searches by the WARP collaboration and ArDM. LUX is developing similar systems that have set improved limits.\n\nThe ZEPLIN (ZonEd Proportional scintillation in LIquid Noble gases) series of experiments was a progressive programme pursued by the UK Dark Matter Collaboration using liquid xenon. It evolved alongside the DRIFT programme which promoted the use of gas-filled TPCs to recover directional information on WIMP scattering. In the late 1980s the UKDMC had explored the potential of different materials and techniques, including cryogenic LiF, CaF, silicon and germanium, from which a programme emerged at Boulby based on room-temperature NaI(Tl) scintillators. The subsequent move to a new target material, liquid xenon, was motivated by the realisation that noble liquid targets are inherently more scalable and could achieve lower energy thresholds and better background discrimination. In particular, external layers of the bulk target, affected more by external backgrounds, can be sacrificed during data analysis if the position of the interactions in known; this leaves an inner fiducial volume with potentially very low background rates. This self-shielding effect (alluded to by the 'zoned' term in the contrived ZEPLIN acronym) explains the faster gain in sensitivity of these targets compared to technologies based on a modular approach adopted with crystal detectors, where each module brings its own background.\n\nZEPLIN-I, a 3 kg liquid xenon target, operated at Boulby from the late 1990s. It used pulse shape discrimination for background rejection, exploiting a small but helpful difference between the timing properties of the scintillation light caused by WIMPs and background interactions. This was followed by two-phase systems ZEPLIN-II and ZEPLIN-III, which were designed and built in parallel at RAL/UCLA and Imperial College, respectively.\n\nZEPLIN-II was the first two-phase system deployed to search for dark matter in the world; it consisted of a 30 kg liquid xenon target topped by a 3 mm layer of gas in a so-called three-electrode configuration: separate electric fields were applied to the bulk of the liquid (WIMP target) and to the gas region above it by using an extra electrode underneath the liquid surface (in addition to an anode grid, located above the gas, and a cathode, at the bottom of the chamber). In ZEPLIN-II an array of 7 photomultipliers viewed the chamber from above in the gas phase.\n\nZEPLIN-III was proposed in the late 1990s, based partly on a similar concept developed at ITEP, and built by Prof. Tim Sumner and his team at Imperial College. It was deployed underground at Boulby in late 2006, where it operated until 2011. It was a two-electrode chamber, where electron emission into the gas was achieved by a strong (4 kV/cm) field in the liquid bulk rather than by an additional electrode. The photomultiplier array contained 31 photon detectors viewing the WIMP target from below, immersed in the cold liquid xenon.\n\nZEPLIN–II and –III were purposely designed in different ways, so that the technologies employed in each sub-system could be appraised and selected for the final experiment proposed by the UKDMC: a tonne-scale xenon target (ZEPLIN-MAX) capable of probing most of the parameter space favoured by theory at that point (1 × 10 pb), although this latter system was never built in the UK for lack of funding.\n\nAlthough the ZEPLIN-III liquid xenon target was built on the same scale as that of its ZEPLIN predecessors, it achieved significant improvements in WIMP sensitivity due to the higher discrimination factor achieved and to a lower overall background. In 2011 it published exclusion limits on the spin-independent WIMP-nucleon elastic scattering cross-section above 3.9 × 10 pb for a 50 GeV WIMP mass. Although not as stringent as results from XENON100, this was achieved with a 10 times smaller fiducial mass and demonstrated the best background discrimination ever achieved in these detectors. The WIMP-neutron spin-dependent cross-section was excluded above 8.0 × 10 pb. It also ruled out an inelastic WIMP scattering model which attempted to reconcile a positive claim from DAMA with the absence of signal in other experiments.\n\n"}
