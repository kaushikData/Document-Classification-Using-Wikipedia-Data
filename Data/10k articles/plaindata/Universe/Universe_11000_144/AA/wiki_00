{"id": "42786320", "url": "https://en.wikipedia.org/wiki?curid=42786320", "title": "2-Decenedioic acid", "text": "2-Decenedioic acid\n\n2-Decendioic acid is a constituent of royal jelly.\n"}
{"id": "2574174", "url": "https://en.wikipedia.org/wiki?curid=2574174", "title": "Acoustic emission", "text": "Acoustic emission\n\nAcoustic emission (AE) is the phenomenon of radiation of acoustic (elastic) waves in solids that occurs when a material undergoes irreversible changes in its internal structure, for example as a result of crack formation or plastic deformation due to aging, temperature gradients or external mechanical forces. In particular, AE is occurring during the processes of \"mechanical loading\" of materials and structures accompanied by structural changes that generate local sources of elastic waves. This results in small surface displacements of a material produced by elastic or stress waves generated when the accumulated elastic energy in a material or on its surface is released rapidly. The waves generated by sources of AE are of practical interest in structural health monitoring (SHM), quality control, system feedback, process monitoring and other fields. In SHM applications, AE is typically used to detect, locate and characterise damage.\n\nAcoustic emission is the transient elastic waves within a material, caused by the rapid release of localized stress energy. An \"event\" source is the phenomenon which releases elastic energy into the material, which then propagates as an elastic wave. Acoustic emissions can be detected in frequency ranges under 1 kHz, and have been reported at frequencies up to 100 MHz, but most of the released energy is within the 1 kHz to 1 MHz range. Rapid stress-releasing events generate a spectrum of stress waves starting at 0 Hz, and typically falling off at several MHz.\n\nThe three major applications of AE techniques are: 1) source location – determine the locations where an \"event\" source occurred; 2) material mechanical performance – evaluate and characterize materials/structures; and 3) health monitoring – monitor the safe operation of a structure, for example, bridges, pressure containers, and pipe lines, etc.\n\nMore recent research has focused on using AE to not only locate but also to characterise the source mechanisms such as crack growth, friction, delamination, matrix cracking, etc. This would give AE the ability to tell the end user what source mechanism is present and allow them to determine whether structural repairs are necessary.\nAE can be related to an irreversible release of energy. It can also be generated from sources not involving material failure, including friction, cavitation and impact.\n\nThe application of acoustic emission to non-destructive testing of materials typically takes place between 100 kHz and 1  MHz. Unlike conventional ultrasonic testing, AE tools are designed for monitoring acoustic emissions produced by the material during failure or stress, and not on the material's effect on externally generated waves. Part failure can be documented during unattended monitoring. The monitoring of the level of AE activity during multiple load cycles forms the basis for many AE safety inspection methods, that allow the parts undergoing inspection to remain in service.\n\nThe technique is used, for example, to study the formation of cracks during the welding process, as opposed to locating them after the weld has been formed with the more familiar ultrasonic testing technique. In a material under active stress, such as some components of an airplane during flight, transducers mounted in an area can detect the formation of a crack at the moment it begins propagating. A group of transducers can be used to record signals, then locate the precise area of their origin by measuring the time for the sound to reach different transducers. The technique is also valuable for detecting cracks forming in pressure vessels and pipelines transporting liquids under high pressures. Also, this technique is used for estimation of corrosion in reinforced concrete structures.\n\nIn addition to non-destructive testing, acoustic emission monitoring has applications in process monitoring. Applications where acoustic emission monitoring has successfully been used include detecting anomalies in fluidized beds, and end points in batch granulation.\n\nStandards for the use of acoustic emission for non-destructive testing of pressure vessels have been developed by the ASME, ISO and the European Community.\n\n\n"}
{"id": "12374512", "url": "https://en.wikipedia.org/wiki?curid=12374512", "title": "Aeroprediction", "text": "Aeroprediction\n\nThe Aeroprediction Code is a semi-empirical computer program that estimates the aerodynamics of weapons over the Mach number range 0 to 20, angle of attack range 0 to 90 degrees and for configurations that have various cross sectional body shapes. Weapons considered include projectiles, missiles, bombs, rockets and mortars. Both static and dynamic aerodynamics are predicted with good accuracy. The code also predicts the trajectory of the weapon using the aerodynamics predicted by the code.\n\nThe code may be used to compute the center of pressure and static margin of missiles.\n\nThe Defense Acquisition Workforce Improvement Act provides insights into how to use aerodynamic prediction codes such as Aeroprediction in the design of missile for US acquisition.\n\n\n1. \"Body Alone Aerodynamics of Guided and Unguided Projectiles at Subsonic, Transonic, and Supersonic Mach Numbers\", NWL TR-3796, Nov 1972.\n\n2. \"Aerodynamic Drag and Lift of General Body Shapes at Subsonic, Transonic, and Supersonic Mach Numbers\", AGARD CP-124, AGARD Conference on Aerodynamic Drag, Izmir, Turkey, April 1973.\n\n3. \"Aerodynamics of Guided and Unguided Weapons: Part I Theory and Application\", NWL TR-3018, Dec 1973 (written with W. McKerley) .\n\n4. \"Aerodynamics of Guided and Unguided Weapons: Part II Computer Program and Usage\", NWL TR-3036, Jan 1974 (written with W. McKerley) .\n\n5/6 \"Static Aerodynamics of Missile Configurations for Mach Number Zero to Three\", AIAA Paper No.74-538, Jun 1974 and Journal of Aircraft, Vol. 12, No.10, Oct 1975.\n\n7. \"Static and Dynamic Aeroballistics of Projectiles and Missiles\", Paper No.9 Presented at the 10th Navy Symposium on Aeroballistics, NSWCDL, Jul 1975 (written with C. Swanson).\n\n8. \"The Effect of Boattail Shape on Magnus \", NSWCDL TR-3581, Dec 1976 (co-authored with G. Graff) .\n9. \"Empirical Method for Predicting the Magnus Characteristics of Spinning Shells \", AIAA Journal, Vol. 15 No.10, Oct 1977, (written with G. Graff) .\n\n10. \"Aerodynamics of Tactical Weapons to Mach Number 3 and Angle of Attack 15 Degrees: Part I - Theory and Application\", NSWCDL TR-3584, Feb 1977 (written with C. Swanson) .\n\n11. \"Aerodynamics of Tactical Weapons to Mach Number 3 and Angle of Attack 15 Degrees: Part II - Computer Program and Usage\", NSWCDL TR-3600, Mar 1977 (written with C. Swanson) .\n\n12. \"Optimal Projectile Shapes for Minimum Total Drag\", NSWCDL TR-3597, May 1977 (written with Hager and F. DeJarnette) .\n\n13. \"Dynamic Derivatives for Missile Configurations to Mach Number Three\", Journal of Spacecraft and Rockets, Vol. 15, No.4, 1978 (written with C. Swanson) .\n\n14/15. \"Aerodynamic Prediction Code for Tactical Weapons\", Paper presented at 11th Navy Aeroballistics Symposium, Warminster, PA, 1978 and presented at 17th AIAA Aerospace Sciences Meeting, New Orleans, 1979 (written with L. Devan and J. Sun) .\n\n16. \"Aerodynamics Design Manual for Tactical Weapons\", NSWC TR 81-156, July 1981 (written with L. Mason, L. Devan and D. McMillan).\n\n17. \"Aerodynamics of Tactical Weapons to Mach Number 8 and Angle of Attack of 180 degrees\", Paper No.82-0250, presented at AIAA 20th Aerospace Sciences meeting in Orlando, FL, Jan 1982 (written with L. Devan and L. Mason).\n\n18. \"Second-Order Shock Expansion Theory Extended to Include Real Gas Effects\", NAVSWC TR 90-683, Feb 1992 (written with M. Armistead, S. Rowles and F. DeJarnette) .\n\n19/20. \"A New Approximate Method for Calculating Real Gas Effects on Missile Configurations\", AIAA Paper No. 92-4637, Atmospheric Flight Mechanics Conference, Aug 1992 (written with Armistead, Rowles, DeJarnette) . Also Journal of Spacecraft and Rockets, Vol. 30, No.1, Jan-Feb 1993.\n\n21. \"New Methods for Predicting Nonlinear Lift, Center of Pressure, and Pitching Moment on Missile Configurations\", NSWCDD/TR-92/217, Jul1992 (written with T. Hymer and L. Devan) .\n\n22. \"A New Semiempirical Method for Computing Nonlinear Angle-of- Attack Aerodynamics on Wing-Body-Tail Configurations\", AIAA Paper No.93-0034 presented at 31st Aerospace Sciences Meeting, Jan 1993 (written with L. Devan and T. Hymer) .\n\n23. \"Incorporation of Boundary Layer Heating Predictive Methodology into the NAVSWC Aeroprediction Code\", NSWCDD TR-93/29, Apr 1993 (written with R. McInville) .\n\n24. \"Improved Empirical Model for Base Drag Prediction on Missile Configurations Based on New Wind Tunnel Data\", NSWCDD TR-92-509, Oct 1992, (written with F. Wilcox of NASA/LRC and T. Hymer of NSWC) .\n\n25/26. \"Base Drag Predictions of Missile Configurations\", AIAA Paper No.93-3629 presented at AIAA Atmospheric Flight Mechanics Conference in Monterey, CA, Aug 1993. (Also Journal of Spacecraft and Rockets, Sept-Oct 1994, Vol. 31, No.5) (written with F. Wilcox of NASA/LRC and T. Hymer of NSWC).\n\n27. \"Improved Aeroprediction Code: Part I- Summary of New Methods and Comparison with Experiment\", NSWCDD TR-93/91, May 1993 (written with T. Hymer and R. McInville) .\n\n28. \"Improved Aeroprediction Code: Part II - Computer Program User's Guide and Listing\", NSWCDD TR-93/241, Aug 1993, (written with T. Hymer and R. McInville) .\n\n29. \"Application of 1993 Version of the Aeroprediction Code (AP93) to Several Missile Configurations\", NSWCDD/TR-93/349, Sep 1993 (written with T. Hymer)\n\n30. \"Planar Nonlinear Missile Aeroprediction Code for All Mach Numbers\", AIAA- Paper No.94-0026, 32nd Aerospace Sciences Meeting, Reno, NV, Jan 1994 (written with R. McInville and T. Hymer) .\n\n31. \"A New Semiempirical Method for Computing Nonlinear Missile Aerodynamics\", AIAA Journal of Spacecraft and Rockets, Nov-Dec 1993 (written with L. Devan and T. Hymer) .\n\n32. \"State-of-the-Art Engineering Aeroprediction Methods with Emphasis on New Semiempirical Techniques for Predicting Nonlinear Aerodynamics on Complete Missile Configurations\", NSWCDD/TR-93/551, Nov 1993.\n\n33. \"Engineering Codes: State-of-the-Art and New Methods\", AGARD Paper No.2 on Missile Aerodynamics Given at Brussels, Belgium and Ankara, Turkey, June 1994.\n\n34. \"Incorporation of Boundary Layer Heating Predictive Methodology into the NAVSWC Aeroprediction Code\", AIAA Paper No.2001, presented at 6th AIAA/ASME Joint Thermo physics and Heat Transfer Conference, Colorado Springs, CO, June 1994 (written with R. McInville) .\n\n35. \"Users guide for an Interactive, Personal Computer Interface for the Aeroprediction Code\", NSWCDD/TR-94/107, June 1994 (written with T. Hymer and C. Downs of Vitro) .\n\n36. \"A New Method for Calculating Wing Alone Aerodynamics to Angle of Attack 180 Degrees\", NSWCDD/TR-94/3, March 1994 (written with R. McInville) .\n\n37. \"An Improved Version of the Naval Surface Warfare Center Aeroprediction Code (AP93)\", Journal of Spacecraft and Rockets, Sept-Oct 1994, Vol. 31, No.5, pp. 783–791 (written with R. McInville and T. Hymer) .\n\n38. \"The 1995 Version of the NSWC Aeroprediction Code:\nPart I - Summary of New Theoretical Methodology\", NSWCDD/TR-94/379, Feb 1995 (written with R. McInville and T. Hymer) .\n\n39. \"The 1995 Version of the NSWC Aeroprediction Code:\nPart II - Computer Program Users Guide and Listing\", NSWCDD/TR-94, March 1995 (written with T. Hymer and R. McInville) .\n\n40. \"Calculation of Wing-Alone Aerodynamics to High Angles of Attack\", Journal of Spacecraft and Rockets, Jan-Feb 1995, Vol. 32, No.1, pp. 187–189 (written with R. McInville) .\n\n41. \"A New Method for Calculating Wing Alone Aerodynamics to Angle of Attack 180 Degrees\", AIAA Paper 95-0757, presented at 33 Aerospace Sciences Meeting, Jan 9-12, 1995 at Reno, NV, (written with R. McInville) .\n\n42. \"Extension of the NSWCDD Aeroprediction Code to the Roll Position of 45 Degrees\", NSWCDD/TR-95/160, Dec. 1995 (written with R. McInville) .\n\n43. \"Extension of the NSWCDD Aeroprediction Code Above Angle of Attack Thirty Degrees\", Paper No.96-0065 34th Aerospace Sciences Meeting in Reno, NV, Jan 15-18, 1996 (written with R. McInville and T. Hymer) .\n\n44. \"Aeroprediction Code for Angle of Attack Above 30 Degrees\", JSR, Vol. 33, No.3, May - June 1996, pp. 366–373, (written with R. McInville and T. Hymer) .\n\n45. \"A New Semiempirical Model for Wing-Tail Interference\", AIAA Paper No.96-3393, AFM, San Diego, CA, July 29–31, 1996 (written with R. McInville) .\n\n46. \"Nonlinear Structural Load Distribution Methodology for the Aeroprediction Code\", NSWC TR 96/133, Sept. 1966 (written with R. McInville and C. Housh of NAWCCL) .\n\n47. \"Calculation of Wing-Alone Aerodynamics to High Angles of Attack\", Journal of Spacecraft and Rockets, Jan-Feb 1995, Vol. 32, No.1, pp. 187–189 (written with R. McInville) .\n\n48. \"Aeroprediction Methodology for Roll Positions of 0 and 45 Degrees\", paper presented at Session 6B of AIAA Missile Sciences Conference, Monterey, CA, 3–5 December 1996 (Papers archived at DTIC/OCP, 8725 John Ray Kingman Road, Suite 0944, Fort Belvoir, VA 22060-6218) . (written by R. McInville)\n\n49. \"New Semiempirical Model for Wing Tail Interference\", JSR, Vol. 34, No.1, Jan-Feb 1997, pp. 48–53. (written by R. McInville)\n\n50. \"Nonlinear Aeroprediction Methodology for roll Position of 45 Degrees\", JSR, Vol. 34 No.1 Jan-Feb 1997, pp. 54–61. (written by R. McInville)\n\n51. \"An Improved Method for Predicting Axial Force at High Angle of Attack\", NSWCDD/TR96/240, Feb 1997. (written by T. Hymer)\n\n52. \"Current Status and Future Plans of the Aeroprediction Code\", invited AIAA Paper No.97-2279, 1997 AIAA Applied Aerodynamics conference, 24 June 1997, Atlanta, GA.\n\n53/54. \"Methods for distributing Semiempirical, Nonlinear, Aerodynamic Loads on Missile Components\", AIAA Paper No. 97-1969, 29th AIAA Fluid Dynamics conference, 30 June-2 July 1997, Snowmass Village, CO (written by R. McInville and C. Housh of NAWCCL and JSR), Vol. 34 No 6 Nov-Dec 1997, pp 744–752.\n55. \"An Improved Semiempirical Method for Calculating Aerodynamics of Missiles With Noncircular Bodies\", NSWCDD/TR-97/20, Sep 97 (written with R. McInville and T. Hymer) .\n\n56/57. \"Improved Methodology for Axial Force Prediction at Angle of Attack, \" AIAA Paper 98-0579, 36th Aerospace Sciences Meeting, Jan 1998 and JSR Vol. 35, No.2, March–April 1998, pp 132–139 (written with T. Hymer) .\n\n58. \"The 1998 Version of the NSWC Aeroprediction Code : Part I - Summary of New Theoretical methodology\", NSWC/TR98/1, Apr 98 (written with R. McInville and T. Hymer) .\n\n59. \"A Review of Some Recent New and Improved Semiempirical Aeroprediction methods\", paper presented to the Applied Vehicle Technology panel of NATO in Sorrento, Italy, 11–15 May 1998 (written with R. McInville and T. Hymer) .\n\n60. \"User's guide for an Interactive Personal Computer Interface for the 1998 Aeroprediction Code (AP98) \", NSWCDD/TR-98/7, Jun 98 (written with T. Hymer and C.Downs) .\n\n61. \"The 1998 Version of the NSWCDD Aeroprediction Code: Part II - Program User's Guide and Source Code Listing\", NSWCDD/TR-97/73, August 1998 (written with R. McInvi11e and T. Hymer) .\n\n62. \"A Robust Method for Calculating Aerodynamics of Noncircular-Cross section Weapons\", AIAA Paper 98-4270, pp. 323–340, presented at AIAA AFM Conference, Boston, Massachusetts, Aug 98 (written with R. McInville and T.Hymer) .\n\n63. \"Review and Extension of Computational Methods for Noncircular-Cross Section Weapons\", JSR, Vol. 35, No.5, Sept-Oct 1998, pp. 584–600 (written with R. McInville and T. Hymer) .\n\n64. \"The 1998 Version of the Aeroprediction Code\", AIAA Paper 99-0762, AIAA 37th Aerospace -Sciences Meeting, Reno, Nevada, Jan. 1999 (written with R. McInville and T.Hymer) .\n\n65. \"A Simplified Method for Predicting Aerodynamics of Multi-Fin Weapons\", NSWCDD/TR-99/19, March 1999 (written with R. McInville and D. Robinson) .\n\n66. \"Application of the 1998 Version of the Aeroprediction Code\", JSR Vol. 36, No.5, Sept-Oct, 1999, pp. 633–645.\n\n67. \"Refinements in the Aeroprediction Code Based on Recent Wind Tunnel Data\", NSWCDD/TR-99/116, December 1999 (written with R. McInville) .\n\n68. \"A Semiempirical method for Predicting Multifin Weapon Aerodynamics\", AIAA Paper 2000-0766, 38th Aerospace Sciences Meeting, Reno, NV, Jan 10-13, 2000 (written with R. McInville and D. Robinson) .\n\n69. \"Improvements in Pitch Damping for the Aeroprediction Code with Particular Emphasis on Flare Configurations\", NSWCDD TR-00/009, April 2000 (written with T. Hymer) .\n\n70. \"Modifications to the Aeroprediction Code Based on Recent Test Data\", AIAA Paper presented at the AIAA Atmospheric Flight Mechanics Conference, Denver, CO, 14–17 August 2000 (written with R. McInville) .\n\n71. Approximate Methods for Weapon Aerodynamics, Book published by AIAA progress in Astronautics and Aeronautics, Vol 186, August 2000\n\n72. Improved Power-on, Base Drag Methodology for the Aeroprediction Code,\" NSWCDD/TR-00/67, Oct 2000. (written with T. Hymer)\n\n73.\"Semiempirical Prediction of Pitch Damping Moments for Configurations with - Flares,\" AIAA paper 2001-0101, Reno, NV, Jan 2001 (written with T. Hymer)\n\n74. \"Evaluation and Improvements to the Aeroprediction Code\nBased on Recent Test Data,\" JSR Vol. 37, No.6, Nov.-Dec. 2000, pp. 720–730. (written with R. McInville and T. Hymer)\n\n75. \"Semiempirical Prediction of Pitch Damping Moments for Configuration with Flares\", JSR Vo138, No. 2, March-Apri12001, pps. 150-158. (written with T. Hymer)\n\n76. \"Improved Power-On, Base Drag Methodology for the Aeroprediction Code,\" NSWCDD/TR-00/67,May,2001. (written with T. Hymer)\n\n77/78. \"An Improved Semiempirical Method for Power-On Base Drag Prediction,\" AIAA Paper No.2001-4328, Aug. 2001, and JSR, Vol, No. 1, Jan. – Feb 2002, pp. (written with T. Hymer)\n\n79. \"A SemiempiricalMethod for Predicting Aerodynamics of Trailing Edge Flaps,\" NSWCDD/TR 01/30, June 2001 (written with T. Hymer)\n\n80/81. \"A Semiempirical Method for Predicting Aerodynamics of Trailing Edge Flaps\", AIAA Paper NO. 2002-4510 Aug. 2002 and JSR, Vol. 40, NO.1, Jan-Feb 2003 (written with T. Hymer).\n\n82. \"The 2002 Version of the Aeroprediction Code: Part I- Summary of New Theoretical Methodology\", NSWCDD/TR-01/108, March 2002 (written with T. Hymer)\n\n83.\"The 2002 version of the Aeroprediction Code: Part II Users Guide\", NSWCDD Technical Report in publication (written with T. Hymer)\n\n84. \"Integration of the Aeroprediction Code with a Point Mass Ballistic Model (TRAMOD) and a Trim Three Degree-of-Freedom Model (MEM),\" NSWCDD/ TR-00/77, March 2002 (written with T. Hymer)\n\n85/86. \"The 2002 Version of the Aeroprediction Code\", AIAA Paper NO. 2003-26, Jan 6-9, 2003 and JSR, Vol. 41, NO.2, March – April 2004, pps. 232-247 (written with T. Hymer).\n\n87/88. \"An Approximate Method to Estimate Wing Trailing-Edge Bluntness Effects on Normal Force\", AIAA Paper NO. 2004-16, Jan. 2004 and JSR Vol. 41, NO. 6, Nov. –Dec. 2004, pps. 932-941 (written with T. Hymer).\n\n89. \"Application of the 2002 Version of the Aeroprediction Code\", RAES Aerospace Aerodynamics Research Conference, 10–12 June 2003, London, United Kingdom (written with T. Hymer).\n\n90. \"The 2005 Version of the Aeroprediction Code Part I- Summary of the New Theoretical Methodology\", API Report NO. 1, Jan. 2004 (written with T. Hymer).\n\n91. \"The 2005 Version of the Aeroprediction Code Part II- Users Guide\", API Report NO. 2, June 2004 (written with T. Hymer, Cornell Downs, and L Moore).\n\n92/93. \"The 2005 Version of the Aeroprediction Code\" AIAA Paper NO. 2004-4715, Aug . 2004 and JSR Vol. 42, No.2, March–April, 2005 (written with T. Hymer).\n\n94/95. \"Improved Aerodynamics for Configurations with Boattails, AIAA Paper Presented at Atmospheric Flight Mechanics Conference Hilton Head Island, SC, August 2007, and JSR, Vol. 45, No. 2, March–April 2008, pps 270-281,(written with L. Moore).\n\n96/97. \"New Methods To Predict Nonlinear Pitch Damping,\" AIAA Paper presented at 46th AIAA Aerospace Sciences Meeting, Reno, NV, Jan. 2008 and JSR, Vol. 45, No. 3, May–June 2008, pp. 495 – 503 (written with L. Moore).\n\n98/99. \"2009 Version of the Aeroprediction Code: AP09,\" AIAA paper presented at 47th AIAA Aerospace Sciences Meeting, Orlando, Fl, Jan. 2009 and JSR, Vol. 45, No.4, July- Aug 2008, pp 677–690 (written with L. Moore).\n\n100/101. \"New Method To Predict Nonlinear Roll Damping Moments,\" AIAA paper presented at the 38th Fluid Dynamics Conference, Seattle, WA., June 2008, and JSR Vol 45, No. 5, Sept. – Oct. 2008, pp 955 – 964 (written with L. Moore).\n\n102. \"The 2009 Version of the Aeroprediction Code: The AP09\", API Report No. 3, Jan. 2008 (written with L. Moore).\n"}
{"id": "29564581", "url": "https://en.wikipedia.org/wiki?curid=29564581", "title": "Aircraft design process", "text": "Aircraft design process\n\nThe aircraft design process is the engineering design process by which aircraft are designed. These depend on many factors such as customer and manufacturer demand, safety protocols, physical and economic constraints etc. For some types of aircraft the design process is regulated by national airworthiness authorities. This article deals with powered aircraft such as airplanes and helicopter designs.\n\nAircraft design is a compromise between many competing factors and constraints and accounts for existing designs and market requirements to produce the best aircraft.\n\nThe design process starts with the aircraft's intended purpose. Commercial airliners are designed for carrying a passenger or cargo payload, long range and greater fuel efficiency where as fighter jets are designed to perform high speed maneuvers and provide close support to ground troops. Some aircraft have specific missions, for instance, amphibious airplanes have a unique design that allows them to operate from both land and water, some fighters, like the Harrier Jump Jet, have VTOL (Vertical Take-off and Landing) ability, helicopters have the ability to hover over an area for a period of time.\n\nThe purpose may be to fit a specific requirement, e.g. as in the historical case of a British Air Ministry specification, or fill a perceived \"gap in the market\"; that is, a class or design of aircraft which does not yet exist, but for which there would be significant demand.\n\nAnother important factor that influences the design of the aircraft are the regulations put forth by national airworthiness authorities.\n\nAirports may also impose limits on aircraft, for instance, the maximum wingspan allowed for a conventional aircraft is 80 m to prevent collisions between aircraft while taxiing.\n\nBudget limitations, market requirements and competition set constraints on the design process and comprise the non-technical influences on aircraft design along with environmental factors. Competition leads to companies striving for better efficiency in the design without compromising performance and incorporating new techniques and technology.\n\nIn the 1950s and ’60s, unattainable project goals were regularly set, but then abandoned, whereas today troubled programs like the Boeing 787 and the Lockheed Martin F-35 have proven far more costly and complex to develop than expected.\nMore advanced and integrated design tools have been developed. Model-based systems engineering predicts potentially problematic interactions, while computational analysis and optimization allows designers to explore more options early in the process. Increasing automation in engineering and manufacturing allows faster and cheaper develoment.\nTechnology advances from materials to manufacturing enable more complex design variations like multifunction parts. Once impossible to design or construct, these can now be 3D printed, but they have yet to prove their utility in applications like the Northrop Grumman B-21 or the re-engined A320neo and 737 MAX. Airbus and Boeing also recognize the economic limits, that the next airliner generation cannot cost more than the previous ones did.\n\nAn increase in the number of aircraft also means greater carbon emissions. Environmental scientists have voiced concern over the main kinds of pollution associated with aircraft, mainly noise and emissions. Aircraft engines have been historically notorious for creating noise pollution and the expansion of airways over already congested and polluted cities have drawn heavy criticism, making it necessary to have environmental policies for aircraft noise. Noise also arises from the airframe, where the airflow directions are changed. Improved noise regulations have forced designers to create quieter engines and airframes. Emissions from aircraft include particulates, carbon dioxide (CO), Sulfur dioxide(SO), Carbon monoxide (CO), various oxides of nitrates and unburnt hydrocarbons. To combat the pollution, ICAO set recommendations in 1981 to control aircraft emissions. Newer, environmentally friendly fuels have been developed and the use of recyclable materials in manufacturing have helped reduce the ecological impact due to aircraft. Environmental limitations also affect airfield compatibility. Airports around the world have been built to suit the topography of the particular region. Space limitations, pavement design, runway end safety areas and the unique location of airport are some of the airport factors that influence aircraft design. However changes in aircraft design also influence airfield design as well, for instance, the recent introduction of new large aircraft (NLAs) such as the superjumbo Airbus A380, have led to airports worldwide redesigning their facilities to accommodate its large size and service requirements.\n\nThe high speeds, fuel tanks, atmospheric conditions at cruise altitudes, natural hazards (thunderstorms, hail and bird strikes) and human error are some of the many hazards that pose a threat to air travel.\n\nAirworthiness is the standard by which aircraft are determined fit to fly. The responsibility for airworthiness lies with national aviation regulatory bodies, manufacturers, as well as owners and operators.\n\nThe International Civil Aviation Organization sets international standards and recommended practices for national authorities to base their regulations on The national regulatory authorities set standards for airworthiness, issue certificates to manufacturers and operators and the standards of personnel training. Every country has its own regulatory body such as the Federal Aviation Authority in USA, DGCA (Directorate General of Civil Aviation) in India, etc.\n\nThe aircraft manufacturer makes sure that the aircraft meets existing design standards, defines the operating limitations and maintenance schedules and provides support and maintenance throughout the operational life of the aircraft. The aviation operators include the passenger and cargo airliners, air forces and owners of private aircraft. They agree to comply with the regulations set by the regulatory bodies, understand the limitations of the aircraft as specified by the manufacturer, report defects and assist the manufacturers in keeping up the airworthiness standards.\n\nMost of the design criticisms these days are built on crashworthiness. Even with the greatest attention to airworthiness, accidents still occur. Crashworthiness is the qualitative evaluation of how aircraft survive an accident. The main objective is to protect the passengers or valuable cargo from the damage caused by an accident. In the case of airliners the stressed skin of the pressurized fuselage provides this feature, but in the event of a nose or tail impact, large bending moments build all the way through the fuselage, causing fractures in the shell, causing the fuselage to break up into smaller sections. So the passenger aircraft are designed in such a way that seating arrangements are away from areas likely to be intruded in an accident, such as near a propeller, engine nacelle undercarriage etc. The interior of the cabin is also fitted with safety features such as oxygen masks that drop down in the event of loss of cabin pressure, lockable luggage compartments, safety belts, lifejackets, emergency doors and luminous floor strips. Aircraft are sometimes designed with emergency water landing in mind, for instance the Airbus A330 has a 'ditching' switch that closes valves and openings beneath the aircraft slowing the ingress of water.\n\nAircraft designers normally rough-out the initial design with consideration of all the constraints on their design. Historically design teams used to be small, usually headed by a Chief Designer who knows all the design requirements and objectives and coordinated the team accordingly. As time progressed, the complexity of military and airline aircraft also grew. Modern military and airline design projects are of such a large scale that every design aspect is tackled by different teams and then brought together. In general aviation a large number of light aircraft are designed and built by amateur hobbyists and enthusiasts.\n\nIn the early years of aircraft design, designers generally used analytical theory to do the various engineering calculations that go into the design process along with a lot of experimentation. These calculations were labour-intensive and time-consuming. In the 1940s, several engineers started looking for ways to automate and simplify the calculation process and many relations and semi-empirical formulas were developed. Even after simplification, the calculations continued to be extensive. With the invention of the computer, engineers realized that a majority of the calculations could be automated, but the lack of design visualization and the huge amount of experimentation involved kept the field of aircraft design stagnant. With the rise of programming languages, engineers could now write programs that were tailored to design an aircraft. Originally this was done with mainframe computers and used low-level programming languages that required the user to be fluent in the language and know the architecture of the computer. With the introduction of personal computers, design programs began employing a more user-friendly approach.\n\nThe main aspects of aircraft design are:\n\nAll aircraft designs involve compromises of these factors to achieve the design mission.\n\nThe wing of a fixed-wing aircraft provides the lift necessary for flight. Wing geometry affects every aspect of an aircraft’s flight. The wing area will usually be dictated by the desired stalling speed but the overall shape of the planform and other detail aspects may be influenced by wing layout factors. The wing can be mounted to the fuselage in high, low and middle positions. The wing design depends on many parameters such as selection of aspect ratio, taper ratio, sweepback angle, thickness ratio, section profile, washout and dihedral. The cross-sectional shape of the wing is its airfoil. The construction of the wing starts with the rib which defines the airfoil shape. Ribs can be made of wood, metal, plastic or even composites.\n\nThe wing must be designed and tested to ensure it can withstand the maximum loads imposed by maneuvering, and by atmospheric gusts.\n\nThe fuselage is the part of the aircraft that contains the cockpit, passenger cabin or cargo hold.\n\nAircraft propulsion may be achieved by specially designed aircraft engines, adapted auto, motorcycle or snowmobile engines, electric engines or even human muscle power. The main parameters of engine design are:\n\nThe thrust provided by the engine must balance the drag at cruise speed and be greater than the drag to allow acceleration. The engine requirement varies with the type of aircraft. For instance, commercial airliners spend more time in cruise speed and need more engine efficiency. High-performance fighter jets need very high acceleration and therefore have very high thrust requirements.\n\nThe weight of the aircraft is the common factor that links all aspects of aircraft design such as aerodynamics, structure, and propulsion, all together. An aircraft's weight is derived from various factors such as empty weight, payload, useful load, etc. The various weights are used to then calculate the center of mass of the entire aircraft. The center of mass must fit within the established limits set by the manufacturer.\n\nThe aircraft structure focuses not only on strength, stiffness, durability (fatigue), fracture toughness, stability, but also on fail-safety, corrosion resistance, maintainability and ease of manufacturing. The structure must be able to withstand the stresses caused by cabin pressurization, if fitted, turbulence and engine or rotor vibrations.\n\nThe design of any aircraft starts out in three phases\n\nThe first design step, involves sketching a variety of possible aircraft configurations that meet the required design specifications. By drawing a set of configurations, designers seek to reach the design configuration that satisfactorily meets all requirements as well as go hand in hand with factors such as aerodynamics, propulsion, flight performance, structural and control systems. This is called design optimization. Fundamental aspects such as fuselage shape, wing configuration and location, engine size and type are all determined at this stage. Constraints to design like those mentioned above are all taken into account at this stage as well. The final product is a conceptual layout of the aircraft configuration on paper or computer screen, to be reviewed by engineers and other designers.\n\nThe design configuration arrived at in the conceptual design phase is then tweaked and remodeled to fit into the design parameters. In this phase, wind tunnel testing and computational fluid dynamic calculations of the flow field around the aircraft are done. Major structural and control analysis is also carried out in this phase. Aerodynamic flaws and structural instabilities if any are corrected and the final design is drawn and finalized. Then after the finalization of the design lies the key decision with the manufacturer or individual designing it whether to actually go ahead with the production of the aircraft. At this point several designs, though perfectly capable of flight and performance, might have been opted out of production due to their being economically nonviable.\n\nThis phase simply deals with the fabrication aspect of the aircraft to be manufactured. It determines the number, design and location of ribs, spars, sections and other structural elements. All aerodynamic, structural, propulsion, control and performance aspects have already been covered in the preliminary design phase and only the manufacturing remains. Flight simulators for aircraft are also developed at this stage.\n\nAn existing aircraft program can be developed for performance and economy gains by stretching the fuselage, increasing the MTOW, enhancing the aerodynamics, installing new engines, new wings or new avionics.\nFor a 9,100 nmi long range at Mach 0.8/FL360, a 10% lower TSFC saves 13% of fuel, a 10% L/D increase saves 12%, a 10% lower OEW saves 6% and all combined saves 28%.\n\n\n\n"}
{"id": "51333097", "url": "https://en.wikipedia.org/wiki?curid=51333097", "title": "Akinbode Oluwafemi", "text": "Akinbode Oluwafemi\n\nAkinbode Oluwafemi is a Nigerian environmental activist, and tobacco control advocate. He is the Deputy Director of Environmental Rights Action/Friends of the Earth Nigeria (ERA/FoEN). He is a recipients of the 2009 Bloomberg Awards for Global Tobacco Control. The award ceremony was held at the 14th World Conference on Tobacco or Health held in Mumbai, India. \n"}
{"id": "876272", "url": "https://en.wikipedia.org/wiki?curid=876272", "title": "Ammonal", "text": "Ammonal\n\nAmmonal is an explosive made up of ammonium nitrate and aluminium powder, not to be confused with T-ammonal which contains trinitrotoluene as well to increase properties such as brisance.\n\nThe ammonium nitrate functions as an oxidizer and the aluminium as fuel. The use of the relatively cheap ammonium nitrate and aluminium makes it a replacement for pure TNT.\n\nThe mixture is affected by humidity because ammonium nitrate is highly hygroscopic. Ammonal's ease of detonation depends on fuel and oxidizer ratios, 95:5 ammonium nitrate and aluminum being fairly sensitive, however not very oxygen balanced. Even copper metal traces are known to sensitize bulk amounts of ammonium nitrate and further increase danger of spontaneous detonation during a fire, most likely due to the formation of tetramines. More oxygen balanced mixtures are not easily detonated, requiring a fairly substantial shock, though it remains more sensitive than trinitrotoluene and C-4.\n\nThe detonation velocity of ammonal is approximately 4,400 metres per second or 9,842 miles per hour.\n\nFrom early 1916, the British Army employed ammonal for their mines during World War I, starting with the Hawthorn Ridge mine during the Battle of the Somme, and reaching a zenith in the mines in the Battle of Messines which were exploded on 7 June 1917 at the start of the Third Battle of Ypres (also known as the Battle of Passchendaele). Several of the mines in the Battle of Messines contained 30,000 lbs (over 13.6 tonnes) of ammonal, and others contained 20,000 lbs (over 9 tonnes). The joint explosion of the ammonal mines beneath the German lines at Messines created 19 large craters, killing 10,000 German soldiers in one of the largest non-nuclear explosions in history. Not all of the mines laid by the British Army at Messines were detonated, however. Two mines were not ignited in 1917 because they had been abandoned before the battle, and four were outside the area of the offensive. On 17 July 1955, a lightning strike set off one of these four latter mines. There were no human casualties, but one cow was killed. Another of the unused mines is believed to have been found in a location beneath a farmhouse, but no attempt has been made to remove it.\n\nAmmonal used for military mining purposes was generally contained within metal cans or rubberised bags to prevent moisture ingress problems. The composition of ammonal used at Messines was 65% ammonium nitrate, 17% aluminium, 15% trinitrotoluene (TNT), and 3% charcoal. Ammonal remains in use as an industrial explosive. Typically, it is used for quarrying or mining purposes.\n\nETA, a Basque separatist organisation, used 250 kg of ammonal in a car bomb in its attack on the Zaragoza barracks on 11 December 1987 in Zaragoza, Spain.\n\nA T-ammonal mixture previously used in hand grenades and shells has the proportions (by mass):\n\n"}
{"id": "59191356", "url": "https://en.wikipedia.org/wiki?curid=59191356", "title": "Auguste Rateau", "text": "Auguste Rateau\n\nAuguste Rateau (13 October 1863 – 13 January 1930) was an engineer and industrialist born in Royan, France, specializing in turbines.\n\nAfter studies, first at the École Polytechnique and then at the École des Mines de Paris, he began his career as a teacher at the École des Mines de Saint-Étienne from 1888 to 1897.\n\nHe then embarked on an industrial career exploiting turbines. He manufactured fans for mines, blowers for steel mills, water pumps, and steam turbines for ships. To this end, he created a design office in Paris and, in 1903, the \"Société pour l’exploitation des appareils Rateau\", which moved in 1917 to La Courneuve where he opened a factory two years later. The company has since been integrated into Alstom, and La Courneuve's premises are still operated by GE Power Service. This entity manages the maintenance of turbomachines, turbo-pumps and turbocompressors installed on nuclear, thermal and industrial sites in France and in some parts of the world.\n\nIn 1928 he founded the \"Association Française de Normalisation\", the French national organization for standardization, and served as president.\n\nRateau was a Commander of the Legion of Honour () and a member of the French Academy of Sciences ().\n"}
{"id": "312943", "url": "https://en.wikipedia.org/wiki?curid=312943", "title": "Blacklight", "text": "Blacklight\n\nA blacklight (or often black light), also referred to as a UV-A light, Wood's lamp, or simply ultraviolet light, is a lamp that emits long-wave (UV-A) ultraviolet light and not much visible light.\n\nOne type of lamp has a violet filter material, either on the bulb or in a separate glass filter in the lamp housing, which blocks most visible light and allows through UV, so the lamp has a dim violet glow when operating. Blacklight lamps which have this filter have a lighting industry designation that includes the letters \"BLB\". This stands for \"blacklight blue\", which is a contradiction in that they are the type that does \"not\" look blue.\n\nA second type of lamp produces ultraviolet but does not have the filter material, so it produces more visible light and has a blue color when operating. These tubes are made for use in \"bug zapper\" insect traps, and are identified by the industry designation \"BL\". \n\nBlacklight sources may be specially designed fluorescent lamps, mercury-vapor lamps, light-emitting diodes (LEDs), lasers, or incandescent lamps; although incandescents produce almost no blacklight (except slightly more for halogen types), and so are not considered true blacklight sources. In medicine, forensics, and some other scientific fields, such a light source is referred to as a Wood's lamp, named after Robert Williams Wood who invented the original Wood's glass UV filters.\n\nAlthough many other types of lamp emit ultraviolet light with visible light, black lights are essential when UV-A light without visible light is needed, particularly in observing fluorescence, the colored glow that many substances emit when exposed to UV. Black lights are employed for decorative and artistic lighting effects, diagnostic and therapeutic uses in medicine, the detection of substances tagged with fluorescent dyes, rock-hunting, the detection of counterfeit money, the curing of plastic resins, attracting insects and the detection of refrigerant leaks affecting refrigerators and air conditioning systems. Strong sources of long-wave ultraviolet light are used in tanning beds. Although the low-power UV-A emitted by black lights is not a hazard to skin or eyes and can be viewed without protection, powerful ultraviolet sources present dangers and require personal protective equipment such as goggles and gloves.\n\nFluorescent black light tubes are typically made in the same fashion as normal fluorescent tubes except that a phosphor that emits UVA light instead of visible white light is used. The type most commonly used for black lights, designated blacklight blue or \"BLB\" by the industry, has a dark blue filter coating on the tube, which filters out most visible light, so that fluorescence effects can be observed. These tubes have a dim violet glow when operating. They should not be confused with \"blacklight\" or \"BL\" tubes, which have no filter coating, and have a brighter blue color. These are made for use in \"bug zapper\" insect traps, where visible light emission is not a problem. The phosphor typically used for a near 368 to 371 nanometer emission peak is either europium-doped strontium fluoroborate (:) or europium-doped strontium borate (:) while the phosphor used to produce a peak around 350 to 353 nanometres is lead-doped barium silicate (:). \"Blacklight Blue\" lamps peak at 365 nm.\n\nManufacturers use different numbering systems for black light tubes. Philips uses one system which is becoming outdated (2010), while the (German) Osram system is becoming dominant outside North America. The following table lists the tubes generating blue, UVA and UVB, in order of decreasing wavelength of the most intense peak. Approximate phosphor compositions, major manufacturer's type numbers and some uses are given as an overview of the types available. \"Peak\" position is approximated to the nearest 10 nm. \"Width\" is the measure between points on the shoulders of the peak that represent 50% intensity.\n\nWood's glass tubes manufactured by Osram use a fairly narrow-band emitting phosphor, europium activated strontium pyroborate with a peak at about 370 nm, whereas North American and Philips Wood's glass tubes use lead-activated calcium metasilicate that emits a wider band with a shorter wavelength peak at about 350 nm. These two types seem to be the most commonly used. Different manufacturers offer either one or the other and sometimes both.\n\nBLB fluorescent lamps tend to run with efficiencies in the 25% range, with an example being a Phillips 40W BLB T12 lamp emitting 9.8W of UVA for 39 Watts of power input.\n\nAnother class of UV fluorescent bulb is designed for use in \"bug zapper\" flying insect traps. Insects are attracted to the UV light, which they are able to see, and are then electrocuted by the device. These bulbs use the same UV-A emitting phosphor blend as the filtered blacklight, but since they do not need to suppress visible light output, they do not use a purple filter material in the bulb. Plain glass blocks out less of the visible mercury emission spectrum, making them appear light blue-violet to the naked eye. These lamps are referred to by the designation \"blacklight\" or \"BL\" in some North American lighting catalogs. These types are not suitable for applications which require the low visible light output of \"BLB\" tubes lamps.\n\nA black light may also be formed by simply using a UV filter coating such as Wood's glass on the envelope of a common incandescent bulb. This was the method that was used to create the very first black light sources. Although incandescent black light bulbs are a cheaper alternative to fluorescent tubes, they are exceptionally inefficient at producing UV light since most of the light emitted by the filament is visible light which must be blocked. Due to its black body spectrum, an incandescent light radiates less than 0.1% of its energy as UV light. Incandescent UV bulbs, due to the necessary absorption of the visible light, become very hot during use. This heat is, in fact, encouraged in such bulbs, since a hotter filament increases the proportion of UVA in the black-body radiation emitted. This high running-temperature drastically reduces the life of the lamp, however, from a typical 1000 hours to around 100 hours.\n\nHigh power mercury vapor black light lamps are made in power ratings of 100 to 1,000 watts. These do not use phosphors, but rely on the intensified and slightly broadened 350–375 nm spectral line of mercury from high pressure discharge at between , depending upon the specific type. These lamps use envelopes of Wood's glass or similar optical filter coatings to block out all the visible light and also the short wavelength (UVC) lines of mercury at 184.4 and 253.7 nm, which are harmful to the eyes and skin. A few other spectral lines, falling within the pass band of the Wood's glass between 300 and 400 nm, contribute to the output.\nThese lamps are used mainly for theatrical purposes and concert displays. They are more efficient UVA producers per unit of power consumption than fluorescent tubes.\n\nUltraviolet light can be generated by some light-emitting diodes, but wavelengths below 380 nm are uncommon and the emission peaks are broad, so only the very lowest energy UV photons are emitted, within predominantly visible light.\n\nA Wood's lamp is a diagnostic tool used in dermatology by which ultraviolet light is shone (at a wavelength of approximately 365 nanometers) onto the skin of the patient; a technician then observes any subsequent fluorescence. For example, porphyrins—associated with some skin diseases—will fluoresce pink. Though the technique for producing a source of ultraviolet light was devised by Robert Williams Wood in 1903 using \"Wood's glass\", it was in 1925 that the technique was used in dermatology by Margarot and Deveze for the detection of fungal infection of hair. It has many uses, both in distinguishing fluorescent conditions from other conditions and in locating the precise boundaries of the condition.\n\nIt is also helpful in diagnosing:\n\nA Wood's lamp may be used to rapidly assess whether an individual is suffering from ethylene glycol poisoning as a consequence of antifreeze ingestion. Manufacturers of ethylene glycol-containing antifreezes commonly add fluorescein, which causes the patient's urine to fluoresce under Wood's lamp.\n\nWood's lamp is useful in diagnosing conditions such as tuberous sclerosis and erythrasma (caused by 'Corynebacterium minutissimum', see above). Additionally, detection of porphyria cutanea tarda can sometimes be made when urine turns pink upon illumination with Wood's lamp. Wood's lamps have also been used to differentiate hypopigmentation from depigmentation such as with vitiligo. A vitiligo patient's skin will appear yellow-green or blue under the Wood's lamp. Its use in detecting melanoma has been reported.\n\nBili light. A type of phototheraphy that uses blue light with a range of 420–470 nm, used to treat neonatal jaundice.\n\nAlthough black lights produce light in the UV range, their spectrum is mostly confined to the longwave UVA region, that is, UV radiation nearest in wavelength to visible light, with low frequency and therefore relatively low energy. While low, there is still some power of a conventional black light in the UVB range. UVA is the safest of the three spectra of UV light, although high exposure to UVA has been linked to the development of skin cancer in humans. The relatively low energy of UVA light does not cause sunburn. UVA is capable of causing damage to collagen fibers, however, so it does have the potential to accelerate skin aging and cause wrinkles. UVA can also destroy vitamin A in the skin.\n\nUVA light has been shown to cause DNA damage, but not directly, like UVB and UVC. Due to its longer wavelength, it is absorbed less and reaches deeper into skin layers, where it produces reactive chemical intermediates such as hydroxyl and oxygen radicals, which in turn can damage DNA and result in a risk of melanoma. The weak output of black lights, however, is not considered sufficient to cause DNA damage or cellular mutations in the way that direct summer sunlight can, although there are reports that overexposure to the type of UV radiation used for creating artificial suntans on sunbeds can cause DNA damage, photoaging (damage to the skin from prolonged exposure to sunlight), toughening of the skin, suppression of the immune system, cataract formation and skin cancer.\n\nUltraviolet radiation is invisible to the human eye, but illuminating certain materials with UV radiation causes the emission of visible light, causing these substances to glow with various colors. This is called \"fluorescence\", and has many practical uses. Black lights are required to observe fluorescence, since other types of ultraviolet lamps emit visible light which drowns out the dim fluorescent glow.\n\nBlack light is commonly used to authenticate oil paintings, antiques and banknotes. Black lights can be used to differentiate real currency from counterfeit notes because, in many countries, legal banknotes have fluorescent symbols on them that only show under a black light. In addition, the paper used for printing money does not contain any of the brightening agents which cause commercially available papers to fluoresce under black light. Both of these features make illegal notes easier to detect and more difficult to successfully counterfeit. The same security features can be applied to identification cards.\n\nOther security applications include the use of pens containing a fluorescent ink, generally with a soft tip, that can be used to \"invisibly\" mark items. If the objects that are so marked are subsequently stolen, a black light can be used to search for these security markings. At some theme parks, nightclubs and at other, day-long (or night-long) events, a fluorescent mark is rubber stamped onto the wrist of a guest who can then exercise the option of leaving and being able to return again without paying another admission fee.\n\nIn medicine, the Wood's lamp is used to check for the characteristic fluorescence of certain dermatophytic fungi such as species of \"Microsporum\" which emit a yellow glow, or corynebacterium which have a red to orange color when viewed under a Wood's lamp. Such light is also used to detect the presence and extent of disorders that cause a loss of pigmentation, such as vitiligo. It can also be used to diagnose other fungal infections such as ringworm, microsporum canis, tinea versicolor; bacterial infections such erythrasma; other skin conditions including acne, scabies, alopecia, porphyria; as well as corneal scratches, foreign bodies in the eye, and blocked tear ducts. \n\nFluorescent materials are also very widely used in numerous applications in molecular biology, often as \"tags\" which bind themselves to a substance of interest (for example, DNA), so allowing their visualization. Black light can also be used to see animal excreta such as urine and vomit that is not always visible to the naked eye.\n\nBlack light is used extensively in non-destructive testing. Fluorescing fluids are applied to metal structures and illuminated with a black light which allows cracks and other weaknesses in the material to be easily detected. It is also used to illuminate pictures painted with fluorescent colors, particularly on black velvet, which intensifies the illusion of self-illumination. The use of such materials, often in the form of tiles viewed in a sensory room under UV light, is common in the United Kingdom for the education of students with profound and multiple learning difficulties. Such fluorescence from certain textile fibers, especially those bearing optical brightener residues, can also be used for recreational effect, as seen, for example, in the opening credits of the James Bond film \"A View to a Kill\". Black light puppetry is also performed in a black light theater.\n\nOne of the innovations for night and all-weather flying used by the US, UK, Japan and Germany during World War II was the use of UV interior lighting to illuminate the instrument panel, giving a safer alternative to the radium-painted instrument faces and pointers, and an intensity that could be varied easily and without visible illumination that would give away an aircraft's position. This went so far as to include the printing of charts that were marked in UV-fluorescent inks, and the provision of UV-visible pencils and slide rules such as the E6B.\n\nThousands of moth and insect collectors all over the world use various types of black lights to attract moth and insect specimens for photography and collecting. It is one of the preferred light sources for attracting insects and moths at night.\n\nIt may also be used to test for LSD, which fluoresces under black light while common substitutes such as 25I-NBOMe do not.\n\nIn addition, if a leak is suspected in a refrigerator or an air conditioning system, a UV tracer dye can be injected into the system along with the compressor lubricant oil and refrigerant mixture. The system is then run in order to circulate the dye across the piping and components and then the system is examined with a blacklight lamp. Any evidence of fluorescent dye then pinpoints the leaking part which needs replacement.\n\n"}
{"id": "30701585", "url": "https://en.wikipedia.org/wiki?curid=30701585", "title": "Carbon Clear", "text": "Carbon Clear\n\nCarbon Clear is a sustainability consultancy specialising in helping organisations to reduce their environmental impact. Carbon Clear is a founding member of the International Carbon Reduction and Offset Alliance (ICROA).\n\nIn 2005, sustainable energy expert Jamal Gore and entrepreneur Mark Chadwick created Carbon Clear to help organisations and individuals to manage the challenges presented by climate change. Since then the market for sustainability management services has grown in line with the need for private and public sector organisations to reduce their greenhouse gas emissions, in response to new climate change legislation and the imperative for voluntary action.\n\nThe company headquarters is in London, UK, and the company now operates offices in the United States, Spain, France and Turkey. It was recognised by the ENDS Report in 2008 as a Quality Offset Provider.\n\nCarbon Clear offers a range of intelligent sustainability services including energy management solutions, carbon footprint services to ISO 14064 Standard and PAS 2050 standards, strategy and target setting and climate change risks and opportunities analysis. It develops its own carbon offset projects, including the award-winning Darfur Low Smoke Stoves project and sources Voluntary Carbon Standard or Gold Standard offset credits from projects around the world.\n\nThe company also offers consultancy services for organisations that must comply with legislation including the UK Government's CRC Energy Efficiency Legislation, the Energy Savings Opportunity Scheme, Mandatory Greenhouse Gas Reporting and the EU Emissions Trading Scheme.\n\nCarbon Clear develop an annual ranking examining the carbon reporting performance of FTSE 100 companies.\n\nCarbon Clear works with over 200 companies, including well known retail banks and retailers, partnering with them to help address complex business and sustainability challenges.\n\n"}
{"id": "1607968", "url": "https://en.wikipedia.org/wiki?curid=1607968", "title": "Castle thunder (sound effect)", "text": "Castle thunder (sound effect)\n\nCastle thunder is a sound effect that consists of the sound of a loud thunderclap during a rainstorm. It was originally recorded for the 1931 film \"Frankenstein\", and has since been used in dozens of films, television programs, and commercials.\n\nAfter its use in \"Frankenstein\", the Castle Thunder was used in dozens of films from the 1930s through the 1980s, including \"Citizen Kane\" (1941), \"Bambi\" (1942), \"Young Frankenstein\" (1974), \"Star Wars\" (1977), \"Ghostbusters\" (1984), \"Back to the Future\" (1985), and \"Big Trouble in Little China\" (1986). Use of the effect in subsequent years has declined because the quality of the original analog recording does not sufficiently hold up in modern sound mixes.\n\nThe effect appears in Disney, \"Peanuts\", and Hanna-Barbera cartoons, including the original \"Scooby-Doo\" animated series. It can also be heard at the Haunted Mansion attraction at Disney theme parks.\n\nThe sound can be found on a few sound effects libraries distributed by Sound Ideas (such as the Network Sound Effects Library, the 20th Century Fox Sound Effects Library and the Hanna-Barbera SoundFX Library).\n\n\n"}
{"id": "3392023", "url": "https://en.wikipedia.org/wiki?curid=3392023", "title": "Centre for Appropriate Technology (Australia)", "text": "Centre for Appropriate Technology (Australia)\n\nThe Centre for Appropriate Technology Inc (CAT) is an Australian indigenous controlled organisation, based in Alice Springs (Central Australia), which aims to improve the lives of indigenous Australians, using appropriate technology to improve their access to a range services. \n\nIt is headquartered in the Desert Knowledge Precinct alongside the Batchelor Institute of Indigenous Tertiary Education.\n\nBushlight is a division of CAT that builds and maintains renewable energy systems in remote homeland communities. These are generally standalone solar (or solar diesel hybrid) electricity systems. They are unusual in that there is a complex but user-friendly interface which assists demand management promoting efficient use of the available power. By 2011 around 120 systems had been installed across northern Australia, while a version of the Bushlight system is being trialled in India.\n\nCAT Projects is an engineering firm based in Alice Springs that is wholly owned by CAT. CAT Projects offers engineering services.\n\n\n "}
{"id": "1572095", "url": "https://en.wikipedia.org/wiki?curid=1572095", "title": "College of the Redwoods", "text": "College of the Redwoods\n\nCollege of the Redwoods (CR) is a public two-year community college with its main campus in Eureka, California. It is part of the Redwoods Community College District that serves four counties and has two branch campuses, as well as three additional sites. On-campus housing is available at the main campus.\n\nCollege of the Redwoods is one of 114 colleges in the California Community College system. The college offers a variety of transfer, vocational, and community-based classes, including its Fine Woodworking Program started by master woodworker James Krenov, a Police Academy, Nursing and Dental Programs, Truck Driving School, Computer Information Sciences, Computer-Aided Drafting, and Digital Media Departments, Yurok language and the Hospitality, Restaurant and Culinary Arts Program (added in 2006). The college is named after the Coast Redwood (\"Sequoia sempervirens\") trees native to the region.\n\nCR has a satellite branch campuses, CR Del Norte in Crescent City, Del Norte County. Another former satellite campus CR Mendocino Coast in Fort Bragg, Mendocino County, was transferred to Mendocino College and renamed as the Mendocino College Coast Center in 2017. CR also has other off-campus sites, including the Bianchi Farm in Shively, the Klamath-Trinity Instructional Site on the Hoopa Valley Tribe reservation, and the Southern Humboldt Instructional Site in Garberville in Southern Humboldt County. The Arcata Instructional Site, the McKinleyville Instructional Site, and the Eureka Downtown Instructional Site were closed in the summer of 2012, though Community Education re-located to a new Eureka downtown site.\n\nThe original Redwoods Community College District was formed in 1964 by a vote of the people of Humboldt County. Founding President Eugene J. Portugal and his wife Dottie Portugal shaped the look of the campus. In 1975, residents of the coastal portion of Mendocino County voted to join the District, and in 1978 Del Norte County similarly joined. The college serves these areas, as well as a portion of Trinity County.\n\nIn 2012, CR's regional accreditor Accrediting Commission for Community and Junior Colleges (ACCJC) placed the college on \"Show Cause\" status, warning the college that its accreditation might be withdrawn. Two years later removed the college from probation and reaffirmed its accreditation.\n\nThe college is part of the Redwoods Community College District, itself part of the California Community Colleges System. The district is governed by the elected seven-member Board of Trustees.\n\nBeginning with the passage of Proposition 13 by California in 1978, College of the Redwoods and most public institutions in the state have suffered declining revenue, and this has continued following the Dot-Com Bust. All of this occurs while simultaneously suffering increasing costs due to inflation, population growth, and increasingly unfunded state and federal mandates. In 2006, voters passed Bond Measure Q/B (Ballot Measure Q in Humboldt, northwest Mendocino and western Trinity counties; Ballot Measure B in Del Norte County) to allow issuance of $40,320,000 in bond funding to upgrade and renovate facilities at the main campus in Eureka and the branch campuses in Crescent City and Fort Bragg. Measure Q Bond Funds were also used to acquire the Garberville Site in Southern Humboldt County.\n\n\n\n"}
{"id": "318352", "url": "https://en.wikipedia.org/wiki?curid=318352", "title": "Corona discharge", "text": "Corona discharge\n\nA corona discharge is an electrical discharge brought on by the ionization of a fluid such as air surrounding a conductor that is electrically charged. Spontaneous corona discharges occur naturally in high-voltage systems unless care is taken to limit the electric field strength. A corona will occur when the strength of the electric field (potential gradient) around a conductor is high enough to form a conductive region, but not high enough to cause electrical breakdown or arcing to nearby objects. It is often seen as a bluish (or other color) glow in the air adjacent to pointed metal conductors carrying high voltages, and emits light by the same property as a gas discharge lamp.\n\nIn many high voltage applications corona is an unwanted side effect. Corona discharge from high voltage electric power transmission lines constitutes an economically significant waste of energy for utilities. In high voltage equipment like Cathode Ray Tube televisions, radio transmitters, X-ray machines and particle accelerators the current leakage caused by coronas can constitute an unwanted load on the circuit. In air, coronas generate gases such as ozone (O) and nitric oxide (NO), and in turn nitrogen dioxide (NO), and thus nitric acid (HNO) if water vapor is present. These gases are corrosive and can degrade and embrittle nearby materials, and are also toxic to people and the environment. Corona discharges can often be suppressed by improved insulation, corona rings, and making high voltage electrodes in smooth rounded shapes. However, controlled corona discharges are used in a variety of processes such as air filtration, photocopiers and ozone generators.\n\nA corona discharge is a process by which a current flows from an electrode with a high potential into a neutral fluid, usually air, by ionizing that fluid so as to create a region of plasma around the electrode. The ions generated eventually pass charge to nearby areas of lower potential, or recombine to form neutral gas molecules.\n\nWhen the potential gradient (electric field) is large enough at a point in the fluid, the fluid at that point ionizes and it becomes conductive. If a charged object has a sharp point, the electric field strength around that point will be much higher than elsewhere. Air near the electrode can become ionized (partially conductive), while regions more distant do not. When the air near the point becomes conductive, it has the effect of increasing the apparent size of the conductor. Since the new conductive region is less sharp, the ionization may not extend past this local region. Outside this region of ionization and conductivity, the charged particles slowly find their way to an oppositely charged object and are neutralized.\n\nAlong with the similar brush discharge, the corona is often called a \"single-electrode discharge\", as opposed to a \"two-electrode discharge\"; an electric arc. A corona only forms when the conductor is widely enough separated from conductors at opposite potential that an arc cannot jump between them. If the geometry and gradient are such that the ionized region continues to grow until it reaches another conductor at a lower potential, a low resistance conductive path between the two will be formed, resulting in an electric arc.\n\nCorona discharge only forms when the electric field (potential gradient) at the surface of the conductor exceeds a critical value, the dielectric strength or disruptive potential gradient of the fluid. In air at atmospheric pressure it is roughly 30 kilovolts per centimeter, but decreases with pressure, so corona is more of a problem at high altitudes. Corona discharge usually forms at highly curved regions on electrodes, such as sharp corners, projecting points, edges of metal surfaces, or small diameter wires. The high curvature causes a high potential gradient at these locations, so that the air breaks down and forms plasma there first. On sharp points in air corona can start at potentials of 2 - 6 kV. In order to suppress corona formation, terminals on high voltage equipment are frequently designed with smooth large diameter rounded shapes like balls or toruses, and corona rings are often added to insulators of high voltage transmission lines.\n\nCoronas may be \"positive\" or \"negative\". This is determined by the polarity of the voltage on the highly curved electrode. If the curved electrode is positive with respect to the flat electrode, it has a \"positive corona\", if it is negative, it has a \"negative corona\". (See below for more details.) The physics of positive and negative coronas are strikingly different. This asymmetry is a result of the great difference in mass between electrons and positively charged ions, with only the electron having the ability to undergo a significant degree of ionising inelastic collision at common temperatures and pressures.\n\nAn important reason for considering coronas is the production of ozone around conductors undergoing corona processes in air. A negative corona generates much more ozone than the corresponding positive corona.\n\nCorona discharge has a number of commercial and industrial applications:\n\n\nCoronas can be used to generate charged surfaces, which is an effect used in electrostatic copying (photocopying). They can also be used to remove particulate matter from air streams by first charging the air, and then passing the charged stream through a comb of alternating polarity, to deposit the charged particles onto oppositely charged plates.\n\nThe free radicals and ions generated in corona reactions can be used to scrub the air of certain noxious products, through chemical reactions, and can be used to produce ozone.\n\nCoronas can generate audible and radio-frequency noise, particularly near electric power transmission lines. Therefore, power transmission equipment is designed to minimise the formation of corona discharge.\n\nCorona discharge is generally undesirable in:\n\nIn many cases coronas can be suppressed by corona rings, toroidal devices that serve to spread the electric field over larger area and decrease the field gradient below the corona threshold.\n\nCorona discharge results when the electric field is strong enough to create a chain reaction: electrons in the air collide with atoms hard enough to ionize them, creating more electrons which ionize more atoms. The diagrams below illustrate at a microscopic scale the process which creates a corona in the air next to a pointed electrode carrying a high negative voltage with respect to ground. The process is: \n\nThermodynamically, a corona is a very \"nonequilibrium\" process, creating a non-thermal plasma. The avalanche mechanism does not release enough energy to heat the gas in the corona region generally and ionize it, as occurs in an electric arc or spark. Only a small number of gas molecules take part in the electron avalanches and are ionized, having energies close to the ionization energy of 1–3 ev, the rest of the surrounding gas is close to ambient temperature.\n\nThe onset voltage of corona or corona inception voltage (CIV) can be found with \"Peek's law\" (1929), formulated from empirical observations. Later papers derived more accurate formulas.\n\nA positive corona is manifested as a uniform plasma across the length of a conductor. It can often be seen glowing blue/white, though many of the emissions are in the ultraviolet. The uniformity of the plasma is caused by the homogeneous source of secondary avalanche electrons described in the mechanism section, below. With the same geometry and voltages, it appears a little smaller than the corresponding negative corona, owing to the lack of a non-ionising plasma region between the inner and outer regions.\n\nA positive corona has much lower density of free electrons compared to a negative corona; perhaps a thousandth of the electron density, and a hundredth of the total number of electrons.\nHowever, the electrons in a positive corona are concentrated close to the surface of the curved conductor, in a region of high potential gradient (and therefore the electrons have a high energy), whereas in a negative corona many of the electrons are in the outer, lower-field areas. Therefore, if electrons are to be used in an application which requires a high activation energy, positive coronas may support a greater reaction constants than corresponding negative coronas; though the total number of electrons may be lower, the number of very high energy electrons may be higher.\n\nCoronas are efficient producers of ozone in air. A positive corona generates much less ozone than the corresponding negative corona, as the reactions which produce ozone are relatively low-energy. Therefore, the greater number of electrons of a negative corona leads to an increased production.\n\nBeyond the plasma, in the \"unipolar region\", the flow is of low-energy positive ions toward the flat electrode.\n\nAs with a negative corona, a positive corona is initiated by an exogenous ionisation event in a region of high potential gradient. The electrons resulting from the ionisation are attracted \"toward\" the curved electrode, and the positive ions repelled from it. By undergoing inelastic collisions closer and closer to the curved electrode, further molecules are ionized in an electron avalanche.\n\nIn a positive corona, secondary electrons, for further avalanches, are generated predominantly in the fluid itself, in the region outside the plasma or avalanche region. They are created by ionization caused by the photons emitted from that plasma in the various de-excitation processes occurring within the plasma after electron collisions, the thermal energy liberated in those collisions creating photons which are radiated into the gas. The electrons resulting from the ionisation of a neutral gas molecule are then electrically attracted back toward the curved electrode, attracted \"into\" the plasma, and so begins the process of creating further avalanches inside the plasma.\n\nA negative corona is manifested in a non-uniform corona, varying according to the surface features and irregularities of the curved conductor. It often appears as tufts of corona at sharp edges, the number of tufts altering with the strength of the field. The form of negative coronas is a result of its source of secondary avalanche electrons (see below). It appears a little larger than the corresponding positive corona, as electrons are allowed to drift out of the ionising region, and so the plasma continues some distance beyond it. The total number of electrons, and electron density is much greater than in the corresponding positive corona. However, they are of a predominantly lower energy, owing to being in a region of lower potential-gradient. Therefore, whilst for many reactions the increased electron density will increase the reaction rate, the lower energy of the electrons will mean that reactions which require a higher electron energy may take place at a lower rate.\n\nNegative coronas are more complex than positive coronas in construction. As with positive coronas, the establishing of a corona begins with an exogenous ionization event generating a primary electron, followed by an electron avalanche.\n\nElectrons ionized from the neutral gas are not useful in sustaining the negative corona process by generating secondary electrons for further avalanches, as the general movement of electrons in a negative corona is outward from the curved electrode. For negative corona, instead, the dominant process generating secondary electrons is the photoelectric effect, from the surface of the electrode itself. The work function of the electrons (the energy required to liberate the electrons from the surface) is considerably lower than the ionization energy of air at standard temperatures and pressures, making it a more liberal source of secondary electrons under these conditions. Again, the source of energy for the electron-liberation is a high-energy photon from an atom within the plasma body relaxing after excitation from an earlier collision. The use of ionized neutral gas as a source of ionization is further diminished in a negative corona by the high-concentration of positive ions clustering around the curved electrode.\n\nUnder other conditions, the collision of the positive species with the curved electrode can also cause electron liberation.\n\nThe difference, then, between positive and negative coronas, in the matter of the generation of secondary electron avalanches, is that in a positive corona they are generated by the gas surrounding the plasma region, the new secondary electrons travelling inward, whereas in a negative corona they are generated by the curved electrode itself, the new secondary electrons travelling outward.\n\nA further feature of the structure of negative coronas is that as the electrons drift outwards, they encounter neutral molecules and, with electronegative molecules (such as oxygen and water vapor), combine to produce negative ions. These negative ions are then attracted to the positive uncurved electrode, completing the 'circuit'.\n\nIonized gases produced in a corona discharge are accelerated by the electric field, producing a movement of gas or \"electrical wind\". The air movement associated with a discharge current of a few hundred microamperes can blow out a small candle flame within about 1 cm of a discharge point. A pinwheel, with radial metal spokes and pointed tips bent to point along the circumference of a circle, can be made to rotate if energized by a corona discharge; the rotation is due to differential electric attraction between the metal spokes and the space charge shield region that surrounds the tips.\n\n\n\n"}
{"id": "1393235", "url": "https://en.wikipedia.org/wiki?curid=1393235", "title": "Cost-exchange ratio", "text": "Cost-exchange ratio\n\nIn anti-ballistic missile defence the cost-exchange ratio is the ratio of the incremental cost to the aggressor of getting one additional warhead through the defence screen, divided by the incremental cost to the defender of offsetting the additional missile. \n\nIn the early part of the Cold War, the cost-exchange ratio favoured the defender, since the cost of missiles increases rapidly with size, and interceptors are generally much smaller than ballistic missiles. This gave a strong impetus to the development and deployment of anti-ballistic missiles in the 1960s.\n\nHowever, the deployment of multiple independently targetable re-entry vehicles (MIRV's) required the defender to deploy multiple interceptors for each enemy missile, thus skewing the cost-exchange ratio in favour of the aggressor.\n\nConsideration of cost-exchange ratios was influential in persuading the United States and the Soviet Union to sign the ABM Treaty.\n"}
{"id": "11539990", "url": "https://en.wikipedia.org/wiki?curid=11539990", "title": "Eanna-shum-iddina kudurru", "text": "Eanna-shum-iddina kudurru\n\nThe Eanna-shum-iddina \"kudurru\" is a boundary stone of governor Eanna-shum-iddina in the Sealand Dynasty of Babylon in the mid 2nd millennium BC. Sealand was the region of southern Mesopotamia along the Persian Gulf.\n\nNB: The British Museum dates this kudurru to 1125-1100 BC:\n\nhttps://www.britishmuseum.org/explore/highlights/highlight_objects/me/b/boundary_stone_kudurru-6.aspx\n\nThe \"Eanna-shum-iddina kudurru\" was a land grant to a person named Gula-eresh, witnessed by his surveyor Amurru-bel-zeri. The iconography of the stone includes cuneiform text, two middle registers with gods, and a larger upper, scenic register of gods, with sky–glyph representations of gods.\n\n\n"}
{"id": "326545", "url": "https://en.wikipedia.org/wiki?curid=326545", "title": "Electricity delivery", "text": "Electricity delivery\n\nElectricity delivery is the process that goes from generation of electricity in the power station to the use by the consumer.\n\nThe main processes in electricity delivery are, by order:\n\n\n"}
{"id": "58504377", "url": "https://en.wikipedia.org/wiki?curid=58504377", "title": "Fitzroy Gasworks", "text": "Fitzroy Gasworks\n\nThe Fitzroy Gasworks was a coal gasification plant in Fitzroy, Victoria. It is notable as the site for the first arc-welded gasholder in the world.\n\nIn 1856 the first gas supply had been instigated in Melbourne, Australia, with the construction of the West Melbourne Gasworks. In the same year, residents of the Fitzroy Ward of the Corporation of Melbourne met at Clarke's Hotel, Smith Street, \"for the purpose of considering on the best means of obtaining a supply of gas within the ward\". Public concern that the City of Melbourne Gas and Coke Company monopoly resulted in excessive price for gas led to the establishment of several local gas companies, the first of which was the Collingwood, Fitzroy and District Gas and Coke Company formed in 1859. The company supplied gas to an area within a six mile radius of the works, which were erected on Reilly Street (now Alexander Parade), North Fitzroy in 1861.\n\nThe original part of the works was on the north side of Alexander Parade and West of Smith Street, was designed by engineer William Elsdon. The works was extended across Gore Street to the west as far as George Street. Opposite the works was the Gasometer Hotel.\n\nThe company amalgamated with the Melbourne and South Melbourne companies in 1878 to form the Metropolitan Gas Company, and the gasworks were known as the \"Fitzroy Station\". The Fitzroy works was not as profitable as the other Metropolitan company's plants as so gas production was gradually reduced and the site redeveloped to accommodate the company's construction workshops and gas storage. The only riveted gasholder erected by the Metropolitan Gas Company was built at Fitzroy in 1919. The Chief engineer Joseph Newell Reeson undertook a number of experiments with gas manufacture, including the application of electric arc welding to fabrication of large structures, and was responsible for the first arc welded large steel structure in the world, the No 3 gasholder, completed in 1923. This was dismantled in 1978, and the site covered over, but buried remains may still be present.\n\nWhile much of the site has been demolished and modern workshop and store buildings erected, there are a number of historic features. The former valve house is located on the corner of Alexander Parade and George Street, and while a Porter prefabricated iron building is located on the northern edge of the site. A further building remaining from the gasworks is a bichrome brick store which was identified in heritage studies but has not been included on heritage registers.\n"}
{"id": "17254227", "url": "https://en.wikipedia.org/wiki?curid=17254227", "title": "Fuel taxes in the United States", "text": "Fuel taxes in the United States\n\nThe United States federal excise tax on gasoline is 18.4 cents per gallon and 24.4 cents per gallon for diesel fuel. The federal tax was last raised in 1993 and is not indexed to inflation, which increased by a total of 64.6 percent from 1993 until 2015. On average, , state and local taxes and fees add 31.04 cents to gasoline and 31.01 cents to diesel, for a total US average fuel tax of 49.44 cents per gallon for gas and 55.41 cents per gallon for diesel.\n\nThe first US state tax on fuel was introduced in February 1919 in Oregon. It was a 1¢/gal tax. In the following decade, all of the US states (48 at the time), along with the District of Columbia, introduced a gasoline tax. By 1939, an average tax of 3.8¢/gal (1¢/L) of fuel was levied by the individual states.\n\nIn the years since being created, state fuel taxes have undergone many revisions. While most fuel taxes were initially levied as a fixed number of cents per gallon, , nineteen states and District of Columbia have fuel taxes with rates that vary alongside changes in the price of fuel, the inflation rate, vehicle fuel-economy, or other factors\n\nThe table below includes state and local taxes and fees. The American Petroleum Institute uses a weighted average of local taxes by population of each municipality to come up with an average tax for the entire state. Similarly, the national average is weighted by volume of fuel sold in each state. Because many of the states with the highest taxes also have higher populations, more states have below average taxes than above average taxes.\n\nMost states exempt gasoline from general sales taxes. However, several states do collect full or partial sales tax in addition to the excise tax. Sales tax is not reflected in the rates below.\n\nThe first federal gasoline tax in the United States was created on June 6, 1932, with the enactment of the Revenue Act of 1932 with a tax of 1¢/gal (0.3¢/L). Since 1993, the US federal gasoline tax has been 18.4¢/gal (4.86¢/L). Unlike most other goods in the US, the price displayed includes all taxes, as opposed to inclusion at the point of purchase.\n\nThen-Secretary of Transportation Mary Peters stated on August 15, 2007, that about 60% of federal gas taxes are used for highway and bridge construction. The remaining 40% goes to earmarked programs. However, revenues from other taxes are also used in federal transportation programs.\n\nFederal fuel taxes raised $35.2 billion in Fiscal Year 2014, with $25.0 billion raised from gasoline taxes and $10.2 billion raised from taxes on diesel and special motor fuels. The tax was last raised in 1993 and is not indexed to inflation. Total inflation from 1993 until 2015 was 64.6 percent.\n\nSome policy advisors believe that an increased tax is needed to fund and sustain the country's transportation infrastructure. As infrastructure construction costs have grown and vehicles have become more fuel efficient, the purchasing power of fixed-rate gas taxes has declined. To offset this loss of purchasing power, The National Surface Transportation Infrastructure Financing Commission issued a detailed report in February 2009 recommending a 10 cent increase in the gasoline tax, a 15 cent increase in the diesel tax, and a reform tying both tax rates to inflation.\n\nCritics of gas tax increases argue that much of the gas tax revenue is diverted to other government programs and debt servicing unrelated to transportation infrastructure. But other researchers have noted that these diversions can occur in both directions, and that gas taxes and \"user fees\" paid by drivers are not high enough to cover the full cost of road-related spending.\n\nSome believe that an increased cost of fuel would also encourage less consumption and reduce America's dependence on foreign oil. Americans sent nearly $430 billion to other countries in 2008 for the cost of imported oil. However, due to increased domestic output (fracking of shale and other energy resource discoveries) as well as rapidly increasing production efficiencies, since 2008 this has already significantly reduced and expected to continue to fall.\n\nAviation gasoline (Avgas): The tax on aviation gasoline is $0.194 per gallon. When used in a fractional ownership program aircraft, gasoline also is subject to a surtax of $0.141 per gallon.\n\nKerosene for use in aviation (Jet fuel): Generally, kerosene is taxed at $0.244 per gallon unless a reduced rate applies.\nFor kerosene removed directly from a terminal into the fuel tank of an aircraft for use in non-commercial aviation, the tax rate is $0.219. The rate of $0.219 also applies if kerosene is removed into any aircraft from a qualified refueler truck, tanker, or tank wagon that is loaded with the kerosene from a terminal that is located within an airport. The airport terminal doesn't need to be a secured airport terminal for this rate to apply. However, the refueler truck, tanker, or tank wagon must meet the requirements discussed under certain refueler trucks, tankers, and tank wagons, treated as terminals, later.\n\nThese taxes mainly fund airport and Air Traffic Control operations by the Federal Aviation Administration (FAA), of which commercial aviation is the biggest user.\n\n\nUS tax system:\n\n"}
{"id": "13976005", "url": "https://en.wikipedia.org/wiki?curid=13976005", "title": "Glitre Energi", "text": "Glitre Energi\n\nGlitre Energi (formerly Energiselskapet Buskerud, \"EB\") is a power company based in Drammen, Norway. The company operates electricity retailing, operates the power grid in Drammen, Nedre Eiker and Kongsberg and provides services within broadband. The company is owned by the City of Drammen (50%) and Buskerud County Municipality (50%). Power production has been spun off in the partially owned subsidiary EB Kraftproduksjon, where EB owns 70% and E-CO Energi owns 30%.\n\nThe first power plant operated by the company, Granfoss power plant, was established in 1903. In 1991 Drammen Energiverk is transformed to the limited company Drammen Kraft. In 1997 it established Drammen Kraft Produksjon in cooperation with Oslo Energi. In 1999 Drammen Kraft and the county owned Buskerud Energi were merged to create EB. In 2003 it bought ten power plants form Norske Skog.\n"}
{"id": "32816724", "url": "https://en.wikipedia.org/wiki?curid=32816724", "title": "Grant M. Wilson", "text": "Grant M. Wilson\n\nGrant McDonald Wilson (May 24, 1931 – September 10, 2012) was an American thermodynamicist. He founded the company Wilco (now Wiltec) in 1977.\n\nWilson was born in Colonia Pacheco, Chihuahua (one of the Mormon colonies in Mexico). He did his undergraduate work at BYU in 1953 and then attended and graduated from Massachusetts Institute of Technology (MIT) with a PhD in Physical Chemistry in 1957.\n\nWhile at MIT he began his career by developing one of the first computer-based Activity coefficient Equations. Known as the Wilson Equation, it is one of the most widely used equations in the field of industrial thermodynamics for the prediction of phase equilibria.\n\nWilson has been a research scientist measuring and reporting physical properties and phase equilibria data for most of his career. He joined Shell Research and Development in California upon graduating from MIT, he then joined Air Products and later moved to PVT Inc. of Houston, Texas. While there he performed a number of Research Projects for the Gas Processors Association. At this time he also did extensive work with cubic equations of state (EOS), he was one of the first to modify the alpha form of the Redlich-Kwong EOS to better represent pure component vapor pressures. He next taught at BYU in Provo, Utah, from 1970 to 1978 while there he was a part of the on-campus research group, the Thermochemical Institute founded by Profs. James J. Christensen and Reed M. Izatt. Wilson left BYU to create the company Wilco (now Wiltec) Research Company in 1977 which measured data for the Chemical Process Industries (CPI).\n\nThe Wilson equation was published by Grant M. Wilson as \"Vapor-Liquid Equilibrium. XI. A New Expression for the Excess Free Energy of Mixing\" in the Journal of the American Chemical Society 86:127-130, 1964.\n\nWilson and his wife, Reta Raphiel were married in the Logan Temple on September 18, 1950. Together in 2002, they served as missionaries for The Church of Jesus Christ of Latter-day Saints in Bangalore, India for two years.\n\nWilson died on September 10, 2012 in Orem, Utah.\n\nWilson's hobby was hiking in the mountains. His favorite climb was Mt. Timpanogos.\n"}
{"id": "683322", "url": "https://en.wikipedia.org/wiki?curid=683322", "title": "HEPA", "text": "HEPA\n\nHigh efficiency particulate air (HEPA), originally called high-efficiency particulate absorber but also sometimes called high-efficiency particulate arresting or high-efficiency particulate arrestance, is a type of air filter. Filters meeting the HEPA standard have many applications, including use in clean rooms for IC fabrication, medical facilities, automobiles, aircraft and homes. The filter must satisfy certain standards of efficiency such as those set by the United States Department of Energy (DOE).\n\nTo qualify as HEPA by industry standards, an air filter must remove (from the air that passes through) 99.97% of particles that have a size greater-than-or-equal-to 0.3 µm. Although the ASME industry standard is not published by any government, it is recognized as an authoritative standard by many governments.\n\nHEPA was commercialized in the 1950s, and the original term became a registered trademark and later a generic term for highly efficient filters.\n\nHEPA filters are composed of a mat of randomly arranged fibres. The fibers are typically composed of fiberglass and possess diameters between 0.5 and 2.0 micrometers. Key factors affecting its functions are fibre diameter, filter thickness, and face velocity. The air space between HEPA filter fibers is typically much greater than 0.3 μm. The common assumption that a HEPA filter acts like a sieve where particles smaller than the largest opening can pass through is incorrect and impractical. Unlike membrane filters at this pore size, where particles as wide as the largest opening or distance between fibers can not pass in between them at all, HEPA filters are designed to target much smaller pollutants and particles. These particles are trapped (they stick to a fiber) through a combination of the following three mechanisms:\n\nDiffusion predominates below the 0.1 μm diameter particle size. Impaction and interception predominate above 0.4 μm. In between, near the most penetrating particle size (MPPS) 0.21 μm, both diffusion and interception are comparatively inefficient. Because this is the weakest point in the filter's performance, the HEPA specifications use the retention of particles near this size (0.3 μm) to classify the filter. However it is possible for particles smaller than the MPPS to not have filtering efficiency greater than that of the MPPS. This is due to the fact that these particles can act as nucleation sites for mostly condensation and form particles near the MPPS.\n\nHEPA filters are designed to arrest very fine particles effectively, but they do not filter out gasses and odor molecules. Circumstances requiring filtration of volatile organic compounds, chemical vapors, cigarette, pet, and/or flatulence odors call for the use of an activated carbon (charcoal) or other type of filter instead of or in addition to a HEPA filter. Carbon cloth filters, claimed to be many times more efficient than the granular activated carbon form at adsorption of gaseous pollutants, are known as HEGA filters (\"High Efficiency Gas Adsorption\") and were originally developed by the British military as a defense against chemical warfare.\n\nThe first stage in the filtration process is made up of a pre-filter which removes most of the larger dust, hair, PM10 and pollen particles from the air. The second stage high-qualitylity HEPA filter, which filters out the finer dust particles which escape the pre-filter. Honeywell uses H11 class HEPA filters for maximum protection from PM2.5 particles, removing almost 99.97% of these pollutants.\n\nHEPA filters, as defined by the United States Department of Energy (DOE) standard adopted by most American industries, remove at least 99.97% of airborne particles 0.3 micrometers (µm) in diameter. The filter's minimal resistance to airflow, or pressure drop, is usually specified around at its nominal volumetric flow rate.\n\nThe specification usually used in the European Union is the European Norm EN 1822:2009. It defines several classes of HEPA filters by their retention at the given most penetrating particle size (MPPS):\n\nToday, a HEPA filter rating is applicable to any highly efficient air filter that can attain the same filter efficiency performance standards as a minimum and is equivalent to the more recent National Institute for Occupational Safety and Health N100 rating for respirator filters. The United States Department of Energy (DOE) has specific requirements for HEPA filters in DOE regulated applications. In addition, companies have begun using a marketing term known as \"True HEPA\" to give consumers assurance that their air filters are indeed certified to meet the HEPA standard.\n\nProducts that claim to be \"HEPA-type\", \"HEPA-like\", \"HEPA-style\" or \"99% HEPA\" do not satisfy these requirements and may not have been tested in independent laboratories. Some of these sub-par quality filters may come reasonably close to HEPA filtration, while others will fall significantly short, making them truly inferior.\n\nHEPA filtration works by mechanical means unlike the ionic and ozone filtration which use negative ions and ozone gas respectively. So, the chances of potential pulmonary side-effects like asthma and allergies is much lower with HEPA purifiers. To ensure that a HEPA filter is working efficiently, they should be checked and changed at least every six months in commercial settings. In residential settings, they can be changed every two to three years. Failing to change a HEPA filter in a timely fashion will result in it putting stress on the machine or system and not removing particles from the air properly.\n\nHEPA filters are critical in the prevention of the spread of airborne bacterial and viral organisms and, therefore, infection. Typically, medical-use HEPA filtration systems also incorporate high-energy ultra-violet light units to kill off the live bacteria and viruses trapped by the filter media. Some of the best-rated HEPA units have an efficiency rating of 99.995%, which assures a very high level of protection against airborne disease transmission.\n\nMany vacuum cleaners also use HEPA filters as part of their filtration systems. This is beneficial for asthma and allergy sufferers, because the HEPA filter traps the fine particles (such as pollen and dust mite feces) which trigger allergy and asthma symptoms. For a HEPA filter in a vacuum cleaner to be effective, the vacuum cleaner must be designed so that \"all\" the air drawn into the machine is expelled through the filter, with none of the air leaking past it. This is often referred to as \"Sealed HEPA\" or sometimes the more vague \"True HEPA\". Vacuum cleaners simply labeled \"HEPA\" may have a HEPA filter, but not all air necessarily passes through it. Finally, vacuum cleaner filters marketed as \"HEPA-like\" will typically use a filter of a \"similar construction\" to HEPA, but without the filtering efficiency. Because of the extra density of a true HEPA filter, HEPA vacuum cleaners require more powerful motors to provide adequate cleaning power.\n\nSome newer models claim to be better than the first models because of \"washable\" filters. Generally, washable true HEPA filters are expensive. Some manufacturers claim filter standards such as \"HEPA 4\", without explaining the meaning behind them. This refers to their Minimum Efficiency Reporting Value (MERV) rating. These ratings are used to rate the ability of an air cleaner filter to remove dust from the air as it passes through the filter. MERV is a standard used to measure the overall efficiency of a filter. The MERV scale ranges from 1 to 20, and measures a filter's ability to remove particles from 10 to 0.3 micrometre in size. Filters with higher ratings not only remove more particles from the air, they also remove smaller particles.\n\nModern airliners use HEPA filters to reduce the spread of airborne pathogens in recirculated air. Critics have expressed concern about the effectiveness and state of repair of air filtering systems, since they think that much of the air in an airplane cabin is recirculated. Almost all of the air in a pressurized aircraft is, in fact, brought in from the outside, circulated through the cabin and then exhausted through outflow valves in the rear of the aircraft.\n\nSome cars have cabin air filters that look like HEPA filters but which do not perform at that level. The confusion is perpetuated by guides for changing car filters which misidentify the filters as HEPA filters. The actual performance of these filters is obscured by manufacturers and difficult to evaluate, as they are not rated with the MERV system, though they typically yield MERV 8-equivalent performance.\n\nMore recently, the Tesla Model X has been attributed to have the world's first HEPA-grade filter. Following the release of the Model X, Tesla has updated the Model S to also have an optional HEPA air filter.\n\nThe original HEPA filter was designed in the 1940s and was used in the Manhattan Project to prevent the spread of airborne radioactive contaminants. It was commercialized in the 1950s, and the original term became a registered trademark and later a generic term for highly efficient filters.\n\nOver the decades filters have evolved to satisfy the higher and higher demands for air quality in various high technology industries, such as aerospace, pharmaceutical drug processing, hospitals, health care, nuclear fuels, nuclear power, and integrated circuit fabrication.\n\n\n\n"}
{"id": "35978769", "url": "https://en.wikipedia.org/wiki?curid=35978769", "title": "Human-powered land vehicle", "text": "Human-powered land vehicle\n\nHuman-powered land vehicles are land vehicles propelled over ground by human power. The main ways to support the weight of a human-powered land vehicle and its contents above the ground are rolling contact; sliding contact; intermittent contact; no contact at all as with anything carried; or some combination of the above. The main methods of using human power to propel a land vehicle are some kind of drivetrain; pushing laterally against the ground with a wheel, skate, or ski that simultaneously moves forward; by pushing against the ground directly with an appendage opposite to the direction of travel; or by propeller. Human-powered land vehicles can be propelled by persons riding in the vehicle or by persons walking or running and not supported by the vehicle.\n\nMany human-powered land vehicles can also be gravity-powered land vehicles, and vice versa, although some of the latter are quite awkward to use as the former. For example: street luges, gravity racers, and snow boards.\n\nThere are four main ways to support the weight of a human-powered land vehicle and its contents: rolling contact as with wheels; sliding contact as with skates, skis, or runners; intermittent contact as with stilts; and no contact at all as with anything carried. Additionally, these four methods may be combined as in wheelbarrows.\n\nThe most common wheeled human-powered land vehicle is the bicycle in all its forms. Other notable examples include:\n\n\n\n\n\nThere are three main methods of using human power to propel a land vehicle: some kind of drivetrain that turns one or more drive wheels; pushing laterally against the ground, to the side relative to the forward motion of the vehicle, with a wheel, skate, or ski that simultaneously moves forward; by pushing against the ground directly with an appendage, such as a hand or a foot, opposite to the direction of travel, or by pushing against the air with a propeller.\n\n\n\n"}
{"id": "46754176", "url": "https://en.wikipedia.org/wiki?curid=46754176", "title": "Jason Wynyard", "text": "Jason Wynyard\n\nJason Wynyard is a champion woodchopper from New Zealand who has won over a hundred world titles in the sport. He is the father of current Kentucky University men's basketball player Tai Wynyard.\n\nWynyard currently holds the world record time for the standing block chop. He recorded a time of 12.11 seconds in the 2003 Stihl Timbersports.\n\nAlong with this Jason Wynyard has earned a legendary status within the competitive woodchopping community by winning the Stihl Timbersports Series 14 times. The years in which he has won the title are: 1997, 1998, 1999, 2000, 2002, 2006, 2009, 2010, 2011, 2012, 2014, 2015, 2016 and 2017. \n"}
{"id": "25168088", "url": "https://en.wikipedia.org/wiki?curid=25168088", "title": "Johnson's figure of merit", "text": "Johnson's figure of merit\n\nJohnson's figure of merit is a measure of suitability of a semiconductor material for high frequency power transistor applications and requirements. More specifically, it is the product of the charge carrier saturation velocity in the material and the electric breakdown field under same conditions, first proposed by Edward O. Johnson of RCA in 1965.\n\nNote that this figure of merit (FoM) is applicable to both field-effect transistors (FETs), and with proper interpretation of the parameters, also to bipolar junction transistors (BJTs).\n\nJFM figures vary wildly between sources - see external links and talk page.\n\n Si GaAs GaN SiC diamond\n JFM 1 11 790 410 5800\n"}
{"id": "5679162", "url": "https://en.wikipedia.org/wiki?curid=5679162", "title": "Landis+Gyr", "text": "Landis+Gyr\n\nLandis+Gyr, is a publicly listed, multinational corporation with 45 subsidiary companies in over 30 countries and headquarters in Zug, Switzerland. Landis+Gyr makes meters and related software for electricity and gas utilities.\n\nLandis+Gyr was originally known as Electrotechnisches Institut Theiler and Company, established on September 11, 1905. It was renamed Landis & Gyr, after Heinrich Landis and Karl Heinrich Gyr, who followed founder Richard Theiler as managers. Landis+Gyr designs and manufactures a range of metering products, systems, and services for electricity, heat, and gas for energy utilities around the world. In 1924, Landis+Gyr opened its first overseas offices in New York and Australia. Besides metering products, with the founding of Cerberus Limited in 1940, Landis+Gyr expanded into fire safety products. Cerberus was later acquired by Elektrowatt Limited and is today a part of Siemens Building Technologies.\nThe company was also known for producing optical phone cards until 2006. Landys+Gyr phone cards were used in many countries such as Israel, Belgium, Switzerland and more.\n\n\nVarious utilities have worked with Landis+Gyr in meeting their consumers' demand for energy management tools by rolling out smart meters. Below are some of the utilities that have worked with Landis+Gyr in deploying smart metering technology to energy consumers.\n\n\nSmart metering is opposed by some, and as a developer of such meters, Landis+Gyr is met with criticism.\n"}
{"id": "17973", "url": "https://en.wikipedia.org/wiki?curid=17973", "title": "Liquid crystal", "text": "Liquid crystal\n\nLiquid crystals (LCs) are a state of matter which has properties between those of conventional liquids and those of solid crystals. For instance, a liquid crystal may flow like a liquid, but its molecules may be oriented in a crystal-like way. There are many different types of liquid-crystal phases, which can be distinguished by their different optical properties (such as textures). The contrasting areas in the textures correspond to domains where the liquid-crystal molecules are oriented in different directions. Within a domain, however, the molecules are well ordered. LC materials may not always be in a liquid-crystal phase (just as water may turn into ice or steam).\n\nLiquid crystals can be divided into thermotropic, lyotropic and metallotropic phases. Thermotropic and lyotropic liquid crystals consist mostly of organic molecules, although a few minerals are also known. Thermotropic LCs exhibit a phase transition into the liquid-crystal phase as temperature is changed. Lyotropic LCs exhibit phase transitions as a function of both temperature and concentration of the liquid-crystal molecules in a solvent (typically water). Metallotropic LCs are composed of both organic and inorganic molecules; their liquid-crystal transition depends not only on temperature and concentration, but also on the inorganic-organic composition ratio.\n\nExamples of liquid crystals can be found both in the natural world and in technological applications. Most contemporary electronic displays use liquid crystals. Lyotropic liquid-crystalline phases are abundant in living systems but can also be found in the mineral world. For example, many proteins and cell membranes are liquid crystals. Other well-known examples of liquid crystals are solutions of soap and various related detergents, as well as the tobacco mosaic virus, and some clays.\n\nIn 1888, Austrian botanical physiologist Friedrich Reinitzer, working at the Karl-Ferdinands-Universität, examined the physico-chemical properties of various derivatives of cholesterol which now belong to the class of materials known as cholesteric liquid crystals. Previously, other researchers had observed distinct color effects when cooling cholesterol derivatives just above the freezing point, but had not associated it with a new phenomenon. Reinitzer perceived that color changes in a derivative cholesteryl benzoate were not the most peculiar feature. He found that cholesteryl benzoate does not melt in the same manner as other compounds, but has two melting points. At it melts into a cloudy liquid, and at it melts again and the cloudy liquid becomes clear. The phenomenon is reversible. Seeking help from a physicist, on March 14, 1888, he wrote to Otto Lehmann, at that time a \"\" in Aachen. They exchanged letters and samples. Lehmann examined the intermediate cloudy fluid, and reported seeing crystallites. Reinitzer's Viennese colleague von Zepharovich also indicated that the intermediate \"fluid\" was crystalline. The exchange of letters with Lehmann ended on April 24, with many questions unanswered. Reinitzer presented his results, with credits to Lehmann and von Zepharovich, at a meeting of the Vienna Chemical Society on May 3, 1888.\n\nBy that time, Reinitzer had discovered and described three important features of cholesteric liquid crystals (the name coined by Otto Lehmann in 1904): the existence of two melting points, the reflection of circularly polarized light, and the ability to rotate the polarization direction of light.\n\nAfter his accidental discovery, Reinitzer did not pursue studying liquid crystals further. The research was continued by Lehmann, who realized that he had encountered a new phenomenon and was in a position to investigate it: In his postdoctoral years he had acquired expertise in crystallography and microscopy. Lehmann started a systematic study, first of cholesteryl benzoate, and then of related compounds which exhibited the double-melting phenomenon. He was able to make observations in polarized light, and his microscope was equipped with a hot stage (sample holder equipped with a heater) enabling high temperature observations. The intermediate cloudy phase clearly sustained flow, but other features, particularly the signature under a microscope, convinced Lehmann that he was dealing with a solid. By the end of August 1889 he had published his results in the Zeitschrift für Physikalische Chemie.\nLehmann's work was continued and significantly expanded by the German chemist Daniel Vorländer, who from the beginning of 20th century until his retirement in 1935, had synthesized most of the liquid crystals known. However, liquid crystals were not popular among scientists and the material remained a pure scientific curiosity for about 80 years.\n\nAfter World War II work on the synthesis of liquid crystals was restarted at university research laboratories in Europe. George William Gray, a prominent researcher of liquid crystals, began investigating these materials in England in the late 1940s. His group synthesized many new materials that exhibited the liquid crystalline state and developed a better understanding of how to design molecules that exhibit the state. His book \"Molecular Structure and the Properties of Liquid Crystals\" became a guidebook on the subject. One of the first U.S. chemists to study liquid crystals was Glenn H. Brown, starting in 1953 at the University of Cincinnati and later at Kent State University. In 1965, he organized the first international conference on liquid crystals, in Kent, Ohio, with about 100 of the world’s top liquid crystal scientists in attendance. This conference marked the beginning of a worldwide effort to perform research in this field, which soon led to the development of practical applications for these unique materials.\n\nLiquid crystal materials became a focus of research in the development of flat panel electronic displays beginning in 1962 at RCA Laboratories. When physical chemist Richard Williams applied an electric field to a thin layer of a nematic liquid crystal at 125 °C, he observed the formation of a regular pattern that he called domains (now known as Williams Domains). This led his colleague George H. Heilmeier to perform research on a liquid crystal-based flat panel display to replace the cathode ray vacuum tube used in televisions. But the para-Azoxyanisole that Williams and Heilmeier used exhibits the nematic liquid crystal state only above 116 °C, which made it impractical to use in a commercial display product. A material that could be operated at room temperature was clearly needed.\n\nIn 1966, Joel E. Goldmacher and Joseph A. Castellano, research chemists in Heilmeier group at RCA, discovered that mixtures made exclusively of nematic compounds that differed only in the number of carbon atoms in the terminal side chains could yield room-temperature nematic liquid crystals. A ternary mixture of Schiff base compounds resulted in a material that had a nematic range of 22–105 °C. Operation at room temperature enabled the first practical display device to be made. The team then proceeded to prepare numerous mixtures of nematic compounds many of which had much lower melting points. This technique of mixing nematic compounds to obtain wide operating temperature range eventually became the industry standard and is still used to tailor materials to meet specific applications.\nIn 1969, Hans Kelker succeeded in synthesizing a substance that had a nematic phase at room temperature, MBBA, which is one of the most popular subjects of liquid crystal research. The next step to commercialization of liquid-crystal displays was the synthesis of further chemically stable substances (cyanobiphenyls) with low melting temperatures by George Gray. That work with Ken Harrison and the UK MOD (RRE Malvern), in 1973, led to design of new materials resulting in rapid adoption of small area LCDs within electronic products.\n\nThese molecules are rod-shaped, some created in the lab and some appearing spontaneously in nature. Since then, two new types of LC molecules have been discovered, both man-made: disc-shaped (created by S. Chandrasekhar's group in India, 1977) and bowl-shaped (invented by Lui Lam in China, 1982, and synthesized in Europe three years later).\n\nIn 1991, when liquid crystal displays were already well established, Pierre-Gilles de Gennes working at the Université Paris-Sud received the Nobel Prize in physics \"for discovering that methods developed for studying order phenomena in simple systems can be generalized to more complex forms of matter, in particular to liquid crystals and polymers\".\n\nA large number of chemical compounds are known to exhibit one or several liquid crystalline phases. Despite significant differences in chemical composition, these molecules have some common features in chemical and physical properties. There are three types of thermotropic liquid crystals: discotics, bowlics and rod-shaped molecules. Discotics are flat disc-like molecules consisting of a core of adjacent aromatic rings; the core in a bowlic is not flat but like a rice bowl (a three-dimensional object). This allows for two dimensional columnar ordering, for both discotics and bowlics. Rod-shaped molecules have an elongated, anisotropic geometry which allows for preferential alignment along one spatial direction.\n\n• The molecular shape should be relatively thin, flat or bowl-like, especially within rigid molecular frameworks. \n• The molecular length should be at least 1.3 nm, consistent with the presence of long alkyl group on many room-temperature liquid crystals. \n• The structure should not be branched or angular, except for the bowlics. \n• A low melting point is preferable in order to avoid metastable, monotropic liquid crystalline phases. Low-temperature mesomorphic behavior in general is technologically more useful, and alkyl terminal groups promote this.\n\nAn extended, structurally rigid, highly anisotropic shape seems to be the main criterion for liquid crystalline behavior, and as a result many liquid crystalline materials are based on benzene rings.\n\nThe various liquid-crystal phases (called mesophases) can be characterized by the type of ordering. One can distinguish positional order (whether molecules are arranged in any sort of ordered lattice) and orientational order (whether molecules are mostly pointing in the same direction), and moreover order can be either short-range (only between molecules close to each other) or long-range (extending to larger, sometimes macroscopic, dimensions). Most thermotropic LCs will have an isotropic phase at high temperature. That is that heating will eventually drive them into a conventional liquid phase characterized by random and isotropic molecular ordering (little to no long-range order), and fluid-like flow behavior. Under other conditions (for instance, lower temperature), a LC might inhabit one or more phases with significant anisotropic orientational structure and short-range orientational order while still having an ability to flow.\n\nThe ordering of liquid crystalline phases is extensive on the molecular scale. This order extends up to the entire domain size, which may be on the order of micrometers, but usually does not extend to the macroscopic scale as often occurs in classical crystalline solids. However some techniques, such as the use of boundaries or an applied electric field, can be used to enforce a single ordered domain in a macroscopic liquid crystal sample. The ordering in a liquid crystal might extend along only one dimension, with the material being essentially disordered in the other two directions.\n\nThermotropic phases are those that occur in a certain temperature range. If the temperature rise is too high, thermal motion will destroy the delicate cooperative ordering of the LC phase, pushing the material into a conventional isotropic liquid phase. At too low temperature, most LC materials will form a conventional crystal. Many thermotropic LCs exhibit a variety of phases as temperature is changed. For instance, on heating a particular type of LC molecule (called mesogen) may exhibit various smectic phases followed by the nematic phase and finally the isotropic phase as temperature is increased. An example of a compound displaying thermotropic LC behavior is para-azoxyanisole.\n\nOne of the most common LC phases is the nematic. The word \"nematic\" comes from the Greek (\"\"), which means \"thread\". This term originates from the thread-like topological defects observed in nematics, which are formally called 'disclinations'. Nematics also exhibit so-called \"hedgehog\" topological defects. In a nematic phase, the \"calamitic\" or rod-shaped organic molecules have no positional order, but they self-align to have long-range directional order with their long axes roughly parallel. Thus, the molecules are free to flow and their center of mass positions are randomly distributed as in a liquid, but still maintain their long-range directional order. Most nematics are uniaxial: they have one axis that is longer and preferred, with the other two being equivalent (can be approximated as cylinders or rods). However, some liquid crystals are biaxial nematics, meaning that in addition to orienting their long axis, they also orient along a secondary axis. Nematics have fluidity similar to that of ordinary (isotropic) liquids but they can be easily aligned by an external magnetic or electric field. Aligned nematics have the optical properties of uniaxial crystals and this makes them extremely useful in liquid-crystal displays (LCD).\n\nScientists have discovered that electrons can unite to flow together in high magnetic fields, to create an \"electronic nematic\" form of matter.\n\nThe smectic phases, which are found at lower temperatures than the nematic, form well-defined layers that can slide over one another in a manner similar to that of soap. The word \"smectic\" originates from the Latin word \"smecticus\", meaning cleaning, or having soap-like properties.\nThe smectics are thus positionally ordered along one direction. In the Smectic A phase, the molecules are oriented along the layer normal, while in the Smectic C phase they are tilted away from it. These phases are liquid-like within the layers. There are many different smectic phases, all characterized by different types and degrees of positional and orientational order.\n\nThe chiral nematic phase exhibits chirality (handedness). This phase is often called the \"cholesteric\" phase because it was first observed for cholesterol derivatives. Only chiral molecules (i.e., those that have no internal planes of symmetry) can give rise to such a phase. This phase exhibits a twisting of the molecules perpendicular to the director, with the molecular axis parallel to the director. The finite twist angle between adjacent molecules is due to their asymmetric packing, which results in longer-range chiral order. In the smectic C* phase (an asterisk denotes a chiral phase), the molecules have positional ordering in a layered structure (as in the other smectic phases), with the molecules tilted by a finite angle with respect to the layer normal. The chirality induces a finite azimuthal twist from one layer to the next, producing a spiral twisting of the molecular axis along the layer normal.\nThe \"chiral pitch\", p, refers to the distance over which the LC molecules undergo a full 360° twist (but note that the structure of the chiral nematic phase repeats itself every half-pitch, since in this phase directors at 0° and ±180° are equivalent). The pitch, p, typically changes when the temperature is altered or when other molecules are added to the LC host (an achiral LC host material will form a chiral phase if doped with a chiral material), allowing the pitch of a given material to be tuned accordingly. In some liquid crystal systems, the pitch is of the same order as the wavelength of visible light. This causes these systems to exhibit unique optical properties, such as Bragg reflection and low-threshold laser emission, and these properties are exploited in a number of optical applications. For the case of Bragg reflection only the lowest-order reflection is allowed if the light is incident along the helical axis, whereas for oblique incidence higher-order reflections become permitted. Cholesteric liquid crystals also exhibit the unique property that they reflect circularly polarized light when it is incident along the helical axis and elliptically polarized if it comes in obliquely.\n\nBlue phases are liquid crystal phases that appear in the temperature range between a chiral nematic phase and an isotropic liquid phase. Blue phases have a regular three-dimensional cubic structure of defects with lattice periods of several hundred nanometers, and thus they exhibit selective Bragg reflections in the wavelength range of visible light corresponding to the cubic lattice. It was theoretically predicted in 1981 that these phases can possess icosahedral symmetry similar to quasicrystals.\n\nAlthough blue phases are of interest for fast light modulators or tunable photonic crystals, they exist in a very narrow temperature range, usually less than a few kelvin. Recently the stabilization of blue phases over a temperature range of more than 60 K including room temperature (260–326 K) has been demonstrated. Blue phases stabilized at room temperature allow electro-optical switching with response times of the order of 10 s.\n\nIn May 2008, the first Blue Phase Mode LCD panel had been developed.\n\nDisk-shaped LC molecules can orient themselves in a layer-like fashion known as the discotic nematic phase. If the disks pack into stacks, the phase is called a discotic columnar. The columns themselves may be organized into rectangular or hexagonal arrays. Chiral discotic phases, similar to the chiral nematic phase, are also known.\n\nBowl-shaped LC molecules, like in discotics, can form columnar phases. Other phases, such as nonpolar nematic, polar nematic, stringbean, donut and onion phases, have been predicted. Bowlic phases, except nonpolar nematic, are polar phases.\n\nA lyotropic liquid crystal consists of two or more components that exhibit liquid-crystalline properties in certain concentration ranges. In the lyotropic phases, solvent molecules fill the space around the compounds to provide fluidity to the system. In contrast to thermotropic liquid crystals, these lyotropics have another degree of freedom of concentration that enables them to induce a variety of different phases.\n\nA compound that has two immiscible hydrophilic and hydrophobic parts within the same molecule is called an amphiphilic molecule. Many amphiphilic molecules show lyotropic liquid-crystalline phase sequences depending on the volume balances between the hydrophilic part and hydrophobic part. These structures are formed through the micro-phase segregation of two incompatible components on a nanometer scale. Soap is an everyday example of a lyotropic liquid crystal.\n\nThe content of water or other solvent molecules changes the self-assembled structures. At very low amphiphile concentration, the molecules will be dispersed randomly without any ordering. At slightly higher (but still low) concentration, amphiphilic molecules will spontaneously assemble into micelles or vesicles. This is done so as to 'hide' the hydrophobic tail of the amphiphile inside the micelle core, exposing a hydrophilic (water-soluble) surface to aqueous solution. These spherical objects do not order themselves in solution, however. At higher concentration, the assemblies will become ordered. A typical phase is a hexagonal columnar phase, where the amphiphiles form long cylinders (again with a hydrophilic surface) that arrange themselves into a roughly hexagonal lattice. This is called the middle soap phase. At still higher concentration, a lamellar phase (neat soap phase) may form, wherein extended sheets of amphiphiles are separated by thin layers of water. For some systems, a cubic (also called viscous isotropic) phase may exist between the hexagonal and lamellar phases, wherein spheres are formed that create a dense cubic lattice. These spheres may also be connected to one another, forming a bicontinuous cubic phase.\n\nThe objects created by amphiphiles are usually spherical (as in the case of micelles), but may also be disc-like (bicelles), rod-like, or biaxial (all three micelle axes are distinct). These anisotropic self-assembled nano-structures can then order themselves in much the same way as thermotropic liquid crystals do, forming large-scale versions of all the thermotropic phases (such as a nematic phase of rod-shaped micelles).\n\nFor some systems, at high concentrations, inverse phases are observed. That is, one may generate an inverse hexagonal columnar phase (columns of water encapsulated by amphiphiles) or an inverse micellar phase (a bulk liquid crystal sample with spherical water cavities).\n\nA generic progression of phases, going from low to high amphiphile concentration, is:\n\nEven within the same phases, their self-assembled structures are tunable by the concentration: for example, in lamellar phases, the layer distances increase with the solvent volume. Since lyotropic liquid crystals rely on a subtle balance of intermolecular interactions, it is more difficult to analyze their structures and properties than those of thermotropic liquid crystals.\n\nSimilar phases and characteristics can be observed in immiscible diblock copolymers.\n\nLiquid crystal phases can also be based on low-melting \"inorganic\" phases like ZnCl that have a structure formed of linked tetrahedra and easily form glasses. The addition of long chain soap-like molecules leads to a series of new phases that show a variety of liquid crystalline behavior both as a function of the inorganic-organic composition ratio and of temperature. This class of materials has been named metallotropic.\n\nThermotropic mesophases are detected and characterized by two major methods, the original method was use of thermal optical microscopy, in which a small sample of the material was placed between two crossed polarizers; the sample was then heated and cooled. As the isotropic phase would not significantly affect the polarization of the light, it would appear very dark, whereas the crystal and liquid crystal phases will both polarize the light in a uniform way, leading to brightness and color gradients. This method allows for the characterization of the particular phase, as the different phases are defined by their particular order, which must be observed. The second method, differential scanning calorimetry (DSC), allows for more precise determination of phase transitions and transition enthalpies. In DSC, a small sample is heated in a way that generates a very precise change in temperature with respect to time. During phase transitions, the heat flow required to maintain this heating or cooling rate will change. These changes can be observed and attributed to various phase transitions, such as key liquid crystal transitions.\n\nLyotropic mesophases are analyzed in a similar fashion, though these experiments are somewhat more complex, as the concentration of mesogen is a key factor. These experiments are run at various concentrations of mesogen in order to analyze that impact.\n\nLyotropic liquid-crystalline phases are abundant in living systems, the study of which is referred to as lipid polymorphism. Accordingly, lyotropic liquid crystals attract particular attention in the field of biomimetic chemistry. In particular, biological membranes and cell membranes are a form of liquid crystal. Their constituent molecules (e.g. phospholipids) are perpendicular to the membrane surface, yet the membrane is flexible. These lipids vary in shape (see page on lipid polymorphism). The constituent molecules can inter-mingle easily, but tend not to leave the membrane due to the high energy requirement of this process. Lipid molecules can flip from one side of the membrane to the other, this process being catalyzed by flippases and floppases (depending on the direction of movement). These liquid crystal membrane phases can also host important proteins such as receptors freely \"floating\" inside, or partly outside, the membrane, e.g. CCT.\n\nMany other biological structures exhibit liquid-crystal behavior. For instance, the concentrated protein solution that is extruded by a spider to generate silk is, in fact, a liquid crystal phase. The precise ordering of molecules in silk is critical to its renowned strength. DNA and many polypeptides can also form LC phases and this too forms an important part of current academic research.\n\nExamples of liquid crystals can also be found in the mineral world, most of them being lyotropics. The first discovered was Vanadium(V) oxide, by Zocher in 1925. Since then, few others have been discovered and studied in detail. The existence of a true nematic phase in the case of the smectite clays family was raised by Langmuir in 1938, but remained open for a very long time and was only solved recently. With the rapid development of nanosciences, and the synthesis of many new anisotropic nanoparticles, the number of such mineral liquid crystals is quickly increasing, with, for example, carbon nanotubes and graphene.\nA lamellar phase was even discovered, HSbPO, which exhibits hyperswelling up to ~250 nm for the interlamellar distance.\n\nAnisotropy of liquid crystals is a property not observed in other fluids. This anisotropy makes flows of liquid crystals behave more differentially than those of ordinary fluids. For example, injection of a flux of a liquid crystal between two close parallel plates (viscous fingering) causes orientation of the molecules to couple with the flow, with the resulting emergence of dendritic patterns. This anisotropy is also manifested in the interfacial energy (surface tension) between different liquid crystal phases. This anisotropy determines the equilibrium shape at the coexistence temperature, and is so strong that usually facets appear. When temperature is changed one of the phases grows, forming different morphologies depending on the temperature change. Since growth is controlled by heat diffusion, anisotropy in thermal conductivity favors growth in specific directions, which has also an effect on the final shape.\n\nMicroscopic theoretical treatment of fluid phases can become quite complicated, owing to the high material density, meaning that strong interactions, hard-core repulsions, and many-body correlations cannot be ignored. In the case of liquid crystals, anisotropy in all of these interactions further complicates analysis. There are a number of fairly simple theories, however, that can at least predict the general behavior of the phase transitions in liquid crystal systems.\n\nAs we already saw above, the nematic liquid crystals are composed of rod-like molecules with the long axes of neighboring molecules aligned approximately to one another. To describe this anisotropic structure, a dimensionless unit vector n called the \"director\", is introduced to represent the direction of preferred orientation of molecules in the neighborhood of any point. Because there is no physical polarity along the director axis, n and -n are fully equivalent.\n\nThe description of liquid crystals involves an analysis of order. A second rank symmetric traceless tensor order parameter is used to describe the orientational order of a nematic liquid crystal, although a scalar order parameter is usually sufficient to describe uniaxial nematic liquid crystals. To make this quantitative, an orientational order parameter is usually defined based on the average of the second Legendre polynomial:\n\nwhere formula_2 is the angle between the liquid-crystal molecular axis and the \"local director\" (which is the 'preferred direction' in a volume element of a liquid crystal sample, also representing its \"local optical axis\"). The brackets denote both a temporal and spatial average. This definition is convenient, since for a completely random and isotropic sample, \"S\" = 0, whereas for a perfectly aligned sample S=1. For a typical liquid crystal sample, \"S\" is on the order of 0.3 to 0.8, and generally decreases as the temperature is raised. In particular, a sharp drop of the order parameter to 0 is observed when the system undergoes a phase transition from an LC phase into the isotropic phase. The order parameter can be measured experimentally in a number of ways; for instance, diamagnetism, birefringence, Raman scattering, NMR and EPR can be used to determine S.\n\nThe order of a liquid crystal could also be characterized by using other even Legendre polynomials (all the odd polynomials average to zero since the director can point in either of two antiparallel directions). These higher-order averages are more difficult to measure, but can yield additional information about molecular ordering.\n\nA positional order parameter is also used to describe the ordering of a liquid crystal. It is characterized by the variation of the density of the center of mass of the liquid crystal molecules along a given vector. In the case of positional variation along the \"z\"-axis the density formula_3 is often given by:\n\nThe complex positional order parameter is defined as formula_5 and formula_6 the average density. Typically only the first two terms are kept and higher order terms are ignored since most phases can be described adequately using sinusoidal functions. For a perfect nematic formula_7 and for a smectic phase formula_8 will take on complex values. The complex nature of this order parameter allows for many parallels between nematic to smectic phase transitions and conductor to superconductor transitions.\n\nA simple model which predicts lyotropic phase transitions is the hard-rod model proposed by Lars Onsager. This theory considers the volume excluded from the center-of-mass of one idealized cylinder as it approaches another. Specifically, if the cylinders are oriented parallel to one another, there is very little volume that is excluded from the center-of-mass of the approaching cylinder (it can come quite close to the other cylinder). If, however, the cylinders are at some angle to one another, then there is a large volume surrounding the cylinder which the approaching cylinder's center-of-mass cannot enter (due to the hard-rod repulsion between the two idealized objects). Thus, this angular arrangement sees a \"decrease\" in the net positional entropy of the approaching cylinder (there are fewer states available to it).\n\nThe fundamental insight here is that, whilst parallel arrangements of anisotropic objects lead to a decrease in orientational entropy, there is an increase in positional entropy. Thus in some case greater positional order will be entropically favorable. This theory thus predicts that a solution of rod-shaped objects will undergo a phase transition, at sufficient concentration, into a nematic phase. Although this model is conceptually helpful, its mathematical formulation makes several assumptions that limit its applicability to real systems.\n\nThis statistical theory, proposed by Alfred Saupe and Wilhelm Maier, includes contributions from an attractive intermolecular potential from an induced dipole moment between adjacent liquid crystal molecules. The anisotropic attraction stabilizes parallel alignment of neighboring molecules, and the theory then considers a mean-field average of the interaction. Solved self-consistently, this theory predicts thermotropic nematic-isotropic phase transitions, consistent with experiment.\n\nMcMillan's model, proposed by William McMillan, is an extension of the Maier–Saupe mean field theory used to describe the phase transition of a liquid crystal from a nematic to a smectic A phase. It predicts that the phase transition can be either continuous or discontinuous depending on the strength of the short-range interaction between the molecules. As a result, it allows for a triple critical point where the nematic, isotropic, and smectic A phase meet. Although it predicts the existence of a triple critical point, it does not successfully predict its value. The model utilizes two order parameters that describe the orientational and positional order of the liquid crystal. The first is simply the average of the second Legendre polynomial and the second order parameter is given by:\n\nThe values \"z, θ\", and \"d\" are the position of the molecule, the angle between the molecular axis and director, and the layer spacing. The postulated potential energy of a single molecule is given by:\n\nHere constant α quantifies the strength of the interaction between adjacent molecules. The potential is then used to derive the thermodynamic properties of the system assuming thermal equilibrium. It results in two self-consistency equations that must be solved numerically, the solutions of which are the three stable phases of the liquid crystal.\n\nIn this formalism, a liquid crystal material is treated as a continuum; molecular details are entirely ignored. Rather, this theory considers perturbations to a presumed oriented sample. The distortions of the liquid crystal are commonly described by the Frank free energy density. One can identify three types of distortions that could occur in an oriented sample: (1) twists of the material, where neighboring molecules are forced to be angled with respect to one another, rather than aligned; (2) splay of the material, where bending occurs perpendicular to the director; and (3) bend of the material, where the distortion is parallel to the director and molecular axis. All three of these types of distortions incur an energy penalty. They are distortions that are induced by the boundary conditions at domain walls or the enclosing container. The response of the material can then be decomposed into terms based on the elastic constants corresponding to the three types of distortions. Elastic continuum theory is a particularly powerful tool for modeling liquid crystal devices and lipid bilayers.\n\nScientists and engineers are able to use liquid crystals in a variety of applications because external perturbation can cause significant changes in the macroscopic properties of the liquid crystal system. Both electric and magnetic fields can be used to induce these changes. The magnitude of the fields, as well as the speed at which the molecules align are important characteristics industry deals with. Special surface treatments can be used in liquid crystal devices to force specific orientations of the director.\n\nThe ability of the director to align along an external field is caused by the electric nature of the molecules. Permanent electric dipoles result when one end of a molecule has a net positive charge while the other end has a net negative charge. When an external electric field is applied to the liquid crystal, the dipole molecules tend to orient themselves along the direction of the field.\n\nEven if a molecule does not form a permanent dipole, it can still be influenced by an electric field. In some cases, the field produces slight re-arrangement of electrons and protons in molecules such that an induced electric dipole results. While not as strong as permanent dipoles, orientation with the external field still occurs.\nThe effects of magnetic fields on liquid crystal molecules are analogous to electric fields. Because magnetic fields are generated by moving electric charges, permanent magnetic dipoles are produced by electrons moving about atoms. When a magnetic field is applied, the molecules will tend to align with or against the field. Electromagnetic radiation, e.g. UV-Visible light, can influence light-responsive liquid crystals which mainly carry at least a photo-switchable unit.\n\nIn the absence of an external field, the director of a liquid crystal is free to point in any direction. It is possible, however, to force the director to point in a specific direction by introducing an outside agent to the system. For example, when a thin polymer coating (usually a polyimide) is spread on a glass substrate and rubbed in a single direction with a cloth, it is observed that liquid crystal molecules in contact with that surface align with the rubbing direction. The currently accepted mechanism for this is believed to be an epitaxial growth of the liquid crystal layers on the partially aligned polymer chains in the near surface layers of the polyimide.\n\nSeveral liquid crystal chemicals also align to a 'command surface' which is in turn aligned by electric field of polarized light. This process is called photoalignment.\n\nThe competition between orientation produced by surface anchoring and by electric field effects is often exploited in liquid crystal devices. Consider the case in which liquid crystal molecules are aligned parallel to the surface and an electric field is applied perpendicular to the cell. At first, as the electric field increases in magnitude, no change in alignment occurs. However at a threshold magnitude of electric field, deformation occurs. Deformation occurs where the director changes its orientation from one molecule to the next. The occurrence of such a change from an aligned to a deformed state is called a Fredericks transition and can also be produced by the application of a magnetic field of sufficient strength.\n\nThe Fredericks transition is fundamental to the operation of many liquid crystal displays because the director orientation (and thus the properties) can be controlled easily by the application of a field.\n\nAs already described, chiral liquid-crystal molecules usually give rise to chiral mesophases. This means that the molecule must possess some form of asymmetry, usually a stereogenic center. An additional requirement is that the system not be racemic: a mixture of right- and left-handed molecules will cancel the chiral effect. Due to the cooperative nature of liquid crystal ordering, however, a small amount of chiral dopant in an otherwise achiral mesophase is often enough to select out one domain handedness, making the system overall chiral.\n\nChiral phases usually have a helical twisting of the molecules. If the pitch of this twist is on the order of the wavelength of visible light, then interesting optical interference effects can be observed. The chiral twisting that occurs in chiral LC phases also makes the system respond differently from right- and left-handed circularly polarized light. These materials can thus be used as polarization filters.\n\nIt is possible for chiral LC molecules to produce essentially achiral mesophases. For instance, in certain ranges of concentration and molecular weight, DNA will form an achiral line hexatic phase. An interesting recent observation is of the formation of chiral mesophases from achiral LC molecules. Specifically, bent-core molecules (sometimes called banana liquid crystals) have been shown to form liquid crystal phases that are chiral. In any particular sample, various domains will have opposite handedness, but within any given domain, strong chiral ordering will be present. The appearance mechanism of this macroscopic chirality is not yet entirely clear. It appears that the molecules stack in layers and orient themselves in a tilted fashion inside the layers. These liquid crystals phases may be ferroelectric or anti-ferroelectric, both of which are of interest for applications.\n\nChirality can also be incorporated into a phase by adding a chiral dopant, which may not form LCs itself. Twisted-nematic or super-twisted nematic mixtures often contain a small amount of such dopants.\n\nLiquid crystals find wide use in liquid crystal displays, which rely on the optical properties of certain liquid crystalline substances in the presence or absence of an electric field. In a typical device, a liquid crystal layer (typically 4 μm thick) sits between two polarizers that are crossed (oriented at 90° to one another). The liquid crystal alignment is chosen so that its relaxed phase is a twisted one (see Twisted nematic field effect). This twisted phase reorients light that has passed through the first polarizer, allowing its transmission through the second polarizer (and reflected back to the observer if a reflector is provided). The device thus appears transparent. When an electric field is applied to the LC layer, the long molecular axes tend to align parallel to the electric field thus gradually untwisting in the center of the liquid crystal layer. In this state, the LC molecules do not reorient light, so the light polarized at the first polarizer is absorbed at the second polarizer, and the device loses transparency with increasing voltage. In this way, the electric field can be used to make a pixel switch between transparent or opaque on command. Color LCD systems use the same technique, with color filters used to generate red, green, and blue pixels. Chiral smectic liquid crystals are used in ferroelectric LCDs which are fast-switching binary light modulators. Similar principles can be used to make other liquid crystal based optical devices.\n\nLiquid crystal tunable filters are used as electrooptical devices, e.g., in hyperspectral imaging.\n\nThermotropic chiral LCs whose pitch varies strongly with temperature can be used as crude liquid crystal thermometers, since the color of the material will change as the pitch is changed. Liquid crystal color transitions are used on many aquarium and pool thermometers as well as on thermometers for infants or baths. Other liquid crystal materials change color when stretched or stressed. Thus, liquid crystal sheets are often used in industry to look for hot spots, map heat flow, measure stress distribution patterns, and so on. Liquid crystal in fluid form is used to detect electrically generated hot spots for failure analysis in the semiconductor industry.\n\nLiquid crystal lenses converge or diverge the incident light by adjusting the refractive index of liquid crystal layer with applied voltage or temperature. Generally, the liquid crystal lenses generate a parabolic refractive index distribution by arranging molecular orientations. Therefore, a plane wave is reshaped into a parabolic wavefront by a liquid crystal lens. The focal length of liquid crystal lenses could be continuously tunable when the external electric field can be properly tuned. Liquid crystal lenses are a kind of adaptive optics. Imaging system can be benefited with focusing correction, image plane adjustment, or changing the range of depth-of-field or depth of focus. Liquid crystal lens is one of the candidates to develop vision correction device for myopia and presbyopia eyes (e.g., tunable eyeglass and smart contact lenses).\n\nLiquid crystal lasers use a liquid crystal in the lasing medium as a distributed feedback mechanism instead of external mirrors. Emission at a photonic bandgap created by the periodic dielectric structure of the liquid crystal gives a low-threshold high-output device with stable monochromatic emission.\n\nPolymer dispersed liquid crystal (PDLC) sheets and rolls are available as adhesive backed Smart film which can be applied to windows and electrically switched between transparent and opaque to provide privacy.\n\nMany common fluids, such as soapy water, are in fact liquid crystals. Soap forms a variety of LC phases depending on its concentration in water.\n\nBowlic columns could be used for fast switches.\n\n"}
{"id": "2091782", "url": "https://en.wikipedia.org/wiki?curid=2091782", "title": "Lot (unit)", "text": "Lot (unit)\n\nA lot is an old unit of weight used in many European countries since the Middle Ages until the beginning of the 20th century. Most often it was defined as either or of a pound (or more precisely of whatever mass value one local pound had at the time). Recorded values range from 10 to 50 grams.\n\nIn the Imperial and US costumary systems of measurement, a lot is of a pound, or an ounce, making it exactly 14.174 761 562 5 grams if derived from the international pound.\n\n\n"}
{"id": "56695515", "url": "https://en.wikipedia.org/wiki?curid=56695515", "title": "Majda's model", "text": "Majda's model\n\nMajda's model is a qualitative model (in mathematical physics) introduced by Andrew Majda in 1981 for the study of interactions in the combustion theory of shock waves and explosive chemical reactions.\n\nThe following definitions are with respect to a Cartesian coordinate system with 2 variables. For functions formula_1, formula_2 of one spatial variable formula_3 representing the Lagrangian specification of the fluid flow field and the time variable formula_4, functions formula_5, formula_6 of one variable formula_7, and positive constants formula_8, the Majda model is a pair of coupled partial differential equations:\n"}
{"id": "3652806", "url": "https://en.wikipedia.org/wiki?curid=3652806", "title": "Metal deactivator", "text": "Metal deactivator\n\nMetal deactivators, or metal deactivating agents (MDA) are fuel additives and oil additives used to stabilize fluids by deactivating (usually by sequestering) metal ions, mostly introduced by the action of naturally occurring acids in the fuel and acids generated in lubricants by oxidative processes with the metallic parts of the systems. Fuels desulfurized by copper sweetening also contain a significant trace amounts of copper.\n\nMetal deactivators inhibit the catalytic effects of such ions, especially copper, retarding the formation of gummy residues (e.g. gels containing copper mercaptide). Even concentrations of copper as low as 0.1 ppm can have detrimental effects.\n\nAn example of a metal deactivator used for gasoline and jet fuels is N,N-Disalicylidene-1,2-propanediamine. It is used in turbine and jet fuels, diesel, heating oil, and greases. It is approved for military and commercial aviation fuels. Benzotriazole and its various derivatives are also common in lubricant formulas. \n"}
{"id": "32341531", "url": "https://en.wikipedia.org/wiki?curid=32341531", "title": "Nahal Zin fuel leak", "text": "Nahal Zin fuel leak\n\nThe Nahal Zin fuel leak () was a severe ecological disaster caused in June 2011 when a backhoe loader struck and ruptured an underground fuel pipeline in southern Israel. 1.5 million liters of jet fuel leaked into the surrounding soil, resulting in localized soil contamination, damage to nearby flora, and wasted fuel. It is considered the worst ecological disaster ever to befall a nature reserve in the history of the State of Israel.\n\nNahal Zin is a 120-kilometer-long intermittent stream in Israel's Negev desert in the south of the country. Its source is at Mount Hemet (918 m) and it flows generally northeast, draining into the salt pan of Sodom at the southern tip of the Dead Sea. Vegetation in the vicinity of Nahal Zin is sparse, but the banks of the stream and occasional springs along the way abound in reeds, juncus, atriplex, and tamarix. Due to the lack of trees, local avifauna are typically ground-breeders. Geologists have determined that Nahal Zin used to flow northwest and drain into the Mediterranean Sea via the Besor Stream; however, as the Jordan Rift Valley formed, it altered course dramatically and channeled its way northeast to the Dead Sea instead.\n\nThe Eilat Ashkelon Pipeline Company operates several pipelines delivering oil and related products to and from the Port of Eilat.\nOn the morning of 29 June 2011, a backhoe loader was engaged in maintenance work at a point where Nahal Zin intersects one of the oil pipelines operated by the Eilat-Ashkelon Pipeline Company (EAPC). It struck and ruptured the pipeline, causing some 1.5 million liters of jet fuel to gush out for several hours and leak into the surrounding soil. With an affected radius of half a kilometer around the rupture point and a depth of several meters, fears were raised that damage to the surrounding environment would be extensive. Within two weeks of the disaster, the Israel Nature and Parks Authority reported that vegetation on the Nahal Zin nature reserve had dried up.\n\nIn an initial bid to contain the situation, dams were set up to impede the flow of fuel downstream and a ditch was dug in order to coax the fuel back to a single point for pumping. EAPC was instructed to pump dry the puddles of jet fuel that had formed on the ground and to transfer the affected soil to a treatment plant for decontamination. Experts consulted to evaluate the extent of the damage estimated that cleaning up the area of the spill would take weeks to months, with fears that if the situation was not resolved by wintertime, rainfall would exacerbate it. A hydrologist from the University of Haifa estimated that, should it emerge that the 25-meter-deep aquifer became polluted, it could take years for the area to be rehabilitated and cleansed of contaminants.\n\nIn August the Nature and Parks Authority reported that most of the cleanup operations had been completed and that work would soon begin to restore the scenery of the area. At the same time, senior contacts in the nature preservation community told \"Haaretz\" that the scale on which earth had been removed from the vicinity of the spill was excessive and had turned it into \"a mining and quarrying site\". An official Nature and Parks Authority report completed in late August indicated that the rehabilitation efforts had indeed taken an ecological toll on the site and seriously disrupted the routines of local wildlife.\n\nEAPC and representatives from Israel's Ministry of the Environment held a ceremony marking the completion of cleanup efforts on 27 September. EAPC stated that it successfully restored the Nahal Zin area to its earlier state prior to the leak.\n\nEAPC denied responsibility for the disaster, stating that the driver of the backhoe loader was there by subcontract and that an on-site park ranger had instructed him to transplant a tree. It attributed the accident to \"profound incompetence\" on the part of the park ranger and the driver.\n\nIsrael's Ministry of the Environment called the disaster the worst ever on an Israeli nature reserve. A spokeswoman for the ministry said that no automatic safety valves were ever installed as a precaution should a leak occur, and further charged EAPC with failing to ensure that it had vehicles capable of negotiating the terrain fast enough to deal with the leak before the fuel had a chance to be absorbed by the soil.\n\nThe chairman of the Knesset Environment and Health Committee urged the government to adopt his recommendation for a revised Petroleum Law and radically alter its approach to fossil fuel-related environmental protection.\n\nA spokeswoman for Greenpeace in Israel called the disaster \"a large, black flag\" for Israel and said more needed to be done to end Israel's reliance on oil and to promote renewable energy alternatives.\n\nIsrael's Green Movement co-chairman at Ben-Gurion University accused Israel's infrastructural institutions of being derelict both in protecting the nation's natural resources and in ensuring that adequate measures are in place for immediate cleanup efforts when they become necessary.\n\nOn 5 September 2011 an EAPC pipeline was again ruptured as a result of maintenance work. A backhoe loader struck the pipeline at a location half a kilometer south of where the first leak occurred in June. EAPC workers promptly arrived at the scene and began sealing the pipeline. By evening, efforts were being undertaken to pump the leaked fuel out of the riverbed. The Ministry of Environmental Protection estimated that 100 m³ of jet fuel had leaked from the pipe. Minister Gilad Erdan called EAPC's actions \"careless\" and said he intends to demand that the EAPC Law, currently granting immunity to the company in environmental matters, be revised in order that EAPC may be held accountable for its actions.\n\n\n"}
{"id": "1977274", "url": "https://en.wikipedia.org/wiki?curid=1977274", "title": "Olkiluoto Nuclear Power Plant", "text": "Olkiluoto Nuclear Power Plant\n\nThe Olkiluoto Nuclear Power Plant () is on Olkiluoto Island, which is on the shore of the Gulf of Bothnia in the municipality of Eurajoki in western Finland. It is one of Finland's two nuclear power plants, the other being the two-unit VVER Loviisa Nuclear Power Plant. The plant is owned and operated by Teollisuuden Voima (TVO), a subsidiary of Pohjolan Voima.\n\nThe Olkiluoto plant consists of two Boiling Water Reactors (BWRs) producing 880 MW and 890 MW of electricity. \nA third reactor, Unit 3, is expected to be online in January 2020.\n\nUnit 3 is an EPR reactor and is under construction since 2005. \nThe start of commercial operation was originally planned for May 2009 but the project is delayed and the latest estimate (as of November 2018) for start of regular production is January 2020. \nIn December 2012, the French multi-national building contractor, Areva, estimated that the full cost of building the reactor will be about €8.5 billion, or almost three times the delivery price of €3 billion.\n\nA decision-in-principle for a fourth reactor to be built at the site was granted by the Finnish parliament in July 2010, but in June 2015 TVO decided that it will not apply for a construction license for Olkiluoto 4.\n\nUnits 1 and 2 consists of two BWRs producing 880 MW and 890 MW of electricity respectively, with unit 1 also planned to be upgraded to 890 MW in spring 2018. The main contractor was ASEA-Atom, now a part of Westinghouse Electric Sweden AB. Turbine generators were supplied by Stal-Laval. The units' architecture was designed by ASEA-Atom. The reactor pressure vessels were constructed by Uddcomb Sweden AB, and reactor internal parts, mechanical components by Finnatom. The electrical equipment was supplied by Strömberg. Unit 1 was constructed by Atomirakennus and unit 2 by Jukola and Työyhtymä. Unit 1 achieved its initial criticality in July 1978 and it started commercial operations in October 1979. Unit 2 achieved its initial criticality in October 1979 and it started commercial operations in July 1982.\n\nMajor upgrades were carried out to the units in 2010 and 2011, including replacement of turbines and generators, isolation valves, electrical switchgear and seawater pumps. The upgrades increased the net electrical output by 20 MWto 880 MW each.\n\nIn 2017, unit 2 was upgraded and modernized, increasing the output further to 890 MW from the beginning of 2018. A similar upgrade to unit 1 is planned for spring 2018. The extended maintenance was also made to prepare for a license renewal application. The license extension was granted in September 2018 and allows the reactors to operate until 2038. \n\nIn February 2005, the Finnish cabinet gave its permission to TVO to construct a new nuclear reactor, making Finland the first Western European country in 15 years to order one. The construction of the unit began in 2005. The start of commercial operation was planned for 2010, but has been pushed back several times. , the estimate for start of regular production is January 2020.\n\nOlkiluoto 3 is the first EPR, which is a type of third generation PWR, to have gone into construction. It will have a nameplate capacity of 1600 MW. Japan Steel Works and Mitsubishi Heavy Industries manufactured the unit's 526-ton reactor pressure vessel.\n\nAt the start of construction, the main contractor was Areva NP (now Framatome, after the sell-off mentioned below), a joint venture of Areva and Siemens. However, in 2009, Siemens sold its one-third share of Areva NP to Areva, which is now the main contractor. Siemens remained on the project as the subcontractor with the main responsibility for constructing the turbine hall. Areva sold its majority stake in Framatome (previously Areva NP), its nuclear reactor and fuel business, to Électricité de France.\n\nAccording to TVO, the construction phase of the project would create a total of about 30,000 person-years of employment directly and indirectly; that the highest number of on-site employees has been almost 4,400; and that the operation phase would create 150 to 200 permanent jobs.\n\nThe first license application for the third unit was made in December 2000 and the date of the unit's entry into service was estimated to be 2010. However, since the start of construction, several delays to the schedule have been announced. In July 2012 TVO announced that the unit would not go into service before 2015, five years after the original estimate. In a statement, the operator said it was \"not pleased with the situation\" although solutions to various problems were being found and work was \"progressing\", and that it was waiting for a new launch date from Areva and Siemens. In February 2014, TVO said that it is could not give an estimate of the plant's startup date, because it was still waiting for the Areva-Siemens consortium to provide it with an updated overall schedule for the project.\" Later the same month it was reported that Areva was shutting down construction due to the dispute over compensations and unfinished automation planning. According to Kauppalehti, the estimated opening was delayed until 2018–2020.\n\nThe delays have been due to various problems with planning, supervision, and workmanship, and have been the subject of an inquiry by STUK, the Finnish nuclear safety regulator. The first problems that surfaced were irregularities in the foundation concrete, and caused a delay of months. Later, it was found that subcontractors had provided heavy forgings that were not up to project standards and which had to be re-cast. An apparent problem constructing the reactor's unique double-containment structure also caused delays, as the welders had not been given proper instructions.\n\nIn 2009, Petteri Tiippana, the director of STUK's nuclear power plant division, told the BBC that it was difficult to deliver nuclear power plant projects on schedule because builders were not used to working to the exacting standards required on nuclear construction sites, since so few new reactors had been built in recent years.\n\nAt the end of 2013, TVO said that the Areva-Siemens consortium plans to reduce workers and subcontractors on the construction site and says that it expects the contractor to provide details about the expected impact on the project's schedule.\n\nAfter the construction of the unit started in 2005, Areva began constructing EPRs in Flamanville, France, and in Taishan, China. However, as of July 2012, the construction of the EPR in France is four years behind schedule. Taishan 1 in China became the first EPR to start power generation on 29 June 2018.\n\nThe main contractor, Areva, is building the unit for a fixed price of €3 billion, so in principle, any construction costs above that price fall on Areva. In July 2012, those overruns were estimated at more than €2 billion, and in December 2012, Areva estimated that the full cost of building the reactor would be about €8.5 billion, well over the previous estimate of €6.4 billion. Because of the delays, TVO and Areva are both seeking compensation from each other through the International Court of Arbitration.\n\nIn October 2013, TVO's demand for compensation from Areva had risen to €1.8 billion, and Areva's from TVO to €2.6 billion. In December 2013, Areva increased its demand to €2.7 billion. In November 2016, the case was still ongoing. On March 10th 2018 French newspaper Le Monde announced that Areva and TVO have reached an agreement. A day later, TVO confirmed that Areva would pay it €450 million in compensation over the delays and lost income. The agreement would settle all legal actions between the two companies.\n\nIn 2009, professor Stephen Thomas wrote, \"Olkiluoto has become an example of all that can go wrong in economic terms with new reactors,\" and that Areva and the TVO \"are in bitter dispute over who will bear the cost overruns and there is a real risk now that the utility will default.\"\n\nThe project has also been criticized by STUK because \"instructions have not been observed in the welding of pipes and the supervision of welding.\" STUK has also noted that there have been delays in submitting proper paperwork.\n\nOlkiluoto 3 was supposed to be the first reactor of 3+ generation which would pave the way for a new wave of identical reactors across Europe, safe, affordable, and delivered on time. The delays and cost overruns have had knock-on effects in other countries.\n\nThe construction workforce includes about 3,800 employees from 500 companies. 80% of the workers are foreigners, mostly from eastern European countries. In 2012 it was reported that one Bulgarian contracting firm is owned by the mafia, and that Bulgarian workers have been required to pay weekly protection fees to the mafia, wages have been unpaid, employees have been told not to join a union and that employers also reneged on social security payments.\n\nOn 14 February 2008, TVO submitted an environmental impact assessment of unit four to the Ministry of Employment and Economy. On 21 April 2010, the government of Finland decided to grant a decision-in-principle to Teollisuuden Voima for the fourth reactor in Olkiluoto. The decision was approved by the parliament on 1 July 2010. If constructed, the fourth unit would be a PWR or a BWR with a power output of 1,000 to 1,800 MW.\n\nIn September 2014, with unit 3 still unfinished, the Finnish government rejected TVO's request for time extension of the unit 4 decision-in-principle. Economic Affairs Minister Jan Vapaavuori referred to the long delay of the 3rd reactor and to unsatisfactory assurances by TVO that the 4th unit would ever be built. Nevertheless PM Stubb stated that the rejection didn't spell the end for the OL4 project, and that TVO would have the opportunity to apply for a construction license before the decision-in-principle expires in June 2015.\n\nIn June 2015 TVO decided not to apply for a construction permit for the Olkiluoto 4 unit because of delays with the unit 3, however saying they are prepared to file for a new decision-in-principle later.\n\nThe \"Onkalo spent nuclear fuel repository\" is a deep geological repository for the final disposal of spent nuclear fuel, the first such repository in the world. It is currently under construction at the Olkiluoto plant by the company Posiva, owned by the nuclear power plant operators Fortum and TVO.\n\nThe waste heat, an output common to all thermal power plants, which heats the cooling water (at 13 °C) is utilized for small-scale agriculture before being pumped back to the sea. The power plant hosts the northernmost vineyard in the world, a 0.1 ha experimental plot that yields 850 kg Zilga grapes annually. Another use is a pond for growing crabs, whitefish and sturgeon for caviar.\n\nIn April 2014 a turbine steam condenser of unit 1 had a small seawater leak, at a rate of two litres per hour. According to the operator, the leak forced to limit the plant output down to 300 MW, but was not serious and was to be repaired in a day.\n\n"}
{"id": "31882928", "url": "https://en.wikipedia.org/wiki?curid=31882928", "title": "Particle-laden flows", "text": "Particle-laden flows\n\nParticle-laden flows refers to a class of two-phase fluid flow, in which one of the phases is continuously connected (referred to as the continuous or carrier phase) and the other phase is made up of small, immiscible, and typically dilute particles (referred to as the dispersed or particle phase). Fine aerosol particles in air is an example of a particle-laden flow; the aerosols are the dispersed phase, and the air is the carrier phase.\n\nThe modeling of two-phase flows has a tremendous variety of engineering and scientific applications: pollution dispersion in the atmosphere, fluidization in combustion processes, aerosol deposition in spray medication, along with many others.\n\nThe starting point for a mathematical description of almost any type of fluid flow is the classical set of Navier–Stokes equations. To describe particle-laden flows, we must modify these equations to account for the effect of the particles on the carrier, or vice versa, or both - a suitable choice of such added complications depend on a variety of the parameters, for instance, how dense the particles are, how concentrated they are, or whether or not they are chemically reactive. In most real world cases, the particles are very small and occur in low concentrations, hence the dynamics are governed primarily by the continuous phase. A possible way to represent the dynamics of the carrier phase is by the following modified Navier-Stokes momentum equation: \n\nwhere formula_2 is a momentum source or sink term, arising from the presence of the particle phase. The above equation is an Eulerian equation, that is, the dynamics are understood from the viewpoint of a fixed point in space. The dispersed phase is typically (though not always) treated in a Lagrangian framework, that is, the dynamics are understood from the viewpoint of fixed particles as they move through space. A usual choice of momentum equation for a particle is:\n\nwhere formula_4 represents the carrier phase velocity and formula_5 represents the particle velocity. formula_6 is the particle relaxation time, and represents a typical timescale of the particle's reaction to changes in the carrier phase velocity - loosely speaking, this can be thought of as the particle's inertia with respect to the fluid with contains it. The interpretation of the above equation is that particle motion is hindered by a drag force. In reality, there are a variety of other forces which act on the particle motion (such as gravity, Basset history and added mass) – as described through for instance the Basset–Boussinesq–Oseen equation. However, for many physical examples, in which the density of the particle far exceeds the density of the medium, the above equation is sufficient. A typical assumption is that the particles are spherical, in which case the drag is modeled using Stokes drag assumption: \n\nHere formula_8 is the particle diameter, formula_9, the particle density and formula_10, the dynamic viscosity of the carrier phase. More sophisticated models contain the correction factor:\n\nwhere formula_12 is the particle Reynolds number, defined as:\n\nIf the mass fraction of the dispersed phase is small, then \"one-way coupling\" between the phases is a reasonable assumption; that is, the dynamics of the particle phase are affected by the carrier phase, but the reverse is not the case. However if the mass fraction of the dispersed phase is large, the interaction of the dynamics between the two phases must be considered - this is \"two-way coupling\".\n\nA problem with the Lagrangian treatment of the dispersed phase is that once the number of particles becomes large, it may require a prohibitive amount of computational power to track a sufficiently large sample of particles required for statistical convergence. In addition, if the particles are sufficiently light, they behave essentially like a second fluid. In this case, an Eulerian treatment of the dispersed phase is sensible.\n\nLike all fluid dynamics-related disciplines, the modelling of particle-laden flows is an enormous challenge for researchers - this is because most flows of practical interest are turbulent.\n\nDirect numerical simulations (DNS) for single-phase flow, let alone two-phase flow, are computationally very expensive; the computing power required for models of practical engineering interest are far out of reach. Since one is often interested in modeling only large scale qualitative behavior of the flow, a possible approach is to decompose the flow velocity into mean and fluctuating components, by the Reynolds-averaged Navier-Stokes (RANS) approach. A compromise between DNS and RANS is large eddy simulation (LES), in which the small scales of fluid motion are modeled and the larger, resolved scales are simulated directly.\n\nExperimental observations, as well as DNS indicate that an important phenomenon to model is preferential concentration. Particles (particularly those with Stokes number close to 1) are known to accumulate in regions of high shear and low vorticity (such as turbulent boundary layers), and the mechanisms behind this phenomenon are not well understood. Moreover, particles are known to migrate down turbulence intensity gradients (this process is known as turbophoresis). These features are particularly difficult to capture using RANS or LES-based models since too much time-varying information is lost.\n\nDue to these difficulties, existing turbulence models tend to be \"ad hoc\", that is, the range of applicability of a given model is usually suited toward a highly specific set of parameters (such as geometry, dispersed phase mass loading and particle reaction time), and are also restricted to low Reynolds numbers (whereas the Reynolds number of flows of engineering interest tend to be very high).\n\n"}
{"id": "2119092", "url": "https://en.wikipedia.org/wiki?curid=2119092", "title": "Peter Bridgewater", "text": "Peter Bridgewater\n\nProfessor Peter Bridgewater (born 31 December 1945) is an Australian conservationist.\n\nBridgewater completed a Bachelor of Science in Botany at Durham University in 1967. He stayed on to complete a doctorate at the same institution.\n\nBridgewater was Chief Scientist of the UK Nature Conservancy Council 1989–1990 and Chief Executive of the Australian Nature Conservation Agency and Director of the National Parks and Wildlife Service (1990–1997). He was Secretary of UNESCO's Man and the Biosphere Programme and Director of its Division of Ecological Sciences 1999–2003. \n\nHe was Chair of the International Whaling Commission from 1995 to 1997. From 2011-2014 he served as a Visiting Professor at the United Nations University in Japan.\n\n"}
{"id": "8605679", "url": "https://en.wikipedia.org/wiki?curid=8605679", "title": "Pipe dope", "text": "Pipe dope\n\nPipe dope is any thread lubricant, thread sealing compound, and anaerobic chemical sealants that are used to make a pipe thread joint leak proof and pressure tight. Although common pipe threads are tapered and therefore will achieve an interference fit during proper assembly, machining and finishing variances usually result in a fit that does not result in 100 percent contact between the mating components. The application of pipe dope prior to assembly will fill the minute voids between the threads, thus making the joint pressure tight. Pipe dope also acts as a lubricant and helps prevent seizing of the mating parts, which can later cause difficulty during disassembly.\n\nA material safety data sheet reports that the \"Permatex\" 51D pipe joint compound contains kaolin, clay, vegetable oil, rosin, ethanol, etc . The ingredients are designed to: \n\nVarious types of pipe dope formulation exist, the appropriate type being determined by the application, e.g., pneumatic, hydraulic, caustic, etc., as well as the expected pressure. Improper selection of the type of pipe dope may result in leakage despite best assembly practices. \n\nPetroleum-based pipe dope is not intended for use on threaded PVC, CPVC or ABS pipe and fittings since it will deteriorate the plastic. Builders in the US are expected to use thread compounds that meet ASTM F2331 - Standard Test Method for Determining Chemical Compatibility of Thread Sealants with Thermoplastic Threaded Pipe and Fittings Materials or thread seal tape on PVC, CPVC and ABS threads.\n"}
{"id": "9529284", "url": "https://en.wikipedia.org/wiki?curid=9529284", "title": "Pokagon Interpretive Center", "text": "Pokagon Interpretive Center\n\nThe interpretive center located in Pokagon State Park, Angola, Indiana, contains animals and displays about Pokagon and its surrounding areas. It is staffed by full-time and part-time naturalists. The Interpretive Center is the start of some interpretive hikes and the adjacent auditorium is the site of some programs.\n\n"}
{"id": "30987765", "url": "https://en.wikipedia.org/wiki?curid=30987765", "title": "Principle of permanence", "text": "Principle of permanence\n\nIn mathematics, the principle of permanence is that a complex function (or functional equation) which is 0 on a set with a non-isolated point is 0 everywhere (or at least on the connected component of its domain which contains the point). There are various statements of the principle, depending on the type of function or equation considered.\n\nFor one variable, the principle of permanence states that if \"f\"(\"z\") is an analytic function defined on an open connected subset \"U\" of the complex numbers C, and there exists a convergent sequence {\"a\"} having a limit \"L\" which is in \"U\", such that \"f\"(\"a\") = 0 for all \"n\", then \"f\"(\"z\") is uniformly zero on \"U\".\n\nOne of the main uses of the principle of permanence is to show that a functional equation that holds for the real numbers also holds for the complex numbers.\n\nAs an example, the function e-ee=0 on the real numbers. By the principle of permanence for functions of two variables, this implies that e-ee=0 for all complex numbers, thus proving one of the laws of exponents for complex exponents.\n\n"}
{"id": "32783626", "url": "https://en.wikipedia.org/wiki?curid=32783626", "title": "S1B reactor", "text": "S1B reactor\n\nThe S1B reactor is a naval reactor used by the United States Navy to provide electricity generation and propulsion on \"Columbia\"-class submarines. The S1B designation stands for:\n\n\nThis pressurized water reactor style nuclear reactor, designed by Knolls Atomic Power Laboratory, is designed to have increased energy density. \n\nThis reactor is designed to operate for 40 years without refueling.\n"}
{"id": "17540734", "url": "https://en.wikipedia.org/wiki?curid=17540734", "title": "Sandwich panel", "text": "Sandwich panel\n\nA sandwich panel is any structure made of three layers: a low-density core, and a thin skin-layer bonded to each side. Sandwich panels are used in applications where a combination of high structural rigidity and low weight is required.\n\nSandwich panels are an example of a sandwich structured composite: the strength and lightness of this technology makes it popular and widespread. Its versatility means that the panels have many applications and come in many forms: the core and skin materials can vary widely and the core may be a honeycomb or a solid filling. Enclosed panels are termed \"cassettes\".\n\nOne obvious application is in aircraft, where mechanical performance and weight-saving are essential. Transportation and automotive applications also exist.\n\nIn building and construction, these prefabricated products designed for use as building envelopes. They appear in industrial and office buildings, in clean and cold rooms and also in private houses, whether renovation or new-build. They combine a high-quality product with high flexibility regarding design. They generally have a good energy-efficiency and sustainability.\n\nIn packaging, applications include fluted polypropylene boards and polypropylene honeycomb boards.\n\nStructural insulated panels, or structural insulating panels, (SIP), are panels used as a building material.\n\nAluminium composite panels (ACP), made of aluminium composite material (ACM), are flat panels consisting of two thin coil-coated aluminium sheets bonded to a non-aluminium core. ACPs are frequently used for external cladding or facades of buildings, insulation, and signage.\n\nACP is mainly used for external and internal architectural cladding or partitions, false ceilings, signage, machine coverings, container construction, etc. Applications of ACP are not limited to external building cladding, but can also be used in any form of cladding such as partitions, false ceilings, etc. ACP is also widely used within the signage industry as an alternative to heavier, more expensive substrates.\n\nACP has been used as a light-weight but very sturdy material in construction, particularly for transient structures like trade show booths and similar temporary elements. It has recently also been adopted as a backing material for mounting fine art photography, often with an acrylic finish using processes like Diasec or other face-mounting techniques. ACP material has been used in famous structures as Spaceship Earth, VanDusen Botanical Garden, the Leipzig branch of the German National Library.\n\nThese structures made optimal use of ACP through its cost, durability, and efficiency. Its flexibility, low weight, and easy forming and processing allow for innovative design with increased rigidity and durability.\nWhere the core material is flammable, the usage must be considered. The standard ACP core is polyethylene (PE) or polyurethane (PU). These materials do not have good fire-resistant (FR) properties unless specially treated and are therefore not generally suitable as a building material for dwellings; several jurisdictions have banned their use completely. Arconic, owner of the Reynobond brand, cautions the prospective buyer. Concerning the core, it says that distance of the panel from the ground is a determinant of \"which materials are safer to use\". In a brochure it has a graphic of a building in flames, with the caption \"[a]s soon as the building is higher than the firefighters’ ladders, it has to be conceived with an incombustible material\". It shows that the Reynobond polyethylene product is for up to circa 10 meters; the fire-retardant product (c. 70% mineral core) from there to up to c. 30 meters, the height of the ladder; and the European A2-rated product (c. 90% mineral core) for anything above that. In this brochure, \"Fire Safety in High-rise Buildings: Our Fire Solutions\", product specification is only given for the last two products.\n\nThe cladding materials, particularly the core, have been implicated as a possible contributing factor in the 2017 Grenfell Tower fire in London, as well as in high-rise building fires in Melbourne, Australia; France; the United Arab Emirates; South Korea; and the United States. Fire-rated cores, such as mineral wool (MW), are an alternative, but are usually more expensive and often not a legal requirement.\n\nThe aluminium sheets can be coated with polyvinylidene fluoride (PVDF), fluoropolymer resins (FEVE), or polyester paint. Aluminium can be painted in any kind of colour, and ACPs are produced in a wide range of metallic and non-metallic colours as well as patterns that imitate other materials, such as wood or marble. The core is commonly low-density polyethylene (PE), or a mix of low-density polyethylene and mineral material to exhibit fire retardant properties.\n\n3A Composites (formerly Alcan Composites & Alusuisse) invented aluminium composites in 1964 - as a joint invention with BASF- and commercial production of Alucobond commenced in 1969. The product was patented in 1971, a patent which expired in 1991. After the expiration of the patent several companies started commercial production such as Reynobond (1991), Alpolic (Mitsubishi Chemicals, 1995), etalbond (1995). Today, it's estimated that more than 200 companies across the world are producing ACP.\n\nSandwich panel construction techniques have experienced considerable development in the last 40 years. Previously, sandwich panels were considered products suitable only for functional constructions and industrial buildings. However, their good insulation characteristics, their versatility, quality and appealing visual appearance, have resulted in a growing and widespread use of the panels across a huge variety of buildings.\n\n\nThe qualities that have produced the rapid growth in the use of sandwich panels, particularly in construction, include:\n\n\n\n\n\n\n\n\n"}
{"id": "43953213", "url": "https://en.wikipedia.org/wiki?curid=43953213", "title": "Steady flight", "text": "Steady flight\n\nSteady flight, unaccelerated flight, or equilibrium flight is a special case in flight dynamics where the aircraft's linear and angular velocity are constant in a body-fixed reference frame. Basic aircraft maneuvers such as level flight, climbs and descents, and coordinated turns can be modeled as steady flight maneuvers. Typical aircraft flight consists of a series of steady flight maneuvers connected by brief, accelerated transitions. Because of this, primary applications of steady flight models include aircraft design, assessment of aircraft performance, flight planning, and using steady flight states as the equilibrium conditions around which flight dynamics equations are expanded.\n\nSteady flight analysis uses three different reference frames to express the forces and moments acting on the aircraft. They are defined as:\n\nThe Euler angles linking these reference frames are:\n\nThe forces acting on an aircraft in flight are the weight, aerodynamic force, and thrust. The weight is easiest to express in the Earth frame, where it has magnitude \"W\" and is in the +\"z\" direction, towards the center of the Earth. The weight is assumed to be constant over time and constant with altitude.\n\nExpressing the aerodynamic force in the wind frame, it has a drag component with magnitude \"D\" opposite the velocity vector in the −\"x\" direction, a side force component with magnitude \"C\" in the +\"y\" direction, and a lift component with magnitude \"L\" in the −\"z\" direction.\n\nIn general, the thrust can have components along each body frame axis. For fixed wing aircraft with engines or propellers fixed relative to the fuselage, thrust is usually closely aligned with the +\"x\" direction. Other types of aircraft, such as rockets and airplanes that use thrust vectoring, can have significant components of thrust along the other body frame axes. In this article, aircraft are assumed to have thrust with magnitude \"T\" and fixed direction +\"x\".\n\nSteady flight is defined as flight where the aircraft's linear and angular velocity vectors are constant in a body-fixed reference frame such as the body frame or wind frame. In the Earth frame, the velocity may not be constant since the airplane may be turning, in which case the airplane has a centripetal acceleration (\"V\"cos(\"γ\"))/\"R\" in the \"x\"-\"y\" plane, where \"V\" is the magnitude of the true airspeed and \"R\" is the turn radius.\n\nThis equilibrium can be expressed along a variety of axes in a variety of reference frames. The traditional \"steady flight equations\" derive from expressing this force balance along three axes: the \"x\"-axis, the radial direction of the aircraft's turn in the \"x\"-\"y\" plane, and the axis perpendicular to \"x\" in the \"x\"-\"z\" plane.\n\nformula_1\n\nformula_2\n\nformula_3\n\nwhere \"g\" is the standard acceleration due to gravity.\n\nThese equations can be simplified with several assumptions that are typical of simple, fixed-wing flight. First, assume that the sideslip \"β\" is zero, or coordinated flight. Second, assume the side force \"C\" is zero. Third, assume that the angle of attack \"α\" is small enough that cos(\"α\")≈1 and sin(\"α\")≈\"α\", which is typical since airplanes stall at high angles of attack. Similarly, assume that the flight-path angle \"γ\" is small enough that cos(\"γ\")≈1 and sin(\"γ\")≈\"γ\", or equivalently that climbs and descents are at small angles relative to horizontal. Finally, assume that thrust is much smaller than lift, \"T\"≪\"L\". Under these assumptions, the equations above simplify to\n\nformula_4\n\nformula_5\n\nformula_6\n\nThese equations show that the thrust must be sufficiently large to cancel drag and the longitudinal component of weight. They also show that the lift must be sufficiently large to support the aircraft weight and accelerate the aircraft through turns.\n\nDividing the second equation by the third equation and solving for \"R\" shows that the turn radius can be written in terms of the true airspeed and the bank angle,\n\nformula_7\n\nThe constant angular velocity in the body frame leads to a balance of moments, as well. Most notably, the pitching moment being zero puts a constraint on the longitudinal motion of the aircraft that can be used to determine the elevator control input.\n\nThe most general maneuver described by the steady flight equations above is a steady climbing or descending coordinated turn. The trajectory the aircraft flies during this maneuver is a helix with \"z\" as its axis and a circular projection on the \"x\"-\"y\" plane. Other steady flight maneuvers are special cases of this helical trajectory.\n\nThe definition of steady flight also allows for other maneuvers that are steady only instantaneously if the control inputs are held constant. These include the steady roll, where there is a constant and non-zero roll rate, and the steady pull up, where there is a constant but non-zero pitch rate.\n\n\n"}
{"id": "19626745", "url": "https://en.wikipedia.org/wiki?curid=19626745", "title": "T-head engine", "text": "T-head engine\n\nA T-head engine is an early type of internal combustion engine that became obsolete after World War I.\nIt is a sidevalve engine that is distinguished from the much more common L-head by its placement of the valves. The intake valves are on one side of the engine block and the exhaust valves on the other. Seen from the end of the crankshaft, in cutaway view, the cylinder and combustion chamber resembles a T - hence the name \"T-head\". An L-head has all valves at the same side. \n\nThis was an early form of crossflow cylinder head. The design was very complex for its day, requiring separate camshafts to operate the intake and exhaust valves. This made the engine much more expensive to produce than a comparable L-head (flathead) engine. Additionally, it was also quite heavy and inefficient for its displacement, producing less horsepower than a flathead or modern overhead valve engine of comparable displacement.\n\nThe reason for this design's popularity from the turn of the 20th century into the 1920s is due to the type of gasoline sold at the time, which ignited at a much lower temperature than modern motor fuels. If the gasoline vapor got too hot or was compressed too much it could explode before being lit by the spark plug, a condition known as engine knocking or detonation. Since detonation was-and still is-a primary cause of catastrophic engine failure, limiting both the temperature and compression of the extremely explosive gasoline vapor was critical for maximum engine reliability. \n\nThe T-Head addressed both of these issues by putting the valves in open alcoves on opposite sides of the cylinder head and having cool water from the radiator enter the engine directly over the intake valves as an extra measure of safety. The heat transfer from the exhaust ports was thus minimized, and the extra volume of the open alcoves that housed the valves also lowered the effective compression ratio of the engine. Detonation was thus eliminated, and the relative inefficiency of the engine design was offset by the dramatic reliability gains achieved and the generally lower speeds of travel and racing common to the era. This superior reliability made the design a favorite racing engine in the earliest days of auto racing, when engine reliability arguably mattered more than maximum performance. As other engine designs improved in reliability thanks to the intense engine development done in World War I, the T-head's inherent compromises in performance caused it to rapidly fall out of favor for racing applications.\n\nThe T-head was rendered obsolete in the passenger car market by a combination of engine advances made in World War I and the introduction of effective anti-knock compounds such as tetraethyl lead in the early 1920s. When combined, these advances made flathead engines just as reliable as T-heads while also being more powerful, lighter, more fuel efficient, and cheaper to manufacture. \nThe T-heads' incredible reliability did however keep them in service in heavy equipment, large trucks, and fire trucks until they were finally phased out of production in the 1950s.\n\nThis engine type was found on cars like Mercedes and Stutz and the last T-head engine in production for personal cars was manufactured by an American company, Locomobile. The Pierce-Arrow company introduced a production four-valve per cylinder T-head motor (Dual Valve Six) in 1918, one of the few, perhaps the only, multi-valve valve-in-block type engines produced. American LaFrance produced T-head engines for their fire engines until the 1950s, though they also built overhead-cam engines in the 1940s.\n"}
{"id": "2906037", "url": "https://en.wikipedia.org/wiki?curid=2906037", "title": "Tears of wine", "text": "Tears of wine\n\nThe phenomenon called tears of wine is manifested as a ring of clear liquid, near the top of a glass of wine, from which droplets continuously form and drop back into the wine. It is most readily observed in a wine which has a high alcohol content. It is also referred to as wine legs, \"fingers\", curtains, or church windows.\n\nThe effect is a consequence of the fact that alcohol has a lower surface tension than water. If alcohol is mixed with water inhomogeneously, a region with a lower concentration of alcohol will pull on the surrounding fluid more strongly than a region with a higher alcohol concentration. The result is that the liquid tends to flow away from regions with higher alcohol concentration. This can be easily and strikingly demonstrated by spreading a thin film of water on a smooth surface and then allowing a drop of alcohol to fall on the center of the film. The liquid will rush out of the region where the drop of alcohol fell.\n\nWine is mostly a mixture of alcohol and water, with dissolved sugars, acids, colourants and flavourants. Where the surface of the wine meets the side of the glass, capillary action makes the liquid climb the side of the glass. As it does so, both alcohol and water evaporate from the rising film, but the alcohol evaporates faster, due to its higher vapor pressure. The resulting decrease in the concentration of alcohol causes the surface tension of the liquid to increase, and this causes more liquid to be drawn up from the bulk of the wine, which has a lower surface tension because of its higher alcohol content. The wine moves up the side of the glass and forms droplets that fall back under their own weight.\n\nThe phenomenon was first correctly explained by physicist James Thomson, the elder brother of Lord Kelvin, in 1855. It is an instance of what is today called the Marangoni effect (or the Gibbs-Marangoni effect): the flow of liquid caused by surface tension gradients.\n\nThe effect can be used to move water droplets around in technical applications.\n\nIt is sometimes claimed incorrectly that wine with \"lots of legs\" is sweeter or of a better quality. In fact the intensity of this phenomenon depends only on alcohol content, and it can be eliminated completely by covering the wine glass (which stops the evaporation of the alcohol). British physicist C. V. Boys argues that the biblical injunction, \"Look not thou upon the wine when it is red, when it giveth his colour in the cup, when it moveth itself aright\", (Proverbs 23:31) refers to this effect. Since the \"tears of wine\" are most noticeable in wine which has a high alcohol content, the author may be suggesting this as a way to identify wines that should be avoided in the interest of sobriety.\n\nOther fluid phenomena that arise in alcohol-water mixtures are \"beading\" and \"viscimetry\". These are more pronounced in liquor than in wine, and both phenomena are more pronounced in stronger liquor.\n\nBeading refers to the formation of stable bubbles when liquor is shaken. This occurs only in liquor that contains more than 46% alcohol. It is an example of the Marangoni effect. Shaking a whisky bottle to form bubbles is referred to as “beating [beading] the whisky”.\n\nViscimetry is the formation of whorls when water is added to a high-alcohol mixture.\n\n\n"}
{"id": "9461091", "url": "https://en.wikipedia.org/wiki?curid=9461091", "title": "Tenax", "text": "Tenax\n\nTenax is the brandname of Toho Tenax owned by Teijin for a carbon fiber.\nTenax-7R is an adhesive particularly suited for welding plastics .\n\n\nTenax is also a brand name for Poly(2,6-diphenylphenylene oxide), a porous polymer.\n\n"}
{"id": "28248422", "url": "https://en.wikipedia.org/wiki?curid=28248422", "title": "Theft of electricity", "text": "Theft of electricity\n\nTheft of electricity is the criminal practice of stealing electrical power. It is a crime and is punishable by fines and/or incarceration. It belongs to the non-technical losses.\n\nAccording to the annual \"Emerging Markets Smart Grid: Outlook 2015\" study by the Northeast Group, LLC, the world loses US$89.3 billion annually to electricity theft. The highest losses were in India ($16.2 billion), followed by Brazil ($10.5 billion) and Russia ($5.1 billion). \n\nPresident of Northeast Group Ben Gardner stated : \"India loses more money to theft than any other country in the world. The state of Maharashtra—which includes Mumbai—alone loses $2.8 billion per year, more than all but eight countries in the world. Nationally, total transmission and distribution losses approach 23% and some states' losses exceed 50%.\"\n\nThere are various types of electrical power theft, including Tapping a line or bypassing the energy meter. According to a study , 80% of worldwide theft occurs in private dwellings and 20% on commercial and industrial premises. The various types of electrical power theft include:\n\nWhat's known as \"Cable Hooking\" is the most used method. 80% of global power theft is by direct tapping from the line. The consumer taps into a power line from a point ahead of the energy meter. This energy consumption is unmeasured and procured with or without switches. \n\nIn this method, the input terminal and output terminal of the energy meter is short-circuited, preventing the energy from registration in the energy meter.\n\nMeters are manipulated via a remote by installing a circuit inside the meter so that the meter can be slowed down at any time. This kind of modification can evade external inspection attempts because the meter is always correct unless the remote is turned on.\n\nThis type of tampering is done to electromechanical meters with a rotating element. Foreign material is placed inside the meter to obstruct the free movement of the disc. A slower rotating disk signals less energy consumption.\n\nThis type of tampering is done on electronic meter to make it either latent damage or permanent damage. Detection can be done correctly in high end meters only.\n\nA number of approaches to detect electricity theft have been proposed. The predominant direction in research and development is employing artificial intelligence, and in particular machine learning methods, to detect customers that steal electricity.\n\n\"Katiyabaaz\" (Powerless), a 2014 Indian documentary film, dealt with issue of power theft in the city of Kanpur, Uttar Pradesh.\n\nOn March 27, 1886 it was reported that electricity espionage was accomplished by unscrupulous persons tapping into Edison Electricity in New York. The Superintendent of the power station sent a power surge into the line to burn out or destroy foreign objects trespassing on the line. \n\n"}
{"id": "22424589", "url": "https://en.wikipedia.org/wiki?curid=22424589", "title": "Trimethylsulfonium", "text": "Trimethylsulfonium\n\nTrimethylsulfonium (systematically named trimethylsulfanium and trimethylsulfur(1+)) is an organic cation with the chemical formula (CH)S (also written as ). It is the simplest sulfonium cation.\n\nSeveral salts of trimethylsulfonium are known:\n\nSulfonium compounds can be synthesised by treating a suitable alkyl halide with a thioether. For example, the reaction of dimethyl sulfide with iodomethane yields trimethylsulfonium iodide:\n\nAn extra oxygen atom can bond to the sulfur atom to yield the trimethylsulfoxonium ion.\n\nGlyphosate herbicide is often supplied as a trimethylsulfonium salt.\nWhen mixed with aluminium bromide, or aluminium chloride or even hydrogen bromide, trimethylsulfonium bromide forms an ionic liquid, which melts at temperatures below standard conditions.\n\n"}
{"id": "44804207", "url": "https://en.wikipedia.org/wiki?curid=44804207", "title": "Tub (unit)", "text": "Tub (unit)\n\nTub was a unit of capacity or of weight used in Britain and elsewhere.\n\nBritish laws for the sale of goods defined a tub of butter as a receptacle of a size which could contain 84 pounds of butter.\n\n1 tub of butter or cheese = 84 pounds\n\n1 tub = 1.5 Firkin (1 Firkin = 56 lbs)\n\n1 tub = \n\nThe \"Oxford English Dictionary\" has quotations illustrating other values of a \"tub\" as a unit:\n\nIn Newfoundland, Canada, a tub of coal was defined as 100 pounds, while a tub of herrings was 16 Imperial gallons and a tub of salt was 18 Imperial gallons.\n"}
{"id": "7393549", "url": "https://en.wikipedia.org/wiki?curid=7393549", "title": "Wonthaggi Wind Farm", "text": "Wonthaggi Wind Farm\n\nWonthaggi wind farm is a wind power station at Wonthaggi in Gippsland, Victoria, Australia. It has six wind turbines, with a total generating capacity of 12 MW of electricity.\n\nEach of the turbines is a German made REpower MM82 turbines, each sitting on an Australian made 66m tall tower. The farm is operated by Origin Energy Australia, and became operational in December 2005.\n\nIn March 2012 one of the rotor blades snapped, Senvion (formerly REpower), the manufacturers of the turbine, replaced the blades and blamed the issue on a manufacturing fault.\n\n\n"}
