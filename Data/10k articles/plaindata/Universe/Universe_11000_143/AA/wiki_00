{"id": "36353093", "url": "https://en.wikipedia.org/wiki?curid=36353093", "title": "1946 KLM Douglas DC-3 Amsterdam accident", "text": "1946 KLM Douglas DC-3 Amsterdam accident\n\nThe 1946 KLM Douglas DC-3 Amsterdam accident was the crash of a KLM Royal Dutch Airlines flight from London to Amsterdam on 14 November 1946. The accident occurred as the Douglas DC-3 was attempting to land at Amsterdam's airport in poor weather. All 26 passengers and crew on board were killed in the accident.\n\nThe DC-3 (actually an ex-military C-47 Skytrain converted for civil use) was on a scheduled flight from London, England to Amsterdam in the Netherlands. The crew was cleared to land the aircraft at Schiphol Airport in poor weather. The first attempt to land failed and the crew had to perform a go-around. The second approach to land also failed. On the third approach to land the aircraft made a sudden turn to the left, apparently trying to line up with the runway. During this turn the Douglas DC-3 struck the ground and crashed. The aircraft caught fire on impact, killing all 21 passengers and five crew on board.\n\nAt the time it happened, the 1946 KLM Amsterdam accident was the worst aviation accident in the history of the Netherlands. Eight days earlier another KLM DC-3 operating on the same route in the opposite direction, crashed on approach to London's Croydon Airport in poor weather. There were no fatalities in the London crash but the aircraft was written off.\n\n"}
{"id": "15296960", "url": "https://en.wikipedia.org/wiki?curid=15296960", "title": "Bent molecular geometry", "text": "Bent molecular geometry\n\nIn chemistry, the term \"bent\" can be applied to certain molecules to describe their molecular geometry. Certain atoms, such as oxygen, will almost always set their two (or more) covalent bonds in non-collinear directions due to their electron configuration. Water (HO) is an example of a bent molecule, as well as its analogues. The bond angle between the two hydrogen atoms is approximately 104.45°. Nonlinear geometry is commonly observed for other triatomic molecules and ions containing only main group elements, prominent examples being nitrogen dioxide (NO), sulfur dichloride (SCl), and methylene (CH).\n\nThis geometry is almost always consistent with VSEPR theory, which usually explains non-collinearity of atoms with a presence of lone pairs. There are several variants of bending, where the most common is AXE where two covalent bonds and two lone pairs of the central atom (A) form a complete 8-electron shell. They have central angles from 104° to 109.5°, where the latter is consistent with a simplistic theory which predicts the tetrahedral symmetry of four sp hybridised orbitals. The most common actual angles are 105°, 107°, and 109°: they vary because of the different properties of the peripheral atoms (X).\n\nOther cases also experience orbital hybridisation, but in different degrees. AXE molecules, such as SnCl, have only one lone pair and the central angle about 120° (the centre and two vertices of an equilateral triangle). They have three sp orbitals. There exist also sd-hybridised AX compounds of transition metals without lone pairs: they have the central angle about 90° and are also classified as bent.\n\n\n"}
{"id": "6168750", "url": "https://en.wikipedia.org/wiki?curid=6168750", "title": "Bimorph", "text": "Bimorph\n\nA bimorph is a cantilever used for actuation or sensing which consists of two active layers. It can also have a passive layer between the two active layers. In contrast, a piezoelectric unimorph has only one active (i.e. piezoelectric) layer and one passive (i.e. non-piezoelectric) layer.\n\nThe term bimorph is most commonly used with piezoelectric bimorphs. In actuator applications, one active layer contracts and the other expands if voltage is applied, thus the bimorph bends. In sensing applications, bending the bimorph produces voltage which can for example be used to measure displacement or acceleration. This mode can also be used for energy harvesting.\nA bimetal could be regarded as a thermally activated bimorph. The first theory about the bending of thermally activated bimorphs was given by Stoney. Newer developments also enabled electrostatically activated bimorphs for the use in microelectromechanical systems.\n\n"}
{"id": "46821627", "url": "https://en.wikipedia.org/wiki?curid=46821627", "title": "Biogen UK", "text": "Biogen UK\n\nBiogen UK Ltd is a leading UK designer, builder, owner and operator of anaerobic digestion plants based in Bedfordshire. It is responsible for the construction of 22 plants to date and currently operates eight food waste anaerobic digestion plants in England, Scotland and Wales.\n\nBiogen’s anaerobic digestion business was established in 2005 with investment from Bedfordia Group. The company’s Twinwoods anaerobic digestion (AD) plant, at Milton Ernest in Bedfordshire was completed in 2006. to recycle food waste along with animal slurry from Bedfordia Farms. In 2008 Biogen acquired technology and engineering company Greenfinch Ltd. Greenfinch was set up in 1993, initially constructing AD plants for the waste water industry and for farmers as a form of managing their livestock waste. In 2012 Biogen secured £24m of investment from construction, property and services company Kier Group, making them joint venture partners alongside Bedfordia Group. Since 2006 Biogen’s main focus has been on providing a sustainable food waste treatment service for local authorities, retailers, pubs, restaurants, hotels, offices and food manufacturers. The company employs 100 people across the UK and has its head office in Milton Ernest, Bedfordshire.\nIn April 2017 the company was bought for an undisclosed sum by Ancala Bioenergy Ltd, an infrastructure investment vehicle managed by Ancala Partners LLP, to provide an established platform for Ancala to expand into the waste-to-energy sector.\n\nBiogen’s AD plants recycle 250,000 tonnes of food waste each year and can generate 11MW of green electricity for the national grid. The anaerobic digestion process also produces is a liquid rich in nitrogen, potash, phosphate and other trace elements that can be stored on site until spreading time and returned to the land as a nutrient rich biofertiliser to grow more crops, completing a closed loop system.\n\nBiogen’s plants are located in Bedfordshire, Hertfordshire, Northamptonshire, Warwickshire, Denbighshire, Caernarfon Rhondda Cynon Taf and Midlothian \n"}
{"id": "205136", "url": "https://en.wikipedia.org/wiki?curid=205136", "title": "Blarney Stone", "text": "Blarney Stone\n\nThe Blarney Stone () is a block of Carboniferous limestone built into the battlements of Blarney Castle, Blarney, about from Cork, Ireland. According to legend, kissing the stone endows the kisser with \"the gift of the gab\" (great eloquence or skill at flattery). The stone was set into a tower of the castle in 1446. The castle is a popular tourist site in Ireland, attracting visitors from all over the world to kiss the stone and tour the castle and its gardens.\n\nThe word \"blarney\" has come to mean \"clever, flattering, or coaxing talk\". Irish politician John O'Connor Power defined it this way: \"Blarney is something more than mere flattery. It is flattery sweetened by humour and flavoured by wit. Those who mix with Irish folk have many examples of it in their everyday experience.\" Letitia Elizabeth Landon described its contemporary meaning in an article entitled 'Blarney Castle' in 1832.\n\nA number of stories attempt to explain the origin of the stone and surrounding legend. An early story involves the goddess Clíodhna. Cormac Laidir McCarthy, the builder of Blarney Castle, being involved in a lawsuit in the 15th century, appealed to Clíodhna for her assistance. She told McCarthy to kiss the first stone he found in the morning on his way to court, and he did so, with the result that he pleaded his case with great eloquence and won. Thus the Blarney Stone is said to impart \"the ability to deceive without offending\". MacCarthy then incorporated it into the parapet of the castle.\n\nThe proprietors of Blarney Castle list several other explanations of the origins of the stone on their website. Many of these suppose that the stone had previously been in Ireland, was taken to Scotland and then returned to Ireland in 1314. The stories listed include one suggesting that the stone was presented to Cormac McCarthy by Robert the Bruce in 1314 in recognition of his support in the Battle of Bannockburn. This legend holds that this was a piece of the Stone of Scone and was installed at McCarthy's castle of Blarney. Although colourful, this folk legend does not account for the fact that it supposes that the stone was removed from Scotland 18 years before Bannockburn, and modern analysis suggests that the stone is not related to the Stone of Scone.\n\nThe ritual of kissing the Blarney Stone, according to the castle's proprietors, has been performed by \"millions of people\", including \"world statesmen, literary giants [and] legends of the silver screen\". The kiss, however, is not casually achieved. To touch the stone with one's lips, the participant must ascend to the castle's peak, then lean over backwards on the parapet's edge. This is traditionally achieved with the help of an assistant. Although the parapet is now fitted with wrought-iron guide rails and protective crossbars, the ritual can still trigger attacks of acrophobia, an extreme or irrational fear of heights.\nBefore the safeguards were installed, the kiss was performed with real risk to life and limb, as participants were grasped by the ankles and dangled bodily from the height. In the Sherlock Holmes radio dramatisation \"The Adventure of the Blarney Stone\" (first broadcast on 18 March 1946), a man attempting to kiss the Blarney Stone falls to his death. Holmes' investigation reveals this as a murder as the man's boots having been surreptitiously greased before the attempt.\n\nWilliam Henry Hurlbert wrote in 1888 that the legend of the stone seemed to be less than a hundred years old at that time, suggesting the tradition began late in the 18th century. The legend of the Blarney Stone was described in \"A classical dictionary of the vulgar tongue\" by Francis Grose, printed 1785.\n\nIt is claimed that the synonymy of \"blarney\" with \"empty flattery\" or \"beguiling talk\" derives from one of two sources. One story involves the goddess Clíodhna and Cormac Laidir MacCarthy (see \"Origins\" above). Another legend suggests that Queen Elizabeth I requested Cormac Teige McCarthy, the Lord of Blarney, be deprived of his traditional land rights. Cormac travelled to see the queen, but was certain he would not persuade her to change her mind as he wasn't an effective speaker. He met an old woman on the way who told him that anyone who kissed a particular stone in Blarney Castle would be given the gift of eloquent speech. Cormac went on to persuade the queen that he should not be deprived of his land.\n\nEchoing the supposed power of the stone, an Irish bard of the early 19th century, Francis Sylvester Mahony, added a number of (humorous) lines to Richard Alfred Millikin's \"The Groves of Blarney\" (right).\n\nAccording to tradition at Texas Tech University, a stone fragment on display since 1939 outside the old Electrical Engineering Building is a missing piece of the Blarney Stone. How this was determined is unknown.\n\n\n"}
{"id": "10731223", "url": "https://en.wikipedia.org/wiki?curid=10731223", "title": "Bulgargaz", "text": "Bulgargaz\n\nBulgargaz is the largest Bulgarian natural gas distribution company. It is a subsidiary of Bulgarian Energy Holding EAD, a holding company established on 18 September 2008. As of November 2009 the company is to be listed on the Bulgarian Stock Exchange - Sofia.\n\n"}
{"id": "3680325", "url": "https://en.wikipedia.org/wiki?curid=3680325", "title": "Conexpo-Con/Agg", "text": "Conexpo-Con/Agg\n\nCONEXPO-CON/AGG is a trade show for the construction industry that takes place every three years. CONEXPO-CON/AGG is a result of the merger of CONEXPO and CON/AGG in 1996. It is held at the Las Vegas Convention Center with the next show held March 10-14, 2020. The International Exposition for Power Transmission (IFPE) is held in conjunction with Conexpo-Con/Agg.\n\nIn 1909, the first CONEXPO was held in Columbus, Ohio, and the first CON/AGG was held in Detroit Michigan in 1928. In 1994 the two show merged, creating CONEXPO-CON/AGG. The first joint show was held in 1996 in Las Vegas, Nevada. CONEXPO-CON/AGG has gone on to become the western hemisphere's largest show for the construction and construction materials industries. \n\nThe world’s first 3D-printed excavator and the new Tech Experience debuted at the 2017.show.\n\n2017 (March 7- 11, 2017) Show Statistics:\n\nThe main topics of the show are:\n\nCONEXPO-CON/AGG is sponsored by:\n\n"}
{"id": "10454415", "url": "https://en.wikipedia.org/wiki?curid=10454415", "title": "DEPA", "text": "DEPA\n\nPublic Gas Corporation of Greece A.E. (DEPA; ) commonly known for its Greek abbreviation DEPA () is the natural gas supply company of Greece. The registered office of the company is based in Irakleio, a suburb of Athens metropolitan area. It operates within the jurisdiction of the Ministry of Development. In 2005, in order to liberalise the natural gas market, DESFA was created as a fully owned subsidiary to transport natural gas within Greece. Since then, DEPA sells gas to large consumers and to the gas supply companies. Natural gas is imported by pipelines from Bulgaria and Turkey and by liquefied natural gas at the Revithoussa LNG Terminal.\n\nIn May 2018 it was announced to sell DEPA's subsidiaries in Thessaly and Thessaloniki to Italian company Eni. It was reported that DEPA would only kept the gas operations in Athens area.\n\n"}
{"id": "13733769", "url": "https://en.wikipedia.org/wiki?curid=13733769", "title": "Darcy friction factor formulae", "text": "Darcy friction factor formulae\n\nIn fluid dynamics, the Darcy friction factor formulae are equations that allow the calculation of the Darcy friction factor, a dimensionless quantity used in the Darcy–Weisbach equation, for the description of friction losses in pipe flow as well as open-channel flow.\n\nThe Darcy friction factor is also known as the \"Darcy–Weisbach friction factor\", \"resistance coefficient\" or simply \"friction factor\"; by definition it is four times larger than the Fanning friction factor.\n\nIn this article, the following conventions and definitions are to be understood:\n\nWhich friction factor formula may be applicable depends upon the type of flow that exists:\n\nTransition (neither fully laminar nor fully turbulent) flow occurs in the range of Reynolds numbers between 2300 and 4000. The value of the Darcy friction factor is subject to large uncertainties in this flow regime.\n\nThe Blasius correlation is the simplest equation for computing the Darcy friction\nfactor. Because the Blasius correlation has no term for pipe roughness, it\nis valid only to smooth pipes. However, the Blasius correlation is sometimes\nused in rough pipes because of its simplicity. The Blasius correlation is valid\nup to the Reynolds number 100000.\n\nThe Darcy friction factor for fully turbulent flow (Reynolds number greater than 4000) in rough conduits can be modeled by the Colebrook–White equation.\n\nThe last formula in the \"Colebrook equation\" section of this article is for free surface flow. The approximations elsewhere in this article are not applicable for this type of flow.\n\nBefore choosing a formula it is worth knowing that in the paper on the Moody chart, Moody stated the accuracy is about ±5% for smooth pipes and ±10% for rough pipes. If more than one formula is applicable in the flow regime under consideration, the choice of formula may be influenced by one or more of the following:\n\nThe phenomenological Colebrook–White equation (or Colebrook equation) expresses the Darcy friction factor \"f\" as a function of Reynolds number Re and pipe relative roughness ε / \"D\", fitting the data of experimental studies of turbulent flow in smooth and rough pipes. \nThe equation can be used to (iteratively) solve for the Darcy–Weisbach friction factor \"f\".\n\nFor a conduit flowing completely full of fluid at Reynolds numbers greater than 4000, it is expressed as:\nor\nwhere:\n\nNote: Some sources use a constant of 3.71 in the denominator for the roughness term in the first equation above.\n\nThe Colebrook equation is usually solved numerically due to its implicit nature. Recently, the Lambert W function has been employed to obtain explicit reformulation of the Colebrook equation.\n\nformula_7\n\nformula_8\n\nor\n\nformula_9\n\nformula_10\n\nwill get:\n\nthen:\n\nAdditional, mathematically equivalent forms of the Colebrook equation are:\nand\n\nThe additional equivalent forms above assume that the constants 3.7 and 2.51 in the formula at the top of this section are exact. The constants are probably values which were rounded by Colebrook during his curve fitting; but they are effectively treated as exact when comparing (to several decimal places) results from explicit formulae (such as those found elsewhere in this article) to the friction factor computed via Colebrook's implicit equation.\n\nEquations similar to the additional forms above (with the constants rounded to fewer decimal places, or perhaps shifted slightly to minimize overall rounding errors) may be found in various references. It may be helpful to note that they are essentially the same equation.\n\nAnother form of the Colebrook-White equation exists for free surfaces. Such a condition may exist in a pipe that is flowing partially full of fluid. For free surface flow:\n\nThe above equation is valid only for turbulent flow. Another approach for estimating \"f\" in free surface flows, which is valid under all the flow regimes (laminar, transition and turbulent) is the following:\n\nformula_18\n\nwhere \"a\" is:\n\nformula_19\n\nand \"b\" is:\n\nformula_20\n\nwhere \"Re\" is Reynolds number where \"h\" is the characteristic hydraulic length (hydraulic radius for 1D flows or water depth for 2D flows) and \"R\" is the hydraulic radius (for 1D flows) or the water depth (for 2D flows). The Lambert W function can be calculated as follows:\n\nformula_21 \n\nThe \"Haaland equation\" was proposed in 1983 by Professor S.E. Haaland of the Norwegian Institute of Technology. It is used to solve directly for the Darcy–Weisbach friction factor \"f\" for a full-flowing circular pipe. It is an approximation of the implicit Colebrook–White equation, but the discrepancy from experimental data is well within the accuracy of the data.\n\nThe Haaland equation is expressed:\n\nThe Swamee–Jain equation is used to solve directly for the Darcy–Weisbach friction factor \"f\" for a full-flowing circular pipe. It is an approximation of the implicit Colebrook–White equation.\n\nSerghides's solution is used to solve directly for the Darcy–Weisbach friction factor \"f\" for a full-flowing circular pipe. It is an approximation of the implicit Colebrook–White equation. It was derived using Steffensen's method.\n\nThe solution involves calculating three intermediate values and then substituting those values into a final equation.\n\nThe equation was found to match the Colebrook–White equation within 0.0023% for a test set with a 70-point matrix consisting of ten relative roughness values (in the range 0.00004 to 0.05) by seven Reynolds numbers (2500 to 10).\n\nGoudar equation is the most accurate approximation to solve directly for the Darcy–Weisbach friction factor \"f\" for a full-flowing circular pipe. It is an approximation of the implicit Colebrook–White equation. Equation has the following form\n\nBrkić shows one approximation of the Colebrook equation based on the Lambert W-function\nThe equation was found to match the Colebrook–White equation within 3.15%.\n\nEarly approximations for smooth pipes by Paul Richard Heinrich Blasius in terms of the Moody friction factor are given in one article of 1913:\n\nJohann Nikuradse in 1932 proposed that this corresponds to a power law correlation for the fluid velocity profile.\n\nMishra and Gupta in 1979 proposed a correction for curved or helically coiled tubes, taking into account the equivalent curve radius, R:\nwith,\n\nwhere \"f\" is a function of:\n\nvalid for:\n\nThe following table lists historical approximations to the Colebrook–White relation for pressure-driven flow. Except of Churchill equation (1977), Cheng (2008) and Bellos et al. (2018) equations are the only formulas that return a correct value for friction factor in the laminar flow region (Reynolds number < 2300). All of the others are for transitional and turbulent flow only.\n\n\n"}
{"id": "13935428", "url": "https://en.wikipedia.org/wiki?curid=13935428", "title": "Docosanoid", "text": "Docosanoid\n\nIn biochemistry, docosanoids are signaling molecules made by the metabolism of twenty-two-carbon fatty acids (EFAs), especially the omega-3 fatty acid, Docosahexaenoic acid (DHA) (i.e. 4\"Z\",7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-docosahexaenoic acid) by lipoxygenase, cyclooxygenase, and cytochrome P450 enzymes. Other docosanoids are metabolites of n-3 docosapentaenoic acid (i.e. 7\"Z\",10\"Z\",13\"Z\",16\"Z\",19\"Z\"-docosahexaenoic acid), n-6 DHA (i.e. 4\"Z\",7\"Z\",10\"Z\",13\"Z\",16\"Z\"-docosahexaenoic acid, and docosatetraenoic acid (i.e. 7\"Z\",10\"Z\",13\"Z\",16\"Z\"-docosatetraenoic acid, DTA, or adrenic acid). Prominent docosanoid metabolites of DHA and n-3 DHA are members of the specialized proresolving mediator class of polyunsaturated fatty acid metabolites that possess potent anti-inflammation, tissue healing, and other activities (see specialized proresolving mediators).\n\nPotently bioactive agents of the specialized proresolving mediator class include:\n\nThese DHA metabolites possess anti-inflammation and tissue-protection activities in animal models of inflammatory diseases; they are proposed to inhibit innate immune responses and thereby to protect from and to resolve a wide range of inflammatory responses in animals and humans. These metabolites are also proposed to contribute to the anti-inflammatory and other beneficial effects of dietary omega-3 fatty acids by being metabolized to them.\n\nDHA can be converted non-enzymatically by free radical-mediated peroxidation to 8 different neurofuran regioisomers termed neuroprostanes and neurofuranes including 4-, 7-, 10-, 11-, 13-, 14-, 17-, and 20-series neurofurans/neuroporstanes for a total of 128 different racemic compounds. The most studied DHA-derived of these products are members of the 4-series, neurofuran 4-Fneuroprostane and 4(\"RS\")-ST-Δ6-8-neurofurane. These metabolites have been used mainly as biomarkers of oxidative stress that are formed in nerve tissues of the central nervous system.\n\nCells metabolize DHA to 17\"S\"-hydroperoxy-4\"Z\",7\"Z\",10\"Z\",13\"Z\",15\"E\",19\"Z\"-docahexaenoicacid acid (17-HpDHA) and then rapidly reduce this hydroperoxide to 17\"S\"-hydroxy-4\"Z\",7\"Z\",10\"Z\",13\"Z\",15\"E\",19\"Z\"-docahexaenoicacid acid (17-HDHA) and similarly metabolize DHA to 13\"S\"-hydroperoxy-4\"Z\",7\"Z\",10\"Z\",14\"Z\",16\"Z\",19\"Z\"-docahexaenoicacid acid (13-HpDHA) and then to 13\"S\"-hydroxy-4\"Z\",7\"Z\",10\"Z\",14\"Z\",16\"Z\",19\"Z\"-docahexaenoicacid acid (13-HDHA). 17-HDHA exhibits potent in vitro as well as in vivo (animal model) anti-inflammatory activity while 17-HpDHA and to a lesser extent 17-HDHA inhibit the growth of cultured human breast cancer cells. Other SPM docosanoids, e.g. RvD1 and RvD2, have anti-growth effects against cancer cells in animal models.\n\nCells can metabolize DHA to products that possess an oxo (i.e. ketone) residue. These products include 13-oxo-DHA (termed EFOXD6) and 17-oxo-DHA (termed 18-EFOXD6). Both oxo metabolites possess anti-inflammatory activity as assesses in in vitro systems (see Specialized proresolving mediators#Oxo-DHA and oxo-DPA metabolites).\n\nCyclooxygenase and Cytochrome P450 oxidase act upon Docosatetraenoic acid to produce dihomoprostaglandins and dihomo-epoxyeicosatrienoic acids and dihomo-EETs.\n"}
{"id": "37732235", "url": "https://en.wikipedia.org/wiki?curid=37732235", "title": "Double layer forces", "text": "Double layer forces\n\nAn electrical double layer develops near charged surfaces (or another charged objects) in aqueous solutions. Within this double layer, the first layer corresponds to the charged surface. These charges may originate from tightly adsorbed ions, dissociated surface groups, or substituted ions within the crystal lattice. The second layer corresponds to the diffuse layer, which contains the neutralizing charge consisting of accumulated counterions and depleted coions. The resulting potential profile between these two objects leads to differences in the ionic concentrations within the gap between these objects with respect to the bulk solution. These differences generate an osmotic pressure, which generates a force between these objects.\n\nThese forces are easily experienced when hands are washed with soap. Adsorbing soap molecules make the skin negatively charged, and the slippery feeling is caused by the strongly repulsive double layer forces. These forces are further relevant in many colloidal or biological systems, and may be responsible for their stability, formation of colloidal crystals, or their rheological properties.\n\nThe most popular model to describe the electrical double layer is the Poisson-Boltzmann (PB) model. This model can be equally used to evaluate double layer forces. Let us discuss this model in the case of planar geometry as shown in the figure on the right. In this case, the electrical potential profile \"ψ\"(\"z\") near a charged interface will only depend on the position \"z\". The corresponding Poisson's equation reads in SI units\nwhere \"ρ\" is the charge density per unit volume, \"ε\" the dielectric permittivity of the vacuum, and \"ε\" the dielectric constant of the liquid. For a symmetric electrolyte consisting of cations and anions having a charge ±\"q\", the charge density can be expressed as\nwhere \"c\" = \"N\"/\"V\" are the concentrations of the cations and anions, where \"N\" are their numbers and \"V\" the sample volume. These profiles can be related to the electrical potential by considering the fact that the chemical potential of the ions is constant. For both ions, this relation can be written as\nwhere formula_4 is the reference chemical potential, \"T\" the absolute temperature, and \"k\" the Boltzmann constant. The reference chemical potential can be eliminated by applying the same equation far away from the surface where the potential is assumed to vanish and concentrations attain the bulk concentration \"c\". The concentration profiles thus become\nwhere \"β\" = 1/(\"kT\"). This relation reflects the Boltzmann distribution of the ions with the energy ±\"qψ\". Inserting these relations into the Poisson equation one obtains the PB equation\nThe potential profile between two plates is normally obtained by solving this equation numerically.\n\nOnce the potential profile is known, the force per unit area between the plates expressed as the disjoining pressure Π can be obtained as follows. The starting point is the Gibbs–Duhem relation for a two component system at constant temperature\nIntroducing the concentrations \"c\" and using the expressions of the chemical potentials \"μ\" given above one finds \nThe concentration difference can be eliminated with the Poisson equation and the resulting equation can be integrated from infinite separation of the plates to the actual separation \"h\" by realizing that \nExpressing the concentration profiles in terms of the potential profiles one obtains \nFrom a known electrical potential profile \"ψ\"(\"z\") one can calculate the disjoining pressure from this equation at any suitable position \"z\". Alternative derivation of the same relation for disjoining pressure involves the stress tensor.\n\nWhen the electric potentials or charge densities are not too high, the PB equation can be simplified to the Debye-Hückel (DH) equation. By expanding the exponential function in the PB equation into a Taylor series, one obtains\n\nThe parameter \"κ\" is referred to as the Debye length, and some representative values for a monovalent salt in water at 25°C with \"ε\" ≃ 80 are given in the table on the right. In non-aqueous solutions, Debye length can be substantially larger than the ones given in the table due to smaller dielectric constants. The DH model represents a good approximation, when the surface potentials are sufficiently low with respect to the limiting values\nThe numerical value refers to a monovalent salt and 25°C. In practice, the DH approximation remains rather accurate up to surface potentials that are comparable to the limiting values given above. The disjoining pressure can be obtained from the PB equation given above, which can also be simplified to the DH case by expanding into Taylor series. The resulting expression is\nThe substantial advantage of the DH model over the PB model is that the forces can be obtained analytically. Some of the relevant cases will be discussed below.\n\n When the surfaces are sufficiently far apart, the potential profiles originating from each individual surface will not be much perturbed by the presence of the other surface. This approximation thus suggests that one can simply add (superpose) the potentials profiles originating from each surface as illustrated the figure. Since the potential profile passes through a minimum at the mid-plane, it is easiest to evaluate the disjoining pressure at the midplane. The solution of the DH equation for an isolated wall reads\nwhere \"z\" is the distance from the surface and \"ψ\" the surface potential. The potential at the midplane is thus given by twice the value of this potential at a distance \"z\" = \"h\"/2. The disjoining pressure becomes\nThe electrostatic double layer force decays in an exponential fashion. Due to the screening by the electrolyte, the range of the force is given by the Debye length and its strength by the surface potential (or surface charge density). This approximation turns out to be exact provided the plate-plate separation is large compared to the Debye length and the surface potentials are low.\n\nThis result can be simply generalized to highly charged surfaces, but only at larger separations. Even if the potential is large close to the surface, it will be small at larger distances, and can be described by the DH equation. However, in this case one has to replace the actual diffuse layer potential \"ψ\" with the effective potential \"ψ\". Within the PB model, this effective potential can be evaluated analytically, and reads\nThe superposition approximation can be easily extended to asymmetric systems. Analogous arguments lead to the expression for the disjoining pressure\nwhere the super-scripted quantities refer to properties of the respective surface. At larger distances, oppositely charged surfaces repel and equally charged ones attract.\n\nWhile the superposition approximation is actually exact at larger distances, it is no longer accurate at smaller separations. Solutions of the DH or PB equations in between the plates provide a more accurate picture at these conditions. Let us only discuss the symmetric situation within the DH model here. This discussion will introduce the notion of \"charge regulation\", which suggests that the surface charge (and the surface potential) may vary (or regulate) upon approach.\n\nThe DH equation can be solved exactly for two plates. The boundary conditions play an important role, and the surface potential and surface charge density \nformula_19 and formula_20 become functions of the surface separation \"h\" and they may differ from the corresponding quantities \"ψ\" and \"σ\" for the isolated surface. When the surface charge remains constant upon approach, one refers to the \"constant charge\" (CC) boundary conditions. In this case, the diffuse layer potential will increase upon approach. On the other hand, when the surface potential is kept constant, one refers to \"constant potential\" (CP) boundary condition. In this case, the surface charge density decreases upon approach. Such decrease of charge can be caused by adsorption of desorption of charged ions from the surface. Such variation of adsorbed species upon approach has also been referred to as \"proximal adsorption\". The ability of the surface to regulate its charge can be quantified by the regulation parameter\nwhere \"C\" = \"ε\" \"ε κ\" is the diffuse layer capacitance and \"C\" the inner (or regulation) capacitance. The CC conditions are found when \"p\" = 1 while the CP conditions for \"p\" = 0. The realistic case will be typically situated in between. By solving the DH equation one can show that diffuse layer potential varies upon approach as\nwhile the surface charged density obey a similar relation \nThe swelling pressure can be found by inserting the exact solution of the DH equation into the expressions above and one finds\nRepulsion is strongest for the CC conditions (\"p\" = 1) while it is weaker for the CP conditions (\"p\" = 0). The result of the superposition approximation is always recovered at larger distances but also for \"p\" = 1/2 at all distances. The latter fact explains why the superposition approximation can be very accurate even at small separations. Surfaces regulate their charge and not infrequently the actual regulation parameter is not far away from 1/2. \nThe situation is exemplified in the figure below. From stability considerations one can show that \"p\" < 1 and that this parameter may also becomes negative. These results can be extended to asymmetric case in a straightforward way.\n\nWhen surface potentials are replaced by effective potentials, this simple DH picture is applicable for more highly charged surfaces at sufficiently larger distances. At shorter distances, however, one may enter the PB regime and the regulation parameter may not remain constant. In this case, one must solve the PB equation together with an appropriate model of the surface charging process. It was demonstrated experimentally that charge regulation effects can become very important in asymmetric systems.\n\nInteractions between various objects were studied within the DH and PB models by many researchers. Some of the relevant results are summarized in the following.\n\nNon-planar geometries: Objects of other than planar geometries can be treated within the Derjaguin approximation, provided their size is substantially larger than the Debye length. This approximation has been used to estimate the force between two charged colloidal particles as shown in the first figure of this article. The exponential nature of these repulsive forces and the fact that its range is given by the Debye length was confirmed experimentally by direct force measurements, including surface forces apparatus, colloidal probe technique, or optical tweezers. The interaction free energy involving two spherical particles within the DH approximation follows the Yukawa or screened Coulomb potential\nwhere \"r\" is the center-to-center distance, \"Q\" is the particle charge, and \"a\" the particle radius. This expression is based on the superposition approximation and is only valid at large separations. This equation can be extended to more highly charged particles by reinterpreting the charge \"Q\" as an effective charge. To address the interactions in other situation, one must resort to numerical solutions of the DH or PB equation.\n\nNon-uniform or patchy charge distribution: Interaction between surfaces with non-uniform and periodic charge distribution has been analyzed within the DH approximation. Such surfaces are referred to have a mosaic or patch-charge distribution. One important conclusion from these studies is that there is an additional attractive electrostatic contribution, which also decays exponentially. When the non-uniformities are arranged in a quadratic lattice with spacing \"b\", the decay length \"q\" of this additional attraction can be expressed as\nAt high salt levels, this attraction is screened as the interaction between uniformly charged surfaces. At lower salt levels, however, the range of this attraction is related to the characteristic size of the surface charge heterogeneities.\n\nThree-body forces: The interactions between weakly charged objects are pair-wise additive due to the linear nature of the DH approximation. On the PB level, however, attractive three-body forces are present. The interaction free energy between three objects 1, 2, and 3 can be expressed as\nwhere \"F\" are the pair free energies and \"ΔF\" is the non-additive three-body contribution. These three-body contributions were found to be attractive on the PB level, meaning that three charged objects repel less strongly than what one would expect on the basis of pair-wise interactions alone.\n\nMore accurate description of double layer interactions can be put forward on the \"primitive model\". This model treats the electrostatic and hard-core interactions between all individual ions explicitly. However, it includes the solvent only in a \"primitive\" way, namely as a dielectric continuum. This model was studied in much detail in the theoretical community. Explicit expressions for the forces are mostly not available, but they are accessible with computer simulations, integral equations, or density functional theories.\n\nThe important finding from these studies is that the PB description represents only a mean-field approximation. This approximation is excellent in the so-called \"weak coupling regime\", that is for monovalent electrolytes and weakly charged surfaces. However, this description breaks down in the \"strong coupling regime\", which may be encountered for multivalent electrolytes, highly charged systems, or non-aqueous solvents. In the strong coupling regime, the ions are strongly correlated, meaning that each ion has an exclusion hole around itself. These correlations lead to strong ion adsorption to charged surfaces, which may lead to charge reversal and crystallization of these ions on the surface. These correlations may also induce attractive forces. The range of these forces is typically below 1 nm.\n\nAround 1990, theoretical and experimental evidence has emerged that forces between charged particles suspended in dilute solutions of monovalent electrolytes might be attractive at larger distances. This evidence is in contradiction with the PB theory discussed above, which always predicts repulsive interactions in these situations. The theoretical treatment leading to these conclusions was strongly criticized. The experimental findings were mostly based on video-microscopy, but the underlying data analysis was questioned concerning the role of impurities, appropriateness of image processing techniques, and the role of hydrodynamic interactions. The majority of the scientific community remains skeptical concerning such like-charge attractions and advocates the classical PB approach discussed above.\n\nDouble layer interactions are relevant in a wide number of phenomena. These forces are responsible for swelling of clays. They may also be responsible for the stabilization of colloidal suspension and will prevent particle aggregation of highly charged colloidal particles in aqueous suspensions. At low salt concentrations, the repulsive double layer forces can become rather long-ranged, and may lead to structuring of colloidal suspensions and eventually to formation of colloidal crystals. Such repulsive forces may further induce blocking of surfaces during particle deposition. Double layer interactions are equally relevant for surfactant aggregates, and may be responsible to the stabilization of cubic phases made of spheroidal micelles or lamellar phases consisting of surfactant or lipid bilayers.\n"}
{"id": "55635955", "url": "https://en.wikipedia.org/wiki?curid=55635955", "title": "Dropel Fabrics", "text": "Dropel Fabrics\n\nDropel Fabrics is an American technology company that develops, manufactures, and licenses sustainable treatments for natural fabrics to make spill proof and stain proof threads. The company is known for creating the world’s first water and stain repellent naturals fabrics that maintain their softness and breathability.\nDropel was founded in 2015 by Sim Gulati following his research in material sciences and innovative textile processes. In 2014, after observing a broader need in apparel for innovation in natural fabrics, Gulati developed cotton fabrics using sustainable nanotechnology treatments for cotton in an effort to supplant less durable and less environmentally friendly clothing applications for polyester and other synthetics. In 2015 the company consulted with Amanda Parkes, Ph.D., termed a “fashion scientist” from Massachusetts Institute of Technology by Industry magazine.\nDropel incubated in New York based fashion accelerator, New York Fashion Tech Lab, and launched at the incubator’s June 2015 demonstration day.\nThe New York Times reported that Dropel “patented a nanotechnology process that bonds hydrophobic polymers with natural fibers on the molecular level to make them water- and stain-repellent, a process that can be licensed by clothing brands.” The company has integrated its technology with brands AREA NYC, CEAM and Mister French. Dropel was part of the inaugural class of Fashion For Good, a sustainable fashion accelerator lead by Kering, Plug and Play Ventures, Galleries Lafayette and the C&A Foundation. Fashion Tech Lab, a venture-capital accelerator led by Russian retail entrepreneur Miroslava Duma, Gaetan Bonhomme, Alex Moore, Cybernaut Venture Capital, and Full Tilt Capital invested in Dropel’s seed round of funding. In 2015, Business Insider named Dropel Fabrics one of the “100 most exciting startups in New York City.”\n"}
{"id": "48599140", "url": "https://en.wikipedia.org/wiki?curid=48599140", "title": "Drought cycle (Brazilian literature)", "text": "Drought cycle (Brazilian literature)\n\nDrought Cycle is the name given to the \"drought novels cycle,\" a Brazilian literary era that had as main theme the life in the Brazilian backlands.\n\nIt began with the publication of O sertanejo of José de Alencar (1876), and lasted until the first decade of the twentieth century. The main characters of the drought cycle literature are bandits, migrants and blesseds. In the cycle stand the Ceará writers.\nGilberto Freyre was influenced by this literary tendency. Other relevant authors are Raquel de Queirós, José Lins do Rego, Jorge Amado, Graciliano Ramos, Antônio Callado, until Guimarães Rosa.\n\n\n\n"}
{"id": "49255451", "url": "https://en.wikipedia.org/wiki?curid=49255451", "title": "Economics of plastics processing", "text": "Economics of plastics processing\n\nThe economics of plastics processing is determined by the type of process. Plastics can be processed with the following methods: machining, compression molding, transfer molding, injection molding, extrusion, rotational molding, blow molding, thermoforming, casting, forging, and foam molding. Processing methods are selected based on equipment cost, production rate, tooling cost, and build volume. High equipment and tooling cost methods are typically used for large production volumes whereas low - medium equipment cost and tooling cost methods are used for low production volumes. Compression molding, transfer molding, injection molding, forging, and foam molding has high equipment and tooling cost. Lower cost processes are machining, extruding, rotational molding, blow molding, thermoforming, and casting. A summary of each process and its cost is displayed figure 1.\n\nOxo-degradable plastics: these are petroleum-based plastics with additives such as transition metals and metals salts that promote the process of fragmentation of the plastic when exposed to a particular environment, such as high temperature or oxygen rich one, for a prolonged period of time. Fragmentation exposes a larger surface area of the plastic to colonies of bacteria that eventually decompose the polymer into its lower energy state components: carbon dioxide and water.\n\nSome aspects to take into account regarding this method to dispose of end-of-life plastics are:\nClassifying a polymer as bio-degradable requires specifications regarding these aspects.\n\nImportant economic aspects that need to be considered when disposing of degradable polymers include:\n\nThe implementation of reusable plastic containers arises as a consequence of concerns with sustainability and environmental impact. Use of recyclable plastic packages is beneficial environmentally but is more expensive. The adoption of reusable plastic containers will amount to an approximate annual increase of 0.058 euros/kg of delivered goods. The cost associated with reusable plastic containers are packaging purchasing costs, transportation costs, labor/handling costs, management costs, and costs resulting from losses. Packaging purchasing costs encompasses the cost of the containers as well as any associated service costs. This cost is reoccurring but is only relevant once every 50 cycles, which is the typical lifetime of reusable plastic containers. One cycle consists of the initial stages of processing plastic containers all the way to the use and recycling of these containers by the consumers. Transportation costs are slightly higher for reusable plastic containers as compared to traditional use and throwaway plastic containers in that these reusable containers need additional transportation to recycling facilities. Reusable plastic containers also require work loading and unloading from trucks as well as quality inspection, this adds additional labor costs. Management costs exists because reusable plastic container stock count needs to be managed. The final cost of reusable plastic containers is the cost incurred when packages are lost or there are errors within the management system. Figure 2 provides a detailed summary of the costs associated with adopting reusable plastic containers.\n\nRecycling plastics presents the difficulty of handling mixed plastics, as unmixed plastics are usually necessary to maintain desirable properties. Mixing many plastics results in diminished material properties, with even just a few percent of polypropylene mixed with polyethylene producing a plastic with significantly reduced tensile strength. An alternative to recycling of these plastics and those which can’t be easily recycled such as thermosets is to use degradation to break the polymers down into monomers of low molecular weight. The products of this process can be used to make high quality polymers however energy stored in the polymer bonds is lost during this process.\n\nAn alternative to economically dispose of plastics is to burn them in an incinerator. Incinerators capable of cleanly burning polymers exist and while they require significant capital investment, the energy produced offsets the economic impact. Since most plastics are produced from petroleum, their molecules consist exclusively or primarily of carbon, oxygen, and hydrogen atoms. With proper design, an incinerator can completely combust these plastics allowing the recovery of energy stored in the original petroleum feedstock which would otherwise escape during processes such as degradation. Some polymers contain chlorine or nitrogen which can result in undesirable combustion products however the use of scrubbers can remove such products. The end result is that many polymers burn more cleanly than coal and as clean as most oils.\n"}
{"id": "7152303", "url": "https://en.wikipedia.org/wiki?curid=7152303", "title": "Eivind Aadland", "text": "Eivind Aadland\n\nEivind Aadland (born 19 September 1956) is a Norwegian conductor and violinist. He has been concert master of the Bergen Philharmonic Orchestra.\n\nEivind was Chief Conductor and Artistic Leader of the Trondheim Symphony Orchestra from 2004-2010, and maintains a regular relationship with many Scandinavian orchestras, including the Oslo and Bergen Philharmonics, Stavanger Symphony and Swedish Chamber Orchestra. At Den Norske Opera in Oslo he has conducted productions of Don Giovanni, Le nozze di Figaro, Die Zauberflöte and Die Fledermaus.\n\nRecent seasons have included performances with the Orchestre du Capitole de Toulouse, the Swedish Radio and Melbourne Symphony Orchestras, the Lausanne and Scottish Chamber Orchestras and the Symphony Orchestras of Gothenburg, the Finnish Radio, SWR Stuttgart and WDR Cologne. Engagements during the 08/09 season included concerts with the Iceland Symphony Orchestra, Tasmanian Symphony Orchestra and the Queensland Orchestra, Royal Flemish Philharmonic and the Orchestre National de Belgique. \nMr. Aadland’s recording output encompasses a diverse range of repertoire and he is a champion of Norwegian and Swedish composers. These include the symphonic works of Eivind Groven, a disc of Norwegian orchestral favourites and the complete music for violin and orchestra of Arne Nordheim with the Stavanger Symphony – all for BIS records. On the Simax label Aadland he has made discs of Irgens Jensen's works with the Bergen Philharmonic, music of Ole Bull with the Trondheim Symphony and music of Gustav Holst, Percy Grainger and Florent Schmitt with The staff band of the Norwegian Armed Forces. Also with the Trondheim Symphony he has recorded two discs devoted to the orchestral music of Ludwig Irgens-Jensen and Gerhard Schjeldrup for CPO Records. He has also recorded for Hyperion, ASV, IMP Classics, and Koch.\nA student of Jorma Panula, Eivind Aadland was encouraged by Mariss Jansons to pursue his conducting career. Previously as a violinist, Eivind Aadland was concertmaster of the Bergen Philharmonic (1981–1989) and Music Director of the European Union Chamber Orchestra during 1987–1997, with whom he directed concerts at concert halls and festivals throughout Europe, and made a number of recordings. Mr. Aadland studied with Yehudi Menhuin and together they performed chamber concerts in Paris and London as well as in Switzerland.\n\n"}
{"id": "3226406", "url": "https://en.wikipedia.org/wiki?curid=3226406", "title": "Electricity provider switching", "text": "Electricity provider switching\n\nElectricity provider switching is the ability of power consumers to have an option—or the \"power to choose\"—their electricity provider in a deregulated electricity market as permitted by a state public utilities governing body.\n\nThe Australian market has been somewhat deregulated, but still sees consumers provided with a narrow band of choices.\n\nElectricity is deregulated in two Canadian provinces: Ontario and Alberta. Both markets showed price spikes in the first year of dereguation, but then settled down into a volatile but reasonably stable environment. Alberta's market is dominated by fossil fuel generation and as such reacts more closely to the price of natural gas. Ontario's generation mix is about 50% nuclear.\n\nThe consumer has the choice between buying from their local utility (Local Distribution Company - LDC) or from one of the deregulated suppliers. There is a large range of contract options from a variable price to 1,3 or 5 year fixed prices. Electricity provider switching is difficult once the consumer is in one of these contracts, unless they are close to the end of a fixed price contract. However, as of January 2010 there is a maximum termination penalty allowed.\n\nA very important element in switching electricity providers in Ontario is understanding the Global Adjustment. This is an adjustment for some commitments government agencies have made on your behalf. It is included in the LDC Regulated Price Plan, but is an additional line item if a contract is signed.\n\nThe consumer has the choice between buying from their local utility (Local Distribution Company - LDC) or from a deregulated suppliers. There are however many fewer of these in Alberta. Electricity provider switching is difficult once the consumer is in one of these contracts, unless they are close to the end of a fixed price contract.\n\nThere is a price comparison service operation in Canada.\n\nIn France, electricity market is totally deregulated and consumers have the choice between the historical formerly state-owned provider EDF and several new private providers like Direct Énergie.\n\nElectricity supply has also been deregulated in the United Kingdom. For a list of suppliers see: \"Category:Power companies of the United Kingdom\" at the foot of the page.\n\nIn deregulated markets such as Texas and Maryland, the state government may require the incumbent utility energy provider to allow for unlimited competition within the marketplace, where the consumer is free to choose any electricity provider. Electricity provider switching is only practical if a customer is either buying from a utility or is at the end of a fixed-price contract with a provider.\n\nCertain U.S. states allow for consumer choice in electricity providers, with Texas being the most-widely watched deregulatory scheme. Many other states are surveying the Texas deregulatory model in order to use its design as a model for the imposition of free market forces within such other power markets.\n\nAs of April 2014, 16 U.S. states and the District of Columbia have deregulated electricity markets. Along with aforementioned Maryland and Texas, electricity deregulation is current in Connecticut, Delaware, Illinois, Maine, Massachusetts, Michigan, Montana, New Hampshire, New Jersey, New York, Ohio, Oregon, Pennsylvania, and Rhode Island. Seven additional U.S. states began the process of electricity deregulation but have suspended efforts: Arizona, Arkansas, California, Nevada, New Mexico, Virginia, and Wyoming.\n\nA broad and diverse group of retail energy suppliers who share the common vision that competitive retail energy markets deliver a more efficient, customer-oriented outcome than a regulated utility structure. \nRetailers Include:\nAEP Energy\n, APG&E Energy Solutions\n, Calpine Energy Solutions\n, Constellations NewEnergy, INC\n, Crius Energy\n, Direct Energy Services LLC\n, Dynegy\n, ENGIE Resources\n, Entrust Energy\n, IGS Energy\n, Just Energy\n, Liberty Power \n, Next Era Energy Services\n, Nordic Energy Services, LLC\n, NRG Energy INC\n, Source Energy \n, Spark Energy\n, Starion Energy\n, Stream Energy\n, Talen Energy\n, TransCanada Power Marketing LTD.\n\n<br>\n"}
{"id": "41033059", "url": "https://en.wikipedia.org/wiki?curid=41033059", "title": "Explicit algebraic stress model", "text": "Explicit algebraic stress model\n\nThe algebraic stress model arises in computational fluid dynamics. Two main approaches can be undertaken. In the first, the transport of the turbulent stresses is assumed proportional to the turbulent kinetic energy; while in the second, convective and diffusive effects are assumed to be negligible. Algebraic stress models can only be used where convective and diffusive fluxes are negligible, i.e. source dominated flows. In order to simplify the existing EASM and to achieve an efficient numerical implementation the underlying tensor basis plays an important role. The five-term tensor basis that is introduced here tries to combine an optimum of accuracy of the complete basis with the advantages of a pure 2d concept. Therefore a suitable five-term basis is identified. Based on that the new model is designed and validated in combination with different eddy-viscosity type background models.\n\nIn the frame work of single-point closures (Reynolds-stress transport models = RSTM) still provide the best representation of flow physics. Due to numeric requirements an explicit formulation based on a low number of tensors is desirable and was already introduced originally most explicit algebraic stress models are formulated using a 10-term basis:\nThe reduction of the tensor basis however requires an enormous mathematical effort, to transform the algebraic stress formulation for a given linear algebraic RSTM into a given tensor basis by keeping all important properties of the underlying model. This transformation can be applied to an arbitrary tensor basis. In the present investigations an optimum set of basis tensors and the corresponding coefficients is to be found.\n\nThe projection method was introduced to enable an approximate solution of the algebraic transport equation of the Reynolds-stresses. In contrast to the approach of the tensor basis is not inserted in the algebraic equation, instead the algebraic equation is projected. Therefore, the chosen basis tensors does not need to form a complete integrity basis. However, the projection will fail if the basis tensor are linear dependent. In the case of a complete basis the projection leads to the same solution as the direct insertion, otherwise an approximate solution in the sense is obtained.\n\nIn order to prove, that the projection method will lead to the same solution as the direct insertion, the EASM for two-dimensional flows is derived. In two-dimensional flows only the tensors are independent.\nThe projection leads then to the same coefficients. This two-dimensional EASM is used as starting point for an optimized EASM which includes three-dimensional effects. For example the shear stress variation in a rotating pipe cannot be predicted with quadratic tensors. Hence, the EASM was extended with a cubic tensor. In order to do not affect the performance in 2D flows, a tensor was chosen that vanish in 2d flows. This offers the concentration of the coefficient determination in 3d flows. A cubic tensor, which vanishes in 3d flow is: \nThe projection with tensors T, T, T and T yields then the coefficients of the EASM.\n\nA direct result of the EASM derivation is a variable formulation of C.As the generators of the extended EASM where chosen to preserve the existing 2D formulation the expression of C remains unchanged:\n\nA are the constants of the underlying pressure-strain model.\nSince η is always positive it might be possible that C becomes singular. Therefore in the first EASM derivation of a regularization was introduced, which prevent a singular by cutting the range of η. However, Wallin et al. pointed out that the regularization deteriorated the performance of the EASM. In their model the methodology was refined to account for the coefficient.\n\nThis leads to a weak non-linear conditional equation for the EASM coefficients and an additional equation for g must be solved. In 3D the equation of g is of 6th order, wherefore a closed solution is only possible in 2D flows, where the equation reduces to 3rd order. In order to circumvent the root finding of a polynomial equation quasi self-consistent approach. He showed that by using a C expression of a realizable linear model instead of the EASM-C expression in the equation of g the same properties of g follows. For a wide range of and the quasi self-consistent approach is almost identical to the fully self-consistent solution. Thus the quality of the EASM is not affected with the advantage of no additional non-linear equation. Since in the projections to determine the EASM coefficients the complexity is reduced by neglecting higher order invariants.\n\n"}
{"id": "3787507", "url": "https://en.wikipedia.org/wiki?curid=3787507", "title": "Ferroics", "text": "Ferroics\n\nFerroics is the generic name given to the study of ferromagnets, ferroelectrics, and ferroelastics. \n\nThe basis of ferroics is to understand the large changes in physical characteristics that occur over a very narrow temperature range. The changes in physical characteristics occur when phase transitions take place around some critical temperature value, normally denoted by formula_1. Above this critical temperature, the crystal is in a nonferroic state and does not exhibit the physical characteristic of interest. Upon cooling the material down below formula_1 it undergoes a spontaneous phase transition. Such a phase transition typically results in only a small deviation from the nonferroic crystal structure, but in altering the shape of the unit cell the point symmetry of the material is reduced. This breaking of symmetry is physically what allows the formation of the ferroic phase. \n\nIn ferroelectrics, upon lowering the temperature below formula_1, a spontaneous dipole moment is induced along an axis of the unit cell. Although individual dipole moments can sometimes be small, the effect of formula_4 unit cells gives rise to an electric field that over the bulk substance that is not insignificant. An important point about ferroelectrics is that they cannot exist in a centrosymmetric crystal. A centrosymmetric crystal is one where a lattice point formula_5 can be mapped onto a lattice point formula_6.\n\nFerromagnets is a term that most people are familiar with, and, as with ferroelastics, the spontaneous magnetization of a ferromagnet can be attributed to a breaking of point symmetry in switching from the paramagnetic to the ferromagnetic phase. In this case, formula_1 is normally known as the Curie Temperature. \n\nIn ferroelastic crystals, in going from the nonferroic (or prototypic phase) to the ferroic phase, a spontaneous strain is induced. An example of a ferroelastic phase transition is when the crystal structure spontaneously changes from a tetragonal structure (a square prism shape) to a monoclinic structure (a general parallelepiped). Here the shapes of the unit cell before and after the phase transition are different, and hence a strain is induced within the bulk. \n\nIn recent years a new class of ferroic materials has been attracting increased interest. These multiferroics exhibit more than one ferroic property simultaneously in a single phase.\n\n"}
{"id": "5823879", "url": "https://en.wikipedia.org/wiki?curid=5823879", "title": "Gas oil ratio", "text": "Gas oil ratio\n\nWhen oil is brought to surface conditions it is usual for some natural gas to come out of solution. The gas/oil ratio (GOR) is the ratio of the volume of gas that comes out of solution, to the volume of oil at standard conditions.\n\nA point to check is whether the volume of oil is measured before or after the gas comes out of solution, since the oil volume will shrink when the gas comes out. \n\nIn fact gas dissolution and oil volume shrinkage will happen at many stages during the path of the hydrocarbon stream from reservoir through the wellbore and processing plant to export. For light oils and rich gas condensates the ultimate GOR of export streams is strongly influenced by the efficiency with which the processing plant strips liquids from the gas phase. Reported GORs may be calculated from export volumes which may not be at standard conditions.\n\nThe GOR is a dimensionless ratio (volume per volume) in metric units, but in field units, it is usually measured in cubic feet of gas per barrel of oil or condensate...\n\nIn the states of Texas and Pennsylvania, the statutory definition of a gas well is one where the GOR is greater than 100,000 ft3/bbl or 100 Kcf/bbl.\nThe state of New Mexico also designates a gas well as having over 100 MCFG per barrel (http://www.nmcpr.state.nm.us/nmac/parts/title19/19.015.0002.htm)\n\nThe Oklahoma Geologic Survey recently published Map of Oklahoma Oil and Gas Fields distinguished by Oil and Gas Ratio this map that defines a gas well as having greater than 20 MCFG per barrel of oil. They go on to define an oil well as having a GOR of less than 5 MCFG/BBL and an oil and gas well between 5 and 20 MCFG/BBl.\n\nIn the newly proposed Information Collection Request for Oil and Gas Facilities (EPA ICR No. 2548.01, OMB Control No. 2060-NEW) the EPA has divided well types into five categories as such:\n1. Heavy Oil (GOR ≤ 300 scf/bbl)\n2. Light Oil (GOR 300 < GOR ≤ 100,000 scf/bbl)\n3. Wet Gas (100,000 < GOR ≤1,000,000 scf/bbl)\n4. Dry Gas (GOR > 1,000,000 scf/bbl)\n5. Coal Bed Methane.\n\nAs two-stroke engines use their crankcase to pressurize the air:fuel mixture before transfer to the cylinder, any standard lubricating oil left there (per practice with four-stroke engines) would be swept up and burnt with the fuel. To provide cylinder lubrication, fuels supplied to two-stroke engines are often mixed with oil so that it can coat the cylinders and bearing surfaces along its path. The ratio of gas to oil is set by the engine manufacturer but ranges from 30:1 to 50:1 per volume unit. Oil remaining in the mixture is burnt with the fuel and results in a familiar blue smoke and odor.\n\n"}
{"id": "52129202", "url": "https://en.wikipedia.org/wiki?curid=52129202", "title": "Hydraulic fracturing in Ukraine", "text": "Hydraulic fracturing in Ukraine\n\nHydraulic fracturing in Ukraine has been used since the 1950s. The first hydraulic fracturing operation was conducted in 1954 for the underground coal gasification project. There has been a strong revival of interest in the hydraulic fracturing industry in Ukraine. According to the U.S. Energy Information Administration, Ukraine has third-largest shale gas reserves in Europe at . Since 2011, approximately 22 domestic and foreign-owned companies have been engaged in hydraulic fracturing in Ukraine.\n\nAt least two companies have backed out of a deal to extract shale gas in Eastern Ukraine due to the threat of military action in that area. There are also other challenges to hydraulic fracturing in Ukraine, such as a lack of a proper regulatory framework for its development, opposition of major EU partners to hydraulic fracturing which may seek to influence Ukraine (France for example has an outright ban). Ukraine's shale gas reserves are also deeper than those in the United States, and thus production is bound to be more expensive, which may make it cost-prohibitive, depending on the prevailing market prices for gas.\n\nConstitutionally, Ukraine's natural resources belong to the people, with government acting as a trustee. A private investor needs to execute a production-sharing agreement, but is never entitled to 100% of its production, as it has to be shared with the state. The level of potential public opposition to hydraulic fracturing also creates uncertainty. On the positive side, Ukraine continues to vigorously pursue reforms designed to achieve energy independence, which portends well for hydraulic fracturing as a helpful option in that regard.\n"}
{"id": "49281", "url": "https://en.wikipedia.org/wiki?curid=49281", "title": "Hydronium", "text": "Hydronium\n\nIn chemistry, hydronium is the common name for the aqueous cation , the type of oxonium ion produced by protonation of water. It is the positive ion present when an Arrhenius acid is dissolved in water, as Arrhenius acid molecules in solution give up a proton (a positive hydrogen ion, H) to the surrounding water molecules (HO).\n\nThe ratio of hydronium ions to hydroxide ions determines a solution's pH. The molecules in pure water auto-dissociate (\"i.e\".: react with each other) into hydronium and hydroxide ions in the following equilibrium:\n\nIn pure water, there is an equal number of hydroxide and hydronium ions, so it is a neutral solution. At 25 °C, water has a pH of 7 (this varies when the temperature changes: see self-ionization of water). A pH value less than 7 indicates an acidic solution, and a pH value more than 7 indicates a basic solution.\n\nAccording to IUPAC nomenclature of organic chemistry, the hydronium ion should be referred to as \"oxonium\". \"Hydroxonium\" may also be used unambiguously to identify it. A draft IUPAC proposal also recommends the use of oxonium and \"oxidanium\" in organic and inorganic chemistry contexts, respectively.\n\nAn oxonium ion is any ion with a trivalent oxygen cation. For example, a protonated hydroxyl group is an oxonium ion, but not a hydronium ion.\n\nSince and N have the same number of electrons, is isoelectronic with ammonia. As shown in the images above, has a trigonal pyramidal molecular geometry with the oxygen atom at its apex. The H–O–H bond angle is approximately 113°, and the center of mass is very close to the oxygen atom. Because the base of the pyramid is made up of three identical hydrogen atoms, the molecule's symmetric top configuration is such that it belongs to the C point group. Because of this symmetry and the fact that it has a dipole moment, the rotational selection rules are Δ\"J\" = ±1 and Δ\"K\" = 0. The transition dipole lies along the \"c\"-axis and, because the negative charge is localized near the oxygen atom, the dipole moment points to the apex, perpendicular to the base plane.\n\nHydronium is the cation that forms from water in the presence of hydrogen ions. These hydrons do not exist in a free state - they are extremely reactive and are solvated by water. An acidic solute is generally the source the hydrons; however, hydroniums exist even in pure water. This special case of water reacting with water to produce hydronium (and hydroxide) ions is commonly known as the self-ionization of water. The resulting hydronium ions are few and short-lived. pH is a measure of the relative activity of hydronium and hydroxide ions in aqueous solutions. In acidic solutions, hydronium is the more active, its excess proton being readily available for reaction with basic species.\n\nThe hydronium ion is very acidic: at 25 °C, its p\"K\" is 0. It is the most acidic species that can exist in water (assuming sufficient water for dissolution): any stronger acid will ionize and protonate a water molecule to form hydronium. The acidity of hydronium is the implicit standard used to judge the strength of an acid in water: strong acids must be better proton donors than hydronium, otherwise a significant portion of acid will exist in a non-ionized state (i.e.: a weak acid). Unlike hydronium in neutral solutions that result from water's autodissociation, hydronium ions in acidic solutions are long-lasting and concentrated, in proportion to the strength of the dissolved acid.\n\npH was originally conceived to be a measure of the hydrogen ion concentration of aqueous solution. We now know that virtually all such free protons quickly react with water to form hydronium; acidity of an aqueous solution is therefore more accurately characterized by its hydronium concentration. In organic syntheses, such as acid catalyzed reactions, the hydronium ion () can be used interchangeably with the H ion; choosing one over the other has no significant effect on the mechanism of reaction.\n\nResearchers have yet to fully characterize the solvation of hydronium ion in water, in part because many different meanings of solvation exist. A freezing-point depression study determined that the mean hydration ion in cold water is approximately : on average, each hydronium ion is solvated by 6 water molecules which are unable to solvate other solute molecules.\n\nSome hydration structures are quite large: the magic ion number structure (called \"magic\" because of its increased stability with respect to hydration structures involving a comparable number of water molecules – this is a similar usage of the word \"magic\" as in nuclear physics) might place the hydronium inside a dodecahedral cage. However, more recent ab initio method molecular dynamics simulations have shown that, on average, the hydrated proton resides on the surface of the cluster. Further, several disparate features of these simulations agree with their experimental counterparts suggesting an alternative interpretation of the experimental results.\n\nTwo other well-known structures are the \"Zundel cation\" and the \"Eigen cation\". The Eigen solvation structure has the hydronium ion at the center of an complex in which the hydronium is strongly hydrogen-bonded to three neighbouring water molecules. In the Zundel complex the proton is shared equally by two water molecules in a symmetric hydrogen bond. Recent work indicates that both of these complexes represent ideal structures in a more general hydrogen bond network defect.\n\nIsolation of the hydronium ion monomer in liquid phase was achieved in a nonaqueous, low nucleophilicity superacid solution (). The ion was characterized by high resolution nuclear magnetic resonance.\n\nA 2007 calculation of the enthalpies and free energies of the various hydrogen bonds around the hydronium cation in liquid protonated water at room temperature and a study of the proton hopping mechanism using molecular dynamics showed that the hydrogen-bonds around the hydronium ion (formed with the three water ligands in the first solvation shell of the hydronium) are quite strong compared to those of bulk water.\n\nA new model was proposed by Stoyanov based on infrared spectroscopy in which the proton exists as an ion. The positive charge is thus delocalized over 6 water molecules.\n\nFor many strong acids, it is possible to form crystals of their hydronium salt that are relatively stable. These salts are sometimes called \"acid monohydrates\". As a rule, any acid with an ionization constant of or higher may do this. Acids whose ionization constant is below generally cannot form stable salts. For example, hydrochloric acid has an ionization constant of , and mixtures with water at all proportions are liquid at room temperature. However, perchloric acid has an ionization constant of , and if liquid anhydrous perchloric acid and water are combined in a 1:1 molar ratio, they react to form solid hydronium perchlorate (·).\n\nThe hydronium ion also forms stable compounds with the carborane superacid . X-ray crystallography shows a C symmetry for the hydronium ion with each proton interacting with a bromine atom each from three carborane anions 320 pm apart on average. The salt is also soluble in benzene. In crystals grown from a benzene solution the solvent co-crystallizes and a · (benzene) cation is completely separated from the anion. In the cation three benzene molecules surround hydronium forming pi-cation interactions with the hydrogen atoms. The closest (non-bonding) approach of the anion at chlorine to the cation at oxygen is 348 pm.\n\nThere are also many examples of hydrated hydronium ions known, such as the ion in , the and ions both found in .\n\nHydronium is an abundant molecular ion in the interstellar medium and is found in diffuse and dense molecular clouds as well as the plasma tails of comets. Interstellar sources of hydronium observations include the regions of Sagittarius B2, Orion OMC-1, Orion BN–IRc2, Orion KL, and the comet Hale–Bopp.\n\nInterstellar hydronium is formed by a chain of reactions started by the ionization of into by cosmic radiation. can produce either or through dissociative recombination reactions, which occur very quickly even at the low (≥10 K) temperatures of dense clouds. This leads to hydronium playing a very important role in interstellar ion-neutral chemistry.\n\nAstronomers are especially interested in determining the abundance of water in various interstellar climates due to its key role in the cooling of dense molecular gases through radiative processes. However, does not have many favorable transitions for ground-based observations. Although observations of HDO (the deuterated version of water) could potentially be used for estimating abundances, the ratio of HDO to is not known very accurately.\n\nHydronium, on the other hand, has several transitions that make it a superior candidate for detection and identification in a variety of situations. This information has been used in conjunction with laboratory measurements of the branching ratios of the various dissociative recombination reactions to provide what are believed to be relatively accurate and HO abundances without requiring direct observation of these species.\n\nAs mentioned previously, is found in both diffuse and dense molecular clouds. By applying the reaction rate constants (\"α\", \"β\", and \"γ\") corresponding to all of the currently available characterized reactions involving , it is possible to calculate \"k\"(\"T\") for each of these reactions. By multiplying these \"k\"(\"T\") by the relative abundances of the products, the relative rates (in cm/s) for each reaction at a given temperature can be determined. These relative rates can be made in absolute rates by multiplying them by the []. By assuming for a dense cloud and for a diffuse cloud, the results indicate that most dominant formation and destruction mechanisms were the same for both cases. It should be mentioned that the relative abundances used in these calculations correspond to TMC-1, a dense molecular cloud, and that the calculated relative rates are therefore expected to be more accurate at . The three fastest formation and destruction mechanisms are listed in the table below, along with their relative rates. Note that the rates of these six reactions are such that they make up approximately 99% of hydronium ion's chemical interactions under these conditions. Finally, it should also be noted that all three destruction mechanisms in the table below are classified as dissociative recombination reactions.\n\nIt is also worth noting that the relative rates for the formation reactions in the table above are the same for a given reaction at both temperatures. This is due to the reaction rate constants for these reactions having \"β\" and \"γ\" constants of 0, resulting in which is independent of temperature.\n\nSince all three of these reactions produce either or OH, these results reinforce the strong connection between their relative abundances and that of . The rates of these six reactions are such that they make up approximately 99% of hydronium ion's chemical interactions under these conditions.\n\nAs early as 1973 and before the first interstellar detection, chemical models of the interstellar medium (the first corresponding to a dense cloud) predicted that hydronium was an abundant molecular ion and that it played an important role in ion-neutral chemistry. However, before an astronomical search could be underway there was still the matter of determining hydronium's spectroscopic features in the gas phase, which at this point were unknown. The first studies of these characteristics came in 1977, which was followed by other, higher resolution spectroscopy experiments. Once several lines had been identified in the laboratory, the first interstellar detection of HO was made by two groups almost simultaneously in 1986. The first, published in June 1986, reported observation of the \"J\" = 1 − 2 transition at in OMC-1 and Sgr B2. The second, published in August, reported observation of the same transition toward the Orion-KL nebula.\n\nThese first detections have been followed by observations of a number of additional HO transitions. The first observations of each subsequent transition detection are given below in chronological order:\n\nIn 1991, the 3 − 2 transition at was observed in OMC-1 and Sgr B2. One year later, the 3 − 2 transition at was observed in several regions, the clearest of which was the W3 IRS 5 cloud.\n\nThe first far-IR 4 − 3 transition at 69.524 µm (4.3121 THz) was made in 1996 near Orion BN-IRc2. In 2001, three additional transitions of HO in were observed in the far infrared in Sgr B2; 2 − 1 transition at 100.577 µm (2.98073 THz), 1 − 1 at 181.054 µm (1.65582 THz) and 2 − 1 at 100.869 µm (2.9721 THz).\n\n\n"}
{"id": "31437286", "url": "https://en.wikipedia.org/wiki?curid=31437286", "title": "Kapitza's pendulum", "text": "Kapitza's pendulum\n\nKapitza's pendulum or Kapitza pendulum is a rigid pendulum in which the pivot point vibrates in a vertical direction, up and down. It is named after Russian Nobel laureate physicist Pyotr Kapitza, who in 1951 developed a theory which successfully explains some of its unusual properties. The unique feature of the Kapitza pendulum is that the vibrating suspension can cause it to balance stably in an inverted position, with the bob above the suspension point. In the usual pendulum with a fixed suspension, the only stable equilibrium position is with the bob hanging below the suspension point; the inverted position is a point of unstable equilibrium, and the smallest perturbation moves the pendulum out of equilibrium. In nonlinear control theory the Kapitza pendulum is used as an example of a parametric oscillator that demonstrates the concept of \"dynamic stabilization\".\n\nThe pendulum was first described by A. Stephenson in 1908, who found that the upper vertical position of the pendulum might be stable when the driving frequency is fast Yet until the 1950s there was no explanation for this highly unusual and counterintuitive phenomenon. Pyotr Kapitza was the first to analyze it in 1951. He carried out a number of experimental studies and as well provided an analytical insight into the reasons of stability by splitting the motion into \"fast\" and \"slow\" variables and by introducing an effective potential. This innovative work created a new subject in physics, that is vibrational mechanics. Kapitza's method is used for description of periodic processes in atomic physics, plasma physics and cybernetical physics. The effective potential which describes the \"slow\" component of motion is described in \"Mechanics\" volume of the Landau's \"Course of Theoretical Physics\".\n\nAnother interesting feature of the Kapitza pendulum system is that the bottom equilibrium position, with the pendulum hanging down below the pivot, is no longer stable. Any tiny deviation from the vertical increases in amplitude with time. Parametric resonance can also occur in this position, and chaotic regimes can be realized in the system when strange attractors are present in the Poincaré section .\n\nDenote the vertical axis as formula_1 and the horizontal axis as formula_2 so that the motion of pendulum happens in the (formula_2-formula_1) plane. The following notation will be used\n\nDenoting the angle between pendulum and downward direction as formula_11 the time dependence of the position of pendulum gets written as\n\nThe potential energy of the pendulum is due to gravity and is defined by of the vertical position as\n\nThe kinetic energy in addition to the standard term formula_14, describing velocity of a mathematical pendulum, there is a contribution due to vibrations of the suspension\n\nThe total energy is given by the sum of the kinetic and potential energies formula_16 and the Lagrangian by their difference formula_17.\n\nThe total energy is conserved in a mathematical pendulum, so time formula_18 dependence of the potential formula_19 and kinetic formula_20 energies is symmetric with respect to the horizontal line. According to the virial theorem the mean kinetic and potential energies in harmonic oscillator are equal. This means that the line of symmetry corresponds to half of the total energy.\n\nIn the case of vibrating suspension the system is no longer a closed one and the total energy is no longer conserved. The kinetic energy is more sensitive to vibration compared to the potential one. The potential energy formula_21 is bound from below and above formula_22 while the kinetic energy is bound only from below formula_23. For high frequency of vibrations formula_5 the kinetic energy can be large compared to the potential energy.\n\nMotion of pendulum satisfies Euler–Lagrange equations. The dependence of the phase formula_11 of the pendulum on its position satisfies the equation:\n\nwhere the Lagrangian formula_27 reads\n\nup to irrelevant total time derivative terms. The differential equation\n\nwhich describes the movement of the pendulum is nonlinear due to the formula_30 factor.\n\nKapitza's pendulum model is more general than the simple pendulum. The Kapitza model reduces to the latter in the limit formula_31. In that limit, the tip of the pendulum describes a circle: formula_32. If the energy in the initial moment is larger than the maximum of the potential energy formula_33 then the trajectory will be closed and cyclic. If the initial energy is smaller formula_34 then the pendulum will oscillate close to the only stable point formula_35.\n\nWhen the suspension is vibrating with a small amplitude formula_36 and with a frequency formula_37 much higher than the proper frequency formula_38, the angle formula_39 may be viewed as a superposition formula_40 of a \"slow\" component formula_41 and a rapid oscillation formula_42 with small amplitude due to the small but rapid vibrations of the suspension. Technically, we perform a perturbative expansion in the \"coupling constants\" formula_43 while treating the ratio formula_44 as fixed. The perturbative treatment becomes exact in the double scaling limit formula_45. More precisely, the rapid oscillation formula_42 is defined as\n\nThe equation of motion for the \"slow\" component formula_41 becomes\n\nTime-averaging over the rapid formula_5-oscillation yields to leading order\n\nThe \"slow\" equation of motion becomes\n\nby introducing an effective potential\n\nIt turns out that the effective potential formula_54 has two minima if formula_55, or equivalently, formula_56. The first minimum is in the same position formula_57 as the mathematical pendulum and the other minimum is in the upper vertical position formula_58. As a result the upper vertical position, which is unstable in a mathematical pendulum, can become stable in Kapitza's pendulum.\n\nThe rotating solutions of the Kapitza's pendulum occur when the pendulum rotates around the pivot point at the same frequency that the pivot point is driven. There are two rotating solutions, one for a rotation in each direction. We shift to the rotating reference frame using formula_59 and the equation for formula_39 becomes:\n\nAgain considering the limit in which formula_5 is much higher than the proper frequency formula_38, we find that the rapid-formula_5 slow-formula_65 limit leads to the equation:\n\nThe effective potential is just that of a simple pendulum equation. There is a stable equilibrium at formula_67 and an unstable equilibrium at formula_68.\n\nInteresting phase portraits might be obtained in regimes which are not accessible within analytic descriptions, for example in the case of large amplitude of the suspension formula_69. Increasing the amplitude of driving oscillations to half of the pendulum length formula_70 leads to the phase portrait shown in the figure.\n\nFurther increase of the amplitude to formula_71) leads to full filling of the internal points of the phase space, if before some points of the phase space were not accessible, now system can reach any of the internal points. This situation holds also for larger values of formula_6.\n\n\n"}
{"id": "24901967", "url": "https://en.wikipedia.org/wiki?curid=24901967", "title": "Karen R. Polenske", "text": "Karen R. Polenske\n\nKaren Rosel Polenske (born March 20, 1937) is an American regional economist specialized in energy, environmental, and infrastructure analyses, and input-output accounts and models, particularly at the subnational scale. She is currently the Peter de Florez Professor of Regional Political Economy at the Massachusetts Institute of Technology (MIT).\n\nPolenske received her undergraduate degree in Home Economics from the Oregon State University in 1959. She holds a master in Public Administration and Economics from Syracuse University and a PhD in Economics from Harvard University, where she was in charge of the regional work at the Harvard Economic Research Project. Working with Wassily W. Leontief, she directed one of the most extensive multiregional input-output research studies of the U.S. economy in history.\n\nShe has been director of the multiregional planning research team at MIT since 1972. She is also past President of the International Input-Output Association. Polenske has been an advisor to international agencies, including the United Nations Development Programme, Intergovernmental Panel on Climate Change, and the World Bank, as well as an economic consultant to the U.S. Bureau of Economic Analysis, Department of Commerce, Department of Justice, and the U.S. Army Corp of Engineers.\n\nIn 2007, Polenske received special recognition from Rajendra K. Pachauri, chair of the Intergovernmental Panel on Climate Change, for having \"contributed substantially to the work of the Intergovernmental Panel on Climate Change (IPCC),\" thus contributing to the IPCC's award of the 2007 Nobel Peace Prize. In 2005 she became a lifetime Regional Science Association International (RSAI) Fellow, in recognition to significant scholarly and research contributions to the field of regional science. In 1999, she received the Margaret McCoy Award, for her outstanding contribution towards the advancement of women in planning at institutions of higher education through service, teaching, and research, and in 1996 she was awarded the Walter Isard Distinguished Scholar Prize for distinguished long-term achievements in the field of Regional Science.\n\nHer publications include eight books, and numerous articles in key economic, energy, environmental, and planning journals, establishing her as a leading political economist.\n\n\n\n\n"}
{"id": "38404191", "url": "https://en.wikipedia.org/wiki?curid=38404191", "title": "Kudurru of Kaštiliašu", "text": "Kudurru of Kaštiliašu\n\nThe kudurru of Kaštiliašu' is a fragment of an ancient Mesopotamian \"narû\", or entitlement stele, recording the legal action taken by Kassite king Kaštiliašu IV (ca. 1232–1225 BC) over land originally granted by his forebear Kurigalzu II (ca. 1332–1308 BC), son of Burna-Buriaš II to Uzub-Šiḫu or -Šipak in grateful recognition of his efforts in the war against Assyria under its king, Enlil-nirari. Along with the Tablet of Akaptaḫa, these are the only extant kudurrus from this king’s short eight-year reign and were both recovered from Elamite Susa, where they had been taken in antiquity, during the French excavations under Jacques de Morgan at the end of the nineteenth century and now reside in the Musée du Louvre.\n\nThe surviving kudurru fragment is a crescent-shaped cross-section with convex surface inscribed with cuneiform and a concave side engraved with relief images. Where the stele tapers to the top, it carries representations of the gods Sîn (crescent moon), Šamaš (solar-disc) and Ištar (eight-pointed star) in bas-relief. Beneath these a demon with a lion’s head, human body and short tail brandishes a knife in one hand and a club or mace in the other. This is Ugallu, “Big Weather Beast”, one of the eleven monsters who were to be conquered by Marduk in the later publication, Enûma Eliš, and who was to feature on apotropaic figurines of the first millennium BC. The seated dog figure of Gula is carved facing the demon.\n\nThe broken text recalls that Kurigalzu had awarded an individual with the Kassite name Uzub-Šiḫu (or -Šipak, a Ksssite deity) a large area of 120 (around 3.75 square miles) of agricultural land for services rendered during the war against Assyria. This suggests a successful outcome in this conflict in marked contrast to the account espoused by the Synchronistic History, an Assyrian polemic chronicle inscription which boasts of Kurigalzu’s apparent defeat at the Battle of Sugagu, a view which was also contradicted in the Babylonian Chronicle P version of these events and also in Assyrian king Adad-nārārī I’s own recollections of his father, Enlil-nirari’s setbacks.\n\nThe text mentions Nimgirabi-Marduk, son of Nazi-…. and Pir-Šamaš, son of Šumat-Šamaš, but their roles are uncertain. The land grant was reconfirmed by Kaštiliašu, possibly to a descendant of the original beneficiary, perhaps due to the failure to provide a sealed legal document during the earlier bequest.\n"}
{"id": "3722468", "url": "https://en.wikipedia.org/wiki?curid=3722468", "title": "Laser scanning at Stonehenge", "text": "Laser scanning at Stonehenge\n\nThe first use of 3D laser scanning at Stonehenge was of the Bronze Age dagger and axes inscribed on the sarsens, which was undertaken in 2002 by a team from Wessex Archaeology and Archaeoptics. They used a Minolta Vivid 900 scanner to analyse and record surfaces of the prehistoric and post-medieval carvings.\n\nThe Bronze Age carvings of a dagger and an axehead were first discovered by archaeologist Richard J. C. Atkinson in 1953 on stone number 53, one of the imposing sarsen trilithons. A contemporary survey in 1956 by Robert Newall revealed that the total number of axes on this stone totalled 14, all on the same face of the stone, looking inwards to the centre of the stone circle. Typologically, the axes have a Middle Bronze Age date. \n\nThe surface of stone 53 containing Bronze Age carvings was laser scanned at a resolution of 0.5mm, resulting in hundreds of thousands of individual 3D measurements known as a point cloud. These data were then processed into a meshed 3D solid model for analysis using custom software written by Archaeoptics called \"Demon3D\". \n\nThe team pioneered some visualisation techniques to enhance the outlines of the known carvings. During this process, the faint outline of two previously unknown axes was spotted in an animation, separate from the carvings recorded by Newall. Subsequent enhancement of the data confirmed that the shapes were of flanged axes, similar in shape to those clearly visible, but either badly eroded, or were originally carved much shallower than their counterparts. The larger of the two carvings differs slightly from the other axes in that it has two 'lugs' along its shaft, and others have interpreted that it could represent either an axe, a mushroom, or a ram's skull. \n\nThe results of these investigations were published in an article entitled \"The Stonehenge Laser Show\" in the November 2003 edition of British Archaeology.\n\nIn 2011 English Heritage commissioned a full laser scan of the visible faces of all stones as Stonehenge in high resolution (sub-millimetre), as well as a lower resolution scan of the ground in the area known as the \"Stonehenge Triangle\".\n\n"}
{"id": "27217887", "url": "https://en.wikipedia.org/wiki?curid=27217887", "title": "Magazine paper", "text": "Magazine paper\n\nMagazine paper are paper grades generally used in printing of magazines.\n\nMagazine papers are made on paper machines from pulp. The pulp may be recycled, mechanical or chemical depending on the magazine quality. Publishers select the type of paper that not only meets their customers' requirements, but also works well in their machinery.\n\nDifferent paper grades are used in magazines:\n\n"}
{"id": "10365776", "url": "https://en.wikipedia.org/wiki?curid=10365776", "title": "Murie Science and Learning Center", "text": "Murie Science and Learning Center\n\nThe Murie Science and Learning Center is a collaboration between the Denali National Park and Preserve, seven additional National Parks and several park partners..\" The Murie Science and Learning Center promotes scientific research to aid park managers and provide science-based education programs and information to students, educational institutions and the visiting public.\n\nThe center is named after the Murie family of naturalists, who made significant, influential studies of arctic ecosystems. Olaus Murie and Margaret Murie were active in the establishment of the Arctic National Wildlife Refuge and The Wilderness Society. Olaus's brother Adolph Murie was equally influential, studying wolves in Denali in the 1930s.\n\nThe additional parks which partner with Denali National Park in this venture:\n\n"}
{"id": "58921", "url": "https://en.wikipedia.org/wiki?curid=58921", "title": "Natural disaster", "text": "Natural disaster\n\nA natural disaster is a major adverse event resulting from natural processes of the Earth; examples are floods, hurricanes, tornadoes, volcanic eruptions, earthquakes, tsunamis, and other geologic processes. A natural disaster can cause loss of life or damage property, and typically leaves some economic damage in its wake, the severity of which depends on the affected population's resilience, or ability to recover and also on the infrastructure available.\n\nAn adverse event will not rise to the level of a disaster if it occurs in an area without vulnerable population. In a vulnerable area, however, such as Nepal during the 2015 earthquake, an earthquake can have disastrous consequences and leave lasting damage, which can require years to repair.\n\nA landslide is described as an outward and downward slope movement of an abundance of slope-forming materials including rock, soil, artificial, or even a combination of these things.\n\nDuring World War I, an estimated 40,000 to 80,000 soldiers died as a result of avalanches during the mountain campaign in the Alps at the Austrian-Italian front. Many of the avalanches were caused by artillery fire.\n\nAn earthquake is the result of a sudden release of energy in the Earth's crust that creates seismic waves. At the Earth's surface, earthquakes manifest themselves by vibration, shaking, and sometimes displacement of the ground. Earthquakes are caused by slippage within geological faults. The underground point of origin of the earthquake is called the \"seismic focus\". The point directly above the focus on the surface is called the \"epicenter\". Earthquakes by themselves rarely kill people or wildlife. It is usually the secondary events that they trigger such as building collapse, fires, tsunamis (seismic sea waves) and volcanoes. Many of these could possibly be avoided by better construction, safety systems, early warning and planning.\n\nWhen natural erosion, human mining or underground excavation makes the ground too weak to support the structures built on it, the ground can collapse and produce a sinkhole. For example, the 2010 Guatemala City sinkhole which killed fifteen people was caused when heavy rain from Tropical Storm Agatha, diverted by leaking pipes into a pumice bedrock, led to the sudden collapse of the ground beneath a factory building.\n\nVolcanoes can cause widespread destruction and consequent disaster in several ways. The effects include the volcanic eruption itself that may cause harm following the explosion of the volcano or falling rocks. Secondly, lava may be produced during the eruption of a volcano, and so as it leaves the volcano the lava destroys many buildings, plants and animals due to its extreme heat. Thirdly, volcanic ash, generally meaning the cooled ash, may form a cloud, and settle thickly in nearby locations. When mixed with water this forms a concrete-like material. In sufficient quantities, ash may cause roofs to collapse under its weight but even small quantities will harm humans if inhaled. Since the ash has the consistency of ground glass it causes abrasion damage to moving parts such as engines. The main killer of humans in the immediate surroundings of a volcanic eruption is the pyroclastic flows, which consist of a cloud of hot volcanic ash which builds up in the air above the volcano and rushes down the slopes when the eruption no longer supports the lifting of the gases. It is believed that Pompeii was destroyed by a pyroclastic flow. A lahar is a volcanic mudflow or landslide. The 1953 Tangiwai disaster was caused by a lahar, as was the 1985 Armero tragedy in which the town of Armero was buried and an estimated 23,000 people were killed.\n\nVolcanoes rated at 8 (the highest level) on the Volcanic Explosivity Index are known as supervolcanoes. According to the Toba catastrophe theory, 75,000 to 80,000 years ago a supervolcanic eruption at what is now Lake Toba in Sumatra reduced the human population to 10,000 or even 1,000 breeding pairs, creating a bottleneck in human evolution, and killed three-quarters of all plant life in the northern hemisphere. However, there is considerable debate regarding the veracity of this theory. The main danger from a supervolcano is the immense cloud of ash, which has a disastrous global effect on climate and temperature for many years.\n\nA violent, sudden and destructive change either in the quality of Earth's water or in the distribution or movement of water on land below the surface or in the atmosphere.\n\nA flood is an overflow of water that 'submerges' land. The EU Floods Directive defines a flood as a temporary covering the land with water which is usually not covered by water. In the sense of 'flowing water', the word may also be applied to the inflow of the tides. Flooding may result from the volume of water within a body of water, such as a river or lake, which overflows, causing some of the water to escape its usual boundaries. While the size of a lake or other body of water will vary with seasonal changes in precipitation and snow melt, it is not a significant flood unless the water covers land used by man, like a village, city or other inhabited area, roads, expanses of farmland, etc.\n\nA tsunami (plural: tsunamis or tsunami; from Japanese: 津波, lit. \"harbour wave\"; English pronunciation: /tsuːˈnɑːmi/), also known as a seismic sea wave or as a tidal wave, is a series of waves in a water body caused by the displacement of a large volume of water, generally in an ocean or a large lake. Tsunamis can be caused by undersea earthquakes such as the 2004 Boxing Day tsunami, or by landslides such as the one in 1958 at Lituya Bay, Alaska, or by volcanic eruptions such as the ancient eruption of Santorini. On March 11, 2011, a tsunami occurred near Fukushima, Japan and spread through the Pacific Ocean.\n\nA limnic eruption occurs when a gas, usually CO, suddenly erupts from deep lake water, posing the threat of suffocating wildlife, livestock and humans. Such an eruption may also cause tsunamis in the lake as the rising gas displaces water. Scientists believe landslides, volcanic activity, or explosions can trigger such an eruption. To date, only two limnic eruptions have been observed and recorded. In 1984, in Cameroon, a limnic eruption in Lake Monoun caused the deaths of 37 nearby residents, and at nearby Lake Nyos in 1986 a much larger eruption killed between 1,700 and 1,800 people by asphyxiation.\n\nCyclone, tropical cyclone, hurricane, and typhoon are different names for the same phenomenon, which is a cyclonic storm system that forms over the oceans. The determining factor on which term is used is based on where they originate. In the Atlantic and Northeast Pacific, the term \"hurricane\" is used; in the Northwest Pacific it is referred to as a \"typhoon\" and \"cyclones\" occur in the South Pacific and Indian Ocean.\nThe deadliest hurricane ever was the 1970 Bhola cyclone; the deadliest Atlantic hurricane was the Great Hurricane of 1780 which devastated Martinique, St. Eustatius and Barbados. Another notable hurricane is Hurricane Katrina, which devastated the Gulf Coast of the United States in 2005.\n\nBlizzards are severe winter storms characterized by heavy snow and strong winds. When high winds stir up snow that has already fallen, it is known as a ground blizzard. Blizzards can impact local economic activities, especially in regions where snowfall is rare. The Great Blizzard of 1888 affected the United States, when many tons of wheat crops were destroyed, and in Asia, 2008 Afghanistan blizzard and the 1972 Iran blizzard were also significant events. The 1993 Superstorm originated in the Gulf of Mexico and traveled north, causing damage in 26 states as well as Canada and leading to more than 300 deaths.\n\nHailstorms are precipitation in the form of ice, with the ice not melting before it hits the ground. Hailstones usually measure between 0.2 inch (5 millimetres) and 6 inches (15 centimetres) in diameter. A particularly damaging hailstorm hit Munich, Germany, on July 12, 1984, causing about $2 billion in insurance claims.\n\nAn ice storm is a type of winter storm characterized by freezing rain. The U.S. National Weather Service defines an ice storm as a storm which results in the accumulation of at least 0.25 inch (6.4 mm) of ice on exposed surfaces. \n\nA cold wave (known in some regions as a cold snap or cold spell) is a weather phenomenon that is distinguished by a cooling of the air. Specifically, as used by the U.S. National Weather Service, a cold wave is a rapid fall in temperature within a 24-hour period requiring substantially increased protection to agriculture, industry, commerce, and social activities. The precise criterion for a cold wave is determined by the rate at which the temperature falls, and the minimum to which it falls. This minimum temperature is dependent on the geographical region and time of year.\n\nA heat wave is a period of unusually and excessively hot weather. The worst heat wave in recent history was the European Heat Wave of 2003. A summer heat wave in Victoria, Australia, created conditions which fuelled the massive bushfires in 2009. Melbourne experienced three days in a row of temperatures exceeding 40 °C (104 °F) with some regional areas sweltering through much higher temperatures. The bushfires, collectively known as \"Black Saturday\", were partly the act of arsonists. The 2010 Northern Hemisphere summer resulted in severe heat waves, which killed over 2,000 people. It resulted in hundreds of wildfires which caused widespread air pollution, and burned thousands of square miles of forest.\n\nDrought is the unusual dryness of soil caused by levels of rainfall significantly below average over a prolonged period. Hot dry winds, shortage of water, high temperatures and consequent evaporation of moisture from the ground can also contribute to conditions of drought. Droughts result in crop failure and shortages of water.\n\nWell-known historical droughts include the 1997–2009 Millennium Drought in Australia led to a water supply crisis across much of the country. As a result, many desalination plants were built for the first time (see list). In 2011, the State of Texas lived under a drought emergency declaration for the entire calendar year and severe economic losses. The drought caused the Bastrop fires.\n\nSevere storms, dust clouds, and volcanic eruptions can generate lightning. Apart from the damage typically associated with storms, such as winds, hail, and flooding, the lightning itself can damage buildings, ignite fires and kill by direct contact. Especially deadly lightning incidents include a 2007 strike in Ushari Dara, a remote mountain village in northwestern Pakistan, that killed 30 people, the crash of LANSA Flight 508 which killed 91 people, and a fuel explosion in Dronka, Egypt caused by lightning in 1994 which killed 469. Most lightning deaths occur in the poor countries of America and Asia, where lightning is common and adobe mud brick housing provides little protection.\n\nA tornado is a violent and dangerous rotating column of air that is in contact with both the surface of the Earth and a cumulonimbus cloud, or the base of a cumulus cloud in rare cases. It is also referred to as a \"twister\" or a \"cyclone\", although the word cyclone is used in meteorology in a wider sense, to refer to any closed low pressure circulation. Tornadoes come in many shapes and sizes, but are typically in the form of a visible condensation funnel, whose narrow end touches the Earth and is often encircled by a cloud of debris and dust. Most tornadoes have wind speeds less than , are approximately across, and travel a few miles (several kilometers) before dissipating. The most extreme tornadoes can attain wind speeds of more than 300 mph (480 km/h), stretch more than two miles (3 km) across, and stay on the ground for dozens of miles (perhaps more than 100 km).\n\nWildfires are large fires which often start in wildland areas. Common causes include lightning and drought but wildfires may also be started by human negligence or arson. They can spread to populated areas and can thus be a threat to humans and property, as well as wildlife. Notable cases of wildfires were the 1871 Peshtigo Fire in the United States, which killed at least 1700 people, and the 2009 Victorian bushfires in Australia.\n\nAsteroids that impact the Earth have led to several major extinction events, including one which created the Chicxulub crater 64.9 million years ago and which is associated with the demise of the dinosaurs. Scientists estimate that the likelihood of death for a living human from a global impact event is comparable to the probability of death from an airliner crash.\n\nNo human death has been definitively attributed to an impact event, but the 1490 Ch'ing-yang event in which over 10,000 people may have died has been linked to a meteor shower. Even asteroids and comets that burn up in the atmosphere can cause significant destruction on the ground due to the air burst explosion: notable air bursts include the Tunguska event in June 1908, which devastated large areas of Siberian countryside, and the Chelyabinsk meteor on 15 February 2013, which caused widespread property damage in the city of Chelyabinsk and injured 1,491.\n\nA solar flare is a phenomenon where the Sun suddenly releases a great amount of solar radiation, much more than normal. Solar flares are unlikely to cause any direct injury, but can destroy electrical equipment. The potential of solar storms to cause disaster was seen during the 1859 Carrington event, which disrupted the telegraph network, and the March 1989 geomagnetic storm which blacked out Quebec. Some major known solar flares include the X20 event on August 16, 1989,<ref name=\"Solar_flare1989/2001\"></ref> and a similar flare on April 2, 2001. The most powerful flare ever recorded occurred on November 4, 2003 (estimated at between X40 and X45).\n\nInternational law, for example Geneva Conventions defines International Red Cross and Red Crescent Movement the Convention on the Rights of Persons with Disabilities, requires that \"States shall take, in accordance with their obligations under international law, including international humanitarian law and international human rights law, all necessary measures to ensure the protection and safety of persons with disabilities in situations of risk, including the occurrence of natural disaster.\" And further United Nations Office for the Coordination of Humanitarian Affairs is formed by General Assembly Resolution 44/182. People displaced due to natural disasters are currently protected under international law (Guiding Principles of International Displacement, Campala Convention of 2009).\n\nAccording to the UN, Asia-Pacific is the world's most disaster prone region. According to ReliefWeb, a person in Asia-Pacific is five times more likely to be hit by a natural disaster than someone living in other regions.\n\nNatural disasters can also affect political relations with countries and vice versa. Violent conflicts within states can exacerbate the impact of natural disasters by weakening the ability of states, communities and individuals to provide disaster relief. Natural disasters can also worsen ongoing conflicts within states by weakening the capacity of states to fight rebels. In developed countries like the US, studies find that incumbents lose votes when the electorate perceives them as responsible for a poor disaster response. In Chinese and Japanese history, it has been routine for era names and/or capital cities and palaces of emperors to be changed after a major natural disaster, chiefly for political reasons such as association with hardships by the populace and fear of upheaveal. (i.e. in East asian government chronicles, such fears were recorded in a low profile way as an unlucky name or place requiring change.) Disasters and responses can dictate political careers; the once popular President Benigno Aquino III of Philippines, following a weak and confused response to Typhoon Yolanda which killed over 6,000 people and survivors were largely left to fend for themselves, this widely accepted sentiment carried over and the President never recovered his popularity, his hand picked successor Mar Roxas lost the subsequent election to a rival party in a landslide vote. Post-disaster mishandling can spread despair as bad news travels fast and far, and contribute to the appeal of electing a strongman out of sheer desperation.\n\nBetween 1995 and 2015, according to the UN’s disaster-monitoring system, the greatest number of natural disasters occurred in America, China and India.\n\nIn 2012, there were 905 natural disasters worldwide, 93% of which were weather-related disasters. Overall costs were US$170 billion and insured losses $70 billion. 2012 was a moderate year. 45% were meteorological (storms), 36% were hydrological (floods), 12% were climatological (heat waves, cold waves, droughts, wildfires) and 7% were geophysical events (earthquakes and volcanic eruptions). Between 1980 and 2011 geophysical events accounted for 14% of all natural catastrophes.\n\nStudies on natural events require complete historical records and strategies related to obtaining and storing reliable records, allowing for both critical interpretation and validation of the sources. Under this point of view the irreplaceable role of traditional repositories (archives) can be supplemented by the use of such web sources as eBay.\n\n\n"}
{"id": "57494", "url": "https://en.wikipedia.org/wiki?curid=57494", "title": "Nu-Wood Decorative Millwork", "text": "Nu-Wood Decorative Millwork\n\nNu-Wood Decorative Millwork is millwork made of a specially formulated polyurethane polymer. The standard density is similar to white pine. It can be cut, sawn, shaped, routed, nailed, stapled, and screwed just like wood. Profiles exist for a variety of uses, indoors and out. Advantages over wood include a surface which will not crack, flake, or blister and a product which is resistant to water, mold, mildew, and insects.\n\nThe polyurethane foam is created from two components which generate heat when combined. One component contains water and the other component reacts with water. The oxygen of the water fuels the exothermic reaction, leaving hydrogen bubbles which causes the compound to foam. Given no compression, the foam rises to fill the space allowed. Based on the mixture and the specific makeup of the two chemicals, it is possible to estimate exactly how much foam is required to fill a given space. More importantly, it is possible to determine the exact amount of mixture required to fill a defined space and achieve a specific packing ratio.\n\nTremendous compression is required to obtain packing equal to the density of white pine. In addition, synthetic wood needs a wood look and feel on the surface. This means the finished product must easily slide out of the mold that provides its shape. It also means the mold must be strong enough to withstand the pressure of the expanding foam. This combination of requirements creates a complex problem that is solved by a variety of techniques.\n\nSilicone rubber is used to create negative space molds from original millwork created by expert woodworkers. The forms are polished, painted with a release agent surrounded by a box made of angled aluminum and filled with the liquid silicone rubber compound. The rubber is generally around one inch thick. Once the rubber hardens the form is removed from the standard, which is shelved for future use. At this time, the form provides a negative space with the exact dimensional and surface characteristics of the woodwork to be produced, plus a small tolerance for expected shrinkage.\n\nThe silicone form and form box pass along a bed of rollers to a paint shop where the silicone form is covered with a thin veneer of water-based paint. The heat and pressure of the expanding foam cause the paint to transfer to the finished piece. The paint protects the silicone form from the chemicals in the polyurethane and provides a release from the piece.\n\nWhile the finished polyurethane product is quite light, consisting largely of closed cells of hydrogen, the silicone form and the metal and wood foam box are very heavy. The manufacturing process is difficult to automate due to the many shapes and sizes and the fact that most orders are unique. Thus, the work requires great strength and is quite hazardous to the back, hands, and feet. Competition produces pricing pressures, and labor is the most expensive component of pricing. As a result, low wages are standard.\n\nNu-Wood Decorative Millwork is manufactured in Syracuse, Indiana. The business was founded to serve the manufactured housing industry by Robert DuBois, who later sold the company to Goshen Rubber Company. Goshen Rubber sold the company to private investors in 1998. In 2001 Nu-Wood was acquired by the Beil Family who operated Nu-Wood building a strong presence in Indianapolis and Cincinnati. In 2010 Nu-Wood was purchased by Len and Marci Morris who also operate Stairsupplies.com. Nu-Wood operates delivery vehicles and serving the continental US and Canada. In early 2008 Nu_wood was purchased and is still operated by Jasper Plastic Solutions in Syracuse, Indiana.\n\nOn 6 December 2001, a worker argument resulted in the shooting death of the general manager, the wounding of six employees and the shooter's suicide. A new operations manager joined the company in 1993 and worked to create a more friendly and stress-free work environment. For more information: http://www.cnn.com/2001/US/12/06/indiana.shooting/\n\n"}
{"id": "4721890", "url": "https://en.wikipedia.org/wiki?curid=4721890", "title": "OK-650 reactor", "text": "OK-650 reactor\n\nThe OK-650 reactor is the nuclear fission reactor used for the powering the Soviet Navy's \nProject 685 Плавник/\"Plavnik\" (Mike), \nProject 971 Щука-Б/\"Shchuka-B\" (Akula), and \nProject 945 Барракуда/\"Barrakuda\", Кондор/\"Kondor\", and Марс/\"Mars\" (Sierra) submarines, and in pairs to power the \nProject 941 Акула/\"Akula\" (Typhoon) and \nProject 949 Гранит/\"Granit\" and Антей/\"Antei\" (Oscar) third generation submarines. \nThis pressurized water reactor (PWR) uses 20-45% enriched uranium-235 fuel to produce 190 MW of power. Developed during the 1970s, these reactors were designed with the aim of minimizing accidents and malfunctions. Monitoring subsystems, designed for rapid detection of leaks, were included, along with newer-generation emergency cooling systems for the main reactor core. The reactor is now also used to power the new Project 955 Borei submarines. It was developed by OKBM Afrikantov.\n"}
{"id": "2436357", "url": "https://en.wikipedia.org/wiki?curid=2436357", "title": "Outcrop", "text": "Outcrop\n\nAn outcrop or rocky outcrop is a visible exposure of bedrock or ancient superficial deposits on the surface of the Earth.\n\nOutcrops do not cover the majority of the Earth's land surface because in most places the bedrock or superficial deposits are covered by a mantle of soil and vegetation and cannot be seen or examined closely. However, in places where the overlying cover is removed through erosion or tectonic uplift, the rock may be exposed, or \"crop out\". Such exposure will happen most frequently in areas where erosion is rapid and exceeds the weathering rate such as on steep hillsides, mountain ridges and tops, river banks, and tectonically active areas. In Finland, glacial erosion during the last glacial maximum (ca. 11000 BC), followed by scouring by sea waves, followed by isostatic uplift has produced a large number of smooth coastal and littoral outcrops.\n\nBedrock and superficial deposits may also be exposed at the Earth's surface due to human excavations such as quarrying and building of transport routes.\n\nOutcrops allow direct observation and sampling of the bedrock \"in situ\" for geologic analysis and creating geologic maps. In situ measurements are critical for proper analysis of geological history and outcrops are therefore extremely important for understanding the geologic time scale of earth history. Some of the types of information that cannot be obtained except from bedrock outcrops or by precise drilling and coring operations, are structural geology features orientations (e.g. bedding planes, fold axes, foliation), depositional features orientations (e.g. paleo-current directions, grading, facies changes), paleomagnetic orientations. Outcrops are also very important for understanding fossil assemblages, and paleo-environment, and evolution as they provide a record of relative changes within geologic strata.\n\nAccurate description, mapping, and sampling for laboratory analysis of outcrops made possible all of the geologic sciences and the development of fundamental geologic laws such as the law of superposition, the principle of original horizontality, principle of lateral continuity, and the principle of faunal succession.\n\nOn Ordnance Survey maps in Great Britain, cliffs are distinguished from outcrops: cliffs have a continuous line along the top edge with lines protruding down; outcrops have a continuous line around each area of bare rock. An outcrop example in California is the Vasquez Rocks, familiar from location shooting use in many films, composed of uplifted sandstone. Yana is another example of outcrops, located in Uttara Kannada district in Karnataka, India.\n\n"}
{"id": "2727018", "url": "https://en.wikipedia.org/wiki?curid=2727018", "title": "Page (paper)", "text": "Page (paper)\n\nA page is one side of a leaf (or sheet) of paper, parchment or other material (or electronic media) in a book, magazine, newspaper, or other collection of sheets, on which text or illustrations can be printed, written or drawn, to create documents. It can be used as a measure of communicating general quantity of information (\"That topic covers twelve pages\") or more specific quantity (\"there are 535 words in a standard page in standard font type\")\n\nThe word \"page\" comes from the Latin term \"pagina\", which means a \"a written page, leaf, sheet\" which in turn comes from an earlier meaning \"to create a row of vines that form a rectangle.\" The Latin word \"pagina\" derives from the verb \"pangere,\" which means to stake out boundaries when planting vineyards.\n\nIn a book, the side of a leaf one reads first is called the recto page, and the back side of that leaf is called the verso page. In a spread, one reads the verso page first and then reads the recto page of the next leaf. In English-language books, the recto page is on the right and the verso page is on the left. By modern convention, these books start with a recto page and hence all recto pages in such books have uneven numbers.\n\nEnglish-language books are read from left to right and the reader flips the pages from right to left. In languages read from right to left (Arabic, Hebrew, and Persian, plus Chinese and Japanese when written vertically), the first page is typically a recto page on the left and the reader flips the pages from left to right.\n\nThe process of placing the various text and graphical elements on the page in a visually organized way is called page layout, and the relative lightness or darkness of the page is referred to as its colour.\n\nIn book typography, a \"cat hairbrush\" refers to a master design of a page, designed by the graphic designer or the typographer of a book, that illustrates how similar pages in the same book can achieve a level of visual consistency. To help maintain the desired consistency, the typical page may employ a grid system.\n\nIn a modern book, a page may contain a header and a footer. Pages may or may not be numbered, but most pages are.\n\n\"...The first printed books had no title pages. As with the manuscripts of the Middle Ages which the first printers sought to imitate as clearly as possible, and with which their books had to compete for a market, the reader launched at once into the text, with no more than a curt phrase at the head of the column which read \"incipit\": \"Here beginneth\"...\n\nThe pages appearing before the main text of a book (including the title page, preface, table of contents, etc.) are collectively called the front matter and those appearing after the main text (appendices, colophon, etc.), the back matter. Placement of the copyright page varies between different typographic traditions: in English-language books it belongs to the front matter; however, in Chinese and Japanese, the copyright page is part of the back matter.\n\nIn English-language typography, the size of a page is traditionally measured in a unit called the pica.\n\nCompound words:\n\nIdiomatic expressions\n\nIn library science, the number of pages in a book forms part of its physical description, coded in subfield $300a in MARC 21 and in subfield $215a in UNIMARC. This description consists of the number of pages (or a list of such numberings separated by commas, if the book contains separately-numbered sections), followed by the abbreviation \"p.\" for \"page(s)\". The number of pages is written in the same style (Arabic or Roman numerals, uppercase or lowercase, etc.) as the numbering in each section. Unnumbered pages are not described.\n\nFor example,\ndescribes a book with two sections, where section one contains 11 pages numbered using uppercase Roman numerals, and section two contains 2050 pages numbered using Arabic numerals; the total number of pages is thus 2061 pages, plus any unnumbered pages.\n\nIf the book contains too many separately-numbered sections, too many unnumbered pages, or only unnumbered pages, the librarian may choose to describe the book as just \"1 v.\" (one volume) when doing original cataloguing.\n\nIn word processors and spreadsheets, the process of dividing a document into actual pages of paper is called pagination. Printing a large page on multiple small pages of paper is sometimes called tiling.\n\nIn early computing, computer output typically consists of monospaced text neatly arranged in equal number of columns and rows on each printed page. Such pages are typically printed using line printers (or, in the case of personal computers, character (usually dot matrix) printers) that accepts a simple code such as ASCII, and the end of a printed page can be indicated by a control character called the form feed.\n\nPage printers, printers that print one page at a time, typically accept page description languages. In the PostScript page description language, the page being described is printed using the \"showpage'’ operator.\n\nThe concept of the \"page\" has been carried over to the World Wide Web where we speak of web \"pages.\" The term web page is simply a document or a computer file. It is usually written in Hypertext Markup Language (HTML), where users can get access by entering a URL in an internet browser.\n\nUsers can print pages in the web. Web pages can be printed by downloading to the hard disk or directly from the browser. Easiness of printing a web page depends on its length. Longer web pages with infinite scrolling are harder to print as the number of unloaded pages is unknown to the user.\n\nClickbait makes printing a web page difficult, as the printed version contains ads. This issue can be overcome using browser extensions such as Print Friendly & PDF in Google Chrome.\n\n"}
{"id": "22242751", "url": "https://en.wikipedia.org/wiki?curid=22242751", "title": "Photocatalytic water splitting", "text": "Photocatalytic water splitting\n\nPhotocatalytic water splitting is an artificial photosynthesis process with photocatalysis in a photoelectrochemical cell used for the dissociation of water into its constituent parts, hydrogen () and oxygen (), using either artificial or natural light. Theoretically, only solar energy (photons), water, and a catalyst are needed. This topic is the focus of much research, but thus far no technology has been commercialized.\n\nHydrogen fuel production has gained increased attention as oil and other nonrenewable fuels become increasingly depleted and expensive. Methods such as photocatalytic water splitting are being investigated to produce hydrogen, a clean-burning fuel. Water splitting holds particular promise since it utilizes water, an inexpensive renewable resource. Photocatalytic water splitting has the simplicity of using a catalyst and sunlight to produce hydrogen out of water. \n\nWhen is split into and , the stoichiometric ratio of its products is 2:1:\n\nThe process of water-splitting is a highly endothermic process (Δ\"H\" > 0). Water splitting occurs naturally in photosynthesis when photon energy is absorbed and converted into the chemical energy through a complex biological pathway (Dolai's S-state diagrams. Ref. Dolai, U.(2017)\"Chemical Scheme of Water-splitting Process during Photosynthesis by the way of Experimental Analysis \".IOSR Journal of Pharmacy and Biological Sciences.12(6):65-67.doi:10.9790/3008-1206026567. ISSN 2319-7676.). However, production of hydrogen from water requires large amounts of input energy, making it incompatible with existing energy generation. For this reason, most commercially produced hydrogen gas is produced from natural gas.\n\nOf the several requirements for an effective photocatalyst for water splitting, the potential difference (voltage) must be 1.23V at 0 pH. Since the minimum band gap for successful water splitting at pH=0 is 1.23 eV, corresponding to light of 1008 nm, the electrochemical requirements can theoretically reach down into infrared light, albeit with negligible catalytic activity. These values are true only for a completely reversible reaction at standard temperature and pressure (1 bar and 25 °C).\n\nTheoretically, infrared light has enough energy to split water into hydrogen and oxygen; however, this reaction is very slow because the wavelength is greater than 750 nm. The potential must be less than 3.0 V to make efficient use of the energy present across the full spectrum of sunlight. Water splitting can transfer charges, but not be able to avoid corrosion for long term stability. Defects within crystalline photocatalysts can act as recombination sites, ultimately lowering efficiency.\n\nUnder normal conditions due to the transparency of water to visible light photolysis can only occur with a radiation wavelength of 180 nm or shorter. We see then that, assuming a perfect system, the minimum energy input is 6.893 eV.\n\nMaterials used in photocatalytic water splitting fulfill the band requirements outlined previously and typically have dopants and/or co-catalysts added to optimize their performance. A sample semiconductor with the proper band structure is titanium dioxide (). However, due to the relatively positive conduction band of , there is little driving force for production, so is typically used with a co-catalyst such as platinum (Pt) to increase the rate of production. It is routine to add co-catalysts to spur evolution in most photocatalysts due to the conduction band placement. Most semiconductors with suitable band structures to split water absorb mostly UV light; in order to absorb visible light, it is necessary to narrow the band gap. Since the conduction band is fairly close to the reference potential for formation, it is preferable to alter the valence band to move it closer to the potential for\n\nPhotocatalysts can suffer from catalyst decay and recombination under operating conditions. Catalyst decay becomes a problem when using a sulfide-based photocatalyst such as cadmium sulfide (CdS), as the sulfide in the catalyst is oxidized to elemental sulfur at the same potentials used to split water. Thus, sulfide-based photocatalysts are not viable without sacrificial reagents such as sodium sulfide to replenish any sulfur lost, which effectively changes the main reaction to one of hydrogen evolution as opposed to water splitting. Recombination of the electron-hole pairs needed for photocatalysis can occur with any catalyst and is dependent on the defects and surface area of the catalyst; thus, a high degree of crystallinity is required to avoid recombination at the defects.\n\nThe conversion of solar energy to hydrogen by means of photocatalysis is one of the most interesting ways to achieve clean and renewable energy systems. However, if this process is assisted by photocatalysts suspended directly in water instead of using a photovoltaic and electrolytic system the reaction is in just one step, and can therefore be more efficient.\n\nPhotocatalysts must confirm to several key principles in order to be considered effective at water splitting. A key principle is that and evolution should occur in a stoichiometric 2:1 ratio; significant deviation could be due to a flaw in the experimental setup and/or a side reaction, both of which do not indicate a reliable photocatalyst for water splitting. The prime measure of photocatalyst effectiveness is quantum yield (QY), which is:\n\nThis quantity is a reliable determination of how effective a photocatalyst is; however, it can be misleading due to varying experimental conditions. To assist in comparison, the rate of gas evolution can also be used; this method is more problematic on its own because it is not normalized, but it can be useful for a rough comparison and is consistently reported in the literature. Overall, the best photocatalyst has a high quantum yield and gives a high rate of gas evolution.\n\nThe other important factor for a photocatalyst is the range of light absorbed; though UV-based photocatalysts will perform better per photon than visible light-based photocatalysts due to the higher photon energy, far more visible light reaches the Earth's surface than UV light. Thus, a less efficient photocatalyst that absorbs visible light may ultimately be more useful than a more efficient photocatalyst absorbing solely light with smaller wavelengths.\n\nThe utility of a material for photocatalytic water splitting will typically be investigated for one of the two redox reactions at a time. To do this, a three component system is employed: a catalyst, a photosensitizer and a sacrificial electron acceptor such as persulfate when investigating water oxidation, and a sacrificial electron donor (for example triethylamine) when studying proton reduction. Employing sacrificial reagents in this manner simplifies research and prevents detrimental charge recombination reactions.\n\nSolid solutions with different Zn concentration (0.2 < \"x\" < 0.35) has been investigated in the production of hydrogen from aqueous solutions containing formula_2 as sacrificial reagents under visible light. Textural, structural and surface catalyst properties were determined by adsorption isotherms, UV–vis spectroscopy, SEM and XRD and related to the activity results in hydrogen production from water splitting under visible light irradiation. It was found that the crystallinity and energy band structure of the solid solutions depend on their Zn atomic concentration. The hydrogen production rate was found to increase gradually when the Zn concentration on photocatalysts increases from 0.2 to 0.3. Subsequent increase in the Zn fraction up to 0.35 leads to lower hydrogen production. Variation in photoactivity is analyzed in terms of changes in crystallinity, level of conduction band and light absorption ability of solid solutions derived from their Zn atomic concentration.\n\n, another catalyst activated by solely UV light and above, does not have the performance or quantum yield of :La. However, it does have the ability to split water without the assistance of cocatalysts and gives a quantum yield of 6.5% along with a water splitting rate of 1.21 mmol/h. This ability is due to the pillared structure of the photocatalyst, which involves pillars connected by triangle units. Loading with NiO did not assist the photocatalyst due to the highly active evolution sites.\n()() has the highest quantum yield in visible light for visible light-based photocatalysts that do not utilize sacrificial reagents as of October 2008. The photocatalyst gives a quantum yield of 5.9% along with a water splitting rate of 0.4 mmol/h. Tuning the catalyst was done by increasing calcination temperatures for the final step in synthesizing the catalyst. Temperatures up to 600 °C helped to reduce the number of defects, though temperatures above 700 °C destroyed the local structure around zinc atoms and was thus undesirable. The treatment ultimately reduced the amount of surface Zn and O defects, which normally function as recombination sites, thus limiting photocatalytic activity. The catalyst was then loaded with at a rate of 2.5 wt % Rh and 2 wt% Cr to yield the best performance.\n\nPhotocatalysts based on cobalt have been reported. Members are tris(bipyridine) cobalt(II), compounds of cobalt ligated to certain cyclic polyamines, and certain cobaloximes.\n\nIn 2014 researchers announced an approach that connected a chromophore to part of a larger organic ring that surrounded a cobalt atom. The process is less efficient than using a platinum catalyst, cobalt is less expensive, potentially reducing total costs. The process uses one of two supramolecular assemblies based on Co(II)-templated coordination of (bpy = 2,2′-bipyridyl) analogues as photosensitizers and electron donors to a cobaloxime macrocycle. The Co(II) centres of both assemblies are high spin, in contrast to most previously described cobaloximes. Transient absorption optical spectroscopies include that charge recombination occurs through multiple ligand states present within the photosensitizer modules.\n\nBismuth vanadate based systems have been demonstrated to have a record solar-to-hydrogen conversion efficiency of 5.2% (highest for metal-oxide photo-electrode) with the advantage of a very simple and cheap catalyst.\n\nTungsten diselenide may have a role in future hydrogen fuel production, as a recent discovery in 2015 by scientists in Switzerland revealed that the compound's own photocatalytic properties might be a key to significantly more efficient electrolysis of water to produce hydrogen fuel.\n\nSystems based on the material class of III-V semiconductors, such as InGaP, enable currently the highest solar-to-hydrogen efficiencies of up to 14%. Long-term stability of these high-cost high-efficiency systems does, however, remain an issue.\n"}
{"id": "5880890", "url": "https://en.wikipedia.org/wiki?curid=5880890", "title": "Plasma parameter", "text": "Plasma parameter\n\nThe plasma parameter is a dimensionless number, denoted by capital Lambda, Λ. The plasma parameter is usually interpreted to be the argument of the Coulomb logarithm, which is the ratio of the maximum impact parameter to the classical distance of closest approach in Coulomb scattering. In this case, the plasma parameter is given by:\n\nwhere\n\nThis expression is typically valid for a plasma in which ion thermal velocities are much less than electron thermal velocities. A detailed discussion of the Coulomb logarithm is available in the \"NRL Plasma Formulary\", pages 34–35.\n\nNote that the word parameter is usually used in plasma physics to refer to bulk plasma properties in general: see plasma parameters. \n\nAn alternative definition of this parameter is given by the average number of electrons in a plasma contained within a Debye sphere (a sphere of radius the Debye length).\nThis definition of the plasma parameter is more frequently (and appropriately) called the Debye number, and is denoted formula_2. In this context, the plasma parameter is defined as\n\nSince these two definitions differ only by a factor of three, they are frequently used interchangeably.\n\nOften the factor of formula_4 is dropped. When the Debye length is given by formula_5, the plasma parameter is given by\n\nwhere\n\nConfusingly, some authors define the plasma parameter as :\n\nA closely related parameter is the plasma coupling formula_8, defined as a ratio of the Coulomb energy to the thermal one:\n\nThe Coulomb energy (per particle) is\n\nwhere for the typical inter-particle distance formula_11 usually is taken the Wigner-Seitz radius. Therefore,\n\nClearly, up to a numeric factor of the order of unity, \n\nIn general, for multicomponent plasmas one defines the coupling parameter for each species \"s\" separately:\n\nHere, \"s\" stands for either electrons or (a type of) ions.\n\nOne of the criteria which determine whether a collection of charged particles can rigorously be termed an ideal plasma is that Λ»1.\nWhen this is the case, collective electrostatic interactions dominate over binary collisions, and the plasma particles can be treated as if they only interact with a smooth background field, rather than through pairwise interactions (collisions). The equation of state of each species in an ideal plasma is that of an ideal gas.\n\nDepending on the magnitude of Λ, plasma properties can be characterized as following:\n"}
{"id": "21056112", "url": "https://en.wikipedia.org/wiki?curid=21056112", "title": "Powder mixture", "text": "Powder mixture\n\nA powder is an assembly of dry particles dispersed in air. If two different powders are mixed perfectly, three theoretical powder mixtures can be obtained: the random mixture, the ordered mixture or the interactive mixture.\n\nA powder can be free-flowing if the particles do not stick together or cohesive if the particles cling to one another to form aggregates. The likelihood of cohesion increases with decreasing size of the powder particles; particles smaller than 100 µm are generally cohesive.\n\nA random mixture can be obtained if two different free-flowing powders of approximately the same particle size, density and shape are mixed (see figure A). Only primary particles are present in this type of mixture, i.e., the particles are not cohesive and do not cling to one another. The mixing time will determine the quality of the random mixture. However, if powders with particles of different size, density or shape are mixed the segregation can occur. Segregation will cause separation of the powders as, for example, lighter particles will be prone to travel to the top of the mixture whereas heavier particles are kept at the bottom.\n\nThe term \"ordered mixture\" was first introduced to describe a completely homogeneous mixture where the two components adhere to each other to form ordered units. However, a completely homogeneous mixture is only achievable in theory and other denotations were introduced later such as adhesive mixture or interactive mixture.\n\nIf a free-flowing powder is mixed with a cohesive powder an interactive mixture can be obtained. The cohesive particles adhere to the free-flowing particles (now called carrier particles) to form interactive units as shown in figure B. An interactive mixture may not contain free aggregates of the cohesive powder, which means that all small particles must be adhered to the larger ones. The difference from an ordered mixture is instead that all carrier particles do not need to be of the same size and a different number of small particles attached to each one. A narrow size range of the carrier particles is preferred to avoid segregation of the interactive units. In practice a combination of a random mixture and an interactive mixture may be obtained which consists of carrier particles, aggregates of the small particles and interactive units.\n\nThe formation of interactive mixtures cannot automatically be assumed, especially if smaller carrier particles or a greater proportion of fine particles are used. If an interactive mixture is to be formed, it is necessary that enough force is exerted by the carrier particles during dry mixing to break up the aggregates formed by the fine particles. Adhesion can then be achieved if the adhesive forces exceed the gravitational forces that otherwise lead to separation of the constituents.\nInteractive mixtures for example can be used in the manufacturing of tablets enhancing the dissolution of poorly soluble drugs or for nasal administration. One common application is for inhalation therapy, where the concept has been used in the development of alternatives to pressurised metered dose inhalers.\n\nThe schematic formation of a random mixture (A) and an interactive mixture (B) by dry mixing two powder materials.\n"}
{"id": "1253870", "url": "https://en.wikipedia.org/wiki?curid=1253870", "title": "Quasiturbine", "text": "Quasiturbine\n\nThe Quasiturbine or Qurbine engine is a proposed pistonless rotary engine using a rhomboidal rotor whose sides are hinged at the vertices. The volume enclosed between the sides of the rotor and the rotor casing provide compression and expansion in a fashion similar to the more familiar Wankel engine, but the hinging at the edges allows the volume ratio to increase. Unlike vane pumps, in which vane extension is generally important and against which the pressure acts to generate the rotation, the Quasiturbine contour seals have a minimal extension and the rotation does not result from pressure against these seals.\n\nPatents for the Quasiturbine (in the most general AC concept with carriages) are held by the family of Gilles Saint-Hilaire of Québec. As well as an internal combustion engine, the Quasiturbine has been proposed as a possible pump design, and a possible Stirling engine. It has been demonstrated as a pneumatic engine using stored compressed air, and as a steam engine.\n\nThere are at least four proposed designs:\n\nThe earliest Quasiturbine design used a three-wheeled \"carriage\" (French \"chariot\", hence \"avec chariots\" or \"AC\" for \"with carriages\") to support each vertex of the rotor. The wheels of these four carriages, making twelve wheels in total, ran around the periphery of the engine chamber.\n\nA prototype of an internal combustion engine to this design was constructed, and enthusiastically reviewed in European Automotive Design magazine September, 1999. The prototype was turned by an external engine for 40 hours.\n\nHowever, ignition with fuel was never achieved. If it was attempted no results were ever released, and development work on this design was suspended.\n\nThe two-port design with carriages was proposed to make possible a new and superior mode of combustion, termed photo-detonation by the Quasiturbine inventors. This resembles detonation, as used in the Bourke engine, akin to knocking and pinging undesirable in common internal combustion engines. , no research has been published supporting this claim. A related idea that flame transfer would be possible through special ports is similarly unsupported.\n\nThe second Quasiturbine design is greatly simplified to eliminate the carriages (French \"sans chariots\" or \"SC\"). At the same time, the ports were duplicated on the opposite side of the housing, thus converting the operation from four strokes per cycle to two and doubling the number of cycles per rotor revolution. This mechanism has been demonstrated running as a pneumatic engine using stored compressed air, and also as a steam engine. This is also the design proposed for use as a pump, and particularly as a supercharger.\n\nThis design uses redesigned blades, longer than those for a similar sized housing of the first type owing to the absence of the carriages, and lacking the distinctive crown contour. Only the basic rotor geometry is common with the earlier design.\n\nA pneumatic engine of this design was demonstrated powering a go-kart in November 2004, and another powering a small car in September 2005, both vehicles using stored compressed air to power the engine. a pneumatic chain saw driven by an air hose from a conventional external compressor is under development.\n\nWith a suitably redesigned housing to allow for thermal expansion, the same rotor design has been demonstrated as a steam engine.\n\nAnother potential variation of this design uses the two sets of ports independently, one as an engine and the other as a pump, thus potentially integrating the functions of a pump and its driving motor in one shaftless unit. One restriction of this usage is that the two fluids must be similar; It would not be possible for example to drive an integrated air pump with hydraulic fluid, as the rotor design is significantly different. no prototype of this variation has been demonstrated.\n\nThis third design combines aspects of the first two. this design is conceptual only. It has not been built, but is used for purposes of illustration. If built it would not support photo-detonation.\n\nMany other designs are possible within the patented Quasiturbine model, with or without carriages and with differing numbers of ports. , which design will be used for further work on the internal combustion version has not been announced.\n\nProf. J. Ignacio Martínez-Artajo (1907-1984) from Universidad Pontificia Comillas (Madrid, Spain) made sketches of an adaptative rotary compressor in the mid seventies, which led to the construction of a railway model missing of proper rotor dynamic solutions. Lately, new researches were carried out by four researchers led by Dr. Gilles Saint-Hilaire, a thermonuclear physicist, consisting of members of his immediate family. The original objective was to make a turbo-shaft turbine engine where the compressor portion and the power portion would be in the same plane. In order to achieve this, they had to disconnect the blades from the main shaft, and chain them around in such a way that a single rotor acts as a compressor for a quarter turn, and as an engine the following quarter of a turn.\n\nThe general concept of the Quasiturbine was first patented in 1996. Small pneumatic and steam units are available from the patent holders for sale or hire for research, academic training and industrial demonstration, as is a book (largely in French) describing the concepts and development of the design. Demonstrations have been undertaken on an Air Gokart in 2004, on “APUQ Air Car” in 2005, on the University of Connecticut “Brash Steam Car” in 2010, and other products (Chainsaw and generator).\n\nThe patent holders have announced that they intend to make similar internal combustion prototypes available for demonstration.\n\n\n"}
{"id": "55861901", "url": "https://en.wikipedia.org/wiki?curid=55861901", "title": "Ring main unit", "text": "Ring main unit\n\nIn an electrical power distribution system, a ring main unit (RMU) is a factory assembled, metal enclosed set of switchgear used at the load connection points of a ring-type distribution network. It includes in one unit two switches that can connect the load to either or both main conductors, and a fusible switch or circuit breaker and switch that feed a distribution transformer. The metal enclosed unit connects to the transformer either through a bus throat of standardized dimensions, or else through cables and is usually installed outdoors. Ring main cables enter and leave the cabinet. This type of switchgear is used for medium-voltage power distribution, from 7200 volts to about 36000 volts. \n\nThe ring main unit was introduced in the United Kingdom and is now widely used in other countries. In North American distribution practice, often the equivalent of a ring main unit is built into a pad-mounted transformer which integrates switches and transformer into a single cabinet.\n\nRing main units can be characterized by their type of insulation, air, oil or gas. The switch used to isolate the transformer can be a fusible switch, or may be a circuit breaker using vacuum or gas-insulated interrupters. The unit may also include protective relays to operate the circuit breaker on a fault. \n\n"}
{"id": "5548079", "url": "https://en.wikipedia.org/wiki?curid=5548079", "title": "SWACO", "text": "SWACO\n\nSWACO is the Solid Waste Authority of Central Ohio. Established by the Ohio General Assembly in 1989, SWACO operates waste management facilities, landfills and coordinates recycling. In 2006, SWACO operated three transfer stations around Franklin County, Ohio.\n"}
{"id": "14656726", "url": "https://en.wikipedia.org/wiki?curid=14656726", "title": "Sacred Planet", "text": "Sacred Planet\n\nSacred Planet is a 2004 documentary film directed by Jon Long and Hairul Salleh Askor. Robert Redford provided narration for the film. The film was released by Walt Disney Pictures on April 22, 2004, and grossed $1,108,356.\n\n\n"}
{"id": "2084871", "url": "https://en.wikipedia.org/wiki?curid=2084871", "title": "Scarf joint", "text": "Scarf joint\n\nA scarf joint (also known as a scarph joint) is a method of joining two members end to end in woodworking or metalworking. The scarf joint is used when the material being joined is not available in the length required. It is an alternative to other joints such as the butt joint and the splice joint and is often favored over these in joinery because it yields a barely visible glue line.\n\nIn woodworking, there are two distinctly different categories of scarf, based on whether the joint has interlocking faces or not. A plain scarf is simply two flat planes meeting on an angle relative to the axis of the stock being joined, and depends entirely on adhesive and/or mechanical fastening (screws, bolts, etc.) for all strength. Hooked, keyed, and nibbed scarfs are some of the many example of interlocking scarfs, offering varying degrees of tensile and compressive strength, though most still depend on mechanical fastening to keep the joint closed.\n\nThe plain scarf is not preferred when strength is required, so it is often used in decorative situations, such as the application of trim or moulding. The use of modern high-strength adhesives can greatly increase the structural performance of a plain scarf.\n\nThe keyed hook scarf is common in ship and boat-building, as well as timber framing and wooden bridge construction. In large timbers such as these the scarf is virtually always secured with through bolts, and is frequently reinforced externally with iron or steel fishplates, and/or strapping.\n\nA scarf joint may also be used to fix problems caused when a board is cut too short for the application. The board can be cut in half with a tapered cut yielding a scarf joint. When the joint is glued together, the tapers are slid against each other so that the two sections are no longer in line with each other. This has the effect of making the board longer. Once the glue has set, the board can be planed down to an even thickness, resulting in a longer but thinner board.\n\nIn traditional timber framing there are many types of scarf joints used to join timbers.\n\nThe joint is formed by cutting opposing tapered ends on each member which are then fitted together. When working with wood, this gives better long grain to long grain gluing surface, which yields a stronger joint than would be achieved with a simple butt joint. The tapers are generally cut at an angle between 1:8 to 1:10. The ends of a plain scarf are \"feathered\" to a fine point which aids in the obscuring of the joint in the finished work, while in other forms of scarf the ends are frequently cut to a blunt \"nib\" which engages a matching shoulder in the mating piece.\n\nWhere scarfed joints are used in the restoration of vintage aircraft most developed countries will only issue an airworthiness certificate if all such joints have used an angle no less than 1:8.\n\nDetermination of the maximum axial force for two pieces joined by adhesive can easily be determined using two equations that can be derived from the geometry of the problem by breaking the axial force component into a tensile force and shear force normal and parallel to the scarf joint. Shear strength is assumed to be equal to σ/2. The following equations need to be adjusted if the shear strength is greater than σ/2. The two equations that give a maximum axial force are F=σ/sin(α)^2 and F=σ/sin(2α), where α is the angle from the horizontal to the joint. Both should be evaluated for a given problem, and the smaller F of the two is the magnitude of the maximum allowable axial force. The first equation accounts for failure in tension. The second equation accounts for failure in shear. Some special angles should be noted or the graphs of two equations should be compared on the same plot. The joint is weakest at α=90° due to tension limits and 45° due to shear limits. However, α=45° will be stronger than α=90° if shear strength is greater than σ/2. The joint is strongest between these two angles at 63.4°. The joint becomes stronger than 63.4° at 25.4°. At a shallow enough angle, strength of the joint continues to increase and failure will occur anywhere in the two pieces, possibly outside the joint.\n\n"}
{"id": "57793836", "url": "https://en.wikipedia.org/wiki?curid=57793836", "title": "Solid-state transformer", "text": "Solid-state transformer\n\nA solid-state transformer (SST), power electronic transformer (PET), or electronic power transformer is actually an AC-to-AC converter, a type of electric power converter that replaces a conventional transformer used in AC electric power distribution. It is more complex than a conventional transformer operating at utility frequency, but it can be smaller and more efficient than a conventional transformer because it operates at high frequency. The main types are \"true\" AC-to-AC converter (with no DC stages) and AC-to-DC-to-DC-to-AC converter (in which an active rectifier supplies power to a DC-to-DC converter, which supplies power to a power inverter). A solid-state transformer usually contains a transformer, inside the AC-to-AC converter or DC-to-DC converter, which provides electrical isolation and carries the full power; this transformer is much smaller because it operates at high frequency. A solid-state transformer can actively regulate voltage and current. Some can convert single-phase power to three-phase power and vice-versa. Variations can input or output DC power to reduce the number of conversions, for greater end-to-end efficiency. As a complex electronic circuit, it must be designed to withstand lightning and other surges. Solid-state transformer is an emerging technology.\n\n"}
{"id": "522723", "url": "https://en.wikipedia.org/wiki?curid=522723", "title": "Solid fuel", "text": "Solid fuel\n\nSolid fuel refers to various forms of solid material that can be burnt to release energy, providing heat and light through the process of combustion. Solid fuels can be contrasted with liquid fuels and gaseous fuels. Common examples of solid fuels include wood, charcoal, peat, coal, Hexamine fuel tablets, wood pellets, corn, wheat, rye, and other grains. Solid fuels are extensively used in rocketry as solid propellants. Solid fuels have been used throughout human history to create fire and solid fuel is still in widespread use throughout the world in the present day.\n\nWood fuel can refer to several fuels such as firewood, charcoal, \"wood chips\" sheets, pellets, and sawdust. The particular form used depends upon factors such as source, quantity, quality and application. In many areas, wood is the most easily available form of fuel, requiring no tools in the case of picking up dead wood, or few tools. Today, burning of wood is the largest use of energy derived from a solid fuel biomass. Wood fuel can be used for cooking and heating, and occasionally for fueling steam engines and steam turbines that generate electricity. Wood may be used indoors in a furnace, stove, or fireplace, or outdoors in a furnace, campfire, or bonfire. As with any fire, burning wood fuel creates numerous by-products, some of which may be useful (heat and steam), and others that are undesirable, irritating or dangerous. There is debate as to whether burning wood can be considered carbon neutral, as technically the wood cannot release more carbon than was sequestered during its growth, although this does not take account of other impacts such as deforestation and rotting has on the carbon footprint. When harvested in a sustainable fashion wood is usually considered to be a renewable solid fuel.\n\nAlthough wood is a form of biomass, the term usually refers to other natural plant material that can be burnt for fuel. Common biomass fuels include waste wheat, straw, nut shells and other fibrous material.\n\nPeat fuel is an accumulation of partially decayed vegetation or organic matter that can be burnt once sufficiently dried.\n\nCoal is a combustible black or brownish-black sedimentary rock usually occurring in rock strata in layers or veins called coal beds or coal seams. Throughout history, coal has been used as an energy resource, primarily burned for the production of electricity and heat, and is also used for industrial purposes, such as refining metals. Coal is the largest source of energy for the generation of electricity worldwide, as well as one of the largest worldwide anthropogenic sources of carbon dioxide releases. The extraction of coal, its use in energy production and its byproducts are all associated with environmental and health effects including climate change. Variations such as smokeless coal can be formed naturally in the form of anthracite, a metamorphosed type of coal with a very high carbon content that gives off a smokeless flame when set alight.\n\nCoke is a fuel with few impurities and a high carbon content, usually made from coal. It is the solid carbonaceous material derived from destructive distillation of low-ash, low-sulfur bituminous coal. Cokes made from coal are grey, hard, and porous. While coke can be formed naturally, the commonly used form is man-made. The form known as petroleum coke, or pet coke, is derived from oil refinery coker units or other cracking processes.\n\nMunicipal solid waste commonly known as trash or garbage in the United States and as rubbish in Britain, is a waste type consisting of everyday items that are discarded by the public. With the correct technology it can be gasified and converted to a viable fuel source. However, this is technology heavy and can only be used where the waste is known not to contain toxic materials.\n\nSolid fuels, compared to liquid fuels or gaseous fuels, are often cheaper, easier to extract, more stable to transport and in many places are more readily available. Coal, in particular, is utilized in the generation of 38.1% of the world’s electricity because it is less expensive and more powerful than its liquid and gas counterparts. However, solid fuels are also heavier to transport, require more destructive methods to extract/burn and often have higher carbon, nitrate and sulphate emissions. With the exception of sustainable wood/biomass solid fuel is normally considered non-renewable as it requires thousands of years to form.\n\n\n"}
{"id": "26434013", "url": "https://en.wikipedia.org/wiki?curid=26434013", "title": "Superstripes", "text": "Superstripes\n\nSuperstripes is a generic name for a phase with spatial broken symmetry that favors the onset of superconducting or superfluid \nquantum order. This scenario emerged in the 1990s when no-homogeneous metallic heterostructures at the atomic limit with a broken spatial symmetry have been found to favor superconductivity. Before a broken spatial symmetry was expected to compete and suppress the superconducting order. The driving mechanism for the amplification of the superconductivity critical temperature in superstripes matter has been proposed to be the shape resonance in the energy gap parameters ∆n that is a type of Fano resonance for coexisting condensates.\n\nThe superstripes show multigap superconductivity near a 2.5 Lifshitz transition where the renormalization of chemical potential at the metal-to-superconductor transition is not negligeable and the self-consistent solution of the gaps equation is required. The superstripes lattice scenario is made of puddles of multigap superstripes matter forming a superconducting network where different gaps are not only different in different portions of the k-space but also in different portions of the real space with a complex scale free distribution of Josephson junctions.\n\nThe term \"superstripes\" was introduced in 2000 at the international conference on \"Stripes and High T Superconductivity\" held in Rome to describe the particular phase of matter where a broken symmetry appearing at a transition from a phase with higher dimensionality N (3D or 2D) to a phase with lower dimensionality N-1 (2D or 1D) favors the superconducting or superfluid phase and it could increase the normal to superconducting transition temperature with the possible emergence of high-temperature superconductivity. The term \"superstripes scenario\" was introduced to make the key difference with the stripes scenario where the phase transition from a phase with higher dimensionality N (like a 2D electron gas) to the phase with broken symmetry and lower dimensionality (like a quasi 1D striped fluid) competes and suppresses the transition temperature to the superfluid phase and favors modulated striped magnetic ordering. In the broken symmetry of superstripes phase the structural modulation coexists and favors high-temperature superconductivity.\n\nThe prediction of high-temperature superconductivity transition temperatures is rightly considered to be one of the most difficult problems in theoretical physics. The problem remained elusive for many years since these materials have generally a very complex structure making unuseful theoretical modelling for a homogeneous system. The advances in experimental investigation on local lattice fluctuations have driven the community to the conclusion that it is a problem of quantum physics in complex matter. A growing paradigm for high-temperature superconductivity in superstripes is that a key term is the quantum interference effect between pairing channels, i.e., a resonance in the exchange-like, Josephson-like pair transfer term between different condensates. The quantum configuration interaction between different pairing channels is a particular case of shape resonance belonging to the group of Fano Feshbach resonances in atomic and nuclear physics. The critical temperature shows a suppression, due to a Fano antiresonance, when the chemical potential is tuned at a band edge where a new Fermi surface spot appears i.e., an \"electronic topological transition\" (ETT) or 2.5 Lifshitz transition or, a metal-to-metal topological transition. The T amplification is switched on when the chemical potential is tuned above the band edge in an energy region away from the band edge of the order of 1 or 2 times the energy cut off of the pairing interaction. The T is further amplified at the shape resonance if in this range the Fermi surface of the appearing fermi surface spot changes its dimensionality (for example the Lifshitz transition for opening a neck in a tubular Fermi surface).\nThe tuning of the chemical potential at the shape resonance can be obtained by changing: the charge density and/or the superlattice structural parameters, and/or the superlattice misfit strain and/or the disorder. Direct evidence for shape resonances in superstripes matter is provided by the anomalous variation of the isotope effect on the critical temperature by tuning the chemical potential.\n\nIt was known that the high-temperature cuprate superconductors have a complex lattice structure. In 1993 it was proposed that these materials belong to a particular class of materials called heterostructures at atomic limit made of a superlattice of superconducting atomic layers intercalated by a different material with the role of spacer.\n\nAll new high-temperature superconducting materials discovered in the years 2001–2013 are heterostructures at atomic limit made of the active atomic layers: honeycomb boron layer in diborides, graphene in intercalated graphite, CoO atomic bbc monolayers in cobaltates, FeAs atomic fluorite monolayers in pnictides, FeSe atomic fluorite monolayers in selenides.\n\nIn these materials the joint effect of (a) increasing the lattice misfit strain to a critical value, and (b) tuning the chemical potential near a Lifshitz transition in presence of electron-electron interactions induce a lattice instability with formation of the network of superconducting striped puddles in an insulating or metallic background.\n\nThis complex scenario has been called \"superstripes scenario\" where the 2D atomic layers show functional lattice inhomogeneities: \"ripples puddles\" of local lattice distortion have been observed in LaCuO in Bi222; striped puddles of ordered dopants in the spacer layers have been seen in superoxygenated LaCuO and in YBaCuO The network of superconducting striped puddles has been found also in MFeAs pnictides and recently in KFeSe selenides\nSelf-organization of lattice defects can be controlled by strain engineering. and photoinduced effects.\n\nSuperstripes (Also called Stripe Phase) can also form in Bose Einstein Condensates (BEC) with Spin orbit coupling. The spin orbit coupling is achieved by selecting 2 spin states from the manifold of hyperfine states to couple with a two photon process. For weak coupling, the resulting Hamiltonian has a spectrum with a double degenerate ground state in the first band. In this regime, the single particle dispersion relation can host a BEC in each minima. The result is that the BEC has 2 momentum components which can interfere in real space. The interference pattern will appear as fringes in the density of the BEC. The periodicity of the fringes is a result of the Raman coupling beam wavelength modified by the coupling strength and by interactions within the BEC. Spin orbit coupling breaks the gauge symmetry of the system and the time reversal symmetry. The formation of the stripes breaks a continuous translational symmetry.\n\nRecent efforts have attempted to observe the stripe phase in a Rubidium-87 BEC, however the stripes were too small and too low contrast to be detected.\n\nIn 2017, two research groups from ETH Zurich and from MIT reported on the first creation of a supersolid with ultracold quantum gases. The MIT group exposed a Bose-Einstein condensate in a double-well potential to light beams that created an effective spin-orbit coupling. The interference between the atoms on the two spin-orbit coupled lattice sites gave rise to a density modulation that establishes a stripe phase with supersolid properties.\n\n"}
{"id": "39951483", "url": "https://en.wikipedia.org/wiki?curid=39951483", "title": "Tetrafluoroberyllate", "text": "Tetrafluoroberyllate\n\nTetrafluoroberyllate or orthofluoroberyllate is an anion containing beryllium and fluorine. The ion has a tetrahedral shape, the same size and outer electron structure as sulfate. Therefore, many compounds that contain sulfate, have equivalents with tetrafluoroberyllate. Examples of these are the Langbeinites, and Tutton's salts.\n\nThe Be–F distance is between 1.45 and 1.53 Å. This bond is sp and has a longer length than the sp bond in BeF gas. In trifluoroberyllates, there are actually BeF tetrahedra arranged in a triangle, so that three fluorine atoms are shared on two tetrahedra each, resulting in a formula of BeF.\n\nIn the tetrafluoroberyllates the tetrahedra can rotate to various degrees. At room temperatures they are hindered from moving. But as temperature increases they can rotate around the three-fold access, with a potential barrier of 12.5 kcal/mol. At higher temperatures the movement can become isotropic with a potential barrier of 14.5 kcal/mol.\n\nSimilar formula compounds have magnesium or zinc in a similar position. e.g. KMgF or (NH)ZnF but these are not as stable.\n\nTetrafluoroberyllate has a biological effect by inhibiting F-ATPase ATP producing enzymes in mitochondria and bacteria. It does this by attempting to react with adenosine diphosphate because it resembles phosphate. However once it does this it remains stuck in the F1 part of the enzyme and inhibits it from further function.\n\nSodium tetrafluoroberyllate has several crystalline forms. Below 220 °C it takes the same form as orthorhombic olivine, and this is called γ phase. Between 220 and 320 it is in the α' form. When temperature is raised above 320 it changes to the hexagonal α form. When cooled the α' form changes to β form at 110° and this can be cooled to 70° before changing back to the γ form. It can be formed by melting sodium fluoride and beryllium fluoride. The gas above molten sodium tetrafluoroberyllate contains BeF and NaF gas.\n\nLithium tetrafluoroberyllate takes on the same crystal form as the mineral phenacite. As a liquid it is proposed for the molten salt reactor, in which it is called FLiBe. The liquid salt has a high specific heat, similar to that of water. The molten salt has a very similar density to the solid. The solid has continuous void channels through it, which reduces its density. LiBeF can be crystallised from aqueous solution using (NH)BeF and LiCl.\n\nPotassium tetrafluoroberyllate has the same structure as anhydrous potassium sulfate, as does rubidium and caesium tetrafluoroberyllate. Potassium tetrafluoroberyllate can make solid solutions with potassium sulfate. It can be used as a starting point to make the non-linear optic crystal KBeBOF which has the highest power handling capacity and shortest UV performance of any borate. It is quite soluble in water, so beryllium can be extracted from soil this in this form.\n\nAmmonium tetrafluoroberyllate decomposes on heating by losing NHF vapour, progressively forming NHBeF, then NHBeF and finally BeF.\n\nThallium tetrafluoroberyllate can be made by dissolving beryllium fluoride and thallium carbonate together in hydrofluoric acid and then evaporating the solution.\n\nRadium tetrafluoroberyllate is used as a standard neutron source. The alpha particles from the radium cause neutrons to be emitted from the beryllium. It is precipitated from a radium chloride solution mixed with potassium tetrafluoroberyllate.\n\nMagnesium tetrafluoroberyllate can be precipitated from a hot saturated solution of ammonium tetrafluoroberyllate and a magnesium salt. However, if the temperature reaches boiling point MgF is precipitated instead.\n\nCalcium tetrafluoroberyllate resembles zircon in the way it melts and crystallises.\n\nStrontium tetrafluoroberyllate can be made in several forms. The Ύ is produced by cooling a melt of SrF and Be and the β from is made by precipitating from a water solution. When melted and heated to 850-1145° Be gas evaporates leaving behind molten SrF.\n\nThe barium tetrafluoroberyllate is very insoluble and can be used for gravimetric analysis of beryllium.\n\nHBeF is an acid that can be produced from AgBeF and HCl. It only exists dissolved in water.\n\nTriglycine tetrafluoroberyllate (TGFB) is ferroelectric with a transition point of 70 °C. The crystals can be formed by dissolving BeF in water, adding HF and then glycine. When the solution is cooled triglycine tetrafluoroberyllate forms. CsBeF and TlBeF in the solution reduce growth on the 001 direction so that tabular shaped crystals of TGFB form. The thallium compound can cut growth on the 001 axis by 99%.\n\nThe Tuttons salt (NH)Mn(BeF).6(HO) is made from a solution of NHBeF mixed with NHMnF.\nThe equivalent of alums are hard to make because the trivalent ion will often form a complex with fluoride in preference to the beryllium fluoride. However the violet coloured acid and rubidium chrome alum exist at chilly temperatures for a few hours.\n\nTutton's salts containing magnesium with fluoroberyllate are difficult to produce, as the solutions tend to precipitate insoluble MgF.\n"}
{"id": "76304", "url": "https://en.wikipedia.org/wiki?curid=76304", "title": "The River (1938 film)", "text": "The River (1938 film)\n\nThe River is a 1938 short documentary film which shows the importance of the Mississippi River to the United States, and how farming and timber practices had caused topsoil to be swept down the river and into the Gulf of Mexico, leading to catastrophic floods and impoverishing farmers. It ends by briefly describing how the Tennessee Valley Authority project was beginning to reverse these problems.\n\nIt was written and directed by Pare Lorentz and, like Lorentz's earlier documentary \"The Plow That Broke the Plains\", was also selected for preservation in the United States National Film Registry by the Library of Congress as being \"culturally, historically, or aesthetically significant\", going into the registry in 1990. The film won the \"best documentary\" category at the 1938 Venice International Film Festival.\n\nBoth films have notable scores by Virgil Thomson that are still heard as concert suites, featuring an adaptation of the hymn \"How Firm a Foundation\". The film was narrated by the American baritone Thomas Hardie Chalmers. Thomson's score was heavily adapted from his own concert work \"Symphony on a Hymn Tune\". \"The River\" later served as the score for the 1983 TV movie \"The Day After\".\n\nThe two films were sponsored by the U.S. government and specifically the Resettlement Administration (RA) to raise awareness about the New Deal. The RA was folded into the Farm Security Administration in 1937, so \"The River\" was officially an FSA production.\n\nThere is also a companion book, \"The River\". The text was nominated for the Pulitzer Prize in poetry in that year.\n\n\n"}
{"id": "41717864", "url": "https://en.wikipedia.org/wiki?curid=41717864", "title": "Thomas H. Stoner Jr.", "text": "Thomas H. Stoner Jr.\n\nThomas H. Stoner Jr. is an energy entrepreneur and writer. He has been a promoter of sustainable development for over 30 years, having built, financed, and owned and operated renewable energy projects throughout the Americas. He has led three companies in the clean technology space, including one of the original CleanTech venture funds backed by the international development banks, including the Multilateral Investment Fund (MIF), a division of the Inter-American Development Bank. In 2011, with the aid of David Schimel of the Jet Propulsion Lab (NASA) and other leading climate figures, Stoner founded Project Butterfly, a research organization that primarily advocates for the global capital markets as being a solution to climate change. In 2013, Stoner released “Small Change, Big Gains: Reflections of an Energy Entrepreneur,” which contains research about transforming the global energy supply to be more reliant upon sustainable fuel sources by the end of the century.\n\nStoner is currently the CEO of Entelligent, a provider of Smart Climate indices, predictive equity portfolio analytics, and advanced data.\n\nStoner received his Masters in Accounting & Finance from the London School of Economics and a BA from Hampshire College, Amherst, Massachusetts.\nIn 1988 he married Laurie Larsen; together they have two children.\n\nIn 1987, Stoner served as founding Director of the Social Venture Network\n\nMost recently, Thomas Stoner Jr. launched Entelligent, a provider of Smart Climate indices, predictive equity portfolio analytics, and advanced data\n\nStoner served as the Managing Partner at New Alchemy Energy Partners, a fund dedicated to developing distributed energy generation projects in North America.\n\nFrom 2008 to 2010, Stoner served as the CEO and Chairman of Evergreen Energy Inc. (NYSE:EEE), a publicly traded coal and clean coal technology company with headquarters in Denver, Colorado.\n\nFrom 1998 to October 2008, Stoner served as CEO of Econergy International, a carbon markets consultancy and owner/operator of renewable energy projects throughout the Americas. Econergy International floated on the AIM under the London Stock Exchange (ECG) in February 2006. Econergy was an independent power developer of renewable energy projects, including wind farms, small hydro, and methane fired power generation facilities throughout Americas as well as one of the leading carbon emissions traders under the Kyoto Protocol. Under Stoner’s leadership Econergy developed the first methodology submitted to the governing body of the clean development mechanism (CDM), the United Nations Framework Convention on Climate Change (UNFCCC). Stoner led the sale of Econergy to GDF Suez, one of the largest utilities in the world. While at Econergy, Stoner helped five international development banks to develop the CleanTechFund, a $25 million private equity fund focused on small-scale energy generation (such as wind, hydro, biomass and geothermal power plants) and energy efficiency projects in Latin America. He served as the Senior Manager of the fund from its formation in 2004 to 2008.\n\nPrior to Econergy, Stoner founded and served as president of the Highland Energy Group, a national energy service company (ESCO) providing demand side management (DSM) services to public utilities, such as the Public Service Company of Colorado, Duke Power, and Texas Utilities. Stoner led the sale of the Company to Eastern Utilities, formerly a NYSE traded public utility headquartered in Boston, Massachusetts.\n\nPrior to Econergy, Stoner was the first acting director and founding board member of the Social Venture Network, a nonprofit membership organization composed of socially responsible business leaders who are committed to creating a more just and sustainable world.\n\nStoner has served as a technical advisor in more than a dozen countries.\n\nProject Butterfly was founded in 2010 and is a collaboration among scientists, business leaders, and the global community to address the threats posed by climate change, and researching opportunities that exist in mitigating it. The initiative, started by Stoner, brought together individuals from the Massachusetts Institute of Technology (MIT), NASA, the National Renewable Energy Lab (NREL), the National Center for Atmospheric Research (NCAR), and the University of Colorado at Boulder. The research and climate modeling performed under Project Butterfly led to the book “Small Change, Big Gains: Reflections of an Energy Entrepreneur”.\n\n\n\n"}
{"id": "45064207", "url": "https://en.wikipedia.org/wiki?curid=45064207", "title": "Voie Sacrée wind farm", "text": "Voie Sacrée wind farm\n\nThe Voie Sacrée wind farm is a wind farme located in the Lorraine region of France. \n\nIt is shared between the cities of Beausite, Raival, Courcelles-sur-aire, Érize-la-petite, and Maurechamp, not far from the Voie Sacrée. The onshore wind farm was proposed and installed in 2007, and today contains 27 Gamesa G90/2000 wind turbines, each with a hub height of 78 meters and diameter of 90 meters. Individually, each of the Voie Sacrée wind turbines has a rated capacity of 2,000 kW, for a total capacity of 54,000 kW for the wind farm. \n\nIn 2013, the power contribution from the Voie Sacrée wind farm accounted for approximately 0.65% of the total wind power in France (8,254 MW). The wind farm developer is SFE Française d’Eoliennes and the farm is currently owned and operated by Sorgenia France. Voie Sacrée is a contributor to French wind power Zone 55.\n\n"}
{"id": "12829891", "url": "https://en.wikipedia.org/wiki?curid=12829891", "title": "Winter of 1946–47 in the United Kingdom", "text": "Winter of 1946–47 in the United Kingdom\n\nThe winter of 1946–47 was a harsh European winter noted for its impact in the United Kingdom. It caused severe hardships in economic terms and living conditions. There were massive disruptions of energy supply for homes, offices and factories. Animal herds froze or starved to death. People suffered from the persistent cold, and many businesses shut down temporarily. When warm weather returned, the ice thawed and flooding was severe in most low-lying areas.\n\nBeginning on 21 January 1947, the UK experienced several cold spells that brought large drifts of snow to the country, blocking roads and railways, which caused problems transporting coal to the electric power stations. Many had to shut down, forcing severe restrictions to cut power consumption, including restricting domestic electricity to nineteen hours per day and cutting some industrial supplies completely. In addition, radio broadcasts were limited, television services were suspended, some magazines were ordered to stop publishing, and newspapers were reduced in size. These measures, on top of the low temperatures, badly affected public morale and the Minister of Fuel and Power, Emanuel Shinwell, became a scapegoat; he received death threats and had to be placed under police guard. Towards the end of February, there were also fears of a food shortage as supplies were cut off and vegetables were frozen into the ground.\n\nMid-March brought warmer air to the country which thawed the snow lying on the ground. This snowmelt rapidly ran off the frozen ground into rivers and caused widespread flooding. More than 100,000 properties were affected, and the British Army and foreign aid agencies were required to provide humanitarian aid. With the cold spell over and the ground thawing, there were no further weather problems. The winter had severe effects on British industries, causing the loss of around 10% per cent of the year's industrial production, 10 to 20% of cereal and potato crops, and a quarter of sheep stocks. The governing Labour Party began to lose its popularity, which led to them losing many seats to the Conservative Party at the 1950 general election; on top of other factors. That winter is also cited as a factor in the devaluation of the pound from $4.03 to $2.80 and the introduction of the Marshall Plan to rebuild war-torn Europe. The effects on the rest of Europe were also severe, with 150 deaths from cold and famine in Berlin, civil disorder in the Netherlands and business closures in the Republic of Ireland.\n\nThe effects of the cold winter were exacerbated by problems in the energy sector which caused coal supplies to become low. The coal and electricity industries had been recently nationalised by Clement Attlee's government and placed under the control of the Minister of Fuel and Power, Manny Shinwell. Shinwell oversaw efforts to increase production, but there were concerns that the coal supply was inadequate. At the start of the winter the coal stockpiles contained enough coal to last for just four weeks, compared to the usual supplies of ten to twelve weeks which existed before the war. However, Shinwell allowed himself to be lulled into a false sense of security by over-optimistic productivity reports from the National Union of Mineworkers (NUM). These reports failed to translate into real production as the government feared to take on the NUM, whose members' absentee rates were 2.5 times those of the pre-war period. The risk of a coal shortage caused the public to buy electric fires to ensure a source of heat for their homes. This, in turn, put a greater strain on the supply of electricity – the monthly demand increase caused by electric fires in 1946 was roughly the same as the annual increase in generating capacity. Shinwell was warned in mid-October that a coal shortage was possible, but gambled on a mild winter to keep consumption low so that he would not have to risk a confrontation with the miners.\n\nThe winter began with two periods of cold weather in December 1946 and January 1947, but the coldest period did not begin until 21 January 1947. The main cause of the cold weather was an anti-cyclone which sat over Scandinavia from 20 January. This high-pressure area blocked the progression of depressions across the Atlantic Ocean and forced them to the south of the United Kingdom, resulting in strong easterly winds which brought snow to eastern and south-eastern England before progressing across the entire country. This cold spell continued and by 30 January the Isles of Scilly were under of snow and the overnight temperature at Writtle, Essex, was . Throughout January the highest recorded temperature in England and Wales was and the minimum was .\n\nThe easterly winds continued into February, which developed into one of the coldest months on record. At Kew Observatory there was no recorded temperature above for the month and only twice was the overnight temperature above . No sunshine at all was recorded at Kew for twenty days from 2 February, whilst across England and Wales the month was the second-dullest February since records began in 1929, with only 30.8 hours of sunshine or 1.1 per day. In contrast, West Scotland was near-record dry and unusually sunny, though still extremely cold. On 20 February the ferry service across the English Channel between Dover and Ostend was suspended due to pack ice off the Belgian coast. In some places snow fell on 26 days out of 28 in the month and a temperature of was recorded at Woburn, Bedfordshire, on 25 February. As a result, railways were badly affected by drifts of light powdery snow and three hundred main roads were made unusable. Several hundred villages were cut off. Ice floes were also seen off the coast of East Anglia, causing a hazard to shipping.\n\nThis cold weather exacerbated the fuel problem. Stockpiles of coal at the pits and depots froze solid and could not be moved. The snow also trapped 750,000 railway wagons of coal and made roads unusable, further hampering transport. A force of 100,000 British and Polish troops and German prisoners of war were put to work clearing snow from the railways by hand, while desperate attempts were made to get fuel to power stations by coal-carrying ships which risked storms, fog and ice to reach their destinations. Despite such expedients, lack of fuel forced many power stations to shut down or reduce their output. The Royal Navy launched Operation Blackcurrant, which used diesel generators aboard submarines to provide supplementary power to coastal towns and dockyards.\n\nShinwell acted to reduce consumption of coal by cutting the electricity supply to industry completely and reducing the domestic supply to 19 hours per day across the country. In consequence factories across the country were forced to shut and up to four million people claimed unemployment benefits. Although so many people were made redundant there was little unrest and no major public disorders. Television services were suspended completely, radio broadcasts were reduced, some magazines were ordered to stop being published, and newspapers were cut in size to four pages or one sheet. Food rations, still in use from the Second World War, were cut to levels lower than in the war years. These measures made little difference to the rate of coal consumption but served to reduce public morale.\n\nDespite Shinwell's actions the fuel supply remained insufficient and blackouts occurred across large swathes of the country, forcing even the staff at Buckingham Palace, the Houses of Parliament and London's Central Electricity Board to work by candlelight. A trade meeting with representatives from Russia and Iceland was also held without light or heating; one of the items discussed was the purchase of coal from Britain. The public was reduced to queuing at gasworks to collect coke for use as fuel. Supplies of aspirin also ran low as it was then a product of coal-tar, thousands of chickens in poultry farms died of the cold, and public transport services were cut to save fuel. Shinwell became increasingly unpopular with the general public and received a bomb threat, after which a four-man police guard was stationed at his house in Tooting. Despite this, he remained very popular with the miners, which made the government wary of dismissing him in case it caused industrial action. By 27 February sea conditions had improved and more than 100 coal ships had managed to unload their cargoes at the power stations, easing the fuel crisis.\n\nDuring this period there was a fear that, despite the rationing, food supplies could run out owing to the effects of the cold on vegetables, livestock and delivery vehicles. In response, the government started a largely unsuccessful campaign to popularise Snoek, an inexpensive South African variety of fish; the public found the fish unpalatable and its stocks were eventually used as cat food. Many winter root vegetables could not be harvested as they were frozen into the ground, and in some areas pneumatic drills were used to excavate them. Frost destroyed of potatoes and, as a result, potatoes were rationed for the first time.\n\nOn 4–5 March came heavy snow which left drifts across much of the country with some lying deep in the Scottish Highlands. On 5 March one of the worst British blizzards of the 20th century occurred. Food supplies were again affected by the snow-bound roads and in some places the police requested permission to break into delivery lorries stranded by the snow.\n\nOn 10 March milder air of began to move north across the country from the south-west, rapidly thawing the snow lying on low ground. However, after such a long frost the ground stayed frozen. The frozen ground caused much surface run off which resulted in widespread flooding. Further heavy snowfalls occurred as the milder air pushed northwards. On 14 March the deepest ever recorded depth of snow lying in an inhabited location of the UK was measured at Forest-in-Teesdale in County Durham at . On 15 March a deepening depression moved in from the Atlantic, bringing heavy rain and gales. It was the start of the wettest March for 300 years. By 16 March winds reached with gusts, causing breaches in dykes in East Anglia that resulted in the flooding of of land, and blowing many trees down. The rivers Thames and Lea flooded in London, causing the Windsor borough engineer Geoffrey Baker to remark: \"We could only cope if we had a spare Thames, or two.\"\n\nOn 17–18 March the River Trent overtopped its banks in Nottingham. Large parts of the city and surrounding areas were flooded, in which 9,000 properties and nearly a hundred industrial premises were affected – some to first-floor height. The suburbs of Long Eaton, West Bridgford and Beeston suffered particularly badly. Two days later, in the lower tidal reaches of the river, the peak of the flood combined with a high spring tide to flood villages and 2,000 properties in Gainsborough. River levels dropped when the floodbank at Morton breached, resulting in the flooding of some of farmland in the Trent valley. The flooding subsided in the west of the country by 20 March but rivers in the east were still rising and the Wharfe, Derwent, Aire and Ouse all burst their banks in the West Riding of Yorkshire. Selby was also badly affected with 70 per cent of houses being flooded. More than 100,000 properties were affected by the flooding and the Army worked to prevent the spread of the floodwater, particularly at pumping plants and power stations. Royal Engineers on national service handed out milk to families with babies and the Australian Red Cross assisted in Gloucester. The people of Canada sent food parcels to villages in Suffolk and the Premier of Ontario, George A. Drew, offered to help distribute them personally. The flooding lasted for about a week, with some waters taking an additional ten days to subside.\n\nThe winter had a lasting effect on Britain's industry; by February 1947 it was already estimated that that year's industrial output would be down by 10 per cent. The effects of the March floods added a further £250–375 million (equivalent to £– billion in ) in damage. Farming was particularly badly hit with cereal and potato harvests down 10 to 20 per cent on the previous two years. Sheep farmers lost one quarter of their flocks and it would be six years before sheep numbers recovered.\n\nIn Wales a disaster fund of £4,575,000 was partly allocated to assist farmers who lost about 4 million sheep.\n\nThe winter had political ramifications and caused the public to lose faith in a Labour government who could not maintain food and electricity supplies. Long-time Labour activist Manny Shinwell was in charge of the coal supply. He repeatedly denied there were problems. He blamed the climate, the railway system, or capitalism generally. The cabinet had to take control away from him and he became the scapegoat.\n\nYoungs et al. conclude that, \"Probably more than anything else, the fuel crisis of 1947 led to a loss of public confidence in the Labour government.\" Labour lost a large number of seats to the Conservative Party but retained a slim majority in the 1950 election.\n\nThe effects of the winter came at a time of heavy government spending with 15 per cent of the GDP being spent on the armed forces and large expenditure on the new National Health Service and post-war reconstruction. This made the currency less stable and, coupled with the emergence of the dollar as the currency of choice for foreign reserves, led the government to slash the Bretton Woods official exchange rate from $4.03 to $2.80. This was a major event in Britain's decline from superpower status. With the country struggling to feed its people at home and those it was responsible for in war-torn Europe, it also caused the US to take a greater interest in Europe and push through the Marshall Plan for assistance to Britain and the continent. In addition, the winter is cited as the reason for the emigration of thousands of British people, particularly to Australia. The winter as a whole was less cold than the winter of 1963 but more snow was recorded.\n\nThe winter affected many other European countries. Similar cold periods and snowfalls were seen in much of Central Europe and the southern Baltic region. De Bilt, near Amsterdam in the Netherlands, experienced its worst winter since 1790. Because of the anticyclone to the north of the United Kingdom, several incoming Atlantic depressions which would otherwise have hit Britain tracked south to the Mediterranean region, resulting in Portugal, Spain, and Southern France having more rain than usual while remaining relatively warm. For example, the February rainfall at Gibraltar was , three times the average. As a result, France experienced both the extreme cold in the north and much rain in the south. The winter caused 150 deaths from the cold and lack of food in Berlin, which was still recovering from its devastation during the final stages of the Second World War. It caused schools in the Netherlands to be closed, led to a mob attack on a goods train carrying coal in Copenhagen, and caused the closure of businesses and the restriction of domestic gas supplies in Ireland.\n\n\n\n\n"}
{"id": "2756123", "url": "https://en.wikipedia.org/wiki?curid=2756123", "title": "Wood veneer", "text": "Wood veneer\n\nIn woodworking, veneer refers to thin slices of wood, usually thinner than 3 mm (1/8 inch), that typically are glued onto core panels (typically, wood, particle board or medium-density fiberboard) to produce flat panels such as doors, tops and panels for cabinets, parquet floors and parts of furniture. They are also used in marquetry. Plywood consists of three or more layers of veneer. Normally, each is glued with its grain at right angles to adjacent layers for strength. Veneer beading is a thin layer of decorative edging placed around objects, such as jewelry boxes. Veneer is also used to replace decorative papers in Wood Veneer HPL. Veneer is also a type of manufactured board.\n\nVeneer is obtained either by \"peeling\" the trunk of a tree or by slicing large rectangular blocks of wood known as flitches. The appearance of the grain and figure in wood comes from slicing through the growth rings of a tree and depends upon the angle at which the wood is sliced. There are three main types of veneer-making equipment used commercially:\n\nEach slicing processes gives a very distinctive type of grain, depending upon the tree species. In any of the veneer-slicing methods, when the veneer is sliced, a distortion of the grain occurs. As it hits the wood, the knife blade creates a \"loose\" side where the cells have been opened up by the blade, and a \"tight\" side.\n\nHistorically veneers were also sawn, but this is more wasteful of wood. Veneering is an ancient art, dating back to at least the ancient Egyptians who used expensive and rare wood veneers over cheaper timbers to produce their furniture and sarcophagi. During the Roman Empire, Romans also used veneered work in mass quantities.\nThe finest and rarest logs are sent to companies that produce veneer. The advantage to this practice is twofold. First, it provides the most financial gain to the owner of the log. Secondly, and of more importance to the woodworker, it greatly expands the amount of usable wood. While a log used for solid lumber is cut into thick pieces, usually no thinner than 1/8 of an inch (3 mm), veneers are cut as thin as 1/40 of an inch (0.6 mm). Depending on the cutting process used by the veneer manufacturer, very little wood is wasted by the saw blade thickness, known as the saw kerf. Accordingly, the yield of a rare grain pattern or wood type is greatly increased, in turn placing less stress on the resource. Some manufacturers even use a very wide knife to \"slice off\" the thin veneer pieces. In this way, none of the wood is wasted. The slices of veneer are always kept in the order in which they are cut from the log and are often sold this way.\n\nThere are a few types of veneers available, each serving a particular purpose.\n\nCompared to wood, one of the primary advantages of using veneer is stability. While solid wood can be prone to warping and splitting, because veneer is made of thin layers of wood glued together, the chances of splitting or cracking are reduced. Further, the glue used provides additional strength, making the end result stronger than natural wood.\n\nSome projects built using wood veneer would not be possible to construct using solid lumber, owing to expansion and contraction caused by fluctuation of temperature and humidity. Another advantage of veneer is sustainability—furniture made with wood veneer uses less wood than the same piece of furniture made with solid wood. Further, veneer may also be more readily available than solid wood as exotic hardwood lumber can be scarce and very expensive.\n\n\nWood veneers are typically sold by the square foot. With the ability to join veneers, even small pieces are usable, resulting in very little waste.\nMany sources sell small packets of veneers that are sequence matched and are ideal for small projects. These make experimenting and practicing much more economical. It is also possible to buy plywood and other substrates with veneered faces for larger projects consisting of casework.\n\n\n\n"}
{"id": "25809401", "url": "https://en.wikipedia.org/wiki?curid=25809401", "title": "Øystein Dahle", "text": "Øystein Dahle\n\nØystein Dahle (born 18 March 1938) is a Norwegian businessperson and organizational leader.\n\nHe was born in Trondheim. He worked in Esso from 1963, and was vice president of Esso Norway from 1985 to 1995. He was the chairman of the Norwegian Trekking Association from 1994 to 2003, and in the Worldwatch Institute from 2002. He is a fellow of the Norwegian Academy of Technological Sciences.\n\nHe is married to Nina Frisak, and is the father of Gro Dahle.\n"}
