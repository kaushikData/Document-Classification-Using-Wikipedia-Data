{"id": "15540624", "url": "https://en.wikipedia.org/wiki?curid=15540624", "title": "1982 Garuda Fokker F28 crash", "text": "1982 Garuda Fokker F28 crash\n\nThe 1982 Garuda Fokker F28 crash (flight number unknown) occurred on 20 March 1982, when a Fokker F28, operated by Garuda Indonesia, overran the runway at Tanjung Karang-Branti Airport (Lampung) in the province of Lampung in Indonesia during very heavy rain. The plane was scheduled to fly from Jakarta to Lampung. The aircraft came to rest just 700 meters from the runway in a field, with the aircraft catching fire. All passengers and crew died.\n\n"}
{"id": "44263676", "url": "https://en.wikipedia.org/wiki?curid=44263676", "title": "A Pound of Flesh for 50p", "text": "A Pound of Flesh for 50p\n\nA Pound of Flesh for 50p, also known as Melting House, was a temporary outdoor sculpture by artist Alex Chinneck, located in London, United Kingdom. Part of the city's Merge Festival, the two-story house sculpture was constructed from 8,000 paraffin wax bricks and it was designed to melt with assistance from a heating apparatus over the course of the installation, It was displayed from September 26 to November 18, 2014, at 40 Southwark Street, SE1 9HP. the structure's roof being gradually lowered as the wax melted. After it had been reduced to \"a pile of hardened goo\", the sculpture was removed.\n\n\n"}
{"id": "3834517", "url": "https://en.wikipedia.org/wiki?curid=3834517", "title": "AgInSbTe", "text": "AgInSbTe\n\nAgInSbTe, or Silver-Indium-Antimony-Tellurium, is a phase change material from the group of chalcogenide glasses, used in rewritable optical discs (such as rewritable CDs) and phase-change memory applications. It is a quaternary compound of silver, indium, antimony, and tellurium. \n\nDuring writing, the material is first erased, initialized into its crystalline state, with long, lower-intensity laser irradiation. The material heats up to its crystallization temperature, but not up to its melting point, and crystallizes in a metastable face-centered cubic structure. Then the information is written on the crystalline phase, by heating spots of it with short (<10 ns), high-intensity laser pulses; the material locally melts and is quickly cooled, remaining in the amorphous phase. As the amorphous phase has lower reflectivity than the crystalline phase, the bitstream can be recorded as \"dark\" amorphous spots on the crystalline background. At low linear velocities, clusters of crystalline material can exist in the amorphous spots.\nAnother similar material is GeSbTe, offering a lower linear density, but with higher overwrite cycles by 1-2 orders of magnitude. It is used in pit-and-groove recording formats, often in rewritable DVDs.\n"}
{"id": "39199923", "url": "https://en.wikipedia.org/wiki?curid=39199923", "title": "Alabama Plating Company Superfund site", "text": "Alabama Plating Company Superfund site\n\nThe Alabama Plating Company Superfund site is a former industrial site in Vincent, Alabama. The site covers 6 acres and was used by the Alabama Plating Company as an electroplating facility between 1956-1986. The facility caused contamination of the ground water with hazardous waste containing heavy metals. After assessment by the United States Environmental Protection Agency (EPA) it was added to the National Priorities List in September 2012 for remedial action. The site cleanup is directed by the federal Superfund program.\n\n"}
{"id": "898", "url": "https://en.wikipedia.org/wiki?curid=898", "title": "Antimony", "text": "Antimony\n\nAntimony is a chemical element with symbol Sb (from ) and atomic number 51. A lustrous gray metalloid, it is found in nature mainly as the sulfide mineral stibnite (SbS). Antimony compounds have been known since ancient times and were powdered for use as medicine and cosmetics, often known by the Arabic name, kohl. Metallic antimony was also known, but it was erroneously identified as lead upon its discovery. The earliest known description of the metal in the West was written in 1540 by Vannoccio Biringuccio.\n\nFor some time, China has been the largest producer of antimony and its compounds, with most production coming from the Xikuangshan Mine in Hunan. The industrial methods for refining antimony are roasting and reduction with carbon or direct reduction of stibnite with iron.\n\nThe largest applications for metallic antimony is an alloy with lead and tin and the lead antimony plates in lead–acid batteries. Alloys of lead and tin with antimony have improved properties for solders, bullets, and plain bearings. Antimony compounds are prominent additives for chlorine and bromine-containing fire retardants found in many commercial and domestic products. An emerging application is the use of antimony in microelectronics.\n\nAntimony is a member of group 15 of the periodic table, one of the elements called pnictogens, and has an electronegativity of 2.05. In accordance with periodic trends, it is more electronegative than tin or bismuth, and less electronegative than tellurium or arsenic. Antimony is stable in air at room temperature, but reacts with oxygen if heated to produce antimony trioxide, SbO.\n\nAntimony is a silvery, lustrous gray metalloid with a Mohs scale hardness of 3, which is too soft to make hard objects; coins of antimony were issued in China's Guizhou province in 1931 but the durability was poor and the minting was soon discontinued. Antimony is resistant to attack by acids.\n\nFour allotropes of antimony are known: a stable metallic form and three metastable forms (explosive, black and yellow). Elemental antimony is a brittle, silver-white shiny metalloid. When slowly cooled, molten antimony crystallizes in a trigonal cell, isomorphic with the gray allotrope of arsenic. A rare explosive form of antimony can be formed from the electrolysis of antimony trichloride. When scratched with a sharp implement, an exothermic reaction occurs and white fumes are given off as metallic antimony forms; when rubbed with a pestle in a mortar, a strong detonation occurs. Black antimony is formed upon rapid cooling of antimony vapor. It has the same crystal structure as red phosphorus and black arsenic, it oxidizes in air and may ignite spontaneously. At 100 °C, it gradually transforms into the stable form. The yellow allotrope of antimony is the most unstable. It has only been generated by oxidation of stibine (SbH) at −90 °C. Above this temperature and in ambient light, this metastable allotrope transforms into the more stable black allotrope.\n\nElemental antimony adopts a layered structure (space group Rm No. 166) in which layers consist of fused, ruffled, six-membered rings. The nearest and next-nearest neighbors form an irregular octahedral complex, with the three atoms in each double layer slightly closer than the three atoms in the next. This relatively close packing leads to a high density of 6.697 g/cm, but the weak bonding between the layers leads to the low hardness and brittleness of antimony.\n\nAntimony has two stable isotopes: Sb with a natural abundance of 57.36% and Sb with a natural abundance of 42.64%. It also has 35 radioisotopes, of which the longest-lived is Sb with a half-life of 2.75 years. In addition, 29 metastable states have been characterized. The most stable of these is Sb with a half-life of 5.76 days. Isotopes that are lighter than the stable Sb tend to decay by β decay, and those that are heavier tend to decay by β decay, with some exceptions.\n\nThe abundance of antimony in the Earth's crust is estimated to be 0.2 to 0.5 parts per million, comparable to thallium at 0.5 parts per million and silver at 0.07 ppm. Even though this element is not abundant, it is found in more than 100 mineral species. Antimony is sometimes found natively (e.g. on Antimony Peak), but more frequently it is found in the sulfide stibnite (SbS) which is the predominant ore mineral.\n\nAntimony compounds are often classified according to their oxidation state: Sb(III) and Sb(V). The +5 oxidation state is more stable.\n\nAntimony trioxide is formed when antimony is burnt in air. In the gas phase, the molecule of the compound is , but it polymerizes upon condensing. Antimony pentoxide () can be formed only by oxidation with concentrated nitric acid. Antimony also forms a mixed-valence oxide, antimony tetroxide (), which features both Sb(III) and Sb(V). Unlike oxides of phosphorus and arsenic, these oxides are amphoteric, do not form well-defined oxoacids, and react with acids to form antimony salts.\n\nAntimonous acid is unknown, but the conjugate base sodium antimonite () forms upon fusing sodium oxide and . Transition metal antimonites are also known. Antimonic acid exists only as the hydrate , forming salts as the antimonate anion . When a solution containing this anion is dehydrated, the precipitate contains mixed oxides.\n\nMany antimony ores are sulfides, including stibnite (), pyrargyrite (), zinkenite, jamesonite, and boulangerite. Antimony pentasulfide is non-stoichiometric and features antimony in the +3 oxidation state and S-S bonds. Several thioantimonides are known, such as and .\n\nAntimony forms two series of halides: and . The trihalides , , , and are all molecular compounds having trigonal pyramidal molecular geometry.\n\nThe trifluoride is prepared by the reaction of with HF:\n\nIt is Lewis acidic and readily accepts fluoride ions to form the complex anions and . Molten is a weak electrical conductor. The trichloride is prepared by dissolving in hydrochloric acid:\nThe pentahalides and have trigonal bipyramidal molecular geometry in the gas phase, but in the liquid phase, is polymeric, whereas is monomeric. is a powerful Lewis acid used to make the superacid fluoroantimonic acid (\"HSbF\").\n\nOxyhalides are more common for antimony than for arsenic and phosphorus. Antimony trioxide dissolves in concentrated acid to form oxoantimonyl compounds such as SbOCl and .\n\nCompounds in this class generally are described as derivatives of Sb. Antimony forms antimonides with metals, such as indium antimonide (InSb) and silver antimonide (). The alkali metal and zinc antimonides, such as NaSb and ZnSb, are more reactive. Treating these antimonides with acid produces the highly unstable gas stibine, :\nStibine can also be produced by treating salts with hydride reagents such as sodium borohydride. Stibine decomposes spontaneously at room temperature. Because stibine has a positive heat of formation, it is thermodynamically unstable and thus antimony does not react with hydrogen directly.\n\nOrganoantimony compounds are typically prepared by alkylation of antimony halides with Grignard reagents. A large variety of compounds are known with both Sb(III) and Sb(V) centers, including mixed chloro-organic derivatives, anions, and cations. Examples include Sb(CH) (triphenylstibine), Sb(CH) (with an Sb-Sb bond), and cyclic [Sb(CH)]. Pentacoordinated organoantimony compounds are common, examples being Sb(CH) and several related halides.\n\nAntimony(III) sulfide, SbS, was recognized in predynastic Egypt as an eye cosmetic (kohl) as early as about 3100 BC, when the cosmetic palette was invented.\n\nAn artifact, said to be part of a vase, made of antimony dating to about 3000 BC was found at Telloh, Chaldea (part of present-day Iraq), and a copper object plated with antimony dating between 2500 BC and 2200 BC has been found in Egypt. Austen, at a lecture by Herbert Gladstone in 1892 commented that \"we only know of antimony at the present day as a highly brittle and crystalline metal, which could hardly be fashioned into a useful vase, and therefore this remarkable 'find' (artifact mentioned above) must represent the lost art of rendering antimony malleable.\"\n\nMoorey was unconvinced the artifact was indeed a vase, mentioning that Selimkhanov, after his analysis of the Tello object (published in 1975), \"attempted to relate the metal to Transcaucasian natural antimony\" (i.e. native metal) and that \"the antimony objects from Transcaucasia are all small personal ornaments.\" This weakens the evidence for a lost art \"of rendering antimony malleable.\"\n\nThe Roman scholar Pliny the Elder described several ways of preparing antimony sulfide for medical purposes in his treatise \"Natural History\". Pliny the Elder also made a distinction between \"male\" and \"female\" forms of antimony; the male form is probably the sulfide, while the female form, which is superior, heavier, and less friable, has been suspected to be native metallic antimony.\n\nThe Roman naturalist Pedanius Dioscorides mentioned that antimony sulfide could be roasted by heating by a current of air. It is thought that this produced metallic antimony.\n\nThe first description of a procedure for isolating antimony is in the 1540 book \"De la pirotechnia\" by Vannoccio Biringuccio, predating the more famous 1556 book by Agricola, \"De re metallica\". In this context Agricola has been often incorrectly credited with the discovery of metallic antimony. The book \"Currus Triumphalis Antimonii\" (The Triumphal Chariot of Antimony), describing the preparation of metallic antimony, was published in Germany in 1604. It was purported to be written by a Benedictine monk, writing under the name Basilius Valentinus in the 15th century; if it were authentic, which it is not, it would predate Biringuccio.\n\nThe metal antimony was known to German chemist Andreas Libavius in 1615 who obtained it by adding iron to a molten mixture of antimony sulfide, salt and potassium tartrate. This procedure produced antimony with a crystalline or starred surface.\n\nWith the advent of challenges to phlogiston theory, it was recognized that antimony is an element forming sulfides, oxides, and other compounds, as do other metals.\n\nThe first natural occurrence of pure antimony in the Earth's crust was described by the Swedish scientist and local mine district engineer Anton von Swab in 1783; the type-sample was collected from the Sala Silver Mine in the Bergslagen mining district of Sala, Västmanland, Sweden.\n\nThe medieval Latin form, from which the modern languages and late Byzantine Greek take their names for antimony, is \"antimonium\". The origin of this is uncertain; all suggestions have some difficulty either of form or interpretation. The popular etymology, from ἀντίμοναχός \"anti-monachos\" or French \"antimoine\", still has adherents; this would mean \"monk-killer\", and is explained by many early alchemists being monks, and antimony being poisonous.\n\nAnother popular etymology is the hypothetical Greek word ἀντίμόνος \"antimonos\", \"against aloneness\", explained as \"not found as metal\", or \"not found unalloyed\". Lippmann conjectured a hypothetical Greek word ανθήμόνιον \"anthemonion\", which would mean \"floret\", and cites several examples of related Greek words (but not that one) which describe chemical or biological efflorescence.\n\nThe early uses of \"antimonium\" include the translations, in 1050–1100, by Constantine the African of Arabic medical treatises. Several authorities believe \"antimonium\" is a scribal corruption of some Arabic form; Meyerhof derives it from \"ithmid\"; other possibilities include \"athimar\", the Arabic name of the metalloid, and a hypothetical \"as-stimmi\", derived from or parallel to the Greek.\n\nThe standard chemical symbol for antimony (Sb) is credited to Jöns Jakob Berzelius, who derived the abbreviation from \"stibium\".\n\nThe ancient words for antimony mostly have, as their chief meaning, kohl, the sulfide of antimony.\nThe Egyptians called antimony \"mśdmt\"; in hieroglyphs, the vowels are uncertain, but the Coptic form of the word is ⲥⲧⲏⲙ (stēm). The Greek word, στίμμι \"stimmi\", is probably a loan word from Arabic or from Egyptian \"stm\" O34:D46-G17-F21:D4 and is used by Attic tragic poets of the 5th century BC. Later Greeks also used στἰβι \"stibi\", as did Celsus and Pliny, writing in Latin, in the first century AD. Pliny also gives the names \"stimi\" , \"larbaris\", alabaster, and the \"very common\" \"platyophthalmos\", \"wide-eye\" (from the effect of the cosmetic). Later Latin authors adapted the word to Latin as \"stibium\". The Arabic word for the substance, as opposed to the cosmetic, can appear as إثمد \"ithmid, athmoud, othmod\", or \"uthmod\". Littré suggests the first form, which is the earliest, derives from \"stimmida\", an accusative for \"stimmi\".\n\nThe British Geological Survey (BGS) reported that in 2005 China was the top producer of antimony with approximately 84% of the world share, followed at a distance by South Africa, Bolivia and Tajikistan. Xikuangshan Mine in Hunan province has the largest deposits in China with an estimated deposit of 2.1 million metric tons.\n\nIn 2016, according to the US Geological Survey, China accounted for 76.9% of total antimony production, followed in second place by Russia with 6.9% and Tajikistan with 6.2%.\n\nChinese production of antimony is expected to decline in the future as mines and smelters are closed down by the government as part of pollution control. Especially due to a new environmental protection law having gone into effect on January 2015 and revised “Emission Standards of Pollutants for Stanum, Antimony, and Mercury” having gone into effect, hurdles for economic production are higher. According to the National Bureau of Statistics in China, by September 2015 50% of antimony production capacity in the Hunan province (the province with biggest antimony reserves in China) had not been used.\n\nReported production of antimony in China has fallen and is unlikely to increase in the coming years, according to the Roskill report. No significant antimony deposits in China have been developed for about ten years, and the remaining economic reserves are being rapidly depleted.\n\nThe world's largest antimony producers, according to Roskill, are listed below:\n\nAccording to statistics from the USGS, current global reserves of antimony will be depleted in 13 years. However, the USGS expects more resources will be found.\n\nThe extraction of antimony from ores depends on the quality and composition of the ore. Most antimony is mined as the sulfide; lower-grade ores are concentrated by froth flotation, while higher-grade ores are heated to 500–600 °C, the temperature at which stibnite melts and separates from the gangue minerals. Antimony can be isolated from the crude antimony sulfide by reduction with scrap iron:\n\nThe sulfide is converted to an oxide; the product is then roasted, sometimes for the purpose of vaporizing the volatile antimony(III) oxide, which is recovered. This material is often used directly for the main applications, impurities being arsenic and sulfide. Antimony is isolated from the oxide by a carbothermal reduction:\n\nThe lower-grade ores are reduced in blast furnaces while the higher-grade ores are reduced in reverberatory furnaces.\n\nAntimony has consistently been ranked high in European and US risk lists concerning criticality of the element indicating the relative risk to the supply of chemical elements or element groups required to maintain the current economy and lifestyle.\n\nWith most of the antimony imported into Europe and the US coming from China, Chinese production is critical to supply. As China is revising and increasing environmental control standards, antimony production is becoming increasingly restricted. Additionally Chinese export quotas for antimony have been decreasing in the past years. These two factors increase supply risk for both Europe and US.\n\nAccording to the BGS Risk List 2015, antimony is ranked second highest (after rare earth elements) on the relative supply risk index. This indicates that it has currently the second highest supply risk for chemical elements or element groups which are of economic value to the British economy and lifestyle.\nFurthermore, antimony was identified as one of 20 critical raw materials for the EU in a report published in 2014 (which revised the initial report published in 2011). As seen in Figure xxx antimony maintains high supply risk relative to its economic importance. 92% of the antimony is imported from China, which is a significantly high concentration of production.\n\nMuch analysis has been conducted in the U.S. toward defining which metals should be called strategic or critical to the nation's security. Exact definitions do not exist, and views as to what constitutes a strategic or critical mineral to U.S. security diverge.\n\nIn 2015, no antimony was mined in the U.S. The metal is imported from foreign countries. From 2011-2014 68% of America's antimony came from China, 14% from India, 4% from Mexico, and 14% from other sources. There are no government stockpiles in place currently.\n\nThe U.S. “Subcommittee on Critical and Strategic Mineral Supply Chains” has screened 78 mineral resources from 1996-2008. It found that a small subset of minerals including antimony has fallen into the category of potentially critical minerals consistently. In the future, a second assessment will be made of the found subset of minerals to identify which should be defined of significant risk and critical to U.S. interests.\n\nAbout 60% of antimony is consumed in flame retardants, and 20% is used in alloys for batteries, plain bearings, and solders.\n\nAntimony is mainly used in the trioxide for flame-proofing compounds, always in combination with halogenated flame retardants except in halogen-containing polymers. The flame retarding effect of antimony trioxide is produced by the formation of halogenated antimony compounds, which react with hydrogen atoms, and probably also with oxygen atoms and OH radicals, thus inhibiting fire. Markets for these flame-retardants include children's clothing, toys, aircraft, and automobile seat covers. They are also added to polyester resins in fiberglass composites for such items as light aircraft engine covers. The resin will burn in the presence of an externally generated flame, but will extinguish when the external flame is removed.\n\nAntimony forms a highly useful alloy with lead, increasing its hardness and mechanical strength. For most applications involving lead, varying amounts of antimony are used as alloying metal. In lead–acid batteries, this addition improves plate strength and charging characteristics. It is used in antifriction alloys (such as Babbitt metal), in bullets and lead shot, electrical cable sheathing, type metal (for example, for linotype printing machines), solder (some \"lead-free\" solders contain 5% Sb), in pewter, and in hardening alloys with low tin content in the manufacturing of organ pipes.\n\nThree other applications consume nearly all the rest of the world's supply. One application is a stabilizer and a catalyst for the production of polyethyleneterephthalate. Another is a fining agent to remove microscopic bubbles in glass, mostly for TV screens; antimony ions interact with oxygen, suppressing the tendency of the latter to form bubbles. The third application is pigments.\n\nAntimony is increasingly being used in semiconductors as a dopant in n-type silicon wafers for diodes, infrared detectors, and Hall-effect devices. In the 1950s, the emitters and collectors of n-p-n alloy junction transistors were doped with tiny beads of a lead-antimony alloy. Indium antimonide is used as a material for mid-infrared detectors.\n\nBiology and medicine have few uses for antimony. Treatments containing antimony, known as antimonials, are used as emetics. Antimony compounds are used as antiprotozoan drugs. Potassium antimonyl tartrate, or tartar emetic, was once used as an anti-schistosomal drug from 1919 on. It was subsequently replaced by praziquantel. Antimony and its compounds are used in several veterinary preparations, such as anthiomaline and lithium antimony thiomalate, as a skin conditioner in ruminants. Antimony has a nourishing or conditioning effect on keratinized tissues in animals.\n\nAntimony-based drugs, such as meglumine antimoniate, are also considered the drugs of choice for treatment of leishmaniasis in domestic animals. Unfortunately, besides having low therapeutic indices, the drugs have minimal penetration of the bone marrow, where some of the \"Leishmania\" amastigotes reside, and curing the disease – especially the visceral form – is very difficult. Elemental antimony as an antimony pill was once used as a medicine. It could be reused by others after ingestion and elimination.\n\nAntimony(III) sulfide is used in the heads of some safety matches.\n\nAntimony sulfides help to stabilize the friction coefficient in automotive brake pad materials.\n\nAntimony is used in bullets, bullet tracers, paint, glass art, and as an opacifier in enamel.\n\nAntimony-124 is used together with beryllium in neutron sources; the gamma rays emitted by antimony-124 initiate the photodisintegration of beryllium. The emitted neutrons have an average energy of 24 keV. Natural antimony is used in startup neutron sources.\n\nThe effects of antimony and its compounds on human and environmental health differ widely. The elemental antimony metal does not affect human and environmental health. Inhalation of antimony trioxide (and similar poorly soluble Sb(III) dust particles such as antimony dust) is considered harmful and suspected of causing cancer. However, these effects are only observed with female rats and after long-term exposure to high dust concentrations. The effects are hypothesized to be attributed to inhalation of poorly soluble Sb particles leading to impaired lung clearance, lung overload, inflammation and ultimately tumour formation, not to exposure to antimony ions (OECD, 2008). Antimony chlorides are corrosive to skin. The effects of antimony are not comparable to arsenic; this might be caused by the significant differences of uptake, metabolism and excretion between arsenic and antimony.\n\nFor oral absorption, ICRP (1994) recommended values of 10% for tartar emetic and 1% for all other antimony compounds. Dermal absorption for metals is estimated at most 1% (HERAG, 2007). Inhalation absorption of antimony trioxide and other poorly soluble Sb(III) substances (such as antimony dust) is estimated at 6.8% (OECD, 2008), whereas a value <1% is derived for Sb(V) substances. Antimony(V) is not quantitatively reduced to antimony(III) in the cell, and both species exist simultaneously.\n\nAntimony is mainly excreted from the human body via urine. Antimony and its compounds do not cause acute human health effects, with the exception of antimony potassium tartrate (\"tartar emetic\"), a prodrug that is intentionally used to treat leishmaniasis patients.\n\nProlonged skin contact with antimony dust may cause dermatitis. However, it was agreed at the European Union level that the skin rashes observed are not substance-specific, but most probably due to a physical blocking of sweat ducts (ECHA/PR/09/09, Helsinki, 6 July 2009). Antimony dust may also be explosive when dispersed in the air; when in a bulk solid it is not combustible.\n\nAntimony is incompatible with strong acids, halogenated acids, and oxidizers; when exposed to newly formed hydrogen it may form stibine (SbH).\n\nThe 8-hour time-weighted average (TWA) is set at 0.5 mg/m by the American Conference of Governmental Industrial Hygienists and by the Occupational Safety and Health Administration (OSHA) as a legal permissible exposure limit (PEL) in the workplace. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5 mg/m as an 8 hour TWA. Antimony compounds are used as catalysts for polyethylene terephthalate (PET) production. Some studies report minor antimony leaching from PET bottles into liquids, but levels are below drinking water guidelines. Antimony concentrations in fruit juice concentrates were somewhat higher (up to 44.7 µg/L of antimony), but juices do not fall under the drinking water regulations. The drinking water guidelines are:\n\nThe TDI proposed by WHO is 6 µg antimony per kilogram of body weight. The IDLH (immediately dangerous to life and health) value for antimony is 50 mg/m.\n\nCertain compounds of antimony appear to be toxic, particularly antimony trioxide and antimony potassium tartrate. Effects may be similar to arsenic poisoning. Occupational exposure may cause respiratory irritation, pneumoconiosis, antimony spots on the skin, gastrointestinal symptoms and cardiac arrhythmias. In addition antimony trioxide is potentially carcinogenic to humans.\n\nAdverse health effects have been observed in humans and animals following inhalation, oral, or dermal exposure to antimony and antimony compounds. Antimony toxicity typically occurs either due to occupational exposure, during therapy or from accidental ingestion. It is unclear if antimony can enter the body through the skin.\n\n\n\n"}
{"id": "35754260", "url": "https://en.wikipedia.org/wiki?curid=35754260", "title": "Biomesh", "text": "Biomesh\n\nBiologic mesh (or biomesh) is a type of surgical mesh made from an organic biomaterial (such as porcine dermis, porcine small intestine submucosa, bovine dermis or pericardium, and the dermis or fascia lata of a cadaveric human). Biologic mesh is primarily indicated for several types of hernia repair, including inguinal and ventral hernias, hernia prophylaxis, and contaminated hernia repairs. However, it has also been used in pelvic floor dysfunction, parotidectomy, and reconstructive plastic surgery. The development of biologic mesh largely has derived from the need of a biocompatible material that addresses \"the problems associated with a permanent synthetic mesh, including chronic inflammation, foreign body reaction, fibrosis, and mesh infection.\" , however, the efficacy and optimal use of biological mesh products remains in question.\n\nThe idea of using organic materials for surgical mesh has been around since at least the late 1950s, though researchers soon learned the materials they tested weren't biocompatible. Research into more compatible biomaterials occurred in the proceeding decades, including the search for cellular-based materials extracted from humans and animals. For example, in 1980, research presented at the first ever World Biomaterials Congress detailed the examined use of dermal collagen of sheep to construct biological mesh for reconstructive surgery. Since then, \"research for developing and improvising the biological material required for the production of these meshes\" has been ongoing.\n\nTypical advantages attributed to biologic meshes include reducing the risk of infection (from using non-biologic surgical meshes) and is absorbed into the resulting scar as part of cellular ingrowth. Commonly described drawbacks include the high cost of the material and its uncertain clinical effectiveness, particularly in regards to the cost. An August 2015 follow-up literature review published by the Canadian Agency for Drugs and Technologies in Health in particular addressed these drawbacks, concluding:\n\nBased on the publications identified for the current report, there remains a lack of sufficient evidence to guide clinical practice regarding the use of biological mesh products ... Several surgical indications are addressed by this collection of [randomized controlled trials (RCTs)] with relatively few studies per indication. Therefore, it is not immediately apparent whether this represents a significant amount of research on the clinical effectiveness of any particular mesh product or for any specific patient population that would support clinical decision making. Further rigorously designed RCTs are required to clarify comparative clinical effectiveness and safety of the many available biological mesh products for most surgical indications in which their use has been suggested.\n\nThe presence of contamination may limit the applicability of permanent synthetic mesh in some procedures such as hernia repair. Biologic mesh may be acceptable for this purpose or for placement in open wounds as a staged closure in complex abdominal wall reconstruction. There is limited data in both of these areas, with some noting a high risk of hernia recurrence and associated infection. The data is mostly limited to animal models and case series. However, the lack of suitable alternatives has made biologic mesh attractive for contaminated field hernia repair.\n"}
{"id": "42471006", "url": "https://en.wikipedia.org/wiki?curid=42471006", "title": "Bosnian maple", "text": "Bosnian maple\n\nBosnian maple or Yugoslavian maple is a type of \"Acer platanoides\", a European mountain maple indigenous to former Yugoslavia. It was a very high grade of maple, very light and very strong, according to some the best wood in the world for making violins, as it had the finest resonance. The classic Italian violin makers probably used wood from Tyrol, or northern Yugoslavia, or Switzerland. The maple has mostly been used for the back plates. It was used by the Gagliano family of luthiers. Portuguese violin maker António Capela uses the Yugoslavian spruce and maple.\n\n"}
{"id": "2567281", "url": "https://en.wikipedia.org/wiki?curid=2567281", "title": "Cambridge Energy Research Associates", "text": "Cambridge Energy Research Associates\n\nCambridge Energy Research Associates (CERA) is a consulting company in the United States that specializes in advising governments and private companies on energy markets, geopolitics, industry trends, and strategy. CERA has research and consulting staff across the globe and covers the oil, gas, power, and coal markets worldwide.\n\nThe company was founded in 1983 by Pulitzer Prize winning author Daniel Yergin, James Rosenfield and Joseph Stanislaw. Comprising experts from many fields within the energy industry, CERA was acquired by IHS Energy in 2004. In 2009 it modified its name to IHS Cambridge Energy Research Associates or IHS CERA as part of the IHS brand integration in which name changes also took place for other endorsed brands under IHS, including IHS Jane's, IHS Global Insight, IHS EViews, and IHS Herold.\n\nSome of the company's largest clients include international energy companies, energy consumers, governments, utilities, technology companies, and financial institutions. Many of them attend \"CERAWeek\", the company's annual conference held at the Hilton Americas Hotel in Houston, Texas. Daily programs usually revolve around the topics of oil, natural gas, electric power, renewables, and technology.\n\nPast keynote speeches have been given by the energy secretaries and ministers of Saudi Arabia, Iraq, Mexico, Norway, and the United States. Other notable speakers have included former United States Secretary of the Treasury Robert Rubin, United States Secretary of Energy Samuel Bodman, former Colorado Senator Gary Hart; former United States Secretary of Commerce Donald L. Evans; former Federal Reserve Chairman Alan Greenspan, Microsoft CEO Steve Ballmer, and former Secretary General of OPEC Rilwanu Lukman.\n\nCERA also manages the Upstream Capital Costs Index.\n\nAs the controversy over peak oil intensified in 2006, CERA found itself on the optimistic side of predictions, forecasting that a peak would not occur before 2030, and this would not be a \"peak\" but rather an \"undulating plateau\". This opinion has been met with criticism by those who believe a peak has already occurred or is imminent. ASPO-USA described CERA's position as having a credibility problem. Chris Skrebowski considered the CERA report to be a polemic that confused stocks and flows.\n\nIn June 2008, Daniel Yergin said that markets have helped fuel a \"shortage psychology\" that the world is \"running out of oil\". He also described $120–$150 per barrel as the \"break point\" for oil, the point where demand erosion would affect the price rise.\n\nCERA researchers have predicted in September 2009 that peak demand has come and gone in the OECD world, likely sometime in 2005. This is not, however, in agreement with peak oil, which is more on the extraction and production side. CERA continues to believe that there is plenty of oil under ground. In predicting peak demand in developed countries, CERA states that long-term demand is softening as a result of demographic and socioeconomic changes in developed countries (such as the aging of OECD populations), improved transportation efficiency, and encroachment by substitutes such as biofuels and natural gas.\n\n\n"}
{"id": "2715977", "url": "https://en.wikipedia.org/wiki?curid=2715977", "title": "Chad–Cameroon Petroleum Development and Pipeline Project", "text": "Chad–Cameroon Petroleum Development and Pipeline Project\n\nThe Chad–Cameroon Petroleum Development and Pipeline Project is a controversial project to develop the production capacity of oilfields near Doba in southern Chad, and to create a pipeline to transport the oil to a floating storage and offloading vessel (FSO), anchored off the coast of Cameroon, near the city of Kribi. It is operated by ExxonMobil (40%) and also sponsored by partners forming the consortium, Petronas (35%) and Chevron (25%). The governments of Chad and Cameroon also have a combined 3% stake in the project. The project was launched on October 18, 2000, and completed in June 2003 (the official inauguration took place in October of the same year).\n\nIt was largely funded by multilateral and bilateral credit financing provided by Western governments. The International Finance Corporation, the private-sector arm of the World Bank, provided $100 million of debt-based financing, and France's export credit agency COFACE and the U.S. Export-Import Bank each provided $200 million; private lenders coordinated by the IFC provided an additional $100 million.\n\nThe original consortium of oil companies involved in the pipeline project were Exxon, Royal Dutch Shell, and Elf Aquitaine. Negotiations started in 1988, with Chad and a consortium of oil companies signing a 30-year oil concession in the southern Chad region of Doba. In 1999, Royal Dutch Shell and Elf Aquitaine dropped the project due to controversies surrounding the project and volatile oil prices. As a result, Exxon opened the project up for bid to a select few corporations and in April 2000, Petronas of Malaysia and Chevron acquired stakes in the project. Exxon then enlisted the support of the World Bank to raise support within the international community. The World Bank agreed on the condition that certain environmental and social standards were enforced both in Chad and Cameroon, and that the revenues be put towards improving social and economic conditions.\n\nSince 1990, President Idriss Deby has been the head of government in Chad, where presidential elections are held every five years. Chad's economy is dependent mainly on agriculture and livestock. It heavily relies on foreign assistance and foreign capital, as well as international investment projects. Because of Chad's landlocked borders, its economy struggles from a lack of resources and instability between rebel groups. The country also serves as a host to thousands of refugees from neighboring Sudan and Central African Republic, as well as many victims of human trafficking. Oil exploitation is estimated at a potential 1.5 billion barrels, and Chinese companies are currently developing an oil refinery and another pipeline in the country.\n\nThe Tchad Oil Transportation Company (TOTCO) manages the pipeline within Chad that is owned by the country. TOTCO is incorporated in Tchad, and is a joint venture between Chad and the Upstream Consortium.\n\nThe Republic of Cameroon has been ruled by President Paul Biya since November 6, 1982. A relatively peaceful and stable nation, Cameroon is a republic with a multiparty presidential regime. Although there are presidential elections every seven years, there are no term limits. A climate of intense corruption has created an unequal distribution of wealth and poor conditions for domestic and international investments.\n\nThe portion of the pipeline owned by Cameroon is managed by the Cameroon Oil Transportation Company (COTCO). This company is incorporated in Cameroon, and is a joint venture between the governments of Cameroon and Chad, and the Upstream Consortium, an independent monitoring institution.\n\nThe World Bank's support was very important for the Consortium of oil companies, as they believed they needed the support of the Bank in order for the project to succeed.\n\nWhile the project's private sponsors, the Upstream Consortium, provided about 95% ($2.2 billion) of the financing for the pipeline, the World Bank also contributed through debt financing. The International Finance Corporation (IFC) provided a loan of about $100 million, $85.8 million of which went to COTCO and $14.2 million of which went to TOTCO, and also helped secure an additional $300 million in private commercial lending. The International Bank for Reconstruction and Development (IBRD) provided $92.9 million to Chad ($39.5 million) and Cameroon ($53.4 million) to finance the joint-venture pipeline companies. Last of the World Bank-provided financing was given through the European Investment Bank, which provided $46.6 million to finance Cameroon and Chad's equity in COTCO and TOTCO.\n\nIncluded in plans for the project was a revenue management law developed by the World Bank. This separated the oil revenues given to Chad into four required areas: a Future Generations Fund, health, education and other development projects, a fund to compensate the Doba region of Chad from where the oil was extracted and government reserves. The revenue management law also created the Petroleum Revenue Oversight Committee, which oversees how the oil revenues are spent, and includes members of both the Chadian government and civil society.\n\nOn September 5, 2008, Chad fully prepaid both the IBRD and IDA components of the World Bank loan totaling $65.7 million from its \"national coffers swollen by more than $1 billion a year in oil revenues\". This ended its involvement in the project. The World Bank noted that, \"over the years, Chad failed to comply with key requirements of this agreement\", including devoting a substantial portion of the oil revenues to poverty reduction programs, and thus it \"concluded that it could not continue to support this project under these circumstances\".\n\nThe pipeline project has been affected by persistent charges and fears about corruption and the diversion of revenues—ostensibly intended for poverty reduction—towards arms purchases, particularly by the regime of Chadian President Idriss Déby. Delphine Djiraibe, a human rights attorney who became one of the leading critics of these arms purchases, was later awarded the 2004 Robert F. Kennedy Human Rights Award for her work against the project.\n\nOpposition leader and parliamentarian Ngarledjy Yorongar of the Front of Action Forces for the Republic (FAR) accused National Assembly President Wadal Abdelkader Kamougue of taking a bribe from Elf, then a partner in the project, in 1997. Yorongar was stripped of his parliamentary immunity and detained for nine months. In November 2000, the World Bank announced that $4 million of a $25 million signing bonus from the oil companies was spent by the Chadian government on weapons. The World Bank required tight restrictions on oil revenues as a condition of its loans. In January 2006, Chad moved to unilaterally increase the portion of oil revenues going to its general fund from 15 to 30 percent.\n\nOn 28 August 2006, President Déby ordered Chevron and Petronas to quit the country.\n\nIn Chad and especially Cameroon, through which the pipeline stretches 890 km of the total 1,070 km, there have been claims of adverse effects of the construction and maintenance of the pipeline on the indigenous communities and environment. One of the largest areas affected is in the coastal Cameroonian town of Kribi. Located off the coast of Kribi is the export terminal facility. There has been much controversy regarding the alleged degradation the coastal reefs during construction. This has not only impacted the underwater habitat, but also the livelihoods of the local people who depend on fishing as their main source of income.\n\nSince completion of the pipeline in 2003, there have been two known oil leaks at the transfer site off the shore of Cameroon. The first occurred on January 15, 2007. Representatives for COTCO claimed that the leak was contained within a few hours and that the amount of spilled was not sufficient to cause any harm, though local fishermen did claim to have seen traces of the oil ashore. The second oil spill was on April 22, 2010, at the same site. COTCO stated that the leaked oil amounted to less than five barrels. Cameroonian NGOs Relufa and Centre pour l'Environnement et le Développement have brought to light the inefficiencies of the oil-spill preparedness plan, as well as the lack of communication between COTCO and the surrounding communities.\n\nNon-governmental organizations have played a large role in mediating the concerns of the international community with the needs of the indigenous communities of Chad and Cameroon. Even before construction commenced on the pipeline project in 2000, international and local NGOs were monitoring the situation and meeting with representatives from the World Bank and the Consortium of gasoline companies. After the completion of the project, NGOs were also very involved with documentation of any problems and worked in conjunction with external independent monitoring agencies.\n\nIn 2005, under the direction of the World Bank's International Advisory Group, a group of stakeholders including COTCO, the CPSP and multiple NGOs created a platform under which complaints registered with the World Bank by organizations and individuals could be resolved. According to FOCARFE (Fondation Camerounaise d'Actions Rationalisees et de Formation sur l'Environnement), more than 300 civil society complaints existed by the close of construction in 2003.\n\nIn November 2006, the stakeholders involved in the project came together to discuss their views, main issues and concerns for a Forum of Information on the Chad-Cameroon Pipeline Project. The stakeholders involved were the Cameroon Oil Transportation Company (COTCO), the Comité de Pilotage et de Suivi des Pipelines (the Pipelines Steering and Monitoring Committee) and a group of four Cameroonian NGOs: CED (Centre pour l'Environnement et le Developpement), RELUFA (Reseau de Lutte contra le Faim), CARFAD (African Center for Applied Forestry Research and Development) and FOCARFE (Fondation Camerounaise d'Actions Rationalisees et de Formation sur l'Environnement). The meeting was held to discuss a wide array of topics, including the monitoring of the pipeline activities, environmental and social compensation plans, CAPECE's capacity building objectives and the involvement of NGOs.\n\nFriends of the Earth is a transnational grassroots environmental network. The organization brings together and mobilizes social and environmental advocacy groups from all over the world to rally behind certain issues. While FOE has been involved in documentation and monitoring of pipeline project since its construction, the organization more recently developed a report in 2008 condemning a World Bank initiative for \"New Climate Funds\". Along with four other advocacy organizations, FOE stated that the World Bank had repeatedly engaged in projects, such as the Chad-Cameroon pipeline, that actually negatively affected the environment and only added to pollution.\nThe Center for Environment and Development is a Cameroon-based NGO founded and run by native lawyer Samuel Nguiffo. The CED's main purpose is to advocate and campaign against the \"liquidation of the regions forests for short-term profit\". Certain exploitations within the Cameroon region include logging, hunting for bushmeat, mining for natural resources and the construction of the Chad-Cameroon pipeline. The CED works to inform local communities about their rights to land and community forest concessions, as well as constant documentation and publications to educate the international community.\nCatholic Relief Services, an international nonprofit humanitarian organization, has been one of the key watchdogs in the pipeline project, even before construction was completed. In a statement made in October 2003, they stated their concerns for the people and environment of both Chad and Cameroon, and anticipated negative effects of the pipeline for the future. One of their main concerns for the project was the potential mismanagement of profit funds by Chad and Cameroon, as well as the ineffectiveness of policies mandated by the World Bank.\n\nThe Cameroon Chad Pipeline Monitoring Project is an initiative created by CRS that supports the efforts of Cameroonian NGOs as they advocate for proper use of profits from the pipeline, as well as the communities and environment surrounding the pipeline. One of the main efforts of the Catholic Relief Services has been to review and correct compensation packages received by those located along the pipeline, as well as advocating for fair salaries for local workers contracted by the oil companies.\n\n\n"}
{"id": "216159", "url": "https://en.wikipedia.org/wiki?curid=216159", "title": "Chemical waste", "text": "Chemical waste\n\nChemical waste is a waste that is made from harmful chemicals (mostly produced by large factories). Chemical waste may fall under regulations such as COSHH in the United Kingdom, or the Clean Water Act and Resource Conservation and Recovery Act in the United States. In the U.S., the Environmental Protection Agency (EPA) and the Occupational Safety and Health Administration (OSHA), as well as state and local regulations also regulate chemical use and disposal. Chemical waste may or may not be classed as hazardous waste. A chemical hazardous waste is a solid, liquid, or gaseous material that displays either a “Hazardous Characteristic” or is specifically “listed” by name as a hazardous waste. There are four characteristics chemical wastes may have to be considered as hazardous. These are Ignitability, Corrosivity, Reactivity, and Toxicity. This type of hazardous waste must be categorized as to its identity, constituents, and hazards so that it may be safely handled and managed. Chemical waste is a broad term and encompasses many types of materials. Consult the Material Safety Data Sheet (MSDS), Product Data Sheet or Label for a list of constituents. These sources should state whether this chemical waste is a waste that needs special disposal.\n\nIn the laboratory, chemical wastes are usually segregated on-site into appropriate waste carboys, and disposed by a specialist contractor in order to meet safety, health, and legislative requirements.\n\nInnocuous aqueous waste (such as solutions of sodium chloride) may be poured down the sink. Some chemicals are washed down with excess water. This includes: concentrated and dilute acids and alkalis, harmless soluble inorganic salts (all drying agents), alcohols containing salts, hypochlorite solutions, fine (tlc grade) silica and alumina. Aqueous waste containing toxic compounds are collected separately.\n\nWaste elemental mercury, spent acids and bases may be collected separately for recycling.\n\nWaste organic solvents are separated into chlorinated and non-chlorinated solvent waste. Chlorinated solvent waste is usually incinerated at high temperature to minimize the formation of dioxins. Non-chlorinated solvent waste can be burned for energy recovery.\n\nIn contrast to this, chemical materials on the \"Red List\" should never be washed down a drain. This list includes: compounds with transitional metals, biocides, cyanides, mineral oils and hydrocarbons, poisonous organosilicon compounds, metal phosphides, phosphorus element, and fluorides and nitrites.\n\nMoreover, the Environmental Protection Agency (EPA) prohibits disposing certain materials down any UVM drain. Including flammable liquids, liquids capable of causing damage to wastewater facilities (this can be determined by the pH), highly viscous materials capable of causing an obstruction in the wastewater system, radioactive materials, materials that have or create a strong odor, wastewater capable of significantly raising the temperature of the system, and pharmaceuticals or endocrine disruptors.\n\nBroken glassware are usually collected in plastic-lined cardboard boxes for landfilling. Due to contamination, they are usually not suitable for recycling. Similarly, used hypodermic needles are collected as sharps and are incinerated as medical waste.\n\nMany chemicals may react adversely when combined. It is recommended that incompatible chemicals be stored in separate areas of the lab.\n\nAcids should be separated from alkalis, metals, cyanides, sulfides, azides, phosphides, and oxidizers. The reason being, when combined acids with these type of compounds, violent exothermic reaction can occur possibly causing flammable gas, and in some cases explosions.\n\nOxidizers should be separated from acids, organic materials, metals, reducing agents, and ammonia. This is because when combined oxidizers with these type of compounds, inflammable, and sometimes toxic compounds can occur.\n\nWhen disposing hazardous laboratory chemical waste, chemical compatibility must be considered. For safe disposal, the container must be chemically compatible with the material it will hold. Chemicals must not react with, weaken, or dissolve the container or lid. Acids or bases should not be stored in metal. Hydrofluoric acid should not store in glass. Gasoline (solvents) should not store or transport in lightweight polyethylene containers such as milk jugs. Moreover, the Chemical Compatibility Guidelines should be considered for more detailed information.\n\nPackaging, labelling, storage are the three requirements for disposing chemical waste.\n\nFor packaging, chemical liquid waste containers should only be filled up to 75% capacity to allow for vapour expansion and to reduce potential spills which could occur from moving overfilled containers. Container material must be compatible with the stored hazardous waste. Finally, wastes must not be packaged in containers that improperly identify other nonexisting hazards.\n\nIn addition to the general packaging requirements mentioned above, incompatible materials should never be mixed together in a single container. Wastes must be stored in containers compatible with the chemicals stored as mentioned in the container compatibility section. Solvent safety cans should be used to collect and temporarily store large volumes (10–20 litres) of flammable organic waste solvents, precipitates, solids or other non-fluid wastes should not be mixed into safety cans.\n\nLabel all containers with the group name from the chemical waste category and an itemized list of the contents. All chemicals or anything contaminated with chemicals posing a significant hazard. All waste must be appropriately packaged.\n\nWhen storing chemical wastes, the containers must be in good condition and should remain closed unless waste is being added. Hazardous waste must be stored safely prior to removal from the laboratory and should not be allowed to accumulate. Container should be sturdy and leakproof, also has to be labeled. All liquid waste must be stored in leakproof containers with a screw- top or other secure lid. Snap caps, mis-sized caps, parafilm and other loose fitting lids are not acceptable. If necessary, transfer waste material to a container that can be securely closed. Keep waste containers closed except when adding waste. Secondary containment should be in place to capture spills and leaks from the primary container, segregate incompatible hazardous wastes, such as acids and bases.\n\nTOXMAP is a Geographic Information System (GIS) from the Division of Specialized Information Services of the United States National Library of Medicine (NLM) that uses maps of the United States to help users visually explore data from the United States Environmental Protection Agency's (EPA) Toxics Release Inventory and Superfund Basic Research Programs. TOXMAP is a resource funded by the US Federal Government. TOXMAP's chemical and environmental health information is taken from NLM's Toxicology Data Network (TOXNET) and PubMed, and from other \nauthoritative sources.\n\nChemical waste in our oceans is becoming a major issue for marine life. There have been many studies conducted to try and prove the effects of chemicals in our oceans. In Canada, many of the studies concentrated on the Atlantic provinces, where fishing and aquaculture are an important part of the economy. \nIn New Brunswick, a study was done on the sea urchin in an attempt to identify the effects of toxic and chemical waste on life beneath the ocean, specifically the waste from salmon farms. Sea urchins were used to check the levels of metals in the environment. It is advantageous to use green sea urchins, Strongylocentrotus droebachiensis, because they are widely distributed, abundant in many locations, and easily accessible. By investigating the concentrations of metals in the green sea urchins, the impacts of chemicals from salmon aquaculture activity could be assessed and detected. Samples were taken at 25-metre intervals along a transect in the direction of the main tidal flow. The study found that there was impacts to at least 75 m based on the intestine metal concentrations. So based on this study it is clear that the metals are contaminating the ocean and negatively affecting aquatic life.\n\nAnother issue regarding chemical waste is the potential risk of surface and groundwater contamination by the heavy metals and radionuclides leached from uranium waste-rock piles (UWRP) A Radionuclide is an atom that has excess nuclear energy, making it unstable. Uranium waste-rock piles refers to Uranium mining, which is the process of extraction of uranium ore from the ground. An example of such threats is in Saskatchewan, Uranium mining and ore processing (milling) can pose a threat to the environment. In open pit mining, large amounts of materials are excavated and disposed off in waste-rock piles. Waste-rock piles from the Uranium mining industry can contain several heavy metals and contaminants that may become mobile under certain conditions. Environmental contaminants may include acid mine drainage, higher concentrations of radionuclides, and non-radioactive metals/metalloids (i.e. As, Mo, Ni, Cu, Zn).\n\nThe leachability of heavy metals and radionuclide from UWRP plays a significant role in determining their potential environmental risks to surrounding surface and groundwater. Substantial differences in the solid-phase partitioning and chemical leachability of Ni and U were observed in the investigated UWRP lithological materials and background organic-rich lake sediment. For Instance, in the uranium-mining district of Northern Saskatchewan, Canada, the sequential extraction results showed that a significant amount of Ni (Nickel) was present in the non-labile residual fraction, while Uranium was mostly distributed in the moderately labile fractions. Although Nickel was much less labile than Uranium, the observed Nickel exceeded Uranium concentrations in leaching]]. The observed Nickel and Uranium concentrations were relatively high in the underlying organic-rich lake sediment. Expressed as the percentage of total metal content, potential leachability decreased in the order U > Ni. Data suggest that these elements could potentially migrate to the water table below the UWRP. Detailed information regarding the solid-phase distribution of contaminants in the UWRP is critical to understand the potential for their environmental transport and mobility.\n\nImage of Uranium risk map may be found here: \nhttp://www.env.gov.nl.ca/env/waterres/cycle/groundwater/well/uranium.pdf\n\n\n"}
{"id": "729584", "url": "https://en.wikipedia.org/wiki?curid=729584", "title": "City car", "text": "City car\n\nA city car (also known as urban car or a mini) is a small car designed to be used primarily in urban areas and conurbations.\n\nThe term is used along with other terms for small cars including \"subcompact\" in North America. The Euro NCAP calls all small cars \"superminis\". The European Commission refers to \"A-segment\" (Utility/city class: entry level small passenger car). In Japan, the \"kei\" car is a specific type of small car. The term microcar is loosely used for very small cars, some of which overlap city car classification.\n\nThe original concept for the city car came about as a result of the growing market for entry level vehicles in the 1920s and 1930s. The Great Depression caused the market for large, luxurious vehicles to collapse. As a result, manufacturers had no choice but to build small, cheap vehicles that people could afford, similar to early 20th century \"runabouts\". However, these vehicles were not specifically branded as vehicles to use in cities or congested areas. They were purely for use as people's cars; cars designed to be cheap, sell a large number of units, and put people who had not owned cars on wheels. A number of these small, cheap cars were sold before the Second World War, including the 1920s Austin 7, 1930s Fiat 500 \"Topolino\" and the 1940s Crosley. \n\nAfter the Second World War, a number of manufacturers introduced microcars, which contained many of the elements that City Cars would come to be known for, such as small size and good maneuverability. Microcars were also among the first cars to marketed on their good parking ability; for example, Iso, and later BMW used the Isetta's front hinged door as a marketing advantage, saying you could pull into a parallel parking spot nose forward and exit onto the sidewalk using the door.\n\nSome of these early city cars included the Bond Minicar and AC Petite in Britain; the Iso Isetta in Italy in 1953; the Fulda, Messerschmitt Kabinenroller, and Brütsh in Germany in 1954 (all two-seaters with a Fulda and Sachs two-stroke engine); the Goggomobil, and the Dornier-designed Zündapp Janus, which placed passengers back to back, and featured two front hinged doors similar to the Isetta's. However, these early microcars weren't true city cars; most were designed, built, and sold cheaply, mainly for poor customers in war ravaged Europe. With the exception of a few examples, these were also not designed for specific use in cities.\nAs the European economy improved, Fiat launched the new 500 in 1957 and in 1959, BMC would introduce its well-known city car, the Mini. DAF joined Italian motoring magazine Quattroruote and Turin coachbuilder OSI to produce the DAF - OSI Citi Car, which used Daffodil mechanicals and interior, and was one of the first cars with a continuously variable transmission. There were other new projects from 1969 through 1972. In 1972, Daihatsu proffered an electric model, while Toyota showed the \"Town Spider\", with a choice of petrol or electric power, and General Motors displayed three two-seaters, one electric, one gasoline, and one hybrid. The GM products were capable of only in 15 seconds and maximum, not good enough for normal city driving, nor could they meet the safety regulations which would be introduced in 1974.\n\nWhile many of these cars can be considered city cars today, these cars have been replaced by larger cars with each passing generation. Exceptions are the smaller Fiats, especially the 1957 Fiat 500 and 126. They were in the region of in length, but had seating for four people, putting them outside the microcar category. In the 1970s the Citroën LNA and related three-door Peugeot 104 offered compact city sized cars of around 3.3 m, and the Fiat Panda followed in 1980.\n\nIn Japan, regulations defining kei cars were established on 8 July 1949. Production of compliant vehicles started around 1955–1958 by Daihatsu, Mitsubishi, Subaru, and Suzuki.\n\nIn the late 1980s, superminis had physically grown so much that many buyers wanted even smaller four-seat cars. In Japan, buyers had a wide selection to choose from in passenger cars, microvan and kei trucks, such as the Honda Today and the Honda Acty, Subaru Sambar and Subaru Vivio, Daihatsu Atrai and Daihatsu Mira, Mitsubishi Minica and Mitsubishi Minicab, and the Suzuki Fronte and Suzuki Wagon R. In Europe a number of small cars were launched, including the Lancia Y10 ( long) in 1985 and the Renault Twingo in 1993, which featured a MPV-like design and interior room, despite its size and height long and tall. Combined with an original exterior and interior design, it quickly became a best-seller, although it was not officially imported to right-hand drive markets including the United Kingdom. However, the Fiat Cinquecento, launched towards the end of 1991, was made for right-hand drive markets, and was hugely popular.\n\nIn 1996, the Ford Ka was presented with its radical New Edge design. Its egg-shaped body did not leave much room in the rear seats, but many customers did not need them and preferred the Ka over more conservative designs. It was especially popular with British buyers.\n\nDuring the second half of the 1990s, South Korean brands Daewoo and Hyundai introduced their city car entries, both for the Asian and European markets. The Hyundai Atos, launched in 1997, was long and high, which was much taller than any European models (usually under ) and provided considerable interior space. Its boxy shape provoked mixed reactions. The Daewoo Matiz followed in 1998 with a Giorgetto Giugiaro design and a moderate height (). These Korean city cars were much cheaper than most of the European models, especially the Opel/Vauxhall Agila (2000), a badge engineered Suzuki Wagon R, Volkswagen Lupo (1999) and the two-seater Smart Fortwo (1998). By the year 2000, city cars had increased massively in popularity with buyers in the space of a decade.\n\nAs small family cars and superminis grew in the 1990s to the 2000s, so did city cars. After some new superminis grew to be over long (like the Ford Fiesta, Opel Corsa, the SEAT Ibiza and the Volkswagen Polo), some automakers developed city car models more than long. The first of these models was the Nissan Micra (2002), which is long and smaller than many superminis of the late 1990s. Other cars are the Citroën C2, Suzuki Swift, Smart Forfour, Tata Indica, and Peugeot 1007 (the last one which can also be labelled as a mini MPV). Fiat launched their new Fiat 500 in 2007, somewhat bigger than the first 500, but still only long.\n\nIn addition, since the 2000s there has been a return of smaller city cars with lengths well under 3,200 mm such as the Tata Nano and Toyota iQ. Slightly larger, but still around 3,300 mm city cars released in the 2010s include the electric Bolloré Bluecar and Mitsubishi i-MiEV, and traditionally powered BYD F0, Citroën C1 and related Toyota Aygo.\n\n"}
{"id": "46613633", "url": "https://en.wikipedia.org/wiki?curid=46613633", "title": "CoGeNT", "text": "CoGeNT\n\nThe CoGeNT experiment has searched for dark matter. It uses a single germanium crystal (~100 grams) as a cryogenic detector for WIMP particles. CoGeNT has operated in the Soudan Underground Laboratory since 2009.\n\nTheir first announcement was an excess of events recorded after 56 days Juan Collar, who presented the results to a conference at the University of California, was quoted: \"If it's real, we're looking at a very beautiful dark-matter signal\".\n\nThis signal conflicts with other searches that have failed to find any evidence, such as XENON and LUX but appears to confirm results from DAMA.\n\nThey observed an annual modulation in the event rate that could indicate light dark matter.\n\nThe annual modulation has continued to be seen in 3 years of data.\n\nHowever more recent work has shown that the excess of events attributed to a tentative dark matter signal was in fact due to an underestimated background from surface events. After accounting for this background there is no evidence for a signal in data from the CoGeNT experiment and no tension with null results from other experiments.\n\n"}
{"id": "873307", "url": "https://en.wikipedia.org/wiki?curid=873307", "title": "Compression lift", "text": "Compression lift\n\nIn aerodynamics, compression lift refers to an aircraft that uses shock waves generated by its own supersonic flight to generate lift. This can lead to dramatic improvements in lift for supersonic/hypersonic aircraft. Clarence Syvertson and Alfred J. Eggers discovered this phenomenon in 1956 as they analyzed abnormalities at the reentry of nuclear warheads.\n\nThe basic concept of compression lift is well known; \"planing\" boats reduce drag by \"surfing\" on their own bow wake in exactly the same fashion. Using this effect in aircraft is more difficult, however, because the \"wake\" is not generated until supersonic speeds are reached, and is highly angled. Aircraft have to be carefully shaped to take full advantage of this effect. In addition the angle of the shock waves varies greatly with speed, making it even more difficult to design a craft that gains significant lift over a wide range of speeds.\n\nTo date the only potential production aircraft that used compression lift has been the XB-70 in the 1960s, although with the cancellation of the program after only two prototypes had been built, it ended up being a testbed only. The compression lift decreased the induced drag of the XB-70 about 30%.\n\nHigher speed designs using compression lift, waveriders, remain an interesting possibility for hypersonic vehicle designs, although only testbed models have been flown. The Boeing X-51 (WaveRider) also uses compression lift.\n\n"}
{"id": "48583431", "url": "https://en.wikipedia.org/wiki?curid=48583431", "title": "Conradson carbon residue", "text": "Conradson carbon residue\n\nConradson carbon residue, commonly known as \"Concarbon\" or \"CCR\" is a laboratory test used to provide an indication of the coke-forming tendencies of an oil. Quantitatively, the test measures the amount of carbonaceous residue remaining after the oil's evaporation and pyrolysis. In general, the test is applicable to petroleum products which are relatively non-volatile, and which decompose on distillation at atmospheric pressure. The phrase \"Conradson carbon residue\" and its common names can refer to either the test or the numerical value obtained from it.\n\nA quantity of sample is weighed, placed in a crucible, and subjected to destructive distillation. During a fixed period of severe heating, the residue undergoes cracking and coking reactions . At the termination of the heating period, the crucible containing the carbonaceous residue is cooled in a desiccator and weighed. The residue remaining is calculated as a percentage of the original sample, and reported as Conradson carbon residue.\n\n\n\n"}
{"id": "49875825", "url": "https://en.wikipedia.org/wiki?curid=49875825", "title": "Cophonicity", "text": "Cophonicity\n\nGiven two interacting atoms A and B, the cophonicity () of the A-B atomic pair is a measure of the overlap of the A and B contributions to a specific range of vibrational frequencies. In the field of condensed matter physics, cophonicity is a metric aimed at the parametrization of the dynamical interactions in terms of the atomic types forming the A-B pair. In connection with other electronic and structural descriptors, such as the covalency metric or the distortion mode analysis from group theory, the A-B pair cophonicity is a guide to properly select either A or B atomic species to tune specific vibrational frequencies of a given system. The cophonicity metric has been originally designed for the study of the atomic motions in transition metal dichalcogenides, but its formulation is general and can be applied to any kind of system, irrespective of the chemical composition and stoichiometry.\n\nConsidering the phonon density of states (pDOS) formula_1 in the first Brillouin zone, the center mass formula_2 of the atom-projected pDOS formula_3 of an atom A is defined as\n\nformula_4\n\nwhere formula_3 is the contribution of the atom A to formula_1 and formula_7; the total density of states of the solid is formula_1, defined as\n\nformula_9\n\nand obtained by summing over all atoms X of the unit cell. The integration interval formula_10 is chosen in such a way that it encompasses all the phonon states relevant for the specific study. The integral at the denominator of the definition of formula_2 is the contribution of the atom A to the states in the frequency range formula_10; we call such quantity the \"phonicity of the A atom\" in that specific frequency range. The phonicity of an atom then represents the amount of phonon states that such atom contributes to form; in this respect, it can be regarded as the phonon counterpart of the atomic valence, that is the number of electrons with which an atom participates to form the electronic states of the system, counted as the integral of the atom-projected electronic density of states.\n\nLet's consider a generic A-B atomic pair. The relative position formula_13 of the center mass of formula_3 with respect to the center mass of formula_15 is given as\n\nformula_16\n\nwhich is specified in the same units of the frequency formula_17. A positive (negative) sign of formula_13 indicates that the A (B) atom contributes more to the high frequency modes of the specified range. The smaller is formula_19, the higher is the mixing of the A and B contributions to the frequency band, and the two atoms have the same weight in the determination of the modes specific of the considered energy range. We define the quantity formula_13 as the \"cophonicity of the A-B atomic pair\", in analogy with the A-B bond covalency definition formulated in terms of atomic contributions to the electronic density of states.\n\nCophonicity is then a characteristic of a specific atomic pair found in a system. According to the metric introduced above, formula_21 would indicate a \"perfect cophonocity\", that is an equal participation of both atomic species to the formation of the phonon states in the considered range of energies.\n\nThe comparison between two cophonicity values, obtained considering distinct atomic couples embedded in two structures with different connectivity, will provide meaningful information under certain conditions. For example, formula_22 in molybdenum disulfide bulk and nanoclusters, despite calculated in the same frequency range, could refer to distinct sets of vibrational modes; this is due to the distinct topologies that, in turn, determine different electronic environments. In this case, any change of the cophonicity by means of substitutional defects could lead to non-correlated changes of the vibrational frequencies. However, if the two structures with distinct topologies are connected by continuous geometric transformations, also the electronic environment of the atomic pairs, thus the vibrational frequencies and the associated modes, are related by such continuous transformations; in this case, the cophonicity of a specific atomic pair in one structure can be mapped into the cophonicity of the second one, granting the transferability of the definition, hence the reliability of the cophonicity comparison.\n"}
{"id": "15185925", "url": "https://en.wikipedia.org/wiki?curid=15185925", "title": "Danelectro Commando", "text": "Danelectro Commando\n\nThe Danelectro Commando is a combo guitar amplifier manufactured by Danelectro from 1954 to 1960. There is some evidence that it may be one of the many different amplifiers used by Little Walter.\n\nThe circuitry of this 30-watt amplifier is typical of its day, with a 5Y3 rectifier tube, one 12AX7 preamp tube, one 12AX7 for phase inversion, a 6SN7 additional gain stage, four 6V6 power tubes, and a 6SJ7 (1954-1958) or a 6AU6 (1958-1960) for the built-in vibrato unit. What makes it unique is the cabinet. Employing a \"suitcase\" design, it contains eight 8-inch Rola 10610 alnicos speakers wired series / parallel in two arrays one in either side of the suitcase, with the amplifier and controls located at the top and bottom of one side. The suitcase opens out so that both sides face forwards.\n\nWhen closed, the amplifier is 22\" × 22\" x 10\". It weighs 38 lbs, relatively lightweight for a valve combo amplifier.\n\nThe same amplifier was made by Danelectro under the following names: Danelectro Commando Model 88, Silvertone 1337, Montgomery Ward 35JDR8419, Montgomery Ward 55JDR8437, Montgomery Ward Airline 85GDR8518 where the first two letters of the Montgomery Ward code backwards represent the first year of design/production (but with the 35JDR8419 only being available in 1954). The schematic is readily available on the web under the name Montgomery Ward GDR 8517a.\n\n"}
{"id": "46417342", "url": "https://en.wikipedia.org/wiki?curid=46417342", "title": "Dugi Archeological Site", "text": "Dugi Archeological Site\n\nThe Dugi Archeological Site is a prehistoric latte stone site on the north side of Rota Island in the Northern Mariana Islands. The site is a rare inland site that survived the intensive sugar cane development introduced by the Japanese during the South Pacific Mandate period of the 1920s and 1930s. It consists of sixteen deteriorated latte stone structures on three high terraces. Some of the latte stones have fallen over and others are missing features normally found at these sites.\n\nThe site was listed on the National Register of Historic Places in 1985.\n\n"}
{"id": "27902014", "url": "https://en.wikipedia.org/wiki?curid=27902014", "title": "EcoAid", "text": "EcoAid\n\nEcoAid is an Arizona-based, international business that provides greenhouse gas emissions consulting, carbon market education and carbon offsets.\n\nEcoAid provides consulting, education, and offset services. EcoAid manages greenhouse gas emissions for their clients. Major retailers, the Securities and Exchange Commission (SEC), and the Environmental Protection Agency (EPA) are creating requirements standards regarding Greenhouse Gas Emissions Reporting.\n\nMajor retailers, the SEC, and the EPA are demanding that organizations report their greenhouse gas emissions.\n\nEcoAid currently offers an online education program, the Carbon Professional School, which teaches the science, policy and business aspects of the Carbon Market.\n\nEcoAid provides carbon offsets from a variety of projects that have been verified through the Climate Action Reserve, the Voluntary Carbon Standard, or the Chicago Climate Exchange.\n\n"}
{"id": "15465576", "url": "https://en.wikipedia.org/wiki?curid=15465576", "title": "Electricity billing in the UK", "text": "Electricity billing in the UK\n\nIn the UK, an electricity supplier is a retailer of electricity. For each supply point the supplier has to pay the various costs of transmission, distribution, meter operation, data collection, tax etc. The supplier then adds in energy costs and the supplier's own charge.\n\nMSP kWh is the amount of electricity consumed at the 'meter supply point', which is the customer's meter. GSP kWh is obtained by multiplying the MSP kWh by the Line Loss Factor (LLF, a figure > 1) to include the amount of electricity lost when it is conducted through the distribution network, from the 'grid supply point' to the customer's meter. Some kWh elements of the bill are charged at MSP and some at GSP. The LLF for a particular supply depends on the DNO and the supply's characteristics and the time and date (day of week, season etc.).\n\nThe consumer pays the supplier according to an agreed tariff, possibly including pass-through costs. A pass-through cost is a cost that is charged to the energy supplier, but is then \"passed through\" directly to the consumer.\n\nTransmission charges, known as \"Transmission Network Use of System\" (TNUoS), are paid to National Grid to cover the expense of running the grid. The charge is calculated annually using the TRIAD method for large levels of demand, or based on usage between 4pm and 7pm for smaller demand levels.\n\nResidual Cashflow Reallocation Cashflow (RCRC), also known as the 'beer fund', is the net remainder of Balancing & Settlement Code (BSC) Trading Charges for a given half-hour, which is payable to (in the case of a surplus) or by (in the case of a deficit) Trading Parties based on their market share of energy volume. These Trading Charges consist of:\n\n\nAs Information Imbalance Charges are always zero, and System Operator BM Cashflow nets with Non-Delivery Charges and BM Unit Cashflow to zero, RCRC is effectively the net of Imbalance Cashflows.\n\nThe distribution charges, known as the \"distribution use of system\" (DUoS) charges, are paid to suppliers and passed on to the distribution network operator (DNO) on whose network the meter point is located. The charges cover:\n\nSupply availability, otherwise known as \"supply capacity\" or \"kVA\", if represented by its measured units, is the maximum kVA power allowed for a particular supply in a particular network and is set before the supply is energised. It is a figure agreed between the consumer and the supplier (set to a level required by the consumer in almost all cases except where power distribution may be physically limited to the supply) at the start of the contract. This supply availability is charged for every month, in effect as a standing charge, despite the fact that the maximum demand recorded in the month may be lower. If the kVA supply availability figure is exceeded (breached, in effect) by the value of the measured monthly maximum demand, also in kVA for this purpose, the higher figure of kVA from the maximum demand may be charged instead of the supply capacity. The new elevated kVA charge figure (or the supplier's nearest higher capacity band figure, if capacity is only allowed in banded levels 50 kVA apart for instance) may stay as the chargeable figure for twelve months depending on the electricity distribution area. This can cause temporary unnecessary high billing, as if a penalty, if the breach was avoidable. Alternatively, the capacity charge can just return to the original availability figure in the subsequent month's bill. Determining the correct capacity figure to allow for the maximum demand required for the supply can be a fine judgement if the capacity charge is to be kept to a minimum, and vigilance of maximum demand and efforts to keep the power demand lower than the agreed capacity can be required to avoid triggering a higher capacity charge during the contract.\n\n• Unit rates – these rates are split into 3 time periods; Red, Amber and Green. These charges vary per distribution company. The chart below shows the applicable time bands for each company.\n\nThis also varies with each distribution area, and is charged if the power factor for a supply is deemed too low.\n\nThe fixed charge is in units of pence / MPAN / day.\n\nNew DUoS charges will come into effect on 1 April 2018 under a proposal known as DCP228. Green and amber rates will rise and red rates will fall.\n\nThe Climate Change Levy is a p/kWh tax on certain electricity use. Exempt supplies include domestic supplies and supplies using less than the \"de minimis\" threshold of 1,000 kWh / month.\n\nSuppliers meet the Renewables Obligation by submitting a certain number of Renewable Obligation Certificates (ROCs) each year to Ofgem, which demonstrates that the certified electricity has come from a renewable source. If a supplier is unable to produce the required number of ROCs, they must pay an equivalent cash amount, the 'cash out price'.\n\nEnergy charges pay per kWh (kilo watt hour).\n\nThe data collection charge is a fee paid to the data collector for determining the energy consumption of the supply.\n\nThe meter operation charge is a fee paid to the meter operator for installing and maintaining the meter.\n\nVAT is payable at the standard rate unless the supply meets certain conditions (e.g. domestic supplies, or supplies that use less than 1000 kWh per month) in which case they are charged at the reduced rate of 5%.\n\nFor a non-half-hourly supply, the NHHDC sets the change of supplier (CoS) read from a meter read, a customer read or a deemed read. A deemed read is one estimated by the NHHDC based on any previous or subsequent readings. A CoS read can be disputed up to final reconciliation. Final reconciliation is fourteen months afterwards. If a normal read comes in after final reconciliation that is lower than the CoS read, the new supplier should credit the customer.\n"}
{"id": "16165115", "url": "https://en.wikipedia.org/wiki?curid=16165115", "title": "Energy and Utilities Board", "text": "Energy and Utilities Board\n\nThe Energy and Utilities Board (EUB) was the governing body of the energy industry in the province of Alberta, Canada. Previously known as the Alberta Energy and Utilities Board (AEUB), the EUB was reorganized on 1 January 2008 into two separate regulatory bodies:\n\n\n1995: The Alberta Energy and Utilities Board (EUB) was created\n\nThe Public Utilities Board and the Energy Resources and Conservation Board (previously the Petroleum and Natural Gas Conservation Board) merged to create the Alberta Energy and Utilities Board (EUB) in order to provide a more streamlined and efficient regulatory process.\n"}
{"id": "9294254", "url": "https://en.wikipedia.org/wiki?curid=9294254", "title": "European Diploma of Protected Areas", "text": "European Diploma of Protected Areas\n\nThe European Diploma of Protected Areas, established in 1965, is a diploma awarded by the Council of Europe to protected areas (natural or semi-natural) of exceptional European conservational interest. It is awarded for a five-year period at a time and is renewable. Over 60 areas in 23 states have received the award so far.\n\n\n"}
{"id": "27739767", "url": "https://en.wikipedia.org/wiki?curid=27739767", "title": "Forces on sails", "text": "Forces on sails\n\nForces on sails result from movement of air that interacts with sails and gives them motive power for sailing craft, including sailing ships, sailboats, windsurfers, ice boats, and sail-powered land vehicles. Similar principles in a rotating frame of reference apply to wind mill sails and wind turbine blades, which are also wind-driven. They are differentiated from forces on wings, and propeller blades, the actions of which are not adjusted to the wind. Kites also power certain sailing craft, but do not employ a mast to support the airfoil and are beyond the scope of this article.\n\nForces on sails depend on wind speed and direction and the speed and direction of the craft. The direction that the craft is traveling with respect to the \"true wind\" (the wind direction and speed over the surface) is called the point of sail. The speed of the craft at a given point of sail contributes to the \"apparent wind\"—the wind speed and direction as measured on the moving craft. The apparent wind on the sail creates a total aerodynamic force, which may be resolved into drag—the force component in the direction of the apparent wind—and lift—the force component normal (90°) to the apparent wind. Depending on the alignment of the sail with the apparent wind, lift or drag may be the predominant propulsive component. Total aerodynamic force also resolves into a forward, propulsive, driving force—resisted by the medium through or over which the craft is passing (e.g. through water, air, or over ice, sand)—and a lateral force, resisted by the underwater foils, ice runners, or wheels of the sailing craft.\n\nFor apparent wind angles aligned with the entry point of the sail, the sail acts as an airfoil and lift is the predominant component of propulsion. For apparent wind angles behind the sail, lift diminishes and drag increases as the predominant component of propulsion. For a given true wind velocity over the surface, a sail can propel a craft to a higher speed, on points of sail when the entry point of the sail is aligned with the apparent wind, than it can with the entry point not aligned, because of a combination of the diminished force from airflow around the sail and the diminished apparent wind from the velocity of the craft. Because of limitations on speed through the water, displacement sailboats generally derive power from sails generating lift on points of sail that include close-hauled through broad reach (approximately 40° to 135° off the wind). Because of low friction over the surface and high speeds over the ice that create high apparent wind speeds for most points of sail, iceboats can derive power from lift further off the wind than displacement boats.\n\nVarious mathematical models address lift and drag by taking into account the density of air, coefficients of lift and drag that result from the shape and area of the sail, and the speed and direction of the apparent wind, among other factors. This knowledge is applied to the design of sails in such a manner that sailors can adjust sails to the strength and direction of the apparent wind in order to provide motive power to sailing craft.\n\nThe combination of a sailing craft's speed and direction with respect to the wind, together with wind strength, generate an apparent wind velocity. When the craft is aligned in a direction where the sail can be adjusted to align with its leading edge parallel to the apparent wind, the sail acts as an airfoil to generate lift in a direction perpendicular to the apparent wind. A component of this lift pushes the craft crosswise to its course, which is resisted by a sailboat's keel, an ice boat's blades or a land-sailing craft's wheels. An important component of lift is directed forward in the direction of travel and propels the craft.\n\nTo understand forces and velocities, discussed here, one must understand what is meant by a \"vector\" and a \"scalar.\" Velocity (V), denoted as boldface in this article, is an example of a vector, because it implies both \"direction\" and \"speed\". The corresponding speed (\"V\" ), denoted as \"italics\" in this article is a scalar value. Likewise, a force vector, F, denotes \"direction\" and \"strength\", whereas its corresponding scalar (\"F\" ) denotes strength alone. Graphically, each vector is represented with an arrow that shows direction and a length that shows speed or strength. Vectors of consistent units (e.g. V in m/s or F in N) may be added and subtracted, graphically, by positioning tips and tails of the arrows, representing the input variables and drawing the resulting derived vector.\n\nLift on a sail (L), acting as an airfoil, occurs in a direction perpendicular to the incident airstream (the apparent wind velocity, V, for the head sail) and is a result of pressure differences between the windward and leeward surfaces and depends on angle of attack, sail shape, air density, and speed of the apparent wind. Pressure differences result from the normal force per unit area on the sail from the air passing around it. The lift force results from the average pressure on the windward surface of the sail being higher than the average pressure on the leeward side. These pressure differences arise in conjunction with the curved air flow. As air follows a curved path along the windward side of a sail, there is a pressure gradient perpendicular to the flow direction with higher pressure on the outside of the curve and lower pressure on the inside. To generate lift, a sail must present an \"angle of attack\" (α) between the chord line of the sail and the apparent wind velocity (V). Angle of attack is a function of both the craft's point of sail and how the sail is adjusted with respect to the apparent wind.\n\nAs the lift generated by a sail increases, so does lift-induced drag, which together with parasitic drag constitutes total drag, (D). This occurs when the angle of attack increases with sail trim or change of course to cause the lift coefficient to increase up to the point of aerodynamic stall, so does the lift-induced drag coefficient. At the onset of stall, lift is abruptly decreased, as is lift-induced drag, but viscous pressure drag, a component of parasitic drag, increases due to the formation of separated flow on the surface of the sail. Sails with the apparent wind behind them (especially going downwind) operate in a stalled condition.\n\nLift and drag are components of the total aerodynamic force on sail (F). Since the forces on the sail are resisted by forces in the water (for a boat) or on the traveled surface (for an ice boat or land sailing craft), their corresponding forces can also be decomposed from total aerodynamic force into driving force (F) and lateral force (F). Driving force overcomes resistance to forward motion. Lateral force is met by lateral resistance from a keel, blade or wheel, but also creates a heeling force.\n\nApparent wind (V) is the air velocity acting upon the leading edge of the most forward sail or as experienced by instrumentation or crew on a moving sailing craft. It is the vector sum of true wind velocity and the apparent wind component resulting from boat velocity (V = -V + V). In nautical terminology, wind speeds are normally expressed in knots and wind angles in degrees. The craft's point of sail affects its velocity (V) for a given true wind velocity (V). Conventional sailing craft cannot derive power from the wind in a \"no-go\" zone that is approximately 40° to 50° away from the true wind, depending on the craft. Likewise, the directly downwind speed of all conventional sailing craft is limited to the true wind speed.\n\nBoat velocity (in black) generates an equal and opposite apparent wind component (not shown), which adds to the true wind to become apparent wind.\n\nSailing craft A is close-hauled. Sailing craft B is on a beam reach. Sailing craft C is on a broad reach.\nA sailboat's speed through the water is limited by the resistance that results from hull drag in the water. Sail boats on foils are much less limited. Ice boats typically have the least resistance to forward motion of any sailing craft. Craft with the higher forward resistance achieve lower forward velocities for a given wind velocity than ice boats, which can travel at speeds several multiples of the true wind speed. Consequently, a sailboat experiences a wider range of apparent wind angles than does an ice boat, whose speed is typically great enough to have the apparent wind coming from a few degrees to one side of its course, necessitating sailing with the sail sheeted in for most points of sail. On conventional sail boats, the sails are set to create lift for those points of sail where it's possible to align the leading edge of the sail with the apparent wind.\n\nFor a sailboat, point of sail affects lateral force significantly. The higher the boat points to the wind under sail, the stronger the lateral force, which requires resistance from a keel or other underwater foils, including daggerboard, centerboard, skeg and rudder. Lateral force also induces heeling in a sailboat, which requires resistance by weight of ballast from the crew or the boat itself and by the shape of the boat, especially with a catamaran. As the boat points off the wind, lateral force and the forces required to resist it become less important. On ice boats, lateral forces are countered by the lateral resistance of the blades on ice and their distance apart, which generally prevents heeling.\n\nEach sailing craft is a system that mobilizes wind force through its sails—supported by spars and rigging—which provide motive power and reactive force from the underbody of a sailboat—including the keel, centerboard, rudder or other underwater foils—or the running gear of an ice boat or land craft, which allows it to be kept on a course. Without the ability to mobilize reactive forces in directions different from the wind direction, a craft would simply be adrift before the wind.\n\nAccordingly, motive and heeling forces on sailing craft are either \"components of\" or \"reactions to\" the total aerodynamic force (F) on sails, which is a function of apparent wind velocity (V) and varies with point of sail. The forward driving force (F) component contributes to boat velocity (V), which is, itself, a determinant of apparent wind velocity. Absent lateral reactive forces to F from a keel (in water), a skate runner (on ice) or a wheel (on land), a craft would only be able to move downwind and the sail would not be able to develop lift.\n\nAt a stable angle of heel (for a sailboat) and a steady speed, aerodynamic and hydrodynamic forces are in balance. Integrated over the sailing craft, the total aerodynamic force (F) is located at the centre of effort (\"CE\"), which is a function of the design and adjustment of the sails on a sailing craft. Similarly, the total hydrodynamic force (F) is located at the centre of lateral resistance (\"CLR\"), which is a function of the design of the hull and its underwater appendages (keel, rudder, foils, etc.). These two forces act in opposition to one another with F a reaction to F.\n\nWhereas ice boats and land-sailing craft resist lateral forces with their wide stance and high-friction contact with the surface, sailboats travel through water, which provides limited resistance to side forces. In a sailboat, side forces are resisted in two ways:\nAll sailing craft reach a constant forward speed (\"V\") for a given wind speed (\"V\") and point of sail, when the forward driving force (F) equals the forward resisting force (R). For an ice boat, the dominant forward resisting force is aerodynamic, since the coefficient of friction on smooth ice is as low as 0.02. Accordingly, high-performance ice boats are streamlined to minimize aerodynamic drag.\n\nThe approximate locus of net aerodynamic force on a craft with a single sail is the centre of effort (\"CE\" ) at the geometric centre of the sail. Filled with wind, the sail has a roughly spherical polygon shape and if the shape is stable, then the location of centre of effort is stable. On sailing craft with multiple sails, the position of centre of effort varies with the sail plan. Sail trim or airfoil profile, boat trim and point of sail also affect \"CE\".\n\nthe camber of the sail and passing through a plane intersecting the centre of effort, normal to the leading edge (luff), roughly perpendicular to the chord of the sail (a straight line between the leading edge (luff)\nand the trailing edge (leech)). Net aerodynamic force with respect to the air stream is usually considered in reference to the direction of the apparent wind (V) over the surface plane (ocean, land or ice) and is decomposed into lift (L), perpendicular with V, and drag (D), in line with V. For windsurfers, lift component vertical to the surface plane is important, because in strong winds windsurfer sails are leaned into the wind to create a vertical lifting component ( F) that reduces drag on the board (hull) through the water. Note that F acts downwards for boats heeling away from the wind, but is negligible under normal conditions.\n\nThe three dimensional vector relationship for net aerodynamic force with respect to apparent wind (V) is:\n\nLikewise, net aerodynamic force may be decomposed into the three translational directions with respect to a boat's course over the surface: surge (forward/astern), sway (starboard/port—relevant to leeway), and heave (up/down). The scalar values and direction of these components can be dynamic, depending on wind and waves (for a boat). In this case, F is considered in reference to the direction of the boat's course and is decomposed into driving force (F), in line with the boat's course, and lateral force (F), perpendicular with the boat's course. Again for windsurfers, the lift component vertical to the surface plane ( F) is important.\n\nThe three dimensional vector relationship for net aerodynamic force with respect to the course over the surface is:\n\nThe values of driving force (\"F\" ) and lateral force (\"F\" ) with apparent wind angle (α), assuming no heeling, relate to the values of lift (\"L\" ) and drag (\"D\" ), as follows:\n\nReactive forces on sailing craft include forward resistance—sailboat's hydrodynamic resistance (\"R\"), an ice boat's sliding resistance or a land sailing craft's rolling resistance in the direction of travel—which are to be minimized in order to increase speed, and lateral force, perpendicular to the direction of travel, which is to be made sufficiently strong to minimize sideways motion and to guide the craft on course.\n\nForward resistance comprises the types of drag that impede a sailboat's speed through water (or an ice boat's speed over the surface) include components of parasitic drag, consisting primarily of form drag, which arises because of the shape of the hull, and skin friction, which arises from the friction of the water (for boats) or air (for ice boats and land sailing craft) against the \"skin\" of the hull that is moving through it. Displacement vessels are also subject to wave resistance from the energy that goes into displacing water into waves and that is limited by hull speed, which is a function of waterline length, Wheeled vehicles' forward speed is subject to rolling friction and ice boats are subject to kinetic or sliding friction. Parasitic drag in water or air increases with the square of speed (\"V\" or \"V\", respectively);\n\nWays to reduce wave-making resistance used on sailing vessels include \"reduced displacement\"—through planing or (as with a windsurfer) offsetting vessel weight with a lifting sail—and \"fine entry\", as with catamarans, where a narrow hull minimizes the water displaced into a bow wave. Sailing hydrofoils also substantially reduce forward friction with an underwater foil that lifts the vessel free of the water.\n\n\nSailing craft with low forward resistance can achieve high velocities with respect to the wind velocity:\n\nLateral force is a reaction supplied by the underwater shape of a sailboat, the blades of an ice boat and the wheels of a land sailing craft. Sailboats rely on keels, centerboards, and other underwater foils, including rudders, that provide lift in the lateral direction, to provide hydrodynamic lateral force (P) to offset the lateral force component acting on the sail (F) and minimize leeway. Such foils provide hydrodynamic lift and, for keels, ballast to offset heeling. They incorporate a wide variety of design considerations.\n\nThe forces on sails that contribute to torque and cause rotation with respect to the boat's longitudinal (fore and aft), horizontal (abeam) and vertical (aloft) rotational axes result in: roll (e.g. heeling). pitch (e.g. pitch-poling), and yaw (e.g. broaching). Heeling, which results from the lateral force component (F), is the most significant rotational effect of total aerodynamic force (F). In stasis, heeling moment from the wind and righting moment from the boat's heel force (\"F\" ) and its opposing hydrodynamic lift force on hull (\"F\" ), separated by a distance (\"h\" = \"heeling arm\"), versus its hydrostatic displacement weight (\"W\" ) and its opposing buoyancy force (\"Δ\"), separated by a distance (\"b\" = \"righting arm\") are in balance:\nSails come in a wide variety of configurations that are designed to match the capabilities of the sailing craft to be powered by them. They are designed to stay within the limitations of a craft's stability and power requirements, which are functions of hull (for boats) or chassis (for land craft) design. Sails derive power from wind that varies in time and with height above the surface. In order to do so, they are designed to adjust to the wind force for various points of sail. Both their design and method for control include means to match their lift and drag capabilities to the available apparent wind, by changing surface area, angle of attack, and curvature.\n\nWind speed increases with height above the surface; at the same time, wind speed may vary over short periods of time as gusts. These considerations may be described empirically.\n\nMeasurements show that wind speed, (\"V\" (\"h\" ) ) varies, according to a power law with height (\"h\" ) above a non-zero measurement height datum (\"h\" —e.g. at the height of the foot of a sail), using a reference wind speed measured at the datum height (\"V\" (\"h\" ) ), as follows:\n\nWhere the power law exponent (\"p\") has values that have been empirically determined to range from 0.11 over the ocean to 0.31 over the land.\n\nThis means that a \"V\" (3 m) = 5-m/s (≈10-knot) wind at 3 m above the water would be approximately \"V\" (15 m) = 6 m/s (≈12 knots) at 15 m above the water. In hurricane-force winds with \"V\" (3 m) = 40-m/s (≈78 knots) the speed at 15 m would be \"V\" (15 m) = 49 m/s (≈95 knots) with \"p\" = 0.128. This suggests that sails that reach higher above the surface can be subject to stronger wind forces that move the centre of effort (\"CE\" ) higher above the surface and increase the heeling moment.\n\nAdditionally, apparent wind direction moves aft with height above water, which may necessitate a corresponding twist in the shape of the sail to achieve attached flow with height.\n\nHsu gives a simple formula for a gust factor (\"G \" ) for winds as a function of the exponent (\"p\" ), above, where \"G\" is the ratio of the wind gust speed to baseline wind speed at a given height:\n\nSo, for a given windspeed and Hsu's recommended value of \"p\" = 0.126, one can expect \"G\" = 1.5 (a 10-knot wind might gust up to 15 knots). This, combined with changes in wind direction suggest the degree to which a sailing craft must adjust to wind gusts on a given course.\n\nA sailing craft's motive system comprises one or more sails, supported by spars and rigging, that derive power from the wind and induce reactive force from the underbody of a sailboat or the running gear of an ice boat or land craft. Depending on the angle of attack of a set of sails with respect to the apparent wind, each sail is providing motive force to the sailing craft either from lift-dominant attached flow or drag-dominant separated flow. Additionally, sails may interact with one another to create forces that are different from the sum of the individual contributions each sail, when used alone.\n\nSails allow progress of a sailing craft to windward, thanks to their ability to generate lift (and the craft's ability to resist the lateral forces that result). Each sail configuration has a characteristic coefficient of lift and attendant coefficient of drag, which can be determined experimentally and calculated theoretically. Sailing craft orient their sails with a favorable angle of attack between the entry point of the sail and the apparent wind even as their course changes. The ability to generate lift is limited by sailing too close to the wind when no effective angle of attack is available to generate lift (luffing) and sailing sufficiently off the wind that the sail cannot be oriented at a favorable angle of attack (running downwind). Instead, past a critical angle of attack, the sail stalls and promotes flow separation.\n\nEach type of sail, acting as an airfoil, has characteristic coefficients of lift (\"C\" ) and lift-induced drag (\"C\" ) at a given angle of attack, which follow that same basic form of:\nWhere force (\"F\") equals \"lift\" (\"L\") for forces measured \"perpendicular\" to the airstream to determine \"C\" = \"C\" or force (\"F\") equals \"drag\" (\"D\") for forces measured \"in line with\" the airstream to determine \"C\" = \"C\" on a sail of area (\"A\") and a given aspect ratio (length to average cord width). These coefficients vary with angle of attack (\"α\" for a headsail) with respect to the incident wind (\"V\" for a headsail). \nThis formulation allows determination of \"C\" and \"C\" experimentally for a given sail shape by varying angle of attack at an experimental wind velocity and measuring force on the sail in the direction of the incident wind (\"D\"—drag) and perpendicular to it (\"L\"—lift). As the angle of attack grows larger, the lift reaches a maximum at some angle; increasing the angle of attack beyond this critical angle of attack causes the upper-surface flow to separate from the convex surface of the sail; there is less deflection of air to windward, so the sail as airfoil generates less lift. The sail is said to be stalled. At the same time, induced drag increases with angle of attack (for the headsail: \"α\" ).\n\n\nFossati presents polar diagrams that relate coefficients of lift and drag for different angles of attack based on the work of Gustave Eiffel, who pioneered wind tunnel experiments on airfoils, which he published in 1910. Among them were studies of cambered plates. The results shown are for plates of varying camber and aspect ratios, as shown. They show that, as aspect ratio decreases, maximum lift shifts further towards increased drag (rightwards in the diagram). They also show that, for lower angles of attack, a higher aspect ratio generates more lift and less drag than for lower aspect ratios.\n\nIf the lift and drag coefficients (\"C\" and \"C\") for a sail at a specified angle of attack are known, then the lift (\"L\") and drag (\"D\") forces produced can be determined, using the following equations, which vary as the square of apparent wind speed (\"V\" ):\n\nGarrett demonstrates how those diagrams translate into lift and drag, for a given sail, on different points of sail, in diagrams similar to these:\nIn these diagrams the direction of travel changes with respect to the apparent wind (V), which is constant for the purpose of illustration. In reality, for a constant true wind, apparent wind would vary with point of sail. Constant V in these examples means that either V or V varies with point of sail; this allows the same polar diagram to be used for comparison with the same conversion of coefficients into units of force (in this case Newtons). In the examples for close-hauled and reach (left and right), the sail's angle of attack (\"α\" ) is essentially constant, although the boom angle over the boat changes with point of sail to trim the sail close to the highest lift force on the polar curve. In these cases, lift and drag are the same, but the decomposition of total aerodynamic force (F) into forward driving force (F) and lateral force (F) vary with point of sail. Forward driving force (F) increases, as the direction of travel is more aligned with the wind, and lateral force (F) decreases.\n\nIn reference to the above diagrams relating lift and drag, Garrett explains that for a maximum speed made good to windward, the sail must be trimmed to an angle of attack that is greater than the maximum lift/drag ratio (more lift), while the hull is operated in a manner that is lower than its maximum lift/drag ratio (more drag).\n\nWhen sailing craft are on a course where the angle of attack between the sail and the apparent wind (\"α\" ) exceeds the point of maximum lift on the \"C\"–\"C\" polar diagram, separation of flow occurs. The separation becomes more pronounced until at \"α\" = 90° lift becomes small and drag predominates. In addition to the sails used upwind, spinnakers provide area and curvature appropriate for sailing with separated flow on downwind points of sail.\n\nAgain, in these diagrams the direction of travel changes with respect to the apparent wind (V), which is constant for the sake of illustration, but would in reality vary with point of sail for a constant true wind. In the left-hand diagram (broad reach), the boat is on a point of sail, where the sail can no longer be aligned into the apparent wind to create an optimum angle of attack. Instead, the sail is in a stalled condition, creating about 80% of the lift as in the upwind examples and drag has doubled. Total aerodynamic force (F) has moved away from the maximum lift value. In the right-hand diagram (running before the wind), lift is one-fifth of the upwind cases (for the same strength apparent wind) and drag has almost quadrupled.\n\n\nA velocity prediction program can translate sail performance and hull characteristics into a polar diagram, depicting boat speed for various windspeeds at each point of sail. Displacement sailboats exhibit a change in what course has the best velocity made good (VMG), depending on windspeed. For the example given, the sailboat achieves best downwind VMG for windspeed of 10 knots and less at a course about 150° off the wind. For higher windspeed the optimum downwind VMG occurs at more than 170° off the wind. This \"downwind cliff\" (abrupt change in optimum downwind course) results from the change of balance in drag forces on the hull with speed.\n\nSailboats often have a jib that overlaps the mainsail—called a genoa. Arvel Gentry demonstrated in 1981 that the genoa and the mainsail interact in a symbiotic manner, owing to the circulation of air between them slowing down in the gap between the two sails (contrary to traditional explanations), which prevents separation of flow along the mainsail. The presence of a jib causes the stagnation line on the mainsail to move forward, which reduces the suction velocities on the main and reduces the potential for boundary layer separation and stalling. This allows higher angles of attack. Likewise, the presence of the mainsail causes the stagnation line on the jib to be shifted forward and allows the boat to point closer to the wind, owing to higher leeward velocities of the air over both sails.\nSails characteristically have a coefficient of lift (\"C\") and coefficient of drag (\"C\") for each apparent wind angle. The planform, curvature and area of a given sail are dominant determinants of each coefficient.\n\nSails are classified as \"triangular sails\", \"quadrilateral fore-and-aft sails\" (gaff-rigged, etc.), and \"square sails\". The top of a triangular sail, the \"head\", is raised by a halyard, The forward lower corner of the sail, the \"tack\", is shackled to a fixed point on the boat in a manner to allow pivoting about that point—either on a mast, e.g. for a mainsail, or on the deck, e.g. for a jib or staysail. The trailing lower corner, the \"clew\", is positioned with an outhaul on a boom or directly with a sheet, absent a boom. Symmetrical sails have two clews, which may be adjusted forward or back.\n\nThe windward edge of a sail is called the \"luff\", the trailing edge, the \"leach\", and the bottom edge the \"foot\". On symmetrical sails, either vertical edge may be presented to windward and, therefore, there are two leaches. On sails attached to a mast and boom, these edges may be curved, when laid on a flat surface, to promote both horizontal and vertical curvature in the cross-section of the sail, once attached. The use of battens allows a sail have an arc of material on the leech, beyond a line drawn from the head to the clew, called the \"roach\".\n\nAs with aircraft wings, the two dominant factors affecting sail efficiency are its planform—primarily sail width versus sail height, expressed as an aspect ratio—and cross-sectional curvature or draft.\n\nIn aerodynamics, the aspect ratio of a sail is the ratio of its length to its breadth (chord). A high aspect ratio indicates a long, narrow sail, whereas a low aspect ratio indicates a short, wide sail. For most sails, the length of the chord is not a constant but varies along the wing, so the aspect ratio \"AR\" is defined as the square of the sail height \"b\" divided by the area \"A\" of the sail planform:\n\nAspect ratio and planform can be used to predict the aerodynamic performance of a sail. For a given sail area, the aspect ratio, which is proportional to the square of the sail height, is of particular significance in determining lift-induced drag, and is used to calculate the induced drag coefficient of a sail formula_12:\n\nwhere formula_14 is the Oswald efficiency number that accounts for the variable sail shapes. This formula demonstrates that a sail's induced drag coefficient decreases with increased aspect ratio.\n\nThe horizontal curvature of a sail is termed \"draft\" and corresponds to the camber of an airfoil. Increasing the draft generally increases the sail's lift force. The Royal Yachting Association categorizes draft by depth and by the placement of the maximum depth as a percentage of the distance from the luff to the leach. Sail draft is adjusted for wind speed to achieve a flatter sail (less draft) in stronger winds and a fuller sails (more draft) in lighter winds. Staysails and sails attached to a mast (e.g. a mainsail) have different, but similar controls to achieve draft depth and position. On a staysail, tightening the luff with the halyard helps flatten the sail and adjusts the position of maximum draft. On a mainsail curving the mast to fit the curvature of the luff helps flatten the sail. Depending on wind strength, Dellenbaugh offers the following advice on setting the draft of a sailboat mainsail:\nPlots by Larsson \"et al\" show that draft is a much more significant factor affecting sail propulsive force than the position of maximum draft.\nThe primary tool for adjusting mainsail shape is mast bend; a straight mast increases draft and lift; a curved mast decreases draft and lift—the backstay tensioner is a primary tool for bending the mast. Secondary tools for sail shape adjustment are the mainsheet, traveler, outhaul, and Cunningham.\n\nSpinnakers have traditionally been optimized to mobilize drag as a more important propulsive component than lift. As sailing craft are able to achieve higher speeds, whether on water, ice or land, the velocity made good (VMG) at a given course off the wind occurs at apparent wind angles that are increasingly further forward with speed. This suggests that the optimum VMG for a given course may be in a regime where a spinnaker may be providing significant lift. Traditional displacement sailboats may at times have optimum VMG courses close to downwind; for these the dominant force on sails is from drag. According to Kimball,\n\"C\" ≈ 4/3 for most sails with the apparent wind angle astern, so drag force on a downwind sail becomes substantially a function of area and wind speed, approximated as follows:\n\nformula_15\nSail design relies on empirical measurements of pressures and their resulting forces on sails, which validate modern analysis tools, including computational fluid dynamics.\n\nModern sail design and manufacture employs wind tunnel studies, full-scale experiments, and computer models as a basis for efficiently harnessing forces on sails.\n\nInstruments for measuring air pressure effects in wind tunnel studies of sails include pitot tubes, which measure air speed and manometers, which measure static pressures and atmospheric pressure (static pressure in undisturbed flow). Researchers plot pressure across the windward and leeward sides of test sails along the chord and calculate pressure coefficients (static pressure difference over wind-induced dynamic pressure).\nResearch results describe airflow around the sail and in the boundary layer. Wilkinson, modelling the boundary layer in two dimensions, described nine regions around the sail:\n\nSail design differs from wing design in several respects, especially since on a sail air flow varies with wind and boat motion and sails are usually deformable airfoils, sometimes with a mast for a leading edge. Often simplifying assumptions are employed when making design calculations, including: a flat travel surface—water, ice or land, constant wind velocity and unchanging sail adjustment.\n\nThe analysis of the forces on sails takes into account the aerodynamic surface force, its centre of effort on a sail, its direction, and its variable distribution over the sail. Modern analysis employs fluid mechanics and aerodynamics airflow calculations for sail design and manufacture, using aeroelasticity models, which combine computational fluid dynamics and structural analysis. Secondary effects pertaining to turbulence and separation of the boundary layer are secondary factors. Computational limitations persist. Theoretical results require empirical confirmation with wind tunnel tests on scale models and full-scale testing of sails. Velocity prediction programs combine elements of hydrodynamic forces (mainly drag) and aerodynamic forces (lift and drag) to predict sailboat performance at various windspeed for all points of sail\n"}
{"id": "29684436", "url": "https://en.wikipedia.org/wiki?curid=29684436", "title": "Fuiono Senio", "text": "Fuiono Senio\n\nFuiono Senio (died 17 May 1997) was a chief and environmentalist from Falealupo village on the island of Savai'i in Western Samoa. A logging company offered funds to the Samoan government to build a school for Senio's village if they could log the rainforest nearby. As the loggers were about to get to work, a visiting biologist offered to raise funds if logging stopped. The village elders agreed to accept the offer and set aside the rainforest from the coast to the interior ridge of Savai'i, as a rainforest preserve. The loggers returned, and Senio ran in front of them and threatened them with a machete, telling them to go away. This brave act became well known throughout the South Pacific, where similar problems affected many poor island communities. He was awarded the Goldman Environmental Prize in 1997, shared with Paul Alan Cox, for their contributions to the protection of rainforests of Falealupo. He died of liver cancer a month after receiving the Goldman prize.\n"}
{"id": "10186684", "url": "https://en.wikipedia.org/wiki?curid=10186684", "title": "Hartlepool Water", "text": "Hartlepool Water\n\nHartlepool Water is a water company that covers the town of Hartlepool in County Durham and surrounding area in the North East of England. Since 1997 it has been owned by Anglian Water.\n\nHartlepool Water does not provide sewerage services. These are provided by Northumbrian Water.\n\nThe company was originally formed as the Hartlepool Gas and Water Company in 1846, was re-incorporated into Hartlepool Water Ltd in 1995 and its company name, though not brand and business name, was renamed in 2006 as Osprey Water Services Ltd of Anglian Water's head office (Registered Office) retaining company number 3017251.\n\n"}
{"id": "19221278", "url": "https://en.wikipedia.org/wiki?curid=19221278", "title": "In-motion scale", "text": "In-motion scale\n\nAn in-motion scale weighs products in a conveyor system. They can be used for rejecting under- and over- weight products. Other types send the weights to a computer for data analysis and statistics. They may also be known as dynamic scales, belt weighers, but most prominently as checkweighers.\n\nIn-motion scales come in numerous variations based on application. A simple version may have only one belt or chain, and use software for transmission-of-weight only. Some are used to sort product by weight. Through the use of an infeed bed, weigh bed, and outfeed bed, in motion scales or dynamic scales can be equipped with metal detectors, vision / camera systems and various types of rejection systems. See checkweigher.\n"}
{"id": "7331693", "url": "https://en.wikipedia.org/wiki?curid=7331693", "title": "Joel Lebowitz", "text": "Joel Lebowitz\n\nJoel L. Lebowitz (born May 10, 1930 in Taceva (Tiachiv), then in Czechoslovakia) is a mathematical physicist widely acknowledged for his outstanding contributions to statistical physics, statistical mechanics and many other fields of Mathematics and Physics.\n\nLebowitz has published more than five hundred papers concerning statistical physics and science in general, and he is one of the founders and editors of the \"Journal of Statistical Physics\", one of the most important peer-reviewed journals concerning scientific research in this area. He has been president of the New York Academy of Sciences.\n\nLebowitz is the George William Hill Professor of Mathematics and Physics at Rutgers University. He is also an active member of the human rights community and a long-term co-Chair of the Committee of Concerned Scientists.\n\nJoel Lebowitz was born in Taceva, then in Czechoslovakia, now Ukraine, in 1930 into a Jewish family. During World War II he was deported with his family to Auschwitz, where his father, his mother, and his younger sister were killed in 1944. After being liberated from the camp, he moved to United States by boat, and he studied in an Orthodox Jewish school, at Brooklyn College, and at Syracuse University. Here he got a PhD in 1956 under the supervision of Peter G. Bergmann. Then he continued his research with Lars Onsager, at Yale University, where he got a faculty position. He moved to the Stevens Institute of Technology in 1957 and to the Belfer Graduate School of Science of Yeshiva University in 1959. Finally he got a faculty position at Rutgers University in 1977, where he holds the prestigious \"George William Hill Professor\" position. During his years at the Yeshiva University and Rutgers University he has been in contact with several scientists, and artists, like Fumio Yoshimura and Kate Millett. In \n1975 he founded the Journal of Statistical Physics. In 1979 he was president of the New York Academy of Sciences. He has been one of the most active supporters of dissident scientists in the former Soviet Union, especially refusenik scientists.\n\nLebowitz has had many important contributions to statistical mechanics and mathematical physics. He proved, along with Elliott Lieb, that the Coulomb interactions obey the thermodynamic limit. He also established what are now known as Lebowitz inequalities for the ferromagnetic Ising model. His current interests are in problems of non-equilibrium statistical mechanics.\n\nHe became editor-in-chief the Journal of Statistical Physics in 1975, one of the most important journals in the field, and has been editor in chief ever since, till he resigned the position in September 2018. Lebowitz hosts a biannual series of conferences held, first at Yeshiva University and later at Rutgers University, which has been running for 60 years. He is also known as a co-editor of an influential review series, 'Phase transitions and critical phenomena'.\n\nHe has been awarded several honors, such as the Boltzmann Medal (1992), the Nicholson Medal (1994) awarded by the American Physical Society, the Delmer S. Fahrney Medal (1995), the Henri Poincaré Prize (2000), the Volterra Award (2001), and many others. Among other recognitions, Lebowitz was awarded the Max Planck Medal in 2007 \"for his important contributions to the statistical physics of equilibrium and non-equilibrium systems, in particular his contributions to the theory of phase transitions, the dynamics of infinite systems, and the stationary non-equilibrium states\" and \"for his promoting of new directions of this field at its farthest front, and for enthusiastically introducing several generations of scientists to the field.\" In 2014 he received the Grande Médaille of the French Academy of Sciences.\n\nHe is a member of the United States National Academy of Sciences. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "45459945", "url": "https://en.wikipedia.org/wiki?curid=45459945", "title": "Lake Robe Game Reserve", "text": "Lake Robe Game Reserve\n\nLake Robe Game Reserve is a protected area located about south of the town of Robe in South Australia. It covers the saline lake, Lake Robe, and some surrounding land and also immediately adjoins the northern boundary of the Little Dip Conservation Park. It was proclaimed on 4 November 1993 to protect \"valuable habitats for a variety of waterbirds, and terrestrial mammals notably the hooded plover (\"Thinomis rubricollis\"), sharp-tailed sandpiper (\"Calidris acuminata\"), and the swamp rat (\"Rattus lutreolus\")\" and to manage recreational duck hunting activity. The game reserve is classified as an IUCN Category VI protected area.\n\n\n"}
{"id": "17746", "url": "https://en.wikipedia.org/wiki?curid=17746", "title": "Lawrencium", "text": "Lawrencium\n\nLawrencium is a synthetic chemical element with symbol Lr (formerly Lw) and atomic number 103. It is named in honor of Ernest Lawrence, inventor of the cyclotron, a device that was used to discover many artificial radioactive elements. A radioactive metal, lawrencium is the eleventh transuranic element and is also the final member of the actinide series. Like all elements with atomic number over 100, lawrencium can only be produced in particle accelerators by bombarding lighter elements with charged particles. Twelve isotopes of lawrencium are currently known; the most stable is Lr with a half-life of 11 hours, but the shorter-lived Lr (half-life 2.7 minutes) is most commonly used in chemistry because it can be produced on a larger scale.\n\nChemistry experiments have confirmed that lawrencium behaves as a heavier homolog to lutetium in the periodic table, and is a trivalent element. It thus could also be classified as the first of the 7th-period transition metals: however, its electron configuration is anomalous for its position in the periodic table, having an sp configuration instead of the sd configuration of its homolog lutetium. This means that lawrencium may be more volatile than expected for its position in the periodic table and have a volatility comparable to that of lead.\n\nIn the 1950s, 1960s, and 1970s, many claims of the synthesis of lawrencium of varying quality were made from laboratories in the Soviet Union and the United States. The priority of the discovery and therefore the naming of the element was disputed between Soviet and American scientists, and while the International Union of Pure and Applied Chemistry (IUPAC) initially established lawrencium as the official name for the element and gave the American team credit for the discovery, this was reevaluated in 1997, giving both teams shared credit for the discovery but not changing the element's name.\n\nIn 1958, scientists at the Lawrence Berkeley National Laboratory claimed the discovery of element 102, now called nobelium. At the same time, they also attempted to synthesize element 103 by bombarding the same curium target used with nitrogen-14 ions. A follow-up on this experiment was not performed, as the target was destroyed. Eighteen tracks were noted, with decay energy around and half-life around  s; the Berkeley team noted that while the cause could be the production of an isotope of element 103, other possibilities could not be ruled out. While the data agrees reasonably with that later discovered for Lr (alpha decay energy 8.87 MeV, half-life 0.6 s), the evidence obtained in this experiment fell far short of the strength required to conclusively demonstrate the synthesis of element 103. Later, in 1960, the Lawrence Berkeley Laboratory attempted to synthesize the element by bombarding Cf with B and B. The results of this experiment were not conclusive.\n\nThe first important work on element 103 was carried out at Berkeley by the nuclear-physics team of Albert Ghiorso, Torbjørn Sikkeland, Almon Larsh, Robert M. Latimer, and their co-workers on February 14, 1961. The first atoms of lawrencium were reportedly produced by bombarding a three-milligram target consisting of three isotopes of the element californium with boron-10 and boron-11 nuclei from the Heavy Ion Linear Accelerator (HILAC). The Berkeley team reported that the isotope 103 was detected in this manner, and that it decayed by emitting an 8.6 MeV alpha particle with a half-life of . This identification was later corrected to be 103, as later work proved that Lr did not have the properties detected, but Lr did. This was considered at the time to be convincing proof of the synthesis of element 103: while the mass assignment was less certain and proved to be mistaken, it did not affect the arguments in favor of element 103 having been synthesized. Scientists at the Joint Institute for Nuclear Research in Dubna (then in the Soviet Union) raised several criticisms: all but one were answered adequately. The exception was that Cf was the most common isotope in the target, and in the reactions with B, Lr could only have been produced by emitting four neutrons, and emitting three neutrons was expected to be much less likely than emitting four or five. This would lead to a narrow yield curve, not the broad one reported by the Berkeley team. A possible explanation was that there was a low number of events attributed to element 103. This was an important intermediate step to the unquestioned discovery of element 103, although the evidence was not completely convincing. The Berkeley team proposed the name \"lawrencium\" with symbol \"Lw\", after Ernest Orlando Lawrence, inventor of the cyclotron. The IUPAC Commission on Nomenclature of Inorganic Chemistry accepted the name, but changed the symbol to \"Lr\". This acceptance of the discovery was later characterized as being hasty by the Dubna team.\n\nThe first work at Dubna on element 103 came in 1965, when they reported to have created 103 in 1965 by bombarding Am with O, identifying it indirectly from its granddaughter fermium-252. The half-life they reported was somewhat too high, possibly due to background events. Later 1967 work on the same reaction identified two decay energies in the ranges 8.35–8.50 MeV and 8.50–8.60 MeV: these were assigned to 103 and 103. Despite repeated attempts, they were unable to confirm assignment of an alpha emitter with a half-life of eight seconds to 103. The Russians proposed the name \"rutherfordium\" for the new element in 1967: this name was later used for element 104.\n\nFurther experiments in 1969 at Dubna and in 1970 at Berkeley demonstrated an actinide chemistry for the new element, so that by 1970 it was known that element 103 is the last actinide. In 1970, the Dubna group reported the synthesis of 103 with half-life 20 s and alpha decay energy 8.38 MeV. However, it was not until 1971, when the nuclear physics team at the University of California at Berkeley successfully performed a whole series of experiments aimed at measuring the nuclear decay properties of the lawrencium isotopes with mass numbers from 255 through 260, that all previous results from Berkeley and Dubna were confirmed, apart from the Berkeley's group initial erroneous assignment of their first produced isotope to 103 instead of the probably correct 103. All final doubts were finally dispelled in 1976 and 1977 when the energies of X-rays emitted from 103 were measured.\nIn 1971, the IUPAC granted the discovery of lawrencium to the Lawrence Berkeley Laboratory, even though they did not have ideal data for the element's existence. However, in 1992, the IUPAC Trans-fermium Working Group (TWG) officially recognized the nuclear physics teams at Dubna and Berkeley as the co-discoverers of lawrencium, concluding that while the 1961 Berkeley experiments were an important step to lawrencium's discovery, they were not yet completely convincing; and while the 1965, 1968, and 1970 Dubna experiments came very close to the needed level of confidence taken together, only the 1971 Berkeley experiments, which clarified and confirmed previous observations, finally resulted in complete confidence in the discovery of element 103. Because the name \"lawrencium\" had been in use for a long time by this point, it was retained by IUPAC, and in August 1997, the International Union of Pure and Applied Chemistry (IUPAC) ratified the name lawrencium and the symbol \"Lr\" during a meeting in Geneva.\n\nLawrencium is the final member of the actinide series and is sometimes considered to be a group 3 element, along with scandium, yttrium, and lutetium, as its filled f-shell is expected to make it resemble the 7th-period transition metals. In the periodic table, it is located to the right of the actinide nobelium, to the left of the 6d transition metal rutherfordium, and under the lanthanide lutetium with which it shares many physical and chemical properties. Lawrencium is expected to be a solid under normal conditions and assume a hexagonal close-packed crystal structure (/ = 1.58), similar to its lighter congener lutetium, though this is not yet known experimentally. The enthalpy of sublimation of lawrencium is estimated to be 352 kJ/mol, close to the value of lutetium and strongly suggesting that metallic lawrencium is trivalent with the 7s and 6d electrons delocalized, a prediction also supported by a systematic extrapolation of the values of heat of vaporization, bulk modulus, and atomic volume of neighboring elements to lawrencium. Specifically, lawrencium is expected to be a trivalent, silvery metal, easily oxidized by air, steam, and acids, and having an atomic volume similar to that of lutetium and a trivalent metallic radius of 171 pm. It is expected to be a rather heavy metal with a density of around 15.6 to 16.6 g/cm. It is also predicted to have a melting point of around 1900 K (1627 °C), not far from the value for lutetium (1925 K).\n\nIn 1949, Glenn T. Seaborg, who devised the actinide concept that elements 89 to 103 formed an actinide series homologous to the lanthanide series from elements 57 to 71, predicted that element 103 (lawrencium) should be its final member and that the Lr ion should be about as stable as Lu in aqueous solution. It was not until decades later that element 103 was finally conclusively synthesized and this prediction was experimentally confirmed.\n\n1969 studies on the element showed that lawrencium reacted with chlorine to form a product that was most likely the trichloride LrCl. Its volatility was found to be similar to that of the chlorides of curium, fermium, and nobelium and much less than that of rutherfordium chloride. In 1970, chemical studies were performed on 1500 atoms of the isotope Lr, comparing it with divalent (No, Ba, Ra), trivalent (Fm, Cf, Cm, Am, Ac), and tetravalent (Th, Pu) elements. It was found that lawrencium coextracted with the trivalent ions, but the short half-life of the Lr isotope precluded a confirmation that it eluted ahead of Md in the elution sequence. Lawrencium occurs as the trivalent Lr ion in aqueous solution and hence its compounds should be similar to those of the other trivalent actinides: for example, lawrencium(III) fluoride (LrF) and hydroxide (Lr(OH)) should both be insoluble in water. Due to the actinide contraction, the ionic radius of Lr should be smaller than that of Md, and it should elute ahead of Md when ammonium α-hydroxyisobutyrate (ammonium α-HIB) is used as an eluant. Later 1987 experiments on the longer-lived isotope Lr confirmed lawrencium's trivalency and that it eluted in roughly the same place as erbium, and found that lawrencium's ionic radius was , larger than would be expected from simple extrapolation from periodic trends. Later 1988 experiments with more lawrencium atoms refined this value to and calculated an enthalpy of hydration value of . It was also pointed out that the actinide contraction at the end of the actinide series was larger than the analogous lanthanide contraction, with the exception of the last actinide, lawrencium: the cause was speculated to be relativistic effects.\n\nIt has been speculated that the 7s electrons are relativistically stabilized, so that in reducing conditions, only the 7p or 6d electron would be ionized, leading to the monovalent Lr ion. However, all experiments to reduce Lr to Lr or Lr in aqueous solution were unsuccessful, similarly to lutetium. On the basis of this, the standard electrode potential of the \"E\"°(Lr→Lr) couple was calculated to be less than −1.56 V, indicating that the existence of Lr ions in aqueous solution was unlikely. The upper limit for the \"E\"°(Lr→Lr) couple was predicted to be −0.44 V: the values for \"E\"°(Lr→Lr) and \"E\"°(Lr→Lr) are predicted to be −2.06 V and +7.9 V. The stability of the group oxidation state in the 6d transition series decreases as Rf > Db > Sg, and lawrencium continues the trend with Lr being more stable than Rf.\n\nIn the molecule lawrencium dihydride (LrH), which is predicted to be bent, the 6d orbital of lawrencium is not expected to play a role in the bonding, unlike that of lanthanum dihydride (LaH). LaH has La–H bond distances of 2.158 Å, while LrH should have shorter Lr–H bond distances of 2.042 Å due to the relativistic contraction and stabilization of the 7s and 7p orbitals involved in the bonding, in contrast to the core-like 5f subshell and the mostly uninvolved 6d subshell. In general, molecular LrH and LrH are expected to resemble the corresponding thallium species (thallium having a 6s6p valence configuration in the gas phase, like lawrencium's 7s7p) more than the corresponding lanthanide species. The electron configurations of Lr and Lr are expected to be 7s and 7s respectively, unlike the lanthanides which tend to be 5d as Ln. However, in species where all three valence electrons of lawrencium are ionized to give at least formally the Lr cation, lawrencium is expected to behave like a typical actinide and the heavier congener of lutetium, especially because the first three ionization potentials of lawrencium are predicted to be similar to those of lutetium. Hence, unlike thallium but like lutetium, lawrencium would prefer to form LrH than LrH, and LrCO is expected to be similar to the also unknown LuCO, both metals having a valence configuration of σπ in their respective monocarbonyls. The pπ–dπ bond is expected to be observed in LrCl just as it is for LuCl and more generally all the LnCl, and the complex anion [Lr(CHSiMe)] is expected to be stable just like its lanthanide congeners, with a configuration of 6d for lawrencium; this 6d orbital would be its highest occupied molecular orbital.\n\nA lawrencium atom has 103 electrons, of which three can act as valence electrons. In 1970, it was predicted that the ground-state electron configuration of lawrencium was [Rn]5f6d7s (ground state term symbol D), following the Aufbau principle and conforming to the [Xe]4f5d6s configuration of lawrencium's lighter homolog lutetium. However, the next year, calculations were published that questioned this prediction, instead expecting an anomalous [Rn]5f7s7p configuration. Though early calculations gave conflicting results, more recent studies and calculations confirm the sp suggestion. 1974 relativistic calculations concluded that the energy difference between the two configurations was small and that it was uncertain which was the ground state. Later 1995 calculations concluded that the sp configuration should be energetically favored, because the spherical s and p orbitals are nearest to the atomic nucleus and thus move quickly enough that their relativistic mass increases significantly.\n\nIn 1988, a team of scientists led by Eichler calculated that lawrencium's enthalpy of adsorption on metal sources would differ enough depending on its electron configuration that it would be feasible to carry out experiments to exploit this fact to measure lawrencium's electron configuration. The sp configuration was expected to be more volatile than the sd configuration, and be more similar to that of the p-block element lead. No evidence for lawrencium being volatile was obtained and the lower limit for the enthalpy of adsorption of lawrencium on quartz or platinum was significantly higher than the estimated value for the sp configuration.\n\nIn 2015, the first ionization energy of lawrencium was measured, using the isotope Lr. The measured value, , agreed very well with the relativistic theoretical prediction of 4.963(15) eV, and also provided a first step into measuring the first ionization energies of the transactinides. This value is the lowest among all the lanthanides and actinides, and supports the sp configuration as the 7p electron is expected to be only weakly bound. This suggests that lutetium and lawrencium behave similarly to the d-block elements (and hence being the true heavier congeners of scandium and yttrium, instead of lanthanum and actinium), and also that lawrencium may behave similarly to the alkali metals sodium and potassium in some ways. Given that the sp configuration is correct, then lawrencium cannot be regarded as a transition metal under the IUPAC definition (\"An element whose atom has an incomplete d sub-shell, or which can give rise to cations with an incomplete d sub-shell\"), unlike its lighter homolog lutetium and the group 3 elements, with which lutetium and lawrencium are sometimes classified. It is nevertheless quite likely that metallic lawrencium will behave similarly to curium, which has an [Rn]5f6d7s configuration, and show the expected [Rn]5f6d7s configuration, which is supported by the earlier volatility experiments.\n\nTwelve isotopes of lawrencium are known, with mass numbers 252–262 and 266; all are radioactive. Additionally, one nuclear isomer is known, with mass number 253. The longest-lived lawrencium isotope, Lr, has a half-life of ten hours and is one of the longest lived superheavy isotopes known to date, suggesting that it is perhaps on the shore of the island of stability of superheavy nuclei. However, shorter-lived isotopes are usually used in chemical experiments because Lr currently can only be produced as a final decay product of even heavier and harder-to-synthesize elements: it was discovered in 2014 in the decay chain of Ts. The isotope Lr (half-life 27 seconds) was used in the first chemical studies on lawrencium: currently, the slightly longer lived isotope Lr (half-life 2.7 minutes) is usually used for this purpose. After Lr, the longest-lived lawrencium isotopes are Lr (3.6 h), Lr (44 min), Lr (2.7 min), Lr (27 s), and Lr (22 s). All other known lawrencium isotopes have half-lives under 20 seconds, and the shortest-lived of them (Lr) has a half-life of only 390 milliseconds. However, the undiscovered isotopes with mass numbers 263 to 265 are expected to have longer half-lives (Lr, 5 h; Lr and Lr, 10 h). The half-lives of lawrencium isotopes mostly increase smoothly from Lr to Lr, with a dip from Lr to Lr.\n\nWhile the lightest (Lr to Lr) and heaviest (Lr) lawrencium isotopes are produced only as alpha decay products of dubnium (\"Z\" = 105) isotopes, the middle isotopes (Lr to Lr) can all be produced by bombarding actinide (americium to einsteinium) targets with light ions (from boron to neon). The two most important isotopes, Lr and Lr, are both in this range. Lr can be produced by bombarding californium-249 with 70 MeV boron-11 ions (producing lawrencium-256 and four neutrons), while Lr can be produced by bombarding berkelium-249 with oxygen-18 (producing lawrencium-260, an alpha particle, and three neutrons).\n\nBoth Lr and Lr have half-lives too short to allow a complete chemical purification process. Early experiments with Lr therefore used rapid solvent extraction, with the chelating agent thenoyltrifluoroacetone (TTA) dissolved in methyl isobutyl ketone (MIBK) as the organic phase, and with the aqueous phase being buffered acetate solutions. Ions of different charge (+2, +3, or +4) will then extract into the organic phase under different pH ranges, but this method will not separate the trivalent actinides and thus Lr must be identified by its emitted 8.24 MeV alpha particles. More recent methods have allowed rapid selective elution with α-HIB to take place in enough time to separate out the longer-lived isotope Lr, which can be removed from the catcher foil with 0.05 M hydrochloric acid.\n\n"}
{"id": "30535801", "url": "https://en.wikipedia.org/wiki?curid=30535801", "title": "Milking the Rhino", "text": "Milking the Rhino\n\nMilking the Rhino is a 2009 documentary film, produced by Kartemquin Films, that examines the relationship between the indigenous African wildlife, the villagers who live amongst this wildlife and conservationists who look to keep tourism dollars coming in. Both the Maasai of Kenya and the Ovahimba of Namibia have spent centuries as cattle farmers. With their lands being turned into protected game reserves, these ancient tribes have turned to tourism as a means of survival.\n\nWhile some environmentalists think that community-based conservation is ideal for these villagers, the dangers of drought and the starvation of their cattle remains a constant reality. Stuck between the always growing Western influence that wants Africa to remain a place for sight-seeing safaris and their own ancient cultures, the Maasai and Himba are at a crossroads of cultural change.\n\nThe Kenyan section of the movie features interviews with Kinanjui Lesenderia, an Ndorobo Maasai elder at Il Ngwesi in Kenya, Ian Craig, former rancher and founder of the Lewa Wildlife Conservancy, James Ole Kinyaga, Senior Host of Kenya’s first community-owned and managed eco-lodge and Helen Gichohi, President of the African Wildlife Foundation.\n\nProduced by Kartemquin Films and directed by David E. Simpson, Milking the Rhino won numerous awards at multiple international film festivals, including Best Documentary at the Pan African Film Festival and San Luis Obispo International Film Festival. On April 7, 2009, Milking the Rhino made its television premiere on PBS's Independent Lens.\n\n"}
{"id": "1810323", "url": "https://en.wikipedia.org/wiki?curid=1810323", "title": "NFI Group Inc.", "text": "NFI Group Inc.\n\nNFI Group Inc. (NFI) is North America's largest bus manufacturer specializing in the manufacturing of heavy-duty transit buses and motorcoaches and the distribution of aftermarket parts. Its headquarters are in Winnipeg, Manitoba, with manufacturing, distribution and service centers in both Canada and the United States. New Flyer manufactures integral buses, building both the coachwork and the supporting chassis. The company currently sells vehicles under three brands: New Flyer Xcelsior transit buses, offered with various drive systems and in several lengths; ARBOC Specialty Vehicles small and mid-sized transit buses; and Motor Coach Industries (MCI) D-Series and J-Series motorcoaches. It also sold Daimler’s Setra S407 and S417 coaches until 2018. It supports MCI, ARBOC, and New Flyer buses with NFI Parts - its parts, service, and training division.\n\nNFI is the largest bus and coach manufacturer and distributor in North America and employs over 6,000 people across 31 facilities. The company had a 45% market share of all heavy-duty transit buses and a 39% market share of all motorcoaches produced for North America in 2016. It is listed on the Toronto Stock Exchange under the symbol NFI, and is a constituent of the S&P/TSX Composite Index.\n\nNew Flyer was founded by John Coval in 1930 as the Western Auto and Truck Body Works Ltd. Reflecting an increased focus on bus manufacturing, it changed its name in 1948 to Western Flyer Coach.\n\nIn the 1960s, the company further focused on the urban transit bus market. In 1971, the then-financially struggling Western Flyer was sold to the Manitoba Development Corporation, an agency of the Manitoba government, and renamed Flyer Industries Limited.\n\nOn July 15, 1986, Jan den Oudsten, a descendant of the family who formed Dutch bus manufacturer Den Oudsten Bussen BV, purchased Flyer Industries from the Manitoba government, changing its name to New Flyer Industries Limited.\n\nUnder new leadership, New Flyer developed, tested, and introduced several innovative designs. The company designed and tested North America's first low-floor bus in 1988 and delivered the first production model, called the D40LF, to the Port Authority of New York and New Jersey in 1991. In 1994, New Flyer delivered the first compressed natural gas bus in North America and the world's first hydrogen fuel cell powered bus. In 1995, the company delivered the first low-floor articulated bus in North America to Strathcona County Transit.\n\nIn March 2002, New Flyer was acquired by KPS Capital Partners, an investment company that specializes in turning around struggling businesses. Later that year Jan den Oudsten retired as CEO. He was later inducted into the American Public Transportation Association's Hall of Fame for his work at the company.\n\nIn 2003, King County Metro in Seattle placed an order for 213 hybrid buses, the world's first large order for hybrid buses.\n\nOn December 15, 2013, New Flyer was purchased by private equity firms, Harvest Partners and Lightyear Capital. The company's CEO, John Marinucci, called the purchase an indicator that the company's operational and financial turnaround had been accomplished. On August 19, 2005, New Flyer became a publicly traded company on the Toronto Stock Exchange.\n\n2005 also saw a restyling of New Flyer's popular low-floor coaches with new front and rear endcaps, to modernize and streamline the exterior appearance of the bus.\n\nIn October 2008, New Flyer was named one of Canada's Top 100 Employers, which was announced in The Globe and Mail newspaper, and the company was featured in \"Maclean's\" newsmagazine. Later that month, New Flyer was also named one of Manitoba's Top Employers, which was announced by the \"Winnipeg Free Press\" newspaper.\n\nThe company converted to a corporate structure from a trust-like structure in October 2011.\n\nIn May 2012, New Flyer and Alexander Dennis announced a joint venture to design and manufacture medium-duty low-floor bus (or midi bus) for the North American market. The bus, called the New Flyer MiDi was based on the design of the Alexander Dennis Enviro200. Alexander Dennis engineered and tested the bus, and it was built and marketed by New Flyer under contract. During the partnership around 200 buses were delivered to 22 operators in Canada and US. In May 2017, New Flyer and Alexander Dennis announced their joint venture would end and production of the bus would transition to Alexander Dennis' new North American factory in Indiana where it is produced alongside the double-deck Enviro500 series bus.\n\nIn June 2012 New Flyer, in a joint venture with Mitsubishi Heavy Industries, the Manitoba Government, Manitoba Hydro and Red River College, unveiled a fully electric battery-powered bus.\n\nBrazilian bus manufacturer Marcopolo S.A. acquired a 19.99% stake of New Flyer on January 23, 2013 for $116 million, the maximum it could acquire without offering to buy out other shareholders.\n\nAs competing manufacturer Daimler exited the North American market, New Flyer purchased the aftermarket parts business for its Orion brand of heavy-duty transit buses for $29 million. New Flyer also agreed to take on two outstanding bus manufacturing contracts for New York City Transit and the other for King County Metro in Seattle (giving New Flyer a total of 194 firm bus orders and options for an additional 291 buses). Under the agreement, New Flyer acquired the Orion parts inventory, the company's accounts, license to use proprietary part designs and agreed to provide parts for customer warranty support.\n\nOn June 21, 2013, New Flyer agreed to acquire competing heavy-duty transit bus manufacturer, North American Bus Industries (NABI). Upon completion of NABI's outstanding orders, New Flyer converted the former NABI factory in Anniston, AL into a fourth facility to produce the Xcelsior heavy-duty transit bus.\n\nIn November 10, 2015, New Flyer agreed to acquire motorcoach manufacturer, Motor Coach Industries from KPS Capital Partners for US$459 million, with the deal closing on December 18, 2015.\n\nOn September 22, 2016, Marcopolo S.A. reduced its stake in New Flyer to 10.8%, although it remains the largest individual shareholder.\n\nOn December 1, 2017, New Flyer acquired small and mid-sized bus manufacturer ARBOC Specialty Vehicles for US$95 million.\n\nCurrent New Flyer model numbers are composed of a model code, a power source code and the length of the bus. Note that not all possible combinations have been offered.\n\nSource: New Flyer Industries Inc.\n\nNew Flyer operates four facilities where new transit buses are manufactured.\n\nOf these facilities, the Winnipeg, St Cloud, and Anniston facilities have full production capability. The Crookston facility performs final assembly on buses from shells that are shipped from Winnipeg.\n\nSpecialty bus manufacturing\n\nNew Flyer operates five facilities that distribute parts to customers. Some of these parts are built by New Flyer and some are OEM parts, built by other companies. The centers are geographically spread out to offer ground delivery service within two-days to all of the US and Canada. These facilities also provide parts for both Orion and NABI buses, after New Flyer purchased NABI and acquired the Orion parts business from Daimler in 2013.\n\nNew Flyer operates facilities that fabricate the components used to build buses. TCB Industries is a wholly owned subsidiary that makes components for both New Flyer and other manufacturers.\n\nNew Flyer service centers are typically located in regions with the company's biggest customers. For these customers, New Flyer performs final assembly, pre-delivery inspection, acceptance, and training services for new buses. The Arnprior center also offers maintenance services for any make and model, including mid-life overhauls and collision repair.\n\n"}
{"id": "3870068", "url": "https://en.wikipedia.org/wiki?curid=3870068", "title": "Neuropreservation", "text": "Neuropreservation\n\nNeuropreservation is a type of cryonics procedure where the brain is preserved with the intention of future resuscitation and regrowth of a healthy body around the brain. Usually the brain is left within the head for physical protection, so the whole head is cryopreserved. A cryonics patient who undergoes neuropreservation is said to be a neuropatient.\n\nThe procedure is often done because vitrification of the entire body is not yet available. Vitrification essentially eliminates the mechanical and chemical damage caused by ice formation, at the cost of cryoprotectant toxicity and side effects of dehydration of tissue due to the blood–brain barrier. Although not a direct consequence of vitrification, storage of the vitrified brain directly in liquid nitrogen raises the further aspect of fractures, which are fewer in number but larger in scale in vitrified tissue than frozen tissue, a consequence of cooling from \"T\" (-135 °C) to liquid nitrogen's boiling point (-196 °C).\n\nNeuropreservation has several advantages over whole body preservation. It costs less; neuropatients are easier to transport in case of legal, social, or physical problems; it is possible to do a better job of perfusing and therefore cryoprotecting the brain when there is no need to consider other tissues, and its smaller volume allows more rapid and less expensive cooling. Aubrey de Grey has theorized that neuropatients will be revived after procedures have been perfected on whole body patients, and, therefore, have better chances for revival.\n\nNeuropreservation was first proposed in 1965 by cryonics co-creator Evan Cooper, proposed again in a speculative scientific paper by gerontologist George M. Martin in 1971, and independently proposed yet again in 1974 by Mike Darwin, and Fred and Linda Chamberlain. The Chamberlains were the founders of the Alcor Life Extension Foundation. In 1976 Fred’s father became the first of many neuropreservation patients at Alcor.\n\nPrior to the year 2000, neuropreservation was performed by surgical separation of the body from the head (called cephalic isolation or \"neuroseparation\") at the end of cryoprotectant perfusion performed on the upper body via the ascending aorta. After that year, Alcor began performing cephalic isolation before cryoprotectant perfusion, in deep hypothermia, and then using the carotid and vetebral arteries directly for perfusion with cryoprotectants.\n\n, Alcor, Oregon Cryonics, and KrioRus are the only cryonics organizations that offer neuropreservation. Other organizations, such as the other major provider, Cryonics Institute, avoid it because they are concerned about neuropreservation's negative effect on the public’s perception of cryonics and, especially, because of the adverse effect on the families of patients. Journalists and horror novelists invariably have a field day with \"frozen severed heads\", and focus not on the scientific or humanitarian purposes of cryonics, but on sensationalizing cryonics as grotesque or ridiculous. Their policy to always preserve the entire body prevents anyone leveling those claims at Cryonics Institute. Their goal is to preserve and eventually revive people, so they avoid processes like neurocryopreservation that could damage or otherwise put their patients at risk. Alcor claims there are good technical justifications for neuropreservation, and that they will continue to offer it. Approximately three-quarters of the cryonics patients stored at Alcor are neuropatients.\n\n"}
{"id": "10178335", "url": "https://en.wikipedia.org/wiki?curid=10178335", "title": "Partial oxidation", "text": "Partial oxidation\n\nPartial oxidation (POX) is a type of chemical reaction. It occurs when a substoichiometric fuel-air mixture is partially combusted in a reformer, creating a hydrogen-rich syngas which can then be put to further use, for example in a fuel cell. A distinction is made between \"thermal partial oxidation\" (TPOX) and \"catalytic partial oxidation\" (CPOX).\n\nPartial oxidation is a technically mature process in which natural gas or a heavy hydrocarbon fuel (heating oil) is mixed with a limited amount of oxygen in an exothermic process.\n\n\nThe formulas given for coal and heating oil show only a typical representative of these complex fuels. Water may be added to lower the combustion temperature and reduce soot formation. Yields are below stoichiometric due to some fuel being fully combusted to carbon dioxide and water.\n\nTPOX (\"thermal partial oxidation\") reaction temperatures are dependent on the air-fuel ratio or oxygen-fuel ratio. Typical reaction temperatures are 1200°C and above.\n\nIn CPOX (\"catalytic partial oxidation\") the use of a catalyst reduces the required temperature to around 800°C – 900°C. \n\nThe choice of reforming technique depends on the sulfur content of the fuel being used. CPOX can be employed if the sulfur content is below 50 ppm. A higher sulfur content can poison the catalyst, so the TPOX procedure is used for such fuels. However, recent research shows that CPOX is possible with sulfur contents up to 400ppm.\n\n1926 – Vandeveer and Parr at the University of Illinois used oxygen to replace air.\n\n"}
{"id": "21031034", "url": "https://en.wikipedia.org/wiki?curid=21031034", "title": "Particle segregation", "text": "Particle segregation\n\nIn particle segregation, particulate solids and also quasi solids, such as foams tend to segregate by virtue of differences in the size, and also physical properties such as volume, density, shape and other properties of particles of which they are composed. The process of segregation occurs during as well as during subsequent handling of completed mix and it is pronounced with free-flowing powders. Powders that are not free flowing or that exhibit high forces of cohesion or adhesion between particles of similar or dissimilar composition are often difficult to mix owing to agglomeration. The clumps of particles can be broken down in such cases by the use of mixtures that generate high shear forces or that subject the powder to impact. When these powders have been mixed, however, they are less susceptible to segregation because of the relatively high interparticulates forces that resist interparticulate motion, leading to unmixing.\n\nParticles segregation is commonly called \"demixing\" in industrial environment.\n\nThe five major segregation mechanisms are\n\nSifting occurs when there is a significant variation of particle diameter in a mix. Interparticle motion causes the finer particles to sift through the coarser ones.\n\nVibration of the mixture is bringing the small particles below the coarse one, which has as an effect to have coarse particles closer to the surface of the mixture.\n\nIn this mechanism, the lighter or fluffier particles form a 'fluidized' layer. Only coarser particles can penetrate the fluidized fines and the finer particles remain in the top layer.\n\nThe finer particles in a mix are susceptible to be airborne in the presence of airflow. They move away from the deposition point whereas the coarser particles tend to remain close to the deposition point.\n\nIt can happen that some components form lumps. Those lumps will create non homogeneity in the mix since locally they will concentrate a lot of material of one case.\n"}
{"id": "29021055", "url": "https://en.wikipedia.org/wiki?curid=29021055", "title": "Plasma medicine", "text": "Plasma medicine\n\nPlasma medicine is an emerging field that combines plasma physics, life sciences and clinical medicine. It is being studied in disinfection, healing, and cancer. Most of the research is in vitro and in animal models.\n\nIt uses ionized gas (physical plasma) for medical uses or dental applications . Plasma, often called the fourth state of matter, is an ionized gas containing positive ions and negative ions or electrons, but is approximately charge neutral on the whole. The plasma sources used for plasma medicine are generally low temperature plasmas, and they generate ions, chemically reactive atoms and molecules, and UV-photons. These plasma-generated active species are useful for several bio-medical applications such as sterilization of implants and surgical instruments as well as modifying biomaterial surface properties. Sensitive applications of plasma, like subjecting human body or internal organs to plasma treatment for medical purposes, are also possible. This possibility is profoundly being investigated by research groups worldwide under the highly-interdisciplinary research field called 'plasma medicine'.\n\nPlasma sources used in plasma medicine are typically \"low temperature\" plasma sources operated at atmospheric pressure. In this context, low temperature refers to temperatures similar to room temperature, usually slightly above. There is a strict upper limit of 50 °C when treating tissue to avoid burns. The plasmas are only partially ionized, with less than 1 ppm of the gas being charged species, and the rest composed of neutral gas.\n\nDielectric-barrier discharges are a type of plasma source that limits the current using a dielectric that covers one or both electrodes. A conventional DBD device comprises two planar electrodes with at least one of them covered with a dielectric material and the electrodes are separated by a small gap which is called the discharge gap. DBDs are usually driven by high AC voltages with frequencies in the kHz range. In order to use DC and 50/60 Hz power sources investigators developed the Resistive Barrier Discharge (RBD). However, for medical application of DBD devices, the human body itself can serve as one of the two electrodes making it sufficient to devise plasma sources that consist of only one electrode covered with a dielectric such as alumina or quartz. DBD for medical applications such as for the inactivation of bacteria, for treatment of skin diseases and wounds, tumor treatment and disinfection of skin surface are currently under investigation. The treatment usually takes place in the room air. They are generally powered by several kilovolt biases using either AC or pulsed power supplies.\n\nAtmospheric pressure plasma jets (APPJs) are a collection of plasma sources that use a gas flow to deliver the reactive species generated in the plasma to the tissue or sample. The gas used is usually helium or argon, sometimes with a small amount (< 5%) of O, HO or N mixed in to increase the production of chemically reactive atoms and molecules. The use of a noble gas keeps temperatures low, and makes it simpler to produce a stable discharge. The gas flow also serves to generate a region where room air is in contact with and diffusing in to the noble gas, which is where much of the reactive species are produced.\n\nThere is a large variety in jet designs used in experiments. Many APPJs use a dielectric to limit current, just like in a DBD, but not all do. Those that use a dielectric to limit current usually consists of a tube made of quartz or alumina, with a high voltage electrode wrapped around the outside. There can also be a grounded electrode wrapped around the outside of the dielectric tube. Designs that do not use a dielectric to limit the current use a high voltage pin electrode at the center of the quartz tube. These devices all generate ionization waves that begin inside the jet and propagate out to mix with the ambient air. Even though the plasma may look continuous, it is actually a series of ionization waves or \"plasma bullets\". This ionization wave may or may not treat the tissue being treated. Direct contact of the plasma with the tissue or sample can result in dramatically larger amounts of reactive species, charged species, and photons being delivered to the sample.\n\nOne type of design that does not use a dielectric to limit the current is two planar electrodes with a gas flow running between them. In this case, the plasma does not exit the jet, and only the neutral atoms and molecules and photons reach the sample.\n\nMost devices of this type produce thin (mm diameter) plasma jets, larger surfaces can be treated simultaneously by joining many such jets or by multielectrode systems. Significantly larger surfaces can be treated than with an individual jet. Further, the distance between the device and the skin is to a certain degree variable, as the skin is not needed as a plasma electrode, significantly simplifying use on the patient.\nLow temperature plasma jets have been used in various biomedical applications ranging from the inactivation of bacteria to the killing of cancer cells.\n\nPlasma medicine can be subdivided into three main fields:\n\nOne of challenges is the application of non-thermal plasmas directly on the surface of human body or on internal organs. Whereas for surface modification and biological decontamination both low-pressure and atmospheric pressure plasmas can be used, for direct therapeutic applications only atmospheric pressure plasma sources are applicable.\n\nThe high reactivity of plasma is a result of different plasma components: electromagnetic radiation (UV/VUV, visible light, IR, high-frequency electromagnetic fields, etc.) on the one hand and ions, electrons and reactive chemical species, primarily radicals, on the other. Besides surgical plasma application like argon plasma coagulation (APC), which is based on high-intensity lethal plasma effects, first and sporadic non-thermal therapeutic plasma applications are documented in literature. However, the basic understanding of mechanisms of plasma effects on different components of living systems is in the early beginning.\nEspecially for the field of direct therapeutic plasma application, a fundamental knowledge of the mechanisms of plasma interaction with living cells and tissue is essential as a scientific basis.\n\nThough many positive results have been seen in the experiments, it is not clear what the dominant mechanism of action is for any applications in plasma medicine. The plasma treatment generates reactive oxygen and nitrogen species, which include free radicals. These species include O, O, OH, HO, HO, NO, ONOOH and many others. This increase the oxidative stress on cells, which may explain the selective killing of cancer cells, which are already oxidatively stressed. Additionally, prokaryotic cells may be more sensitive to the oxidative stress than eukaryotic cells, allowing for selective killing of bacteria.\n\nIt is known that electric fields can influence cell membranes from studies on electroporation. Electric fields on the cells being treated by a plasma jet can be high enough to produce electroporation, which may directly influence the cell behavior, or may simply allow more reactive species to enter the cell. Both physical and chemical properties of plasma are known to induce uptake of nanomaterials in cells. For example, the uptake of 20 nm gold nanoparticles can be stimulated in cancer cells using non-lethal doses of cold plasma. Uptake mechanisms involve both energy dependent endocytosis and energy independent transport across cell membranes . \n\nThe role of the immune system in plasma medicine has recently become very convincing. It is possible that the reactive species introduced by a plasma recruit a systemic immune response.\n"}
{"id": "223356", "url": "https://en.wikipedia.org/wiki?curid=223356", "title": "Power inverter", "text": "Power inverter\n\nA power inverter, or inverter, is an electronic device or circuitry that changes direct current (DC) to alternating current (AC).\nThe input voltage, output voltage and frequency, and overall power handling depend on the design of the specific device or circuitry. The inverter does not produce any power; the power is provided by the DC source.\n\nA power inverter can be entirely electronic or may be a combination of mechanical effects (such as a rotary apparatus) and electronic circuitry.\nStatic inverters do not use moving parts in the conversion process.\n\nCircuitry that performs the opposite function, converting AC to DC, is called a rectifier.\n\nA typical power inverter device or circuit requires a relatively stable DC power source capable of supplying enough current for the intended power demands of the system. The input voltage depends on the design and purpose of the inverter. Examples include:\n\nAn inverter can produce a square wave, modified sine wave, pulsed sine wave, pulse width modulated wave (PWM) or sine wave depending on circuit design. \nThe two dominant commercialized waveform types of inverters as of 2007 are modified sine wave and square wave.\n\nThere are two basic designs for producing household plug-in voltage from a lower-voltage DC source, the first of which uses a switching boost converter to produce a higher-voltage DC and then converts to AC. The second method converts DC to AC at battery level and uses a line-frequency transformer to create the output voltage.\n\nThis is one of the simplest waveforms an inverter design can produce and is best suited to low-sensitivity applications such as lighting and heating. Square wave output can produce \"humming\" when connected to audio equipment and is generally unsuitable for sensitive electronics.\n\nA power inverter device which produces a multiple step sinusoidal AC waveform is referred to as a \"sine wave inverter\". To more clearly distinguish the inverters with outputs of much less distortion than the \"modified sine wave\" (three step) inverter designs, the manufacturers often use the phrase \"pure sine wave inverter\". Almost all consumer grade inverters that are sold as a \"pure sine wave inverter\" do not produce a smooth sine wave output at all, just a less choppy output than the square wave (two step) and modified sine wave (three step) inverters. However, this is not critical for most electronics as they deal with the output quite well.\n\nWhere power inverter devices substitute for standard line power, a sine wave output is desirable because many electrical products are engineered to work best with a sine wave AC power source. The standard electric utility provides a sine wave, typically with minor imperfections but sometimes with significant distortion.\nSine wave inverters with more than three steps in the wave output are more complex and have significantly higher cost than a modified sine wave, with only three steps, or square wave (one step) types of the same power handling. Switch-mode power supply (SMPS) devices, such as personal computers or DVD players, function on quality modified sine wave power. AC motors directly operated on non-sinusoidal power may produce extra heat, may have different speed-torque characteristics, or may produce more audible noise than when running on sinusoidal power.\n\nThe modified sine wave output of such an inverter is the sum of two square waves one of which is phase shifted 90 degrees relative to the other. The result is three level waveform with equal intervals of zero volts; peak positive volts; zero volts; peak negative volts and then zero volts. This sequence is repeated. The resultant wave very roughly resembles the shape of a sine wave. Most inexpensive consumer power inverters produce a modified sine wave rather than a pure sine wave.\n\nThe waveform in commercially available modified-sine-wave inverters resembles a square wave but with a pause during the polarity reversal. Switching states are developed for positive, negative and zero voltages. Generally, the peak voltage to RMS voltage ratio does not maintain the same relationship as for a sine wave. The DC bus voltage may be actively regulated, or the \"on\" and \"off\" times can be modified to maintain the same RMS value output up to the DC bus voltage to compensate for DC bus voltage variations.\n\nThe ratio of on to off time can be adjusted to vary the RMS voltage while maintaining a constant frequency with a technique called pulse width modulation (PWM). The generated gate pulses are given to each switch in accordance with the developed pattern to obtain the desired output. Harmonic spectrum in the output depends on the width of the pulses and the modulation frequency. When operating induction motors, voltage harmonics are usually not of concern; however, harmonic distortion in the current waveform introduces additional heating and can produce pulsating torques.\n\nNumerous items of electric equipment will operate quite well on modified sine wave power inverter devices, especially loads that are resistive in nature such as traditional incandescent light bulbs. Items with a switch-mode power supply operate almost entirely without problems, but if the item has a mains transformer, this can overheat depending on how marginally it is rated.\n\nHowever, the load may operate less efficiently owing to the harmonics associated with a modified sine wave and produce a humming noise during operation. This also affects the efficiency of the system as a whole, since the manufacturer's nominal conversion efficiency does not account for harmonics. Therefore, pure sine wave inverters may provide significantly higher efficiency than modified sine wave inverters.\n\nMost AC motors will run on MSW inverters with an efficiency reduction of about 20% owing to the harmonic content. However, they may be quite noisy. A series LC filter tuned to the fundamental frequency may help.\n\nA common modified sine wave inverter topology found in consumer power inverters is as follows:\nAn onboard microcontroller rapidly switches on and off power MOSFETs at high frequency like ~50 kHz. The MOSFETs directly pull from a low voltage DC source (such as a battery). This signal then goes through step-up transformers (generally many smaller transformers are placed in parallel to reduce the overall size of the inverter) to produce a higher voltage signal. The output of the step-up transformers then gets filtered by capacitors to produce a high voltage DC supply. Finally, this DC supply is pulsed with additional power MOSFETs by the microcontroller to produce the final modified sine wave signal.\n\nThe AC output frequency of a power inverter device is usually the same as standard power line frequency, 50 or 60 hertz\n\nIf the output of the device or circuit is to be further conditioned (for example stepped up) then the frequency may be much\nhigher for good transformer efficiency.\n\nThe AC output voltage of a power inverter is often regulated to be the same as the grid line voltage, typically 120 or 240 VAC at the distribution level, even when there are changes in the load that the inverter is driving. This allows the inverter to power numerous devices designed for standard line power.\n\nSome inverters also allow selectable or continuously variable output voltages.\n\nA power inverter will often have an overall power rating expressed in watts or kilowatts. This describes the power that \nwill be available to the device the inverter is driving and, indirectly, the power that will be needed from the DC source.\nSmaller popular consumer and commercial devices designed to mimic line power typically range from 150 to 3000 watts.\n\nNot all inverter applications are solely or primarily concerned with power delivery; in some cases the frequency and\nor waveform properties are used by the follow-on circuit or device.\n\nThe runtime of an inverter powered by batteries is dependent on the battery power and the amount of power being drawn from the inverter at a given time. As the amount of equipment using the inverter increases, the runtime will decrease. In order to prolong the runtime of an inverter, additional batteries can be added to the inverter.\n\nWhen attempting to add more batteries to an inverter, there are two basic options for installation:\n\nAn inverter converts the DC electricity from sources such as batteries or fuel cells to AC electricity. The electricity can be at any required voltage; in particular it can operate AC equipment designed for mains operation, or rectified to produce DC at any desired voltage.\n\nAn uninterruptible power supply (UPS) uses batteries and an inverter to supply AC power when mains power is not available. When mains power is restored, a rectifier supplies DC power to recharge the batteries.\n\nInverter circuits designed to produce a variable output voltage range are often used within motor speed controllers.\nThe DC power for the inverter section can be derived from a normal AC wall outlet or some other source. Control and \nfeedback circuitry is used to adjust the final output of the inverter section which will ultimately determine the \nspeed of the motor operating under its mechanical load. Motor speed control needs are numerous and include \nthings like: industrial motor driven equipment, electric vehicles, rail transport systems, and power tools.\nSwitching states are developed for positive, negative and zero voltages as per the patterns given in the switching Table 1.The generated gate pulses are given to each switch in accordance with the developed pattern and thus the output is obtained.\n\nAn inverter can be used to control the speed of the compressor motor to drive variable refrigerant flow in a refrigeration or air conditioning system to regulate system performance. Such installations are known as inverter compressors. Traditional methods of refrigeration regulation use single-speed compressors switched on and off periodically; inverter-equipped systems have a variable-frequency drive that control the speed of the motor and thus the compressor and cooling output. The variable-frequency AC from the inverter drives a brushless or induction motor, the speed of which is proportional to the frequency of the AC it is fed, so the compressor can be run at variable speeds—eliminating compressor stop-start cycles increases efficiency. A microcontroller typically monitors the temperature in the space to be cooled, and adjusts the speed of the compressor to maintain the desired temperature. The additional electronics and system hardware add cost to the equipment, but can result in substantial savings in operating costs.\n\nGrid-tied inverters are designed to feed into the electric power distribution system. They transfer synchronously with the line and have as little harmonic content as possible. They also need a means of detecting the presence of utility power for safety reasons, so as not to continue to dangerously feed power to the grid during a power outage.\n\nSynchronverters are inverters that are designed to simulate a rotating generator, and can be used to help stabilize grids. They can be designed to react faster than normal generators to changes in grid frequency, and can give conventional generators a chance to respond to very sudden changes in demand or production.\n\nA solar inverter is a balance of system (BOS) component of a photovoltaic system and can be used for both grid-connected and off-grid systems. Solar inverters have special functions adapted for use with photovoltaic arrays, including maximum power point tracking and anti-islanding protection.\nSolar micro-inverters differ from conventional inverters, as an individual micro-inverter is attached to each solar panel. This can improve the overall efficiency of the system. The output from several micro-inverters is then combined and often fed to the electrical grid.\n\nInverters convert low frequency main AC power to higher frequency for use in induction heating. To do this, AC power is first rectified to provide DC power. The inverter then changes the DC power to high frequency AC power. Due to the reduction in the number of DC sources employed, the structure becomes more reliable and the output voltage has higher resolution due to an increase in the number of steps so that the reference sinusoidal voltage can be better achieved. This configuration has recently become very popular in AC power supply and adjustable speed drive applications. This new inverter can avoid extra clamping diodes or voltage balancing capacitors.\n\nThere are three kinds of level shifted modulation techniques, namely:\n\nWith HVDC power transmission, AC power is rectified and high voltage DC power is transmitted to another location. At the receiving location, an inverter in a static inverter plant converts the power back to AC. The inverter must be synchronized with grid frequency and phase and minimize harmonic generation.\n\nElectroshock weapons and tasers have a DC/AC inverter to generate several tens of thousands of V AC out of a small 9 V DC battery. First the 9 V DC is converted to 400–2000 V AC with a compact high frequency transformer, which is then rectified and temporarily stored in a high voltage capacitor until a pre-set threshold voltage is reached. When the threshold (set by way of an airgap or TRIAC) is reached, the capacitor dumps its entire load into a pulse transformer which then steps it up to its final output voltage of 20–60 kV. A variant of the principle is also used in electronic flash and bug zappers, though they rely on a capacitor-based voltage multiplier to achieve their high voltage.\n\nTypical applications for power inverters include:\n\nIn one simple inverter circuit, DC power is connected to a transformer through the center tap of the primary winding. A switch is rapidly switched back and forth to allow current to flow back to the DC source following two alternate paths through one end of the primary winding and then the other. The alternation of the direction of current in the primary winding of the transformer produces alternating current (AC) in the secondary circuit.\n\nThe electromechanical version of the switching device includes two stationary contacts and a spring supported moving contact. The spring holds the movable contact against one of the stationary contacts and an electromagnet pulls the movable contact to the opposite stationary contact. The current in the electromagnet is interrupted by the action of the switch so that the switch continually switches rapidly back and forth. This type of electromechanical inverter switch, called a vibrator or buzzer, was once used in vacuum tube automobile radios. A similar mechanism has been used in door bells, buzzers and tattoo machines.\n\nAs they became available with adequate power ratings, transistors and various other types of semiconductor switches have been incorporated into inverter circuit designs. Certain ratings, especially for large systems (many kilowatts) use thyristors (SCR). SCRs provide large power handling capability in a semiconductor device, and can readily be controlled over a variable firing range.\n\nThe switch in the simple inverter described above, when not coupled to an output transformer, produces a square voltage waveform due to its simple off and on nature as opposed to the sinusoidal waveform that is the usual waveform of an AC power supply. Using Fourier analysis, periodic waveforms are represented as the sum of an infinite series of sine waves. The sine wave that has the same frequency as the original waveform is called the fundamental component. The other sine waves, called \"harmonics\", that are included in the series have frequencies that are integral multiples of the fundamental frequency.\n\nFourier analysis can be used to calculate the total harmonic distortion (THD). The total harmonic distortion (THD) is the square root of the sum of the squares of the harmonic voltages divided by the fundamental voltage:\nformula_1\n\nThere are many different power circuit topologies and control strategies used in inverter designs. Different design approaches address various issues that may be more or less important depending on the way that the inverter is intended to be used.\n\nThe issue of waveform quality can be addressed in many ways. Capacitors and inductors can be used to filter the waveform. If the design includes a transformer, filtering can be applied to the primary or the secondary side of the transformer or to both sides. Low-pass filters are applied to allow the fundamental component of the waveform to pass to the output while limiting the passage of the harmonic components. If the inverter is designed to provide power at a fixed frequency, a resonant filter can be used. For an adjustable frequency inverter, the filter must be tuned to a frequency that is above the maximum fundamental frequency.\n\nSince most loads contain inductance, feedback rectifiers or antiparallel diodes are often connected across each semiconductor switch to provide a path for the peak inductive load current when the switch is turned off. The antiparallel diodes are somewhat similar to the \"freewheeling diodes\" used in AC/DC converter circuits.\n\nFourier analysis reveals that a waveform, like a square wave, that is anti-symmetrical about the 180 degree point contains only odd harmonics, the 3rd, 5th, 7th, etc. Waveforms that have steps of certain widths and heights can attenuate certain lower harmonics at the expense of amplifying higher harmonics. For example, by inserting a zero-voltage step between the positive and negative sections of the square-wave, all of the harmonics that are divisible by three (3rd and 9th, etc.) can be eliminated. That leaves only the 5th, 7th, 11th, 13th etc. The required width of the steps is one third of the period for each of the positive and negative steps and one sixth of the period for each of the zero-voltage steps.\n\nChanging the square wave as described above is an example of pulse-width modulation (PWM). Modulating, or regulating the width of a square-wave pulse is often used as a method of regulating or adjusting an inverter's output voltage. When voltage control is not required, a fixed pulse width can be selected to reduce or eliminate selected harmonics. Harmonic elimination techniques are generally applied to the lowest harmonics because filtering is much more practical at high frequencies, where the filter components can be much smaller and less expensive. \"Multiple pulse-width\" or \"carrier based\" PWM control schemes produce waveforms that are composed of many narrow pulses. The frequency represented by the number of narrow pulses per second is called the \"switching frequency\" or \"carrier frequency\". These control schemes are often used in variable-frequency motor control inverters because they allow a wide range of output voltage and frequency adjustment while also improving the quality of the waveform.\n\nMultilevel inverters provide another approach to harmonic cancellation. Multilevel inverters provide an output waveform that exhibits multiple steps at several voltage levels. For example, it is possible to produce a more sinusoidal wave by having split-rail direct current inputs at two voltages, or positive and negative inputs with a central ground. By connecting the inverter output terminals in sequence between the positive rail and ground, the positive rail and the negative rail, the ground rail and the negative rail, then both to the ground rail, a stepped waveform is generated at the inverter output. This is an example of a three level inverter: the two voltages and ground.\n\nResonant inverters produce sine waves with LC circuits to remove the harmonics from a simple square wave. Typically there are several series- and parallel-resonant LC circuits, each tuned to a different harmonic of the power line frequency. This simplifies the electronics, but the inductors and capacitors tend to be large and heavy. Its high efficiency makes this approach popular in large uninterruptible power supplies in data centers that run the inverter continuously in an \"online\" mode to avoid any switchover transient when power is lost.\n\nA closely related approach uses a ferroresonant transformer, also known as a constant voltage transformer, to remove harmonics and to store enough energy to sustain the load for a few AC cycles. This property makes them useful in standby power supplies to eliminate the switchover transient that otherwise occurs during a power failure while the normally idle inverter starts and the mechanical relays are switching to its output.\n\nA proposal suggested in \"Power Electronics\" magazine utilizes two voltages as an improvement over the common commercialized technology, which can only apply DC bus voltage in either direction or turn it off. The proposal adds intermediate voltages to the common design. Each cycle sees the following sequence of delivered voltages: v1, v2, v1, 0, −v1, −v2, −v1.\n\nThree-phase inverters are used for variable-frequency drive applications and for high power applications such as HVDC power transmission. A basic three-phase inverter consists of three single-phase inverter switches each connected to one of the three load terminals. For the most basic control scheme, the operation of the three switches is coordinated so that one switch operates at each 60 degree point of the fundamental output waveform. This creates a line-to-line output waveform that has six steps. The six-step waveform has a zero-voltage step between the positive and negative sections of the square-wave such that the harmonics that are multiples of three are eliminated as described above. When carrier-based PWM techniques are applied to six-step waveforms, the basic overall shape, or \"envelope\", of the waveform is retained so that the 3rd harmonic and its multiples are cancelled.\n\nTo construct inverters with higher power ratings, two six-step three-phase inverters can be connected in parallel for a higher current rating or in series for a higher voltage rating. In either case, the output waveforms are phase shifted to obtain a 12-step waveform. If additional inverters are combined, an 18-step inverter is obtained with three inverters etc. Although inverters are usually combined for the purpose of achieving increased voltage or current ratings, the quality of the waveform is improved as well.\n\nCompared to other household electric devices, inverters are large in size and volume. In 2014 Google together with IEEE started an open competition to build a (much) smaller power inverter, with a $1,000,000 prize.\n\nFrom the late nineteenth century through the middle of the twentieth century, DC-to-AC power conversion was accomplished using rotary converters or motor-generator sets (M-G sets). In the early twentieth century, vacuum tubes and gas-filled tubes began to be used as switches in inverter circuits. The most widely used type of tube was the thyratron.\n\nThe origins of electromechanical inverters explain the source of the term \"inverter\". Early AC-to-DC converters used an induction or synchronous AC motor direct-connected to a generator (dynamo) so that the generator's commutator reversed its connections at exactly the right moments to produce DC. A later development is the synchronous converter, in which the motor and generator windings are combined into one armature, with slip rings at one end and a commutator at the other and only one field frame. The result with either is AC-in, DC-out. With an M-G set, the DC can be considered to be separately generated from the AC; with a synchronous converter, in a certain sense it can be considered to be \"mechanically rectified AC\". Given the right auxiliary and control equipment, an M-G set or rotary converter can be \"run backwards\", converting DC to AC. Hence an inverter is an inverted converter.\n\nSince early transistors were not available with sufficient voltage and current ratings for most inverter applications, it was the 1957 introduction of the thyristor or silicon-controlled rectifier (SCR) that initiated the transition to solid state inverter circuits.\n\nThe \"commutation\" requirements of SCRs are a key consideration in SCR circuit designs. SCRs do not turn off or \"commutate\" automatically when the gate control signal is shut off. They only turn off when the forward current is reduced to below the minimum holding current, which varies with each kind of SCR, through some external process. For SCRs connected to an AC power source, commutation occurs naturally every time the polarity of the source voltage reverses. SCRs connected to a DC power source usually require a means of forced commutation that forces the current to zero when commutation is required. The least complicated SCR circuits employ natural commutation rather than forced commutation. With the addition of forced commutation circuits, SCRs have been used in the types of inverter circuits described above.\n\nIn applications where inverters transfer power from a DC power source to an AC power source, it is possible to use AC-to-DC controlled rectifier circuits operating in the inversion mode. In the inversion mode, a controlled rectifier circuit operates as a line commutated inverter. This type of operation can be used in HVDC power transmission systems and in regenerative braking operation of motor control systems.\n\nAnother type of SCR inverter circuit is the current source input (CSI) inverter. A CSI inverter is the dual of a six-step voltage source inverter. With a current source inverter, the DC power supply is configured as a current source rather than a voltage source. The inverter SCRs are switched in a six-step sequence to direct the current to a three-phase AC load as a stepped current waveform. CSI inverter commutation methods include load commutation and parallel capacitor commutation. With both methods, the input current regulation assists the commutation. With load commutation, the load is a synchronous motor operated at a leading power factor.\n\nAs they have become available in higher voltage and current ratings, semiconductors such as transistors or IGBTs that can be turned off by means of control signals have become the preferred switching components for use in inverter circuits.\n\nRectifier circuits are often classified by the number of current pulses that flow to the DC side of the rectifier per cycle of AC input voltage. A single-phase half-wave rectifier is a one-pulse circuit and a single-phase full-wave rectifier is a two-pulse circuit. A three-phase half-wave rectifier is a three-pulse circuit and a three-phase full-wave rectifier is a six-pulse circuit.\n\nWith three-phase rectifiers, two or more rectifiers are sometimes connected in series or parallel to obtain higher voltage or current ratings. The rectifier inputs are supplied from special transformers that provide phase shifted outputs. This has the effect of phase multiplication. Six phases are obtained from two transformers, twelve phases from three transformers and so on. The associated rectifier circuits are 12-pulse rectifiers, 18-pulse rectifiers and so on...\n\nWhen controlled rectifier circuits are operated in the inversion mode, they would be classified by pulse number also. Rectifier circuits that have a higher pulse number have reduced harmonic content in the AC input current and reduced ripple in the DC output voltage. In the inversion mode, circuits that have a higher pulse number have lower harmonic content in the AC output voltage waveform.\n\nThe large switching devices for power transmission applications installed until 1970 predominantly used mercury-arc valves.\nModern inverters are usually solid state (static inverters). A modern design method features components arranged in an H bridge configuration. \nThis design is also quite popular with smaller-scale consumer devices.\n\nUsing 3-D printing and novel semiconductors, researchers at the Department of Energy's Oak Ridge National Laboratory have created a power inverter that could make electric vehicles lighter, more powerful and more efficient.\n\n\n\n"}
{"id": "3191864", "url": "https://en.wikipedia.org/wiki?curid=3191864", "title": "Production string", "text": "Production string\n\nThe production string is a part of an oil well that is composed of the production tubing and other completion components and serves as the conduit through which the production fluid flows from the oil reservoir to the surface through the wellhead. Its purpose is to both contain the fluids from contaminating the environment or eroding the other well structures, such as the casing.\n\n"}
{"id": "23322", "url": "https://en.wikipedia.org/wiki?curid=23322", "title": "Protactinium", "text": "Protactinium\n\nProtactinium (formerly protoactinium) is a chemical element with symbol Pa and atomic number 91. It is a dense, silvery-gray actinide metal which readily reacts with oxygen, water vapor and inorganic acids. It forms various chemical compounds in which protactinium is usually present in the oxidation state +5, but it can also assume +4 and even +3 or +2 states. Concentrations of protactinium in the Earth's crust are typically a few parts per trillion, but may reach up to a few parts per million in some uraninite ore deposits. Because of its scarcity, high radioactivity and high toxicity, there are currently no uses for protactinium outside scientific research, and for this purpose, protactinium is mostly extracted from spent nuclear fuel.\n\nProtactinium was first identified in 1913 by Kasimir Fajans and Oswald Helmuth Göhring and named \"brevium\" because of the short half-life of the specific isotope studied, i.e. protactinium-234. A more stable isotope of protactinium, Pa, was discovered in 1917/18 by Otto Hahn and Lise Meitner, and they chose the name proto-actinium, but the IUPAC finally named it \"protactinium\" in 1949 and confirmed Hahn and Meitner as discoverers. The new name meant \"(nuclear) precursor of actinium\" and reflected that actinium is a product of radioactive decay of protactinium. John Arnold Cranston (working with Frederick Soddy and Ada Hitchins) is also credited with discovering the most stable isotope in 1915, but delayed his announcement due to being called up for service in the First World War.\n\nThe longest-lived and most abundant (nearly 100%) naturally occurring isotope of protactinium, protactinium-231, has a half-life of 32,760 years and is a decay product of uranium-235. Much smaller trace amounts of the short-lived nuclear isomer protactinium-234m occur in the decay chain of uranium-238. Protactinium-233 results from the decay of thorium-233 as part of the chain of events used to produce uranium-233 by neutron irradiation of thorium-232. It is an undesired intermediate product in thorium-based nuclear reactors and is therefore removed from the active zone of the reactor during the breeding process. Analysis of the relative concentrations of various uranium, thorium and protactinium isotopes in water and minerals is used in radiometric dating of sediments which are up to 175,000 years old and in modeling of various geological processes.\n\nIn 1871, Dmitri Mendeleev predicted the existence of an element between thorium and uranium. The actinide element group was unknown at the time. Therefore, uranium was positioned below tungsten in group VI, and thorium below zirconium in group IV, leaving the space below tantalum in group V empty and, until the 1950s, periodic tables were published with this structure. For a long time chemists searched for eka-tantalum as an element with similar chemical properties to tantalum, making a discovery of protactinium nearly impossible. Tantalum's heavier analogue was later found to be the transuranic element dubnium.\n\nIn 1900, William Crookes isolated protactinium as an intensely radioactive material from uranium; however, he could not characterize it as a new chemical element and thus named it uranium-X (UX). Crookes dissolved uranium nitrate in ether, the residual aqueous phase contains most of the and . His method was still used in the 1950s to isolate and from uranium compounds. Protactinium was first identified in 1913, when Kasimir Fajans and Oswald Helmuth Göhring encountered the isotope Pa during their studies of the decay chains of uranium-238: → → → . They named the new element \"brevium\" (from the Latin word, \"brevis\", meaning brief or short) because of its short half-life, 6.7 hours for . In 1917/18, two groups of scientists, Otto Hahn and Lise Meitner of Germany and Frederick Soddy and John Cranston of Great Britain, independently discovered another isotope of protactinium, Pa having a much longer half-life of about 32,000 years. Thus the name \"brevium\" was changed to \"protoactinium\" as the new element was part of the decay chain of uranium-235 as the parent of actinium (from = \"protos\" meaning \"first\", \"before\"). For ease of pronunciation, the name was shortened to \"protactinium\" by the IUPAC in 1949. The discovery of protactinium completed one of the last gaps in the early versions of the periodic table, proposed by Mendeleev in 1869, and it brought to fame the involved scientists.\n\nAristid von Grosse produced 2 milligrams of PaO in 1927, and in 1934 first isolated elemental protactinium from 0.1 milligrams of PaO. He used two different procedures: in the first one, protactinium oxide was irradiated by 35 keV electrons in vacuum. In another method, called the van Arkel–de Boer process, the oxide was chemically converted to a halide (chloride, bromide or iodide) and then reduced in a vacuum with an electrically heated metallic filament:\n\nIn 1961, the United Kingdom Atomic Energy Authority (UKAEA) produced 127 grams of 99.9% pure protactinium-231 by processing 60 tonnes of waste material in a 12-stage process, at a cost of about 500,000 USD. For many years, this was the world's only significant supply of protactinium, which was provided to various laboratories for scientific studies. Oak Ridge National Laboratory in the US provided protactinium at a cost of about 280 USD/gram.\n\nTwenty-nine radioisotopes of protactinium have been discovered, the most stable being Pa with a half-life of 32,760 years, Pa with a half-life of 27 days, and Pa with a half-life of 17.4 days. All of the remaining isotopes have half-lives shorter than 1.6 days, and the majority of these have half-lives less than 1.8 seconds. Protactinium also has two nuclear isomers, Pa (half-life 1.2 milliseconds) and Pa (half-life 1.17 minutes).\n\nThe primary decay mode for isotopes of protactinium lighter than (and including) the most stable isotope Pa (i.e., Pa to Pa) is alpha decay and the primary mode for the heavier isotopes (i.e., Pa to Pa) is beta decay. The primary decay products of isotopes of protactinium lighter than (and including) Pa are actinium isotopes and the primary decay products for the heavier isotopes of protactinium are uranium isotopes.\n\nProtactinium is one of the rarest and most expensive naturally occurring elements. It is found in the form of two isotopes – Pa and Pa, with the isotope Pa occurring in two different energy states. Nearly all natural protactinium is protactinium-231. It is an alpha emitter and is formed by the decay of uranium-235, whereas the beta radiating protactinium-234 is produced as a result of . Nearly all uranium-238 (99.8%) decays first to the shorter-lived Pa isomer.\n\nProtactinium occurs in uraninite (pitchblende) at concentrations of about 0.3-3 parts Pa per million parts (ppm) of ore. Whereas the usual content is closer to 0.3 ppm (e.g. in Jáchymov, Czech Republic), some ores from the Democratic Republic of the Congo have about 3 ppm. Protactinium is homogeneously dispersed in most natural materials and in water, but at much lower concentrations on the order of one part per trillion, that corresponds to the radioactivity of 0.1 picocuries (pCi)/g. There is about 500 times more protactinium in sandy soil particles than in water, even the water present in the same sample of soil. Much higher ratios of 2,000 and above are measured in loam soils and clays, such as bentonite.\n\nTwo major protactinium isotopes, Pa and Pa, are produced from thorium in nuclear reactors; both are undesirable and are usually removed, thereby adding complexity to the reactor design and operation. In particular, Th via (\"n\",2\"n\") reactions produces Th which quickly (half-life 25.5 hours) decays to Pa. The last isotope, while not a transuranic waste, has a long half-life of 32,760 years and is a major contributor to the long term radiotoxicity of spent nuclear fuel.\n\nProtactinium-233 is formed upon neutron capture by Th. It further either decays to uranium-233 or captures another neutron and converts into the non-fissile uranium-234. Pa has a relatively long half-life of 27 days and high cross section for neutron capture (the so-called \"neutron poison\"). Thus instead of rapidly decaying to the useful U, a significant fraction of Pa converts to non-fissile isotopes and consumes neutrons, degrading the reactor efficiency. To avoid this, Pa is extracted from the active zone of thorium molten salt reactors, during their operation, so that it only decays to U. This is achieved using several meters tall columns of molten bismuth with lithium dissolved in it. In a simplified scenario, lithium selectively reduces protactinium salts to protactinium metal which is then extracted from the molten-salt cycle, and bismuth is merely a carrier. It is chosen because of its low melting point (271 °C), low vapor pressure, good solubility for lithium and actinides, and immiscibility with molten halides.\n\nBefore the advent of nuclear reactors, protactinium was separated for scientific experiments from uranium ores. Nowadays, it is mostly produced as an intermediate product of nuclear fission in thorium high-temperature reactors:\n\nProtactinium metal can be prepared by reduction of its fluoride with calcium fluoride, lithium or barium at a temperature of 1300–1400 °C.\n\nProtactinium is an actinide which is positioned in the periodic table to the left of uranium and to the right of thorium, and many of its physical properties are intermediate between those two actinides. So, protactinium is more dense and rigid than thorium but is lighter than uranium, and its melting point is lower than that of thorium and higher than that of uranium. The thermal expansion, electrical and thermal conductivities of these three elements are comparable and are typical of post-transition metals. The estimated shear modulus of protactinium is similar to that of titanium. Protactinium is a metal with silvery-gray luster that is preserved for some time in air. Protactinium easily reacts with oxygen, water vapor and acids, but not with alkalis.\n\nAt room temperature, protactinium crystallizes in the body-centered tetragonal structure which can be regarded as distorted body-centered cubic lattice; this structure does not change upon compression up to 53 GPa. The structure changes to face-centered cubic (\"fcc\") upon cooling from high temperature, at about 1200 °C. The thermal expansion coefficient of the tetragonal phase between room temperature and 700 °C is 9.9/°C.\n\nProtactinium is paramagnetic and no magnetic transitions are known for it at any temperature. It becomes superconductive at temperatures below 1.4 K. Protactinium tetrachloride is paramagnetic at room temperature but turns ferromagnetic upon cooling to 182 K.\n\nProtactinium exists in two major oxidation states, +4 and +5, both in solids and solutions, and the +3 and +2 states were observed in some solid phases. As the electron configuration of the neutral atom is [Rn]5f6d7s, the +5 oxidation state corresponds to the low-energy (and thus favored) 5f configuration. Both +4 and +5 states easily form hydroxides in water with the predominant ions being Pa(OH), , and Pa(OH), all colorless. Other known protactinium ions include , , PaF, , , and .\n\nHere \"a\", \"b\" and \"c\" are lattice constants in picometers, No is space group number and \"Z\" is the number of formula units per unit cell; \"fcc\" stands for the face-centered cubic symmetry. Density was not measured directly but calculated from the lattice parameters.\n\nProtactinium oxides are known for the metal oxidation states +2, +4 and +5. The most stable is white pentoxide PaO, which can be produced by igniting protactinium(V) hydroxide in air at a temperature of 500 °C. Its crystal structure is cubic, and the chemical composition is often non-stoichiometric, described as PaO. Another phase of this oxide with orthorhombic symmetry has also been reported. The black dioxide PaO is obtained from the pentoxide by reducing it at 1550 °C with hydrogen. It is not readily soluble in either dilute or concentrated nitric, hydrochloric or sulfuric acids, but easily dissolves in hydrofluoric acid. The dioxide can be converted back to pentoxide by heating in oxygen-containing atmosphere to 1100 °C. The monoxide PaO has only been observed as a thin coating on protactinium metal, but not in an isolated bulk form.\n\nProtactinium forms mixed binary oxides with various metals. With alkali metals \"A\", the crystals have a chemical formula APaO and perovskite structure, or APaO and distorted rock-salt structure, or APaO where oxygen atoms form a hexagonal close-packed lattice. In all these materials, protactinium ions are octahedrally coordinated. The pentoxide PaO combines with rare-earth metal oxides RO to form various nonstoichiometric mixed-oxides, also of perovskite structure.\n\nProtactinium oxides are basic; they easily convert to hydroxides and can form various salts, such as sulfates, phosphates, nitrates, etc. The nitrate is usually white but can be brown due to radiolytic decomposition. Heating the nitrate in air at 400 °C converts it to the white protactinium pentoxide. The polytrioxophosphate Pa(PO) can be produced by reacting difluoride sulfate PaFSO with phosphoric acid (HPO) under inert gas atmosphere. Heating the product to about 900 °C eliminates the reaction by-products such as hydrofluoric acid, sulfur trioxide and phosphoric anhydride. Heating to higher temperatures in an inert atmosphere decomposes Pa(PO) into the diphosphate PaPO, which is analogous to diphosphates of other actinides. In the diphosphate, the PO groups form pyramids of C symmetry. Heating PaPO in air to 1400 °C decomposes it into the pentoxides of phosphorus and protactinium.\n\nProtactinium(V) fluoride forms white crystals where protactinium ions are arranged in pentagonal bipyramids and coordinated by 7 other ions. The coordination is the same in protactinium(V) chloride, but the color is yellow. The coordination changes to octahedral in the brown protactinium(V) bromide and is unknown for protactinium(V) iodide. The protactinium coordination in all its tetrahalides is 8, but the arrangement is square antiprismatic in protactinium(IV) fluoride and dodecahedral in the chloride and bromide. Brown-colored protactinium(III) iodide has been reported where protactinium ions are 8-coordinated in a bicapped trigonal prismatic arrangement.\nProtactinium(V) fluoride and protactinium(V) chloride have a polymeric structure of monoclinic symmetry. There, within one polymeric chain, all the halide atoms lie in one graphite-like plane and form planar pentagons around the protactinium ions. The coordination 7 of protactinium originates from the 5 halide atoms and two bonds to protactinium atoms belonging to the nearby chains. These compounds easily hydrolyze in water. The pentachloride melts at 300 °C and sublimates at even lower temperatures.\n\nProtactinium(V) fluoride can be prepared by reacting protactinium oxide with either bromine pentafluoride or bromine trifluoride at about 600 °C, and protactinium(IV) fluoride is obtained from the oxide and a mixture of hydrogen and hydrogen fluoride at 600 °C; a large excess of hydrogen is required to remove atmospheric oxygen leaks into the reaction.\n\nProtactinium(V) chloride is prepared by reacting protactinium oxide with carbon tetrachloride at temperature of 200–300 °C. The by-products (such as PaOCl) are removed by fractional sublimation. Reduction of protactinium(V) chloride with hydrogen at about 800 °C yields protactinium(IV) chloride – a yellow-green solid which sublimes in vacuum at 400 °C; it can also be obtained directly from protactinium dioxide by treating it with carbon tetrachloride at 400 °C.\n\nProtactinium bromides are produced by the action of aluminium bromide, hydrogen bromide, carbon tetrabromide or a mixture of hydrogen bromide and thionyl bromide on protactinium oxide. An alternative reaction is between protactinium pentachloride and hydrogen bromide or thionyl bromide. Protactinium(V) bromide has two similar monoclinic forms, one is obtained by sublimation at 400–410 °C and another by sublimation at slightly lower temperature of 390–400 °C.\n\nProtactinium iodides result from the oxides and aluminium iodide or ammonium iodide heated to 600 °C. Protactinium(III) iodide was obtained by heating protactinium(V) iodide in vacuum. As with oxides, protactinium forms mixed halides with alkali metals. Among those, most remarkable is NaPaF where protactinium ion is symmetrically surrounded by 8 F ions which form a nearly perfect cube.\n\nMore complex protactinium fluorides are also known such as PaF and ternary fluorides of the types MPaF (M = Li, Na, K, Rb, Cs or NH), MPaF (M = K, Rb, Cs or NH) and MPaF (M = Li, Na, Rb, Cs), all being white crystalline solids. The MPaF formula can be represented as a combination of MF and PaF. These compounds can be obtained by evaporating a hydrofluoric acid solution containing these both complexes. For the small alkali cations like Na, the crystal structure is tetragonal, whereas it lowers to orthorphombic for larger cations K, Rb, Cs or NH. A similar variation was observed for the MPaF fluorides, namely the crystal symmetry was dependent on the cation and differed for CsPaF and MPaF (M = K, Rb or NH).\n\nOxyhalides and oxysulfides of protactinium are known. PaOBr has a monoclinic structure composed of double-chain units where protactinium has coordination 7 and is arranged into pentagonal bipyramids. The chains are interconnected through oxygen and bromine atoms, and each oxygen atom is related to three protactinium atoms. PaOS is a light-yellow non-volatile solid with a cubic crystal lattice isostructural to that of other actinide oxysulfides. It is obtained by reacting protactinium(V) chloride with a mixture of hydrogen sulfide and carbon disulfide at 900 °C.\n\nIn hydrides and nitrides, protactinium has a low oxidation state of about +3. The hydride is obtained by direct action of hydrogen on the metal at 250 °C, and the nitride is a product of ammonia and protactinium tetrachloride or pentachloride. This bright yellow solid is stable to heating to 800 °C in vacuum. Protactinium carbide PaC is formed by reduction of protactinium tetrafluoride with barium in a carbon crucible at a temperature of about 1400 °C. Protactinium forms borohydrides which include Pa(BH). It has an unusual polymeric structure with helical chains where the protactinium atom has coordination number of 12 and is surrounded by six BH ions.\n\nProtactinium(IV) forms a tetrahedral complex tetrakis(cyclopentadienyl)protactinium(IV) (or Pa(CH)) with four cyclopentadienyl rings, which can be synthesized by reacting protactinium(IV) chloride with molten Be(CH). One ring can be substituted with a halide atom. Another organometallic complex is golden-yellow bis(π-cyclooctatetraene) protactinium, or protactinocene, Pa(CH), which is analogous in structure to uranocene. There, the metal atom is sandwiched between two cyclooctatetraene ligands. Similar to uranocene, it can be prepared by reacting protactinium tetrachloride with dipotassium cyclooctatetraenide, KCH, in tetrahydrofuran.\n\nAlthough protactinium is located in the periodic table between uranium and thorium, which both have numerous applications, owing to its scarcity, high radioactivity and high toxicity, there are currently no uses for protactinium outside scientific research.\n\nProtactinium-231 arises from the decay of uranium-235 formed in nuclear reactors, and by the reaction Th + n → Th + 2n and subsequent beta decay. It was once thought to be able to support a nuclear chain reaction, which could in principle be used to build nuclear weapons: the physicist once estimated the associated critical mass as . However, the possibility of criticality of Pa has been ruled out since then.\n\nWith the advent of highly sensitive mass spectrometers, an application of Pa as a tracer in geology and paleoceanography has become possible. So, the ratio of protactinium-231 to thorium-230 is used for radiometric dating of sediments which are up to 175,000 years old and in modeling of the formation of minerals. In particular, its evaluation in oceanic sediments allowed to reconstruct the movements of North Atlantic water bodies during the last melting of Ice Age glaciers. Some of the protactinium-related dating variations rely on the analysis of the relative concentrations for several long-living members of the uranium decay chain – uranium, protactinium, and thorium, for example. These elements have 6, 5 and 4 valence electrons and thus favor +6, +5 and +4 oxidation states, respectively, and show different physical and chemical properties. So, thorium and protactinium, but not uranium compounds are poorly soluble in aqueous solutions, and precipitate into sediments; the precipitation rate is faster for thorium than for protactinium. Besides, the concentration analysis for both protactinium-231 (half-life 32,760 years) and thorium-230 (half-life 75,380 years) allows to improve the accuracy compared to when only one isotope is measured; this double-isotope method is also weakly sensitive to inhomogeneities in the spatial distribution of the isotopes and to variations in their precipitation rate.\n\nProtactinium is both toxic and highly radioactive and thus all manipulations with it are performed in a sealed glove box. Its major isotope Pa has a specific activity of per gram and primarily emits alpha-particles with an energy of 5 MeV, which can be stopped by a thin layer of any material. However, it slowly decays, with a half-life of 32,760 years, into Ac, which has a specific activity of per gram, emits both alpha and beta radiation, and has a much shorter half-life of 22 years. Ac, in turn, decays into lighter isotopes with even shorter half-lives and much greater specific activities (SA), as summarized in the table below showing the decay chain of protactinium-231.\n\nAs protactinium is present in small amounts in most natural products and materials, it is ingested with food or water and inhaled with air. Only about 0.05% of ingested protactinium is absorbed into the blood and the remainder is excreted. From the blood, about 40% of the protactinium\ndeposits in the bones, about 15% goes to the liver, 2% to the kidneys, and the rest leaves the body. The biological half-life of protactinium is about 50 years in the bones, whereas in other organs the kinetics has a fast and slow component. So in the liver 70% of protactinium have a half-life of 10 days and 30% remain for 60 days. The corresponding values for kidneys are 20% (10 days) and 80% (60 days). In all these organs, protactinium promotes cancer via its radioactivity. The maximum safe dose of Pa in the human body is , which corresponds to 0.5 micrograms of Pa. This isotope is 2.5 times more toxic than hydrocyanic acid. The maximum allowed concentrations of Pa in the air in Germany is .\n\n\n"}
{"id": "32389923", "url": "https://en.wikipedia.org/wiki?curid=32389923", "title": "RCP Design Global", "text": "RCP Design Global\n\nRCP Design Global or RCP is an independent design agency based in Tours and Paris (France) founded by Régine Charvet-Pello in . RCP is predominantly based in the transport and mobility design, and specialises in urban transport, High-speed rail, interiors, public spaces and street furniture. RCP is the French leader on sensory design.\n\n1986 : \n1996 : \n2002 : \n2003 : \n2004 : \n2006 : \n2007 : \n2009 : \n2010 : \n2011 : \n\nRCP is most notable for the number of national and international design awards they have won over the years.\n\n\nRCP Design Global\nHead Office\n\n56 avenue Marcel Dassault, 37200 Tours France\n\nRCP Design Global\nParis Office\n\n4 Place d'Estienne d'Orves, 75009 Paris France\n\n\n"}
{"id": "8744903", "url": "https://en.wikipedia.org/wiki?curid=8744903", "title": "The Legacy of Luna", "text": "The Legacy of Luna\n\nThe Legacy of Luna is a book written by Julia Butterfly Hill about her experiences while protecting a tree named Luna. It is based on a true story, written like a diary of two years spent in an ancient redwood.\n\nThe book was published by HarperCollins Publishers Inc. in 2000.\n\nA film adaptation of \"The Legacy of Luna\" called \"Luna\" was scheduled to be released in 2010, directed by Deepa Mehta. Rachel Weisz was set to star as Hill and actively worked to get the project off the ground, saying:\n\n"}
{"id": "5360814", "url": "https://en.wikipedia.org/wiki?curid=5360814", "title": "Tungsten disilicide", "text": "Tungsten disilicide\n\nTungsten silicide (WSi) is an inorganic compound, a silicide of tungsten. It is an electrically conductive ceramic material.\n\nTungsten silicide can react violently with substances such as strong acids, fluorine, oxidizers, and interhalogens.\n\nIt is used in microelectronics as a contact material, with resistivity 60–80 μΩ cm; it forms at 1000 °C. It is often used as a shunt over polysilicon lines to increase their conductivity and increase signal speed. Tungsten silicide layers can be prepared by chemical vapor deposition, e.g. using monosilane or dichlorosilane with tungsten hexafluoride as source gases. The deposited film is non-stoichiometric, and requires annealing to convert to more conductive stoichiometric form. Tungsten silicide is a replacement for earlier tungsten films. Tungsten silicide is also used as a barrier layer between silicon and other metals, e.g. tungsten.\n\nTungsten silicide also finds use in microelectromechanical systems and for oxidation-resistant coatings.\n\nFilms of tungsten silicide can be plasma-etched using e.g. nitrogen trifluoride gas.\n"}
{"id": "31440", "url": "https://en.wikipedia.org/wiki?curid=31440", "title": "Turbopump", "text": "Turbopump\n\nA turbopump is a propellant pump with two main components: a rotodynamic pump and a driving gas turbine, usually both mounted on the same shaft, or sometimes geared together. The purpose of a turbopump is to produce a high-pressure fluid for feeding a combustion chamber or other use.\nThere are two types of turbopumps: a centrifugal pump, where the pumping is done by throwing fluid outward at high speed, or an axial-flow pump, where alternating rotating and static blades progressively raise the pressure of a fluid.\n\nAxial-flow pumps have small diameters but give relatively modest pressure increases. Although multiple compression stages are needed, axial flow pumps work well with low-density fluids. Centrifugal pumps are far more powerful for high-density fluids but require large diameters for low-density fluids.\n\nTurbopumps operate in much the same way as turbocharger units for vehicles: higher fuel pressures allow fuel to be supplied to higher-pressure combustion chambers for higher-performance engines.\n\nHigh-pressure pumps for larger missiles had been discussed by rocket pioneers such as Hermann Oberth. In mid-1935 Wernher von Braun initiated a fuel pump project at the southwest German firm \"Klein, Schanzlin & Becker\" that was experienced in building large fire-fighting pumps. The V-2 rocket design used hydrogen peroxide decomposed through a Walter steam generator to power the uncontrolled turbopump produced at the Heinkel plant at Jenbach, so V-2 turbopumps and combustion chamber were tested and matched to prevent the pump from overpressurizing the chamber. The first engine fired successfully in September, and on August 16, 1942, a trial rocket stopped in mid-air and crashed due to a failure in the turbopump. The first successful V-2 launch was on October 3, 1942. \n\nThe principal engineer for turbopump development at Aerojet was George Bosco. During the second half of 1947, Bosco and his group learned about the pump work of others and made preliminary design studies. Aerojet representatives visited Ohio State University where Florant was working on hydrogen pumps, and consulted Dietrich Singelmann, a German pump expert at Wright Field. Bosco subsequently used Singelmann's data in designing Aerojet's first hydrogen pump.\n\nBy mid-1948, Aerojet had selected centrifugal pumps for both liquid hydrogen and liquid oxygen. They obtained some German radial-vane pumps from the Navy and tested them during the second half of the year.\n\nBy the end of 1948, Aerojet had designed, built, and tested a liquid hydrogen pump (15 cm diameter). Initially, it used ball bearings that were run clean and dry, because the low temperature made conventional lubrication impractical. The pump was first operated at low speeds to allow its parts to cool down to operating temperature. When temperature gauges showed that liquid hydrogen had reached the pump, an attempt was made to accelerate from 5000 to 35 000 revolutions per minute. The pump failed and examination of the pieces pointed to a failure of the bearing, as well as the impeller. After some testing, super-precision bearings, lubricated by oil that was atomized and directed by a stream of gaseous nitrogen, were used. On the next run, the bearings worked satisfactorily but the stresses were too great for the brazed impeller and it flew apart. A new one was made by milling from a solid block of aluminum. The next two runs with the new pump were a great disappointment; the instruments showed no significant flow or pressure rise. The problem was traced to the exit diffuser of the pump, which was too small and insufficiently cooled during the cool-down cycle so that it limited the flow. This was corrected by adding vent holes in the pump housing; the vents were opened during cool down and closed when the pump was cold. With this fix, two additional runs were made in March 1949 and both were successful. Flow rate and pressure were found to be in approximate agreement with theoretical predictions. The maximum pressure was 26 atmospheres () and the flow was 0.25 kilogram per second.\n\nThe Space Shuttle Main Engine's turbopumps spun at over 30,000 rpm, delivering 150 lb (68 kg) of liquid hydrogen and 896 lb (406 kg) of liquid oxygen to the engine per second.\n\nMost turbopumps are centrifugal - the fluid enters the pump near the axis and the rotor accelerates the fluid to high speed. The fluid then passes through a diffuser which is a progressively enlarging pipe, which permits recovery of the dynamic pressure. The diffuser turns the high kinetic energy into high pressures (hundreds of bars is not uncommon), and if the outlet backpressure is not too high, high flow rates can be achieved.\n\nAxial turbopumps also exist. In this case the axle essentially has propellers attached to the shaft, and the fluid is forced by these parallel with the main axis of the pump. Generally, axial pumps tend to give much lower pressures than centrifugal pumps, and a few bars is not uncommon. They are, however, still useful – axial pumps are commonly used as \"inducers\" for centrifugal pumps, which raise the inlet pressure of the centrifugal pump enough to prevent excessive cavitation from occurring therein.\n\nTurbopumps have a reputation for being extremely hard to design to get optimal performance. Whereas a well engineered and debugged pump can manage 70–90% efficiency, figures less than half that are not uncommon. Low efficiency may be acceptable in some applications, but in rocketry this is a severe problem. Turbopumps in rockets are important and problematic enough that launch vehicles using one have been caustically described as a \"turbopump with a rocket attached\"–up to 55% of the total cost has been ascribed to this area.\n\nCommon problems include:\n\nIn addition, the precise shape of the rotor itself is critical.\n\nSteam turbine-powered turbopumps are employed when there is a source of steam, e.g. the boilers of steam ships. Gas turbines are usually used when electricity or steam is not available and place or weight restrictions permit the use of more efficient sources of mechanical energy.\n\nOne of such cases are rocket engines, which need to pump fuel and oxidizer into their combustion chamber. This is necessary for large liquid rockets, since forcing the fluids or gases to flow by simple pressurizing of the tanks is often not feasible; the high pressure needed for the required flow rates would need strong and heavy tanks.\n\nRamjet motors are also usually fitted with turbopumps, the turbine being driven either directly by external freestream ram air or internally by airflow diverted from combustor entry. In both cases the turbine exhaust stream is dumped overboard.\n\n\n"}
{"id": "1537736", "url": "https://en.wikipedia.org/wiki?curid=1537736", "title": "VIRGOHI21", "text": "VIRGOHI21\n\nVIRGOHI21 is an extended region of neutral hydrogen (HI) in the Virgo cluster discovered in 2005. Analysis of its internal motion indicates that it may contain a large amount of dark matter, as much as a small galaxy. Since VIRGOHI21 apparently contains no stars, this would make it one of the first detected dark galaxies. Skeptics of this interpretation argue that VIRGOHI21 is simply a tidal tail of the nearby galaxy NGC 4254.\n\nVIRGOHI21 was detected through radio telescope observations of its neutral hydrogen 21 cm emissions. The detected hydrogen has a mass of about 100 million solar masses and is about 50 million light-years away. By analyzing the Doppler shift of the emissions, astronomers determined that the gas has a high velocity-profile width; that is, different parts of the cloud are moving at high speed relative to other parts. Follow-up Hubble space telescope deep observations of the region have detected very few stars (a few hundred).\n\nIf the high velocity-profile width of VIRGOHI21 is interpreted as rotation, it is far too fast to be consistent with the gravity of the detected hydrogen. Rather, it implies the presence of a dark matter halo with tens of billions of solar masses. Given the very small number of stars detected, this implies a mass-to-light ratio of about 500, far greater than that of a normal galaxy (around 50). The large gravity of the dark matter halo in this interpretation explains the perturbed nature of the nearby spiral galaxy NGC 4254 and the bridge of neutral hydrogen extending between the two entities.\n\nUnder this interpretation, VIRGOHI21 would be the first discovery of the dark galaxies anticipated by simulations of dark-matter theories. Although other dark-galaxy candidates have previously been observed, follow-up observations indicated that these were either very faint ordinary galaxies or tidal tails. VIRGOHI21 is considered the best current candidate for a dark galaxy.\n\nSensitive maps covering a much wider area, obtained at Westerbork Synthesis Radio Telescope (WSRT) and at Arecibo Observatory revealed that VIRGOHI21 is embedded within a much more extensive tail originating in NGC 4254. Both the distribution of the HI gas and its velocity field can be reproduced by a model involving NGC 4254 in a high-speed collision with another galaxy (probably NGC 4192), which is now somewhat distant. Other debris tails of this magnitude have been found to be common features in the Virgo cluster, where the high density of galaxies makes interactions frequent. These results suggest that VIRGOHI21 is not an unusual object, given its location at the edge of the densest region of the Virgo cluster.\n\nThe original paper describing VIRGOHI21 as a dark galaxy provides several objections to the tidal-tail interpretation: that high-velocity interactions do not generally produce significant tails, that the high velocity needed is out-of-place in this part of the Virgo cluster and that the observed velocity profile is opposite from that expected in a tidal tail. In addition, according to Robert Minchin of the Arecibo Observatory, \"If the hydrogen in VIRGOHI21 had been pulled out of a nearby galaxy, the same interaction should have pulled out stars as well\". Proponents of the tidal-tail interpretation counter these objections with simulations and argue that the apparently inverted velocity profile is due to the orientation of the tail with respect to Earth-based observers.\n\nAlthough the nature of VIRGOHI21 remains a contentious issue, its identification as a dark galaxy seems much less certain now than immediately after its discovery.\n\n\n"}
{"id": "11168620", "url": "https://en.wikipedia.org/wiki?curid=11168620", "title": "Vila Parisi", "text": "Vila Parisi\n\nVila Parisi is a \"favela\" (slum) in Cubatão, Brazil, that was the site of a major industrial oil spill fire on February 25, 1984. 700,000 liters of gas were released, 1,000 homes were destroyed, and 100 people died. The geography of Cubatão prevented rapid dispersal of air pollutants released from the burning fuels. The German sociologist Ulrich Beck used the case of Vila Parisi (\"The dirtiest chemical town in the world\") as an example of the \"destructive powers of the developed risk industry.\"\n"}
{"id": "474767", "url": "https://en.wikipedia.org/wiki?curid=474767", "title": "Viologen", "text": "Viologen\n\nViologens are organic compounds with the formula (CHNR). In some viologens, the pyridyl groups are further modified.\n\nThe viologen paraquat (R = methyl), is a widely used herbicide.\n\nOther viologens have been commercialized because they can change color reversibly many times through reduction and oxidation. The name viologen alludes to violet, one color it can exhibit, and the radical cation (CHNR) is colored intensely blue.\n\nAs bipyridinium derivatives, the viologens are related to 4,4'-bipyridyl. The basic nitrogen centers in these compounds are alkylated to give viologens:\nThe alkylation is a form of quaternization. When the alkylating agent is a small alkyl halide, such as methyl chloride and methyl bromide, the viologen salt is often water soluble. A wide variety of alkyl substituents have been investigated. Common derivatives are methyl (see paraquat), long chain alkyl, and benzyl.\n\nViologens, in their dicationic form, typically undergo two one-electron reductions. The first reduction affords the deeply colored radical cation:\nThe radical cations are blue for 4,4'-viologens and green for 2,2'-derivatives. The second reduction yields a yellow quinoid compounds:\n\nThe electron transfer is fast because the redox process induces little structural change.\nViologens have highly reversible redox reactions, and are relatively inexpensive among redox-active organic compounds. They are convenient colorimetric reagents for biochemical redox reactions.\n\nTheir tendency to form host-guest complexes is key to the molecular machines recognized by the 2016 Nobel Prize in Chemistry.\nViologens are used in the negative electrolytes of some experimental flow batteries. Viologens have been modified to optimize their performance in such batteries, e.g. by incorporating them into redox-active polymers.\n\nViologen catalysts have been reported to have the potential to oxidize glucose and other carbohydrates catalytically in a mildly alkaline solution, which makes direct carbohydrate fuel cells possible.\n\nDiquat is an isomer of viologens, being derived from 2,2'-bipyridine (instead of the 4,4'-isomer). It also is a potent herbicide that functions by disrupting electron-transfer.\nExtended viologens have been developed based on conjugated oligomers such as based on aryl, ethylene, and thiophene units are inserted between the pyridine units. The bipolaron di-octyl bis(4-pyridyl)biphenyl viologen 2 in \"scheme 2\" can be reduced by sodium amalgam in DMF to the neutral viologen 3.\n\nThe resonance structures of the quinoid 3a and the biradical 3b contribute equally to the hybrid structure. The driving force for the contributing 3b is the restoration of aromaticity with the biphenyl unit. It has been established using X-ray crystallography that the molecule is, in effect, coplanar with slight nitrogen pyramidalization, and that the central carbon bonds are longer (144 pm) than what would be expected for a double bond (136 pm). Further research shows that the diradical exists as a mixture of triplets and singlets, although an ESR signal is absent. In this sense, the molecule resembles Tschischibabin's hydrocarbon, discovered during 1907. It also shares with this molecule a blue color in solution, and a metallic-green color as crystals.\n\nCompound 3 is a very strong reducing agent, with a redox potential of −1.48 V.\n\nViologens with 2,2'-, 4,4'-, or 2,4'-bipyridylium are highly toxic because these bipyridyl molecules readily form stable free radicals. The delocalization of charge, which allows for the molecule to stay as a free radical and these structures can be easily stabilized because the nitrogens can be readily hydrogenated. When in the body, these viologens interfere with electron transport chain, often causing cell death. These molecules act as redox cycling agents and are able to transfer their electron to molecular oxygen. Once the electron has been transferred to the molecular oxygen, it forms a superoxide radical that causes disproportionation, simultaneous reduction and oxidation.\n\nThese reactive free radicals can cause oxidative stress, which leads to cell death and one example of this is lipid peroxidation. When in a cellular system, the superoxide radicals react with unsaturated lipids, which contain a reactive hydrogen, and produce lipid hydroperoxides. These lipid hydroperoxides then decompose into lipid free radicals, and causes a chain reaction of lipid peroxidation, damaging the cellular macromolecules and eventually causing cell death. The superoxide radicals have also been found to deplete NADPH, alter other redox reactions that naturally occur in the organism, and interfere with how iron is stored and released in the body.\n\nThe widely used herbicide paraquat is a viologen. This application is the largest consumer of this class of compounds.\n\nViologens have been commercialized as electrochromic systems because of their ability to change color reversibly many times upon reduction and oxidation. In some applications, N-heptyl viologens are used. Conducting solid supports such as titania and indium tin oxide have been used.\n\n"}
{"id": "1715834", "url": "https://en.wikipedia.org/wiki?curid=1715834", "title": "Water-gas shift reaction", "text": "Water-gas shift reaction\n\nThe water-gas shift reaction (WGSR) describes the reaction of carbon monoxide and water vapor to form carbon dioxide and hydrogen (the mixture of carbon monoxide and hydrogen (not water) is known as water gas):\nThe water gas shift reaction was discovered by Italian physicist Felice Fontana in 1780. It was not until much later that the industrial value of this reaction was realized. Before the early 20th century, hydrogen was obtained by reacting steam under high pressure with iron to produce iron, iron oxide and hydrogen. With the development of industrial processes that required hydrogen, such as the Haber–Bosch ammonia synthesis, a less expensive and more efficient method of hydrogen production was needed. As a resolution to this problem, the WGSR was combined with the gasification of coal to produce a pure hydrogen product. As the idea of hydrogen economy gains popularity, the focus on hydrogen as a replacement fuel source for hydrocarbons is increasing.\n\nThe WGSR is an important industrial reaction that is used in the manufacture of ammonia, hydrocarbons, methanol, and hydrogen. It is also often used in conjunction with steam reforming of methane and other hydrocarbons. In the Fischer–Tropsch process, the WGSR is one of the most important reactions used to balance the H/CO ratio. It provides a source of hydrogen at the expense of carbon monoxide, which is important for the production of high purity hydrogen for use in ammonia synthesis.\n\nThe water-gas shift reaction may be an undesired side reaction in processes involving water and carbon monoxide, e.g. the rhodium-based Monsanto process. The iridium-based Cativa process uses less water, which suppresses this reaction.\n\nThe WGSR can aid in the efficiency of fuel cells by increasing hydrogen production. The WGSR is considered a critical component in the reduction of carbon monoxide concentrations in cells that are susceptible to carbon monoxide poisoning such as the proton exchange membrane (PEM) fuel cell. The benefits of this application are two-fold: not only would the water gas shift reaction effectively reduce the concentration of carbon monoxide, but it would also increase the efficiency of the fuel cells by increasing hydrogen production. Unfortunately, current commercial catalysts that are used in industrial water gas shift processes are not compatible with fuel cell applications. With the high demand for clean fuel and the critical role of the water gas shift reaction in hydrogen fuel cells, the development of water gas shift catalysts for the application in fuel cell technology is an area of current research interest.\n\nCatalysts for fuel cell application would need to operate at low temperatures. Since the WGSR is slow at lower temperatures where equilibrium favors hydrogen production, WGS reactors require large amounts of catalysts, which increases their cost and size beyond practical application. The commercial LTS catalyst used in large scale industrial plants is also pyrophoric in its inactive state and therefore presents safety concerns for consumer applications. Developing a catalyst that can overcome these limitations is relevant to implementation of a hydrogen economy.\n\nThe equilibrium of this reaction shows a significant temperature dependence and the equilibrium constant decreases with an increase in temperature, that is, higher carbon monoxide conversion is observed at lower temperatures.\n\nThe water gas shift reaction is a moderately exothermic reversible reaction. Therefore, with increasing temperature the reaction rate increases but the conversion of reactants to products becomes less favorable. Due to its exothermic nature, high carbon monoxide conversion is thermodynamically favored at low temperatures. Despite the thermodynamic favorability at low temperatures, the reaction is kinetically favored at high temperatures. \nThe water-gas shift reaction is sensitive to temperature, with a tendency to shift towards reactants as temperature increases due to Le Chatelier's principle. Over the temperature range 600–2000 K, the equilibrium constant for the WGSR has the following relationship: \nIn order to take advantage of both the thermodynamics and kinetics of the reaction, the industrial scale water gas shift reaction is conducted in multiple adiabatic stages consisting of a high temperature shift (HTS) followed by a low temperature shift (LTS) with intersystem cooling. The initial HTS takes advantage of the high reaction rates, but is thermodynamically limited, which results in incomplete conversion of carbon monoxide and a 2-4% carbon monoxide exit composition. To shift the equilibrium toward hydrogen production, a subsequent low temperature shift reactor is employed to produce a carbon monoxide exit composition of less than 1%. The transition from the HTS to the LTS reactors necessitates intersystem cooling. Due to the different reaction conditions, different catalysts must be employed at each stage to ensure optimal activity. The commercial HTS catalyst is the iron oxide–chromium oxide catalyst and the LTS catalyst is a copper-based catalyst. The order proceeds from high to low temperature due to the susceptibility of the copper catalyst to poisoning by sulfur that may remain after the steam reformation process. This necessitates the removal of the sulfur compounds prior to the LTS reactor by a guard bed in order to protect the copper catalyst. Conversely, the iron used in the HTS reaction is generally more robust and resistant toward poisoning by sulfur compounds. While both the HTS and LTS catalysts are commercially available, their specific composition varies based on vendor. An important limitation for the HTS is the HO/CO ratio where low ratios may lead to side reactions such as the formation of metallic iron, methanation, carbon deposition, and Fischer–Tropsch reaction.\n\nThe typical composition of a commercial LTS catalyst has been reported as 32-33% CuO, 34-53% ZnO, 15-33% AlO. The active catalytic species is CuO. The function of ZnO is to provide structural support as well as prevent the poisoning of copper by sulfur. The AlO prevents dispersion and pellet shrinkage. The LTS shift reactor operates at a range of 200–240 °C. The upper temperature limit is due to the susceptibility of copper to thermal sintering. These lower temperatures also reduce the occurrence of side reactions that are observed in the case of the HTS. Noble metals such as platinum, supported on ceria, have also been used for LTS.\n\nThe typical composition of commercial HTS catalyst has been reported as 74.2% FeO, 10.0% CrO, 0.2% MgO (remaining percentage attributed to volatile components). The chromium acts to stabilize the iron oxide and prevents sintering. The operation of HTS catalysts occurs within the temperature range of 310 C to 450 C. The temperature increases along the length of the reactor due to the exothermic nature of the reaction. As such, the inlet temperature is maintained at 350 C to prevent the exit temperature from exceeding 550 C. Industrial reactors operate at a range from atmospheric pressure to 8375 kPa (82.7 atm).\n\nWhile the WGSR has been extensively studied for over a hundred years, the mechanism remains under debate. A universal rate expression and mechanistic understanding have proven elusive, reflecting the many reaction variables, vagarities of the catalyst, and the proprietary nature of commercial processes.\n\nTwo main mechanisms have been proposed: an associative ‘Langmuir–Hinshelwood’ mechanism, and a regenerative ‘redox’ mechanism. While the regenerative mechanism is generally implemented to describe the WGS at higher temperatures, at low temperature both the redox and associative mechanisms are suitable explanations.\n\nIn 1920 Armstrong and Hilditch first proposed the associative mechanism. In this mechanism CO and HO are adsorbed onto the surface of the metal catalyst followed by the formation of an intermediate and the desorption of H and CO. In the initial step, HO dissociates into a metal adsorbed OH and H. The hydroxide then reacts with CO to form a carboxyl or formate intermediate which subsequently decomposes into CO and the metal adsorbed H, which ultimately yields H. While this mechanism may be valid under LTS conditions, the redox mechanism which does not involve any long lived surface intermediates is a more suitable explanation of the WGS mechanism at higher temperatures.\n\nThe regenerative ‘redox’ mechanism is the most commonly accepted mechanism for the WGSR. It involves a regenerative change in the oxidation state of the catalytic metal. In this mechanism, HO is activated first by the abstraction of H from water followed by dissociation or disproportionation of the resulting OH to afford atomic O. The CO is then oxidized by the atomic O forming CO which returns the catalytic surface back to its pre-reaction state. Alternatively, CO may be directly oxidized by the OH to form a carboxyl intermediate, followed by the dissociation or disproportionation of the carboxyl. Finally H is recombined to H and CO and H are desorbed from the metal. The principal difference in these mechanisms is the formation of CO. The redox mechanism generates CO by reaction with adsorbed oxygen, while the associative mechanism forms CO via the dissociation of an intermediate. The mechanism of decarboxylation is debated; it may involve β-hydride elimination, or it may require the action of an external base.\n\nMetal carbonyls catalyze the WGSR in solution. The mechanism entails nucleophilic attack of water or hydroxide on a M-CO center, generating a metallacarboxylic acid.\n\nThe WGSR is exergonic, with the following thermodynamic parameters at room temperature (298 K):\n\nIn aqueous solution, the reaction is less exergonic.\n\nDepending on the reaction conditions, the equilibrium for the water gas shift can be pushed in either the forward or reverse direction. The reversibility of the WGSR is important in the production of ammonia, methanol, and Fischer–Tropsch synthesis where the ratio of H/CO is critical. The RWGS reaction is also gaining interest in the context of the human missions to Mars primarily for its potential to produce water and oxygen. The Mars atmosphere is about 95% CO which can be utilized by the RWGS reaction given a source of hydrogen. Coupling the RWGS with the water electrolysis process will yield methane and oxygen. Post electrolysis, the hydrogen produced can be recycled back into the RWGS reactor for the continued conversion of CO. Because this reaction is only mildly endothermic, the thermal power needed to drive this reaction can potentially be produced by a Sabatier reactor.\n\n"}
{"id": "6262231", "url": "https://en.wikipedia.org/wiki?curid=6262231", "title": "Water distribution on Earth", "text": "Water distribution on Earth\n\nWater is distributed across earth. Most water in the Earth's atmosphere and crust comes from the world ocean's saline seawater, while freshwater accounts for only 2.5% of the total. Because the oceans that cover roughly 78% of the area of the Earth reflect blue light, the Earth appears blue from space, and is often referred to as the \"blue planet\" and the \"Pale Blue Dot\". An estimated 1.5 to 11 times the amount of water in the oceans may be found hundreds of miles deep within the Earth's interior, although not in liquid form.\n\nThe oceanic crust is young, thin and dense, with none of the rocks within it dating from any older than the breakup of Pangaea. Because water is much denser than any gas, this means that water will flow into the \"depressions\" formed as a result of the high density of oceanic crust. (On a planet like Venus, with no water, the depressions appear to form a vast plain above which rise plateaux). Since the low density rocks of the continental crust contain large quantities of easily eroded salts of the alkali and alkaline earth metals, salt has, over billions of years, accumulated in the oceans as a result of evaporation returning the fresh water to land as rain and snow.\n\nAs a result, the vast bulk of the water on Earth is regarded as \"saline\" or \"salt water\", with an average salinity of 35‰ (or 3.5%, roughly equivalent to 34 grams of salts in 1 kg of seawater), though this varies slightly according to the amount of runoff received from surrounding land. In all, water from oceans and marginal seas, saline groundwater and water from saline closed lakes amount to over 97% of the water on Earth, though no closed lake stores a globally significant amount of water. \"Saline\" groundwater is seldom considered except when evaluating water quality in arid regions.\n\nThe remainder of the Earth's water constitutes the planet's \"fresh water\" resource. Typically, fresh water is defined as water with a salinity of \"less than 1 percent that of the oceans\" - i.e. below around 0.35‰. Water with a salinity between this level and 1‰ is typically referred to as \"marginal water\" because it is marginal for many uses by humans and animals. The ratio of salt water to fresh water on Earth is around 40 to 1.\n\nThe planet's fresh water is also very unevenly distributed. Although in warm periods such as the Mesozoic and Paleogene when there were no glaciers anywhere on the planet all fresh water was found in rivers and streams, today most fresh water exists in the form of ice, snow, groundwater and soil moisture, with only 0.3% in liquid form on the surface. Of the liquid surface fresh water, 87% is contained in lakes, 11% in swamps, and only 2% in rivers. Small quantities of water also exist in the atmosphere and in living beings. Of these sources, only river water is generally valuable.\n\nMost lakes are in very inhospitable regions such as the glacial lakes of Canada, Lake Baikal in Russia, Lake Khövsgöl in Mongolia, and the African Great Lakes. The North American Great Lakes, which contain 21% of the world's fresh water by volume, are the exception. They are located in a hospitable region, which is heavily populated. The Great Lakes Basin is home to 33 million people. The Canadian cities of Toronto, Hamilton, Ontario, St. Catharines, Niagara, Oshawa, Windsor, and Barrie, and the United States cities of Duluth, Milwaukee, Chicago, Gary, Detroit, Cleveland, Buffalo, and Rochester, are all located on shores of the Great Lakes.\n\nAlthough the total volume of groundwater is known to be much greater than that of river runoff, a large proportion of this groundwater is saline and should therefore be classified with the saline water above. There is also a lot of \"fossil\" groundwater in arid regions that has never been renewed for thousands of years; this must not be seen as renewable water.\n\nHowever, fresh groundwater is of great value, especially in arid countries such as India. Its distribution is broadly similar to that of surface river water, but it is easier to store in hot and dry climates because groundwater storages are much more shielded from evaporation than are dams. In countries such as Yemen, groundwater from erratic rainfall during the rainy season is the major source of irrigation water.\n\nBecause groundwater recharge is much more difficult to accurately measure than surface runoff, groundwater is not generally used in areas where even fairly limited levels of surface water are available. Even today, estimates of total groundwater recharge vary greatly for the same region depending on what source is used, and cases where fossil groundwater is exploited beyond the recharge rate (including the Ogallala Aquifer) are very frequent and almost always not seriously considered when they were first developed.\n\nThe total volume of water on Earth is estimated at 1.386 billion km³ (333 million cubic miles), with 97.5% being salt water and 2.5% being fresh water. Of the fresh water, only 0.3% is in liquid form on the surface.\nIn addition, the lower mantle of inner earth may hold as much as 5 times more water than all surface water combined (all oceans, all lakes, all rivers).\n\nThe total volume of water in rivers is estimated at 2,120 km³ (510 cubic miles), or 2% of the surface fresh water on Earth. Rivers and basins are often compared not according to their static volume, but to their flow of water, or surface runoff. The distribution of river runoff across the Earth's surface is very uneven.\n\nThere can be huge variations within these regions. For example, as much as a quarter of Australia's limited renewable fresh water supply is found in almost uninhabited Cape York Peninsula. Also, even in well-watered continents, there are areas that are extremely short of water, such as Texas in North America, whose renewable water supply totals only 26 km³/year in an area of 695,622 km², or South Africa, with only 44 km³/year in 1,221,037 km². The areas of greatest concentration of renewable water are:\n\n\nVariability of water availability is important both for the functioning of aquatic species and also for the availability of water for human use: water that is only available in a few wet years must not be considered renewable. Because most global runoff comes from areas of very low climatic variability, the total global runoff is generally of low variability.\n\nIndeed, even in most arid zones, there tends to be few problems with variability of runoff because most usable sources of water come from high mountain regions which provide highly reliable glacier melt as the chief source of water, which also comes in the summer peak period of high demand for water. This historically aided the development of many of the great civilizations of ancient history, and even today allows for agriculture in such productive areas as the San Joaquin Valley.\n\nHowever, in Australia and Southern Africa, the story is different. Here, runoff variability is much higher than in other continental regions of the world with similar climates. Typically temperate (Köppen climate classification C) and arid (Köppen climate classification B) climate rivers in Australia and Southern Africa have as much as three times the coefficient of variation of runoff of those in other continental regions. The reason for this is that, whereas all other continents have had their soils largely shaped by Quaternary glaciation and mountain building, soils of Australia and Southern Africa have been largely unaltered since at least the early Cretaceous and generally since the previous ice age in the Carboniferous. Consequently, available nutrient levels in Australian and Southern African soils tend to be orders of magnitude lower than those of similar climates in other continents, and native flora compensate for this through much higher rooting densities (e.g. proteoid roots) to absorb minimal phosphorus and other nutrients. Because these roots absorb so much water, runoff in typical Australian and Southern African rivers does not occur until about 300 mm (12 inches) or more of rainfall has occurred. In other continents, runoff will occur after quite light rainfall due to the low rooting densities.\n\nThe consequence of this is that many rivers in Australia and Southern Africa (as compared to \"extremely few\" in other continents) are theoretically impossible to regulate because rates of evaporation from dams mean a storage sufficiently large to theoretically regulate the river to a given level would actually allow very little draft to be used. Examples of such rivers include those in the Lake Eyre Basin. Even for other Australian rivers, a storage three times as large is needed to provide a third the supply of a comparable climate in southeastern North America or southern China. It also affects aquatic life, favouring strongly those species able to reproduce rapidly after high floods so that some will survive the next drought.\n\nTropical (Köppen climate classification A) climate rivers in Australia and Southern Africa do not, in contrast, have markedly lower runoff ratios than those of similar climates in other regions of the world. Although soils in tropical Australia and southern Africa are even poorer than those of the arid and temperate parts of these continents, vegetation can use organic phosphorus or phosphate dissolved in rainwater as a source of the nutrient. In cooler and drier climates these two related sources tend to be virtually useless, which is why such specialised means are needed to extract the most minimal phosphorus.\n\nThere are other isolated areas of high runoff variability, though these are basically due to erratic rainfall rather than different hydrology. These include:\n\nIt is estimated an additional 1.5 to eleven times the amount of water in the oceans is contained in the Earth's interior, and some scientists have hypothesized that the water in the mantle is part of a \"whole-Earth water cycle\".\nThe water in the mantle is dissolved in various minerals near the transition zone between Earth's upper and lower mantle. At temperatures of and extreme pressures found deep underground, water breaks down into hydroxyls and oxygen. The existence of water was experimentally predicted in 2002, and direct evidence of the water was found in 2014 based on tests on a sample of ringwoodite. Further evidence for large quantities of water in the mantle was found in observations of melting in the transition zone from the USArray project. Liquid water is not present within the ringwoodite, rather the components of water (hydrogen and oxygen) are held within as hydroxide ions.\n\n"}
{"id": "20373503", "url": "https://en.wikipedia.org/wiki?curid=20373503", "title": "Yttrium", "text": "Yttrium\n\nYttrium is a chemical element with symbol Y and atomic number 39. It is a silvery-metallic transition metal chemically similar to the lanthanides and has often been classified as a \"rare-earth element\". Yttrium is almost always found in combination with lanthanide elements in rare-earth minerals, and is never found in nature as a free element. Y is the only stable isotope, and the only isotope found in the Earth's crust.\n\nIn 1787, Carl Axel Arrhenius found a new mineral near Ytterby in Sweden and named it \"ytterbite\", after the village. Johan Gadolin discovered yttrium's oxide in Arrhenius' sample in 1789, and Anders Gustaf Ekeberg named the new oxide \"yttria\". Elemental yttrium was first isolated in 1828 by Friedrich Wöhler.\n\nThe most important uses of yttrium are LEDs and phosphors, particularly the red phosphors in television set cathode ray tube (CRT) displays. Yttrium is also used in the production of electrodes, electrolytes, electronic filters, lasers, superconductors, various medical applications, and tracing various materials to enhance their properties.\n\nYttrium has no known biological role. Exposure to yttrium compounds can cause lung disease in humans.\n\nYttrium is a soft, silver-metallic, lustrous and highly crystalline transition metal in group 3. As expected by periodic trends, it is less electronegative than its predecessor in the group, scandium, and less electronegative than the next member of period 5, zirconium; additionally, it is more electronegative to its successor in its group, lanthanum, being closer in electronegativity to the later lanthanides due to the lanthanide contraction. Yttrium is the first d-block element in the fifth period.\n\nThe pure element is relatively stable in air in bulk form, due to passivation of a protective oxide () film that forms on the surface. This film can reach a thickness of 10 µm when yttrium is heated to 750 °C in water vapor. When finely divided, however, yttrium is very unstable in air; shavings or turnings of the metal can ignite in air at temperatures exceeding 400 °C. Yttrium nitride (YN) is formed when the metal is heated to 1000 °C in nitrogen.\n\nThe similarities of yttrium to the lanthanides are so strong that the element has historically been grouped with them as a rare-earth element, and is always found in nature together with them in rare-earth minerals. Chemically, yttrium resembles those elements more closely than its neighbor in the periodic table, scandium, and if physical properties were plotted against atomic number, it would have an apparent number of 64.5 to 67.5, placing it between the lanthanides gadolinium and erbium.\n\nIt often also falls in the same range for reaction order, resembling terbium and dysprosium in its chemical reactivity. Yttrium is so close in size to the so-called 'yttrium group' of heavy lanthanide ions that in solution, it behaves as if it were one of them. Even though the lanthanides are one row farther down the periodic table than yttrium, the similarity in atomic radius may be attributed to the lanthanide contraction.\n\nOne of the few notable differences between the chemistry of yttrium and that of the lanthanides is that yttrium is almost exclusively trivalent, whereas about half the lanthanides can have valences other than three; nevertheless, only for four of the fifteen lanthanides are these other valences important in aqueous solution (Ce, Sm, Eu, and Yb).\n\nAs a trivalent transition metal, yttrium forms various inorganic compounds, generally in the oxidation state of +3, by giving up all three of its valence electrons. A good example is yttrium(III) oxide (), also known as yttria, a six-coordinate white solid.\n\nYttrium forms a water-insoluble fluoride, hydroxide, and oxalate, but its bromide, chloride, iodide, nitrate and sulfate are all soluble in water. The Y ion is colorless in solution because of the absence of electrons in the d and f electron shells.\n\nWater readily reacts with yttrium and its compounds to form . Concentrated nitric and hydrofluoric acids do not rapidly attack yttrium, but other strong acids do.\n\nWith halogens, yttrium forms trihalides such as yttrium(III) fluoride (), yttrium(III) chloride (), and yttrium(III) bromide () at temperatures above roughly 200 °C. Similarly, carbon, phosphorus, selenium, silicon and sulfur all form binary compounds with yttrium at elevated temperatures.\n\nOrganoyttrium chemistry is the study of compounds containing carbon–yttrium bonds. A few of these are known to have yttrium in the oxidation state 0. (The +2 state has been observed in chloride melts, and +1 in oxide clusters in the gas phase.) Some trimerization reactions were generated with organoyttrium compounds as catalysts. These syntheses use as a starting material, obtained from and concentrated hydrochloric acid and ammonium chloride.\n\nHapticity is a term to describe the coordination of a group of contiguous atoms of a ligand bound to the central atom; it is indicated by the Greek character \"eta\", η. Yttrium complexes were the first examples of complexes where carboranyl ligands were bound to a d-metal center through a η-hapticity. Vaporization of the graphite intercalation compounds graphite–Y or graphite– leads to the formation of endohedral fullerenes such as Y@C. Electron spin resonance studies indicated the formation of Y and (C) ion pairs. The carbides YC, YC, and YC can be hydrolyzed to form hydrocarbons.\n\nYttrium in the Solar System was created through stellar nucleosynthesis, mostly by the s-process (≈72%), but also by the r-process (≈28%). The r-process consists of rapid neutron capture of lighter elements during supernova explosions. The s-process is a slow neutron capture of lighter elements inside pulsating red giant stars.\nYttrium isotopes are among the most common products of the nuclear fission of uranium in nuclear explosions and nuclear reactors. In the context of nuclear waste management, the most important isotopes of yttrium are Y and Y, with half-lives of 58.51 days and 64 hours, respectively. Though Y has a short half-life, it exists in secular equilibrium with its long-lived parent isotope, strontium-90 (Sr) with a half-life of 29 years.\n\nAll group 3 elements have an odd atomic number, and therefore few stable isotopes. Scandium has one stable isotope, and yttrium itself has only one stable isotope, Y, which is also the only isotope that occurs naturally. However, the lanthanide rare earths contain elements of even atomic number and many stable isotopes. Yttrium-89 is thought to be more abundant than it otherwise would be, due in part to the s-process, which allows enough time for isotopes created by other processes to decay by electron emission (neutron → proton). Such a slow process tends to favor isotopes with atomic mass numbers (A = protons + neutrons) around 90, 138 and 208, which have unusually stable atomic nuclei with 50, 82, and 126 neutrons, respectively. Y has a mass number close to 90 and has 50 neutrons in its nucleus.\n\nAt least 32 synthetic isotopes of yttrium have been observed, and these range in atomic mass number from 76 to 108. The least stable of these is Y with a half-life of >150 ns (Y has a half-life of >200 ns) and the most stable is Y with a half-life of 106.626 days. Apart from the isotopes Y, Y, and Y, with half-lives of 58.51 days, 79.8 hours, and 64 hours, respectively, all the other isotopes have half-lives of less than a day and most of less than an hour.\n\nYttrium isotopes with mass numbers at or below 88 decay primarily by positron emission (proton → neutron) to form strontium (Z = 38) isotopes. Yttrium isotopes with mass numbers at or above 90 decay primarily by electron emission (neutron → proton) to form zirconium (Z = 40) isotopes. Isotopes with mass numbers at or above 97 are also known to have minor decay paths of β delayed neutron emission.\n\nYttrium has at least 20 metastable (\"excited\") isomers ranging in mass number from 78 to 102. Multiple excitation states have been observed for Y and Y. While most of yttrium's isomers are expected to be less stable than their ground state, Y, Y, Y, Y, Y, Y, and Y have longer half-lives than their ground states, as these isomers decay by beta decay rather than isomeric transition.\n\nIn 1787, army lieutenant and part-time chemist Carl Axel Arrhenius found a heavy black rock in an old quarry near the Swedish village of Ytterby (now part of the Stockholm Archipelago). Thinking that it was an unknown mineral containing the newly discovered element tungsten, he named it \"ytterbite\" and sent samples to various chemists for analysis.\nJohan Gadolin at the University of Åbo identified a new oxide (or \"earth\") in Arrhenius' sample in 1789, and published his completed analysis in 1794. Anders Gustaf Ekeberg confirmed the identification in 1797 and named the new oxide \"yttria\". In the decades after Antoine Lavoisier developed the first modern definition of chemical elements, it was believed that earths could be reduced to their elements, meaning that the discovery of a new earth was equivalent to the discovery of the element within, which in this case would have been \"yttrium\".\n\nIn 1843, Carl Gustaf Mosander found that samples of yttria contained three oxides: white yttrium oxide (yttria), yellow terbium oxide (confusingly, this was called 'erbia' at the time) and rose-colored erbium oxide (called 'terbia' at the time). A fourth oxide, ytterbium oxide, was isolated in 1878 by Jean Charles Galissard de Marignac. New elements were later isolated from each of those oxides, and each element was named, in some fashion, after Ytterby, the village near the quarry where they were found (see ytterbium, terbium, and erbium). In the following decades, seven other new metals were discovered in \"Gadolin's yttria\". Since yttria was found to be a mineral and not an oxide, Martin Heinrich Klaproth renamed it gadolinite in honor of Gadolin.\n\nFriedrich Wöhler mistakenly thought he had isolated the metal in 1828 from a volatile chloride he supposed to be yttrium chloride, but Heinrich Rose proved otherwise in 1843 and correctly isolated the element himself that year.\n\nUntil the early 1920s, the chemical symbol Yt was used for the element, after which Y came into common use.\n\nIn 1987, yttrium barium copper oxide was found to achieve high-temperature superconductivity. It was only the second material known to exhibit this property, and it was the first known material to achieve superconductivity above the (economically important) boiling point of nitrogen.\n\nYttrium is found in most rare-earth minerals, it is found in some uranium ores, but is never found in the Earth's crust as a free element. About 31 ppm of the Earth's crust is yttrium, making it the 28th most abundant element, 400 times more common than silver. Yttrium is found in soil in concentrations between 10 and 150 ppm (dry weight average of 23 ppm) and in sea water at 9 ppt. Lunar rock samples collected during the American Apollo Project have a relatively high content of yttrium.\nYttrium has no known biological role, though it is found in most, if not all, organisms and tends to concentrate in the liver, kidney, spleen, lungs, and bones of humans. Normally, as little as 0.5 milligrams is found in the entire human body; human breast milk contains 4 ppm. Yttrium can be found in edible plants in concentrations between 20 ppm and 100 ppm (fresh weight), with cabbage having the largest amount. With as much as 700 ppm, the seeds of woody plants have the highest known concentrations.\n\n\"This REY-rich mud has great potential as a rare-earth metal resource because of the enormous amount available and its advantageous mineralogical features,\" the study reads. The study shows that more than 16 million tons of rare-earth elements could be \"exploited in the near future.\"\n\nIncluding ytrrium (Y), which is used in products like camera lenses and mobile phone screens, the rare-earth elements found are: Europium (EU), Terbium (Tb) and Dysprosium (Dy).\n\nSince yttrium is chemically so similar to the lanthanides, it occurs in the same ores (rare-earth minerals) and is extracted by the same refinement processes. A slight distinction is recognized between the light (LREE) and the heavy rare-earth elements (HREE), but the distinction is not perfect. Yttrium is concentrated in the HREE group because of its ion size, though it has a lower atomic mass.\nRare-earth elements (REEs) come mainly from four sources:\n\nOne method for obtaining pure yttrium from the mixed oxide ores is to dissolve the oxide in sulfuric acid and fractionate it by ion exchange chromatography. With the addition of oxalic acid, the yttrium oxalate precipitates. The oxalate is converted into the oxide by heating under oxygen. By reacting the resulting yttrium oxide with hydrogen fluoride, yttrium fluoride is obtained. When quaternary ammonium salts are used as extractants, most yttrium will remain in the aqueous phase. When the counter-ion is nitrate, the light lanthanides are removed, and when the counter-ion is thiocyanate, the heavy lanthanides are removed. In this way, yttrium salts of 99.999% purity are obtained. In the usual situation, where yttrium is in a mixture that is two-thirds heavy-lanthanide, yttrium should be removed as soon as possible to facilitate the separation of the remaining elements.\n\nAnnual world production of yttrium oxide had reached 600 tonnes by 2001; by 2014 it had increased to 7,000 tons. Global reserves of yttrium oxide were estimated in 2014 to be more than 500,000 tons. The leading countries for these reserves included Australia, Brazil, China, India, and the United States. Only a few tonnes of yttrium metal are produced each year by reducing yttrium fluoride to a metal sponge with calcium magnesium alloy. The temperature of an arc furnace of greater than 1,600 °C is sufficient to melt the yttrium.\n\nThe red component of color television cathode ray tubes is typically emitted from an yttria () or yttrium oxide sulfide () host lattice doped with europium (III) cation (Eu) phosphors. The red color itself is emitted from the europium while the yttrium collects energy from the electron gun and passes it to the phosphor. Yttrium compounds can serve as host lattices for doping with different lanthanide cations. Tb can be used as a doping agent to produce green luminescence. As such yttrium compounds such as yttrium aluminium garnet (YAG) are useful for phosphors and are an important component of white LEDs.\n\nYttria is used as a sintering additive in the production of porous silicon nitride. It is used as a common starting material for material science and for producing other compounds of yttrium.\n\nYttrium compounds are used as a catalyst for ethylene polymerization. As a metal, yttrium is used on the electrodes of some high-performance spark plugs. Yttrium is used in gas mantles for propane lanterns as a replacement for thorium, which is radioactive.\n\nCurrently under development is yttrium-stabilized zirconia as a solid electrolyte and as an oxygen sensor in automobile exhaust systems.\n\nYttrium is used in the production of a large variety of synthetic garnets, and yttria is used to make yttrium iron garnets (, also \"YIG\"), which are very effective microwave filters. Yttrium, iron, aluminium, and gadolinium garnets (e.g. Y(Fe,Al)O and Y(Fe,Ga)O) have important magnetic properties. YIG is also very efficient as an acoustic energy transmitter and transducer. Yttrium aluminium garnet ( or YAG) has a hardness of 8.5 and is also used as a gemstone in jewelry (simulated diamond). Cerium-doped yttrium aluminium garnet (YAG:Ce) crystals are used as phosphors to make white LEDs.\n\nYAG, yttria, yttrium lithium fluoride (), and yttrium orthovanadate () are used in combination with dopants such as neodymium, erbium, ytterbium in near-infrared lasers. YAG lasers can operate at high power and are used for drilling and cutting metal. The single crystals of doped YAG are normally produced by the Czochralski process.\n\nSmall amounts of yttrium (0.1 to 0.2%) have been used to reduce the grain sizes of chromium, molybdenum, titanium, and zirconium. Yttrium is used to increase the strength of aluminium and magnesium alloys. The addition of yttrium to alloys generally improves workability, adds resistance to high-temperature recrystallization, and significantly enhances resistance to high-temperature oxidation (see graphite nodule discussion below).\n\nYttrium can be used to deoxidize vanadium and other non-ferrous metals. Yttria stabilizes the cubic form of zirconia in jewelry.\n\nYttrium has been studied as a nodulizer in ductile cast iron, forming the graphite into compact nodules instead of flakes to increase ductility and fatigue resistance. Having a high melting point, yttrium oxide is used in some ceramic and glass to impart shock resistance and low thermal expansion properties. Those same properties make such glass useful in camera lenses.\n\nThe radioactive isotope yttrium-90 is used in drugs such as Yttrium Y 90-DOTA-tyr3-octreotide and Yttrium Y 90 ibritumomab tiuxetan for the treatment of various cancers, including lymphoma, leukemia, liver, ovarian, colorectal, pancreatic and bone cancers. It works by adhering to monoclonal antibodies, which in turn bind to cancer cells and kill them via intense β-radiation from the yttrium-90 (see Monoclonal antibody therapy).\n\nA technique called radioembolization is used to treat hepatocellular carcinoma and liver metastasis. Radioembolization is a low toxicity, targeted liver cancer therapy that uses millions of tiny beads made of glass or resin containing radioactive yttrium-90. The radioactive microspheres are delivered directly to the blood vessels feeding specific liver tumors/segments or lobes. It is minimally invasive and patients can usually be discharged after a few hours. This procedure may not eliminate all tumors throughout the entire liver, but works on one segment or one lobe at a time and may require multiple procedures.\n\nAlso see Radioembolization in the case of combined cirrhosis and Hepatocellular carcinoma.\n\nNeedles made of yttrium-90, which can cut more precisely than scalpels, have been used to sever pain-transmitting nerves in the spinal cord, and yttrium-90 is also used to carry out radionuclide synovectomy in the treatment of inflamed joints, especially knees, in sufferers of conditions such as rheumatoid arthritis.\n\nA neodymium-doped yttrium-aluminium-garnet laser has been used in an experimental, robot-assisted radical prostatectomy in canines in an attempt to reduce collateral nerve and tissue damage, and erbium-doped lasers are coming into use for cosmetic skin resurfacing.\n\nYttrium is a key ingredient in the yttrium barium copper oxide (YBaCuO, aka 'YBCO' or '1-2-3') superconductor developed at the University of Alabama and the University of Houston in 1987. This superconductor is notable because the operating superconductivity temperature is above liquid nitrogen's boiling point (77.1 K). Since liquid nitrogen is less expensive than the liquid helium required for metallic superconductors, the operating costs for applications would be less.\n\nThe actual superconducting material is often written as YBaCuO, where \"d\" must be less than 0.7 for superconductivity. The reason for this is still not clear, but it is known that the vacancies occur only in certain places in the crystal, the copper oxide planes, and chains, giving rise to a peculiar oxidation state of the copper atoms, which somehow leads to the superconducting behavior.\n\nThe theory of low temperature superconductivity has been well understood since the BCS theory of 1957. It is based on a peculiarity of the interaction between two electrons in a crystal lattice. However, the BCS theory does not explain high temperature superconductivity, and its precise mechanism is still a mystery. What is known is that the composition of the copper-oxide materials must be precisely controlled for superconductivity to occur.\n\nThis superconductor is a black and green, multi-crystal, multi-phase mineral. Researchers are studying a class of materials known as perovskites that are alternative combinations of these elements, hoping to develop a practical high-temperature superconductor.\nYttrium currently has no biological role, and it can be highly toxic to humans and other animals.\n\nWater-soluble compounds of yttrium are considered mildly toxic, while its insoluble compounds are non-toxic. In experiments on animals, yttrium and its compounds caused lung and liver damage, though toxicity varies with different yttrium compounds. In rats, inhalation of yttrium citrate caused pulmonary edema and dyspnea, while inhalation of yttrium chloride caused liver edema, pleural effusions, and pulmonary hyperemia.\n\nExposure to yttrium compounds in humans may cause lung disease. Workers exposed to airborne yttrium europium vanadate dust experienced mild eye, skin, and upper respiratory tract irritation—though this may be caused by the vanadium content rather than the yttrium. Acute exposure to yttrium compounds can cause shortness of breath, coughing, chest pain, and cyanosis. The Occupational Safety and Health Administration (OSHA) limits exposure to yttrium in the workplace to 1 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) recommended exposure limit (REL) is 1 mg/m over an 8-hour workday. At levels of 500 mg/m, yttrium is immediately dangerous to life and health. Yttrium dust is flammable.\n\n\n"}
