{"id": "9239718", "url": "https://en.wikipedia.org/wiki?curid=9239718", "title": "2000-watt society", "text": "2000-watt society\n\nThe 2000-watt society is an environmental vision, first introduced in 1998 by the Swiss Federal Institute of Technology in Zürich (ETH Zurich), which pictures the average First World citizen reducing their overall average primary energy usage to no more than 2,000 watts (i.e. 2 kilowatt-hours per hour or 48 kWh per day) by the year 2050, without lowering their standard of living.\n\nThe concept addresses not only personal or household energy use, but the total for the whole society, including embodied energy, divided by the population.\n\nTwo thousand watts is approximately the current world average rate of total primary energy use. This compares to averages of around 6,000 watts in western Europe, 12,000 watts in the United States, 1,500 watts in China, 1,000 watts in India, 500 watts in South Africa and only 300 watts in Bangladesh. Switzerland itself, currently using an average of around 5,000 watts, was last a 2000-watt society in the 1960s.\n\nIt is further envisaged that the use of carbon-based fuels would be ultimately cut to no more than 500 watts per person within 50 to 100 years.\n\nThe vision was developed in response to concerns about climate change, energy security, and the future availability of energy supplies. It is supported by the Swiss Federal Office of Energy, the Association of Swiss Architects and Engineers, and other bodies.\n\nBreakdown of average energy consumption of 5.1 kW by a Swiss person as of July 2008:\n\nResearchers in Switzerland believe that this vision is achievable, despite a projected 65% increase in economic growth by 2050, by using new low-carbon technologies and techniques.\n\nIt is envisaged that achieving the aim of a 2000-watt society will require, amongst other measures, a complete reinvestment in the country's capital assets; refurbishment of the nation's building stock to bring it up to low-energy building standards; significant improvements in the efficiency of road transport, aviation and energy-intensive material use; the possible introduction of high-speed maglev trains; the use of renewable energy sources, district heating, microgeneration and related technologies; and a refocusing of research into new priority areas.\n\nAs a result of the intensified research and development effort required, it is hoped that Switzerland will become a leader in the technologies involved. Indeed, the idea has a great deal of government backing, due to fears about climate change.\n\nLaunched in 2001 and located in the metropolitan area of Basel, 'Pilot Region Basel' aims to develop and commercialise some of the technologies involved. The pilot is a partnership between industry, universities, research institutes and the authorities, coordinated by Novatlantis. Participation is not restricted to locally based organisations. The city of Zurich joined the project in 2005 and the canton of Geneva declared its interest in 2008.\n\nWithin the pilot region, the projects in progress include demonstration buildings constructed to \"MINERGIE\" or \"Passivhaus\" standards, electricity generation from renewable energy sources, and vehicles using natural gas, hydrogen and biogas. The aim is to put research into practice, seek continuous improvements, and to communicate progress to all interested parties, including the public.\n\nThe \"smart living lab\", based in Fribourg, reunites researches from the School of Engineering and Architecture of Fribourg, the Fribourg University and the Lausanne Swiss Federal Institute of Technology (EPFL). Together, they design the smart living building, which will be both a sustainable structure and an evolving building and whose erection is to take place in 2020. It will house the activities of some 100 researchers, offering laboratories, offices, conference rooms and some experimental dwellings. In this multiple-use context, the building will become an experimental field of studies in itself, and aims to find solutions to energy consumption and the greenhouse gas emissions that it generates.\n\nThis construction is the group's first case study, and research projects have been established to help it meet the lab's ambitious goals: limiting its consumption and emissions to the values set for 2050 by the 2000-watt society vision, while considering the whole life cycle of its components. These goals are to be met by 2021, that is, almost 30 years ahead of the 2000-watt-society vision's deadline.\n\n\n"}
{"id": "19480825", "url": "https://en.wikipedia.org/wiki?curid=19480825", "title": "Arthur A. Seeligson Jr.", "text": "Arthur A. Seeligson Jr.\n\nArthur Addison Seeligson Jr. (October 29, 1920 - April 17, 2001) was an American oilman, rancher, and a Thoroughbred racehorse owner/breeder.\n\nSeeligson was born in San Antonio, Texas, the son of Ramona (née Frates) and Arthur Addison Seeligson Sr. His paternal great-grandfather, Henry Seeligson, was a Jewish Confederate soldier whose father served as mayor of Galveston, Texas. Through his great-grandfather, Seeligson was a relative of composer Louis Moreau Gottschalk. Arthur Seeligson's brother, Frates Slick Seeligson (1923-2006), was a rancher and a member of the Texas House of Representatives from 1953-1960. His first cousin, once removed, Lamar Smith has been a Republican Congressman from Texas since 1987.\n\nArthur Seeligson was raised a Protestant. His family moved to Oklahoma when he was still a boy. After studying at Phillips Exeter Academy in Exeter, New Hampshire and then graduating from Yale University in 1942, Arthur Seeligson Jr. followed in his fathers footsteps as a successful investor in the oil and gas industry in Kansas.\n\nSeeligson returned to live in San Antonio and it was his home at the time of his death in 2001.\n\nArthur Seeligson was involved in Thoroughbred horse racing for more than forty years. He had stakes race winners both in the United States and in Europe. He most notably bred and raced Avatar, winner of the 1975 Santa Anita Derby and the American Classic, the Belmont Stakes.\n\nFor a time, Arthur Seeligson was a co-owner of the now defunct Hialeah Park Race Track in Hialeah, Florida. He was a member of the Board of Directors of the National Museum of Racing and Hall of Fame in Saratoga Springs, New York.\nSeeligson's daughter, Ramona Seeligson Bass, was a driving force behind the creation of \"Texas Wild!\" at the Fort Worth Zoo. The Zoo's Arthur A. Seeligson Conservation Fund was established by his family and friends to benefit Texas wildlife.\n\n"}
{"id": "43536", "url": "https://en.wikipedia.org/wiki?curid=43536", "title": "Beeswax", "text": "Beeswax\n\nBeeswax (cera alba) is a natural wax produced by honey bees of the genus \"Apis\". The wax is formed into scales by eight wax-producing glands in the abdominal segments of worker bees, which discard it in or at the hive. The hive workers collect and use it to form cells for honey storage and larval and pupal protection within the beehive. Chemically, beeswax consists mainly of esters of fatty acids and various long-chain alcohols.\n\nBeeswax has been used since prehistory as man's first plastic, as a lubricant and waterproofing agent, in lost wax casting of metals and glass, as a polish for wood and leather and for making candles, as an ingredient in cosmetics and as an artistic medium in encaustic painting. \n\nBeeswax is edible, having similar negligible toxicity to plant waxes, and is approved for food use in most countries and in the European Union under the E number E901.\n\nThe wax is formed by worker bees, which secrete it from eight wax-producing mirror glands on the inner sides of the sternites (the ventral shield or plate of each segment of the body) on abdominal segments 4 to 7. The sizes of these wax glands depend on the age of the worker, and after many daily flights, these glands begin to gradually atrophy.\n\nThe new wax is initially glass-clear and colorless, becoming opaque after mastication and adulteration with pollen by the hive worker bees. Also, the wax becomes progressively more yellow or brown by incorporation of pollen oils and propolis. The wax scales are about across and thick, and about 1100 are required to make a gram of wax.\n\nHoney bees use the beeswax to build honeycomb cells in which their young are raised with honey and pollen cells being capped for storage. For the wax-making bees to secrete wax, the ambient temperature in the hive must be 33 to 36 °C (91 to 97 °F).\n\nThe amount of honey used by bees to produce wax has not been accurately determined. The book, \"Beeswax Production, Harvesting, Processing and Products\", suggests 1 kg of beeswax is used to store 22 kg honey. According to Whitcomb's 1946 experiment, 6.66 to 8.80 kg of honey yields 1 kg of wax. Another study estimated that 24 to 30 kg of honey are produced per kg of wax.\n\nWhen beekeepers extract the honey, they cut off the wax caps from each honeycomb cell with an uncapping knife or machine. Its color varies from nearly white to brownish, but most often a shade of yellow, depending on purity, the region, and the type of flowers gathered by the bees. Wax from the brood comb of the honey bee hive tends to be darker than wax from the honeycomb. Impurities accumulate more quickly in the brood comb. Due to the impurities, the wax must be rendered before further use. The leftovers are called slumgum.\n\nThe wax may be clarified further by heating in water. As with petroleum waxes, it may be softened by dilution with mineral oil or vegetable oil to make it more workable at room temperature.\n\nBeeswax is a tough wax formed from a mixture of several chemical compounds.\n\nAn approximate chemical formula for beeswax is CHCOOCH. Its main components are palmitate, palmitoleate, and oleate esters of long-chain (30–32 carbons) aliphatic alcohols, with the ratio of triacontanyl palmitate CH(CH)O-CO-(CH)CH to cerotic acid CH(CH)COOH, the two principal components, being 6:1. Beeswax can be classified generally into European and Oriental types. The saponification value is lower (3–5) for European beeswax, and higher (8–9) for Oriental types.\n\nBeeswax has a relatively low melting point range of 62 to 64 °C (144 to 147 °F). If beeswax is heated above 85 °C (185 °F) discoloration occurs. The flash point of beeswax is 204.4 °C (400 °F). Density at 15 °C is 958 to 970 kg/m³.\n\nWhen natural beeswax is cold, it is brittle, at room temperature it is tenacious, its fracture is dry and granular, it also softens at human body temperature. The specific gravity at is from 0.958 to 0.975, that of melted wax at compared with water at is 0.822.\nCandle-making has long involved the use of beeswax, which is highly flammable, and this material was traditionally prescribed for the making of the Paschal candle or \"Easter candle\". Beeswax candles are purported to be superior to other wax candles, because they burn brighter and longer, do not bend, and burn \"cleaner\". It is further recommended for the making of other candles used in the liturgy of the Roman Catholic Church. Beeswax is also the candle constituent of choice in the Orthodox Church.\n\nRefined beeswax plays a prominent role in art materials both as a binder in encaustic paint and as a stabilizer in oil paint to add body. \n\nBeeswax is an ingredient in surgical bone wax, which is used during surgery to control bleeding from bone surfaces; shoe polish and furniture polish can both use beeswax as a component, dissolved in turpentine or sometimes blended with linseed oil or tung oil; modeling waxes can also use beeswax as a component; pure beeswax can also be used as an organic surfboard wax. Beeswax blended with pine rosin is used for waxing, and can serve as an adhesive to attach reed plates to the structure inside a squeezebox. It can also be used to make Cutler's resin, an adhesive used to glue handles onto cutlery knives. It is used in Eastern Europe in egg decoration; it is used for writing, via resist dyeing, on batik eggs (as in \"pysanky\") and for making beaded eggs.\nBeeswax is used by percussionists to make a surface on tambourines for thumb rolls. It can also be used as a metal injection moulding binder component along with other polymeric binder materials.\n\nBeeswax was formerly used in the manufacture of phonograph cylinders. It may still be used to seal formal legal or royal decree and academic parchments such as placing an awarding stamp imprimatur of the university upon completion of postgraduate degrees.\n\nPurified and bleached beeswax is used in the production of food, cosmetics, and pharmaceuticals. The three main types of beeswax products are yellow, white, and beeswax absolute. Yellow beeswax is the crude product obtained from the honeycomb, white beeswax is bleached or filtered yellow beeswax, and beeswax absolute is yellow beeswax treated with alcohol. In food preparation, it is used as a coating for cheese; by sealing out the air, protection is given against spoilage (mold growth). Beeswax may also be used as a food additive E901, in small quantities acting as a glazing agent, which serves to prevent water loss, or used to provide surface protection for some fruits. Soft gelatin capsules and tablet coatings may also use E901. Beeswax is also a common ingredient of natural chewing gum.The wax monoesters in beeswax are poorly hydrolysed in the guts of humans and other mammals, so they have insignificant nutritional value. Some birds, such as honeyguides, can digest beeswax. Beeswax is the main diet of wax moth larvae.\n\nUse of beeswax in skin care and cosmetics has been increasing. A German study found beeswax to be superior to similar barrier creams (usually mineral oil-based creams such as petroleum jelly), when used according to its protocol.\nBeeswax is used in lip balm, lip gloss, hand creams, salves, and moisturizers; and in cosmetics such as eye shadow, blush, and eye liner. Beeswax is also an important ingredient in moustache wax and hair pomades, which make hair look sleek and shiny.\nBeeswax was among the first plastics to be used, alongside other natural polymers such as gutta-percha, horn, tortoiseshell, and shellac. For thousands of years, beeswax has had a wide variety of applications; it has been found in the tombs of Egypt, in wrecked Viking ships, and in Roman ruins. Beeswax never goes bad and can be heated and reused. Historically, it has been used:\n\n\n\n"}
{"id": "43946", "url": "https://en.wikipedia.org/wiki?curid=43946", "title": "Biofilm", "text": "Biofilm\n\nA biofilm comprises any syntrophic consortium of microorganisms in which cells stick to each other and often also to a surface. These adherent cells become embedded within a slimy extracellular matrix that is composed of extracellular polymeric substances (EPS). The cells within the biofilm produce the EPS components, which are typically a polymeric conglomeration of extracellular polysaccharides, proteins, lipids and DNA. Because they have three-dimensional structure and represent a community lifestyle for microorganisms, they have been metaphorically described as \"cities for microbes\".\n\nBiofilms may form on living or non-living surfaces and can be prevalent in natural, industrial and hospital settings. The microbial cells growing in a biofilm are physiologically distinct from planktonic cells of the same organism, which, by contrast, are single-cells that may float or swim in a liquid medium. Biofilms can form on the teeth of most animals as dental plaque, where they may cause tooth decay and gum disease.\n\nMicrobes form a biofilm in response to various different factors, which may include cellular recognition of specific or non-specific attachment sites on a surface, nutritional cues, or in some cases, by exposure of planktonic cells to sub-inhibitory concentrations of antibiotics. A cell that switches to the biofilm mode of growth undergoes a phenotypic shift in behavior in which large suites of genes are differentially regulated.\n\nA biofilm may also be considered a hydrogel, which is a complex polymer that contains many times its dry weight in water. Biofilms are not just bacterial slime layers but biological systems; the bacteria organize themselves into a coordinated functional community. Biofilms can attach to a surface such as a tooth, rock, or surface, and may include a single species or a diverse group of microorganisms. The biofilm bacteria can share nutrients and are sheltered from harmful factors in the environment, such as desiccation, antibiotics, and a host body's immune system. A biofilm usually begins to form when a free-swimming bacterium attaches to a surface.\n\nThe formation of a biofilm begins with the attachment of free-floating microorganisms to a surface. The first colonist bacteria of a biofilm may adhere to the surface initially by the weak van der Waals forces and hydrophobic effects. If the colonists are not immediately separated from the surface, they can anchor themselves more permanently using cell adhesion structures such as pili.\n\nHydrophobicity can also affect the ability of bacteria to form biofilms. Bacteria with increased hydrophobicity have reduced repulsion between the substratum and the bacterium. Some bacteria species are not able to attach to a surface on their own successfully due to their limited motility but are instead able to anchor themselves to the matrix or directly to other, earlier bacteria colonists. Non-motile bacteria cannot recognize surfaces or aggregate together as easily as motile bacteria.\n\nDuring surface colonization bacteria cells are able to communicate using quorum sensing (QS) products such as N-acyl homoserine lactone (AHL). Once colonization has begun, the biofilm grows by a combination of cell division and recruitment. Polysaccharide matrices typically enclose bacterial biofilms. In addition to the polysaccharides, these matrices may also contain material from the surrounding environment, including but not limited to minerals, soil particles, and blood components, such as erythrocytes and fibrin. The final stage of biofilm formation is known as dispersion, and is the stage in which the biofilm is established and may only change in shape and size.\n\nThe development of a biofilm may allow for an aggregate cell colony (or colonies) to be increasingly resistant to antibiotics. Cell-cell communication or quorum sensing has been shown to be involved in the formation of biofilm in several bacterial species.\n\nBiofilms are the product of a microbial developmental process. The process is summarized by five major stages of biofilm development (see illustration on the right):\n\n\nDispersal of cells from the biofilm colony is an essential stage of the biofilm life cycle. Dispersal enables biofilms to spread and colonize new surfaces. Enzymes that degrade the biofilm extracellular matrix, such as dispersin B and deoxyribonuclease, may contribute to biofilm dispersal. Enzymes that degrade the biofilm matrix may be useful as anti-biofilm agents. Recent evidence has shown that a fatty acid messenger, \"cis\"-2-decenoic acid, is capable of inducing dispersion and inhibiting growth of biofilm colonies. Secreted by \"Pseudomonas aeruginosa\", this compound induces cyclo heteromorphic cells in several species of bacteria and the yeast \"Candida albicans\".\nNitric oxide has also been shown to trigger the dispersal of biofilms of several bacteria species at sub-toxic concentrations. Nitric oxide has the potential for the treatment of patients that suffer from chronic infections caused by biofilms.\n\nIt is generally assumed that cells dispersed from biofilms immediately go into the planktonic growth phase. However, recent studies have shown that the physiology of dispersed cells from \"Pseudomonas aeruginosa\" biofilms is highly different from those of planktonic and biofilm cells. Hence, the dispersal process is a unique stage during the transition from biofilm to planktonic lifestyle in bacteria. Dispersed cells are found to be highly virulent against macrophages and \"Caenorhabditis elegans\", but highly sensitive towards iron stress, as compared with planktonic cells.\n\nBiofilms are usually found on solid substrates submerged in or exposed to an aqueous solution, although they can form as floating mats on liquid surfaces and also on the surface of leaves, particularly in high humidity climates. Given sufficient resources for growth, a biofilm will quickly grow to be macroscopic (visible to the naked eye). Biofilms can contain many different types of microorganism, e.g. bacteria, archaea, protozoa, fungi and algae; each group performs specialized metabolic functions. However, some organisms will form single-species films under certain conditions. The social structure (cooperation/competition) within a biofilm depends highly on the different species present.\n\nThe EPS matrix consists of exopolysaccharides, proteins and nucleic acids. A large proportion of the EPS is more or less strongly hydrated, however, hydrophobic EPS also occur; one example is cellulose which is produced by a range of microorganisms. This matrix encases the cells within it and facilitates communication among them through biochemical signals as well as gene exchange. The EPS matrix also traps extracellular enzymes and keeps them in close proximity to the cells. Thus, the matrix represents an external digestion system and allows for stable synergistic microconsortia of different species (Wingender and Flemming, Nat. Rev. Microbiol. 8, 623-633). Some biofilms have been found to contain water channels that help distribute nutrients and signalling molecules. This matrix is strong enough that under certain conditions, biofilms can become fossilized (Stromatolites).\n\nBacteria living in a biofilm usually have significantly different properties from free-floating bacteria of the same species, as the dense and protected environment of the film allows them to cooperate and interact in various ways. One benefit of this environment is increased resistance to detergents and antibiotics, as the dense extracellular matrix and the outer layer of cells protect the interior of the community. In some cases antibiotic resistance can be increased a thousandfold. Lateral gene transfer is often facilitated within bacterial and archaeal biofilms and leads to a more stable biofilm structure. Extracellular DNA is a major structural component of many different microbial biofilms. Enzymatic degradation of extracellular DNA can weaken the biofilm structure and release microbial cells from the surface.\n\nHowever, biofilms are not always less susceptible to antibiotics. For instance, the biofilm form of \"Pseudomonas aeruginosa\" has no greater resistance to antimicrobials than do stationary-phase planktonic cells, although when the biofilm is compared to logarithmic-phase planktonic cells, the biofilm does have greater resistance to antimicrobials. This resistance to antibiotics in both stationary-phase cells and biofilms may be due to the presence of persister cells.\n\nBiofilms are ubiquitous in organic life. Nearly every species of microorganism have mechanisms by which they can adhere to surfaces and to each other. Biofilms will form on virtually every non-shedding surface in non-sterile aqueous or humid environments. Biofilms can grow in the most extreme environments: from, for example, the extremely hot, briny waters of hot springs ranging from very acidic to very alkaline, to frozen glaciers.\n\nBiofilms can be found on rocks and pebbles at the bottoms of most streams or rivers and often form on the surfaces of stagnant pools of water. Biofilms are important components of food chains in rivers and streams and are grazed by the aquatic invertebrates upon which many fish feed. Biofilms are found on the surface of and inside plants. They can either contribute to crop disease or, as in the case of nitrogen-fixing Rhizobium on roots, exist symbiotically with the plant. Examples of crop diseases related to biofilms include Citrus Canker, Pierce's Disease of grapes, and Bacterial Spot of plants such as peppers and tomatoes.\n\nRecent studies in 2003 discovered that the immune system supports bio-film development in the large intestine. This was supported mainly with the fact that the two most abundantly produced molecules by the immune system also support bio-film production and are associated with the bio-films developed in the gut. This is especially important because the appendix holds a mass amount of these bacterial bio-films. This discovery helps to distinguish the possible function of the appendix and the idea that the appendix can help reinoculate the gut with good gut flora.\n\nIn the human environment, biofilms can grow in showers very easily since they provide a moist and warm environment for the biofilm to thrive. Biofilms can form inside water and sewage pipes and cause clogging and corrosion. Biofilms on floors and counters can make sanitation difficult in food preparation areas. Biofilm in soil can cause bioclogging. Biofilms in cooling- or heating-water systems are known to reduce heat transfer. Biofilms in marine engineering systems, such as pipelines of the offshore oil and gas industry, can lead to substantial corrosion problems. Corrosion is mainly due to abiotic factors; however, at least 20% of corrosion is caused by microorganisms that are attached to the metal subsurface (i.e., microbially influenced corrosion).\n\nBacterial adhesion to boat hulls serves as the foundation for biofouling of seagoing vessels. Once a film of bacteria forms, it is easier for other marine organisms such as barnacles to attach. Such fouling can reduce maximum vessel speed by up to 20%, prolonging voyages and consuming fuel. Time in dry dock for refitting and repainting reduces the productivity of shipping assets, and the useful life of ships is also reduced due to corrosion and mechanical removal (scraping) of marine organisms from ships' hulls.\n\nStromatolites are layered accretionary structures formed in shallow water by the trapping, binding and cementation of sedimentary grains by microbial biofilms, especially of cyanobacteria. Stromatolites include some of the most ancient records of life on Earth, and are still forming today.\n\nWithin the human body, biofilms are present on the teeth as dental plaque, where they may cause tooth decay and gum disease. These biofilms can either be in an uncalcified state that can be removed by dental instruments, or a calcified state which is more difficult to remove. Removal techniques can also include antimicrobials.\n\nDental plaque is an oral biofilm that adheres to the teeth and consists of many species of both bacteria and fungi (such as \"Streptococcus mutans\" and \"Candida albicans\"), embedded in salivary polymers and microbial extracellular products. The accumulation of microorganisms subjects the teeth and gingival tissues to high concentrations of bacterial metabolites which results in dental disease. Biofilm on the surface of teeth is frequently subject to oxidative stress and acid stress. Dietary carbohydrates can cause a dramatic decrease in pH in oral biofilms to values of 4 and below (acid stress). A pH of 4 at body temperature of 37 °C causes depurination of DNA, leaving apurinic (AP) sites in DNA, especially loss of guanine.\n\nThe dental plaque biofilm can result in the disease dental caries if it is allowed to develop over time. An ecologic shift away from balanced populations within the dental biofilm is driven by certain (cariogenic) microbiological populations beginning to dominate when the environment favours them. The shift to an acidogenic, aciduric, and cariogenic microbiological population develops and is maintained by frequent consumption of fermentable dietary carbohydrate. The resulting activity shift in the biofilm (and resulting acid production within the biofilm, at the tooth surface) is associated with an imbalance between demineralization and remineralisation leading to net mineral loss within dental hard tissues (enamel and then dentin), the sign and symptom being a carious lesion. By preventing the dental plaque biofilm from maturing or by returning it back to a non-cariogenic state, dental caries can be prevented and arrested. This can be achieved though the behavioural step of reducing the supply of fermentable carbohydrates (i.e. sugar intake) and frequent removal of the biofilm (i.e. toothbrushing).\n\nA peptide pheromone quorum sensing signaling system in \"S. mutans\" includes the Competence Stimulating Peptide (CSP) that controls genetic competence. Genetic competence is the ability of a cell to take up DNA released by another cell. Competence can lead to genetic transformation, a form of sexual interaction, favored under conditions of high cell density and/or stress where there is maximal opportunity for interaction between the competent cell and the DNA released from nearby donor cells. This system is optimally expressed when \"S. mutans\" cells reside in an actively growing biofilm. Biofilm grown \"S. mutans\" cells are genetically transformed at a rate 10- to 600-fold higher than \"S. mutans\" growing as free-floating planktonic cells suspended in liquid.\n\nWhen the biofilm, containing \"S. mutans\" and related oral streptococci, is subjected to acid stress, the competence regulon is induced, leading to resistance to being killed by acid. As pointed out by Michod et al., transformation in bacterial pathogens likely provides for effective and efficient recombinational repair of DNA damages. It appears that \"S. mutans\" can survive the frequent acid stress in oral biofilms, in part, through the recombinational repair provided by competence and transformation.\n\nMany different bacteria form biofilms, including gram-positive (e.g. \"Bacillus\" spp, \"Listeria monocytogenes\", \"Staphylococcus\" spp, and lactic acid bacteria, including \"Lactobacillus plantarum\" and \"Lactococcus lactis\") and gram-negative species (e.g. \"Escherichia coli\", or \"Pseudomonas aeruginosa\"). Cyanobacteria also form biofilms in aquatic environments.\n\nBiofilms are formed by bacteria that colonize plants, e.g. \"Pseudomonas putida\", \"Pseudomonas fluorescens\", and related pseudomonads which are common plant-associated bacteria found on leaves, roots, and in the soil, and the majority of their natural isolates form biofilms. Several nitrogen-fixing symbionts of legumes such as \"Rhizobium leguminosarum\" and \"Sinorhizobium meliloti\" form biofilms on legume roots and other inert surfaces.\n\nAlong with bacteria, biofilms are also generated by archaea and by a range of eukaryotic organisms, including fungi e.g. Cryptococcus laurentii and microalgae. Among microalgae, one of the main progenitors of biofilms are diatoms, which colonise both fresh and marine environments worldwide.\n\nFor other species in disease-associated biofilms and biofilms arising from eukaryotes see below.\n\nBiofilms have been found to be involved in a wide variety of microbial infections in the body, by one estimate 80% of all infections. Infectious processes in which biofilms have been implicated include common problems such as bacterial vaginosis, urinary tract infections, catheter infections, middle-ear infections, formation of dental plaque, gingivitis, coating contact lenses, and less common but more lethal processes such as endocarditis, infections in cystic fibrosis, and infections of permanent indwelling devices such as joint prostheses, heart valves, and intervertebral disc. More recently it has been noted that bacterial biofilms may impair cutaneous wound healing and reduce topical antibacterial efficiency in healing or treating infected skin wounds. Early detection of biofilms in wounds is crucial to successful chronic wound management. Although many techniques have developed to identify planktonic bacteria in viable wounds, few have been able to quickly and accurately identify bacterial biofilms. Future studies are needed to find means of identifying and monitoring biofilm colonization at the bedside to permit timely initiation of treatment.\n\nIt has recently been shown that biofilms are present on the removed tissue of 80% of patients undergoing surgery for chronic sinusitis. The patients with biofilms were shown to have been denuded of cilia and goblet cells, unlike the controls without biofilms who had normal cilia and goblet cell morphology. Biofilms were also found on samples from two of 10 healthy controls mentioned. The species of bacteria from intraoperative cultures did not correspond to the bacteria species in the biofilm on the respective patient's tissue. In other words, the cultures were negative though the bacteria were present. New staining techniques are being developed to differentiate bacterial cells growing in living animals, e.g. from tissues with allergy-inflammations.\n\nResearch has shown that sub-therapeutic levels of β-lactam antibiotics induce biofilm formation in \"Staphylococcus aureus\". This sub-therapeutic level of antibiotic may result from the use of antibiotics as growth promoters in agriculture, or during the normal course of antibiotic therapy. The biofilm formation induced by low-level methicillin was inhibited by DNase, suggesting that the sub-therapeutic levels of antibiotic also induce extracellular DNA release. Moreover, from an evolutionary point of view, the creation of the tragedy of the commons in pathogenic microbes may provide advanced therapeutic ways for chronic infections caused by biofilms via genetically engineered invasive cheaters who can invade wild-types ‘cooperators’ of pathogenic bacteria until cooperator populations go to extinction or overall population ‘cooperators and cheaters ’ go to extinction.\n\n\"P. aeruginosa\" represents a commonly used biofilm model organism since it is involved in different types of biofilm-associated infections. Examples of such infections include chronic wounds, chronic otitis media, chronic prostatitis and chronic lung infections in cystic fibrosis (CF) patients. About 80% of CF patients have chronic lung infection, caused mainly by \"P. aeruginosa\" growing in a non-surface attached biofilms surround by PMN. The infection remains present despite aggressive antibiotic therapy and is a common cause of death in CF patients due to constant inflammatory damage to the lungs. In patients with CF, one therapy for treating early biofilm development is to employ DNase to structurally weaken the biofilm.\n\n\"S. pneumoniae\" is the main cause of community-acquired pneumonia and meningitis in children and the elderly, and of septicemia in HIV-infected persons. When \"S. pneumonia\" grows in biofilms, genes are specifically expressed that respond to oxidative stress and induce competence. Formation of a biofilm depends on competence stimulating peptide (CSP). CSP also functions as a quorum-sensing peptide. It not only induces biofilm formation, but also increases virulence in pneumonia and meningitis.\n\nIt has been proposed that competence development and biofilm formation is an adaptation of \"S. pneumoniae\" to survive the defenses of the host. In particular, the host’s polymorphonuclear leukocytes produce an oxidative burst to defend against the invading bacteria, and this response can kill bacteria by damaging their DNA. Competent \"S. pneumoniae\" in a biofilm have the survival advantage that they can more easily take up transforming DNA from nearby cells in the biofilm to use for recombinational repair of oxidative damages in their DNA. Competent \"S. pneumoniae\" can also secrete an enzyme (murein hydrolase) that destroys non-competent cells (fratricide) causing DNA to be released into the surrounding medium for potential use by the competent cells.\n\nInfections associated with the biofilm growth usually are challenging to eradicate. This is mostly due to the fact that mature biofilms display tolerance towards antibiotics and the immune response. Biofilms often form on the inert surfaces of implanted devices such as catheters, prosthetic cardiac valves and intrauterine devices.\n\nThe rapidly expanding worldwide industry for biomedical devices and tissue engineering related products is already at $180 billion per year, yet this industry continues to suffer from microbial colonization. No matter the sophistication, microbial infections can develop on all medical devices and tissue engineering constructs. 60-70% of nosocomial or hospital acquired infections are associated with the implantation of a biomedical device. This leads to 2 million cases annually in the U.S., costing the healthcare system over $5 billion in additional healthcare expenses.\n\nBiofilms can also be harnessed for constructive purposes. For example, many sewage treatment plants include a secondary treatment stage in which waste water passes over biofilms grown on filters, which extract and digest organic compounds. In such biofilms, bacteria are mainly responsible for removal of organic matter (BOD), while protozoa and rotifers are mainly responsible for removal of suspended solids (SS), including pathogens and other microorganisms. Slow sand filters rely on biofilm development in the same way to filter surface water from lake, spring or river sources for drinking purposes. What we regard as clean water is effectively a waste material to these microcellular organisms. Biofilms can help eliminate petroleum oil from contaminated oceans or marine systems. The oil is eliminated by the hydrocarbon-degrading activities of microbial communities, in particular by a remarkable recently discovered group of specialists, the so-called hydrocarbonoclastic bacteria (HCB).\nBiofilms are used in microbial fuel cells (MFCs) to generate electricity from a variety of starting materials, including complex organic waste and renewable biomass.\nBiofilms are also relevant for the improvement of metal dissolution in bioleaching industry\n\nBiofilms have become problematic in several food industries due to the ability to form on plants and during industrial processes. Bacteria can survive long periods of time in water, animal manure, and soil, causing biofilm formation on plants or in the processing equipment. The buildup of biofilms can affect the heat flow across a surface and increase surface corrosion and frictional resistance of fluids. These can lead to a loss of energy in a system and overall loss of products. Along with economic problems, biofilm formation on food poses a health risk to consumers due to the ability to make the food more resistant to disinfectants As a result, from 1996 to 2010 the Center for Disease Control and Prevention estimated 48 million foodborne illnesses per year. Biofilms have been connected to about 80% of bacterial infections in the United States.\n\nIn produce, microorganisms attach to the surfaces and biofilms develop internally. During the washing process, biofilms resist sanitization and allow bacteria to spread across the produce. This problem is also found in ready-to-eat foods, because the foods go through limited cleaning procedures before consumption Due to the perishability of dairy products and limitations in cleaning procedures, resulting in the buildup of bacteria, dairy is susceptible to biofilm formation and contamination. The bacteria can spoil the products more readily and contaminated products pose a health risk to consumers. One bacteria that can be found in various industries and is a major cause of foodborne disease is Salmonella. Large amounts of salmonella contamination can be found in the poultry processing industry as about 50% of salmonella strains can produce biofilms on poultry farms. Salmonella increases the risk of foodborne illnesses when the poultry products are not cleaned and cooked correctly. Salmonella is also found in the seafood industry where biofilms form from seafood borne pathogens on the seafood itself as well as in water. Shrimp products are commonly affected by salmonella because of unhygienic processing and handling techniques The preparation practices of shrimp and other seafood products can allow for bacteria buildup on the products.\n\nNew forms of cleaning procedures are being tested in order to reduce biofilm formation in these processes which will lead to safer and more productive food processing industries. These new forms of cleaning procedures also have a profound effect on the environment, often releasing toxic gases into the groundwater reservoirs.\n\nIn shellfish and algae farms, biofouling microbial species tend to block nets and cages and ultimately outcompete the farmed species for space and food. Bacterial biofilms start the colonization process by creating microenvironments that are more favorable for biofouling species. In the marine environment, biofilms could reduce the hydrodynamic efficiency of ships and propellers, lead to pipeline blockage and sensor malfunction, and increase the weight of appliances deployed in seawater. Numerous studies have shown that biofilm can be a reservoir for potentially pathogenic bacteria in freshwater aquaculture. As mentioned previously, biofilms can be difficult to eliminate even when antibiotics or chemicals are used in high doses. The role that biofilm plays as reservoirs of bacterial fish pathogens has not been explored in detail but it certainly deserves to be studied.\n\nAlong with bacteria, biofilms are often initiated and produced by eukaryotic microbes. The biofilms produced by eukaryotes is usually occupied by bacteria and other eukaryotes alike, however the surface is cultivated and EPS is secreted initially by the eukaryote. Both fungi and microalgae are known to form biofilms in such a way. Biofilms of fungal origin are important aspects of human infection and fungal pathagenicity, as the fungal infection is more resistant to antifungals.\n\nIn the environment, fungal biofilms are an area of ongoing research. One key area of research is fungal biofilms on plants. For example, in the soil, plant associated fungi including mycorrhiza have been shown to decompose organic matter, protect plants from bacterial pathogens.\n\nBiofilms in aquatic environments are often founded by diatoms. The exact purpose of these biofilms is unknown, however there is evidence that the EPS produced by diatoms facilitates both cold and salinity stress. These eukaryotes interact with a diverse range of other organisms within a region known as the phycosphere, but importantly are the bacteria associated with diatoms, as it has been shown that although diatoms excrete EPS, they only do so when interacting with certain bacteria species.\n\n\n\n\n\n"}
{"id": "4894039", "url": "https://en.wikipedia.org/wiki?curid=4894039", "title": "Boundary layer thickness", "text": "Boundary layer thickness\n\nThis page describes some parameters used to characterize the properties of a boundary layer formed by fluid flow along a wall. The boundary layer concept was first described by Ludwig Prandtl. Consider a stationary body with a fluid flowing around it, like the semi-infinite flat plate with air flowing over the top of the plate (assume the flow and the plate extends to infinity in the positive/negative direction perpendicular to the formula_1 plane). At the solid walls of the body the fluid satisfies a no-slip boundary condition and has zero velocity, but as you move away from the wall, the velocity of the flow asymptotically approaches the free stream mean velocity. Therefore, it is impossible to define a sharp point at which the boundary layer becomes the free stream, yet this layer has a well-defined characteristic thickness. The parameters below provide a useful definition of this characteristic, measurable thickness. Also included in this boundary layer description are some parameters useful in describing the shape of the boundary layer.\n\nThe boundary layer thickness, δ, is the distance across a boundary layer from the wall to a point where the flow velocity has essentially reached the 'free stream' velocity, formula_2. This distance is defined normal to the wall. It is customarily defined as the point formula_3 where:\nat a point on the wall formula_5. For laminar boundary layers over a flat plate, the Blasius solution to the flow governing equations gives:\n\nFor turbulent boundary layers over a flat plate, the boundary layer thickness is given by:\n\nwhere\n\nThe turbulent boundary layer thickness formula assumes 1) the flow is turbulent right from the start of the boundary layer and 2) the turbulent boundary layer behaves in a geometrically similar manner (i.e. the velocity profiles are geometrically similar along the flow in the x-direction, differing only by stretching factors in formula_17 and formula_18). Neither one of these assumptions are true for the general turbulent boundary layer case so care must be excersised in applying this formula.\n\nThe velocity thickness can also be referred to as the Soole ratio, although the gradient of the thickness over distance would be adversely proportional to that of velocity thickness.\n\nThe displacement thickness, δ or δ is the distance by which a surface would have to be moved in the direction perpendicular to its normal vector away from the reference plane in an inviscid fluid stream of velocity formula_2 to give the same flow rate as occurs between the surface and the reference plane in a real fluid.\n\nIn practical aerodynamics, the displacement thickness essentially modifies the shape of a body immersed in a fluid to allow an inviscid solution. It is commonly used in aerodynamics to overcome the difficulty inherent in the fact that the fluid velocity in the boundary layer approaches asymptotically to the free stream value as distance from the wall increases at any given location.\n\nThe definition of the displacement thickness for compressible flow is based on mass flow rate:\n\nThe definition for incompressible flow can be based on volumetric flow rate, as the density is constant:\n\nwhere formula_22 and formula_2 are the density and velocity in the 'free stream' outside the boundary layer, and formula_17 is the coordinate normal to the wall.\n\nFor turbulent boundary layer calculations, the time averaged density and velocity at the edge of the boundary layer must be used. In the equations above, formula_22 and formula_2 are therefore replaced with formula_27 and formula_28.\n\nThe displacement thickness is used to calculate the boundary layer's shape factor.\n\nThe momentum thickness, θ or δ, is the distance by which a surface would have to be moved parallel to itself towards the reference plane in an inviscid fluid stream of velocity formula_2 to give the same total momentum as exists between the surface and the reference plane in a real fluid.\n\nThe definition of the momentum thickness for compressible flow is based on mass flow rate:\n\nThe definition for incompressible flow can be based on volumetric flow rate, as the density is constant:\n\nWhere formula_22 and formula_2 are the density and velocity in the 'free stream' outside the boundary layer, and formula_17 is the coordinate normal to the wall.\n\nFor turbulent boundary layer calculations, the time averaged density and velocity at the edge of the boundary layer must be used. In the equations above, formula_22 and formula_2 are therefore replaced with formula_27 and formula_28.\n\nFor a flat plate at zero angle of attack with a laminar boundary layer, the Blasius solution gives.\n"}
{"id": "661808", "url": "https://en.wikipedia.org/wiki?curid=661808", "title": "Bravais lattice", "text": "Bravais lattice\n\nIn geometry and crystallography, a Bravais lattice, named after , is an infinite array of discrete points generated by a set of discrete translation operations described in three dimensional space by:\n\nwhere \"n\" are any integers and a are \"primitive vectors\" which lie in different directions (not necessarily mutually perpendicular) and span the lattice. This discrete set of vectors must be closed under vector addition and subtraction. For any choice of position vector R, the lattice looks exactly the same.\n\nWhen the discrete points are atoms, ions, or polymer strings of solid matter, the Bravais lattice concept is used to formally define a \"crystalline arrangement\" and its (finite) frontiers. A crystal is made up of a periodic arrangement of one or more atoms (the \"basis\", or \"motif\") repeated at each lattice point. Consequently, the crystal looks the same when viewed from any equivalent lattice point, namely those separated by the translation of one unit cell.\n\nTwo Bravais lattices are often considered equivalent if they have isomorphic symmetry groups. In this sense, there are 14 possible Bravais lattices in three-dimensional space. The 14 possible symmetry groups of Bravais lattices are 14 of the 230 space groups.\n\nIn two-dimensional space, there are 5 Bravais lattices, grouped into four crystal families.\n\nThe unit cells are specified according to the relative lengths of the cell edges (\"a\" and \"b\") and the angle between them (\"θ\"). The area of the unit cell can be calculated by evaluating the norm , where a and b are the lattice vectors. The properties of the crystal families are given below:\n\nIn three-dimensional space, there are 14 Bravais lattices. These are obtained by combining one of the seven lattice systems with one of the centering types. The centering types identify the locations of the lattice points in the unit cell as follows:\n\n\nNot all combinations of lattice systems and centering types are needed to describe all of the possible lattices, as it can be shown that several of these are in fact equivalent to each other. For example, the monoclinic I lattice can be described by a monoclinic C lattice by different choice of crystal axes. Similarly, all A- or B-centred lattices can be described either by a C- or P-centering. This reduces the number of combinations to 14 conventional Bravais lattices, shown in the table below.\n\nThe unit cells are specified according to the relative lengths of the cell edges (\"a\", \"b\", \"c\") and the angles between them (\"α\", \"β\", \"γ\"). The volume of the unit cell can be calculated by evaluating the triple product , where a, b, and c are the lattice vectors. The properties of the lattice systems are given below:\n\nIn four dimensions, there are 64 Bravais lattices. Of these, 23 are primitive and 41 are centered. Ten Bravais lattices split into enantiomorphic pairs.\n\n\n"}
{"id": "3120918", "url": "https://en.wikipedia.org/wiki?curid=3120918", "title": "Bulahdelah tornado", "text": "Bulahdelah tornado\n\nThe Bulahdelah Tornado was an intense tornado which occurred near the town of Bulahdelah ( north-northeast of Newcastle), New South Wales on 1 January 1970, and is thought to be the most destructive tornado ever documented in Australia. It is thought to be an F4 or F5 on the Fujita scale; however, no official rating has been made public.\n\nThe tornado left a damage path long and 1–1.6 km (0.6–1 mi) wide through the Bulahdelah State Forest. It is estimated that the tornado destroyed over one million trees. A caravan was destroyed and a 2-ton (2,000 kg) tractor was lifted into the air, landing upside down. The tornado was reported by witnesses as a swirling black cloud surrounded by flying debris, and producing a thunderous roaring sound. The weather system that produced the tornado was a classic set-up for violent tornadoes, something somewhat rarely seen outside of the United States, Canada, Bangladesh, and adjacent areas of India.\n\n"}
{"id": "53151364", "url": "https://en.wikipedia.org/wiki?curid=53151364", "title": "Chrysler minivans (NS)", "text": "Chrysler minivans (NS)\n\nThe third-generation Chrysler minivans are a series of passenger minivans that were marketed by the Chrysler Corporation (later DaimlerChrysler) from the 1996 to 2000 model years. Designated the NS platform by Chrysler, these minivans were sold by Chrysler, Dodge and Plymouth divisions in passenger configurations; minivans were exported under the Chrysler brand. While the second-generation AS platform was a revision of the original vans, the NS platform marked the first ground-up redesign of the Chrysler vans since their 1984 introduction, ending the use of components from K-Car derivatives.\n\nThe first non-compact minivans in North America to adopt four sliding passenger doors, the third-generation Chrysler minivans saw its form factor adopted by many competitors. In addition to chief competitors Ford Windstar, Honda Odyssey, and Toyota Sienna, the NS-platform configuration was adopted by the Chevrolet Venture, Oldsmobile Silhouette, and Pontiac (Trans Sport) Montana, and the Mercury Villager/Nissan Quest.\n\nIn line with the first and second-generation minivans, the third-generation minivans were assembled at Windsor Assembly in Windsor, Ontario, Canada, with additional production sourced from Saint Louis (South) Assembly in Fenton, Missouri. To supplement exports from the United States, production of the Chrysler Voyager was sourced from Graz, Austria (in the Eurostar joint venture factory between Chrysler and Steyr-Daimler-Puch).\n\nDevelopment of the NS platform minivans commenced in 1990; with a team lead by Tom Gale and Chris Theodore, design work (by Don Renkert) was approved in September 1991 with a design freeze in May 1992. In addition to becoming the first all-new Chrysler minivan in a decade, the third-generation vans were also a response to the development of a front-wheel drive replacement for the Ford Aerostar (unveiled in 1994 as the 1995 Ford Windstar).\n\nWhile the S/AS-platform vans did not share direct structural underpinnings with the Chrysler K-Cars, in contrast to their predecessors, the NS vans were the first generation designed from the ground up as a minivan. One of the first Chrysler vehicles designed with CATIA, which allowed for much tighter design tolerances.\n\nIn its effort to design the third-generation minivans, Chrysler benchmarked the vehicle from various sources of data, using customer input, warranty data, and research based on various other minivans (the Ford Aerostar, Mercury Villager, and Toyota Previa) During its market research, Chrysler sought owner feedback on adding a $300 driver-side sliding door option from minivan owners, with 85% of participants answering that they would buy the van with the second sliding door, even as an extra-cost option. Ironically, the feature was planned for the first-generation minivans, but was removed in 1980 due to concerns over additional tooling costs.\n\nAlongside the sliding door, several designs underwent consideration for the floor layout. Initially, a lower floor height was considered (research feedback felt that the step-in height was too high), but was rejected as it jeopardized the higher driving position sought by buyers. For production, a compromise design was approved, retaining the same floor height as the previous generation eased into lower door sills. In addition to preserving the higher driving position, the floor design allowed the fitment of all-wheel drive without major modification, larger wheels and tires, CNG tanks, and batteries for electric vehicles.\n\nWhile the V6 powertrains of the previous-generation platform were retained, during the development of the NS platform, several configurations underwent configuration. To improve the body structure, a mid-engine powertrain (in line with the Toyota Previa) was initially considered, but was rejected for cost and complexity considerations. A transition to a longitudinal-mounted powertrain (to simplify V6 engine access and all-wheel drive configuration) underwent strong consideration; the transverse layout was chosen in order to reduce overall length.\n\nIn an extensive shift from the boxy design of the first two generations, the body adopted cab-forward design, shifting the dashboard and windshield forward. In contrast to the controversial styling of the GM APV minivans, the roof was moved upward (nearly 3 inches) and the cowl moved lower, allowing for a less radical windshield angle. To move the base of the windshield back several inches (to further improve visibility), Chrysler designed the windshield wiper module/lower windshield cowl to be removable (allowing for improved engine bay access).\n\nIn total, the development of the NS platform would cost Chrysler Corporation $2.8 billion dollars (approximately $4.6 billion in 2017 dollars), the most costly vehicle ever developed by Chrysler at the time.\n\nThe third-generation Chrysler minivans use the Chrysler NS platform designation. Using a completely new unibody chassis, the NS platform uses a 113.3 inch wheelbase for standard-wheelbase vans (1.3 inches longer) and a 119.3 inch wheelbase for long-wheelbase vans (same as 1994-1995 AS-platform vans). Front-wheel drive is the standard drivetrain configuration with all-wheel drive offered as an option.\n\nThe suspension of the NS platform is a modified version of the previous two generations. In front, the use of MacPherson struts continued, with leaf springs and a beam axle in the rear. During its development, a number of suspension configurations were considered, including a 4-wheel double wishbone layout. Rear leaf springs were retained largely in an effort to provide an ideal handling balance with a load in addition to maximize interior load space.\n\nIn a major modification, the front track width was widened three inches, allowing for lower mounting of the engine and transaxle, reduction in turning radius, and a lower cowl height.\n\nFront-wheel drive vans had front-wheel disc brakes and rear drum brakes. From 1997, all-wheel drive vans were fitted with four-wheel disc brakes.\nFor 1996, the Chrysler minivans received the first new four-cylinder engine in the powertrain line since 1987, with the introduction of a 2.4L engine (shared with the JA-platform cars). While smaller in displacement than its 2.5L predecessor, the 150hp engine nearly matched the 3.0L V6 in both output and fuel economy.\n\nIn states that did not observe California emission standards, the Mitsubishi 3.0L V6 was the standard V6 option. In those that followed California emissions standards, the Chrysler 3.3L V6 was the standard V6 option. In the Dodge Caravan ES and Chrysler Town & Country LXi, the 3.8L V6 was standard.\n\nIn 1999, the 3.8L V6 became optional on the Plymouth Voyager Expresso; in Canada, the 2.4L engine was dropped, with the 3.0L V6 becoming the standard engine.\n\nIn the United States and Canada, a manual transmission was no longer available, with a 3-speed TorqueFlite automatic fitted to the 2.4L and 3.0L engines. The 3.3L and 3.8L engines were fitted with a 4-speed Ultradrive automatic.\n\nFor the first time, export vehicles were fitted with different powertrains, with the standard engine being a SOHC 2.0L engine with an optional DOHC 2.0L engine (from the Neon); a VM Motori 2.5L inline-4 was offered, becoming the first diesel Chrysler minivan. All three engines were offered with either a 5-speed manual or an automatic transmission. In addition to the four-cylinder engines, export vehicles were offered the 3.3L and 3.8L V6 engines offered in American-market minivans. AWD models were not sold in the United Kingdom.\nIn an extensive shift from the second-generation minivans, the NS-platform minivans abandoned the boxy body design of the previous two Chrysler minivan generations. While (on average) three inches taller than their predecessors, the new design lowered the exterior coefficient of drag from 0.39 to 0.35 (matching the Ford Windstar, bested only by the GM APV minivans). In the redesign, no body panels were carried over, with all three brands adopting new model badging.\n\nA central feature of the redesign (later becoming a standard feature on some trim levels) was the driver-side sliding door. While first used on compact minivans, including the Nissan Stanza Wagon and Nissan Axxess, the NS-platform minivans were the first mid-size minivans to adapt the feature. Though again using a 3-track mounting system for the sliding doors, the system was redesigned to consume less interior space; the exterior door track was hidden below the side windows. In addition to its styling, the function of the liftgate was changed for the first time, with an exterior handle available for the first time, eliminating the pop-and-lift maneuver of the two previous generations (adapted from the K-car station wagon). The exterior door handles were redesigned, switching from a flip-up design to a pull-out design.\n\nAlongside the exterior, major changes were made to the interior of the NS-platform minivans. Again offered with 7 passenger seating, the latching system for the 2nd and 3rd row seats was redesigned; in a single motion, owners could release 2nd and 3rd-row bench seats by floor latch, raising them onto rollers to move them out of the vehicle (the lighter 2nd-row bucket seats did not have them). For the first time, the seatbacks of front bucket seats folded forward. The mounting positions for the third-row seat were moved inboard, matching the second row, eliminating a separate set of seat mounts on the floor.\n\nWhile styled far differently from its 1995 predecessor, the dashboard of the NS-platform minivans adopted a similar layout to the second-generation minivans. For the first time since 1988, each Chrysler minivan shared the same steering wheel (among the only Chrysler Pentastars visible to the driver). While the glovebox was enlarged, the underseat storage drawer made its return as an option, complemented by a second locking storage compartment mounted to the side of the passenger seat. As a first for minivans, the NS minivans introduced dual-zone climate control (as an option) and adjustable cupholders.\n\nThe 1996–2000 Dodge Grand Caravan received a \"Marginal\" rating in the Insurance Institute for Highway Safety's 40 mph offset test. The structural performance and restraints were graded \"Acceptable\", but the foot injuries were very high.\n\nIn the NHTSA crash tests, it received 4 stars for the driver and front passenger in the frontal-impact. In the side-impact test, it received 5 stars for the driver, and 3 stars for the rear occupant, and resulted in a fuel leak that could cause a fire hazard.\n\nAccording to EuroNCAP crash test results, the 1999 model Chrysler Voyager did so badly in the frontal impact that it earned no points, making it the worst of the group. The body structure became unstable and the steering column was driven back into the driver's chest and head'. The 2007 model Chrysler Voyager fared little better, achieving just 19% in the frontal impact test, with an overall score of 2 stars out of a possible 5. However, chest compression measurements on the test dummy 'indicated an unacceptably high risk of serious or fatal injury. As a result, the final star in the adult occupant rating is struck-through'.\n\nDespite the bad results in the EuroNCAP crash tests, statistics from the real world indicate that this is not the whole picture. Folksam is a Swedish insurance company that in May 2009 published a report on injuries and survivability of 172 car models. The 88–96 generation got a real world rating of \"Average\", and the 96-00 generation got a rating called \"Safest\" (at least 30% safer than the average car.)\n\nIntroduced in March 1995 for the 1996 model year, the third-generation Chrysler minivans were marketed by the Chrysler, Dodge, and Plymouth divisions. In line with the second generation, Chrysler marketed the minivans as the Plymouth Voyager, Dodge Caravan, and Chrysler Town & Country in both standard wheelbase and long-wheelbase (\"Grand\") configurations. The Dodge Caravan C/V was withdrawn from the model line; all NS-platform vans were of passenger van configuration.\nAgain positioned as the flagship of the Chrysler minivans, the third-generation marked a major expansion of the Town & Country model range, from one model to three. For the first time, a short-wheelbase configuration was offered (LX for 1996, SX for 1997-1999). The previously-standard long-wheelbase configuration was offered in two trim levels (LX and LXi). The short-wheelbase LX and SX models replaced the Plymouth Voyager and Grand Voyager LX. In a shift from the simulated woodgrain trim of the previous generation, the Town & Country was distinguished by chrome exterior moldings. Unique features for the Town & Country include pre-programmed driver's seat and mirror, standard leather interior, 8-way power adjustable front seats, Infinity sound system with cassette/CD player.\n\nMarking the retirement of the Chrysler crystal Pentastar hood ornament, the Town & Country adopted the Chrysler medallion badge. As part of a 1998 mid-cycle exterior revision, the front fascia underwent a revision, replacing the waterfall grille with a black eggcrate grille and a winged version of the Chrysler ribboned badge.\n\nIn 1997, the driver-side sliding door became standard on the Town & Country (1996 examples produced without one are very rare). For 1999, the middle-row bench seat configuration was deleted in favor of rear bucket seats.\nOutside the United States and Canada, Chrysler exported the third-generation minivans to a number of global markets under the Chrysler (Grand) Voyager nameplate. Using the body and interior of the Dodge Caravan, the Voyager was produced by Saint Louis Assembly in Fenton, Missouri and by Eurostar in Graz, Austria. For the first time, right-hand drive configurations were produced, allowing wide sales of the Chrysler Voyager in Australia and the United Kingdom. Alongside the 3.0L, 3.3L, and 3.8L V6 engines of domestic-market minivans, the Chrysler Voyager was produced with 2.0L 4-cylinder engines shared with the Neon, along with a 2.5L diesel engine. Along with automatic transmissions, a 5-speed manual transmission was offered (the only third-generation Chrysler minivan produced with a manual transmission).\n\nIn Brazil, Chrysler minivans were offered under the Chrysler Caravan nameplate, using Dodge Caravan bodyshells.\nIn a break from its predecessors, the third-generation Dodge Caravan was marketed distinctly from the Plymouth Voyager. Replacing its role as the Dodge counterpart of the Voyager, the Caravan was remarketed to serve as the mid-range model of the Chrysler minivan product line. In contrast to the chrome trim moldings of the Chrysler Town & Country, base, SE and LE trims shared gray moldings with the Voyager with model-specific front bumpers; the ES trim and vans equipped with the Sport package featured body-colored moldings.\n\nAs with the AS-platform vans, 7-passenger seating was offered on the Dodge Caravan, regardless of body length. In 1999, the driver-side sliding door became an option on all versions of the Caravan.\n\nFor 1999, the Dodge Caravan ES was equipped with an Autostick gear selector for its automatic transmission, a first for a minivan. In contrast to previous uses (the Eagle Vision TSi and the Plymouth Prowler), the Caravan ES placed the system selector on the end of the column-mounted shifter.\nIn an effort to reposition the Plymouth division during the mid-1990s, Chrysler sought to focus the Plymouth brand on entry-level vehicles. In place of serving as a direct counterpart of the Dodge Caravan, the third-generation Plymouth Voyager became the entry-level Chrysler minivan. In a major change, for the first time, the Voyager and Caravan visually differed from one another. Along with a dark gray eggcrate grille (a body-colored grille became optional starting with model year 1998), the Voyager adopted matte gray bumpers across all trim levels along with matte gray side moldings. Voyagers featured unique front bumpers not shared with other models and did not come with fog lights in all trim levels.\n\n1996 marked the introduction of the Plymouth \"sailboat\" grille emblem; examples produced before calendar 1996 were produced with the Chrysler Pentastar, with latter vehicles bearing the sailboat emblem.\n\nA Rallye option package (carried from the previous generation) was available on the SE trim. For 1998, the Rallye trim was renamed Expresso.\n\nIn 1999, Dodge introduced the Caravan EPIC, a fully electric minivan. The EPIC was powered by 28 12-volt NiMH batteries and was capable of traveling up to on a single charge. The EPIC was sold as a fleet-only lease vehicle. Production of the EPIC was discontinued in 2001. Only a few hundred of these vehicles were produced and sold. After the leases expired they were returned and crushed. Approximately 10 vans remain in private hands today.\n\nIn 1999, coinciding with the 15th anniversary of Chrysler minivan production, Chrysler unveiled three concept minivans, largely intended to further advance the divisional styling and design of each vehicle.\n\nThe Chrysler Pacifica, derived from the Chrysler Town & Country, was given an exterior and interior restyling. Similar to a full-size conversion van, the Pacifica was given a raised roof for increased headroom, with a full-length skylight. In a 2+2+2 seating layout, the Pacifica was fitted with power-operated middle-row seats (with power-operated footrests). The exterior adapted design elements of the flagship Chrysler LHS sedan, including a (smaller) version of its grille and modified versions of its headlamps.From 2004 to 2008, the Chrysler Pacifica nameplate saw use on a production vehicle, as a CUV derived from the Grand Caravan/Town & Country. For 2017, the Pacifica nameplate was again revived, used on the current generation of Chrysler-division minivans.\n\nAs a follow-up to the 1996 Dodge Caravan ESS, the 1999 Dodge Caravan R/T replaced the 3.8L V6 with the 3.5L V6 of the Chrysler 300M, increasing output to 253 hp, paired with the 4-speed AutoStick transmission. The Dodge Caravan adapted several styling elements from the Dodge Viper RT/10, including hoodscoops (carried directly over from the Viper), and a redesigned lower fascia that shifted the Dodge \"crosshair\" grille into the lower bumper (with the addition of foglamps from the Viper). The interior featured a performance-oriented theme, with brushed aluminum trim for the trim and door panels, with black leather seats and racing-style pedals.From 2011 to 2016, Dodge produced the Grand Caravan R/T as an trim variant, though the option was largely cosmetic.\n\nDerived from the Plymouth Voyager, the Plymouth Voyager XG was a four-seat minivan geared towards active lifestyles of younger drivers. Externally similar to the standard Voyager, the XG was distinguished by its 17-inch wheels (shared with the Plymouth Prowler), and body-color bumpers and grille. The interior was given a full-length retractable fabric sunroof, with a large removable storage pod placed behind the seats.The Voyager XG featured the powertrain of the export-market Chrysler Voyager, with a 2.5L diesel engine and a 5-speed manual transmission.\n\nThe new minivans earned unanimous critical acclaim: the Dodge Caravan was the 1996 Motor Trend Car of the Year (the first and only minivan to ever win the award), and the vans were on \"Car and Driver\" magazine's Ten Best list for 1996 and 1997.\n"}
{"id": "17229545", "url": "https://en.wikipedia.org/wiki?curid=17229545", "title": "Cryogenic Rare Event Search with Superconducting Thermometers", "text": "Cryogenic Rare Event Search with Superconducting Thermometers\n\nThe Cryogenic Rare Event Search with Superconducting Thermometers (CRESST) is a collaboration of European experimental particle physics groups involved in the construction of cryogenic detectors for direct dark matter searches. The participating institutes are the Max Planck Institute for Physics (Munich), Technische Universität München, Universität Tübingen, University of Oxford (Great Britain) and the Istituto Nazionale di Fisica Nucleare (INFN, Italy).\n\nThe CRESST collaboration currently runs an array of cryogenic detectors in the underground laboratory of the Gran Sasso National Laboratory. The modular detectors used by CRESST facilitate discrimination of background radiation events by the simultaneous measurement of phonon and photon signals from scintillating calcium tungstate crystals. By cooling the detectors to temperatures of a few millikelvin, the excellent discrimination and energy resolution of the detectors allows identification of rare particle events.\n\nCRESST-I took data in 2000 using sapphire detectors with tungsten thermometers. CRESST-II uses CaWO crystal scintillating calorimeters. It was prototyped in 2004 and had a 47.9 kg-day commissioning run in 2007 and operated 2009 to 2011. CRESST-II Phase 1 experiment observed excess events above known background that could be understood to constitute a dark matter signal. However, later analysis showed that these excess events were due to a previously uncounted for excess of background from the detector itself and not a true signal from dark matter. The source of the excess backgound in the detector was removed for Phase 2.\n\nPhase 2 has a new CaWO crystal with better radiopurity, improved detectors, and significantly reduced background. It began July 2013 to explore excess signals in the prior run. The results of Phase 2 showed no signal above expected background, proving that the result of Phase 1 had indeed been due to excess background by components of the detector.\n\nCRESST-I first detected the alpha decay of tungsten. CRESST-II phase 1 full results were published in 2012. New phase 2 results have been presented on July 2014 with a limit on spin-independent WIMP-nucleon scattering for WIMP masses below 3 GeV/c.\n\nIn 2015 the CRESST detectors were upgraded by a sensitivity factor of 100 allowing dark matter particles with a mass around that of a proton to be detected.\n\nThe EURECA experiment is a planned successor to CRESST, ultimately aiming to run an array of detectors with a total mass of around 1 tonne.\n\n"}
{"id": "3950770", "url": "https://en.wikipedia.org/wiki?curid=3950770", "title": "Dihomo-γ-linolenic acid", "text": "Dihomo-γ-linolenic acid\n\nDihomo-γ-linolenic acid (DGLA) is a 20-carbon ω−6 fatty acid. In physiological literature, it is given the name 20:3 (ω−6). DGLA is a carboxylic acid with a 20-carbon chain and three \"cis\" double bonds; the first double bond is located at the sixth carbon from the omega end. DGLA is the elongation product of γ-linolenic acid (GLA; 18:3, ω−6). GLA, in turn, is a desaturation product (Delta 6 desaturase) of linoleic acid (18:2, ω−6). DGLA is made in the body by the elongation of GLA, by an efficient enzyme which does not appear to suffer any form of (dietary) inhibition. DGLA is an extremely uncommon fatty acid, found only in trace amounts in animal products. DGLA production from GLA is enhanced when high levels of alpha-linolenic acid are present, blocking the arachidonic acid pathway.\n\nThe eicosanoid metabolites of DGLA are:\n\nAll of these effects are anti-inflammatory. This is in marked contrast with the analogous metabolites of arachidonic acid (AA), which are the series-2 thromboxanes and prostanoids and the series-4 leukotrienes. In addition to yielding anti-inflammatory eicosanoids, DGLA competes with AA for COX and lipoxygenase, inhibiting the production of AA's eicosanoids.\n\nTaken orally in a small study, DGLA produced antithrombotic effects. Supplementing dietary GLA increases serum DGLA, as well as serum AA levels. Cosupplementation with GLA and EPA lowers serum AA levels by blocking Δ-5-desaturase activity, while also lowering leukotriene synthesis in neutrophils.\n\n"}
{"id": "35283814", "url": "https://en.wikipedia.org/wiki?curid=35283814", "title": "Elastic and plastic strain", "text": "Elastic and plastic strain\n\nInternal strain within a metal is either elastic or plastic. In the case of elastic strain this is observed as a distortion of the crystal lattice, in the case of plastic strain this is observed by the presence of dislocations –the displacement of part of the crystal lattice. Such strain effects can result in unwanted cracking of the material, as is the case with residual plastic strain. In other cases deliberate introduction of plastic strain results in a strengthening of the material and other performance enhancing behaviors, for example in the manufacture of semi-conductors and solar cells.\n\nAs an illustration, if you hang a weight on a spring it extends in direct proportion to the load. That is the same effect that occurs in the elastic deformation part of the standard tensile test.\n\nThis is normally written as: applied stress = Young’s modulus * strain\n\nThat is: σ = Y * e\n\nCommonly known as: Hooke’s Law.\n\nWhere stress is a force ( a vector property) divided by the cross sectional area. Strain is the displacement (also a vector quantity) per unit length and is also a vector quantity. A vector quantity requires three numbers to define its direction. It can be represented then by a number with one subscript, Ui, where i takes the numbers 1 to 3. Such a quantity can be referred to as a tensor of rank 1. The rank refers to the number of subscripts. The strains e can be formally written as a differential or gradient. So that, if u1 was the extension in the x direction, we would write the strain as δu1/δx. \nWe also know that when we stretch something it gets thinner in the direction normal to the stretch direction. This is the Poisson effect and the ratio extension /contraction is the Poisson ratio.\n\nThis coupling of the different strains caused in a sample by the application of a stress classifies strain as a tensor property. In fact it is a second rank tensor because it consists of a 3 x 3 matrix and hence each element will need two suffixes to locate any one of its components in the matrix, i.e. eij. The physical significance of this will be apparent later.\n\nWe set up a co-ordinate system based on some reference axes in the sample. A convenient one for the case illustrated above is included in the figure. It is a right handed system when reading the axes in the sequence x y z. If we start at a point on the positive x axis and rotate towards the positive y axis in a clockwise direction then positive z axis would point in the direction of movement of a right handed screw.\n\nTo keep it as clear as possible which reference axes we are using it is conventional often to refer to these axes as 1 2 and 3. This is to allow easy identification of the strain terms as depicted in the tenor 3x3 matrix. The directions 1 2 and 3 are synonymous with the x y and z directions.\n\nIn a sample strained in tension (uni-axial tension) as in the above figure, the z axis has been strained in the positive sense, i.e. it becomes longer. The strain term for this deformation is e33. The applied force is acting in the z direction. The first subscript refers to the axis that is being strained and the second to the direction it is being strained. Thus the first subscript 3 refers to the strain is occurring along the z axis and the second subscript to the fact that the strain is in the direction of the z axis. Likewise for the strains e11 and e 22. The strains e 11, e 22 and e 33 are known as tensile or normal strains.\nThe force along z can also be resolved onto inclined planes causing these planes to shear resulting in shear strains. It will also cause the crystal to rotate (called a rigid body rotation). To illustrate this I include a more general deformation, as shown next.\n\nIllustration of tensile and shear strains plus rotation in x z plane.\nx y z right handed axis system, u1 u2 u3 displacements in x y z directions.\nThe figure shows only deformation, that is change of length and direction, in the x z plane for clarity. The tensile strains e11 and e 33 are shown plus the two strains e 13 and e 31. Note the differential form of these latter strains as shown in the figure. Let us consider for now that there is no rotation. Then e 13 and e 31 represent the shear components alone and are in fact equal. The first of the subscript in both of e 13 and e31 signifies the plane that is sheared and the second component the axis defining the direction of shear. Hence the first shear component e13 describes the shear of the plane normal to the x axis and in the z direction whilst the second, e31 defines the shear of the plane normal to the z axis in the x direction. \nSimple (mechanical) shear versus physics\nNow some of you may say, ‘surely shear of a plane distorts a crystal’ as shown in the left sketch below.\n\nSimple shear. (Engineering) Shear (Physics)\nWell this is true and is how engineers express it. As shown in the right hand figure, the shear strain as used in engineering is twice that used by physicists. Engineering strain measures the total strain in the xz plane and is often given the symbol γ. On the other hand, e31, which remember = δu3/δx and in this case where there is no rotation, is simply the average of the strains on the z and x faces i.e. ε31 = ½(e31+e13). Note I have changed the symbol from e31 to ε31. The symbol e31 refers to the strains as measured and may contain a rotational component. The symbol ε31 refers specifically to the shear strain component included in the e31 term. You may need to note this when involved in discussion with engineers. For consistency the Greek symbol ε is used instead of e for the tensile strains as well.\nTensors in Strain\nTensors: Geometric entities introduced into mathematics and physics to extend the notion of scalars, (geometric) vectors, and matrices. Many physical quantities are naturally regarded not as vectors themselves, but as correspondences between one set of vectors and another.\n\nThere is a wide variety of strain measurement techniques, depending on the resolution and precision required. Larger scale methods include the use of strain gauges. For higher precision Raman Spectroscopy and recently emerging techniques such as EBSD are used.\nTraditional measurement of residual strain in metals is through x-ray diffraction. Unfortunately this has major shortcomings in today’s applications and material’s development environment:\nResult: Diffraction volume is too large and individual grains (crystal lattices) cannot be observed or measured accurately.\n\nThe advanced application of the alternative diffraction technique of Electron Backscatter Diffraction (EBSD) overcomes these limitations. Software applications such as CrossCourt3 have enabled practical measurement at the nanoscale quite possible.\n\nThe technique of EBSD produces patterns from the metal’s crystallography that consist of a well-defined geometric arrangement of bright bands edged by sharp lines. These are known as Kikuchi bands and Kikuchi lines respectively. Elastic strain changes the width of these bands, and the angles between them plus a general rotation of the entire pattern. The total result is a distortion that involves dilation of the crystal lattice (or unit cell) as a result of tensile and compressive strains, shear strains and rotation. A complete description of internal elastic strain requires that all these crystallographic parameters be measured. This is a benefit of advanced or high accuracy EBSD.\n\n"}
{"id": "29763411", "url": "https://en.wikipedia.org/wiki?curid=29763411", "title": "Ellipse Law", "text": "Ellipse Law\n\nThe Law of the Ellipse, or Stodola's cone law, provides a method for calculating the highly nonlinear dependence of extraction pressures with a flow for multistage turbine with high backpressure, when the turbine nozzles are not choked. It is important in turbine off-design calculations.\n\nWe consider a multistage turbine, like in the picture. The design calculation is done for design flow rate (formula_1, the flow expected for the most uptime). The other parameters for design are the temperature and pressure at the stage group intake, formula_2 and formula_3, respectively the extraction pressure at the stage group outlet formula_4 (the symbol formula_5 is used for the pressure after a stage nozzles, pressure does not interfere in relations here).\n\nFor off-design calculations, the off-design flow rate is formula_6, respectively the temperature and pressure at the stage group intake are formula_7 and formula_8 and the outlet pressure is formula_9.\n\nStodola established experimentally that the relationship between these three parameters represented in Cartesian coordinate system has the shape of a degenerate quadric surface, the cone directrix being an ellipse. For a constant initial pressure formula_8 the flow rate depends on the outlet pressure formula_9 as an arc of ellipse in a plane parallel to formula_12\n\nFor very low outlet pressure formula_9, like for condensing turbines, flow rates do not change with the outlet pressure, but drops very quickly with the increase of the backpressure. For a given outlet pressure formula_9, flow rates changes depending on the inlet pressure formula_8 as an arc of hyperbola in a plane parallel to formula_16.\n\nUsually, Stodola's cone do not represent absolute flow rates and pressures, but relative to the maximum flow rate and pressures, the maximum values of the diagram having in this case the value of 1. The maximum flow rate has the symbol formula_17 and the maximum pressures at the inlet and outlet have the symbols formula_18 and formula_19. The pressure ratios for design flow rate at the intake and outlet are formula_20 and formula_21, and the off-design ratios are formula_22 and formula_23.\n\nIf the speed of sound is reached in a stage, the group of stages can be analyzed till that stage, which is the last in the group, the remaining stages forming another group of analysis. This division is imposed by the stage working in \"limited\" (choked) mode. The cone is shifted in the formula_24 axis direction, appearing a triangular surface, depending on the critical pressure ratio formula_25, where formula_26 is the outlet critical pressure of the stage group.\n\nThe analytical expression of the flow ratio is:\n\nFor condensing turbine the ratio formula_28 is very low, previous relation reduces to:\nsimplified relationship obtained theoretically by Gustav Flügel (1885–1967).\n\nIn the event that the variation of inlet temperature is low, the relationship is simplified:\nFor condensing turbines formula_31, so in this case:\n\nDuring operation, the above relations allow the assessment of the flow rate depending on the operating pressure of a stage.\n\n\n\n"}
{"id": "28672294", "url": "https://en.wikipedia.org/wiki?curid=28672294", "title": "Energy management", "text": "Energy management\n\nEnergy management includes planning and operation of energy production and energy consumption units. Objectives are resource conservation, climate protection and cost savings, while the users have permanent access to the energy they need. It is connected closely to environmental management, production management, logistics and other established business functions. The VDI-Guideline 4602 released a definition which includes the economic dimension: “Energy management is the proactive, organized and systematic coordination of procurement, conversion, distribution and use of energy to meet the requirements, taking into account environmental and economic objectives”.\n\nOne of initial steps for an effective energy cost control program is the base line energy assessment, which examines the pattern of existing energy usage by the government or any sub-entity of the government or private organization. This program will set the reference point for improvements in energy efficiency. Energy efficiency can improve the existing energy usage and benchmarking of every individual section such as area, sub-area and the industry etc .\n\nIt is important to integrate the energy management in the organizational structure, so that the energy management can be implemented. Responsibilities and the interaction of the decision makers should be regularized. The delegation of functions and competencies extend from the top management to the executive worker. Furthermore, a comprehensive coordination can ensure the fulfillment of the tasks.\n\nIt is advisable to establish a separate organizational unit “energy management” in large or energy-intensive companies. This unit supports the senior management and keeps track. It depends on the basic form of the organizational structure, where this unit is connected. In case of a functional organization the unit is located directly between the first (CEO) and the second hierarchical level (corporate functions such as production, procurement, marketing). In a divisional organization, there should be a central and several sector-specific energy management units. So the diverse needs of the individual sectors and the coordination between the branches and the head office can be fulfilled. In a matrix organization the energy management can be included as a matrix function and thus approach most functions directly.\n\nFacility management is an important part of energy management, because a huge proportion (average 25 per cent) of complete operating costs are energy costs. According to the International Facility Management Association (IFMA), facility management is \"a profession that encompasses multiple disciplines to ensure functionality of the built environment by integrating people, place, processes and technology.\"\n\nThe central task of energy management is to reduce costs for the provision of energy in buildings and facilities without compromising work processes. Especially the availability and service life of the equipment and the ease of use should remain the same. The German Facility Management Association (GEFMA e.V.) has published guidelines (e.g. GEFMA 124-1 and 124-2), which contain methods and ways of dealing with the integration of energy management in the context of a successful facility management. In this topic the facility manager has to deal with economic, ecological, risk-based and quality-based targets. He tries to minimize the total cost of the energy-related processes (supply, distribution and use).\n\nThe most important key figure in this context is kilowatt-hours per square meter per year (kWh/m²a). Based on this key figure properties can be classified according to their energy consumption.\n\n\nIn comparison, the Passive house (Passivhaus in German) ultra-low-energy standard, currently undergoing adoption in some other European countries, has a maximum space heating requirement of 15 kWh/m²a. A Passive House is a very well-insulated and virtually air-tight building. It does not require a conventional heating system. It is heated by solar gain and internal gains from people. Energy losses are minimized.\n\nThere are also buildings that produce more energy (for example by solar water heating or photovoltaic systems) over the course of a year than it imports from external sources. These buildings are called energy-plus-houses.\n\nIn addition, the work regulations manage competencies, roles and responsibilities. Because the systems also include risk factors (e.g., oil tanks, gas lines), you must ensure that all tasks are clearly described and distributed. A clear regulation can help to avoid liability risks.\n\nLogistics is the management of the flow of resources between the point of origin and the point of destination in order to meet some requirements, for example of customers or corporations. Especially the core logistics task, transportation of the goods, can save costs and protect the environment through efficient energy management. The relevant factors are the choice of means of transportation, duration and length of transportation and cooperation with logistics service providers.\n\nThe logistics causes more than 14% percent of CO2 emissions worldwide. For this reason the term Green Logistics is becoming increasingly important.\n\nPossible courses of action in terms of green logistics are:\n\n\nBesides transportation of goods, the transport of persons should be an important part of the logistic strategy of organizations. In case of business trips it is important to attract attention to the choice and the proportionality of the means of transport. It should be balanced whether a physical presence is mandatory or a telephone or video conference is just as useful. Home Office is another possibility in which the company can protect the environment indirectly.\n\nProcurement is the acquisition of goods or services. Energy prices fluctuate constantly, which can significantly affect the energy bill of organizations. Therefore poor energy procurement decisions can be expensive. Organizations can control and reduce energy costs by taking a proactive and efficient approach to buying energy. Even a change of the energy source can be a profitable and eco-friendly alternative.\n\nProduction is the act of creating output, a good or service which has value and contributes to the utility of individuals. This central process may differ depending on the industry. Industrial companies have facilities that require a lot of energy. Service companies, in turn, do not need many materials, their energy-related focus is mainly facility management or Green IT. Therefore the energy-related focus has to be identified first, then evaluated and optimize.\n\nUsually, production is the area with the largest energy consumption within an organization. Therefore also the production planning and control becomes very important. It deals with the operational, temporal, quantitative and spatial planning, control and management of all processes that are necessary in the production of goods and commodities. The \"production planner\" should plan the production processes so that they operate in an energy efficient way. For example, strong power consumer can be moved into the night time. Peaks should be avoided for the benefit of a unified load profile.\n\nThe impending changes in the structure of energy production require an increasing demand for storage capacity. The Production planning and control has to deal with the problem of limited storability of energy. In principle there is the possibility to store energy electrically, mechanically or chemically. Another trend-setting technology is lithium-based electrochemical storage, which can be used in electric vehicles or as an option to control the power grid. The German Federal Ministry of Economics and Technology realized the significance of this topic and established an initiative with the aim to promote technological breakthroughs and support the rapid introduction of new energy storage.\n\nMaintenance is the combination of all technical and administrative actions, including supervision actions, intended to retain an item in, or restore it to, a state in which it can perform a required function. Detailed maintenance is essential to support the energy management. Hereby power losses and cost increases can be avoided.\n\nThrough the energy efficiency it management is remain the key for the any industrial user across globe , to achieve the energy management goal for the federal government or industry the efficiency of water and energy resources play a vital role\n\nExamples of how it is possible to save energy and costs with the help of maintenance:\n\n\nA long-term energy strategy should be part of the overall strategy of a company. This strategy may include the objective of increasing the use of renewable energies. Furthermore, criteria for decisions on energy investments, such as yield expectations, are determined. By formulating an energy strategy companies have the opportunity to avoid risks and to assure a competitive advance against their business rivals.\n\nAccording to Kals there are the following energy strategies:\n\n\nIn reality, you usually find hybrid forms of different strategies.\n\nMany companies are trying to promote its image and time protect the climate through a proactive and public energy strategy. General Motors (GM) strategy is based on continuous improvement. Furthermore they have six principles: e.g. restoring and preserving the environment, reducing waste and pollutants, educating the public about environmental conservation, collaboration for the development of environmental laws and regulations.\n\nNokia created its first climate strategy in 2006. The strategy tries to evaluate the energy consumption and greenhouse gas emissions of products and operations and sets reduction targets accordingly. Furthermore, their environmental efforts is based on four key issues: substance management, energy efficiency, recycling, promoting environmental sustainability.\n\nThe energy strategy of Volkswagen (VW) is based on environmentally friendly products and a resource-efficient production according to the \"Group Strategy 2018\". Almost all locations of the Group are certified to the international standard ISO 14001 for environmental management systems.\n\nWhen looking at the energy strategies of companies it is important to you have the topic greenwashing in mind. This is a form of propaganda in which green strategies are used to promote the opinion that an organization's aims are environmentally friendly.\n\nEven many countries formulate energy strategies. The Swiss Federal Council decided in May 2011 to resign nuclear energy medium-dated. The nuclear power plants will be shut down at the end of life and will not be replaced. In Compensation they put the focus on energy efficiency, renewable energies, fossil energy sources and the development of water power.\n\nThe European Union has clear instructions for its members. The \"20-20-20-targets\" include, that the Member States have to reduce greenhouse gas emissions by 20% below 1990 levels, increase energy efficiency by 20% and achieve a 20% share of renewable energy in total energy consumption by 2020.\n\nThe basis of every energy strategy is the corporate culture and the related ethical standards applying in the company. Ethics, in the sense of business ethics, examines ethical principles and moral or ethical issues that arise in a business environment. Ethical standards can appear in company guidelines, energy and environmental policies or other documents.\n\nThe most relevant ethical ideas for the energy management are:\n\n\n\nManagement of energy in a particular context:\n"}
{"id": "39848086", "url": "https://en.wikipedia.org/wiki?curid=39848086", "title": "Fiber optic current sensor", "text": "Fiber optic current sensor\n\nA fiber optic current sensor (FOCS) is a current sensor for measuring direct current. By using a single-ended optical fiber around the current conductor that utilizes the magneto-optic effect (Faraday effect), FOCS measures uni- or bidirectional DC currents of up to 600 kA within ±0.1% of the measured value.\n\nBecause it does not need a magnetic yoke, an FOCS is smaller and lighter than Hall effect current sensors, and suffers no reduction in accuracy due to saturation effects. Because magnetic field sensing is distributed around circumference, it is unaffected by stray magnetic fields, and there is no need for magnetic centering. It also does not need recalibration after installation or during its service life. Because the optical fiber is inherently insulating, electrical isolation is easier to maintain.\n\nThe optical phase detection circuit, light source and digital signal processor are contained within the sensor electronics; this technology has been proven in highly demanding applications such as navigation systems in the air, on land and at sea.\n\nIn 2013, ABB launched a 420 kV Disconnecting Circuit Breaker (DCB) with integrated FOCS. Since one FOCS replaces many conventional current transformers, engineering and design of the substation is simplified. By reducing the material needed, including insulation, a 420 kV DCB with integrated FOCS can reduce a substation’s footprint by over 50% compared to the conventional solution of live tank breakers with disconnectors and current transformers.\n"}
{"id": "49080", "url": "https://en.wikipedia.org/wiki?curid=49080", "title": "Georg Ohm", "text": "Georg Ohm\n\nGeorg Simon Ohm (; ; 16 March 1789 – 6 July 1854) was a German physicist and mathematician. As a school teacher, Ohm began his research with the new electrochemical cell, invented by Italian scientist Alessandro Volta. Using equipment of his own creation, Ohm found that there is a direct proportionality between the potential difference (voltage) applied across a conductor and the resultant electric current. This relationship is known as Ohm's law.\n\nGeorg Simon Ohm was born into a Protestant family in Erlangen, Brandenburg-Bayreuth (then a part of the Holy Roman Empire), son to Johann Wolfgang Ohm, a locksmith and Maria Elizabeth Beck, the daughter of a tailor in Erlangen. Although his parents had not been formally educated, Ohm's father was a respected man who had educated himself to a high level and was able to give his sons an excellent education through his own teachings. Of the seven children of the family only three survived to adulthood: Georg Simon, his younger brother Martin, who later became a well-known mathematician, and his sister Elizabeth Barbara. His mother died when he was ten.\n\nFrom early childhood, Georg and Martin were taught by their father who brought them to a high standard in mathematics, physics, chemistry and philosophy. Georg Simon attended Erlangen Gymnasium from age eleven to fifteen where he received little in the area of scientific training, which sharply contrasted with the inspired instruction that both Georg and Martin received from their father. This characteristic made the Ohms bear a resemblance to the Bernoulli family, as noted by Karl Christian von Langsdorf, a professor at the University of Erlangen.\n\nGeorg Ohm's father, concerned that his son was wasting his educational opportunity, sent Ohm to Switzerland. There in September 1806 Ohm accepted a position as a mathematics teacher in a school in Gottstadt bei Nidau.\n\nKarl Christian von Langsdorf left the University of Erlangen in early 1809 to take up a post in the University of Heidelberg. Ohm wanted to restart his mathematical studies with Langsdorf in Heidelberg. Langsdorf, however, advised Ohm to pursue mathematical studies on his own, and suggested that Ohm read works of Euler, Laplace and Lacroix. Rather reluctantly Ohm took his advice but he left his teaching post in Gottstatt Monastery in March 1809 to become a private tutor in Neuchâtel. For two years he carried out his duties as a tutor while he followed Langsdorf's advice and continued his private study of mathematics. Then in April 1811 he returned to the University of Erlangen.\n\nOhm's own studies prepared him for his doctorate which he received from the University of Erlangen on October 25, 1811. He immediately joined the faculty there as a lecturer in mathematics but left after three semesters because of unpromising prospects. He could not survive on his salary as a lecturer. The Bavarian government offered him a post as a teacher of mathematics and physics at a poor quality school in Bamberg which Ohm accepted in January 1813. Unhappy with his job, Georg began writing an elementary textbook on geometry as a way to prove his abilities. That school was closed in February 1816. The Bavarian government then sent Ohm to an overcrowded school in Bamberg to help out with the teaching of mathematics.\nAfter his assignment in Bamberg, Ohm sent his completed manuscript to King Wilhelm III of Prussia. The King was satisfied with Ohm's book, and offered Ohm a position at the Jesuit Gymnasium of Cologne on 11 September 1817. This school had a reputation for good science education and Ohm was required to teach physics in addition to mathematics. The physics laboratory was well equipped, allowing Ohm to begin experiments in physics. As the son of a locksmith, Ohm had some practical experience with mechanical devices.\n\nOhm published \"Die galvanische Kette, mathematisch bearbeitet\" (\"The Galvanic Circuit Investigated Mathematically\") in 1827. Ohm's college did not appreciate his work and Ohm resigned from his position. He then made an application to, and was employed by, the Polytechnic School of Nuremberg. Ohm arrived at the Polytechnic School of Nuremberg in 1833, and in 1852 he became a professor of experimental physics at the University of Munich.\n\nIn 1849, Ohm published \"Beiträge zur Molecular-Physik\", (in English: Molecular Physics). In the preface of this work he stated he hoped to write a second and third volume \"and if God gives me length of days for it, a fourth\". However, on finding that an original discovery recorded in it was being anticipated by a Swedish scientist he did not publish it, stating: \"The episode has given a fresh and deep sense for my mind to the saying 'Man proposes, and God disposes'. The project that gave the first impetus to my inquiry has been dissipated into mist, and a new one, undesigned by me, has been accomplished in its place.\"\n\nOhm died in Munich in 1854, and is buried in the Alter Südfriedhof. A collection of his family letters would be compiled in a German book, which shows that he used to sign some of his letters with the expression \"Gott befohlen, G S Ohm,\" meaning \"Commended to God\".\n\nOhm's law first appeared in the famous book \"Die galvanische Kette, mathematisch bearbeitet\" (tr., \"The Galvanic Circuit Investigated Mathematically\") (1827) in which he gave his complete theory of electricity. In this work, he stated his law for electromotive force acting between the extremities of any part of a circuit is the product of the strength of the current, and the resistance of that part of the circuit.\n\nThe book begins with the mathematical background necessary for an understanding of the rest of the work. While his work greatly influenced the theory and applications of current electricity, it was coldly received at that time. Ohm presents his theory as one of contiguous action, a theory which opposed the concept of action at a distance. Ohm believed that the communication of electricity occurred between \"contiguous particles\" which is the term he himself used. The paper is concerned with this idea, and in particular with illustrating the differences in this scientific approach of Ohm's and the approaches of Joseph Fourier and Claude-Louis Navier.\n\nA study of the conceptual framework used by Ohm in producing Ohm's law has been presented by Archibald. The work of Ohm marked the early beginning of the subject of circuit theory, although this did not become an important field until the end of the century.\n\nOhm's acoustic law, sometimes called the acoustic phase law or simply Ohm's law, states that a musical sound is perceived by the ear as a set of a number of constituent pure harmonic tones. It is well known to be not quite true.\n\nHis writings were numerous. With his first paper in 1825, Ohm looks into the decrease in the electromagnetic force produced by a wire as the length of the wire increased. In 1826, he gave a description of conduction in circuits modeled on Fourier's study of heat conduction. This paper continue Ohm's deduction of results from experimental evidence and, particularly in the second, he was able to propose laws which went a long way to explaining results of others working on galvanic electricity. The most important was his pamphlet published in Berlin in 1827, with the title \"Die galvanische Kette mathematisch bearbeitet\". This work, the germ of which had appeared during the two preceding years in the journals of Schweigger and Poggendorff, has exerted an important influence on the development of the theory and applications of electric current. Ohm's name has been incorporated in the terminology of electrical science in Ohm's Law (which he first published in \"Die galvanische Kette\"...), the proportionality of current and voltage in a resistor, and adopted as the SI unit of resistance, the ohm (symbol Ω).\n\nAlthough Ohm's work strongly influenced theory, at first it was received with little enthusiasm. However, his work was eventually recognized by the Royal Society with its award of the Copley Medal in 1841. He became a foreign member of the Royal Society in 1842, and in 1845 he became a full member of the Bavarian Academy of Sciences and Humanities. At some extent, Charles Wheatstone drew attention to the definitions which Ohm had introduced in the field of physics.\n\n\n\n"}
{"id": "10064702", "url": "https://en.wikipedia.org/wiki?curid=10064702", "title": "Hot zone (environment)", "text": "Hot zone (environment)\n\nHot zone, also written as hot-zone or hotzone, refers to an area that is considered to be dangerous. It generally entails special equipment to protect occupants, because there is a high risk of infection.\n\nThe term \"hot zone\" was likely coined during the Cold War where it described locations rendered hazardous due to nuclear contamination. The term was later extended to areas or locations considered to be hazardous such as Level-4 biosafety labs, places in which there is active conflict, and so forth.\n\nThe term \"hot zone\" was popularized by the book \"The Hot Zone\" by Richard Preston, and the film \"Outbreak\".\n\nHazardous biological agents can induce a hot zone, as occupants are in danger of becoming infected. These biological contaminants can include many things. Some common examples would be bacteria, viruses, and fungi. Many times these are used as biological warfare, or biowarfare, agents with the intent to harm.\n\nPrecautions are taken in a gradient level of protection.\n\nIn 2009, the outbreak of swine influenza happened in most places of the whole world. The swine influenza originated from a Mexican woman, and it was transmitted from person to person by air with a rapid speed. North America and Mexico were the first places to be affected by the virus. According to the World Health Organization (WHO), there were 14,142 deaths of swine influenza, and 1,311,522 people had this virus in their bodies in 2009. Spain, China, the United States, and South Korea are considered to be hot zones of swine influenza. There were 155,051 people who had this virus in Spain, 120,498 in China, 107,939 in United State and 101,182 in South Korea. The people in those areas can very easily get \nthis virus. The virus is transmittable through the air, such as through coughing, sneezing or touching something containing the virus. Epidemics of acute respiratory disease of pigs are caused by the virus disease, swine influenza. Swine influenza belongs to the type A of the family Orthomyxoviridae. The patients with swine influenza virus have symptoms consisting of headaches, chills, fatigue, fever, cough and sore throat.\n\nIn Peru deforestation is leading to the distribution of Malaria. With the forests being destroyed the wildlife is as well, leaving malaria looking for a new host. Malaria is a very dangerous disease, so much so that vaccinations are required for traveling into places suspected of housing the disease. Malaria is a disease that is found predominantly in third world, low-income countries. Malaria along with other insect diseases are learning to adapt to life in the city. In Peru a port city called Iquitos the population has been growing in the past 10 years making it prime for mosquitoes to flourish. These mosquitoes also bring in Dengue Fever, in which 5 percent of its victims will die.\n\nWith an increased world population in the early 21st century water borne diseases have become the most pressing hot zone. With a lot of the worlds population moving into the city it is hard separating sanitation and clean water. In the early 90's a cholera epidemic broke out in a fishing village in Lima, Peru. Many thought that this was coming from the seafood, but it was really from the water the seafood was cleaned with. Cholera starts when infected human waste seeps into the water supply of a community. Not having a clean water supply is something that affects a third world country the most,though there are cases of poor water in the States. In Maryland's Chesapeake Bay fisherman have seen a decline in their catch over the last couple of years. A few years ago fisherman of Maryland's Chesapeake Bay area started noticing lesions on the crabs and fish they caught and soon the fisherman were sick themselves. Industrial waste, sewage and pesticides have slowly sunk into the Chesapeake Bay over the past decade.\n\nHazardous chemicals can induce a hot zone, as occupants are in danger of it disrupting their biological processes. For example, if while driving down a highway, an oil tanker gets into a car accident and spills its load on the pavement and surrounding grass areas, those areas affected will be considered hot zones. To correct these incidents, fire departments will deploy their HAZMAT, or hazardous materials, teams to reder the area safe once again. These types of spills can be harmful to people and to the environment because they are not meant to be there and therefore contaminate the land.\n\nRadioactive contamination can create a hot zone, as occupants are in danger of being exposed to radiation.\n\nIn March 2011, a 9.0-magnitude earthquake and its accompanying tsunami struck a nuclear power station in Fukushima Daiichi area of north-eastern Japan. A number of safety systems were badly \ndamaged by the tsunami leading to a loss-of-coolant (LOCA) event which damaged the nuclear core of several reactors. The Nuclear and Industrial Safety Agency (NISA) announced that the subsequent release of radioactivity into atmosphere qualified as the highest level of radiological event scale, INES level 7. The radioactive materials released in Fukushima Daiichi area are mostly iodine-131 and cesium-137.\n\nThe Nuclear and Industrial Safety Agency estimated the cancer consequence of the Fukushima Daiichi accident. From the government statistics, around the two million people who live within 80 kilometre radius of the nuclear plate, and about one million people live in areas contaminated with cesium-137.\n\nThe loss of coolant further caused hydrogen explosions in the facility. As the fuel temperature went up, zirconium alloy cladding reacted with the hot steam removing oxygen from water molecule, leaving hydrogen gas. The hydrogen gas was ultimately vented off into the reactor building, because of the design of the facility, mixing with air and creating an explosive environment.\n\nDiseases are estimated that thyroid cancer is the main cancer which affected by the nuclear accident. High amount of radioactive iodine mainly causes thyroid cancer, and most of the cases are the result of releasing iodine-131. If people consume food and water contaminated by iodine-131, iodine-131 (which has a half-life of 8 days) concentrates in the thyroid. The most contaminated food and drink are raw milk and vegetables in Fukushima Daiichi area. Milk production was blocked after six days of the nuclear explosion.\n\nNuclear accidents are very serious matters. As you can see from the above statements, they can cause massive panic, disease, and not to mention the fact that humans and other organisms may not be able to inhabit the affected area for many years to come. A perfect example of this is the nuclear accident in Chernobyl, Ukraine. Chernobyl is near Pripyat, Ukraine and also the country of Belarus. Chernobyl is now a ghost town. They had a malfunction with their nuclear power plant, and now there is still a hot zone there. This hot zone actually has a name, the Chernobyl Exclusion Zone.\n\nViolence can induce a hot zone, as occupants are subject to attacks, crossfire, or even direct fire targeted at them specifically. The most identifiable violent hot zones are in war zones, such as the war in Afghanistan. Soldiers are constantly fighting with other soldiers and insurgents to attempt to accomplish tactical goals. Hot zones are not good places to appear as a member of either of the opposing teams, because one may be prone to attacks or capture. Although war zones may have the most targeted attacks, crime ridden neighborhoods are the most common areas for crossfire to occur, and subsequently the most victims of crossfire are here.\n\n\nHot Zone Forensics: Chemical, Biological and Radiological Evidence Collection, by: Steven C. Drielak (Publisher: Charles C. Thomas June 2004)\n"}
{"id": "34906562", "url": "https://en.wikipedia.org/wiki?curid=34906562", "title": "Huanghe Hydropower Golmud Solar Park", "text": "Huanghe Hydropower Golmud Solar Park\n\nHuanghe Hydropower's Golmud Solar park is a 200 megawatt (MW) photovoltaic power station located in Golmud, Qinghai Province, China. Construction began in August 2009, and it was commissioned on October 29, 2011. 80 MW was provided by Yingli. The project won the 2012 China Quality Power Project Award. Output is expected to be 317 GWh per year.\n\nAlso in Golmud is the 20 MW Qinghai Golmud Solar Park completed in 2011 by Longyuan Power, as well as others. There are a total of 570 MW of solar parks in Golmud, many of which are located in the Golmud Desert Cluster, with 500 MW more expected in 2012.\n\n\n"}
{"id": "37423368", "url": "https://en.wikipedia.org/wiki?curid=37423368", "title": "Hurricane Sandy", "text": "Hurricane Sandy\n\nHurricane Sandy (unofficially referred to as Superstorm Sandy) was the deadliest and most destructive hurricane of the 2012 Atlantic hurricane season. Inflicting nearly $70 billion (2012 USD) in damage, it was the second-costliest hurricane on record in the United States until surpassed by Hurricanes Harvey and Maria in 2017. The eighteenth named storm, tenth hurricane, and second major hurricane of the year, Sandy was a Category 3 storm at its peak intensity when it made landfall in Cuba. While it was a Category 2 hurricane off the coast of the Northeastern United States, the storm became the largest Atlantic hurricane on record (as measured by diameter, with tropical-storm-force winds spanning ). At least 233 people were killed along the path of the storm in eight countries.\n\nSandy developed from a tropical wave in the western Caribbean Sea on October 22, quickly strengthened, and was upgraded to Tropical Storm Sandy six hours later. Sandy moved slowly northward toward the Greater Antilles and gradually intensified. On October 24, Sandy became a hurricane, made landfall near Kingston, Jamaica, re-emerged a few hours later into the Caribbean Sea and strengthened into a Category 2 hurricane. On October 25, Sandy hit Cuba as a Category 3 hurricane, then weakened to a Category 1 hurricane. Early on October 26, Sandy moved through the Bahamas. On October 27, Sandy briefly weakened to a tropical storm and then restrengthened to a Category 1 hurricane. Early on October 29, Sandy curved west-northwest (the \"left turn\" or \"left hook\") and then moved ashore near Brigantine, New Jersey, just to the northeast of Atlantic City, as a post-tropical cyclone with hurricane-force winds.\n\nIn Jamaica, winds left 70% of residents without electricity, blew roofs off buildings, killed one person, and caused about $100 million (2012 USD) in damage. Sandy's outer bands brought flooding to Haiti, killing at least 54, causing food shortages, and leaving about 200,000 homeless; the hurricane also caused two deaths in the Dominican Republic. In Puerto Rico, one man was swept away by a swollen river. In Cuba, there was extensive coastal flooding and wind damage inland, destroying some 15,000 homes, killing 11, and causing $2 billion (2012 USD) in damage. Sandy caused two deaths and an estimated $700 million (2012 USD) in damage in The Bahamas.\n\nIn the United States, Hurricane Sandy affected 24 states, including the entire eastern seaboard from Florida to Maine and west across the Appalachian Mountains to Michigan and Wisconsin, with particularly severe damage in New Jersey and New York. Its storm surge hit New York City on October 29, flooding streets, tunnels and subway lines and cutting power in and around the city. Damage in the United States amounted to $65 billion (2012 USD). In Canada, two were killed in Ontario, and the storm caused an estimated $100 million (2012 CAD) in damage throughout Ontario and Quebec.\nHurricane Sandy began as a low pressure system which developed sufficient organized convection to be classified as Tropical Depression Eighteen on October 22 south of Kingston, Jamaica. It moved slowly at first due to a ridge to the north. Low wind shear and warm waters allowed for strengthening, and the system was named Tropical Storm Sandy late on October 22. Early on October 24, an eye began developing, and it was moving steadily northward due to an approaching trough. Later that day, the National Hurricane Center (NHC) upgraded Sandy to hurricane status about south of Kingston, Jamaica. At about 1900 UTC that day, Sandy made landfall near Kingston with winds of about 85 mph (140 km/h). Just offshore Cuba, Sandy rapidly intensified to winds of 115 mph (185 km/h), and at that intensity it made landfall just west of Santiago de Cuba at 0525 UTC on October 25. Operationally, Sandy was assessed to have peaked as a high-end Category 2 hurricane with maximum sustained winds of 110 mph (175 km/h).\nAfter Sandy exited Cuba, the structure became disorganized, and it turned to the north-northwest over the Bahamas. By October 27, Sandy was no longer fully tropical, as evidenced by the development of frontal structures in the outer circulation. Despite strong shear, Sandy maintained convection due to influence from an approaching trough; the same that turned the hurricane to the northeast. After briefly weakening to a tropical storm, Sandy re-intensified into a hurricane, and on October 28 an eye began redeveloping. The storm moved around an upper-level low over the eastern United States and also to the southwest of a ridge over Atlantic Canada, turning it to the northwest. Sandy briefly re-intensified to Category 2 intensity on the morning of October 29, around which time it had become an extremely large hurricane with a wind diameter of over 1,000 miles (1,609 km), and an unusually low central barometric pressure of 940 mbar, possibly due to the very large size of the system. This pressure set records for many cities across the Northeastern United States for the lowest pressures ever observed. The convection diminished while the hurricane accelerated toward the New Jersey coast, and the cyclone was no longer tropical by 2100 UTC on October 29. About 2½ hours later, Sandy made landfall near Brigantine, New Jersey, with winds of 80 mph (130 km/h). During the next four days, Sandy's remnants drifted northward and then northeastward over Ontario, before merging with another low pressure area over Eastern Canada on November 2.\n\nOn October 23, 2012, the path of Hurricane Sandy was correctly predicted by the European Centre for Medium-Range Weather Forecasts (ECMWF) headquartered in Reading, England nearly eight days in advance of its striking the American East Coast. The computer model noted that the storm would turn west towards land and strike the New York/New Jersey region on October 29, rather than turn east and head out to the open Atlantic as most hurricanes in this position do. By October 27, four days after the ECMWF made its prediction, the National Weather Service and National Hurricane Center confirmed the path of the hurricane predicted by the European model. The National Weather Service was criticized for not employing its higher-resolution forecast models the way that its European counterpart did. A hardware and software upgrade completed at the end of 2013 enabled the weather service to make more accurate predictions, and do so far more in advance than the technology in 2012 had allowed.\n\nAccording to NCAR senior climatologist Kevin E. Trenberth, \"The answer to the oft-asked question of whether an event is caused by climate change is that it is the wrong question. All weather events are affected by climate change because the environment in which they occur is warmer and moister than it used to be.\" Although NOAA meteorologist Martin Hoerling attributes Sandy to \"little more than the coincidental alignment of a tropical storm with an extratropical storm\", Trenberth does agree that the storm was caused by \"natural variability\" but adds that it was \"enhanced by global warming\". One factor contributing to the storm's strength was abnormally warm sea surface temperatures offshore the East Coast of the United States—more than above normal, to which global warming had contributed . As the temperature of the atmosphere increases, the capacity to hold water increases, leading to stronger storms and higher rainfall amounts.\n\nAs they move north, Atlantic hurricanes typically are forced east and out to sea by the Prevailing Westerlies. In Sandy's case, this typical pattern was blocked by a ridge of high pressure over Greenland resulting in a negative North Atlantic Oscillation, forming a kink in the jet stream, causing it to double back on itself off the East Coast. Sandy was caught up in this northwesterly flow. The blocking pattern over Greenland also stalled an Arctic front which combined with the cyclone. Mark Fischetti of \"Scientific American\" said that the jet stream's unusual shape was caused by the melting of Arctic ice. Trenberth said that while a negative North Atlantic Oscillation and a blocking anticyclone were in place, the null hypothesis remained that this was just the natural variability of weather. Sea level at New York and along the New Jersey coast has increased by nearly a foot (300 mm) over the last hundred years, which contributed to the storm surge. Harvard geologist Daniel P. Schrag calls Hurricane Sandy's 13-foot (4 m) storm surge an example of what will, by mid-century, be the \"new norm on the Eastern seaboard\".\n\nAfter the storm became a tropical cyclone on October 22, the Government of Jamaica issued a tropical storm watch for the entire island. Early on October 23, the watch was replaced with a tropical storm warning and a hurricane watch was issued. At 1500 UTC, the hurricane watch was upgraded to a hurricane warning, while the tropical storm warning was discontinued. In preparation of the storm, many residents stocked up on supplies and reinforced roofing material. Acting Prime Minister Peter Phillips urged people to take this storm seriously, and also to take care of their neighbors, especially the elderly, children, and disabled. Government officials shut down schools, government buildings, and the airport in Kingston on the day prior to the arrival of Sandy. Meanwhile, numerous and early curfews were put in place to protect residents, properties, and to prevent crime. Shortly after Jamaica issued its first watch on October 22, the Government of Haiti issued a tropical storm watch for Haiti. By late October 23, it was modified to a tropical storm warning.\n\nThe Government of Cuba posted a hurricane watch for the Cuban Provinces of Camagüey, Granma, Guantánamo, Holguín, Las Tunas, and Santiago de Cuba at 1500 UTC on October 23. Only three hours later, the hurricane watch was switched to a hurricane warning. The Government of the Bahamas, at 1500 UTC on October 23, issued a tropical storm watch for several Bahamian islands, including the Acklins, Cat Island, Crooked Island, Exuma, Inagua, Long Cay, Long Island, Mayaguana, Ragged Island, Rum Cay, and San Salvador Island. Later that day, another tropical storm watch was issued for Abaco Islands, Andros Island, the Berry Islands, Bimini, Eleuthera, Grand Bahama, and New Providence. By early on October 24, the tropical storm watch for Cat Island, Exuma, Long Island, Rum Cay, and San Salvador was upgraded to a tropical storm warning.\n\nAt 1515 UTC on October 26, the Bermuda Weather Service issued a tropical storm watch for Bermuda, reflecting the enormous size of the storm and the anticipated wide-reaching impacts.\n\nMuch of the East Coast of the United States, in Mid-Atlantic and New England regions, had a good chance of receiving gale-force winds, flooding, heavy rain and possibly snow early in the week of October 28 from an unusual hybrid of Hurricane Sandy and a winter storm producing a Fujiwhara effect. Government weather forecasters said there was a 90% chance that the East Coast would be impacted by the storm. Jim Cisco of the Hydrometeorological Prediction Center coined the term \"Frankenstorm\", as Sandy was expected to merge with a storm front a few days before Halloween. As coverage continued, several media outlets began eschewing this term in favor of \"superstorm\". Utilities and governments along the East Coast attempted to head off long-term power failures Sandy might cause. Power companies from the Southeast to New England alerted independent contractors to be ready to help repair storm damaged equipment quickly and asked employees to cancel vacations and work longer hours. Researchers from Johns Hopkins University, using a computer model built on power outage data from previous hurricanes, conservatively forecast that 10 million customers along the Eastern Seaboard would lose power from the storm.\n\nThrough regional offices in Atlanta, Philadelphia, New York City, and Boston, the Federal Emergency Management Agency (FEMA) monitored Sandy, closely coordinating with state and tribal emergency management partners in Florida and the Southeast, Mid-Atlantic, and New England states. President Obama signed emergency declarations on October 28 for several states expected to be impacted by Sandy, allowing them to request federal aid and make additional preparations in advance of the storm. Flight cancellations and travel alerts on the U.S. East Coast were put in place in the Mid-Atlantic and the New England areas. Over 5,000 commercial airline flights scheduled for October 28 and 29 were canceled by the afternoon of October 28 and Amtrak canceled some services through October 29 in preparation for the storm. In addition, the National Guard and U.S. Air Force put as many as 45,000 personnel in at least seven states on alert for possible duty in response to the preparations and aftermath of Sandy.\n\nSchools on the Treasure Coast announced closures for October 26 in anticipation of Sandy. A Russian was allowed to stay in Jacksonville to avoid Sandy; the port is not far from Kings Bay Naval Submarine Base.\n\nAt 0900 UTC on October 26, a tropical storm watch was issued from the mouth of the Savannah River in South Carolina to Oregon Inlet, North Carolina, including Pamlico Sound. Twelve hours later, the portion of the tropical storm watch from the Santee River in South Carolina to Duck, North Carolina, including Pamlico Sound, was upgraded to a warning. Governor of North Carolina Beverly Perdue declared a state of emergency for 38 eastern counties on October 26, which took effect on the following day. By October 29, the state of emergency was extended to 24 counties in western North Carolina, with up to a foot (30 cm) of snow attributed to Sandy anticipated in higher elevations. The National Park Service closed at least five sections of the Blue Ridge Parkway.\n\nOn October 26, Governor of Virginia Bob McDonnell declared a state of emergency. The U.S. Navy sent more than twenty-seven ships and forces to sea from Naval Station Norfolk for their protection. Governor McDonnell authorized the National Guard to activate 630 personnel ahead of the storm. Republican Party presidential candidate Mitt Romney canceled campaign appearances scheduled for October 28 in Virginia Beach, Virginia, and New Hampshire October 30 because of Sandy. Vice President Joe Biden canceled his appearance on October 27 in Virginia Beach and an October 29 campaign event in New Hampshire. President Barack Obama canceled a campaign stop with former President Bill Clinton in Virginia scheduled for October 29, as well as a trip to Colorado Springs, Colorado, the next day because of the impending storm.\n\nOn October 26, Mayor of Washington, D.C. Vincent Gray declared a state of emergency, which President Obama signed on October 28. The United States Office of Personnel Management announced federal offices in the Washington, D.C. area would be closed to the public on October 29–30. In addition, Washington D.C. Metro service, both rail and bus, was canceled on October 29 due to expected high winds, the likelihood of widespread power outages, and the closing of the federal government. The Smithsonian Institution closed for the day of October 29.\n\nGovernor of Maryland Martin O'Malley declared a state of emergency on October 26. By the following day, Smith Island residents were evacuated with the assistance of the Maryland Natural Resources Police, Dorchester County opened two shelters for those in flood prone areas, and Ocean City initiated Phase I of their Emergency Operations Plan. Baltimore Gas and Electric Co. put workers on standby and made plans to bring in crews from other states. On October 28, President Obama declared an emergency in Maryland and signed an order authorizing the Federal Emergency Management Agency to aid in disaster relief efforts. Also, numerous areas were ordered to be evacuated including part of Ocean City, Worcester County, Wicomico County, and Somerset County. Officials warned that more than a hundred million tons of dirty sediment mixed with tree limbs and debris floating behind Conowingo Dam could be eventually poured into the Chesapeake Bay, posing a potential environmental threat.\n\nThe Maryland Transit Administration canceled all service for October 29 and 30. The cancellations applied to buses, light rail, and Amtrak and MARC train service. On October 29, six shelters opened in Baltimore, and early voting was canceled for the day. Maryland Insurance Commissioner Therese M. Goldsmith activated an emergency regulation requiring pharmacies to refill prescriptions regardless of their last refill date. On October 29, the Chesapeake Bay Bridge over the Chesapeake Bay and the Millard E. Tydings Memorial Bridge and Thomas J. Hatem Memorial Bridge over the Susquehanna River were closed to traffic in the midday hours.\n\nOn October 28, Governor Markell declared a state of emergency, with coastal areas of Sussex County evacuated. In preparation for the storm, the Delaware Department of Transportation suspended some weekend construction projects, removed traffic cones and barrels from construction sites, and removed several span-wire overhead signs in Sussex County. Delaware Route 1 through Delaware Seashore State Park was closed due to flooding. Delaware roads were closed to the public, except for emergency and essential personnel, and tolls on I-95 and Delaware Route 1 were waived. DART First State transit service was also suspended during the storm.\n\nPreparations began on October 26, when officials in Cape May County advised residents on barrier islands to evacuate. There was also a voluntary evacuation for Mantoloking, Bay Head, Barnegat Light, Beach Haven, Harvey Cedars, Long Beach, Ship Bottom, and Stafford in Ocean County. Governor of New Jersey Chris Christie ordered all residents of barrier islands from Sandy Hook to Cape May to evacuate and closed Atlantic City casinos. Tolls were suspended on the northbound Garden State Parkway and the westbound Atlantic City Expressway starting at 6 a.m. on October 28. President Obama signed an emergency declaration for New Jersey, allowing the state to request federal funding and other assistance for actions taken before Sandy's landfall.\n\nOn October 28, Mayor of Hoboken Dawn Zimmer ordered residents of basement and street-level residential units to evacuate, due to possible flooding. On October 29, residents of Logan Township were ordered to evacuate. Jersey Central Power & Light told employees to prepare to work extended shifts. Most schools, colleges and universities were closed October 29 while at least 509 out of 580 school districts were closed October 30. Although tropical storm conditions were inevitable and hurricane-force winds were likely, the National Hurricane Center did not issue any tropical cyclone watches or warnings for New Jersey, because Sandy was forecast to become extratropical before landfall and thus would not be a tropical cyclone.\n\nPreparations in Pennsylvania began when Governor Tom Corbett declared a state of emergency on October 26. Mayor of Philadelphia Michael Nutter asked residents in low-lying areas and neighborhoods prone to flooding to leave their homes by 1800 UTC October 28 and move to safer ground. The Philadelphia International Airport suspended all flight operations for October 29. On October 29, Philadelphia shut down its mass transit system. On October 28, Mayor of Harrisburg Linda D. Thompson declared a state of disaster emergency for the city to go into effect at 5 a.m. October 29. Electric utilities in the state brought in crews and equipment from other states such as New Mexico, Texas, and Oklahoma, to assist with restoration efforts.\n\nGovernor Andrew Cuomo declared a statewide state of emergency and asked for a pre-disaster declaration on October 26, which President Obama signed later that day. By October 27, major carriers canceled all flights into and out of JFK, LaGuardia, and Newark-Liberty airports, and the Metro North and Long Island Rail Roads suspended service. The Tappan Zee Bridge was closed, and later the Brooklyn Battery Tunnel and Holland Tunnel were also closed. On Long Island, an evacuation was ordered for South Shore, including areas south of Sunrise Highway, north of Route 25A, and in elevations of less than above sea level on the North Shore. In Suffolk County, mandatory evacuations were ordered for residents of Fire Island and six towns. Most schools closed in Nassau and Suffolk counties on October 29.\n\nNew York City began taking precautions on October 26. Governor Cuomo ordered the closure of MTA and its subway on October 28, and the MTA suspended all subway, bus, and commuter rail service beginning at 2300 UTC. After Hurricane Irene nearly submerged subways and tunnels in 2011, entrances and grates were covered just before Sandy, but were still flooded. PATH train service and stations as well as the Port Authority Bus Terminal were shut down in the early morning hours of October 29.\n\nLater on October 28, officials activated the coastal emergency plan, with subway closings and the evacuation of residents in areas hit by Hurricane Irene in 2011. More than 76 evacuation shelters were open around the city. On October 29, Mayor Michael Bloomberg ordered public schools closed and called for a mandatory evacuation of Zone A, which comprised areas near coastlines or waterways. Additionally, 200 National Guard troops were deployed in the city. NYU Langone Medical Center canceled all surgeries and medical procedures, except for emergency procedures. Additionally, one of NYU Langone Medical Center's backup generators failed on October 29, prompting the evacuation of hundreds of patients, including those from the hospital's various intensive care units. U.S. stock trading was suspended for October 29–30.\n\nConnecticut Governor Dannel Malloy partially activated the state's Emergency Operations Center on October 26 and signed a Declaration of Emergency the next day. On October 28, President Obama approved Connecticut's request for an emergency declaration, and hundreds of National Guard personnel were deployed. On October 29, Governor Malloy ordered road closures for all state highways. Numerous mandatory and partial evacuations were issued in cities across Connecticut.\n\nMassachusetts Governor Deval Patrick ordered state offices to be closed October 29 and recommended schools and private businesses close. On October 28, President Obama issued a Pre-Landfall Emergency Declaration for Massachusetts. Several shelters were opened, and many schools were closed. The Massachusetts Bay Transportation Authority shut down all services on the afternoon of October 29. On October 28, Vermont Governor Peter Shumlin, New Hampshire Governor John Lynch, and Maine's Governor Paul LePage all declared states of emergency.\n\nThe National Weather Service issued a storm warning for Lake Huron on October 29 that called for wave heights of , and possibly as high as . Lake Michigan waves were expected to reach , with a potential of on October 30. Flood warnings were issued in Chicago on October 29, where wave heights were expected to reach in Cook County and in northwest Indiana. Gale warnings were issued for Lake Michigan and Green Bay in Wisconsin until the morning of October 31, and waves of in Milwaukee and in Sheboygan were predicted for October 30. The actual waves reached about but were less damaging than expected. The village of Pleasant Prairie, Wisconsin urged a voluntary evacuation of its lakefront area, though few residents signed up, and little flooding actually occurred.\n\nMichigan was impacted by a winter storm system coming in from the west, mixing with cold air streams from the Arctic and colliding with Hurricane Sandy. The forecasts slowed shipping traffic on the Great Lakes, as some vessels sought shelter away from the peak winds, except those on Lake Superior. Detroit-based DTE Energy released 100 contract line workers to assist utilities along the eastern U.S. with storm response, and Consumers Energy did the same with more than a dozen employees and 120 contract employees. Due to the widespread power outages, numerous schools had to close, especially in St. Clair County and areas along Lake Huron north of Metro Detroit.\n\nAs far as Ohio's western edge, areas were under a wind advisory. All departing flights at Cleveland Hopkins International Airport were canceled until October 30 at 3 p.m.\n\nGovernor of West Virginia Earl Ray Tomblin declared a state of emergency ahead of storm on October 29. Up to 2 to 3 feet (0.6–0.9 m) of snow was forecast for mountainous areas of the state.\n\nThe Canadian Hurricane Centre issued its first preliminary statement for Hurricane Sandy on October 25 from Southern Ontario to the Canadian Maritimes, with the potential for heavy rain and strong winds. On October 29, Environment Canada issued severe wind warnings for the Great Lakes and St. Lawrence Valley corridor, from Southwestern Ontario as far as Quebec City. On October 30, Environment Canada issued storm surge warnings along the mouth of the St. Lawrence River. Rainfall warnings were issued for the Charlevoix region in Quebec, as well as for several counties in New Brunswick, and Nova Scotia, where about of rain was to be expected. Freezing rain warnings were issued for parts of Northern Ontario.\n\nAt least 233 people were killed across the United States, the Caribbean, and Canada, as a result of the storm.\n\nJamaica was the first country directly affected by Sandy, which was also the first hurricane to make landfall on the island since Hurricane Gilbert, 24 years prior. Trees and power lines were snapped and shanty houses were heavily damaged, both from the winds and flooding rains. More than 100 fishermen were stranded in outlying Pedro Cays off Jamaica's southern coast. Stones falling from a hillside crushed one man to death as he tried to get into his house in a rural village near Kingston. After 6 days another fatality recorded as a 27-year-old man, died due to electrocution, attempting a repair. The country's sole electricity provider, the Jamaica Public Service Company, reported that 70 percent of its customers were without power. More than 1,000 people went to shelters. Jamaican authorities closed the island's international airports, and police ordered 48-hour curfews in major towns to keep people off the streets and deter looting. Most buildings in the eastern portion of the island lost their roofs. Damage was assessed at approximately $100 million throughout the country.\n\nIn Haiti, which was still recovering from both the 2010 earthquake and the ongoing cholera outbreak, at least 54 people died, and approximately 200,000 were left homeless as a result of four days of ongoing rain from Hurricane Sandy. Heavy damage occurred in Port-Salut after rivers overflowed their banks. In the capital of Port-au-Prince, streets were flooded by the heavy rains, and it was reported that \"the whole south of the country is underwater\". Most of the tents and buildings in the city's sprawling refugee camps and the Cité Soleil neighborhood were flooded or leaking, a repeat of what happened earlier in the year during the passage of Hurricane Isaac. Crops were also wiped out by the storm and the country would be making an appeal for emergency aid. Damage in Haiti was estimated at $750 million (2012 USD), making it the costliest tropical cyclone in Haitian history. In the month following Sandy, a resurgence of cholera linked to the storm killed at least 44 people and infected more than 5,000 others.\n\nIn the neighboring Dominican Republic, two people were killed and 30,000 people evacuated. An employee of CNN estimated 70% of the streets in Santo Domingo were flooded. One person was killed in Juana Díaz, Puerto Rico after being swept away by a swollen river.\n\nAt least 55,000 people were evacuated before Hurricane Sandy's arrival. While moving ashore, the storm produced waves up to 29 feet (9 meters) and a 6-foot (2 meter) storm surge that caused extensive coastal flooding. There was widespread damage, particularly to Santiago de Cuba where 132,733 homes were damaged, of which 15,322 were destroyed and 43,426 lost their roof. Electricity and water services were knocked out, and most of the trees in the city were damaged. Total losses throughout Santiago de Cuba province is estimated as high as $2 billion (2012 USD). Sandy killed 11 people in the country – nine in Santiago de Cuba Province and two in Guantánamo Province; most of the victims were trapped in destroyed houses. This makes Sandy the deadliest hurricane to hit Cuba since 2005, when Hurricane Dennis killed 16 people.\n\nA NOAA automated station at Settlement Point on Grand Bahama Island reported sustained winds of 49 mph (74 km/h) and a wind gust of 63 mph (102 km/h). One person died from falling off his roof while attempting to fix a window shutter in the Lyford Cay area on New Providence. Another died in the Queen's Cove area on Grand Bahama Island where he drowned after the sea surge trapped him in his apartment. Portions of the Bahamas lost power or cellular service, including an islandwide power outage on Bimini. Five homes were severely damaged near Williams's Town. Overall damage in the Bahamas was about $700 million (2012 USD), with the most severe damage on Cat Island and Exuma where many houses were heavily damaged by wind and storm surge.\n\nOwing to the sheer size of the storm, Sandy also impacted Bermuda with high winds and heavy rains. On October 28, a weak F0 tornado touched down in Sandys Parish, damaging homes and businesses. During a three-day span, the storm produced of rain at the L.F. Wade International Airport. The strongest winds were recorded on October 29: sustained winds reached and gusts peaked at , which produced scattered minor damage.\n\nA total of 24 U.S. states were in some way affected by Sandy. The hurricane caused tens of billions of dollars in damage in the United States, destroyed thousands of homes, left millions without electric service, and caused 71 direct deaths in nine states, including 49 in New York, 10 in New Jersey, 3 in Connecticut, 2 each in Pennsylvania and Maryland, and 1 each in New Hampshire, Virginia and West Virginia. There were also 2 direct deaths from Sandy in U.S. coastal waters in the Atlantic Ocean, about 90 miles (150 km) off the North Carolina coast, which are not counted in the U.S. total. In addition, the storm resulted in 87 indirect deaths. In all, a total of 160 people were killed due to the storm, making Sandy the deadliest hurricane to hit the United States mainland since Hurricane Katrina in 2005 and the deadliest to hit the U.S. East Coast since Hurricane Agnes in 1972.\n\nDue to flooding and other storm-related problems, Amtrak canceled all Acela Express, Northeast Regional, Keystone, and Shuttle services for October 29 and 30. More than 13,000 flights were canceled across the U.S. on October 29, and more than 3,500 were called off October 30. From October 27 through early November 1, airlines canceled a total of 19,729 flights, according to FlightAware.\n\nOn October 31, over 6 million customers were still without power in 15 states and the District of Columbia. The states with the most customers without power were New Jersey with 2,040,195 customers; New York with 1,933,147; Pennsylvania with 852,458; and Connecticut with 486,927.\n\nThe New York Stock Exchange and Nasdaq reopened on October 31 after a two-day closure for the storm. More than 1,500 FEMA personnel were along the East Coast working to support disaster preparedness and response operations, including search and rescue, situational awareness, communications and logistical support. In addition, 28 teams containing 294 FEMA Corps members were pre-staged to support Sandy responders. Three federal urban search and rescue task forces were positioned in the Mid-Atlantic and ready to deploy as needed.\n\nOn November 2, the American Red Cross announced they had 4,000 disaster workers across storm damaged areas, with thousands more \"en route\" from other states. Nearly 7,000 people spent the night in emergency shelters across the region.\n\n\"\", a live telethon on November 2 that featured rock and pop stars such as Bruce Springsteen, Billy Joel, Jon Bon Jovi, Mary J. Blige, Sting and Christina Aguilera, raised around $23 million for American Red Cross hurricane relief efforts.\n\nAs of November 5, 2012, the National Hurricane Center ranks Hurricane Sandy the second-costliest US hurricane since 1900 in constant 2010 dollars, and the sixth-costliest after adjusting for inflation, population and property values. Their report also states that due to global warming the number of future hurricanes will \"either decrease or remain essentially unchanged\" overall, but the ones that do form will likely be stronger, with fiercer winds and heavier rains.\n\nScientists at the University of Utah reported the energy generated by Sandy was equivalent to \"small earthquakes between magnitudes 2 and 3\".\n\nIn South Florida, Sandy lashed the area with rough surf, strong winds, and brief squalls. Along the coast of Miami-Dade County, waves reached , but may have been as high as in Palm Beach County. In the former county, minor pounding occurred on a few coastal roads. Further north in Broward County, State Road A1A was inundated with sand and water, causing more than a stretch of the road to be closed for the entire weekend. Additionally, coastal flooding extended inland up to 2 blocks in some locations and a few houses in the area suffered water damage. In Manalapan, which is located in southern Palm Beach County, several beachfront homes were threatened by erosion. The Lake Worth Pier was also damaged by rough seas. In Palm Beach County alone, losses reached $14 million. Sandy caused closures and cancellations of some activities at schools in Palm Beach, Broward and Miami-Dade counties. Storm surge from Sandy also caused flooding and beach erosion along coastal areas in South Florida. Gusty winds also impacted South Florida, peaking at in Jupiter and Fowey Rocks Light, which is near Key Biscayne. The storm left power outages across the region, which left many traffic lights out of order.\n\nIn east-central Florida, damage was minor, though the storm left about 1,000 people without power. Airlines at Miami International Airport canceled more than 20 flights to or from Jamaica or the Bahamas, while some airlines flying from Fort Lauderdale–Hollywood International Airport canceled a total of 13 flights to the islands. The Coast Guard rescued two sea men in Volusia County off New Smyrna Beach on the morning of October 26. Brevard and Volusia Counties schools canceled all extracurricular activities for October 26, including football.\n\nTwo panther kittens escaped from the White Oak Conservation Center in Nassau County after the hurricane swept a tree into the fence of their enclosure; they were missing for 24 hours before being found in good health.\n\nOn October 28, Governor Bev Perdue declared a state of emergency in 24 western counties, due to snow and strong winds.\n\nNorth Carolina was spared from major damage for the most part (except at the immediate coastline), though winds, rain, and mountain snow affected the state through October 30. Ocracoke and Highway 12 on Hatteras Island were flooded with up to of water, closing part of the highway, while 20 people on a fishing trip were stranded on Portsmouth Island.\n\nThere were three Hurricane Sandy-related deaths in the state.\n\nOn October 29, the Coast Guard responded to a distress call from \"Bounty\", which was built for the 1962 movie \"Mutiny on the Bounty\". It was taking on water about 90 miles (150 km) southeast of Cape Hatteras. Sixteen people were on board. The Coast Guard said the 16 people abandoned ship and got into two lifeboats, wearing survival suits and life jackets. The ship sank after the crew got off. The Coast Guard rescued 14 crew members; another was found hours later but was unresponsive and later died. The search for the captain, Robin Walbridge, was suspended on November 1, after efforts lasting more than 90 hours and covering approximately 12,000 square nautical miles (41,100 km²).\n\nOn October 29, snow was falling in parts of the state. Gov. Bob McDonnell announced on October 30 that Virginia had been \"spared a significant event\", but cited concerns about rivers cresting and consequent flooding of major arteries. Virginia was awarded a federal disaster declaration, with Gov. McDonnell saying he was \"delighted\" that President Barack Obama and FEMA were on it immediately. At Sandy's peak, more than 180,000 customers were without power, most of whom were located in Northern Virginia. There were three Hurricane Sandy-related fatalities in the state.\n\nThe Supreme Court and the United States Government Office of Personnel Management were closed on October 30, and schools were closed for two days. MARC train and Virginia Railway Express were closed on October 30, and Metro rail and bus service were on Sunday schedule, opening at 2 p.m., until the system closes.\n\nAt least 100 feet (30 m) of a fishing pier in Ocean City was destroyed. Governor Martin O'Malley said the pier was \"half-gone\". Due to high winds, the Chesapeake Bay Bridge and the Millard E. Tydings Memorial Bridge on I-95 were closed. During the storm, the Mayor of Salisbury instituted a Civil Emergency and a curfew. Interstate 68 in far western Maryland and northern West Virginia closed due to heavy snow, stranding multiple vehicles and requiring assistance from the National Guard. Workers in Howard County tried to stop a sewage overflow caused by a power outage October 30. Raw sewage spilled at a rate of 2 million gallons per hour. It was unclear how much sewage had flowed into the Little Patuxent River. Over 311,000 people were left without power as a result of the storm.\n\nBy the afternoon of October 29, rainfall at Rehoboth Beach totaled . Other precipitation reports include nearly at Indian River Inlet and more than in Dover and Bear. At 4 p.m. on October 29, Delmarva Power reported on its website that more than 13,900 customers in Delaware and portions of the Eastern Shore of Maryland had lost electric service as high winds brought down trees and power lines. About 3,500 of those were in New Castle County, 2,900 were in Sussex, and more than 100 were in Kent County. Some residents in Kent and Sussex Counties experienced power outages that lasted up to nearly six hours. At the peak of the storm, more than 45,000 customers in Delaware were without power. The Delaware Memorial Bridge speed limit was reduced to and the two outer lanes in each direction were closed. Officials planned to close the span entirely if sustained winds exceeded . A wind gust of was measured at Lewes just before 2:30 p.m. on October 29. Delaware Route 1 was closed due to water inundation between Dewey Beach and Fenwick Island. In Dewey Beach, flood waters were in depth. Following the impact in Delaware, President Barack Obama declared the entire state a federal disaster area, providing money and agencies for disaster relief in the wake of Hurricane Sandy.\n\nA 50-foot (15 m) piece of the Atlantic City Boardwalk washed away. Half the city of Hoboken flooded; the city of 50,000 had to evacuate two of its fire stations, the EMS headquarters, and the hospital. With the city cut off from area hospitals and fire suppression mutual aid, the city's Mayor asked for National Guard help. In the early morning of October 30, authorities in Bergen County, New Jersey, evacuated residents after a berm overflowed and flooded several communities. Police Chief of Staff Jeanne Baratta said there were up to five feet (1.5 m) of water in the streets of Moonachie and Little Ferry. The state Office of Emergency Management said rescues were undertaken in Carlstadt. Baratta said the three towns had been \"devastated\" by the flood of water. At the peak of the storm, more than 2,600,000 customers were without power. There were 43 Hurricane Sandy-related deaths in the state of New Jersey. Damage in the state was estimated at $36.8 billion.\n\nPhiladelphia Mayor Michael Nutter said the city would have no mass transit operations on any lines October 30. All major highways in and around the city of Philadelphia were closed on October 29 during the hurricane, including Interstate 95, the Blue Route portion of Interstate 476, the Vine Street Expressway, Schuylkill Expressway (I-76), and the Roosevelt Expressway; U.S. Route 1. The highways reopened at 4 a.m. on October 30. The Delaware River Port Authority also closed its major crossings over the Delaware River between Pennsylvania and New Jersey due to high winds, including the Commodore Barry Bridge, the Walt Whitman Bridge, the Ben Franklin Bridge and the Betsy Ross Bridge. Trees and powerlines were downed throughout Altoona, and four buildings partially collapsed. More than 1.2 million were left without power. The Pennsylvania Emergency Management Agency reported 14 deaths believed to be related to Sandy.\n\nNew York governor Andrew Cuomo called National Guard members to help in the state. Storm impacts in Upstate New York were much more limited than in New York City; there was some flooding and a few downed trees. Rochester area utilities reported slightly fewer than 19,000 customers without power, in seven counties. In the state as a whole, however, more than 2,000,000 customers were without power at the peak of the storm.\n\nMayor of New York City Michael Bloomberg announced that New York City public schools would be closed on Tuesday, October 30 and Wednesday, October 31, but they remained closed through Friday, November 2. CUNY and NYU canceled all classes and campus activities for October 30. The New York Stock Exchange was closed for trading for two days, the first weather closure of the exchange since 1985. It was also the first two-day weather closure since the Great Blizzard of 1888.\n\nThe East River overflowed its banks, flooding large sections of Lower Manhattan. Battery Park had a water surge of 13.88 ft. Seven subway tunnels under the East River were flooded. The Metropolitan Transportation Authority said that the destruction caused by the storm was the worst disaster in the 108-year history of the New York City subway system. Sea water flooded the Ground Zero construction site. Over 10 billion gallons of raw and partially treated sewage were released by the storm, 94% of which went into waters in and around New York and New Jersey. In addition, a four-story Chelsea building's facade crumbled and collapsed, leaving the interior on full display; however, no one was hurt by the falling masonry. The Atlantic Ocean storm surge also caused considerable flood damage to homes, buildings, roadways, boardwalks and mass transit facilities in low-lying coastal areas of the outer boroughs of Queens, Brooklyn and Staten Island.\n\nAfter receiving many complaints that holding the marathon would divert needed resources, Mayor Bloomberg announced late afternoon November 2 that the New York City Marathon had been canceled. The event was to take place on Sunday, November 4. Marathon officials said that they did not plan to reschedule.\n\nGas shortages throughout the region led to an effort by the U.S. federal government to bring in gasoline and set up mobile truck distribution at which people could receive up to 10 gallons of gas, free of charge. This caused lines of up to 20 blocks long and was quickly suspended. On Thursday, November 8, Mayor Bloomberg announced odd-even rationing of gasoline would be in effect beginning November 9 until further notice.\n\nOn November 26, Governor Cuomo called Sandy \"more impactful\" than Hurricane Katrina, and estimated costs to New York at $42 billion. Approximately 100,000 residences on Long Island were destroyed or severely damaged, including 2,000 that were rendered uninhabitable. There were 53 Hurricane Sandy-related deaths in the state of New York. In 2016, the hurricane was determined to have been the worst to strike the New York City area since at least 1700.\n\nWind gusts to 83 mph were recorded on outer Cape Cod and Buzzards Bay. Nearly 300,000 customers were without power in Massachusetts, and roads and buildings were flooded. Over 100,000 customers lost power in Rhode Island. Most of the damage was along the coastline, where some communities were flooded. Mount Washington, New Hampshire saw the strongest measured wind gust from the storm at 140 mph. Nearly 142,000 customers lost power in the state.\n\nSandy's rain became snow in the Appalachian mountains, leading to blizzard conditions in some areas, especially West Virginia, when a tongue of dense and heavy Arctic air pushed south through the region. This would normally cause a Nor'easter, prompting some to dub Sandy a \"nor'eastercane\" or \"Frankenstorm\". There was of snowfall in 28 of West Virginia's 55 counties. The highest snowfall accumulation was near Richwood. Other significant totals include in Snowshoe, in Quinwood, and in Davis, Flat Top, and Huttonsville. By the morning of October 31, there were still 36 roads closed due to downed trees, powerlines, and snow in the road. Approximately 271,800 customers lost power during the storm.\nThere were reports of collapsed buildings in several counties due to the sheer weight of the wet, heavy snow. Overall, there were seven fatalities related to Hurricane Sandy and its remnants in West Virginia, including John Rose, Sr., the Republican candidate for the state's 47th district in the state legislature, who was killed in the aftermath of the storm by a falling tree limb broken off by the heavy snowfall. Governor Earl Ray Tomblin asked President Obama for a federal disaster declaration, and on October 30, President Obama approved a state of emergency declaration for the state.\n\nWind gusts at Cleveland Burke Lakefront Airport were reported at . On October 30, hundreds of school districts canceled or delayed school across the state with at least 250,000 homes and businesses without power. Damage was reported across the state including the Rock and Roll Hall of Fame which lost parts of its siding. Snow was reported in some parts of eastern Ohio and south of Cleveland. Snow and icy roads also were reported south of Columbus.\n\nThe US Department of Energy reported that more than 120,000 customers lost power in Michigan as a result of the storm. The National Weather Service said that waves up to 23 feet (7 m) high were reported on southern Lake Huron.\n\nMore than one foot (300 mm) of snow fell in eastern Kentucky as Sandy merged with an Arctic front. Winter warnings in Harlan, Letcher, and Pike County were put into effect until October 31.\n\nThe remnants of Sandy produced high winds along Lake Huron and Georgian Bay, where gusts were measured at 105 km/h (63 mph). A 121 km/h (72 mph) gust was measured on top of the Bluewater Bridge. One woman died after being hit by a piece of flying debris in Toronto. At least 145,000 customers across Ontario lost power, and a Bluewater Power worker was electrocuted in Sarnia while working to restore power. Around 49,000 homes and businesses lost power in Quebec during the storm, with nearly 40,000 of those in the Laurentides region of the province, as well as more than 4,000 customers in the Eastern Townships and 1,700 customers in Montreal. Hundreds of flights were canceled. Around 14,000 customers in Nova Scotia lost power during the height of the storm. The Insurance Bureau of Canada's preliminary damage estimate was over $100 million for the nation.\n\nSeveral media organizations contributed to the immediate relief effort: Disney–ABC Television Group held a \"Day of Giving\" on Monday, November 5, raising $17 million on their television stations for the American Red Cross and NBC raised $23 million during their \"\" telethon the same day. On October 31, 2012, News Corporation donated $1 million to relief efforts in the New York metropolitan area. As of December 2013, the NGO Hurricane Sandy New Jersey Relief Fund had distributed much of the funding raised in New Jersey.\n\nOn November 6, the United Nations and World Food Programme promised humanitarian aid to at least 500,000 people in Santiago de Cuba.\n\nOn December 12, 2012, the \"\" took place at Madison Square Garden in New York City. Various television channels in the United States and internationally aired the four-hour concert which was expected to reach over 1 billion people worldwide, featuring Bon Jovi, Eric Clapton, Dave Grohl, Billy Joel, Alicia Keys, Chris Martin, Paul McCartney, The Rolling Stones, Bruce Springsteen and the E Street Band, Roger Waters, Eddie Vedder, Kanye West, and The Who. Web sites including Fuse.tv, MTV.com, YouTube, and the sites of AOL and Yahoo! planned to stream the performance.\n\nOn December 28, 2012, the United States Senate approved an emergency Hurricane Sandy relief bill to provide $60 billion for US states affected by Sandy, but the House in effect postponed action until the next session which began January 3 by adjourning without voting on the bill. On January 4, 2013 House leaders pledged to vote on a flood insurance bill and an aid package by January 15. On January 28, the Senate passed the $50.5 billion Sandy aid bill by a count of 62–36. which President Obama signed into law January 29.\n\nIn January 2013, \"The New York Times\" reported that those affected by the hurricane were still struggling to recover.\n\nIn June 2013, NY Governor Andrew Cuomo set out to centralize recovery and rebuilding efforts in impacted areas of New York State by establishing the Governor's Office of Storm Recovery (GOSR). He aimed to address communities' most urgent needs, and to identify innovative and enduring solutions to strengthen the State's infrastructure and critical systems. Operating under the umbrella of New York Rising, GOSR utilized approximately $3.8 billion in flexible funding made available by the U.S. Department of Housing & Urban Development's (HUD) Community Development Block Grant Disaster Recovery (CDBG-DR) program to concentrate aid in four main areas: housing, small business, infrastructure, and the community reconstruction.\n\nOn December 6, 2013, an analysis of Federal Emergency Management Agency data showed that fewer than half of those affected who requested disaster recovery assistance had received any, and a total of 30,000 residents of New York and New Jersey remained displaced.\n\nIn March 2014, \"Newsday\" reported, that 17 months after the hurricane people displaced from rental units on Long Island faced unique difficulties due to lack of affordable rental housing and delays in housing program implementations by New York State. Close to 9,000 rental units on Long Island were damaged by Hurricane Sandy in October 2012, and Hurricane Irene and Tropical Storm Lee in 2011 per the NY State Governor's Office of Storm Recovery (GOSR). New York State officials said that additional assistance would soon be available from the HUD's Community Development Block Grant funds via the New York Rising program. On March 15, 2014, a group of those who remained displaced by the hurricane organized a protest at the Nassau Legislative building in Mineola, New York, to raise awareness of their frustration with the timeline for receiving financial assistance from the New York Rising program.\n\n, the GOSR released a press statement, that the New York Rising Community Reconstruction Program had distributed more than $280 million in payments to 6,388 homeowners for damage from Hurricane Sandy, Hurricane Irene or Tropical Storm Lee. Every eligible homeowner who had applied by January 20, 2014 had been issued a check for home reconstruction, including over 4,650 Nassau residents for over $201 million and over 1,350 Suffolk residents for over $65 million. The State also had made offers over $293 million to buyout homes of 709 homeowners.\n\nHurricane Sandy sparked much political commentary. Many scientists said warming oceans and greater atmospheric moisture were intensifying storms while rising sea levels were worsening coastal effects. November 2012 Representative Henry Waxman of California, the top Democrat of the House Energy and Commerce Committee, requested a hearing in the lame duck session on links between climate change and Hurricane Sandy. Some news outlets labeled the storm the October surprise of the 2012 United States Presidential election, while Democrats and Republicans accused each other of politicizing the storm.\n\nThe storm hit the United States one week before its general United States elections, and affected the presidential campaign, local and state campaigns in storm-damaged areas. New Jersey Governor Chris Christie, one of Mitt Romney's leading supporters, praised President Barack Obama and his reaction to the hurricane, and toured storm-damaged areas of his state with the president. It was reported at the time that Sandy might affect elections in several states, especially by curtailing early voting. \"The Economist\" wrote, \"the weather is supposed to clear up well ahead of election day, but the impact could be felt in the turnout of early voters.\" ABC News predicted this might be offset by a tendency to clear roads and restore power more quickly in urban areas. The storm ignited a debate over whether Republican presidential nominee Mitt Romney in 2011 proposed to eliminate the Federal Emergency Management Agency (FEMA). The next day the Romney campaign promised to keep FEMA funded, but did not explain what other parts of the federal budget it would cut to pay for it. Beyond the election, \"National Defense Magazine\" said Sandy \"might cause a rethinking (in the USA) of how climate change threatens national security\".\n\nIn his news conference on November 14, 2012, President Obama said, \"we can't attribute any particular weather event to climate change. What we do know is the temperature around the globe is increasing faster than was predicted even 10 years ago. We do know that the Arctic ice cap is melting faster than was predicted even five years ago. We do know that there have been extraordinarily — there have been an extraordinarily large number of severe weather events here in North America, but also around the globe. And I am a firm believer that climate change is real, that it is impacted by human behavior and carbon emissions. And as a consequence, I think we've got an obligation to future generations to do something about it.\"\n\nOn January 30, 2015, days after the U.S. Army Corps of Engineers released a post-Sandy report examining flood risks for 31,200 miles (50,210 km) of the North Atlantic coast, President Obama issued an executive order directing federal agencies, state and local governments drawing federal funds to adopt stricter building and siting standards to reflect scientific projections that future flooding will be more frequent and intense due to climate change.\n\nPower outages and flooding in the area closed the New York Stock Exchange and other financial markets on both October 29 and 30, a weather-related closure that last happened in 1888. When markets reopened on October 31, investors were relieved that it closed relatively flat that day. A week later, the National Association of Insurance Commissioners Capital Markets Bureau noted a slight uptick in the market (0.8%) and suggested that the negative economic impact of Hurricane Sandy was offset by the expected positive impacts of rebuilding.\n\nAs Hurricane Sandy approached the United States, forecasters and journalists gave it several different unofficial names, at first related to its projected snow content, then to its proximity to Halloween, and eventually to the overall size of the storm. Early nicknames included \"Snowicane Sandy\" and \"Snor'eastercane Sandy\". The most popular Halloween-related nickname was \"Frankenstorm\", coined by Jim Cisco, a forecaster at the Hydrometeorological Prediction Center. CNN banned the use of the term, saying it trivialized the destruction.\n\nThe severe and widespread damage the storm caused in the United States, as well as its unusual merger with a frontal system, resulted in the nicknaming of the hurricane \"Superstorm Sandy\" by the media, public officials, and several organizations, including U.S. government agencies. This persisted as the most common nickname well into 2013. The term was also embraced by climate change proponents as a term for the new type of storms caused by global warming, while other writers used the term but maintained that it was too soon to blame the storm on climate change. Meanwhile, \"Popular Science\" called it \"an imaginary scare-term that exists exclusively for shock value\".\n\nThousands of homeowners were denied their flood insurance claims based upon fraudulent engineers' reports, according to the whistleblowing efforts of Andrew Braum, an engineer who claimed that at least 175 of his more than 180 inspections were doctored. As a result, a class-action racketeering lawsuit has been filed against several insurance companies and their contract engineering firms. , the Federal Emergency Management Agency planned to review all flood insurance claims.\n\nNew Jersey hospitals saw a spike in births nine months after Sandy, causing some to believe that there was a post-Sandy baby boom. The Monmouth Medical Center saw a 35% jump, and two other hospitals saw 20% increases. An expert stated that post-storm births that year were higher than in past disasters.\n\nBecause of the exceptional damage and deaths caused by the storm in many countries, the name \"Sandy\" was later retired by the World Meteorological Organization, and will never be used again for a North Atlantic hurricane. The name was replaced with \"Sara\" for the 2018 Atlantic hurricane season. However, because \"Sara\" was not used in 2018, it could be used in 2024.\n\nIn the Syfy television movie \"\", a radar image of Sandy can be seen (close to its landfall in New Jersey), but represented fictionally as an unnamed tropical storm heading towards Washington D.C. The storm is also featured in an episode of \"Shades of Blue.\"\n\n\nInformational notes\nCitations\nFurther reading\n\n"}
{"id": "9974179", "url": "https://en.wikipedia.org/wiki?curid=9974179", "title": "Jean Bernard Sindeu", "text": "Jean Bernard Sindeu\n\nJean-Bernard Sindeu is a Cameroonian politician who served in the government of Cameroon as Minister of Energy and Water Resources from 2006 to 2009. Considered a technocrat, he was appointed to that position on 22 September 2006. He was previously the First Assistant Mayor of Bana, which is located in West Province; he remains in that position as of 2007.\n\nSindeu was dismissed from the government on 30 June 2009.\n"}
{"id": "11927499", "url": "https://en.wikipedia.org/wiki?curid=11927499", "title": "Lateritic nickel ore deposits", "text": "Lateritic nickel ore deposits\n\nLateritic nickel ore deposits are surficial, weathered rinds formed on ultramafic rocks.\nThey account for 73% of the continental world nickel resources and will be in the future the dominant source for the mining of nickel.\n\nLateritic nickel ores formed by intensive tropical weathering of olivine-rich ultramafic rocks such as dunite, peridotite and komatiite and their serpentinized derivatives, serpentinite which consist largely of the magnesium silicate serpentine and contains approx. 0.3% nickel. This initial nickel content is strongly enriched in the course of lateritization. \nTwo kinds of lateritic nickel ore have to be distinguished: limonite types and silicate types.\n\nLimonite type laterites (or oxide type) are highly enriched in iron due to very strong leaching of magnesium and silica. They consist largely of goethite and contain 1-2% nickel incorporated in goethite. Absence of the limonite zone in the ore deposits is due to erosion.\n\nSilicate type (or saprolite type) nickel ore formed beneath the limonite zone. It contains generally 1.5-2.5% nickel and consists largely of Mg-depleted serpentine in which nickel is incorporated. In pockets and fissures of the serpentinite rock green garnierite can be present in minor quantities, but with high nickel contents - mostly 20-40%. It is bound in newly formed phyllosilicate minerals. All the nickel in the silicate zone is leached downwards (absolute nickel concentration) from the overlying goethite zone.\n\nTypical nickel laterite ore deposits are very large tonnage, low-grade deposits located close to the surface. They are typically in the range of 20 million tonnes and upwards (this being a contained resource of 200,000 tonnes of nickel at 1%) with some examples approaching a billion tonnes of material. Thus, typically, nickel laterite ore deposits contain many billions of dollars of in-situ value of contained metal.\n\nOre deposits of this type are restricted to the weathering mantle developed above ultramafic rocks. As such they tend to be tabular, flat and really large, covering many square kilometres of the Earth's surface. However, at any one time the area of a deposit being worked for the nickel ore is much smaller, usually only a few hectares. The typical nickel laterite mine often operates as either an open cut mine or a strip mine.\n\nNickel laterites are a very important type of nickel ore deposit. They are growing to become the most important source of nickel metal for world demand (currently second to sulfide nickel ore deposits).\n\nNickel laterites are generally mined via open cut mining methods. Nickel is extracted from the ore by a variety of process routes. Hydrometallurgical processes include high-pressure acid leach (HPAL) and heap leach, both of which are generally followed by solvent extraction - electrowinning (SX-EW) for recovery of nickel. Another hydrometallurgical routes is the Caron process, which consists of roasting followed by ammonia leaching and precipitation as nickel carbonate. Additionally, ferronickel is produced by the rotary kiln - electric furnace (RKEF) process.\n\nHigh pressure acid leach processing is employed for two types of nickel laterite ores. 1) Ores with a limonitic character such as the deposits of the Moa district in Cuba and southeast New Caledonia at Goro where nickel is bound in goethite and asbolite. 2) Ores of a predominantly nontronitic character, such as many deposits in Western Australia, where nickel is bound within clay or secondary silicate substrates in the ores. The nickel (+/- cobalt) metal is liberated from such minerals only at low pH and high temperatures, generally in excess of 250 °C.\n\nThe advantages of HPAL plants are that they are not as selective toward the type of ore minerals, grades and nature of mineralisation. The disadvantage is the energy required to heat the ore material and acid, and the wear and tear hot acid causes upon plant and equipment. Higher energy costs demand higher ore grades.\n\nHeap leach treatment of nickel laterites is primarily applicable to clay-poor oxide-rich ore types where clay contents are low enough to allow percolation of acid through the heap. Generally, this route of production is much cheaper - up to half the cost of production - due to the lack of need to heat and pressurise the ore and acid.\n\nOre is ground, agglomerated, and perhaps mixed with clay-poor rock, to prevent compaction of the clay-like materials and so maintain permeability. The ore is stacked on impermeable plastic membranes and acid is percolated over the heap, generally for 3 to 4 months, at which stage 60% to 70% of the nickel-cobalt content is liberated into acid solution, which is then neutralised with limestone and a nickel-cobalt hydroxide intermediate product is generated, generally then sent to a smelter for refining.\n\nThe advantage of heap leach treatment of nickeliferous laterite ores is that the plant and mine infrastructure are much cheaper - up to 25% of the cost of a HPAL plant - and less risky from a technological point of view. However, they are somewhat limited in the types of ore which can be treated.\n\nA recent development in the extraction of nickel laterite ores is a particular grade of tropical deposits, typified by examples at Acoje in the Philippines, developed on ophiolite sequence ultramafics. This ore is so rich in limonite (generally grading 47% to 59% iron, 0.8 to 1.5% nickel and trace cobalt) that it is essentially similar to low-grade iron ore. As such, certain steel smelters in China have developed a process for blending nickel limonite ore with conventional iron ore to produce stainless steel feed products.\n\nAnother new method of extracting nickel from laterite ores is currently being demonstrated at a full-scale test plant at the CSIRO facility in Perth Australia. The DNi process uses nitric acid, instead of sulphuric acid, to extract the nickel within a few hours and then the nitric acid is recycled. The DNi process has the major advantage of being able to treat both limonite and saprolite lateritic ores and is estimated to have less than half the capital and operating costs of HPAL or FerroNickel processes.\n\n"}
{"id": "9263122", "url": "https://en.wikipedia.org/wiki?curid=9263122", "title": "Lead scandium tantalate", "text": "Lead scandium tantalate\n\nLead scandium tantalate (PST) is a ferroelectric ceramic material with perovskite structure. It has the formula Pb(ScTa)O. The x is usually about 0.5.\n\nLike structurally similar lead zirconate titanate and barium strontium titanate, PST can be used for manufacture of uncooled focal plane array infrared imaging sensors for thermal cameras. Both bulk and thin film structures are used.\n\nIt is a mixed oxide of lead, scandium, and tantalum.\n"}
{"id": "41638092", "url": "https://en.wikipedia.org/wiki?curid=41638092", "title": "Lithuanian District Heating Association", "text": "Lithuanian District Heating Association\n\nLithuanian District Heating Association was established on 24 February 1998. LDHA represents the interests and rights of the Lithuanian district heat utilities, organizations and others associated energy structures in the district heating sector. Members of association produce and supply around 95-99 % of the total heat through the district heating network in Lithuania.\n\nThe association has 42 members, 31 of the total membership comprise district heating companies and 11 companies whose activities are closely linked to the heat sector.\n"}
{"id": "40283", "url": "https://en.wikipedia.org/wiki?curid=40283", "title": "Melting point", "text": "Melting point\n\nThe melting point (or, rarely, liquefaction point) of a substance is the temperature at which it changes state from solid to liquid. At the melting point the solid and liquid phase exist in equilibrium. The melting point of a substance depends on pressure and is usually specified at a standard pressure such as 1 atmosphere or 100 kPa.\n\nWhen considered as the temperature of the reverse change from liquid to solid, it is referred to as the freezing point or crystallization point. Because of the ability of some substances to supercool, the freezing point is not considered as a characteristic property of a substance. When the \"characteristic freezing point\" of a substance is determined, in fact the actual methodology is almost always \"the principle of observing the disappearance rather than the formation of ice\", that is, the melting point.\n\nFor most substances, melting and freezing points are approximately equal. For example, the melting point \"and\" freezing point of mercury is 234.32 kelvins (−38.83 °C or −37.89 °F). However, certain substances possess differing solid-liquid transition temperatures. For example, agar melts at 85 °C (185 °F) and solidifies from ; such direction dependence is known as hysteresis. The melting point of ice at 1 atmosphere of pressure is very close to ; this is also known as the ice point. In the presence of nucleating substances, the freezing point of water is not always the same as the melting point. In the absence of nucleators water can exist as a supercooled liquid down to −48.3 °C (−55 °F, 224.8 K) before freezing.\n\nThe chemical element with the highest melting point is tungsten, at ; this property makes tungsten excellent for use as filaments in light bulbs. The often-cited carbon does not melt at ambient pressure but sublimes at about ; a liquid phase only exists above pressures of and estimated (see ). Tantalum hafnium carbide (TaHfC) is a refractory compound with a very high melting point of 4215 K (3942 °C, 7128 °F). At the other end of the scale, helium does not freeze at all at normal pressure even at temperatures close to absolute zero; a pressure of more than twenty times normal atmospheric pressure is necessary.\n\nMany laboratory techniques exist for the determination of melting points.\nA Kofler bench is a metal strip with a temperature gradient (range from room temperature to 300 °C). Any substance can be placed on a section of the strip, revealing its thermal behaviour at the temperature at that point. Differential scanning calorimetry gives information on melting point together with its enthalpy of fusion.\nA basic melting point apparatus for the analysis of crystalline solids consists of an oil bath with a transparent window (most basic design: a Thiele tube) and a simple magnifier. The several grains of a solid are placed in a thin glass tube and partially immersed in the oil bath. The oil bath is heated (and stirred) and with the aid of the magnifier (and external light source) melting of the individual crystals at a certain temperature can be observed. In large/small devices, the sample is placed in a heating block, and optical detection is automated.\n\nThe measurement can also be made continuously with an operating process. For instance, oil refineries measure the freeze point of diesel fuel online, meaning that the sample is taken from the process and measured automatically. This allows for more frequent measurements as the sample does not have to be manually collected and taken to a remote laboratory.\n\nFor refractory materials (e.g. platinum, tungsten, tantalum, some carbides and nitrides, etc.) the extremely high melting point (typically considered to be above, say, 1800 °C) may be determined by heating the material in a black body furnace and measuring the black-body temperature with an optical pyrometer. For the highest melting materials, this may require extrapolation by several hundred degrees. The spectral radiance from an incandescent body is known to be a function of its temperature. An optical pyrometer matches the radiance of a body under study to the radiance of a source that has been previously calibrated as a function of temperature. In this way, the measurement of the absolute magnitude of the intensity of radiation is unnecessary. However, known temperatures must be used to determine the calibration of the pyrometer. For temperatures above the calibration range of the source, an extrapolation technique must be employed. This extrapolation is accomplished by using Planck's law of radiation. The constants in this equation are not known with sufficient accuracy, causing errors in the extrapolation to become larger at higher temperatures. However, standard techniques have been developed to perform this extrapolation.\n\nConsider the case of using gold as the source (mp = 1063 °C). In this technique, the current through the filament of the pyrometer is adjusted until the light intensity of the filament matches that of a black-body at the melting point of gold. This establishes the primary calibration temperature and can be expressed in terms of current through the pyrometer lamp. With the same current setting, the pyrometer is sighted on another black-body at a higher temperature. An absorbing medium of known transmission is inserted between the pyrometer and this black-body. The temperature of the black-body is then adjusted until a match exists between its intensity and that of the pyrometer filament. The true higher temperature of the black-body is then determined from Planck's Law. The absorbing medium is then removed and the current through the filament is adjusted to match the filament intensity to that of the black-body. This establishes a second calibration point for the pyrometer. This step is repeated to carry the calibration to higher temperatures. Now, temperatures and their corresponding pyrometer filament currents are known and a curve of temperature versus current can be drawn. This curve can then be extrapolated to very high temperatures.\n\nIn determining melting points of a refractory substance by this method, it is necessary to either have black body conditions or to know the emissivity of the material being measured. The containment of the high melting material in the liquid state may introduce experimental difficulties. Melting temperatures of some refractory metals have thus been measured by observing the radiation from a black body cavity in solid metal specimens that were much longer than they were wide. To form such a cavity, a hole is drilled perpendicular to the long axis at the center of a rod of the material. These rods are then heated by passing a very large current through them, and the radiation emitted from the hole is observed with an optical pyrometer. The point of melting is indicated by the darkening of the hole when the liquid phase appears, destroying the black body conditions. Today, containerless laser heating techniques, combined with fast pyrometers and spectro-pyrometers, are employed to allow for precise control of the time for which the sample is kept at extreme temperatures. Such experiments of sub-second duration address several of the challenges associated with more traditional melting point measurements made at very high temperatures, such as sample vaporization and reaction with the container.\n\nFor a solid to melt, heat is required to raise its temperature to the melting point. However, further heat needs to be supplied for the melting to take place: this is called the heat of fusion, and is an example of latent heat.\n\nFrom a thermodynamics point of view, at the melting point the change in Gibbs free energy (ΔG) of the material is zero, but the enthalpy (\"H\") and the entropy (\"S\") of the material are increasing (ΔH, ΔS > 0). Melting phenomenon happens when the Gibbs free energy of the liquid becomes lower than the solid for that material. At various pressures this happens at a specific temperature. It can also be shown that:\n\nHere \"T\", \"ΔS\" and \"ΔH\" are respectively the temperature at the melting point, change of entropy of melting and the change of enthalpy of melting.\n\nThe melting point is sensitive to extremely large changes in pressure, but generally this sensitivity is orders of magnitude less than that for the boiling point, because the solid-liquid transition represents only a small change in volume. If, as observed in most cases, a substance is more dense in the solid than in the liquid state, the melting point will increase with increases in pressure. Otherwise the reverse behavior occurs. Notably, this is the case of water, as illustrated graphically to the right, but also of Si, Ge, Ga, Bi. With extremely large changes in pressure, substantial changes to the melting point are observed. For example, the melting point of silicon at ambient pressure (0.1 MPa) is 1415 °C, but at pressures in excess of 10 GPa it decreases to 1000 °C.\n\nMelting points are often used to characterize organic and inorganic compounds and to ascertain their purity. The melting point of a pure substance is always higher and has a smaller range than the melting point of an impure substance or, more generally, of mixtures. The higher the quantity of other components, the lower the melting point and the broader will be the melting point range, often referred to as the \"pasty range\". The temperature at which melting begins for a mixture is known as the \"solidus\" while the temperature where melting is complete is called the \"liquidus\". Eutectics are special types of mixtures that behave like single phases. They melt sharply at a constant temperature to form a liquid of the same composition. Alternatively, on cooling a liquid with the eutectic composition will solidify as uniformly dispersed, small (fine-grained) mixed crystals with the same composition.\n\nIn contrast to crystalline solids, glasses do not possess a melting point;\non heating they undergo a smooth glass transition into a viscous liquid.\nUpon further heating, they gradually soften, which can be characterized by certain softening points.\n\nThe freezing point of a solvent is depressed when another compound is added, meaning that a solution has a lower freezing point than a pure solvent. This phenomenon is used in technical applications to avoid freezing, for instance by adding salt or ethylene glycol to water.\n\nIn organic chemistry, Carnelley's rule, established in 1882 by Thomas Carnelley, states that \"high molecular symmetry is associated with high melting point\". Carnelley based his rule on examination of 15,000 chemical compounds. For example, for three structural isomers with molecular formula CH the melting point increases in the series isopentane −160 °C (113 K) n-pentane −129.8 °C (143 K) and neopentane −16.4 °C (256.8 K). Likewise in xylenes and also dichlorobenzenes the melting point increases in the order meta, ortho and then para. Pyridine has a lower symmetry than benzene hence its lower melting point but the melting point again increases with diazine and triazines. Many cage-like compounds like adamantane and cubane with high symmetry have relatively high melting points.\n\nA high melting point results from a high heat of fusion, a low entropy of fusion, or a combination of both. In highly symmetrical molecules the crystal phase is densely packed with many efficient intermolecular interactions resulting in a higher enthalpy change on melting.\n\nAn attempt to predict the bulk melting point of crystalline materials was first made in 1910 by Frederick Lindemann. The idea behind the theory was the observation that the average amplitude of thermal vibrations increases with increasing temperature. Melting initiates when the amplitude of vibration becomes large enough for adjacent atoms to partly occupy the same space. The Lindemann criterion states that melting is expected when the vibration root mean square amplitude exceeds a threshold value.\n\nAssuming that all atoms in a crystal vibrate with the same frequency \"ν\", the average thermal energy can be estimated using the equipartition theorem as\nwhere \"m\" is the atomic mass, \"ν\" is the frequency, \"u\" is the average vibration amplitude, \"k\" is the Boltzmann constant, and \"T\" is the absolute temperature. If the threshold value of \"u\" is \"ca\" where \"c\" is the Lindemann constant and \"a\" is the atomic spacing, then the melting point is estimated as\nSeveral other expressions for the estimated melting temperature can be obtained depending on the estimate of the average thermal energy. Another commonly used expression for the Lindemann criterion is\nFrom the expression for the Debye frequency for \"ν\", we have\nwhere \"θ\" is the Debye temperature and \"h\" is the Planck constant. Values of \"c\" range from 0.15–0.3 for most materials.\n\nIn February 2011, Alfa Aesar released over 10,000 melting points of compounds from their catalog as open data. This dataset has been used to create a random forest model for melting point prediction which is now freely available. Open melting point data are also available from \"Nature Precedings\". High quality data mined from patents and also models developed with these data were published by Tetko \"et al\".\n\n\n"}
{"id": "40133142", "url": "https://en.wikipedia.org/wiki?curid=40133142", "title": "Mettler Toledo", "text": "Mettler Toledo\n\nMETTLER TOLEDO () is a multinational manufacturer of scales and analytical instruments. It is the largest provider of weighing instruments for use in laboratory, industrial, and food retailing applications. The company also provides various analytical instruments, process analytics instruments, and end-of-line inspection systems. The company operates worldwide; 70% of net sales are derived in equal part from Europe and from the Americas; Asian business is included in the remaining 30%.\n\nAllen DeVilbiss, Jr. (1873-1911) was an inventor who lived in Toledo, Ohio, USA. He became interested in the concept of weighing machines and conceived the idea of an automatic computing pendulum scale. He was able to prove his concept with a local butcher, who realized that customers appreciated the automatic computation which eliminated the risk of overcharging. While his invention gained in popularity, DeVilbiss was not interested in making it a viable business.\n\nIn 1900, Henry Theobald (1868-1924) was fired from his job at the National Cash Register Company. He decided to start his own business, and was convinced that selling the automatic computing scales could be a good business. He solicited additional financial investors and purchased the company from DeVilbiss. On July 10, 1901, the \"Toledo Computing Scale and Cash Register Company\" was incorporated.\n\nBy May 1902, Theobald's company was selling more than 100 cash registers with scales per month. By that time, John H. Patterson, Theobald's boss from NCR, threatened to sue the new company for patent infringements. As an alternative, Patterson offered to purchase all cash register patents and property, along with the stipulation that Theobald would no longer engage in the cash register business. The investors, who worried about the costs of patent infringement litigation, agreed to the sale in June 1902. Since cash registers were no longer part of Theobald's business, he changed the name to \"Toledo Computing Scale Company\". Theobald later coined the phrase \"No Springs, Honest Weight\" as a slogan for the new company.\n\nIn the years that followed, Theobald realized that the weighing scale was the most important part of the retail transaction between the merchant and the customer. He felt that many of his competitors who used spring scale technology, especially Dayton Scale Company, were allowing merchants to cheat their customers by incorrectly calculating the total price of a measured good. He campaigned for more government regulation of weights and measures to eliminate dishonest weighing systems. On October 1, 1907, Massachusetts adopted the first weights and measures laws in the United States.\n\nIn 1912, the company name changed once again, to \"Toledo Scale Company\". Additionally, a new scale line featuring a double pedulum mechanism and a dial face was introduced and was most suited for industrial applications.\n\nIn 1945, Dr. Erhard Mettler, a Swiss engineer, started a precision mechanics company in Küsnacht, Switzerland. He invented the substitution principle with a single-pan balance, capable of being produced in series. Analytical balances with a single weighing pan gradually replaced conventional two-pan balances in the laboratory.\n\nMettler diversified its product line in 1970 with the introduction of its automated titration systems, and the acquisition of balance manufacturer Microwa AG. Mettler acquired its 500-employee competitor August Sauter KG, of Albstadt-Ebingen, Germany, in 1971, for its specialized industrial and retail scales.\n\nIn 1980, Dr. Mettler sold his business to Ciba-Geigy AG. A third pillar - after laboratory and industrial - was created: the retail business. Technological progress had made it possible to advance retail scales to instruments for the management of perishable goods.\n\nIn 1989, Reliance Electric sold the Toledo Scale division to Ciba-Geigy AG. The division was then merged with Mettler Instruments. The merger vastly increased the global scope of the company, which, as a result, operated in 18 countries. Mettler acquired another competitor, Ohaus Corp., in 1990. In 1992, the company was incorporated as \"Mettler Toledo, Inc.\"\n\nIn 1996/97, Mettler Toledo, Inc. was sold by Ciba-Geigy AG to the New York-based AEA Investors Inc., in preparation for a subsequent initial public offering. The initial public offering was completed and began trading on the New York Stock Exchange, under the ticker symbol .\n\nThe resultant company is currently headquartered in Greifensee, Switzerland, with offices for its many brands based around the world.\n\nMettler-Toledo is a global manufacturer and marketer of precision instruments for use in laboratory, industrial and food retailing applications. They are geographically diversified with sales in 2012 derived 34% from Europe, 34% from the Americas, and 32% from Asia and other countries. The Company has an extensive global sales and service organization with approximately 6,000, or approximately one-half, of the employees providing sales and service in 36 countries. The Company has a manufacturing presence in Europe, the United States and China.\n\n1952, the measurement of up to one ten-millionth of a gram became possible with the laboratory balance products. \"The Mettler Balance\" proved to be a catchphrase in the laboratory. Consequently, with the advancement of microprocessor technology over the years, a wide range of products for the laboratory were invented, like automated titrators and thermal analyzers. Mettler-Toledo laboratory products and technology were able to improve processes in research and development, drug discovery and quality control. Application-specific software for balances provides data analysis opportunities to enhance accuracy, productivity and compliance. The most recent product development for the laboratory was the array-based UV/VIS spectrophotometer which encompasses four different units including UV5, UV7, UV5Bio and UV5Nano.\n\nIn 2001, Mettler-Toledo purchased Rainin Instrument, LLC, which is now a wholly owned subsidiary of Mettler-Toledo. Rainin supplies precision manual liquid handling instruments and calibration services worldwide. Under the Rainin brand, METTLER TOLEDO provides advanced liquid handling solutions for life scientists worldwide, offering a wide selection of ergonomic manual pipettes, electronic pipettes, multichannel pipettes, patented BioClean LTS and universal pipette tips. Rainin developed the first complete pipetting solution called Pipetting 360°, which significantly increases the reproducibility of basic research steps. An estimated 3 million scientists have benefited from improved research results and reductions in repetitive strain injuries (RSI) with the ergonomic Rainin LTS pipettes and tips.\n\nThe Mettler-Toledo AutoChem Inc. develops process analytical technology (PAT), automated reactors, and in situ sampling for use in the chemical, pharmaceutical, and academia industries. Specifically, in situ spectroscopy and automated sampling provides continuous analysis of chemical reactions; inline particle size analysis enables crystallization, suspension, and emulsion development with real-time particle size and shape measurements; and automated reactors and reaction calorimetry provides process knowledge to eliminate scale-up and safety incidents. As METTLER TOLEDO built the franchise from a startup operation in the early 1990s, several companies were acquired. These acquisitions included ASI Applied System Inc., Lasentec®, and the Virtual Lab software business from Avantium to assemble the technology platforms required to develop and maintain its market leading position.\n\nThornton, Inc. Process Analytics Division founded in 1964 develops, manufactures, markets, and supplies instrumentation and sensors used for liquid process measurement and control applications.The instrumentation provides pure water treatment measurement & control for the pharmaceutical industry; and parameters of conductivity/resistivity, TOC, pH, DO, dissolved ozone and flow for the semiconductor and power industries. Thornton acquired microbial detection startup Instant BioScan, Inc. in 2015 to become the first manufacturer of instrumentation within the pharmaceutical industry to offer real-time measurement solutions for all three regulated parameters by global pharmacopeias for ultra pure water, adding bioburden analysis to their portfolio.\n\nFounded in 1948, INGOLD specializes in solutions for pH, DO, conductivity, turbidity and CO2 for process analytics applications in chemical, pharmaceutical and food & beverage industries.\n\nThe Company manufactures numerous industrial weighing instruments and related terminals and offer software for the pharmaceutical, chemical, food and other industries. In addition, it manufactures metal detection and other end-of-line product inspection systems used in production and packaging. The Company supplies automatic identification and data capture solutions, which integrate in-motion weighing, dimensioning and identification technologies for transport, shipping and logistics customers. The Company also offers heavy industrial scales and related software.\n\nMettler-Toledo Cargoscan Dimensioning solutions are used by international transport companies, small carriers, warehouses and distribution centres around the world. Systems are all approved according to international weights and measures standards, ensuring high instrument performance. It has been shown that laser range finder technology provides high accuracy in the industry making an impact on a production's revenue.\n\nHi-Speed division joined Mettler-Toledo in 1981 to provide weighing solutions for laboratory, industrial, and food retailing applications. The Product Inspection Team provides the checkweighers, metal detection devices, and other end-of-line inspection systems for local and multinational companies worldwide\n\nSafeline started in 1989 with the launch of the world’s first digital industrial metal detector, and was acquired by METTLER TOLEDO in 1997. The Safeline product was an advanced metal detector the used digital technology for increased sensitivity and accuracy.\n\nTo expand the detection solutions, METTLER TOLEDO acquired AVS Raytech and AVS Metrology in 2000. AVS Raytech manufactured x-ray inspection equipment. The x-ray inspection equipment combined with the detection products allowed companies to identify multiple contaminants as well as verifying product integrity and portion control.\n\nMettler-Toledo CI-Vision is a wholly owned subsidiary of Mettler-Toledo Inc. The Mettler-Toledo Product Inspection Group, consisting of CI-Vision, Hi-Speed and Safeline, is a superior supplier of in-line checkweighers, metal detectors, machine vision systems and x-ray inspection systems.\n\nMettler-Toledo actively participates in the United Way organization.The company offers employees opportunities to contribute financially or by supporting volunteer activity during paid company hours.\n"}
{"id": "1181990", "url": "https://en.wikipedia.org/wiki?curid=1181990", "title": "Mighty Ducks (TV series)", "text": "Mighty Ducks (TV series)\n\nMighty Ducks (also known as Mighty Ducks: The Animated Series) is an American animated television series that aired on ABC and the syndicated programming block \"The Disney Afternoon\", the last show produced by the block, in the fall of 1996. The show was inspired by the live-action Mighty Ducks films and the NHL team, the Anaheim Ducks. Twenty-six episodes were produced in total. The series most recently aired on Toon Disney but was removed from schedules in November 2004.\n\nThe series' main theme, composed by Carl Swander Johnson, is performed by Mickey Thomas of Jefferson Starship and Elvin Bishop fame.\n\nDisneyQuest, an \"Indoor Interactive Theme Park\" located in the Disney Springs area of the Walt Disney World Resort, had an attraction loosely based on the program called \"Mighty Ducks Pinball Slam\". Wildwing was the only character from the show featured, first as a life-size cutout at the front of the queue and then again as the goalie in the game.\n\nIn another universe exists a planet populated entirely by humanoid ducks. Dubbed \"Puckworld\" by its inhabitants, it is an icy planet, perfectly suited to the Ducks' favorite pastime, hockey. For the citizens of Puckworld, hockey was not simply a sport, but a way of life, occupying virtually every aspect of day-to-day existence.\n\nLegend has it that centuries ago, during an invasion by a reptilian race called Saurians, a duck named Drake DuCaine became the planet's savior over the Saurians' Overlords. The legend tells that DuCaine did so with a high-tech goalie mask. With it, DuCaine sent the Saurians to a mysterious \"Dimensional Limbo\".\n\nThe last of the Saurians escape Dimensional Limbo and returns to Puckworld with an armada of robotic attack ships. The group of four is led by the last of the Saurian Overlords, Lord Dragaunus, who is assisted by his minions Siege, Chameleon and Wraith. They invade the planet and enslave the people of Puckworld. After some time, a resistance is formed by Canard, who has found The Mask of Drake DuCaine. With it, the wearer of the Mask could see through the Saurians' invisibility cloaks. Canard forms a band of Ducks to fight Dragaunus. The members of his team consists of Wildwing, Nosedive, Tanya, Duke, Mallory and Grin. They go on a mission to destroy Dragaunus's fortress the Master Tower and free the planet from the Saurians' control. While the mission is successful, Dragaunus and his forces manage to escape in their ship, the Raptor. The Saurians open up a dimensional gateway to escape through, but Canard and the others follow him into the portal with the Aerowing, intent on stopping them.\n\nDragaunus attempts to get rid of the Ducks inside the portal by attacking them with an electromagnetic worm that will grow until it can swallow the Aerowing. In a desperate attempt to get rid of the worm, Canard sacrifices his own life by throwing himself to the worm. Before doing so, however, Canard gave the Mask, and leadership of the team, to Wildwing Flashblade, his best friend. Both the Raptor and the Aerowing leave the portal and enter a different dimension, landing in the Earth city of Anaheim, California. The Ducks meet Phil Palmfeather, a human who becomes their manager and makes them a legitimate NHL team. Their arena, only known as The Pond in the show, has a hockey rink that doubles as a landing pad for the Aerowing above and has a formal HQ below. On Earth, the Ducks and Dragaunus continue their fight, with Dragaunus's plans of conquest often curtailed by damage to the Raptor's power source and his efforts to find a new source of power, although there are other villains that also challenge the six Ducks.\n\n\n\n\n\n\nA direct-to-video feature film titled \"Mighty Ducks the Movie: The First Face-Off\" was released on VHS on April 8, 1997. It comprises three episodes of the series (\"The First Face-Off\" Parts 1 and 2, and \"Duck Hard\") edited into one continuous movie.\n\n\n\n"}
{"id": "19430763", "url": "https://en.wikipedia.org/wiki?curid=19430763", "title": "Mountain Conservation Trust of Georgia", "text": "Mountain Conservation Trust of Georgia\n\nThe Mountain Conservation Trust of Georgia (MCTGA) is a 501(c)3 nonprofit land trust that promotes land protection, collaborative partnerships and education in order to conserve natural resources, especially the mountains and foothills of North Georgia, founded in 1991.\n\nIt has since been accredited by the Land Trust Alliance's Accreditation Commission in 2008. Its headquarters are in Jasper, Georgia, and it has a permanent protection of approximately .\n"}
{"id": "43291919", "url": "https://en.wikipedia.org/wiki?curid=43291919", "title": "Noncommutative projective geometry", "text": "Noncommutative projective geometry\n\nIn mathematics, noncommutative projective geometry is a noncommutative analog of projective geometry in the setting of noncommutative algebraic geometry.\n\n\nBy definition, the Proj of a graded ring \"R\" is the quotient category of the category of finitely generated graded modules over \"R\" by the subcategory of torsion modules. If \"R\" is a commutative Noetherian graded ring generated by degree-one elements, then the Proj of \"R\" in this sense is equivalent to the category of coherent sheaves on the usual Proj of \"R\". Hence, the construction can be thought of as a generalization of the Proj construction for a commutative graded ring.\n\n\n"}
{"id": "14318820", "url": "https://en.wikipedia.org/wiki?curid=14318820", "title": "Nordic swan", "text": "Nordic swan\n\nThe Nordic Ecolabel or Nordic swan is the official sustainability ecolabel for the Nordic countries, introduced by the Nordic Council of Ministers in 1989. This is done by a voluntary license system where the applicant agrees to follow a certain criteria set outlined by the Nordic Ecolabelling in cooperation with stakeholders. These criteria include environmental, quality and health arguments. The criteria levels promote products and services belonging to the most environmentally sound and take into account factors such as free trade and proportionality (cost vs. benefits).\n\nThe Nordic Ecolabel now covers 67 different product groups, from hand soap to furniture to hotels. Products must verify compliance using methods such as samples from independent laboratories, certificates and control visits. The label is usually valid for three years, after which the criteria are revised and the company must reapply for a license.\n\nThe Nordic Ecolabel first appeared in the United States through a small offering of Nordic products. KCK Industries recognized the value of branding quality eco-friendly products and through a partnership with ABENA introduced Bambo Nature, an environmentally friendly baby diaper. The success of this offering has led to the expansion of the ecolabel in the U.S.\n\nNorway and Sweden implemented the Nordic swan in 1989, Finland in 1990, Iceland in 1991 and Denmark in 1998.\n\n"}
{"id": "5900871", "url": "https://en.wikipedia.org/wiki?curid=5900871", "title": "Onium compound", "text": "Onium compound\n\nIn chemistry, an onium ion is a cation formally obtained by the protonation of mononuclear parent hydride of a pnictogen (group 15 of the periodic table), chalcogen (group 16), or halogen (group 17). The oldest-known onium ion, and the namesake for the class, is ammonium, , the protonated derivative of ammonia, .\n\nThe name onium is also used for cations that would result from the substitution of hydrogen atoms in those ions by other groups, such as organic radicals, or halogens; such as tetraphenylphosphonium, . The substituent groups may be divalent or trivalent, yielding ions such as iminium and nitrilium.\n\nA simple onium ion has a charge of +1. A larger ion that has two onium ion subgroups is called a double onium ion, and has a charge of +2. A triple onium ion has a charge of +3, and so on.\n\nCompounds of an onium cation and some other negative ion are known as onium compounds or onium salts.\n\nOnium ions and onium compounds are inversely analogous to -ate ions and ate complexes:\n\n\n\n\n\n\n\n\n\n\n\n\nThe extra bond is added to a less-common parent hydride, a carbene analog, typically named \"-ene\" or \"-ylene\", which is neutral with 2 fewer bonds than the more-common hydride, typically named \"-ane\" or \"-ine\".\n\n\n"}
{"id": "7135260", "url": "https://en.wikipedia.org/wiki?curid=7135260", "title": "Pan-European Oil Pipeline", "text": "Pan-European Oil Pipeline\n\nThe Pan-European Oil Pipeline (PEOP) is a proposed oil pipeline from Constanţa in Romania via Serbia and Croatia to Rijeka and from there through Slovenia to Trieste in Italy. The aim of the pipeline is to bypass Turkish straits in the transportation of Russian and Caspian oil to Central Europe. In Trieste the pipeline will be connected with the Transalpine Pipeline, running to Austria and Germany.\n\nThe project was originally proposed in 2002. Signing of the memorandum of understanding on the construction of the pipeline was several times delayed until on 3 April 2007 it was signed by officials of Croatia, Italy, Romania, Serbia, and Slovenia during an energy forum in Zagreb. On 22 April 2008 Romanian, Serbian and Croatian companies signed an agreement establishing the Pan-European Oil Pipeline Project Development Company (PEOP PDC). On 10 July 2008 Shareholders adopted the Statute and appointing the Managing Board of the PEOP PDC Plc.\n\nThe national governments of Romania, Serbia and Croatia are favorable about the project. Most engaged is the President of Romania, Traian Băsescu, who has cited a study estimating the benefits of the project for Romania over 20 years of operation in the range between US$2.27 to 4.39 billion. The Government of Serbia is also politically motivated so as to decrease its dependence on crude oil supply from Croatia.\n\nIn 2006, the Government of Slovenia did not support the project because the long stretch would pass the environmentally sensitive Karst terrain and no national interest exists regarding oil supply. Nevertheless, Slovenian representatives in 2009 expressed support for the project.\n\nOn January 15, 2010, JANAF decided to freeze its involvement in PEOP. The Romanian and Serbian companies responded by saying they would still build the pipeline from the Black Sea to the Pančevo refinery.\n\nThe long pipeline is expected to cost about €3.5 billion. The capacity of the pipeline will be . It is planned to be operational by 2012.\n\nThe pipeline project is developed by the London-registered Pan-European Oil Pipeline Project Development Company, comprising Romanian companies Conpet Ploiești and Oil Terminal Constanța, Serbian company Transnafta and Croatian company JANAF. One of the main purposes of the company is promotion of the Pan European Oil Pipeline and subsequently attracting new investors. Italian and Slovenian companies are invited to participate in the project.\n\n\n"}
{"id": "51302790", "url": "https://en.wikipedia.org/wiki?curid=51302790", "title": "Particle density (particle count)", "text": "Particle density (particle count)\n\nParticle density, in the context of particle counts, is a measurement of the number of particles in a spatial unit of a particle-bearing medium. Where the spatial unit is one of volume, the medium is likely to a fluid; in another class of cases, the unit is one of area of a surface, and the particles rest on or adhere to that surface.\n"}
{"id": "2577563", "url": "https://en.wikipedia.org/wiki?curid=2577563", "title": "Petroleum Revenue Oversight and Control Committee", "text": "Petroleum Revenue Oversight and Control Committee\n\nThe Petroleum Revenue Oversight and Control Committee (\"Collège de Contrôle et de Surveillance des Ressources Pétrolières)\" is a Chadian government watchdog committee in charge of overseeing the government's use of petrol reserves and revenues.\n\nThe committee is composed of:\nSource: Article 6 and 7 of establishing decree. \n\nThe committee exercises control over the oil revenues from the oil fields at Kome, Miandoum and Bolobo.\n\n\n"}
{"id": "17837413", "url": "https://en.wikipedia.org/wiki?curid=17837413", "title": "Pressure shale", "text": "Pressure shale\n\nPressure shale is shale that has been exposed to high pressures that causes it to fracture, usually into large concave pieces. In mud logging, watching for this type of shale is one method of monitoring for the possibility of drilling into high pressures that might cause a blowout.\n"}
{"id": "2792596", "url": "https://en.wikipedia.org/wiki?curid=2792596", "title": "Quintrix", "text": "Quintrix\n\nQuintrix is a name given to a flat and wide television tube made by Panasonic. Quintrix tubes were first introduced to the market in 1974. The word originates from the Latin word \"quintum\", which means \"fifth\". So far there are three models of Quintrix available: \n\nThe first Quintrix cathode ray tubes featured a prefocus lens that reduced beam diffusion, giving a sharper picture.\n\nManufactured in Malaysia and also in Wales with an MX-6 core, the Quintrix model was the standard television type for Hong Kong's etv project in 1999.\n\n"}
{"id": "56880784", "url": "https://en.wikipedia.org/wiki?curid=56880784", "title": "Relative temperature index", "text": "Relative temperature index\n\nThe Relative temperature index (RTI) is a characteristic parameter related to the thermal degradation of plastic materials.\n\nThe RTI is part of the thermal-aging program of the UL 746B standard from UL.\n\nDuring the process of the UL 746B program, the degradation of certain properties of the material like dielectric and mechanic strength, is investigated with regards to thermal-aging. The RTI is the temperature in degrees C, at which the properties have decreased to 50 percent of their initial value after a long-term exposure to this temperature.\n\nThough the RTI is an index, it is given in Celsius units.\nThe UL 746B standard distinguishes between three sub-categories of the RTI:\n"}
{"id": "37166421", "url": "https://en.wikipedia.org/wiki?curid=37166421", "title": "Rue de la Commune", "text": "Rue de la Commune\n\nRue de la Commune () is a road in Old Montreal which is well used both by Montrealers and by tourists, since it is the home of the Pointe-à-Callière Museum and the Old Port of Montreal. The road follows the original shore of the Saint Lawrence River. The buildings along the north side of the road are former commercial buildings.\n\nIn 1651, the governor of Montréal, Paul Chomedey de Maisonneuve granted land to Jean de Saint-Père to be used as pasture. This 'commune' (commons) is a strip of land one arpent wide with 40 arpents of shoreline.\n\nThe river bank was the site of a tow path, and became a road, lined with grain elevators from 1879. A proposed elevated highway along the river over the Rue de la Commune spurred a movement to preserve the district. Dutch-born architect and urban planner Daniel van Ginkel played a major role in saving the district from destruction during the early 1960s. As assistant director of the city of Montreal's newly formed planning department, he persuaded authorities to abandon plans for an expressway that would have cut through the old city. In 1964, most of Old Montreal was classified as an historic district\n\nIn 1970, the road was renamed from \"rue des Commissaires\", in memory of early colonial days. After the Port of Montreal was moved, the area became a recreational area in 1992.\n"}
{"id": "1029755", "url": "https://en.wikipedia.org/wiki?curid=1029755", "title": "Solar azimuth angle", "text": "Solar azimuth angle\n\nThe solar azimuth angle is the azimuth angle of the Sun's position. This horizontal coordinate defines the Sun's relative direction along the local horizon, whereas the solar zenith angle (or its complementary angle solar elevation) defines the Sun's apparent altitude.\n\nThere are several conventions for the solar azimuth; however, it is traditionally defined as the angle between a line due south and the shadow cast by a vertical rod on Earth. This convention states the angle is positive if the line is east of south and negative if it is west of south. For example, due east would be 90° and due west would be -90°. Another convention is the reverse; it also has the origin at due south, but measures angles clockwise, so that due east is now negative and west now positive.\n\nHowever, despite tradition, the most commonly accepted convention for analyzing solar irradiation, e.g. for solar energy applications, is clockwise from due north, so east is 90°, south is 180°, and west is 270°. This is the definition used by NREL in their solar position calculators and is also the convention used in the formulas presented here. However, Landsat photos and other USGS products, while also defining azimuthal angles relative to due north, take counterclockwise angles as negative.\n\n\"Note: Both of these formulas assume the north-clockwise convention.\" The solar azimuth angle can be calculated to a good approximation with the following formula, however angles should be interpreted with care because the inverse sine, i.e. x = sin(y) or x = arcsin(y), has multiple solutions, only one of which will be correct.\nThe following formulas can also be used to approximate the solar azimuth angle, but these formulas use cosine, so the azimuth angle as shown by a calculator will always be positive, and should be interpreted as the angle between zero and 180 degrees when the hour angle, \"h\", is negative (morning) and the angle between 180 and 360 degrees when the hour angle, \"h\", is positive (afternoon). (These two formulas are equivalent if you assume the \"solar elevation angle\" approximation formula).\n\nThe formulas use the following terminology:\n\n\n"}
{"id": "14388299", "url": "https://en.wikipedia.org/wiki?curid=14388299", "title": "Space derby", "text": "Space derby\n\nThe space derby is a racing event for Cub Scouts in the Boy Scouts of America that is similar to the pinewood derby car race. Cub Scouts (the young-age division of the Boy Scouts of America) race miniature balsa wood gliders that are propelled by a rubber band and propeller.\n\nThe space derby kit consists of a balsa wood block, propeller assembly, rubber bands, plastic sheet (for fins), and a mounting bracket. The wood block comes out of the box with a drilled out center hole. The block is carved into the desired shape, sanded and painted. The mounting bracket and fins can be added either before or after painting. The rubber band is inserted through the center hole with one end on the propeller and the other held in a cross-piece at the rear.\n\nThe completed gliders are wound up with as many as 100 or more turns on the propeller and suspended on a string from a separate bracket with the propeller held in place. Two, three or four string lanes are typically used. The gliders are held in place and launched when the mechanism releases the propeller.\n\n"}
{"id": "81511", "url": "https://en.wikipedia.org/wiki?curid=81511", "title": "Stall (fluid dynamics)", "text": "Stall (fluid dynamics)\n\nIn fluid dynamics, a stall is a reduction in the lift coefficient generated by a foil as angle of attack increases. This occurs when the critical angle of attack of the foil is exceeded. The critical angle of attack is typically about 15 degrees, but it may vary significantly depending on the fluid, foil, and Reynolds number.\n\nStalls in fixed-wing flight are often experienced as a sudden reduction in lift as the pilot increases the wing's angle of attack and exceeds its critical angle of attack (which may be due to slowing down below stall speed in level flight). A stall does not mean that the engine(s) have stopped working, or that the aircraft has stopped moving—the effect is the same even in an unpowered glider aircraft. Vectored thrust in manned and unmanned aircraft is used to surpass the stall limit, thereby giving rise to post-stall technology.\n\nBecause stalls are most commonly discussed in connection with aviation, this article discusses stalls as they relate mainly to aircraft, in particular fixed-wing aircraft. The principles of stall discussed here translate to foils in other fluids as well.\n\nA stall is a condition in aerodynamics and aviation wherein the angle of attack increases beyond a certain point such that lift begins to decrease. The angle at which this occurs is called the \"critical angle of attack\". This critical angle is dependent upon the airfoil section or profile of the wing, its planform, its aspect ratio, and other factors, but is typically in the range of 8 to 20 degrees relative to the incoming wind (\"relative wind\") for most subsonic airfoils. The critical angle of attack is the angle of attack on the lift coefficient versus angle-of-attack curve at which the maximum lift coefficient occurs.\n\nFlow separation begins to occur at small angles of attack while attached flow over the wing is still dominant. As angle of attack increases, the separated regions on the top of the wing increase in size and hinder the wing's ability to create lift. At the critical angle of attack, separated flow is so dominant that additional increases in angle of attack produce \"less\" lift and more drag.\n\nA fixed-wing aircraft during a stall may experience buffeting or a change in attitude. Most aircraft are designed to have a gradual stall with characteristics that will warn the pilot and give them time to react. For example, an aircraft that does not buffet before the stall may have an audible alarm or a stick shaker installed to simulate the feel of a buffet by vibrating the stick fore and aft. The critical angle of attack in steady straight and level flight can be attained only at low airspeed. Attempts to increase the angle of attack at higher airspeeds can cause a high-speed stall or may merely cause the aircraft to climb.\n\nAny yaw of the aircraft as it enters the stall regime can result in autorotation, which is also sometimes referred to as a \"spin\". As air no longer flows smoothly over the wings during a stall, aileron control of roll becomes less effective, whilst simultaneously the tendency for the ailerons to generate adverse yaw increases. This characteristic increases the lift from the advancing wing and increases the likelihood of the aircraft entering into a spin.\n\nThe graph shows that the greatest amount of lift is produced as the critical angle of attack is reached (which in early-20th century aviation was called the \"burble point\"). This angle is 17.5 degrees in this case, but it varies from airfoil to airfoil. In particular, for aerodynamically thick airfoils (thickness to chord ratios of around 10%), the critical angle is higher than with a thin airfoil of the same camber. Symmetric airfoils have lower critical angles (but also work efficiently in inverted flight). The graph shows that, as the angle of attack exceeds the critical angle, the lift produced by the airfoil decreases.\n\nThe information in a graph of this kind is gathered using a model of the airfoil in a wind tunnel. Because aircraft models are normally used, rather than full-size machines, special care is needed to make sure that data is taken in the same Reynolds number regime (or scale speed) as in free flight. The separation of flow from the upper wing surface at high angles of attack is quite different at low Reynolds number from that at the high Reynolds numbers of real aircraft. High-pressure wind tunnels are one solution to this problem. In general, steady operation of an aircraft at an angle of attack above the critical angle is not possible because, after exceeding the critical angle, the loss of lift from the wing causes the nose of the aircraft to fall, reducing the angle of attack again. This nose drop, independent of control inputs, indicates the pilot has actually stalled the aircraft.\n\nThis graph shows the stall angle, yet in practice most pilot operating handbooks (POH) or generic flight manuals describe stalling in terms of airspeed. This is because all aircraft are equipped with an airspeed indicator, but fewer aircraft have an angle of attack indicator. An aircraft's stalling speed is published by the manufacturer (and is required for certification by flight testing) for a range of weights and flap positions, but the stalling angle of attack is not published.\n\nAs speed reduces, angle of attack has to increase to keep lift constant until the critical angle is reached. The airspeed at which this angle is reached is the (1g, unaccelerated) stalling speed of the aircraft in that particular configuration. Deploying flaps/slats decreases the stall speed to allow the aircraft to take off and land at a lower speed.\n\nA fixed-wing aircraft can be made to stall in any pitch attitude or bank angle or at any airspeed but deliberate stalling is commonly practiced by reducing the speed to the unaccelerated stall speed, at a safe altitude. Unaccelerated (1g) stall speed varies on different fixed-wing aircraft and is represented by colour codes on the airspeed indicator. As the plane flies at this speed, the angle of attack must be increased to prevent any loss of altitude or gain in airspeed (which corresponds to the stall angle described above). The pilot will notice the flight controls have become less responsive and may also notice some buffeting, a result of the turbulent air separated from the wing hitting the tail of the aircraft.\n\nIn most light aircraft, as the stall is reached, the aircraft will start to descend (because the wing is no longer producing enough lift to support the aircraft's weight) and the nose will pitch down. Recovery from the stall involves lowering the aircraft nose, to decrease the angle of attack and increase the air speed, until smooth air-flow over the wing is restored. Normal flight can be resumed once recovery is complete. The maneuver is normally quite safe, and, if correctly handled, leads to only a small loss in altitude (20–30 m/50–100 ft). It is taught and practised in order for pilots to recognize, avoid, and recover from stalling the aircraft. A pilot is required to demonstrate competency in controlling an aircraft during and after a stall for certification in the United States, and it is a routine maneuver for pilots when getting to know the handling of an unfamiliar aircraft type. The only dangerous aspect of a stall is a lack of altitude for recovery.\n\nA special form of asymmetric stall in which the aircraft also rotates about its yaw axis is called a spin. A spin can occur if an aircraft is stalled and there is an asymmetric yawing moment applied to it. This yawing moment can be aerodynamic (sideslip angle, rudder, adverse yaw from the ailerons), thrust related (p-factor, one engine inoperative on a multi-engine non-centreline thrust aircraft), or from less likely sources such as severe turbulence. The net effect is that one wing is stalled before the other and the aircraft descends rapidly while rotating, and some aircraft cannot recover from this condition without correct pilot control inputs (which must stop yaw) and loading. A new solution to the problem of difficult (or impossible) stall-spin recovery is provided by the ballistic parachute recovery system.\n\nThe most common stall-spin scenarios occur on takeoff (departure stall) and during landing (base to final turn) because of insufficient airspeed during these maneuvers. Stalls also occur during a go-around manoeuvre if the pilot does not properly respond to the out-of-trim situation resulting from the transition from low power setting to high power setting at low speed. Stall speed is increased when the wing surfaces are contaminated with ice or frost creating a rougher surface, and heavier airframe due to ice accumulation.\n\nStalls occur not only at slow airspeed, but at any speed when the wings exceed their critical angle of attack. Attempting to increase the angle of attack at 1g by moving the control column back normally causes the aircraft to climb. However, aircraft often experience higher g-forces, such as when turning steeply or pulling out of a dive. In these cases, the wings are already operating at a higher angle of attack to create the necessary force (derived from lift) to accelerate in the desired direction. Increasing the g-loading still further, by pulling back on the controls, can cause the stalling angle to be exceeded, even though the aircraft is flying at a high speed. These \"high-speed stalls\" produce the same buffeting characteristics as 1g stalls and can also initiate a spin if there is also any yawing.\n\nOne symptom of an approaching stall is slow and sloppy controls. As the speed of the aircraft decreases approaching the stall, there is less air moving over the wing, and, therefore, less air will be deflected by the control surfaces (ailerons, elevator, and rudder) at this lower speed. Some buffeting may also be felt from the turbulent flow above the wings as the stall is reached. The stall warning will sound, if fitted, in most aircraft 5 to 10 knots above the stall speed.\n\nDifferent aircraft types have different stalling characteristics. A benign stall is one where the nose drops gently and the wings remain level throughout. Slightly more demanding is a stall in which one wing stalls slightly before the other, causing that wing to drop sharply, with the possibility of entering a spin. A dangerous stall is one in which the nose rises, pushing the wing deeper into the stalled state and potentially leading to an unrecoverable \"deep stall\". This can occur in some T-tailed aircraft wherein the turbulent airflow from the stalled wing can blanket the control surfaces at the tail.\n\nStalls depend only on angle of attack, not airspeed. However, the more slowly an airplane goes, the greater the angle of attack it needs to produce lift equal to the aircraft's weight. As the speed decreases further, at some point this angle will be equal to the critical (stall) angle of attack. This speed is called the \"stall speed\". An aircraft flying at its stall speed cannot climb, and an aircraft flying below its stall speed cannot stop descending. Any attempt to do so by increasing angle of attack, without first increasing airspeed, will result in a stall.\n\nThe actual stall speed will vary depending on the airplane's weight, altitude, configuration, and vertical and lateral acceleration. Guidelines for the case of zero acceleration are provided by the following V speeds:\n\nOn an airspeed indicator, the bottom of the white arc indicates V at maximum weight, while the bottom of the green arc indicates V at maximum weight. While an aircraft's V speed is computed by design, its V and V speeds must be demonstrated empirically by flight testing.\n\nThe normal stall speed, specified by the V values above, always refers to straight and level flight, where the load factor is equal to 1g. However, if the aircraft is turning or pulling up from a dive, additional lift is required to provide the vertical or lateral acceleration, and so the stall speed is higher. An accelerated stall is a stall that occurs under such conditions.\n\nConsidering, for example, a banked turn, the lift required is equal to the weight of the aircraft plus extra lift to provide the centripetal force necessary to perform the turn; that is:\n\nwhere:\n\nTo achieve the extra lift, the lift coefficient, and so the angle of attack, will have to be higher than it would be in straight and level flight at the same speed. Therefore, given that the stall always occurs at the same critical angle of attack, by increasing the load factor (e.g., by tightening the turn) such critical angle – and the stall – will be reached with the airspeed remaining well above the normal stall speed, that is:\n\nwhere:\n\nThe table that follows gives some examples of the relation between the angle of bank and the square root of the load factor. It derives from the trigonometric relation (secant) between formula_2 and formula_4.\n\nFor example, in a turn with bank angle of 45°, V is 19% higher than V.\n\nIt should be noted that, according to Federal Aviation Administration (FAA) terminology, the above example illustrates a so-called turning flight stall, while the term \"accelerated\" is used to indicate an \"accelerated turning stall\" only, that is, a turning flight stall where the airspeed decreases at a given rate.\n\nA notable example of air accident involving a low-altitude turning flight stall is the 1994 Fairchild Air Force Base B-52 crash.\n\nDynamic stall is a non-linear unsteady aerodynamic effect that occurs when airfoils rapidly change the angle of attack. The rapid change can cause a strong vortex to be shed from the leading edge of the aerofoil, and travel backwards above the wing. The vortex, containing high-velocity airflows, briefly increases the lift produced by the wing. As soon as it passes behind the trailing edge, however, the lift reduces dramatically, and the wing is in normal stall.\n\nDynamic stall is an effect most associated with helicopters and flapping wings, though also occurs in wind turbines, and due to gusting airflow. During forward flight, some regions of a helicopter blade may incur flow that reverses (compared to the direction of blade movement), and thus includes rapidly changing angles of attack. Oscillating (flapping) wings, such as those of insects—including the most famous one, the bumblebee—may rely almost entirely on dynamic stall for lift production, provided the oscillations are fast compared to the speed of flight, and the angle of the wing changes rapidly compared to airflow direction.\n\nStall delay can occur on airfoils subject to a high angle of attack and a three-dimensional flow. When the angle of attack on an airfoil is increasing rapidly, the flow will remain substantially attached to the airfoil to a significantly higher angle of attack than can be achieved in steady-state conditions. As a result, the stall is delayed momentarily and a lift coefficient significantly higher than the steady-state maximum is achieved. The effect was first noticed on propellers.\n\nA \"deep stall\" (or \"super-stall\") is a dangerous type of stall that affects certain aircraft designs, notably jet aircraft with a T-tail configuration and rear-mounted engines. In these designs, the turbulent wake of a stalled main wing, nacelle-pylon wakes and the wake from the fuselage \"blanket\" the horizontal stabilizer, rendering the elevators ineffective and preventing the aircraft from recovering from the stall. Taylor states T-tail propeller aircraft, unlike jet aircraft, do not usually require a stall recovery system during stall flight testing due to increased airflow over the wing root from the prop wash. Nor do they have rear mounted nacelles which can contribute substantially to the problem. The A400M was fitted with a vertical tail booster for some flight tests in case of deep stall.\n\nTrubshaw gives a broad definition of deep stall as penetrating to such angles of attack formula_11 that pitch control effectiveness is reduced by the wing and nacelle wakes. He also gives a definition that relates deep stall to a locked-in condition where recovery is impossible. This is a single value of formula_11, for a given aircraft configuration, where there is no pitching moment, i.e. a trim point.\n\nTypical values both for the range of deep stall, as defined above, and the locked-in trim point are given for the Douglas DC-9 Series 10 by Schaufele. These values are from wind tunnel tests for an early design. The final design had no locked in trim point so recovery from the deep stall region was possible, as required to meet certification rules. Normal stall beginning at the 'g' break (sudden decrease of the vertical load factor) was at 18 degrees formula_11, deep stall started at about 30 degrees and the locked-in unrecoverable trim point was at 47 degrees.\n\nThe very high formula_11 for a deep stall locked-in condition occurs well beyond the normal stall but can be attained very rapidly as the aircraft is unstable beyond the normal stall and requires immediate action to arrest it. The loss of lift causes high sink rates which, together with the low forward speed at the normal stall, give a high formula_11 with little or no rotation of the aircraft. BAC 1-11 G-ASHG, during stall flight tests before the type was modified to prevent a locked-in deep stall condition, descended at over and struck the ground in a flat attitude moving only forward after initial impact. Sketches which show how the wing wake blankets the tail may be misleading if they imply that deep stall requires a high body angle. Taylor and Ray show how the aircraft attitude in the deep stall is relatively flat, even less than during the normal stall, with very high negative flight path angles.\n\nEffects similar to deep stall had been known to occur on some aircraft designs before the term was coined. A prototype Gloster Javelin (serial \"WD808\") was lost in a crash on 11 June 1953, to a \"locked in\" stall However, Waterton states that the trimming tailplane was found to be the wrong way for recovery. Low speed handling tests were being done to assess a new wing. Handley Page Victor \"XL159\" was lost to a \"stable stall\" on 23 March 1962. It had been clearing the fixed droop leading edge with the test being stall approach, landing configuration, C of G aft. The brake parachute had not been streamed as it may have hindered rear crew escape.\n\nThe name \"deep stall\" first came into widespread use after the crash of the prototype BAC 1-11 G-ASHG on 22 October 1963, which killed its crew. This led to changes to the aircraft, including the installation of a stick shaker (see below) to clearly warn the pilot of an impending stall. Stick shakers are now a standard part of commercial airliners. Nevertheless, the problem continues to cause accidents; on 3 June 1966, a Hawker Siddeley Trident (G-ARPY), was lost to deep stall; deep stall is suspected to be cause of another Trident (the British European Airways Flight 548 \"G-ARPI\") crash – known as the \"Staines Disaster\" – on 18 June 1972 when the crew failed to notice the conditions and had disabled the stall recovery system. On 3 April 1980, a prototype of the Canadair Challenger business jet crashed after initially entering a deep stall from 17,000 ft and having both engines flame-out. It recovered from the deep stall after deploying the anti-spin parachute but crashed after being unable to jettison the chute or relight the engines. One of the test pilots was unable to escape from the aircraft in time and was killed. On the 26 July 1993, a Canadair CRJ-100 was lost in flight testing due to a deep stall. It has been reported that a Boeing 727 entered a deep stall in a flight test, but the pilot was able to rock the airplane to increasingly higher bank angles until the nose finally fell through and normal control response was recovered. A 727 accident on 1 December 1974, has also been attributed to a deep stall. The crash of West Caribbean Airways Flight 708 in 2005 was also attributed to a deep stall.\n\nReports on the crash of Air France Flight 447 have stated that the accident involved a deep stall entered at and continued for more than three minutes until impact, but this was a steady state conventional stall because the aircraft (an Airbus A330) did not have a T-tail.\n\nCanard-configured aircraft are also at risk of getting into a deep stall. Two Velocity aircraft crashed due to locked-in deep stalls. Testing revealed that the addition of leading-edge cuffs to the outboard wing prevented the aircraft from getting into a deep stall. The Piper Advanced Technologies PAT-1, N15PT, another canard-configured aircraft, also crashed in an accident attributed to a deep stall. Wind tunnel testing of the design at the NASA Langley Research Center showed that it was vulnerable to a deep stall.\n\nIn the early 1980s, a Schweizer SGS 1-36 sailplane was modified for NASA's controlled deep-stall flight program.\n\nAircraft with a swept wing suffer from a particular form of stalling behaviour at low speed. At high speed the airflow over the wing tends to progress directly along the chord, but as the speed is reduced a sideways component due to the angle of the leading edge has time to build up. Airflow at the root is affected only by the angle of the wing, but at a point further along the span, the airflow is affected both by the angle as well as any sideways component of the airflow from the air closer to the root. This results in a pattern of airflow that is progressively \"sideways\" as one moves toward the wingtip.\n\nAs it is only the airflow along the chord that contributes to lift, this means that the wing begins to develop less lift at the tip than the root. In extreme cases, this can lead to the wingtip entering stall long before the wing as a whole. In this case the average lift of the wing as a whole moves forward; the inboard sections are continuing to generate lift and are generally in front of the center of gravity (C of G), while the tips are no longer contributing and are behind the C of G. This produces a strong nose-up pitch in the aircraft, which can lead to more of the wing stalling, the lift moving further forward, and so forth. This chain reaction is considered very dangerous and was known as the pitch-up.\n\nTip stall can be prevented in a number of ways, at least one of which is found on almost all modern aircraft. An early solution was the addition of wing fences to re-direct sideways moving air back towards the rear of the wing. A similar solution is the dog-tooth notch seen on some aircraft, like the Avro Arrow. A more common modern solution is to use some degree of washout.\n\nFixed-wing aircraft can be equipped with devices to prevent or postpone a stall or to make it less (or in some cases more) severe, or to make recovery easier.\n\nStall warning systems often involve inputs from a broad range of sensors and systems to include a dedicated angle of attack sensor.\n\nBlockage, damage, or inoperation of stall and angle of attack (AOA) probes can lead to unreliability of the stall warning, and cause the stick pusher, overspeed warning, autopilot, and yaw damper to malfunction.\n\nIf a forward canard is used for pitch control, rather than an aft tail, the canard is designed to meet the airflow at a slightly greater angle of attack than the wing. Therefore, when the aircraft pitch increases abnormally, the canard will usually stall first, causing the nose to drop and so preventing the wing from reaching its critical AOA. Thus, the risk of main wing stalling is greatly reduced. However, if the main wing stalls, recovery becomes difficult, as the canard is more deeply stalled and angle of attack increases rapidly.\n\nIf an aft tail is used, the wing is designed to stall before the tail. In this case, the wing can be flown at higher lift coefficient (closer to stall) to produce more overall lift.\n\nMost military combat aircraft have an angle of attack indicator among the pilot's instruments, which lets the pilot know precisely how close to the stall point the aircraft is. Modern airliner instrumentation may also measure angle of attack, although this information may not be directly displayed on the pilot's display, instead driving a stall warning indicator or giving performance information to the flight computer (for fly by wire systems).\n\nAs a wing stalls, aileron effectiveness is reduced, making the plane hard to control and increasing the risk of a spin starting. Post stall, steady flight beyond the stalling angle (where the coefficient of lift is largest) requires engine thrust to replace lift as well as alternative controls to replace the loss of effectiveness of the ailerons. For high-powered aircraft, the loss of lift (and increase in drag) beyond the stall angle is less of a problem than maintaining control. Some aircraft may be subject to post-stall gyration (e.g. the F-4) or susceptible to entering a flat-spin (e.g. F-14). Control beyond-stall can be provided by reaction control systems (e.g. NF-104A), vectored thrust, as well as a rolling stabilator (or taileron). The enhanced manoeuvering capability by flights at very high angles of attack can provide a tactical advantage for military fighters such as the F-22 Raptor. Short term stalls at 90–120° (e.g. Pugachev's Cobra) are sometimes performed at airshows. The highest angle of attack in sustained flight so far demonstrated was 70 degrees in the X-31 at the Dryden Flight Research Center. Sustained post-stall flight is a type of supermaneuverability.\n\nExcept for flight training, airplane testing, and aerobatics, a stall is usually an undesirable event. Spoilers (sometimes called lift dumpers), however, are devices that are intentionally deployed to create a carefully controlled flow separation over part of an aircraft's wing to reduce the lift it generates, increase the drag, and allow the aircraft to descend more rapidly without gaining speed. Spoilers are also deployed asymmetrically (one wing only) to enhance roll control. Spoilers can also be used on aborted take-offs and after main wheel contact on landing to increase the aircraft's weight on its wheels for better braking action.\n\nUnlike powered airplanes, which can control descent by increasing or decreasing thrust, gliders have to increase drag to increase the rate of descent. In high-performance gliders, spoiler deployment is extensively used to control the approach to landing.\n\nSpoilers can also be thought of as \"lift reducers\" because they reduce the lift of the wing in which the spoiler resides. For example, an uncommanded roll to the left could be reversed by raising the right wing spoiler (or only a few of the spoilers present in large airliner wings). This has the advantage of avoiding the need to increase lift in the wing that is dropping (which may bring that wing closer to stalling).\n\nOtto Lilienthal died while flying in 1896 as the result of a stall. Wilbur Wright encountered stalls for the first time in 1901, while flying his second glider. Awareness of Lilienthal's accident and Wilbur's experience, motivated the Wright Brothers to design their plane in \"canard\" configuration. This made recoveries from stalls easier and more gentle. The design saved the brothers' lives more than once.\n\nThe aircraft engineer Juan de la Cierva worked on his \"Autogiro\" project to develop a rotary wing aircraft which, he hoped, would be unable to stall and which therefore would be safer than aeroplanes. In developing the resulting \"autogyro\" aircraft, he solved many engineering problems which made the helicopter possible.\n\n\n"}
{"id": "34453685", "url": "https://en.wikipedia.org/wiki?curid=34453685", "title": "Tait equation", "text": "Tait equation\n\nIn fluid mechanics, the Tait equation is an equation of state, used to relate liquid density to pressure. The equation was originally published by Peter Guthrie Tait in 1888 in the form \nwhere formula_2 is the reference pressure (taken to be 1 atmosphere), formula_3 is the current pressure, formula_4 is the volume of fresh water at the reference pressure, formula_5 is the volume at the current pressure, and formula_6 are experimentally determined parameters.\nAround 1895, the original isothermal Tait equation was replaced by Tammann with an equation of the form\nThe temperature-dependent version of the above equation is popularly known as the Tait equation and is commonly written as\n\nor in the integrated form\n\nwhere\n\nThe expression for the pressure in terms of the specific volume is\n\nThe tangent bulk modulus at pressure formula_3 is given by\n\nAnother popular isothermal equation of state that goes by the name \"Tait equation\" is the Murnaghan model which is sometimes expressed as\nwhere formula_20 is the specific volume at pressure formula_3, formula_22 is the specific volume at pressure formula_2, formula_24 is the bulk modulus at formula_2, and formula_26 is a material parameter.\n\nThis equation, in pressure form, can be written as\nwhere formula_28 are mass densities at formula_29, respectively.\nFor pure water, typical parameters are formula_2 = 101,325 Pa, formula_31 = 1000 kg/cu.m, formula_24 = 2.15 GPa, and formula_26 = 7.15.\n\nNote that this form of the Tate equation of state is identical to that of the Murnaghan equation of state.\n\nThe tangent bulk modulus predicted by the MacDonald-Tait model is\n\nA related equation of state that can be used to model liquids is the Tumlirz equation (sometimes called the Tammann equation and originally proposed by Tumlirz in 1909 and Tammann in 1911 for pure water). This relation has the form\nwhere formula_36 is the specific volume, formula_3 is the pressure, formula_38 is the salinity, formula_39 is the temperature, and formula_40 is the specific volume when formula_41, and formula_42 are parameters that can be fit to experimental data.\n\nThe Tumlirz-Tammann version of the Tait equation for fresh water, i.e., when formula_43, is\nFor pure water, the temperature-dependence of formula_45 are:\nIn the above fits, the temperature formula_39 is in degrees Celsius, formula_2 is in bars, formula_40 is in cc/gm, and formula_50 is in bars-cc/gm.\n\nThe inverse Tumlirz-Tammann-Tait relation for the pressure as a function of specific volume is\n\nThe Tumlirz-Tammann-Tait formula for the instantaneous tangent bulk modulus of pure water is a quadratic function of formula_3 (for an alternative see )\n\n"}
{"id": "40425485", "url": "https://en.wikipedia.org/wiki?curid=40425485", "title": "Traditional rice of Sri Lanka", "text": "Traditional rice of Sri Lanka\n\nRice in Sri Lanka has played an important role in the country's functioning and survival for centuries. Rice continues to be a staple of traditional Sri Lankan cuisine today.\n\nSri Lankan people may have started cultivating rice as early as 800 B.C., according to documentary evidence. Further evidence of early rice cultivation is the construction, since 390 B.C., of massive irrigation structures, reservoirs, and interconnected canals. From ancient times, rice cultivation was not only an economic activity, but a way of life for the people of Sri Lanka. Some varieties of rice have been passed down for generations, and are called traditional, indigenous, or heirloom.\n\nOnce renowned as the granary of the east, Sri Lanka offered more than 2000 indigenous rice varieties to the rest of the world. Rice cultivation in Sri Lanka was once considered sacred. The process remained sustainable due to the methods used for production, as well as the sanctity associated with the process of rice cultivation.\n\nWith the European colonization of Sri Lanka during the 16th and 18th centuries, more emphasis was given to other plantation crops. In the 20th century, however, rice was once again given attention. With an increase in the country's population, a new series of rice varieties, called \"the H series,\" was introduced in the 1950s. Fertilizers were also introduced at this time to increase harvest yield. As a result, the average yield of rice increased from 0.65 metric ton/hectare (mt/ha) to 1.73 mt/ha in 1950.\n\nUnfortunately, many of the new rices of Sri Lanka contained lower concentrations of glutamic acid, vitamins, and fiber, and a higher glycemic index than the traditional varieties. While the new rices were being produced in greater quantities, it was not as nutritious as the traditional rice that had once sustained the Sri Lankan people.\n\nBy the 1980s, 90% of the farmland in Sri Lanka was being used to cultivate the \"semi-dwarf\" (newly improved) rice variety.\n\nCurrently, 95% of the rice produced in Sri Lanka are hybrid varieties. These are harvested using non-organic fertilizer and pesticides which are needed to produce larger harvests with lower costs.\n\nHowever, traditional rice is gradually making a comeback. This is due to increased global demand for organic food.\n\nAs the translated name implies, this is a fragrant white rice with an exquisite aroma.\n\nIts milky taste makes Suwandel a common choice for festive occasions and ceremonies. Nutritionally, the rice consists of 90% carbohydrates, 7% crude protein, 0.7% crude fat, and 0.1% crude fiber. Suwandel is known to contain higher amounts of glutamic acid and vitamins than other, more common rice varieties.\n\nSuwandel is an heirloom rice variety, cultivated organically with traditional rain-fed methods in the southern lowlands of Sri Lanka. Because of this, cultivation takes longer than other varieties of rice. It is usually 5 to 6 months before harvest. Heirloom rice cultivation in Sri Lanka is a sacred process.\n\nKalu Heenati is literally translated as \"dark, fine grain.\" It is a highly nutritious red rice that is considered to be good for daily consumption.\n\nThis is a reddish-brown rice variety with a unique texture. It is low in carbohydrates, and rich in protein and fiber. Ma-Wee is also proven to have a 25% to 30% lower glycemic index (GI) than other common rice varieties. It is 84.5% carbohydrates, 9.4% protein, 3.6% fat, and 1.1% fiber.\n\nMa-Wee rice is best when soaked prior to boiling. One traditional dish calls for the rice to be cooked with chopped spring onion and leeks, and served with bottle gourd sautéed in spices and coconut milk.\n\nMa-Wee was loved by the queens of Sri Lanka, who believed it helped them maintain a trim, shapely figure.\n\nMa-Wee is also revered for its historical importance in religious ceremonies. According to folklore, Ma-Wee has been placed in the caskets of sacred relics and the pinnacle (kotha) of dagabas.\n\nThe word \"Pachchaperumal\" means \"The Lord Buddha’s color.\" It is a wholesome red rice variety. When cooked, it takes on a deep, rich burgundy color. It is rich in nutrients and proteins, and is considered an excellent choice for an every day meal. It is also said to be part of a good diet for people with diabetes and cardiovascular disease.\n\nPachchaperumal has long been considered a divine rice in traditional Sinhalese culture. Traditionally, it was often used in alms-giving.\nA nutritious red rice variety rich in proteins and fiber, kuruluthuda has a unique, pleasant taste.\n\nOther varieties include Rathdel, Madathawalu, and Hetadha Wee.\n\nRice has a sacred association among Buddhist, Hindu, and Muslim populations alike. It is said that rice cooked with coconut milk was the first offering made to Buddha, and to this day the dish is a staple of Sri Lankan culture during sacred festivals and important events.\n"}
{"id": "44381526", "url": "https://en.wikipedia.org/wiki?curid=44381526", "title": "Transition modeling", "text": "Transition modeling\n\nTransition modeling is the use of a model to predict the change from laminar and turbulence flows in fluids and their respective effects on the overall solution. The complexity and lack of understanding of the underlining physics of the problems makes simulating the interaction between laminar and turbulent flow to be difficult and very case specific. Transition does have the wide range of turbulence options available for most CFD applications for the following reasons. Transition involves a wide range of scales where the energy and momentum transfer are strong influenced by inertial or non-linear effects that are unique to the simulation. Transition also occurs using different ways, (such as natural or bypass) where modelling all options are difficult. Most CFD code uses RANS where averaging eliminates linear disturbance. \n\nThe following is a list of commonly employed transition models in modern engineering applications.\n\n\n"}
{"id": "1623599", "url": "https://en.wikipedia.org/wiki?curid=1623599", "title": "Trimethylglycine", "text": "Trimethylglycine\n\nTrimethylglycine (TMG) is an that occurs in plants. Trimethylglycine was the first betaine discovered; originally it was simply called betaine because, in the 19th century, it was discovered in sugar beets. Since then, many other betaines have been discovered, and the more specific name \"glycine betaine\" distinguishes this one.\n\nTrimethylglycine is an \"N\"-trimethylated amino acid. This quaternary ammonium exists as the zwitterion at neutral pH. Strong acids such as hydrochloric acid convert TMG to various salts, with HCl yielding betaine hydrochloride:\nDemethylation of TMG gives dimethylglycine. Degradation of TMG yields trimethylamine, the scent of putrefying fish.\n\nProcessing sucrose from sugar beets yields glycine betaine as a byproduct. The value of the TMG rivals that of the sugar content in sugar beets. \n\nIn most organisms, glycine betaine is biosynthesized by oxidation of choline in two steps. The intermediate, betaine aldehyde, is generated by the action of the enzyme mitochondrial choline oxidase (choline dehydrogenase, EC 1.1.99.1). Betaine aldehyde is further oxidised in the mitochondria in mice to betaine by the enzyme betaine aldehyde dehydrogenase (EC 1.2.1.8). In humans betaine aldehyde activity is performed by a nonspecific cystosolic aldehyde dehydrogenase enzyme (EC 1.2.1.3) \n\nTMG is an organic osmolyte. Sugar beet was cultivated from Sea beet, which requires osmolytes in order to survive in the salty soils of coastal areas. TMG also occurs in high concentrations (~10 mM) in many marine invertebrates, such as crustaceans and molluscs. It serves as a potent appetitive attractant to generalist carnivores such as the predatory sea-slug \"Pleurobranchaea californica\".\n\nTMG is an important cofactor in methylation, a process that occurs in every mammalian cell donating methyl groups (-CH) for other processes in the body. These processes include the synthesis of neurotransmitters such as dopamine and serotonin. Methylation is also required for the biosynthesis of melatonin and the electron transport chain constituent coenzyme Q, as well as the methylation of DNA for epigenetics.\n\nThe major step in the methylation cycle is the remethylation of homocysteine, a compound which is naturally generated during demethylation of the essential amino acid methionine. Despite its natural formation, homocysteine has been linked to inflammation, depression, specific forms of dementia, and various types of vascular disease. The remethylation process that detoxifies homocysteine and converts it back to methionine can occur via either of two pathways. The pathway present in virtually all cells involves the enzyme methionine synthase (MS), which requires vitamin B as a cofactor, and also depends indirectly on folate and other B vitamins. The second pathway (restricted to liver and kidney in most mammals) involves betaine-homocysteine methyltransferase (BHMT) and requires TMG as a cofactor. During normal physiological conditions, the two pathways contribute equally to removal of homocysteine in the body. Further degredation of betaine, via the enzyme dimethylglycine dehydrogenase produces folate, thus contributing back to methionine synthase. Betaine is thus involved in the synthesis of many biologically important molecules, and may be even more important in situations where the major pathway for the regeneration of methionine from homocysteine has been compromised by genetic polymorphisms such as mutations in the MS gene.\n\nFactory farms supplement fodder with TMG and lysine to increase livestocks' muscle mass (and, therefore, \"carcass yield\", the amount of usable meat).\n\nSalmon farms apply TMG to relieve the osmotic pressure on the salmon's cells when workers transfer the fish from freshwater to saltwater.\n\nTMG supplementation decreases the amount of adipose tissue in pigs; however, research in human subjects has shown no effect on body weight, body composition, or resting energy expenditure.\n\nAlthough TMG supplementation decreases the amount of adipose tissue in pigs, research on human subjects has shown no effect on body weight, body composition, or resting energy expenditure when used in conjunction with a low calorie diet. The Food and Drug Administration of the United States approved anhydrous trimethylglycine (also known by the brand name Cystadane) for the treatment of homocystinuria, a disease caused by abnormally high homocysteine levels at birth. TMG is also used as the hydrochloride salt (marketed as betaine hydrochloride or betaine HCl). Betaine hydrochloride was once permitted in over-the-counter (OTC) drugs as a gastric aid in the United States. US Code of Federal Regulations, Title 21, Section 310.540, which became effective on November 10, 1993, banned betaine hydrochloride from being used in OTC products due to insufficient evidence to classify it as \"generally recognized as safe and effective\". \n\nTMG supplementation may cause diarrhea, stomach upset, or nausea. TMG supplementation lowers homocysteine but also raises LDL-cholesterol.\n\nTrimethylglycine can act as an adjuvant of the polymerase chain reaction (PCR) process, and other DNA polymerase-based assays such as DNA sequencing. By an unknown mechanism, it aids in the prevention of secondary structures in the DNA molecules, and prevents problems associated with the amplification and sequencing of GC-rich regions. Trimethylglycine makes guanosine and cytidine (strong binders) behave with thermodynamics similar to those of thymidine and adenosine (weak binders). It has been determined under experiment that it is best used at a final concentration of 1 M.\n\nLaboratory studies and two clinical trials have indicated that TMG is a potential treatment of non-alcoholic steatohepatitis.\n\nTMG has been proposed as a treatment for depression. In theory, it would increase \"S\"-adenosylmethionine (SAMe) by remethylating homocysteine. The same homocysteine-to-methionine result could be achieved by supplementing with folic acid and vitamin B12, methionine then serving as a precursor to synthesis of SAMe. SAMe as a dietary supplement has been shown to work as a nonspecific antidepressant.\n\nResearch with the goal of developing environmentally safe biomimetic ship coatings is using TMG, among others, as a non-toxic anti-fouling coating.\n\nIn the book from Amersham Biosciences/GE Healthcare, Ion Exchange Chromatography & Chromatofocusing - Principles and Methods, page 48. \"Zwitterionic additives such as betaine can prevent precipitation and can be used at high concentrations without interfering with the gradient elution\"\n\n"}
{"id": "36592756", "url": "https://en.wikipedia.org/wiki?curid=36592756", "title": "Tuirial Dam", "text": "Tuirial Dam\n\nTuirial dam is an earthfill and gravity dam near Kolasib in the state of Mizoram in India. The primary purpose of the dam is hydroelectric power production. The Cabinet Committee on Economic Affairs (CCEA) approved the 60 MW Tuirial Hydro Electric Project (THEP) project costing Rs 913 crore in 2010. The Project was Inaugurated by Narendra Modi (using a remote control from AR Ground) on 16 December 2017.\n\nTuirial Hydel Project is the 2nd largest earthen dam and the largest in India. The height of the dam is 97 Metres. The dam has 3 split gates, 2 Turbine and 2 tunnel. Tuirial Hydel Dam has two 30MW Turbine.. The dam is estimated to produce 250 million units of electrical energy every year.\n\nConstruction of 60 MW hydroelectric power station began in 1998 by NEEPCO but was halted in 2004 by the Tuirial Crop Compensation Claimant Association of which the Mizo National Front then Chief Minister Pu Zoramthanga's relatives figured among nine people named in Central Bureau of Investigation charge sheet. When Pu Lalthanhawla, who had started the project came back in power, he took initiatives to restart the project. The Cabinet Committee on Economic Affairs (CCEA) approved the 60 MW Tuirial Hydro Electric Project (THEP) project costing Rs 913 crore in 2010. \n"}
{"id": "14441688", "url": "https://en.wikipedia.org/wiki?curid=14441688", "title": "UW Hybrid Vehicle Team", "text": "UW Hybrid Vehicle Team\n\nThe Wisconsin Hybrid Vehicle Team consists mainly of undergraduate students from the University of Wisconsin–Madison who work together to build a hybrid electric vehicle.\n\nMany of the members have little to no previous automotive experience. New team members are taught alongside veteran student members. This passing of knowledge ensures a legacy that is carried on to younger generations.\n\nThe team is subdivided into five major groups to handle necessary tasks. The Mechanical and Drivetrain groups work to design and implement new components, renovate old parts, and perform general vehicle maintenance. The Control and Electrical Groups are responsible for maintaining all of the vehicle computer and electrical components; this includes developing a complex control strategy and programing the electronic control unit, rewiring the vehicle to accommodate the hybrid systems, and maintaining the battery pack as well as other electrically sensitive components. The Outreach group is responsible for publishing newsletters, maintaining the website, obtaining positive relationships with local businesses and community, and involving the team in community events.\nEach group is led by a veteran team member who meet to coordinate projects and make decisions for the vehicle's progress. Each leader is responsible to teach each of the groups attendants safely.\n\nThe EcoCar Challenge is a three-year competition sponsored by the U.S. DOE, GM, and Argonne National Labs for 2008-2010. Seventeen universities from the US and Canada were selected to participate.\nIn the first year, teams concentrated on designing, modeling, and simulating their teams proposed vehicle architecture. Additionally, teams worked to create a functional HIL (hardware-in-the-loop) to demonstrate their capability to manage a vehicle's ECU. The first year's competition was completed in June 2009 in Toronto, OH Canada.\n\nIn the first year of the competition, the UWHVT developed an architecture that was a powersplit hybrid design. Technical papers were created at intervals throughout the year dictating decisions and results of simulations. It consists of a prototype traction motor, the 150 kW Continental AG Sapphire drive. A Weber MPE 750 engine (operating on E85) and the Delphi DU174 motor (found in the EV1) complete the genset. A clutch implemented in between the DU174 and the Weber allow for efficient all electric drive, while a clutch implemented in between the DU174 and the rear differential allows the Weber engine to be directly coupled to the road at highway speeds. Lastly, the battery pack located in the spare rear wheel compartment will be a Johnson Controls Saft 41Ah lithium-ion pack.\n\nThe UWHVT expects to receive its stock Saturn Vue in August 2009 and proceed building the vehicle in year 2. Competition for year two of the challenge will be in Yuma, AZ at GM's new proving ground.\n\n"}
{"id": "32431", "url": "https://en.wikipedia.org/wiki?curid=32431", "title": "Vanadium", "text": "Vanadium\n\nVanadium is a chemical element with symbol V and atomic number 23. It is a hard, silvery-grey, ductile, and malleable transition metal. The elemental metal is rarely found in nature, but once isolated artificially, the formation of an oxide layer (passivation) somewhat stabilizes the free metal against further oxidation.\n\nAndrés Manuel del Río discovered compounds of vanadium in 1801 in Mexico by analyzing a new lead-bearing mineral he called \"brown lead\", and presumed its qualities were due to the presence of a new element, which he named \"erythronium\" (derived from Greek for \"red\") since, upon heating, most of the salts turned red. Four years later, however, he was (erroneously) convinced by other scientists that erythronium was identical to chromium. Chlorides of vanadium were generated in 1830 by Nils Gabriel Sefström who thereby proved that a new element was involved, which he named \"vanadium\" after the Scandinavian goddess of beauty and fertility, Vanadís (Freyja). Both names were attributed to the wide range of colors found in vanadium compounds. Del Rio's lead mineral was later renamed vanadinite for its vanadium content. In 1867 Henry Enfield Roscoe obtained the pure element.\n\nVanadium occurs naturally in about 65 minerals and in fossil fuel deposits. It is produced in China and Russia from steel smelter slag; other countries produce it either from magnetite directly, flue dust of heavy oil, or as a byproduct of uranium mining. It is mainly used to produce specialty steel alloys such as high-speed tool steels. The most important industrial vanadium compound, vanadium pentoxide, is used as a catalyst for the production of sulfuric acid.\n\nLarge amounts of vanadium ions are found in a few organisms, possibly as a toxin. The oxide and some other salts of vanadium have moderate toxicity. Particularly in the ocean, vanadium is used by some life forms as an active center of enzymes, such as the vanadium bromoperoxidase of some ocean algae.\n\nVanadium was discovered by Andrés Manuel del Río, a Spanish-Mexican mineralogist, in 1801. Del Río extracted the element from a sample of Mexican \"brown lead\" ore, later named vanadinite. He found that its salts exhibit a wide variety of colors, and as a result he named the element \"panchromium\" (Greek: παγχρώμιο \"all colors\"). Later, Del Río renamed the element \"erythronium\" (Greek: ερυθρός \"red\") because most of the salts turned red upon heating. In 1805, the French chemist Hippolyte Victor Collet-Descotils, backed by del Río's friend Baron Alexander von Humboldt, incorrectly declared that del Río's new element was only an impure sample of chromium. Del Río accepted Collet-Descotils' statement and retracted his claim.\n\nIn 1831, the Swedish chemist Nils Gabriel Sefström rediscovered the element in a new oxide he found while working with iron ores. Later that same year, Friedrich Wöhler confirmed del Río's earlier work. Sefström chose a name beginning with V, which had not been assigned to any element yet. He called the element \"vanadium\" after Old Norse \"Vanadís\" (another name for the Norse Vanr goddess Freyja, whose attributes include beauty and fertility), because of the many beautifully colored chemical compounds it produces. In 1831, the geologist George William Featherstonhaugh suggested that vanadium should be renamed \"rionium\" after del Río, but this suggestion was not followed.\nThe isolation of vanadium metal proved difficult. In 1831, Berzelius reported the production of the metal, but Henry Enfield Roscoe showed that Berzelius had in fact produced the nitride, vanadium nitride (VN). Roscoe eventually produced the metal in 1867 by reduction of vanadium(II) chloride, VCl, with hydrogen. In 1927, pure vanadium was produced by reducing vanadium pentoxide with calcium.\n\nThe first large-scale industrial use of vanadium was in the steel alloy chassis of the Ford Model T, inspired by French race cars. Vanadium steel allowed for reduced weight while simultaneously increasing tensile strength (ca. 1905). For the first decade most of the vanadium ore was mined by American Vanadium Company from the Minas Ragra in Peru. With the rising demands for uranium which was partially mined as carnotite, a vanadium mineral the uranium mining supplied a large share of the needed vanadium.\n\nGerman chemist Martin Henze discovered vanadium in the hemovanadin proteins found in blood cells (or coelomic cells) of Ascidiacea (sea squirts) in 1911.\n\nVanadium is a medium-hard, ductile, steel-blue metal. It is electrically conductive and thermally insulating. Some sources describe vanadium as \"soft\", perhaps because it is ductile, malleable and not brittle. Vanadium is harder than most metals and steels (see Hardnesses of the elements (data page) and iron). It has good resistance to corrosion and it is stable against alkalis and sulfuric and hydrochloric acids. It is oxidized in air at about 933 K (660 °C, 1220 °F), although an oxide passivation layer forms even at room temperature.\n\nNaturally occurring vanadium is composed of one stable isotope, V, and one radioactive isotope, V. The latter has a half-life of 1.5×10 years and a natural abundance of 0.25%. V has a nuclear spin of , which is useful for NMR spectroscopy. Twenty-four artificial radioisotopes have been characterized, ranging in mass number from 40 to 65. The most stable of these isotopes are V with a half-life of 330 days, and V with a half-life of 16.0 days. The remaining radioactive isotopes have half-lives shorter than an hour, most below 10 seconds. At least four isotopes have metastable excited states. Electron capture is the main decay mode for isotopes lighter than V. For the heavier ones, the most common mode is beta decay. The electron capture reactions lead to the formation of element 22 (titanium) isotopes, while beta decay leads to element 24 (chromium) isotopes.\n\nThe chemistry of vanadium is noteworthy for the accessibility of the four adjacent oxidation states 2–5. In aqueous solution, vanadium forms metal aquo complexes of which the colours are lilac [V(HO)], green [V(HO)], blue [VO(HO)], yellow VO. Vanadium(II) compounds are reducing agents, and vanadium(V) compounds are oxidizing agents. Vanadium(IV) compounds often exist as vanadyl derivatives, which contain the VO center.\n\nAmmonium vanadate(V) (NHVO) can be successively reduced with elemental zinc to obtain the different colors of vanadium in these four oxidation states. Lower oxidation states occur in compounds such as V(CO), and substituted derivatives.\n\nThe most commercially important compound is vanadium pentoxide. It is used as a catalyst for the production of sulfuric acid. This compound oxidizes sulfur dioxide () to the trioxide (). In this redox reaction, sulfur is oxidized from +4 to +6, and vanadium is reduced from +5 to +4:\n\nThe catalyst is regenerated by oxidation with air:\nSimilar oxidations are used in the production of maleic anhydride, phthalic anhydride, and several other bulk organic compounds.\n\nThe vanadium redox battery utilizes all four oxidation states; one electrode uses the +5/+4 couple and the other uses the +3/+2 couple. Conversion of these oxidation states is illustrated by the reduction of a strongly acidic solution of a vanadium(V) compound with zinc dust or amalgam. The initial yellow color characteristic of the pervanadyl ion [VO(HO)] is replaced by the blue color of [VO(HO)], followed by the green color of [V(HO)] and then the violet color of [V(HO)].\n\nIn aqueous solution, vanadium(V) forms an extensive family of oxyanions. The interrelationships in this family are described by the predominance diagram, which shows at least 11 species, depending on pH and concentration. The tetrahedral orthovanadate ion, , is the principal species present at pH 12-14. Similar in size and charge to phosphorus(V), vanadium(V) also parallels its chemistry and crystallography. Orthovanadate V is used in protein crystallography to study the biochemistry of phosphate. The tetrathiovanadate [VS] is analogous to the orthovanadate ion.\n\nAt lower pH values, the monomer [HVO] and dimer [VO] are formed, with the monomer predominant at vanadium concentration of less than c. 10M (pV > 2, where pV is equal to the minus value of the logarithm of the total vanadium concentration/M). The formation of the divanadate ion is analogous to the formation of the dichromate ion. As the pH is reduced, further protonation and condensation to polyvanadates occur: at pH 4-6 [HVO] is predominant at pV greater than ca. 4, while at higher concentrations trimers and tetramers are formed. Between pH 2-4 decavanadate predominates, its formation from orthovanadate is represented by this condensation reaction:\n\nIn decavanadate, each V(V) center is surrounded by six oxide ligands. Vanadic acid, HVO exists only at very low concentrations because protonation of the tetrahedral species [HVO] results in the preferential formation of the octahedral [VO(HO)] species. In strongly acidic solutions, pH<2. [VO(HO)] is the predominant species, while the oxide VO precipitates from solution at high concentrations. The oxide is formally the inorganic anhydride of vanadic acid. The structures of many vanadate compounds have been determined by X-ray crystallography.\nThe Pourbaix diagram for vanadium in water, which shows the redox potentials between various vanadium species in different oxidation states, is also complex.\n\nVanadium(V) forms various peroxo complexes, most notably in the active site of the vanadium-containing bromoperoxidase enzymes. The species VO(O)(HO) is stable in acidic solutions. In alkaline solutions, species with 2, 3 and 4 peroxide groups are known; the last forms violet salts with the formula MV(O) nHO (M = Li, Na, etc.), in which the vanadium has an 8-coordinate dodecahedral structure.\n\nTwelve binary halides, compounds with the formula VX (n=2..5), are known. VI, VCl, VBr, and VI do not exist or are extremely unstable. In combination with other reagents, VCl is used as a catalyst for polymerization of dienes. Like all binary halides, those of vanadium are Lewis acidic, especially those of V(IV) and V(V). Many of the halides form octahedral complexes with the formula VXL (X = halide; L = other ligand).\n\nMany vanadium oxyhalides (formula VOX) are known. The oxytrichloride and oxytrifluoride (VOCl and VOF) are the most widely studied. Akin to POCl, they are volatile, adopt tetrahedral structures in the gas phase, and are Lewis acidic.\n\nComplexes of vanadium(II) and (III) are relatively exchange inert and reducing. Those of V(IV) and V(V) are oxidants. Vanadium ion is rather large and some complexes achieve coordination numbers greater than 6, as is the case in [V(CN)]. Oxovanadium(V) also forms 7 coordinate coordination complexes with tetradentate ligands and peroxides and these complexes are used for oxidative brominations and thioether oxidations. The coordination chemistry of V is dominated by the vanadyl center, VO, which binds four other ligands strongly and one weakly (the one trans to the vanadyl center). An example is vanadyl acetylacetonate (V(O)(OCH)). In this complex, the vanadium is 5-coordinate, square pyramidal, meaning that a sixth ligand, such as pyridine, may be attached, though the association constant of this process is small. Many 5-coordinate vanadyl complexes have a trigonal bypyramidal geometry, such as VOCl(NMe). The coordination chemistry of V is dominated by the relatively stable dioxovanadium coordination complexes which are often formed by aerial oxidation of the vanadium(IV) precursors indicating the stability of the +5 oxidation state and ease of interconversion between the +4 and +5 states.\n\nOrganometallic chemistry of vanadium is well developed, although it has mainly only academic significance. Vanadocene dichloride is a versatile starting reagent and even finds some applications in organic chemistry. Vanadium carbonyl, V(CO), is a rare example of a paramagnetic metal carbonyl. Reduction yields V (isoelectronic with Cr(CO)), which may be further reduced with sodium in liquid ammonia to yield V (isoelectronic with Fe(CO)).\n\nThe cosmic abundance of vanadium in the universe is 0.0001%, making the element nearly as common as copper or zinc. Vanadium is detected spectroscopically in light from the Sun and sometimes in the light from other stars.\n\nVanadium is the 20th most abundant element in the earth's crust; metallic vanadium is rare in nature (known as the mineral vanadium, native vanadium), but vanadium compounds occur naturally in about 65 different minerals. Economically significant examples include patrónite (VS), vanadinite (), and carnotite (). Much of the world's vanadium production is sourced from vanadium-bearing magnetite found in ultramafic gabbro bodies. Vanadium is mined mostly in South Africa, north-western China, and eastern Russia. In 2013 these three countries mined more than 97% of the 79,000 tonnes of produced vanadium.\n\nVanadium is also present in bauxite and in deposits of crude oil, coal, oil shale and tar sands. In crude oil, concentrations up to 1200 ppm have been reported. When such oil products are burned, traces of vanadium may cause corrosion in engines and boilers. An estimated 110,000 tonnes of vanadium per year are released into the atmosphere by burning fossil fuels.\n\nThe vanadyl ion is abundant in seawater, having an average concentration of 30 nM. Some mineral water springs also contain the ion in high concentrations. For example, springs near Mount Fuji contain as much as 54 μg per liter.\n\nMost vanadium is used as a steel alloy called ferrovanadium. Ferrovanadium is produced directly by reducing a mixture of vanadium oxide, iron oxides and iron in an electric furnace. The vanadium ends up in pig iron produced from vanadium-bearing magnetite. Depending on the ore used, the slag contains up to 25% of vanadium.\n\nVanadium metal is obtained by a multistep process that begins with the roasting of crushed ore with NaCl or NaCO at about 850 °C to give sodium metavanadate (NaVO). An aqueous extract of this solid is acidified to give \"red cake\", a polyvanadate salt, which is reduced with calcium metal. As an alternative for small-scale production, vanadium pentoxide is reduced with hydrogen or magnesium. Many other methods are also in use, in all of which vanadium is produced as a byproduct of other processes. Purification of vanadium is possible by the crystal bar process developed by Anton Eduard van Arkel and Jan Hendrik de Boer in 1925. It involves the formation of the metal iodide, in this example vanadium(III) iodide, and the subsequent decomposition to yield pure metal:\n\nApproximately 85% of vanadium produced is used as ferrovanadium or as a steel additive. The considerable increase of strength in steel containing small amounts of vanadium was discovered in the early 20th century. Vanadium forms stable nitrides and carbides, resulting in a significant increase in the strength of steel. From that time on, vanadium steel was used for applications in axles, bicycle frames, crankshafts, gears, and other critical components. There are two groups of vanadium steel alloys. Vanadium high-carbon steel alloys contain 0.15% to 0.25% vanadium, and high-speed tool steels (HSS) have a vanadium content of 1% to 5%. For high-speed tool steels, a hardness above HRC 60 can be achieved. HSS steel is used in surgical instruments and tools. Powder-metallurgic alloys contain up to 18% percent vanadium. The high content of vanadium carbides in those alloys increases wear resistance significantly. One application for those alloys is tools and knives.\n\nVanadium stabilizes the beta form of titanium and increases the strength and temperature stability of titanium. Mixed with aluminium in titanium alloys, it is used in jet engines, high-speed airframes and dental implants. The most common alloy for seamless tubing is Titanium 3/2.5 containing 2.5% vanadium, the titanium alloy of choice in the aerospace, defense and bicycle industries. Another common alloy, primarily produced in sheets, is Titanium 6AL-4V, a titanium alloy with 6% aluminium and 4% vanadium.\n\nSeveral vanadium alloys show superconducting behavior. The first A15 phase superconductor was a vanadium compound, VSi, which was discovered in 1952. Vanadium-gallium tape is used in superconducting magnets (17.5 teslas or 175,000 gauss). The structure of the superconducting A15 phase of VGa is similar to that of the more common NbSn and NbTi.\n\nIt has been proposed that a small amount, 40 to 270 ppm, of vanadium in Wootz steel and Damascus steel significantly improved the strength of the product, though the source of the vanadium is unclear.\n\nVanadium compounds are used extensively as catalysts; for example, the most common oxide of vanadium, vanadium pentoxide VO, is used as a catalyst in manufacturing sulfuric acid by the contact process and as an oxidizer in maleic anhydride production. Vanadium pentoxide is used in ceramics. Vanadium is an important component of mixed metal oxide catalysts used in the oxidation of propane and propylene to acrolein, acrylic acid or the ammoxidation of propylene to acrylonitrile. In service, the oxidation state of vanadium changes dynamically and reversibly with the oxygen and the steam content of the reacting feed mixture. Another oxide of vanadium, vanadium dioxide VO, is used in the production of glass coatings, which blocks infrared radiation (and not visible light) at a specific temperature. Vanadium oxide can be used to induce color centers in corundum to create simulated alexandrite jewelry, although alexandrite in nature is a chrysoberyl.\n\nThe Vanadium redox battery, a type of flow battery, is an electrochemical cell consisting of aqueous vanadium ions in different oxidation states. Batteries of the type were first proposed in the 1930s and developed commercially from the 1980s onwards. Cells use +5 and +2 formal oxidization state ions, and (as of 2016) are used commercially for small scale (c. 0.1 - 10 MW, 0.1 - 100 GJ) grid energy storage.\n\nVanadate can be used for protecting steel against rust and corrosion by conversion coating. Vanadium foil is used in cladding titanium to steel because it is compatible with both iron and titanium. The moderate thermal neutron-capture cross-section and the short half-life of the isotopes produced by neutron capture makes vanadium a suitable material for the inner structure of a fusion reactor.\n\nLithium vanadium oxide has been proposed for use as a high energy density anode for lithium ion batteries, at 745 Wh/L when paired with a lithium cobalt oxide cathode. Vanadium phosphates have been proposed as the cathode in the lithium vanadium phosphate battery, another type of lithium ion battery.\n\nHealth benefits of vanadium and its potential as an anticancer agent have been reviewed. Vanadium is more important in marine environments than terrestrial.\n\nA number of species of marine algae produce vanadium bromoperoxidase as well as the closely related chloroperoxidase (which may use a heme or vanadium cofactor) and iodoperoxidases. The bromoperoxidase produces an estimated 1–2 million tons of bromoform and 56,000 tons of bromomethane annually. Most naturally occurring organobromine compounds are produced by this enzyme, catalyzing the following reaction (R-H is hydrocarbon substrate):\n\nA vanadium nitrogenase is used by some nitrogen-fixing micro-organisms, such as \"Azotobacter\". In this role, vanadium replaces more common molybdenum or iron, and gives the nitrogenase slightly different properties.\n\nVanadium is essential to ascidians and tunicates, where it is stored in the highly acidified vacuoles of certain blood cell types, designated \"vanadocytes\". Vanabins (vanadium binding proteins) have been identified in the cytoplasm of such cells. The concentration of vanadium in the blood of ascidians is as much as ten million times higher than the surrounding seawater, which normally contains 1 to 2 µg/l. The function of this vanadium concentration system and these vanadium-bearing proteins is still unknown, but the vanadocytes are later deposited just under the outer surface of the tunic where they may deter predation.\n\n\"Amanita muscaria\" and related species of macrofungi accumulate vanadium (up to 500 mg/kg in dry weight). Vanadium is present in the coordination complex amavadin in fungal fruit-bodies. The biological importance of the accumulation is unknown. Toxic or peroxidase enzyme functions have been suggested.\n\nDeficiencies in vanadium result in reduced growth in rats. The U.S. Institute of Medicine has not confirmed that vanadium is an essential nutrient for humans, so neither a Recommended Dietary Intake nor an Adequate Intake have been established. Dietary intake is estimated at 6 to 18 µg/day, with less than 5% absorbed. The Tolerable Upper Intake Level (UL) of dietary vanadium, beyond which adverse effects may occur, is set at 1.8 mg/day.\n\nVanadyl sulfate as a dietary supplement has been researched as a means of increasing insulin sensitivity or otherwise improving glycemic control in people who are diabetic. Some of the trials had significant treatment effects, but were deemed as being of poor study quality. The amounts of vanadium used in these trials (30 to 150 mg) far exceeded the safe upper limit. The conclusion of the systemic review was \"There is no rigorous evidence that oral vanadium supplementation improves glycaemic control in type 2 diabetes. The routine use of vanadium for this purpose cannot be recommended.\"\n\nIn astrobiology, it has been suggested that discrete vanadium accumulations on Mars could be a potential microbial biosignature, when used in conjunction with Raman spectroscopy and morphology.\n\nAll vanadium compounds should be considered toxic. Tetravalent VOSO has been reported to be at least 5 times more toxic than trivalent VO. The Occupational Safety and Health Administration (OSHA) has set an exposure limit of 0.05 mg/m for vanadium pentoxide dust and 0.1 mg/m for vanadium pentoxide fumes in workplace air for an 8-hour workday, 40-hour work week. The National Institute for Occupational Safety and Health (NIOSH) has recommended that 35 mg/m of vanadium be considered immediately dangerous to life and health, that is, likely to cause permanent health problems or death.\n\nVanadium compounds are poorly absorbed through the gastrointestinal system. Inhalation of vanadium and vanadium compounds results primarily in adverse effects on the respiratory system. Quantitative data are, however, insufficient to derive a subchronic or chronic inhalation reference dose. Other effects have been reported after oral or inhalation exposures on blood parameters, liver, neurological development, and other organs in rats.\n\nThere is little evidence that vanadium or vanadium compounds are reproductive toxins or teratogens. Vanadium pentoxide was reported to be carcinogenic in male rats and in male and female mice by inhalation in an NTP study, although the interpretation of the results has recently been disputed. The carcinogenicity of vanadium has not been determined by the United States Environmental Protection Agency.\n\nVanadium traces in diesel fuels are the main fuel component in high temperature corrosion. During combustion, vanadium oxidizes and reacts with sodium and sulfur, yielding vanadate compounds with melting points as low as 530 °C, which attack the passivation layer on steel and render it susceptible to corrosion. The solid vanadium compounds also abrade engine components.\n\n\n\n"}
{"id": "649711", "url": "https://en.wikipedia.org/wiki?curid=649711", "title": "Vector boson", "text": "Vector boson\n\nIn particle physics, a vector boson is a boson with the spin equal to 1. The vector bosons regarded as elementary particles in the Standard Model are the gauge bosons, the force carriers of fundamental interactions: the photon of electromagnetism, the W and Z bosons of the weak interaction, and the gluons of the strong interaction. Some composite particles are vector bosons, for instance any vector meson (quark and antiquark). During the 1970s and 1980s, intermediate vector bosons—vector bosons of \"intermediate\" mass (a mass between the two of the vector mesons)—drew much attention in particle physics.\n\nThe Z and W particles interact with the Higgs boson as shown in the Feynman diagram. \n\nThe name \"vector boson\" arises from quantum field theory. The component of such a particle's spin along any axis has the three eigenvalues −\"ħ\", 0, and +\"ħ\" (where \"ħ\" is the reduced Planck constant), meaning that any measurement of its spin can only yield one of these values. (This is, at least, true for massive vector bosons; the situation is a bit different for massless particles such as the photon, for reasons beyond the scope of this article. See Wigner's classification.) The space of spin states therefore is a discrete degree of freedom consisting of three states, the same as the number of components of a vector in three-dimensional space. Quantum superpositions of these states can be taken such that they transform under rotations just like the spatial components of a rotating vector (the so named 3 representation of SU(2)). If the vector boson is taken to be the quantum of a field, the field is a vector field, hence the name.\n\n"}
{"id": "22100350", "url": "https://en.wikipedia.org/wiki?curid=22100350", "title": "World Human Powered Vehicle Association", "text": "World Human Powered Vehicle Association\n\nThe World Human Powered Vehicle Association is dedicated to promoting the design and development of human-powered vehicles. Its main focus is coordinating between national HPV clubs with regard to competitions and records. The WHPVA also supports the Human Power eJournal, with a broader focus including stationary uses of human power.\n\nThe WHPVA was originally called the IHPVA (International Human Powered Vehicle Association), which was founded in 1976 in the USA and was for many years an association of individual members. In 1997 the IHPVA was reorganized into an international association with national organizations as members and an American association which adopted the name Human Powered Vehicle Association HPVA.\n\nDue to conflicts regarding record keeping and copyrights, the HPVA left the IHPVA in 2004. In 2008 the HPVA decided to rename itself to IHPVA while the IHPVA of this time decided to keep its name, resulting in two organizations with the same name.\n\nIn a hostile takeover, the American IHPVA also seized control of the domain name ihpva.org away from the international IHPVA. The IHPVA tried to regain its domain by appealing to the ICANNs ombudsman but was unsuccessful and in 2009 itself renamed to WHPVA.\n\nThe WHPVA maintains speed and distance records for various times and distances for land, water and air vehicles. The best hour record is currently held by Francesco Russo of Switzerland with a total distance of . Since 2009, this recordbase has forked from the previously identical one of the IHPVA.\n\n"}
