{"id": "1987017", "url": "https://en.wikipedia.org/wiki?curid=1987017", "title": "2001 Eastern North America heat wave", "text": "2001 Eastern North America heat wave\n\nA rather cool and uneventful summer along the East Coast of the United States (with a more average heat pattern occurring in the Midwest/Great Lakes regions) changed abruptly when a ridge of high pressure centered off the coast of South Carolina strengthened in late July.\n\nIt began in early August for areas of the Midwest and western Great Lakes before spreading eastward and intensifying. It waned in most areas by the middle of the month, and although fairly short in duration compared with some other continental heat waves, it was very intense at its peak. \n\nThe high humidity and high temperatures led to major heat wave that overtook the major Northeast Megalopolis. Temperatures in Central Park, New York City reached a peak of . The temperature reached in Newark, New Jersey.\n\nMeanwhile, in Ontario and Quebec, extreme temperatures were also reported daily during the first week of August. Ottawa recorded its second-hottest day ever when the mercury approached on August 9 and at the Toronto Airport it hit on the same day, the hottest day there since 1955 with four straight days topping . Numerous records were shattered during the heatwave. Even in Nova Scotia, surrounded by the relatively cool waters of the Atlantic Ocean, temperatures still broke in some locations. Glace Bay, which has a sub-Arctic climate reached a record-breaking on August 10.\n\nAt least four New Yorkers died of hyperthermia. Chicago had at least 21 deaths.\n\nThe Tom Perrotta novel \"Little Children\" takes place in the middle of the heatwave.\n"}
{"id": "39352379", "url": "https://en.wikipedia.org/wiki?curid=39352379", "title": "Aksu Dam", "text": "Aksu Dam\n\nThe Aksu Dam is an embankment dam in the preliminary stages of construction near the town of Aksu on the Çoruh River in Erzurum Province, Turkey. The dam is part of the Çoruh Development Plan and its construction will be supervised by Turkey's State Hydraulic Works. Construction on access roads and diversion tunnels began in 2010.\n\n"}
{"id": "44514117", "url": "https://en.wikipedia.org/wiki?curid=44514117", "title": "Application of CFD in thermal power plants", "text": "Application of CFD in thermal power plants\n\nComputational fluid dynamics (CFD) are used to understand complex thermal flow regimes in power plants. The thermal power plant may be divided into different subsectors and the CFD analysis applied to critical equipment/components - mainly different types of heat exchangers - which are of crucial significance for efficient and trouble free long-term operation of the plant.\n\nThe thermal power station subsystem involves multiphase flow, phase transformation and complex chemical reaction associated with conjugate heat transfer.\n\nFinite difference method describes the unknowns of the flow problem by means of point samples at the node points of a grid co-ordinate lines. Taylor series expansions are used to generate finite difference approximations of derivatives in terms of point samples at each grid point and its immediate neighbours. Those derivatives appearing in the governing equations are replaced by finite differences yielding an algebraic equation.\n\nFinite element method uses piece wise functions valid on elements to describe the local variations of unknown flow variables. Here also a set of algebraic equations are generated to determine unknown co-efficients.\n\nFinite volume method is probably the most popular method used for numerical discretization in CFD. This method is similar in some ways to the finite difference method. This approach involves the discretization of the spatial domain into finite control volumes. The governing equations in their differential form are integrated over each control volume. The resulting integral conservation laws are exactly satisfied for each control volume and for the entire domain, which is a distinct advantage of the finite volume method. Each integral term is then converted into a discrete form, thus yielding discretised equations at the centroids, or nodal points, of the control volumes.\n\nWhen fossil fuels are burned, Nitric oxide and Nitrogen dioxide are produced. These pollutants initiate reactions which result in production of ozone and acid rain. NOx formation takes place due to (1) High temperature combustion i.e. thermal NOx and (2)Nitrogen bound to fuel i.e. fuel NOx and which is insignificant. In the majority of cases the level of thermal NOx can be reduced by lowering flame temperature. This can be done by modifying the burner to create a larger (hence lower temperature) flame, in turn reducing the NOx formation. The role of CFD analysis is vital for design and analysis of such low NOx burners. Many available CFD tools, such as CFX, Fluent, Star CCM++ with different models as RNG k-ε turbulence models with hybrid and CONDIF upwind differencing schemes has been used for analysis purpose and the data obtained with these analysis helped in modifying the burner design in turn lowering the adverse effect on the environment due to NOx formation during combustion.\n\nThe economiser is a crucial component for efficient performance of a thermal power plant. It is a non-steaming type of heat exchanger which is placed in the convective zone of the furnace. It takes the heat energy of the flue gases for heating the feed water before it enters the boiler drum. The thermal efficiency/boiler efficiency largely depends on the performance of the economiser. CFD analysis helps in optimizing the thermal performance of the economiser by analysing the pressure, velocity and temperature distribution, and to identify the critical areas for further improvement with the result obtained by CFD analysis.\n\nSuperheaters, which are generally placed in the radiant zone of the furnace, are used for increasing the temperature of dry saturated steam coming out from boiler drum and to maintain the required parameters before sending it to the steam turbine. The thermal efficiency of a thermal power plant depends on the performance of the superheater. The CFD analysis of superheaters is done at design stage and later at the troubleshooting and performance evaluation during the operation of the plant. The CFD results obtained can be useful for the maintenance engineer to make suitable predictions of the tube life and make suitable arrangements for the high temperature zone to reduce the erosion of the tube coil and restricting the tube leakage problem. CFD analysis consists of modelling the superheater and doing analysis to study the velocity, pressure and temperature distribution of the steam inside the superheater. The uneven temperature distribution of steam in the tube leads to boiler tube leakage. CFD also helps to study the effect of the operating parameters on the tube erosion rate. Thermal power plants operates round the year and it is not always possible to shut down and analyse the problem. CFD helps in this.\n\nIn a thermal power plant combustion of fuel, especially pulverized coal, is of significant importance. Proper and complete combustion, with the required proportions of air and fuel, is required for total energy transfer to water for steam generation and to reduce pollutants. CFD models based on fundamental conservation equations of mass, energy, chemical species and momentum can be used to simulate the flow of air and coal through the burners. The results obtained from CFD analyses give insight to identify the potential areas for improvement.\n\nThere are some other areas of importance where CFD can play a significant role in performance and efficiency improvement. The unbalanced coal/air flow in the pipe systems of coal fired power plants leads to non-uniform combustion in the furnace, and hence an overall lower efficiency of the boiler. A common solution to this problem is to put orifices in the pipe systems to balance the flow. If the orifices are sized to balance clean airflow to individual burners connected to a pulverizer, the coal/airflow would still be unbalanced and vice versa. The CFD with standard k–e two-phase flow model can be used to calculate pressure drop coefficients for the coal/air as well as the clean air flow.\n\nThe CFD is also used to obtain the numerical solution to address the problem of water wall erosion of the furnace of a thermal power plant. This is caused by flame misalignment, thermal attack and erosion due to the contact with chemicals. The flame misalignment occurs because of alteration in fluid dynamics factors due to burner geometry. CFD results show velocity profiles, pressure profiles, streamlines and other data that is helpful in understanding the fluid flow phenomena inside the equipment. It is clearly evident from above examples how crucial is the application of CFD in addressing the bottlenecks in thermal power plants, improving power plant efficiency and assisting in maintenance decisions.\n\n"}
{"id": "391252", "url": "https://en.wikipedia.org/wiki?curid=391252", "title": "Cadillac Desert", "text": "Cadillac Desert\n\nCadillac Desert, by Marc Reisner, is a 1986 book published by Viking () about land development and water policy in the western United States. Some scholars have described the book as Reisner's magnum opus. Subtitled \"The American West and its Disappearing Water,\" it gives the history of the Bureau of Reclamation and U.S. Army Corps of Engineers, and their struggle to remake the American West. The book's main conclusion is that development-driven policies, formed when settling the West was the country's main concern, are having serious long-term negative effects on the environment and water quantity. The book was revised and updated in 1993. A portion of the 1993 update was printed in the inaugural edition of the \"Hastings West-Northwest Journal of Environmental Law and Policy\".\n\nA four-part television documentary based on the revised book was produced by KTEH, the PBS affiliate in San Jose, California, in 1996. The parts are entitled \"Mulholland’s Dream\", \"An American Nile\", \"The Mercy of Nature\", and \"The Last Oasis\". Additionally, Reisner's book inspired fictional works about the effects of climate change (so-called climate fiction), such as Paolo Bacigalupi's near-future thriller \"The Water Knife\" (2015), in which \"Cadillac Desert\" is frequently mentioned by several characters as crucial anticipation of the environmental decline they experience, or Claire Vaye Watkins's \"Gold Fame Citrus\" (2015) who references \"Cadillac Desert\" as a source in her acknowledgments\".\" \n\n\n"}
{"id": "61275", "url": "https://en.wikipedia.org/wiki?curid=61275", "title": "Cathodoluminescence", "text": "Cathodoluminescence\n\nCathodoluminescence is an optical and electromagnetic phenomenon in which electrons impacting on a luminescent material such as a phosphor, cause the emission of photons which may have wavelengths in the visible spectrum. A familiar example is the generation of light by an electron beam scanning the phosphor-coated inner surface of the screen of a television that uses a cathode ray tube. Cathodoluminescence is the inverse of the photoelectric effect, in which electron emission is induced by irradiation with photons.\n\nLuminescence in a semiconductor results when an electron in the conduction band recombines with a hole in the valence band. The excess energy of this transition can be emitted in form of a photon. The energy (color) of the photon, and the probability that a photon and not a phonon will be emitted, depends on the material, its purity, and the presence of defects. However, first the electron has to be excited from the valence band into the conduction band. In cathodoluminescence, this occurs as the result of an impinging high energy electron beam onto a semiconductor. However, these primary electrons carry far too much energy to directly excite electrons. Instead, the inelastic scattering of the primary electrons in the crystal leads to the emission of secondary electrons, Auger electrons and X-rays, which in turn can scatter as well. Such a cascade of scattering events leads to up to 10 secondary electrons per incident electron. These secondary electrons can excite valence electrons into the conduction band when they have a kinetic energy about three times the band gap energy of the material formula_1. The excess energy is transferred to phonons and thus heats the lattice. One of the advantages of excitation with an electron beam is that the band gap energy of materials that are investigated is not limited by the energy of the incident light as in the case of photoluminescence. Therefore, in cathodoluminescence, the \"semiconductor\" examined can, in fact, be almost any non-metallic material. In terms of band structure, classical semiconductors, insulators, ceramics, gemstones, minerals, and glasses can be treated the same way.\n\nIn geology, mineralogy, materials science and semiconductor engineering, a scanning electron microscope fitted with a cathodoluminescence detector, or an optical cathodoluminescence microscope, may be used to examine internal structures of semiconductors, rocks, ceramics, glass, etc. in order to get information on the composition, growth and quality of the material.\n\nIn these instruments a focused beam of electrons impinges on a sample and induces it to emit light that is collected by an optical system, such as an elliptical mirror. From there, a fiber optic will transfer the light out of the microscope where it is separated into its component wavelengths by a monochromator and is then detected with a photomultiplier tube. By scanning the microscope's beam in an X-Y pattern and measuring the light emitted with the beam at each point, a map of the optical activity of the specimen can be obtained (cathodoluminescence imaging). Instead, by measuring the wavelength dependence for a fixed point or a certain area, the spectral characteristics can be recorded (cathodoluminescence spectroscopy). Furthermore, if the photomultiplier tube is replaced with a CCD camera, an entire spectrum can be measured at each point of a map (hyperspectral imaging). Moreover, the optical properties of an object can be correlated to structural properties observed with the electron microscope.\n\nThe primary advantages to the electron microscope based technique is its spatial resolution. In a scanning electron microscope, the attainable resolution is on the order of a few ten nanometers, while in a (scanning) transmission electron microscope, nanometer-sized features can be resolved. Additionally, it is possible to perform nanosecond- to picosecond-level time-resolved measurements if the electron beam can be \"chopped\" into nano- or pico-second pulses by a beam-blanker or with a pulsed electron source. These advanced techniques are useful for examining low-dimensional semiconductor structures, such a quantum wells or quantum dots.\n\nWhile an electron microscope with a cathodoluminescence detector provides high magnification, an optical cathodoluminescence microscope benefits from its ability to show actual visible color features directly through the eyepiece. More recently developed systems try to combine both an optical and an electron microscope to take advantage of both these techniques. \n\nAlthough direct bandgap semiconductors such as GaAs or GaN are most easily examined by these techniques, indirect semiconductors such as silicon also emit weak cathodoluminescence, and can be examined as well. In particular, the luminescence of dislocated silicon is different from intrinsic silicon, and can be used to map defects in integrated circuits.\n\nRecently, cathodoluminescence performed in electron microscopes is also being used to study surface plasmon resonances in metallic nanoparticles. Surface plasmons in metal nanoparticles can absorb and emit light, though the process is different from that in semiconductors. Similarly, cathodoluminescence has been exploited as a probe to map the local density of states of planar dielectric photonic crystals and nanostructured photonic materials.\n\n\n\n"}
{"id": "865304", "url": "https://en.wikipedia.org/wiki?curid=865304", "title": "Chevrolet S-10", "text": "Chevrolet S-10\n\nThe Chevrolet S-10 is a compact pickup truck that was produced by Chevrolet. It was the first domestically built compact pickup of the big three American automakers. When it was first introduced as a \"quarter-ton pickup\" in 1981 for the 1982 model year, the GMC version was known as the S-15 and later renamed the GMC Sonoma. A high-performance version was released in 1991 and given the name of GMC Syclone. The pickup was also sold by Isuzu as the Hombre from 1996 through 2000, but only in North America. There was also an SUV version, the Chevrolet S-10 Blazer/GMC S-15 Jimmy. An electric version was leased as a fleet vehicle in 1997 and 1998. Together, these pickups are often referred to as the S-series.\n\nIn North America, the S-series was replaced by the Chevrolet Colorado, GMC Canyon, and Isuzu i-Series in 2004.\n\nThe S-Series ended production in Brazil in 2012, being replaced by the Chevrolet Colorado, but still with the name S-10.\n\nThe first compact truck from the big three automakers was the rebadged Isuzu KB sold since 1972 as the Chevrolet LUV, and Ford quickly responded with the Mazda-built Ford Courier the same year. However, the 1973 Arab oil embargo forced GM to consider designing a domestically produced compact pickup truck. As usual, parts from other GM chassis lines (primarily from the GM G-body intermediates) were incorporated. Track width was similar to the former GM H-body subcompacts (Vega/Monza). The first S-series pickups were introduced in 1981 for the 1982 model year. The base engine (manufactured in Japan and imported) was a 1.9 L Isuzu four-cylinder (RPO LR1) shared with the LUV and Isuzu P'UP, with a 2.8 L V6 as an option. The Chevrolet and GMC models were identical apart from the grille, tailgate and assorted insignia. An extended cab and \"Insta-Trac\" four-wheel drive were added the next year along with two new engines - a 2.0 L four-cylinder engine (RPO LQ2) from the J-platform automobiles along with an Isuzu 2.2 L (RPO LQ7) four-cylinder diesel engine.\n\nThe sport utility S-10 Blazer and S-15 Jimmy debuted for 1983; GM was the first to introduce the compact sport utility, followed by Ford and then Jeep in 1984. This occurred again when 4-door variants were introduced in March 1990 as 1991 models alongside the badge-engineered Oldsmobile Bravada.\n\nNew heavy-duty and off-road suspensions appeared in 1984 along with a hydraulic clutch, while the big news for 1985 was the discontinuation of the Cavalier's 2.0 L OHV I4 in favor of Pontiac's 2.5 L \"Iron Duke\" OHV I4. The OHV-derived 2.2 L diesel engine and 1.9 L SOHC gas engine, both from Isuzu, were gone the next year, leaving just the Iron Duke and updated 2.8 L V6. A much-welcomed 4.3 L V6 was added for 1988, and anti-lock brakes came the next year.\nThe GMC S-15 became the GMC Sonoma in 1991, and the Sierra trim packages were dropped to avoid confusion with the new GMC Sierra full-size pickup. The GMC Syclone also appeared that year. The Sonoma GT bowed in 1992. Added to this was the 4.3 L V6 Vortec W-code engine. This generation's last year was 1993.\n\nThe S-10 Baja is an optional appearance package that was put on any four-wheel drive S-10 (regular-cab with short-box, regular-cab with long box and extended-cab with short box) from 1989 to 1991. The Baja was available in three colors: Midnight Black, Apple Red and Frost White. The Baja option also included: a roll bar with off-road lights, front tubular grille guard with fog lights, tubular rear bumper, an underbody shield package (transfer case shield, front differential shield, fuel tank shield, oil pan/steering linkage shields), a suspension package, Chevrolet windshield banner, Baja decals on the box sides and one inch wide body striping. Extra cost Baja options included a cargo-net end-gate, aluminum \"Outlaw\" wheels and a special box-mounted spare tire carrier with aluminum wheel. In 1991 the S10 Bajas came with special \"BAJA\" embroidered red and grey bucket seats and unique red door panel trim.\n\nThe S-10 Cameo and Cameo EL were appearance packages available for the two-wheel drive S-10 between 1989 and 1991. When first introduced in 1989, the Cameo had three color choices; Apple Red, Frost White or Midnight Black. In 1991, two-tone paint schemes were available, as well as additional exterior features such as a wraparound front bumper with fog lamps, lower body ground-effects moldings and wheel flares, a flush-fitting tailgate valance, rear roll pan (state laws permitting), and \"Cameo\" lettering on the doors and tailgate. Other appearance packages included the S-10 Back Country, predecessor to the Baja, the S-10 Top Gun edition, and the GMC Sonoma SST.\n\nThe Sonoma GT was a performance package available on the two-wheel drive regular cab short bed Sonoma. It was available for the 1992 model year only as an entry-level version of the GMC Syclone. A total of 806 were built. The truck was powered by an enhanced Vortec 4.3 L V6. It featured central multi-port fuel injection and produced and of torque. It was equipped with a 4L60 automatic transmission and a limited-slip differential with 3.42:1 gearing.\n\nModified by Production Automotive Services of Troy, Michigan, it was fitted with the Syclone interior package featuring black cloth bucket seats with red piping, a special gauge package, and a floor shift console.\n\nSonoma GT color breakdown;\n\nSome 1992 and 1993 Sonomas came with a factory equipped L35 W-code engine. For 1993 no specialty labeling or limited edition tags were known to be used with the W-code engine. Production totals for these vehicles are unknown.\n\nThe Syclone came with an 4.3 L LB4 V6 with lower compression pistons and a turbocharger. They produced ~280 hp.\n\nThe second-generation trucks arrived for the 1994 model year. All of the special models (the Syclone, Typhoon, and Sonoma GT) were discontinued, but the changes to the pickup brought it in line with its major competitor the Ford Ranger. The Iron Duke 4-cylinder and 2.8 L 60° V6 engines were retired, the 4.3 L Vortec V6 was enhanced, and a new 2.2 L 4-cylinder engine (which had been introduced in 1990 on various front-wheel drive GM compact and mid-size platforms) became the engines of choice to power the second generation of S-10s. In compliance with the Clean Air Act, all second generation S-10s and Sonomas equipped with air conditioning used CFC-free R134a refrigerant beginning in the 1994 model year. The all new 1994 S-10 didn't offer any airbag, presumably as a temporary measure to economize the introduction of the new body styles, as well as to gradually phase out steering wheel designs that didn't accommodate for airbags, though the vehicle itself was slated for airbag capability.\n\nMany of the chassis components were the same as the first generation (the A-frames between the first and second generation were the same although they were originally sourced from GM's G-body vehicle lineup), along with the steering knuckle, leaf springs, and differential assembly but suspension and axles were greatly enhanced. Lower A-frames for the two-wheel drive model had 1/4-inch thicker steering stops - the second generation A-frames are commonly used as an upgrade for the first generation. Sport utility models (Blazer, Jimmy, Bravada) came with thicker front and rear sway bars.\n\nGenerally, for the two-wheel drive trucks, the 8.5-inch rear end was only used when it came with both a manual transmission and the large V6 engine; it was an option for four-wheel drive trucks with either transmission. This was also the year that GM introduced the ZR2 off-road package.\n\nFor 1995, a driver's side air bag was added as well as daytime running lights. In 1996 the 4.3 L engine was refreshed, and a third (rear) door was added for extended cab models, along with the sportside bed option. In 1998, the exterior, interior, brakes, and 2.2 L I4 engine were refreshed, along with a \"next-generation\" supplemental restraint system that added a passenger-side air bag. The SS package was replaced by the \"Xtreme\" package. In 2001, a crew cab option was added and was only available with four-wheel drive and an automatic transmission. For the 2004 model year, the regular and extended cab models were discontinued; only the crew cab model was retained.\n\nBase two-wheel drive models came with 15 x 6.5-inch wheels with directional vents, Xtreme and ZQ8 models came with 16 x 8-inch wheels while four-wheel drive models (including the ZR2) used 15 x 7-inch wheels. The wheels used on the first generation were discontinued.\n\nThe Chevrolet S-10 SS was a high-performance version of the S-10, introduced in 1994. Fewer than 3,000 SSs were produced yearly on average. When introduced, the SS was only sold in three colors: Onyx Black, Summit White, and Apple Red. The SS was discontinued in 1998 and was replaced by the S-10 Xtreme for the 1999 model year.\n\nA 4.3 L V6 (which was optional on regular S-10s) was the standard engine used in the SS, producing between 180 and . The SS included a lowered suspension (starting with the 1996 model year), cosmetic changes such as a different grille, body-colored bumpers, 16-inch wheels (available from 1996 to 1998, similar in design to the 1991 and 1992 Camaro Z28 with Chevrolet \"bowtie\" logo center caps), and other minor cosmetic differences. All SS versions were regular cabs, Xtremes were available with the \"third door\" extended cab. A step-side version was available from 1996 to 1998 and until 2003 on the Xtreme.\n\nThe ZR2 package was an off-road package available for the second generation S-10. The ZR2 package included a wider track width, a boxed ladder-type frame with modified suspension mounting points, larger wheel and axle bearings, 31-inch all-terrain tires, a suspension lift (approximately 3 inches more ground clearance versus a regular four-wheel drive S-10), upgraded Bilstein suspension, fender flares, alloy wheels, and an 8.5-inch Chevrolet 10-bolt rear differential with 3.73:1 gears and an Eaton MLocker (coded as G80). \n\nIn 1996, Isuzu replaced its P'up with a version of the Louisiana-built Chevrolet S-10, the Isuzu Hombre, based on the Brazilian market S-10 (the front grille and fenders are based on the Brazilian S-10 along with the truck bed sheetmetal). The Hombre differed from its GM siblings mostly in the front sheetmetal, with different lights, grille, front bumper and front fenders, which were more flared out. The rear quarter panels were also different, as they had a slight flare over the wheel well to match the front fenders. The Hombre had a much smaller range of equipment options compared to the S-10 and Sonoma; a\"Spacecab\" extended cab, V6 engine and four-wheel drive were added for 1997 and available until 1998.\n\nTwo trim levels were offered: the base S and the uplevel XS. The XS had features like a cassette tape deck, higher-grade interior fabric, a tachometer, sliding rear window, and a split 60/40 seatback. Hombres were equipped with the Chevrolet S-series 15 x 7 steel wheels (with 8 directional vents) - the Hombre wheels were painted black (the S10, Sonoma, and Blazer/Jimmy wheels were painted silver) since a majority were equipped with wheel covers with the Isuzu logo. Hombres were also available with the S-10's aluminum wheels with Isuzu center caps.\n\nSlow sales resulted in production ending in 2000. It would be another six years before Isuzu re-entered the pick-up market with the i-Series, which formed the basis for the S-10's successor, the Colorado.\n\nAlthough the North American version of the S-series was discontinued in 2004, the second generation S-10 was still being built in Brazil until 2012, when it was replaced by a Brazilian-built version of the Chevrolet Colorado called the S-10.\n\nIn Brazil, until 2014, the third generation S-10 offered a 2.4 L 147 hp \"Flexpower\" flex-fuel engine or a 2.8 L 180 hp \"Duramax\" diesel engine. For 2015 models, the diesel engine was refreshed, resulting in an increased power output of 200 hp. The flex-fuel (gasoline/ethanol) engine had an upgrade for some versions (LT and LTZ) and offered 206 hp with a 2.5 L LCV Ecotec with direct fuel injection. Also, for the first time in Brazil, Chevrolet offered the flex-fuel S-10 with four-wheel drive.\n\nFor the 2017 model, the S-10 received a facelift and the flex-fuel 2.4 L engine was dropped, with the remaining engine choices being unchanged. For 2018, the flex-fuel S-10 now offers an automatic transmission, currently the market trend on these vehicles in Brazil.\n"}
{"id": "27625335", "url": "https://en.wikipedia.org/wiki?curid=27625335", "title": "Chloryl", "text": "Chloryl\n\nIn chemistry, chloryl refers to a triatomic cation with chemical formula . This species has the same general structure as chlorite () but it is electronically different, with chlorine having a +5 oxidation state (rather than the +3 of chlorite). This makes it a rare example of a positively charged oxychloride. Chloryl compounds, such as and [ClO][RuF], are all highly reactive and react violently with water and most organic compounds.\n\nThe cation is isoelectronic with , and has a bent structure with a bond angle close to 120°. The Cl–O bond is of bond order 1.5, with its Lewis structure consisting of a double bond and a dative bond which does not utilize d-orbitals.\n\nThe red color of is caused by electron transitions into an antibonding orbital. The analogous transition in is not in the visible spectrum, so is colorless. The strength of interaction with the counterion affects the energy of this antibonding orbital; thus, in colorless chloryl compounds, strong interactions with the counterion, corresponding with the higher covalent character of the bonding, shift the transition energy out of the visible spectrum.\n\nThere are two categories of chloryl compounds. The first category is colorless, and includes chloryl fluoride (). These are moderately reactive. Although named as an ionic \"chloryl\" compound, chloryl fluoride is more a covalent compound than an ionic compound of fluoride and chloryl cation. \n\nThe second category features red-colored compounds that are highly reactive. These include chloryl fluorosulfate, , and dichloryl trisulfate, . These chloryl compounds form red solutions in fluorosulfuric acid, and do contain a red-colored cation which dissociates in solution. In the solid state, the Raman and infrared spectra indicate strong interactions with the counterion. Not all chloryl compounds in the solid state are necessarily ionic. The reaction products of with and are assumed to be molecular adducts rather than true salts.\n\nOne notable chloryl compound is dichlorine hexoxide, which exists as an ionic compound more accurately described as \"chloryl perchlorate\", . It is a red fuming liquid under standard conditions.\n\nChloryl compounds are best prepared by the reaction of with a strong Lewis acid. For example:\n\nOther synthesis routes are also possible, including:\n\nMetathesis reactions may be carried out with strong Lewis bases. For example, the reaction of the hexafluoroplatinate salt with nitryl fluoride yields the nitronium salt:\n"}
{"id": "2996488", "url": "https://en.wikipedia.org/wiki?curid=2996488", "title": "Choked flow", "text": "Choked flow\n\nChoked flow is a compressible flow effect. The parameter that becomes \"choked\" or \"limited\" is the fluid velocity.\n\nChoked flow is a fluid dynamic condition associated with the Venturi effect. When a flowing fluid at a given pressure and temperature passes through a constriction (such as the throat of a convergent-divergent nozzle or a valve in a pipe) into a lower pressure environment the fluid velocity increases. At initially subsonic upstream conditions, the conservation of mass principle requires the fluid velocity to increase as it flows through the smaller cross-sectional area of the constriction. At the same time, the Venturi effect causes the static pressure, and therefore the density, to decrease downstream beyond the constriction. Choked flow is a limiting condition where the mass flow will not increase with a further decrease in the downstream pressure environment while upstream pressure is fixed. Note that the limited parameter is velocity, and thus mass flow can be increased with increased upstream pressure (increased fluid density).\n\nFor homogeneous fluids, the physical point at which the choking occurs for adiabatic conditions, is when the exit plane velocity is at sonic conditions; i.e., at a Mach number of 1. At choked flow, the mass flow rate can be increased only by increasing density upstream and at the choke point.\n\nThe choked flow of gases is useful in many engineering applications because the mass flow rate is independent of the downstream pressure, and depends only on the temperature and pressure and hence the density of the gas on the upstream side of the restriction. Under choked conditions, valves and calibrated orifice plates can be used to produce a desired mass flow rate.\n\nIf the fluid is a liquid, a different type of limiting condition (also known as choked flow) occurs when the Venturi effect acting on the liquid flow through the restriction causes a decrease of the liquid pressure beyond the restriction to below that of the liquid's vapor pressure at the prevailing liquid temperature. At that point, the liquid will partially flash into bubbles of vapor and the subsequent collapse of the bubbles causes cavitation. Cavitation is quite noisy and can be sufficiently violent to physically damage valves, pipes and associated equipment. In effect, the vapor bubble formation in the restriction prevents the flow from increasing any further.\n\nAll gases flow from upstream higher pressure sources to downstream lower pressure sources. There are several situations in which choked flow occurs, such as the change of cross section in a de Laval nozzle or flow through an orifice plate.\n\nAssuming ideal gas behaviour, steady-state choked flow occurs when the downstream pressure falls below a critical value formula_1. That critical value can be calculated from the dimensionless critical pressure ratio equation\nwhere formula_3 is the heat capacity ratio formula_4 of the gas and where formula_5 is the total (stagnation) upstream pressure.\n\nFor air with a heat capacity ratio formula_6, then formula_7; other gases have formula_3 in the range 1.09 (e.g. butane) to 1.67 (monatomic gases), so the critical pressure ratio varies in the range formula_9, which means that, depending on the gas, choked flow usually occurs when the downstream static pressure drops to below 0.487 to 0.587 times the absolute pressure in stagnant upstream source vessel.\n\nWhen the gas velocity is choked, the equation for the mass flow rate is:\n\nThe mass flow rate is primarily dependent on the cross-sectional area formula_11 of the nozzle throat and the upstream pressure formula_12, and only weakly dependent on the temperature formula_13. The rate does not depend on the downstream pressure at all. All other terms are constants that depend only on the composition of the material in the flow. \"Although the gas velocity reaches a maximum and becomes choked, the mass flow rate is not choked\". The mass flow rate can still be increased if the upstream pressure is increased as this increases the density of the gas entering the orifice.\nThe value of formula_14 can be calculated using the below expression:\n\nThe above equations calculate the steady state mass flow rate for the pressure and temperature existing in the upstream pressure source.\n\nIf the gas is being released from a closed high-pressure vessel, the above steady state equations may be used to approximate the \"initial\" mass flow rate. Subsequently, the mass flow rate will decrease during the discharge as the source vessel empties and the pressure in the vessel decreases. Calculating the flow rate versus time since the initiation of the discharge is much more complicated, but more accurate. Two equivalent methods for performing such calculations are explained and compared online.\n\nThe technical literature can be very confusing because many authors fail to explain whether they are using the universal gas law constant R which applies to any ideal gas or whether they are using the gas law constant R which only applies to a specific individual gas. The relationship between the two constants is R = R / M where M is the molecular weight of the gas.\n\nIf the upstream conditions are such that the gas cannot be treated as ideal, there is no closed form equation for evaluating the choked mass flow. Instead, the gas expansion should be calculated by reference to real gas property tables, where the expansion takes place at constant enthalpy.\n\nThe minimum pressure ratios required for choked conditions to occur (when some typical industrial gases are flowing) are presented in Table 1. The ratios were obtained using the criterion that choked flow occurs when the ratio of the absolute upstream pressure to the absolute downstream pressure is equal to or greater than formula_16, where formula_3 is the specific heat ratio of the gas. The minimum pressure ratio may be understood as the ratio between the upstream pressure and the pressure at the nozzle throat when the gas is traveling at Mach 1; if the upstream pressure is too low compared to the downstream pressure, sonic flow cannot occur at the throat.\n\nNotes:\n\nThe flow through a venturi nozzle achieves a much lower nozzle pressure than downstream pressure. Therefore, the pressure ratio is the comparison between the upstream and nozzle pressure. Therefore, flow through a venturi can reach Mach 1 with a much lower upstream to downstream ratio.\n\nThe flow of real gases through thin-plate orifices never becomes fully choked. The mass flow rate through the orifice continues to increase as the downstream pressure is lowered to a perfect vacuum, though the mass flow rate increases slowly as the downstream pressure is reduced below the critical pressure. Cunningham (1951) first drew attention to the fact that choked flow will not occur across a standard, thin, square-edged orifice.\n\nIn the case of upstream air pressure at atmospheric pressure and vacuum conditions downstream of an orifice, both the air velocity and the mass flow rate becomes choked or limited when sonic velocity is reached through the orifice.\n\nFigure 1a shows the flow through the nozzle when it is completely subsonic (i.e. the nozzle is not choked). The flow in the chamber accelerates as it converges toward the throat, where it reaches its maximum (subsonic) speed at the throat. The flow then decelerates through the diverging section and exhausts into the ambient as a subsonic jet. Lowering the back pressure, in this state, will increase the flow speed everywhere in the nozzle.\n\nWhen the back pressure, p, is lowered enough, the flow speed is Mach 1 at the throat, as in figure 1b. The flow pattern is exactly the same as in subsonic flow, except that the flow speed at the throat has just reached Mach 1. Flow through the nozzle is now choked since further reductions in the back pressure can't move the point of M=1 away from the throat. However, the flow pattern in the diverging section does change as you lower the back pressure further.\n\nAs p is lowered below that needed to just choke the flow, a region of supersonic flow forms just downstream of the throat. Unlike in subsonic flow, the supersonic flow accelerates as it moves away from the throat. This region of supersonic acceleration is terminated by a normal shock wave. The shock wave produces a near-instantaneous deceleration of the flow to subsonic speed. This subsonic flow then decelerates through the remainder of the diverging section and exhausts as a subsonic jet. In this regime if you lower or raise the back pressure you move the shock wave away from (increase the length of supersonic flow in the diverging section before the shock wave) the throat.\n\nIf the p is lowered enough, the shock wave will sit at the nozzle exit (figure 1d). Due to the very long region of acceleration (the entire nozzle length) the flow speed will reach its maximum just before the shock front. However, after the shock the flow in the jet will be subsonic.\n\nLowering the back pressure further causes the shock to bend out into the jet (figure 1e), and a complex pattern of shocks and reflections is set up in the jet which will involve a mixture of subsonic and supersonic flow, or (if the back pressure is low enough) just supersonic flow. Because the shock is no longer perpendicular to the flow near the nozzle walls, it deflects the flow inward as it leaves the exit producing an initially contracting jet. This is referred as overexpanded flow because in this case the pressure at the nozzle exit is lower than that in the ambient (the back pressure)- i.e. the flow has been expanded by the nozzle too much.\n\nA further lowering of the back pressure changes and weakens the wave pattern in the jet. Eventually the back pressure will be low enough so that it is now equal to the pressure at the nozzle exit. In this case, the waves in the jet disappear altogether (figure 1f), and the jet will be uniformly supersonic. This situation, since it is often desirable, is referred to as the 'design condition'.\n\nFinally, if the back pressure is lowered even further we will create a new imbalance between the exit and back pressures (exit pressure greater than back pressure), figure 1g. In this situation (called 'underexpanded') what we call expansion waves (that produce gradual turning perpendicular to the axial flow and acceleration in the jet) form at the nozzle exit, initially turning the flow at the jet edges outward in a plume and setting up a different type of complex wave pattern.\n\n\n"}
{"id": "42421632", "url": "https://en.wikipedia.org/wiki?curid=42421632", "title": "City of Anatol", "text": "City of Anatol\n\nCity of Anatol (German: Stadt Anatol) is a 1936 German drama film directed by Viktor Tourjansky and starring Gustav Fröhlich, Brigitte Horney and Fritz Kampers. It is based on a 1932 novel \"City of Anatol\" by Bernhard Kellermann. The film is set in a small city in the Balkans, where the discovery of oil leads to a major boom. A separate French language version \"Wells in Flames\" (\"Puits en flames\") was made, also directed by Tourjansky but featuring a different cast.\n\n\n"}
{"id": "2995992", "url": "https://en.wikipedia.org/wiki?curid=2995992", "title": "Duoplasmatron", "text": "Duoplasmatron\n\nThe Duoplasmatron is an ion source in which a cathode filament emits electrons into a vacuum chamber. A gas such as argon is introduced in very small quantities into the chamber, where it becomes charged or ionized through interactions with the free electrons from the cathode, forming a plasma. The plasma is then accelerated through a series of at least two highly charged grids, and becomes an ion beam, moving at fairly high speed from the aperture of the device.\n\nThe duoplasmatron was invented by Manfred von Ardenne. Later development by Harold R. Kaufman resulted in the \"Kaufman Duoplasmatron\" which has been used for applications as diverse as semiconductor manufacture, and spacecraft propulsion.\n\n\n"}
{"id": "2746102", "url": "https://en.wikipedia.org/wiki?curid=2746102", "title": "Earth from the Air", "text": "Earth from the Air\n\nEarth from the Air is a popular collection of environmental photographs taken from the air by Yann Arthus-Bertrand. They have been published in a number of books together with text describing environmental concerns related to the photographs.\n\nLarge format versions of many of the photographs have also been exhibited in various cities. In London they were on display outside the Natural History Museum; and were subsequently exhibited outside City Hall for most of 2005, together with a giant world map on the ground showing where each photograph was taken.\n\n"}
{"id": "48876147", "url": "https://en.wikipedia.org/wiki?curid=48876147", "title": "Esterling", "text": "Esterling\n\nThe esterling is an obsolete Belgian unit of mass.\n\n"}
{"id": "22529429", "url": "https://en.wikipedia.org/wiki?curid=22529429", "title": "Extended rotation forest", "text": "Extended rotation forest\n\nAn extended rotation forest is a forest stand for which the harvest age is increased beyond the optimum economic harvest age to provide larger trees, wildlife habitat, and other non-timber values. \n\nAdvantages of extended rotation forestry included enhanced carbon storage, better wood quality and the ability to create habitat for old growth dependent species. The main disadvantages of extended rotations is the lower present value of the stand and timber supply issues. These impacts can be mitigated by the application of commercial thinning. In the Pacific Northwest of the United States, commercially thinned stands have yet to reach cumulation age in spite of reaching ages of over 100 years on good to moderate sites. In managed for values other than timber, extended rotations are being considered. In Oregon, some environmental groups are calling for rotations as long as 250 years. The argument centres on the assertion that short-rotation management on either biological or financial rotations leads to significant loses of wood production potential as well as external costs to society.\n"}
{"id": "2193888", "url": "https://en.wikipedia.org/wiki?curid=2193888", "title": "Force-free magnetic field", "text": "Force-free magnetic field\n\nA force-free magnetic field is a magnetic field that arises when the plasma pressure is so small, relative to the magnetic pressure, that the plasma pressure may be ignored, and so only the magnetic pressure is considered. For a force free field, the electric current density is either zero or parallel to the magnetic field. The name \"force-free\" comes from being able to neglect the force from the plasma.\n\nNeglecting the effects of gravity, the Navier-Stokes equation for a plasma, in steady state, reads\n\nformula_1\n\nwhere formula_2 is the thermal pressure, formula_3 is the magnetic field and formula_4 is the electric current. Assuming that the gas pressure formula_5 is small compared to the magnetic pressure, i.e.,\n\nformula_6\n\nthen the pressure term can be neglected. Here formula_7 is the magnetic permeability of the plasma. Therefore,\n\nformula_8.\n\nThis equation implies that:\nformula_9. e.g. the current density is either\nzero or parallel to the magnetic field, and where formula_10 is a spatial-varying function\nto be determined. Combining this equation with Maxwell's equations:\n\nformula_11\n\nformula_12\n\nand the vector identity:\n\nformula_13\n\nleads to a pair of equations for formula_10 and formula_15:\n\nformula_16\n\nformula_17\n\nIn the corona of the sun, the ratio of the gas pressure to the magnetic pressure is ~0.004, and so there the magnetic field is force-free.\n\n\n\nand\n\n\n"}
{"id": "25351637", "url": "https://en.wikipedia.org/wiki?curid=25351637", "title": "Grane Oil Pipeline", "text": "Grane Oil Pipeline\n\nGrane oil pipeline () is a pipeline system in western Norway. It is long and runs from Grane oil field to Sture terminal, located north of Bergen, Norway. The pipeline operations commenced with start of production in the Grane oil field.\n\nThe pipeline diameter is . Capacity of Grane oil pipeline is of oil. The greatest sea depth pipeline passes through is - through the Norwegian Trench. Over 110,000 tonnes of rock and gravel was used on the seabed to establish the correct foundation for the pipeline.\nThe estimated lifetime of the pipeline is expected to be 30 years.\n\nThe Grane pipeline is operated by Statoil and includes other partners.\n\nTotal investment at start-up was nearly 1.6 billion NOK's.\n\n\n"}
{"id": "11162073", "url": "https://en.wikipedia.org/wiki?curid=11162073", "title": "Hassi R'Mel integrated solar combined cycle power station", "text": "Hassi R'Mel integrated solar combined cycle power station\n\nThe Hassi R'Mel integrated solar combined cycle power station is a hybrid power station near Hassi R'Mel in Algeria. The plant combines a 25 MW parabolic trough concentrating solar power array, covering an area of over 180,000 m, in conjunction with a 130 MW combined cycle gas turbine plant, so cutting carbon emissions compared to a traditional power station. The output from the solar array is used in the steam turbine.\n\nThe construction contract was signed on January 5, 2007 and the plant was developed by New Energy Algeria (NEAL), a joint venture between Sonatrach, Sonelgaz, and SIM.\n\nThe station began producing electricity in June 2011. It was inaugurated July 14, 2011.\n\n\n"}
{"id": "55244", "url": "https://en.wikipedia.org/wiki?curid=55244", "title": "Hypersonic speed", "text": "Hypersonic speed\n\nIn aerodynamics, a hypersonic speed is one that greatly exceeds the speed of sound, particularly Mach 5 and above.\n\nThe precise Mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around Mach 5. The hypersonic regime is often alternatively defined as speeds where ramjets do not produce net thrust.\n\nWhile the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the absence of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. The peculiarity in hypersonic flows are as follows:\n\nAs a body's Mach number increases, the density behind a bow shock generated by the body also increases, which corresponds to a decrease in volume behind the shock due to conservation of mass. Consequently, the distance between the bow shock and the body decreases at higher Mach numbers.\n\nAs Mach numbers increase, the entropy change across the shock also increases, which results in a strong entropy gradient and highly vortical flow that mixes with the boundary layer.\n\nA portion of the large kinetic energy associated with flow at high Mach numbers transforms into internal energy in the fluid due to viscous effects. The increase in internal energy is realized as an increase in temperature. Since the pressure gradient normal to the flow within a boundary layer is approximately zero for low to moderate hypersonic Mach numbers, the increase of temperature through the boundary layer coincides with a decrease in density. This causes the bottom of the boundary layer to expand, so that the boundary layer over the body grows thicker and can often merge with the shock wave near the body leading edge.\n\nHigh temperatures due to a manifestation of viscous dissipation cause non-equilibrium chemical flow properties such as vibrational excitation and dissociation and ionization of molecules resulting in convective and radiative heat-flux.\n\nAlthough \"subsonic\" and \"supersonic\" usually refer to speeds below and above the local speed of sound respectively, aerodynamicists often use these terms to refer to particular ranges of Mach values. This occurs because a \"transonic regime\" exists around M=1 where approximations of the Navier–Stokes equations used for subsonic design no longer apply, partly because the flow locally exceeds M=1 even when the freestream Mach number is below this value.\n\nThe \"supersonic regime\" usually refers to the set of Mach numbers for which linearised theory may be used; for example, where the (air) flow is not chemically reacting and where heat transfer between air and vehicle may be reasonably neglected in calculations.\n\nGenerally, NASA defines \"high\" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Among the aircraft operating in this regime are the Space Shuttle and (theoretically) various developing spaceplanes.\n\nIn the following table, the \"regimes\" or \"ranges of Mach values\" are referenced instead of the usual meanings of \"subsonic\" and \"supersonic\".\n\nThe categorization of airflow relies on a number of similarity parameters, which allow the simplification of a nearly infinite number of test cases into groups of similarity. For transonic and compressible flow, the Mach and Reynolds numbers alone allow good categorization of many flow cases.\n\nHypersonic flows, however, require other similarity parameters. First, the analytic equations for the oblique shock angle become nearly independent of Mach number at high (~>10) Mach numbers. Second, the formation of strong shocks around aerodynamic bodies means that the freestream Reynolds number is less useful as an estimate of the behavior of the boundary layer over a body (although it is still important). Finally, the increased temperature of hypersonic flows mean that real gas effects become important. For this reason, research in hypersonics is often referred to as aerothermodynamics, rather than aerodynamics.\n\nThe introduction of real gas effects means that more variables are required to describe the full state of a gas. Whereas a stationary gas can be described by three variables (pressure, temperature, adiabatic index), and a moving gas by four (flow velocity), a hot gas in chemical equilibrium also requires state equations for the chemical components of the gas, and a gas in nonequilibrium solves those state equations using time as an extra variable. This means that for a nonequilibrium flow, something between 10 and 100 variables may be required to describe the state of the gas at any given time. Additionally, rarefied hypersonic flows (usually defined as those with a Knudsen number above 0.1) do not follow the Navier–Stokes equations.\n\nHypersonic flows are typically categorized by their total energy, expressed as total enthalpy (MJ/kg), total pressure (kPa-MPa), stagnation pressure (kPa-MPa), stagnation temperature (K), or flow velocity (km/s).\n\nWallace D. Hayes developed a similarity parameter, similar to the Whitcomb area rule, which allowed similar configurations to be compared.\n\nHypersonic flow can be approximately separated into a number of regimes. The selection of these regimes is rough, due to the blurring of the boundaries where a particular effect can be found.\n\nIn this regime, the gas can be regarded as an ideal gas. Flow in this regime is still Mach number dependent. Simulations start to depend on the use of a constant-temperature wall, rather than the adiabatic wall typically used at lower speeds. The lower border of this region is around Mach 5, where ramjets become inefficient, and the upper border around Mach 10-12.\n\nThis is a subset of the perfect gas regime, where the gas can be considered chemically perfect, but the rotational and vibrational temperatures of the gas must be considered separately, leading to two temperature models. See particularly the modeling of supersonic nozzles, where vibrational freezing becomes important.\n\nIn this regime, diatomic or polyatomic gases (the gases found in most atmospheres) begin to dissociate as they come into contact with the bow shock generated by the body. Surface catalysis plays a role in the calculation of surface heating, meaning that the type of surface material also has an effect on the flow. The lower border of this regime is where any component of a gas mixture first begins to dissociate in the stagnation point of a flow (which for nitrogen is around 2000 K). At the upper border of this regime, the effects of ionization start to have an effect on the flow.\n\nIn this regime the ionized electron population of the stagnated flow becomes significant, and the electrons must be modeled separately. Often the electron temperature is handled separately from the temperature of the remaining gas components. This region occurs for freestream flow velocities around 10–12 km/s. Gases in this region are modeled as non-radiating plasmas.\n\nAbove around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. The modeling of gases in this regime is split into two classes:\nThe modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.\n\n\n\n\n"}
{"id": "34493193", "url": "https://en.wikipedia.org/wiki?curid=34493193", "title": "IEEE Smart Grid", "text": "IEEE Smart Grid\n\nIEEE Smart Grid is an initiative launched by IEEE to help provide expertise and guidance for individuals and organizations involved in the modernization and optimization of the power grid, better known as the \"smart grid\". IEEE Smart Grid encompasses an array of activities, including development of new smart grid-related standards, best practices, publications, and conferences and educational opportunities.\n\nIEEE's involvement in the emerging smart grid reaches back to its development of power and energy standards. In 2007, the Energy Independence and Security Act (EISA) was signed into law by President George W. Bush. EISA directed that under the auspices of the US Department of Energy (US DOE), the National Institute of Standards and Technology (NIST) be tasked with the development of a framework including protocols and model standards for information management to achieve interoperability of smart grid devices and systems. In Title XIII, Section 1305(a)(2) of EISA, IEEE was named as one of the Standard Setting Organizations (SSOs) whose work would be vital in the development of standards for NIST's Smart Grid Interoperability Framework. As part of the first phase of NIST's Smart Grid Interoperability Framework, 16 initial standards were offered for adoption, including three developed by IEEE: C37.118, IEEE 1547, and IEEE 1686-2007.\n\nIn May 2009, IEEE announced the launch of a new smart grid effort targeted to the power engineering, communications, and information technology disciplines. The first project in this effort, titled \"The IEEE Standard 2030 Guide for Smart Grid Interoperability of Energy Technology and Information Technology Operation with the Electric Power System (EPS) and End-Use Applications and Loads\" (IEEE 2030) established a knowledge framework for understanding and defining smart grid interoperability of the electric power system with end use applications, setting the stage for future smart grid-related standards.\n\nIn January 2010, IEEE launched the first phase of IEEE Smart Grid, a new global initiative designed to bring together the organization's broad array of resources to provide expertise and guidance for those involved in Smart Grid worldwide. Later that year, IEEE published inaugural editions of two new publications, cross-discipline archival journals entitled \"IEEE Transactions on Smart Grid\", and \"IEEE Transactions on Sustainable Energy\".\n\nIEEE's approach to the smart grid is to view it as a large \"System of Systems\" wherein individual smart grid domains based on the NIST Smart Grid Conceptual Model are expanded into three layers: Power and Energy, Communications, and IT/Computer. IEEE considers the Communications and IT/Computer layers to be enabling infrastructure for the Power and Energy layer.\n\nAs part of the first phase of IEEE Smart Grid, IEEE unveiled the IEEE Smart Grid Portal in January 2010. As originally designed, the site served as an online clearinghouse providing smart grid-focused news, information, commentary, videos, and event information for a broad audience including business and industry, academic, and government users, as well as consumers. The portal is also home to the initiative's monthly digest, the \"IEEE Smart Grid Newsletter\".\n\nThe IEEE Smart Grid Portal was relaunched in September 2011. The redesigned site included improved search capabilities and other new features, such as a broader selection of video interviews and Q&A's with industry experts, business leaders, and researchers. It also facilitated greater user access to approved and in-development IEEE smart grid standards and an expanded conference calendar.\n\nIEEE continues to work closely with NIST on its standards roadmap and conformance testing/certification framework for the smart grid. The organization also collaborates with other global standards bodies to effectively facilitate standards coordination and to ensure the intensifying smart grid movement’s success.\n\nThere are more than 100 standards that have been approved or in development relating to the smart grid. Among the broad number of systems and technologies addressed by these standards are broadband over power line, cyber security, distributed energy resources, Distributed Network Protocol (DNP3), and Greenhouse gas emissions credits, among others.\n\n\n\nIEEE Smart Grid and its participating societies sponsor and host major conferences, symposiums, and other events on a global basis. Dedicated to providing a forum for the sharing of ideas and information about smart grid concepts, deployments, and technology advancements, IEEE Smart Grid events generally span several days in length and attract attendees from around the world. Conference programs often consist of educational tracks with keynote speeches, panel discussions, and roundtables led by industry notables, researchers, engineers, academics, policymakers, and other key stakeholders.\n\nAmong IEEE Smart Grid flagship events are its Innovative Smart Grid Technologies (ISGT) Conference, SmartGridComm, and Smart Grid World Forum. Other recent major IEEE Smart Grid events include:\n\n\n\nIEEE Smart Grid is also home to smaller workshops, sessions, and continuing education courses, like the IEEE PES \"Plain Talk About the Electric Power System\" series, which are hosted by various societies and their chapters.\n\nThrough its Smart Grid Initiative, societies, working groups, committees and sub-committees, IEEE has published nearly 5,000 papers, manuscripts, guides, and other documents relating to the smart grid, including:\n\nIn January 2011, IEEE unveiled the \"IEEE Smart Grid Newsletter\", a monthly electronic digest. The publication's content focuses on practical and technical information, as well as commentary and opinion on emerging smart grid technologies, new standards, global deployments, and other smart grid-related subject matter. Contributors include industry leaders, prominent researchers and academics, and advocates and policy experts. Dr. S. Massoud Amin, Director and Honeywell/H.W. Sweatt Chair, Technological Leadership Institute, and Professor of Electrical and Computer Engineering, University of Minnesota, is the publication's Chairman, and William Sweet, Emeritus News Editor, IEEE Spectrum, is its Managing Editor.\n\nIntended to disseminate smart grid research results on energy generation, transmission, distribution and delivery, \"IEEE Transactions on Smart Grid\" offers original research on smart grid theories, technologies, design, policies, and implementation. It also accepts manuscripts on smart grid design and implementation, and evaluation of energy systems involving smart grid technologies and applications. Published quarterly, \"IEEE Transactions on Smart Grid\" is jointly produced by multiple IEEE societies, including its Computational Intelligence Society, Communications Society, Computer Society, Control Systems Society, Industry Applications Society, Industrial Electronics Society, Instrumentation and Measurement Society, Power Electronics Society, Power & Energy Society, and Signal Processing Society. In August 2011, the journal's \"Protecting Smart Grid Automation Systems Against Cyberattacks,\" authored by IEEE members Dong Wei and Yan Lu became the three millionth document in IEEE Xplore, IEEE's extensive digital library. Mohammad Shahidehpour is the current Editor-in-Chief of \"IEEE Transactions on Smart Grid\".\n\nA quarterly, cross-discipline journal featuring original research, \"IEEE Transactions on Sustainable Energy\" focuses on sustainable energy generation, transmission, distribution and delivery. Manuscripts concentrating on sustainable energy power systems design, implementation, and evaluations, as well as surveys of existing work are also accepted for consideration. Launched in April 2010, \"IEEE Transactions on Sustainable Energy\" is jointly published by IEEE's Industry Applications Society, Industrial Electronics Society, Instrumentation and Measurement Society, IEEE Oceanic Engineering Society, Power Electronics Society, Power & Energy Society, Photonics Society, and the Society on Social Implications of Technology. The journal's current Editor-in-Chief is Saifur Rahman, Joseph Loring Professor of ECE at Virginia Polytechnic Institute and State University (Virginia Tech).\n\nIEEE is home to 38 technical Societies encompassing a large array of disciplines and specialized fields of interest such as aerospace and electronic systems, medicine and biology engineering, magnetics, robotics, and vehicular technology. Many of these societies are engaged in smart grid-related work, however, due to their specific areas of expertise, several have emerged as leaders in smart grid technology development and promotion:\n\nThe IEEE Communications Society (IEEE ComSoc) is an IEEE professional society of more than 8,800 communications industry professionals. As part of its goal to foster original work in all aspects of communications science, engineering, and technology, IEEE ComSoc sponsors a variety of publications, conferences, educational programs, local activities, and technical committees. IEEE ComSoc is a key stakeholder in IEEE Smart Grid, sponsoring a variety of related standards like IEEE 1901-2010 and conferences like the IEEE International Symposium on Power Line Communications and its Applications (ISPLC) and SmartGridComm, one of IEEE Smart Grid's flagship events. Along with a number of its sister societies, IEEE ComSoc jointly publishes \"IEEE Transactions on Smart Grid\".\n\nThe IEEE Computer Society is a professional society for computing professionals and researchers based in Washington, DC. With 85,000 members worldwide, it is the largest IEEE society. The group offers a variety of resources, including certification programs, printed and electronic books and journals, and conferences and events. The IEEE Computer Society is an active participant in IEEE Smart Grid, through the sponsorship of smart grid-applicable standards and practices, like IEEE 1471 and IEEE 42010-2011, and events such as ISGT. It is also one of the publishing societies for \"IEEE Transactions on Smart Grid\".\n\nThe IEEE Power & Energy Society (IEEE PES) is home to 28,000 electric power energy professionals. Among the technical subject matter addressed by IEEE PES are electric power research, as well as system design, installation, and operation. The society aims to ensure the safe, sustainable, economic and reliable conversion, generation, transmission, distribution, storage and usage of electric energy, including its measurement and control. IEEE PES is one of the main proponents of IEEE Smart Grid, sponsoring numerous smart grid-related standards, such as IEEE 487-2007, IEEE 1138-2009, IEEE 1222-2003, and IEEE 1615-2007. The group frequently sponsors smart grid-related events on a national, international, and regional basis, including ISGT, IEEE Power Systems Conference & Exposition (PSCE), and the IEEE PES Plain Talk series.\n\nThe IEEE Society on Social Implications of Technology (SSIT) focuses on the environmental, health, and safety implications of technology. Among the topics addressed by its 2,000 global members are ethics and professional responsibility, the history of electrotechnology, public policy, and technology-related societal issues. IEEE SSIT members are generally professionals from multiple disciplines, including energy, information technology, and telecommunications. The group publishes \"IEEE Technology and Society Magazine\", a quarterly journal. IEEE SSIT sponsors or contributes regularly to IEEE Smart Grid standards and events, and is one of the publishing societies of \"IEEE Transactions on Sustainable Energy\".\n\nMany of the participating IEEE Smart Grid members have had demonstrable impact on the smart grid ecosystem. Often called upon as smart grid experts by the media, members appear frequently in magazines and newsletters such as \"Electric Energy Online\", \"Electric Light & Power\" magazine, and \"FierceSmartGrid\", as well as on broadcast programs like NPR's \"Science Friday\" and \"blogtalkradio\". \"greentechgrid\" unveiled \"The Networked Grid 100: Movers and Shakers of the Smart Grid\" in February 2010, naming IEEE Fellow, John D. McDonald, as one of their listees. In November 2011, \"FierceEnergy\" magazine announced its inaugural \"Power Players -- The 15 Most Influential People in Energy\" list, which included four IEEE Smart Grid members: IEEE Life Member Dick DeBlasio, IEEE Fellow Erich Gunther, McDonald, and IEEE Computer Society member, Andres Carvallo.\n\nIEEE Smart Grid members have also had impact in other areas, including public policy. In May 2004, Erich Gunther was appointed by the US DOE as a member of its GridWise Architecture Council (GWAC), and currently serves as its chair. IEEE Smart Grid member, John D. McDonald, was selected as the inaugural member of the US DOE Electric Advisory Committee, which he served on during 2008. He was followed by IEEE Smart Grid Chair, Wanda Reder, who was appointed to the committee in 2010.\n\nOn July 1, 2010, McDonald along with Dr. George W. Arnold, National Coordinator for Smart Grid, NIST; Mason W. Emnett, Associate Director of The Office of Energy Policy and Innovation, Federal Energy Regulatory Commission (FERC); Conrad Eustis, Director of Retail Technology Development, Portland General Electric; and Lillie Coney, Associate Director, Electronic Privacy Information Center were called upon to brief the U.S. House of Representatives’ Subcommittee on Technology and Innovation, House Committee on Science and Technology. At the hearing entitled \"Smart Grid Architecture and Standards: Assessing Coordination and Progress\", he provided testimony on the progress of standards for Smart Grid interoperability and cyber security, stating: \"Also crucial to this undertaking are developing system architecture and standards that provide the essential foundation for bringing together the electrical and communications infrastructure, and for evolving technology to meet many and disparate needs. Both the IEEE through its Standards Association and NIST have already shown tremendous progress in these areas.\"\n\n\n"}
{"id": "2332266", "url": "https://en.wikipedia.org/wiki?curid=2332266", "title": "Iodine heptafluoride", "text": "Iodine heptafluoride\n\nIodine heptafluoride, also known as iodine(VII) fluoride or iodine fluoride, is an interhalogen compound with the chemical formula IF. It has an unusual pentagonal bipyramidal structure, as predicted by VSEPR theory. The molecule can undergo a pseudorotational rearrangement called the Bartell mechanism, which is like the Berry mechanism but for a heptacoordinated system.\nIt forms colourless crystals, which melt at 4.5 °C: the liquid range is extremely narrow, with the boiling point at 4.77 °C. The dense vapor has a mouldy, acrid odour. The molecule has D symmetry.\n\nIF is prepared by passing F through liquid IF at 90 °C, then heating the vapours to 270 °C. Alternatively, this compound can be prepared from fluorine and dried palladium or potassium iodide to minimize the formation of IOF, an impurity arising by hydrolysis. Iodine heptafluoride is also produced as a by-product when dioxygenyl hexafluoroplatinate is used to prepare other platinum(V) compounds such as potassium hexafluoroplatinate(V), using potassium fluoride in iodine pentafluoride solution:\n\nIF is highly irritating to both the skin and the mucous membranes. It also is a strong oxidizer and can cause fire on contact with organic material.\n\n"}
{"id": "10663645", "url": "https://en.wikipedia.org/wiki?curid=10663645", "title": "Leona Canyon Regional Open Space Preserve", "text": "Leona Canyon Regional Open Space Preserve\n\nLeona Canyon Regional Open Space Preserve is a regional park located in Oakland, CA that is part of the East Bay Regional Parks system. It is located off Keller Avenue near Oak Knoll Naval Hospital and it extends to Merritt College.\n\n"}
{"id": "55665052", "url": "https://en.wikipedia.org/wiki?curid=55665052", "title": "Liquid marbles", "text": "Liquid marbles\n\nLiquid marbles are non-stick droplets (normally aqueous) wrapped by micro- or nano-metrically scaled hydrophobic, colloidal particles (Teflon, polyethylene, lycopodium powder, carbon black, etc.); representing a platform for a diversity of chemical and biological applications. Liquid marbles are also found naturally; aphids convert honeydew droplets into marbles. A variety of non-organic and organic liquids may be converted into liquid marbles. Liquid marbles demonstrate elastic properties and do not coalesce when bounced or pressed lightly. Liquid marbles demonstrate a potential as micro-reactors, micro-containers for growing micro-organisms and cells, micro-fluidics devices, and have even been used in unconventional computing. Liquid marbles remain stable on solid and liquid surfaces. Statics and dynamics of rolling and bouncing of liquid marbles were reported. Liquid marbles coated with poly-disperse and mono-disperse particles have been reported. Liquid marbles are not hermetically coated by solid particles but connected to the gaseous phase. Kinetics of the evaporation of liquid marbles has been investigated.\n\nLiquid marbles were first reported by P. Aussillous and D. Quere in 2001, who described a new method to construct portable water droplets in the atmospheric environment with hydrophobic coating on their surface to prevent the contact between water and the solid ground (Figure 1). Liquid marbles provide a new approach to transport liquid mass on the solid surface, which sufficiently transform the inconvenient glass containers into flexible, user-specified hydrophobic coating composed of powders of hydrophobic materials. Since then, the applications of liquid marbles in no-loss mass transport, microfluidics and microreactors have been extensively investigated. However, liquid marbles only reflect the water behavior at the solid-air interface, while there is no report on the water behavior at the liquid-liquid interface, as a result of the so-called coalescence cascade phenomenon.\n\nWhen a water droplet is in contact with a water reservoir, it will quickly pinch off from the reservoir and form a smaller daughter droplet, while this daughter droplet will continue to go through a similar contact-pinch off-splitting process until completed coalescence into the reservoir, the combination or summary of these self-similar coalescence processes is called coalescence cascade. The underlying mechanism of coalescence cascade has been studied in detail but there has been mere attempt to control and make use of it.. Until recently, Liu et al. has filled this void by proposing a new method to control coalescence cascade by using nanostructured coating at the liquid-liquid interface, —the interfacial liquid marbles.\n\nSimilar to liquid marbles at the solid-air interface, the interfacial liquid marbles are constructed on the hexane/water interface using water droplets with a surface coating composed of nanoscale materials with special wettability (Figure 2). To realize interfacial water marbles at hexane/water interface, the individual particle size of the surface coating layer should be as small as possible, so that the contact line between the particles and the water reservoir can be minimized; special wettability with mixed hydrophobicity and hydrophilicity is also preferred for the interfacial water marble formation. The interfacial water marble can be fabricated by firstly coating a water droplet with nanomaterials with special wettability, e.g. hybrid carbon nanowires, graphene oxide. Afterwards a secondary coating layer of polyvinylidene fluoride (PVDF) is applied onto the coated water droplet. The doubly-coated water droplet is then cast into the hexane/water mixture and eventually settled at the hexane/water interface to form the interfacial water marble. During this process, the PVDF coating quickly diffused into hexane to balance the hydrophobic interaction between hexane and the water droplet, while the nanomaterials quickly self-assembled into a nanostructured protective layer on the droplet surface through the Marangoni effect.\n\nThe interfacial water marble can completely resist coalescence cascade and exist nearly permanently at the hexane/water interface, providing that the hexane phase is not depleted by vaporization. The interfacial water marbles can also realize a series of stimuli-responsive motions by integrating the functional materials into the surface coating layer. Due to their uniqueness in both form and behavior, the interfacial water marbles are speculated to have remarkable applications in microfluidics, microreactors and mass-transport.\n"}
{"id": "186821", "url": "https://en.wikipedia.org/wiki?curid=186821", "title": "Micrometeoroid", "text": "Micrometeoroid\n\nA micrometeoroid is a tiny meteoroid; a small particle of rock in space, usually weighing less than a gram. A micrometeorite is such a particle that survives passage through the Earth's atmosphere and reaches the Earth's surface.\n\nMicrometeoroids are very small pieces of rock or metal broken off from larger chunks of rock and debris often dating back to the birth of the Solar System. Micrometeoroids are extremely common in space. Tiny particles are a major contributor to space weathering processes. When they hit the surface of the Moon, or any airless body (Mercury, the asteroids, etc.), the resulting melting and vaporization causes darkening and other optical changes in the regolith. In order to understand the micrometeoroid population better, a number of spacecraft (including Lunar Orbiter 1, Luna 3, Mars 1 and Pioneer 5) have carried micrometeoroid detectors.\n\nIn 1957 Hans Peterson conducted one of the first direct measurements of the fall of space dust on the Earth, estimating it to be 14,300,000 tons per year. If this were true, then the Moon would be covered to a very great depth as there are limited forms of erosion to remove this material. In 1961 Arthur C. Clarke popularized this possibility in his novel \"A Fall of Moondust\". This was cause for some concern among the groups attempting to land on the Moon, so a series of new studies followed to better characterize the issue. This included the launch of several spacecraft designed to directly measure the micrometeorite flux (Pegasus satellite program) or directly measure the dust on the surface of the Moon (Surveyor Program). These showed that the flux was much lower than earlier estimates, around 10,000 to 20,000 tons per year, and that the surface of the Moon is relatively rocky. Most lunar samples returned during the Apollo Program have micrometeorite impacts marks, typically called \"zap pits\", on their upper surfaces.\nMicrometeoroids have less stable orbits than meteoroids, due to their greater surface area to mass ratio. Micrometeoroids that fall to Earth can provide information on millimeter scale heating events in the solar nebula. Meteorites and micrometeorites (as they are known upon arrival at the Earth's surface) can only be collected in areas where there is no terrestrial sedimentation, typically polar regions. Ice is collected and then melted and filtered so the micrometeorites can be extracted under a microscope.\n\nSufficiently small micrometeoroids avoid significant heating on entry into the Earth's atmosphere. Collection of such particles by high flying aircraft began in the 1970s, since which time these samples of stratosphere-collected interplanetary dust (called \"Brownlee particles\" before their extraterrestrial origin was confirmed) have become an important component of the extraterrestrial materials available for study in laboratories on Earth.\n\nMicrometeoroids pose a significant threat to space exploration. The average velocity of micrometeoroids relative to a spacecraft in orbit is 10 kilometers per second (22,500 mph). Resistance to micrometeoroid impact is a significant design challenge for spacecraft and space suit designers (\"See Thermal Micrometeoroid Garment\"). While the tiny sizes of most micrometeoroids limits the damage incurred, the high velocity impacts will constantly degrade the outer casing of spacecraft in a manner analogous to sandblasting. Long term exposure can threaten the functionality of spacecraft systems.\n\nImpacts by small objects with extremely high velocity (10 kilometers per second) are a current area of research in terminal ballistics. (Accelerating objects up to such velocities is difficult; current techniques include linear motors and shaped charges.) The risk is especially high for objects in space for long periods of time, such as satellites. They also pose major engineering challenges in theoretical low-cost lift systems such as rotovators, space elevators, and orbital airships.\n\n\n"}
{"id": "38567477", "url": "https://en.wikipedia.org/wiki?curid=38567477", "title": "Ministry of Mines and Energy (Togo)", "text": "Ministry of Mines and Energy (Togo)\n\nThe Ministry of Mines and Energy () is a ministry of the government of Togo. The head office is in Lomé. As of 2013 Dammipi Noupokou is the minister.\n\n"}
{"id": "2786041", "url": "https://en.wikipedia.org/wiki?curid=2786041", "title": "Oblique shock", "text": "Oblique shock\n\nAn oblique shock wave, unlike a normal shock, is inclined with respect to the incident upstream flow direction. It will occur when a supersonic flow encounters a corner that effectively turns the flow into itself and compresses. The upstream streamlines are uniformly deflected after the shock wave. The most common way to produce an oblique shock wave is to place a wedge into supersonic, compressible flow. Similar to a normal shock wave, the oblique shock wave consists of a very thin region across which nearly discontinuous changes in the thermodynamic properties of a gas occur. While the upstream and downstream flow directions are unchanged across a normal shock, they are different for flow across an oblique shock wave.\n\nIt is always possible to convert an oblique shock into a normal shock by a Galilean transformation.\n\nFor a given Mach number, M, and corner angle, θ, the oblique shock angle, β, and the downstream Mach number, M, can be calculated. Unlike after a normal shock where M must always be less than 1, in oblique shock M can be supersonic (weak shock wave) or subsonic (strong shock wave). Weak solutions are often observed in flow geometries open to atmosphere (such as on the outside of a flight vehicle). Strong solutions may be observed in confined geometries (such as inside a nozzle intake). Strong solutions are required when the flow needs to match the downstream high pressure condition. Discontinuous changes also occur in the pressure, density and temperature, which all rise downstream of the oblique shock wave.\n\nUsing the continuity equation and the fact that the tangential velocity component does not change across the shock, trigonometric relations eventually lead to the θ-β-M equation which shows θ as a function of M β, and ɣ, where ɣ is the Heat capacity ratio.\n\nformula_1\n\nIt is more intuitive to want to solve for β as a function of M and θ, but this approach is more complicated, the results of which are often contained in tables or calculated through a numerical method.\n\nWithin the θ-β-M equation, a maximum corner angle, θ, exists for any upstream Mach number. When θ > θ, the oblique shock wave is no longer attached to the corner and is replaced by a detached bow shock. A θ-β-M diagram, common in most compressible flow textbooks, shows a series of curves that will indicate θ for each Mach number. The θ-β-M relationship will produce two β angles for a given θ and M, with the larger angle called a strong shock and the smaller called a weak shock. The weak shock is almost always seen experimentally.\n\nThe rise in pressure, density, and temperature after an oblique shock can be calculated as follows:\n\nformula_2\n\nformula_3\n\nformula_4\n\nM is solved for as follows:\n\nformula_5\n\nOblique shocks are often preferable in engineering applications when compared to normal shocks. This can be attributed to the fact that using one or a combination of oblique shock waves results in more favourable post-shock conditions (smaller increase in entropy, less stagnation pressure loss, etc) when compared to utilizing a single normal shock. An example of this technique can be seen in the design of supersonic aircraft engine intakes or supersonic inlets. A type of these inlets is wedge-shaped to compress air flow into the combustion chamber while minimizing thermodynamic losses. Early supersonic aircraft jet engine intakes were designed using compression from a single normal shock, but this approach caps the maximum achievable Mach number to roughly 1.6. Concorde (which first flew in 1969) used variable geometry wedge-shaped intakes to achieve a maximum speed of Mach 2.2. A similar design was used on the F-14 Tomcat (the F-14D was first delivered in 1994) and achieved a maximum speed of Mach 2.34.\n\nMany supersonic aircraft wings are designed around a thin diamond shape. Placing a diamond-shaped object at an angle of attack relative to the supersonic flow streamlines will result in two oblique shocks propagating from the front tip over the top and bottom of the wing, with Prandtl-Meyer expansion fans created at the two corners of the diamond closest to the front tip. When correctly designed, this generates lift.\n\nAs the Mach number of the upstream flow becomes increasingly hypersonic, the equations for the pressure, density, and temperature after the oblique shock wave reach a mathematical limit. The pressure and density ratios can then be expressed as:\n\nformula_6\n\nformula_7\n\nFor a perfect atmospheric gas approximation using γ = 1.4, the hypersonic limit for the density ratio is 6. However, hypersonic post-shock dissociation of O and N into O and N lowers γ, allowing for higher density ratios in nature. The hypersonic temperature ratio is:\n\nformula_8\n\n\n\n"}
{"id": "25950683", "url": "https://en.wikipedia.org/wiki?curid=25950683", "title": "Operating temperature", "text": "Operating temperature\n\nAn operating temperature is the temperature at which an electrical or mechanical device operates. The device will operate effectively within a specified temperature range (which part; TJ usually) which varies based on the device function and application context, and ranges from the minimum operating temperature to the maximum operating temperature (or peak operating temperature). Outside this range of safe operating temperatures the device may fail. Aerospace and military-grade (MIL-STD or FAA?) devices generally operate over a broader temperature range than industrial devices; commercial-grade devices generally have the narrowest operating temperature range.\n\nIt is one component of reliability engineering.\n\nSimilarly, biological systems have a viable temperature range, which might be referred to as an \"operating temperature\".\n\nMost devices are manufactured in several temperature grades. Broadly accepted (?) grades are:\n\nNevertheless, each manufacturer defines its own temperature grades so designers must pay close attention to actual datasheet specifications. For example, Altera uses five temperature grades for its products:\n\n\nThe use of such grades ensures that a device is suitable for its application, and will withstand the environmental conditions in which it is used. Normal operating temperature ranges are affected by several factors, such as the power dissipation of the device. These factors are used to define a \"threshold temperature\" of a device, i.e. its maximum normal operating temperature, and a maximum operating temperature beyond which the device will no longer function. Between these two temperatures, the device will operate at a non-peak level. For instance, a resistor may have a threshold temperature of 70 °C and a maximum temperature of 155 °C, between which it exhibits a thermal derating.\n\nFor electrical devices, the operating temperature may be the junction temperature (T) of the semiconductor in the device. The junction temperature is affected by the ambient temperature, and for integrated circuits, is given by the equation:\n\nin which T is the junction temperature in °C, T is the ambient temperature in °C, P is the power dissipation of the integrated circuit in W, and R is the junction to ambient thermal resistance in °C/W.\n\nElectrical and mechanical devices used in military and aerospace applications may need to endure greater environmental variability, including temperature range.\n\nIn the United States Department of Defense has defined the United States Military Standard for all products used by the United States Armed Forces. A product's environmental design and test limits to the conditions that it will undergo throughout its service life are specified in MIL-STD-810, the \"Department of Defense Test Method Standard for Environmental Engineering Considerations and Laboratory Tests\".\n\nThe MIL-STD-810G standard specifies that the \"operating temperature stabilization is attained when the temperature of the functioning part(s) of the test item considered to have the longest thermal lag is changing at a rate of no more than 2.0 °C (3.6 °F) per hour.\" It also specifies procedures to assess the performance of materials to extreme temperature loads.\n\nMilitary engine turbine blades experience two significant deformation stresses during normal service, creep and thermal fatigue. Creep life of a material is \"highly dependent on operating temperature\", and creep analysis is thus an important part of design validation. Some of the effects of creep and thermal fatigue may be mitigated by integrating cooling systems into the device's design, reducing the peak temperature experienced by the metal.\n\nCommercial and retail products are manufactured to less stringent requirements than those for military and aerospace applications. For example, microprocessors produced by Intel Corporation are manufactured to three grades: commercial, industrial and extended.\n\nBecause some devices generate heat during operation, they may require thermal management to ensure they are within their specified operating temperature range; specifically, that they are operating at or below the maximum operating temperature of the device. Cooling a microprocessor mounted in a typical commercial or retail configuration requires \"a heatsink properly mounted to the processor, and effective airflow through the system chassis\". Systems are designed to protect the processor from unusual operating conditions, such as \"higher than normal ambient air temperatures or failure of a system thermal management component (such as a system fan)\", though in \"a properly designed system, this feature should never become active\". Cooling and other thermal management techniques may affect performance and noise level. Noise mitigation strategies may be required in residential applications to ensure that the noise level does not become uncomfortable.\n\nBattery service life and efficacy is affected by operating temperature. Efficacy is determined by comparing the service life achieved by the battery as a percentage of its service life achieved at 20 °C versus temperature. Ohmic load and operating temperature often jointly determine a battery's discharge rate. Moreover, if the expected operating temperature for a primary battery deviates from the typical 10 °C to 25 °C range, then operating temperature \"will often have an influence on the type of battery selected for the application\". Energy reclamation from partially depleted lithium sulfur dioxide battery has been shown to improve when \"appropriately increasing the battery operating temperature\".\n\nMammals attempt to maintain a comfortable body temperature under various conditions by thermoregulation, part of mammalian homeostasis. The lowest normal temperature of a mammal, the basal body temperature, is achieved during sleep. In women, it is affected by ovulation, causing a biphasic pattern which may be used as a component of fertility awareness.\n\nIn humans, the hypothalamus regulates metabolism, and hence the basal metabolic rate. Amongst its functions is the regulation of body temperature. The core body temperature is also one of the classic phase markers for measuring the timing of an individual's Circadian rhythm.\n\nChanges to the normal human body temperature may result in discomfort. The most common such change is a fever, a temporary elevation of the body's thermoregulatory set-point, typically by about 1–2 °C (1.8–3.6 °F). Hyperthermia is an acute condition caused by the body absorbing more heat than it can dissipate, whereas hypothermia is a condition in which the body's core temperature drops below that required for normal metabolism, and which is caused by the body's inability to replenish the heat that is being lost to the environment.\n\n"}
{"id": "8603211", "url": "https://en.wikipedia.org/wiki?curid=8603211", "title": "Plasma-enhanced chemical vapor deposition", "text": "Plasma-enhanced chemical vapor deposition\n\nPlasma-enhanced chemical vapor deposition (PECVD) is a chemical vapor deposition process used to deposit thin films from a gas state (vapor) to a solid state on a substrate. Chemical reactions are involved in the process, which occur after creation of a plasma of the reacting gases. The plasma is generally created by radio frequency (RF) (alternating current (AC)) frequency or direct current (DC) discharge between two electrodes, the space between which is filled with the reacting gases.\n\nA plasma is any gas in which a significant percentage of the atoms or molecules are ionized. Fractional ionization in plasmas used for deposition and related materials processing varies from about 10 in typical capacitive discharges to as high as 5–10% in high density inductive plasmas. Processing plasmas are typically operated at pressures of a few millitorr to a few torr, although arc discharges and inductive plasmas can be ignited at atmospheric pressure. Plasmas with low fractional ionization are of great interest for materials processing because electrons are so light, compared to atoms and molecules, that energy exchange between the electrons and neutral gas is very inefficient. Therefore, the electrons can be maintained at very high equivalent temperatures – tens of thousands of kelvins, equivalent to several electronvolts average energy—while the neutral atoms remain at the ambient temperature. These energetic electrons can induce many processes that would otherwise be very improbable at low temperatures, such as dissociation of precursor molecules and the creation of large quantities of free radicals.\n\nA second benefit of deposition within a discharge arises from the fact that electrons are more mobile than ions. As a consequence, the plasma is normally more positive than any object it is in contact with, as otherwise a large flux of electrons would flow from the plasma to the object. The difference in voltage between the plasma and the objects in its contacts normally occurs across a thin sheath region. Ionized atoms or molecules that diffuse to the edge of the sheath region feel an electrostatic force and are accelerated towards the neighboring surface. Thus, all surfaces exposed to the plasma receive energetic ion bombardment. The potential across the sheath surrounding an electrically-isolated object (the floating potential) is typically only 10–20 V, but much higher sheath potentials are achievable by adjustments in reactor geometry and configuration. Thus, films can be exposed to energetic ion bombardment during deposition. This bombardment can lead to increases in density of the film, and help remove contaminants, improving the film's electrical and mechanical properties. When a high-density plasma is used, the ion density can be high enough that significant sputtering of the deposited film occurs; this sputtering can be employed to help planarize the film and fill trenches or holes.\n\nA simple DC discharge can be readily created at a few torr between two conductive electrodes, and may be suitable for deposition of conductive materials. However, insulating films will quickly extinguish this discharge as they are deposited. It is more common to excite a capacitive discharge by applying an AC or RF signal between an electrode and the conductive walls of a reactor chamber, or between two cylindrical conductive electrodes facing one another. The latter configuration is known as a parallel plate reactor. Frequencies of a few tens of Hz to a few thousand Hz will produce time-varying plasmas that are repeatedly initiated and extinguished; frequencies of tens of kilohertz to tens of megahertz result in reasonably time-independent discharges.\n\nExcitation frequencies in the low-frequency (LF) range, usually around 100 kHz, require several hundred volts to sustain the discharge. These large voltages lead to high-energy ion bombardment of surfaces. High-frequency plasmas are often excited at the standard 13.56 MHz frequency widely available for industrial use; at high frequencies, the displacement current from sheath movement and scattering from the sheath assist in ionization, and thus lower voltages are sufficient to achieve higher plasma densities. Thus one can adjust the chemistry and ion bombardment in the deposition by changing the frequency of excitation, or by using a mixture of low- and high-frequency signals in a dual-frequency reactor. Excitation power of tens to hundreds of watts is typical for an electrode with a diameter of 200 to 300 mm.\n\nCapacitive plasmas are usually very lightly ionized, resulting in limited dissociation of precursors and low deposition rates. Much denser plasmas can be created using inductive discharges, in which an inductive coil excited with a high-frequency signal induces an electric field within the discharge, accelerating electrons in the plasma itself rather than just at the sheath edge. Electron cyclotron resonance reactors and helicon wave antennas have also been used to create high-density discharges. Excitation powers of 10 kW or more are often used in modern reactors.\n\n<br>\nWorking at Standard Telecommunication Laboratories (STL), Harlow, Essex, Swann discovered that RF discharge promoted the deposition of silicon compounds onto the quartz glass vessel wall. Several internal STL publications were followed in 1964 by French, British and US patent applications. An article was published in the August 1965 volume of Solid State Electronics.\n\nPlasma deposition is often used in semiconductor manufacturing to deposit films conformally (covering sidewalls) and onto wafers containing metal layers or other temperature-sensitive structures. PECVD also yields some of the fastest deposition rates while maintaining film quality (such as roughness, defects/voids), as compared with sputter deposition and thermal/electron-beam evaporation, often at the expense of uniformity.\n\nSilicon dioxide can be deposited using a combination of silicon precursor gasses like dichlorosilane or silane and oxygen precursors, such as oxygen and nitrous oxide, typically at pressures from a few millitorr to a few torr. Plasma-deposited silicon nitride, formed from silane and ammonia or nitrogen, is also widely used, although it is important to note that it is not possible to deposit a pure nitride in this fashion. Plasma nitrides always contain a large amount of hydrogen, which can be bonded to silicon (Si-H) or nitrogen (Si-NH); this hydrogen has an important influence on IR and UV absorption, stability, mechanical stress, and electrical conductivity. This is often used as a surface and bulk passivating layer for commercial multicrystalline silicon photovoltaic cells.\n\nSilicon Dioxide can also be deposited from a tetraethoxysilane (TEOS) silicon precursor in an oxygen or oxygen-argon plasma. These films can be contaminated with significant carbon and hydrogen as silanol, and can be unstable in air. Pressures of a few torr and small electrode spacings, and/or dual frequency deposition, are helpful to achieve high deposition rates with good film stability.\n\nHigh-density plasma deposition of silicon dioxide from silane and oxygen/argon has been widely used to create a nearly hydrogen-free film with good conformality over complex surfaces, the latter resulting from intense ion bombardment and consequent sputtering of the deposited molecules from vertical onto horizontal surfaces.\n\n"}
{"id": "11515823", "url": "https://en.wikipedia.org/wiki?curid=11515823", "title": "Polariton superfluid", "text": "Polariton superfluid\n\nPolariton superfluid is predicted to be a state of the exciton-polaritons system that combines the characteristics of lasers with those of excellent electrical conductors.\nResearchers look for this state in a solid state optical microcavity coupled with quantum well excitons. The idea is to create an ensemble of particles known as exciton-polaritons and trap them.\nWave behavior in this state results in a light beam similar to that from a laser but possibly more energy efficient.\n\nUnlike traditional superfluids that need temperatures of approximately ~4 K, the polariton superfluid could in principle be stable at much higher temperatures, and might soon be demonstrable at room temperature. Evidence for polariton superfluidity was reported in by Alberto Amo and coworkers, based on the suppressed scattering of the polaritons during their motion. \n\nAlthough several other researchers are working in the same field, the terminology and conclusions are not completely shared by the different groups. In particular, important properties of superfluids, such as zero viscosity, and of lasers, such as perfect optical coherence, are a matter of debate . Although, there is clear indication of quantized vortices when the pump beam has orbital angular momentum.\nFurthermore, clear evidence has been demonstrated also for superfluid motion of polaritons, in terms of the Landau criterion and the suppression of scattering from defects when the flow velocity is slower than the speed of sound in the fluid.\nThe same phenomena have been demonstrated in an organic exciton polariton fluid, representing the first achievement of room-temperature superfluidity of a hybrid fluid of photons and excitons.\n\n"}
{"id": "51322981", "url": "https://en.wikipedia.org/wiki?curid=51322981", "title": "Polish Society for the Protection of Birds", "text": "Polish Society for the Protection of Birds\n\nPolish Society for the Protection of Birds, OTOP (from Polish name: Ogólnopolskie Towarzystwo Ochrony Ptaków) – Non-governmental organization set up in Poland in 1991. Among OTOP's tasks are preserving wild living birds, their nests, gathering information about them and study their life. Furthermore, OTOP popularize ornithology among people from various social groups and different ages. OTOP is the Polish partner to BirdLife International.\n\nWhen Gerard Sawicki was the president of society, he run the TV program \"Ptakolub(birdlover)\" transmitted on TVP1 at the end of the 20th century. The current president of society is Dariusz Bukaciński.\n\nFor OTOP works Małgorzata Górska, the 2010 Goldman Environmental Prize Winner. In April 2008 a website OTOPJunior was created in order to encourage the youngest to get interested in birds.\n\n"}
{"id": "57001960", "url": "https://en.wikipedia.org/wiki?curid=57001960", "title": "Potassium amyl xanthate", "text": "Potassium amyl xanthate\n\nPotassium amyl xanthate is an organosulfur compound with the chemical formula CH(CH)OCSK. It is a pale yellow powder or pellet with a pungent odor, soluble in water. It is widely used in the mining industry for the separation of ores using the flotation process. \nPotassium amyl xanthate is prepared by reacting amyl alcohol with carbon disulfide and potassium hydroxide.\n\nPotassium amyl xanthate is a pale yellow powder that is relatively stable between pH 8 and 13 with a maximum of stability at pH 10.\n\nThe is 480 mg/kg (oral, rats) for potassium pentylxanthate.\n\nIt is a biodegradable compound. \n"}
{"id": "492220", "url": "https://en.wikipedia.org/wiki?curid=492220", "title": "Power Rangers Lost Galaxy", "text": "Power Rangers Lost Galaxy\n\nPower Rangers Lost Galaxy is an American television series and the seventh season of the \"Power Rangers\" franchise, based on the Super Sentai series \"Seijuu Sentai Gingaman\". The series was the first to follow the Sentai tradition of a new cast with each new series.\n\nThe character Kendrix (Valerie Vernon) was killed off two-thirds through the season, marking the first time that a Ranger was killed off in the series. Kendrix was written out of the show so actress Vernon could be treated for leukemia. Her treatment was successful and she returned for the season finale. This particular season of \"Power Rangers\" has been noted to be the most expensive production of the series to date.\n\nSet after the events of \"Power Rangers in Space\", four young adults from Angel Grove (Leo and Mike Corbett, Kai Chen, and Kendrix Morgan) are leaving for the space colony Terra Venture seeking a new world like Earth. They later meet a mechanic named Damon Henderson, and on the Moon, a jungle girl named Maya who leads them to five mystical swords known as the Quasar Sabers on her home planet of Mirinoi. After pulling the blades out of a stone, Mike falls into a crevice, but not before passing his sword onto younger brother Leo.\n\nWith the Quasar Sabers, the teens transform into the Galaxy Power Rangers and use them to battle space villains from two different parts of the galaxy. These villains include Scorpius, Trakeena, and Captain Mutiny. Along the way, they discover several Zords known as Galactabeasts and make an ally in the form of the mysterious Magna Defender, a galactic warrior. Though he later dies, he manages to reveal that his host body is Mike Corbett and passes the Magna Defender powers to him.\n\nWhen revives the evil Psycho Rangers, the Rangers of \"Power Rangers in Space\" show up to aid the Galaxy Rangers in destroying them. During this saga, Kendrix Morgan, the Pink Ranger, sacrifices herself to protect the Pink Space Ranger and Terra Venture from Psycho Pink. Karone, sister of the Red Space Ranger Andros and former evil princess Astronema, is given the powers of the Pink Galaxy Ranger from Kendrix (who appears as a spirit), and joins the Rangers in the battle to protect Terra Venture.\n\nAfter this, Deviot returns to Trakeena, but, by this time, she has learned of his treachery and tries to destroy him, chasing him through the ship and into the cocoon that Scorpius made fusing them together and driving Trakeena insane and, using her army armed with bombs, cripple Terra Venture and destroys the Stratoforce and Centaurus Megazords. This forces the colony's people to evacuate to a nearby planet. Trakeena gives chase but the Rangers destroy her ship by self-destructing the Astro Megaship, but the explosion leaves Leo stranded on the moon. Trakeena, who survives the blast also, albeit scarred and injured, finally reaches her breaking point and uses the cocoon to transform into an insectoid creature to enter Terra Venture's wreck and use her power to set it on a crash course to the planet below to destroy it and the people below. Leo enters and soon followed by the other Rangers try to stop her, but are slowly overmatched by Trakeena's new power. She is only defeated after Leo, using his Battlizer, blasts her at point-blank range nearly destroying himself in the process. By this time the colony is nearly crashed but the Galaxy Megazord diverts its course and lands it on a large clearing in an explosion, the colonists, having feared the worse for the Rangers, are relieved and overjoyed when they come out of the explosion on the Galactabeasts. After this, the Zords reveal to the Rangers they had landed on is Mirinoi and return the sabers to the stone altar. This restores its petrified inhabits and, to the joy of the Rangers, restores Kendrix to life as well. After this the colonists settle on Mirinoi as the series ends.\n\nGalaxy Rangers\n\nAllies\nSpace Rangers\nVillains\nPsycho Rangers\n"}
{"id": "14410264", "url": "https://en.wikipedia.org/wiki?curid=14410264", "title": "Proximity communication", "text": "Proximity communication\n\nProximity communication is a Sun microsystems technology of wireless chip-to-chip communications. Partly by Robert Drost and Ivan Sutherland. Research done as part of High Productivity Computing Systems DARPA project.\n\nProximity communication replaces wires by capacitive coupling, promises significant increase in communications speed between chips in an electronic system, among other benefits.\nPartially funded by a $50 million award from the Defense Advanced Research Projects Agency.\n\nComparing traditional area ball bonding, proximity communication has one order smaller scale, so it can be two order densier (in terms of connection number/PIN) than ball bonding. This technique requires very good alignment between chips and very small gaps between transmitting (Tx) and receiving (Rx) parts (2-3 micrometers), which can be destroyed by thermal expansion, vibration, dust, etc.\n\nChip transmitter consists (according to presentation slide) of big 32x32 array of very small Tx micropads, 4x4 array of bigger Rx micropads (four times bigger than tx micropad), and two linear arrays of 14 X vernier and 14 Y vernier.\n\nProximity communication can be used with 3D packing on chips in Multi-Chip Module, allowing to connect several MCM without sockets and wires. \n\nSpeed was up to 1.35 Gbit/s/channel in tests of 16 channel systems. BER < 10. Static power is 3.6 mW/channel, dynamic power is 3.9 pJ/bit.\n\n"}
{"id": "40247", "url": "https://en.wikipedia.org/wiki?curid=40247", "title": "Pulsed inductive thruster", "text": "Pulsed inductive thruster\n\nPulsed inductive thrusters (or PITs) are a form of ion thruster, used in spacecraft propulsion. It is a plasma propulsion engine using perpendicular electric and magnetic fields to accelerate a propellant with no electrode.\n\nA nozzle releases a puff of gas which spreads across a flat spiraling induction coil of wire about 1 meter across. A bank of capacitors releases a pulse of high voltage electric current of tens of kilovolts lasting 10 microseconds into the coil, generating a radial magnetic field. This induces a circular electrical field in the gas, ionizing it and causing charged particles (free electrons and ions) to revolve in the opposite direction as the original pulse of current. Because the motion of this induced current flow is perpendicular to the magnetic field, the plasma is accelerated out into space by the Lorentz force at a high exhaust velocity (10 to 100 km/s).\n\nUnlike an electrostatic ion thruster which uses an electric field to accelerate only one species (positive ions), a PIT uses the Lorentz body force acting upon all charged particles within a quasi-neutral plasma. Unlike most other ion and plasma thrusters, it also requires no electrodes (which are susceptible to erosion) and its power can be scaled up simply by increasing the number of pulses per second. A 1-megawatt system would pulse 200 times per second.\n\nPulsed inductive thrusters can maintain constant specific impulse and thrust efficiency over a wide range of input power levels by adjusting the pulse rate to maintain a constant discharge energy per pulse. It has demonstrated efficiency greater than 50%.\n\nPulsed inductive thrusters can use a wide range of gases as a propellant, such as water, hydrazine, ammonia, argon, xenon… Due to this ability, it has been suggested to use PITs for Martian missions: an orbiter could refuel by scooping CO from the atmosphere of Mars, compressing the gas and liquefying it into storage tanks for the return journey or another interplanetary mission, whilst orbiting the planet.\n\nEarly development began with fundamental proof-of-concept studies performed in the mid-1960s. NASA conducts experiments on this device since the early 1980s.\n\nNGST (Northrop Grumman Space Technology), as a contractor for NASA, built several experimental PITs.\n\nResearch efforts during the first period (1965-1973) were aimed at understanding the structure of an inductive current sheet and evaluating different concepts for propellant injection and preionization.\n\nIn the second period (1979-1988), the focus shifted more towards developing a true propulsion system and increasing the performance of the base design through incremental design changes, with the build of Mk I and Mk IV prototypes.\n\nThe third period (1991-today) began with the introduction of a new PIT thruster design known as the Mk V. It evolved into the Mk VI, developed to reproduce Mk V single-shot tests, which completely characterize thruster performance. It uses an improved coil of hollow copper tube construction and an improved propellant valve, but is electrically identical to the Mk V, using the same capacitors and switches. The Mk VII (early 2000s) has the same geometry as Mk VI, but is designed for high pulse frequency and long-duration firing with a liquid-cooled coil, longer-life capacitors, and fast, high-power solid-state switches. The goal for Mk VII is to demonstrate up to 50 pulses per second at the rated efficiency and impulse bit at 200 kW of input power in a single thruster. Mk VII design is the base for the most recent NuPIT (Nuclear-electric PIT).\n\nThe PIT has obtained relatively high performance in the laboratory environment, but it still requires additional advancements in switching technology and energy storage before becoming practical for high-power in-space applications, with the need for a nuclear-based onboard power source.\n\nFARAD, which stands for Faraday Accelerator with Radio-frequency Assisted Discharge, is a lower-power alternative to the PIT that has the potential for space operation using current technologies.\n\nIn the PIT, both propellant ionization and acceleration are performed by the HV pulse of current in the induction coil, while FARAD uses a separate inductive RF discharge to preionize the propellant before it is accelerated by the current pulse. This preionization allows FARAD to operate at much\nlower discharge energies than the PIT (100 joules per pulse vs 4 kilojoules per pulse) and allows for a reduction in the thruster's size.\n"}
{"id": "47825960", "url": "https://en.wikipedia.org/wiki?curid=47825960", "title": "RRB Energy", "text": "RRB Energy\n\nRRB Energy Limited is a privately owned company that serves in the realm of wind power generation based out of New Delhi. The production plants of the company are based in Delhi and Tamil Nadu. RRB Energy has nationwide presence in India from Tamil Nadu, Maharashtra, Madhya Pradesh, Karnataka, Gujarat and Rajasthan. The company also has a government approved R&D facility which develops higher MW capacity turbines.\n\nThe company was incorporated in 1987 under the brand name Vestas RRB India Limited and was renamed as RRB Energy Limited in 2008. The company was founded by Rakesh Bakshi who introduced the concept of Wind Energy to the Country. The company’s main objective has been to produce world class Wind Electric Generators (WEGs). RRB Energy was the first company to Introduce pitch regulated WEGs in India and is one of the oldest manufacturers of WEGs.\n\nThe company produces the following win turbines:\n\nThe company also provides:\nRRB Energy provides its products and services under the following models:\n\nIn 2011, a delegation from Uruguay visited RRB's production unit in Poonamallee to explore business association with the company. In 2012, a high level delegation from Chile visited the company's manufacturing units to struck business association deal. In 2015, company had a meeting with China-based companies led by China Photovoltaic Industry Association (CPIA) to explore possibilities for striking a partnership.\n\n\n"}
{"id": "7099209", "url": "https://en.wikipedia.org/wiki?curid=7099209", "title": "Recursive recycling", "text": "Recursive recycling\n\nRecursive recycling is a technique where a function, in order to accomplish a task, calls itself with some part of the task or output from a previous step. In municipal solid waste and waste reclamation processing it is the process of extracting and converting materials from recycled materials derived from the previous step until all subsequent levels of output are extracted or used.\n\nSolid waste or municipal solid waste can be treated, sanitized and separated under steam in a pressure vessel (waste autoclave). Following the processing under steam and removal of toxic materials via condensate filtering, usable recyclables are immediately extracted for reuse (plastics, ferrous metals, aluminum, glass, wood, etc.).\n\nOrganic materials from the original waste stream are converted to a fiber using steam at 60 psi and 160 °C. The converted organics (sanitary fiber) is size reduced by 85% and can be used to produce bio-fuels using acidic-hydrolysis or enzymatic-hydrolysis as ethanol or may be used as refuse derived fuel.\n\nAfter the monosacrides are extracted for distillation, the remaining residue (used fiber) can be used as a feed stock for electricity production.\n\nFinally, the non-toxic ash from the combusted fiber can be collected and used as a filler for preparation in super concrete and then\nreused in combination with similar materials (gravel, stones, pottery, glass) to form aggregate for construction materials.\n\nIn true recursive recycling and conservation processing the ability to divert all materials in the waste stream from landfill at greater than 99 percent is a concept based on outputs used to provide the next level of processing, reuse, conservation and market delivery of the derivatives.\n\nThe concept of recursive recycling has been proved up in small scale facilities (thermal hydrolysis, plasma, etc.) but has not been widely accepted because of the financial impact it may have on existing protocols in waste management. One pilot facility operated commercially in Wales for approximately six years. However, the core equipment was moved to another location while the original facility was scheduled for retrofit. No further information about the facility's capacity or the equipment movement has been made available via open source release.\n\nSince that pilot commercial facility stopped operating, the concept of recursive recycling has not met with as much success as originally anticipated by environmentalists and conservationists. There are a number of companies operating autoclaves with limited success across the globe (the autoclave operating in Anaheim California was de-commissioned in c. late 2007-early 2008) but the full concept of waste treatment using thermal hydrolysis technology has not been fully realized because of several misconceptions in the autoclave and related steam treatment technologies. There is available engineering background to demonstrate successful testing that can be validated in physical production facilities but because of a lack of participation and general knowledge is a closely held secret, the application of technologies to achieve full recursive levels has not been accepted.\n\nA number of companies are working on technology to support this concept. That may bring about change to conservation and recycling when associated advances prove out as successful. However, given the current state related areas of technology the growth to full-scale production appears to be limited because the technology and demonstrating it is available in larger capacities has not been demonstrated commercially.\n"}
{"id": "16811582", "url": "https://en.wikipedia.org/wiki?curid=16811582", "title": "Roll program", "text": "Roll program\n\nA roll program or tilt maneuver is an aerodynamic maneuver that alters the attitude of a vertically launched space launch vehicle. The maneuver is used to place the spacecraft on a proper heading toward its intended orbit.\n\nA roll program is completed shortly after the vehicle clears the tower. In the case of a crewed mission, a crew member (usually the commander) reports the roll to mission control which is then acknowledged by the capsule communicator.\n\nDuring the launch of a space shuttle, the roll program was simultaneously accompanied by a pitch maneuver and yaw maneuver.\n\nThe roll program occurred during a shuttle launch for the following reasons:\n\nThe RAGMOP computer program (Northrop) in 1971-1972 discovered a ~20% payload increase by rolling upside down. It went from ~40,000 lb to ~48,000 lb to a 150 NM equatorial orbit without violating any constraints (max Q, 3 G limit, etc.). So the incentive to roll was initially for the payload increase by minimizing drag losses and moment balancing losses by keeping the main engine thrust vectors more parallel to the SRBs.\n\nTitan II and Saturn V launches also required roll programs.\n"}
{"id": "8286330", "url": "https://en.wikipedia.org/wiki?curid=8286330", "title": "Sawlog", "text": "Sawlog\n\nThe term sawlog is a log of suitable size for sawing into lumber, processed at a sawmill. This is in contrast to those other parts of the stem that are designated pulpwood. Sawlogs will be greater in diameter, straighter and have a lower knot frequency.\n\nSawlogs most often come from the \"butt end\" of the stem and are the most financially valuable part of the tree.\n\n"}
{"id": "10501886", "url": "https://en.wikipedia.org/wiki?curid=10501886", "title": "Société nationale d'électricité", "text": "Société nationale d'électricité\n\nSociété nationale d'électricité (SNEL) is the national electricity company of the Democratic Republic of the Congo. Its head office building is located in the district of La Gombe in the capital city, Kinshasa. SNEL operates the Inga Dam facility on the Congo River, and also operates thermal power plants.\n\n"}
{"id": "31675463", "url": "https://en.wikipedia.org/wiki?curid=31675463", "title": "Spring system", "text": "Spring system\n\nIn engineering and physics, a spring system or spring network is a model of physics described as a graph with a position at each vertex and a spring of given stiffness and length along each edge. This generalizes Hooke's law to higher dimensions. This simple model can be used to solve the pose of static systems from crystal lattice to springs. A spring system can be thought of as the simplest case of the finite element method for solving problems in statics. Assuming linear springs and small deformation (or restricting to one-dimensional motion) a spring system can be cast as a (possibly overdetermined) system of linear equations or equivalently as an energy minimization problem.\n\nIf the nominal lengths, \"L\", of the springs are known to be 1 and 2 units respectively, then the system can be solved as followed:\nConsider the simple case of three nodes connected by two springs. Then the stretching of the two springs is given as a function of the positions of the nodes by\nLet \"A\" be that \"connectivity matrix\", relating each degree of freedom to the direction each spring pulls on it.\nSo the forces on the springs is\nwhere \"K\" is a diagonal matrix giving the stiffnesses of all of the springs. Then the force on the nodes is given by left multiplying by formula_3, which we set to zero to find equilibrium:\nwhich gives the linear equation:\nNow, formula_6 is singular, because all solutions are equivalent up to rigid-body translation. Let us prescribe a Dirichlet boundary condition, e.g., formula_7. \n\nSuppose \"K\" is the identity and so \nIf we plug in formula_7 we have\nIncorporating the 2 to the left-hand side gives\nand removing rows of the system that we already know, and simplifying, leaves us with\nso we can then solve\nThat is, formula_15, as prescribed, and formula_16, leaving the first spring slack, and formula_17, leaving the second spring slack.\n\n\n"}
{"id": "853610", "url": "https://en.wikipedia.org/wiki?curid=853610", "title": "Squib (explosive)", "text": "Squib (explosive)\n\nA squib is a miniature explosive device used in a wide range of industries, from special effects to military applications. It resembles a tiny stick of dynamite, both in appearance and construction, although with considerably less explosive power. Squibs consist of two electrical leads which are separated by a plug of insulating material, a small bridge wire or electrical resistance heater, and a bead of heat-sensitive chemical composition in which the bridge wire is embedded. Squibs can be used for generating mechanical force, or to provide pyrotechnic effects for both film and live theatrics. Squibs can be used for shattering or propelling a variety of materials.\n\nA squib generally consists of a small tube filled with an explosive substance, with a detonator running through the length of its core, similar to a stick of dynamite. Also similar to dynamite, the detonator can be a slow-burning fuse, or as is more common today, a wire connected to a remote electronic trigger. Squibs range in size, anywhere from .08\" up to 6/10\" (~2 to 15 millimeters) in diameter.\n\nIn the North American film industry, the term squib is often used to refer variously to: electric matches and detonators (used as initiators to trigger larger pyrotechnics). Squibs are generally (but not always) the main explosive element in an effect, and as such are regularly used as “bullet hits”. Conventional squibs fire once, with the exception of eSquibs, which fire 200 or more times before depletion. \n\nSquibs were once used in coal mining to break coal away from rock. In the 1870s, some versions of the device were patented and mass-produced as \"Miners' Safety Squibs\".\n\nToday, squibs are widely used in the motion picture special effects industry to simulate bullet impacts on inanimate objects. Items such as sand, soil or wood splinters may be attached to the squib to simulate the \"splash\" that occurs when bullets pierce different materials.\n\nAlthough squibs were once used even for the simulation of bullet hits on live actors, such use has been largely phased out in favor of more advanced devices that are safer for the actor, such as miniature compressed gas packs. These alternate devices are often still referred to as \"squibs\" even though they do not use explosive substances. The devices (whether explosive or not) are coupled with small balloons filled with fake blood (blood squibs) and often other materials to simulate shattered bone and tissue.\n\nSquibs are used in emergency mechanisms where gas pressure needs to be generated quickly in confined spaces, while not harming any surrounding persons or mechanical parts. In this form, squibs may be called gas generators. One such mechanism is the inflation of automobile air bags. In military aircraft, squibs are used to deploy countermeasures, and are also implemented during ejection to propel the canopy and ejection seat away from a crippled aircraft. They are also used to deploy parachutes. Squibs are also used in automatic fire extinguishers, to pierce seals that retain liquids such as halon, fluorocarbon, or liquid nitrogen.\n\nSquibs were originally made from parchment tubes, or the shaft of a feather, and filled with fine black powder. They were then sealed at the ends with wax. They were sometimes used to ignite the main propellant charge in cannon.\n\nSquibs are mentioned in the prominent tort case from eighteenth-century England, \"Scott v. Shepherd\", 96 Eng. Rep. 525 (K.B. 1773). A lit squib was thrown into a crowded market by Shepherd, and landed on the table of a gingerbread merchant. A bystander, to protect himself and the gingerbread, threw the squib across the market, where it landed in the goods of another merchant. The merchant grabbed the squib and tossed it away, accidentally hitting Scott in the face, putting out one of his eyes.\n\nThe first documented use of squibs to simulate bullet impacts in movies was in the 1955 Polish film \"Pokolenie\" by Andrzej Wajda, where for the first time audiences were presented with a realistic representation of a bullet impacting on an on-camera human being, complete with blood spatter. The creator of the effect, Kazimierz Kutz, used a condom with fake blood and dynamite. \n\nWhile most modern squibs used by professionals are insulated from moisture, older uninsulated squibs needed to be kept dry in order to ignite, thus a \"damp squib\" was literally one that failed to perform because it got wet. Often misheard as \"damp squid\", the phrase \"damp squib\" has since come into general use to mean anything that fails to meet expectations. The word \"squib\" has come to take on a similar meaning even when used alone, as a diminutive comparison to a full explosive.\n\n"}
{"id": "12724814", "url": "https://en.wikipedia.org/wiki?curid=12724814", "title": "The Water Engine", "text": "The Water Engine\n\nThe Water Engine is a play by David Mamet that centers on the violent suppression of a disruptive alternative energy technology.\n\nCharles Lang works at a menial job at a factory and lives with his blind sister Rita in an apartment in Chicago during the 1934 World’s Fair. But he is also an amateur inventor, and the play centers around a machine he designs that can create electricity from distilled water. Seeking to patent his idea, he finds a lawyer, Morton Gross, in the phone book and shows him the machine, but Gross’s motivations seem to differ from Lang’s. Gross recruits another lawyer, Lawrence Oberman, and together they menace Lang and eventually his sister. It is heavily implied that the two of them serve the corporate establishment whose profits Lang’s engine threatens.\n\nBy the time Lang realizes he is being taken advantage of, the lawyers have him trapped. He attempts to contact a newspaper reporter, but Gross and Oberman hold his sister hostage to prevent him from telling his story. He then meets a barker at the World’s Fair right before it closes for the night who tells him of a chain letter he has just received, which gives him an idea.\n\nThe lawyers try to force Lang into giving them his plans, but he says he no longer has them; the audience finds out from a scene in the newspaper reporter’s office that he and Rita have been killed. The play ends with Bernie, a young friend of the family who has previously shown mechanical aptitude, receiving the plans for the Water Engine in the mail.\n\nThe Century of Progress theme of the 1934 Chicago World's Fair informs that of the play. Technology is interspersed throughout the dialogue as the voices of various announcing figures, over radios, on physical soapboxes, and, in the case of the Chain Letter, of indeterminate origin, reinforce the notion of a rising tide of change as they herald the advent of a new technological era. The superstition represented by the Chain Letter contrasts with its eventual saving of Lang's invention and yet also coincides with it, as both the inventor and the letter seek explanations and justice in a world that often—particularly in the cases of both the lawyers, the knowingly bombastic newspaper reporter Dave Murray, and the Fair itself—seems more intent on flowery rhetoric than on the pursuit of truth or the greatest good of society.\n\nThe play plays with the form of daytime radio serials, as its plot and structure, with clearly defined heroes and antagonists, riffs off the suspense thrillers that were popular around the time the play is set. That it was originally written as a radio play positions it as an homage to the genre.\n\nOriginally written as a radio play for the NPR drama showcase \"Earplay\", \"The Water Engine\" was first staged at The St. Nicholas Theater in Chicago and later at The Public Theater in New York by Steven Schachter. It opened on December 20, 1977 and ran for 63 performances. The cast included Dwight Schultz as Charles Lang, David Sabin as Morton Gross, and Bill Moor as Lawrence Oberman. On February 28, 1978, it transferred to the Plymouth Theatre on Broadway as a double-bill with a short Mamet play entitled \"Mr. Happiness\", and ran for 24 performances. In this production Patti LuPone was featured as Rita. The play was nominated for the Drama Desk Award for Outstanding New Play.\n\nThe play was adapted by Mamet, Steven Bognar, Julia Reichert, and Martin Goldstein for a 1992 made-for-cable television movie produced by Donald P. Borchers, directed by Steven Schachter and starring William H. Macy as Charles Lang, John Mahoney as Mason (instead of Morton) Gross, Joe Mantegna as Lawrence Oberman, and Patti LuPone as Rita. Charles Durning, Treat Williams, Andrea Marcovicci, Peter Michael Goetz, Rebecca Pidgeon, Felicity Huffman, Ricky Jay, and Joanna Miles also were in the cast. It was produced by Amblin Television and broadcast by TNT.\n\n\n"}
{"id": "29529485", "url": "https://en.wikipedia.org/wiki?curid=29529485", "title": "The Windward Road", "text": "The Windward Road\n\nThe Windward Road: Adventures of a Naturalist on Remote Caribbean Shores, was written by Archie Carr and originally published in 1956. It is an account of Dr. Carr's travels around the Caribbean to study sea turtles and their migratory and behavior patterns, especially Kemp's ridley, a species about which little was known at the time. This book led to the formation of The Brotherhood of the Green Turtle, which later became the Caribbean Conservation Corporation, and is now known as the Sea Turtle Conservancy. It was awarded the 1957 John Burroughs Medal for nature writing, which is awarded annually by the American Museum of Natural History. The chapter entitled \"The Black Beach\", originally published in Mademoiselle, won a 1956 O. Henry Award.\n"}
{"id": "9564542", "url": "https://en.wikipedia.org/wiki?curid=9564542", "title": "Toroflux", "text": "Toroflux\n\nThe Toroflux or Torofluxus is a toy that was invented in mid-1990s by Jochen Valett. It is a ribbon of steel which is woven into a torus spring. It is often sold pre-attached to a larger plastic ring. The spring flows downwards along the plastic ring, creating a shimmering effect like a silver bubble. The spring tension in the steel tightens the inner spiral core, causing it to cling to the plastic ring — just like a silver bubble.\n\nIn 2010, California-based company \"Flowtoys\" purchased the rights to the Toroflux and began manufacturing it as a consumer item.\n\n"}
{"id": "32115739", "url": "https://en.wikipedia.org/wiki?curid=32115739", "title": "Wind hybrid power systems", "text": "Wind hybrid power systems\n\nWind hybrid power systems combines wind turbines with other storage and/or generation sources. One of the key issues with wind energy is its intermittent nature. This has led to numerous methods of storing energy.\nA wind-hydro system generates electric energy combining wind turbines and pumped storage. The combination has been the subject of long-term discussion, and an experimental plant, which also tested wind turbines, was implemented by Nova Scotia Power at its Wreck Cove hydro electric power site in the late 1970s, but was decommissioned within ten years. Since, no other system has been implemented at a single location as of late 2010.\n\nWind-hydro stations dedicate all, or a significant portion, of their wind power resources to pumping water into pumped storage reservoirs. These reservoirs are an implementation of grid energy storage.\n\nWind and its generation potential is inherently variable. However, when this energy source is used to pump water into reservoirs at an elevation (the principle behind pumped storage), the potential energy of the water is relatively stable and can be used to generate electrical power by releasing it into a hydropower plant when needed. The combination has been described as particularly suited to islands that are not connected to larger grids.\n\nDuring the 1980s, an installation was proposed in the Netherlands. The IJsselmeer would be used as the reservoir, with wind turbines located on its dike. Feasibility studies have been conducted for installations on the island of Ramea (Newfoundland and Labrador) and on the Lower Brule Indian Reservation (South Dakota).\n\nAn installation at Ikaria Island, Greece, had entered the construction phase as of 2010.\n\nThe island of El Hierro is where the first world's first wind-hydro power station is expected to be complete. Current TV called this \"a blueprint for a sustainable future on planet Earth\". It was designed to cover between 80-100% of the island's power and was set to be operational in 2012. However, these expectations were not realized in practice, probably due to inadequate reservoir volume and persistent problems with grid stability.\n\n100% renewable energy systems require an over-capacity of wind or solar power.\n\nOne method of storing wind energy is the production of hydrogen through the electrolysis of water. This hydrogen is subsequently used to generate electricity during periods when demand can not be matched by wind alone. The energy in the stored hydrogen can be converted into electrical power through fuel cell technology or a combustion engine linked to an electrical generator.\n\nSuccessfully storing hydrogen has many issues which need to be overcome, such as embrittlement of the materials used in the power system.\n\nThis technology is being developed in many countries and has even seen a recent IPO of an Australian firm called Wind Hydrogen that looks to commercialise this technology in both Australia and the UK. Essentially Wind Hydrogen offers a source of domestic and vehicular energy for rural communities where current energy transmission costs are prohibitive. Test sites include:\n\nA wind-diesel hybrid power system combines diesel generators and wind turbines, usually alongside ancillary equipment such as energy storage, power converters, and various control components, to generate electricity. They are designed to increase capacity and reduce the cost and environmental impact of electrical generation in remote communities and facilities that are not linked to a power grid. Wind-diesel hybrid systems reduce reliance on diesel fuel, which creates pollution and is costly to transport.\n\nWind-diesel generating systems have been under development and trialled in a number of locations during the latter part of the 20th century. A growing number of viable sites have been developed with increased reliability and minimized technical support costs in remote communities.\n\nThe successful integration of wind energy with diesel generating sets relies on complex controls to ensure correct sharing of intermittent wind energy and controllable diesel generation to meet the demand of the usually variable load. The common measure of performance for wind diesel systems is Wind Penetration which is the ratio between Wind Power and Total Power delivered, e.g. 60% wind penetration implies that 60% of the system power comes from the wind. Wind Penetration figures can be either peak or long term. Sites such as Mawson Station, Antarctica, as well as Coral Bay and Bremer Bay in Australia have peak wind penetrations of around 90%. Technical solutions to the varying wind output include controlling wind output using variable speed wind turbines (e.g. Enercon, Denham, Western Australia), controlling demand such as the heating load (e.g. Mawson), storing energy in a flywheel (e.g. Powercorp, Coral Bay). Some installations are now being converted to wind hydrogen systems such as on Ramea in Canada which is due for completion in 2010.\n\nThe following is a, probably incomplete, list of isolated communities utilizing commercial Wind-Diesel hybrid systems with a significant proportion of the energy being derived from wind.\n\nRecently, in Northern Canada wind-diesel hybrid power systems were built by the mining industry. In remote locations at Lac de Gras, in Canada's Northwest Territories, and Katinniq, Ungava Peninsula, Nunavik, two systems are used to save fuel at mines. There is another system in Argentina.\n\nAt power stations that use compressed air energy storage (CAES), electrical energy is used to compress air and store it in underground facilities such as caverns or abandoned mines. During later periods of high electrical demand, the air is released to power turbines, generally using supplemental natural gas. Power stations that make significant use of CAES are operational in McIntosh, Alabama, Germany, and Japan. System disadvantages include some energy losses in the CAES process; also, the need for supplemental use of fossil fuels such as natural gas means that these systems do not completely make use of renewable energy.\n\nThe Iowa Stored Energy Park, projected to begin commercial operation in 2015, will use wind farms in Iowa as an energy source in conjunction with CAES.\n\nThe Pearl River Tower in Guangzhou, China, will mix solar panel on its windows and several wind turbines at different stories of its structure, allowing this tower to be energy positive.\n\nIn several parts of China & India, there are lighting pylons with combinations of solar panels and wind-turbines at their top. This allows space already used for lighting to be used more efficiently with two complementary energy productions units. Most common models use horizontal axis wind-turbines, but now models are appearing with vertical axis wind-turbines, using a helicoidal shaped, twisted-Savonius system.\n\n\n"}
{"id": "38161108", "url": "https://en.wikipedia.org/wiki?curid=38161108", "title": "Wind power in Georgia", "text": "Wind power in Georgia\n\nWind power in Georgia consists of one wind farm, completed in 2013 with 20 MW of capacity. \n\n"}
