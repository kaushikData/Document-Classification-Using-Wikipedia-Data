{"id": "57218367", "url": "https://en.wikipedia.org/wiki?curid=57218367", "title": "ADD Grup", "text": "ADD Grup\n\nADD Grup, is a developer and manufacturer of smart metering solutions. The company is based in Chișinău, Moldova.\n\nThe company’s primary product is the ADDAX IMS system for electricity metering, multi-utility metering and streetlight management. The system offers a suite of interconnected hardware and software tools, including electricity meters, communication endpoints, network infrastructure and data management applications. Supported functionalities include ToU metering, load profile, load control, demand response, fraud detection, imbalance control and prepayment.\n\nADD Grup was originally known as ADD, established in 1992.\n\nIn 2014, ADD Grup launched the Universal Platform that \"allows to change the technology on basis of which smart meters and routers operate\" and this was exhibited through a project in Latvia.\n\nAs of 2016, the company has supplied over 5 million smart meters worldwide.\n\nIn 2017, ADD Grup was chosen by the representatives of the European Council as a successful hi-tech company for a video campaign to promote success stories of companies exporting from Moldova to the European Union.\n"}
{"id": "2214854", "url": "https://en.wikipedia.org/wiki?curid=2214854", "title": "Advanced Tactical Laser", "text": "Advanced Tactical Laser\n\nThe Advanced Tactical Laser (ATL) program was a US military program to mount a high energy laser weapon on an aircraft, initially the AC-130 gunship, for use against ground targets in urban or other areas where minimizing collateral damage is important. The laser was a 100 kilowatt-class chemical oxygen iodine laser (COIL). It was expected to have a tactical range of approximately twenty kilometers and weigh about 5,000–7,000 kg. This program is distinct from the Airborne Laser, which was a much larger system designed to destroy enemy missiles in the boost phase.\n\nIn 1996, the blue-beam air-to-ground tactical laser was test-fired from an AC-130 (AC-X Son of Spectre) aircraft at the northern annex of the White Sands Proving Grounds near Fort Wingate, New Mexico. Raytheon was later awarded the contract to add the High Energy Microwave Weapon to the same platform where both systems would be operationally available for combat use at the same time. \n\nIn 2002, the Special Operations Command entered into a contract with the Boeing Company, specifically the Lasers and Electro-Optics Systems division in West Hills, CA, to produce a prototype laser system on a test aircraft. This effort was heavily supported by Boeing-SVS Inc. in Albuquerque, NM.\n\nOn January 18, 2006, the U.S. Air Force's 46th Test Wing handed over to Boeing a C-130H Hercules transport aircraft for use in the ATL program. Both the laser and the aircraft have undergone testing in the summer of 2006 culminating in the systems joint combined tests in 2007 with full-scale development afterward.\n\nBoeing announced that on December 4, 2007 the installation of the laser on the C-130H Hercules was completed in preparation for further testing and a demonstration in 2008.\n\nThere has been some fear that an airborne laser system could be used to attack targets which would usually be considered 'off-limits', due to the weapon's 'plausible deniability'. Since no such weapon has ever been observed before, its effects would be hard to identify, meaning that there would rarely be conclusive proof of a laser strike. On August 13, 2008 Boeing announced the first test firing of the \"high-energy chemical laser\" mounted in a Hercules transport plane. The test firing was controlled via the ATL beam control system, which acquired a ground target and fired as \"directed by ATL's battle management system.\" The ATL weighs 12,000 pounds. Boeing said that the laser hit a 3 by 3 foot target at Kirtland Air Force Base, New Mexico.\n\nAccording to a November 5, 2008 article there was a recent Air Force Scientific Advisory Board report stating \"the Advanced Tactical Laser testbed has no operational utility.\" That does not mean it's not necessarily a good idea, but that it may need further development. The Air Force Research Laboratory continues to run tests and develop the platform. There is some discussion of converting to solid state lasers from the existing chemical lasers. Being much smaller and lighter, solid state lasers might be deployable on smaller platforms. The existing chemical laser platform is being used to develop more advanced control software and hardware and to reduce problems such as \"jitter\".\n\nOn June 18, 2009 it was announced that the ATL was successfully fired in flight for the first time. The system was fired from a 46th Test Wing NC-130H aircraft while flying over White Sands Missile Range, successfully hitting a target board on the ground.\n\nAug. 30, 2009 Boeing and the U.S. Air Force \"defeated\" a ground target from the air with the Advanced Tactical Laser (ATL) aircraft.\n\nThe advanced tactical laser was discontinued after successful testing.\n\n\n"}
{"id": "4593346", "url": "https://en.wikipedia.org/wiki?curid=4593346", "title": "Aerodynamic center", "text": "Aerodynamic center\n\nThe torques or moments acting on an airfoil moving through a fluid can be accounted for by the net lift and net drag applied at some point on the airfoil, and a separate net pitching moment about that point whose magnitude varies with the choice of where the lift is chosen to be applied. The aerodynamic center is the point at which the pitching moment coefficient for the airfoil does not vary with lift coefficient (i.e. angle of attack), making analysis simpler. \n\nThe lift and drag forces can be applied at a single point, the center of pressure, about which they exert zero torque. However, the location of the center of pressure moves significantly with a change in angle of attack and is thus impractical for analysis. Instead, the 25% chord position, or aerodynamic center, is used, about which the forces and moment are generated. It can be shown that at the 25% chord position the moment generated varies little as angle of attack changes. The concept of the aerodynamic center (AC) is important in aerodynamics. It is fundamental in the science of stability of aircraft in flight.\n\nIn precision experimentation with real airfoils and advanced analysis the aerodynamic center is observed to change location slightly as angle of attack varies. In most literature however the aerodynamic center is assumed to be fixed at the 25% chord position. Conversely, this means that if the aerodynamic center is assumed to be fixed at 25% chord, the moment about the aerodynamic center can be 'not constant'. A large portion of cambered airfoils have non-constant moments about the 25% chord position because the location of the aerodynamic center does move slightly. For most analysis the non-constant moment about the 25% chord position is not significant enough to warrant consideration, but is important to keep in mind. \n\nFor symmetric airfoils in subsonic flight the aerodynamic center is located approximately 25% of the chord from the leading edge of the airfoil. This point is described as the quarter-chord point. This result also holds true for 'thin-airfoils'. For non-symmetric (cambered) airfoils the quarter-chord is only an approximation for the aerodynamic center.\n\nA similar concept is that of center of pressure. The location of the center of pressure varies with changes of lift coefficient and angle of attack. This makes the center of pressure unsuitable for use in analysis of longitudinal static stability. For more explanation on this topic, see movement of center of pressure.\n\nFor longitudinal static stability: formula_3     and    formula_4\n\nFor directional static stability:   formula_5     and    formula_6\n\nWhere:\n\nFor a force acting away from the aerodynamic center, which is away from the reference point:\n\nWhich for small angles formula_10 and formula_11, formula_12, formula_13, formula_14 simplifies to:\n\nGeneral Case: From the definition of the AC it follows that\n\nThe Static Margin can then be used to quantify the AC:\n\nwhere:\n\nSM = Static Margin\n\n"}
{"id": "290053", "url": "https://en.wikipedia.org/wiki?curid=290053", "title": "Airfoil", "text": "Airfoil\n\nAn airfoil (American English) or aerofoil (British English) is the cross-sectional shape of a wing, blade (of a propeller, rotor, or turbine), or sail (as seen in cross-section).\n\nAn airfoil-shaped body moved through a fluid produces an aerodynamic force. The component of this force perpendicular to the direction of motion is called lift. The component parallel to the direction of motion is called drag. Subsonic flight airfoils have a characteristic shape with a rounded leading edge, followed by a sharp trailing edge, often with a symmetric curvature of upper and lower surfaces. Foils of similar function designed with water as the working fluid are called hydrofoils.\n\nThe lift on an airfoil is primarily the result of its angle of attack and shape. When oriented at a suitable angle, the airfoil deflects the oncoming air (for fixed-wing aircraft, a downward force), resulting in a force on the airfoil in the direction opposite to the deflection. This force is known as aerodynamic force and can be resolved into two components: lift and drag. Most foil shapes require a positive angle of attack to generate lift, but cambered airfoils can generate lift at zero angle of attack. This \"turning\" of the air in the vicinity of the airfoil creates curved streamlines, resulting in lower pressure on one side and higher pressure on the other. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flowfield about the airfoil has a higher average velocity on the upper surface than on the lower surface. The lift force can be related directly to the average top/bottom velocity difference without computing the pressure by using the concept of circulation and the Kutta-Joukowski theorem.\n\nA fixed-wing aircraft's wings, horizontal, and vertical stabilizers are built with airfoil-shaped cross sections, as are helicopter rotor blades. Airfoils are also found in propellers, fans, compressors and turbines. Sails are also airfoils, and the underwater surfaces of sailboats, such as the centerboard and keel, are similar in cross-section and operate on the same principles as airfoils. Swimming and flying creatures and even many plants and sessile organisms employ airfoils/hydrofoils: common examples being bird wings, the bodies of fish, and the shape of sand dollars. An airfoil-shaped wing can create downforce on an automobile or other motor vehicle, improving traction.\n\nAny object, such as a flat plate, a building, or the deck of a bridge, with an angle of attack in a moving fluid will generate an aerodynamic force perpendicular to the flow. Airfoils are more efficient lifting shapes, able to generate more lift than similarly sized flat plates, and to generate lift with significantly less drag.\n\nA lift and drag curve obtained in wind tunnel testing is shown on the right. The curve represents an airfoil with a positive camber so some lift is produced at zero angle of attack. With increased angle of attack, lift increases in a roughly linear relation, called the \"slope\" of the lift curve. At about 18 degrees this airfoil stalls, and lift falls off quickly beyond that. The drop in lift can be explained by the action of the upper-surface boundary layer, which separates and greatly thickens over the upper surface at and past the stall angle. The thickened boundary layer's displacement thickness changes the airfoil's effective shape, in particular it reduces its effective camber, which modifies the overall flow field so as to reduce the circulation and the lift. The thicker boundary layer also causes a large increase in pressure drag, so that the overall drag increases sharply near and past the stall point.\n\nAirfoil design is a major facet of aerodynamics. Various airfoils serve different flight regimes. Asymmetric airfoils can generate lift at zero angle of attack, while a symmetric airfoil may better suit frequent inverted flight as in an aerobatic airplane. In the region of the ailerons and near a wingtip a symmetric airfoil can be used to increase the range of angles of attack to avoid spin–stall. Thus a large range of angles can be used without boundary layer separation. Subsonic airfoils have a round leading edge, which is naturally insensitive to the angle of attack. The cross section is not strictly circular, however: the radius of curvature is increased before the wing achieves maximum thickness to minimize the chance of boundary layer separation. This elongates the wing and moves the point of maximum thickness back from the leading edge.\n\nSupersonic airfoils are much more angular in shape and can have a very sharp leading edge, which is very sensitive to angle of attack. A supercritical airfoil has its maximum thickness close to the leading edge to have a lot of length to slowly shock the supersonic flow back to subsonic speeds. Generally such transonic airfoils and also the supersonic airfoils have a low camber to reduce drag divergence. Modern aircraft wings may have different airfoil sections along the wing span, each one optimized for the conditions in each section of the wing.\n\nMovable high-lift devices, flaps and sometimes slats, are fitted to airfoils on almost every aircraft. A trailing edge flap acts similarly to an aileron; however, it, as opposed to an aileron, can be retracted partially into the wing if not used.\n\nA laminar flow wing has a maximum thickness in the middle camber line. Analyzing the Navier–Stokes equations in the linear regime shows that a negative pressure gradient along the flow has the same effect as reducing the speed. So with the maximum camber in the middle, maintaining a laminar flow over a larger percentage of the wing at a higher cruising speed is possible. However, some surface contamination will disrupt the laminar flow, making it turbulent. For example, with rain on the wing, the flow will be turbulent. Under certain conditions, insect debris on the wing will cause the loss of small regions of laminar flow as well. Before NASA's research in the 1970s and 1980s the aircraft design community understood from application attempts in the WW II era that laminar flow wing designs were not practical using common manufacturing tolerances and surface imperfections. That belief changed after new manufacturing methods were developed with composite materials (e.g., graphite fiber) and machined metal methods were introduced. NASA's research in the 1980s revealed the practicality and usefulness of laminar flow wing designs and opened the way for laminar flow applications on modern practical aircraft surfaces, from subsonic general aviation aircraft to transonic large transport aircraft, to supersonic designs.\n\nSchemes have been devised to define airfoils – an example is the NACA system. Various airfoil generation systems are also used. An example of a general purpose airfoil that finds wide application, and predates the NACA system, is the Clark-Y. Today, airfoils can be designed for specific functions using inverse design programs such as PROFOIL, XFOIL and AeroFoil. XFOIL is an online program created by Mark Drela that will design and analyze subsonic isolated airfoils.\n\nThe various terms related to airfoils are defined below:\n\n\nThe geometry of the airfoil is described with a variety of terms :\n\n\nThe shape of the airfoil is defined using the following geometrical parameters:\n\nSome important parameters to describe an airfoil's shape are its \"camber\" and its \"thickness\". For example, an airfoil of the NACA 4-digit series such as the NACA 2415 (to be read as 2 – 4 – 15) describes an airfoil with a camber of 0.02 chord located at 0.40 chord, with 0.15 chord of maximum thickness.\n\nFinally, important concepts used to describe the airfoil's behaviour when moving through a fluid are:\n\nThin airfoil theory is a simple theory of airfoils that relates angle of attack to lift for incompressible, inviscid flows. It was devised by German-American mathematician Max Munk and further refined by British aerodynamicist Hermann Glauert and others in the 1920s. The theory idealizes the flow around an airfoil as two-dimensional flow around a thin airfoil. It can be imagined as addressing an airfoil of zero thickness and infinite wingspan.\n\nThin airfoil theory was particularly notable in its day because it provided a sound theoretical basis for the following important properties of airfoils in two-dimensional flow:\n\nAs a consequence of (3), the section lift coefficient of a symmetric airfoil of infinite wingspan is:\n\nAlso as a consequence of (3), the section lift coefficient of a cambered airfoil of infinite wingspan is:\n\nThin airfoil theory does not account for the stall of the airfoil, which usually occurs at an angle of attack between 10° and 15° for typical airfoils. In the mid-late 2000's, however, a theory predicting the onset of leading-edge stall was proposed by Wallace J. Morris II in his doctoral thesis. Morris's subsequent refinements contain the details on the current state of theoretical knowledge on the leading-edge stall phenomenon. Morris's theory predicts the critical angle of attack for leading-edge stall onset as the condition at which a global separation zone is predicted in the solution for the inner flow. Morris's theory demonstrates that a subsonic flow about a thin airfoil can be described in terms of an outer region, around most of the airfoil chord, and an inner region, around the nose, that asymptotically match each other. As the flow in the outer region is dominated by classical thin airfoil theory, Morris's equations exhibit many components of thin airfoil theory.\n\nThe airfoil is modeled as a thin lifting mean-line (camber line). The mean-line, y(x), is considered to produce a distribution of vorticity formula_9 along the line, s. By the Kutta condition, the vorticity is zero at the trailing edge. Since the airfoil is thin, x (chord position) can be used instead of s, and all angles can be approximated as small.\n\nFrom the Biot–Savart law, this vorticity produces a flow field formula_10 where\n\nformula_12 is the location where induced velocity is produced, formula_13 is the location of the vortex element producing the velocity and formula_1 is the chord length of the airfoil.\n\nSince there is no flow normal to the curved surface of the airfoil, formula_10 balances that from the component of main flow formula_16, which is locally normal to the plate – the main flow is locally inclined to the plate by an angle formula_17. That is:\n\nThis integral equation can by solved for formula_19, after replacing x by\n\nas a Fourier series in formula_21 with a modified lead term formula_22\n\nThat is\n\n(These terms are known as the Glauert integral).\n\nThe coefficients are given by\n\nand\n\nBy the Kutta–Joukowski theorem, the total lift force F is proportional to\n\nand its moment M about the leading edge to\n\nThe calculated Lift coefficient depends only on the first two terms of the Fourier series, as\n\nThe moment M about the leading edge depends only on formula_29 and formula_30, as\n\nThe moment about the 1/4 chord point will thus be,\n\nFrom this it follows that the center of pressure is aft of the 'quarter-chord' point 0.25 c, by\n\nThe aerodynamic center, AC, is at the quarter-chord point. The AC is where the pitching moment M' does not \"vary\" with a change in lift coefficient, i.e.,\n\n\n\n"}
{"id": "20145735", "url": "https://en.wikipedia.org/wiki?curid=20145735", "title": "Cariphalte", "text": "Cariphalte\n\nCariphalte is a brand of hot-pour rubberised bitumen sealant (bitumen technology) manufactured by Shell Bitumen, used for race track and expansion joints.\n\n"}
{"id": "3594804", "url": "https://en.wikipedia.org/wiki?curid=3594804", "title": "Cyclic stress", "text": "Cyclic stress\n\nCyclic stress is the distribution of forces (aka stresses) that change over time in a repetitive fashion. As an example, consider one of the large wheels used to drive an aerial lift such as a ski lift. The wire cable wrapped around the wheel exerts a downward force on the wheel and the drive shaft supporting the wheel. Although the shaft, wheel, and cable move, the force remains nearly vertical relative to the ground. Thus a point on the surface of the drive shaft will undergo tension when it is pointing towards the ground and compression when it is pointing to the sky.\n\nCyclic stress is frequently encountered in rotating machinery where a bending moment is applied to a rotating part. This is called a \"cyclic bending stress\" and the aerial lift above is a good example. However, \"cyclic axial stresses\" and \"cyclic torsional stresses\" also exist. An example of cyclic axial stress would be a bungee cord (see bungee jumping), which must support the mass of people as they jump off structures such as bridges. When a person reaches the end of a cord, the cord deflects elastically and stops the person's descent. This creates a large axial stress in the cord. A fraction of the elastic potential energy stored in the cord is typically transferred back to the person, throwing the person upwards some fraction of the distance he or she fell. The person then falls on the cord again, inducing stress in the cord. This happens multiple times per jump. The same cord is used for several jumps, creating cyclical stresses in the cord that could eventually cause failure if not replaced.\n\nWhen cyclic stresses are applied to a material, even though the stresses do not cause plastic deformation, the material may fail due to fatigue. Fatigue failure is typically modeled by decomposing cyclic stresses into mean and alternating components. Mean stress is the time average of the principal stress. The definition of alternating stress varies between different sources. It is either defined as the difference between the minimum and the maximum stress, or the difference between the mean and maximum stress. Engineers try to design mechanisms whose parts are subjected to a single type (bending, axial, or torsional) of cyclic stress because this more closely matches experiments used to characterize fatigue failure in different materials.\n"}
{"id": "31858593", "url": "https://en.wikipedia.org/wiki?curid=31858593", "title": "DC distribution system (ship propulsion)", "text": "DC distribution system (ship propulsion)\n\nThe DC distribution system has been proposed, as a replacement for the present AC power distribution system for ships with electric propulsion.\n\nThis concept represents a new way of distributing energy for low-voltage installations on ships. It can be used for any electrical ship application up to 20 megawatts and operates at a nominal voltage of 1000V DC. The DC distribution system is simply an extension of the multiple DC links that already exist in all propulsion and thruster drives, which usually account for more than 80 percent of the electrical power consumption on electric propulsion vessels.\n\nIn addition to boosting efficiency by up to 20 percent, other benefits include space and weight savings of up to 30 percent and flexible placement of electrical equipment. This allows for significantly more cargo space and a more functional vessel layout where the electrical system is designed around the vessel functions and not vice versa.\n\nThe efficiency improvement is mainly achieved from the system no longer being locked at a specific frequency (usually 60 Hz on ships), even though a 60 Hz power source can also be connected to the grid. This new freedom of being able to control each power source totally independently opens up numerous ways of optimizing fuel consumption.\nThe reduced weight and footprint of the installed electrical equipment will vary depending on the ship type and application. One comparison using the DC distribution system instead of the traditional AC system for a Platform Supply Vessel (PSV), reduced the weight of the electrical system components from to .\n\nOn land, the solar panels on several buildings in Sweden are connected via DC to smooth production and consumption, bypassing the AC grid and its inverters.\n\nThe biggest potential for fuel savings lies in the ease with which energy storage devices, such as batteries or super capacitors, can be added to the system. Energy storage will help the engines level out load variations from the thrusters and other large loads.\n\nDC distribution system allows for new ways of thinking regarding operational optimization. The system is flexible and can combine different energy sources such as engines, turbines, and fuel cells. This means that there is the potential to implement an energy management system that takes into account varying fuel prices and the availability of different fuels.\n\nBecause the main AC switchboard with its AC circuit breakers and protection relays is omitted from the new design, a new protection philosophy that fulfills class requirements is needed for selectivity and equipment protection. ABB has proposed a solution for protecting the DC distribution system using a combination of fuses and controlled turn-off semiconductor power devices. Because all energy-producing components have controllable switching devices, the fault current can be blocked much faster than is possible with traditional circuit breakers with associated protection relays.\n\n\n"}
{"id": "445899", "url": "https://en.wikipedia.org/wiki?curid=445899", "title": "David Kay", "text": "David Kay\n\nDavid A. Kay (born c. 1940) is a weapons expert, political commentator, and senior fellow at the Potomac Institute for Policy Studies. He is best known for his time as United Nations Chief Weapons Inspector following the first Gulf War and for leading of the Iraq Survey Group's search for weapons of mass destruction following the 2003 invasion of Iraq. Upon presentation of the Group's finding that there had been significant errors in pre-war intelligence concerning Iraq's weapons programs, Kay resigned. The ensuing controversy served as impetus for the formation of the Iraq Intelligence Commission.\n\nKay received a Bachelor of Arts degree from the University of Texas at Austin, and also a master's in International Affairs and a Ph.D. from Columbia University's School of International and Public Affairs.\n\nKay was an Assistant Professor of Political Science at the University of Wisconsin (Madison). Kay later worked for the International Atomic Energy Agency (IAEA) in an administrative position as head of the Evaluation Section and, as recommended by the US Mission to that Agency, he was named the UN Chief Weapons Inspector from 1991 to 1992. Following that, he was Vice President of Science Applications International Corporation (SAIC) from 1993 to 2002. While at SAIC, he worked alongside Steven Hatfill until March 2002. Then, he was appointed as a Special Advisor for Strategy regarding Iraqi Weapons of Mass Destruction (WMD) Programs. He received the International Atomic Energy Agency's Distinguished Service Award and the U.S. Secretary of State's Commendation. (SAIC was contracted by the U.S. to build prototype Mobile Weapons Laboratories in fall of 2001.)\n\nAfter the 1991 Gulf War, Kay led teams of inspectors of the International Atomic Energy Agency in Iraq to search out and destroy banned chemical, biological, and nuclear weapons. Following the U.S. invasion of Iraq, he returned to the country, working with the Central Intelligence Agency and U.S. military in 2003 and 2004 to determine if Saddam Hussein's regime had continued developing banned weapons.\n\nThe research of his team determined that the Iraqi unconventional weapons programs had mostly been held in check, with only small amounts of banned material uncovered (this included a number of vials containing biological agents stored in the home refrigerators of Iraqi scientists, for example). None of these substances had been \"weaponized\" — no such agents were found in missiles or artillery, and none could be easily installed. These discoveries indicate that some of the primary reasons President George W. Bush used for going to war with Iraq did not reflect the true situation in that country, and contradicted statements made by Kay himself in the lead-up to the war.\n\nBefore the 2003 war, as U.S. Government officials were pushing the idea that Saddam Hussein was in possession of WMD, many people would direct reporters toward David Kay to reinforce their point of view. In September 2002, Kay told \"U.S. News & World Report\" that \"Iraq stands in clear violation of international orders to rid itself of these weapons.\" His credibility as a former U.N. weapons inspector convinced many observers.\n\nOn January 23, 2004, Kay resigned, stating that Iraq did not have WMD and that \"I think there were stockpiles at the end of the first Gulf War and a combination of U.N. inspectors and unilateral Iraqi action got rid of them.\" Kay was replaced by Charles Duelfer and spent the following days discussing his discoveries and opinions with the news media and the U.S. political establishment. On January 28, 2004, he testified that “[i]t turns out that we were all wrong” and “I believe that the effort that has been directed to this point has been sufficiently intense that it is highly unlikely that there were large stockpiles of deployed, militarized chemical weapons there.” However, Kay defended the Bush administration, saying that even \"if\" Iraq did not have weapons stockpiles, this did not mean the it wasn't dangerous. Kay also blamed faulty intelligence gathering for the prewar WMD conclusions. On February 2, 2004, Kay met with George W. Bush at the White House and maintained that Bush was right to go to war in Iraq and characterized Saddam Hussein's government as “far more dangerous than even we anticipated” when it was thought he had WMDs ready to deploy.\n\nIn testimony on the progress of the Iraq Survey Group on October 2, 2003, he revealed to House and Senate committees that the ISG had found that Iraq had a network of clandestine laboratories containing equipment that should have been (but was not) disclosed to UN inspectors. He also said that the ISG found an undeclared prison laboratory complex and an undeclared Unmanned Aerial Vehicle production facility. The Iraq Survey Group also found out that a UAV had been test flown out to a range of 500 kilometers, even though the agreed upon limit was 150 kilometers. Kay said that Iraq lied to the UN about the range of that particular UAV.\n\nHe testified that Iraq had done research on Congo Crimean Hemorrhagic Fever and Brucella but had not declared this to the UN. Iraq also continued research and development work on anthrax and ricin without declaring it to the UN.\n\nKay told the committees that, between 1999 and 2002, Iraq attempted to obtain missile technology from North Korea that would allow them to build missiles with a range of 1300 kilometers, far beyond the UN limit of 150 kilometers that Iraq agreed upon in UN Resolution 687. They also sought anti-ship missiles with a range of 300 kilometers from North Korea.\n\n\"With regard to delivery systems, the ISG team has discovered sufficient evidence to date to conclude that the Iraqi regime was committed to delivery system improvements that would have, if OIF had not occurred, dramatically breached UN restrictions placed on Iraq after the 1991 Gulf War,\" Kay testified.\n\nAfter the interview, Kay told National Public Radio that Iraq \"had a large number of WMD program-related activities.\" He said \"So there was a WMD program. It was going ahead. It was rudimentary in many areas.\" Kay also said that Iraq had been trying to weaponize ricin \"right up until\" Operation Iraqi Freedom; a claim not supported by the Iraq Survey Group final report.\n\nOn 23 January 2004, the head of the ISG, David Kay, resigned his position, stating that he believed WMD stockpiles would not be found in Iraq. \"I don't think they existed,\" commented Kay. \"What everyone was talking about is stockpiles produced after the end of the last Gulf War and I don't think there was a large-scale production program in the nineties.\" In a briefing to the Senate Armed Services Committee (SASC), Kay criticized the pre-war WMD intelligence and the agencies that produced it, saying \"It turns out that we were all wrong, probably in my judgment, and that is most disturbing.\" Sometime earlier, CIA director George Tenet had asked David Kay to delay his departure: \"If you resign now, it will appear that we don't know what we're doing. That the wheels are coming off.\"\n\nKay told the SASC during his oral report the following, though: \"Based on the intelligence that existed, I think it was reasonable to reach the conclusion that Iraq posed an imminent threat. Now that you know reality on the ground as opposed to what you estimated before, you may reach a different conclusion — although I must say I actually think what we learned during the inspection made Iraq a more dangerous place, potentially, than, in fact, we thought it was even before the war.\"\n\nKay's team established that the Iraqi regime had the production capacity and know-how to produce chemical and biological weaponry if international economic sanctions were lifted, a policy change which was actively being sought by a number of United Nations member states. Kay also believed some components of the former Iraqi regime's WMD program had been moved to Syria shortly before the 2003 invasion,([3]) though the Duelfer Report Addenda (see below) later reported there was no evidence of this.\n\nKay explained the situation in Iraq before the war further in a 1 February 2004 interview on Fox News Sunday: \"I think Iraq was a dangerous place and becoming more dangerous, because, in fact, what we observe is that the regime itself was coming apart. It was descending into worse the part of moral depravity and corruption. Saddam was isolated in a fantasy land capable of wreaking tremendous harm and terror on his individual citizens, but corruption, money gain was the root cause. At the same time that we know there were terrorist groups in state still seeking WMD capability. Iraq, although I found no weapons, had tremendous capabilities in this area. A marketplace phenomena was about to occur, if it did not occur; sellers meeting buyers. And I think that would have been very dangerous if the war had not intervened.\" [sic] \n\nOn 6 February 2004, George W. Bush convened the Iraq Intelligence Commission, an independent inquiry into the intelligence used to justify the Iraq war and the failure to find WMD. This was shortly followed by the conclusion of a similar inquiry in the United Kingdom, the Butler Review, which was boycotted by the two main opposition parties due to disagreements on its scope and independence.([4]) In 2003, the US-sponsored search for WMD had been budgeted for $400 million, with an additional $600 million added in 2004.\n\nKay's successor, named by CIA Director George Tenet, was former UN weapons inspector Charles Duelfer, who stated at the time that the chances of finding any WMD stockpiles in Iraq were \"close to nil.\"\n\n\n\n"}
{"id": "460983", "url": "https://en.wikipedia.org/wiki?curid=460983", "title": "Ecotone", "text": "Ecotone\n\nAn ecotone is a transition area between two biomes. It is where two communities meet and integrate. It may be narrow or wide, and it may be local (the zone between a field and forest) or regional (the transition between forest and grassland ecosystems). An ecotone may appear on the ground as a gradual blending of the two communities across a broad area, or it may manifest itself as a sharp boundary line.\n\nThe word ecotone was coined from a combination of \"eco\"(logy) plus \"-tone\", from the Greek \"tonos\" or tension – in other words, a place where ecologies are in tension.\n\nThere are several distinguishing features of an ecotone. First, an ecotone can have a sharp vegetation transition, with a distinct line between two communities. For example,\na change in colors of grasses or plant life can indicate an ecotone. Second, a change in physiognomy (physical appearance of a plant species) can be a key indicator. Water bodies, such as estuaries, can also have a region of transition, and the boundary is characterized by the differences in heights of the macrophytes or plant species present in the areas because this distinguishes the two areas' accessibility to light. Scientists look at color variations and changes in plant height. Third, a change of species can signal an ecotone. There will be specific organisms on one side of an ecotone or the other.\n\nOther factors can illustrate or obscure an ecotone, for example, migration and the establishment of new plants. These are known as spatial mass effects, which are noticeable because some organisms will not be able to form self-sustaining populations if they cross the ecotone. If different species can survive in both communities of the two biomes, then the ecotone is considered to have species richness; ecologists measure this when studying the food chain and success of organisms. Lastly, the abundance of exotic species in an ecotone can reveal the type of biome or efficiency of the two communities sharing space. Because an ecotone is the zone in which two communities integrate, many different forms of life have to live together and compete for space. Therefore, an ecotone can create a diverse ecosystem.\n\nChanges in the physical environment may produce a sharp boundary, as in the example of the interface between areas of forest and cleared land. Elsewhere, a more gradually blended interface area will be found, where species from each community will be found together as well as unique local species. Mountain ranges often create such ecotones, due to the wide variety of climatic conditions experienced on their slopes. They may also provide a boundary between species due to the obstructive nature of their terrain. Mont Ventoux in France is a good example, marking the boundary between the flora and fauna of northern and southern France. Most wetlands are ecotones. The spatial variation of ecotones often form due to disturbances, creating patches that separate patches of vegetation. Different intensity of disturbances can cause landslides, land shifts, or movement of sediment that can create these vegetation patches and ecotones.\n\nPlants in competition extend themselves on one side of the ecotone as far as their ability to maintain themselves allows. Beyond this competitors of the adjacent community take over. As a result, the ecotone represents a shift in dominance. Ecotones are particularly significant for mobile animals, as they can exploit more than one set of habitats within a short distance. The ecotone contains not only species common to the communities on both sides; it may also include a number of highly adaptable species that tend to colonize such transitional areas. The phenomenon of increased variety of plants as well as animals at the community junction is called the edge effect and is essentially due to a locally broader range of suitable environmental conditions or ecological niches.\n\nAn ecotone is often associated with an ecocline: a \"physical transition zone\" between two systems. The ecotone and ecocline concepts are sometimes confused: an ecocline can signal an ecotone chemically (ex: pH or salinity gradient), or microclimatically (hydrothermal gradient) between two ecosystems.\n\nIn contrast:\n\n\n"}
{"id": "22287615", "url": "https://en.wikipedia.org/wiki?curid=22287615", "title": "Elektroprivreda Republike Srpske", "text": "Elektroprivreda Republike Srpske\n\nElektroprivreda Republike Srpske () or Elektroprivreda RS, is a Bosnian state-owned integrated power company with headquarters in Trebinje, Republika Srpska.\n\nIt is the largest employer in Republika Srpska and second-largest power utility in Bosnia and Herzegovina (after Elektroprivreda Bosne i Hercegovine). \n\n"}
{"id": "4576714", "url": "https://en.wikipedia.org/wiki?curid=4576714", "title": "Essential fatty acid interactions", "text": "Essential fatty acid interactions\n\nThe effects on humans of the ω-3 (omega-3) and ω-6 (omega-6) essential fatty acids (EFAs) are best characterized by their interactions; they cannot be understood separately.\n\nArachidonic acid (AA) is a 20-carbon ω-6 conditionally essential fatty acid. It sits at the head of the \"arachidonic acid cascade\" – more than 20 different signalling paths that control a wide array of bodily functions, but especially those functions involving inflammation, cell growth and the central nervous system. Most AA in the human body derives from dietary linoleic acid (another essential fatty acid, 18:2 ω-6), which is derived from nuts, seeds, vegetable oils and animal fats.\n\nIn the inflammatory response, two other groups of dietary essential fatty acids form cascades that parallel and compete with the arachidonic acid cascade. EPA (20:5 ω-3) provides the most important competing cascade. It is ingested from oily fish, and algae oil or derived from dietary alpha-linolenic acid found in, for instance, walnuts, hemp oil and flax oil. DGLA (20:3 ω-6) provides a third, less prominent cascade. It derives from dietary GLA (18:3 ω-6) found in, e.g. borage oil. These two parallel cascades soften the inflammatory promoting effects of certain eicosanoids made from AA. Low dietary intake of these less inflammatory promoting essential fatty acids, especially the ω-3s, is correlated with a variety of inflammation-related diseases.\n\nToday, the usual diet in industrial countries contains much less ω-3 fatty acids than the diet of a century ago and a much greater amount of air pollution on a daily basis that evokes the inflammatory response. The diet from a century ago had much less ω-3 than the diet of early hunter-gatherers but also much less pollution than today. We can also look at the ratio of ω-3 to ω-6 in comparisons of their diets. These changes have been accompanied by increased rates of many diseases – the so-called diseases of civilization – that involve inflammatory processes. There is now very strong evidence that several of these diseases are ameliorated by increasing dietary ω-3, and good evidence for many others. There is also more preliminary evidence showing that dietary ω-3 can ease symptoms in several psychiatric disorders. Nonetheless, fish oil supplement studies have failed to support claims of preventing heart attacks or strokes.\n\nResearch regarding krill oil, another animal-based omega-3 source, is ongoing. Preliminary studies appear to indicate that the DHA and EPA omega-3 fatty acids found in krill oil may be more bio-available than in fish oil. Additionally, krill oil contains astaxanthin, a marine-source keto-carotenoid antioxidant that may act synergistically with EPA and DHA.\n\nEicosanoids are signalling molecules derived from the essential fatty acids (EFA); they are a major pathway by which the EFAs act in the body. There are four classes of eicosanoid and two or three series within each class. Before discussing eicosanoid action, we will explain the series nomenclature.\n\nThe plasma membranes of cells contain phospholipids, which are composed of a hydrophilic phosphate head and two hydrophobic fatty acid tails. Some of these fatty acids are 20-carbon polyunsaturated essential fatty acids – AA, EPA or DGLA. In response to a variety of inflammatory signals, these EFAs are cleaved out of the phospholipid and released as free fatty acids. Next, the EFA is oxygenated (by either of two pathways), then further modified, yielding the eicosanoids.   Cyclooxygenase (COX) oxidation removes two C=C double bonds, leading to the TX, PG and PGI series.\nLipoxygenase oxidation removes no C=C double bonds, and leads to the LK.\n\nAfter oxidation, the eicosanoids are further modified, making a \"series\". Members of a series are differentiated by an \"ABC...\" letter, and are numbered by the number of double bonds, which does not change within a series. For example, cyclooxygenase action upon AA (with 4 double bonds) leads to the series-2 thromboxanes (TXA, TXB... ) each with two double bonds. Cyclooxygenase action on EPA (with 5 double bonds) leads to the series-3 thromboxanes (TXA, TXB... ) each with three double bonds. There are exceptions to this pattern, some of which indicate stereochemistry (PGF).\n\nTable (1) shows these sequences for AA (20:4 ω-6). The sequences for EPA (20:5 ω-3) and DGLA (20:3 ω-6) are analogous.\n\nAll the prostenoids are substituted prostanoic acids.\nCyberlipid Center's Prostenoid page illustrates the parent compound and the rings associated with each series–letter.\n\nThe IUPAC and the IUBMB use the equivalent term Icosanoid.\n\nIn the arachidonic acid cascade, dietary linoleic acid (18:2 ω-6) is desaturated and lengthened to form arachidonic acid, esterified into a phospholipid in the cell membrane. Next, in response to many inflammatory stimuli, such as air pollution, smoking, second-hand smoke, hydrogenated vegetable oils and other exogenous toxins; phospholipase is generated and cleaves this phospholipid, releasing AA as a free fatty acid. AA can then be oxygenated and then further modified to form eicosanoids – autocrine and paracrine agents that bind receptors on the cell or its neighbors to alert the immune system of the cell damage. Alternatively, AA can diffuse into the cell nucleus and interact with transcription factors to control DNA transcription for cytokines or other hormones.\n\nThe eicosanoids from AA generally promote inflammation. Those from GLA (\"via\" DGLA) and from EPA are generally less inflammatory, or inactive, or even anti-inflammatory. (This generalization is qualified: an eicosanoid may be pro-inflammatory in one tissue and anti-inflammatory in another. \"See\" discussion of PGE at Calder or Tilley.)\n\nFigure (2) shows the ω-3 and -6 synthesis chains, along with the major eicosanoids from AA, EPA and DGLA.\n\nDietary ω-3 and GLA counter the inflammatory effects of AA's eicosanoids in three ways – displacement, competitive inhibition and direct counteraction.\n\nDietary ω-3 decreases tissue concentrations of AA.\nAnimal studies show that increased dietary ω-3 results in decreased AA in brain and other tissue. Linolenic acid (18:3 ω-3) contributes to this by displacing linoleic acid (18:2 ω-6) from the elongase and desaturase enzymes that produce AA. EPA inhibits phospholipase A2's release of AA from cell membrane.   Other mechanisms involving the transport of EFAs may also play a role.\n\nThe reverse is also true – high dietary linoleic acid decreases the body's conversion of α-linolenic acid to EPA. However, the effect is not as strong; the desaturase has a higher affinity for α-linolenic acid than it has for linoleic acid.\n\nDGLA and EPA compete with AA for access to the cyclooxygenase and lipoxygenase enzymes. So the presence of DGLA and EPA in tissues lowers the output of AA's eicosanoids. For example, dietary GLA increases tissue DGLA and lowers TXB. Likewise, EPA inhibits the production of series-2 PG and TX. Although DGLA forms no LTs, a DGLA derivative blocks the transformation of AA to LTs.\n\nSome DGLA and EPA derived eicosanoids counteract their AA derived counterparts. For example, DGLA yields PGE, which powerfully counteracts PGE.   EPA yields the antiaggregatory prostacyclin PGI It also yields the leuokotriene LTB which vitiates the action of the AA-derived LTB.\n\nDietary oxidized linoleic acid (LA, 18:2 ω-6) is inflammatory. In the body, LA is desaturated to form GLA (18:3 ω-6), yet dietary GLA is anti-inflammatory. Some observations partially explain this paradox: LA competes with α-linolenic acid, (ALA, 18:3 ω-3) for Δ6-desaturase, and thereby eventually inhibits formation of anti-inflammatory EPA (20:5 ω-3). In contrast, GLA does not compete for Δ6-desaturase. GLA's elongation product DGLA (20:3 ω-6) competes with 20:4 ω-3 for the Δ5-desaturase, and it might be expected that this would make GLA inflammatory, but it is not, perhaps because this step isn't rate-determining. Δ6-desaturase does appear to be the rate-limiting step; 20:4 ω-3 does not significantly accumulate in bodily lipids.\n\nDGLA inhibits inflammation through both competitive inhibition and direct counteraction (see above.) Dietary GLA leads to sharply increased DGLA in the white blood cells' membranes, where LA does not. This may reflect white blood cells' lack of desaturase. Supplementing dietary GLA increases serum DGLA without increasing serum AA.\n\nIt is likely that some dietary GLA eventually forms AA and contributes to inflammation. Animal studies indicate the effect is small. The empirical observation of GLA's actual effects argues that DGLA's anti-inflammatory effects dominate.\n\nEicosanoid signaling paths are complex.\nIt is therefore difficult to characterize the action of any particular eicosanoid.\nFor example, PGE binds four receptors, dubbed EP.\nEach is coded by a separate gene, and some exist in multiple isoforms.\nEach EP receptor in turn couples to a G protein.\nThe EP, EP and one isoform of the EP receptors couple to G.\nThis increases intracellular cAMP and is anti-inflammatory.\nEP and other EP isoforms couple to G.\nThis leads to increased intracellular calcium and is pro-inflammatory.\nFinally, yet another EP isoform couples to G, which both decreases cAMP and increases calcium.\nMany immune-system cells express multiple receptors that couple these apparently opposing pathways. \nPresumably, EPA-derived PGE has a somewhat different effect of on this system, but it is not well-characterized.\n\nThe arachidonic acid cascade proceeds somewhat differently in the brain. Neurohormones, neuromodulators or neurotransmitters act as first messengers. They activate phospholipidase to release AA from neuron cell membranes as a free fatty acid. During its short lifespan, free AA may affect the activity of the neuron's ion channels and protein kinases. Or it may be metabolized to form eicosanoids, epoxyeicosatrienoic acids (EETs), neuroprotectin D or various endocannabinoids (anandamide and its analogs.)\n\nThe actions of eicosanoids within the brain are not as well characterized as they are in inflammation. It is theorized that they act within the neuron as second messengers controlling presynaptic inhibition and the activation of protein kinase C. They also act as paracrine mediators, acting across synapses to nearby cells. Although detail on the effects of these signals is scant, (Piomelli, 2000) comments\nNeurons in the CNS are organized as interconnected groups of functionally related cells (e.g., in sensory systems). A diffusible factor released from a neuron into the interstitial fluid, and able to interact with membrane receptors on adjacent cells, would be ideally used to \"synchronize\" the activity of an ensemble of interconnected neural cells. Furthermore, during development and in certain forms of learning, postsynaptic cells may secrete regulatory factors which diffuse back to the presynaptic component, determining its survival as an active terminal, the amplitude of its sprouting, and its efficacy in secreting neurotransmitters—a phenomenon known as retrograde regulation. The participation of arachidonic acid metabolites in retrograde signaling and in other forms of local modulation of neuronal activity has been proposed.\nThe EPA and DGLA cascades are also present in the brain and their eicosanoid metabolites have been detected. The ways in which these differently affect mental and neural processes are not nearly as well characterized as are the effects in inflammation.\n\nFigure (2) shows two pathways from EPA to DHA, including the exceptional Sprecher's shunt.\n\n5-LO acts at the fifth carbon from the carboxyl group.\nOther lipoxygenases—8-LO, 12-LO and 15-LO—make other eicosanoid-like products.\nTo act, 5-LO uses the nuclear-membrane enzyme 5-lipoxygenase-activating protein (FLAP), first to a hydroperoxyeicosatetraenoic acid (HPETE), then to the first leuokotriene, LTA.\n\n"}
{"id": "323260", "url": "https://en.wikipedia.org/wiki?curid=323260", "title": "Explosive booster", "text": "Explosive booster\n\nAn explosive booster is a sensitive explosive charge that acts as a bridge between a (relatively weak) conventional detonator and a low-sensitivity (but typically high-energy) explosive such as TNT. By itself, the initiating detonator would not deliver sufficient energy to set off the low-sensitivity charge. However, it detonates the primary charge (the booster), which then delivers an explosive shockwave that is sufficient to detonate the secondary, main, high-energy charge. \n\nUnlike C4 plastic explosive, not all explosives can be detonated simply by inserting a detonator and firing it.\n\nAn initiator such as a shock tube, cannon fuse, or even a conventional detonator does not deliver sufficient shock to detonate charges comprising TNT, Composition B, ANFO and many other high explosives. Therefore, some form of \"booster\" is required to amplify the energy released by the detonator so that the main charge will detonate. \n\nAt first, picric acid was used as a booster to detonate TNT, though it was superseded due to the inherent danger of picrate formation. Tetryl replaced picric acid because it is more stable, and was once a very popular chemical for booster charges, particularly during World War II. However, since then Tetryl has largely been replaced by other compositions, e.g. a small cylinder or pellet of phlegmatized RDX (e.g. CH-6 or Composition A-5) or PETN (slightly larger than the actual detonator) into which the detonator itself is inserted.\n\nNote: booby traps and improvised explosive devices frequently use plastic explosive as the booster charge, for example, some C4 or Semtex stuffed into the empty fuze pocket of a 120mm mortar shell. This is because any standard detonator will initiate plastic explosive as is.\n\nWhen encountered in connection with artillery shells or air dropped bombs, a booster charge is sometimes referred to as the \"gaine\". See detonators.\n\nAt a purely technical level, a sufficiently large detonator would initiate high explosives without the need for a booster charge. However, there are very good reasons why this method is never used. Firstly, there is a major safety issue, i.e. detonators are (like all primary explosives) much more sensitive to shock, heat, and friction than an explosive booster. Therefore, minimising the amount of primary explosive that users must store or carry greatly reduces the likelihood of serious accidents. An additional economic reason for using explosive booster charges is that chemical compounds used in detonators (e.g. lead styphnate) are comparatively expensive to produce and encapsulate when compared to the manufacturing costs of explosive boosters.\n\nA common form for boosters is to cast the explosive material into a cylindrical shell made of cardboard or plastic; these are accordingly known as cast boosters. \n"}
{"id": "46762634", "url": "https://en.wikipedia.org/wiki?curid=46762634", "title": "Fuel polishing", "text": "Fuel polishing\n\nFuel polishing is the technical cleaning process used to remove or filter microbial contamination from oil and hydrocarbon fuel in storage. It is essentially the removal of water, sediment and microbial contamination from such fuels as diesel, red diesel and biodiesel. This fuel contamination, also known as 'fuel bugs' or 'diesel bugs' build up over time in stored fuels if they aren't tested and treated on a regular basis. \n\nOver any given period of time fuels in storage can develop pools of sludge, sediment and water caused by microbial contamination. This contamination can quite easily find its way into fuel at one of the many exchange points in its journey from source to end user. Left untreated, the levels of contamination worsen to the point where damage to machinery, pumps and engines is likely. When fuel is pumped into a tank, condensation, atmospheric absorption, and a faulty tank are usually the main causes of fuel contamination. Microbes find their way into fuel and cause problems as they multiply and grow.\n\nIn April 2010, Cathay Pacific Flight 780, an aircraft carrying over 300 passengers, had a close call when fuel contaminated with spherical particles damaged the aircraft's engines, resulting in 57 passenger injuries as the pilots had to land the aircraft at twice the normal landing speed.\n\nThe symptoms of diesel bugs are easy to find. Important things to check over and look out for are:\n\n\nMicrobial contamination is significantly accelerated when higher biodiesel content content occurs along with lower sulphur content.\n\nAlmost everything on Earth contains elements of water; oil and fuel are no exceptions. While very small amounts exist in fuels to start with, stored fuel will become a breeding ground for the microbial bacteria and over time, the levels of damage change from dissolved to emulsified and finally free.\n\n\nThe contaminants found in fuel can be made up of a number of things, predominantly \"Hormoconis Resinae\", which is typically the main contaminant when microbial contamination is present, along with bacteria \"Pseudomonas Aeruginosa\", and fungi such as yeasts and moulds like \"Yarrowia tropicalis\".\n\nIn essence \"polishing\" is an advanced level filtration of the fuel, monitored and controlled via a central processing unit. Its application ensures the need for fuel additives is either massively reduced or totally removed. A typical process involves a single pass of the fuel through different apparatus, removing increasingly finer contaminant in each stage. The application of different filter technologies is vital; a single process is not sufficient to extract the different forms of water and matter found in fuel.\n\nDuring the fuel polishing process, multiple stages are required to effectively remove the solid particulates, wet foreign matter and water from the fuel. This multi-stage process ensures the high efficiency of the process; however, ensuring dense matter removal first is key, as is the removal of large matter on the suction side of the internal pumping stage. High pressurization of fuel contamination is notoriously dangerous and as such the majority of the polishing process should be carried out on the suction side. The pressure side process is often referred to as the final or finishing stage, where 2 micron particulate is removed and the fuel's final water content is extracted to within the efficiency of the filter stages. \n\nThe amount of time required to complete the fuel polishing process depends on several factors. One of the most important is the specific fuel polishing machine used. Fuel cleaning machines can polish at rates from 8 litres per minute to upwards of 400 litres per minute, however the factors which govern the process are about more than just flow rate. In fact a high flow rate can be detrimental to the overall effect due to increased pressures in the filter stages and adverse disturbance in the fuel tank. Another factor can be the quality of the fuel. If the fuel is highly contaminated it will typically take longer to clean than a batch of cleaner fuel of the same volume. The third factor to take into account is the volume of fuel to be processed. It may seem obvious but the larger the volume of fuel that requires polishing, the longer it will take to complete the cleaning process.\n"}
{"id": "1499520", "url": "https://en.wikipedia.org/wiki?curid=1499520", "title": "Funnel cloud", "text": "Funnel cloud\n\nA funnel cloud is a funnel-shaped cloud of condensed water droplets, associated with a rotating column of wind and extending from the base of a cloud (usually a cumulonimbus or towering cumulus cloud) but not reaching the ground or a water surface. A funnel cloud is usually visible as a cone-shaped or needle like protuberance from the main cloud base. Funnel clouds form most frequently in association with supercell thunderstorms.\n\nIf a funnel cloud touches the ground it becomes a tornado. Most tornadoes begin as funnel clouds, but many funnel clouds do not make ground contact and so do not become tornadoes. Also, a tornado does not necessarily need to have an associated condensation funnel. If strong cyclonic winds are occurring at the surface (and connected to a cloud base, regardless of condensation), then the feature is a tornado. Some tornadoes may appear only as a debris swirl, with no obvious funnel cloud extending below the rotating cloud base.\n\nIn cloud nomenclature, any funnel- or inverted-funnel-shaped cloud descending from cumulus or cumulonimbus clouds is technically described as an accessory feature called \"tuba\". The terms \"tuba\" and \"funnel cloud\" are nearly but not exactly synonymous; a wall cloud, for example, is also a form of \"tuba\".\n\nCold-air funnel clouds (vortices) are usually short-lived and generally much weaker than the vortices produced by supercells. Although cold-air funnels rarely make ground contact, they may touch down briefly and become weak tornadoes or waterspouts.\nUnlike the related phenomenon associated with severe thunderstorms, cold-air funnels are generally associated with partly cloudy skies in the wake of cold fronts, where atmospheric instability and moisture is sufficient to support towering cumulus clouds but not precipitation. The mixing of cooler air in the lower troposphere with air flowing in a different direction in the middle troposphere causes the rotation on a horizontal axis, which, when deflected vertically by atmospheric conditions, can become a funnel cloud.\n\nThey are a common sight along the Pacific Coast of the United States, particularly in the spring or fall.\n\nOn July 29, 2013, a cold-core funnel cloud touched down as an EF0 tornado in Ottawa, Canada, causing extensive damage in the form of downed trees on a golf course. No advance weather watches or warnings were issued by Environment Canada, and the tornado was spawned from one of the few non-severe storm clouds moving through the area.\n\n\n"}
{"id": "48745116", "url": "https://en.wikipedia.org/wiki?curid=48745116", "title": "High-confinement mode", "text": "High-confinement mode\n\nHigh-confinement mode or H-mode is an operating mode possible in toroidal magnetic confinement fusion devices - mostly tokamaks but also in stellarators.\nIn this mode the plasma is more stable and better confined.\n\nIt was discovered by Fritz Wagner in 1982 during neutral beam heating of the plasma at ASDEX. It has since been reproduced in all major toroidal confinement devices and is planned in the operation of ITER. Its self-consistent theoretical description was a topic of research in 2007. It was still considered a mystery with multiple competing theories (eg. predator-prey model) in 2016.\n\nPrior to the H-mode’s discovery, all tokamaks operated in what is now called the or low-confinement mode. The is characterized by relatively large amounts of turbulence, which allows energy to escape the confined plasma. Moreover, it was observed that as the heating power applied to an plasma increased, the confinement decreased. However, it was discovered in 1982 on the ASDEX tokamak that if the heating power applied using neutral beams was increased beyond a certain critical value, then the plasma spontaneously transitioned into a higher confinement state. This new state was called the and the old lower confinement state was in turn called the Due to its improved confinement properties, quickly became the desired operating regime for most tokamak reactor designs.\n\n"}
{"id": "9957281", "url": "https://en.wikipedia.org/wiki?curid=9957281", "title": "Holding Slovenske elektrarne", "text": "Holding Slovenske elektrarne\n\nHolding Slovenske elektrarne (HSE) is a state-owned power generation company in Slovenia. It is the largest company in Slovenia and was established by a government decision on 26 July 2001. The company consists of hydroelectric plants based on the Drava, Sava, and Soča rivers and coal-fired power plants in Brestanica, Šoštanj, and Velenje.\n\nHSE has the following subsidiaries:\n\nOn 28 August 2007 HSE acquired the Ruse Iztok Power Plant in Bulgaria for €81 million.\n\n"}
{"id": "561845", "url": "https://en.wikipedia.org/wiki?curid=561845", "title": "Jacob Bekenstein", "text": "Jacob Bekenstein\n\nJacob David Bekenstein (; May 1, 1947 – August 16, 2015) was a Mexican-born Israeli-American theoretical physicist who made fundamental contributions to the foundation of black hole thermodynamics and to other aspects of the connections between information and gravitation.\n\nBekenstein was born in Mexico City in 1947, to parents Joseph and Esther (\"née\" Vladaslavotsky), Polish Jews who had migrated to Mexico. He moved to the United States during his early life, gaining U.S. citizenship in 1968. He was also a citizen of Israel.\n\nAs a student, Bekenstein attended the Polytechnic Institute of Brooklyn, now known as the New York University Polytechnic School of Engineering, obtaining both an undergraduate degree and a Master of Science degree in 1969. He went on to receive a Doctor of Philosophy degree from Princeton University, working under the direction of John Archibald Wheeler, in 1972.\n\nBy 1972, Bekenstein was already making a name for himself in the field of theoretical physics. He published three groundbreaking and influential papers regarding the black hole stellar phenomenon, which was not well understood at the time, postulating the no-hair theorem and coming up with a theory on black hole thermodynamics that year. In the years to come, Bekenstein continued his exploration of black holes, publishing papers on their entropy and quantum mass, among other subjects.\n\nBekenstein was a postdoctoral fellow at the University of Texas at Austin from 1972 to 1974. He then moved to Israel to lecture and teach at Ben-Gurion University in Beersheba, becoming a full professor by 1978 and head of the astrophysics department by 1983. He left Ben-Gurion University to become a professor at the Hebrew University of Jerusalem in 1990, becoming head of its theoretical physics department three years later. He was elected to the Israel Academy of Sciences and Humanities in 1997. He was a visiting scholar at the Institute for Advanced Study in 2009 and 2010.\n\nIn addition to his lectures and residencies around the world, Bekenstein continued to serve as Polak professor of theoretical physics at the Hebrew University up until his death at the age of 68, in Helsinki, Finland. He died unexpectedly on August 16, 2015, just months after receiving the American Physical Society's Einstein Prize \"for his ground-breaking work on black hole entropy, which launched the field of black hole thermodynamics and transformed the long effort to unify quantum mechanics and gravitation.\"\n\nIn 1972, Bekenstein was the first to suggest that black holes should have a well-defined entropy. He wrote that a black hole's entropy was proportional to the area of its (the black hole's) event horizon. Bekenstein also formulated the generalized second law of thermodynamics, black hole thermodynamics, for systems including black holes. Both contributions were affirmed when Stephen Hawking (and, independently, Zeldovich and others) proposed the existence of Hawking radiation two years later. Hawking had initially opposed Bekenstein's idea on the grounds that a black hole could not radiate energy and therefore could not have entropy. However, in 1974, Hawking performed a lengthy calculation that convinced him that particles can indeed be emitted from black holes. Today this is known as Hawking radiation. Bekenstein's doctoral adviser, John Archibald Wheeler, also worked with him to develop the no-hair theorem, a reference to Wheeler's saying that \"black holes have no hair,\" in the early 1970s. Bekenstein was the first physicist to postulate such a theorem. His suggestion was proven to be unstable, but it was influential in the development of the field.\n\nBased on his black-hole thermodynamics work, Bekenstein also demonstrated the Bekenstein bound: there is a maximum to the amount of information that can potentially be stored in a given finite region of space which has a finite amount of energy (which is similar to the holographic principle).\n\nIn 1982, Bekenstein was the first person to develop a rigorous framework to generalize the laws of electromagnetism to handle inconstant physical constants. His framework replaces the fine-structure constant by a scalar field. However, this framework for changing constants did not incorporate gravity.\n\nIn 2004, Bekenstein greatly boosted Mordehai Milgrom's theory of Modified Newtonian Dynamics (MOND) by developing a relativistic version. It is known as TeVeS for Tensor/Vector/Scalar and it introduces three different fields in space time to replace the one gravitational field.\n\nBekenstein had three children with his wife, Bilha. All three children, Yehonadav, Uriya and Rivka Bekenstein, became scientists. Bekenstein was known as a religious man and a believer, being quoted as saying: \"I look at the world as a product of God, He set very specific laws and we delight in discovering them through scientific work.\"\n\n\n\n"}
{"id": "2471842", "url": "https://en.wikipedia.org/wiki?curid=2471842", "title": "Kevin Smith (conservationist)", "text": "Kevin Smith (conservationist)\n\nKevin David Smith (21 November 1953 – 16 August 2005) was a New Zealand conservationist.\n\nSmith was born in Owhango, a small town in the King Country. He was active in the area of forest conservation. In 1985 Smith was employed by Royal Forest and Bird Protection Society of New Zealand as a West Coast conservation officer before being appointed as the Society’s conservation director in 1989. He remained as the director until 2000 when he was employed as an adviser to Minister of Conservation Sandra Lee-Vercoe. He continued as an adviser to Minister of Conservation Chris Carter.\n\nHe has been credited as playing a major role in conservation efforts in New Zealand, including being instrumental in ending the logging of native forests on the West Coast.\n\nHe died suddenly on 16 August 2005 while cycling with his daughter in Wellington.\n\n"}
{"id": "4243751", "url": "https://en.wikipedia.org/wiki?curid=4243751", "title": "Koenigsegg CCX", "text": "Koenigsegg CCX\n\nThe Koenigsegg CCX is a mid-engined sports car manufactured by Swedish automotive manufacturer Koenigsegg Automotive AB. The project began with the aim of making a global car, designed and engineered to comply with global safety and environment regulations, particularly to enter the United States car market. To sell cars in the US many alterations were made to the design of the CCR; the previously used Ford Modular engine was replaced by an in-house developed Koenigsegg engine designed to run on 91 octane fuel, readily available in the United States, and to meet the Californian emission standards.\n\nThe name \"CCX\" is an abbreviation for \"Competition Coupé X,\" the X commemorating the 10th anniversary (X being the Roman numeral for ten) of the completion and test drive of the first CC prototype in 1996.\n\nThe CCX was unveiled at the 2006 Geneva Motor Show, sporting body modifications to meet US vehicle regulations and a new in-house developed 4.7&nhas a twin turbo in the wheel it has turbo in its eahast and mason has fake jordans bsp;L twin supercharged V8 engine capable of generating a maximum power output of at 7,000 rpm and of torque at 5,500 rpm while running on 91 octane gasoline. \n\nThe new engine is of all aluminium construction, made out of 356 aluminium with a T7 heat treatment to further enhance block integrity and cylinder bore chill during casting. Specifically created and casted for Koenigsegg by Grainger & Worrall, a casting specialist with F1 experience in drivetrain components, the engine was built, assembled and tested at their Ängelholm production plant. The engine is lubricated with a dry sump system with a separate oil pump and the pistons are cooled by means of an internal cooler that sprays oil onto them in order to run high cylinder pressure with 91 octane fuel\nAvailable transmissions are a CIMA 6-speed manual and a 6-speed sequential manual transmission. Power is fed to the wheels through a torque-sensitive limited slip differential.\n\nThe chassis is made from carbon fibre reinforced with kevlar and aluminium honeycomb like the previous models. While the body keeps the targa top body style and the dihedral synchro-helix actuation doors, it is completely reworked. There is a new front bumper design, enhanced brake cooling, fog lamps, US patented head lamps, a new fresh air intake on the bonnet that acts as ram air booster, air intakes behind the front wheels to enhance airflow and a glass window over the engine.\n\nThe CCX has frontal area of and a ., with a CdA of . It also has a flat underside with venturi tunnels at the rear and an optional rear spoiler to improve downforce. At there is of downforce over the front axle and over the rear. The car is longer to comply with the US rear impact regulations and to free space around the rear muffler. On the interior, there is of extra headroom as well as specifically designed Sparco carbon fibre seats.\n\nFirst in the industry carbon fibre wheels are optional equipment, lighter than the standard forged alloy \"telephone-dial\" wheels, both using center locking nuts. Diameter is 19 inches at the front and 20 inches at the rear equipped with 255/35 Y19 front, 335/30 Y20 rear Michelin Pilot Sport 2 tires, 8 piston caliper carbon ceramic brakes measuring in diameter at the front and 6 piston caliper at the rear are optional, saving another of unsprung weight.\n\nThe CCXR is a more \"environment friendly\" version of the CCX, powered by the same engine, but converted to use E85 and E100 ethanol fuel, as well as standard 98 octane petrol. The CCXR required modified fuel injectors, upgraded fuel lines and piston rings, and a higher boost setting on the superchargers. In standard form, bodywork of the CCXR features heavily optimized aerodynamics, along with optional diffusers to enhance airflow, including an optional front splitter with an aerodynamic nolder. When run on ethanol, the power increases to at 7000 rpm and of torque at 5600 rpm. This is a direct result of the cooling properties of ethanol in the engine's combustion chambers along with the added boost, made possible by ethanol's higher octane rating when compared to gasoline. Due to the lower specific energy content of ethanol, the CCXR burns slightly more fuel than the CCX with a combined fuel consumption of under the EU cycle.\n\nIn March 2009 the CCXR was chosen by Forbes as one of the ten most beautiful cars in history.\n\nAt the 2008 Geneva Motor Show Koenigsegg presented two special edition models, the CCX Edition and the CCXR Edition, both fitted with a remapped, 4.8 L twin-supercharged V8 engine and limited to 2 and 4 units respectively. The modifications to the engine increase the power of the CCX Edition to on normal gasoline and of torque while the CCXR Edition ratings remained unchanged over the CCXR. Later, Koenigsegg also built 2 CCXR Special Edition cars which when compared to the old CCXR Edition had updated aerodynamics and an F1 Paddleshift gearbox.\n\nThe Edition models are more track oriented compared to the standard models, being equipped with stiffer springs and anti-roll bars, new dampers and a lowered chassis, a bare carbon body, unique 11 spoke aluminium wheels with a large adjustable twin-deck rear wing, and a larger front splitter and side skirts, all of which make the car capable of producing of downforce at . The interior is also reworked and features; colour matched leather carpets, Koenigsegg Edition side step plates, Edition chronograph instrument cluster, and features a special version of the Koenigsegg Chronocluster including a redesigned centre console. All other extra equipment for the Koenigsegg CCX and CCXR Edition comes as standard which include optional carbon fibre wheels, a rearview camera, Satnav or Bluetooth, amplifiers, and a complete Inconell exhaust system.\n\nThe CCXR Trevita is a limited edition of the Koenigsegg CCXR featuring a diamond weave carbon fibre finish. \"Trevita\" is an abbreviation in Swedish and translates into \"three whites\". The Koenigsegg Proprietary Diamond Weave, fully developed by Koenigsegg, is a new and unique method to manufacture the carbon fibre used for the CCXR Trevita. By utilising this new and unique method, Koenigsegg managed to coat the fibres with a diamond finish. The fibre treatment is conducted carefully in small quantities, prior to further processing the pre-production material.\n\nInitially, three CCXR Trevitas were planned to be produced, however, because of the complexity to make the special diamond weave carbon-fibre, only 2 were ever made, making it one of the rarest vehicles manufactured by Koenigsegg. Both cars featured the Koenigsegg Shimmering Diamond Weave bodywork, twin-deck carbon fibre rear wing, inconel exhaust system, carbon ceramic brakes with ABS, airbags, F1 paddle-shift gearbox, chrono instrument cluster, infotainment system, tire monitoring system and a hydraulic lifting system. The Trevita is also one of the most expensive sports cars, costing US$4.8 million. \nIn order to compete in the FIA GT Championship Koenigsegg created the CCGT race car, based on the production CC model range; making its debut appearance at the 2007 Geneva Motor Show and built to comply with the ACO and FIA regulations for the GT1 class.\n\nThe CCGT's engine is based on the unit used in the Koenigsegg CCX with the superchargers removed and the capacity increased to 5.0 L to compensate for the loss of power. Due to the already lightweight construction of the road-going model it was based on, the weight was easily reduced under the minimum , which means that the Ballast can be placed optimally in order to meet the mandatory weight.\n\nAs the car neared completion, the FIA GT1 regulations were suddenly changed, according to the altered regulations, there had to be a minimum of 350 road cars produced per year of the model that was to compete, something that Koenigsegg was unable to achieve, prohibiting the CCGT from racing.\n\nThe CCX was produced between 2006 and 2010 and the total production amounted to 49 cars (30 CCX, 9 CCXR, 6 CCX/CCXR Edition, 2 CCXR Special Edition and 2 CCXR Trevita). One of them was a CCX used for Crash tests and the other was a CCXR which is still a factory test car. Some CCX cars have later been upgraded to CCXR-specifications.\n\n\nIn 2007, the CCX was the fastest car to complete Top Gear's Power Lap with a time of 1:17.6 (until it was beaten by the Ascari A10 with a time of 1:17.3). The car originally lapped the circuit in 1:20.4, but was then fitted with an optional rear wing to provide downforce after The Stig spun it off the track at 130mph due to a power steering failure. The Stig purportedly recommended this modification, predicting that the car would then be the fastest ever round \"Top Gear\"<nowiki>'</nowiki>s track but Koenigsegg later stated that the improvement was due to adjustments to the chassis and suspension settings and not the addition of the rear spoiler. Despite this, the Stig's spoiler-idea remained the credited reason for the improved lap time. The name of the car on the lap board as \"Koeniggggsenisseggsegnignigsegigisegccx2 with the Top Gear wing\", reportedly because none of the Top Gear Presenters knew how to spell Koenigsegg, and also poking fun at the long and hard pronunciation of the manufacturer's name.\n\n"}
{"id": "4265190", "url": "https://en.wikipedia.org/wiki?curid=4265190", "title": "Kutta condition", "text": "Kutta condition\n\nThe Kutta condition is a principle in steady-flow fluid dynamics, especially aerodynamics, that is applicable to solid bodies with sharp corners, such as the trailing edges of airfoils. It is named for German mathematician and aerodynamicist Martin Kutta.\n\nKuethe and Schetzer state the Kutta condition as follows:\n\nIn fluid flow around a body with a sharp corner, the Kutta condition refers to the flow pattern in which fluid approaches the corner from both directions, meets at the corner, and then flows away from the body. None of the fluid flows around the sharp corner.\n\nThe Kutta condition is significant when using the Kutta–Joukowski theorem to calculate the lift created by an airfoil with a sharp trailing edge. The value of circulation of the flow around the airfoil must be that value which would cause the Kutta condition to exist.\n\nApplying 2-D potential flow, if an airfoil with a sharp trailing edge begins to move with a positive angle of attack through air, the two stagnation points are initially located on the underside near the leading edge and on the topside near the trailing edge, just as with the cylinder. As the air passing the underside of the airfoil reaches the trailing edge it must flow around the trailing edge and along the topside of the airfoil toward the stagnation point on the topside of the airfoil. Vortex flow occurs at the trailing edge and, because the radius of the sharp trailing edge is zero, the speed of the air around the trailing edge should be infinitely fast. Though real fluids cannot move at infinite speed, they can move very fast. The high airspeed around the trailing edge causes strong viscous forces to act on the air adjacent to the trailing edge of the airfoil and the result is that a strong vortex accumulates on the topside of the airfoil, near the trailing edge. As the airfoil begins to move it carries this vortex, known as the starting vortex, along with it. Pioneering aerodynamicists were able to photograph starting vortices in liquids to confirm their existence.\n\nThe vorticity in the starting vortex is matched by the vorticity in the bound vortex in the airfoil, in accordance with Kelvin's circulation theorem. As the vorticity in the starting vortex progressively increases the vorticity in the bound vortex also progressively increases and causes the flow over the topside of the airfoil to increase in speed. The starting vortex is soon cast off the airfoil and is left behind, spinning in the air where the airfoil left it. The stagnation point on the topside of the airfoil then moves until it reaches the trailing edge. The starting vortex eventually dissipates due to viscous forces.\n\nAs the airfoil continues on its way, there is a stagnation point at the trailing edge. The flow over the topside conforms to the upper surface of the airfoil. The flow over both the topside and the underside join up at the trailing edge and leave the airfoil travelling parallel to one another. This is known as the Kutta condition.\n\nWhen an airfoil is moving with a positive angle of attack, the starting vortex has been cast off and the Kutta condition has become established, there is a finite circulation of the air around the airfoil. The airfoil is generating lift, and the magnitude of the lift is given by the Kutta–Joukowski theorem.\n\nOne of the consequences of the Kutta condition is that the airflow over the topside of the airfoil travels much faster than the airflow under the underside. A parcel of air which approaches the airfoil along the stagnation streamline is cleaved in two at the stagnation point, one half traveling over the topside and the other half traveling along the underside. The flow over the topside is so much faster than the flow along the underside that these two halves never meet again. They do not even re-join in the wake long after the airfoil has passed. This is sometimes known as \"cleavage\". There is a popular fallacy called the equal transit-time fallacy that claims the two halves rejoin at the trailing edge of the airfoil. This fallacy is in conflict with the phenomenon of cleavage that has been understood since Martin Kutta's discovery.\n\nWhenever the speed or angle of attack of an airfoil changes there is a weak starting vortex which begins to form, either above or below the trailing edge. This weak starting vortex causes the Kutta condition to be re-established for the new speed or angle of attack. As a result, the circulation around the airfoil changes and so too does the lift in response to the changed speed or angle of attack.\n\nThe Kutta condition gives some insight into why airfoils usually have sharp trailing edges, even though this is undesirable from structural and manufacturing viewpoints.\n\nIn irrotational, inviscid, incompressible flow (potential flow) over an airfoil, the Kutta condition can be implemented by calculating the stream function over the airfoil surface. \nThe same Kutta condition implementation method is also used for solving two dimensional subsonic (subcritical) inviscid steady compressible flows over isolated airfoils.\nThe viscous correction for the Kutta condition can be found in some of the recent studies.\n\nThe Kutta condition allows an aerodynamicist to incorporate a significant effect of viscosity while neglecting viscous effects in the underlying conservation of momentum equation. It is important in the practical calculation of lift on a wing.\n\nThe equations of conservation of mass and conservation of momentum applied to an inviscid fluid flow, such as a potential flow, around a solid body result in an infinite number of valid solutions. One way to choose the correct solution would be to apply the viscous equations, in the form of the Navier–Stokes equations. However, these normally do not result in a closed-form solution. The Kutta condition is an alternative method of incorporating some aspects of viscous effects, while neglecting others, such as skin friction and some other boundary layer effects.\n\nThe condition can be expressed in a number of ways. One is that there cannot be an infinite change in velocity at the trailing edge. Although an inviscid fluid can have abrupt changes in velocity, in reality viscosity smooths out sharp velocity changes. If the trailing edge has a non-zero angle, the flow velocity there must be zero. At a cusped trailing edge, however, the velocity can be non-zero although it must still be identical above and below the airfoil. Another formulation is that the pressure must be continuous at the trailing edge.\n\nThe Kutta condition does not apply to unsteady flow. Experimental observations show that the stagnation point (one of two points on the surface of an airfoil where the flow speed is zero) begins on the top surface of an airfoil (assuming positive effective angle of attack) as flow accelerates from zero, and moves backwards as the flow accelerates. Once the initial transient effects have died out, the stagnation point is at the trailing edge as required by the Kutta condition.\n\nMathematically, the Kutta condition enforces a specific choice among the infinite allowed values of circulation.\n\n\n"}
{"id": "46381123", "url": "https://en.wikipedia.org/wiki?curid=46381123", "title": "Lightweighting", "text": "Lightweighting\n\nLightweighting is a concept in the auto industry about building cars and trucks that are less heavy as a way to achieve better fuel efficiency and handling. Carmakers make parts from carbon fiber, windshields from plastic, and bumpers out of aluminum foam, as ways to lessen vehicle load. Replacing car parts with lighter materials does not lessen overall safety for drivers, according to one view, since many plastics have a high strength-to-weight ratio. The search to replace car parts with lighter ones is not limited to any one type of part; according to a spokesman for Ford Motor Company, engineers strive for lightweighting \"anywhere we can.\" Using lightweight materials such as plastics can mean less strain on the engine and better gas mileage as well as improved handling. One material sometimes used to reduce weight is carbon fiber. The auto industry has used the term for many years, as the effort to keep making cars lighter is ongoing. \n\nAnother common material used for lightweighting is aluminum. Incorporating aluminum has grown continuously to not only meet CAFE standards, but to also improve automotive performance. A vehicle with lower weight has better acceleration, braking and handling. In addition, lighter vehicles can tow and haul larger loads because the engine is not carrying unnecessary weight. Even though aluminum is light, it does not sacrifice strength: aluminum body structure is equal in strength to steel and can absorb twice as much crash-induced energy.\n"}
{"id": "30294290", "url": "https://en.wikipedia.org/wiki?curid=30294290", "title": "Lokta paper", "text": "Lokta paper\n\nLokta paper is a wildcrafted, handmade artisan paper indigenous to Nepal.\n\nNepalese handmade lokta paper is made from the fibrous inner bark of high elevation evergreen shrubs primarily from two species of \"Daphne\" (plant) (Greek: meaning \"Laurel\"): \"Daphne bholua\" and \"Daphne papyracea\", known collectively and vernacularly as lokta bushes.\n\nLokta bushes proliferate in open clusters or colonies on the southern slopes of Nepal's Himalayan forests between 1,600 and 4,000 m (c.5,250–13,000 ft).\n\nHistorically the handcrafting of lokta paper occurred in the rural areas of Nepal, most notably in the Baglung District. Today raw lokta paper is produced in more than 22 districts in Nepal, but finished lokta paper products are produced only in Kathmandu Valley and Janakpur.\n\nLokta paper's durability and resistance to tearing, humidity, insects and mildew have traditionally made lokta paper the preferred choice for the recording of official government records (see photo on right) and sacred religious texts.\n\nThe earliest surviving lokta paper document appears in Nepal's National Archives in Kathmandu in the form of the sacred Buddhist text, the Karanya Buha Sutra. The Karanya Buha Sutra was written in Lichchhavi script and block printed on lokta paper and is estimated to be between 1,000 and 1,900 years old.\n\nWith the introduction of paper craft imports from Tibet in the 1930s, the production of handmade lokta paper began to decline. By the 1960s competition from commercially mass-produced paper from India placed the Nepalese handmade paper industry in a state of terminal decline with only a few families in Baglung and neighboring Parbat District retaining the traditional knowledge of handmade lokta paper production.\n\nIn the 1970s interest in rejuvenating lokta craft paper making occurred as the tourism industry in Nepal began to grow. Moreover, an effective conservation program was started in 1970 for the development of national parks and wildlife reserves in Nepal to provide raw materials for the development of forest based industries such as the production of lokta paper.\n\nIn the 1980s the United Nations Children's Fund (UNICEF) and the Agricultural Development Bank of Nepal/Small Farmer Development Program (ADBN/SFDP) launched the CDHP (Community Development and Health Project) project to revive Nepal's indigenous paper making processes.\n\nIn the late 1980s and early 1990s, with the growth of the popularity of lokta paper on the rise, Nepalese social and environmental entrepreneurs sought out and developed international trading partners and the export market for handmade lokta paper was established. Today the handmade paper industry in Nepal is growing at a rate of 15% per year. \nOnce the lokta paper is produced it can last for some millenniums (2000–3500 years).\n\nLokta paper production is a forest-based industry. It relies as much on a ready supply of Daphne bark as it does on the skills of traditional paper makers and block printers, and on markets for end products. There are four main steps in manufacturing and marketing of lokta paper and lokta paper craft products: 1) harvesting the lokta bark 2) processing the paper pulp 3) producing craft products from the finished paper 4) marketing the final products.\n\nThe Daphne shrub, a subspecies of laurel, grows wildly and covers more than million hectares of forest land in 55 districts of Nepal, of which 25 districts have an abundant supply. Like the laurel plants (laurus nobilis) of Turkey and Syria, the Daphne shrub is one of the world's few underutilized species.\n\nLokta is a non-wood forest product (NWFP) harvested from protected areas (national parks, reserves, conservation areas) and is an important reservoir of biological resources maintained under in situ condition in the unique and diverse Himalayan ecosystems. When harvested, the lokta bush automatically regenerates to a fully grown 4-5 meter plant with in 5–7 years.\n\nWomen's involvement: The poor rural women of Nepal have traditionally been the principal forest users. Current economic conditions have reinforced the local employment of women, as many men are leaving the rural villages in search of employment.\n\nThe Cottage Industry Department of Nepal reports 377 registered handmade paper production industries, out of approximately\n600 units operating in the country. Of these, 175 manufacture about 30,000 metric tons of paper products each year. Yet, despite\nthis major increase in handmade paper production in recent years, large scale lokta resources remain untapped. Handmade lokta-based craft paper products continue to offer considerable economic sustainability for poor rural Nepalese women due to their high-quality niche market potential.\n\nAlthough the traditional uses of lokta paper were largely restricted to government documents or religious texts, today lokta paper use is wide spread. Lokta paper is used for prayer flags, book bindings, restaurant menus (see photo on right), wallpaper wrapping paper to retain the potency of incense, spices and medicine, packaging, origami, Nathan Miller Chocolate bean to bar chocolate packaging, and even dresses.\n"}
{"id": "47568", "url": "https://en.wikipedia.org/wiki?curid=47568", "title": "Low Earth orbit", "text": "Low Earth orbit\n\nA low Earth orbit (LEO) is defined by Space-Track.org as an Earth-centered orbit with at least 11.25 periods per day (an orbital period of 128 minutes or less) and an eccentricity less than 0.25. Most of the manmade objects in space are in LEO orbits. A histogram of the mean motion of the cataloged objects shows that the number of objects drops significantly beyond 11.25. \n\nThere is a large variety of other sources that define LEO in terms of altitude. The altitude of an object in an elliptic orbit can vary significantly along the orbit. Even for circular orbits, the altitude above ground can vary by as much as (especially for polar orbits) due to the oblateness of Earth's spheroid figure and local topography. While definitions in terms of altitude are inherently ambiguous, most of them fall within the range specified by an orbit period of 128 minutes because, according to Kepler's third law, this corresponds to a semi-major axis of . For circular orbits, this in turn corresponds to an altitude of above the mean radius of Earth, which is consistent with some of the upper limits in the LEO definitions in terms of altitude.\n\nThe LEO region is defined by some sources as the region in space that LEO orbits occupy. Some highly elliptical orbits may pass through the LEO region near their lowest altitude (or perigee) but are not in an LEO Orbit because their highest altitude (or apogee) exceeds . Sub-orbital objects can also reach the LEO region but are not in an LEO orbit because they re-enter the atmosphere. The distinction between LEO orbits and the LEO region is especially important for analysis of possible collisions between objects which may not themselves be in LEO but could collide with satellites or debris in LEO orbits. \n\nThe International Space Station conducts operations in LEO. All crewed space stations to date, as well as the majority of satellites, have been in LEO. The altitude record for human spaceflights in LEO was Gemini 11 with an apogee of . Apollo 8 was the first mission to carry humans beyond LEO on Dec 21–27, 1968. The Apollo program continued during the four-year period spanning 1968 through 1972 with 24 astronauts who flew lunar flights but since then there have been no human spaceflights beyond LEO.\n\nThe mean orbital velocity needed to maintain a stable low Earth orbit is about 7.8 km/s, but reduces with increased orbital altitude. Calculated for circular orbit of 200 km it is 7.79 km/s and for 1500 km it is 7.12 km/s. The delta-v needed to achieve low Earth orbit starts around 9.4 km/s. Atmospheric and gravity drag associated with launch typically adds 1.3–1.8 km/s to the launch vehicle delta-v required to reach normal LEO orbital velocity of around .\n\nThe pull of gravity in LEO is only slightly less than on the earth's surface. This is because the distance to LEO from the earth's surface is far less than the earth's radius. However, an object in orbit is, by definition, in free fall, since there is no force holding it up. As a result objects in orbit, including people, experience a sense of weightlessness, even though they are not actually without weight.\n\nObjects in LEO encounter atmospheric drag from gases in the thermosphere (approximately 80–500 km above the surface) or exosphere (approximately 500 km and up), depending on orbit height. Due to atmospheric drag, satellites do not usually orbit below 300 km. Objects in LEO orbit Earth between the denser part of the atmosphere and below the inner Van Allen radiation belt.\n\nEquatorial low Earth orbits (ELEO) are a subset of LEO. These orbits, with low inclination to the Equator, allow rapid revisit times and have the lowest delta-v requirement (i.e., fuel spent) of any orbit. Orbits with a high inclination angle to the equator are usually called polar orbits.\n\nHigher orbits include medium Earth orbit (MEO), sometimes called intermediate circular orbit (ICO), and further above, geostationary orbit (GEO). Orbits higher than low orbit can lead to early failure of electronic components due to intense radiation and charge accumulation.\n\nIn 2017, a very-low LEO orbit began to be seen in regulatory filings. This orbit, referred to as \"VLEO\", requires the use of novel technologies for orbit raising because they operate in orbits that would ordinarily decay too soon to be economically useful.\n\nA low Earth orbit requires the lowest amount of energy for satellite placement. It provides high bandwidth and low communication latency. Satellites and space stations in LEO are more accessible for crew and servicing. \n\nSince it requires less energy to place a satellite into a LEO, and a satellite there needs less powerful amplifiers for successful transmission, LEO is used for many communication applications, such as the Iridium phone system. Some communication satellites use much higher geostationary orbits, and move at the same angular velocity as the Earth as to appear stationary above one location on the planet.\n\nSatellites in LEO have a small momentary field of view, only able to observe and communicate with a fraction of the Earth at a time, meaning a network (or \"constellation\") of satellites is required to in order to provide continuous coverage. Satellites in lower regions of LEO also suffer from fast orbital decay, requiring either periodic reboosting to maintain a stable orbit, or launching replacement satellites when old ones re-enter.\n\n\nThe LEO environment is becoming congested with space debris because of the frequency of object launches. This has caused growing concern in recent years, since collisions at orbital velocities can easily be dangerous, and even deadly. Collisions can produce even more space debris in the process, creating a domino effect, something known as Kessler Syndrome. The Joint Space Operations Center, part of United States Strategic Command (formerly the United States Space Command), currently tracks more than 8,500 objects larger than 10 cm in LEO. However, a limited Arecibo Observatory study suggested there could be approximately one million objects larger than 2 millimeters, which are too small to be visible from Earth-based observatories.\n\n"}
{"id": "15732860", "url": "https://en.wikipedia.org/wiki?curid=15732860", "title": "Masha (unit)", "text": "Masha (unit)\n\nA masha is a traditional Indian unit of mass, now standardized as .\n\nGrain is usually taken is rice <br>\n8 grains of rice = 1 Ratti <br>\n8 Ratti = 1 Masha <br>\n12 Masha = 1 Tola <br>\n5 Tola = 1 chattank <br>\n16 chittank = 1 sair (kg) <br>\n40 sair (kg) = 1 mann (40kg) <br>\n25 Mann = 1 Ton (1000 KG) <br>\n"}
{"id": "6913647", "url": "https://en.wikipedia.org/wiki?curid=6913647", "title": "Mexal 1500", "text": "Mexal 1500\n\nMexal 1500 is a high explosive compound. It is a mixture of Ammonium Nitrate and Aluminium.\n"}
{"id": "3404079", "url": "https://en.wikipedia.org/wiki?curid=3404079", "title": "N-Reactor", "text": "N-Reactor\n\nThe N-Reactor was a water/graphite-moderated nuclear reactor constructed during the Cold War and operated by the U.S. government at the Hanford Site in Washington; it began production in 1963.\n\nIt was a one-of-a-kind design in the U.S., being both a plutonium production reactor for nuclear weapons and, from 1966, producing steam to allow production of electricity to feed the civilian power grid via the Washington Public Power Supply System or WPPSS.\n\nIn an improvement on the earlier Hanford reactors, N-Reactor was built with a confinement building (although not a containment building). In the event of an accidental release of steam, air and steam would vent through filters that confined any radioactive particles present. It was partially moderated with graphite, but had a negative void coefficient due to also using moderation from the coolant water, meaning it was thermally stable.\n\nThe reactor was shut down in 1987 when the Secretary of Energy determined that no more plutonium was needed and placed on cold standby in 1988, with \"final deactivation\" beginning in 1994 and completing in 1998. Deactivation consisted of shutdown and isolation of operational systems and the cleanup of radiological and hazardous waste. N-Reactor (the 105-N Reactor and the 109-N Heat Exchanger Building) were placed in Interim Safe Storage (ISS) in 2012. ISS consists of the removal of the fuel storage basin, ancillary support facilities, and most portions of the shield wall that surround the 105-N Reactor. In addition, the 109-N Heat Exchanger Building was removed up to the steam generator cells. A new steel roof was installed over the remaining structures. Before and after pictures are found in the facility status change form. Final room status report of the current as-left condition of accessible rooms in the ISS has been documented.\n\n\n"}
{"id": "5416131", "url": "https://en.wikipedia.org/wiki?curid=5416131", "title": "NRANK", "text": "NRANK\n\nNRANK, or National Rank, is a ranking of the rarity of a species within a nation. Each nation can assign their own NRANK, based on information from conservation data centres, natural heritage programmes and expert scientists.\n"}
{"id": "46924779", "url": "https://en.wikipedia.org/wiki?curid=46924779", "title": "Nebojsa Nakicenovic", "text": "Nebojsa Nakicenovic\n\nNebojsa Nakicenovic (also Nebojša Nakićenović) (born 1949, Belgrade, (former) Yugoslavia) is an energy economist.\n\nHe is Deputy Director General/ Deputy CEO of the International Institute for Applied System Analysis (IIASA) in Laxenburg, Austria and former Full Professor of Energy Economics at the Vienna University of Technology, Austria. He is originally from Montenegro and is now citizen of Austria.\n\nHe holds a bachelor's degree (B.A.) in Economics, from Princeton University, Princeton, NJ, United States, Masters (M.A.) and Doctorate Degree (Ph.D) in Economics and Computer Science from the University of Vienna, Austria, and a Doctorate Degree (Ph.D.) Honoris Causa in Engineering Sciences, from the Russian Academy of Sciences, Moscow, Russia. In 2018, Nakicenovic was inducted into the International Academy for Systems and Cybernetic Sciences.\n\nNebojsa Nakicenovic has been involved in all Assessment Reports of the Intergovernmental Panel on Climate Change (IPCC). In 2000, he published, together with Robert Swart, the Special Report on Emissions Scenarios. He was the Director of the Global Energy Assessment (GEA), which was published in 2012. He was Member of the Steering Committee of the Austrian Assessment Report On Climate Change 2014, which was published in September 2014.\n\nHe is a Member of several (international) scientific advisory boards and initiatives, amongst others: the German Advisory Council on Global Change, Earth League, the Steering Committee of Renewable Energy Policy Network for the 21st Century (REN21), the Global Carbon Project, the UN Secretary-General's High-Level Group on Sustainable Energy For All, and the Climate Change Centre Austria.\n\n"}
{"id": "28220", "url": "https://en.wikipedia.org/wiki?curid=28220", "title": "Nicolas Léonard Sadi Carnot", "text": "Nicolas Léonard Sadi Carnot\n\nNicolas Léonard Sadi Carnot (; 1 June 1796 – 24 August 1832) was a French military engineer and physicist, often described as the \"father of thermodynamics\". Like Copernicus, he published only one book, the \"Reflections on the Motive Power of Fire\" (Paris, 1824), in which he expressed, at the age of 27 years, the first successful theory of the maximum efficiency of heat engines. In this work he laid the foundations of an entirely new discipline, thermodynamics. Carnot's work attracted little attention during his lifetime, but it was later used by Rudolf Clausius and Lord Kelvin to formalize the second law of thermodynamics and define the concept of entropy.\n\nNicolas Léonard Sadi Carnot was born in Paris into a family that was distinguished in both science and politics. He was the first son of Lazare Carnot, an eminent mathematician, military engineer and leader of the French Revolutionary Army. Lazare chose his son's third given name (by which he would always be known) after the Persian poet Sadi of Shiraz. Sadi was the elder brother of statesman Hippolyte Carnot and the uncle of Marie François Sadi Carnot, who would serve as President of France from 1887 to 1894.\n\nAt the age of 16, Sadi Carnot became a cadet in the École Polytechnique in Paris, where his classmates included Michel Chasles and Gaspard-Gustave Coriolis. The École Polytechnique was intended to train engineers for military service, but its professors included such eminent scientists as André-Marie Ampère, François Arago, Joseph Louis Gay-Lussac, Louis Jacques Thénard and Siméon Denis Poisson, and the school had become renowned for its mathematical instruction. After graduating in 1814, Sadi became an officer in the French army's corps of engineers. His father Lazare had served as Napoleon's minister of the interior during the \"Hundred Days\", and after Napoleon's final defeat in 1815 Lazare was forced into exile. Sadi's position in the army, under the restored Bourbon monarchy of Louis XVIII, became increasingly difficult.\n\nSadi Carnot was posted to different locations, he inspected fortifications, tracked plans and wrote many reports. It appears his recommendations were ignored and his career was stagnating. On 15 September 1818 he took a six-month leave to prepare for the entrance examination of Royal Corps of Staff and School of Application for the Service of the General Staff.\n\nIn 1819, Sadi transferred to the newly formed General Staff, in Paris. He remained on call for military duty, but from then on he dedicated most of his attention to private intellectual pursuits and received only two-thirds pay. Carnot befriended the scientist Nicolas Clément and attended lectures on physics and chemistry. He became interested in understanding the limitation to improving the performance of steam engines, which led him to the investigations that became his \"Reflections on the Motive Power of Fire\", published in 1824.\n\nCarnot retired from the army in 1828, without a pension. He was interned in a private asylum in 1832 as suffering from \"mania\" and \"general delirum\", and he died of cholera shortly thereafter, aged 36, at the hospital in Ivry-sur-Seine.\n\nWhen Carnot began working on his book, steam engines had achieved widely recognized economic and industrial importance, but there had been no real scientific study of them. Newcomen had invented the first piston-operated steam engine over a century before, in 1712; some 50 years after that, James Watt made his celebrated improvements, which were responsible for greatly increasing the efficiency and practicality of steam engines. Compound engines (engines with more than one stage of expansion) had already been invented, and there was even a crude form of internal-combustion engine, with which Carnot was familiar and which he described in some detail in his book. Although there existed some intuitive understanding of the workings of engines, scientific theory for their operation was almost nonexistent. In 1824 the principle of conservation of energy was still poorly developed and controversial, and an exact formulation of the first law of thermodynamics was still more than a decade away; the mechanical equivalence of heat would not be formulated for another two decades. The prevalent theory of heat was the caloric theory, which regarded heat as a sort of weightless and invisible fluid that flowed when out of equilibrium.\n\nEngineers in Carnot's time had tried, by means such as highly pressurized steam and the use of fluids, to improve the efficiency of engines. In these early stages of engine development, the efficiency of a typical engine—the useful work it was able to do when a given quantity of fuel was burned—was only 3%.\n\nCarnot wanted to answer two questions about the operation of heat engines: \"Is the work available from a heat source potentially unbounded?\" and \"Can heat engines in principle be improved by replacing the steam with some other working fluid or gas?\" He attempted to answer these in a memoir, published as a popular work in 1824 when he was only 28 years old. It was entitled \"Réflexions sur la Puissance Motrice du Feu\" (\"Reflections on the Motive Power of Fire\"). The book was plainly intended to cover a rather wide range of topics about heat engines in a rather popular fashion; equations were kept to a minimum and called for little more than simple algebra and arithmetic, except occasionally in the footnotes, where he indulged in a few arguments involving some calculus. He discussed the relative merits of air and steam as working fluids, the merits of various aspects of steam engine design, and even included some ideas of his own regarding possible improvements of the practical nature. The most important part of the book was devoted to an abstract presentation of an idealized engine that could be used to understand and clarify the fundamental principles that are generally applied to all heat engines, independent of their design.\n\nPerhaps the most important contribution Carnot made to thermodynamics was his abstraction of the essential features of the steam engine, as they were known in his day, into a more general and idealized heat engine. This resulted in a model thermodynamic system upon which exact calculations could be made, and avoided the complications introduced by many of the crude features of the contemporary steam engine. By idealizing the engine, he could arrive at clear and indisputable answers to his original two questions.\n\nHe showed that the efficiency of this idealized engine is a function only of the two temperatures of the reservoirs between which it operates. He did not, however, give the exact form of the function, which was later shown to be (T−T)/T, where T is the absolute temperature of the hotter reservoir. (Note: This equation probably came from Kelvin.) No thermal engine operating any other cycle can be more efficient, given the same operating temperatures.\n\nThe Carnot cycle is the most efficient possible engine, not only because of the (trivial) absence of friction and other incidental wasteful processes; the main reason is that it assumes no conduction of heat between parts of the engine at different temperatures. Carnot knew that the conduction of heat between bodies at different temperatures is a wasteful and irreversible process, which must be eliminated if the heat engine is to achieve maximum efficiency.\n\nRegarding the second point, he also was quite certain that the maximum efficiency attainable did not depend upon the exact nature of the working fluid. He stated this for emphasis as a general proposition:\n\nFor his \"motive power of heat\", we would today say \"the efficiency of a reversible heat engine\", and rather than \"transfer of caloric\" we would say \"the reversible transfer of entropy ∆S\" or \"the reversible transfer of heat at a given temperature Q/T\". He knew intuitively that his engine would have the maximum efficiency, but was unable to state what that efficiency would be.\n\nHe concluded:\n\nand\nIn an idealized model, the caloric transported from a hot to a cold body by a frictionless heat engine that lacks of conductive heat flow, driven by a difference of temperature, yielding work, could also be used to transport the caloric back to the hot body by reversing the motion of the engine consuming the same amount of work, a concept subsequently known as thermodynamic reversibility. Carnot further postulated that no caloric is lost during the operation of his idealized engine. The process being completely reversible, executed by this kind of heat engine is the most efficient possible process. The assumption that heat conduction driven by a temperature difference cannot exist, so that no caloric is lost by the engine, guided him to design the Carnot-cycle to be operated by his idealized engine. The cycle is consequently composed of adiabatic processes where no heat/caloric ∆S = 0 flows and isothermal processes where heat is transferred ∆S > 0 but no temperature difference ∆T = 0 exist. The proof of the existence of a maximum efficiency for heat engines is as follows:\n\nAs the cycle named after him doesn't waste caloric, the reversible engine has to use this cycle. Imagine now two large bodies, a hot and a cold one. He postulates now the existence of a heat machine with a greater efficiency. We couple now two idealized machine but of different efficiencies and connect them to the same hot and the same cold body. The first and less efficient one lets a constant amount of entropy ∆S = Q/T flow from hot to cold during each cycle, yielding an amount of work denoted W. If we use now this work to power the other more efficient machine, it would, using the amount of work W gained during each cycle by the first machine, make an amount of entropy ∆S' > ∆S flow from the cold to the hot body. The net effect is a flow of ∆S' − ∆S ≠ 0 of entropy from the cold to the hot body, while no net work is done. Consequently, the cold body is cooled down and the hot body rises in temperature. As the difference of temperature rises now the yielding of work by the first is greater in the successive cycles and due to the second engine difference in temperature of the two bodies stretches by each cycle even more. In the end this set of machines would be a perpetuum mobile that cannot exist. This proves that the assumption of the existence of a more efficient engine was wrong so that an heat engine that operates the Carnot cycle must be the most efficient one. This means that a frictionless heat engine that lacks of conductive heat flow driven by a difference of temperature shows maximum possible efficiency.\n\nHe concludes further that the choice of the working fluid, its density or the volume occupied by it cannot change this maximum efficiency. Using the equivalence of any working gas used in heat engines he deduced that the difference in the specific heat of a gas measured at constant pressure and at constant volume must be constant for all gases.\nBy comparing the operation of his hypothetical heat engines for two different volumes occupied by the same amount of working gas he correctly deduces the relation between entropy and volume for an isothermal process:\n\nformula_1\n\nCarnot's book received very little attention from his contemporaries. The only reference to it within a few years after its publication was in a review in the periodical \"Revue Encyclopédique\", which was a journal that covered a wide range of topics in literature. The impact of the work had only become apparent once it was modernized by Émile Clapeyron in 1834 and then further elaborated upon by Clausius and Kelvin, who together derived from it the concept of entropy and the second law of thermodynamics.\n\nOn Carnot's religious views, he was a Philosophical theist. As a deist, he believed in divine causality, stating that \"what to an ignorant man is chance, cannot be chance to one better instructed,\" but he did not believe in divine punishment. He criticized established religion, though at the same time spoke in favor of \"the belief in an all-powerful Being, who loves us and watches over us.\"\n\nHe was a reader of Blaise Pascal, Molière and Jean de La Fontaine.\n\nCarnot died during a cholera epidemic in 1832, at the age of 36. \nBecause of the contagious nature of cholera, many of Carnot's belongings and writings were buried together with him after his death. As a consequence, only a handful of his scientific writings survived.\n\nAfter the publication of \"Reflections on the Motive Power of Fire\", the book quickly went out of print and for some time was very difficult to obtain. Kelvin, for one, had a difficult time getting a copy of Carnot's book. In 1890 an English translation of the book was published by R. H. Thurston; this version has been reprinted in recent decades by Dover and by Peter Smith, most recently by Dover in 2005. Some of Carnot's posthumous manuscripts have also been translated into English.\n\nCarnot published his book in the heyday of steam engines. His theory explained why steam engines using superheated steam were better because of the higher temperature of the consequent hot reservoir. Carnot's theories and efforts did not immediately help improve the efficiency of steam engines; his theories only helped to explain why one existing practice was superior to others. It was only towards the end of the nineteenth century that Carnot's ideas, namely that a heat engine can be made more efficient if the temperature of its hot reservoir is increased, were put into practice. Carnot's book did, however, eventually have a real impact on the design of practical engines. Rudolf Diesel, for example, used Carnot's theories to design the diesel engine, in which the temperature of the hot reservoir is much higher than that of a steam engine, resulting in an engine which is more efficient.\n\n\n\nThe text of part of an earlier version of this article was taken from the public domain resource \"A Short Account of the History of Mathematics\" by W. W. Rouse Ball (4th Edition, 1908)\n\n"}
{"id": "25742620", "url": "https://en.wikipedia.org/wiki?curid=25742620", "title": "North Sea Offshore Grid", "text": "North Sea Offshore Grid\n\nThe North Sea Offshore Grid, officially the North Seas Countries Offshore Grid Initiative (NSCOGI), is a collaboration between EU member-states and Norway to create an integrated offshore energy grid which links wind farms and other renewable energy sources across the northern seas of Europe. It is one of several proposed European super grid schemes.\n\nThe North Sea Offshore Grid was proposed by the European Commission in the Second Strategic Energy Review, published in November 2008. The initiative was identified as one of the six priority energy infrastructure actions of the European Union. According to the European Commission, the North Sea Offshore Grid should become one of the building blocks of a future European super grid.\n\nThe political declaration of the North Seas Countries Offshore Grid Initiative was signed on 7 December 2009 at the European Union Energy Council. The declaration was signed by Germany, United Kingdom, France, Denmark, Sweden, the Netherlands, Belgium, Ireland and Luxembourg.\n\nThe European Commission planned to publish a \"Blueprint for a North Sea Grid\" in 2010.\n\nElectricity would be transmitted via high-voltage direct current cables, allowing it to be sold and exchanged in all involved countries. It would also make it easier to optimise energy production, and make the system overall less susceptible to the climate; Norway's hydroelectric power plants could act as a \"giant battery\", storing the power produced and releasing it at peak times, or when wind strength is low. Several high-voltage direct current interconnectors such as proposed cable between Norway and the United Kingdom have been seen as integral parts of the project.\n\nMinister for Communications, Energy and Natural Resources for the Government of Ireland, Eamon Ryan, said of the initiative:\nA techno-economic study into the North Sea Offshore Grid, has been set up within the European Union's Intelligent Energy Europe programme, to consider the technical, economic, policy and regulatory aspects of the possible grid, focused on the North Sea and Baltic region.\n\nFriends of the Supergrid, a group of companies and organisations interested in promoting the concept and influencing the development of a super grid within Europe, has taken an interest in the North Sea Grid proposals. The organisation has proposed that Phase I of the supergrid should integrate the UK's North Sea renewables with interconnections to Germany and Norway.\n\n\n"}
{"id": "32868181", "url": "https://en.wikipedia.org/wiki?curid=32868181", "title": "Ottawa River timber trade", "text": "Ottawa River timber trade\n\nThe Ottawa River timber trade, also known as the Ottawa Valley timber trade or Ottawa River lumber trade, was the nineteenth century production of wood products by Canada on areas of the Ottawa River destined for British and American markets. It was the major industry of the historical colonies of Upper Canada and Lower Canada and it created an entrepreneur known as a lumber baron. The trade in squared timber and later sawed lumber led to population growth and prosperity to communities in the Ottawa Valley, especially the city of Bytown (now Ottawa, the capital of Canada). The product was chiefly red and white pine. The industry lasted until around 1900 as both markets and supplies decreased.\n\nThe industry came about following Napoleon's 1806 Continental Blockade in Europe causing the United Kingdom to require a new source for timber especially for its navy and shipbuilding. Later the U.K.'s application of gradually increasing preferential tariffs increased Canadian imports. The first part of the industry, the trade in squared timber lasted until about the 1850s. The transportation for the raw timber was first by means of floating down the Ottawa River, proved possible in 1806 by Philemon Wright. Squared timber would be assembled into large rafts which held living quarters for men on their six week journey to Quebec City, which had large exporting facilities and easy access to the Atlantic Ocean.\n\nThe second part of the industry involved the trade of sawed lumber, and the American lumber barons and lasted chiefly from about 1850 to 1900-1910. The Reciprocity Treaty caused a shift to American markets. The source of timber in Britain changed, where its access to timber in the Baltic region was restored, and it no longer provided the protective tariffs. Entrepreneurs in the United States at that time then began to build their operations near the Ottawa River, creating some of the world's largest sawmills at the time. These men, known as lumber barons, with names such as John Rudolphus Booth and Henry Franklin Bronson created mills which contributed to the prosperity and growth of Ottawa. The sawed lumber industry benefited from transportation improvements, first the Rideau Canal linking Ottawa with Kingston, Ontario on Lake Ontario, and much later railways that began to be created between Canadian cities.\n\nShortly after 1900, the last raft went down the Ottawa River. Supplies of pine were dwindling and there was also a decreased demand. By this time, the United Kingdom was able to resume its supply from the Baltic Region and their policies especially the reduction in protectionism of their colonies led to a decrease in markets in the U.K. Shipbuilding turned towards steel. Before 1950 many operations began to discontinue, and later many mills were completely removed and the spoiled land began to be restored in Urban Renewal policies in Ottawa. The industry had contributed greatly to population increases and economic growth of Ontario and Quebec.\n\nUpper and Lower Canada's major industry in terms of employment and value of the product was the timber trade. The largest supplier of square red and white pine to the British market originated from the Ottawa River and the Ottawa Valley had \"rich red and white pine forests\" Bytown (later called Ottawa), was a major lumber and sawmill centre of Canada.\n\nIn 1806, Napoleon ordered a blockade to European ports, blocking Britain's access to timber required for the navy from the Baltic Sea. The British naval shipyards were desperately in need of lumber.\n\nBritish tariff concessions fostered the growth of the Canadian timber trade. The British government instituted the tariff on the importation of foreign timber in 1795 in need of alternate sources for its navy and to promote the industry in its North American colonies. The \"Colonial Preference\" was first 10 shillings per load, increasing to 25 in 1805 and after Napoleon's blockade ended, it was increased to 65 in 1814.\n\nIn 1821 the tariff was reduced to 55 shillings and was abolished in 1842. The United Kingdom resumed its trade in Baltic timber. The change in Britain's tariff preferences was a result of Britain moving to Free Trade in 1840. The 1840s saw a gradual move from protectionism in Great Britain\n\nWhen the Ottawa River first began to be used for floating timber en route to markets, squared timber was the preference by the British for resawing, and it \"became the main export\". Britain imported 15,000 loads of timber from Canada in 1805, and from the colonies, 30,000 in 1807, and nearly 300,000 in 1820.\n\nThe reciprocity treaty of 1854 allowed for duty-free export of Ottawa Valley's lumber into the United States. Both the market was changing, as well as the entrepreneurs running the businesses.\n\nAn American September 30, 1869 statement showed that lumber was, by far Canada's biggest export to the U.S. Here are the top 3 (The definition of \"Canada\" for some reason, had Quebec in a separate category):\n\nAlso in 1869, about a third of the lumber manufactured at Ottawa was shipped to foreign countries, and the area employed 6000 men in cutting and rafting logs, about 5,500 in the preparation of squared timber for European markets, and about 5,000 at the mills in Ottawa.\n\nSomewhere between 1848 and 1861, a large increase in the number of sawmills in \"the town\" had occurred:\n\nHere is the production of some companies in 1873, M feet of lumber and number of employees and their 1875 address listed, where available.\n\nThe 1875 lumber merchants list had Jos Aumond, Batson & Carrier, Bennett, Benson & Co., H. B. D. Bruce, T. C. Brougham, T. W. Currier & Co., G. B. Hall, Hamilton & Bros., J. T. Lambert, Moses W. Linton, M. McDougall, John Moir, Isaac Moore, Robert Nagle, R. Ryan, Albert W. Soper, Wm. Stubbs and\nWm. Mackey, 99 Daly, Robert Skead, 288 Sparks, Hon. James Skead, 262 Wellington, William Skead, 10 Bell, Joseph Smith, 286 Sussex\n\nUpper and Lower Canada's major industry in terms of employment and value of the product was the timber trade. Bytown (later called Ottawa), was a major lumber and sawmill centre of Canada. When the Ottawa River first began to be used for floating timber en route to markets, squared timber was the preference. This required the logs to be skillfully shaped with broadaxes giving the whole log a squared appearance. It was wasteful but squared pine was preferred by the British for resawing. The timber was bound with other sticks into two related configurations, cribs, and rafts. (See the following section for a detailed explanation.) Squared timber \"became the main export\" and was easy to ship overseas and could be moved by \"pegged cribs\". The rafts were floated on the Ottawa River to markets in Quebec.\n\nIn the early days the raftsmen were mostly French Canadian. The 1830s saw a large number of immigrants from Ireland other British Isles, and English speaking raftsmen began to appear. Competition for jobs led to animosity and hatred. Many Irish had come to Canada after the Rideau Canal's construction to escape the poverty in Ireland. An unruly group called the Shiners began to develop; jobless, alcohol-consuming and living in houses along the canal.\n\nThe first lumbering on the south side of the Kim River near Ottawa was by Braddish Billings, a former employee of Philemon Wright, with William M?? where they cut timber in Gloucester Township in 1810.\nThe industry began in Bytown with St. Louis, who in 1830 used the bywash (a section, that no longer exists, of the early Rideau Canal which drained into the Rideau River) near Cumberland and York. In 2001 he moved to Rideau Falls. Thomas McKay acquired the mill in 1837.\n\nIn 1843, Philip Thompson and Daniel McLachlin harnessed the Chaudière Falls for use with grist and sawmills. In 1852, the Chaudière saw A.H. Baldwin, John Rudolphus Booth, Henry Franklin Bronson and Weston, J.J. Harris, Pattee and Perley, John Rochester, Levi Young. All were American except for Rochester. J.?. Turgeon operated a sawmill in the canal basin (another no longer existing area of the canal used for turning watercraft, just south of the bridge by the entrance).\n\nSometime in the 1850s the islands at the Chaudière Falls became occupied with the express intent of harvesting the enormous power of the falls. An auction on September 1, 1852 had lots on Victoria Island and Amelia island going to \"Harris, Bronson and Co., and Perley and Pattee, both lumber operators in the Lake Champlain / Lake George area\". Levi Young was on the mainland. \"Harris and Bronson\" mills had a capacity of 100,000 logs annually, more than twice that of nearby mills of Blasdell, Currier and Co., and Philip Thompson.\n\nThe Ottawa River was the means of transporting logs to Quebec, using timber rafting. Sticks were trapped by a boom \"at the mouth of the tributary\" to be assembled into cribs, each crib consisting of 30 or more sticks of timber. Then the cribs, up to 100 of them, were joined together into a raft that served as the \"riverman's home for the month-long journey downriver to Quebec. The crew lived in bunk houses right on the raft, and one of the cribs contained the cookery.\n\nThere were two principal types of assemblages of logs, a dram and a crib. The crib was usually used on the Ottawa River wheres the dram was used on Lake Ontario and the St. Lawrence. A crib consisted of two layers of logs where were about twenty-four feet wide at most, as they were designed to get along the rapids at the Chaudière Falls and Des Chats, whereas drams could be more than a hundred feet wide.\n\nRafts destined for Quebec have 2000 to 3000 pieces, almost all of them pine. The rafts are made up in cribs; each crib has 25 pieces.\n\nRafts were powered by oars, or occasionally sails. Rafts had to be dismantled and reassembled to get past rapids and obstructions. At Chaudière Falls 20 days could be lost in hauling the timber overland. Timber slides were an idea to solve that problem.\n\nThe first timber slide on the Ottawa River was built on the North Side near the Chaudière Falls by Ruggles Wright, son of Philemon following a visit to Scandinavia to learn of lumbering techniques there. The slide was 26 feet wide and was used to bypass the falls. Prior to this, bypassing the falls was a difficult task, and at times met with fatalities. His first slide was built in 1829 and during the next few years, other locations on the river began to employ them.\n\nThe trip to the timber shipping yards in Quebec, headquarters of many lumber exporting firms, often took as long as six weeks.\n\nPointer boat is a boat commissioned by Booth to move white pine down the Ottawa River built by John Cockburn first in Ottawa who then moved to Pembroke, whose marina now holds its monument.\n\nPhilemon Wright, the founder of Wright's Town, which became Gatineau, Quebec built the first timber raft, called Columbo, to go down the Ottawa River on June 11, 1806, taking 35 days to get to Montreal alone. It was manned by Philemon, his 18 year-old son Tiberius and three crewmen - London Oxford, Martin Ebert and John Tarnower - and they ended up at the Port of Quebec. The raft had to be broken up into cribs to clear the Long-Sault Rapids (the original Anishinaabe name was Kinodjiwan - meaning long-rapids - invisible since the river was dammed at the Carillon Generating Station). The first timber slide on the Ottawa River was built by Philemon's son, Ruggles Wright, on the North Side near the Chaudière Falls following a visit to Scandinavia to learn of lumbering techniques there.. Philemon had an employee, Nicholas Sparks (politician) - a lumberman in his own right - who owned the land that would eventually form the heart of Bytown (Ottawa's first name) and whose name was given to Sparks Street. \n\nHenry Franklin Bronson (1817-1889), was an American who became one of the earliest major lumber barons, working on the Chaudière in the 1850s Bronson with his partner, John Harris in 1852 bought some land on Victoria Island, and the rights to use the water for industry. Harris and Bronson set up a large plant incorporating some modern features, which ushered in other entrepreneurs in an \"American Invasion\" to follow. Bronson had a son Erskine Henry Bronson who later assumed control of his father's business.\n\nJohn Rudolphus Booth(1827-1925), a Canadian became one of the largest lumber barons, and one of Canada's most successful entrepreneurs; he also worked at the Chaudière. He had once helped build Andrew Leamy's sawmill in Hull, and later began producing shingles near the Chaudière Falls in a rented sawmill. He later built his own sawmill, was the lumber supplier for the Parliament buildings, and his name became widely known. With profits, he financed a large sawmill at the falls. In 1865, he was the location's third largest producer and twenty-five years later he had the highest daily output in the world.\n\nWilliam Goodhue Perley (1820-1890) was a part, in 1852 on the Chaudière of Perley and Pattee, both Americans. His partner, William Goodhue Perley (1820-1890) had a son, George Halsey Perley (1857-1938) who was also in the business. David Pattee (1778-1851), although he seems to have in common the sawmills, and some connections to Ottawa, was probably not part of this firm.\n\nThere were several companies and individuals who created some timber operations, before the huge American influx. There were two waves of American lumberers. In 1853, Baldwin, Bronson, Harris, Leamy and Young began to erect lumber mills, and from 1856 to 1860, Perley, Pattee, Booth and Eddy followed. \n\nAllan Gilmour, Sr. (1775-1849) was part of a Scottish merchant family whose lumber interests began in Canada in New Brunswick, then Montreal and then Bytown in 1841. In 1840, after his Montreal boss retired, Allan and his cousin James from Scotland took over the lumber business He dealt in square timber, and built mills on the Gatineau River, the South Nation River east of Ottawa, the Blanche River near Pembroke, and a mill in Trenton, Ontario. The firm employed over 1000 in the winter time. Their mills used more modern features in sawing and lifting, and turning logs over. Allan Gilmour was associated with the firm Pollok, Gilmour and Company.\n\nThomas McKay (1792-1855), sometimes considered as one of the founding fathers of Ottawa for his work in building, as well as politics, built a sawmill at New Edinburgh. He was also known for building Rideau Hall, locks of the Rideau Canal, and the Bytown Museum. McKay also was on the Legislative Council of the Province of Canada.\n\nJames Maclaren (1818-1892) who once established industry Wakefield, Quebec, in 1853, he leased a sawmill in New Edinburgh from Thomas McKay with partners and in 1861, he bought out his partners and, in 1866, he purchased the mills after McKay's death. In 1864, again with partners, he bought sawmills at Buckingham, Quebec, later buying out his partners.\n\nOther import names include James Skead (1817-1844), John Rochester (1822-1894), Daniel McLachlin (1810-1872) and John Egan (1811-1857).\n\nA few perhaps less famous people in the industry, but have made contributions in other areas, mostly politics, are William Borthwick (1848-1928) and James Davidson (1856-1913), Andrew Leamy (1816-1868), William Stewart (1803-1856), William Hamilton, George Hamilton (1781-1839).\n\nThe industry contributed to the population growth in Ontario and Quebec both indirectly, as a result of its economic boost, as well as directly, when ships from Quebec City went to ports such as Liverpool and returned with hopeful immigrants, providing cheap transportation. It also stimulated economic growth in both provinces, and J.R. Booth contributed greatly to the construction of the Canada Atlantic Railway.\n\nThere also was an environmental impact. The huge industrial operations at LeBreton Flats and the Chaudiere Falls caused pollution and damage to the lands. The beauty of the Chaudiere Falls had been completely changed by industry. The National Capital Commission removed a lot of the industrial structures in Ottawa and Hull in the 1960s. LeBreton, for various reasons, remained unoccupied for decades.\n\nLeBreton Flats and the Chaudière Falls were the locations of some of Canada's largest lumber mills, including those of Booth and Bronson. All of that is now gone now as part of the Greber Plan's efforts at beautifying the capital of Canada.\n\nBronson Avenue was named after the lumber baron. The Bank of Ottawa was founded due to the industry. The ByWard Market came about as part of Lower Town to serve the needs of Bytown's lumbery population. Booth House still exists.\n\nOttawa Central Railway still carries lumber as one of its major commodities.\n\nHog's Back Falls were as John MacTaggart, in 1827, described them as \"a noted ridge of rocks, called the Hog's Back, from the circumstances of raftsmen with their wares [timber rafts] sticking on it in coming down the stream.\"\n\nList of designated heritage properties in Ottawa lists the Carkner Lumber Mill in Osgoode, Watson's Mill in Rideau-Goldbourne.\n\nThe Ottawa Valley is a large swath of land, much of it along the Ottawa River. Renfrew, Ontario is often associated with the name. The Ottawa-Bonnechere Graben is a geologically related area.\n\nUpper Canada was a name given to areas in present day Ontario in the 18th and 19th centuries, until 1840 or 1841, when the Province of Canada formed. In 1867, this also no longer existed with the Confederation of Canada when Ontario and Quebec became officially named, and became two of the four provinces of Canada.\n\nEastern Ontario's Irish Catholics mainly from Cork along with the Franco-Ontarians made up the majority of Rideau Canal builders and were heavily employed in the area's extensive lumber industry.\n\nGatineau was called Columbia Falls Village by Philemon Wright, Wright's Town (or Wrightstown) by most and Wright's Village by some during Philemon Wright's life . It later became Hull, Quebec in 1875 and then Gatineau, Quebec in 2002.\n\nBuckingham, Quebec contained the mills of the J. MacLaren & Co. by James Maclaren.\n\nFassett, Quebec along with Notre-Dame-de-Bonsecours, Quebec became of interest economically for its oaks, pines, and maples, during the Napoleonic blockade. Its large oaks are of \"high quality and particularly of large size, suitable for the construction of vessels.\"\n\nAreas affected by the lumber industry on the Ottawa River include Arnprior, Hawkesbury, Ontario, Stittsville, Ontario, North Gower, Ontario, Kemptville, Ontario, Carleton Place, Ontario, Pembroke, Ontario, and Lachute.\n\nHighlands East, Ontario Gooderham (not on the Ottawa River) southwest of Ottawa still has an active mill.\n\n\n\n\n"}
{"id": "16521792", "url": "https://en.wikipedia.org/wiki?curid=16521792", "title": "Particle size analysis", "text": "Particle size analysis\n\nParticle size analysis, particle size measurement, or simply particle sizing is the collective name of the technical procedures, or laboratory techniques which determines the size range, and/or the average, or mean size of the particles in a powder or liquid sample.\n\nParticle size analysis is part of particle science, and its determination is carried out generally in particle technology laboratories.\n\nThe particle size measurement is typically achieved by means of devices called Particle Size Analyzers (PSA) which are based on different technologies, such as high definition image processing, analysis of Brownian motion, gravitational settling of the particle and light scattering (Rayleigh and Mie scattering) of the particles.\n\nThe particle size can have considerable importance in a number of industries including the chemical,food, mining, forestry, agriculture, nutrition, pharmaceutical, energy, and aggregate industries.\n\nThere are a large number of methods for the determination of particle size, and it is important to state at the outset, that these different methods are not expected to give identical results: the size of a particle depends on the method used for its measurement, and it is important to choose that method for its determination which is relevant to its use.\n\nParticle size is quickly measured using the fineness of grind gauge. The gauge consists of a steel block with a series of very small parallel grooves machined into it. The grooves decrease in depth from one end of the block to the other, according to a scale stamped next to them. A typical Hegman gauge is 170mm by 65mm by 15mm, with a channel of grooves running lengthwise, 12.5mm across and narrowing uniformly in depth from 100 μm to zero.[1]\n\nA Hegman gauge is used by puddling a sample of paint at the deep end of the gauge and drawing the paint down with a flat edge along the grooves. The paint fills the grooves, and the location where a regular, significant \"pepperyness\" in the appearance of the coating appears, marks the coarsest-ground dispersed particles.[1] The reading is taken from the scale marked next to the grooves, in dimensionless \"Hegman units\" and/or mils or micrometres.[2] \n\nThe Hegman gauge is used in the coatings industry and measures from 100 μm to zero. THE NPIRI gauge is used for ink's finer particles and measures from 25 μm to zero\n\nThe size of materials being processed in an operation is very important. Having oversize material being conveyed will cause damage to equipment and slow down production. Particle-size analysis also helps the effectiveness of SAG Mills when crushing material.\n\nThe Fisher sub-sieve sizer is still used.\n\nThe gradation of soils affects water and nutrient holding and drainage capabilities. For sand-based soils, particle size can be the dominant characteristic affecting soil performances and hence crop\n\nParticle-size analysis in the agriculture industry is paramount because unwanted materials will contaminate products if they are not detected. By having an automated particle size analyzer, companies can closely monitor their processes.\n\nWood particles used to make various types of products rely on particle-size analysis to maintain high quality standards. By doing so, companies reduce waste and become more productive.\n\nHaving properly sized particles allow aggregate companies to create long-lasting roads and other products.\n\nParticle size analyzers are used also in biology to measure protein aggregation.\n\n"}
{"id": "32698123", "url": "https://en.wikipedia.org/wiki?curid=32698123", "title": "Polarizing organic photovoltaics", "text": "Polarizing organic photovoltaics\n\nPolarizing organic photovoltaics (ZOPV) is a concept for harvesting energy from Liquid crystal display screens, developed by engineers from UCLA. This concept enables devices to utilize external light and the LCD screen's backlight using photovoltaic polarizers. Photovoltaic polarizers convert this light into electricity which can be used to power the device. This concept also provides multifunctional capability to devices with LCD screens as they act as photovoltaic devices and also as polarisers.\n\nA liquid crystal display (LCD) is a flat panel display, electronic visual display, video display that uses the light modulating properties of liquid crystals (LCs). LCs do not emit light directly. They are used in a wide range of applications, including computer monitors, television, instrument panels, laptops tablet computers etc. They are common in consumer devices such as video players, gaming devices, clocks, watches, calculators, and telephones.\n\nUp to three-fourths of the light energy wasted from LCD backlight illumination can be retrieved and utilized using polarizing organic photovoltaics. They can utilize external light energy also apart from backlight illumination using photovoltaic polarizers, which are present within the structure of the LCD screen.\n\n80% to 90% of the total energy utilized by any device with an LCD screen is used up by the backlight illumination. As polarizing organic photovoltaics can recycle up to 75% of wasted light energy, the efficiency of the device is increased.\n\nThis simply incorporates additional conversion efficiency losses. These devices harvest their own light. The article cited above, \"Photovoltaics Could Charge A Phone Using Its Own Backlight\" is bogus and makes claims that would violate the 1st and 2nd laws of thermodynamics if true. Such a device thus could not be patented and commercialized. See also: Perpetual Motion Machine\n\n\n"}
{"id": "628748", "url": "https://en.wikipedia.org/wiki?curid=628748", "title": "RenewableUK", "text": "RenewableUK\n\nRenewableUK, formerly known as the 'British Wind Energy Association' (BWEA), is the trade association for wind power, wave power and tidal power industries in the United Kingdom. RenewableUK has over 660 corporate members, from wind, wave and tidal stream power generation and associated industries.\n\nThe association carries out research, and co-ordinates statistics and intelligence on marine and wind power in the UK and its waters. It also represents its members internationally, and to Government, regional bodies and local authorities in the UK.\n\nA number of universities active in wind energy in the 1970s met under umbrella of the ITDG Wind Panel (Intermediate Technology Development Group). The BWEA was formed from the ITDG Wind Panel along with other interested parties and representatives from industry, to promote wind power in the United Kingdom. The inaugural meeting of the BWEA took place on 17 November 1978 at the Rutherford Laboratory with Peter Musgrove of Reading University as chairman.\n\nIn 2004 the British Wind Energy Association expanded its remit to include wave and tidal energy, and to use the Association's experience to guide these technologies along the same path to commercialisation. In December 2009, to reflect this expansion in the industries it represented, members resolved to adopt the new name of RenewableUK.\n\nIn 2008 the then BWEA was found by the Advertising Standards Authority (ASA) to have overstated the carbon dioxide emissions displaced by wind generation. The BWEA and its members had claimed a value of 860 grams per kilowatt hour but the ASA found this was exaggerated by 100% and ordered the BWEA to reduce the figure to 430 grams.\n\nThe members of RenewableUK are represented in Wales by RenewableUK Cymru, the Cardiff-based branch of the organisation. The remit in Wales expanded in February 2014 to include all renewable energy technologies and energy storage.\n\n\n"}
{"id": "2133664", "url": "https://en.wikipedia.org/wiki?curid=2133664", "title": "Rogun Dam", "text": "Rogun Dam\n\nRogun Dam is an embankment dam under construction on the Vakhsh River in southern Tajikistan. The dam is situated 110 Km from Dushanbe. It is one of the planned hydroelectric power plants of Vakhsh Cascade. \n\nConstruction of the dam began in the Soviet era, in 1976, but was abandoned after the collapse of the Soviet Union. Over three decades only preliminary construction has been carried out on the dam. Due to its controversial state, construction was suspended in August 2012 pending World Bank reports. The project was restarted by the Tajik government with Chinese help in 2017. The power plant's first unit was commissioned in November 2018.\n\nThe dam has drawn complaints from neighboring Uzbekistan, which fears it will negatively impact its lucrative cotton crops. The dispute over the project has contributed significantly to bitter relations between the two former Soviet republics.\n\nThe Rogun Dam was first proposed in 1959 and a technical scheme was developed by 1965. Construction began in 1976, however the project stalled after the collapse of the Soviet Union. An agreement on finishing the construction was signed between Tajikistan and Russia in 1994. Since the agreement was not implemented, it was denounced by Tajikistan parliament. In October 2004, an agreement was signed with RUSAL in which RUSAL agreed to complete the Rogun facility, to build a new aluminum plant and to rebuild the Tursunzade Aluminum Smelter. In February 2007, a new partnership between Russia and Tajikistan to complete the dam was announced, but later was refused by Russia because of disagreement concerning the controlling stake in the project. In May 2008, Tajikistan announced that construction of the dam had resumed. By December 2010, one of the river diversion tunnels was renovated and the second expected to commence in June or July 2011. Construction of the dam was suspended in August 2012 pending the World Bank assessment.\n\nIn 2010, Tajikistan launched an IPO to raise US$1.4billion to finish construction of the dam. By April 26 of that year the Tajik government had raised just US$184 million, enough for two years of construction.\nOn July 1, 2016 the state commission in charge of the project had chosen the Italian company Salini Impregilo to carry out the construction for $3.9 billion. The project is broken down into four components, with the most expensive one involving the building of a 335-meter-high rockfill dam which will entail costs of around $1.95 billion.\nOn October 29, 2016 Tajik president Emomali Rahmon officially launched the construction of the dam. At the ceremony, the river's flow was ceremonially diverted through the reconstructed diversion tunnels. The construction of the dam is expected to take two years. The power plant's first unit was commissioned in November 2018 and second turbine is expected to be comissioned by 2019.\nRogun was listed as the highest dam in the world — high — but this is a projected height. In reality the dam was only circa \n\nhigh until 1993 when it was destroyed in a flood. three projects are under consideration: the original, , and two alternatives, and , all having their advantages and drawbacks.\n\nThe hydroelectric power plant is expected to have six turbines with total capacity of 3,600MW. When complete, it is expected to produce 17.1TWh of electrical power per year.\n\nIn response to the request of the bordering countries and especially Uzbekistan, the World Bank has financed the Techno-Economic Assessment Study (TEAS) conducted by consortium of Coyne et Bellier, Electroconsult and IPA Energy + Water Economics, and Environmental and Social Impact Assessment (ESIA) conducted by Pöyry. The reports, originally slated to be released in February 2012, were delayed until mid-2014. The ESIA was published on 16 June 2014 and the TEAS in July 2014. Overall, the ESIA stated that \"Most impacts are rather small and easily mitigated, if mitigation is required at \nall.\" and that \"There is no impact of the category \"strong negative, mitigation not possible\", \nwhich would have to be considered as a no-go for the project.\" All parties, including Central Asian states met in Almaty in July 2014 for the 5th Riparian Meeting to discuss findings within the TEAS and ESIA.\n\nThe project has raised tensions with Uzbekistan over the impact of the dam on its cotton irrigation systems. In February 2010, Uzbek Prime Minister Shavkat Mirziyoyev sent a letter to his Tajik counterpart demanding an independent examination of the possible consequences of the dam. During October 2010, Uzbek President Islam Karimov called the Rogun hydropower plants a \"stupid project.\"\n"}
{"id": "4457346", "url": "https://en.wikipedia.org/wiki?curid=4457346", "title": "S6 NBC Respirator", "text": "S6 NBC Respirator\n\nThe S6 NBC Respirator was a protective gas mask issued to the British Armed Forces. It was developed in the 1950s and first issued for general service from 1966 to 1986, when the S10 was pressed into service. Currently, the S6 is not used by the British military.\n\nIt's usually worn in conjunction with the NBC No. 1 Mk III CBRN protective suit.\n\nThe S6 was developed by the Defence Science and Technology Laboratory, Porton Down from the 1950s and manufactured by Avon Rubber in Melksham, Wiltshire and the Birmingham & Leyland Rubber Company. Although made obsolete by the introduction of the S10, production of the S6 continued in Turkey after the adoption of the new mask by British forces. During the widely documented Iranian Embassy Siege of 1980, the S6 is the mask seen being worn by SAS troops.\n\nThe Turkish military adopted its own version of the S6 in 1990 with the name SR10 under license by MKEK. SR10 and its newer version the SR10-ST with a D12 drinking tube attached.\n\nThe S6 respirator provided protection to the face, eyes, lung and throat of the wearer against all known chemical agents of that time. The filter of the respirator did not filter out carbon dioxide. The S6 features an innovative air seal around the inside of the face piece to improve the fit and comfort of the mask; a tap inside the nose cup was to allow the equalisation of pressure inside the air seal, in different climatic conditions. \n\nThe wearer's natural body heat would then warm the trapped air to inflate the air seal, thus getting a better seal around the face. The mask could also be made to benefit left-handed people, where the filter canister is positioned on the right of the face mask instead of the left. The location of the filter on the left or right side of the mask was to allow the rifle to be shouldered, aimed and fired correctly.\n\n\n"}
{"id": "11607100", "url": "https://en.wikipedia.org/wiki?curid=11607100", "title": "Steam separator", "text": "Steam separator\n\nA steam separator, sometimes referred to as a moisture separator, is a device for separating water droplets from steam. The simplest type of steam separator is the steam dome on a steam locomotive. Stationary boilers and nuclear reactors may have more complex devices which impart a \"spin\" to the steam so that water droplets are thrown outwards by centrifugal force and collected. All separators require steam traps to collect the water droplets that they remove.\n\nIt is important to remove water droplets from steam because:\n\n\n\n"}
{"id": "760571", "url": "https://en.wikipedia.org/wiki?curid=760571", "title": "Stele", "text": "Stele\n\nA stele ( ) is a stone or wooden slab, generally taller than it is wide, erected in the ancient world as a monument. Grave stelae were often used for funerary or commemorative purposes. Stelae as slabs of stone would also be used as ancient Greek and Roman government notices or as boundary markers to mark borders or property lines.\n\nThe surface of the stele usually has text, ornamentation, or both. The ornamentation may be inscribed, carved in relief, or painted.\n\nSteles are occasionally erected as memorials to battles. For example, along with other memorials, there are more than half-a-dozen steles erected on the battlefield of Waterloo at the locations of notable actions by participants in battle.\n\nTraditional Western gravestones may technically be considered the modern equivalent of ancient stelae, though the term is very rarely applied in this way. Equally, stele-like forms in non-Western cultures may be called by other terms, and the words \"stele\" and \"stelae\" are most consistently applied in archaeological contexts to objects from Europe, the ancient Near East and Egypt, China, and sometimes Pre-Columbian America.\n\nSteles have also been used to publish laws and decrees, to record a ruler's exploits and honors, to mark sacred territories or mortgaged properties, as territorial markers, as the boundary steles of Akhenaton at Amarna, or to commemorate military victories. They were widely used in the ancient Near East, Mesopotamia, Greece, Egypt, Somalia, Eritrea, Ethiopia, and, most likely independently, in China and elsewhere in the Far East, and, independently, by Mesoamerican civilisations, notably the Olmec and Maya.\n\nThe large number of steles, including inscriptions, surviving from ancient Egypt and in Central America constitute one of the largest and most significant sources of information on those civilisations, in particular Maya stelae. The most famous example of an inscribed stela leading to increased understanding is the Rosetta Stone, which led to the breakthrough allowing Egyptian hieroglyphs to be read. An informative stele of Tiglath-Pileser III is preserved in the British Museum. Two steles built into the walls of a church are major documents relating to the Etruscan language.\n\nStanding stones (menhirs), set up without inscriptions from Libya in North Africa to Scotland, were monuments of pre-literate Megalithic cultures in the Late Stone Age. The Pictish stones of Scotland, often intricately carved, date from between the 6th and 9th centuries.\n\nAn obelisk is a specialized kind of stele. The Insular high crosses of Ireland and Britain are \"specialized steles\". Totem poles of North and South America that are made out of stone may also be considered a specialized type of stele. Gravestones, typically with inscribed name and often with inscribed epitaph, are among the most common types of stele seen in Western culture.\n\nMost recently, in the \"Memorial to the Murdered Jews of Europe\" in Berlin, the architect Peter Eisenman created a field of some 2,700 blank steles. The memorial is meant to be read not only as the field, but also as an erasure of data that refer to memory of the Holocaust.\n\nMany steles have been used since the First Dynasty of Egypt. These vertical slabs of stone depict tombstones, religious usage, and boundaries.\n\nUrartian steles were freestanding stone obelisks that served a variety of purposes, sometimes they were located within temple complexes, or set within monumental rock-cut niches (such as the niche of the Rock of Van, discovered by Marr and Orbeli in 1916) or erected beside tombs. Others stood in isolated positions and, such as the Kelashin Stele, had a commemorative function or served as boundary markers. Although sometimes plain, most bore a cuneiform inscription that would detail the stele's function or the reasons for its erection. The steel from Van's \"western niche\" contained annals of the reign of Sarduri II, with events detailed yearly and with each year separated by the phrase \"For the God Haldi I accomplished these deeds\". Urartian steles are sometimes found reused as Christian Armenian gravestones or as spolia in Armenian churches - Maranci suggests this reuse was a deliberate desire to capitalize on the potency of the past. Some scholars have suggested Urartian steles may have influenced the development of the Armenian khachkar.\n\nGreek funerary markers, especially in Attica, had a long and evolutionary history in Athens. From public and extravagant processional funerals to different types of pottery used to store ashes after cremation, visibility has always been a large part of Ancient Greek funerary markers in Athens. Regarding stelai (Greek plural of stele), in the period of the Archaic style in Ancient Athens (600 BCE) stele often showed certain archetypes of figures, such as the male athlete. Generally their figures were singular, though there are instances of two or more figures from this time period. Moving into the 6th and 5th centuries BCE, Greek stelai declined and then rose in popularity again in Athens and evolved to show scenes with multiple figures, often of a family unit or a household scene. One such notable example is the Stele of Hegeso. Typically grave stelai are made of marble and carved in relief, and like most Ancient Greek sculpture they were vibrantly painted. For more examples of stelai, the Getty Museum's published Catalog of Greek Funerary Sculpture is a valuable resource\n\nSteles (Chinese: \"bēi\" 碑) have been the major medium of stone inscription in China since the Tang dynasty. Chinese steles are generally rectangular stone tablets upon which Chinese characters are carved intaglio with a funerary, commemorative, or edifying text. They can commemorate talented writers and officials, inscribe poems, portraits, or maps, and frequently contain the calligraphy of famous historical figures. In additional to their commemorative value, many Chinese steles are regarded as exemplars of traditional Chinese calligraphic scripts, especially the clerical script.\n\nChinese steles from before the Tang dynasty are rare: there are a handful from before the Qin dynasty, roughly a dozen from the Western Han, 160 from the Eastern Han, and several hundred from the Wei, Jin, Northern and Southern, and Sui dynasties. During the Han dynasty, tomb inscriptions (, \"mùzhì\") containing biographical information on deceased people began to be written on stone tablets rather than wooden ones.\n\nErecting steles at tombs or temples eventually became a widespread social and religious phenomenon. Emperors found it necessary to promulgate laws, regulating the use of funerary steles by the population. The Ming dynasty laws, instituted in the 14th century by its founder the Hongwu Emperor, listed a number of stele types available as status symbols to various ranks of the nobility and officialdom: the top noblemen and mandarins were eligible for steles installed on top of a stone tortoise and crowned with hornless dragons, while the lower-level officials had to be satisfied with steles with plain rounded tops, standing on simple rectangular pedestals.\n\nSteles are found at nearly every significant mountain and historical site in China. The First Emperor made five tours of his domain in the 3rd century BC and had Li Si make seven stone inscriptions commemorating and praising his work, of which fragments of two survive. One of the most famous mountain steles is the high stele at Mount Tai with the personal calligraphy of Emperor Xuanzong of Tang commemorating his imperial sacrifices there in 725.\n\nA number of such stone monuments have preserved the origin and history of China's minority religious communities. The 8th-century Christians of Xi'an left behind the Nestorian Stele, which survived adverse events of the later history by being buried underground for several centuries. Steles created by the Kaifeng Jews in 1489, 1512, and 1663, have survived the repeated flooding of the Yellow River that destroyed their synagogue several times, to tell us something about their world. China's Muslim have a number of steles of considerable antiquity as well, often containing both Chinese and Arabic text.\n\nThousands of steles, surplus to the original requirements, and no longer associated with the person they were erected for or to, have been assembled in Xi'an's Stele Forest Museum, which is a popular tourist attraction. Elsewhere, many unwanted steles can also be found in selected places in Beijing, such as Dong Yue Miao, the Five Pagoda Temple, and the Bell Tower, again assembled to attract tourists and also as a means of solving the problem faced by local authorities of what to do with them. The long, wordy, and detailed inscriptions on these steles are almost impossible to read for most are lightly engraved on white marble in characters only an inch or so in size, thus being difficult to see since the slabs are often 3m or more tall.\n\nThere are more than 100,000 surviving stone inscriptions in China. However, only approximately 30,000 have been transcribed or had rubbings made, and fewer than those 30,000 have been formally studied.\n\nMaya stelae were fashioned by the Maya civilization of ancient Mesoamerica. They consist of tall sculpted stone shafts or slabs and are often associated with low circular stones referred to as altars, although their actual function is uncertain. Many stelae were sculpted in low relief, although plain monuments are found throughout the Maya region. The sculpting of these monuments spread throughout the Maya area during the Classic Period (250–900 AD), and these pairings of sculpted stelae and circular altars are considered a hallmark of Classic Maya civilization. The earliest dated stela to have been found \"in situ\" in the Maya lowlands was recovered from the great city of Tikal in Guatemala. During the Classic Period almost every Maya kingdom in the southern lowlands raised stelae in its ceremonial centre.\n\nStelae became closely associated with the concept of divine kingship and declined at the same time as this institution. The production of stelae by the Maya had its origin around 400 BC and continued through to the end of the Classic Period, around 900, although some monuments were reused in the Postclassic (c. 900–1521). The major city of Calakmul in Mexico raised the greatest number of stelae known from any Maya city, at least 166, although they are very poorly preserved.\n\nHundreds of stelae have been recorded in the Maya region, displaying a wide stylistic variation. Many are upright slabs of limestone sculpted on one or more faces, with available surfaces sculpted with figures carved in relief and with hieroglyphic text. Stelae in a few sites display a much more three-dimensional appearance where locally available stone permits, such as at Copán and Toniná. Plain stelae do not appear to have been painted nor overlaid with stucco decoration, but most Maya stelae were probably brightly painted in red, yellow, black, blue and other colours.\n\nOgham stones are vertical grave and boundary markers, erected at hundreds of sites in Ireland throughout the first millennium AD, bearing inscriptions in the Primitive Irish language. They have been occasionally been described as \"steles.\"\n\nThe Horn of Africa contains many stelae. In the highlands of Ethiopia and Eritrea, the Axumites erected a number of large stelae, which served a religious purpose in pre-Christian times. One of these granite columns is the largest such structure in the world, standing at 90 feet.\n\nAdditionally, Tiya is one of nine megalithic pillar sites in the central Gurage Zone of Ethiopia. As of 1997, 118 stele were reported in the area. Along with the stelae in the Hadiya Zone, the structures are identified by local residents as \"Yegragn Dingay\" or \"Gran's stone\", in reference to Imam Ahmad ibn Ibrahim al-Ghazi (Ahmad \"Gurey\" or \"Gran\"), ruler of the Adal Sultanate.\n\nThe stelae at Tiya and other areas in central Ethiopia are similar to those on the route between Djibouti City and Loyada in Djibouti. In the latter area, there are a number of anthropomorphic and phallic stelae, which are associated with graves of rectangular shape flanked by vertical slabs. The Djibouti-Loyada stelae are of uncertain age, and some of them are adorned with a T-shaped symbol.\n\nNear the ancient northwestern town of Amud in Somalia, whenever an old site had the prefix \"Aw\" in its name (such as the ruins of \"Aw Bare\" and \"Aw Bube\"), it denoted the final resting place of a local saint. Surveys by A.T. Curle in 1934 on several of these important ruined cities recovered various artefacts, such as pottery and coins, which point to a medieval period of activity at the tail end of the Adal Sultanate's reign. Among these settlements, Aw Barkhadle is surrounded by a number of ancient stelae. Burial sites near Burao likewise feature old stelae.\n\n\n\n\n"}
{"id": "214934", "url": "https://en.wikipedia.org/wiki?curid=214934", "title": "Stonemasonry", "text": "Stonemasonry\n\nThe craft of stonemasonry (or stonecraft) involves creating buildings, structures, and sculpture using stone from the earth, and is one of the oldest trades in human history. These materials have been used to construct many of the long-lasting, ancient monuments, artifacts, cathedrals, and cities in a wide variety of cultures. Famous works of stonemasonry include the Egyptian Pyramids, the Taj Mahal, Cusco's Incan Wall, Easter Island's statues, Angkor Wat, Borobudur, Tihuanaco, Tenochtitlan, Persepolis, the Parthenon, Stonehenge, Great Wall of China, Chartres Cathedral, and Pumapunku.\n\nMasonry is the craft of shaping rough pieces of rock into accurate geometrical shapes, at times simple, but some of considerable complexity, and then arranging the resulting stones, often together with mortar, to form structures.\n\nThe basic tools, methods and skills of the banker mason have existed as a trade for thousands of years.\n\nThe modern stonemason undergoes comprehensive training, both in the classroom and in the working environment. Hands-on skill is complemented by intimate knowledge of each stone type, its application and best uses, and how to work and fix each stone in place. The mason may be skilled and competent to carry out one or all of the various branches of stonemasonry. In some areas the trend is towards specialization, in other areas towards adaptability.\n\nStonemasons use all types of natural stone: igneous, metamorphic and sedimentary; while some also use artificial stone as well.\n\n\n\nMany of the world's most famous buildings have been built of sedimentary stone, from Durham Cathedral to St Peter's in Rome. There are two main types of sedimentary stone used in masonry work, limestones and sandstones. Examples of limestones include Bath and Portland stone. Yorkstone and Sydney sandstone are most commonly used sandstone.\n\nTypes of stonemasonry are:\nToday’s stonemasons undergo training that is quite comprehensive and is done both in the work environment and in the classroom. It isn’t enough to have hands-on skill anymore. One must also have knowledge of the types of stones as well as its best uses and how to work it as well as how to fix it in place.\n\nTraditionally medieval stonemasons served a seven-year apprenticeship. A similar system still operates today.\n\nA modern apprenticeship lasts three years. This combines on-site learning through personal experience, the experience of the tradesmen and college work where apprentices are given an overall experience of the building, hewing and theory work involved in masonry. In some areas colleges offer courses which teach not only the manual skills but also related fields such as drafting and blueprint reading or construction conservation. Electronic Stonemasonry training resources enhance traditional delivery techniques. Hands-on workshops are a good way to learn about stonemasonry also. Those wishing to become stonemasons should have little problem working at heights, possess reasonable hand-eye co-ordination, be moderately physically fit, and have basic mathematical ability. Most of these things can be developed while learning.\n\nStonemasons use a wide variety of tools to handle and shape stone blocks (ashlar) and slabs into finished articles. The basic tools for shaping the stone are a mallet, chisels, and a metal straight edge. With these one can make a flat surface - the basis of all stonemasonry.\n\nChisels come in a variety of sizes and shapes, dependent upon the function for which they are being used and have many different names depending on locality. There are different chisels for different materials and sizes of material being worked, for removing large amounts of material and for putting a fine finish on the stone.\n\nMixing mortar is normally done today with mortar mixers which usually use a rotating drum or rotating paddles to mix the mortar.\n\nThe masonry trowel is used for the application of the mortar between and around the stones as they are set into place. Filling in the gaps (joints) with mortar is referred to as pointing. Pointing in smaller joints can be accomplished using tuck pointers, pointing trowels, and margin trowels, among other tools.\n\nA mason's hammer has a long thin head and is called a Punch Hammer. It would be used with a chisel or splitter for a variety of purposes\n\nA walling hammer (catchy hammer) can be used in place of a hammer and chisel or pincher to produce rubble or pinnings or snecks.\n\nStonemasons use a lewis together with a crane or block and tackle to hoist building stones into place.\n\nToday power tools such as compressed-air chisels, abrasive spinners and angle grinders are much used: these save time and money, but are hazardous and require just as much skill as the hand tools that they augment. But many of the basic tools of stonemasonry have remained virtually the same throughout vast amounts of time, even thousands of years, for instance when comparing chisels that can be bought today with chisels found at the pyramids of Giza the common sizes and shapes are virtually unchanged.\n\nStonemasonry is one of the earliest trades in civilization's history. During the time of the Neolithic Revolution and domestication of animals, people learned how to use fire to create quicklime, plasters, and mortars. They used these to fashion homes for themselves with mud, straw, or stone, and masonry was born.\n\nThe Ancients heavily relied on the stonemason to build the most impressive and long lasting monuments to their civilizations. The Egyptians built their pyramids, the civilizations of Central America had their step pyramids, the Persians their palaces, the Greeks their temples, and the Romans their public works and wonders (See Roman Architecture). Among the famous ancient stonemasons is Sophroniscus, the father of Socrates, who was a stone-cutter.\n\nCastle building was an entire industry for the medieval stonemasons. When the Western Roman Empire fell, building in dressed stone decreased in much of Western Europe, and there was a resulting increase in timber-based construction. Stone work experienced a resurgence in the 9th and 10th centuries in Europe, and by the 12th century religious fervour resulted in the construction of thousands of impressive churches and cathedrals in stone across Western Europe.\n\nMedieval stonemasons' skills were in high demand, and members of the guild, gave rise to three classes of stonemasons: apprentices, journeymen, and master masons. Apprentices were indentured to their masters as the price for their training, journeymen were qualified craftsmen who were paid by the day, and master masons were considered freemen who could travel as they wished to work on the projects of the patrons and could operate as self-employed craftsmen and train apprentices. During the Renaissance, the stonemason's guild admitted members who were not stonemasons, and eventually evolved into the Society of Freemasonry; fraternal groups which observe the traditional culture of stonemasons, but are not typically involved in modern construction projects.\n\nA medieval stonemason would often carve a personal symbol onto their block to differentiate their work from that of other stonemasons. This also provided a simple ‘quality assurance’ system.\n\nThe Renaissance saw stonemasonry return to the prominence and sophistication of the Classical age. The rise of the Humanist philosophy gave people the ambition to create marvelous works of art. The centre stage for the Renaissance would prove to be Italy, where city-states such as Florence erected great structures, including the Cathedral of Santa Maria del Fiore, the Fountain of Neptune, and the Laurentian Library which was planned and built by Michelangelo Buonarroti, a famous stonemason of the Renaissance.\n\nWhen Europeans settled the Americas, they brought the stonemasonry techniques of their respective homelands with them. Settlers used what materials were available, and in some areas stone was the material of choice. In the first waves, building mimicked that of Europe, to eventually be replaced by unique architecture later on.\n\nIn the 20th century, stonemasonry saw its most radical changes in the way the work is accomplished. Prior to the first half of the century, most heavy work was executed by draft animals or human muscle power. With the arrival of the internal combustion engine, many of these hard aspects of the trade have been made simpler and easier. Cranes and forklifts have made moving and laying heavy stones relatively easy for the stonemasons. Motor powered mortar mixers have saved much in time and energy as well. Compressed-air powered tools have made working of stone less time-intensive. Petrol and electric powered abrasive saws can cut through stone much faster and with more precision than chiseling alone. Carbide-tipped chisels can stand up to much more abuse than the steel and iron chisels made by blacksmiths of old.\n\nCategories:\n\n"}
{"id": "39119664", "url": "https://en.wikipedia.org/wiki?curid=39119664", "title": "Suhail Al Mazroui", "text": "Suhail Al Mazroui\n\nSuhail Mohammed Faraj Al Mazroui (Arabic: سهيل محمد فرج المزروعي) is an Emirati businessman and politician, who has been serving as the minister of energy in the United Arab Emirates since March 2013.\n\nAl Mazroui was born on the 3rd of July 1973 in Dubai, United Arab Emirates.\n\nMazroui holds a petroleum engineering degree, which he received from the University of Tulsa, 1996.\n\nMazroui worked at the Abu Dhabi National Oil Company (ADNOC) for 10 years where he specialized in reservoir engineering and production operations. He served as chief executive officer of ADNOC until November 1994. Yusuf bin Omeir bin Yusuf replaced him in the post.\n\nMazroui also worked at Royal Dutch Shell for more than one year and at Dolphin Energy limited as director. He served in various capacities in international oil and gas projects in Nigeria, North Sea, Brunei and the Netherlands. In 2009, he was named vice chairman of Sorouh Real Estate company. He was the deputy chief executive of state-owned Mubadala Oil and Gas until March 2013. He is a member of the advisory committee of Abu Dhabi’s Supreme Petroleum Council.\n\nIn a reshuffle of 12 March 2013, he was appointed energy minister to the cabinet led by prime minister Mohammed bin Rashid Al Maktoum, replacing Mohammed bin Dhaen Al Hamli in the post. Mazroui also became the head of the federal electricity and water authority.\n\nIn April 2015 he was appointed to the position of managing director of IPIC.\n\nSuhail bin Mohammed Faraj Faris Ghanem Al Mazroui is married and lives in Abu Dhabi with his wife and 4 children . Suhail bin Mohammed's uncle is H.E. Suhail Faris Ghanem Ateish Al Mazrouei, the Chairman of the Dubai Investments Board of Directors and a member of the Supreme Petroleum Council of Abu Dhabi, who is the father of the minister of youth, H.E. Shamma Al Mazrui, making Suhail bin Mohammed Faraj and Shamma bint Suhail bin Faris cousins.\n"}
{"id": "2863776", "url": "https://en.wikipedia.org/wiki?curid=2863776", "title": "Tank truck", "text": "Tank truck\n\nA tank truck, ‘’’gas truck’’’, ‘’’fuel truck’’’, or tanker truck (United States usage) or tanker (United Kingdom usage), is a motor vehicle designed to carry liquefied loads, dry bulk cargo or gases on roads. The largest such vehicles are similar to railroad tank cars which are also designed to carry liquefied loads. Many variants exist due to the wide variety of liquids that can be transported. Tank trucks tend to be large; they may be insulated or non-insulated; pressurized or non-pressurized; and designed for single or multiple loads (often by means of internal divisions in their tank). Some are semi-trailer trucks. They are difficult to drive due to their high center of gravity.\n\nTank trucks are described by their size or volume capacity. Large trucks typically have capacities ranging from . In Australia, road trains up to four trailers in length (known as Quad tankers) carry loads in excess of 120,000 L. Longer road trains transporting liquids are also in use.\n\nA tank truck is distinguished by its shape, usually a cylindrical tank upon the vehicle lying horizontally. Some less visible distinctions amongst tank trucks have to do with their intended use: compliance with human food regulations, refrigeration capability, acid resistance, pressurization capability, and more. The tanks themselves will almost always contain multiple compartments or baffles to prevent load movement destabilizing the vehicle.\n\nLarge tank trucks are used for example to transport gasoline to filling stations. They also transport a wide variety of liquid goods such as liquid sugar, molasses, milk, wine, juices, water, gasoline, diesel, and industrial chemicals.\n\nTank trucks are constructed of various materials depending on what products they are hauling. These materials include aluminium, carbon steel, stainless steel, and fiberglass reinforced plastic (FRP).\n\nSome tank trucks are able to carry multiple products at once due to compartmentalization of the tank into 2, 3, 4, 5, 6 or in some rare cases more tank compartments. This allows for an increased number of delivery options. These trucks are commonly used to carry different grades of gasoline to service stations to carry all products needed in one trip.\n\nSmaller tank trucks, with a capacity of less than are typically used to deal with light liquid cargo within a local community. A common example is vacuum truck used to empty several septic tanks and then deliver the collected fecal sludge to a treatment site. These tank trucks typically have a maximum capacity of . They are equipped with a pumping system to serve their particular need.\n\nAnother common use is to deliver fuel such as liquified petroleum gas (LPG) to households, commerces and industries. The smallest of these trucks usually carry about of LPG under pressure. Typically LPG tank trucks carry up to 3499 US gallons of product (usually liquid propane), on a 2 axle bobtail truck. 3500 US gallons (13,200 L; 2,900 imp gal) and greater requires a 3 axle truck (tank wagon). Some companies are using lightweight steel to carry more gallons on single-axle trucks. Notably, one U.S. manufacturer has built a 3700 US gallon tank truck, fitting it on a single axle. \n\nTank trucks are also used to fuel aircraft at airports.\n\n"}
{"id": "41842064", "url": "https://en.wikipedia.org/wiki?curid=41842064", "title": "Tetranitratoaluminate", "text": "Tetranitratoaluminate\n\nTetranitratoaluminate is an anion of aluminium and nitrate groups with formula [Al(NO)] that can form salts called tetranitratoaluminates. It is unusual in being a nitrate complex of a light element.\n\nBy substituting boron for aluminium tetranitratoborates result. Aluminium can coordinate more nitrates resulting in pentanitratoaluminates and hexanitratoaluminates.\n\nBy replacing nitrate with perchlorate, the tetraperchloratoaluminate ion results.\n\nWhen hydrated aluminium nitrate reacts with dinitrogen pentoxide it forms a nitronium salt: [NO][Al(NO)].\n\nA way to make a tetranitratoaluminate salt of a cation is to treat the chloride of the cation and aluminium chloride with liquid dinitrogen tetroxide pure or dissolved in nitromethane. The reaction is started a liquid nitrogen temperatures and then warmed up. Dark red nitrosyl chloride is formed as a byproduct. The byproducts and solvents can then be evaporated. The tetramethylammonium salt can form this way.\n\nThe tetranitratoaluminate group has two bidentate nitrate groups attached in a square around the aluminium, and with two other monodentate nitrates attached via one oxygen only, perpendicular, up and down from the square.\n\nTetranitratoaluminate salts are not completely stable and can decompose to the nitrates and aluminium oxy-nitrates.\n\nWhen nitronium tetranitratoaluminate is sublimed it can form anhydrous aluminium nitrate.\n\nNitronium tetranitratoaluminate dissolved in a nitric acid and dinitrogen pentoxide mixture yields the hexanitratoaluminate complex. In water is it converted to the hexaaqua complex with six water molecules replacing the nitrate groups.\n\nTetraethyl ammonium tetranitratoaluminate along with nitronium tetranitratoaluminate were the first to be discovered.\n\n1-Ethyl-4,5-dimethyl-tetrazolium tetranitratoaluminate is an oxygen balanced ionic liquid, This liquid salt is stable when moisture is excluded. It is soluble in methyl nitrate. It solidifies to a glass at -46°, starts to slowly decompose at 75°, and inflames, without oxygen required, around 200°. When it burns it produces alumina, nitrogen, water and carbon monoxide. It is being proposed as a rocket propellant, because it has better performance than hydrazine.\n\nRubidium and Caesium also form salts.\n\nTetramethyl ammonium tetranitratoaluminate forms monoclinic crystals with a=12.195Å, b=9.639Å c=12.908Å, α=90° β=110.41° γ=90° formula weight 349.17 formulas per unit cell= 4 Unit cell volume is 1422Å calculated density 1.631 g/cm.\n"}
{"id": "30393", "url": "https://en.wikipedia.org/wiki?curid=30393", "title": "Thermophile", "text": "Thermophile\n\nA thermophile is an organism—a type of extremophile—that thrives at relatively high temperatures, between . Many thermophiles are archaea. Thermophilic eubacteria are suggested to have been among the earliest bacteria.\n\nThermophiles are found in various geothermally heated regions of the Earth, such as hot springs like those in Yellowstone National Park (see image) and deep sea hydrothermal vents, as well as decaying plant matter, such as peat bogs and compost.\n\nThermophiles can survive at high temperatures, whereas other bacteria would be damaged and sometimes killed if exposed to the same temperatures. \n\nThe enzymes in thermophiles necessarily function at high temperatures. Some of these enzymes are used in molecular biology, for example, heat-stable DNA polymerases for PCR), and in washing agents.\n\n\"Thermophile\" is derived from the (\"thermotita\"), meaning heat, and (\"philia\"), love.\n\nThermophiles can be classified in various ways. One classification sorts these organisms according to their optimal growth temperatures:\n\nIn a related classification, thermophiles are sorted as follows: \n\nMany of the hyperthermophiles Archea require elemental sulfur for growth. Some are anaerobes that use the sulfur instead of oxygen as an electron acceptor during cellular respiration. Some are lithotrophs that oxidize sulphur to create sulfuric acid as an energy source, thus requiring the microorganism to be adapted to very low pH (i.e., it is an acidophile as well as thermophile). These organisms are inhabitants of hot, sulfur-rich environments usually associated with volcanism, such as hot springs, geysers, and fumaroles. In these places, especially in Yellowstone National Park, zonation of microorganisms according to their temperature optima occurs. Often, these organisms are colored, due to the presence of photosynthetic pigments.\n\nThermophiles can be discriminated from mesophiles from genomic features. For example, the GC-content levels in the coding regions of some signatures genes were consistently identified as correlated with the temperature range condition when the association analysis was applied to mesophilic and thermophilic organisms regardless of their phylogeny, oxygen requirement, salinity, or habitat conditions.\n\n\"Sulfolobus solfataricus\" and \"Sulfolobus acidocaldarius\" are hyperthermophilic archaea. When these organisms are exposed to the DNA damaging agents UV irradiation, bleomycin or mitomycin C, species-specific cellular aggregation is induced. In \"S. acidocaldarius\", UV-induced cellular aggregation mediates chromosomal marker exchange with high frequency. Recombination rates exceed those of uninduced cultures by up to three orders of magnitude. Frols et al. and Ajon et al.(2011) hypothesized that cellular aggregation enhances species-specific DNA transfer between \"Sulfolobus\" cells in order to provide increased repair of damaged DNA by means of homologous recombination. Van Wolferen et al., in discussing DNA exchange in the hyperthermophiles under extreme conditions, noted that DNA exchange likely plays a role in repair of DNA via homologous recombination. They suggested that this process is crucial under DNA damaging conditions such as high temperature. Also it has been suggested that DNA transfer in \"Sulfolobus\" may be a primitive form of sexual interaction similar to the more well-studied bacterial transformation systems that are associated with species-specific DNA transfer between cells leading to homologous recombinational repair of DNA damage [see Transformation (genetics)].\n\n\n"}
{"id": "338963", "url": "https://en.wikipedia.org/wiki?curid=338963", "title": "Third rail", "text": "Third rail\n\nA third rail is a method of providing electric power to a railway locomotive or train, through a semi-continuous rigid conductor placed alongside or between the rails of a railway track. It is used typically in a mass transit or rapid transit system, which has alignments in its own corridors, fully or almost fully segregated from the outside environment. Third rail systems are always supplied from direct current electricity.\n\nThe third-rail system of electrification is unrelated to the third rail used in dual gauge railways.\n\nThird-rail systems are a means of providing electric traction power to trains using an additional rail (called a \"conductor rail\") for the purpose. On most systems, the conductor rail is placed on the sleeper ends outside the running rails, but in some systems a central conductor rail is used. The conductor rail is supported on ceramic insulators (known as \"pots\") or insulated brackets, typically at intervals of around .\n\nThe trains have metal contact blocks called collector shoes (or contact shoes or pickup shoes) which make contact with the conductor rail. The traction current is returned to the generating station through the running rails. In the US, the conductor rail is usually made of high conductivity steel or steel bolted to aluminum to increase the conductivity. Elsewhere in the world, extruded aluminum conductors with stainless steel contact surface or cap, is the preferred technology due to its lower electrical resistance, longer life, and lighter weight. The running rails are electrically connected using wire bonds or other devices, to minimize resistance in the electric circuit. Contact shoes can be positioned below, above, or beside the third rail, depending on the type of third rail used; these third rails are referred to as bottom-contact, top-contact, or side-contact, respectively.\n\nThe conductor rails have to be interrupted at level crossings, crossovers, and substation gaps. Tapered rails are provided at the ends of each section, to allow a smooth engagement of the train's contact shoes.\n\nThe position of contact between the train and the rail varies: some of the earliest systems used top contact, but later developments use side or bottom contact, which enabled the conductor rail to be covered, protecting track workers from accidental contact and protecting the conductor rail from snow and leaf fall.\n\nBecause third rail systems present electric shock hazards close to the ground, high voltages (above 1500 V) are not considered safe. A very high current must therefore be used to transfer adequate power, resulting in high resistive losses, and requiring relatively closely spaced feed points (electrical substations).\n\nThe electrified rail threatens electrocution of anyone wandering or falling onto the tracks. This can be avoided by using platform screen doors, or the risk can be reduced by placing the conductor rail on the side of the track away from the platform, when allowed by the station layout. The risk can also be reduced by having an insulated coverboard to protect the third rail from contact, although many systems do not use one.\n\nIn some modern systems such as the Ground-level power supply (first used in the tramway of Bordeaux), the safety problem is avoided by splitting the power rail in small segments, each of which is only powered when fully covered by a train.\n\nThere is also a risk of pedestrians walking onto the tracks at level crossings. In the US, a 1992 Supreme Court of Illinois decision affirmed a $1.5 million verdict against the Chicago Transit Authority for failing to stop an intoxicated person from walking onto the tracks at a level crossing in an attempt to urinate. The Paris Metro has graphic warning signs pointing out the danger of electrocution from urinating on third rails, precautions which Chicago did not have.\n\nThe end ramps of conductor rails (where they are interrupted, or change sides) present a practical limitation on speed due to the mechanical impact of the shoe, and is considered the upper limit of practical third-rail operation. The world speed record for a third rail train is attained on 11 April 1988 by a British Class 442 EMU.\n\nIn the event of a collision with a foreign object, the beveled end ramps of bottom running systems can facilitate the hazard of having the third rail penetrate the interior of a passenger car. This is believed to have contributed to the death of five passengers in the Valhalla train crash of 2015.\n\nThird rail systems using top contact are prone to accumulations of snow, or ice formed from refrozen snow, and this can interrupt operations. Some systems operate dedicated de-icing trains to deposit an oily fluid or antifreeze (such as propylene glycol) on the conductor rail to prevent the frozen build-up. The third rail can also be heated to alleviate the problem of ice.\n\nUnlike third rail systems, overhead line equipment can be affected by strong winds or freezing rain bringing the wires down and stopping all trains. Thunderstorms can also disable the power with lightning strikes on systems with overhead wires, disabling trains if there is a power surge or a break in the wires.\n\n Because of the gaps in the conductor rail (e.g., at level crossings and junctions) a train can stop in a position where all of its power pickup shoes are in gaps, so that no traction power is available. The train is then said to be \"gapped\". Another train must then be brought up behind the stranded train to push it on to the conductor rail, or a jumper cable may be used to supply enough power to the train to get one of its contact shoes back on the third rail. Avoiding this problem requires a minimum length of trains that can be run on a line. Locomotives have either had the backup of an on-board diesel engine system (e.g., British Rail Class 73), or have been connected to shoes on the rolling stock (e.g. Metropolitan Railway).\n\nThe first idea for feeding electricity to a train from an external source was by using both rails on which a train runs, whereby each rail is a conductor for each polarity, and is insulated by the sleepers. This method is used by most scale model trains, however it does not work so well for large trains as the sleepers are not good insulators. Furthermore, the electric connection requires insulated wheels or insulated axles, but most insulation materials have poor mechanical properties compared with metals used for this purpose, leading to a less stable train vehicle. Nevertheless, it was sometimes used at the beginning of the development of electric trains. The oldest electric railway in the world, the Volk's Railway in Brighton, England was originally electrified at 50 volts DC using this system (it is now a three rail system). Other railway systems that used it were the Gross-Lichterfelde Tramway and the Ungerer Tramway.\n\nThe third rail is usually located outside the two running rails, but on some systems it is mounted between them. The electricity is transmitted to the train by means of a sliding shoe, which is held in contact with the rail. On many systems, an insulating cover is provided above the third rail to protect employees working near the track; sometimes the shoe is designed to contact the side (called \"side running\") or bottom (called \"bottom running\" or \"under-running\") of the third rail, allowing the protective cover to be mounted directly to its top surface. When the shoe slides along the top surface, it is referred to as \"top running\". When the shoe slides along the bottom surface, it is less affected by the build-up of snow, ice, or leaves, and reduces the chances of a person being electrocuted by coming in contact with the rail. Examples of systems using under-running third rail include Metro-North in the New York metropolitan area; the SEPTA Market-Frankford Line in Philadelphia; and London's Docklands Light Railway.\n\nElectric traction systems (where electric power is generated at a remote power station and transmitted to the trains) are considerably more cost-effective than diesel or steam units, where separate power units must be carried on each train. This advantage is especially marked in urban and rapid transit systems with a high traffic density.\n\nBecause of mechanical limitations on the contact to the third rail, trains that use this method of power supply achieve lower speeds than those using overhead electric wires and a pantograph. Nevertheless, they may be preferred inside the cities as there is no need for very high speed and they cause less visual pollution.\n\nThe third rail is an alternative to overhead lines that transmit power to trains by means of pantographs attached to the trains. Whereas overhead-wire systems can operate at 25 kV or more, using alternating current (AC), the smaller clearance around a live rail imposes a maximum of about 1500 V (Line 4, Guangzhou Metro, Line 5, Guangzhou Metro, Line 3, Shenzhen Metro), and direct current (DC) is used. Trains on some lines or networks use both power supply modes (see below).\n\nAll third rail systems throughout the world are energised with DC supplies. Some of the reasons for this are historical. Early traction engines were DC motors, and the then available rectifying equipment was large, expensive and impractical to install onboard trains. Also, transmission of the relatively high currents required results in higher losses with AC than DC. Substations for a DC system will have to be (typically) about apart, though the actual spacing depends on the carrying capacity; maximum speed and service frequency of the line. The Docklands Light Railway (DLR) uses a third rail which is tiny in section compared with the usual; thus fewer substations are required. The DLR was able to do this (in the 1980s) because it was a totally new build with custom-built trains and had no need for a formal connection to an existing 'heavy' third rail system.\n\nOne method for reducing current losses (and thus increase the spacing of feeder/sub stations, a major cost in third rail electrification) is to use a composite conductor rail of a hybrid aluminium/steel design. The aluminium is a better conductor of electricity, and a running face of stainless steel gives better wear.\n\nThere are several ways of attaching the stainless steel to the aluminium. The oldest is a co-extruded method, where the stainless steel is extruded with the aluminium. This method has suffered, in isolated cases, from de-lamination (where the stainless steel separates from the aluminium); this is said to have been eliminated in the latest co-extruded rails. A second method is an aluminium core, upon which two stainless steel sections are fitted as a cap and linear welded along the centre line of the rail. Because aluminium has a higher coefficient of thermal expansion than steel, the aluminium and steel must be positively locked to provide a good current collection interface. A third method rivets aluminium bus strips to the web of the steel rail. The photo below-right of a Chicago L car's shoe depicts such a rail, showing the aluminum bus strips filling the space below the contact surface of the rail.\n\nAs with overhead wires, the return current usually flows through one or both running rails, and leakage to ground is not considered serious. Where trains run on rubber tyres, as on parts of the Lyon Metro, Paris Métro, Mexico City metro, Santiago Metro, Sapporo Municipal Subway, and on all of the Montreal Metro and some automated guideway transit systems (e.g. the Astram Line), a live rail must be provided to feed the current. The return is effected through the rails of the conventional track between these guide bars (\"see rubber-tyred metro\").\n\nAnother design, with a third rail (current feed, outside the running rails) and fourth rail (current return, midway between the running rails), is used by a few steel-wheel systems; see fourth rail. The London Underground is the largest of these, (see railway electrification in Great Britain). The main reason for using the fourth rail to carry the return current is to avoid this current flowing through the original metal tunnel linings which were never intended to carry current, and which would suffer electrolytic corrosion should such currents flow in them.\n\nAnother four-rail system is line M1 of the Milan Metro, where current is drawn by a lateral, flat bar with side contact, with return via a central rail with top contact. Along some sections on the northern part of the line an overhead line is also in place, to allow line M2's trains (that use pantographs and higher voltage, and have no contact shoes) to access a depot located on line M1. In depots, line M1 trains use pantographs because of safety reasons, with transition made near the depots away from revenue tracks.\n\nThird rail electrification is less visually obtrusive than overhead electrification. In 2011, greenery and aesthetics inspired the Bangalore Metro in India to incorporate a third rail system.\n\nSeveral systems use a third rail for part of the route, and other motive power such as overhead catenary or diesel power for the remainder. These may exist because of the connection of separately-owned railways using the different motive systems, local ordinances, or other historical reasons.\n\nSeveral types of British trains have been able to operate on both overhead and third rail systems, including class British Rail Class 313, 319, 325, 350, 365, 375/6, 377/2, 377/5, 377/7, 378/2, 387, 373, 395 and 700 EMUs, plus Class 92 locomotives.\n\nOn the southern region of British Rail, freight yards had overhead wiring to avoid the hazards of a third rail. The locomotives were fitted with a pantograph as well as pick-up shoes.\n\nThe Class 373 used for international services operated by Eurostar via the Channel Tunnel uses overhead collection at 25 kV AC for most of its journey, with sections of 3 kV DC on Belgian lines between the Belgian high speed section and Brussels Midi station or 1.5 kV DC on southern French lines for seasonal services. As originally delivered, the Class 373 units were additionally fitted with 750 V DC collection shoes, designed for the journey in London via the suburban commuter lines to Waterloo. A switch between third-rail and overhead collection was performed while running at speed, initially at Continental Junction near Folkestone, and later on at Fawkham Junction after the opening of the first section of the Channel Tunnel Rail Link. Between Kensington Olympia railway station and North Pole depot further switchovers were necessary.\n\nThe dual system did cause some problems. Failure to retract the shoes when entering France caused severe damage to trackside equipment, leading to SNCF installing a pair of concrete blocks at the Calais end of both tunnels to break off the third rail shoes if they had not been retracted. An accident occurred in the UK when a Eurostar driver failed to retract the pantograph before entering the third rail system, damaging a signal gantry and the pantograph.\n\nOn 14 November 2007, Eurostar's passenger operations were transferred to St Pancras railway station and maintenance operations to Temple Mills depot making the 750 VDC third rail collection equipment redundant and leading to its removal from the fleet. All speed limits on the English Eurostar lines are posted in km/h and the lineside signs on non high speed sections are white numerals on a black background (instead of the UK standard black numerals on a white background) as a reminder. The trains themselves are no longer fitted with a speedometer capable of indicating in miles per hour (the indication used to change automatically when the collector shoes were deployed).\n\nIn 2009, Southeastern began operating domestic services over High Speed 1 trackage from St Pancras using its new Class 395 EMUs. These services operate on the High Speed line as far as or , before transferring to the classic lines to serve north and mid Kent. As a consequence, these trains are dual voltage enabled, as the majority of the routes over which they operate are third rail electrified.\n\nIn London, the North London Line changes its power supply once between Richmond and Stratford at . The route was originally third rail throughout but several technical electrical earthing problems, plus part of the route also being covered already by overhead electric wires provided for electrical-hauled freight and Regional Eurostar services led to the change.\n\nAlso in London, the West London Line changes power supply between Shepherd's Bush and Willesden Junction, where it meets the North London Line. South of the changeover point, the WLL is third rail electrified, north of there, it is overhead.\n\nThe cross-city Thameslink service runs on the Southern Region third rail network from Farringdon southwards and on overhead line northwards to Bedford. The changeover is made whilst stationary at Farringdon when heading southbound, and at City Thameslink when heading northbound.\n\nOn the Moorgate to Hertford and Welwyn suburban service routes, the East Coast Main Line sections are 25 kV AC, with a changeover to third rail made at Drayton Park railway station. A third rail is still used in the tunnel section of the route, because the size of the tunnels leading to Moorgate station was too small to allow overhead electrification.\n\nThe North Downs Line is not electrified on those parts of the line where the North Downs service has exclusive use.\n\nThe electrified portions of the line are\n\nThe Stockholm metro is served by a third rail system.\n\nThe new tramway in Bordeaux (France) uses a novel system with a third rail in the center of the track. The third rail is separated into long conducting and long isolation segments. Each conducting segment is attached to an electronic circuit which will make the segment live once it lies fully beneath the tram (activated by a coded signal sent by the train) and switch it off before it becomes exposed again. This system (called \"Alimentation par Sol\" (APS), meaning \"current supply via ground\") is used in various locations around the city but especially in the historic centre: elsewhere the trams use the conventional overhead lines, see also ground-level power supply. In summer 2006 it was announced that two new French tram systems would be using APS over part of their networks. These will be Angers and Reims, with both systems expected to open around 2009–2010.\n\nThe French Culoz–Modane railway was electrified with 1500 V DC third rail, later converted to overhead wires at the same voltage. Stations had overhead wires from the beginning.\n\nThe French branch line which serves Chamonix and the Mont Blanc region (Saint-Gervais-le-Fayet to Vallorcine) is third rail (top contact) and metre gauge. It continues in Switzerland, partly with the same third rail system, partly with an overhead line.\n\nThe long Train Jaune line in the Pyrenees also features a third rail.\n\nTo mitigate investment costs, the Rotterdam Metro, basically a third-rail-powered system, has been given some outlying branches built on surface as light rail (called \"Sneltram\" in Dutch), with numerous level crossings protected with barriers and traffic lights. These branches have overhead wires. Similarly, in Amsterdam one \"Sneltram\" route goes on Metro tracks and passes to surface alignment in the suburbs, which it shares with standard trams. In most recent developments, the RandstadRail project also requires Rotterdam Metro trains to run under wires on their way along the former mainline railway to The Hague.\n\nSneltram is operated by Gemeentelijk Vervoerbedrijf in Amsterdam lightrail with third rail and switching to overhead on the traditional tramway shared with Trams in Amsterdam. Line 51 to Amstelveen runs metro service between Amsterdam Centraal and Station Zuid. At Amsterdam Zuid it switches from third rail to pantograph and catenary wires. From there to Amstelveen Centrum it shares its track with tram line 5. The light rail vehicles on this line are capable of using both 600 V DC and 750 V DC.\n\nIn all the subways of post-Soviet countries, the is made to the same standard. In particular, because carbon impurities increase electrical resistance, all third rails are made using low-carbon steel.\n\nPerhaps in some metros of the former Soviet Union profile and cross section of conductor rail are the same parameters of conventional track.\n\nThe natural, pre-installation length of the conductor rail is . During installation, the contact rail segments are welded together to produce conductor rails of varying length. In curved sections with a radius of or more, straightaways, and tunnels, the contact rail is welded to a length of ; at surface running, ; and, on tight curves and park paths, .\n\nPost-Soviet third rail installations use the bottom-contact (Wilgus-Sprague) system; on top of the rail is high strength plastic casing with sufficient structural integrity to support a man's weight. Voltage is 825 volts DC.\n\nIn New York City, electric trains that must use the third rail leaving Grand Central Terminal on the former New York Central Railroad (now Metro-North Railroad) switch to overhead lines at Pelham when they need to operate out onto the former New York, New Haven and Hartford Railroad (now Metro North's New Haven Line) line to Connecticut. The switch is made \"on the fly\", and controlled from the engineer's position.\n\nAlso in New York City where diesel exhaust would pose a health hazard in underground station areas, Metro-North, Long Island Rail Road and Amtrak use special diesel locomotives that can also be electrically powered by third-rail. This kind of locomotive (for example the General Electric P32AC-DM or the EMD DM30AC of LIRR), can transition between the two modes while underway. The third-rail auxiliary system is not as powerful as the diesel engine, so on open-air (non-tunnel) trackage the engines typically run in diesel mode, even where third rail power is available.\n\nIn New York City, and in Washington, D.C., local ordinances once required electrified street railways to draw current from a third rail and return the current to a fourth rail, both installed in a continuous vault underneath the street and accessed by means of a collector that passed through a slot between the running rails. When streetcars on such systems entered territory where overhead lines were allowed, they stopped over a pit where a man detached the collector (\"plow\") and the motorman placed a trolley pole on the overhead. In the US, all these conduit feed powered systems have been discontinued, and either replaced or abandoned altogether.\n\nSome sections of the former London tram system also used the conduit current collection system, also with some tramcars that could collect power from both overhead and under-road sources.\n\nThe Blue Line of Boston's MBTA uses third rail electrification from the start of the line downtown to Airport station, where it switches to overhead catenary for the remainder of the line to Wonderland. The outermost section of the Blue Line runs very close to the Atlantic Ocean, and there were concerns about possible snow and ice buildup on a third rail so near to the water. Overhead catenary is not used in the underground section, because of tight clearances in the 1904 tunnel under Boston Harbor. The MBTA Orange Line's Hawker Siddeley 01200 series rapid transit cars (essentially a longer version of the Blue Line's 0600's) recently had their pantograph mounting points removed during a maintenance program; these mounts would have been used for pantographs which would have been installed had the Orange Line been extended north of its current terminus.\n\nDual power supply method was also used on some US interurban railways that made use of newer third rail in suburban areas, and existing overhead streetcar (trolley) infrastructure to reach downtown, for example the Skokie Swift in Chicago.\n\nBay Area Rapid Transit in and around San Francisco uses 1000 V DC.\n\nA railway can be electrified with an overhead wire and a third rail at the same time. This was the case, for example, on the Hamburg S-Bahn between 1940 and 1955. A modern example is Birkenwerder Railway Station near Berlin, which has third rails on both sides and overhead wires. Most of the Penn Station complex in New York City is also electrified with both systems. However, such systems have problems with the interaction of the different electrical supplies. If one supply is DC and the other AC, an undesired premagnetization of the AC transformers can occur. For this reason, dual electrification is usually avoided.\n\nDespite various technical possibilities for operating rolling stock with dual power collecting modes, a desire to achieve full compatibility of entire networks seems to have been the incentive for conversions from third rail to overhead supply (or vice versa).\n\nSuburban corridors in Paris from Gare Saint-Lazare, Gare des Invalides (both CF Ouest) and Gare d'Orsay (CF PO), were electrified from 1924, 1901, 1900 respectively. They all changed to overhead wires by stages after they became part of a wide scale electrification project of the SNCF network in the 1960s–70s.\n\nIn the Manchester area, the L&YR Bury line was first electrified with overhead wires (1913), then changed to third rail (1917; see also Railway electrification in Great Britain) and then back again in 1992 to overhead wires in the course of its adaptation for the Manchester Metrolink. Trams in city centre streets, carrying collector shoes projecting from their bogies, were considered too dangerous for pedestrians and motor traffic to attempt dual-mode technology (in Amsterdam and Rotterdam \"Sneltram\" vehicles go out to surface in suburbs, not in busy central areas). The same thing happened to the West Croydon – Wimbledon Line in Greater London (originally electrified by the Southern Railway) when Tramlink was opened in 2000.\n\nThree lines out of five making up the core of Barcelona Metro network changed to overhead power supply from third rail. This operation was also done by stages and completed in 2003.\n\nThe opposite transition took place in South London. The South London Line of the LBSCR network between Victoria and London Bridge was electrified with catenary in 1909. The system was later extended to Crystal Palace, Coulsdon North and Sutton. In the course of mainline third rail electrification in southeast England, the lines were converted by 1929.\n\nThe reasons for building the overhead powered Tyne & Wear Metro network roughly on lines of the long-gone third-rail Tyneside Electrics system in Newcastle area are likely to have roots in economy and psychology rather than in the pursuit of compatibility. At the time of the Metro opening (1980), the third rail system had already been removed from the existing lines, there were no third-rail light rail vehicles on the market and the latter technology was confined to much more costly heavy rail stock. Also the far-going change of image was desired: the memories of the last stage of operation of the Tyneside Electrics were far from being favourable. This was the construction of the system from scratch after 11 years of ineffective diesel service.\n\nThe first overhead feed to German electric trains appeared on the \"Hamburg-Altonaer Stadt- und Vorortbahn\" in 1907. Thirty years later, the main-line railway operator, Deutsche Reichsbahn, influenced by the success of the third-rail Berlin S-Bahn, decided to switch what was now called Hamburg S-Bahn to third rail. The process began in 1940 and was not finished until 1955.\n\nIn 1976–1981, the third-rail Vienna U-Bahn U4 Line substituted the Donaukanallinie and Wientallinie of the \"Stadtbahn\", built c1900 and first electrified with overhead wires in 1924. This was part of a big project of consolidated U-Bahn network construction. The other electric \"Stadtbahn\" line, whose conversion into heavy rail stock was rejected, still operates under wires with light rail cars (as U6), though it has been thoroughly modernised and significantly extended. As the platforms on the Gürtellinie were not suitable for raising without much intervention into historic Otto Wagner's station architecture, the line would anyway remain incompatible with the rest of the U-Bahn network. Therefore, an attempt of conversion to third rail would have been pointless. In Vienna, paradoxically, the wires were retained for aesthetic (and economic) reasons.\n\nThe older lines in the west of the Oslo T-bane system were built with overhead lines while the eastern lines were built with third rail, although the entire system has since been converted to third rail. Prior to the conversion, the now-retired OS T1300 and OS T2000 trains could operate on both systems.\n\nThe western portion of the Skokie Swift of the Chicago 'L' changed from catenary wire to third rail in 2004, making it fully compatible with the rest of the system.\n\nSome high third rail voltages (1200 volts and above) include:\n\nIn Germany during the early Third Reich, a railway system with gauge width was planned. For this \"Breitspurbahn\" railway system, electrification with a voltage of 100 kV taken from a third rail was considered, in order to avoid damage to overhead wires from oversize rail-mounted anti-aircraft guns. However such a power system would not have worked, as it is not possible to insulate a third rail for such high voltages in close proximity to the rails. The whole project did not progress any further owing to the onset of World War II.\n\nThird-rail electrification systems are, apart from on-board batteries, the oldest means of supplying electric power to trains on railways using their own corridors, particularly in cities. Overhead power supply was initially almost exclusively used on tramway-like railways, though it also appeared slowly on mainline systems.\n\nAn experimental electric train using this method of power supply was developed by the German firm of Siemens & Halske and shown at the Berlin Industrial Exposition of 1879, with its third rail between the running rails. Some early electric railways used the running rails as the current conductor, as with the 1883-opened Volk's Electric Railway in Brighton. It was given an additional power rail in 1886, and is still operating. The Giant's Causeway Tramway followed, equipped with an elevated outside third rail in 1883, later converted to overhead wire. The first railway to use the central third rail was the Bessbrook and Newry Tramway in Ireland, opened in 1885 but now, like the Giant's Causeway line, closed.\n\nAlso in the 1880s, third-rail systems began to be used in public urban transport. Trams were first to benefit from it: they used conductors in conduit below the road surface (see Conduit current collection), usually on selected parts of the networks. This was first tried in Cleveland (1884) and in Denver (1885) and later spread to many big tram networks (e.g. New York, Chicago, Washington DC, London, Paris, all of which are closed) and Berlin (the third rail system in the city was abandoned in the first years of the 20th century after heavy snowfall.) The system was tried in the beachside resort of Blackpool, UK but was soon abandoned as sand and saltwater was found to enter the conduit and cause breakdowns, and there was a problem with voltage drop. Some sections of tramway track still have the slot rails visible.\n\nA third rail supplied power to the world's first electric underground railway, the City & South London Railway, which opened in 1890 (now part of the Northern line of the London Underground). In 1893, the world's second third-rail powered city railway opened in Britain, the Liverpool Overhead Railway (closed 1956 and dismantled). The first US third-rail powered city railway in revenue use was the 1895 Metropolitan West Side Elevated, which soon became part of the Chicago 'L'. In 1901, Granville Woods, a prominent African-American inventor, was granted a , covering various proposed improvements to third rail systems. This has been cited to claim that he invented the third rail system of current distribution. However, by that time there had been numerous other patents for electrified third-rail systems, including Thomas Edison's of 1882, and third rails had been in successful use for over a decade, in installations including the rest of Chicago 'elevateds', as well as those used in Brooklyn Rapid Transit Company, not to mention the development outside the US.\n\nIn Paris, a third rail appeared in 1900 in the main-line tunnel connecting the Gare d'Orsay to the rest of the CF Paris-Orléans network. Main-line third-rail electrification was later expanded to some suburban services.\n\nThe Woodford haulage system was used on industrial tramways, specifically in quarries and strip mines in the early decades of the 20th century. This used a 250 Volt center third rail to power remotely-controlled self-propelled side dump cars. The remote control system was operated like a model railroad, with the third rail divided into multiple blocks that could be set to power, coast, or brake by switches in the control center.\n\nTop contact or gravity type third rail seems to be the oldest form of power collection. Railways pioneering in using less hazardous types of third rail were the New York Central Railroad on the approach to New York's Grand Central Terminal (1907 – another case of a third-rail mainline electrification), Philadelphia's Market Street Subway-Elevated (1907), and the Hochbahn in Hamburg (1912) — all had bottom contact rail, also known as the Wilgus-Sprague system. However, the Manchester-Bury Line of the Lancashire & Yorkshire Railway tried side contact rail in 1917. These technologies appeared in wider use only at the turn of the 1920s and in the 1930s on, e.g., large-profile lines of the Berlin U-Bahn, the Berlin S-Bahn and the Moscow Metro. The Hamburg S-Bahn has used a side contact third rail at 1200 V DC since 1939.\n\nIn 1956 the world's first rubber-tyred railway line, Line 11 of Paris Metro, opened. The conductor rail evolved into a pair of guiding rails required to keep the bogie in proper position on the new type of track. This solution was modified on the 1971 Namboku Line of Sapporo Subway, where a centrally placed guiding/return rail was used plus one power rail placed laterally as on conventional railways.\nThe third-rail technology at street tram lines has recently been revived in the new system of Bordeaux (2004). This is a completely new technology (see below).\n\nThird-rail systems are not considered obsolete. There are, however, countries (particularly Japan, South Korea, Spain) more eager to adopt overhead wiring for their urban railways. But at the same time, there were (and still are) many new third rail systems built elsewhere, including technologically advanced countries (e.g. Copenhagen Metro, Taipei Metro, Wuhan Metro). Bottom powered railways (it may be too specific to use the term 'third rail') are also usually used with systems having rubber-tyred trains, whether it is a heavy metro (except two other lines of Sapporo Subway) or a small capacity people mover (PM). New electrified railway systems tend to use overhead for regional and long-distance systems. Third-rail systems using lower voltages than overhead systems still require many more supply points.\n\nIn 1906, the Lionel electric trains became the first model trains to use a third rail to power the locomotive. Lionel track uses a third rail in the center, while the two outer rails are electrically connected together. This solved the problem two-rail model trains have when the track is arranged to loop back on itself, as ordinarily this causes a short-circuit. (Even if the loop was gapped, the locomotive would create a short and stop as it crossed the gaps.) Lionel electric trains also operate on alternating current. The use of alternating current means that a Lionel locomotive cannot be reversed by changing polarity; instead, the locomotive sequences among several states (forward, neutral, backward, for example) each time it is started.\n\nMärklin three-rail trains use a short spike of DC voltage to reverse a relay within the locomotive while it is stopped. Märklin's track does not have an actual third rail; instead, a series of short pins provide the current, taken up by a long \"shoe\" under the engine. This shoe is long enough to always be in contact with several pins. This is known as the stud contact system and has certain advantages when used on outdoor model railway systems. The ski collector rubs over the studs and thus inherently self cleans. When both track rails are used for the return in parallel there is much less chance of current interruption due to dirt on the line.\n\nMany model train sets today use only two rails, usually associated with Z, N, HO or G-Gauge systems. These are typically powered by direct current (DC) where the voltage and polarity of the current controls the speed and direction of the DC motor in the train. A growing exception is Digital Command Control (DCC), where bi-polar DC is delivered to the rails at a constant voltage, along with digital signals that are decoded within the locomotive. The bi-polar DC carries digital information to indicate the command and the locomotive that is being commanded, even when multiple locomotives are present on the same track. The aforementioned Lionel O-Gauge system remains popular today as well. With its three rail track and AC power implementation.\n\nSome model railroads realistically mimic the third rail configurations of their full-sized counterparts although most do not draw power from the third rail.\n\n"}
{"id": "50899238", "url": "https://en.wikipedia.org/wiki?curid=50899238", "title": "Tornadoes of 1962", "text": "Tornadoes of 1962\n\nThis page documents the tornadoes and tornado outbreaks of 1962, primarily in the United States. Most tornadoes form in the U.S., although some events may take place internationally. Tornado statistics for older years like this often appear significantly lower than modern years due to fewer reports or confirmed tornadoes.\n"}
{"id": "34422", "url": "https://en.wikipedia.org/wiki?curid=34422", "title": "Zirconium", "text": "Zirconium\n\nZirconium is a chemical element with symbol Zr and atomic number 40. The name \"zirconium\" is taken from the name of the mineral zircon, the most important source of zirconium. It is a lustrous, grey-white, strong transition metal that closely resembles hafnium and, to a lesser extent, titanium. Zirconium is mainly used as a refractory and opacifier, although small amounts are used as an alloying agent for its strong resistance to corrosion. Zirconium forms a variety of inorganic and organometallic compounds such as zirconium dioxide and zirconocene dichloride, respectively. Five isotopes occur naturally, three of which are stable. Zirconium compounds have no known biological role.\n\nZirconium is a lustrous, greyish-white, soft, ductile, malleable metal that is solid at room temperature, though it is hard and brittle at lesser purities. In powder form, zirconium is highly flammable, but the solid form is much less prone to ignition. Zirconium is highly resistant to corrosion by alkalis, acids, salt water and other agents. However, it will dissolve in hydrochloric and sulfuric acid, especially when fluorine is present. Alloys with zinc are magnetic at less than 35 K.\n\nThe melting point of zirconium is 1855 °C (3371 °F), and the boiling point is 4371 °C (7900 °F). Zirconium has an electronegativity of 1.33 on the Pauling scale. Of the elements within the d-block with known electronegativities, zirconium has the fifth lowest electronegativity after hafnium, yttrium, lanthanum, and actinium.\n\nAt room temperature zirconium exhibits a hexagonally close-packed crystal structure, α-Zr, which changes to β-Zr, a body-centered cubic crystal structure, at 863 °C. Zirconium exists in the β-phase until the melting point.\n\nNaturally occurring zirconium is composed of five isotopes. Zr, Zr, Zr and Zr are stable, although Zr is predicted to undergo double beta decay (not observed experimentally) with a half-life of more than 1.10×10 years. Zr has a half-life of 2.4×10 years, and is the longest-lived radioisotope of zirconium. Of these natural isotopes, Zr is the most common, making up 51.45% of all zirconium. Zr is the least common, comprising only 2.80% of zirconium.\n\nTwenty-eight artificial isotopes of zirconium have been synthesized, ranging in atomic mass from 78 to 110. Zr is the longest-lived artificial isotope, with a half-life of 1.53×10 years. Zr, the heaviest isotope of zirconium, is the most radioactive, with an estimated half-life of 30 milliseconds. Radioactive isotopes at or above mass number 93 decay by electron emission, whereas those at or below 89 decay by positron emission. The only exception is Zr, which decays by electron capture.\n\nFive isotopes of zirconium also exist as metastable isomers: Zr, Zr, Zr, Zr, Zr and Zr. Of these, Zr has the shortest half-life at 131 nanoseconds. Zr is the longest lived with a half-life of 4.161 minutes.\n\nZirconium has a concentration of about 130 mg/kg within the Earth's crust and about 0.026 μg/L in sea water. It is not found in nature as a native metal, reflecting its intrinsic instability with respect to water. The principal commercial source of zirconium is zircon (ZrSiO), a silicate mineral, which is found primarily in Australia, Brazil, India, Russia, South Africa and the United States, as well as in smaller deposits around the world. As of 2013, two-thirds of zircon mining occurs in Australia and South Africa. Zircon resources exceed 60 million tonnes worldwide and annual worldwide zirconium production is approximately 900,000 tonnes. Zirconium also occurs in more than 140 other minerals, including the commercially useful ores baddeleyite and kosnarite.\n\nZirconium is relatively abundant in S-type stars, and it has been detected in the sun and in meteorites. Lunar rock samples brought back from several Apollo missions to the moon have a high zirconium oxide content relative to terrestrial rocks.\n\nZirconium is a by-product of the mining and processing of the titanium minerals ilmenite and rutile, as well as tin mining. From 2003 to 2007, while prices for the mineral zircon steadily increased from $360 to $840 per tonne, the price for unwrought zirconium metal decreased from $39,900 to $22,700 per ton. Zirconium metal is much higher priced than zircon because the reduction processes are expensive.\n\nCollected from coastal waters, zircon-bearing sand is purified by spiral concentrators to remove lighter materials, which are then returned to the water because they are natural components of beach sand. Using magnetic separation, the titanium ores ilmenite and rutile are removed.\n\nMost zircon is used directly in commercial applications, but a small percentage is converted to the metal. Most Zr metal is produced by the reduction of the zirconium(IV) chloride with magnesium metal in the Kroll process. The resulting metal is sintered until sufficiently ductile for metalworking.\n\nCommercial zirconium metal typically contains 1–3% of hafnium, which is usually not problematic because the chemical properties of hafnium and zirconium are very similar. Their neutron-absorbing properties differ strongly, however, necessitating the separation of hafnium from zirconium for nuclear reactors. Several separation schemes are in use. The liquid-liquid extraction of the thiocyanate-oxide derivatives exploits the fact that the hafnium derivative is slightly more soluble in methyl isobutyl ketone than in water. This method is used mainly in United States.\n\nZr and Hf can also be separated by fractional crystallization of potassium hexafluorozirconate (KZrF), which is less soluble in water than the analogous hafnium derivative.\n\nFractional distillation of the tetrachlorides, also called extractive distillation, is used primarily in Europe.\n\nThe product of a quadruple VAM (vacuum arc melting) process, combined with hot extruding and different rolling applications is cured using high-pressure, high-temperature gas autoclaving. This produces reactor-grade zirconium that is about 10 times more expensive than the hafnium-contaminated commercial grade.\n\nHafnium must be removed from zirconium for nuclear applications because hafnium has a neutron absorption cross-section 600 times greater than zirconium. The separated hafnium can be used for reactor control rods.\n\nLike other transition metals, zirconium forms a wide range of inorganic compounds and coordination complexes. In general, these compounds are colourless diamagnetic solids wherein zirconium has the oxidation state +4. Far fewer Zr(III) compounds are known, and Zr(II) is very rare.\n\nThe most common oxide is zirconium dioxide, ZrO, also known as \"zirconia\". This clear to white-coloured solid has exceptional fracture toughness and chemical resistance, especially in its cubic form. These properties make zirconia useful as a thermal barrier coating, although it is also a common diamond substitute. Zirconium monoxide, ZrO, is also known and S-type stars are recognised by detection of its emission lines in the visual spectrum.\n\nZirconium tungstate has the unusual property of shrinking in all dimensions when heated, whereas most other substances expand when heated. Zirconyl chloride is a rare water-soluble zirconium complex with the relatively complicated formula [Zr(OH)(HO)]Cl.\n\nZirconium carbide and zirconium nitride are refractory solids. The carbide is used for drilling tools and cutting edges. Zirconium hydride phases are also known.\n\nLead zirconate titanate (PZT) is the most commonly used piezoelectric material, with applications such as ultrasonic transducers, hydrophones, common rail injectors, piezoelectric transformers and micro-actuators.\n\nAll four common halides are known, ZrF, ZrCl, ZrBr, and ZrI. All have polymeric structures and are far less volatile than the corresponding monomeric titanium tetrahalides. All tend to hydrolyse to give the so-called oxyhalides and dioxides.\n\nThe corresponding tetraalkoxides are also known. Unlike the halides, the alkoxides dissolve in nonpolar solvents. Dihydrogen hexafluorozirconate is used in the metal finishing industry as an etching agent to promote paint adhesion.\n\nOrganozirconium chemistry is the study of compounds containing a carbon-zirconium bond. The first such compound was zirconocene dibromide ((CH)ZrBr), reported in 1952 by Birmingham and Wilkinson. Schwartz's reagent, prepared in 1970 by P. C. Wailes and H. Weigold, is a metallocene used in organic synthesis for transformations of alkenes and alkynes.\n\nZirconium is also a component of some Ziegler-Natta catalysts, used to produce polypropylene. This application exploits the ability of zirconium to reversibly form bonds to carbon. Most complexes of Zr(II) are derivatives of zirconocene, one example being (CMe)Zr(CO).\n\nThe zirconium-containing mineral zircon and related minerals (jargoon, hyacinth, jacinth, ligure) were mentioned in biblical writings. The mineral was not known to contain a new element until 1789, when Klaproth analyzed a jargoon from the island of Ceylon (now Sri Lanka). He named the new element Zirkonerde (zirconia). Humphry Davy attempted to isolate this new element in 1808 through electrolysis, but failed. Zirconium metal was first obtained in an impure form in 1824 by Berzelius by heating a mixture of potassium and potassium zirconium fluoride in an iron tube.\n\nThe \"crystal bar process\" (also known as the \"Iodide Process\"), discovered by Anton Eduard van Arkel and Jan Hendrik de Boer in 1925, was the first industrial process for the commercial production of metallic zirconium. It involves the formation and subsequent thermal decomposition of zirconium tetraiodide, and was superseded in 1945 by the much cheaper Kroll process developed by William Justin Kroll, in which zirconium tetrachloride is reduced by magnesium:\n\nApproximately 900,000 tonnes of zirconium ores were mined in 1995, mostly as zircon.\n\nMost zircon is used directly in high-temperature applications. This material is refractory, hard, and resistant to chemical attack. Because of these properties, zircon finds many applications, few of which are highly publicized. Its main use is as an opacifier, conferring a white, opaque appearance to ceramic materials. Because of its chemical resistance, zircon is also used in aggressive environments, such as moulds for molten metals.\n\nZirconium dioxide (ZrO) is used in laboratory crucibles, in metallurgical furnaces, and as a refractory material. Because it is mechanically strong and flexible, it can be sintered into ceramic knives and other blades. Zircon (ZrSiO) and the cubic zirconia (ZrO) are cut into gemstones for use in jewelry.\n\nZirconia is a component in some abrasives, such as grinding wheels and sandpaper.\n\nA small fraction of the zircon is converted to the metal, which finds various niche applications. Because of zirconium's excellent resistance to corrosion, it is often used as an alloying agent in materials that are exposed to aggressive environments, such as surgical appliances, light filaments, and watch cases. The high reactivity of zirconium with oxygen at high temperatures is exploited in some specialised applications such as explosive primers and as getters in vacuum tubes. The same property is (probably) the purpose of including Zr nano-particles as pyrophoric material in explosive weapons such as the BLU-97/B Combined Effects Bomb. Burning zirconium was used as a light source in some photographic flashbulbs.\n\nCladding for nuclear reactor fuels consumes about 1% of the zirconium supply, mainly in the form of zircaloys. The desired properties of these alloys are a low neutron-capture cross-section and resistance to corrosion under normal service conditions. Efficient methods for removing the hafnium impurities were developed to serve this purpose.\n\nOne disadvantage of zirconium alloys is that zirconium reacts with water at high temperatures, producing hydrogen gas and accelerated degradation of the fuel rod cladding:\nThis exothermic reaction is very slow below 100 °C, but at temperature above 900 °C the reaction is rapid. Most metals undergo similar reactions. The redox reaction is relevant to the instability of fuel assemblies at high temperatures. This reaction was responsible for a small hydrogen explosion first observed inside the reactor building of Three Mile Island nuclear power plant in 1979, but at that time, the containment building was not damaged. The same reaction occurred in the reactors 1, 2 and 3 of the Fukushima I Nuclear Power Plant (Japan) after the reactor cooling was interrupted by the earthquake and tsunami disaster of March 11, 2011 leading to the Fukushima I nuclear accidents. After venting the hydrogen in the maintenance hall of those three reactors, the mixture of hydrogen with atmospheric oxygen exploded, severely damaging the installations and at least one of the containment buildings. To avoid explosion, the direct venting of hydrogen to the open atmosphere would have been a preferred design option. Now, to prevent the risk of explosion in many pressurized water reactor (PWR) containment buildings, a catalyst-based recombiner is installed that converts hydrogen and oxygen into water at room temperature before the hazard arises.\n\nMaterials fabricated from zirconium metal and ZrO are used in space vehicles where resistance to heat is needed.\n\nHigh temperature parts such as combustors, blades, and vanes in jet engines and stationary gas turbines are increasingly being protected by thin ceramic layers, usually composed of a mixture of zirconia and yttria.\n\nThe isotope Zr has been applied to the tracking and quantification of molecular antibodies with positron emission tomography (PET) cameras (a method called \"immuno-PET\"). Immuno-PET has reached a maturity of technical development and is now entering the phase of wide-scale clinical applications. Until recently, radiolabeling with Zr was a complicated procedure requiring multiple steps. In 2001–2003 an improved multistep procedure was developed using a succinylated derivative of desferrioxamine B (N-sucDf) as a bifunctional chelate, and a better way of binding Zr to mAbs was reported in 2009. The new method is fast, consists of only two steps, and uses two widely available ingredients: Zr and the appropriate chelate.\n\nZirconium-bearing compounds are used in many biomedical applications, including dental implants and crowns, knee and hip replacements, middle-ear ossicular chain reconstruction, and other restorative and prosthetic devices.\n\nZirconium binds urea, a property that has been utilized extensively to the benefit of patients with chronic kidney disease. For example, zirconium is a primary component of the sorbent column dependent dialysate regeneration and recirculation system known as the REDY system, which was first introduced in 1973. More than 2,000,000 dialysis treatments have been performed using the sorbent column in the REDY system. Although the REDY system was superseded in the 1990s by less expensive alternatives, new sorbent-based dialysis systems are being evaluated and approved by the U.S. Food and Drug Administration (FDA). Renal Solutions developed the DIALISORB technology, a portable, low water dialysis system. Also, developmental versions of a Wearable Artificial Kidney have incorporated sorbent-based technologies.\n\nSodium zirconium cyclosilicate is under investigation for oral therapy in the treatment of hyperkalemia. It is a highly selective oral sorbent designed specifically to trap potassium ions in preference to other ions throughout the gastrointestinal tract.\n\nA mixture of monomeric and polymeric Zr and Al complexes with hydroxide, chloride and glycine, called Aluminium zirconium tetrachlorohydrex gly or AZG, is used in a preparation as an antiperspirant in many deodorant products. It is selected for its ability to obstruct pores in the skin and prevent sweat from leaving the body.\n\nZirconium carbonate (3ZrO·CO·HO) was used in lotions to treat poison ivy but was discontinued because it occasionally caused skin reactions.\n\nAlthough zirconium has no known biological role, the human body contains, on average, 250 milligrams of zirconium, and daily intake is approximately 4.15 milligrams (3.5 milligrams from food and 0.65 milligrams from water), depending on dietary habits. Zirconium is widely distributed in nature and is found in all biological systems, for example: 2.86 μg/g in whole wheat, 3.09 μg/g in brown rice, 0.55 μg/g in spinach, 1.23 μg/g in eggs, and 0.86 μg/g in ground beef. Further, zirconium is commonly used in commercial products (e.g. deodorant sticks, aerosol antiperspirants) and also in water purification (e.g. control of phosphorus pollution, bacteria- and pyrogen-contaminated water).\n\nShort-term exposure to zirconium powder can cause irritation, but only contact with the eyes requires medical attention. Persistent exposure to zirconium tetrachloride results in increased mortality in rats and guinea pigs and a decrease of blood hemoglobin and red blood cells in dogs. However, in a study of 20 rats given a standard diet containing ~4% zirconium oxide, there were no adverse effects on growth rate, blood and urine parameters, or mortality. The U.S. Occupational Safety and Health Administration (OSHA) legal limit (permissible exposure limit) for zirconium exposure is 5 mg/m over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) recommended exposure limit (REL) is 5 mg/m over an 8-hour workday and a short term limit of 10 mg/m. At levels of 25 mg/m, zirconium is immediately dangerous to life and health. However, zirconium is not considered an industrial health hazard. Furthermore, reports of zirconium-related adverse reactions are rare and, in general, rigorous cause-and-effect relationships have not been established. No evidence has been validated that zirconium is carcinogenic or genotoxic.\n\nAmong the numerous radioactive isotopes of zirconium, Zr is among the most common. It is released as a product of U, mainly in nuclear plants and during nuclear weapons tests in the 1950s and 1960s. It has a very long half-life (1.53 million years), its decay emits only low energy radiations, and it is not considered as highly hazardous.\n\n"}
{"id": "39651912", "url": "https://en.wikipedia.org/wiki?curid=39651912", "title": "Świnoujście LNG terminal", "text": "Świnoujście LNG terminal\n\nŚwinoujście LNG terminal (also referred as Terminal LNG in Świnoujście, Polskie LNG or Baltic LNG) is a liquefied natural gas import terminal at Świnoujście, Poland. It is operated by Polskie LNG S.A., a subsidiary of Gaz-System.\n\nDiscussions about the project started in 2006. The project was originally developed by PGNiG through its subsidiary Polskie LNG S.A. In January 2008, SNC-Lavalin was chosen for the front-end engineering design. The engineering, procurement and construction contract was signed with a consortium of Saipem, Techint, Snamprogetti, and PBG. Construction started in March 2011. After creation of Gaz-System and its separation from PGNiG, the newly created company took over the ownership of Polskie LNG S.A. The terminal was inaugurated by prime minister Ewa Kopacz on 12 October 2015.\n\nThe terminal has unloading jetty for large LNG tankers, two storage tanks and regasification train. The terminal's initial regasification capacity is , and with the construction of the third tank its capacity is due to expand to reach satisfying approximately 50% of Poland's annual gas demand. The total cost of the terminal is €950 million (PLN 3.5billion). The first LNG delivery to the terminal is expected on 11 December 2015.\n\nThere is a plan to create a gas corridor from the Świnoujście terminal to Adria LNG terminal in Croatia.\n\n"}
{"id": "37403098", "url": "https://en.wikipedia.org/wiki?curid=37403098", "title": "Šumarski list", "text": "Šumarski list\n\nŠumarski list is one of the oldest, still-publishing forestry journals in the world. It was established in October 1876 and is published by the Croatian Forestry Society.\n\n"}
