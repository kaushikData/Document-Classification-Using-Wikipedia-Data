{"id": "8023439", "url": "https://en.wikipedia.org/wiki?curid=8023439", "title": "Anti-shock body", "text": "Anti-shock body\n\nAn anti-shock body (also known as Whitcomb body or Küchemann carrot) is a pod positioned on the leading edge or trailing edge of an aircraft's aerodynamic surfaces to reduce wave drag at transonic speeds (Mach 0.8–1.0).\n\nMost jet airliners have a cruising speed between Mach 0.8 and 0.85. Wave drag can be minimized, for aircraft operating in the transonic regime (Mach 0.8–1.0), by having a cross-sectional area which changes smoothly along the length of the aircraft. This is known as the area rule, and is the operating principle behind the design of anti-shock bodies. Reducing wave drag improves fuel efficiency.\n\nAnti-shock bodies, apparently developed in the Soviet Union's TsAGI research institute and serving a dual purpose as undercarriage bogie fairings, were first applied to the Tupolev Tu-16 jet bomber. The Tu-16 flew for the first time in April 1952, and the bodies were to remain a unique feature of the company's products up to and including the Tupolev Tu-154 designed in the mid-1960s.\n\nOutside the Soviet Union the theory was developed in the early 1950s by Richard Whitcomb at NASA, and Dietrich Küchemann at the British Royal Aircraft Establishment, leading to their appearance on aircraft such as the Convair 990 and the Handley Page Victor bomber. They became known as \"Whitcomb bodies\" or \"Küchemann carrots\".\n\nModern jet aircraft use supercritical airfoils, which decrease wave drag significantly. Further optimization can be achieved through careful design of wing body fairings, engine nacelles, flap track fairings, and wingtip fuel tanks. Due to the prevalence of supercritical airfoils, other design features with the sole purpose of minimizing wave drag (such as the anti-shock bodies on the Convair 990) are rare in modern aircraft.\n\nOn the Hawker Sea Hawk, the prototypes experienced high tail drag when approaching transonic speeds. To resolve this, an anti-shock body was subsequently fitted to the leading edge of the junction between the vertical fin and tailplane.\n\n"}
{"id": "23268911", "url": "https://en.wikipedia.org/wiki?curid=23268911", "title": "Asphalt roll roofing", "text": "Asphalt roll roofing\n\nAsphalt roll roofing or membrane is a roofing material commonly used for buildings that feature a low sloped roof pitch in North America. The material is based on the same materials used in asphalt shingles; an organic felt or fiberglass mat, saturated with asphalt, and faced with granular stone aggregate. \n\nRoll roofing is usually restricted to a lightweight mat compared to shingles, as it must be rolled for shipment. Rolls are typically by in size. Due to its light weight compared to shingles, roll roofing is regarded as an inexpensive, temporary material. Its broad width makes it vulnerable to temperature-induced shrinkage and tearing as it expands and contracts.\n\nOther names for this material are \"asphalt prepared roofing, asphaltic felt, cold-process roofing, prepared roofing, rolled roofing, rolled strip roofing, roofing felt, sanded bituminous felt, saturated felt, self-finished roofing felt.\"\n\nRoll roofing is normally applied parallel to the eaves from the bottom of the roof upwards, lapping each new roll in the same manner as shingles. Its use is restricted to roofs with a pitch of less than 2:12. To avoid penetrating the exposed membrane with nails, adhesive or \"lap cement\" must be used at the bottom edge to keep it from being lifted by the wind. The upper edge of the roll is nailed and covered by the next roll.\n\nThe main uses are:\n\nSeveral variations of bitumen roofing felt are available. \n\nFibre content:\n\nBitumen:\n\nUnderside:\n\nTopping:\n\nDouble coverage:\n\n\nhttp://wiki.diyfaq.org.uk/index.php?title=Roofing_felt\n"}
{"id": "28513579", "url": "https://en.wikipedia.org/wiki?curid=28513579", "title": "Athanase Dupré", "text": "Athanase Dupré\n\nLouis Victoire Athanase Dupré (28 December 1808 – 10 August 1869) was a French mathematician and physicist noted for his 1860s publications on the mechanical theory of heat (thermodynamics); work that was said to have inspired the publications of engineer Francois Massieu and his Massieu functions; which in turn inspired the work of American engineer Willard Gibbs and his fundamental equations.\n\n"}
{"id": "29585048", "url": "https://en.wikipedia.org/wiki?curid=29585048", "title": "Atomenergomash", "text": "Atomenergomash\n\nAtomenergomash (AEM) () is a power engineering Company located in Russia. Аtomenergomash JSC (AEM Holding company, AEM, Group) is one of the leading Russian power engineering companies, a supplier of effi cient integrated solutions for nuclear and thermal power plants, natural gas and petrochemical industry, shipbuilding, and special steel market.\n<br>It is the mechanical engineering division of Rosatom. Together with its subsidiaries it employees more than 17,000 people. CEO of Atomenergomash is Andrey NIKIPELOV.\n\nAEM was established in 2006 as part of Rosatom State Atomic Energy Corporation.\n<br>The Holding includes about 30 power engineering companies, R&D, manufacturing, construction and construction companies located in Russia,\nUkraine, the Czech Republic, and Hungary.\n<br>The Holding’s equipment is installed in more than 20 countries. 13% of global Nuclear Power Plants (NPP) and 40% of Thermal Power Plants (TPP) in Russia and the former Soviet Union countries run our equipment.\n<br>The Holding employs over 17,000 people. Order backlog USD 7,4 bln\n\nAtomenergomash is the largest power engineering holdings in Russia, that offers a full range of solutions in the areas of design, manufacture, and supply of equipment for nuclear and thermal energy, gas and petrochemical, shipbuilding industries, and the market of special steels.\n<br>ATOMENERGOMASH enterprises have all equipment and qualifications required for performance of all process operations from foundry to testing and packaging of finished products.\n<br>MANUFACTURING CAPABILITIES\n<br>CHARACTERISTICS OF MANUFACTURING EQUIPMENT\n\nAtomenergomash incorporates the following companies:\n\nAtomenergomash's subsidiary, ZiO-Podolsk (51% owned by Atomenergomash), is the only manufacturer of steam generators for nuclear plants in Russia. Atomenergomash has also a joint venture with Alstom to manufacture \"Arabelle\" steam turbine and generators. It has also subsidiaries in Hungary and the Czech Republic. In October 2010, Atomenergomash and Ukraine's nuclear power company Energoatom agreed to establish a consortium for production of equipment for Ukrainian nuclear power plants. In November 2010, Atomenergomash announced its plan to start manufacturing wind turbines and developing wind farms.\n\n"}
{"id": "12805204", "url": "https://en.wikipedia.org/wiki?curid=12805204", "title": "Balakovo Nuclear Power Plant", "text": "Balakovo Nuclear Power Plant\n\nBalakovo nuclear power station ( []) is located in the city of Balakovo, Saratov Oblast, Russia, about 900 kilometers south-east of Moscow. It consists of four operational reactors; a fifth unit is still under construction. Owner and operator of the nuclear power station is Rosenergoatom.\n\nBalakovo NPP participates in a twinning program between nuclear power stations in Europe and Russia; since 1990 it has been in partnership with Biblis Nuclear Power Plant.\n\nThe Balakovo Nuclear Power Plant has four operating units:\n\nIn 2018 Rosatom announced it had developed a thermal annealing technique for reactor pressure vessels which ameliorates radiation damage and extends service life by between 15 and 30 years. This had been demonstrated on unit 1.\n\nOn 27 June 1985 during startup of the first reactor unit, a human error (later attributed to inexperience and haste) unexpectedly opened a pressurizer relief valve, and 300 °C steam entered the staff work area. Fourteen people were killed. This event is cited as one of the predecessors of the Chernobyl disaster.\n\n\n"}
{"id": "41176389", "url": "https://en.wikipedia.org/wiki?curid=41176389", "title": "Buildings Energy Efficiency Ordinance", "text": "Buildings Energy Efficiency Ordinance\n\nTo improve building energy efficiency, the Hong Kong Government formulated a Buildings Energy Efficiency Ordinance (建築物能源效益條例) which was passed by the Legislative Council in November 2010. Under the Ordinance, certain prescribed types of buildings have to comply with Building Energy Code (BEC) and/or Energy Audit Code (EAC). The Ordinance came into full operation on 21 September 2012.\n\nUnder the Ordinance, building services installations including electrical, air-conditioning, lighting and lift and escalator installations in newly constructed buildings are required to meet the minimum energy efficiency standards and requirements as specified in the Code of Practice for Energy Efficiency of Building Services Installation. Existing buildings will also be required to comply with the requirement when undergoing major retrofitting works. The standards stipulated in the Code, which was published in February 2012 are more stringent than those in the last version promulgated in 2007, which have been implemented on a voluntary basis. Most of the new standards are comparable to those adopted in the US, Europe and the Asia-Pacific region, while some standards are not specified in overseas jurisdictions.\n\nIn addition, the central building services installations of commercial buildings and commercial portions of composite buildings are required to carry out energy audits in accordance with the Code of Practice for Building Energy Audit every 10 years, and the results have to be displayed in a conspicuous position at the main entrance of the buildings concerned for public inspection.\n\nImplementation of the Ordinance is operated by the Electrical and Mechanical Services Department\n"}
{"id": "7849878", "url": "https://en.wikipedia.org/wiki?curid=7849878", "title": "Candle warmer", "text": "Candle warmer\n\nA candle warmer is an electric warmer that melts a candle or scented wax to release its scent. The candle warmer shown is intended to be used with jar candles or candles in cups, not with taper candles or candles without containers large enough to accommodate all the melted wax. Some candle warmers have a built-in bowl in which the candle is placed.\n\nThe advantages of using a candle warmer include the absence of open flame and the soot that often results from burning wax. The main disadvantage of a plate candle warmer is candle life. While the candle can still be burned (provided the wick is still exposed), the wax no longer contains any fragrances. Many warmers are designed to be used with \"wickless\" candles, which are blocks or lumps of scented candle wax with no wick. It is also used to keep hot beverages such as coffee warm by placing the mug on the warmer.\n"}
{"id": "56731778", "url": "https://en.wikipedia.org/wiki?curid=56731778", "title": "Dirac matter", "text": "Dirac matter\n\nThe term Dirac matter refers to a class of condensed matter systems which can be effectively described by the Dirac equation. Even though the Dirac equation itself was formulated for fermions,\nthe quasi-particles present within Dirac matter can be of any statistics. As a consequence, Dirac matter can be distinguished in fermionic, bosonic or anyonic Dirac matter. Prominent examples of Dirac matter are Graphene, topological insulators, Dirac semimetals, Weyl semimetals, various high-temperature superconductors with formula_1-wave pairing and liquid Helium-3. The effective theory of such systems is classified by a specific choice of the Dirac mass, the Dirac velocity, the Dirac matrices and the space-time curvature. The universal treatment of the class of Dirac matter in terms of an effective theory leads to a common features with respect to the density of states, the heat capacity and impurity scattering.\n\nMembers of the class of Dirac matter differ significantly in nature. However, all examples of Dirac matter are unified by similarities within the algebraic structure of an effective theory describing them. \n\nThe general definition of Dirac matter is a condensed matter system where the quasi-particle excitations can be described in curved spacetime by the generalised Dirac equation:\nIn the above definition formula_3 denotes a covariant vector depending on the formula_4-dimensional momentum formula_5 (formula_1 space formula_7 time dimension), formula_8 is the vierbein describing the curvature of the space, formula_9 the quasi-particle mass and formula_10 the Dirac velocity. Note that since in Dirac matter the Dirac equation gives the effective theory of the quasiparticles, the energy from the mass term is formula_11, not the rest mass formula_12 of a massive particle. formula_13 refers to a set of Dirac matrices, where the defining for the construction is given by the anticommutation relation,\nformula_15 is the Minkowski metric with signature (+ - - -) and formula_16 is the formula_17-dimensional unit matrix.\nIn all equations, implicit summation over formula_18 and formula_19 is used (Einstein convention). Furthermore, formula_20 is the wavefunction. The unifying feature of all Dirac matter is the matrix structure of the equation describing the quasi-particle excitations.\n\nIn the limit where formula_21, i.e. the covariant derivative, conventional Dirac matter is obtained. However, this general definition allows the description of matter with higher order dispersion relations and in curved spacetime as long as the effective Hamiltonian exhibits the matrix structure specific to the Dirac equation.\n\nThe majority of experimental realisations of Dirac matter to date are in the limit of formula_22 which therefore defines conventional Dirac matter in which the quasiparticles are described by the Dirac equation in curved space-time,\nHere, formula_24 denotes the covariant derivative. As an example, for the flat metric, the energy of a free Dirac particle differs significantly from the classical kinetic energy where energy is proportional to momentum squared:\nHere, formula_26 is the cyclotron frequency which is linearly dependent of the applied magnetic field and the charge of the particle. There are two distinct features between the Landau level quantization for 2D Schrödinger fermions (ordinary matter) and 2D Dirac fermions. First, the energy for Schrödinger fermions is linearly dependent with respect to the integer quantum number formula_27, whereas it exhibits a square-root dependence for the Dirac fermions. This key difference plays an important role in the experimental verification of Dirac matter . Furthermore, for formula_28 there exists a 0 energy level for Dirac fermions which is independent of the cyclotron frequency formula_26 and with that of the applied magnetic field. For example, the existence of the zeroth Landau level gives rise to a quantum Hall effect where the Hall conductance in quantized at half-integer values .\n\nIn the context of Fermionic quasiparticles, the Dirac velocity is identical to the Fermi velocity; in bosonic systems, no Fermi velocity exists, so the Dirac velocity is a more general property of such systems.\n\nGraphene is a 2-dimensional crystalline allotrope of carbon, where the carbon atoms are arranged on an hexagonal lattice.\nEach carbon atom forms formula_30-bonds to the three neighboring atoms that lie in the graphene plane at angles of 120formula_31. These bonds are mediated by three of carbon's four electrons while the fourth electron, which occupies a formula_32 orbital, mediates an out-of-plane -bond that leads to the electronic bands at the Fermi level. The unique transport properties and the semimetallic state of graphene are the result of the delocalized electrons occupying these pformula_33 orbitals .\n\nThe semimetallic state corresponds to a linear crossing of energy bands at the formula_34 and formula_35 points of graphene's hexagonal Brillouin zone. At these two points, the electronic structure can be effectively described by the Hamiltonian\nHere, formula_37 and formula_38 are two of the three Pauli matrices.\nThe factor formula_39 indicates whether the Hamiltonian describes is centred on the formula_34 or formula_35 valley at the corner of hexagonal Brillouin zone. For graphene, the Dirac velocity is about formula_42 eV formula_43. An energy gap in the dispersion of graphene can be obtained from a low-energy Hamiltionain of the form \nwhich now contains a mass term formula_45. There are several distinct ways of introducing a mass term, and the results have different characteristics. The most practical approach for creating a gap (introducing a mass term) is to break the sublattice symmetry of the lattice where each carbon atom is slightly different to its nearest but identical to its next-nearest neighbours; an effect that may result from substrate effects.\n\nA topological insulator is a material that behaves as an insulator in its interior (bulk) but whose surface contains conducting states. This property represents a non-trivial, symmetry protected topological order. As a consequence, electrons in topological insulators can only move along the surface of the material. In the bulk of a non-interacting topological insulator, the Fermi level is positioned within the gap between the conduction and valence bands. On the surface, there are special states within the bulk energy gap which can be effectively described by a Dirac Hamiltonian: \nwhere formula_47 is normal to the surface and formula_48 is in the real spin basis. However, if we rotate spin by a unitary operator, formula_49, we will end up with the standard notation of Dirac Hamiltonian, formula_50. \nSuch Dirac cones emerging on the surface of 3-dimensional crystals were observed in experiment, e.g.: bismuth selenide (Biformula_51Siformula_52) , tin telluride (SnTe) .\n\nThe low-energy properties some semiconducting transition metal dichalcogenide monolayers,\ncan be described by a two-dimensional massive (gapped) Dirac Hamiltonian with an additional term describing a strong spin–orbit coupling: \n\nThe spin-orbit coupling formula_54 provides a large spin-splitting in the valence band and formula_55 indicates the spin degree of freedom. As for graphene, formula_56 gives the valley degree of freedom - whether near the formula_34 or formula_58 point of the hexagonal Brillouin zone. Transition metal dichalcogenide monolayers are often discussed in reference to potential applications in valleytronics.\n\nWeyl semimetals, for example strontium silicide (SrSiformula_51), Tantalum arsenide (TaAs) , have a Hamiltonian that is very similar to that of graphene, but now includes all three Pauli matrices and the linear crossings occur in 3D:\nSince all three Pauli matrices are present, there is no further Pauli matrix that could open a gap in the spectrum and Weyl points are therefore topologically protected. Tilting of the linear cones so the Dirac velocity varies leads to type II Weyl semimetals .\nOne distinct, experimentally observable feature of Weyl semimetals is that the surface states form Fermi arcs since the Fermi surface does not form a closed loop.\n\nIn crystals that are symmetric under inversion and time reversal, electronic energy bands are two-fold degenerate. This degeneracy is referred to as Kramers degeneracy. Therefore semimetals with linear crossings of two energy bands (two-fold degeneracy) at the Fermi energy exhibit a four-fold degeneracy at the crossing point. The effective Hamiltonian for these states can be written as \nThis has exactly the matrix structure of Dirac matter. Examples of experimentally realised Dirac semimetals are sodium bismuthide (Naformula_52Bi) and cadmium arsenide (Cdformula_52Asformula_51) \n\nWhile historic interest focussed on fermionic quasiparticles that have potential for technological applications, particularly in electronics, the mathematical structure of the Dirac equation is not restricted to the statistics of the particles. This has led to recent development of the concept of bosonic Dirac matter.\n\nIn the case of bosons, there is no Pauli exclusion principle to confine excitations close to the chemical potential (Fermi energy for fermions) so the entire Brillouin zone must be included. At low temperatures, the bosons will collect at the lowest energy point, the formula_65-point of the lower band. Energy must be added to excite the quasiparticles to the vicinity of the linear crossing point.\n\nSeveral examples of Dirac matter with fermionic quasi-particles occur in systems where there is a hexagonal crystal lattice; so bosonic quasiparticles on an hexagonal lattice are the natural candidates for bosonic Dirac matter. In fact, the underlying symmetry of a crystal structure strongly constrains and protects the emergence of linear band crossings. Typical bosonic quasiparticles in condensed matter are magnons, phonons, polaritons and plasmons.\n\nExisting examples of bosonic Dirac matter include transition metal halides such as CrXformula_52 (X= Cl, Br, I), where the magnon spectrum exhibits linear crossings , granular superconductors in a honeycomb lattice and hexagonal arrays of semiconductor microcavities hosting microcavity polaritons with linear crossings . Like graphene, all these systems have an hexagonal lattice structure.\n\nAnyonic Dirac matter is a hypothetical field which is rather unexplored to date. An anyon is a type of quasiparticle that can only occur in two-dimensional systems. Considering bosons and fermions, the interchange of two particles contributes a factor of 1 or -1 to the wave function. In contrast, the operation of exchanging two identical anyons causes a global phase shift. Anyons are generally classified as \"abelian\" or \"non-abelian,\" according to whether the elementary excitations of the theory transform under an abelian representation of the braid group or a non-abelian one. Abelian anyons have been detected in connection to the fractional quantum Hall effect. The possible construction of anyonic Dirac matter relies on the symmetry protection of crossings of anyonic energy bands. In comparison to bosons and fermions the situation gets more complicated as translations in space do not necessarily commute. Additionally, for given spatial symmetries, the group structure describing the anyon strongly depends on the specific phase of the anyon interchange. For example, for bosons, a rotation of a particle about 2 i.e., 360formula_31, will not change its wave function. For fermions, a rotation of a particle about 2, will contribute a factor of formula_68 to its wave function, whereas a 4 rotation, i.e., a rotation about 720formula_31, will give the same wave function as before. For anyons, an even higher degree of rotation can be necessary, e.g., 6, 8, etc, to leave the wave function invariant.\n"}
{"id": "1049602", "url": "https://en.wikipedia.org/wiki?curid=1049602", "title": "Direct methanol fuel cell", "text": "Direct methanol fuel cell\n\nDirect-methanol fuel cells or DMFCs are a subcategory of proton-exchange fuel cells in which methanol is used as the fuel. Their main advantage is the ease of transport of methanol, an energy-dense yet reasonably stable liquid at all environmental conditions.\n\nEfficiency is quite low for these cells, so they are targeted especially to portable applications, where energy and power density are more important than efficiency.\n\nA more efficient version of a direct fuel cell would play a key role in the theoretical use of methanol as a general energy transport medium, in the hypothesized methanol economy.\n\nIn contrast to indirect methanol fuel cells, where methanol is reacted to hydrogen by steam reforming, DMFCs use a methanol solution (usually around 1M, i.e. about 3% in mass) to carry the reactant into the cell; common operating temperatures are in the range 50–120 °C, where high temperatures are usually pressurized.\nDMFCs themselves are more efficient at high temperatures and pressures, but these conditions end up causing so many losses in the complete system that the advantage is lost; therefore, atmospheric-pressure configurations are currently preferred.\n\nBecause of the methanol cross-over, a phenomenon by which methanol diffuses through the membrane without reacting, methanol is fed as a weak solution: this decreases efficiency significantly, since crossed-over methanol, after reaching the air side (the cathode), immediately reacts with air; though the exact kinetics are debated, the end result is a reduction of the cell voltage.\nCross-over remains a major factor in inefficiencies, and often half of the methanol is lost to cross-over. Methanol cross-over and/or its effects can be alleviated by (a) developing alternative membranes (e.g.), (b) improving the electro-oxidation process in the catalyst layer and improving the structure of the catalyst and gas diffusion layers (e.g. ), and (c) optimizing the design of the flow field and the membrane electrode assembly (MEA) which can be achieved by studying the current density distributions (e.g. ).\n\nOther issues include the management of carbon dioxide created at the anode, the sluggish dynamic behavior, and the ability to maintain the solution water.\n\nThe only waste products with these types of fuel cells are carbon dioxide and water.\n\nCurrent DMFCs are limited in the power they can produce, but can still store a high energy content in a small space. This means they can produce a small amount of power over a long period of time. This makes them ill-suited for powering large vehicles (at least directly), but ideal for smaller vehicles such as forklifts and tuggers and consumer goods such as mobile phones, digital cameras or laptops. Military applications of DMFCs are an emerging application since they have low noise and thermal signatures and no toxic effluent. These applications include power for man-portable tactical equipment, battery chargers, and autonomous power for test and training instrumentation. Units are available with power outputs between 25 watts and 5 kilowatts with durations up to 100 hours between refuelings.\n\nMethanol is a liquid from -97.0 °C to 64.7 °C at atmospheric pressure.\nThe energy density of methanol is an order of magnitude greater than even highly compressed hydrogen, and 15 times higher than Lithium-ion batteries.\n\nMethanol is toxic and flammable.\nHowever, the International Civil Aviation Organization's (ICAO) Dangerous Goods Panel (DGP) voted in November 2005 to allow passengers to carry and use micro fuel cells and methanol fuel cartridges when aboard airplanes to power laptop computers and other consumer electronic devices.\nOn September 24, 2007, the US Department of Transportation issued a proposal to allow airline passengers to carry fuel cell cartridges on board.\nThe Department of Transportation issued a final ruling on April 30, 2008, permitting passengers and crew to carry an approved fuel cell with an installed methanol cartridge and up to two additional spare cartridges.\nIt is worth noting that 200 ml maximum methanol cartridge volume allowed in the final ruling is double the 100 ml limit on liquids allowed by the Transportation Security Administration in carry-on bags.\n\nThe DMFC relies upon the oxidation of methanol on a catalyst layer to form carbon dioxide. Water is consumed at the anode and is produced at the cathode. Protons (H) are transported across the proton exchange membrane - often made from Nafion - to the cathode where they react with oxygen to produce water. Electrons are transported through an external circuit from anode to cathode, providing power to connected devices.\n\nThe half-reactions are:\n\nMethanol and water are adsorbed on a catalyst usually made of platinum and ruthenium particles, and lose protons until carbon dioxide is formed. As water is consumed at the anode in the reaction, pure methanol cannot be used without provision of water via either passive transport such as back diffusion (osmosis), or active transport such as pumping. The need for water limits the energy density of the fuel.\n\nPlatinum is used as a catalyst for both half-reactions. This contributes to the loss of cell voltage potential, as any methanol that is present in the cathode chamber will oxidize. If another catalyst could be found for the reduction of oxygen, the problem of methanol crossover would likely be significantly lessened. Furthermore, platinum is very expensive and contributes to the high cost per kilowatt of these cells.\n\nDuring the methanol oxidation reaction carbon monoxide (CO) is formed, which strongly adsorbs onto the platinum catalyst, reducing the number of available reaction sites and thus the performance of the cell. The addition of other metals, such as ruthenium or gold, to the platinum catalyst tends to ameliorate this problem. In the case of platinum-ruthenium catalysts, the oxophilic nature of ruthenium is believed to promote the formation of hydroxyl radicals on its surface, which can then react with carbon monoxide adsorbed on the platinum atoms. The water in the fuel cell is oxidized to a hydroxy radical via the following reaction: HO → OH• + H + e. The hydroxy radical then oxidizes carbon monoxide to produce carbon dioxide, which is released from the surface as a gas: CO + OH• → CO + H + e.\n\nUsing these OH groups in the half reactions, they are also expressed as:\n\nMethanol on the anodic side is usually in a weak solution (from 1M to 3M), because methanol in high concentrations has the tendency to diffuse through the membrane to the cathode, where its concentration is about zero because it is rapidly consumed by oxygen. Low concentrations help in reducing the cross-over, but also limit the maximum attainable current.\n\nThe practical realization is usually that a solution loop enters the anode, exits, is refilled with methanol, and returns to the anode again. Alternatively, fuel cells with optimized structures can be directly fed with high concentration methanol solutions or even pure methanol.\n\nThe water in the anodic loop is lost because of the anodic reaction, but mostly because of the associated water drag: every proton formed at the anode drags a number of water molecules to the cathode. Depending on temperature and membrane type, this number can be between 2 and 6.\n\nA direct methanol fuel cell is usually part of a larger system including all the ancillary units that permit its operation. Compared to most other types of fuel cells, the ancillary system of DMFCs is relatively complex. The main reasons for its complexity are:\n\n\n\n\n"}
{"id": "28179547", "url": "https://en.wikipedia.org/wiki?curid=28179547", "title": "Durant-Dort Carriage Company", "text": "Durant-Dort Carriage Company\n\nDurant-Dort Carriage Company was a manufacturer of horse-drawn vehicles in Flint, Michigan. Founded in 1886, by 1900 it was the largest carriage manufacturer in the country.\n\nThis very successful business made the partners rich men and it became the core on which William C Durant and J Dallas Dort began to build General Motors.\n\nDurant sold out of this business in 1914 and it stopped manufacturing carriages in 1917. Durant-Dort Carriage Company was dissolved in 1924.\n\nThe premises were taken over by J Dallas Dort's Dort Motor Car Company which he closed in 1924.\n\nIn 1886 William C. Durant rode in a friend's spring-suspension road-cart built by the Coldwater Road-Cart Company of Coldwater, Michigan. Impressed with the smoothness of the ride Durant went to Coldwater and bought the road-cart's patent and manufacturing rights from Schmedlin and O'Brien for $1500. With Josiah Dallas Dort as an equal partner he founded Flint Road-Cart. Dort as president handled administrative details for the firm and manufacturing arrangements — to begin with the carts were made for them by William A. Paterson — while Durant handled sales and promotion. Their first office was in Durant's fire insurance agency in downtown Flint.\n\nHe had bought the rights to the road-cart with borrowed money so newly married Durant immediately left Flint and set up a chain of jobbers to sell the carts as far away as Madison, Milwaukee and Chicago. With just one finished cart at home he returned from his first trip with orders for 600 road-carts. Flint Road-Cart sold 4000 carts its first year, and grew quickly from there.\n\nIn 1893 they incorporated Flint Road-Cart Company with a substantial capital, much of it raised from local investors, and leased a factory on Water Street originally used by the Flint Woolen Mills. There they assembled their road-carts from bought-in components. After that Flint Road-Cart expanded by starting or buying other businesses that produced not only vehicles, but the components for vehicles as well. They marketed them as \"Blue Ribbon Vehicles\".\n\nFlint Road-Cart Company changed its name to Durant-Dort Carriage Company in November 1895. By 1900 they were building 50,000 vehicles each year from around 14 locations and they were a major rival of Flint Wagon Works. In 1906 they were making 480 vehicles each day with 1,000 workers. Durant-Dort owned not just the Flint manufacturing works, but also other vehicle assembly plants in Michigan, Georgia, and Ontario, together with timberland, lumber mills, a wheel manufacturer, the Flint Axle Works, and the Flint Varnish Works.\n\nA separate business named Diamond Buggy Company was established in 1896 to built low-priced carts sold for cash only. The first plant manager was A.B.C. Hardy.\n\nDurant decided making their own components instead of buying them in would give Durant-Dort better control over costs and the ability to improve efficiency. \nAll component factories were relocated to Flint to further speed production. These extra activities placed a lot of pressure on Durant's friends. Dallas Dort left the business in 1898 and didn't return until 1900.\n\nHardy was sent on a tour of Europe in 1901. On that holiday he became fascinated by automobiles. In 1902 he established his Flint Automobile Company and built over fifty cars with Weston-Mott axles and W F Stewart bodies. The Association of Licensed Automobile Manufacturers demanded a licence fee of $50 for each engine Hardy had built so he ended production and \"moved to Iowa\".\n\nDurant-Dort continued making horse-drawn vehicles until 1917 but from 1915 the factory and office buildings refocused on the manufacture of Dort Motor Car Company automobiles. J. Dallas Dort began his own independent automotive business, Dort Motor Car Company, in 1915. Dort used the old Durant-Dort buildings but added more to them. Dort shipped 9000 cars in its first year. J Dallas Dort decided to retire and liquidated Dort Motor Car Company in 1924 and died the following year.\n\nAlexander Brownell Cullen Hardy (1869-1948) began working at Durant-Dort in 1889. By 1895, he was supervising production of the Diamond, a low-cost buggy. In 1898, J. Dallas Dort took a two-year leave of absence from his position as president of Durant-Dort, and Hardy stepped into his place. After Dort's return in 1900, Hardy took his own leave of absence, and while touring Europe discovered the automobile. On his return, he supposedly told Durant to \"get out of the carriage business before the automobile ruins you.\" Although Durant didn't act at the time, Hardy struck out on his own and established the Flint Automobile Company, Flint's first automotive manufacturer, in 1901. However, the company's Roadster failed to distinguish itself from the popular, lower-priced Oldsmobile, and in 1903 the Flint Automobile Company folded. Hardy returned to Durant-Dort and wound up as vice-president of General Motors until his retirement in 1925.\n\nDurant began to lose interest in Flint activities and set up an office in New York. A.B.C. Hardy tried to interest him in automobiles. Eventually James Whiting of Flint Wagon Works persuaded Durant to take what became a consuming interest in Flint Wagon Works' Buick automobile venture. Durant used his own capital and that of Durant-Dort to buy control of Buick. David Buick, already a minority partner in his own business, was left with a single share of his enterprise. However, Durant agreed to keep Buick on as an employee, and Buick remained with the firm until 1906, when Durant bought out his single share for $100,000.\n\nOther automobile pioneers were associated with the Durant-Dort Carriage Company. R. S. McLaughlin headed the McLaughlin Motor Car Company in Oshawa, Ontario. Its carriage builder parent was started in 1867 and by 1900 built more carriages than any other Canadian business. W. C. Durant and his Canadian-born son-in-law and business confidant, Dr. Edwin Campbell, were friends with the McLaughlins and they made cross stock-holdings in each other's automobile businesses. Campbell was a school friend of R. S. McLaughlin.\n\nCharles W. Nash began working at Durant-Dort in 1891 working in the cushion department, but soon worked his way up to foreman, and, by 1898, factory superintendent. Nash was named a director and vice-president of the firm in 1900, a position he held until 1913. In 1910, Nash was hired as general manager of General Motors, and in 1917 founded Nash Motors.\n\n"}
{"id": "272483", "url": "https://en.wikipedia.org/wiki?curid=272483", "title": "Effective radiated power", "text": "Effective radiated power\n\nEffective radiated power (ERP), synonymous with equivalent radiated power, is an IEEE standardized definition of directional radio frequency (RF) power, such as that emitted by a radio transmitter. It is the total power in watts that would have to be radiated by a half-wave dipole antenna to give the same radiation intensity (signal strength in watts per square meter) as the actual source at a distant receiver located in the direction of the antenna's strongest beam (main lobe). ERP measures the combination of the power emitted by the transmitter and the ability of the antenna to direct that power in a given direction. It is equal to the input power to the antenna multiplied by the gain of the antenna. It is used in electronics and telecommunications, particularly in broadcasting to quantify the apparent power of a broadcasting station experienced by listeners in its reception area.\n\nAn alternate parameter that measures the same thing is effective (or equivalent) isotropic radiated power (EIRP). Effective isotropic radiated power is the total power that would have to be radiated by a hypothetical isotropic antenna to give the same signal strength as the actual source in the direction of the antenna's strongest beam. The difference between EIRP and ERP is that ERP compares the actual antenna to a half-wave dipole antenna, while EIRP compares it to a theoretical isotropic antenna. Since a half-wave dipole antenna has a gain of 1.64, or 2.15 decibels compared to an isotropic radiator, if ERP and EIRP are expressed in watts their relation is \nIf they are expressed in decibels\n\nEffective radiated power and effective isotropic radiated power both measure the amount of power a radio transmitter and antenna (or other source of electromagnetic waves) radiates in a specific direction: in the direction of maximum signal strength (the \"main lobe\") of its radiation pattern. This maximum radiated power is dependent on two factors: the total power output and the radiation pattern of the antenna – how much of that power is radiated in the desired direction. The latter factor is quantified by the antenna gain, which is the ratio of the signal strength radiated by an antenna to that radiated by a standard antenna. For example, a 1,000-watt transmitter feeding an antenna with a gain of 4 (6 dBi) will have the same signal strength in the direction of its main lobe, and thus the same ERP and EIRP, as a 4,000-watt transmitter feeding an antenna with a gain of 1 (0 dBi). So ERP and EIRP are measures of radiated power that can compare different combinations of transmitters and antennas on an equal basis.\n\nThe difference between ERP and EIRP is that antenna gain has traditionally been measured in two different units, comparing the antenna to two different standard antennas; an isotropic antenna and a half-wave dipole antenna: \nIn contrast to an isotropic antenna, the dipole has a \"donut-shaped\" radiation pattern, its radiated power is maximum in directions perpendicular to the antenna, declining to zero on the antenna axis. Since the radiation of the dipole is concentrated in horizontal directions, the gain of a half-wave dipole is greater than that of an isotropic antenna. The isotropic gain of a half-wave dipole is 1.64, or in decibels 10 log 1.64 = 2.15 dBi, so\nIn decibels\n\nThe two measures EIRP and ERP are based on the two different standard antennas above:\n\n\nSince the two definitions of gain only differ by a constant factor, so do ERP and EIRP\nIn decibels\n\nThe transmitter is usually connected to the antenna through a transmission line. Since the transmission line may have significant losses formula_19, the power applied to the antenna is usually less than the output power of the transmitter formula_20. The relation of ERP and EIRP to transmitter output power is\nLosses in the antenna itself are included in the gain.\n\nAntenna gain is closely related to directivity and often incorrectly used interchangeably. However, gain is always less than directivity by a factor called radiation efficiency, η. Whereas directivity is entirely a function of wavelength and the geometry and type of antenna, gain takes into account the losses that always occur within a real antenna. Specifically, accelerating charge (time varying current) causes electromagnetic radiation per Maxwell's equations. Therefore, antennas use a current distribution on radiating elements to generate electromagnetic energy that propagates away from the antenna. This coupling is never 100% efficient (by Laws of Thermodynamics), and therefore antenna gain will always be less than directivity by this efficiency factor.\n\nBecause ERP is calculated as antenna gain (in a given direction) as compared with the maximum directivity of a half-wave dipole antenna, it creates a mathematically virtual effective dipole antenna oriented in the direction of the receiver. In other words, a notional receiver in a given direction from the transmitter would receive the same power if the source were replaced with an ideal dipole oriented with maximum directivity and matched polarization towards the receiver and with an antenna input power equal to the ERP. The receiver would not be able to determine a difference. Maximum directivity of an ideal half-wave dipole is a constant, i.e., 0 dBd = 2.15 dBi. Therefore, ERP is always 2.15 dB less than EIRP. The ideal dipole antenna could be further replaced by an isotropic radiator (a purely mathematical device which cannot exist in the real world), and the receiver cannot know the difference so long as the input power is increased by 2.15 dB.\n\nUnfortunately, the distinction between dBd and dBi is often left unstated and the reader is sometimes forced to infer which was used. For example, a Yagi-Uda antenna is constructed from several dipoles arranged at precise intervals to create better energy focusing (directivity) than a simple dipole. Since it is constructed from dipoles, often its antenna gain is expressed in dBd, but listed only as dB. Obviously this ambiguity is undesirable with respect to engineering specifications. A Yagi-Uda antenna's maximum directivity is 8.77 dBd = 10.92 dBi. Its gain necessarily must be less than this by the factor η, which must be negative in units of dB. Neither ERP nor EIRP can be calculated without knowledge of the power accepted by the antenna, i.e., it is not correct to use units of dBd or dBi with ERP and EIRP. Let us assume a 100-watt (20 dBW) transmitter with losses of 6 dB prior to the antenna. ERP < 22.77dBW and EIRP < 24.92dBW, both less than ideal by η in dB. Assuming that the receiver is in the first side-lobe of the transmitting antenna, and each value is further reduced by 7.2 dB, which is the decrease in directivity from the main to side-lobe of a Yagi-Uda. Therefore, anywhere along the side-lobe direction from this transmitter, a blind receiver could not tell the difference if a Yagi-Uda was replaced with either an ideal dipole (oriented towards the receiver) or an isotropic radiator with antenna input power increased by 1.57 dB.\n\nPolarization has not been taken into account so far, but it must be properly clarified. When considering the dipole radiator previously we assumed that it was perfectly aligned with the receiver. Now assume, however, that the receiving antenna is circularly polarized, and there will be a minimum 3 dB polarization loss regardless of antenna orientation. If the receiver is also a dipole, it is possible to align it orthogonally to the transmitter such that theoretically zero energy is received. However, this polarization loss is not accounted for in the calculation of ERP or EIRP. Rather, the receiving system designer must account for this loss as appropriate. For example, a cellular telephone tower has a fixed linear polarization, but the mobile handset must function well at any arbitrary orientation. Therefore, a handset design might provide dual polarization receive on the handset so that captured energy is maximized regardless of orientation, or the designer might use a circularly polarized antenna and account for the extra 3 dB of loss with amplification.\n\nFor example, an FM radio station which advertises that it has 100,000 watts of power actually has 100,000 watts ERP, and \"not\" an actual 100,000-watt transmitter. The transmitter power output (TPO) of such a station typically may be 10,000 to 20,000 watts, with a gain factor of 5 to 10 (5× to 10×, or 7 to 10 dB). In most antenna designs, gain is realized primarily by concentrating power toward the horizontal plane and suppressing it at upward and downward angles, through the use of phased arrays of antenna elements. The distribution of power versus elevation angle is known as the vertical pattern. When an antenna is also directional horizontally, gain and ERP will vary with azimuth (compass direction). Rather than the average power over all directions, it is the apparent power in the direction of the antenna's main lobe that is quoted as a station's ERP (this statement is just another way of stating the definition of ERP). This is particularly applicable to the huge ERPs reported for shortwave broadcasting stations, which use very narrow beam widths to get their signals across continents and oceans.\n\nERP for FM radio in the United States is always relative to a theoretical reference half-wave dipole antenna. (That is, when calculating ERP, the most direct approach is to work with antenna gain in dBd). To deal with antenna polarization, the Federal Communications Commission (FCC) lists ERP in both the horizontal and vertical measurements for FM and TV. Horizontal is the standard for both, but if the vertical ERP is larger it will be used instead.\n\nThe maximum ERP for US FM broadcasting is usually 100,000 watts (FM Zone II) or 50,000 watts (in the generally more densely populated Zones I and I-A), though exact restrictions vary depending on the class of license and the antenna height above average terrain (HAAT). Some stations have been grandfathered in or, very infrequently, been given a waiver, and can exceed normal restrictions.\n\nFor most microwave systems, a completely non-directional isotropic antenna (one which radiates equally and perfectly well in every direction – a physical impossibility) is used as a reference antenna, and then one speaks of EIRP (effective \"isotropic\" radiated power) rather than ERP. This includes satellite transponders, radar, and other systems which use microwave dishes and reflectors rather than dipole-style antennas.\n\nIn the case of medium wave (AM) stations in the United States, power limits are set to the actual transmitter power output, and ERP is not used in normal calculations. Omnidirectional antennas used by a number of stations radiate the signal equally in all directions. Directional arrays are used to protect co- or adjacent channel stations, usually at night, but some run directionally 24 hours. While antenna efficiency and ground conductivity are taken into account when designing such an array, the FCC database shows the station's transmitter power output, not ERP.\n\nEffective monopole radiated power (EMRP) may be used in Europe, particularly in relation to mediumwave broadcasting antennas. This is the same as ERP, except that a short vertical antenna (i.e. a short monopole) is used as the reference antenna instead of a half-wave dipole.\n\nThe height above average terrain for VHF and higher frequencies is extremely important when considering ERP, as the signal coverage (broadcast range) produced by a given ERP dramatically increases with antenna height. Because of this, it is possible for a station of only a few hundred watts ERP to cover more area than a station of a few thousand watts ERP, if its signal travels above obstructions on the ground.\n\n"}
{"id": "17355007", "url": "https://en.wikipedia.org/wiki?curid=17355007", "title": "Electricity pricing", "text": "Electricity pricing\n\nElectricity pricing (sometimes referred to as electricity tariff or the price of electricity) varies widely from country to country and may vary significantly from locality to locality within a particular country. Many factors go into determining an electricity tariff, such as the price of power generation, government subsidies, local weather patterns, transmission and distribution infrastructure, and industry regulation. “Electricity prices generally reflect the cost to build, finance, maintain, and operate power plants and the electricity grid.” Some utilities are for-profit, and their prices will also include a financial return for shareholders and owners. Electricity tariffs vary by type of customer, typically by residential, commercial, and industrial connections. Electricity price forecasting is the method by which a generator, utility company, or large industrial consumer can predict the wholesale prices of electricity with reasonable accuracy. The cost to supply electricity varies minute by minute.\nIn standard regulated monopoly markets, electricity rates typically vary for residential, commercial and industrial customers. The rates are determined through a regulatory process that is overseen by a public utility commission.\n\nPrices for any single class of electricity customer can vary by time-of-day called TOU or time of use or by the capacity or nature of the supply circuit (e.g., 5 kW, 12 kW, 18 kW, 24 kW are typical in some of the large developed countries); for industrial customers, single-phase vs. 3-phase, etc. Prices are usually highest for commercial and residential consumers because of the additional costs associated with stepping down their distribution voltage. The price of power for industrial customers is relatively the same as the wholesale price of electricity, because they consume more power at higher voltages. Supplying electricity at transmission-level high voltages is more efficient, and therefore less expensive.\n\nThe two most common distinctions between customer classes are load size and usage profile. In many cases, time-of-use (TOU) and load factor are more significant factors than load size. Contribution to peak-load is an extremely important factor in determining customer rate class. Consumer loads may be characterized as peak, off-peak, baseload, and seasonal. Utilities rate each load differently, because each has different implications for a power system.\n\nThe inclusion of renewable energy distributed generation and AMI in the modern electricity grid has introduced many alternative rate structures. Simple (or fixed) rate, tiered (or step) rate, TOU, demand rates, tiered within TOU, seasonal, and weekend/holiday rates are among the few residential rate structures offered by modern utilities. The simple rate charges a specific dollar per kilowatt ($/kWh) consumed. The tiered rate is one of the more common residential rate programs, and it charges a higher rate as customer usage increases. TOU and demand rates are structured to help maintain/control a utility’s peak demand. The concept at its core is to discourage customers from contributing to peak-load times by charging them more money to use power at that time.\n\nA feed-in tariff (FIT) is an energy-supply policy that supports the development of renewable power generation. FITs give financial benefits to renewable power producers. In the United States, FIT policies guarantee that eligible renewable generators will have their electricity purchased by their utility. The FIT contract contains a guaranteed period of time (usually 15–20 years) that payments in dollars per kilowatt hour ($/kWh) will be made for the full output of the system.\n\nNet metering is another billing mechanism that supports the development of renewable power generation, specifically, solar power. The mechanism credits solar energy system owners for the electricity their system adds to the grid. Residential customers with rooftop PV system will typically generate more electricity than their home consumes during daylight hours, so net metering is particularly advantageous. During this time where generation is greater than consumption, the home’s electricity meter will run backwards to provide a credit on the homeowner’s electricity bill.\n\nThe cost also differs by the power source. The net present value of the unit-cost of electricity over the lifetime of a generating asset is known as the levelized cost of electricity (LCOE). LCOE is the best value to compare different methods of generation on a consistent basis.\n\nIn the U.S. the estimated LCOE for different sources are:\nThe table below shows simple comparison of electricity tariffs in industrialised countries and territories around the world, expressed in US dollars. The comparison does not take into account factors including fluctuating international exchange rates, a country's purchasing power, government electricity subsidies or retail discounts that are often available in deregulated electricity markets.\n\nFor example, in 2012, Hawaii residents had the highest average residential electricity rate in the United States (37.34¢/kWh), while Louisiana residents had the lowest average residential electricity costs (8.37¢/kWh). Even in the contiguous United States the gap is significant, with New York residents having the highest average residential electricity rates in the lower 48 U.S. states (17.62¢/kWh).\n\n Denotes countries with government subsidized electricity tariffs.\n\nThe U.S. Energy Information Administration (EIA) also publishes an incomplete list of international energy prices, while the International Energy Agency (IEA) provides a thorough, quarterly review.\n\nThe following table shows electricity prices both for household and non-household consumers within the European Union (EU) and Iceland, Liechtenstein, Norway, Albania, Republic of Macedonia, Montenegro, Serbia, Turkey, Bosnia and Herzegovina, Kosovo, Moldova and Ukraine.\n\nElectricity price forecasting is the process of using mathematical models to predict what electricity prices will be in the future.\n\nThe simplest model for day ahead forecasting is to ask each generation source to bid on blocks of generation and choose the cheapest bids. If not enough bids are submitted, the price is increased. If too many bids are submitted the price can reach zero or become negative. The offer price includes the generation cost as well as the transmission cost, along with any profit. Power can be sold or purchased from adjoining power pools.\n\nThe concept of independent system operators (ISOs) fosters competition for generation among wholesale market participants by unbundling operation of transmission and generation. ISOs use bid-based markets to determine economic dispatch.\n\nWind and solar power are non-dispatchable. Such power is normally sold before any other bids, at a pre-determined rate for each supplier. Any excess is sold to another grid operator, or stored, using pumped-storage hydroelectricity, or in the worst case, curtailed. Curtailment could potentially significantly impact solar power’s economic and environmental benefits at greater PV penetration levels. Allocation is done by bidding.\n\nThe effect of the recent introduction of smart grids and integrating distributed renewable generation has been increased uncertainty of future supply, demand and prices. This uncertainty has driven much research into the topic of forecasting.\n\nElectricity cannot be stored as easily as gas, it is produced at the exact moment of demand. All of the factors of supply and demand will therefore have an immediate impact on the price of electricity on the spot market. In addition to production costs, electricity prices are set by supply and demand. However, some fundamental drivers are the most likely to be considered.\n\nShort-term prices are impacted the most by weather. Demand due to heating in the winter and cooling in the summer are the main drivers for seasonal price spikes. In 2017, the United States is scheduled to add 13 GW of natural-gas fired generation to its capacity. Additional natural-gas fired capacity is driving down the price of electricity, and increasing demand.\n\nA country’s natural resource endowment, as well as their regulations in place greatly influence tariffs from the supply side. The supply side of the electricity supply is most influenced by fuel prices, and CO allowance prices. The EU carbon prices have doubled since 2017, making it a significant driving factor of price.\n\nStudies show that generally demand for electricity is driven largely by temperature. Heating demand in the winter and cooling demand (air conditioners) in the summer are what primarily drive the seasonal peaks in most regions. Heating degree days and cooling degree days help measure energy consumption by referencing the outdoor temperature above and below 65 degrees Fahrenheit, a commonly accepted baseline.\n\nIn terms of renewable sources like solar and wind, weather impacts supply. California’s duck curve[cite] shows the difference between electricity demand and the amount of solar energy available throughout the day. On a sunny day, solar power floods the electricity generation market and then drops during sunless evening, when electricity demand peaks.\n\nSnowpack, streamflows, seasonality, salmon, etc. all affect the amount of water that can flow through a dam at any given time. Forecasting these variables predicts the available potential energy for a dam for a given period. Some regions such as the Egypt, China and the Pacific Northwest get significant generation from hydroelectric dams. In 2015, SAIDI and SAIFI more than doubled from the previous year in Zambia due to low water reserves in their hydroelectric dams caused by insufficient rainfall.\n\nWhether planned or unplanned, outages affect the total amount of power that is available to the grid. Outages undermine electricity supply, which in turn effects price.\n\nDuring times of economic hardship, many factories cut back production due to a reduction of consumer demand and therefore reduce production-related electrical demand.\n\nGlobal Markets\n\nThe UK has been a net importer of energy for over a decade, and as their generation capacity and reserves decrease the level of importing is reaching an all-time high. Their fuel price's dependence on international markets has a huge effect on the cost of electricity, especially if the exchange rate falls. Being energy dependent makes their electricity prices vulnerable to world events, as well.\n\nGovernment Regulation\n\nGovernments may choose to make electricity tariffs affordable for their population through subsidies to producers and consumers. Most countries characterized as having low energy access have electric power utilities that do not recover any of their capital and operating costs, due to high subsidy levels.\n\nIn the United States, federal interventions and subsidies for energy can be classified as tax expenditure, direct expenditures, research and development (R&D), and DOE loan guarantees. Most federal subsidies in 2016 were to support developing renewable energy supplies, and energy efficiency measures.\n\nExcessive Total Harmonic Distortions (THD) and not unity Power Factor (PF) is costly at every level of the electricity market. Cost of PF and THD impact is difficult to estimate, but both can potentially cause heat, vibrations, malfunctioning and even meltdowns. Power factor is the ratio of real to apparent power in a power system. Drawing more current results in a lower power factor. Larger currents require costlier infrastructure to minimize power loss, so consumers with low power factors get charged a higher electricity rate by their utility. True power factor is made of displacement power factor and THD. Power quality is typically monitored at the transmission level. A spectrum of compensation devices mitigate bad outcomes, but improvements can be achieved only with real-time correction devices (old style switching type, modern low-speed DSP driven and near real-time). Most modern devices reduce problems, while maintaining return on investment and significant reduction of ground currents. Power quality problems can cause erroneous responses from many kinds of analog and digital equipment, where the response could be unpredictable.\n\nMost common distribution network and generation is done with 3 phase structures, with special attention paid to the phase balancing and resulting reduction of ground current. It is true for industrial or commercial networks where most power is used in 3 phase machines, but light commercial and residential users do not have real-time phase balancing capabilities. Often this issue leads to unexpected equipment behavior or malfunctions and in extreme cases fires. For example, sensitive professional analogue or digital recording equipment must be connected to well-balanced and grounded power networks. To determine and mitigate the cost of the unbalanced electricity network, electric companies in most cases charge by demand or as a separate category for heavy unbalanced loads. A few simple techniques are available for balancing that require fast computing and real-time modeling.\n\n\n"}
{"id": "21893074", "url": "https://en.wikipedia.org/wiki?curid=21893074", "title": "FOGBANK", "text": "FOGBANK\n\nFOGBANK is a code name given to a material used in nuclear weapons such as the W76, W78 and W80.\n\nFOGBANK's precise nature is classified; in the words of former Oak Ridge general manager Dennis Ruddy, \"The material is classified. Its composition is classified. Its use in the weapon is classified, and the process itself is classified.\" Department of Energy Nuclear Explosive Safety documents simply describe it as a material \"used in nuclear weapons and nuclear explosives\" along with lithium hydride (LiH) and lithium deuteride (LiD), beryllium (Be), uranium hydride (UH), and plutonium hydride.\n\nHowever National Nuclear Security Administration (NNSA) Administrator Tom D’Agostino disclosed the role of FOGBANK in the weapon: \"There's another material in the—it's called interstage material, also known as fog bank\", and arms experts believe that FOGBANK is an aerogel material which acts as an interstage material in a nuclear warhead; i.e., a material designed to become a superheated plasma following the detonation of the weapon's fission stage, the plasma then triggering the fusion-stage detonation.\n\nIt has been revealed by unclassified official sources that FOGBANK was originally manufactured in Facility 9404-11 of the Y-12 National Security Complex in Oak Ridge, Tennessee from 1975 until 1989, when the final batch of W76 warheads was completed. After that the facility was mothballed, and finally slated for decommissioning by 1993. Only a small pilot plant was left, which had been used to produce small batches of FOGBANK for testing purposes.\n\nIn 1996, the US government decided that large numbers of its nuclear weapons would require replacement, refurbishing, or decommissioning. Accordingly, the Department of Energy set up a refurbishment program aimed at extending the service lives of older nuclear weapons. In 2000, the NNSA specified a life-extension program for W76 warheads that would enable them to remain in service until at least 2040.\n\nIt was soon realized that the FOGBANK material was a potential source of problems for the program, as few records of its manufacturing process had been retained when it was originally manufactured in the 1980s, and nearly all staff members who had expertise in its production had either retired or left the agency. The NNSA briefly investigated sourcing a substitute for FOGBANK, but eventually decided that since FOGBANK had been produced previously, they would be able to repeat it. Additionally, \"Los Alamos computer simulations at that time were not sophisticated enough to determine conclusively that an alternate material would function as effectively as Fogbank,\" according to a Los Alamos publication.\n\nManufacture involves the moderately toxic, highly volatile solvent acetonitrile, which presents a hazard for workers (causing three evacuations in March 2006 alone).\n\nWith Facility 9404-11 long since decommissioned, a brand new production facility was required. Delays arose during its construction, and in addition, engineers repeatedly encountered failure in their efforts to produce FOGBANK. As one deadline after the other expired, and the schedule was pushed back again and again, the NNSA eventually decided to invest $23 million to attempt to find an alternative to FOGBANK.\n\nIn March 2007, engineers finally devised a manufacturing process for FOGBANK. Unfortunately, the material turned out to have problems when tested, and in September 2007 the FOGBANK project was upgraded to \"Code Blue\" status by the NNSA, making it a major priority. In 2008, following the expenditure of a further $69 million, the NNSA finally managed to manufacture FOGBANK, and 7 months later, the first refurbished warhead was handed over to the US Navy, nearly a decade after the commencement of the refurbishment program. However, in May 2009 a US Navy spokesman said that they had not received any refurbished weapons to date. The Energy Department stated that the current plan was to begin shipping refurbished weapons in the fall of 2009, two years behind schedule.\n\nThe experience of reverse engineering FOGBANK produced some improvements in scientific knowledge of the process. The new production scientists noticed that certain problems in production resembled those noted by the original team. These problems were traced to a particular impurity in the final product that was required to meet quality standards. A root cause investigation showed that input materials were subject to cleaning processes that had not existed during the original production run. This cleaning removed a substance that generated the required impurity. With the implicit role of this substance finally understood, the production scientists can control output quality better than during the original run.\n"}
{"id": "25513303", "url": "https://en.wikipedia.org/wiki?curid=25513303", "title": "Fluff pulp", "text": "Fluff pulp\n\nFluff pulp (also called comminution pulp or fluffy pulp) is a type of chemical pulp made from long fibre softwoods. Important parameters for fluff pulp are bulk and water absorbency.\n\nFluff pulp was first developed for use in disposable sanitary napkins. Kotex first advertisement for products made with wood pulp (Cellucotton) appeared in 1921. Disposable diaper producers also were early to convert to fluff pulp because of its low cost and high absorbency. Normal usage of fluff pulp in a diaper was about 55 grams. In the 1980s started the commercialization of air-laid paper, which gave better bulk, porosity, strength, softness and water absorption properties compared with normal tissue paper. Also in the 1980s started the use of superabsorbents in diapers and reduced the need for fluff pulp and is now down to 15 grams or even less. The demand of the pulp in diapers has gone from being the absorbent of liquid to giving the products dry and wet strength.\n\nMore than 90% of the fluff pulps are fully bleached chemical softwood pulps, of which more than 90% are kraft pulps.\n\nThe most common raw material source for fluff pulps are southern bleached softwood kraft (SBSK) from loblolly pine. SBSK from other species and NBSK are also used to make fluff pulp. Thicker fibres are preferred to improve the bulk.\n\nFluff pulp is normally made rolls (reels) on a drying machine (a simplified Fourdrinier machine). The objective of the drying / sheeting operation is to produce a uniform sheet (paper density, moisture and strength) to the converting operation. The pulp may be impregnated with debonders before drying to ease defibration.\n\nThe worldwide production of fluff pulps amounts to about 3.5 million tons.\n\nFluff pulps are used as raw material in the absorbent core of personal care products such as diapers, feminine hygiene products, air-laid absorbent towelling, as such or with superabsorbents and/or synthetic fibres. More than 80% of the pulps are used in baby diapers.\n\nThe most demanding application of fluff pulps is in air-laid products, used in serving utensils, various towel applications in homes, in the industry and in hospitals. Fluff pulp for air-laid products are defibrized in a hammermill. Defibration is the process of freeing the fibres from each other before entering the papermachine. Important parameters for dry defibration are shredding energy and knot content.\n"}
{"id": "6061729", "url": "https://en.wikipedia.org/wiki?curid=6061729", "title": "Fluid–structure interaction", "text": "Fluid–structure interaction\n\nFluid–structure interaction (FSI) is the interaction of some movable or deformable structure with an internal or surrounding fluid flow. Fluid–structure interactions can be stable or oscillatory. In oscillatory interactions, the strain induced in the solid structure causes it to move such that the source of strain is reduced, and the structure returns to its former state only for the process to repeat. \n\nFluid–structure interactions are a crucial consideration in the design of many engineering systems, e.g. aircraft, spacecraft, engines and bridges. Failing to consider the effects of oscillatory interactions can be catastrophic, especially in structures comprising materials susceptible to fatigue. Tacoma Narrows Bridge (1940), the first Tacoma Narrows Bridge, is probably one of the most infamous examples of large-scale failure. Aircraft wings and turbine blades can break due to FSI oscillations. Fluid–structure interaction has to be taken into account for the analysis of aneurysms in large arteries and artificial heart valves. A reed actually produces sound because the system of equations governing its dynamics has oscillatory solutions. The dynamic of reed valves used in two strokes engines and compressors is governed by FSI. The act of \"blowing a raspberry\" is another such example. Fluid–structure interactions also occur in moving containers, where liquid oscillations due to the container motion impose substantial magnitudes of forces and moments to the container structure that affect the stability of the container transport system in a highly adverse manner. Another prominent example is the start up of a rocket engine, e.g. Space Shuttle main engine (SSME), where FSI can lead to considerable unsteady side loads on the nozzle structure.\n\nFluid–structure interaction problems and multiphysics problems in general are often too complex to solve analytically and so they have to be analyzed by means of experiments or numerical simulation. Research in the fields of computational fluid dynamics and computational structural dynamics is still ongoing but the maturity of these fields enables numerical simulation of fluid-structure interaction. Two main approaches exist for the simulation of fluid–structure interaction problems:\nThe monolithic approach requires a code developed for this particular combination of physical problems whereas the partitioned approach preserves software modularity because an existing flow solver and structural solver are coupled. Moreover, the partitioned approach facilitates solution of the flow equations and the structural equations with different, possibly more efficient techniques which have been developed specifically for either flow equations or structural equations. On the other hand, development of stable and accurate coupling algorithm is required in partitioned simulations. In conclusion, the partitioned approach allows reusing existing software which is an attractive advantage. However, stability of the coupling method needs to be taken into consideration.\n\nIn addition, the treatment of meshes introduces another classification of FSI analysis. The two classifications are the conforming mesh methods and the non-conforming mesh methods.\n\nThe Newton–Raphson method or a different fixed-point iteration can be used to solve FSI problems. Methods based on Newton–Raphson iteration are used in both the monolithic \n\nWhereas Newton–Raphson methods solve the flow and structural problem for the state in the entire fluid and solid domain, it is also possible to reformulate an FSI problem as a system with only the degrees of freedom in the interface’s position as unknowns. This domain decomposition condenses the error of the FSI problem into a subspace related to the interface. The FSI problem can hence be written as either a root finding problem or a fixed point problem, with the interface’s position as unknowns.\n\nInterface Newton–Raphson methods solve this root-finding problem with Newton–Raphson iterations, e.g. with an approximation of the Jacobian from a linear reduced-physics model. The interface quasi-Newton method with approximation for the inverse of the Jacobian from a least-squares model couples a black-box flow solver and structural solver by means of the information that has been gathered during the coupling iterations. This technique is based on the interface block quasi-Newton technique with an approximation for the Jacobians from least-squares models which reformulates the FSI problem as a system of equations with both the interface’s position and the stress distribution on the interface as unknowns. This system is solved with block quasi-Newton iterations of the Gauss–Seidel type and the Jacobians of the flow solver and structural solver are approximated by means of least-squares models.\n\nThe fixed-point problem can be solved with fixed-point iterations, also called (block) Gauss–Seidel iterations, which means that the flow problem and structural problem are solved successively until the change is smaller than the convergence criterion. However, the iterations converge slowly if at all, especially when the interaction between the fluid and the structure is strong due to a high fluid/structure density ratio or the incompressibility of the fluid. The convergence of the fixed point iterations can be stabilized and accelerated by Aitken relaxation and steepest descent relaxation, which adapt the relaxation factor in each iteration based on the previous iterations.\n\nIf the interaction between the fluid and the structure is weak, only one fixed-point iteration is required within each time step. These so-called staggered or loosely coupled methods do not enforce the equilibrium on the fluid–structure interface within a time step but they are suitable for the simulation of aeroelasticity with a heavy and rather stiff structure.\nSeveral studies have analyzed the stability of partitioned algorithms for the simulation of fluid-structure interaction\n\n\n\n\n"}
{"id": "8091089", "url": "https://en.wikipedia.org/wiki?curid=8091089", "title": "Fuhrländer Wind Turbine Laasow", "text": "Fuhrländer Wind Turbine Laasow\n\nFuhrländer Wind Turbine Laasow is a wind turbine, built in 2006 near the village of Laasow, Brandenburg, Germany. It consists of a 160-metre lattice tower, which carries a rotor 90 metres in diameter. Until two slightly taller wind turbines opened in Poland in 2012, this Fuhrländer was the tallest wind turbine in the world. Its power output is 2.5 MW.\n\n\n"}
{"id": "21301778", "url": "https://en.wikipedia.org/wiki?curid=21301778", "title": "Gelation", "text": "Gelation\n\nGelation (gel transition) is the formation of a gel from a system with branched polymers. Branched polymers can form links between the chains, which lead to progressively larger polymers. As the linking continues, larger branched polymers are obtained and at a certain extent of the reaction links between the polymer result in the formation of a single macroscopic molecule. At that point in the reaction, which is defined as gel point, the system loses fluidity and viscosity becomes very large. The onset of gelation, or gel point, is accompanied by a sudden increase in viscosity. This \"infinite\" sized polymer is called the gel or network, which does not dissolve in the solvent, but can swell in it.\n\nGelation can occur either by physical linking or by chemical crosslinking. While the physical gels involve physical bonds, chemical gelation involves covalent bonds. The first quantitative theories of chemical gelation were formulated in the 1940s by Flory and Stockmayer. Critical percolation theory was successfully applied to gelation in 1970s. A number of growth models (diffusion limited aggregation, cluster-cluster aggregation, kinetic gelation) were developed in the 1980s to describe the kinetic aspects of aggregation and gelation.\n\nIt is important to be able to predict the onset of gelation, since it is an irreversible process that dramatically changes the properties of the system.\n\nTwo common analytical methods to determine the changes are listed.\n\nAccording to the Carothers equation number-average degree of polymerization formula_1 is given by\n\nformula_2\n\nwhere formula_3 is the extent of the reaction and formula_4 is the average functionality of reaction mixture. For the gel formula_1 can be considered to be infinite, thus the critical extent of the reaction at the gel point is found as\n\nformula_6\n\nIf formula_3 is greater or equal to formula_8, gelation occurs.\n\nFlory and Stockmayer used a statistical approach to derive an expression to predict the gel point by calculating when formula_1 approaches infinite size. The statistical approach assumes that (1) the reactivity of the functional groups of the same type is the same and independent of the molecular size and (2) there are no intramolecular reactions between the functional groups on the same molecule.\n\nConsider the polymerization of bifunctional molecules formula_10, formula_11 and multifunctional formula_12, where formula_13 is the functionality. The extends of the functional groups are formula_14and formula_15,respectively. The ratio of all A groups, both reacted and unreacted, that are part of branched units, to the total number of A groups in the mixture is defined as formula_16 .This will lead to the following reaction\n\nformula_17\n\nThe probability of obtaining the product of the reaction above is given by formula_18, since the probability that a B group reach with a branched unit is formula_19 and the probability that a B group react with non-branched A is formula_20.\n\nThis relation yields to an expression for the extent of reaction of A functional groups at the gel point\n\nformula_21\n\nwhere r is the ratio of all A groups to all B groups. If more than one type of multifunctional branch unit is present and average formula_13 value is used for all monomer molecules with functionality greater than 2.\n\nNote that the relation does not apply for reaction systems containing monofunctional reactants and/or both A and B type of branch units.\n"}
{"id": "43194879", "url": "https://en.wikipedia.org/wiki?curid=43194879", "title": "Geometric transformation", "text": "Geometric transformation\n\nA geometric transformation is any bijection of a set having some geometric structure to itself or another such set. Specifically, \"A geometric transformation is a function whose domain and range are sets of points. Most often the domain and range of a geometric transformation are both R or both R. Often geometric transformations are required to be 1-1 functions, so that they have inverses.\" The study of geometry may be approached via the study of these transformations.\n\nGeometric transformations can be classified by the dimension of their operand sets (thus distinguishing between planar transformations and those of space, for example). They can also be classified according to the properties they preserve:\n\nEach of these classes contains the previous one.\n\n\nTransformations of the same type form groups that may be sub-groups of other transformation groups.\n\n\n"}
{"id": "33270768", "url": "https://en.wikipedia.org/wiki?curid=33270768", "title": "Haaken C. Mathiesen Jr.", "text": "Haaken C. Mathiesen Jr.\n\nHaaken Christian Mathiesen (9 October 1896 – 30 July 1975) was a Norwegian businessperson, who founded the Norwegian branch of Texaco.\n\nHe was born in Trondhjem as a son of landowner and politician Christian Pierre Mathiesen (1870–1953) and Celina Ihlen (1874–1948). On the paternal side he was a grandson of Haaken C. Mathiesen, a nephew of Haaken L. Mathiesen and a first cousin of Jørgen Mathiesen. On the maternal side he was a grandson of Jacob Thurmann Ihlen and Belgian citizen Ambrosine Pauline Rouquet. Through his mother's sisters he was a nephew of Arthur Knagenhjelm and Jens Gran Gleditsch and a second cousin of Kai Knagenhjelm. Through his grandfather's brothers Niels and Wincentz Thurmann Ihlen he was a second cousin of Nils, Joakim and Alf Ihlen.\n\nIn 1931 he married Magdalene Bull Wilhelmsen, a daughter of Finn Wilhelmsen and Magdalene Fredrikke Bull.\n\nHe finished his secondary education in 1914. He attended Harvard University and studied elsewhere in France and Germany. He was the founder of Texaco Norway, and chaired Texaco Norway and Norsk Texaco Oil. After mergers and acquisitions through history, the company is now known as YX Energi.\n\nHe was a member of the board or supervisory council of many companies, including Wilh. Wilhelmsen, Fred. Olsen & Co, Forsikringsselskapet Norden, Den norske Creditbank, Arthur Mathiesen & Co, Elkem, Norsk Elektrisk & Brown Boveri, Tønsberg Margarinfabrik and Farmand Fabriker. He died in July 1975 and was buried in Gamle Aker.\n"}
{"id": "55500494", "url": "https://en.wikipedia.org/wiki?curid=55500494", "title": "Iodine Satellite", "text": "Iodine Satellite\n\nIodine Satellite (iSat) is a technology demonstration satellite of the CubeSat format that will undergo high changes in velocity from a primary propulsion system by using a Hall thruster, and iodine as a propellant. The spacecraft will also perform changes of its orbital altitude, and demonstrate deorbit capabilities to reduce space junk.\n\niSat is being developed by NASA's Glenn Research Center, and it was planned as a secondary payload for launch in mid-2018, but launch has been delayed to allow for the propulsion system development to mature. The mission is planned to last one year before deorbit. \n\nElectrically powered spacecraft propulsion uses electricity, typically from solar panels, to accelerate the propellant and produce thrust. The technology can be scaled up to be used on small satellites up to . iSat will also demonstrate advanced power management and thermal control capabilities developed for spacecraft of its size.\n\nThe satellite is a 12U CubeSat format, with dimensions of about 20 cm × 20 cm × 30 cm. Its solar arrays produce 100 W.\n\nThe propulsion maturation is a partnership between NASA and the U.S. Air Force. iSat's iodine propulsion system consists of a 200 watt Hall thruster (BHT-200-I) developed by Busek Co, a cathode, a tank to store solid iodine, a power processing unit (PPU) and the feed system to supply the iodine. The cathode technology is planned to enable heaterless cathode conditioning, significantly increasing total system efficiency.\n\nA key advantage to using iodine as a propellant is that it provides a high density times specific impulse, it is three times as fuel efficient as the commonly flown xenon, it may be stored in the tank as an unpressurized solid, and it is not a hazardous propellant. 1U with 5 kg of iodine on a 12U vehicle can provide a change of velocity of 4 km/s ΔV, perform a 20,000km altitude change, 30° inclination change from LEO, or an 80° inclination change from GEO. During operations, the tank is heated to vaporize the propellant. The thruster then ionizes the vapor and accelerates it via magnetic and electrostatic fields, resulting in high specific impulse. The satellite has full three-axis attitude control capability by using momentum wheels and magnetic torque rods to rotate. iSat also counts with a passive thermal control system.\n\nThere is an emerging and rapidly growing market for small satellites, although they are significantly limited by primary propulsion. In 2013, NASA Marshall Space Flight Center competitively selected a project for the maturation of an iodine flight operational feed system. This demonstration flight will address not just propulsion, but the process to integrate commercial off the shelves components as well as custom designed components, opening affordable options of utilizing iodine propulsion systems for national security and for NASA's Discovery class missions. \nThe iodine thruster will allow iSat to alter its orbital inclination and elevation, opening up a wider range of mission objectives than previously possible with spacecraft of this size, such as transferring from a geosynchronous orbit to geostationary orbit, enter and manage lunar orbits, and be deployed to explore near Earth asteroids, Mars and Venus. As a demonstration, the spacecraft will use its propulsion to lower altitude from its initial orbit of about 600 km to a circular orbit of about 300 km. Then it will perform a plane change maneuver, complete any final operational maneuvers and continue to lower its closest approach to Earth, and de-orbit the spacecraft in less than 90 days following the end of the mission.\n\n"}
{"id": "37067022", "url": "https://en.wikipedia.org/wiki?curid=37067022", "title": "James Cargas", "text": "James Cargas\n\nJames Cargas is an American energy attorney, and was the Democratic nominee in the 2012,2014 and 2016 United States House of Representatives election in Texas' 7th Congressional District. He is a life-long democrat, who garnered the highest vote percentage (44%) in 2016 (opposing John Culberson) of any Democrat in Texas Congressional District 7 since 1964. Cargas ran for the Democratic nomination in District 7 again in 2018.\n\nIn 1988 after graduating from the University of Michigan with a double major in English and Communications, James Cargas became the Deputy Press Secretary to David Bonior, then Member of the U.S. House of Representatives from Michigan's 12th Congressional District.\n\nIn 1992, he graduated from the American University Washington College of Law where he served as President of the Environmental Law Society and Articles' Editor of The American University Journal of International Law and Policy. Upon graduation, he worked first in a large law firm and later as environmental counsel to a major interstate pipeline company. He is admitted to practice law in Texas, Washington DC, Michigan and West Virginia.\n\nHe left the private sector and joined the Clinton White House and the President’s Council on Sustainable Development.\n\nAfter leaving the White House to work on the Al Gore Presidential primary campaign in Washington DC, Iowa and Texas, Cargas rejoined the administration as a Special Assistant in the U.S. Department of Energy in the Office of Fossil Energy under Secretary of Energy Bill Richardson. He also served as the Energy Department's liaison to the President's Southwest Border Task Force. He was awarded the Exceptional Service Award of the Department of Energy\n\nHe permanently moved to Houston and became Deputy Director of the non-profit organization North American Energy Standards Board. After three years, he returned to private practice where he represented several startup companies in the energy marketing, directional drilling, and other industries.\n\nIn 2008, Houston Mayor Bill White hired Cargas as the Senior Assistant City Attorney for Energy of the City of Houston responsible for advising the Mayor on all aspects of energy procurement and energy transactional matters. Today, he works for Mayor Sylvester Turner in the same function and also advises the City on contract, environmental, renewable energy, real estate and regulatory matters.\n\nCargas is a founding member of the Oil Patch Democrats, a Texas-based political organization promoting \"realistic energy policy\" and Democratic candidates.\n\nCargas is married to Dr. Dorina Papageorgiou, a neuroscientist at Baylor College of Medicine in Houston, Texas.\n\nCargas ran in the 2012 election for the United States House of Representatives, in Texas' 7th Congressional District. He won the Democratic nomination for Texas' 7th Congressional District after a contested primary. Cargas received 36.43% of the vote against incumbent John Culberson (R) in the general election on November 6, 2012. He has been endorsed by the Houston Chronicle, The National Herald, NEO Magazine, and Washington Monthly.\n\nThe Texas House of Representatives district maps were the subject of courtroom battles in the lead up to the 2012 election. Although the federal court rejected maps drawn by the State Legislature due to racial discrimination, the final court-appointed maps included several of the original boundaries at the request of the Texas attorney general. The 7th District lost Memorial Park, Montrose, Spring Branch and Rice University to the 2nd District.\n\nCargas campaigned on a platform consisting of the following central issues:\n\nCargas won the Democratic nomination in the 2014 Democratic Primary for Texas' 7th Congressional District with 62.2% of the vote following an endorsement by the Houston Chronicle. In the general election on November 4, 2014 Cargas received 34.55% of the vote against incumbent John Culberson (R).\n\nCargas ran again in Texas' 7th Congressional District in 2016. Cargas garnered the highest vote percentage (44%) of any Democrat in Texas 7 since 1964.\n\nCargas ran again in Texas' 7th Congressional District in 2018, but was defeated in the Democratic Primary.\n\n"}
{"id": "53567687", "url": "https://en.wikipedia.org/wiki?curid=53567687", "title": "Lake Tuz Natural Gas Storage", "text": "Lake Tuz Natural Gas Storage\n\nLake Tuz Natural Gas Storage () is an underground natural gas storage facility under construction in Aksaray Province, central Turkey. It was developed artificially in a salt formation.\n\nThe storage facility is situated near Sultanhanı town in Aksaray Province south of LakeTuz at a depth of . It was established by creating salt caverns. The twelve man-made salt caverns with a total volume of can hold natural gas. Daily delivery from the storage can be up to when needed.\n\nThe geological structure of the area is suitable for large underground natural gas storage facilities. The salt formation covers an area of . It is long and around thick. It has a salt dome structure. To create the caverns in the salt formation, fresh water was brought by pipeline from Hirfanlı Dam at distance. Using the process of solution mining, water was injected through a borehole into the salt formation, and the saline water, which is formed after dissolution of salt in water, was pumped back to the surface leaving a void in the formation. The salt water was transported to far away Lake Tuz by pipeline. Gas pressure inside the storage is nearly .\n\nThe contract for the project was signed between the Turkish BOTAŞ and the Chinese Tianchen Engineering Company (TCC) in 2012, and the construction started in 2013. For the financing, World Bank provided a loan in amount US$325 million in 2006. Another loan in amount of US$400 million was secured in 2014. The first phase of the storage construction completed in February 2017. It is expected that storage will be fully in service in 2019. When completed, the facility will be capable of storing 10% of the natural gas consumed. Planned goal for total capacity is set to 20% of the consumed natural gas. The cost of the development was US$700 million.\n\nThe storage ensures supply safety in accordance with hourly, daily and seasonal needs. It eliminates supply-demand disparity. Stored natural gas will be withdrawn in times of extreme cold weather or when the water level in the dams are reduced in drought. It also helps price stability.\n\n"}
{"id": "41838704", "url": "https://en.wikipedia.org/wiki?curid=41838704", "title": "Lal Pir Power", "text": "Lal Pir Power\n\nThe Lal Pir Power Company Limited, was incorporated in 1994, located in Mehmood Kot, District Muzaffargarh, Punjab, Pakistan. Lal Pir Power Plant produces 362 MW of electricity whereas PakGen produces 365MW of electricity. It is owned by Nishat Group. \n\nThe plant started its operation in 1997 and was operated by American company AES Corporation. In 2010, Nishat Group acquired the plant from AES.\n\n\n"}
{"id": "523173", "url": "https://en.wikipedia.org/wiki?curid=523173", "title": "Linear separability", "text": "Linear separability\n\nIn Euclidean geometry, linear separability is a property of a pair of sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are \"linearly separable\" if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if line is replaced by hyperplane.\n\nThe problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept.\n\nLet formula_1 and formula_2 be two sets of points in an \"n\"-dimensional Euclidean space. Then formula_1 and formula_2 are \"linearly separable\" if there exist \"n\" + 1 real numbers formula_5, such that every point formula_6 satisfies formula_7 and every point formula_8 satisfies formula_9, where formula_10 is the formula_11-th component of formula_12.\n\nEquivalently, two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap).\n\nThree non-collinear points in two classes ('+' and '-') are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all '+' case is not shown, but is similar to the all '-' case):\n\nHowever, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need \"two\" straight lines and thus is not linearly separable: \n\nNotice that three points which are collinear and of the form \"+ ⋅⋅⋅ — ⋅⋅⋅ +\" are also not linearly separable.\n\nA Boolean function in \"n\" variables can be thought of as an assignment of \"0\" or \"1\" to each vertex of a Boolean hypercube in \"n\" dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be \"linearly separable\" provided these two sets of points are linearly separable.\n\nClassifying data is a common task in machine learning.\nSuppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a \"new\" data point will be in. In the case of support vector machines, a data point is viewed as a \"p\"-dimensional vector (a list of \"p\" numbers), and we want to know whether we can separate such points with a (\"p\" − 1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the \"maximum-margin hyperplane\" and the linear classifier it defines is known as a \"maximum margin classifier\".\n\nMore formally, given some training data formula_13, a set of \"n\" points of the form\n\nwhere the \"y\" is either 1 or −1, indicating the set to which the point formula_15 belongs. Each formula_16 is a \"p\"-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having formula_17 from those having formula_18. Any hyperplane can be written as the set of points formula_19 satisfying\n\nwhere formula_21 denotes the dot product and formula_22 the (not necessarily normalized) normal vector to the hyperplane. The parameter formula_23 determines the offset of the hyperplane from the origin along the normal vector formula_22.\n\nIf the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance.\n\n"}
{"id": "4915703", "url": "https://en.wikipedia.org/wiki?curid=4915703", "title": "Nubuck", "text": "Nubuck\n\nNubuck (pronounced ) is top-grain cattle leather that has been sanded or buffed on the grain side, or outside, to give a slight nap of short protein fibers, producing a velvet-like surface. It is resistant to wear, and may be white or coloured.\n\nNubuck is similar to suede, but is created from the outer side of a hide, giving it more strength and thickness and a fine grain. It is generally more expensive than suede, and must be coloured or dyed heavily to cover up the sanding and stamping process.\n\nNubuck characteristics are similar to aniline leather. It is soft to the touch, scratches easily, and water drops darken it temporarily (it dries to its original color).\n\nThe word nubuck probably comes from \"new\" + \"buck(skin)\".\n\n"}
{"id": "1415168", "url": "https://en.wikipedia.org/wiki?curid=1415168", "title": "Nuclear density", "text": "Nuclear density\n\nNuclear density is the density of the nucleus of an atom, averaging about 2.3×10 kg/m. The descriptive term \"nuclear density\" is also applied to situations where similarly high densities occur, such as within neutron stars.\n\nThe nuclear density for a typical nucleus can be approximately calculated from the size of the nucleus, which itself can be approximated based on the number of protons and neutrons in it. The radius of a typical nucleus, in terms of number of nucleons, is\nformula_1\nwhere formula_2 is the mass number and formula_3 is 1.25 fm, with typical deviations of up to 0.2 fm from this value. The density of the nucleus is thus:\n"}
{"id": "22929865", "url": "https://en.wikipedia.org/wiki?curid=22929865", "title": "Paraho process", "text": "Paraho process\n\nThe Paraho process is an above ground retorting technology for shale oil extraction. The name \"Paraho\" is delivered from the words \"para homem\", which means in Portuguese \"for mankind\".\n\nThe Paraho process was invented by John B. Jones, Jr., later president of the Paraho Development Corporation, and developed by Development Engineering, Inc., in the late 1960s. Its design was based on a gas combustion retort developed by the United States Bureau of Mines and the earlier Nevada–Texas–Utah Retort. In the late 1940s, these retorts were tested in the Oil Shale Experiment Station at Anvil Points in Rifle, Colorado. In 1971, the Standard Oil of Ohio started to cooperate with Mr. John B. Jones providing financial support for obtaining an oil shale lease at Anvil Points. In May 1972, the lease was approved. Before leasing a track at Anvil Points, a test of using the Paraho Direct process for limestone calcination in cement kilns was carried out.\n\nThe consortium for developing the Anvil Points lease – the Paraho Development Corporation – was formed in 1973. In addition to the Standard Oil of Ohio, other participants of the consortium were Atlantic Richfield, Carter Oil, Chevron Research, Cleveland-Cliffs Iron, Gulf Oil, Kerr-McKee, Marathon Oil, Arthur G. McKee, Mobil Research, Phillips Petroleum Company, Shell Development, Southern California Edison, Standard Oil Company (Indiana), Sun Oil, Texaco, and the Webb-Chambers-Gary-McLoraine Group. Shale oil retorting started in 1974 when two operational retorts – pilot plant and semiworks – were put into operation. The semiworks unit achieved a maximum throughput capacity of 290 tons (263 tonnes) of raw oil shale per day. In March 1976, the Paraho Development Corporation tested a modification of its technology – the Paraho Indirect process. The Anvil Points lease was closed in 1978.\n\nIn 1976–1978, under the contracts with the United States Navy, Paraho technology was used for production of 100,000 barrels of crude shale oil. It was tested for using as military transportation fuels. The Gary Western Refinery in Fruita, Colorado, refined the Paraho shale oil for production of gasoline, jet fuels, diesel fuel marine, and heavy fuel oil. Paraho JP-4 aviation fuel was tested by the United States Air Force in the T-39 jet aircraft flight, which took a place between the Wright Patterson Air Force Base (Dayton, Ohio) and the Carswell Air Force Base (Fort Worth, Texas). In addition, the Paraho heavy fuel oil was used for fueling a Cleveland-Cliffs Iron ore carrier during its 7-day cruise on Great Lakes. On 13 June 1980, the Department of Energy awarded $4.4 million contract (participants providing additional $3.7 million) for an 18-month study to construct an 18,000 TPD modular demonstration shale oil plant producing 10,000 BPD on a lease 40 miles southeast of Vernal, Utah. The demonstration module was never built.\n\nIn 1982, Paraho’s semi-works plant was torn down when the Anvil Points station was decommissioned, but the pilot plant was moved to an adjacent plot of private land.\n\nIn 1987, Paraho reorganized as New Paraho and began production of SOMAT asphalt additive used in test strips in 5 States. In 1991, New PARAHO reported successful tests of SOMAT shale oil asphalt additive.\n\nOn 28 June 2000, Shale Technologies purchased Paraho Development Corporation and became owner of the proprietary information relating to the Paraho oil shale retorting technologies.\n\nOn 14 August 2008, Queensland Energy Resources announced that it will use the Paraho Indirect technology for its Stuart Oil Shale Project.\n\nThe Paraho process can be operated in two different heating modes, which are direct and indirect. The Paraho Direct process evolved from gas combustion retort technology and is classified as an internal combustion method. Accordingly, the Paraho Direct retort is a vertical shaft retort similar to the Kiviter and Fushun retorts, used correspondingly in Estonia and China. However, compared to the earlier gas combustion retorts the Paraho retort's raw oil shale feeding mechanism, gas distributor, and discharge grate have different designs. In the Paraho Direct process, the crushed and screened raw oil shale is fed into the top of the retort through a rotating distributor. The oil shale descends the retort as a moving bed. The oil shale is heated by the rising combustion gases from the lower part of the retort and the kerogen in the shale decomposes at about to oil vapour, shale oil gas and spent shale. Heat for pyrolysis comes from the combustion of char in the spent shale. The combustion takes place where air is injected at two levels in the middle of the retort below the pyrolysis section raising the temperature of the shale and the gas to to . Collecting tubes at the top of the retort carry shale oil mist, evolved gases and combustion gases into the product separation unit, where oil, water and dust are separated from the gases. For combined removal of liquid droplets and particulates, a wet electrostatic precipitator is used. Cleaned gases from the precipitator are compressed in a compressor. Part of the gas from the compressor is recycled to the bottom of the retort to cool the combusted shale (shale ash) and carry the recovered heat back up the retort. Cooled shale ash exits the retort through the discharge grate in the bottom of the retort. After processing, shale ash is disposed of. The liquid oil is separated from produced water and may be further refined into high quality products. The mixture of evolved gases and combustions gases is available for use as a low quality fuel gas for drying or power generation.\n\nThe Paraho Indirect is classified as an externally generated hot gas technology . The Paraho Indirect retort configuration is similar to the Paraho Direct except that a part of the gas from the compressor is heated to between to in a separate furnace and injected into the retort instead of air. No combustion occurs in the Paraho Indirect retort itself. As a result, the fuel gas from the Paraho Indirect is not diluted with combustion gases and the char remains on the disposed spent shale.\n\nThe main advantage of the Paraho process is simplicity in process and design; it has few moving parts and therefore low construction and operating costs compared with more sophisticated technologies. The Paraho retort also consumes no water, which is especially important for oil shale extraction in areas with water scarcity.\nA disadvantage common to both the Paraho Direct and Paraho Indirect is that neither are able to process oil shale particles smaller than about . These fines may account for 10 to 30 per cent of the crushed feed.\n\n\n"}
{"id": "23759", "url": "https://en.wikipedia.org/wiki?curid=23759", "title": "Perfect crystal", "text": "Perfect crystal\n\nCrystalline materials (mainly metals and alloys, but also stoichiometric salts and other materials) are made up of solid regions of ordered matter (atoms placed in one of a number of ordered formations called Bravais lattices). These regions are known as crystals. A perfect crystal is the one that contains no point, linear, or planar imperfections. There are a wide variety of crystallographic defects.\n\nThe hypothetical concept of a perfect crystal is important in the basic formulation of the third law of thermodynamics.\n\nIn crystallography, the phrase 'perfect crystal' can be used to mean “no line or planar defects”, as it is difficult to measure small quantities of point defects in an otherwise defect-free crystal.\n\nImperfections are created by various thermodynamic processes. \n"}
{"id": "3765064", "url": "https://en.wikipedia.org/wiki?curid=3765064", "title": "Phosphocholine", "text": "Phosphocholine\n\nPhosphocholine is an intermediate in the synthesis of phosphatidylcholine in tissues. Phosphocholine is made in a reaction, catalyzed by choline kinase, that converts ATP and choline into phosphocholine and ADP. Phosphocholine is a molecule found, for example, in lecithin.\n\nIn nematodes and human placentas, phosphocholine is selectively attached to other proteins as a posttranslational modification to suppress an immune response by their hosts.\n\nIt is also one of the binding targets of C-reactive protein (CRP). Thus, when a cell is damaged, CRP binds to phosphocholine, beginning the recognition and phagocytotic immunologic response.\n\nPhosphocholine is a natural constituent of hens' eggs (and many other eggs) often used in biomimetic membrane studies.\n\n\n"}
{"id": "2273245", "url": "https://en.wikipedia.org/wiki?curid=2273245", "title": "Pittsburgh toilet", "text": "Pittsburgh toilet\n\nA Pittsburgh toilet, often called a \"Pittsburgh potty\", is a common fixture in pre-World War II houses built in Pittsburgh, Pennsylvania, United States. It consists of an ordinary flush toilet installed in the basement, with no surrounding walls. Most of these toilets are paired with a crude basement shower apparatus and large sink, which often doubles as a laundry basin. Also, because western Pennsylvania is a steep topographical zone, many basements have their own entryway, allowing homeowners to enter from their yard or garage, cleanse themselves in their basement, and then ascend their basement stairs refreshed. \n\nAs Pittsburgh was historically an industrial town, toilets such as these were said to be used by steelworkers and miners: grimy from the day's labor, they could use an exterior door to enter the basement directly from outside and use the basement's shower and toilet before heading upstairs. This usage is largely unverified by historians. The Pittsburgh toilet may have been used to divert sewer backups out of the living space of the house. The toilet in the basement would overflow from the sewer backup because it is the lowest point in the system, and the mess would be relatively easy to clean compared to an upstairs bathroom. \n\n"}
{"id": "31248281", "url": "https://en.wikipedia.org/wiki?curid=31248281", "title": "Plant A Tree Today Foundation", "text": "Plant A Tree Today Foundation\n\nThe Plant A Tree Today Foundation (PATT) is a non-governmental environmental organization with primary operations in Thailand and Indonesia. Established in 2005, PATT attempts to raise environmental awareness and foster better practices in less developed nations around the world, planting trees as a means to combat deforestation and climate change.\n\nThe Plant A Tree Today Foundation (PATT) was organised in November 2005 by Andrew Steel, who established the organization as a means of combating ongoing deforestation in less-developed nations around the world through public education and tree-planting campaigns. The group is a registered charitable organisation in the United Kingdom since 2006.\n\nShortly after its launch PATT began to work with UK fundraising consultant The Midas Partnership, which assisted in the generation of initial capitalisation of the group. Fundraising activities were conducted both in the UK and in Thailand, a nation of primary concern to the organisation.\n\nIt is also a registered foundation in Thailand. Steel remains the Managing Director of the foundation, assisted by Charlotte Whalley.\n\nPATT works to raise awareness of global environmental issues, campaign for better environmental practices and take action against deforestation and climate change by planting trees. It implements tree planting projects and carbon offsetting with environmentally conscious businesses, provides environmental education to schools in developing countries, and funds community development projects in rural communities centered on tree planting and reforestation.\n\nReforestation projects sponsored by PATT attempt to generate broad participation among the affected communities. For example, a 2008 reforestation project in Phetchaburi, Thailand, brought together school children, monks, governmental officers, and local villagers with forestation staff in planting over 2500 trees. Native tree species were replanted in a forest degraded by selective logging in an effort to restore animal habitat. The restored lowland deciduous tropical forest was to be used in the future as a tool for future environmental education.\n\nIn 2009 the Carbon Bank and Village Development Project sponsored by PATT was awarded a 2009 United Nations-backed SEED Award. The SEED Initiative, founded by the United Nations Environmental Programme, the United Nations Development Programme and the International Union for the Conservation of Nature, identifies and supports promising locally oriented enterprises in developing countries which are working to improve livelihoods and manage natural resources sustainably.\n\nThe group's Carbon Bank and Village Development Project targeted 48 rural villages in Thailand, helping to establish community forests and a micro-finance system in each, managed by the local communities themselves. Ultimately it is hoped that degraded land in these communities will be reforested and climate change lessened through the project, while at the same time village economies are enhanced.\n\nIn 2012, PATT launched a programme to join with almost 600 primary schools in the UK to provide educational materials, including online lesson plans and activities to be downloaded by teachers and used in classes. The educational materials are linked with the national curriculum, allowing the future generation to become aware of climate change and the problems which come with global warming.\n\nThe programme includes an intranet site for students and teachers to communicate and view lesson plans, as well as being able to interact with others in the programme. Facts are supported by resources, such as live webcam and video sessions from the wildlife from the middle of the forests, showing the reforestation projects being carried out by PATT. Maps are also available to pin point on-going and future projects.\n\nThe Plant A Tree Today Foundation operates a number of projects across the globe:\nThailand\nIndonesia\n\nIndia\n\nSouth Africa\n\nMalaysia\n\nThe organisation states that it intends to plant 1 million trees in Thailand in 2011.\n\n\n"}
{"id": "12127712", "url": "https://en.wikipedia.org/wiki?curid=12127712", "title": "Plutonium-244", "text": "Plutonium-244\n\nPlutonium-244 (Pu) is an isotope of plutonium that has a half-life of 80 million years. This is longer than any of the other isotopes of plutonium and longer than any other actinide isotope except for the three naturally abundant ones: uranium-235 (704 million years), uranium-238 (4.468 billion years), and thorium-232 (14.05 billion years). \n\nAccurate measurements, beginning in the early 1970s, have detected primordial plutonium-244, making it the shortest-lived primordial nuclide. The amount of Pu in the pre-Solar nebula (4.57×10 years ago) was estimated as 0.008 of amount of U. As the age of the Earth is about 57 half-lives of Pu, the amount of plutonium-244 left should be very small; Hoffman \"et al.\" estimated its content in the rare-earth mineral bastnasite as =1.0×10 g/g, which corresponded to the content in the Earth crust as low as 3×10 g/g (i.e. the total mass of plutonium-244 in the Earth crust is about 9 g). Since plutonium-244 cannot be easily produced by natural neutron capture in the low neutron activity environment of uranium ores (see below), its presence cannot plausibly be explained by any other means than creation by r-process nucleosynthesis in supernovas. Plutonium-244 thus should be the second shortest-lived and the heaviest primordial isotope yet detected or theoretically predicted.\n\nHowever, the detection of primordial Pu in 1971 is not confirmed by recent, more sensitive measurements using the method of accelerator mass spectrometry. In this study, no traces of plutonium-244 in the samples of bastnasite (taken from the same mine as in the early study) were observed, so only an upper limit on the Pu content was obtained: < 0.15×10 g/g, which is 370 (or less) atoms per gram of the sample, at least 7 times lower than the abundance measured by Hoffman \"et al.\"\n\nLive interstellar plutonium-244 has been detected in meteorite dust in marine sediments, although the levels detected are much lower than would be expected from current modelling of the in-fall from the interstellar medium.\n\nUnlike plutonium-238, plutonium-239, plutonium-240, plutonium-241, and plutonium-242, plutonium-244 is not produced in quantity by the nuclear fuel cycle, because further neutron capture on plutonium-242 produces plutonium-243 which has a short halflife (5 hours) and quickly beta decays to americium-243 before having much opportunity to further capture neutrons in any but very high neutron flux environments. However, a nuclear weapon explosion can produce some plutonium-244 by rapid successive neutron capture.\n"}
{"id": "3508478", "url": "https://en.wikipedia.org/wiki?curid=3508478", "title": "Polyester", "text": "Polyester\n\nPolyester is a category of polymers that contain the ester functional group in their main chain. As a specific material, it most commonly refers to a type called polyethylene terephthalate (PET). Polyesters include naturally occurring chemicals, such as in the cutin of plant cuticles, as well as synthetics such as polybutyrate. Natural polyesters and a few synthetic ones are biodegradable, but most synthetic polyesters are not. The material is used extensively in clothing.\n\nDepending on the chemical structure, polyester can be a thermoplastic or thermoset. There are also polyester resins cured by hardeners; however, the most common polyesters are thermoplastics. Examples of thermoset polyesters include the Desmophen brand from Bayer. The OH group is reacted with an Isocyanate functional compound in a 2 component system producing coatings which may optionally be pigmented. \n\nFabrics woven or knitted from polyester thread or yarn are used extensively in apparel and home furnishings, from shirts and pants to jackets and hats, bed sheets, blankets, upholstered furniture and computer mouse mats. Industrial polyester fibers, yarns and ropes are used in car tire reinforcements, fabrics for conveyor belts, safety belts, coated fabrics and plastic reinforcements with high-energy absorption. Polyester fiber is used as cushioning and insulating material in pillows, comforters and upholstery padding. Polyester fabrics are highly stain-resistant—in fact, the only class of dyes which \"can\" be used to alter the color of polyester fabric are what are known as disperse dyes.\n\nPolyester fibers are sometimes spun together with natural fibers to produce a cloth with blended properties. Cotton-polyester blends (polycotton) can be strong, wrinkle and tear-resistant, and reduce shrinking. Synthetic fibers using polyester have high water, wind and environmental resistance compared to plant-derived fibers. They are less fire resistant and can melt when ignited.\n\nPolyester blends have been renamed so as to suggest their similarity or even superiority to natural fibers (for example, China silk, which is a term in the textiles industry for a 100% polyester fiber woven to resemble the sheen and durability of insect-derived silk).\n\nPolyesters are also used to make bottles, films, tarpaulin, canoes, liquid crystal displays, holograms, filters, dielectric film for capacitors, film insulation for wire and insulating tapes. Polyesters are widely used as a finish on high-quality wood products such as guitars, pianos and vehicle/yacht interiors. Thixotropic properties of spray-applicable polyesters make them ideal for use on open-grain timbers, as they can quickly fill wood grain, with a high-build film thickness per coat. Cured polyesters can be sanded and polished to a high-gloss, durable finish.\n\nLiquid crystalline polyesters are among the first industrially used liquid crystal polymers. They are used for their mechanical properties and heat-resistance. These traits are also important in their application as an abradable seal in jet engines.\n\nNatural polyesters could have played a significant role in the origins of life. Long heterogeneous polyester chains are known to easily form in a one-pot reaction without catalyst under simple prebiotic conditions.\n\nPolyesters as thermoplastics may change shape after the application of heat. While combustible at high temperatures, polyesters tend to shrink away from flames and self-extinguish upon ignition. Polyester fibers have high tenacity and E-modulus as well as low water absorption and minimal shrinkage in comparison with other industrial fibers.\n\nUnsaturated polyesters (UPR) are thermosetting resins. They are used in the liquid state as casting materials, in sheet molding compounds, as fiberglass laminating resins and in non-metallic auto-body fillers. They are also used as the thermoset polymer matrix in pre-pregs. Fiberglass-reinforced unsaturated polyesters find wide application in bodies of yachts and as body parts of cars.\n\nAccording to the composition of their main chain, polyesters can be:\n\nIncreasing the aromatic parts of polyesters increases their glass transition temperature, melting temperature, thermal stability, chemical stability...\n\nPolyesters can also be telechelic oligomers like the polycaprolactone diol (PCL) and the polyethylene adipate diol (PEA). They are then used as prepolymers.\n\nPolyester is a synthetic polymer made of purified terephthalic acid (PTA) or its dimethyl ester dimethyl terephthalate (DMT) and monoethylene glycol (MEG). With 18% market share of all plastic materials produced, it ranges third after polyethylene (33.5%) and polypropylene (19.5%).\n\nThe main raw materials are described as follows:\n\n\nTo make a polymer of high molecular weight a catalyst is needed. The most common catalyst is antimony trioxide (or antimony tri-acetate):\n\n\nIn 2008, about 10,000 tonnes SbO were used to produce around 49 million tonnes polyethylene terephthalate.\n\nPolyester is described as follows:\n\n\nThere are several reasons for the importance of polyester:\n\n\nIn the following table, the estimated world polyester production is shown. Main applications are textile polyester, bottle polyester resin, film polyester mainly for packaging and specialty polyesters for engineering plastics. According to this table, the world's total polyester production might exceed 50 million tons per annum before the year 2010.\n\nThe raw materials PTA, DMT, and MEG are mainly produced by large chemical companies which are sometimes integrated down to the crude oil refinery where \"p\"-Xylene is the base material to produce PTA and liquefied petroleum gas (LPG) is the base material to produce MEG.\n\nAfter the first stage of polymer production in the melt phase, the product stream divides into two different application areas which are mainly textile applications and packaging applications. In the following table, the main applications of textile and packaging of polyester are listed.\n\nAbbreviations:\n\nA comparable small market segment (much less than 1 million tonnes/year) of polyester is used to produce engineering plastics and masterbatch.\n\nIn order to produce the polyester melt with a high efficiency, high-output processing steps like staple fiber (50–300 tonnes/day per spinning line) or POY /FDY (up to 600 tonnes/day split into about 10 spinning machines) are meanwhile more and more vertically integrated direct processes. This means the polymer melt is directly converted into the textile fibers or filaments without the common step of pelletizing. We are talking about full vertical integration when polyester is produced at one site starting from crude oil or distillation products in the chain oil → benzene → PX → PTA → PET melt → fiber/filament or bottle-grade resin. Such integrated processes are meanwhile established in more or less interrupted processes at one production site. Eastman Chemicals were the first to introduce the idea of closing the chain from PX to PET resin with their so-called INTEGREX process. The capacity of such vertically integrated production sites is >1000 tonnes/day and can easily reach 2500 tonnes/day.\n\nBesides the above-mentioned large processing units to produce staple fiber or yarns, there are ten thousands of small and very small processing plants, so that one can estimate that polyester is processed and recycled in more than 10 000 plants around the globe. This is without counting all the companies involved in the supply industry, beginning with engineering and processing machines and ending with special additives, stabilizers and colors. This is a gigantic industry complex and it is still growing by 4–8% per year, depending on the world region.\n\nSynthesis of polyesters is generally achieved by a polycondensation reaction. See \"condensation reactions in polymer chemistry\".\nThe general equation for the reaction of a diol with a diacid is :\n\nIn this classical method, an alcohol and a carboxylic acid react to form a carboxylic ester.\nTo assemble a polymer, the water formed by the reaction must be continually removed by azeotrope distillation.\n\nThe acid begins as an acid chloride, and thus the polycondensation proceeds with emission of hydrochloric acid (HCl) instead of water. This method can be carried out in solution or as an enamel.\n\nAliphatic polyesters can be assembled from lactones under very mild conditions, catalyzed anionically, cationically or metallorganically.\nA number of catalytic methods for the copolymerization of epoxides with cyclic anhydrides have also recently been shown to provide a wide array of functionalized polyesters, both saturated and unsaturated.\n\nThe futuro house was made of fibreglass-reinforced polyester plastic; polyester-polyurethane, and poly(methylmethacrylate) one of them was found to be degrading by Cyanobacteria and Archaea.\n\nUnsaturated polyesters are thermosetting resins. They are generally copolymers prepared by polymerizing one or more diol with saturated and unsaturated dicarboxylic acids (maleic acid, fumaric acid...) or their anhydrides. The double bond of unsaturated polyesters reacts with a vinyl monomer, usually styrene, resulting in a 3-D cross-linked structure. This structure acts as a thermoset. The exothermic cross-linking reaction is initiated through a catalyst, usually an organic peroxide such as methyl ethyl ketone peroxide or benzoyl peroxide.\n\nA team at Plymouth University in the UK spent 12 months analysing what happened when a number of synthetic materials were washed at different temperatures in domestic washing machines, using different combinations of detergents, to quantify the microfibres shed. They found that an average washing load of 6 kg could release an estimated 137,951 fibres from polyester-cotton blend fabric, 496,030 fibres from polyester and 728,789 from acrylic. Those fibers add to the general microplastics pollution.\n\n\n\n"}
{"id": "1131259", "url": "https://en.wikipedia.org/wiki?curid=1131259", "title": "Pounamu", "text": "Pounamu\n\nPounamu refers to several types of hard, durable and highly valued nephrite jade, bowenite, or serpentinite stone found in southern New Zealand. Pounamu is the Māori name. These rocks are also generically known as \"greenstone\" in New Zealand English.\n\nThere are two systems for classifying pounamu. Geologically, the rock falls into the three categories named above, but Māori classify pounamu by appearance. The main classifications are \"kawakawa\", \"kahurangi\", \"īnanga\", and \"tangiwai\". The first three are nephrite jade, while \"tangiwai\" is a form of bowenite.\n\n\nIn modern usage \"pounamu\" almost always refers to nephrite jade. Pounamu is generally found in rivers in specific parts of the South Island as nondescript boulders and stones. These are difficult to identify as pounamu without cutting them open.\n\n plays a very important role in Māori culture. It is considered a (treasure) and therefore protected under the Treaty of Waitangi. increase in (prestige) as they pass from one generation to another. The most prized are those with known histories going back many generations. These are believed to have their own and were often given as gifts to seal important agreements. \n\nFunctional tools were widely worn for both practical and ornamental reasons, and continued to be worn as purely ornamental pendants () even after they were no longer used as tools.\n\nPounamu is found on the West Coast, Fiordland and western Southland. \nIt is typically recovered from rivers and beaches where it has been transported to after being eroded from the mountains. However, pounamu has also been quarried by Māori from the mountains, even above the snow line. The group of rocks where pounamu comes from are called ophiolites. Ophiolites are slices of the deep ocean crust and part of the mantle. When these deep mantle rocks (serpentinite) and crustal rock (mafic igneous rocks) are heated up (metamorphosed) together, pounamu can be formed at their contact. \n\nPounamu has been formed in New Zeland in three main locations. The Dun Mountain Ophiolite Belt has been metamorphosed in western Southland and pounamu from this belt is found along the eastern and northern edge of Fiordland. The Anita Bay Dunite near Milford Sound is a small but highly prized source of pounamu.<ref>\n"}
{"id": "25601", "url": "https://en.wikipedia.org/wiki?curid=25601", "title": "Rhodium", "text": "Rhodium\n\nRhodium is a chemical element with symbol Rh and atomic number 45. It is a rare, silvery-white, hard, corrosion-resistant and chemically inert transition metal. It is a noble metal and a member of the platinum group. It has only one naturally occurring isotope, Rh. Naturally occurring rhodium is usually found as the free metal, alloyed with similar metals, and rarely as a chemical compound in minerals such as bowieite and rhodplumsite. It is one of the rarest and most valuable precious metals.\n\nRhodium is found in platinum or nickel ores together with the other members of the platinum group metals. It was discovered in 1803 by William Hyde Wollaston in one such ore, and named for the rose color of one of its chlorine compounds, produced after it reacted with the powerful acid mixture aqua regia.\n\nThe element's major use (approximately 80% of world rhodium production) is as one of the catalysts in the three-way catalytic converters in automobiles. Because rhodium metal is inert against corrosion and most aggressive chemicals, and because of its rarity, rhodium is usually alloyed with platinum or palladium and applied in high-temperature and corrosion-resistive coatings. White gold is often plated with a thin rhodium layer to improve its appearance while sterling silver is often rhodium-plated for tarnish resistance.\n\nRhodium detectors are used in nuclear reactors to measure the neutron flux level.\n\nRhodium (Greek \"rhodon\" (ῥόδον) meaning \"rose\") was discovered in 1803 by William Hyde Wollaston, soon after his discovery of palladium. He used crude platinum ore presumably obtained from South America. His procedure involved dissolving the ore in aqua regia and neutralizing the acid with sodium hydroxide (NaOH). He then precipitated the platinum as ammonium chloroplatinate by adding ammonium chloride (). Most other metals like copper, lead, palladium and rhodium were precipitated with zinc. Diluted nitric acid dissolved all but palladium and rhodium. Of these, palladium dissolved in aqua regia but rhodium did not, and the rhodium was precipitated by the addition of sodium chloride as . After being washed with ethanol, the rose-red precipitate was reacted with zinc, which displaced the rhodium in the ionic compound and thereby released the rhodium as free metal.\n\nAfter the discovery, the rare element had only minor applications; for example, by the turn of the century, rhodium-containing thermocouples were used to measure temperatures up to 1800 °C. The first major application was electroplating for decorative uses and as corrosion-resistant coating. The introduction of the three-way catalytic converter by Volvo in 1976 increased the demand for rhodium. The previous catalytic converters used platinum or palladium, while the three-way catalytic converter used rhodium to reduce the amount of NO in the exhaust.\n\nRhodium is a hard, silvery, durable metal that has a high reflectance. Rhodium metal does not normally form an oxide, even when heated. Oxygen is absorbed from the atmosphere only at the melting point of rhodium, but is released on solidification. Rhodium has both a higher melting point and lower density than platinum. It is not attacked by most acids: it is completely insoluble in nitric acid and dissolves slightly in aqua regia.\n\nRhodium belongs to group 9 of the periodic table, but the configuration of electrons in the outermost shells is atypical for the group. This anomaly is also observed in the neighboring elements, niobium (41), ruthenium (44), and palladium (46).\n\nThe common oxidation state of rhodium is +3, but oxidation states from +0 to +6 are also observed.\n\nUnlike ruthenium and osmium, rhodium forms no volatile oxygen compounds. The known stable oxides include , , , , and . Halogen compounds are known in nearly the full range of possible oxidation states. Rhodium(III) chloride, rhodium(IV) fluoride, rhodium(V) fluoride and rhodium(VI) fluoride are examples. The lower oxidation states are stable only in the presence of ligands.\n\nThe best-known rhodium-halogen compound is the Wilkinson's catalyst chlorotris(triphenylphosphine)rhodium(I). This catalyst is used in the hydroformylation or hydrogenation of alkenes.\n\nNaturally occurring rhodium is composed of only one isotope, Rh. The most stable radioisotopes are Rh with a half-life of 3.3 years, Rh with a half-life of 207 days, Rh with a half-life of 2.9 years, and Rh with a half-life of 16.1 days. Twenty other radioisotopes have been characterized with atomic weights ranging from 92.926 u (Rh) to 116.925 u (Rh). Most of these have half-lives shorter than an hour, except Rh (20.8 hours) and Rh (35.36 hours). It has numerous meta states, the most stable being Rh (0.141 MeV) with a half-life of about 2.9 years and Rh (0.157 MeV) with a half-life of 4.34 days (see isotopes of rhodium).\n\nIn isotopes weighing less than 103 (the stable isotope), the primary decay mode is electron capture and the primary decay product is ruthenium In isotopes greater than 103, the primary decay mode is beta emission and the primary product is palladium.\n\nRhodium is one of the rarest elements in the Earth's crust, comprising an estimated 0.0002 parts per million (2 × 10). Its rarity affects its price and its use in commercial applications.\n\nThe industrial extraction of rhodium is complex because the ores are mixed with other metals such as palladium, silver, platinum, and gold and there are very few rhodium-bearing minerals. It is found in platinum ores and extracted as a white inert metal that is difficult to fuse. Principal sources are located in South Africa; in river sands of the Ural Mountains; and in North America, including the copper-nickel sulfide mining area of the Sudbury, Ontario, region. Although the quantity at Sudbury is very small, the large amount of processed nickel ore makes rhodium recovery cost-effective.\n\nThe main exporter of rhodium is South Africa (approximately 80% in 2010) followed by Russia. The annual world production is 30 tonnes. The price of rhodium is highly variable. In 2007, rhodium cost approximately eight times more than gold, 450 times more than silver, and 27,250 times more than copper by weight. In 2008, the price briefly rose above $10,000 per ounce ($350,000 per kilogram). The economic slowdown of the 3rd quarter of 2008 pushed rhodium prices sharply back below $1,000 per ounce ($35,000 per kilogram); the price rebounded to $2,750 by early 2010 ($97,000 per kilogram) (more than twice the gold price), but in late 2013, the prices were less than $1000.\n\nPolitical and financial problems led to very low oil prices and oversupply, causing most metals to drop in price. The economies of China, India and other emerging countries slowed in 2014 and 2015. In 2014 alone, 23,722,890 motor vehicles were produced in China, excluding motorbikes. This resulted in a rhodium price of 740.00 US-$ per Troy ounce (31.1 grams) in late November 2015.\n\nRhodium is a fission product of uranium-235: each kilogram of fission product contains a significant amount of the lighter platinum group metals. Used nuclear fuel is therefore a potential source of rhodium, but the extraction is complex and expensive, and the presence of rhodium radioisotopes requires a period of cooling storage for multiple half-lives of the longest-lived isotope (about 10 years). These factors make the source unattractive and no large-scale extraction has been attempted.\n\nThe primary use of this element is in automobiles as a catalytic converter, changing harmful unburned hydrocarbons, carbon monoxide, and nitrogen oxide exhaust emissions into less noxious gases. Of 30,000 kg of rhodium consumed worldwide in 2012, 81% (24,300 kg) went into this application, and 8,060 kg was recovered from old converters. About 964 kg of rhodium was used in the glass industry, mostly for production of fiberglass and flat-panel glass, and 2,520 kg was used in the chemical industry.\n\nRhodium is preferable to the other platinum metals in the reduction of nitrogen oxides to nitrogen and oxygen:\n\nRhodium catalysts are used in a number of industrial processes, notably in catalytic carbonylation of methanol to produce acetic acid by the Monsanto process. It is also used to catalyze addition of hydrosilanes to molecular double bonds, a process important in manufacture of certain silicone rubbers. Rhodium catalysts are also used to reduce benzene to cyclohexane.\n\nThe complex of a rhodium ion with BINAP is a widely used chiral catalyst for chiral synthesis, as in the synthesis of menthol.\n\nRhodium finds use in jewelry and for decorations. It is electroplated on white gold and platinum to give it a reflective white surface at time of sale, after which the thin layer wears away with use. This is known as rhodium flashing in the jewelry business. It may also be used in coating sterling silver to protect against tarnish (silver sulfide, AgS, produced from atmospheric hydrogen sulfide, HS). Solid (pure) rhodium jewelry is very rare, more because of the difficulty of fabrication (high melting point and poor malleability) than because of the high price. The high cost ensures that rhodium is applied only as an electroplate.\n\nRhodium has also been used for honors or to signify elite status, when more commonly used metals such as silver, gold or platinum were deemed insufficient. In 1979 the \"Guinness Book of World Records\" gave Paul McCartney a rhodium-plated disc for being history's all-time best-selling songwriter and recording artist.\n\nRhodium is used as an alloying agent for hardening and improving the corrosion resistance of platinum and palladium. These alloys are used in furnace windings, bushings for glass fiber production, thermocouple elements, electrodes for aircraft spark plugs, and laboratory crucibles. Other uses include:\n\nBeing a noble metal, pure rhodium is inert. However, chemical complexes of rhodium can be reactive. Median lethal dose (LD) for rats is 198 mg of rhodium chloride () per kilogram of body weight. Like the other noble metals, all of which are too inert to occur as chemical compounds in nature, rhodium has not been found to serve any biological function. In elemental form, the metal is harmless.\n\nPeople can be exposed to rhodium in the workplace by inhalation. The Occupational Safety and Health Administration (OSHA) has specified the legal limit (Permissible exposure limit) for rhodium exposure in the workplace at 0.1 mg/m over an 8-hour workday, and the National Institute for Occupational Safety and Health (NIOSH) has set the recommended exposure limit (REL), at the same level. At levels of 100 mg/m, rhodium is immediately dangerous to life or health. For soluble compounds, the PEL and REL are both 0.001 mg/m.\n\n\n"}
{"id": "26008674", "url": "https://en.wikipedia.org/wiki?curid=26008674", "title": "Senftleben–Beenakker effect", "text": "Senftleben–Beenakker effect\n\nThe Senftleben–Beenakker effect is the dependence on a magnetic or electric field of transport properties (such as viscosity and heat conductivity) of polyatomic gases. The effect is caused by the precession of the (magnetic or electric) dipole of the gas molecules between collisions. The resulting rotation of the molecule averages out the nonspherical part of the collision cross-section, if the field is large enough that the precession time is short compared to the time between collisions (this requires a very dilute gas). The change in the collision cross-section, in turn, can be measured as a change in the transport properties.\n\nThe magnetic field dependence of the transport properties can also include a transverse component; for example, a heat flow perpendicular to both temperature gradient and magnetic field. This is the molecular analogue of the Hall effect and Righi-Leduc effect for electrons. A key difference is that the gas molecules are neutral, unlike the electrons, so the magnetic field exerts no Lorentz force. An analogous magnetotransverse heat conductivity has been discovered for photons and phonons.\n\nThe Senftleben–Beenakker effect owes its name to the physicists Hermann Senftleben (Münster University, Germany) and Jan J.M. Beenakker () (Leiden University, The Netherlands), who discovered it, respectively, for paramagnetic gases (such as NO and O) and diamagnetic gases (such as N and CO). The change in the transport properties is smaller in a diamagnetic gas, because the magnetic moment is not intrinsic (as it is in a paramagnetic gas), but induced by the rotation of a nonspherical molecule. The importance of the effect is that it provides information on the angular dependence of the intermolecular potential. The theory to extract that information from transport measurements is based on the Waldmann-Snider equation (a quantum mechanical version of the Boltzmann equation for gases with rotating molecules). The entire field is reviewed in a two-volume monograph.\n\n\n"}
{"id": "13203670", "url": "https://en.wikipedia.org/wiki?curid=13203670", "title": "Shell Rotella T", "text": "Shell Rotella T\n\nShell Rotella T is a line of heavy duty engine lubrication products produced by Royal Dutch Shell. The line includes engine oils, gear oils and coolants. The oil carries both the American Petroleum Institute (API) diesel \"C\" rating as well as the API gasoline engine \"S\" rating. Ratings differ based on the oil. Rotella oils like T3 15w-40 meets both the API CJ-4 and SM specifications, and may be used in both gasoline and diesel engines. However, it is formulated specifically for vehicles without catalytic converters, containing phosphorus levels beyond the 600-800ppm range. Therefore, Rotella is not recommended for gasoline vehicles with catalytic converters due to the higher risk of damaging these emission controls. Newer formulations of Rotella T6 however are API SM rated as safe for pre-2011 gasoline vehicles.\n\nThe Rotella product family is categorized by Shell into the following product families:\n\nIn the engine oil family, there are four basic oil sub-families:\n\nBoth the multigrade conventional oil (10W-30 and 15W-40) and the synthetic SAE 5W-40 meet the newest API certification of CJ-4/SM.\n\nShell is marketing their new CJ-4/SM oil as \"Triple Protection,\" meaning it provides enhanced qualities for engine wear, soot control and engine cleanliness. Shell's Rotella website indicates that on-road testing confirms the new Triple Protection technology produces better anti-wear characteristics than their existing CI-4+ rated Rotella oil. This is achieved despite a lower zinc and phosphorus additive level as called for by the API CJ-4 specification. (The 15W-40 Rotella T with Triple Protection oil has approximately 1200 ppm of zinc and 1100 ppm phosphorus at the time of manufacture.)\n\nThe Shell Rimula brand is multi-national and comparable in all aspects, including the classification names. (i.e. T-5, T-6, Etc.)\n\nRotella competes with similar lubrication products from other oil manufacturers. Some notable competitive products are:\n\nThough marketed as an engine oil for diesel trucks, Rotella oil has found popularity with motorcyclists as well. The lack of \"friction modifiers\" in Rotella means they do not interfere with wet clutch operations. This is called a \"shared sump\" design, which is unlike automobiles which maintain separate oil reservoirs - one for the engine and one for the transmission. Used oil analysis (UOA) reports on BobIsTheOilGuy.com have shown wear metals levels comparable to oils marketed as motorcycle-specific.\n\nBoth Rotella T 15W-40 conventional and, Rotella T6 5W-40 Synthetic both list the JASO MA standard; this information can be found on the bottle adjacent to the SAE/API rating stamp. JASO is an acronym that stands for \"The Japanese Automotive Standards Organization.\" Note that the 10W-30 conventional oil does not list JASO-MA.\n\nLikewise with motorcycles, though marketed as an engine oil for diesel trucks, Rotella T6 5w-40 synthetic oil has also found popularity with drivers and tuners of gasoline powered vehicles that utilize turbocharging or other forms of forced induction. Several owners of high performance model cars have adopted its use due to its high heat tolerance and its resistance to shearing. Rotella T6 is a Non Energy Conserving Oil, and does not meet GF-5 Oil specifications. When Rotella T6 was revised for the API specification(for use in spark ignition engines), its Zinc levels were effectively reduced. Higher(content) Zinc Additives(ZDDP) are required for flat tappet engines and cartridge bearings, which In previous formulations Rotella T6 had desirable levels of Zinc(ZDDP).\n\n"}
{"id": "188972", "url": "https://en.wikipedia.org/wiki?curid=188972", "title": "Silane", "text": "Silane\n\nSilane is an inorganic compound with chemical formula, SiH, making it a group 14 hydride. It is a colourless, pyrophoric gas with a sharp, repulsive smell, somewhat similar to that of acetic acid. Silane is of practical interest as a precursor to elemental silicon.\n\n\"Silanes\" refers to many compounds with four substituents on silicon, including an organosilicon compound. Examples include trichlorosilane (SiHCl), tetramethylsilane (Si(CH)), and tetraethoxysilane (Si(OCH)).\n\nSilane can be produced by several routes. Typically, it arises from the reaction of hydrogen chloride with magnesium silicide:\n\nIt is also prepared from metallurgical grade silicon in a two-step process. First, silicon is treated with hydrogen chloride at about 300 °C to produce trichlorosilane, HSiCl, along with hydrogen gas, according to the chemical equation:\n\nThe trichlorosilane is then converted to a mixture of silane and silicon tetrachloride. This redistribution reaction requires a catalyst:\n\nThe most commonly used catalysts for this process are metal halides, particularly aluminium chloride. This is referred to as a redistribution reaction, which is a double displacement involving the same central element. It may also be thought of as a disproportionation reaction even though there is no change in the oxidation number for silicon (Si has a nominal oxidation number IV in all three species). However, the utility of the oxidation number concept for a covalent molecule, even a polar covalent molecule, is ambiguous. The silicon atom could be rationalized as having the highest formal oxidation state and partial positive charge in SiCl and the lowest formal oxidation state in SiH since Cl is far more electronegative than is H.\n\nAn alternative industrial process for the preparation of very high purity silane, suitable for use in the production of semiconductor grade silicon, starts with metallurgical grade silicon, hydrogen, and silicon tetrachloride and involves a complex series of redistribution reactions (producing byproducts that are recycled in the process) and distillations. The reactions are summarized below:\n\nThe silane produced by this route can be thermally decomposed to produce high-purity silicon and hydrogen in a single pass.\n\nStill other industrial routes to silane involve reduction of SiF with sodium hydride (NaH) or reduction of SiCl with lithium aluminum hydride (LiAlH).\n\nAnother commercial production of silane involves reduction of silicon dioxide (SiO) under Al and H gas in a mixture of NaCl and aluminum chloride (AlCl) at high pressures:\n\nIn 1857, the German chemists Heinrich Buff and Friedrich Woehler discovered silane among the products formed by the action of hydrochloric acid on aluminum silicide, which they had previously prepared. They called the compound \"siliciuretted hydrogen\".\n\nFor classroom demonstrations, silane can be produced by heating sand with magnesium powder to produce magnesium silicide (MgSi), then pouring the mixture into hydrochloric acid. The magnesium silicide reacts with the acid to produce silane gas, which burns on contact with air and produces tiny explosions. This may be classified as a heterogeneous acid-base chemical reaction since the isolated Si ion in the MgSi antifluorite structure can serve as a Brønsted–Lowry base capable of accepting four protons. It can be written as:\n\nIn general, the alkaline-earth metals form silicides with the following stoichiometries: MSi, MSi, and MSi. In all cases, these substances react with Brønsted–Lowry acids to produce some type of hydride of silicon that is dependent on the Si anion connectivity in the silicide. The possible products include SiH and/or higher molecules in the homologous series SiH, a polymeric silicon hydride, or a silicic acid. Hence, MSi with their zigzag chains of Si anions (containing two lone pairs of electrons on each Si anion that can accept protons) yield the polymeric hydride (SiH).\n\nYet another small-scale route for the production of silane is from the action of sodium amalgam on dichlorosilane, SiHCl, to yield monosilane along with some yellow polymerized silicon hydride (SiH).\n\nSilane is the silicon analogue of methane. Because of the greater electronegativity of hydrogen in comparison to silicon, this Si–H bond polarity is the opposite of that in the C–H bonds of methane. One consequence of this reversed polarity is the greater tendency of silane to form complexes with transition metals. A second consequence is that silane is pyrophoric — it undergoes spontaneous combustion in air, without the need for external ignition. However, the difficulties in explaining the available (often contradictory) combustion data are ascribed to the fact that silane itself is stable and that the natural formation of larger silanes during production, as well as the sensitivity of combustion to impurities such as moisture and to the catalytic effects of container surfaces causes its pyrophoricity. Above 420 °C, silane decomposes into silicon and hydrogen; it can therefore be used in the chemical vapor deposition of silicon.\n\nThe Si–H bond strength is around 384 kJ/mol, which is about 20% weaker than the H–H bond in H. Consequently, compounds containing Si–H bonds are much more reactive than is H. The strength of the Si–H bond is modestly affected by other substituents: the Si–H bond strengths in SiHF, SiHCl, and SiHMe are respectively 419, 382, and 398 kJ/mol.\n\nSeveral industrial and medical applications exist for silane and functionalized silanes. For instance, silanes are used as coupling agents to adhere fibers such as glass fibers and carbon fibers to certain polymer matrices, stabilizing the composite material. In other words, silane coats the glass fibers to create better adhesion to the polymer matrix. They can also be used to couple a bio-inert layer on a titanium implant. Other applications include water repellents, masonry protection, control of graffiti, applying polycrystalline silicon layers on silicon wafers when manufacturing semiconductors, and sealants. The semiconductor industry used about 300 metric tons per year of silane in the late 1990s. More recently, a growth in low-cost solar photovoltaic module manufacturing has led to substantial consumption of silane for depositing (PECVD) hydrogenated amorphous silicon (a-Si:H) on glass and other substrates like metal and plastic. The PECVD process is relatively inefficient at materials utilization with approximately 85% of the silane being wasted. To reduce that waste and the ecological footprint of a-Si:H-based solar cells further several recycling efforts have been developed.\n\nSilane is also used in supersonic combustion ramjets to initiate combustion in the compressed air stream. As it can burn using carbon dioxide as an oxidizer it is a candidate fuel for engines operating on Mars.\n\nSilane and similar compounds containing Si—H bonds are used as reducing agents in organic and organometallic chemistry.\n\nSilane methacrylates are used in dentistry as part of tooth-colored, composite filling material. Silane methacrylates act as a coupling agent between the hard, silicate-based, ceramic filler and the organic, resin-based oligomer matrix.\n\nA number of fatal industrial accidents produced by combustion and detonation of leaked silane in air have been reported.\n\nIf a leaking stream of silane is obstructed or confined, energy release due to combustion is more concentrated leading to increasing reaction speed and burning velocity – up to gas phase detonation and potentially severe damages. Silane may autoignite at under 54 °C (130 °F).\n\nSiH(g) + 2O(g) → SiO(s) + 2HO(g) with ΔH = –1517 kJ/mol = –47.23 kJ/g\n\nHazardous byproducts of combustion\n\nSiH(g) + O(g) → SiO(s) + 2H(g)\n\nSiH(g) + O(g) → SiHO(s) + HO(g)\n\n2SiH(g) + O(g) → 2SiHO(s) + 2H(g)\n\nSiHO(s) + O(g) → SiO(s) + HO(g)\n\nFor lean mixtures a two-stage reaction process has been proposed, which consists of a silane consumption process and a hydrogen oxidation process. The heat of SiO (s) condensation increases the burning velocity due to thermal feedback.\n\nDiluted silane mixtures with inert gases such as nitrogen or argon are even more likely to ignite when leaked into open air, compared to pure silane: even a 1% mixture of silane in pure nitrogen easily ignites when exposed to air.\n\nIn Japan, in order to reduce the danger of silane for amorphous silicon solar cell manufacturing, several companies began to dilute silane with hydrogen gas. This resulted in a symbiotic benefit of making more stable solar photovoltaic cells as it reduced the Staebler-Wronski Effect.\n\nUnlike methane, silane is fairly toxic: the lethal concentration in air for rats (LC) is 0.96% (9,600 ppm) over a 4-hour exposure. In addition, contact with eyes may form silicic acid with resultant irritation.\n\nIn regards to occupational exposure of silane to workers, the US National Institute for Occupational Safety and Health has set a recommended exposure limit of 5 ppm (7 mg/m) over an eight-hour time-weighted average.\n\n"}
{"id": "1530612", "url": "https://en.wikipedia.org/wiki?curid=1530612", "title": "Solar cooker", "text": "Solar cooker\n\nA solar cooker is a device which uses the energy of direct sunlight to heat, cook or pasteurise drink and other food materials. Many solar cookers currently in use are relatively inexpensive, low-tech devices, although some are as powerful or as expensive as traditional stoves, and advanced, large-scale solar cookers can cook for hundreds of people. Because they use no fuel and cost nothing to operate, many nonprofit organizations are promoting their use worldwide in order to help reduce fuel costs (especially where monetary reciprocity is low) and air pollution, and to slow down the deforestation and desertification caused by gathering firewood for cooking.\n\n1) Concentrating sunlight: A mirrored surface with high specular reflectivity is used to concentrate light from the sun on to a small cooking area. Depending on the geometry of the surface, sunlight can be concentrated by several orders of magnitude producing temperatures high enough to melt salt and smelt metal. For most household solar cooking applications, such high temperatures are not really required. Solar cooking products, thus, are typically designed to achieve temperatures of (baking temperatures) to (grilling/searing temperatures) on a sunny day.\n\n2) Converting light energy to heat energy: Solar cookers concentrate sunlight onto a receiver such as a cooking pan. The interaction between the light energy and the receiver material converts light to heat. This conversion is maximized by using materials that conduct and retain heat. Pots and pans used on solar cookers should be matte black in color to maximize the absorption.\n\n3) Trapping heat energy: It is important to reduce convection by isolating the air inside the cooker from the air outside the cooker. Simply using a glass lid on your pot enhances light absorption from the top of the pan and provides a greenhouse effect that improves heat retention and minimizes convection loss. This \"glazing\" transmits incoming visible sunlight but is opaque to escaping infrared thermal radiation. In resource constrained settings, a high-temperature plastic bag can serve a similar function, trapping air inside and making it possible to reach temperatures on cold and windy days similar to those possible on hot days.\n\nDifferent kinds of solar cookers use somewhat different methods of cooking, but most follow the same basic principles.\n\nFood is prepared as if for an oven or stove top. However, because food cooks faster when it is in smaller pieces, food placed inside a solar cooker is usually cut into smaller pieces than it might otherwise be. For example, potatoes are usually cut into bite-sized pieces rather than roasted whole. For very simple cooking, such as melting butter or cheese, a lid may not be needed and the food may be placed on an uncovered tray or in a bowl. If several foods are to be cooked separately, then they are placed in different containers.\n\nThe container of food is placed inside the solar cooker, which may be elevated on a brick, rock, metal trivet, or other heat sink, and the solar cooker is placed in direct sunlight. Foods that cook quickly may be added to the solar cooker later. Rice for a mid-day meal might be started early in the morning, with vegetables, cheese, or soup added to the solar cooker in the middle of the morning. Depending on the size of the solar cooker and the number and quantity of cooked foods, a family may use one or more solar cookers.\n\nA solar oven is turned towards the sun and left until the food is cooked. Unlike cooking on a stove or over a fire, which may require more than an hour of constant supervision, food in a solar oven is generally not stirred or turned over, both because it is unnecessary and because opening the solar oven allows the trapped heat to escape and thereby slows the cooking process. If wanted, the solar oven may be checked every one to two hours, to turn the oven to face the sun more precisely and to ensure that shadows from nearby buildings or plants have not blocked the sunlight. If the food is to be left untended for many hours during the day, then the solar oven is often turned to face the point where the sun will be when it is highest in the sky, instead of towards its current position.\n\nThe cooking time depends primarily on the equipment being used, the amount of sunlight at the time, and the quantity of food that needs to be cooked. Air temperature, wind, and latitude also affect performance. Food cooks faster in the two hours before and after the local solar noon than it does in either the early morning or the late afternoon. Large quantities of food, and food in large pieces, take longer to cook. As a result, only general figures can be given for cooking time. With a small solar panel cooker, it might be possible to melt butter in 15 minutes, to bake cookies in 2 hours, and to cook rice for four people in 4 hours. With a high performing parabolic solar cooker, you may be able to grill a steak in minutes. However, depending on local conditions and the solar cooker type, these projects could take half as long, or twice as long.\n\nIt is difficult to burn food in a solar cooker. Food that has been cooked even an hour longer than necessary is usually indistinguishable from minimally cooked food. The exception to this rule is some green vegetables, which quickly change from a perfectly cooked bright green to olive drab, while still retaining the desirable texture.\n\nFor most foods, such as rice, the typical person would be unable to tell how it was cooked from looking at the final product. There are some differences, however: Bread and cakes brown on their tops instead of on the bottom. Compared to cooking over a fire, the food does not have a smoky flavor.\n\nA box cooker has a transparent glass or plastic top, and it may have additional reflectors to concentrate sunlight into the box. The top can usually be removed to allow dark pots containing food to be placed inside. One or more reflectors of shiny metal or foil-lined material may be positioned to bounce extra light into the interior of the oven chamber. Cooking containers and the inside bottom of the cooker should be dark-colored or black. Inside walls should be reflective to reduce radiative heat loss and bounce the light towards the pots and the dark bottom, which is in contact with the pots. The box should have insulated sides. Thermal insulation for the solar box cooker must be able to withstand temperatures up to 150 °C (300 °F) without melting or out-gassing. Crumpled newspaper, wool, rags, dry grass, sheets of cardboard, etc. can be used to insulate the walls of the cooker. Metal pots and/or bottom trays can be darkened either with flat-black spray paint (one that is non-toxic when warmed), black tempera paint, or soot from a fire. The solar box cooker typically reaches a temperature of 150 °C (300 °F). This is not as hot as a standard oven, but still hot enough to cook food over a somewhat longer period of time.\nPanel solar cookers are inexpensive solar cookers that use reflective panels to direct sunlight to a cooking pot that is enclosed in a clear plastic bag.\n\nSolar Oven science experiments are regularly done as projects in high schools and colleges, such as the \"Solar Oven Throwdown\" at the University of Arizona. These projects prove that it is possible to both achieve high temperatures, as well as predict the high temperatures using mathematical models.\n\nParabolic solar cookers concentrate sunlight to a single point. When this point is focused on the bottom of a pot, it can heat the pot quickly to very high temperatures which can often be comparable with the temperatures achieved in gas and charcoal grills. These types of solar cookers are widely used in several regions of the world, most notably in China and India where hundreds of thousands of families currently use parabolic solar cookers for preparing food and heating water. Some parabolic solar cooker projects in China abate between 1-4 tons of carbon dioxide per year and receive carbon credits through the Clean Development Mechanism (CDM) and Gold Standard.\nSome parabolic solar cookers incorporate cutting edge materials and designs which lead to solar energy efficiencies >90%. Others are large enough to feed thousands of people each day, such as the solar bowl at Auroville in India, which makes 2 meals per day for 1,000 people.\n\nIf a reflector is axially symmetrical and shaped so its cross-section is a parabola, it has the property of bringing parallel rays of light (such as sunlight) to a point \"focus\". If the axis of symmetry is aimed at the sun, any object that is located at the focus receives highly concentrated sunlight, and therefore becomes very hot. This is the basis for the use of this kind of reflector for solar cooking.\n\nParaboloids are compound curves, which are more difficult to make with simple equipment than single curves. Although paraboloidal solar cookers can cook as well as or better than a conventional stove, they are difficult to construct by hand. Frequently, these reflectors are made using many small segments that are all single curves which together approximate compound curves.\n\nAlthough paraboloids are difficult to make from flat sheets of solid material, they can be made quite simply by rotating open-topped containers which hold liquids. The top surface of a liquid which is being rotated at constant speed around a vertical axis naturally takes the form of a paraboloid. Centrifugal force causes material to move outward from the axis of rotation until a deep enough depression is formed in the surface for the force to be balanced by the levelling effect of gravity. It turns out that the depression is an exact paraboloid. (See Liquid mirror telescope.) If the material solidifies while it is rotating, the paraboloidal shape is maintained after the rotation stops, and can be used to make a reflector. This rotation technique is sometimes used to make paraboloidal mirrors for astronomical telescopes, and has also been used for solar cookers. Devices for constructing such paraboloids are known as rotating furnaces.\n\nParaboloidal reflectors generate high temperatures and cook quickly, but require frequent adjustment and supervision for safe operation. Several hundred thousand exist, mainly in China. They are especially useful for individual household and large-scale institutional cooking.\n\nA Scheffler cooker (named after its inventor, Wolfgang Scheffler) uses a large ideally paraboloidal reflector which is rotated around an axis that is parallel with the earth's using a mechanical mechanism, turning at 15 degrees per hour to compensate for the earth's rotation. The axis passes through the reflector's centre of mass, allowing the reflector to be turned easily. The cooking vessel is located at the focus which is on the axis of rotation, so the mirror concentrates sunlight onto it all day. The mirror has to be occasionally tilted about a perpendicular axis to compensate for the seasonal variation in the sun's declination. This perpendicular axis does not pass through the cooking vessel. Therefore, if the reflector were a rigid paraboloid, its focus would not remain stationary at the cooking vessel as the reflector tilts. To keep the focus stationary, the reflector's shape has to vary. It remains paraboloidal, but its focal length and other parameters change as it tilts. The Scheffler reflector is therefore flexible, and can be bent to adjust its shape. It is often made up of a large number of small plane sections, such as glass mirrors, joined together by flexible plastic. A framework that supports the reflector includes a mechanism that can be used to tilt it and also bend it appropriately. The mirror is never exactly paraboloidal, but it is always close enough for cooking purposes.\n\nSometimes the rotating reflector is located outdoors and the reflected sunlight passes through an opening in a wall into an indoor kitchen, often a large communal one, where the cooking is done.\n\nParaboloidal reflectors that have their centres of mass coincident with their focal points are useful. They can be easily turned to follow the sun's motions in the sky, rotating about any axis that passes through the focus. Two perpendicular axes can be used, intersecting at the focus, to allow the paraboloid to follow both the sun's daily motion and its seasonal one. The cooking pot stays stationary at the focus. If the paraboloidal reflector is axially symmetrical and is made of material of uniform thickness, its centre of mass coincides with its focus if the depth of the reflector, measured along its axis of symmetry from the vertex to the plane of the rim, is 1.8478 times its focal length. The radius of the rim of the reflector is 2.7187 times the focal length. The angular radius of the rim, as seen from the focal point, is 72.68 degrees.\nParabolic troughs are used to concentrate sunlight for solar-energy purposes. Some solar cookers have been built that use them in the same way. Generally, the trough is aligned with its focal line horizontal and east-west. The food to be cooked is arranged along this line. The trough is pointed so its axis of symmetry aims at the sun at noon. This requires the trough to be tilted up and down as the seasons progress. At the equinoxes, no movement of the trough is needed during the day to track the sun. At other times of year, there is a period of several hours around noon each day when no tracking is needed. Usually, the cooker is used only during this period, so no automatic sun tracking is incorporated into it. This simplicity makes the design attractive, compared with using a paraboloid. Also, being a single curve, the trough reflector is simpler to construct. However, it suffers from lower efficiency.\n\nIt is possible to use two parabolic troughs, curved in perpendicular directions, to bring sunlight to a point focus as does a paraboloidal reflector.The incoming light strikes one of the troughs, which sends it toward a line focus. The second trough intercepts the converging light and focuses it to a point.\n\nCompared with a single paraboloid, using two partial troughs has important advantages. Each trough is a single curve, which can be made simply by bending a flat sheet of metal. Also, the light that reaches the targeted cooking pot is directed approximately downward, which reduces the danger of damage to the eyes of anyone nearby. On the other hand, there are disadvantages. More mirror material is needed, increasing the cost, and the light is reflected by two surfaces instead of one, which inevitably increases the amount that is lost.\n\nThe two troughs are held in a fixed orientation relative to each other by being both fixed to a frame. The whole assembly of frame and troughs has to be moved to track the sun as it moves in the sky. Commercially made cookers that use this method are available.In practical applications (like in car-headlights), concave mirrors are of parabolic shape\n\nSpherical reflectors operate much like paraboloidal reflectors, such that the axis of symmetry is pointed towards the sun so that light is concentrated to a focus. However, the focus of a spherical reflector will not be a point focus because it suffers from a phenomenon known as spherical aberration. Some concentrating dishes (such as satellite dishes) that do not require a precise focus opt for a spherical curvature over a paraboloid. If the radius of the rim of spherical reflector is small compared with the radius of curvature of its surface (the radius of the sphere of which the reflector is a part), the reflector approximates a paraboloidal one with focal length equal to half of the radius of curvature.\n\nEvacuated tube solar cookers are essentially a vacuum sealed between two layers of glass. The vacuum allows the tube to act both as a \"super\" greenhouse and an insulator. The central cooking tube is made from borosilicate glass, which is resistant to thermal shock, and has a vacuum beneath the surface to insulate the interior. The inside of the tube is lined with copper, stainless steel, and aluminum nitrile to better absorb and conduct heat from the sun's rays. Some vacuum tube solar cookers incorporate lightweight designs which allow great portability (such as the GoSun stove)\n\nPortable vacuum tube cookers such as the GoSun allow users to cook freshly caught fish on the beach without needing to light a fire.\n\nAdvantages\n\nDisadvantages\n\nCardboard, aluminium foil, and plastic bags for well over 10,000 solar cookers have been donated to the Iridimi refugee camp and Touloum refugee camps in Chad by the combined efforts of the Jewish World Watch, the Dutch foundation KoZon, and Solar Cookers International. The refugees construct the cookers themselves, using the donated supplies and locally purchased Arabic gum. It has also significantly reduced the amount of time women spend tending open fires each day, with the results that they are healthier and they have more time to grow vegetables for their families and make handicrafts for export. By 2007, the Jewish World Watch had trained 4,500 women and had provided 10,000 solar cookers to refugees. The project has also reduced the number of foraging trips by as much as 70 percent, thus reducing the number of attacks.\n\nSome Gazans have started to make solar cookers made from cement bricks and mud mixed with straw and two sheets of glass. About 40 to 45 Palestinian households reportedly have started using these solar cookers, including some made with mirrors.\n\n"}
{"id": "56426809", "url": "https://en.wikipedia.org/wiki?curid=56426809", "title": "Sulfonyl nitrene", "text": "Sulfonyl nitrene\n\nA sulfonyl nitrene is a chemical compound with generic formula R-SON. Known sulfonyl nitrenes include methyl sulfonyl nitrene, trifluoromethyl sulfonyl nitrene, and tolyl sulfonyl nitrene. Also fluorosufoinyl nitrene FSON exists, but rearranges to FNSO. Preparation of sulfonyl nitrenes can be by heating the sulfonyl azide, (RSON.\n"}
{"id": "19941230", "url": "https://en.wikipedia.org/wiki?curid=19941230", "title": "Tapis crude", "text": "Tapis crude\n\nTapis crude is a Malaysian crude oil used as a pricing benchmark in Singapore. Tapis is very light, with an API gravity of 43°-45°, and very sweet, with only about 0.04% sulfur. While it is not traded on a market like Brent Crude or West Texas Intermediate (WTI), it is often used as an oil marker for Asia and Australia.\n\nThe price of Tapis in Singapore is often considerably higher than the price of benchmark crude oils such as Brent or WTI (those commonly referenced in market commentaries). This is because its greater aromaticity (i.e. higher ° API) allows for greater production of higher-value products, such as petrol, than from Brent or WTI. Its high price is also due to the purity of the blend. Because it contains less sulfur it requires less refinery processing than sourer crude oils such as Brent Oil and WTI.\nMalaysia is the only country which has significant amounts of oil produced as Tapis, from fields located in the South China Sea.\n"}
{"id": "87945", "url": "https://en.wikipedia.org/wiki?curid=87945", "title": "Terminator (solar)", "text": "Terminator (solar)\n\nA terminator or twilight zone is a moving line that divides the daylit side and the dark night side of a planetary body. A terminator is defined as the locus of points on a planet or moon where the line through its parent star is tangent. An observer on the terminator of such an orbiting body with an atmosphere would experience twilight due to light scattering by particles in the gaseous layer.\n\nOn Earth, the terminator is a circle with a diameter that is approximately that of Earth. The terminator passes through any point on Earth's surface twice a day, at sunrise and at sunset, apart from polar regions where this only occurs when the point is not experiencing midnight sun or polar night. The circle separates the portion of Earth experiencing daylight from that experiencing darkness (night). While a little over one half of Earth is illuminated at any point in time (with exceptions during eclipses), the terminator path varies by time of day due to Earth's rotation on its axis. The terminator path also varies by time of year due to Earth's orbital revolution around the Sun; thus, the plane of the terminator is nearly parallel to planes created by lines of longitude during the equinoxes, and its maximum angle is approximately 23.5° to the pole during the solstices.\n\nAt the Equator, under flat conditions (without obstructions like mountains or at a height above any such obstructions), the terminator moves at approximately 1,668 kilometers per hour (1,036 miles per hour). This speed can appear to increase when near obstructions, such as the height of a mountain, as the shadow of the obstruction will be cast over the ground in advance of the terminator along a flat landscape. The speed of the terminator decreases as it approaches the poles, where it can reach a speed of zero (full-day sunlight or darkness).\n\nSupersonic aircraft like jet fighters or Concorde and Tupolev Tu-144 supersonic transports are the only aircraft able to overtake the maximum speed of the terminator at the equator. However, slower vehicles can overtake the terminator at higher latitudes, and it is possible to walk faster than the terminator at the poles, near to the equinoxes. The visual effect is that of seeing the sun rise in the west, or set in the east.\n\nStrength of radio propagation changes between day- and night-side of the ionosphere. This is primarily because the D layer, which absorbs high frequency signals, disappears rapidly on the dark side of the terminator, whereas the E and F layers above the D layer take longer to form. This time-difference puts the ionosphere into a unique intermediate state along the terminator, called the “grey line”.\n\nAmateur radio operators take advantage of conditions along the terminator to perform long distance communications. Called \"gray-line\" or \"grey-line\" propagation, this signal path is a type of skywave propagation. Under good conditions, radio waves can travel along the terminator to antipodal points.\n\nThe lunar terminator is the division between the illuminated and dark hemispheres of the Moon. It is the lunar equivalent of the division between night and day on the Earth spheroid, although the Moon's much lower rate of rotation means it takes longer for it to pass across the surface.\n\nDue to the angle at which sunlight strikes this portion of the Moon, shadows cast by craters and other geological features are elongated, thereby making such features more apparent to the observer. This phenomenon is similar to the lengthening of shadows on Earth when the Sun is low in the sky. For this reason, much lunar photographic study centers on the illuminated area near the lunar terminator, and the resulting shadows, provide accurate descriptions of the terrain.\n\nThe lunar terminator (or tilt) illusion is an optical illusion arising from the erroneous expectation of an observer on Earth that the direction of sunlight illuminating the Moon (i.e. a line perpendicular to the terminator) should correspond with the position of the Sun, but does not appear to do so. The cause of the illusion is simply the observer is not taking into account that the observed slope of a light ray will change across the sky because of the lack of visual clues to establish 3D perspective.\n\nExamination of a terminator can yield information about the surface of a planetary body; for example, the presence of an atmosphere can create a fuzzier terminator. As the particles within an atmosphere are at a higher elevation, the light source can remain visible even after it has set at ground level. These particles scatter the light, reflecting some of it to the ground. Hence, the sky can remain illuminated even after the sun has set.\n\nLow Earth orbit satellites take advantage of the fact that certain polar orbits set near the terminator do not suffer from eclipse, therefore their solar cells are continuously lit by sunlight. Such orbits are called dawn-dusk orbits, a type of Sun-synchronous orbit. This prolongs the operational life of a LEO satellite, as onboard battery life is prolonged. It also enables specific experiments that require minimum interference from the Sun, as the designers can opt to install the relevant sensors on the dark side of the satellite.\n\n\n"}
{"id": "23972169", "url": "https://en.wikipedia.org/wiki?curid=23972169", "title": "The Dinosauria", "text": "The Dinosauria\n\nThe Dinosauria is an extensive book on dinosaurs, compiled by David B. Weishampel, Peter Dodson, and Halszka Osmólska. It has been published in 2 editions, with the first edition published in 1990, consisting of material from 23 scientists. The second, greatly revised edition, was published in 2004, with material from 43 scientists. Both editions were published by University of California Press.\n\nThe book covers a wide range of topics about dinosaurs, including their systematics, anatomy, and history. It has been lauded as \"the best scholarly reference work available on dinosaurs\" and \"an historically unparalleled compendium of information\" and by Padian (1991) as a \"monumental work\" which features the work of 23 dinosaur specialists: \"an instant classic\".\n\n\n"}
{"id": "25957874", "url": "https://en.wikipedia.org/wiki?curid=25957874", "title": "Timeline of volcanism on Earth", "text": "Timeline of volcanism on Earth\n\nThis timeline of volcanism on Earth is a list of major volcanic eruptions of approximately at least magnitude 6 on the Volcanic Explosivity Index (VEI) or equivalent sulfur dioxide emission around the Quaternary period.\n\nSome eruptions cooled the global climate—inducing a volcanic winter—depending on the amount of sulfur dioxide emitted and the magnitude of the eruption. Before the Holocene epoch, the criteria are less strict because of scarce data availability, partly since later eruptions have destroyed the evidence. So, the known large eruptions after the Paleogene period are listed, and especially those relating to the Yellowstone hotspot, the Santorini caldera, and the Taupo Volcanic Zone. Only some eruptions before the Neogene period are listed.\n\nActive volcanoes such as Stromboli, Mount Etna and Kilauea do not appear on this list, but some back-arc basin volcanoes that generated calderas do appear. Some dangerous volcanoes in \"populated areas\" appear many times: so Santorini, six times and Yellowstone hotspot, twenty-one times. The Bismarck volcanic arc, New Britain, and the Taupo Volcanic Zone, New Zealand, appear often too.\n\nIn addition to the events listed below, are many examples of eruptions in the Holocene on the Kamchatka Peninsula, which are described in a supplemental table by Peter Ward.\n\nThe Holocene epoch begins 11,700 years BP (10,000 C years ago).\n\n\nThis is a sortable summary of 27 major eruptions in the last 2000 years with VEI ≥6, implying an average of about 1.3 per century. The count does not include the notable VEI 5 eruptions of Mount St. Helens and Mount Vesuvius. Date uncertainties, tephra volumes, and references are also not included.\n\nNote:\nCaldera names tend to change over time. For example, Okataina Caldera, Haroharo Caldera, Haroharo volcanic complex, Tarawera volcanic complex had the same magma source in the Taupo Volcanic Zone. Yellowstone Caldera, Henry's Fork Caldera, Island Park Caldera, Heise Volcanic Field had all Yellowstone hotspot as magma source.\n\n2.588 ± 0.005 million years BP, the Quaternary period and Pleistocene epoch begin.\n\n\nApproximately 5.332 million years BP, the Pliocene epoch begins. Most eruptions before the Quaternary period have an unknown VEI.\n\n\nApproximately 23.03 million years BP, the Neogene period and Miocene epoch begin.\n\n\n\n \n \n\nThe global dimming through volcanism (ash aerosol and sulfur dioxide) is quite independent of the eruption VEI. When sulfur dioxide (boiling point at standard state: -10 °C) reacts with water vapor, it creates sulfate ions (the precursors to sulfuric acid), which are very reflective; ash aerosol on the other hand absorbs Ultraviolet. Global cooling through volcanism is the sum of the influence of the global dimming and the influence of the high albedo of the deposited ash layer. The lower snow line and its higher albedo might prolong this cooling period. Bipolar comparison showed six sulfate events: Tambora (1815), Cosigüina (1835), Krakatoa (1883), Agung (1963), and El Chichón (1982), and the 1808/1809 mystery eruption. And the atmospheric transmission of direct solar radiation data from the Mauna Loa Observatory (MLO), Hawaii (19°32'N) detected only five eruptions:\n\nBut very large sulfur dioxide emissions overdrive the oxidizing capacity of the atmosphere. Carbon monoxide's and methane's concentration goes up (greenhouse gases), global temperature goes up, ocean's temperature goes up, and ocean's carbon dioxide solubility goes down.\n\n\n"}
{"id": "42047992", "url": "https://en.wikipedia.org/wiki?curid=42047992", "title": "Tokwe Mukorsi Dam", "text": "Tokwe Mukorsi Dam\n\nThe Tokwe Mukorsi Dam is a concrete-face rock-fill dam on the Tokwe River, just downstream of its confluence with the Mukorsi River, about south of Masvingo in Masvingo Province, Zimbabwe. Construction on the dam began in June 1998 but stalled in 2008. Salini Impregilo began to finish the dam in 2011. Heavy flooding in February 2014 caused a partial failure on 4 February, on the downstream face of the dam. By late February the dam had not been fully breached but the unplanned rising reservoir behind the dam caused evacuations upstream. Both upstream and downstream, over 20,000 people were evacuated. Construction of the dam was suspended in June 2014 due to a lack of funding. The power station was expected to be commissioned by the end of 2015. It is tall and creates a reservoir, the largest the in country. The associated hydroelectric power station will have a installed capacity.\n\nIn May 2016 the government released $35 million to Salini Impregilio to enable the Italian contractor resume construction work that stopped two years ago owing to payment problems. The Dam was eventually completed in December 2016 and commissioned in May 2017\n"}
{"id": "35945712", "url": "https://en.wikipedia.org/wiki?curid=35945712", "title": "Topological degeneracy", "text": "Topological degeneracy\n\nTopological degeneracy is a phenomenon in quantum many-body physics, that the ground state of a gapped many-body system becomes degenerate in the large system size limit, and that such a degeneracy cannot be lifted by any local perturbations as long as the system size is large.\n\nTopological degeneracy can be used to protect qubits which allows topological quantum computation.\nIt is believed that topological degeneracy implies topological order (or long-range entanglement ) in the ground state. Many-body states with topological degeneracy are described by topological quantum field theory at low energies.\n\nTopological degeneracy was first introduced to physically define topological order.\nIn two-dimensional space, the topological degeneracy depends on the topology of space, and the topological degeneracy on high genus Riemann surfaces encode all information on the quantum dimensions and the fusion algebra of the quasiparticles. In particular, the topological degeneracy on torus is equal to the number of quasiparticles types.\n\nThe topological degeneracy also appears in the situation with topological defects (such as vortices, dislocations, holes in 2D sample, ends of a 1D sample, etc.), where the topological degeneracy depends on the number of defects. Braiding those topological defect leads to topologically protected non-Abelian geometric phase, which can be used to perform topologically protected quantum computation.\n\nThe topological degeneracy also appear in non-interacting fermion systems (such as p+ip superconductors) with trapped defects (such as vortices). In non-interacting fermion systems, there is only one type of topological degeneracy\nwhere number of the degenerate states is given by formula_1, where\nformula_2 is the number of the defects (such as the number of vortices).\nSuch topological degeneracy is referred as \"Majorana zero-mode\" on the defects.\n\nIn contrast, there are many types of topological degeneracy for interacting systems.\nA systematic description of topological degeneracy is given by tensor category (or monoidal category) theory.\n\n"}
{"id": "8378625", "url": "https://en.wikipedia.org/wiki?curid=8378625", "title": "Vermont Land Trust", "text": "Vermont Land Trust\n\nThe Vermont Land Trust is a non-profit environmental organization in the U.S. state of Vermont, working to conserve productive, recreational, and scenic lands which give the state and its communities their rural character.\n\nThe mission of the Vermont Land Trust is that current and future generations are deeply connected to the land and benefit from its deliberate protection and responsible stewardship. \n\nThe Vermont Land Trust was founded in 1977 by a group of citizens concerned about the rapidly accelerating development that threatened open space in Vermont. The founding group feared that state legislation Act 250 and local zoning was not strong enough to protect the rural character of the state.\n\nThe trust provides the money to purchase undeveloped land when necessary. It then protects the land with a special easement which prevents development. It then sells the land to interested purchasers, which may be the state government. In selling the land, the trust principal is continually renewed.\n\nIt works with The Nature Conservancy.\n\n\n"}
{"id": "27124596", "url": "https://en.wikipedia.org/wiki?curid=27124596", "title": "WELS rating", "text": "WELS rating\n\nWater Efficiency Labelling and Standard (WELS) is a labeling scheme initiated by the Australian Government to help Australian households conserve water and money. On 1 July 2006, it became mandatory across Australia to carry a (WELS) Water Rating label when selling showers, washing machines, dishwashers, toilet equipment, urinal equipment and tap equipment intended for use over kitchen sinks, bathroom basins, laundry tubs or ablution troughs.\n\n\n"}
{"id": "6060598", "url": "https://en.wikipedia.org/wiki?curid=6060598", "title": "Yorkshire and Humber Assembly", "text": "Yorkshire and Humber Assembly\n\nYorkshire and Humber Assembly was the regional chamber for the Yorkshire and the Humber region of England. It closed on 31 March 2009. The responsibilities of the assembly were assumed by a Joint Regional Board consisting of members of Yorkshire Forward, the Regional Development Agency, and Local Government Yorkshire and Humber, a regional partnership of local authorities.\n"}
